{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "#from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2 as cv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time, math\n",
    " \n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import collections\n",
    "import hickle as hkl\n",
    "\n",
    "sys.path.insert(0, '../data/')\n",
    "import get_pid_train_test as db\n",
    "import auxiliary as af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0,'../py-MDNet/modules')\n",
    "# from sample_generator import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def plot_current_state(ped, c,fno):\n",
    "    # load image for current location\n",
    "    img,bb = load_image(ped,c,fno,db_no)\n",
    "\n",
    "    dpi = 80.0\n",
    "    #figsize = (img.size[0]/dpi, img.size[1]/dpi)\n",
    "    figsize = (img.shape[0]/dpi, img.shape[1]/dpi)\n",
    "    fig = plt.figure(frameon=False, figsize=figsize, dpi=dpi)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "\n",
    "    # get image and rect handle\n",
    "    imAX = ax.imshow(img, aspect='normal')\n",
    "    rect = plt.Rectangle(tuple(bb[0,:2]),bb[0,2],bb[0,3], \n",
    "        linewidth=3, edgecolor=\"#ff0000\", zorder=1, fill=False)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    plt.pause(.01)\n",
    "    plt.draw()\n",
    "    #fig.savefig(os.path.join(savefig_dir,'0000.jpg'),dpi=dpi)\n",
    "    \n",
    "    return imAX, rect\n",
    "    \n",
    "def plot_second(ped,c,curr_frame, imAX,rect):\n",
    "    img,bb =  load_image(ped,c,curr_frame,db_no)\n",
    "    #if np.array(img).shape[0] > 0:\n",
    "    if img != []:\n",
    "        imAX.set_data(img)\n",
    "    #print (bb)\n",
    "\n",
    "    #if bb.shape[0] > 0:\n",
    "    if bb != []:\n",
    "        rect.set_xy(bb[0,:2])\n",
    "        rect.set_width(bb[0,2])\n",
    "        rect.set_height(bb[0,3])\n",
    "        print ('Correct camera')\n",
    "    elif c!= num_camera-1:\n",
    "        print ('Wrong camera')\n",
    "\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    plt.pause(1)\n",
    "    plt.draw()\n",
    "    #fig.savefig(os.path.join(savefig_dir,'%04d.jpg'%(i)),dpi=dpi)\n",
    "\n",
    "def get_reward_gt(ped, curr_frame, c):\n",
    "    y = afc.find_target_camera(ped,curr_frame)\n",
    "    # get reward (give reward at end of episode)\n",
    "    if y == num_camera-1 and y == c:\n",
    "        reward = 0\n",
    "    elif y == c:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "        \n",
    "    return reward,y\n",
    "\n",
    "def get_next_step(ped,c,curr_frame, state):\n",
    "    # update current state and history\n",
    "    ispresent,this_state = get_state_vector(ped, c,curr_frame)\n",
    "    if ispresent:\n",
    "        next_state = this_state\n",
    "    else:\n",
    "        # use previous state\n",
    "        next_state = state\n",
    "    \n",
    "    # get correct label from ground truth\n",
    "    reward,y = get_reward_gt(ped, curr_frame,c)\n",
    "\n",
    "    return next_state,reward,y,ispresent\n",
    "\n",
    "def test_func(pTest, iloc='first', eloc='last', fixLoc=-1, isdebug=0, req_inc=1):\n",
    "    policy_net.eval()\n",
    "    rsT,accT = [],[]\n",
    "    Qvalues = []\n",
    "    numTrAllP = []\n",
    "    \n",
    "    for p in range(pTest.shape[0]): \n",
    "        reward_sum = 0\n",
    "        accP = []\n",
    "        inc = 1\n",
    "        aaa = 1\n",
    "        Qval_1p= []\n",
    "        numTr = 0\n",
    "        \n",
    "        # load p'th person data\n",
    "        ped = np.copy(pTest[p])\n",
    "        # camera index and frame index starts from zero\n",
    "        ped[:,0] -= 1\n",
    "        ped[:,1] -= 1\n",
    "        \n",
    "        # Initialize with current state with start frame\n",
    "        if iloc == 'first':\n",
    "            startIDX = 0\n",
    "        elif iloc == 'rand':\n",
    "            startIDX = np.random.randint( 0,ped.shape[0]-20 )\n",
    "        elif iloc == 'fix':\n",
    "            startIDX = fixLoc\n",
    "        if startIDX > ped.shape[0]:\n",
    "            continue\n",
    "        myPos = ped[startIDX,0:]\n",
    "        print ('Initial position: ',myPos)\n",
    "        \n",
    "        curr_camera = myPos[0]\n",
    "        curr_frame = myPos[1]\n",
    "        \n",
    "        # Initialize history variable (one-hot encoding)\n",
    "        ch = np.zeros((h_len,cityflow_cam))\n",
    "        occ_len = 0.01\n",
    "        # Make initial state\n",
    "        x_t,c_t,te_tau,r_t = make_state_vector(ped, curr_camera,curr_frame,ch,occ_len)\n",
    "        prev_rt = r_t[0:4]\n",
    "        #print (state.size())\n",
    "        num_steps = 0\n",
    "        prev_camera = curr_camera\n",
    "        count_curr_c = 0\n",
    "        \n",
    "        if render: # show current location\n",
    "            plot_current_state(ped, curr_camera,curr_frame)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "        # select an action from the current state\n",
    "        hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "        #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "        state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "        #state = torch.cat([state_xt, hidden.detach().flatten().view(1,-1)], dim=1)\n",
    "        state = torch.cat([state_xt, hidden[1,].detach()], dim=1)\n",
    "        \n",
    "        while(curr_frame <= ped[-1,1]): # alltime-6):\n",
    "            \n",
    "            if use_cuda:\n",
    "                state_in = Variable(state)\n",
    "                value_c = policy_net(state_in)\n",
    "            else:\n",
    "                state_in = Variable(state)\n",
    "                value_c = policy_net(state_in)\n",
    "                \n",
    "            # Only exploitation for testing\n",
    "            camera_index = torch.argmax(value_c)\n",
    "            c = camera_index.detach().cpu().numpy()\n",
    "            \n",
    "            occ_max_val = 12000000\n",
    "            aaa += 1\n",
    "            if aaa > 1 and occ_len > occ_max_val:\n",
    "                c = c #np.array(num_camera-1)\n",
    "            if occ_len > occ_max_val and aaa%50 == 0:\n",
    "                aaa = 1\n",
    "                c = np.array(np.random.randint(num_camera))\n",
    "\n",
    "            # find target for the next frame\n",
    "            curr_frame += fpsc\n",
    "            num_steps += 1\n",
    "            \n",
    "            # get correct label from ground truth\n",
    "            reward,y = get_reward_gt(ped, curr_frame,c)\n",
    "            #if req_inc:\n",
    "            if inc==1 and y!=num_camera-1:\n",
    "                # inside a camera\n",
    "                if req_inc:\n",
    "                    accP.append((y,y))\n",
    "                    c = y\n",
    "                else:\n",
    "                    accP.append((y,c.item(0)))\n",
    "            elif inc==0 and y==c.item(0) and y!=num_camera-1:\n",
    "                # transitioning to second camera\n",
    "                accP.append((y,c.item(0)))\n",
    "                inc = 1\n",
    "                numTr += 1\n",
    "            elif inc==1 and y==num_camera-1:\n",
    "                # moving out of a camera FOV\n",
    "                inc = 0\n",
    "                accP.append((y,c.item(0)))\n",
    "            else:\n",
    "                # Making transition\n",
    "                accP.append((y,c.item(0)))\n",
    "                #print ('Another case',y,c.item(0))\n",
    "                    \n",
    "            #else:\n",
    "            #    accP.append((y,c.item(0)))\n",
    "            \n",
    "            # get the current bounding box\n",
    "            bbox = ped[ np.logical_and(ped[:,0]==c,ped[:,1]==curr_frame),2:]\n",
    "            if bbox.shape[0] > 0: # and np.random.rand < 0.95:\n",
    "                bbox = bbox[0]\n",
    "                rt = np.zeros((8))\n",
    "                rt[0] = bbox[0]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[1] = bbox[1]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[2] = bbox[2]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[3] = bbox[3]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[4] = rt[0] - prev_rt[0] if occ_len < 0.2 else 0\n",
    "                rt[5] = rt[1] - prev_rt[1] if occ_len < 0.2 else 0\n",
    "                rt[6] = rt[2] - prev_rt[2] if occ_len < 0.2 else 0\n",
    "                rt[7] = rt[3] - prev_rt[3] if occ_len < 0.2 else 0\n",
    "                curr_camera = c\n",
    "                \n",
    "                # make next_state vector\n",
    "                this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "                x_t = np.concatenate((this_cam, rt.ravel()))\n",
    "                x_t[x_t==0] = -10\n",
    "                x_t[x_t==1] = 10\n",
    "                x_t = x_t.reshape(1,-1)\n",
    "                if use_cuda:\n",
    "                    x_t = torch.from_numpy(x_t).float().cuda()\n",
    "                    \n",
    "                ispresent = 1\n",
    "                prev_rt = rt[0:4]\n",
    "                    \n",
    "            else:\n",
    "                ispresent = 0\n",
    "                \n",
    "            # count the time of prev_camera selection\n",
    "            if ispresent:\n",
    "                occ_len = 0.01\n",
    "            else:\n",
    "                occ_len += 1\n",
    "            #hcount = np.array(-occ_max_val + (occ_len/500)*(occ_max_val-(-occ_max_val)))\n",
    "            hcount = np.array(10*np.log(occ_len))\n",
    "            \n",
    "            # update current state and history\n",
    "            ch[1:,] = ch[0:-1,]\n",
    "            ch[0,0:num_camera] = afc.make_one_hot_camera(c)\n",
    "            ch[0,num_camera:] = 0\n",
    "            \n",
    "            this_cam = afc.make_one_hot_camera(c)\n",
    "            c_t = this_cam.reshape(1,-1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                c_t = torch.from_numpy(c_t).float().cuda()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float().cuda()\n",
    "            else:\n",
    "                c_t = torch.from_numpy(c_t).float()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float()\n",
    "                \n",
    "            if isdebug:\n",
    "                print ( np.where(rt.ravel()))\n",
    "                print ( np.where(ch))\n",
    "                print (c, curr_frame)\n",
    "                print ('isPresent', ispresent)\n",
    "                \n",
    "            # make next_state vector\n",
    "            hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "            #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "            next_state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "            #next_state = torch.cat([next_state_xt, hidden.detach().flatten().view(1,-1)], dim=1)\n",
    "            next_state = torch.cat([next_state_xt, hidden[1,].detach()], dim=1)\n",
    "            \n",
    "            # store current reward\n",
    "            reward_sum += reward\n",
    "            Qval_1p.append((list(value_c.detach().cpu().numpy()[0]),hcount.ravel()[0],reward,False,y,c,state.detach().cpu().numpy()))\n",
    "                        \n",
    "            #state = next_state\n",
    "            #state_xt = next_state_xt\n",
    "            state = next_state #torch.cat([state_xt, enc_history], dim=1)\n",
    "            prev_camera = c\n",
    "            \n",
    "            if render:\n",
    "                plot_second()\n",
    "            if eloc != 'last':\n",
    "                if num_steps > eloc:\n",
    "                    break\n",
    "            \n",
    "        # stack episodic reward \n",
    "        Qvalues.append((np.stack(Qval_1p)))\n",
    "        rsT.append((reward_sum,num_steps))\n",
    "        accT.append(accP)\n",
    "        numTrAllP.append(numTr)\n",
    "        \n",
    "    return rsT, accT, Qvalues, numTrAllP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 1500\n",
    "replay_memory_size = 20000\n",
    "#epsilon = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n",
    "eps = np.finfo(np.float32).eps.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of person in data set:  (1, 49)\n",
      "Total number of person in data set:  (1, 49)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "db_no = 4\n",
    "[pALL,num_camera,alltime,fps] = db.get_pid(set_no=db_no, train_flag='train')\n",
    "num_camera += 1  # occlusion is also considered as a FOV\n",
    "fpsc = 2\n",
    "pALL = np.array(pALL)\n",
    "\n",
    "# load test set for current data set\n",
    "[pTest,num_camera,alltime,fps] = db.get_pid(set_no=db_no, train_flag='test')\n",
    "num_camera += 1  # occlusion is also considered as a FOV\n",
    "pTest = np.array(pTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpoch = 100000\n",
    "d = 10\n",
    "region_size = (d,d)\n",
    "\n",
    "h_len = 10\n",
    "\n",
    "# Load auxiliary functions using an object\n",
    "afc = af.AuxiliaryFunction(num_camera=num_camera, d=d, h_len=h_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required parameters\n",
    "lstm_size = 256\n",
    "hidden_size1 = 4096\n",
    "hidden_size2 = 2048\n",
    "hidden_size3 = 256\n",
    "\n",
    "input_size = lstm_size + num_camera+ 4*2 +1\n",
    "\n",
    "# Required network\n",
    "class NextCamera(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NextCamera, self).__init__()\n",
    "        \n",
    "        # make decoder layers\n",
    "        self.fch1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fch2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fch3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fco = nn.Linear(hidden_size3, num_camera)\n",
    "        \n",
    "        # Activation function \n",
    "        self.tanh = nn.Tanh() #ReLU()\n",
    "        self.relu = nn.ReLU() #ReLU()\n",
    "        #self.linear = nn.Linear() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.fch1(x))\n",
    "        x = self.relu(self.fch2(x))\n",
    "        x = self.relu(self.fch3(x))\n",
    "        x = self.fco(x)\n",
    "            \n",
    "        return x # nn.functional.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "if use_cuda:\n",
    "    policy_net = NextCamera().float().cuda()\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    \n",
    "else:\n",
    "    policy_net = NextCamera().float()\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "if use_cuda:\n",
    "    target_net = NextCamera().cuda()\n",
    "    target_net.float().cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# use ADAM as optimizer since we can load the whole data to train\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_network(replay_memory_pos,pos_prob, replay_memory_neg, replay_memory_cx, update_criteria):\n",
    "\n",
    "    # sample random minibatch\n",
    "    minibatch_pos = random.choices(replay_memory_pos, k=min(len(replay_memory_pos), 500), weights=pos_prob)\n",
    "    #minibatch_pos = random.sample(replay_memory_pos, min(len(replay_memory_pos), 300)) #int(batch_size/3)))\n",
    "    minibatch_posneg = minibatch_pos + random.sample(replay_memory_neg, min(len(replay_memory_neg), 500)) # int(batch_size/3)))\n",
    "    minibatch = minibatch_posneg + random.sample(replay_memory_cx, min(len(replay_memory_cx), 500)) #int(batch_size/3)))\n",
    "    \n",
    "    # unpack minibatch\n",
    "    #state_xt = tuple(d[0] for d in minibatch)\n",
    "    state = torch.cat(tuple(d[0] for d in minibatch))\n",
    "    #prev_ch = tuple(d[1] for d in minibatch)\n",
    "    action = torch.cat(tuple(d[1] for d in minibatch))\n",
    "    reward = torch.cat(tuple(d[2] for d in minibatch))\n",
    "    #next_state_xt = tuple(d[4] for d in minibatch)\n",
    "    next_state = torch.cat(tuple(d[3] for d in minibatch))\n",
    "    #ch = tuple(d[5] for d in minibatch)\n",
    "    \n",
    "    # num samples of different categories\n",
    "    numRew = torch.stack([torch.sum(reward>=0.2),torch.sum(reward==-1),torch.sum(reward==0.01)]).data.cpu().numpy()\n",
    "    \n",
    "    # get output for the next state\n",
    "    next_output = target_net(next_state)\n",
    "\n",
    "    # set y_j to r_j for terminal state, otherwise to r_j + gamma*max(Q)\n",
    "    y = torch.cat(tuple(reward[i] if minibatch[i][4] \\\n",
    "                        else reward[i] + gamma * torch.max(next_output[i]) \\\n",
    "                        for i in range(len(minibatch))))\n",
    "\n",
    "    # extract Q-value\n",
    "    q_value = torch.sum(policy_net(state) * action, dim=1)\n",
    "\n",
    "    # PyTorch accumulates gradients by default, so they need to be reset in each pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # returns a new Tensor, detached from the current graph, the result will never require gradient\n",
    "    y = y.detach()\n",
    "\n",
    "    #print (y, q_value)\n",
    "    # calculate loss\n",
    "    loss = criterion(q_value, y)\n",
    "\n",
    "    # do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # copy weights from policy_net to target_net\n",
    "    if update_criteria == 10:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        update_criteria = 0\n",
    "    update_criteria += 1\n",
    "    \n",
    "    return loss.data,numRew,update_criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = src #self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size, dim]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size, dim]\n",
    "        \n",
    "        embedded = input #self.dropout(self.embedding(input))\n",
    "        #embedded[np.arange(embedded.size),a] = 1\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.1):\n",
    "        \n",
    "        #src = [src len, batch size, dim]\n",
    "        #trg = [trg len, batch size, dim]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output #.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityflow_cam = 41\n",
    "INPUT_DIM = cityflow_cam\n",
    "OUTPUT_DIM = cityflow_cam\n",
    "ENC_EMB_DIM = cityflow_cam\n",
    "DEC_EMB_DIM = cityflow_cam\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).float().cuda()\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).float().cuda()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Seq2Seq(enc, dec, device).float().to(device)\n",
    "criterion_ae = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(41, 41)\n",
       "    (rnn): LSTM(41, 256, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(41, 41)\n",
       "    (rnn): LSTM(41, 256, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=256, out_features=41, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# laod encoder model\n",
    "model.load_state_dict(torch.load('tut1-model_AICity_lstmSize256_manyDB.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "running_reward = None\n",
    "xs,rs,cprs = [],[],[]\n",
    "episode_number = 0\n",
    "episode_durations = []\n",
    "episode_reward = []\n",
    "validation_reward= []\n",
    "replay_memory_pos = []\n",
    "pos_prob = []\n",
    "replay_memory_neg = []\n",
    "replay_memory_cx = []\n",
    "M = np.zeros((num_camera,num_camera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state_vector(ped, curr_camera,curr_frame, ch,occ_len):\n",
    "    numSamples = 30\n",
    "    overlap_thres = [0.9, 1]\n",
    "        \n",
    "    # read image\n",
    "    img,bbox,p = afc.load_image(ped,curr_camera,curr_frame,db_no)\n",
    "    imw, imh = (320,240) #img.size\n",
    "    hc = np.array(10*np.log(occ_len))\n",
    "    \n",
    "    if p:\n",
    "        rt = np.zeros((8))\n",
    "        rt[0] = bbox[0]/imw -(np.random.rand()-0.5)/100\n",
    "        rt[1] = bbox[1]/imh -(np.random.rand()-0.5)/100\n",
    "        rt[2] = bbox[2]/imw -(np.random.rand()-0.5)/100\n",
    "        rt[3] = bbox[3]/imh -(np.random.rand()-0.5)/100\n",
    "        rt[4] = 0\n",
    "        rt[5] = 0\n",
    "        rt[6] = 0\n",
    "        rt[7] = 0\n",
    "        #print (np.where(rt.ravel()))\n",
    "        \n",
    "        # make next_state vector\n",
    "        #this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "        #state = np.concatenate((this_cam, rt.ravel()))\n",
    "        #state = np.concatenate((state, hc.ravel()))\n",
    "        #state = np.concatenate((state, ch.ravel()))\n",
    "        #state = state.reshape(1,-1)\n",
    "        #state[state==0] = -10\n",
    "        #state[state==1] = 10\n",
    "        \n",
    "        # make next_state vector\n",
    "        this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "        xt = np.concatenate((this_cam, rt.ravel()))\n",
    "        xt[xt==0] = -10\n",
    "        xt[xt==1] = 10\n",
    "        xt = xt.reshape(1,-1)\n",
    "        \n",
    "        # make history vector\n",
    "        c_t = this_cam.reshape(1,-1)\n",
    "        \n",
    "        if use_cuda:\n",
    "            xt = torch.from_numpy(xt).float().cuda()\n",
    "            c_t = torch.from_numpy(c_t).float().cuda()\n",
    "            hc = torch.from_numpy(hc.reshape(1,-1)).float().cuda()\n",
    "        else:\n",
    "            xt = torch.from_numpy(xt).float()\n",
    "            c_t = torch.from_numpy(c_t).float()\n",
    "            hc = torch.from_numpy(hc.reshape(1,-1)).float()\n",
    "    else:\n",
    "        print ('Target is not present in ',c,curr_frame)\n",
    "        xt,h_t = [],[]\n",
    "    \n",
    "    return xt,c_t,hc,rt #p,state,rt\n",
    "\n",
    "def append_reward(rs,num_steps):\n",
    "    if len(rs) > 0:\n",
    "        # stack episodic reward \n",
    "        epR = np.vstack(rs)\n",
    "        rs = []\n",
    "\n",
    "        # append the episodic reward\n",
    "        #episode_number += 1\n",
    "        #episode_durations.append(num_steps)\n",
    "        reward_stat = [num_steps,np.std(epR),np.sum(epR)]\n",
    "        episode_reward.append(reward_stat)\n",
    "    \n",
    "    return rs\n",
    "\n",
    "def reinit_ae(ch):\n",
    "    # Initialize history variable (one-hot encoding)\n",
    "    if use_cuda:\n",
    "        ch = torch.from_numpy(ch).float().cuda()\n",
    "        enc_h = torch.zeros(1,lstm_size).float().cuda()\n",
    "        enc_c = torch.zeros(1,lstm_size).float().cuda()\n",
    "    else:\n",
    "        enc_h = torch.zeros(1,lstm_size).float()\n",
    "        enc_c = torch.zeros(1,lstm_size).float()\n",
    "        \n",
    "    # encode whole camera history\n",
    "    for i in range(seq_len-1,-1,-1):\n",
    "        #print (ch[i,:])\n",
    "        x = ch[i,:].view(1,-1)\n",
    "        h_lstm,enc = ae_enc((enc_h,enc_c), x)\n",
    "        (enc_h,enc_c) = h_lstm\n",
    "\n",
    "    return h_lstm,enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "occ_max_val = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpData = []\n",
    "allEpData = []\n",
    "numRew=[]\n",
    "\n",
    "numUpdateRew=[]\n",
    "update_criteria = 0\n",
    "episode_count = 0\n",
    "steps_count = 0\n",
    "initialEpsilon = 0.170962 #0.2\n",
    "finalEpsilon = 0.01\n",
    "epsilon = initialEpsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trExplored = {}\n",
    "for i in range(num_camera):\n",
    "    for j in range(num_camera):\n",
    "        trExplored[str(i)+'-'+str(j)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "if resume:\n",
    "    epoch = 600\n",
    "    epsilon = 0.170962\n",
    "    aaa = np.load('./EpData/.npy', allow_pickle=True)\n",
    "    episode_count = aaa[2]\n",
    "    steps_count = aaa[3]\n",
    "    episode_reward_pre = aaa[0]\n",
    "    validation_reward_pre = aaa[1]\n",
    "    policy_net.load_state_dict(torch.load('./models/')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1544: ep_len:1190 episode reward: total was -35.810000. running mean: -12.233972\n",
      "ep 1544: ep_len:500 episode reward: total was 4.900000. running mean: -12.062632\n",
      "ep 1544: ep_len:53 episode reward: total was 23.500000. running mean: -11.707006\n",
      "ep 1544: ep_len:2930 episode reward: total was -73.310000. running mean: -12.323036\n",
      "ep 1544: ep_len:808 episode reward: total was -15.930000. running mean: -12.359105\n",
      "ep 1544: ep_len:43 episode reward: total was 20.000000. running mean: -12.035514\n",
      "ep 1544: ep_len:607 episode reward: total was 32.690000. running mean: -11.588259\n",
      "ep 1544: ep_len:347 episode reward: total was 19.110000. running mean: -11.281277\n",
      "ep 1544: ep_len:710 episode reward: total was -21.770000. running mean: -11.386164\n",
      "ep 1544: ep_len:736 episode reward: total was 13.200000. running mean: -11.140302\n",
      "ep 1544: ep_len:893 episode reward: total was 29.640000. running mean: -10.732499\n",
      "ep 1544: ep_len:183 episode reward: total was 90.000000. running mean: -9.725174\n",
      "ep 1544: ep_len:1469 episode reward: total was -5.490000. running mean: -9.682823\n",
      "ep 1544: ep_len:2827 episode reward: total was 5.820000. running mean: -9.527794\n",
      "epsilon:0.009992 episode_count: 23308. steps_count: 24940961.000000\n",
      "ep 1545: ep_len:1007 episode reward: total was -52.720000. running mean: -9.959716\n",
      "ep 1545: ep_len:669 episode reward: total was -18.730000. running mean: -10.047419\n",
      "ep 1545: ep_len:77 episode reward: total was 35.500000. running mean: -9.591945\n",
      "ep 1545: ep_len:2998 episode reward: total was -56.360000. running mean: -10.059626\n",
      "ep 1545: ep_len:632 episode reward: total was -3.100000. running mean: -9.990029\n",
      "ep 1545: ep_len:68 episode reward: total was 29.500000. running mean: -9.595129\n",
      "ep 1545: ep_len:125 episode reward: total was 61.000000. running mean: -8.889178\n",
      "ep 1545: ep_len:58 episode reward: total was 26.000000. running mean: -8.540286\n",
      "ep 1545: ep_len:679 episode reward: total was -15.660000. running mean: -8.611483\n",
      "ep 1545: ep_len:636 episode reward: total was 9.230000. running mean: -8.433068\n",
      "ep 1545: ep_len:636 episode reward: total was -49.360000. running mean: -8.842338\n",
      "ep 1545: ep_len:718 episode reward: total was -8.320000. running mean: -8.837114\n",
      "ep 1545: ep_len:650 episode reward: total was 3.510000. running mean: -8.713643\n",
      "ep 1545: ep_len:76 episode reward: total was 35.000000. running mean: -8.276507\n",
      "ep 1545: ep_len:608 episode reward: total was 0.860000. running mean: -8.185142\n",
      "ep 1545: ep_len:46 episode reward: total was 18.500000. running mean: -7.918290\n",
      "epsilon:0.009992 episode_count: 23324. steps_count: 24950644.000000\n",
      "ep 1546: ep_len:865 episode reward: total was 31.130000. running mean: -7.527807\n",
      "ep 1546: ep_len:897 episode reward: total was 25.510000. running mean: -7.197429\n",
      "ep 1546: ep_len:36 episode reward: total was 16.500000. running mean: -6.960455\n",
      "ep 1546: ep_len:3102 episode reward: total was -40.260000. running mean: -7.293450\n",
      "ep 1546: ep_len:774 episode reward: total was 31.840000. running mean: -6.902116\n",
      "ep 1546: ep_len:49 episode reward: total was 21.500000. running mean: -6.618095\n",
      "ep 1546: ep_len:1414 episode reward: total was -25.420000. running mean: -6.806114\n",
      "ep 1546: ep_len:3624 episode reward: total was -87.260000. running mean: -7.610653\n",
      "ep 1546: ep_len:1266 episode reward: total was -77.340000. running mean: -8.307946\n",
      "ep 1546: ep_len:709 episode reward: total was 38.040000. running mean: -7.844467\n",
      "ep 1546: ep_len:642 episode reward: total was -18.860000. running mean: -7.954622\n",
      "ep 1546: ep_len:117 episode reward: total was 55.500000. running mean: -7.320076\n",
      "ep 1546: ep_len:29 episode reward: total was 13.000000. running mean: -7.116875\n",
      "ep 1546: ep_len:637 episode reward: total was -2.890000. running mean: -7.074606\n",
      "ep 1546: ep_len:2833 episode reward: total was -42.970000. running mean: -7.433560\n",
      "epsilon:0.009992 episode_count: 23339. steps_count: 24967638.000000\n",
      "ep 1547: ep_len:756 episode reward: total was -97.650000. running mean: -8.335725\n",
      "ep 1547: ep_len:698 episode reward: total was -107.430000. running mean: -9.326667\n",
      "ep 1547: ep_len:2868 episode reward: total was -66.980000. running mean: -9.903201\n",
      "ep 1547: ep_len:788 episode reward: total was -0.710000. running mean: -9.811269\n",
      "ep 1547: ep_len:78 episode reward: total was 36.000000. running mean: -9.353156\n",
      "ep 1547: ep_len:52 episode reward: total was 23.000000. running mean: -9.029624\n",
      "ep 1547: ep_len:854 episode reward: total was 45.200000. running mean: -8.487328\n",
      "ep 1547: ep_len:3640 episode reward: total was -123.070000. running mean: -9.633155\n",
      "ep 1547: ep_len:552 episode reward: total was 8.140000. running mean: -9.455423\n",
      "ep 1547: ep_len:714 episode reward: total was 48.680000. running mean: -8.874069\n",
      "ep 1547: ep_len:1504 episode reward: total was -0.650000. running mean: -8.791828\n",
      "ep 1547: ep_len:43 episode reward: total was 18.500000. running mean: -8.518910\n",
      "ep 1547: ep_len:75 episode reward: total was 36.000000. running mean: -8.073721\n",
      "ep 1547: ep_len:632 episode reward: total was -4.960000. running mean: -8.042584\n",
      "ep 1547: ep_len:2843 episode reward: total was -3.470000. running mean: -7.996858\n",
      "ep 1547: ep_len:38 episode reward: total was 16.000000. running mean: -7.756889\n",
      "epsilon:0.009992 episode_count: 23355. steps_count: 24983773.000000\n",
      "ep 1548: ep_len:1111 episode reward: total was 0.840000. running mean: -7.670920\n",
      "ep 1548: ep_len:743 episode reward: total was 11.900000. running mean: -7.475211\n",
      "ep 1548: ep_len:103 episode reward: total was 48.500000. running mean: -6.915459\n",
      "ep 1548: ep_len:711 episode reward: total was 23.210000. running mean: -6.614205\n",
      "ep 1548: ep_len:42 episode reward: total was 18.000000. running mean: -6.368063\n",
      "ep 1548: ep_len:88 episode reward: total was 42.500000. running mean: -5.879382\n",
      "ep 1548: ep_len:57 episode reward: total was 25.500000. running mean: -5.565588\n",
      "ep 1548: ep_len:1051 episode reward: total was -57.330000. running mean: -6.083232\n",
      "ep 1548: ep_len:3551 episode reward: total was -52.150000. running mean: -6.543900\n",
      "ep 1548: ep_len:803 episode reward: total was -29.280000. running mean: -6.771261\n",
      "ep 1548: ep_len:888 episode reward: total was 55.480000. running mean: -6.148748\n",
      "ep 1548: ep_len:2329 episode reward: total was -780.690000. running mean: -13.894161\n",
      "ep 1548: ep_len:90 episode reward: total was 40.500000. running mean: -13.350219\n",
      "ep 1548: ep_len:145 episode reward: total was 69.500000. running mean: -12.521717\n",
      "ep 1548: ep_len:38 episode reward: total was 16.000000. running mean: -12.236500\n",
      "ep 1548: ep_len:1105 episode reward: total was -4.970000. running mean: -12.163835\n",
      "ep 1548: ep_len:2849 episode reward: total was -9.540000. running mean: -12.137596\n",
      "ep 1548: ep_len:65 episode reward: total was 31.000000. running mean: -11.706220\n",
      "epsilon:0.009992 episode_count: 23373. steps_count: 24999542.000000\n",
      "ep 1549: ep_len:682 episode reward: total was -5.890000. running mean: -11.648058\n",
      "ep 1549: ep_len:191 episode reward: total was -3.210000. running mean: -11.563678\n",
      "ep 1549: ep_len:2817 episode reward: total was -21.030000. running mean: -11.658341\n",
      "ep 1549: ep_len:523 episode reward: total was -15.140000. running mean: -11.693158\n",
      "ep 1549: ep_len:94 episode reward: total was 44.000000. running mean: -11.136226\n",
      "ep 1549: ep_len:85 episode reward: total was 41.000000. running mean: -10.614864\n",
      "ep 1549: ep_len:1116 episode reward: total was -8.200000. running mean: -10.590715\n",
      "ep 1549: ep_len:310 episode reward: total was 11.880000. running mean: -10.366008\n",
      "ep 1549: ep_len:3150 episode reward: total was -507.000000. running mean: -15.332348\n",
      "ep 1549: ep_len:766 episode reward: total was 16.560000. running mean: -15.013424\n",
      "ep 1549: ep_len:728 episode reward: total was 19.700000. running mean: -14.666290\n",
      "ep 1549: ep_len:89 episode reward: total was 43.000000. running mean: -14.089627\n",
      "ep 1549: ep_len:154 episode reward: total was 72.500000. running mean: -13.223731\n",
      "ep 1549: ep_len:46 episode reward: total was 21.500000. running mean: -12.876494\n",
      "ep 1549: ep_len:1526 episode reward: total was 18.790000. running mean: -12.559829\n",
      "ep 1549: ep_len:2792 episode reward: total was 5.140000. running mean: -12.382830\n",
      "ep 1549: ep_len:49 episode reward: total was 21.500000. running mean: -12.044002\n",
      "epsilon:0.009992 episode_count: 23390. steps_count: 25014660.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1550: ep_len:1103 episode reward: total was -3.280000. running mean: -11.956362\n",
      "ep 1550: ep_len:742 episode reward: total was -30.060000. running mean: -12.137398\n",
      "ep 1550: ep_len:77 episode reward: total was 37.000000. running mean: -11.646024\n",
      "ep 1550: ep_len:2972 episode reward: total was -55.690000. running mean: -12.086464\n",
      "ep 1550: ep_len:500 episode reward: total was -48.260000. running mean: -12.448200\n",
      "ep 1550: ep_len:59 episode reward: total was 28.000000. running mean: -12.043718\n",
      "ep 1550: ep_len:642 episode reward: total was 1.200000. running mean: -11.911280\n",
      "ep 1550: ep_len:3662 episode reward: total was -94.470000. running mean: -12.736868\n",
      "ep 1550: ep_len:602 episode reward: total was -10.310000. running mean: -12.712599\n",
      "ep 1550: ep_len:850 episode reward: total was 10.770000. running mean: -12.477773\n",
      "ep 1550: ep_len:669 episode reward: total was 2.480000. running mean: -12.328195\n",
      "ep 1550: ep_len:716 episode reward: total was -42.500000. running mean: -12.629913\n",
      "ep 1550: ep_len:2797 episode reward: total was -1.860000. running mean: -12.522214\n",
      "epsilon:0.009992 episode_count: 23403. steps_count: 25030051.000000\n",
      "ep 1551: ep_len:748 episode reward: total was -102.780000. running mean: -13.424792\n",
      "ep 1551: ep_len:785 episode reward: total was 0.610000. running mean: -13.284444\n",
      "ep 1551: ep_len:3065 episode reward: total was -7.970000. running mean: -13.231300\n",
      "ep 1551: ep_len:625 episode reward: total was -4.210000. running mean: -13.141087\n",
      "ep 1551: ep_len:41 episode reward: total was 19.000000. running mean: -12.819676\n",
      "ep 1551: ep_len:86 episode reward: total was 41.500000. running mean: -12.276479\n",
      "ep 1551: ep_len:1828 episode reward: total was -36.240000. running mean: -12.516114\n",
      "ep 1551: ep_len:679 episode reward: total was 22.300000. running mean: -12.167953\n",
      "ep 1551: ep_len:861 episode reward: total was 19.980000. running mean: -11.846474\n",
      "ep 1551: ep_len:843 episode reward: total was 64.000000. running mean: -11.088009\n",
      "ep 1551: ep_len:877 episode reward: total was 34.010000. running mean: -10.637029\n",
      "ep 1551: ep_len:53 episode reward: total was 25.000000. running mean: -10.280658\n",
      "ep 1551: ep_len:25 episode reward: total was 11.000000. running mean: -10.067852\n",
      "ep 1551: ep_len:96 episode reward: total was 45.000000. running mean: -9.517173\n",
      "ep 1551: ep_len:605 episode reward: total was 8.030000. running mean: -9.341702\n",
      "ep 1551: ep_len:47 episode reward: total was 22.000000. running mean: -9.028285\n",
      "ep 1551: ep_len:43 episode reward: total was 18.500000. running mean: -8.753002\n",
      "epsilon:0.009992 episode_count: 23420. steps_count: 25041358.000000\n",
      "ep 1552: ep_len:657 episode reward: total was 11.490000. running mean: -8.550572\n",
      "ep 1552: ep_len:1142 episode reward: total was -10.730000. running mean: -8.572366\n",
      "ep 1552: ep_len:63 episode reward: total was 30.000000. running mean: -8.186642\n",
      "ep 1552: ep_len:2980 episode reward: total was -11.150000. running mean: -8.216276\n",
      "ep 1552: ep_len:1237 episode reward: total was -70.620000. running mean: -8.840313\n",
      "ep 1552: ep_len:500 episode reward: total was -18.970000. running mean: -8.941610\n",
      "ep 1552: ep_len:625 episode reward: total was 23.990000. running mean: -8.612294\n",
      "ep 1552: ep_len:654 episode reward: total was -28.580000. running mean: -8.811971\n",
      "ep 1552: ep_len:837 episode reward: total was 36.350000. running mean: -8.360351\n",
      "ep 1552: ep_len:1129 episode reward: total was -16.150000. running mean: -8.438248\n",
      "ep 1552: ep_len:57 episode reward: total was 27.000000. running mean: -8.083865\n",
      "ep 1552: ep_len:601 episode reward: total was 1.220000. running mean: -7.990827\n",
      "ep 1552: ep_len:2894 episode reward: total was 9.310000. running mean: -7.817818\n",
      "epsilon:0.009992 episode_count: 23433. steps_count: 25054734.000000\n",
      "ep 1553: ep_len:1397 episode reward: total was 11.840000. running mean: -7.621240\n",
      "ep 1553: ep_len:679 episode reward: total was -5.860000. running mean: -7.603628\n",
      "ep 1553: ep_len:3009 episode reward: total was -12.980000. running mean: -7.657391\n",
      "ep 1553: ep_len:1247 episode reward: total was 7.140000. running mean: -7.509418\n",
      "ep 1553: ep_len:53 episode reward: total was 25.000000. running mean: -7.184323\n",
      "ep 1553: ep_len:111 episode reward: total was 54.000000. running mean: -6.572480\n",
      "ep 1553: ep_len:1108 episode reward: total was -2.220000. running mean: -6.528955\n",
      "ep 1553: ep_len:329 episode reward: total was 19.600000. running mean: -6.267666\n",
      "ep 1553: ep_len:665 episode reward: total was -29.110000. running mean: -6.496089\n",
      "ep 1553: ep_len:777 episode reward: total was -2.560000. running mean: -6.456728\n",
      "ep 1553: ep_len:629 episode reward: total was -6.710000. running mean: -6.459261\n",
      "ep 1553: ep_len:61 episode reward: total was 27.500000. running mean: -6.119668\n",
      "ep 1553: ep_len:42 episode reward: total was 19.500000. running mean: -5.863472\n",
      "ep 1553: ep_len:1157 episode reward: total was -125.010000. running mean: -7.054937\n",
      "ep 1553: ep_len:2897 episode reward: total was -34.190000. running mean: -7.326288\n",
      "ep 1553: ep_len:47 episode reward: total was 20.500000. running mean: -7.048025\n",
      "epsilon:0.009992 episode_count: 23449. steps_count: 25068942.000000\n",
      "ep 1554: ep_len:1149 episode reward: total was -32.510000. running mean: -7.302644\n",
      "ep 1554: ep_len:1588 episode reward: total was -49.420000. running mean: -7.723818\n",
      "ep 1554: ep_len:24 episode reward: total was 10.500000. running mean: -7.541580\n",
      "ep 1554: ep_len:2984 episode reward: total was -17.230000. running mean: -7.638464\n",
      "ep 1554: ep_len:716 episode reward: total was -12.550000. running mean: -7.687579\n",
      "ep 1554: ep_len:53 episode reward: total was 25.000000. running mean: -7.360704\n",
      "ep 1554: ep_len:81 episode reward: total was 37.500000. running mean: -6.912097\n",
      "ep 1554: ep_len:1030 episode reward: total was -23.200000. running mean: -7.074976\n",
      "ep 1554: ep_len:3638 episode reward: total was -157.770000. running mean: -8.581926\n",
      "ep 1554: ep_len:771 episode reward: total was -37.550000. running mean: -8.871607\n",
      "ep 1554: ep_len:795 episode reward: total was 15.550000. running mean: -8.627391\n",
      "ep 1554: ep_len:638 episode reward: total was 0.940000. running mean: -8.531717\n",
      "ep 1554: ep_len:28 episode reward: total was 9.500000. running mean: -8.351399\n",
      "ep 1554: ep_len:1085 episode reward: total was -10.090000. running mean: -8.368785\n",
      "ep 1554: ep_len:2842 episode reward: total was -9.910000. running mean: -8.384198\n",
      "epsilon:0.009992 episode_count: 23464. steps_count: 25086364.000000\n",
      "ep 1555: ep_len:852 episode reward: total was 11.840000. running mean: -8.181956\n",
      "ep 1555: ep_len:216 episode reward: total was -0.140000. running mean: -8.101536\n",
      "ep 1555: ep_len:2960 episode reward: total was -22.440000. running mean: -8.244921\n",
      "ep 1555: ep_len:823 episode reward: total was 13.870000. running mean: -8.023772\n",
      "ep 1555: ep_len:140 episode reward: total was 67.000000. running mean: -7.273534\n",
      "ep 1555: ep_len:706 episode reward: total was 3.070000. running mean: -7.170098\n",
      "ep 1555: ep_len:330 episode reward: total was 11.900000. running mean: -6.979397\n",
      "ep 1555: ep_len:1575 episode reward: total was -8.110000. running mean: -6.990704\n",
      "ep 1555: ep_len:7373 episode reward: total was -223.100000. running mean: -9.151796\n",
      "ep 1555: ep_len:699 episode reward: total was -19.350000. running mean: -9.253779\n",
      "ep 1555: ep_len:99 episode reward: total was 48.000000. running mean: -8.681241\n",
      "ep 1555: ep_len:96 episode reward: total was 46.500000. running mean: -8.129428\n",
      "ep 1555: ep_len:981 episode reward: total was -39.640000. running mean: -8.444534\n",
      "ep 1555: ep_len:2827 episode reward: total was -9.640000. running mean: -8.456489\n",
      "epsilon:0.009992 episode_count: 23478. steps_count: 25106041.000000\n",
      "ep 1556: ep_len:643 episode reward: total was 5.810000. running mean: -8.313824\n",
      "ep 1556: ep_len:500 episode reward: total was 10.470000. running mean: -8.125986\n",
      "ep 1556: ep_len:3029 episode reward: total was -34.060000. running mean: -8.385326\n",
      "ep 1556: ep_len:511 episode reward: total was -19.330000. running mean: -8.494772\n",
      "ep 1556: ep_len:88 episode reward: total was 42.500000. running mean: -7.984825\n",
      "ep 1556: ep_len:948 episode reward: total was 59.860000. running mean: -7.306376\n",
      "ep 1556: ep_len:3638 episode reward: total was -18.440000. running mean: -7.417713\n",
      "ep 1556: ep_len:859 episode reward: total was 13.440000. running mean: -7.209136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1556: ep_len:634 episode reward: total was 12.360000. running mean: -7.013444\n",
      "ep 1556: ep_len:600 episode reward: total was 43.540000. running mean: -6.507910\n",
      "ep 1556: ep_len:149 episode reward: total was 71.500000. running mean: -5.727831\n",
      "ep 1556: ep_len:1499 episode reward: total was 29.850000. running mean: -5.372052\n",
      "ep 1556: ep_len:2812 episode reward: total was -17.900000. running mean: -5.497332\n",
      "epsilon:0.009992 episode_count: 23491. steps_count: 25121951.000000\n",
      "ep 1557: ep_len:1128 episode reward: total was -3.520000. running mean: -5.477559\n",
      "ep 1557: ep_len:1277 episode reward: total was -26.790000. running mean: -5.690683\n",
      "ep 1557: ep_len:42 episode reward: total was 18.000000. running mean: -5.453776\n",
      "ep 1557: ep_len:2971 episode reward: total was -17.310000. running mean: -5.572338\n",
      "ep 1557: ep_len:744 episode reward: total was 29.550000. running mean: -5.221115\n",
      "ep 1557: ep_len:38 episode reward: total was 17.500000. running mean: -4.993904\n",
      "ep 1557: ep_len:116 episode reward: total was 53.500000. running mean: -4.408965\n",
      "ep 1557: ep_len:47 episode reward: total was 22.000000. running mean: -4.144875\n",
      "ep 1557: ep_len:1870 episode reward: total was -83.550000. running mean: -4.938926\n",
      "ep 1557: ep_len:3662 episode reward: total was -25.790000. running mean: -5.147437\n",
      "ep 1557: ep_len:578 episode reward: total was 5.610000. running mean: -5.039863\n",
      "ep 1557: ep_len:710 episode reward: total was 6.750000. running mean: -4.921964\n",
      "ep 1557: ep_len:678 episode reward: total was 9.540000. running mean: -4.777344\n",
      "ep 1557: ep_len:55 episode reward: total was 24.500000. running mean: -4.484571\n",
      "ep 1557: ep_len:669 episode reward: total was 3.250000. running mean: -4.407225\n",
      "ep 1557: ep_len:2762 episode reward: total was -0.150000. running mean: -4.364653\n",
      "ep 1557: ep_len:49 episode reward: total was 21.500000. running mean: -4.106007\n",
      "epsilon:0.009992 episode_count: 23508. steps_count: 25139347.000000\n",
      "ep 1558: ep_len:617 episode reward: total was 7.340000. running mean: -3.991546\n",
      "ep 1558: ep_len:1629 episode reward: total was -76.220000. running mean: -4.713831\n",
      "ep 1558: ep_len:60 episode reward: total was 27.000000. running mean: -4.396693\n",
      "ep 1558: ep_len:3120 episode reward: total was -9.290000. running mean: -4.445626\n",
      "ep 1558: ep_len:797 episode reward: total was 10.920000. running mean: -4.291970\n",
      "ep 1558: ep_len:96 episode reward: total was 46.500000. running mean: -3.784050\n",
      "ep 1558: ep_len:649 episode reward: total was -18.480000. running mean: -3.931009\n",
      "ep 1558: ep_len:328 episode reward: total was 11.940000. running mean: -3.772299\n",
      "ep 1558: ep_len:616 episode reward: total was -39.090000. running mean: -4.125476\n",
      "ep 1558: ep_len:761 episode reward: total was 12.930000. running mean: -3.954921\n",
      "ep 1558: ep_len:1465 episode reward: total was 9.090000. running mean: -3.824472\n",
      "ep 1558: ep_len:686 episode reward: total was 15.950000. running mean: -3.626728\n",
      "ep 1558: ep_len:46 episode reward: total was 15.500000. running mean: -3.435460\n",
      "epsilon:0.009992 episode_count: 23521. steps_count: 25150217.000000\n",
      "ep 1559: ep_len:1073 episode reward: total was -0.550000. running mean: -3.406606\n",
      "ep 1559: ep_len:686 episode reward: total was -11.490000. running mean: -3.487440\n",
      "ep 1559: ep_len:2868 episode reward: total was -21.590000. running mean: -3.668465\n",
      "ep 1559: ep_len:879 episode reward: total was -51.330000. running mean: -4.145081\n",
      "ep 1559: ep_len:62 episode reward: total was 29.500000. running mean: -3.808630\n",
      "ep 1559: ep_len:34 episode reward: total was 15.500000. running mean: -3.615543\n",
      "ep 1559: ep_len:674 episode reward: total was -10.670000. running mean: -3.686088\n",
      "ep 1559: ep_len:626 episode reward: total was 22.040000. running mean: -3.428827\n",
      "ep 1559: ep_len:714 episode reward: total was -9.240000. running mean: -3.486939\n",
      "ep 1559: ep_len:851 episode reward: total was 62.550000. running mean: -2.826569\n",
      "ep 1559: ep_len:1126 episode reward: total was -17.680000. running mean: -2.975104\n",
      "ep 1559: ep_len:61 episode reward: total was 29.000000. running mean: -2.655353\n",
      "ep 1559: ep_len:734 episode reward: total was -5.960000. running mean: -2.688399\n",
      "ep 1559: ep_len:2806 episode reward: total was -58.800000. running mean: -3.249515\n",
      "epsilon:0.009992 episode_count: 23535. steps_count: 25163411.000000\n",
      "ep 1560: ep_len:683 episode reward: total was 9.240000. running mean: -3.124620\n",
      "ep 1560: ep_len:620 episode reward: total was -18.700000. running mean: -3.280374\n",
      "ep 1560: ep_len:3103 episode reward: total was 8.230000. running mean: -3.165270\n",
      "ep 1560: ep_len:675 episode reward: total was -0.330000. running mean: -3.136917\n",
      "ep 1560: ep_len:1163 episode reward: total was 21.760000. running mean: -2.887948\n",
      "ep 1560: ep_len:3983 episode reward: total was -3538.600000. running mean: -38.245069\n",
      "ep 1560: ep_len:1572 episode reward: total was -83.990000. running mean: -38.702518\n",
      "ep 1560: ep_len:732 episode reward: total was -6.560000. running mean: -38.381093\n",
      "ep 1560: ep_len:613 episode reward: total was -57.780000. running mean: -38.575082\n",
      "ep 1560: ep_len:195 episode reward: total was 97.010000. running mean: -37.219231\n",
      "ep 1560: ep_len:91 episode reward: total was 44.000000. running mean: -36.407039\n",
      "ep 1560: ep_len:1428 episode reward: total was 25.710000. running mean: -35.785868\n",
      "ep 1560: ep_len:2861 episode reward: total was -56.580000. running mean: -35.993810\n",
      "ep 1560: ep_len:46 episode reward: total was 18.500000. running mean: -35.448872\n",
      "epsilon:0.009992 episode_count: 23549. steps_count: 25181176.000000\n",
      "ep 1561: ep_len:1052 episode reward: total was -0.760000. running mean: -35.101983\n",
      "ep 1561: ep_len:698 episode reward: total was -29.770000. running mean: -35.048663\n",
      "ep 1561: ep_len:2930 episode reward: total was 2.520000. running mean: -34.672977\n",
      "ep 1561: ep_len:1188 episode reward: total was -36.130000. running mean: -34.687547\n",
      "ep 1561: ep_len:52 episode reward: total was 21.500000. running mean: -34.125671\n",
      "ep 1561: ep_len:125 episode reward: total was 59.500000. running mean: -33.189415\n",
      "ep 1561: ep_len:70 episode reward: total was 33.500000. running mean: -32.522520\n",
      "ep 1561: ep_len:1086 episode reward: total was 5.640000. running mean: -32.140895\n",
      "ep 1561: ep_len:3839 episode reward: total was -27.430000. running mean: -32.093786\n",
      "ep 1561: ep_len:594 episode reward: total was 3.910000. running mean: -31.733748\n",
      "ep 1561: ep_len:831 episode reward: total was 35.710000. running mean: -31.059311\n",
      "ep 1561: ep_len:658 episode reward: total was -9.750000. running mean: -30.846218\n",
      "ep 1561: ep_len:57 episode reward: total was 25.500000. running mean: -30.282756\n",
      "ep 1561: ep_len:57 episode reward: total was 24.000000. running mean: -29.739928\n",
      "ep 1561: ep_len:1061 episode reward: total was -12.270000. running mean: -29.565229\n",
      "ep 1561: ep_len:2841 episode reward: total was -37.240000. running mean: -29.641976\n",
      "epsilon:0.009992 episode_count: 23565. steps_count: 25198315.000000\n",
      "ep 1562: ep_len:1136 episode reward: total was 9.730000. running mean: -29.248257\n",
      "ep 1562: ep_len:192 episode reward: total was 0.630000. running mean: -28.949474\n",
      "ep 1562: ep_len:2895 episode reward: total was -0.630000. running mean: -28.666279\n",
      "ep 1562: ep_len:500 episode reward: total was -3.860000. running mean: -28.418217\n",
      "ep 1562: ep_len:84 episode reward: total was 40.500000. running mean: -27.729034\n",
      "ep 1562: ep_len:66 episode reward: total was 30.000000. running mean: -27.151744\n",
      "ep 1562: ep_len:571 episode reward: total was 9.860000. running mean: -26.781627\n",
      "ep 1562: ep_len:3660 episode reward: total was -48.030000. running mean: -26.994110\n",
      "ep 1562: ep_len:1649 episode reward: total was -1246.210000. running mean: -39.186269\n",
      "ep 1562: ep_len:7286 episode reward: total was -600.850000. running mean: -44.802907\n",
      "ep 1562: ep_len:1040 episode reward: total was 6.530000. running mean: -44.289578\n",
      "ep 1562: ep_len:1475 episode reward: total was 31.230000. running mean: -43.534382\n",
      "ep 1562: ep_len:2893 episode reward: total was -834.670000. running mean: -51.445738\n",
      "epsilon:0.009992 episode_count: 23578. steps_count: 25221762.000000\n",
      "ep 1563: ep_len:1144 episode reward: total was 6.200000. running mean: -50.869281\n",
      "ep 1563: ep_len:700 episode reward: total was -81.150000. running mean: -51.172088\n",
      "ep 1563: ep_len:2946 episode reward: total was -36.770000. running mean: -51.028067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1563: ep_len:646 episode reward: total was -8.260000. running mean: -50.600386\n",
      "ep 1563: ep_len:153 episode reward: total was 75.000000. running mean: -49.344382\n",
      "ep 1563: ep_len:625 episode reward: total was -0.990000. running mean: -48.860839\n",
      "ep 1563: ep_len:3773 episode reward: total was -27.050000. running mean: -48.642730\n",
      "ep 1563: ep_len:1569 episode reward: total was -167.430000. running mean: -49.830603\n",
      "ep 1563: ep_len:909 episode reward: total was 63.660000. running mean: -48.695697\n",
      "ep 1563: ep_len:568 episode reward: total was -1.560000. running mean: -48.224340\n",
      "ep 1563: ep_len:96 episode reward: total was 22.010000. running mean: -47.521996\n",
      "ep 1563: ep_len:643 episode reward: total was 11.260000. running mean: -46.934176\n",
      "ep 1563: ep_len:46 episode reward: total was 21.500000. running mean: -46.249835\n",
      "epsilon:0.009992 episode_count: 23591. steps_count: 25235580.000000\n",
      "ep 1564: ep_len:1436 episode reward: total was 2.130000. running mean: -45.766036\n",
      "ep 1564: ep_len:699 episode reward: total was -19.930000. running mean: -45.507676\n",
      "ep 1564: ep_len:2958 episode reward: total was -5.230000. running mean: -45.104899\n",
      "ep 1564: ep_len:666 episode reward: total was -7.040000. running mean: -44.724250\n",
      "ep 1564: ep_len:1384 episode reward: total was 21.750000. running mean: -44.059508\n",
      "ep 1564: ep_len:3637 episode reward: total was -107.330000. running mean: -44.692213\n",
      "ep 1564: ep_len:811 episode reward: total was -33.600000. running mean: -44.581291\n",
      "ep 1564: ep_len:751 episode reward: total was 28.420000. running mean: -43.851278\n",
      "ep 1564: ep_len:720 episode reward: total was -39.040000. running mean: -43.803165\n",
      "ep 1564: ep_len:42 episode reward: total was 19.500000. running mean: -43.170133\n",
      "ep 1564: ep_len:50 episode reward: total was 23.500000. running mean: -42.503432\n",
      "ep 1564: ep_len:826 episode reward: total was -27.770000. running mean: -42.356098\n",
      "ep 1564: ep_len:2872 episode reward: total was -23.780000. running mean: -42.170337\n",
      "ep 1564: ep_len:64 episode reward: total was 30.500000. running mean: -41.443633\n",
      "epsilon:0.009992 episode_count: 23605. steps_count: 25252496.000000\n",
      "ep 1565: ep_len:985 episode reward: total was -54.170000. running mean: -41.570897\n",
      "ep 1565: ep_len:691 episode reward: total was -26.400000. running mean: -41.419188\n",
      "ep 1565: ep_len:59 episode reward: total was 28.000000. running mean: -40.724996\n",
      "ep 1565: ep_len:2951 episode reward: total was -38.290000. running mean: -40.700646\n",
      "ep 1565: ep_len:650 episode reward: total was 21.680000. running mean: -40.076840\n",
      "ep 1565: ep_len:53 episode reward: total was 25.000000. running mean: -39.426071\n",
      "ep 1565: ep_len:147 episode reward: total was 67.500000. running mean: -38.356811\n",
      "ep 1565: ep_len:75 episode reward: total was 34.500000. running mean: -37.628242\n",
      "ep 1565: ep_len:79 episode reward: total was 33.500000. running mean: -36.916960\n",
      "ep 1565: ep_len:741 episode reward: total was -98.420000. running mean: -37.531990\n",
      "ep 1565: ep_len:332 episode reward: total was 16.480000. running mean: -36.991870\n",
      "ep 1565: ep_len:672 episode reward: total was -133.220000. running mean: -37.954152\n",
      "ep 1565: ep_len:738 episode reward: total was 61.540000. running mean: -36.959210\n",
      "ep 1565: ep_len:629 episode reward: total was -46.470000. running mean: -37.054318\n",
      "ep 1565: ep_len:1132 episode reward: total was -22.180000. running mean: -36.905575\n",
      "ep 1565: ep_len:2908 episode reward: total was -14.360000. running mean: -36.680119\n",
      "epsilon:0.009992 episode_count: 23621. steps_count: 25265338.000000\n",
      "ep 1566: ep_len:704 episode reward: total was -9.800000. running mean: -36.411318\n",
      "ep 1566: ep_len:725 episode reward: total was -76.520000. running mean: -36.812405\n",
      "ep 1566: ep_len:2914 episode reward: total was 2.850000. running mean: -36.415781\n",
      "ep 1566: ep_len:1214 episode reward: total was -21.410000. running mean: -36.265723\n",
      "ep 1566: ep_len:80 episode reward: total was 38.500000. running mean: -35.518066\n",
      "ep 1566: ep_len:1814 episode reward: total was -96.920000. running mean: -36.132085\n",
      "ep 1566: ep_len:3566 episode reward: total was -38.900000. running mean: -36.159764\n",
      "ep 1566: ep_len:1563 episode reward: total was -26.960000. running mean: -36.067767\n",
      "ep 1566: ep_len:870 episode reward: total was 58.910000. running mean: -35.117989\n",
      "ep 1566: ep_len:651 episode reward: total was -22.500000. running mean: -34.991809\n",
      "ep 1566: ep_len:98 episode reward: total was 46.000000. running mean: -34.181891\n",
      "ep 1566: ep_len:1502 episode reward: total was 13.780000. running mean: -33.702272\n",
      "ep 1566: ep_len:2805 episode reward: total was -39.360000. running mean: -33.758849\n",
      "epsilon:0.009992 episode_count: 23634. steps_count: 25283844.000000\n",
      "ep 1567: ep_len:978 episode reward: total was -58.710000. running mean: -34.008361\n",
      "ep 1567: ep_len:670 episode reward: total was -24.780000. running mean: -33.916077\n",
      "ep 1567: ep_len:2903 episode reward: total was -58.640000. running mean: -34.163316\n",
      "ep 1567: ep_len:557 episode reward: total was -32.980000. running mean: -34.151483\n",
      "ep 1567: ep_len:176 episode reward: total was 85.000000. running mean: -32.959968\n",
      "ep 1567: ep_len:94 episode reward: total was 44.000000. running mean: -32.190369\n",
      "ep 1567: ep_len:41 episode reward: total was 17.500000. running mean: -31.693465\n",
      "ep 1567: ep_len:500 episode reward: total was -28.250000. running mean: -31.659030\n",
      "ep 1567: ep_len:314 episode reward: total was 5.340000. running mean: -31.289040\n",
      "ep 1567: ep_len:1227 episode reward: total was -82.070000. running mean: -31.796850\n",
      "ep 1567: ep_len:7328 episode reward: total was -278.090000. running mean: -34.259781\n",
      "ep 1567: ep_len:500 episode reward: total was -150.270000. running mean: -35.419883\n",
      "ep 1567: ep_len:202 episode reward: total was 96.500000. running mean: -34.100685\n",
      "ep 1567: ep_len:54 episode reward: total was 25.500000. running mean: -33.504678\n",
      "ep 1567: ep_len:1107 episode reward: total was -8.290000. running mean: -33.252531\n",
      "ep 1567: ep_len:2928 episode reward: total was -65.980000. running mean: -33.579806\n",
      "epsilon:0.009992 episode_count: 23650. steps_count: 25303423.000000\n",
      "ep 1568: ep_len:1111 episode reward: total was -24.750000. running mean: -33.491508\n",
      "ep 1568: ep_len:1226 episode reward: total was -40.430000. running mean: -33.560893\n",
      "ep 1568: ep_len:77 episode reward: total was 35.500000. running mean: -32.870284\n",
      "ep 1568: ep_len:2937 episode reward: total was -61.600000. running mean: -33.157581\n",
      "ep 1568: ep_len:1228 episode reward: total was -4.930000. running mean: -32.875305\n",
      "ep 1568: ep_len:1470 episode reward: total was -6.590000. running mean: -32.612452\n",
      "ep 1568: ep_len:336 episode reward: total was -4.020000. running mean: -32.326527\n",
      "ep 1568: ep_len:1525 episode reward: total was -24.070000. running mean: -32.243962\n",
      "ep 1568: ep_len:882 episode reward: total was 45.810000. running mean: -31.463422\n",
      "ep 1568: ep_len:500 episode reward: total was 19.230000. running mean: -30.956488\n",
      "ep 1568: ep_len:95 episode reward: total was 44.500000. running mean: -30.201923\n",
      "ep 1568: ep_len:44 episode reward: total was 20.500000. running mean: -29.694904\n",
      "ep 1568: ep_len:709 episode reward: total was -51.540000. running mean: -29.913355\n",
      "ep 1568: ep_len:2767 episode reward: total was -455.130000. running mean: -34.165522\n",
      "epsilon:0.009992 episode_count: 23664. steps_count: 25318330.000000\n",
      "ep 1569: ep_len:614 episode reward: total was -57.720000. running mean: -34.401066\n",
      "ep 1569: ep_len:500 episode reward: total was 17.910000. running mean: -33.877956\n",
      "ep 1569: ep_len:2934 episode reward: total was -41.440000. running mean: -33.953576\n",
      "ep 1569: ep_len:1083 episode reward: total was -2.470000. running mean: -33.638740\n",
      "ep 1569: ep_len:1103 episode reward: total was -116.400000. running mean: -34.466353\n",
      "ep 1569: ep_len:3731 episode reward: total was -13.410000. running mean: -34.255789\n",
      "ep 1569: ep_len:798 episode reward: total was -35.510000. running mean: -34.268332\n",
      "ep 1569: ep_len:7569 episode reward: total was -96.420000. running mean: -34.889848\n",
      "ep 1569: ep_len:787 episode reward: total was -20.580000. running mean: -34.746750\n",
      "ep 1569: ep_len:63 episode reward: total was 28.500000. running mean: -34.114282\n",
      "ep 1569: ep_len:167 episode reward: total was 80.500000. running mean: -32.968139\n",
      "ep 1569: ep_len:1092 episode reward: total was 20.710000. running mean: -32.431358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1569: ep_len:2892 episode reward: total was -8.310000. running mean: -32.190144\n",
      "epsilon:0.009992 episode_count: 23677. steps_count: 25341663.000000\n",
      "ep 1570: ep_len:3653 episode reward: total was -907.990000. running mean: -40.948143\n",
      "ep 1570: ep_len:709 episode reward: total was -23.010000. running mean: -40.768762\n",
      "ep 1570: ep_len:3080 episode reward: total was -40.830000. running mean: -40.769374\n",
      "ep 1570: ep_len:1477 episode reward: total was 15.790000. running mean: -40.203780\n",
      "ep 1570: ep_len:87 episode reward: total was 40.500000. running mean: -39.396742\n",
      "ep 1570: ep_len:83 episode reward: total was 38.500000. running mean: -38.617775\n",
      "ep 1570: ep_len:1117 episode reward: total was -12.230000. running mean: -38.353897\n",
      "ep 1570: ep_len:3693 episode reward: total was -127.730000. running mean: -39.247658\n",
      "ep 1570: ep_len:1239 episode reward: total was -83.210000. running mean: -39.687282\n",
      "ep 1570: ep_len:841 episode reward: total was 50.690000. running mean: -38.783509\n",
      "ep 1570: ep_len:921 episode reward: total was 39.520000. running mean: -38.000474\n",
      "ep 1570: ep_len:54 episode reward: total was 25.500000. running mean: -37.365469\n",
      "ep 1570: ep_len:93 episode reward: total was 42.000000. running mean: -36.571814\n",
      "ep 1570: ep_len:740 episode reward: total was -49.330000. running mean: -36.699396\n",
      "ep 1570: ep_len:2814 episode reward: total was -56.960000. running mean: -36.902002\n",
      "ep 1570: ep_len:53 episode reward: total was 25.000000. running mean: -36.282982\n",
      "epsilon:0.009992 episode_count: 23693. steps_count: 25362317.000000\n",
      "ep 1571: ep_len:1095 episode reward: total was 8.000000. running mean: -35.840152\n",
      "ep 1571: ep_len:687 episode reward: total was -28.160000. running mean: -35.763351\n",
      "ep 1571: ep_len:56 episode reward: total was 26.500000. running mean: -35.140717\n",
      "ep 1571: ep_len:2909 episode reward: total was -14.510000. running mean: -34.934410\n",
      "ep 1571: ep_len:761 episode reward: total was -22.860000. running mean: -34.813666\n",
      "ep 1571: ep_len:54 episode reward: total was 25.500000. running mean: -34.210529\n",
      "ep 1571: ep_len:53 episode reward: total was 25.000000. running mean: -33.618424\n",
      "ep 1571: ep_len:1516 episode reward: total was -59.730000. running mean: -33.879540\n",
      "ep 1571: ep_len:3558 episode reward: total was -826.120000. running mean: -41.801944\n",
      "ep 1571: ep_len:779 episode reward: total was -20.550000. running mean: -41.589425\n",
      "ep 1571: ep_len:7212 episode reward: total was -99.790000. running mean: -42.171431\n",
      "ep 1571: ep_len:611 episode reward: total was -7.310000. running mean: -41.822816\n",
      "ep 1571: ep_len:71 episode reward: total was 34.000000. running mean: -41.064588\n",
      "ep 1571: ep_len:128 episode reward: total was 61.000000. running mean: -40.043942\n",
      "ep 1571: ep_len:600 episode reward: total was -20.950000. running mean: -39.853003\n",
      "ep 1571: ep_len:2914 episode reward: total was 7.400000. running mean: -39.380473\n",
      "ep 1571: ep_len:39 episode reward: total was 13.500000. running mean: -38.851668\n",
      "epsilon:0.009992 episode_count: 23710. steps_count: 25385360.000000\n",
      "ep 1572: ep_len:1429 episode reward: total was -8.990000. running mean: -38.553052\n",
      "ep 1572: ep_len:709 episode reward: total was -33.720000. running mean: -38.504721\n",
      "ep 1572: ep_len:57 episode reward: total was 27.000000. running mean: -37.849674\n",
      "ep 1572: ep_len:3026 episode reward: total was -29.850000. running mean: -37.769677\n",
      "ep 1572: ep_len:739 episode reward: total was -23.080000. running mean: -37.622780\n",
      "ep 1572: ep_len:500 episode reward: total was 21.320000. running mean: -37.033353\n",
      "ep 1572: ep_len:287 episode reward: total was -18.930000. running mean: -36.852319\n",
      "ep 1572: ep_len:839 episode reward: total was 11.620000. running mean: -36.367596\n",
      "ep 1572: ep_len:800 episode reward: total was 5.310000. running mean: -35.950820\n",
      "ep 1572: ep_len:654 episode reward: total was 5.340000. running mean: -35.537912\n",
      "ep 1572: ep_len:776 episode reward: total was -95.330000. running mean: -36.135833\n",
      "ep 1572: ep_len:2707 episode reward: total was -30.610000. running mean: -36.080574\n",
      "ep 1572: ep_len:57 episode reward: total was 27.000000. running mean: -35.449768\n",
      "epsilon:0.009992 episode_count: 23723. steps_count: 25397940.000000\n",
      "ep 1573: ep_len:1171 episode reward: total was -5.140000. running mean: -35.146671\n",
      "ep 1573: ep_len:734 episode reward: total was -70.770000. running mean: -35.502904\n",
      "ep 1573: ep_len:64 episode reward: total was 29.000000. running mean: -34.857875\n",
      "ep 1573: ep_len:3046 episode reward: total was -23.630000. running mean: -34.745596\n",
      "ep 1573: ep_len:1104 episode reward: total was 1.780000. running mean: -34.380340\n",
      "ep 1573: ep_len:59 episode reward: total was 28.000000. running mean: -33.756537\n",
      "ep 1573: ep_len:1780 episode reward: total was -129.190000. running mean: -34.710872\n",
      "ep 1573: ep_len:651 episode reward: total was -0.720000. running mean: -34.370963\n",
      "ep 1573: ep_len:1263 episode reward: total was -79.970000. running mean: -34.826953\n",
      "ep 1573: ep_len:7371 episode reward: total was -388.030000. running mean: -38.358984\n",
      "ep 1573: ep_len:600 episode reward: total was 4.820000. running mean: -37.927194\n",
      "ep 1573: ep_len:53 episode reward: total was 25.000000. running mean: -37.297922\n",
      "ep 1573: ep_len:53 episode reward: total was 25.000000. running mean: -36.674943\n",
      "ep 1573: ep_len:1383 episode reward: total was -46.570000. running mean: -36.773893\n",
      "ep 1573: ep_len:2811 episode reward: total was -52.700000. running mean: -36.933154\n",
      "ep 1573: ep_len:31 episode reward: total was 12.500000. running mean: -36.438823\n",
      "epsilon:0.009992 episode_count: 23739. steps_count: 25420114.000000\n",
      "ep 1574: ep_len:630 episode reward: total was 4.570000. running mean: -36.028735\n",
      "ep 1574: ep_len:967 episode reward: total was -3.500000. running mean: -35.703447\n",
      "ep 1574: ep_len:2980 episode reward: total was -48.070000. running mean: -35.827113\n",
      "ep 1574: ep_len:500 episode reward: total was -2.560000. running mean: -35.494442\n",
      "ep 1574: ep_len:26 episode reward: total was 11.500000. running mean: -35.024497\n",
      "ep 1574: ep_len:48 episode reward: total was 22.500000. running mean: -34.449252\n",
      "ep 1574: ep_len:1662 episode reward: total was -418.860000. running mean: -38.293360\n",
      "ep 1574: ep_len:3593 episode reward: total was -70.370000. running mean: -38.614126\n",
      "ep 1574: ep_len:1231 episode reward: total was -53.510000. running mean: -38.763085\n",
      "ep 1574: ep_len:685 episode reward: total was 9.140000. running mean: -38.284054\n",
      "ep 1574: ep_len:645 episode reward: total was 0.220000. running mean: -37.899013\n",
      "ep 1574: ep_len:62 episode reward: total was 29.500000. running mean: -37.225023\n",
      "ep 1574: ep_len:1463 episode reward: total was -11.770000. running mean: -36.970473\n",
      "ep 1574: ep_len:2728 episode reward: total was -17.240000. running mean: -36.773168\n",
      "epsilon:0.009992 episode_count: 23753. steps_count: 25437334.000000\n",
      "ep 1575: ep_len:1089 episode reward: total was -9.300000. running mean: -36.498437\n",
      "ep 1575: ep_len:1647 episode reward: total was -71.570000. running mean: -36.849152\n",
      "ep 1575: ep_len:2985 episode reward: total was -36.820000. running mean: -36.848861\n",
      "ep 1575: ep_len:3198 episode reward: total was -385.810000. running mean: -40.338472\n",
      "ep 1575: ep_len:118 episode reward: total was 54.500000. running mean: -39.390087\n",
      "ep 1575: ep_len:84 episode reward: total was 40.500000. running mean: -38.591187\n",
      "ep 1575: ep_len:1078 episode reward: total was -0.470000. running mean: -38.209975\n",
      "ep 1575: ep_len:3563 episode reward: total was -54.050000. running mean: -38.368375\n",
      "ep 1575: ep_len:893 episode reward: total was -119.870000. running mean: -39.183391\n",
      "ep 1575: ep_len:715 episode reward: total was 5.880000. running mean: -38.732757\n",
      "ep 1575: ep_len:500 episode reward: total was 26.710000. running mean: -38.078330\n",
      "ep 1575: ep_len:70 episode reward: total was 32.000000. running mean: -37.377546\n",
      "ep 1575: ep_len:205 episode reward: total was 99.500000. running mean: -36.008771\n",
      "ep 1575: ep_len:67 episode reward: total was 32.000000. running mean: -35.328683\n",
      "ep 1575: ep_len:500 episode reward: total was 31.330000. running mean: -34.662096\n",
      "ep 1575: ep_len:2810 episode reward: total was -67.560000. running mean: -34.991075\n",
      "epsilon:0.009992 episode_count: 23769. steps_count: 25456856.000000\n",
      "ep 1576: ep_len:823 episode reward: total was 12.500000. running mean: -34.516165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1576: ep_len:667 episode reward: total was -17.430000. running mean: -34.345303\n",
      "ep 1576: ep_len:3027 episode reward: total was -6.270000. running mean: -34.064550\n",
      "ep 1576: ep_len:500 episode reward: total was 3.490000. running mean: -33.689005\n",
      "ep 1576: ep_len:65 episode reward: total was 31.000000. running mean: -33.042114\n",
      "ep 1576: ep_len:1504 episode reward: total was -221.950000. running mean: -34.931193\n",
      "ep 1576: ep_len:3646 episode reward: total was -60.290000. running mean: -35.184781\n",
      "ep 1576: ep_len:756 episode reward: total was -71.550000. running mean: -35.548434\n",
      "ep 1576: ep_len:842 episode reward: total was 35.950000. running mean: -34.833449\n",
      "ep 1576: ep_len:895 episode reward: total was 4.620000. running mean: -34.438915\n",
      "ep 1576: ep_len:976 episode reward: total was -96.070000. running mean: -35.055226\n",
      "ep 1576: ep_len:2817 episode reward: total was 4.900000. running mean: -34.655673\n",
      "epsilon:0.009992 episode_count: 23781. steps_count: 25473374.000000\n",
      "ep 1577: ep_len:1154 episode reward: total was -18.290000. running mean: -34.492017\n",
      "ep 1577: ep_len:614 episode reward: total was -9.460000. running mean: -34.241696\n",
      "ep 1577: ep_len:3076 episode reward: total was -6.360000. running mean: -33.962879\n",
      "ep 1577: ep_len:616 episode reward: total was -124.240000. running mean: -34.865651\n",
      "ep 1577: ep_len:33 episode reward: total was 15.000000. running mean: -34.366994\n",
      "ep 1577: ep_len:1019 episode reward: total was -3.110000. running mean: -34.054424\n",
      "ep 1577: ep_len:649 episode reward: total was -22.110000. running mean: -33.934980\n",
      "ep 1577: ep_len:3887 episode reward: total was -893.710000. running mean: -42.532730\n",
      "ep 1577: ep_len:797 episode reward: total was 40.390000. running mean: -41.703503\n",
      "ep 1577: ep_len:1495 episode reward: total was -34.070000. running mean: -41.627168\n",
      "ep 1577: ep_len:67 episode reward: total was 32.000000. running mean: -40.890896\n",
      "ep 1577: ep_len:1060 episode reward: total was -18.360000. running mean: -40.665587\n",
      "ep 1577: ep_len:2821 episode reward: total was 13.510000. running mean: -40.123831\n",
      "epsilon:0.009992 episode_count: 23794. steps_count: 25490662.000000\n",
      "ep 1578: ep_len:592 episode reward: total was -4.200000. running mean: -39.764593\n",
      "ep 1578: ep_len:912 episode reward: total was 18.040000. running mean: -39.186547\n",
      "ep 1578: ep_len:3013 episode reward: total was -22.100000. running mean: -39.015682\n",
      "ep 1578: ep_len:675 episode reward: total was -54.880000. running mean: -39.174325\n",
      "ep 1578: ep_len:57 episode reward: total was 27.000000. running mean: -38.512582\n",
      "ep 1578: ep_len:1514 episode reward: total was 10.540000. running mean: -38.022056\n",
      "ep 1578: ep_len:330 episode reward: total was -0.100000. running mean: -37.642835\n",
      "ep 1578: ep_len:696 episode reward: total was -21.010000. running mean: -37.476507\n",
      "ep 1578: ep_len:7384 episode reward: total was -19.920000. running mean: -37.300942\n",
      "ep 1578: ep_len:500 episode reward: total was 43.300000. running mean: -36.494932\n",
      "ep 1578: ep_len:500 episode reward: total was 25.290000. running mean: -35.877083\n",
      "ep 1578: ep_len:2831 episode reward: total was -5.430000. running mean: -35.572612\n",
      "ep 1578: ep_len:41 episode reward: total was 19.000000. running mean: -35.026886\n",
      "epsilon:0.009992 episode_count: 23807. steps_count: 25509707.000000\n",
      "ep 1579: ep_len:1073 episode reward: total was 5.510000. running mean: -34.621517\n",
      "ep 1579: ep_len:1668 episode reward: total was -66.240000. running mean: -34.937702\n",
      "ep 1579: ep_len:58 episode reward: total was 27.500000. running mean: -34.313325\n",
      "ep 1579: ep_len:3032 episode reward: total was -13.880000. running mean: -34.108992\n",
      "ep 1579: ep_len:500 episode reward: total was -8.660000. running mean: -33.854502\n",
      "ep 1579: ep_len:69 episode reward: total was 31.500000. running mean: -33.200957\n",
      "ep 1579: ep_len:1133 episode reward: total was 2.350000. running mean: -32.845447\n",
      "ep 1579: ep_len:635 episode reward: total was 15.860000. running mean: -32.358393\n",
      "ep 1579: ep_len:591 episode reward: total was -40.630000. running mean: -32.441109\n",
      "ep 1579: ep_len:739 episode reward: total was -2.940000. running mean: -32.146098\n",
      "ep 1579: ep_len:500 episode reward: total was -7.840000. running mean: -31.903037\n",
      "ep 1579: ep_len:78 episode reward: total was 36.000000. running mean: -31.224006\n",
      "ep 1579: ep_len:687 episode reward: total was 7.790000. running mean: -30.833866\n",
      "ep 1579: ep_len:2860 episode reward: total was -32.780000. running mean: -30.853328\n",
      "epsilon:0.009992 episode_count: 23821. steps_count: 25523330.000000\n",
      "ep 1580: ep_len:608 episode reward: total was -5.200000. running mean: -30.596794\n",
      "ep 1580: ep_len:1135 episode reward: total was -9.760000. running mean: -30.388427\n",
      "ep 1580: ep_len:71 episode reward: total was 31.000000. running mean: -29.774542\n",
      "ep 1580: ep_len:2976 episode reward: total was -17.520000. running mean: -29.651997\n",
      "ep 1580: ep_len:590 episode reward: total was -10.430000. running mean: -29.459777\n",
      "ep 1580: ep_len:49 episode reward: total was 23.000000. running mean: -28.935179\n",
      "ep 1580: ep_len:163 episode reward: total was 74.000000. running mean: -27.905827\n",
      "ep 1580: ep_len:661 episode reward: total was 0.380000. running mean: -27.622969\n",
      "ep 1580: ep_len:3631 episode reward: total was -29.130000. running mean: -27.638039\n",
      "ep 1580: ep_len:1281 episode reward: total was -69.170000. running mean: -28.053359\n",
      "ep 1580: ep_len:690 episode reward: total was 3.100000. running mean: -27.741825\n",
      "ep 1580: ep_len:500 episode reward: total was -13.750000. running mean: -27.601907\n",
      "ep 1580: ep_len:66 episode reward: total was 31.500000. running mean: -27.010888\n",
      "ep 1580: ep_len:500 episode reward: total was 20.850000. running mean: -26.532279\n",
      "ep 1580: ep_len:2889 episode reward: total was -38.760000. running mean: -26.654556\n",
      "epsilon:0.009992 episode_count: 23836. steps_count: 25539140.000000\n",
      "ep 1581: ep_len:678 episode reward: total was -19.250000. running mean: -26.580511\n",
      "ep 1581: ep_len:500 episode reward: total was 16.750000. running mean: -26.147206\n",
      "ep 1581: ep_len:41 episode reward: total was 19.000000. running mean: -25.695734\n",
      "ep 1581: ep_len:2956 episode reward: total was -29.320000. running mean: -25.731976\n",
      "ep 1581: ep_len:1675 episode reward: total was -78.240000. running mean: -26.257057\n",
      "ep 1581: ep_len:138 episode reward: total was 64.500000. running mean: -25.349486\n",
      "ep 1581: ep_len:75 episode reward: total was 31.500000. running mean: -24.780991\n",
      "ep 1581: ep_len:500 episode reward: total was 19.530000. running mean: -24.337881\n",
      "ep 1581: ep_len:665 episode reward: total was 31.740000. running mean: -23.777102\n",
      "ep 1581: ep_len:4089 episode reward: total was -709.560000. running mean: -30.634931\n",
      "ep 1581: ep_len:711 episode reward: total was 16.640000. running mean: -30.162182\n",
      "ep 1581: ep_len:500 episode reward: total was 33.990000. running mean: -29.520660\n",
      "ep 1581: ep_len:1197 episode reward: total was -11.980000. running mean: -29.345254\n",
      "ep 1581: ep_len:2873 episode reward: total was 14.090000. running mean: -28.910901\n",
      "ep 1581: ep_len:49 episode reward: total was 23.000000. running mean: -28.391792\n",
      "epsilon:0.009992 episode_count: 23851. steps_count: 25555787.000000\n",
      "ep 1582: ep_len:602 episode reward: total was -7.960000. running mean: -28.187474\n",
      "ep 1582: ep_len:983 episode reward: total was 17.040000. running mean: -27.735199\n",
      "ep 1582: ep_len:3000 episode reward: total was -14.150000. running mean: -27.599347\n",
      "ep 1582: ep_len:557 episode reward: total was -15.810000. running mean: -27.481454\n",
      "ep 1582: ep_len:58 episode reward: total was 24.500000. running mean: -26.961639\n",
      "ep 1582: ep_len:673 episode reward: total was -2.530000. running mean: -26.717323\n",
      "ep 1582: ep_len:3717 episode reward: total was -14.130000. running mean: -26.591450\n",
      "ep 1582: ep_len:821 episode reward: total was 0.450000. running mean: -26.321035\n",
      "ep 1582: ep_len:860 episode reward: total was 54.290000. running mean: -25.514925\n",
      "ep 1582: ep_len:616 episode reward: total was -7.260000. running mean: -25.332376\n",
      "ep 1582: ep_len:53 episode reward: total was 25.000000. running mean: -24.829052\n",
      "ep 1582: ep_len:187 episode reward: total was 92.000000. running mean: -23.660761\n",
      "ep 1582: ep_len:1453 episode reward: total was 10.350000. running mean: -23.320654\n",
      "ep 1582: ep_len:2874 episode reward: total was 5.040000. running mean: -23.037047\n",
      "epsilon:0.009992 episode_count: 23865. steps_count: 25572241.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1583: ep_len:826 episode reward: total was -28.270000. running mean: -23.089377\n",
      "ep 1583: ep_len:701 episode reward: total was -53.990000. running mean: -23.398383\n",
      "ep 1583: ep_len:47 episode reward: total was 22.000000. running mean: -22.944399\n",
      "ep 1583: ep_len:3064 episode reward: total was -2415.720000. running mean: -46.872155\n",
      "ep 1583: ep_len:1449 episode reward: total was 19.800000. running mean: -46.205434\n",
      "ep 1583: ep_len:97 episode reward: total was 47.000000. running mean: -45.273379\n",
      "ep 1583: ep_len:47 episode reward: total was 22.000000. running mean: -44.600646\n",
      "ep 1583: ep_len:1054 episode reward: total was -65.380000. running mean: -44.808439\n",
      "ep 1583: ep_len:643 episode reward: total was 21.050000. running mean: -44.149855\n",
      "ep 1583: ep_len:516 episode reward: total was -33.390000. running mean: -44.042256\n",
      "ep 1583: ep_len:712 episode reward: total was 41.280000. running mean: -43.189034\n",
      "ep 1583: ep_len:1540 episode reward: total was -6.320000. running mean: -42.820343\n",
      "ep 1583: ep_len:43 episode reward: total was 18.500000. running mean: -42.207140\n",
      "ep 1583: ep_len:164 episode reward: total was 77.500000. running mean: -41.010068\n",
      "ep 1583: ep_len:1431 episode reward: total was 16.130000. running mean: -40.438668\n",
      "ep 1583: ep_len:2891 episode reward: total was -24.970000. running mean: -40.283981\n",
      "ep 1583: ep_len:68 episode reward: total was 31.000000. running mean: -39.571141\n",
      "epsilon:0.009992 episode_count: 23882. steps_count: 25587534.000000\n",
      "ep 1584: ep_len:871 episode reward: total was -0.980000. running mean: -39.185230\n",
      "ep 1584: ep_len:500 episode reward: total was 2.330000. running mean: -38.770078\n",
      "ep 1584: ep_len:2982 episode reward: total was -74.860000. running mean: -39.130977\n",
      "ep 1584: ep_len:811 episode reward: total was 15.800000. running mean: -38.581667\n",
      "ep 1584: ep_len:95 episode reward: total was 46.000000. running mean: -37.735850\n",
      "ep 1584: ep_len:65 episode reward: total was 26.500000. running mean: -37.093492\n",
      "ep 1584: ep_len:1042 episode reward: total was -70.550000. running mean: -37.428057\n",
      "ep 1584: ep_len:3913 episode reward: total was -194.960000. running mean: -39.003376\n",
      "ep 1584: ep_len:1243 episode reward: total was -65.510000. running mean: -39.268443\n",
      "ep 1584: ep_len:795 episode reward: total was 16.370000. running mean: -38.712058\n",
      "ep 1584: ep_len:1096 episode reward: total was -10.390000. running mean: -38.428838\n",
      "ep 1584: ep_len:85 episode reward: total was 41.000000. running mean: -37.634549\n",
      "ep 1584: ep_len:27 episode reward: total was 12.000000. running mean: -37.138204\n",
      "ep 1584: ep_len:1157 episode reward: total was -8.950000. running mean: -36.856322\n",
      "ep 1584: ep_len:2790 episode reward: total was -16.060000. running mean: -36.648358\n",
      "epsilon:0.009992 episode_count: 23897. steps_count: 25605006.000000\n",
      "ep 1585: ep_len:1130 episode reward: total was -3.800000. running mean: -36.319875\n",
      "ep 1585: ep_len:1269 episode reward: total was -43.030000. running mean: -36.386976\n",
      "ep 1585: ep_len:3122 episode reward: total was -37.180000. running mean: -36.394906\n",
      "ep 1585: ep_len:543 episode reward: total was -42.210000. running mean: -36.453057\n",
      "ep 1585: ep_len:67 episode reward: total was 27.500000. running mean: -35.813527\n",
      "ep 1585: ep_len:59 episode reward: total was 26.500000. running mean: -35.190391\n",
      "ep 1585: ep_len:731 episode reward: total was -51.470000. running mean: -35.353188\n",
      "ep 1585: ep_len:345 episode reward: total was 11.100000. running mean: -34.888656\n",
      "ep 1585: ep_len:4097 episode reward: total was -575.210000. running mean: -40.291869\n",
      "ep 1585: ep_len:766 episode reward: total was 53.090000. running mean: -39.358050\n",
      "ep 1585: ep_len:758 episode reward: total was -15.820000. running mean: -39.122670\n",
      "ep 1585: ep_len:597 episode reward: total was -9.840000. running mean: -38.829843\n",
      "ep 1585: ep_len:2871 episode reward: total was -29.910000. running mean: -38.740645\n",
      "epsilon:0.009992 episode_count: 23910. steps_count: 25621361.000000\n",
      "ep 1586: ep_len:1144 episode reward: total was -3.480000. running mean: -38.388038\n",
      "ep 1586: ep_len:745 episode reward: total was -8.040000. running mean: -38.084558\n",
      "ep 1586: ep_len:2970 episode reward: total was -26.260000. running mean: -37.966312\n",
      "ep 1586: ep_len:1226 episode reward: total was -27.300000. running mean: -37.859649\n",
      "ep 1586: ep_len:57 episode reward: total was 25.500000. running mean: -37.226053\n",
      "ep 1586: ep_len:1065 episode reward: total was -56.180000. running mean: -37.415592\n",
      "ep 1586: ep_len:337 episode reward: total was 6.460000. running mean: -36.976836\n",
      "ep 1586: ep_len:1481 episode reward: total was -26.770000. running mean: -36.874768\n",
      "ep 1586: ep_len:7311 episode reward: total was -43.390000. running mean: -36.939920\n",
      "ep 1586: ep_len:500 episode reward: total was 35.870000. running mean: -36.211821\n",
      "ep 1586: ep_len:68 episode reward: total was 32.500000. running mean: -35.524703\n",
      "ep 1586: ep_len:647 episode reward: total was -5.820000. running mean: -35.227656\n",
      "ep 1586: ep_len:2857 episode reward: total was -31.650000. running mean: -35.191879\n",
      "epsilon:0.009992 episode_count: 23923. steps_count: 25641769.000000\n",
      "ep 1587: ep_len:632 episode reward: total was -4.470000. running mean: -34.884660\n",
      "ep 1587: ep_len:675 episode reward: total was -38.870000. running mean: -34.924514\n",
      "ep 1587: ep_len:2932 episode reward: total was -86.090000. running mean: -35.436169\n",
      "ep 1587: ep_len:691 episode reward: total was -11.210000. running mean: -35.193907\n",
      "ep 1587: ep_len:1528 episode reward: total was 37.920000. running mean: -34.462768\n",
      "ep 1587: ep_len:3874 episode reward: total was -9.110000. running mean: -34.209240\n",
      "ep 1587: ep_len:568 episode reward: total was -10.070000. running mean: -33.967848\n",
      "ep 1587: ep_len:709 episode reward: total was 38.810000. running mean: -33.240069\n",
      "ep 1587: ep_len:621 episode reward: total was -9.620000. running mean: -33.003869\n",
      "ep 1587: ep_len:70 episode reward: total was 32.000000. running mean: -32.353830\n",
      "ep 1587: ep_len:118 episode reward: total was 57.500000. running mean: -31.455292\n",
      "ep 1587: ep_len:881 episode reward: total was 19.990000. running mean: -30.940839\n",
      "ep 1587: ep_len:2828 episode reward: total was 0.440000. running mean: -30.627030\n",
      "epsilon:0.009992 episode_count: 23936. steps_count: 25657896.000000\n",
      "ep 1588: ep_len:989 episode reward: total was -360.120000. running mean: -33.921960\n",
      "ep 1588: ep_len:737 episode reward: total was -57.910000. running mean: -34.161840\n",
      "ep 1588: ep_len:3040 episode reward: total was -33.140000. running mean: -34.151622\n",
      "ep 1588: ep_len:771 episode reward: total was 28.500000. running mean: -33.525106\n",
      "ep 1588: ep_len:1470 episode reward: total was -1.170000. running mean: -33.201555\n",
      "ep 1588: ep_len:335 episode reward: total was 11.030000. running mean: -32.759239\n",
      "ep 1588: ep_len:779 episode reward: total was -20.610000. running mean: -32.637747\n",
      "ep 1588: ep_len:676 episode reward: total was 5.990000. running mean: -32.251469\n",
      "ep 1588: ep_len:1472 episode reward: total was -0.970000. running mean: -31.938655\n",
      "ep 1588: ep_len:55 episode reward: total was 26.000000. running mean: -31.359268\n",
      "ep 1588: ep_len:45 episode reward: total was 19.500000. running mean: -30.850675\n",
      "ep 1588: ep_len:649 episode reward: total was 8.460000. running mean: -30.457569\n",
      "ep 1588: ep_len:46 episode reward: total was 21.500000. running mean: -29.937993\n",
      "epsilon:0.009992 episode_count: 23949. steps_count: 25668960.000000\n",
      "ep 1589: ep_len:779 episode reward: total was -13.590000. running mean: -29.774513\n",
      "ep 1589: ep_len:827 episode reward: total was -51.660000. running mean: -29.993368\n",
      "ep 1589: ep_len:49 episode reward: total was 23.000000. running mean: -29.463434\n",
      "ep 1589: ep_len:3021 episode reward: total was -80.440000. running mean: -29.973200\n",
      "ep 1589: ep_len:500 episode reward: total was 2.420000. running mean: -29.649268\n",
      "ep 1589: ep_len:37 episode reward: total was 17.000000. running mean: -29.182775\n",
      "ep 1589: ep_len:77 episode reward: total was 35.500000. running mean: -28.535948\n",
      "ep 1589: ep_len:644 episode reward: total was -39.740000. running mean: -28.647988\n",
      "ep 1589: ep_len:341 episode reward: total was 11.120000. running mean: -28.250308\n",
      "ep 1589: ep_len:762 episode reward: total was -26.140000. running mean: -28.229205\n",
      "ep 1589: ep_len:7270 episode reward: total was 9.780000. running mean: -27.849113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1589: ep_len:500 episode reward: total was 26.830000. running mean: -27.302322\n",
      "ep 1589: ep_len:35 episode reward: total was 16.000000. running mean: -26.869299\n",
      "ep 1589: ep_len:66 episode reward: total was 31.500000. running mean: -26.285606\n",
      "ep 1589: ep_len:640 episode reward: total was -2.510000. running mean: -26.047850\n",
      "ep 1589: ep_len:2788 episode reward: total was -24.780000. running mean: -26.035171\n",
      "ep 1589: ep_len:48 episode reward: total was 22.500000. running mean: -25.549819\n",
      "epsilon:0.009992 episode_count: 23966. steps_count: 25687344.000000\n",
      "ep 1590: ep_len:1136 episode reward: total was -18.470000. running mean: -25.479021\n",
      "ep 1590: ep_len:775 episode reward: total was -3.530000. running mean: -25.259531\n",
      "ep 1590: ep_len:49 episode reward: total was 20.000000. running mean: -24.806936\n",
      "ep 1590: ep_len:2992 episode reward: total was -16.180000. running mean: -24.720666\n",
      "ep 1590: ep_len:828 episode reward: total was -20.660000. running mean: -24.680060\n",
      "ep 1590: ep_len:146 episode reward: total was 68.500000. running mean: -23.748259\n",
      "ep 1590: ep_len:680 episode reward: total was 0.570000. running mean: -23.505077\n",
      "ep 1590: ep_len:3543 episode reward: total was -8.800000. running mean: -23.358026\n",
      "ep 1590: ep_len:558 episode reward: total was 12.330000. running mean: -23.001145\n",
      "ep 1590: ep_len:740 episode reward: total was -16.060000. running mean: -22.931734\n",
      "ep 1590: ep_len:1508 episode reward: total was 10.930000. running mean: -22.593117\n",
      "ep 1590: ep_len:93 episode reward: total was 40.500000. running mean: -21.962186\n",
      "ep 1590: ep_len:1075 episode reward: total was 15.450000. running mean: -21.588064\n",
      "ep 1590: ep_len:2769 episode reward: total was -3.850000. running mean: -21.410683\n",
      "epsilon:0.009992 episode_count: 23980. steps_count: 25704236.000000\n",
      "ep 1591: ep_len:1439 episode reward: total was 6.080000. running mean: -21.135776\n",
      "ep 1591: ep_len:500 episode reward: total was 5.790000. running mean: -20.866518\n",
      "ep 1591: ep_len:44 episode reward: total was 20.500000. running mean: -20.452853\n",
      "ep 1591: ep_len:3033 episode reward: total was -45.510000. running mean: -20.703425\n",
      "ep 1591: ep_len:679 episode reward: total was -21.610000. running mean: -20.712490\n",
      "ep 1591: ep_len:59 episode reward: total was 28.000000. running mean: -20.225366\n",
      "ep 1591: ep_len:78 episode reward: total was 36.000000. running mean: -19.663112\n",
      "ep 1591: ep_len:657 episode reward: total was -13.750000. running mean: -19.603981\n",
      "ep 1591: ep_len:657 episode reward: total was 14.730000. running mean: -19.260641\n",
      "ep 1591: ep_len:1544 episode reward: total was -80.020000. running mean: -19.868235\n",
      "ep 1591: ep_len:652 episode reward: total was -0.320000. running mean: -19.672752\n",
      "ep 1591: ep_len:581 episode reward: total was 15.430000. running mean: -19.321725\n",
      "ep 1591: ep_len:63 episode reward: total was 30.000000. running mean: -18.828507\n",
      "ep 1591: ep_len:1086 episode reward: total was 10.330000. running mean: -18.536922\n",
      "ep 1591: ep_len:2793 episode reward: total was -29.560000. running mean: -18.647153\n",
      "epsilon:0.009992 episode_count: 23995. steps_count: 25718101.000000\n",
      "ep 1592: ep_len:4906 episode reward: total was -3798.630000. running mean: -56.446982\n",
      "ep 1592: ep_len:956 episode reward: total was 29.350000. running mean: -55.589012\n",
      "ep 1592: ep_len:3066 episode reward: total was -47.500000. running mean: -55.508122\n",
      "ep 1592: ep_len:820 episode reward: total was -14.430000. running mean: -55.097340\n",
      "ep 1592: ep_len:54 episode reward: total was 25.500000. running mean: -54.291367\n",
      "ep 1592: ep_len:80 episode reward: total was 38.500000. running mean: -53.363453\n",
      "ep 1592: ep_len:58 episode reward: total was 26.000000. running mean: -52.569819\n",
      "ep 1592: ep_len:500 episode reward: total was 58.270000. running mean: -51.461421\n",
      "ep 1592: ep_len:3710 episode reward: total was -64.240000. running mean: -51.589206\n",
      "ep 1592: ep_len:786 episode reward: total was -31.440000. running mean: -51.387714\n",
      "ep 1592: ep_len:672 episode reward: total was -1.470000. running mean: -50.888537\n",
      "ep 1592: ep_len:1091 episode reward: total was -5.420000. running mean: -50.433852\n",
      "ep 1592: ep_len:38 episode reward: total was 17.500000. running mean: -49.754513\n",
      "ep 1592: ep_len:60 episode reward: total was 28.500000. running mean: -48.971968\n",
      "ep 1592: ep_len:621 episode reward: total was -16.150000. running mean: -48.643749\n",
      "ep 1592: ep_len:2756 episode reward: total was -16.800000. running mean: -48.325311\n",
      "epsilon:0.009992 episode_count: 24011. steps_count: 25738275.000000\n",
      "ep 1593: ep_len:952 episode reward: total was -88.870000. running mean: -48.730758\n",
      "ep 1593: ep_len:200 episode reward: total was 6.890000. running mean: -48.174550\n",
      "ep 1593: ep_len:3025 episode reward: total was -22.880000. running mean: -47.921605\n",
      "ep 1593: ep_len:500 episode reward: total was -8.090000. running mean: -47.523289\n",
      "ep 1593: ep_len:905 episode reward: total was 66.230000. running mean: -46.385756\n",
      "ep 1593: ep_len:358 episode reward: total was 12.150000. running mean: -45.800398\n",
      "ep 1593: ep_len:540 episode reward: total was 2.200000. running mean: -45.320394\n",
      "ep 1593: ep_len:767 episode reward: total was 9.650000. running mean: -44.770690\n",
      "ep 1593: ep_len:500 episode reward: total was 58.330000. running mean: -43.739684\n",
      "ep 1593: ep_len:183 episode reward: total was 90.000000. running mean: -42.402287\n",
      "ep 1593: ep_len:577 episode reward: total was -0.590000. running mean: -41.984164\n",
      "ep 1593: ep_len:2787 episode reward: total was 4.320000. running mean: -41.521122\n",
      "ep 1593: ep_len:49 episode reward: total was 23.000000. running mean: -40.875911\n",
      "epsilon:0.009992 episode_count: 24024. steps_count: 25749618.000000\n",
      "ep 1594: ep_len:1455 episode reward: total was -4.230000. running mean: -40.509452\n",
      "ep 1594: ep_len:1692 episode reward: total was -142.770000. running mean: -41.532057\n",
      "ep 1594: ep_len:2927 episode reward: total was -22.850000. running mean: -41.345237\n",
      "ep 1594: ep_len:845 episode reward: total was 32.830000. running mean: -40.603484\n",
      "ep 1594: ep_len:101 episode reward: total was 49.000000. running mean: -39.707450\n",
      "ep 1594: ep_len:69 episode reward: total was 31.500000. running mean: -38.995375\n",
      "ep 1594: ep_len:679 episode reward: total was 25.580000. running mean: -38.349621\n",
      "ep 1594: ep_len:326 episode reward: total was 13.570000. running mean: -37.830425\n",
      "ep 1594: ep_len:538 episode reward: total was -7.950000. running mean: -37.531621\n",
      "ep 1594: ep_len:789 episode reward: total was -5.460000. running mean: -37.210905\n",
      "ep 1594: ep_len:983 episode reward: total was -6.530000. running mean: -36.904096\n",
      "ep 1594: ep_len:51 episode reward: total was 24.000000. running mean: -36.295055\n",
      "ep 1594: ep_len:500 episode reward: total was 44.280000. running mean: -35.489304\n",
      "ep 1594: ep_len:2897 episode reward: total was 9.370000. running mean: -35.040711\n",
      "epsilon:0.009992 episode_count: 24038. steps_count: 25763470.000000\n",
      "ep 1595: ep_len:1388 episode reward: total was 12.700000. running mean: -34.563304\n",
      "ep 1595: ep_len:1656 episode reward: total was -109.280000. running mean: -35.310471\n",
      "ep 1595: ep_len:2976 episode reward: total was -75.320000. running mean: -35.710566\n",
      "ep 1595: ep_len:776 episode reward: total was 25.400000. running mean: -35.099461\n",
      "ep 1595: ep_len:75 episode reward: total was 36.000000. running mean: -34.388466\n",
      "ep 1595: ep_len:1424 episode reward: total was 21.230000. running mean: -33.832281\n",
      "ep 1595: ep_len:336 episode reward: total was 23.560000. running mean: -33.258358\n",
      "ep 1595: ep_len:591 episode reward: total was -36.680000. running mean: -33.292575\n",
      "ep 1595: ep_len:713 episode reward: total was -17.370000. running mean: -33.133349\n",
      "ep 1595: ep_len:500 episode reward: total was 18.560000. running mean: -32.616416\n",
      "ep 1595: ep_len:80 episode reward: total was 38.500000. running mean: -31.905251\n",
      "ep 1595: ep_len:791 episode reward: total was -52.810000. running mean: -32.114299\n",
      "ep 1595: ep_len:2844 episode reward: total was -63.060000. running mean: -32.423756\n",
      "epsilon:0.009992 episode_count: 24051. steps_count: 25777620.000000\n",
      "ep 1596: ep_len:830 episode reward: total was -23.180000. running mean: -32.331318\n",
      "ep 1596: ep_len:199 episode reward: total was 11.290000. running mean: -31.895105\n",
      "ep 1596: ep_len:2916 episode reward: total was -68.980000. running mean: -32.265954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1596: ep_len:623 episode reward: total was -0.180000. running mean: -31.945095\n",
      "ep 1596: ep_len:132 episode reward: total was 64.500000. running mean: -30.980644\n",
      "ep 1596: ep_len:1421 episode reward: total was -296.030000. running mean: -33.631137\n",
      "ep 1596: ep_len:3786 episode reward: total was -183.530000. running mean: -35.130126\n",
      "ep 1596: ep_len:865 episode reward: total was -27.880000. running mean: -35.057625\n",
      "ep 1596: ep_len:781 episode reward: total was -8.580000. running mean: -34.792848\n",
      "ep 1596: ep_len:664 episode reward: total was 0.650000. running mean: -34.438420\n",
      "ep 1596: ep_len:62 episode reward: total was 29.500000. running mean: -33.799036\n",
      "ep 1596: ep_len:606 episode reward: total was -7.790000. running mean: -33.538945\n",
      "ep 1596: ep_len:2904 episode reward: total was -75.060000. running mean: -33.954156\n",
      "ep 1596: ep_len:38 episode reward: total was 16.000000. running mean: -33.454614\n",
      "epsilon:0.009992 episode_count: 24065. steps_count: 25793447.000000\n",
      "ep 1597: ep_len:631 episode reward: total was 7.390000. running mean: -33.046168\n",
      "ep 1597: ep_len:712 episode reward: total was -48.160000. running mean: -33.197306\n",
      "ep 1597: ep_len:2968 episode reward: total was -59.780000. running mean: -33.463133\n",
      "ep 1597: ep_len:1047 episode reward: total was 10.730000. running mean: -33.021202\n",
      "ep 1597: ep_len:996 episode reward: total was -44.750000. running mean: -33.138490\n",
      "ep 1597: ep_len:3628 episode reward: total was -104.450000. running mean: -33.851605\n",
      "ep 1597: ep_len:1617 episode reward: total was -45.800000. running mean: -33.971089\n",
      "ep 1597: ep_len:719 episode reward: total was -5.070000. running mean: -33.682078\n",
      "ep 1597: ep_len:500 episode reward: total was 20.850000. running mean: -33.136757\n",
      "ep 1597: ep_len:63 episode reward: total was 27.000000. running mean: -32.535390\n",
      "ep 1597: ep_len:691 episode reward: total was -8.410000. running mean: -32.294136\n",
      "ep 1597: ep_len:2720 episode reward: total was -37.000000. running mean: -32.341195\n",
      "ep 1597: ep_len:37 episode reward: total was 15.500000. running mean: -31.862783\n",
      "epsilon:0.009992 episode_count: 24078. steps_count: 25809776.000000\n",
      "ep 1598: ep_len:1404 episode reward: total was 24.980000. running mean: -31.294355\n",
      "ep 1598: ep_len:735 episode reward: total was -41.610000. running mean: -31.397511\n",
      "ep 1598: ep_len:53 episode reward: total was 25.000000. running mean: -30.833536\n",
      "ep 1598: ep_len:3006 episode reward: total was -60.660000. running mean: -31.131801\n",
      "ep 1598: ep_len:832 episode reward: total was -1.500000. running mean: -30.835483\n",
      "ep 1598: ep_len:90 episode reward: total was 40.500000. running mean: -30.122128\n",
      "ep 1598: ep_len:500 episode reward: total was 25.450000. running mean: -29.566407\n",
      "ep 1598: ep_len:335 episode reward: total was 3.040000. running mean: -29.240343\n",
      "ep 1598: ep_len:649 episode reward: total was -41.640000. running mean: -29.364339\n",
      "ep 1598: ep_len:7342 episode reward: total was -106.100000. running mean: -30.131696\n",
      "ep 1598: ep_len:600 episode reward: total was -0.960000. running mean: -29.839979\n",
      "ep 1598: ep_len:172 episode reward: total was 80.000000. running mean: -28.741579\n",
      "ep 1598: ep_len:47 episode reward: total was 19.000000. running mean: -28.264163\n",
      "ep 1598: ep_len:62 episode reward: total was 29.500000. running mean: -27.686522\n",
      "ep 1598: ep_len:980 episode reward: total was -116.620000. running mean: -28.575856\n",
      "ep 1598: ep_len:2895 episode reward: total was -18.600000. running mean: -28.476098\n",
      "epsilon:0.009992 episode_count: 24094. steps_count: 25829478.000000\n",
      "ep 1599: ep_len:717 episode reward: total was -48.420000. running mean: -28.675537\n",
      "ep 1599: ep_len:684 episode reward: total was -47.650000. running mean: -28.865282\n",
      "ep 1599: ep_len:2987 episode reward: total was -33.560000. running mean: -28.912229\n",
      "ep 1599: ep_len:661 episode reward: total was -0.090000. running mean: -28.624006\n",
      "ep 1599: ep_len:153 episode reward: total was 75.000000. running mean: -27.587766\n",
      "ep 1599: ep_len:673 episode reward: total was 2.950000. running mean: -27.282389\n",
      "ep 1599: ep_len:3612 episode reward: total was -103.050000. running mean: -28.040065\n",
      "ep 1599: ep_len:597 episode reward: total was 21.820000. running mean: -27.541464\n",
      "ep 1599: ep_len:804 episode reward: total was -2.980000. running mean: -27.295850\n",
      "ep 1599: ep_len:794 episode reward: total was 26.980000. running mean: -26.753091\n",
      "ep 1599: ep_len:193 episode reward: total was 92.000000. running mean: -25.565560\n",
      "ep 1599: ep_len:1104 episode reward: total was -2.260000. running mean: -25.332505\n",
      "ep 1599: ep_len:2835 episode reward: total was 13.340000. running mean: -24.945779\n",
      "ep 1599: ep_len:30 episode reward: total was 13.500000. running mean: -24.561322\n",
      "epsilon:0.009992 episode_count: 24108. steps_count: 25845322.000000\n",
      "ep 1600: ep_len:1140 episode reward: total was 8.200000. running mean: -24.233708\n",
      "ep 1600: ep_len:500 episode reward: total was 22.600000. running mean: -23.765371\n",
      "ep 1600: ep_len:57 episode reward: total was 27.000000. running mean: -23.257718\n",
      "ep 1600: ep_len:3043 episode reward: total was -27.960000. running mean: -23.304740\n",
      "ep 1600: ep_len:629 episode reward: total was -3.810000. running mean: -23.109793\n",
      "ep 1600: ep_len:84 episode reward: total was 39.000000. running mean: -22.488695\n",
      "ep 1600: ep_len:638 episode reward: total was -37.290000. running mean: -22.636708\n",
      "ep 1600: ep_len:353 episode reward: total was 7.200000. running mean: -22.338341\n",
      "ep 1600: ep_len:586 episode reward: total was -44.810000. running mean: -22.563058\n",
      "ep 1600: ep_len:717 episode reward: total was 20.400000. running mean: -22.133427\n",
      "ep 1600: ep_len:1500 episode reward: total was -1.180000. running mean: -21.923893\n",
      "ep 1600: ep_len:103 episode reward: total was 48.500000. running mean: -21.219654\n",
      "ep 1600: ep_len:500 episode reward: total was 0.640000. running mean: -21.001057\n",
      "ep 1600: ep_len:2872 episode reward: total was 6.180000. running mean: -20.729247\n",
      "epsilon:0.009992 episode_count: 24122. steps_count: 25858044.000000\n",
      "ep 1601: ep_len:674 episode reward: total was 46.490000. running mean: -20.057054\n",
      "ep 1601: ep_len:683 episode reward: total was -41.510000. running mean: -20.271584\n",
      "ep 1601: ep_len:37 episode reward: total was 17.000000. running mean: -19.898868\n",
      "ep 1601: ep_len:2985 episode reward: total was -44.120000. running mean: -20.141079\n",
      "ep 1601: ep_len:797 episode reward: total was 21.510000. running mean: -19.724568\n",
      "ep 1601: ep_len:39 episode reward: total was 16.500000. running mean: -19.362323\n",
      "ep 1601: ep_len:33 episode reward: total was 15.000000. running mean: -19.018700\n",
      "ep 1601: ep_len:762 episode reward: total was -11.740000. running mean: -18.945913\n",
      "ep 1601: ep_len:3932 episode reward: total was -21.720000. running mean: -18.973653\n",
      "ep 1601: ep_len:638 episode reward: total was -95.250000. running mean: -19.736417\n",
      "ep 1601: ep_len:805 episode reward: total was 11.020000. running mean: -19.428853\n",
      "ep 1601: ep_len:973 episode reward: total was -7.170000. running mean: -19.306264\n",
      "ep 1601: ep_len:82 episode reward: total was 38.000000. running mean: -18.733202\n",
      "ep 1601: ep_len:784 episode reward: total was -24.970000. running mean: -18.795570\n",
      "ep 1601: ep_len:2872 episode reward: total was -52.710000. running mean: -19.134714\n",
      "ep 1601: ep_len:54 episode reward: total was 25.500000. running mean: -18.688367\n",
      "epsilon:0.009992 episode_count: 24138. steps_count: 25874194.000000\n",
      "ep 1602: ep_len:1179 episode reward: total was -3.530000. running mean: -18.536783\n",
      "ep 1602: ep_len:500 episode reward: total was 22.720000. running mean: -18.124215\n",
      "ep 1602: ep_len:3027 episode reward: total was -24.770000. running mean: -18.190673\n",
      "ep 1602: ep_len:641 episode reward: total was 3.150000. running mean: -17.977266\n",
      "ep 1602: ep_len:47 episode reward: total was 20.500000. running mean: -17.592494\n",
      "ep 1602: ep_len:500 episode reward: total was 41.920000. running mean: -16.997369\n",
      "ep 1602: ep_len:3899 episode reward: total was -126.420000. running mean: -18.091595\n",
      "ep 1602: ep_len:1267 episode reward: total was -79.900000. running mean: -18.709679\n",
      "ep 1602: ep_len:879 episode reward: total was 57.570000. running mean: -17.946882\n",
      "ep 1602: ep_len:680 episode reward: total was -8.520000. running mean: -17.852613\n",
      "ep 1602: ep_len:96 episode reward: total was 45.000000. running mean: -17.224087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1602: ep_len:128 episode reward: total was 61.000000. running mean: -16.441846\n",
      "ep 1602: ep_len:1066 episode reward: total was 10.580000. running mean: -16.171628\n",
      "ep 1602: ep_len:2775 episode reward: total was -16.860000. running mean: -16.178512\n",
      "ep 1602: ep_len:35 episode reward: total was 16.000000. running mean: -15.856727\n",
      "epsilon:0.009992 episode_count: 24153. steps_count: 25890913.000000\n",
      "ep 1603: ep_len:1144 episode reward: total was 5.460000. running mean: -15.643559\n",
      "ep 1603: ep_len:737 episode reward: total was -27.980000. running mean: -15.766924\n",
      "ep 1603: ep_len:61 episode reward: total was 27.500000. running mean: -15.334255\n",
      "ep 1603: ep_len:3024 episode reward: total was -19.470000. running mean: -15.375612\n",
      "ep 1603: ep_len:574 episode reward: total was -14.630000. running mean: -15.368156\n",
      "ep 1603: ep_len:59 episode reward: total was 26.500000. running mean: -14.949474\n",
      "ep 1603: ep_len:500 episode reward: total was 22.570000. running mean: -14.574280\n",
      "ep 1603: ep_len:682 episode reward: total was 7.420000. running mean: -14.354337\n",
      "ep 1603: ep_len:940 episode reward: total was -35.210000. running mean: -14.562893\n",
      "ep 1603: ep_len:692 episode reward: total was -8.800000. running mean: -14.505264\n",
      "ep 1603: ep_len:946 episode reward: total was 23.770000. running mean: -14.122512\n",
      "ep 1603: ep_len:162 episode reward: total was 75.000000. running mean: -13.231287\n",
      "ep 1603: ep_len:500 episode reward: total was 41.890000. running mean: -12.680074\n",
      "ep 1603: ep_len:2842 episode reward: total was -1118.990000. running mean: -23.743173\n",
      "ep 1603: ep_len:69 episode reward: total was 33.000000. running mean: -23.175741\n",
      "epsilon:0.009992 episode_count: 24168. steps_count: 25903845.000000\n",
      "ep 1604: ep_len:927 episode reward: total was -137.290000. running mean: -24.316884\n",
      "ep 1604: ep_len:773 episode reward: total was -11.630000. running mean: -24.190015\n",
      "ep 1604: ep_len:3079 episode reward: total was -11.990000. running mean: -24.068015\n",
      "ep 1604: ep_len:535 episode reward: total was -33.200000. running mean: -24.159335\n",
      "ep 1604: ep_len:47 episode reward: total was 22.000000. running mean: -23.697741\n",
      "ep 1604: ep_len:1394 episode reward: total was -122.580000. running mean: -24.686564\n",
      "ep 1604: ep_len:3583 episode reward: total was -49.260000. running mean: -24.932298\n",
      "ep 1604: ep_len:3932 episode reward: total was -457.060000. running mean: -29.253575\n",
      "ep 1604: ep_len:614 episode reward: total was -12.300000. running mean: -29.084040\n",
      "ep 1604: ep_len:753 episode reward: total was -6.780000. running mean: -28.860999\n",
      "ep 1604: ep_len:68 episode reward: total was 31.000000. running mean: -28.262389\n",
      "ep 1604: ep_len:637 episode reward: total was -3.390000. running mean: -28.013665\n",
      "ep 1604: ep_len:46 episode reward: total was 21.500000. running mean: -27.518529\n",
      "ep 1604: ep_len:45 episode reward: total was 18.000000. running mean: -27.063343\n",
      "epsilon:0.009992 episode_count: 24182. steps_count: 25920278.000000\n",
      "ep 1605: ep_len:1467 episode reward: total was 21.170000. running mean: -26.581010\n",
      "ep 1605: ep_len:696 episode reward: total was -39.870000. running mean: -26.713900\n",
      "ep 1605: ep_len:54 episode reward: total was 25.500000. running mean: -26.191761\n",
      "ep 1605: ep_len:3083 episode reward: total was -598.400000. running mean: -31.913843\n",
      "ep 1605: ep_len:678 episode reward: total was 7.890000. running mean: -31.515805\n",
      "ep 1605: ep_len:30 episode reward: total was 13.500000. running mean: -31.065647\n",
      "ep 1605: ep_len:43 episode reward: total was 20.000000. running mean: -30.554990\n",
      "ep 1605: ep_len:630 episode reward: total was -4.530000. running mean: -30.294740\n",
      "ep 1605: ep_len:628 episode reward: total was 1.740000. running mean: -29.974393\n",
      "ep 1605: ep_len:1586 episode reward: total was -26.790000. running mean: -29.942549\n",
      "ep 1605: ep_len:765 episode reward: total was -0.690000. running mean: -29.650024\n",
      "ep 1605: ep_len:712 episode reward: total was 21.690000. running mean: -29.136623\n",
      "ep 1605: ep_len:91 episode reward: total was 39.500000. running mean: -28.450257\n",
      "ep 1605: ep_len:58 episode reward: total was 26.000000. running mean: -27.905755\n",
      "ep 1605: ep_len:607 episode reward: total was -20.510000. running mean: -27.831797\n",
      "ep 1605: ep_len:2905 episode reward: total was -28.080000. running mean: -27.834279\n",
      "ep 1605: ep_len:52 episode reward: total was 21.500000. running mean: -27.340936\n",
      "epsilon:0.009992 episode_count: 24199. steps_count: 25934363.000000\n",
      "ep 1606: ep_len:961 episode reward: total was -68.160000. running mean: -27.749127\n",
      "ep 1606: ep_len:1600 episode reward: total was -33.690000. running mean: -27.808536\n",
      "ep 1606: ep_len:89 episode reward: total was 40.000000. running mean: -27.130450\n",
      "ep 1606: ep_len:1484 episode reward: total was 22.900000. running mean: -26.630146\n",
      "ep 1606: ep_len:28 episode reward: total was 11.000000. running mean: -26.253844\n",
      "ep 1606: ep_len:500 episode reward: total was 56.280000. running mean: -25.428506\n",
      "ep 1606: ep_len:654 episode reward: total was 15.560000. running mean: -25.018621\n",
      "ep 1606: ep_len:599 episode reward: total was -32.560000. running mean: -25.094035\n",
      "ep 1606: ep_len:708 episode reward: total was -0.920000. running mean: -24.852294\n",
      "ep 1606: ep_len:636 episode reward: total was 16.480000. running mean: -24.438971\n",
      "ep 1606: ep_len:782 episode reward: total was -21.360000. running mean: -24.408182\n",
      "ep 1606: ep_len:2825 episode reward: total was -26.610000. running mean: -24.430200\n",
      "epsilon:0.009992 episode_count: 24211. steps_count: 25945229.000000\n",
      "ep 1607: ep_len:1100 episode reward: total was -6.740000. running mean: -24.253298\n",
      "ep 1607: ep_len:618 episode reward: total was -34.390000. running mean: -24.354665\n",
      "ep 1607: ep_len:34 episode reward: total was 12.500000. running mean: -23.986118\n",
      "ep 1607: ep_len:2890 episode reward: total was -44.590000. running mean: -24.192157\n",
      "ep 1607: ep_len:546 episode reward: total was -8.660000. running mean: -24.036835\n",
      "ep 1607: ep_len:156 episode reward: total was 72.000000. running mean: -23.076467\n",
      "ep 1607: ep_len:634 episode reward: total was 2.130000. running mean: -22.824402\n",
      "ep 1607: ep_len:611 episode reward: total was 19.990000. running mean: -22.396258\n",
      "ep 1607: ep_len:536 episode reward: total was 3.110000. running mean: -22.141196\n",
      "ep 1607: ep_len:732 episode reward: total was 4.340000. running mean: -21.876384\n",
      "ep 1607: ep_len:583 episode reward: total was -8.480000. running mean: -21.742420\n",
      "ep 1607: ep_len:46 episode reward: total was 21.500000. running mean: -21.309996\n",
      "ep 1607: ep_len:74 episode reward: total was 34.000000. running mean: -20.756896\n",
      "ep 1607: ep_len:948 episode reward: total was -64.910000. running mean: -21.198427\n",
      "ep 1607: ep_len:2848 episode reward: total was -18.240000. running mean: -21.168843\n",
      "ep 1607: ep_len:69 episode reward: total was 33.000000. running mean: -20.627154\n",
      "epsilon:0.009992 episode_count: 24227. steps_count: 25957654.000000\n",
      "ep 1608: ep_len:1466 episode reward: total was 21.010000. running mean: -20.210783\n",
      "ep 1608: ep_len:904 episode reward: total was -12.340000. running mean: -20.132075\n",
      "ep 1608: ep_len:70 episode reward: total was 32.000000. running mean: -19.610754\n",
      "ep 1608: ep_len:2892 episode reward: total was -48.990000. running mean: -19.904547\n",
      "ep 1608: ep_len:1484 episode reward: total was 29.940000. running mean: -19.406101\n",
      "ep 1608: ep_len:673 episode reward: total was 9.740000. running mean: -19.114640\n",
      "ep 1608: ep_len:3569 episode reward: total was -164.540000. running mean: -20.568894\n",
      "ep 1608: ep_len:1532 episode reward: total was -98.260000. running mean: -21.345805\n",
      "ep 1608: ep_len:7368 episode reward: total was -1618.910000. running mean: -37.321447\n",
      "ep 1608: ep_len:613 episode reward: total was -6.160000. running mean: -37.009832\n",
      "ep 1608: ep_len:70 episode reward: total was 33.500000. running mean: -36.304734\n",
      "ep 1608: ep_len:628 episode reward: total was 14.060000. running mean: -35.801087\n",
      "ep 1608: ep_len:2852 episode reward: total was 19.450000. running mean: -35.248576\n",
      "epsilon:0.009992 episode_count: 24240. steps_count: 25981775.000000\n",
      "ep 1609: ep_len:715 episode reward: total was -41.500000. running mean: -35.311090\n",
      "ep 1609: ep_len:762 episode reward: total was -34.450000. running mean: -35.302479\n",
      "ep 1609: ep_len:2986 episode reward: total was -54.860000. running mean: -35.498054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1609: ep_len:1658 episode reward: total was -53.680000. running mean: -35.679874\n",
      "ep 1609: ep_len:30 episode reward: total was 12.000000. running mean: -35.203075\n",
      "ep 1609: ep_len:58 episode reward: total was 27.500000. running mean: -34.576044\n",
      "ep 1609: ep_len:684 episode reward: total was 0.530000. running mean: -34.224984\n",
      "ep 1609: ep_len:3717 episode reward: total was -10.090000. running mean: -33.983634\n",
      "ep 1609: ep_len:1168 episode reward: total was -63.170000. running mean: -34.275498\n",
      "ep 1609: ep_len:794 episode reward: total was 21.010000. running mean: -33.722643\n",
      "ep 1609: ep_len:499 episode reward: total was 42.710000. running mean: -32.958316\n",
      "ep 1609: ep_len:912 episode reward: total was 11.430000. running mean: -32.514433\n",
      "ep 1609: ep_len:2929 episode reward: total was 2.900000. running mean: -32.160289\n",
      "ep 1609: ep_len:58 episode reward: total was 26.000000. running mean: -31.578686\n",
      "epsilon:0.009992 episode_count: 24254. steps_count: 25998745.000000\n",
      "ep 1610: ep_len:646 episode reward: total was -13.910000. running mean: -31.401999\n",
      "ep 1610: ep_len:924 episode reward: total was 7.330000. running mean: -31.014679\n",
      "ep 1610: ep_len:2996 episode reward: total was -61.830000. running mean: -31.322832\n",
      "ep 1610: ep_len:1697 episode reward: total was -86.100000. running mean: -31.870604\n",
      "ep 1610: ep_len:93 episode reward: total was 42.000000. running mean: -31.131898\n",
      "ep 1610: ep_len:1122 episode reward: total was 26.490000. running mean: -30.555679\n",
      "ep 1610: ep_len:3634 episode reward: total was -46.330000. running mean: -30.713422\n",
      "ep 1610: ep_len:1573 episode reward: total was -4.580000. running mean: -30.452088\n",
      "ep 1610: ep_len:674 episode reward: total was -0.030000. running mean: -30.147867\n",
      "ep 1610: ep_len:594 episode reward: total was -7.790000. running mean: -29.924288\n",
      "ep 1610: ep_len:87 episode reward: total was 40.500000. running mean: -29.220045\n",
      "ep 1610: ep_len:661 episode reward: total was -5.610000. running mean: -28.983945\n",
      "ep 1610: ep_len:2812 episode reward: total was -49.260000. running mean: -29.186705\n",
      "epsilon:0.009992 episode_count: 24267. steps_count: 26016258.000000\n",
      "ep 1611: ep_len:661 episode reward: total was -45.040000. running mean: -29.345238\n",
      "ep 1611: ep_len:959 episode reward: total was 27.850000. running mean: -28.773286\n",
      "ep 1611: ep_len:65 episode reward: total was 29.500000. running mean: -28.190553\n",
      "ep 1611: ep_len:2978 episode reward: total was -13.440000. running mean: -28.043048\n",
      "ep 1611: ep_len:663 episode reward: total was 23.510000. running mean: -27.527517\n",
      "ep 1611: ep_len:71 episode reward: total was 34.000000. running mean: -26.912242\n",
      "ep 1611: ep_len:1121 episode reward: total was 0.940000. running mean: -26.633720\n",
      "ep 1611: ep_len:3847 episode reward: total was -783.250000. running mean: -34.199882\n",
      "ep 1611: ep_len:782 episode reward: total was -16.750000. running mean: -34.025384\n",
      "ep 1611: ep_len:7208 episode reward: total was -71.940000. running mean: -34.404530\n",
      "ep 1611: ep_len:952 episode reward: total was 12.290000. running mean: -33.937584\n",
      "ep 1611: ep_len:61 episode reward: total was 27.500000. running mean: -33.323209\n",
      "ep 1611: ep_len:695 episode reward: total was -36.650000. running mean: -33.356476\n",
      "ep 1611: ep_len:2716 episode reward: total was -5.480000. running mean: -33.077712\n",
      "epsilon:0.009992 episode_count: 24281. steps_count: 26039037.000000\n",
      "ep 1612: ep_len:1490 episode reward: total was 23.400000. running mean: -32.512935\n",
      "ep 1612: ep_len:500 episode reward: total was 11.730000. running mean: -32.070505\n",
      "ep 1612: ep_len:50 episode reward: total was 23.500000. running mean: -31.514800\n",
      "ep 1612: ep_len:2879 episode reward: total was -78.710000. running mean: -31.986752\n",
      "ep 1612: ep_len:500 episode reward: total was -31.280000. running mean: -31.979685\n",
      "ep 1612: ep_len:111 episode reward: total was 54.000000. running mean: -31.119888\n",
      "ep 1612: ep_len:1429 episode reward: total was -134.350000. running mean: -32.152189\n",
      "ep 1612: ep_len:669 episode reward: total was 21.310000. running mean: -31.617567\n",
      "ep 1612: ep_len:749 episode reward: total was -25.160000. running mean: -31.552991\n",
      "ep 1612: ep_len:781 episode reward: total was 36.190000. running mean: -30.875561\n",
      "ep 1612: ep_len:567 episode reward: total was 16.170000. running mean: -30.405106\n",
      "ep 1612: ep_len:45 episode reward: total was 21.000000. running mean: -29.891055\n",
      "ep 1612: ep_len:179 episode reward: total was 86.500000. running mean: -28.727144\n",
      "ep 1612: ep_len:50 episode reward: total was 23.500000. running mean: -28.204873\n",
      "ep 1612: ep_len:84 episode reward: total was 40.500000. running mean: -27.517824\n",
      "ep 1612: ep_len:1144 episode reward: total was -1.860000. running mean: -27.261246\n",
      "ep 1612: ep_len:2812 episode reward: total was -33.600000. running mean: -27.324633\n",
      "epsilon:0.009992 episode_count: 24298. steps_count: 26053076.000000\n",
      "ep 1613: ep_len:629 episode reward: total was 14.960000. running mean: -26.901787\n",
      "ep 1613: ep_len:870 episode reward: total was 5.100000. running mean: -26.581769\n",
      "ep 1613: ep_len:2981 episode reward: total was -30.400000. running mean: -26.619952\n",
      "ep 1613: ep_len:680 episode reward: total was -2.530000. running mean: -26.379052\n",
      "ep 1613: ep_len:39 episode reward: total was 18.000000. running mean: -25.935261\n",
      "ep 1613: ep_len:96 episode reward: total was 43.500000. running mean: -25.240909\n",
      "ep 1613: ep_len:650 episode reward: total was -18.440000. running mean: -25.172900\n",
      "ep 1613: ep_len:3653 episode reward: total was -71.330000. running mean: -25.634471\n",
      "ep 1613: ep_len:500 episode reward: total was -28.280000. running mean: -25.660926\n",
      "ep 1613: ep_len:833 episode reward: total was 39.220000. running mean: -25.012117\n",
      "ep 1613: ep_len:953 episode reward: total was 19.620000. running mean: -24.565796\n",
      "ep 1613: ep_len:86 episode reward: total was 40.000000. running mean: -23.920138\n",
      "ep 1613: ep_len:759 episode reward: total was -69.340000. running mean: -24.374336\n",
      "ep 1613: ep_len:2812 episode reward: total was -9.790000. running mean: -24.228493\n",
      "epsilon:0.009992 episode_count: 24312. steps_count: 26068617.000000\n",
      "ep 1614: ep_len:708 episode reward: total was -25.440000. running mean: -24.240608\n",
      "ep 1614: ep_len:1675 episode reward: total was -63.240000. running mean: -24.630602\n",
      "ep 1614: ep_len:77 episode reward: total was 36.510000. running mean: -24.019196\n",
      "ep 1614: ep_len:91 episode reward: total was 42.500000. running mean: -23.354004\n",
      "ep 1614: ep_len:514 episode reward: total was -15.960000. running mean: -23.280064\n",
      "ep 1614: ep_len:1409 episode reward: total was -98.190000. running mean: -24.029163\n",
      "ep 1614: ep_len:323 episode reward: total was 25.940000. running mean: -23.529472\n",
      "ep 1614: ep_len:687 episode reward: total was -30.670000. running mean: -23.600877\n",
      "ep 1614: ep_len:807 episode reward: total was 46.590000. running mean: -22.898968\n",
      "ep 1614: ep_len:480 episode reward: total was 24.730000. running mean: -22.422678\n",
      "ep 1614: ep_len:115 episode reward: total was 56.000000. running mean: -21.638452\n",
      "ep 1614: ep_len:97 episode reward: total was 45.500000. running mean: -20.967067\n",
      "ep 1614: ep_len:634 episode reward: total was -0.900000. running mean: -20.766396\n",
      "ep 1614: ep_len:2784 episode reward: total was 7.690000. running mean: -20.481833\n",
      "epsilon:0.009992 episode_count: 24326. steps_count: 26079018.000000\n",
      "ep 1615: ep_len:632 episode reward: total was 1.100000. running mean: -20.266014\n",
      "ep 1615: ep_len:501 episode reward: total was -3.600000. running mean: -20.099354\n",
      "ep 1615: ep_len:2942 episode reward: total was -34.120000. running mean: -20.239561\n",
      "ep 1615: ep_len:1575 episode reward: total was -66.300000. running mean: -20.700165\n",
      "ep 1615: ep_len:136 episode reward: total was 66.500000. running mean: -19.828163\n",
      "ep 1615: ep_len:57 episode reward: total was 27.000000. running mean: -19.359882\n",
      "ep 1615: ep_len:862 episode reward: total was 10.960000. running mean: -19.056683\n",
      "ep 1615: ep_len:3761 episode reward: total was -9.080000. running mean: -18.956916\n",
      "ep 1615: ep_len:1624 episode reward: total was -11.200000. running mean: -18.879347\n",
      "ep 1615: ep_len:631 episode reward: total was -2.180000. running mean: -18.712353\n",
      "ep 1615: ep_len:500 episode reward: total was -2.510000. running mean: -18.550330\n",
      "ep 1615: ep_len:64 episode reward: total was 29.000000. running mean: -18.074827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1615: ep_len:132 episode reward: total was 63.000000. running mean: -17.264078\n",
      "ep 1615: ep_len:620 episode reward: total was 2.600000. running mean: -17.065437\n",
      "ep 1615: ep_len:2777 episode reward: total was -3.650000. running mean: -16.931283\n",
      "epsilon:0.009992 episode_count: 24341. steps_count: 26095832.000000\n",
      "ep 1616: ep_len:1201 episode reward: total was 23.700000. running mean: -16.524970\n",
      "ep 1616: ep_len:779 episode reward: total was -3.240000. running mean: -16.392121\n",
      "ep 1616: ep_len:72 episode reward: total was 33.000000. running mean: -15.898199\n",
      "ep 1616: ep_len:3053 episode reward: total was -20.220000. running mean: -15.941417\n",
      "ep 1616: ep_len:1126 episode reward: total was -1.030000. running mean: -15.792303\n",
      "ep 1616: ep_len:48 episode reward: total was 19.500000. running mean: -15.439380\n",
      "ep 1616: ep_len:1657 episode reward: total was -390.630000. running mean: -19.191286\n",
      "ep 1616: ep_len:3973 episode reward: total was -85.280000. running mean: -19.852174\n",
      "ep 1616: ep_len:559 episode reward: total was 0.370000. running mean: -19.649952\n",
      "ep 1616: ep_len:7287 episode reward: total was -392.050000. running mean: -23.373952\n",
      "ep 1616: ep_len:500 episode reward: total was 36.900000. running mean: -22.771213\n",
      "ep 1616: ep_len:36 episode reward: total was 16.500000. running mean: -22.378501\n",
      "ep 1616: ep_len:185 episode reward: total was 91.000000. running mean: -21.244716\n",
      "ep 1616: ep_len:66 episode reward: total was 31.500000. running mean: -20.717268\n",
      "ep 1616: ep_len:627 episode reward: total was -10.670000. running mean: -20.616796\n",
      "ep 1616: ep_len:2806 episode reward: total was -10.060000. running mean: -20.511228\n",
      "ep 1616: ep_len:75 episode reward: total was 36.000000. running mean: -19.946116\n",
      "epsilon:0.009992 episode_count: 24358. steps_count: 26119882.000000\n",
      "ep 1617: ep_len:1098 episode reward: total was 6.740000. running mean: -19.679254\n",
      "ep 1617: ep_len:803 episode reward: total was 12.160000. running mean: -19.360862\n",
      "ep 1617: ep_len:2852 episode reward: total was -31.170000. running mean: -19.478953\n",
      "ep 1617: ep_len:884 episode reward: total was 58.270000. running mean: -18.701464\n",
      "ep 1617: ep_len:60 episode reward: total was 28.500000. running mean: -18.229449\n",
      "ep 1617: ep_len:1033 episode reward: total was -65.590000. running mean: -18.703055\n",
      "ep 1617: ep_len:3939 episode reward: total was 3.170000. running mean: -18.484324\n",
      "ep 1617: ep_len:668 episode reward: total was 9.540000. running mean: -18.204081\n",
      "ep 1617: ep_len:765 episode reward: total was 2.560000. running mean: -17.996440\n",
      "ep 1617: ep_len:1132 episode reward: total was -4.000000. running mean: -17.856476\n",
      "ep 1617: ep_len:93 episode reward: total was 43.500000. running mean: -17.242911\n",
      "ep 1617: ep_len:1000 episode reward: total was -0.200000. running mean: -17.072482\n",
      "ep 1617: ep_len:2724 episode reward: total was -17.250000. running mean: -17.074257\n",
      "ep 1617: ep_len:54 episode reward: total was 25.500000. running mean: -16.648514\n",
      "epsilon:0.009992 episode_count: 24372. steps_count: 26136987.000000\n",
      "ep 1618: ep_len:1462 episode reward: total was 27.890000. running mean: -16.203129\n",
      "ep 1618: ep_len:833 episode reward: total was 17.020000. running mean: -15.870898\n",
      "ep 1618: ep_len:96 episode reward: total was 46.500000. running mean: -15.247189\n",
      "ep 1618: ep_len:849 episode reward: total was -0.590000. running mean: -15.100617\n",
      "ep 1618: ep_len:50 episode reward: total was 23.500000. running mean: -14.714611\n",
      "ep 1618: ep_len:58 episode reward: total was 27.500000. running mean: -14.292465\n",
      "ep 1618: ep_len:608 episode reward: total was 2.910000. running mean: -14.120440\n",
      "ep 1618: ep_len:331 episode reward: total was 18.000000. running mean: -13.799236\n",
      "ep 1618: ep_len:791 episode reward: total was -12.380000. running mean: -13.785043\n",
      "ep 1618: ep_len:662 episode reward: total was 1.830000. running mean: -13.628893\n",
      "ep 1618: ep_len:1432 episode reward: total was 3.740000. running mean: -13.455204\n",
      "ep 1618: ep_len:67 episode reward: total was 30.500000. running mean: -13.015652\n",
      "ep 1618: ep_len:60 episode reward: total was 28.500000. running mean: -12.600495\n",
      "ep 1618: ep_len:97 episode reward: total was 47.000000. running mean: -12.004490\n",
      "ep 1618: ep_len:1169 episode reward: total was 0.360000. running mean: -11.880846\n",
      "ep 1618: ep_len:2819 episode reward: total was -7.210000. running mean: -11.834137\n",
      "epsilon:0.009992 episode_count: 24388. steps_count: 26148371.000000\n",
      "ep 1619: ep_len:1477 episode reward: total was 18.550000. running mean: -11.530296\n",
      "ep 1619: ep_len:704 episode reward: total was -10.380000. running mean: -11.518793\n",
      "ep 1619: ep_len:2995 episode reward: total was -52.210000. running mean: -11.925705\n",
      "ep 1619: ep_len:524 episode reward: total was 1.090000. running mean: -11.795548\n",
      "ep 1619: ep_len:69 episode reward: total was 31.500000. running mean: -11.362592\n",
      "ep 1619: ep_len:552 episode reward: total was 30.560000. running mean: -10.943366\n",
      "ep 1619: ep_len:3697 episode reward: total was -28.900000. running mean: -11.122933\n",
      "ep 1619: ep_len:1270 episode reward: total was -67.240000. running mean: -11.684103\n",
      "ep 1619: ep_len:7304 episode reward: total was -236.550000. running mean: -13.932762\n",
      "ep 1619: ep_len:1557 episode reward: total was -6.550000. running mean: -13.858935\n",
      "ep 1619: ep_len:60 episode reward: total was 28.500000. running mean: -13.435345\n",
      "ep 1619: ep_len:818 episode reward: total was -6.970000. running mean: -13.370692\n",
      "ep 1619: ep_len:2918 episode reward: total was -17.550000. running mean: -13.412485\n",
      "ep 1619: ep_len:48 episode reward: total was 21.000000. running mean: -13.068360\n",
      "epsilon:0.009992 episode_count: 24402. steps_count: 26172364.000000\n",
      "ep 1620: ep_len:1140 episode reward: total was -0.400000. running mean: -12.941677\n",
      "ep 1620: ep_len:1623 episode reward: total was -71.690000. running mean: -13.529160\n",
      "ep 1620: ep_len:2955 episode reward: total was -968.350000. running mean: -23.077368\n",
      "ep 1620: ep_len:1124 episode reward: total was -24.770000. running mean: -23.094295\n",
      "ep 1620: ep_len:106 episode reward: total was 48.500000. running mean: -22.378352\n",
      "ep 1620: ep_len:78 episode reward: total was 36.000000. running mean: -21.794568\n",
      "ep 1620: ep_len:65 episode reward: total was 29.500000. running mean: -21.281622\n",
      "ep 1620: ep_len:998 episode reward: total was -2.980000. running mean: -21.098606\n",
      "ep 1620: ep_len:3622 episode reward: total was -14.680000. running mean: -21.034420\n",
      "ep 1620: ep_len:803 episode reward: total was 6.480000. running mean: -20.759276\n",
      "ep 1620: ep_len:681 episode reward: total was 44.100000. running mean: -20.110683\n",
      "ep 1620: ep_len:606 episode reward: total was -21.380000. running mean: -20.123376\n",
      "ep 1620: ep_len:120 episode reward: total was 57.000000. running mean: -19.352143\n",
      "ep 1620: ep_len:76 episode reward: total was 36.500000. running mean: -18.793621\n",
      "ep 1620: ep_len:717 episode reward: total was 10.780000. running mean: -18.497885\n",
      "ep 1620: ep_len:2899 episode reward: total was -7.660000. running mean: -18.389506\n",
      "epsilon:0.009992 episode_count: 24418. steps_count: 26189977.000000\n",
      "ep 1621: ep_len:1464 episode reward: total was 25.580000. running mean: -17.949811\n",
      "ep 1621: ep_len:1651 episode reward: total was -69.510000. running mean: -18.465413\n",
      "ep 1621: ep_len:3007 episode reward: total was -49.510000. running mean: -18.775859\n",
      "ep 1621: ep_len:784 episode reward: total was 25.910000. running mean: -18.329000\n",
      "ep 1621: ep_len:64 episode reward: total was 30.500000. running mean: -17.840710\n",
      "ep 1621: ep_len:33 episode reward: total was 15.000000. running mean: -17.512303\n",
      "ep 1621: ep_len:752 episode reward: total was -40.150000. running mean: -17.738680\n",
      "ep 1621: ep_len:4054 episode reward: total was -128.970000. running mean: -18.850993\n",
      "ep 1621: ep_len:1582 episode reward: total was -13.830000. running mean: -18.800783\n",
      "ep 1621: ep_len:724 episode reward: total was 40.240000. running mean: -18.210375\n",
      "ep 1621: ep_len:670 episode reward: total was -3.280000. running mean: -18.061072\n",
      "ep 1621: ep_len:62 episode reward: total was 29.500000. running mean: -17.585461\n",
      "ep 1621: ep_len:1517 episode reward: total was 12.430000. running mean: -17.285306\n",
      "ep 1621: ep_len:2851 episode reward: total was -1.530000. running mean: -17.127753\n",
      "ep 1621: ep_len:65 episode reward: total was 31.000000. running mean: -16.646476\n",
      "epsilon:0.009992 episode_count: 24433. steps_count: 26209257.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1622: ep_len:500 episode reward: total was -0.860000. running mean: -16.488611\n",
      "ep 1622: ep_len:723 episode reward: total was -26.670000. running mean: -16.590425\n",
      "ep 1622: ep_len:3008 episode reward: total was -41.140000. running mean: -16.835921\n",
      "ep 1622: ep_len:500 episode reward: total was -3.090000. running mean: -16.698461\n",
      "ep 1622: ep_len:41 episode reward: total was 19.000000. running mean: -16.341477\n",
      "ep 1622: ep_len:130 episode reward: total was 62.000000. running mean: -15.558062\n",
      "ep 1622: ep_len:54 episode reward: total was 24.000000. running mean: -15.162481\n",
      "ep 1622: ep_len:1022 episode reward: total was -30.320000. running mean: -15.314057\n",
      "ep 1622: ep_len:298 episode reward: total was 22.200000. running mean: -14.938916\n",
      "ep 1622: ep_len:889 episode reward: total was -31.680000. running mean: -15.106327\n",
      "ep 1622: ep_len:623 episode reward: total was 12.220000. running mean: -14.833064\n",
      "ep 1622: ep_len:500 episode reward: total was 8.850000. running mean: -14.596233\n",
      "ep 1622: ep_len:54 episode reward: total was 25.500000. running mean: -14.195271\n",
      "ep 1622: ep_len:113 episode reward: total was 55.000000. running mean: -13.503318\n",
      "ep 1622: ep_len:72 episode reward: total was 33.000000. running mean: -13.038285\n",
      "ep 1622: ep_len:1122 episode reward: total was -17.230000. running mean: -13.080202\n",
      "ep 1622: ep_len:2840 episode reward: total was -23.860000. running mean: -13.188000\n",
      "ep 1622: ep_len:65 episode reward: total was 29.500000. running mean: -12.761120\n",
      "epsilon:0.009992 episode_count: 24451. steps_count: 26221811.000000\n",
      "ep 1623: ep_len:1439 episode reward: total was 5.310000. running mean: -12.580409\n",
      "ep 1623: ep_len:685 episode reward: total was -4.300000. running mean: -12.497605\n",
      "ep 1623: ep_len:3061 episode reward: total was -108.880000. running mean: -13.461429\n",
      "ep 1623: ep_len:510 episode reward: total was -14.660000. running mean: -13.473414\n",
      "ep 1623: ep_len:56 episode reward: total was 26.500000. running mean: -13.073680\n",
      "ep 1623: ep_len:149 episode reward: total was 71.500000. running mean: -12.227943\n",
      "ep 1623: ep_len:1088 episode reward: total was -0.400000. running mean: -12.109664\n",
      "ep 1623: ep_len:363 episode reward: total was 18.260000. running mean: -11.805967\n",
      "ep 1623: ep_len:681 episode reward: total was -3.460000. running mean: -11.722508\n",
      "ep 1623: ep_len:600 episode reward: total was -4.300000. running mean: -11.648283\n",
      "ep 1623: ep_len:699 episode reward: total was -1.900000. running mean: -11.550800\n",
      "ep 1623: ep_len:101 episode reward: total was 47.500000. running mean: -10.960292\n",
      "ep 1623: ep_len:647 episode reward: total was -15.280000. running mean: -11.003489\n",
      "ep 1623: ep_len:2795 episode reward: total was -12.860000. running mean: -11.022054\n",
      "ep 1623: ep_len:61 episode reward: total was 29.000000. running mean: -10.621833\n",
      "epsilon:0.009992 episode_count: 24466. steps_count: 26234746.000000\n",
      "ep 1624: ep_len:990 episode reward: total was -82.920000. running mean: -11.344815\n",
      "ep 1624: ep_len:711 episode reward: total was -42.750000. running mean: -11.658867\n",
      "ep 1624: ep_len:3006 episode reward: total was -31.550000. running mean: -11.857778\n",
      "ep 1624: ep_len:647 episode reward: total was 2.440000. running mean: -11.714800\n",
      "ep 1624: ep_len:61 episode reward: total was 29.000000. running mean: -11.307652\n",
      "ep 1624: ep_len:500 episode reward: total was 37.910000. running mean: -10.815476\n",
      "ep 1624: ep_len:632 episode reward: total was 24.520000. running mean: -10.462121\n",
      "ep 1624: ep_len:763 episode reward: total was -25.300000. running mean: -10.610500\n",
      "ep 1624: ep_len:618 episode reward: total was -16.150000. running mean: -10.665895\n",
      "ep 1624: ep_len:678 episode reward: total was 3.580000. running mean: -10.523436\n",
      "ep 1624: ep_len:59 episode reward: total was 28.000000. running mean: -10.138202\n",
      "ep 1624: ep_len:212 episode reward: total was 103.000000. running mean: -9.006820\n",
      "ep 1624: ep_len:87 episode reward: total was 39.000000. running mean: -8.526751\n",
      "ep 1624: ep_len:1449 episode reward: total was 5.320000. running mean: -8.388284\n",
      "ep 1624: ep_len:2845 episode reward: total was 0.060000. running mean: -8.303801\n",
      "epsilon:0.009992 episode_count: 24481. steps_count: 26248004.000000\n",
      "ep 1625: ep_len:751 episode reward: total was -75.480000. running mean: -8.975563\n",
      "ep 1625: ep_len:196 episode reward: total was 5.230000. running mean: -8.833507\n",
      "ep 1625: ep_len:3006 episode reward: total was -51.000000. running mean: -9.255172\n",
      "ep 1625: ep_len:557 episode reward: total was -8.740000. running mean: -9.250021\n",
      "ep 1625: ep_len:659 episode reward: total was 3.730000. running mean: -9.120220\n",
      "ep 1625: ep_len:351 episode reward: total was 19.670000. running mean: -8.832318\n",
      "ep 1625: ep_len:1559 episode reward: total was -7.230000. running mean: -8.816295\n",
      "ep 1625: ep_len:770 episode reward: total was 6.930000. running mean: -8.658832\n",
      "ep 1625: ep_len:654 episode reward: total was -18.740000. running mean: -8.759644\n",
      "ep 1625: ep_len:60 episode reward: total was 27.000000. running mean: -8.402047\n",
      "ep 1625: ep_len:741 episode reward: total was -66.490000. running mean: -8.982927\n",
      "ep 1625: ep_len:2870 episode reward: total was 4.320000. running mean: -8.849898\n",
      "epsilon:0.009992 episode_count: 24493. steps_count: 26260178.000000\n",
      "ep 1626: ep_len:665 episode reward: total was -17.240000. running mean: -8.933799\n",
      "ep 1626: ep_len:500 episode reward: total was 10.930000. running mean: -8.735161\n",
      "ep 1626: ep_len:60 episode reward: total was 27.000000. running mean: -8.377809\n",
      "ep 1626: ep_len:3010 episode reward: total was -30.450000. running mean: -8.598531\n",
      "ep 1626: ep_len:1196 episode reward: total was -23.560000. running mean: -8.748146\n",
      "ep 1626: ep_len:35 episode reward: total was 16.000000. running mean: -8.500664\n",
      "ep 1626: ep_len:90 episode reward: total was 43.500000. running mean: -7.980658\n",
      "ep 1626: ep_len:1039 episode reward: total was -48.360000. running mean: -8.384451\n",
      "ep 1626: ep_len:3627 episode reward: total was -440.240000. running mean: -12.703006\n",
      "ep 1626: ep_len:1180 episode reward: total was -37.370000. running mean: -12.949676\n",
      "ep 1626: ep_len:763 episode reward: total was 15.980000. running mean: -12.660380\n",
      "ep 1626: ep_len:811 episode reward: total was -26.400000. running mean: -12.797776\n",
      "ep 1626: ep_len:1153 episode reward: total was 6.680000. running mean: -12.602998\n",
      "ep 1626: ep_len:2753 episode reward: total was 12.400000. running mean: -12.352968\n",
      "ep 1626: ep_len:61 episode reward: total was 24.500000. running mean: -11.984438\n",
      "epsilon:0.009992 episode_count: 24508. steps_count: 26277121.000000\n",
      "ep 1627: ep_len:1436 episode reward: total was 1.210000. running mean: -11.852494\n",
      "ep 1627: ep_len:941 episode reward: total was 13.160000. running mean: -11.602369\n",
      "ep 1627: ep_len:3010 episode reward: total was -46.790000. running mean: -11.954245\n",
      "ep 1627: ep_len:560 episode reward: total was -11.580000. running mean: -11.950503\n",
      "ep 1627: ep_len:1104 episode reward: total was -3.270000. running mean: -11.863698\n",
      "ep 1627: ep_len:660 episode reward: total was 23.640000. running mean: -11.508661\n",
      "ep 1627: ep_len:635 episode reward: total was -28.650000. running mean: -11.680074\n",
      "ep 1627: ep_len:691 episode reward: total was 29.070000. running mean: -11.272574\n",
      "ep 1627: ep_len:969 episode reward: total was 3.920000. running mean: -11.120648\n",
      "ep 1627: ep_len:212 episode reward: total was 103.000000. running mean: -9.979441\n",
      "ep 1627: ep_len:79 episode reward: total was 36.500000. running mean: -9.514647\n",
      "ep 1627: ep_len:634 episode reward: total was -8.980000. running mean: -9.509300\n",
      "ep 1627: ep_len:2764 episode reward: total was -11.520000. running mean: -9.529407\n",
      "ep 1627: ep_len:65 episode reward: total was 31.000000. running mean: -9.124113\n",
      "epsilon:0.009992 episode_count: 24522. steps_count: 26290881.000000\n",
      "ep 1628: ep_len:1189 episode reward: total was 25.510000. running mean: -8.777772\n",
      "ep 1628: ep_len:500 episode reward: total was 14.940000. running mean: -8.540595\n",
      "ep 1628: ep_len:75 episode reward: total was 36.000000. running mean: -8.095189\n",
      "ep 1628: ep_len:3006 episode reward: total was -38.800000. running mean: -8.402237\n",
      "ep 1628: ep_len:676 episode reward: total was -1.870000. running mean: -8.336914\n",
      "ep 1628: ep_len:128 episode reward: total was 62.500000. running mean: -7.628545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1628: ep_len:749 episode reward: total was -13.370000. running mean: -7.685960\n",
      "ep 1628: ep_len:651 episode reward: total was 19.540000. running mean: -7.413700\n",
      "ep 1628: ep_len:564 episode reward: total was -27.860000. running mean: -7.618163\n",
      "ep 1628: ep_len:780 episode reward: total was 15.330000. running mean: -7.388682\n",
      "ep 1628: ep_len:699 episode reward: total was -2.480000. running mean: -7.339595\n",
      "ep 1628: ep_len:62 episode reward: total was 28.000000. running mean: -6.986199\n",
      "ep 1628: ep_len:1204 episode reward: total was 17.700000. running mean: -6.739337\n",
      "ep 1628: ep_len:35 episode reward: total was 16.000000. running mean: -6.511943\n",
      "epsilon:0.009992 episode_count: 24536. steps_count: 26301199.000000\n",
      "ep 1629: ep_len:615 episode reward: total was 22.900000. running mean: -6.217824\n",
      "ep 1629: ep_len:687 episode reward: total was -14.510000. running mean: -6.300746\n",
      "ep 1629: ep_len:72 episode reward: total was 34.500000. running mean: -5.892738\n",
      "ep 1629: ep_len:2980 episode reward: total was -44.360000. running mean: -6.277411\n",
      "ep 1629: ep_len:1474 episode reward: total was 32.660000. running mean: -5.888037\n",
      "ep 1629: ep_len:92 episode reward: total was 44.500000. running mean: -5.384156\n",
      "ep 1629: ep_len:52 episode reward: total was 23.000000. running mean: -5.100315\n",
      "ep 1629: ep_len:950 episode reward: total was 4.190000. running mean: -5.007412\n",
      "ep 1629: ep_len:3650 episode reward: total was -176.400000. running mean: -6.721338\n",
      "ep 1629: ep_len:529 episode reward: total was -22.150000. running mean: -6.875624\n",
      "ep 1629: ep_len:7360 episode reward: total was -126.090000. running mean: -8.067768\n",
      "ep 1629: ep_len:709 episode reward: total was 6.920000. running mean: -7.917890\n",
      "ep 1629: ep_len:49 episode reward: total was 23.000000. running mean: -7.608711\n",
      "ep 1629: ep_len:574 episode reward: total was 6.020000. running mean: -7.472424\n",
      "ep 1629: ep_len:2990 episode reward: total was -43.630000. running mean: -7.834000\n",
      "epsilon:0.009992 episode_count: 24551. steps_count: 26323982.000000\n",
      "ep 1630: ep_len:1144 episode reward: total was 17.250000. running mean: -7.583160\n",
      "ep 1630: ep_len:654 episode reward: total was -67.270000. running mean: -8.180028\n",
      "ep 1630: ep_len:2972 episode reward: total was -9.550000. running mean: -8.193728\n",
      "ep 1630: ep_len:823 episode reward: total was 7.140000. running mean: -8.040391\n",
      "ep 1630: ep_len:59 episode reward: total was 25.000000. running mean: -7.709987\n",
      "ep 1630: ep_len:745 episode reward: total was 1.220000. running mean: -7.620687\n",
      "ep 1630: ep_len:594 episode reward: total was 20.560000. running mean: -7.338880\n",
      "ep 1630: ep_len:653 episode reward: total was -28.990000. running mean: -7.555391\n",
      "ep 1630: ep_len:867 episode reward: total was 37.950000. running mean: -7.100337\n",
      "ep 1630: ep_len:1090 episode reward: total was 29.930000. running mean: -6.730034\n",
      "ep 1630: ep_len:143 episode reward: total was 68.500000. running mean: -5.977734\n",
      "ep 1630: ep_len:52 episode reward: total was 24.500000. running mean: -5.672956\n",
      "ep 1630: ep_len:118 episode reward: total was 57.500000. running mean: -5.041227\n",
      "ep 1630: ep_len:1159 episode reward: total was 0.830000. running mean: -4.982515\n",
      "ep 1630: ep_len:2853 episode reward: total was -3.340000. running mean: -4.966089\n",
      "epsilon:0.009992 episode_count: 24566. steps_count: 26337908.000000\n",
      "ep 1631: ep_len:1425 episode reward: total was 15.640000. running mean: -4.760029\n",
      "ep 1631: ep_len:744 episode reward: total was -11.650000. running mean: -4.828928\n",
      "ep 1631: ep_len:93 episode reward: total was 45.000000. running mean: -4.330639\n",
      "ep 1631: ep_len:544 episode reward: total was -18.970000. running mean: -4.477033\n",
      "ep 1631: ep_len:36 episode reward: total was 16.500000. running mean: -4.267262\n",
      "ep 1631: ep_len:727 episode reward: total was -12.090000. running mean: -4.345490\n",
      "ep 1631: ep_len:607 episode reward: total was 16.130000. running mean: -4.140735\n",
      "ep 1631: ep_len:924 episode reward: total was -32.340000. running mean: -4.422727\n",
      "ep 1631: ep_len:844 episode reward: total was 21.740000. running mean: -4.161100\n",
      "ep 1631: ep_len:1466 episode reward: total was -7.120000. running mean: -4.190689\n",
      "ep 1631: ep_len:135 episode reward: total was 64.500000. running mean: -3.503782\n",
      "ep 1631: ep_len:765 episode reward: total was -53.910000. running mean: -4.007844\n",
      "ep 1631: ep_len:2763 episode reward: total was -9.200000. running mean: -4.059766\n",
      "epsilon:0.009992 episode_count: 24579. steps_count: 26348981.000000\n",
      "ep 1632: ep_len:1109 episode reward: total was -7.260000. running mean: -4.091768\n",
      "ep 1632: ep_len:500 episode reward: total was 21.370000. running mean: -3.837151\n",
      "ep 1632: ep_len:77 episode reward: total was 37.000000. running mean: -3.428779\n",
      "ep 1632: ep_len:3078 episode reward: total was -22.260000. running mean: -3.617091\n",
      "ep 1632: ep_len:565 episode reward: total was -2.600000. running mean: -3.606920\n",
      "ep 1632: ep_len:62 episode reward: total was 29.500000. running mean: -3.275851\n",
      "ep 1632: ep_len:1010 episode reward: total was -44.610000. running mean: -3.689193\n",
      "ep 1632: ep_len:633 episode reward: total was 22.420000. running mean: -3.428101\n",
      "ep 1632: ep_len:681 episode reward: total was -19.370000. running mean: -3.587520\n",
      "ep 1632: ep_len:7232 episode reward: total was -207.010000. running mean: -5.621745\n",
      "ep 1632: ep_len:1471 episode reward: total was 2.450000. running mean: -5.541027\n",
      "ep 1632: ep_len:83 episode reward: total was 40.000000. running mean: -5.085617\n",
      "ep 1632: ep_len:42 episode reward: total was 18.000000. running mean: -4.854761\n",
      "ep 1632: ep_len:1451 episode reward: total was 24.930000. running mean: -4.556913\n",
      "ep 1632: ep_len:45 episode reward: total was 21.000000. running mean: -4.301344\n",
      "ep 1632: ep_len:43 episode reward: total was 18.500000. running mean: -4.073331\n",
      "epsilon:0.009992 episode_count: 24595. steps_count: 26367063.000000\n",
      "ep 1633: ep_len:1407 episode reward: total was 16.560000. running mean: -3.866997\n",
      "ep 1633: ep_len:500 episode reward: total was 29.580000. running mean: -3.532527\n",
      "ep 1633: ep_len:2939 episode reward: total was -64.140000. running mean: -4.138602\n",
      "ep 1633: ep_len:1358 episode reward: total was -111.720000. running mean: -5.214416\n",
      "ep 1633: ep_len:115 episode reward: total was 56.000000. running mean: -4.602272\n",
      "ep 1633: ep_len:95 episode reward: total was 43.000000. running mean: -4.126249\n",
      "ep 1633: ep_len:65 episode reward: total was 31.000000. running mean: -3.774987\n",
      "ep 1633: ep_len:1458 episode reward: total was 24.640000. running mean: -3.490837\n",
      "ep 1633: ep_len:305 episode reward: total was 15.810000. running mean: -3.297828\n",
      "ep 1633: ep_len:1615 episode reward: total was -59.750000. running mean: -3.862350\n",
      "ep 1633: ep_len:7221 episode reward: total was -1.200000. running mean: -3.835727\n",
      "ep 1633: ep_len:500 episode reward: total was 37.390000. running mean: -3.423469\n",
      "ep 1633: ep_len:67 episode reward: total was 32.000000. running mean: -3.069235\n",
      "ep 1633: ep_len:960 episode reward: total was -8.750000. running mean: -3.126042\n",
      "ep 1633: ep_len:2772 episode reward: total was 0.530000. running mean: -3.089482\n",
      "epsilon:0.009992 episode_count: 24610. steps_count: 26388440.000000\n",
      "ep 1634: ep_len:880 episode reward: total was -90.900000. running mean: -3.967587\n",
      "ep 1634: ep_len:1295 episode reward: total was -82.160000. running mean: -4.749511\n",
      "ep 1634: ep_len:90 episode reward: total was 40.500000. running mean: -4.297016\n",
      "ep 1634: ep_len:629 episode reward: total was 7.240000. running mean: -4.181646\n",
      "ep 1634: ep_len:84 episode reward: total was 39.000000. running mean: -3.749829\n",
      "ep 1634: ep_len:1497 episode reward: total was -211.890000. running mean: -5.831231\n",
      "ep 1634: ep_len:366 episode reward: total was 14.770000. running mean: -5.625219\n",
      "ep 1634: ep_len:1636 episode reward: total was -29.360000. running mean: -5.862567\n",
      "ep 1634: ep_len:661 episode reward: total was -1.450000. running mean: -5.818441\n",
      "ep 1634: ep_len:612 episode reward: total was 56.060000. running mean: -5.199657\n",
      "ep 1634: ep_len:46 episode reward: total was 21.500000. running mean: -4.932660\n",
      "ep 1634: ep_len:647 episode reward: total was 13.750000. running mean: -4.745833\n",
      "ep 1634: ep_len:2911 episode reward: total was -2.890000. running mean: -4.727275\n",
      "ep 1634: ep_len:51 episode reward: total was 22.500000. running mean: -4.455002\n",
      "epsilon:0.009992 episode_count: 24624. steps_count: 26399845.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1635: ep_len:953 episode reward: total was -79.250000. running mean: -5.202952\n",
      "ep 1635: ep_len:1621 episode reward: total was -83.240000. running mean: -5.983323\n",
      "ep 1635: ep_len:70 episode reward: total was 33.500000. running mean: -5.588490\n",
      "ep 1635: ep_len:3027 episode reward: total was -24.120000. running mean: -5.773805\n",
      "ep 1635: ep_len:1227 episode reward: total was -28.300000. running mean: -5.999067\n",
      "ep 1635: ep_len:31 episode reward: total was 14.000000. running mean: -5.799076\n",
      "ep 1635: ep_len:857 episode reward: total was 17.180000. running mean: -5.569285\n",
      "ep 1635: ep_len:3568 episode reward: total was -57.730000. running mean: -6.090892\n",
      "ep 1635: ep_len:3143 episode reward: total was -590.440000. running mean: -11.934383\n",
      "ep 1635: ep_len:776 episode reward: total was 32.950000. running mean: -11.485540\n",
      "ep 1635: ep_len:586 episode reward: total was -7.870000. running mean: -11.449384\n",
      "ep 1635: ep_len:168 episode reward: total was 79.500000. running mean: -10.539890\n",
      "ep 1635: ep_len:631 episode reward: total was 13.800000. running mean: -10.296491\n",
      "ep 1635: ep_len:2715 episode reward: total was -10.390000. running mean: -10.297426\n",
      "ep 1635: ep_len:52 episode reward: total was 24.500000. running mean: -9.949452\n",
      "epsilon:0.009992 episode_count: 24639. steps_count: 26419270.000000\n",
      "ep 1636: ep_len:981 episode reward: total was -61.630000. running mean: -10.466258\n",
      "ep 1636: ep_len:915 episode reward: total was -20.100000. running mean: -10.562595\n",
      "ep 1636: ep_len:2858 episode reward: total was -30.720000. running mean: -10.764169\n",
      "ep 1636: ep_len:1607 episode reward: total was -33.380000. running mean: -10.990327\n",
      "ep 1636: ep_len:63 episode reward: total was 28.500000. running mean: -10.595424\n",
      "ep 1636: ep_len:106 episode reward: total was 51.500000. running mean: -9.974470\n",
      "ep 1636: ep_len:1432 episode reward: total was -201.990000. running mean: -11.894625\n",
      "ep 1636: ep_len:634 episode reward: total was 8.200000. running mean: -11.693679\n",
      "ep 1636: ep_len:641 episode reward: total was -39.150000. running mean: -11.968242\n",
      "ep 1636: ep_len:764 episode reward: total was 55.250000. running mean: -11.296060\n",
      "ep 1636: ep_len:660 episode reward: total was -0.640000. running mean: -11.189499\n",
      "ep 1636: ep_len:71 episode reward: total was 32.500000. running mean: -10.752604\n",
      "ep 1636: ep_len:80 episode reward: total was 38.500000. running mean: -10.260078\n",
      "ep 1636: ep_len:987 episode reward: total was -29.940000. running mean: -10.456877\n",
      "ep 1636: ep_len:2791 episode reward: total was -7.550000. running mean: -10.427809\n",
      "epsilon:0.009992 episode_count: 24654. steps_count: 26433860.000000\n",
      "ep 1637: ep_len:1001 episode reward: total was -30.560000. running mean: -10.629131\n",
      "ep 1637: ep_len:739 episode reward: total was -26.490000. running mean: -10.787739\n",
      "ep 1637: ep_len:3005 episode reward: total was -96.670000. running mean: -11.646562\n",
      "ep 1637: ep_len:1447 episode reward: total was 31.040000. running mean: -11.219696\n",
      "ep 1637: ep_len:149 episode reward: total was 68.500000. running mean: -10.422499\n",
      "ep 1637: ep_len:53 episode reward: total was 25.000000. running mean: -10.068274\n",
      "ep 1637: ep_len:500 episode reward: total was 6.950000. running mean: -9.898092\n",
      "ep 1637: ep_len:354 episode reward: total was 14.190000. running mean: -9.657211\n",
      "ep 1637: ep_len:848 episode reward: total was 32.430000. running mean: -9.236338\n",
      "ep 1637: ep_len:733 episode reward: total was -13.220000. running mean: -9.276175\n",
      "ep 1637: ep_len:617 episode reward: total was 15.760000. running mean: -9.025813\n",
      "ep 1637: ep_len:130 episode reward: total was 36.500000. running mean: -8.570555\n",
      "ep 1637: ep_len:109 episode reward: total was 53.000000. running mean: -7.954850\n",
      "ep 1637: ep_len:980 episode reward: total was -38.520000. running mean: -8.260501\n",
      "ep 1637: ep_len:2864 episode reward: total was -36.960000. running mean: -8.547496\n",
      "epsilon:0.009992 episode_count: 24669. steps_count: 26447389.000000\n",
      "ep 1638: ep_len:1455 episode reward: total was 33.510000. running mean: -8.126921\n",
      "ep 1638: ep_len:732 episode reward: total was -30.420000. running mean: -8.349852\n",
      "ep 1638: ep_len:56 episode reward: total was 26.500000. running mean: -8.001353\n",
      "ep 1638: ep_len:3001 episode reward: total was -37.540000. running mean: -8.296740\n",
      "ep 1638: ep_len:685 episode reward: total was -7.380000. running mean: -8.287573\n",
      "ep 1638: ep_len:52 episode reward: total was 24.500000. running mean: -7.959697\n",
      "ep 1638: ep_len:500 episode reward: total was 19.660000. running mean: -7.683500\n",
      "ep 1638: ep_len:661 episode reward: total was 17.650000. running mean: -7.430165\n",
      "ep 1638: ep_len:718 episode reward: total was -23.670000. running mean: -7.592563\n",
      "ep 1638: ep_len:737 episode reward: total was 35.780000. running mean: -7.158838\n",
      "ep 1638: ep_len:1183 episode reward: total was -14.600000. running mean: -7.233249\n",
      "ep 1638: ep_len:63 episode reward: total was 30.000000. running mean: -6.860917\n",
      "ep 1638: ep_len:56 episode reward: total was 26.500000. running mean: -6.527308\n",
      "ep 1638: ep_len:85 episode reward: total was 41.000000. running mean: -6.052034\n",
      "ep 1638: ep_len:650 episode reward: total was 20.700000. running mean: -5.784514\n",
      "ep 1638: ep_len:2850 episode reward: total was -316.940000. running mean: -8.896069\n",
      "epsilon:0.009992 episode_count: 24685. steps_count: 26460873.000000\n",
      "ep 1639: ep_len:625 episode reward: total was 21.660000. running mean: -8.590508\n",
      "ep 1639: ep_len:732 episode reward: total was -12.220000. running mean: -8.626803\n",
      "ep 1639: ep_len:2901 episode reward: total was -97.350000. running mean: -9.514035\n",
      "ep 1639: ep_len:500 episode reward: total was 10.690000. running mean: -9.311995\n",
      "ep 1639: ep_len:64 episode reward: total was 30.500000. running mean: -8.913875\n",
      "ep 1639: ep_len:1473 episode reward: total was 24.630000. running mean: -8.578436\n",
      "ep 1639: ep_len:346 episode reward: total was 20.110000. running mean: -8.291552\n",
      "ep 1639: ep_len:1244 episode reward: total was -64.370000. running mean: -8.852336\n",
      "ep 1639: ep_len:680 episode reward: total was -9.800000. running mean: -8.861813\n",
      "ep 1639: ep_len:689 episode reward: total was -1.360000. running mean: -8.786795\n",
      "ep 1639: ep_len:159 episode reward: total was 76.500000. running mean: -7.933927\n",
      "ep 1639: ep_len:1117 episode reward: total was -7.390000. running mean: -7.928488\n",
      "ep 1639: ep_len:2789 episode reward: total was -6.470000. running mean: -7.913903\n",
      "ep 1639: ep_len:63 episode reward: total was 28.500000. running mean: -7.549764\n",
      "epsilon:0.009992 episode_count: 24699. steps_count: 26474255.000000\n",
      "ep 1640: ep_len:811 episode reward: total was -26.400000. running mean: -7.738266\n",
      "ep 1640: ep_len:501 episode reward: total was 30.110000. running mean: -7.359783\n",
      "ep 1640: ep_len:2964 episode reward: total was -10.040000. running mean: -7.386585\n",
      "ep 1640: ep_len:646 episode reward: total was 17.260000. running mean: -7.140120\n",
      "ep 1640: ep_len:49 episode reward: total was 23.000000. running mean: -6.838718\n",
      "ep 1640: ep_len:138 episode reward: total was 64.500000. running mean: -6.125331\n",
      "ep 1640: ep_len:82 episode reward: total was 39.500000. running mean: -5.669078\n",
      "ep 1640: ep_len:621 episode reward: total was -0.090000. running mean: -5.613287\n",
      "ep 1640: ep_len:642 episode reward: total was 24.010000. running mean: -5.317054\n",
      "ep 1640: ep_len:3046 episode reward: total was -666.610000. running mean: -11.929984\n",
      "ep 1640: ep_len:659 episode reward: total was -5.700000. running mean: -11.867684\n",
      "ep 1640: ep_len:1425 episode reward: total was 13.860000. running mean: -11.610407\n",
      "ep 1640: ep_len:109 episode reward: total was 50.000000. running mean: -10.994303\n",
      "ep 1640: ep_len:35 episode reward: total was 16.000000. running mean: -10.724360\n",
      "ep 1640: ep_len:1489 episode reward: total was 29.200000. running mean: -10.325116\n",
      "ep 1640: ep_len:2752 episode reward: total was -42.550000. running mean: -10.647365\n",
      "epsilon:0.009992 episode_count: 24715. steps_count: 26490224.000000\n",
      "ep 1641: ep_len:892 episode reward: total was 21.390000. running mean: -10.326992\n",
      "ep 1641: ep_len:213 episode reward: total was 3.870000. running mean: -10.185022\n",
      "ep 1641: ep_len:3008 episode reward: total was -30.400000. running mean: -10.387171\n",
      "ep 1641: ep_len:539 episode reward: total was -12.960000. running mean: -10.412900\n",
      "ep 1641: ep_len:147 episode reward: total was 72.000000. running mean: -9.588771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1641: ep_len:643 episode reward: total was 5.670000. running mean: -9.436183\n",
      "ep 1641: ep_len:360 episode reward: total was 16.210000. running mean: -9.179721\n",
      "ep 1641: ep_len:536 episode reward: total was -25.110000. running mean: -9.339024\n",
      "ep 1641: ep_len:792 episode reward: total was 4.700000. running mean: -9.198634\n",
      "ep 1641: ep_len:643 episode reward: total was -17.980000. running mean: -9.286447\n",
      "ep 1641: ep_len:2500 episode reward: total was -167.070000. running mean: -10.864283\n",
      "ep 1641: ep_len:2848 episode reward: total was -11.110000. running mean: -10.866740\n",
      "epsilon:0.009992 episode_count: 24727. steps_count: 26503345.000000\n",
      "ep 1642: ep_len:727 episode reward: total was -4.550000. running mean: -10.803573\n",
      "ep 1642: ep_len:738 episode reward: total was -9.960000. running mean: -10.795137\n",
      "ep 1642: ep_len:90 episode reward: total was 43.500000. running mean: -10.252186\n",
      "ep 1642: ep_len:598 episode reward: total was -12.280000. running mean: -10.272464\n",
      "ep 1642: ep_len:40 episode reward: total was 14.000000. running mean: -10.029739\n",
      "ep 1642: ep_len:153 episode reward: total was 72.000000. running mean: -9.209442\n",
      "ep 1642: ep_len:1364 episode reward: total was -264.980000. running mean: -11.767147\n",
      "ep 1642: ep_len:3715 episode reward: total was -188.880000. running mean: -13.538276\n",
      "ep 1642: ep_len:1237 episode reward: total was -17.270000. running mean: -13.575593\n",
      "ep 1642: ep_len:635 episode reward: total was 10.470000. running mean: -13.335137\n",
      "ep 1642: ep_len:1126 episode reward: total was -32.340000. running mean: -13.525186\n",
      "ep 1642: ep_len:60 episode reward: total was 28.500000. running mean: -13.104934\n",
      "ep 1642: ep_len:202 episode reward: total was 99.500000. running mean: -11.978885\n",
      "ep 1642: ep_len:62 episode reward: total was 28.000000. running mean: -11.579096\n",
      "ep 1642: ep_len:129 episode reward: total was 61.500000. running mean: -10.848305\n",
      "ep 1642: ep_len:500 episode reward: total was 7.010000. running mean: -10.669722\n",
      "ep 1642: ep_len:2902 episode reward: total was -12.710000. running mean: -10.690124\n",
      "epsilon:0.009992 episode_count: 24744. steps_count: 26517623.000000\n",
      "ep 1643: ep_len:1180 episode reward: total was 18.040000. running mean: -10.402823\n",
      "ep 1643: ep_len:716 episode reward: total was -9.370000. running mean: -10.392495\n",
      "ep 1643: ep_len:74 episode reward: total was 34.000000. running mean: -9.948570\n",
      "ep 1643: ep_len:2842 episode reward: total was -29.930000. running mean: -10.148384\n",
      "ep 1643: ep_len:500 episode reward: total was 17.090000. running mean: -9.876001\n",
      "ep 1643: ep_len:46 episode reward: total was 20.000000. running mean: -9.577241\n",
      "ep 1643: ep_len:160 episode reward: total was 78.500000. running mean: -8.696468\n",
      "ep 1643: ep_len:64 episode reward: total was 30.500000. running mean: -8.304503\n",
      "ep 1643: ep_len:1479 episode reward: total was -226.610000. running mean: -10.487558\n",
      "ep 1643: ep_len:3907 episode reward: total was -62.420000. running mean: -11.006883\n",
      "ep 1643: ep_len:4193 episode reward: total was -847.070000. running mean: -19.367514\n",
      "ep 1643: ep_len:833 episode reward: total was 41.250000. running mean: -18.761339\n",
      "ep 1643: ep_len:1498 episode reward: total was 17.820000. running mean: -18.395525\n",
      "ep 1643: ep_len:65 episode reward: total was 31.000000. running mean: -17.901570\n",
      "ep 1643: ep_len:142 episode reward: total was 66.500000. running mean: -17.057554\n",
      "ep 1643: ep_len:50 episode reward: total was 23.500000. running mean: -16.651979\n",
      "ep 1643: ep_len:885 episode reward: total was 0.320000. running mean: -16.482259\n",
      "ep 1643: ep_len:2864 episode reward: total was -73.020000. running mean: -17.047637\n",
      "ep 1643: ep_len:47 episode reward: total was 22.000000. running mean: -16.657160\n",
      "epsilon:0.009992 episode_count: 24763. steps_count: 26539168.000000\n",
      "ep 1644: ep_len:830 episode reward: total was -7.020000. running mean: -16.560789\n",
      "ep 1644: ep_len:1608 episode reward: total was -38.020000. running mean: -16.775381\n",
      "ep 1644: ep_len:3031 episode reward: total was 1.480000. running mean: -16.592827\n",
      "ep 1644: ep_len:805 episode reward: total was 11.240000. running mean: -16.314499\n",
      "ep 1644: ep_len:152 episode reward: total was 73.000000. running mean: -15.421354\n",
      "ep 1644: ep_len:50 episode reward: total was 23.500000. running mean: -15.032140\n",
      "ep 1644: ep_len:873 episode reward: total was 28.580000. running mean: -14.596019\n",
      "ep 1644: ep_len:311 episode reward: total was 4.360000. running mean: -14.406459\n",
      "ep 1644: ep_len:1102 episode reward: total was -49.960000. running mean: -14.761994\n",
      "ep 1644: ep_len:756 episode reward: total was 4.300000. running mean: -14.571374\n",
      "ep 1644: ep_len:724 episode reward: total was -2.230000. running mean: -14.447960\n",
      "ep 1644: ep_len:111 episode reward: total was 52.500000. running mean: -13.778481\n",
      "ep 1644: ep_len:39 episode reward: total was 18.000000. running mean: -13.460696\n",
      "ep 1644: ep_len:591 episode reward: total was -8.400000. running mean: -13.410089\n",
      "ep 1644: ep_len:2869 episode reward: total was -17.970000. running mean: -13.455688\n",
      "epsilon:0.009992 episode_count: 24778. steps_count: 26553020.000000\n",
      "ep 1645: ep_len:1089 episode reward: total was -7.460000. running mean: -13.395731\n",
      "ep 1645: ep_len:757 episode reward: total was -6.220000. running mean: -13.323974\n",
      "ep 1645: ep_len:45 episode reward: total was 21.000000. running mean: -12.980734\n",
      "ep 1645: ep_len:2920 episode reward: total was -30.010000. running mean: -13.151027\n",
      "ep 1645: ep_len:1135 episode reward: total was -0.940000. running mean: -13.028916\n",
      "ep 1645: ep_len:38 episode reward: total was 17.500000. running mean: -12.723627\n",
      "ep 1645: ep_len:1513 episode reward: total was -82.320000. running mean: -13.419591\n",
      "ep 1645: ep_len:500 episode reward: total was 14.070000. running mean: -13.144695\n",
      "ep 1645: ep_len:988 episode reward: total was -12.590000. running mean: -13.139148\n",
      "ep 1645: ep_len:844 episode reward: total was 44.600000. running mean: -12.561757\n",
      "ep 1645: ep_len:969 episode reward: total was 22.290000. running mean: -12.213239\n",
      "ep 1645: ep_len:83 episode reward: total was 40.000000. running mean: -11.691107\n",
      "ep 1645: ep_len:112 episode reward: total was 51.500000. running mean: -11.059196\n",
      "ep 1645: ep_len:119 episode reward: total was 56.500000. running mean: -10.383604\n",
      "ep 1645: ep_len:631 episode reward: total was -8.000000. running mean: -10.359768\n",
      "ep 1645: ep_len:2816 episode reward: total was 5.470000. running mean: -10.201470\n",
      "epsilon:0.009992 episode_count: 24794. steps_count: 26567579.000000\n",
      "ep 1646: ep_len:871 episode reward: total was -11.370000. running mean: -10.213155\n",
      "ep 1646: ep_len:1012 episode reward: total was 12.010000. running mean: -9.990924\n",
      "ep 1646: ep_len:2978 episode reward: total was -50.610000. running mean: -10.397115\n",
      "ep 1646: ep_len:500 episode reward: total was 21.230000. running mean: -10.080843\n",
      "ep 1646: ep_len:57 episode reward: total was 25.500000. running mean: -9.725035\n",
      "ep 1646: ep_len:639 episode reward: total was -21.000000. running mean: -9.837785\n",
      "ep 1646: ep_len:500 episode reward: total was 31.150000. running mean: -9.427907\n",
      "ep 1646: ep_len:1287 episode reward: total was -79.820000. running mean: -10.131828\n",
      "ep 1646: ep_len:729 episode reward: total was 29.030000. running mean: -9.740209\n",
      "ep 1646: ep_len:578 episode reward: total was 32.050000. running mean: -9.322307\n",
      "ep 1646: ep_len:79 episode reward: total was 38.000000. running mean: -8.849084\n",
      "ep 1646: ep_len:106 episode reward: total was 51.500000. running mean: -8.245593\n",
      "ep 1646: ep_len:644 episode reward: total was -1.420000. running mean: -8.177337\n",
      "ep 1646: ep_len:2815 episode reward: total was -17.650000. running mean: -8.272064\n",
      "epsilon:0.009992 episode_count: 24808. steps_count: 26580374.000000\n",
      "ep 1647: ep_len:1408 episode reward: total was 31.080000. running mean: -7.878543\n",
      "ep 1647: ep_len:1692 episode reward: total was -50.210000. running mean: -8.301858\n",
      "ep 1647: ep_len:56 episode reward: total was 26.500000. running mean: -7.953839\n",
      "ep 1647: ep_len:2866 episode reward: total was -55.770000. running mean: -8.432001\n",
      "ep 1647: ep_len:632 episode reward: total was -10.420000. running mean: -8.451881\n",
      "ep 1647: ep_len:37 episode reward: total was 15.500000. running mean: -8.212362\n",
      "ep 1647: ep_len:115 episode reward: total was 53.000000. running mean: -7.600239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1647: ep_len:110 episode reward: total was 53.500000. running mean: -6.989236\n",
      "ep 1647: ep_len:650 episode reward: total was 21.950000. running mean: -6.699844\n",
      "ep 1647: ep_len:3805 episode reward: total was -464.180000. running mean: -11.274645\n",
      "ep 1647: ep_len:742 episode reward: total was -55.960000. running mean: -11.721499\n",
      "ep 1647: ep_len:812 episode reward: total was 16.810000. running mean: -11.436184\n",
      "ep 1647: ep_len:1109 episode reward: total was -2.210000. running mean: -11.343922\n",
      "ep 1647: ep_len:155 episode reward: total was 74.500000. running mean: -10.485483\n",
      "ep 1647: ep_len:54 episode reward: total was 24.000000. running mean: -10.140628\n",
      "ep 1647: ep_len:126 episode reward: total was 55.500000. running mean: -9.484222\n",
      "ep 1647: ep_len:697 episode reward: total was -66.930000. running mean: -10.058680\n",
      "ep 1647: ep_len:2832 episode reward: total was -7.140000. running mean: -10.029493\n",
      "epsilon:0.009992 episode_count: 24826. steps_count: 26598272.000000\n",
      "ep 1648: ep_len:1131 episode reward: total was -18.090000. running mean: -10.110098\n",
      "ep 1648: ep_len:829 episode reward: total was 14.810000. running mean: -9.860897\n",
      "ep 1648: ep_len:2973 episode reward: total was -21.300000. running mean: -9.975288\n",
      "ep 1648: ep_len:691 episode reward: total was -2.350000. running mean: -9.899035\n",
      "ep 1648: ep_len:1044 episode reward: total was -33.160000. running mean: -10.131645\n",
      "ep 1648: ep_len:4159 episode reward: total was -115.100000. running mean: -11.181328\n",
      "ep 1648: ep_len:647 episode reward: total was -20.390000. running mean: -11.273415\n",
      "ep 1648: ep_len:849 episode reward: total was 64.460000. running mean: -10.516081\n",
      "ep 1648: ep_len:500 episode reward: total was 12.840000. running mean: -10.282520\n",
      "ep 1648: ep_len:55 episode reward: total was 24.500000. running mean: -9.934695\n",
      "ep 1648: ep_len:1127 episode reward: total was -18.190000. running mean: -10.017248\n",
      "ep 1648: ep_len:2897 episode reward: total was -350.350000. running mean: -13.420575\n",
      "ep 1648: ep_len:53 episode reward: total was 23.500000. running mean: -13.051370\n",
      "epsilon:0.009992 episode_count: 24839. steps_count: 26615227.000000\n",
      "ep 1649: ep_len:686 episode reward: total was 13.040000. running mean: -12.790456\n",
      "ep 1649: ep_len:688 episode reward: total was -24.080000. running mean: -12.903351\n",
      "ep 1649: ep_len:44 episode reward: total was 20.500000. running mean: -12.569318\n",
      "ep 1649: ep_len:100 episode reward: total was 48.500000. running mean: -11.958625\n",
      "ep 1649: ep_len:693 episode reward: total was -4.940000. running mean: -11.888438\n",
      "ep 1649: ep_len:58 episode reward: total was 24.500000. running mean: -11.524554\n",
      "ep 1649: ep_len:178 episode reward: total was 81.500000. running mean: -10.594308\n",
      "ep 1649: ep_len:40 episode reward: total was 18.500000. running mean: -10.303365\n",
      "ep 1649: ep_len:755 episode reward: total was 27.720000. running mean: -9.923132\n",
      "ep 1649: ep_len:3884 episode reward: total was -19.530000. running mean: -10.019200\n",
      "ep 1649: ep_len:784 episode reward: total was -53.190000. running mean: -10.450908\n",
      "ep 1649: ep_len:741 episode reward: total was -9.810000. running mean: -10.444499\n",
      "ep 1649: ep_len:976 episode reward: total was 47.970000. running mean: -9.860354\n",
      "ep 1649: ep_len:37 episode reward: total was 15.500000. running mean: -9.606751\n",
      "ep 1649: ep_len:81 episode reward: total was 39.000000. running mean: -9.120683\n",
      "ep 1649: ep_len:969 episode reward: total was -38.960000. running mean: -9.419076\n",
      "ep 1649: ep_len:2867 episode reward: total was 3.010000. running mean: -9.294786\n",
      "ep 1649: ep_len:41 episode reward: total was 17.500000. running mean: -9.026838\n",
      "epsilon:0.009992 episode_count: 24857. steps_count: 26628849.000000\n",
      "ep 1650: ep_len:1493 episode reward: total was -2.560000. running mean: -8.962169\n",
      "ep 1650: ep_len:683 episode reward: total was -13.780000. running mean: -9.010348\n",
      "ep 1650: ep_len:2955 episode reward: total was -0.510000. running mean: -8.925344\n",
      "ep 1650: ep_len:706 episode reward: total was -11.700000. running mean: -8.953091\n",
      "ep 1650: ep_len:59 episode reward: total was 28.000000. running mean: -8.583560\n",
      "ep 1650: ep_len:58 episode reward: total was 26.000000. running mean: -8.237724\n",
      "ep 1650: ep_len:907 episode reward: total was 72.830000. running mean: -7.427047\n",
      "ep 1650: ep_len:629 episode reward: total was 22.410000. running mean: -7.128677\n",
      "ep 1650: ep_len:920 episode reward: total was -45.630000. running mean: -7.513690\n",
      "ep 1650: ep_len:771 episode reward: total was -2.040000. running mean: -7.458953\n",
      "ep 1650: ep_len:1098 episode reward: total was -40.700000. running mean: -7.791363\n",
      "ep 1650: ep_len:1167 episode reward: total was -47.080000. running mean: -8.184250\n",
      "ep 1650: ep_len:2892 episode reward: total was -36.750000. running mean: -8.469907\n",
      "epsilon:0.009992 episode_count: 24870. steps_count: 26643187.000000\n",
      "ep 1651: ep_len:1113 episode reward: total was 1.560000. running mean: -8.369608\n",
      "ep 1651: ep_len:1253 episode reward: total was -102.780000. running mean: -9.313712\n",
      "ep 1651: ep_len:92 episode reward: total was 41.500000. running mean: -8.805575\n",
      "ep 1651: ep_len:823 episode reward: total was 37.360000. running mean: -8.343919\n",
      "ep 1651: ep_len:147 episode reward: total was 70.500000. running mean: -7.555480\n",
      "ep 1651: ep_len:834 episode reward: total was 15.880000. running mean: -7.321125\n",
      "ep 1651: ep_len:4131 episode reward: total was -134.870000. running mean: -8.596614\n",
      "ep 1651: ep_len:1601 episode reward: total was -96.310000. running mean: -9.473748\n",
      "ep 1651: ep_len:870 episode reward: total was 67.090000. running mean: -8.708110\n",
      "ep 1651: ep_len:693 episode reward: total was 13.730000. running mean: -8.483729\n",
      "ep 1651: ep_len:49 episode reward: total was 23.000000. running mean: -8.168892\n",
      "ep 1651: ep_len:500 episode reward: total was 29.310000. running mean: -7.794103\n",
      "ep 1651: ep_len:45 episode reward: total was 12.000000. running mean: -7.596162\n",
      "epsilon:0.009992 episode_count: 24883. steps_count: 26655338.000000\n",
      "ep 1652: ep_len:1019 episode reward: total was -61.690000. running mean: -8.137100\n",
      "ep 1652: ep_len:1282 episode reward: total was -57.040000. running mean: -8.626129\n",
      "ep 1652: ep_len:70 episode reward: total was 32.000000. running mean: -8.219868\n",
      "ep 1652: ep_len:3002 episode reward: total was -20.640000. running mean: -8.344069\n",
      "ep 1652: ep_len:792 episode reward: total was 33.950000. running mean: -7.921129\n",
      "ep 1652: ep_len:42 episode reward: total was 19.500000. running mean: -7.646917\n",
      "ep 1652: ep_len:141 episode reward: total was 69.000000. running mean: -6.880448\n",
      "ep 1652: ep_len:76 episode reward: total was 36.500000. running mean: -6.446644\n",
      "ep 1652: ep_len:1366 episode reward: total was -38.170000. running mean: -6.763877\n",
      "ep 1652: ep_len:4065 episode reward: total was -60.520000. running mean: -7.301439\n",
      "ep 1652: ep_len:1220 episode reward: total was -40.490000. running mean: -7.633324\n",
      "ep 1652: ep_len:769 episode reward: total was -2.120000. running mean: -7.578191\n",
      "ep 1652: ep_len:861 episode reward: total was 10.520000. running mean: -7.397209\n",
      "ep 1652: ep_len:118 episode reward: total was 57.010000. running mean: -6.753137\n",
      "ep 1652: ep_len:23 episode reward: total was 8.500000. running mean: -6.600606\n",
      "ep 1652: ep_len:114 episode reward: total was 54.000000. running mean: -5.994600\n",
      "ep 1652: ep_len:650 episode reward: total was 10.610000. running mean: -5.828554\n",
      "ep 1652: ep_len:2854 episode reward: total was -7.280000. running mean: -5.843068\n",
      "epsilon:0.009992 episode_count: 24901. steps_count: 26673802.000000\n",
      "ep 1653: ep_len:821 episode reward: total was -18.150000. running mean: -5.966137\n",
      "ep 1653: ep_len:662 episode reward: total was -18.800000. running mean: -6.094476\n",
      "ep 1653: ep_len:3091 episode reward: total was -7.130000. running mean: -6.104831\n",
      "ep 1653: ep_len:500 episode reward: total was 12.740000. running mean: -5.916383\n",
      "ep 1653: ep_len:60 episode reward: total was 27.000000. running mean: -5.587219\n",
      "ep 1653: ep_len:1042 episode reward: total was -8.820000. running mean: -5.619547\n",
      "ep 1653: ep_len:349 episode reward: total was 11.570000. running mean: -5.447651\n",
      "ep 1653: ep_len:793 episode reward: total was -18.880000. running mean: -5.581975\n",
      "ep 1653: ep_len:7236 episode reward: total was -21.160000. running mean: -5.737755\n",
      "ep 1653: ep_len:611 episode reward: total was -13.250000. running mean: -5.812878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1653: ep_len:57 episode reward: total was 27.000000. running mean: -5.484749\n",
      "ep 1653: ep_len:83 episode reward: total was 40.000000. running mean: -5.029901\n",
      "ep 1653: ep_len:670 episode reward: total was 20.190000. running mean: -4.777702\n",
      "ep 1653: ep_len:2835 episode reward: total was -20.730000. running mean: -4.937225\n",
      "ep 1653: ep_len:61 episode reward: total was 29.000000. running mean: -4.597853\n",
      "epsilon:0.009992 episode_count: 24916. steps_count: 26692673.000000\n",
      "ep 1654: ep_len:765 episode reward: total was -71.300000. running mean: -5.264875\n",
      "ep 1654: ep_len:680 episode reward: total was -44.100000. running mean: -5.653226\n",
      "ep 1654: ep_len:45 episode reward: total was 19.500000. running mean: -5.401694\n",
      "ep 1654: ep_len:3081 episode reward: total was -54.470000. running mean: -5.892377\n",
      "ep 1654: ep_len:867 episode reward: total was 17.280000. running mean: -5.660653\n",
      "ep 1654: ep_len:148 episode reward: total was 72.500000. running mean: -4.879046\n",
      "ep 1654: ep_len:500 episode reward: total was 11.400000. running mean: -4.716256\n",
      "ep 1654: ep_len:3845 episode reward: total was -282.890000. running mean: -7.497993\n",
      "ep 1654: ep_len:2159 episode reward: total was -234.250000. running mean: -9.765513\n",
      "ep 1654: ep_len:735 episode reward: total was 12.360000. running mean: -9.544258\n",
      "ep 1654: ep_len:1077 episode reward: total was 40.570000. running mean: -9.043116\n",
      "ep 1654: ep_len:127 episode reward: total was 60.010000. running mean: -8.352584\n",
      "ep 1654: ep_len:1458 episode reward: total was 1.220000. running mean: -8.256859\n",
      "ep 1654: ep_len:2799 episode reward: total was -4.280000. running mean: -8.217090\n",
      "ep 1654: ep_len:53 episode reward: total was 23.500000. running mean: -7.899919\n",
      "epsilon:0.009992 episode_count: 24931. steps_count: 26711012.000000\n",
      "ep 1655: ep_len:1492 episode reward: total was 6.850000. running mean: -7.752420\n",
      "ep 1655: ep_len:747 episode reward: total was -14.920000. running mean: -7.824096\n",
      "ep 1655: ep_len:2859 episode reward: total was -67.990000. running mean: -8.425755\n",
      "ep 1655: ep_len:671 episode reward: total was 0.900000. running mean: -8.332497\n",
      "ep 1655: ep_len:19 episode reward: total was 8.000000. running mean: -8.169172\n",
      "ep 1655: ep_len:65 episode reward: total was 31.000000. running mean: -7.777481\n",
      "ep 1655: ep_len:652 episode reward: total was -3.200000. running mean: -7.731706\n",
      "ep 1655: ep_len:367 episode reward: total was 15.790000. running mean: -7.496489\n",
      "ep 1655: ep_len:504 episode reward: total was -38.560000. running mean: -7.807124\n",
      "ep 1655: ep_len:806 episode reward: total was 31.720000. running mean: -7.411853\n",
      "ep 1655: ep_len:608 episode reward: total was -5.200000. running mean: -7.389734\n",
      "ep 1655: ep_len:641 episode reward: total was 7.040000. running mean: -7.245437\n",
      "ep 1655: ep_len:2916 episode reward: total was -8.190000. running mean: -7.254882\n",
      "epsilon:0.009992 episode_count: 24944. steps_count: 26723359.000000\n",
      "ep 1656: ep_len:793 episode reward: total was -34.660000. running mean: -7.528933\n",
      "ep 1656: ep_len:500 episode reward: total was 13.810000. running mean: -7.315544\n",
      "ep 1656: ep_len:3055 episode reward: total was -52.830000. running mean: -7.770689\n",
      "ep 1656: ep_len:576 episode reward: total was 19.590000. running mean: -7.497082\n",
      "ep 1656: ep_len:39 episode reward: total was 18.000000. running mean: -7.242111\n",
      "ep 1656: ep_len:132 episode reward: total was 61.500000. running mean: -6.554690\n",
      "ep 1656: ep_len:1005 episode reward: total was -47.690000. running mean: -6.966043\n",
      "ep 1656: ep_len:3629 episode reward: total was -36.220000. running mean: -7.258583\n",
      "ep 1656: ep_len:959 episode reward: total was -37.040000. running mean: -7.556397\n",
      "ep 1656: ep_len:775 episode reward: total was 18.890000. running mean: -7.291933\n",
      "ep 1656: ep_len:638 episode reward: total was -26.580000. running mean: -7.484813\n",
      "ep 1656: ep_len:65 episode reward: total was 31.000000. running mean: -7.099965\n",
      "ep 1656: ep_len:200 episode reward: total was 94.000000. running mean: -6.088966\n",
      "ep 1656: ep_len:106 episode reward: total was 50.000000. running mean: -5.528076\n",
      "ep 1656: ep_len:1092 episode reward: total was 15.130000. running mean: -5.321495\n",
      "ep 1656: ep_len:2864 episode reward: total was -18.170000. running mean: -5.449980\n",
      "epsilon:0.009992 episode_count: 24960. steps_count: 26739787.000000\n",
      "ep 1657: ep_len:1174 episode reward: total was -14.340000. running mean: -5.538880\n",
      "ep 1657: ep_len:698 episode reward: total was -3.310000. running mean: -5.516592\n",
      "ep 1657: ep_len:3024 episode reward: total was -48.150000. running mean: -5.942926\n",
      "ep 1657: ep_len:1193 episode reward: total was -36.680000. running mean: -6.250297\n",
      "ep 1657: ep_len:95 episode reward: total was 46.000000. running mean: -5.727794\n",
      "ep 1657: ep_len:608 episode reward: total was 6.880000. running mean: -5.601716\n",
      "ep 1657: ep_len:4090 episode reward: total was -190.220000. running mean: -7.447898\n",
      "ep 1657: ep_len:777 episode reward: total was 8.580000. running mean: -7.287619\n",
      "ep 1657: ep_len:660 episode reward: total was -2.780000. running mean: -7.242543\n",
      "ep 1657: ep_len:673 episode reward: total was -4.150000. running mean: -7.211618\n",
      "ep 1657: ep_len:55 episode reward: total was 24.500000. running mean: -6.894502\n",
      "ep 1657: ep_len:641 episode reward: total was -4.050000. running mean: -6.866057\n",
      "ep 1657: ep_len:2816 episode reward: total was -3.070000. running mean: -6.828096\n",
      "ep 1657: ep_len:26 episode reward: total was 11.500000. running mean: -6.644815\n",
      "epsilon:0.009992 episode_count: 24974. steps_count: 26756317.000000\n",
      "ep 1658: ep_len:1158 episode reward: total was 9.710000. running mean: -6.481267\n",
      "ep 1658: ep_len:723 episode reward: total was -84.070000. running mean: -7.257154\n",
      "ep 1658: ep_len:2962 episode reward: total was -32.820000. running mean: -7.512783\n",
      "ep 1658: ep_len:1816 episode reward: total was -628.120000. running mean: -13.718855\n",
      "ep 1658: ep_len:37 episode reward: total was 17.000000. running mean: -13.411666\n",
      "ep 1658: ep_len:58 episode reward: total was 27.500000. running mean: -13.002550\n",
      "ep 1658: ep_len:500 episode reward: total was 34.940000. running mean: -12.523124\n",
      "ep 1658: ep_len:3687 episode reward: total was -36.400000. running mean: -12.761893\n",
      "ep 1658: ep_len:1601 episode reward: total was -41.670000. running mean: -13.050974\n",
      "ep 1658: ep_len:7237 episode reward: total was 9.330000. running mean: -12.827164\n",
      "ep 1658: ep_len:2188 episode reward: total was -189.870000. running mean: -14.597593\n",
      "ep 1658: ep_len:56 episode reward: total was 26.500000. running mean: -14.186617\n",
      "ep 1658: ep_len:47 episode reward: total was 22.000000. running mean: -13.824751\n",
      "ep 1658: ep_len:1497 episode reward: total was 5.710000. running mean: -13.629403\n",
      "ep 1658: ep_len:2795 episode reward: total was -3.430000. running mean: -13.527409\n",
      "epsilon:0.009992 episode_count: 24989. steps_count: 26782679.000000\n",
      "ep 1659: ep_len:1104 episode reward: total was -8.470000. running mean: -13.476835\n",
      "ep 1659: ep_len:718 episode reward: total was -31.050000. running mean: -13.652567\n",
      "ep 1659: ep_len:2999 episode reward: total was -42.310000. running mean: -13.939141\n",
      "ep 1659: ep_len:1124 episode reward: total was -18.220000. running mean: -13.981950\n",
      "ep 1659: ep_len:51 episode reward: total was 24.000000. running mean: -13.602130\n",
      "ep 1659: ep_len:113 episode reward: total was 52.000000. running mean: -12.946109\n",
      "ep 1659: ep_len:1914 episode reward: total was -83.620000. running mean: -13.652848\n",
      "ep 1659: ep_len:4009 episode reward: total was -218.120000. running mean: -15.697519\n",
      "ep 1659: ep_len:562 episode reward: total was -37.980000. running mean: -15.920344\n",
      "ep 1659: ep_len:700 episode reward: total was 27.570000. running mean: -15.485441\n",
      "ep 1659: ep_len:736 episode reward: total was -21.090000. running mean: -15.541486\n",
      "ep 1659: ep_len:185 episode reward: total was 91.000000. running mean: -14.476071\n",
      "ep 1659: ep_len:36 episode reward: total was 16.500000. running mean: -14.166311\n",
      "ep 1659: ep_len:106 episode reward: total was 51.500000. running mean: -13.509647\n",
      "ep 1659: ep_len:1000 episode reward: total was -48.020000. running mean: -13.854751\n",
      "ep 1659: ep_len:2862 episode reward: total was 5.130000. running mean: -13.664903\n",
      "epsilon:0.009992 episode_count: 25005. steps_count: 26800898.000000\n",
      "ep 1660: ep_len:1066 episode reward: total was 5.650000. running mean: -13.471754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1660: ep_len:681 episode reward: total was -14.570000. running mean: -13.482737\n",
      "ep 1660: ep_len:57 episode reward: total was 27.000000. running mean: -13.077910\n",
      "ep 1660: ep_len:3007 episode reward: total was -26.770000. running mean: -13.214830\n",
      "ep 1660: ep_len:682 episode reward: total was -16.680000. running mean: -13.249482\n",
      "ep 1660: ep_len:45 episode reward: total was 21.000000. running mean: -12.906987\n",
      "ep 1660: ep_len:1318 episode reward: total was -51.170000. running mean: -13.289617\n",
      "ep 1660: ep_len:634 episode reward: total was 5.320000. running mean: -13.103521\n",
      "ep 1660: ep_len:1265 episode reward: total was -68.230000. running mean: -13.654786\n",
      "ep 1660: ep_len:825 episode reward: total was 47.590000. running mean: -13.042338\n",
      "ep 1660: ep_len:702 episode reward: total was -33.650000. running mean: -13.248415\n",
      "ep 1660: ep_len:66 episode reward: total was 31.500000. running mean: -12.800931\n",
      "ep 1660: ep_len:1477 episode reward: total was -18.730000. running mean: -12.860221\n",
      "ep 1660: ep_len:2850 episode reward: total was -16.140000. running mean: -12.893019\n",
      "ep 1660: ep_len:40 episode reward: total was 18.500000. running mean: -12.579089\n",
      "epsilon:0.009992 episode_count: 25020. steps_count: 26815613.000000\n",
      "ep 1661: ep_len:657 episode reward: total was 27.150000. running mean: -12.181798\n",
      "ep 1661: ep_len:213 episode reward: total was -9.690000. running mean: -12.156880\n",
      "ep 1661: ep_len:3105 episode reward: total was -3.040000. running mean: -12.065711\n",
      "ep 1661: ep_len:1246 episode reward: total was -28.050000. running mean: -12.225554\n",
      "ep 1661: ep_len:37 episode reward: total was 17.000000. running mean: -11.933299\n",
      "ep 1661: ep_len:71 episode reward: total was 34.000000. running mean: -11.473966\n",
      "ep 1661: ep_len:500 episode reward: total was -2.880000. running mean: -11.388026\n",
      "ep 1661: ep_len:3748 episode reward: total was -96.640000. running mean: -12.240546\n",
      "ep 1661: ep_len:607 episode reward: total was -19.510000. running mean: -12.313240\n",
      "ep 1661: ep_len:7511 episode reward: total was -160.890000. running mean: -13.799008\n",
      "ep 1661: ep_len:500 episode reward: total was 26.030000. running mean: -13.400718\n",
      "ep 1661: ep_len:61 episode reward: total was 29.000000. running mean: -12.976711\n",
      "ep 1661: ep_len:688 episode reward: total was 16.400000. running mean: -12.682943\n",
      "ep 1661: ep_len:2780 episode reward: total was -26.690000. running mean: -12.823014\n",
      "ep 1661: ep_len:54 episode reward: total was 24.000000. running mean: -12.454784\n",
      "epsilon:0.009992 episode_count: 25035. steps_count: 26837391.000000\n",
      "ep 1662: ep_len:1120 episode reward: total was 3.360000. running mean: -12.296636\n",
      "ep 1662: ep_len:1579 episode reward: total was -63.130000. running mean: -12.804970\n",
      "ep 1662: ep_len:3015 episode reward: total was -42.490000. running mean: -13.101820\n",
      "ep 1662: ep_len:500 episode reward: total was 15.530000. running mean: -12.815502\n",
      "ep 1662: ep_len:58 episode reward: total was 27.500000. running mean: -12.412347\n",
      "ep 1662: ep_len:122 episode reward: total was 58.000000. running mean: -11.708223\n",
      "ep 1662: ep_len:69 episode reward: total was 33.000000. running mean: -11.261141\n",
      "ep 1662: ep_len:1434 episode reward: total was -20.510000. running mean: -11.353630\n",
      "ep 1662: ep_len:609 episode reward: total was -144.780000. running mean: -12.687893\n",
      "ep 1662: ep_len:854 episode reward: total was -2.960000. running mean: -12.590614\n",
      "ep 1662: ep_len:710 episode reward: total was 15.420000. running mean: -12.310508\n",
      "ep 1662: ep_len:644 episode reward: total was 0.210000. running mean: -12.185303\n",
      "ep 1662: ep_len:110 episode reward: total was 53.500000. running mean: -11.528450\n",
      "ep 1662: ep_len:67 episode reward: total was 32.000000. running mean: -11.093166\n",
      "ep 1662: ep_len:81 episode reward: total was 39.000000. running mean: -10.592234\n",
      "ep 1662: ep_len:632 episode reward: total was 4.430000. running mean: -10.442012\n",
      "ep 1662: ep_len:2896 episode reward: total was -5.030000. running mean: -10.387892\n",
      "epsilon:0.009992 episode_count: 25052. steps_count: 26851891.000000\n",
      "ep 1663: ep_len:671 episode reward: total was -4.570000. running mean: -10.329713\n",
      "ep 1663: ep_len:742 episode reward: total was -25.560000. running mean: -10.482016\n",
      "ep 1663: ep_len:72 episode reward: total was 34.500000. running mean: -10.032195\n",
      "ep 1663: ep_len:3000 episode reward: total was -63.000000. running mean: -10.561873\n",
      "ep 1663: ep_len:500 episode reward: total was -28.710000. running mean: -10.743355\n",
      "ep 1663: ep_len:1433 episode reward: total was -1.510000. running mean: -10.651021\n",
      "ep 1663: ep_len:677 episode reward: total was 21.790000. running mean: -10.326611\n",
      "ep 1663: ep_len:558 episode reward: total was 6.490000. running mean: -10.158445\n",
      "ep 1663: ep_len:644 episode reward: total was 7.530000. running mean: -9.981560\n",
      "ep 1663: ep_len:619 episode reward: total was 34.380000. running mean: -9.537945\n",
      "ep 1663: ep_len:831 episode reward: total was 19.160000. running mean: -9.250965\n",
      "ep 1663: ep_len:2828 episode reward: total was 6.660000. running mean: -9.091856\n",
      "epsilon:0.009992 episode_count: 25064. steps_count: 26864466.000000\n",
      "ep 1664: ep_len:738 episode reward: total was -19.590000. running mean: -9.196837\n",
      "ep 1664: ep_len:701 episode reward: total was -78.580000. running mean: -9.890669\n",
      "ep 1664: ep_len:2998 episode reward: total was -39.070000. running mean: -10.182462\n",
      "ep 1664: ep_len:668 episode reward: total was -26.840000. running mean: -10.349037\n",
      "ep 1664: ep_len:37 episode reward: total was 17.000000. running mean: -10.075547\n",
      "ep 1664: ep_len:60 episode reward: total was 28.500000. running mean: -9.689792\n",
      "ep 1664: ep_len:1171 episode reward: total was -26.840000. running mean: -9.861294\n",
      "ep 1664: ep_len:621 episode reward: total was 23.490000. running mean: -9.527781\n",
      "ep 1664: ep_len:1505 episode reward: total was -21.450000. running mean: -9.647003\n",
      "ep 1664: ep_len:731 episode reward: total was 48.120000. running mean: -9.069333\n",
      "ep 1664: ep_len:566 episode reward: total was 22.310000. running mean: -8.755540\n",
      "ep 1664: ep_len:37 episode reward: total was 15.500000. running mean: -8.512984\n",
      "ep 1664: ep_len:771 episode reward: total was -75.280000. running mean: -9.180654\n",
      "ep 1664: ep_len:2859 episode reward: total was 12.080000. running mean: -8.968048\n",
      "epsilon:0.009992 episode_count: 25078. steps_count: 26877929.000000\n",
      "ep 1665: ep_len:797 episode reward: total was -70.980000. running mean: -9.588167\n",
      "ep 1665: ep_len:1645 episode reward: total was -38.290000. running mean: -9.875186\n",
      "ep 1665: ep_len:2974 episode reward: total was -47.070000. running mean: -10.247134\n",
      "ep 1665: ep_len:614 episode reward: total was -3.120000. running mean: -10.175862\n",
      "ep 1665: ep_len:68 episode reward: total was 32.500000. running mean: -9.749104\n",
      "ep 1665: ep_len:519 episode reward: total was 30.780000. running mean: -9.343813\n",
      "ep 1665: ep_len:3673 episode reward: total was -85.270000. running mean: -10.103075\n",
      "ep 1665: ep_len:933 episode reward: total was -17.180000. running mean: -10.173844\n",
      "ep 1665: ep_len:804 episode reward: total was 28.740000. running mean: -9.784705\n",
      "ep 1665: ep_len:1415 episode reward: total was 12.510000. running mean: -9.561758\n",
      "ep 1665: ep_len:82 episode reward: total was 39.500000. running mean: -9.071141\n",
      "ep 1665: ep_len:500 episode reward: total was 7.780000. running mean: -8.902629\n",
      "ep 1665: ep_len:2875 episode reward: total was -1.990000. running mean: -8.833503\n",
      "ep 1665: ep_len:68 episode reward: total was 32.500000. running mean: -8.420168\n",
      "epsilon:0.009992 episode_count: 25092. steps_count: 26894896.000000\n",
      "ep 1666: ep_len:1134 episode reward: total was 2.330000. running mean: -8.312666\n",
      "ep 1666: ep_len:500 episode reward: total was 19.930000. running mean: -8.030240\n",
      "ep 1666: ep_len:3157 episode reward: total was -54.800000. running mean: -8.497937\n",
      "ep 1666: ep_len:510 episode reward: total was -49.610000. running mean: -8.909058\n",
      "ep 1666: ep_len:135 episode reward: total was 64.500000. running mean: -8.174967\n",
      "ep 1666: ep_len:43 episode reward: total was 18.500000. running mean: -7.908218\n",
      "ep 1666: ep_len:1415 episode reward: total was 12.360000. running mean: -7.705536\n",
      "ep 1666: ep_len:346 episode reward: total was 15.090000. running mean: -7.477580\n",
      "ep 1666: ep_len:923 episode reward: total was -21.110000. running mean: -7.613904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1666: ep_len:631 episode reward: total was -32.090000. running mean: -7.858665\n",
      "ep 1666: ep_len:611 episode reward: total was -9.770000. running mean: -7.877779\n",
      "ep 1666: ep_len:44 episode reward: total was 20.500000. running mean: -7.594001\n",
      "ep 1666: ep_len:103 episode reward: total was 45.500000. running mean: -7.063061\n",
      "ep 1666: ep_len:1075 episode reward: total was 8.500000. running mean: -6.907430\n",
      "ep 1666: ep_len:2861 episode reward: total was -3.240000. running mean: -6.870756\n",
      "ep 1666: ep_len:47 episode reward: total was 22.000000. running mean: -6.582048\n",
      "epsilon:0.009992 episode_count: 25108. steps_count: 26908431.000000\n",
      "ep 1667: ep_len:1444 episode reward: total was -20.260000. running mean: -6.718828\n",
      "ep 1667: ep_len:772 episode reward: total was -12.650000. running mean: -6.778140\n",
      "ep 1667: ep_len:3044 episode reward: total was -42.440000. running mean: -7.134758\n",
      "ep 1667: ep_len:919 episode reward: total was 50.230000. running mean: -6.561111\n",
      "ep 1667: ep_len:127 episode reward: total was 62.000000. running mean: -5.875500\n",
      "ep 1667: ep_len:73 episode reward: total was 32.000000. running mean: -5.496745\n",
      "ep 1667: ep_len:1061 episode reward: total was -41.580000. running mean: -5.857577\n",
      "ep 1667: ep_len:312 episode reward: total was 4.310000. running mean: -5.755901\n",
      "ep 1667: ep_len:1563 episode reward: total was -34.340000. running mean: -6.041742\n",
      "ep 1667: ep_len:874 episode reward: total was 60.820000. running mean: -5.373125\n",
      "ep 1667: ep_len:536 episode reward: total was 7.410000. running mean: -5.245294\n",
      "ep 1667: ep_len:81 episode reward: total was 36.000000. running mean: -4.832841\n",
      "ep 1667: ep_len:176 episode reward: total was 85.000000. running mean: -3.934512\n",
      "ep 1667: ep_len:1074 episode reward: total was -7.980000. running mean: -3.974967\n",
      "ep 1667: ep_len:2882 episode reward: total was -3.390000. running mean: -3.969118\n",
      "epsilon:0.009992 episode_count: 25123. steps_count: 26923369.000000\n",
      "ep 1668: ep_len:921 episode reward: total was -81.330000. running mean: -4.742726\n",
      "ep 1668: ep_len:671 episode reward: total was -36.890000. running mean: -5.064199\n",
      "ep 1668: ep_len:2887 episode reward: total was -66.820000. running mean: -5.681757\n",
      "ep 1668: ep_len:815 episode reward: total was 14.430000. running mean: -5.480640\n",
      "ep 1668: ep_len:165 episode reward: total was 79.500000. running mean: -4.630833\n",
      "ep 1668: ep_len:85 episode reward: total was 41.000000. running mean: -4.174525\n",
      "ep 1668: ep_len:1094 episode reward: total was -28.560000. running mean: -4.418380\n",
      "ep 1668: ep_len:3541 episode reward: total was -212.840000. running mean: -6.502596\n",
      "ep 1668: ep_len:2171 episode reward: total was -406.700000. running mean: -10.504570\n",
      "ep 1668: ep_len:806 episode reward: total was 17.170000. running mean: -10.227824\n",
      "ep 1668: ep_len:500 episode reward: total was 36.270000. running mean: -9.762846\n",
      "ep 1668: ep_len:84 episode reward: total was 40.500000. running mean: -9.260217\n",
      "ep 1668: ep_len:97 episode reward: total was 47.000000. running mean: -8.697615\n",
      "ep 1668: ep_len:500 episode reward: total was 42.530000. running mean: -8.185339\n",
      "ep 1668: ep_len:2774 episode reward: total was -0.920000. running mean: -8.112686\n",
      "ep 1668: ep_len:55 episode reward: total was 26.000000. running mean: -7.771559\n",
      "epsilon:0.009992 episode_count: 25139. steps_count: 26940535.000000\n",
      "ep 1669: ep_len:1042 episode reward: total was -49.930000. running mean: -8.193143\n",
      "ep 1669: ep_len:814 episode reward: total was 5.900000. running mean: -8.052212\n",
      "ep 1669: ep_len:66 episode reward: total was 28.500000. running mean: -7.686690\n",
      "ep 1669: ep_len:2908 episode reward: total was -44.360000. running mean: -8.053423\n",
      "ep 1669: ep_len:1102 episode reward: total was -28.540000. running mean: -8.258289\n",
      "ep 1669: ep_len:134 episode reward: total was 64.000000. running mean: -7.535706\n",
      "ep 1669: ep_len:75 episode reward: total was 36.000000. running mean: -7.100349\n",
      "ep 1669: ep_len:53 episode reward: total was 25.000000. running mean: -6.779345\n",
      "ep 1669: ep_len:839 episode reward: total was 30.810000. running mean: -6.403452\n",
      "ep 1669: ep_len:621 episode reward: total was 20.100000. running mean: -6.138417\n",
      "ep 1669: ep_len:671 episode reward: total was -5.940000. running mean: -6.136433\n",
      "ep 1669: ep_len:597 episode reward: total was -10.730000. running mean: -6.182369\n",
      "ep 1669: ep_len:723 episode reward: total was -4.540000. running mean: -6.165945\n",
      "ep 1669: ep_len:117 episode reward: total was 55.500000. running mean: -5.549286\n",
      "ep 1669: ep_len:97 episode reward: total was 45.500000. running mean: -5.038793\n",
      "ep 1669: ep_len:604 episode reward: total was -6.860000. running mean: -5.057005\n",
      "ep 1669: ep_len:2739 episode reward: total was -18.810000. running mean: -5.194535\n",
      "epsilon:0.009992 episode_count: 25156. steps_count: 26953737.000000\n",
      "ep 1670: ep_len:995 episode reward: total was -63.470000. running mean: -5.777289\n",
      "ep 1670: ep_len:681 episode reward: total was -23.140000. running mean: -5.950916\n",
      "ep 1670: ep_len:58 episode reward: total was 26.000000. running mean: -5.631407\n",
      "ep 1670: ep_len:2949 episode reward: total was -54.130000. running mean: -6.116393\n",
      "ep 1670: ep_len:645 episode reward: total was 27.570000. running mean: -5.779529\n",
      "ep 1670: ep_len:29 episode reward: total was 13.000000. running mean: -5.591734\n",
      "ep 1670: ep_len:100 episode reward: total was -34.490000. running mean: -5.880717\n",
      "ep 1670: ep_len:625 episode reward: total was 7.430000. running mean: -5.747609\n",
      "ep 1670: ep_len:671 episode reward: total was 16.650000. running mean: -5.523633\n",
      "ep 1670: ep_len:583 episode reward: total was -24.640000. running mean: -5.714797\n",
      "ep 1670: ep_len:7385 episode reward: total was -299.190000. running mean: -8.649549\n",
      "ep 1670: ep_len:635 episode reward: total was -7.830000. running mean: -8.641354\n",
      "ep 1670: ep_len:195 episode reward: total was 94.500000. running mean: -7.609940\n",
      "ep 1670: ep_len:46 episode reward: total was 21.500000. running mean: -7.318841\n",
      "ep 1670: ep_len:1388 episode reward: total was -34.260000. running mean: -7.588252\n",
      "ep 1670: ep_len:2790 episode reward: total was -30.360000. running mean: -7.815970\n",
      "ep 1670: ep_len:46 episode reward: total was 21.500000. running mean: -7.522810\n",
      "epsilon:0.009992 episode_count: 25173. steps_count: 26973558.000000\n",
      "ep 1671: ep_len:920 episode reward: total was -79.690000. running mean: -8.244482\n",
      "ep 1671: ep_len:1094 episode reward: total was -24.030000. running mean: -8.402337\n",
      "ep 1671: ep_len:3078 episode reward: total was -16.020000. running mean: -8.478514\n",
      "ep 1671: ep_len:577 episode reward: total was -53.870000. running mean: -8.932429\n",
      "ep 1671: ep_len:38 episode reward: total was 16.000000. running mean: -8.683104\n",
      "ep 1671: ep_len:107 episode reward: total was 52.000000. running mean: -8.076273\n",
      "ep 1671: ep_len:556 episode reward: total was -76.310000. running mean: -8.758611\n",
      "ep 1671: ep_len:349 episode reward: total was 8.050000. running mean: -8.590524\n",
      "ep 1671: ep_len:759 episode reward: total was -17.440000. running mean: -8.679019\n",
      "ep 1671: ep_len:812 episode reward: total was 28.230000. running mean: -8.309929\n",
      "ep 1671: ep_len:664 episode reward: total was -8.870000. running mean: -8.315530\n",
      "ep 1671: ep_len:138 episode reward: total was 66.000000. running mean: -7.572374\n",
      "ep 1671: ep_len:52 episode reward: total was 24.500000. running mean: -7.251651\n",
      "ep 1671: ep_len:1151 episode reward: total was -35.120000. running mean: -7.530334\n",
      "ep 1671: ep_len:2863 episode reward: total was -25.860000. running mean: -7.713631\n",
      "epsilon:0.009992 episode_count: 25188. steps_count: 26986716.000000\n",
      "ep 1672: ep_len:668 episode reward: total was -34.900000. running mean: -7.985495\n",
      "ep 1672: ep_len:500 episode reward: total was 21.160000. running mean: -7.694040\n",
      "ep 1672: ep_len:3055 episode reward: total was -27.250000. running mean: -7.889599\n",
      "ep 1672: ep_len:800 episode reward: total was -27.460000. running mean: -8.085303\n",
      "ep 1672: ep_len:656 episode reward: total was 14.930000. running mean: -7.855150\n",
      "ep 1672: ep_len:329 episode reward: total was 25.600000. running mean: -7.520599\n",
      "ep 1672: ep_len:1482 episode reward: total was -30.770000. running mean: -7.753093\n",
      "ep 1672: ep_len:758 episode reward: total was -11.840000. running mean: -7.793962\n",
      "ep 1672: ep_len:3385 episode reward: total was -428.230000. running mean: -11.998322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1672: ep_len:181 episode reward: total was 86.000000. running mean: -11.018339\n",
      "ep 1672: ep_len:41 episode reward: total was 19.000000. running mean: -10.718156\n",
      "ep 1672: ep_len:1068 episode reward: total was -34.940000. running mean: -10.960374\n",
      "ep 1672: ep_len:2820 episode reward: total was 14.600000. running mean: -10.704770\n",
      "ep 1672: ep_len:55 episode reward: total was 24.500000. running mean: -10.352723\n",
      "epsilon:0.009992 episode_count: 25202. steps_count: 27002514.000000\n",
      "ep 1673: ep_len:844 episode reward: total was -35.610000. running mean: -10.605295\n",
      "ep 1673: ep_len:648 episode reward: total was -47.220000. running mean: -10.971442\n",
      "ep 1673: ep_len:3014 episode reward: total was -37.720000. running mean: -11.238928\n",
      "ep 1673: ep_len:500 episode reward: total was 27.930000. running mean: -10.847239\n",
      "ep 1673: ep_len:48 episode reward: total was 22.500000. running mean: -10.513766\n",
      "ep 1673: ep_len:59 episode reward: total was 26.500000. running mean: -10.143629\n",
      "ep 1673: ep_len:1084 episode reward: total was -37.720000. running mean: -10.419392\n",
      "ep 1673: ep_len:3752 episode reward: total was -375.360000. running mean: -14.068798\n",
      "ep 1673: ep_len:1990 episode reward: total was -118.450000. running mean: -15.112610\n",
      "ep 1673: ep_len:817 episode reward: total was 26.230000. running mean: -14.699184\n",
      "ep 1673: ep_len:587 episode reward: total was -6.360000. running mean: -14.615792\n",
      "ep 1673: ep_len:62 episode reward: total was 29.500000. running mean: -14.174635\n",
      "ep 1673: ep_len:618 episode reward: total was 1.720000. running mean: -14.015688\n",
      "ep 1673: ep_len:2843 episode reward: total was -3.450000. running mean: -13.910031\n",
      "epsilon:0.009992 episode_count: 25216. steps_count: 27019380.000000\n",
      "ep 1674: ep_len:1162 episode reward: total was -11.780000. running mean: -13.888731\n",
      "ep 1674: ep_len:773 episode reward: total was -14.480000. running mean: -13.894644\n",
      "ep 1674: ep_len:50 episode reward: total was 23.500000. running mean: -13.520697\n",
      "ep 1674: ep_len:3072 episode reward: total was -3.740000. running mean: -13.422890\n",
      "ep 1674: ep_len:787 episode reward: total was 4.100000. running mean: -13.247661\n",
      "ep 1674: ep_len:132 episode reward: total was 63.000000. running mean: -12.485185\n",
      "ep 1674: ep_len:619 episode reward: total was -2.060000. running mean: -12.380933\n",
      "ep 1674: ep_len:347 episode reward: total was -0.360000. running mean: -12.260724\n",
      "ep 1674: ep_len:793 episode reward: total was -14.380000. running mean: -12.281916\n",
      "ep 1674: ep_len:793 episode reward: total was -10.840000. running mean: -12.267497\n",
      "ep 1674: ep_len:823 episode reward: total was 33.480000. running mean: -11.810022\n",
      "ep 1674: ep_len:81 episode reward: total was 36.000000. running mean: -11.331922\n",
      "ep 1674: ep_len:49 episode reward: total was 23.000000. running mean: -10.988603\n",
      "ep 1674: ep_len:98 episode reward: total was 44.500000. running mean: -10.433717\n",
      "ep 1674: ep_len:585 episode reward: total was 20.920000. running mean: -10.120180\n",
      "ep 1674: ep_len:2786 episode reward: total was -26.300000. running mean: -10.281978\n",
      "ep 1674: ep_len:29 episode reward: total was 13.000000. running mean: -10.049158\n",
      "epsilon:0.009992 episode_count: 25233. steps_count: 27032359.000000\n",
      "ep 1675: ep_len:610 episode reward: total was -11.240000. running mean: -10.061066\n",
      "ep 1675: ep_len:758 episode reward: total was -10.880000. running mean: -10.069256\n",
      "ep 1675: ep_len:73 episode reward: total was 35.000000. running mean: -9.618563\n",
      "ep 1675: ep_len:2904 episode reward: total was -78.070000. running mean: -10.303078\n",
      "ep 1675: ep_len:781 episode reward: total was -24.680000. running mean: -10.446847\n",
      "ep 1675: ep_len:35 episode reward: total was 16.000000. running mean: -10.182378\n",
      "ep 1675: ep_len:97 episode reward: total was 47.000000. running mean: -9.610555\n",
      "ep 1675: ep_len:97 episode reward: total was 45.500000. running mean: -9.059449\n",
      "ep 1675: ep_len:646 episode reward: total was 0.770000. running mean: -8.961154\n",
      "ep 1675: ep_len:4105 episode reward: total was -2054.710000. running mean: -29.418643\n",
      "ep 1675: ep_len:767 episode reward: total was -15.830000. running mean: -29.282757\n",
      "ep 1675: ep_len:845 episode reward: total was 22.600000. running mean: -28.763929\n",
      "ep 1675: ep_len:696 episode reward: total was 9.330000. running mean: -28.382990\n",
      "ep 1675: ep_len:49 episode reward: total was 23.000000. running mean: -27.869160\n",
      "ep 1675: ep_len:128 episode reward: total was 58.000000. running mean: -27.010468\n",
      "ep 1675: ep_len:1053 episode reward: total was -62.900000. running mean: -27.369363\n",
      "ep 1675: ep_len:2908 episode reward: total was -25.110000. running mean: -27.346770\n",
      "epsilon:0.009992 episode_count: 25250. steps_count: 27048911.000000\n",
      "ep 1676: ep_len:731 episode reward: total was -17.490000. running mean: -27.248202\n",
      "ep 1676: ep_len:990 episode reward: total was 36.800000. running mean: -26.607720\n",
      "ep 1676: ep_len:31 episode reward: total was 14.000000. running mean: -26.201643\n",
      "ep 1676: ep_len:2897 episode reward: total was -23.010000. running mean: -26.169727\n",
      "ep 1676: ep_len:500 episode reward: total was 5.110000. running mean: -25.856929\n",
      "ep 1676: ep_len:1473 episode reward: total was 15.570000. running mean: -25.442660\n",
      "ep 1676: ep_len:3638 episode reward: total was -51.280000. running mean: -25.701033\n",
      "ep 1676: ep_len:517 episode reward: total was -33.870000. running mean: -25.782723\n",
      "ep 1676: ep_len:740 episode reward: total was 0.070000. running mean: -25.524196\n",
      "ep 1676: ep_len:580 episode reward: total was 10.610000. running mean: -25.162854\n",
      "ep 1676: ep_len:74 episode reward: total was 35.500000. running mean: -24.556225\n",
      "ep 1676: ep_len:781 episode reward: total was -81.240000. running mean: -25.123063\n",
      "ep 1676: ep_len:2918 episode reward: total was -98.680000. running mean: -25.858632\n",
      "epsilon:0.009992 episode_count: 25263. steps_count: 27064781.000000\n",
      "ep 1677: ep_len:1178 episode reward: total was -26.770000. running mean: -25.867746\n",
      "ep 1677: ep_len:1210 episode reward: total was -92.100000. running mean: -26.530069\n",
      "ep 1677: ep_len:3023 episode reward: total was -36.190000. running mean: -26.626668\n",
      "ep 1677: ep_len:678 episode reward: total was 10.310000. running mean: -26.257301\n",
      "ep 1677: ep_len:36 episode reward: total was 15.000000. running mean: -25.844728\n",
      "ep 1677: ep_len:163 episode reward: total was 78.500000. running mean: -24.801281\n",
      "ep 1677: ep_len:674 episode reward: total was -6.880000. running mean: -24.622068\n",
      "ep 1677: ep_len:640 episode reward: total was 18.050000. running mean: -24.195347\n",
      "ep 1677: ep_len:650 episode reward: total was -18.000000. running mean: -24.133394\n",
      "ep 1677: ep_len:683 episode reward: total was -0.220000. running mean: -23.894260\n",
      "ep 1677: ep_len:500 episode reward: total was 9.340000. running mean: -23.561917\n",
      "ep 1677: ep_len:593 episode reward: total was -0.300000. running mean: -23.329298\n",
      "ep 1677: ep_len:2796 episode reward: total was -26.660000. running mean: -23.362605\n",
      "epsilon:0.009992 episode_count: 25276. steps_count: 27077605.000000\n",
      "ep 1678: ep_len:1474 episode reward: total was -0.240000. running mean: -23.131379\n",
      "ep 1678: ep_len:692 episode reward: total was -33.700000. running mean: -23.237065\n",
      "ep 1678: ep_len:3079 episode reward: total was -69.140000. running mean: -23.696095\n",
      "ep 1678: ep_len:687 episode reward: total was 23.230000. running mean: -23.226834\n",
      "ep 1678: ep_len:154 episode reward: total was 69.500000. running mean: -22.299566\n",
      "ep 1678: ep_len:1486 episode reward: total was -2.480000. running mean: -22.101370\n",
      "ep 1678: ep_len:3668 episode reward: total was -28.240000. running mean: -22.162756\n",
      "ep 1678: ep_len:1520 episode reward: total was -59.870000. running mean: -22.539829\n",
      "ep 1678: ep_len:760 episode reward: total was 6.890000. running mean: -22.245530\n",
      "ep 1678: ep_len:1066 episode reward: total was -16.710000. running mean: -22.190175\n",
      "ep 1678: ep_len:56 episode reward: total was 26.500000. running mean: -21.703273\n",
      "ep 1678: ep_len:67 episode reward: total was 30.500000. running mean: -21.181241\n",
      "ep 1678: ep_len:1181 episode reward: total was -7.930000. running mean: -21.048728\n",
      "ep 1678: ep_len:2825 episode reward: total was -23.890000. running mean: -21.077141\n",
      "epsilon:0.009992 episode_count: 25290. steps_count: 27096320.000000\n",
      "ep 1679: ep_len:589 episode reward: total was -9.370000. running mean: -20.960069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1679: ep_len:735 episode reward: total was -2.920000. running mean: -20.779669\n",
      "ep 1679: ep_len:2942 episode reward: total was -46.280000. running mean: -21.034672\n",
      "ep 1679: ep_len:534 episode reward: total was -43.310000. running mean: -21.257425\n",
      "ep 1679: ep_len:65 episode reward: total was 31.000000. running mean: -20.734851\n",
      "ep 1679: ep_len:117 episode reward: total was 57.000000. running mean: -19.957503\n",
      "ep 1679: ep_len:68 episode reward: total was 31.000000. running mean: -19.447928\n",
      "ep 1679: ep_len:44 episode reward: total was 19.000000. running mean: -19.063448\n",
      "ep 1679: ep_len:500 episode reward: total was -12.760000. running mean: -19.000414\n",
      "ep 1679: ep_len:3652 episode reward: total was -62.370000. running mean: -19.434110\n",
      "ep 1679: ep_len:600 episode reward: total was -11.340000. running mean: -19.353169\n",
      "ep 1679: ep_len:745 episode reward: total was 1.530000. running mean: -19.144337\n",
      "ep 1679: ep_len:1195 episode reward: total was -33.670000. running mean: -19.289593\n",
      "ep 1679: ep_len:191 episode reward: total was 94.000000. running mean: -18.156698\n",
      "ep 1679: ep_len:79 episode reward: total was 38.000000. running mean: -17.595131\n",
      "ep 1679: ep_len:623 episode reward: total was -14.720000. running mean: -17.566379\n",
      "ep 1679: ep_len:2719 episode reward: total was 13.010000. running mean: -17.260615\n",
      "epsilon:0.009992 episode_count: 25307. steps_count: 27111718.000000\n",
      "ep 1680: ep_len:636 episode reward: total was -61.450000. running mean: -17.702509\n",
      "ep 1680: ep_len:1633 episode reward: total was -66.480000. running mean: -18.190284\n",
      "ep 1680: ep_len:3011 episode reward: total was -104.110000. running mean: -19.049481\n",
      "ep 1680: ep_len:500 episode reward: total was 15.160000. running mean: -18.707387\n",
      "ep 1680: ep_len:46 episode reward: total was 21.500000. running mean: -18.305313\n",
      "ep 1680: ep_len:500 episode reward: total was 9.680000. running mean: -18.025460\n",
      "ep 1680: ep_len:676 episode reward: total was 16.820000. running mean: -17.677005\n",
      "ep 1680: ep_len:567 episode reward: total was -42.920000. running mean: -17.929435\n",
      "ep 1680: ep_len:881 episode reward: total was 47.640000. running mean: -17.273741\n",
      "ep 1680: ep_len:1110 episode reward: total was -20.380000. running mean: -17.304803\n",
      "ep 1680: ep_len:51 episode reward: total was 24.000000. running mean: -16.891755\n",
      "ep 1680: ep_len:85 episode reward: total was 39.500000. running mean: -16.327838\n",
      "ep 1680: ep_len:955 episode reward: total was -111.820000. running mean: -17.282759\n",
      "ep 1680: ep_len:2858 episode reward: total was 9.840000. running mean: -17.011532\n",
      "epsilon:0.009992 episode_count: 25321. steps_count: 27125227.000000\n",
      "ep 1681: ep_len:500 episode reward: total was 14.060000. running mean: -16.700816\n",
      "ep 1681: ep_len:919 episode reward: total was 17.100000. running mean: -16.362808\n",
      "ep 1681: ep_len:3050 episode reward: total was -64.970000. running mean: -16.848880\n",
      "ep 1681: ep_len:1121 episode reward: total was -30.370000. running mean: -16.984091\n",
      "ep 1681: ep_len:677 episode reward: total was -8.040000. running mean: -16.894650\n",
      "ep 1681: ep_len:681 episode reward: total was 25.840000. running mean: -16.467304\n",
      "ep 1681: ep_len:1286 episode reward: total was -63.060000. running mean: -16.933231\n",
      "ep 1681: ep_len:678 episode reward: total was 2.800000. running mean: -16.735899\n",
      "ep 1681: ep_len:689 episode reward: total was 20.330000. running mean: -16.365240\n",
      "ep 1681: ep_len:42 episode reward: total was 18.000000. running mean: -16.021587\n",
      "ep 1681: ep_len:975 episode reward: total was -42.610000. running mean: -16.287471\n",
      "ep 1681: ep_len:2804 episode reward: total was -14.520000. running mean: -16.269797\n",
      "epsilon:0.009992 episode_count: 25333. steps_count: 27138649.000000\n",
      "ep 1682: ep_len:975 episode reward: total was -29.050000. running mean: -16.397599\n",
      "ep 1682: ep_len:704 episode reward: total was -38.720000. running mean: -16.620823\n",
      "ep 1682: ep_len:2961 episode reward: total was -39.330000. running mean: -16.847914\n",
      "ep 1682: ep_len:710 episode reward: total was -21.880000. running mean: -16.898235\n",
      "ep 1682: ep_len:60 episode reward: total was 28.500000. running mean: -16.444253\n",
      "ep 1682: ep_len:92 episode reward: total was 43.000000. running mean: -15.849810\n",
      "ep 1682: ep_len:501 episode reward: total was 29.470000. running mean: -15.396612\n",
      "ep 1682: ep_len:3773 episode reward: total was -238.140000. running mean: -17.624046\n",
      "ep 1682: ep_len:721 episode reward: total was -7.270000. running mean: -17.520506\n",
      "ep 1682: ep_len:691 episode reward: total was 38.620000. running mean: -16.959101\n",
      "ep 1682: ep_len:586 episode reward: total was 50.960000. running mean: -16.279910\n",
      "ep 1682: ep_len:56 episode reward: total was 26.500000. running mean: -15.852110\n",
      "ep 1682: ep_len:752 episode reward: total was -87.960000. running mean: -16.573189\n",
      "ep 1682: ep_len:2865 episode reward: total was -65.560000. running mean: -17.063057\n",
      "epsilon:0.009992 episode_count: 25347. steps_count: 27154096.000000\n",
      "ep 1683: ep_len:1158 episode reward: total was -11.820000. running mean: -17.010627\n",
      "ep 1683: ep_len:660 episode reward: total was 1.840000. running mean: -16.822121\n",
      "ep 1683: ep_len:3046 episode reward: total was -26.870000. running mean: -16.922599\n",
      "ep 1683: ep_len:572 episode reward: total was 2.840000. running mean: -16.724973\n",
      "ep 1683: ep_len:73 episode reward: total was 35.000000. running mean: -16.207724\n",
      "ep 1683: ep_len:67 episode reward: total was 32.000000. running mean: -15.725646\n",
      "ep 1683: ep_len:52 episode reward: total was 24.500000. running mean: -15.323390\n",
      "ep 1683: ep_len:1075 episode reward: total was 0.240000. running mean: -15.167756\n",
      "ep 1683: ep_len:647 episode reward: total was 15.790000. running mean: -14.858179\n",
      "ep 1683: ep_len:1259 episode reward: total was -51.210000. running mean: -15.221697\n",
      "ep 1683: ep_len:796 episode reward: total was 21.340000. running mean: -14.856080\n",
      "ep 1683: ep_len:591 episode reward: total was 0.690000. running mean: -14.700619\n",
      "ep 1683: ep_len:81 episode reward: total was 37.500000. running mean: -14.178613\n",
      "ep 1683: ep_len:193 episode reward: total was 92.000000. running mean: -13.116827\n",
      "ep 1683: ep_len:48 episode reward: total was 22.500000. running mean: -12.760658\n",
      "ep 1683: ep_len:69 episode reward: total was 33.000000. running mean: -12.303052\n",
      "ep 1683: ep_len:613 episode reward: total was -6.160000. running mean: -12.241621\n",
      "ep 1683: ep_len:2790 episode reward: total was -25.150000. running mean: -12.370705\n",
      "ep 1683: ep_len:47 episode reward: total was 20.500000. running mean: -12.041998\n",
      "epsilon:0.009992 episode_count: 25366. steps_count: 27167933.000000\n",
      "ep 1684: ep_len:1434 episode reward: total was -1.230000. running mean: -11.933878\n",
      "ep 1684: ep_len:500 episode reward: total was 9.030000. running mean: -11.724239\n",
      "ep 1684: ep_len:2963 episode reward: total was -30.250000. running mean: -11.909497\n",
      "ep 1684: ep_len:603 episode reward: total was -7.270000. running mean: -11.863102\n",
      "ep 1684: ep_len:133 episode reward: total was 65.000000. running mean: -11.094471\n",
      "ep 1684: ep_len:105 episode reward: total was 51.000000. running mean: -10.473526\n",
      "ep 1684: ep_len:748 episode reward: total was -7.840000. running mean: -10.447191\n",
      "ep 1684: ep_len:3929 episode reward: total was -209.550000. running mean: -12.438219\n",
      "ep 1684: ep_len:1540 episode reward: total was -42.680000. running mean: -12.740637\n",
      "ep 1684: ep_len:817 episode reward: total was 35.820000. running mean: -12.255030\n",
      "ep 1684: ep_len:625 episode reward: total was 16.910000. running mean: -11.963380\n",
      "ep 1684: ep_len:48 episode reward: total was 21.000000. running mean: -11.633746\n",
      "ep 1684: ep_len:105 episode reward: total was 48.000000. running mean: -11.037409\n",
      "ep 1684: ep_len:500 episode reward: total was 36.870000. running mean: -10.558335\n",
      "ep 1684: ep_len:2830 episode reward: total was -36.720000. running mean: -10.819951\n",
      "ep 1684: ep_len:57 episode reward: total was 25.500000. running mean: -10.456752\n",
      "epsilon:0.009992 episode_count: 25382. steps_count: 27184870.000000\n",
      "ep 1685: ep_len:698 episode reward: total was 10.650000. running mean: -10.245684\n",
      "ep 1685: ep_len:760 episode reward: total was -24.590000. running mean: -10.389128\n",
      "ep 1685: ep_len:2977 episode reward: total was -18.940000. running mean: -10.474636\n",
      "ep 1685: ep_len:759 episode reward: total was 15.280000. running mean: -10.217090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1685: ep_len:132 episode reward: total was 61.500000. running mean: -9.499919\n",
      "ep 1685: ep_len:879 episode reward: total was 34.100000. running mean: -9.063920\n",
      "ep 1685: ep_len:347 episode reward: total was 15.590000. running mean: -8.817381\n",
      "ep 1685: ep_len:1278 episode reward: total was -68.680000. running mean: -9.416007\n",
      "ep 1685: ep_len:785 episode reward: total was 10.660000. running mean: -9.215247\n",
      "ep 1685: ep_len:1509 episode reward: total was -4.000000. running mean: -9.163094\n",
      "ep 1685: ep_len:97 episode reward: total was 42.500000. running mean: -8.646463\n",
      "ep 1685: ep_len:1130 episode reward: total was 41.500000. running mean: -8.144999\n",
      "ep 1685: ep_len:2851 episode reward: total was -49.430000. running mean: -8.557849\n",
      "epsilon:0.009992 episode_count: 25395. steps_count: 27199072.000000\n",
      "ep 1686: ep_len:1420 episode reward: total was 22.600000. running mean: -8.246270\n",
      "ep 1686: ep_len:787 episode reward: total was -27.530000. running mean: -8.439108\n",
      "ep 1686: ep_len:64 episode reward: total was 29.000000. running mean: -8.064716\n",
      "ep 1686: ep_len:2975 episode reward: total was -99.060000. running mean: -8.974669\n",
      "ep 1686: ep_len:526 episode reward: total was -28.240000. running mean: -9.167323\n",
      "ep 1686: ep_len:500 episode reward: total was -5.750000. running mean: -9.133149\n",
      "ep 1686: ep_len:606 episode reward: total was 26.160000. running mean: -8.780218\n",
      "ep 1686: ep_len:1166 episode reward: total was -35.520000. running mean: -9.047616\n",
      "ep 1686: ep_len:803 episode reward: total was 19.630000. running mean: -8.760840\n",
      "ep 1686: ep_len:622 episode reward: total was 16.850000. running mean: -8.504731\n",
      "ep 1686: ep_len:35 episode reward: total was 16.000000. running mean: -8.259684\n",
      "ep 1686: ep_len:100 episode reward: total was 47.000000. running mean: -7.707087\n",
      "ep 1686: ep_len:666 episode reward: total was 8.510000. running mean: -7.544916\n",
      "ep 1686: ep_len:2809 episode reward: total was -11.190000. running mean: -7.581367\n",
      "epsilon:0.009992 episode_count: 25409. steps_count: 27212151.000000\n",
      "ep 1687: ep_len:1114 episode reward: total was -8.310000. running mean: -7.588653\n",
      "ep 1687: ep_len:709 episode reward: total was -4.850000. running mean: -7.561267\n",
      "ep 1687: ep_len:3022 episode reward: total was -32.510000. running mean: -7.810754\n",
      "ep 1687: ep_len:1100 episode reward: total was -19.470000. running mean: -7.927347\n",
      "ep 1687: ep_len:67 episode reward: total was 30.500000. running mean: -7.543073\n",
      "ep 1687: ep_len:1161 episode reward: total was 6.800000. running mean: -7.399642\n",
      "ep 1687: ep_len:663 episode reward: total was 27.190000. running mean: -7.053746\n",
      "ep 1687: ep_len:604 episode reward: total was -27.030000. running mean: -7.253508\n",
      "ep 1687: ep_len:684 episode reward: total was -17.690000. running mean: -7.357873\n",
      "ep 1687: ep_len:1497 episode reward: total was -25.970000. running mean: -7.543995\n",
      "ep 1687: ep_len:144 episode reward: total was 69.000000. running mean: -6.778555\n",
      "ep 1687: ep_len:1144 episode reward: total was -27.110000. running mean: -6.981869\n",
      "ep 1687: ep_len:2916 episode reward: total was -28.700000. running mean: -7.199050\n",
      "epsilon:0.009992 episode_count: 25422. steps_count: 27226976.000000\n",
      "ep 1688: ep_len:810 episode reward: total was -49.550000. running mean: -7.622560\n",
      "ep 1688: ep_len:1300 episode reward: total was -51.810000. running mean: -8.064434\n",
      "ep 1688: ep_len:46 episode reward: total was 17.000000. running mean: -7.813790\n",
      "ep 1688: ep_len:2965 episode reward: total was -19.090000. running mean: -7.926552\n",
      "ep 1688: ep_len:500 episode reward: total was 28.420000. running mean: -7.563087\n",
      "ep 1688: ep_len:1026 episode reward: total was -30.490000. running mean: -7.792356\n",
      "ep 1688: ep_len:3902 episode reward: total was -38.150000. running mean: -8.095932\n",
      "ep 1688: ep_len:600 episode reward: total was 20.310000. running mean: -7.811873\n",
      "ep 1688: ep_len:791 episode reward: total was 18.100000. running mean: -7.552754\n",
      "ep 1688: ep_len:633 episode reward: total was -38.240000. running mean: -7.859627\n",
      "ep 1688: ep_len:87 episode reward: total was 42.000000. running mean: -7.361030\n",
      "ep 1688: ep_len:500 episode reward: total was 38.000000. running mean: -6.907420\n",
      "ep 1688: ep_len:2875 episode reward: total was -32.960000. running mean: -7.167946\n",
      "epsilon:0.009992 episode_count: 25435. steps_count: 27243011.000000\n",
      "ep 1689: ep_len:647 episode reward: total was -19.440000. running mean: -7.290666\n",
      "ep 1689: ep_len:500 episode reward: total was 27.530000. running mean: -6.942460\n",
      "ep 1689: ep_len:99 episode reward: total was 45.000000. running mean: -6.423035\n",
      "ep 1689: ep_len:511 episode reward: total was -37.480000. running mean: -6.733605\n",
      "ep 1689: ep_len:149 episode reward: total was 68.500000. running mean: -5.981269\n",
      "ep 1689: ep_len:46 episode reward: total was 21.500000. running mean: -5.706456\n",
      "ep 1689: ep_len:674 episode reward: total was 1.970000. running mean: -5.629691\n",
      "ep 1689: ep_len:674 episode reward: total was 30.270000. running mean: -5.270695\n",
      "ep 1689: ep_len:623 episode reward: total was -38.010000. running mean: -5.598088\n",
      "ep 1689: ep_len:740 episode reward: total was 47.290000. running mean: -5.069207\n",
      "ep 1689: ep_len:604 episode reward: total was 36.660000. running mean: -4.651915\n",
      "ep 1689: ep_len:82 episode reward: total was 38.000000. running mean: -4.225396\n",
      "ep 1689: ep_len:47 episode reward: total was 22.000000. running mean: -3.963142\n",
      "ep 1689: ep_len:106 episode reward: total was 51.500000. running mean: -3.408510\n",
      "ep 1689: ep_len:1204 episode reward: total was -24.270000. running mean: -3.617125\n",
      "ep 1689: ep_len:2771 episode reward: total was -37.040000. running mean: -3.951354\n",
      "epsilon:0.009992 episode_count: 25451. steps_count: 27252488.000000\n",
      "ep 1690: ep_len:939 episode reward: total was -78.010000. running mean: -4.691940\n",
      "ep 1690: ep_len:1617 episode reward: total was -33.480000. running mean: -4.979821\n",
      "ep 1690: ep_len:49 episode reward: total was 21.500000. running mean: -4.715023\n",
      "ep 1690: ep_len:2942 episode reward: total was 20.510000. running mean: -4.462772\n",
      "ep 1690: ep_len:842 episode reward: total was 7.160000. running mean: -4.346545\n",
      "ep 1690: ep_len:101 episode reward: total was 44.500000. running mean: -3.858079\n",
      "ep 1690: ep_len:76 episode reward: total was 36.500000. running mean: -3.454498\n",
      "ep 1690: ep_len:862 episode reward: total was 15.370000. running mean: -3.266253\n",
      "ep 1690: ep_len:315 episode reward: total was 11.750000. running mean: -3.116091\n",
      "ep 1690: ep_len:1224 episode reward: total was -73.900000. running mean: -3.823930\n",
      "ep 1690: ep_len:793 episode reward: total was -1.660000. running mean: -3.802291\n",
      "ep 1690: ep_len:572 episode reward: total was 34.440000. running mean: -3.419868\n",
      "ep 1690: ep_len:68 episode reward: total was 31.000000. running mean: -3.075669\n",
      "ep 1690: ep_len:1114 episode reward: total was -15.290000. running mean: -3.197812\n",
      "ep 1690: ep_len:2855 episode reward: total was 16.110000. running mean: -3.004734\n",
      "epsilon:0.009992 episode_count: 25466. steps_count: 27266857.000000\n",
      "ep 1691: ep_len:927 episode reward: total was -54.220000. running mean: -3.516887\n",
      "ep 1691: ep_len:500 episode reward: total was 15.980000. running mean: -3.321918\n",
      "ep 1691: ep_len:2953 episode reward: total was 4.730000. running mean: -3.241399\n",
      "ep 1691: ep_len:530 episode reward: total was -18.100000. running mean: -3.389985\n",
      "ep 1691: ep_len:43 episode reward: total was 20.000000. running mean: -3.156085\n",
      "ep 1691: ep_len:41 episode reward: total was 19.000000. running mean: -2.934524\n",
      "ep 1691: ep_len:500 episode reward: total was 26.800000. running mean: -2.637179\n",
      "ep 1691: ep_len:358 episode reward: total was 22.800000. running mean: -2.382807\n",
      "ep 1691: ep_len:627 episode reward: total was -25.210000. running mean: -2.611079\n",
      "ep 1691: ep_len:847 episode reward: total was 19.800000. running mean: -2.386968\n",
      "ep 1691: ep_len:1126 episode reward: total was 33.870000. running mean: -2.024399\n",
      "ep 1691: ep_len:72 episode reward: total was 33.000000. running mean: -1.674155\n",
      "ep 1691: ep_len:500 episode reward: total was 26.150000. running mean: -1.395913\n",
      "ep 1691: ep_len:2816 episode reward: total was -13.850000. running mean: -1.520454\n",
      "epsilon:0.009992 episode_count: 25480. steps_count: 27278697.000000\n",
      "ep 1692: ep_len:525 episode reward: total was -13.840000. running mean: -1.643649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1692: ep_len:1255 episode reward: total was -42.160000. running mean: -2.048813\n",
      "ep 1692: ep_len:31 episode reward: total was 14.000000. running mean: -1.888325\n",
      "ep 1692: ep_len:2959 episode reward: total was -73.600000. running mean: -2.605442\n",
      "ep 1692: ep_len:870 episode reward: total was 86.640000. running mean: -1.712987\n",
      "ep 1692: ep_len:90 episode reward: total was 43.500000. running mean: -1.260857\n",
      "ep 1692: ep_len:937 episode reward: total was 53.540000. running mean: -0.712849\n",
      "ep 1692: ep_len:3677 episode reward: total was -19.580000. running mean: -0.901520\n",
      "ep 1692: ep_len:2576 episode reward: total was -189.740000. running mean: -2.789905\n",
      "ep 1692: ep_len:670 episode reward: total was 40.650000. running mean: -2.355506\n",
      "ep 1692: ep_len:620 episode reward: total was 14.370000. running mean: -2.188251\n",
      "ep 1692: ep_len:74 episode reward: total was 34.000000. running mean: -1.826368\n",
      "ep 1692: ep_len:36 episode reward: total was 16.500000. running mean: -1.643105\n",
      "ep 1692: ep_len:109 episode reward: total was 51.500000. running mean: -1.111674\n",
      "ep 1692: ep_len:742 episode reward: total was -42.240000. running mean: -1.522957\n",
      "ep 1692: ep_len:2772 episode reward: total was -18.360000. running mean: -1.691327\n",
      "ep 1692: ep_len:31 episode reward: total was 14.000000. running mean: -1.534414\n",
      "epsilon:0.009992 episode_count: 25497. steps_count: 27296671.000000\n",
      "ep 1693: ep_len:628 episode reward: total was 19.480000. running mean: -1.324270\n",
      "ep 1693: ep_len:640 episode reward: total was 7.210000. running mean: -1.238927\n",
      "ep 1693: ep_len:3017 episode reward: total was -33.120000. running mean: -1.557738\n",
      "ep 1693: ep_len:501 episode reward: total was -16.370000. running mean: -1.705861\n",
      "ep 1693: ep_len:167 episode reward: total was 80.500000. running mean: -0.883802\n",
      "ep 1693: ep_len:110 episode reward: total was 52.000000. running mean: -0.354964\n",
      "ep 1693: ep_len:1356 episode reward: total was -178.510000. running mean: -2.136514\n",
      "ep 1693: ep_len:640 episode reward: total was 24.390000. running mean: -1.871249\n",
      "ep 1693: ep_len:1408 episode reward: total was -81.610000. running mean: -2.668637\n",
      "ep 1693: ep_len:739 episode reward: total was -25.980000. running mean: -2.901750\n",
      "ep 1693: ep_len:781 episode reward: total was -4.480000. running mean: -2.917533\n",
      "ep 1693: ep_len:52 episode reward: total was 24.500000. running mean: -2.643358\n",
      "ep 1693: ep_len:672 episode reward: total was 8.780000. running mean: -2.529124\n",
      "ep 1693: ep_len:2856 episode reward: total was -45.280000. running mean: -2.956633\n",
      "epsilon:0.009992 episode_count: 25511. steps_count: 27310238.000000\n",
      "ep 1694: ep_len:1131 episode reward: total was -19.160000. running mean: -3.118666\n",
      "ep 1694: ep_len:1337 episode reward: total was -68.610000. running mean: -3.773580\n",
      "ep 1694: ep_len:2858 episode reward: total was -47.890000. running mean: -4.214744\n",
      "ep 1694: ep_len:685 episode reward: total was -3.830000. running mean: -4.210896\n",
      "ep 1694: ep_len:125 episode reward: total was 61.000000. running mean: -3.558788\n",
      "ep 1694: ep_len:115 episode reward: total was 56.000000. running mean: -2.963200\n",
      "ep 1694: ep_len:2845 episode reward: total was -2138.020000. running mean: -24.313768\n",
      "ep 1694: ep_len:3953 episode reward: total was -475.830000. running mean: -28.828930\n",
      "ep 1694: ep_len:542 episode reward: total was 6.420000. running mean: -28.476441\n",
      "ep 1694: ep_len:716 episode reward: total was 3.230000. running mean: -28.159376\n",
      "ep 1694: ep_len:1110 episode reward: total was -8.260000. running mean: -27.960382\n",
      "ep 1694: ep_len:146 episode reward: total was 70.000000. running mean: -26.980779\n",
      "ep 1694: ep_len:675 episode reward: total was 31.930000. running mean: -26.391671\n",
      "ep 1694: ep_len:2865 episode reward: total was 0.020000. running mean: -26.127554\n",
      "epsilon:0.009992 episode_count: 25525. steps_count: 27329341.000000\n",
      "ep 1695: ep_len:1100 episode reward: total was 4.840000. running mean: -25.817879\n",
      "ep 1695: ep_len:1623 episode reward: total was -76.370000. running mean: -26.323400\n",
      "ep 1695: ep_len:2988 episode reward: total was -30.500000. running mean: -26.365166\n",
      "ep 1695: ep_len:1260 episode reward: total was -42.600000. running mean: -26.527514\n",
      "ep 1695: ep_len:50 episode reward: total was 22.000000. running mean: -26.042239\n",
      "ep 1695: ep_len:1024 episode reward: total was 2.890000. running mean: -25.752917\n",
      "ep 1695: ep_len:346 episode reward: total was 19.160000. running mean: -25.303787\n",
      "ep 1695: ep_len:586 episode reward: total was 17.140000. running mean: -24.879350\n",
      "ep 1695: ep_len:821 episode reward: total was 38.100000. running mean: -24.249556\n",
      "ep 1695: ep_len:790 episode reward: total was 0.660000. running mean: -24.000461\n",
      "ep 1695: ep_len:68 episode reward: total was 32.500000. running mean: -23.435456\n",
      "ep 1695: ep_len:24 episode reward: total was 10.500000. running mean: -23.096101\n",
      "ep 1695: ep_len:775 episode reward: total was -21.940000. running mean: -23.084540\n",
      "ep 1695: ep_len:2685 episode reward: total was -37.870000. running mean: -23.232395\n",
      "ep 1695: ep_len:58 episode reward: total was 27.500000. running mean: -22.725071\n",
      "epsilon:0.009992 episode_count: 25540. steps_count: 27343539.000000\n",
      "ep 1696: ep_len:656 episode reward: total was -10.590000. running mean: -22.603720\n",
      "ep 1696: ep_len:642 episode reward: total was -32.820000. running mean: -22.705883\n",
      "ep 1696: ep_len:2950 episode reward: total was -59.640000. running mean: -23.075224\n",
      "ep 1696: ep_len:500 episode reward: total was -14.450000. running mean: -22.988972\n",
      "ep 1696: ep_len:68 episode reward: total was 32.500000. running mean: -22.434082\n",
      "ep 1696: ep_len:500 episode reward: total was 47.860000. running mean: -21.731141\n",
      "ep 1696: ep_len:668 episode reward: total was 21.180000. running mean: -21.302030\n",
      "ep 1696: ep_len:1168 episode reward: total was -43.730000. running mean: -21.526310\n",
      "ep 1696: ep_len:694 episode reward: total was 26.660000. running mean: -21.044447\n",
      "ep 1696: ep_len:648 episode reward: total was -12.740000. running mean: -20.961402\n",
      "ep 1696: ep_len:97 episode reward: total was 44.000000. running mean: -20.311788\n",
      "ep 1696: ep_len:1099 episode reward: total was -3.320000. running mean: -20.141870\n",
      "ep 1696: ep_len:2880 episode reward: total was -15.660000. running mean: -20.097052\n",
      "ep 1696: ep_len:37 episode reward: total was 15.500000. running mean: -19.741081\n",
      "epsilon:0.009992 episode_count: 25554. steps_count: 27356146.000000\n",
      "ep 1697: ep_len:1088 episode reward: total was -10.990000. running mean: -19.653570\n",
      "ep 1697: ep_len:1644 episode reward: total was -73.010000. running mean: -20.187135\n",
      "ep 1697: ep_len:67 episode reward: total was 32.000000. running mean: -19.665263\n",
      "ep 1697: ep_len:2924 episode reward: total was -51.490000. running mean: -19.983511\n",
      "ep 1697: ep_len:1199 episode reward: total was -26.560000. running mean: -20.049275\n",
      "ep 1697: ep_len:57 episode reward: total was 25.500000. running mean: -19.593783\n",
      "ep 1697: ep_len:713 episode reward: total was -36.990000. running mean: -19.767745\n",
      "ep 1697: ep_len:615 episode reward: total was 8.890000. running mean: -19.481167\n",
      "ep 1697: ep_len:827 episode reward: total was -19.110000. running mean: -19.477456\n",
      "ep 1697: ep_len:855 episode reward: total was 44.470000. running mean: -18.837981\n",
      "ep 1697: ep_len:1087 episode reward: total was 45.450000. running mean: -18.195101\n",
      "ep 1697: ep_len:80 episode reward: total was 38.500000. running mean: -17.628150\n",
      "ep 1697: ep_len:221 episode reward: total was 103.000000. running mean: -16.421869\n",
      "ep 1697: ep_len:38 episode reward: total was 17.500000. running mean: -16.082650\n",
      "ep 1697: ep_len:1048 episode reward: total was -48.800000. running mean: -16.409824\n",
      "ep 1697: ep_len:45 episode reward: total was 18.000000. running mean: -16.065725\n",
      "ep 1697: ep_len:40 episode reward: total was 18.500000. running mean: -15.720068\n",
      "epsilon:0.009992 episode_count: 25571. steps_count: 27368694.000000\n",
      "ep 1698: ep_len:1479 episode reward: total was -0.410000. running mean: -15.566968\n",
      "ep 1698: ep_len:1253 episode reward: total was -50.260000. running mean: -15.913898\n",
      "ep 1698: ep_len:72 episode reward: total was 33.000000. running mean: -15.424759\n",
      "ep 1698: ep_len:2990 episode reward: total was -70.230000. running mean: -15.972811\n",
      "ep 1698: ep_len:606 episode reward: total was -10.270000. running mean: -15.915783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1698: ep_len:117 episode reward: total was 55.500000. running mean: -15.201625\n",
      "ep 1698: ep_len:876 episode reward: total was 40.430000. running mean: -14.645309\n",
      "ep 1698: ep_len:673 episode reward: total was 30.320000. running mean: -14.195656\n",
      "ep 1698: ep_len:641 episode reward: total was -4.870000. running mean: -14.102399\n",
      "ep 1698: ep_len:671 episode reward: total was 32.300000. running mean: -13.638375\n",
      "ep 1698: ep_len:519 episode reward: total was -7.590000. running mean: -13.577892\n",
      "ep 1698: ep_len:51 episode reward: total was 21.000000. running mean: -13.232113\n",
      "ep 1698: ep_len:57 episode reward: total was 27.000000. running mean: -12.829792\n",
      "ep 1698: ep_len:697 episode reward: total was -10.830000. running mean: -12.809794\n",
      "ep 1698: ep_len:2901 episode reward: total was -0.660000. running mean: -12.688296\n",
      "epsilon:0.009992 episode_count: 25586. steps_count: 27382297.000000\n",
      "ep 1699: ep_len:692 episode reward: total was -52.840000. running mean: -13.089813\n",
      "ep 1699: ep_len:972 episode reward: total was 18.370000. running mean: -12.775215\n",
      "ep 1699: ep_len:2961 episode reward: total was -48.270000. running mean: -13.130163\n",
      "ep 1699: ep_len:1666 episode reward: total was -67.280000. running mean: -13.671661\n",
      "ep 1699: ep_len:856 episode reward: total was 31.440000. running mean: -13.220544\n",
      "ep 1699: ep_len:299 episode reward: total was 5.800000. running mean: -13.030339\n",
      "ep 1699: ep_len:965 episode reward: total was -33.950000. running mean: -13.239535\n",
      "ep 1699: ep_len:584 episode reward: total was -16.520000. running mean: -13.272340\n",
      "ep 1699: ep_len:1044 episode reward: total was -10.820000. running mean: -13.247817\n",
      "ep 1699: ep_len:602 episode reward: total was -12.330000. running mean: -13.238639\n",
      "ep 1699: ep_len:2848 episode reward: total was -7.010000. running mean: -13.176352\n",
      "epsilon:0.009992 episode_count: 25597. steps_count: 27395786.000000\n",
      "ep 1700: ep_len:657 episode reward: total was -26.350000. running mean: -13.308089\n",
      "ep 1700: ep_len:1272 episode reward: total was -40.430000. running mean: -13.579308\n",
      "ep 1700: ep_len:3057 episode reward: total was -18.500000. running mean: -13.628515\n",
      "ep 1700: ep_len:1057 episode reward: total was -7.780000. running mean: -13.570030\n",
      "ep 1700: ep_len:37 episode reward: total was 17.000000. running mean: -13.264329\n",
      "ep 1700: ep_len:143 episode reward: total was 67.000000. running mean: -12.461686\n",
      "ep 1700: ep_len:86 episode reward: total was 41.500000. running mean: -11.922069\n",
      "ep 1700: ep_len:761 episode reward: total was -11.750000. running mean: -11.920348\n",
      "ep 1700: ep_len:3576 episode reward: total was -68.610000. running mean: -12.487245\n",
      "ep 1700: ep_len:574 episode reward: total was 14.150000. running mean: -12.220872\n",
      "ep 1700: ep_len:780 episode reward: total was 37.160000. running mean: -11.727064\n",
      "ep 1700: ep_len:781 episode reward: total was -5.490000. running mean: -11.664693\n",
      "ep 1700: ep_len:90 episode reward: total was 40.500000. running mean: -11.143046\n",
      "ep 1700: ep_len:117 episode reward: total was 57.000000. running mean: -10.461616\n",
      "ep 1700: ep_len:81 episode reward: total was 39.000000. running mean: -9.967000\n",
      "ep 1700: ep_len:1161 episode reward: total was -20.570000. running mean: -10.073030\n",
      "ep 1700: ep_len:2851 episode reward: total was -7.930000. running mean: -10.051599\n",
      "ep 1700: ep_len:42 episode reward: total was 19.500000. running mean: -9.756083\n",
      "epsilon:0.009992 episode_count: 25615. steps_count: 27412909.000000\n",
      "ep 1701: ep_len:871 episode reward: total was -10.010000. running mean: -9.758622\n",
      "ep 1701: ep_len:500 episode reward: total was 1.960000. running mean: -9.641436\n",
      "ep 1701: ep_len:3029 episode reward: total was -68.890000. running mean: -10.233922\n",
      "ep 1701: ep_len:639 episode reward: total was -8.930000. running mean: -10.220883\n",
      "ep 1701: ep_len:54 episode reward: total was 25.500000. running mean: -9.863674\n",
      "ep 1701: ep_len:89 episode reward: total was 41.500000. running mean: -9.350037\n",
      "ep 1701: ep_len:672 episode reward: total was -0.530000. running mean: -9.261837\n",
      "ep 1701: ep_len:4023 episode reward: total was -108.390000. running mean: -10.253118\n",
      "ep 1701: ep_len:672 episode reward: total was -32.190000. running mean: -10.472487\n",
      "ep 1701: ep_len:742 episode reward: total was 14.100000. running mean: -10.226762\n",
      "ep 1701: ep_len:747 episode reward: total was -8.860000. running mean: -10.213095\n",
      "ep 1701: ep_len:64 episode reward: total was 29.000000. running mean: -9.820964\n",
      "ep 1701: ep_len:72 episode reward: total was 33.000000. running mean: -9.392754\n",
      "ep 1701: ep_len:1171 episode reward: total was -14.960000. running mean: -9.448427\n",
      "ep 1701: ep_len:2833 episode reward: total was -19.580000. running mean: -9.549742\n",
      "ep 1701: ep_len:53 episode reward: total was 22.000000. running mean: -9.234245\n",
      "epsilon:0.009992 episode_count: 25631. steps_count: 27429140.000000\n",
      "ep 1702: ep_len:818 episode reward: total was -10.900000. running mean: -9.250902\n",
      "ep 1702: ep_len:772 episode reward: total was -7.880000. running mean: -9.237193\n",
      "ep 1702: ep_len:2881 episode reward: total was -21.860000. running mean: -9.363421\n",
      "ep 1702: ep_len:1131 episode reward: total was -12.090000. running mean: -9.390687\n",
      "ep 1702: ep_len:35 episode reward: total was 16.000000. running mean: -9.136780\n",
      "ep 1702: ep_len:71 episode reward: total was 34.000000. running mean: -8.705413\n",
      "ep 1702: ep_len:500 episode reward: total was 52.210000. running mean: -8.096258\n",
      "ep 1702: ep_len:631 episode reward: total was 26.870000. running mean: -7.746596\n",
      "ep 1702: ep_len:1141 episode reward: total was -25.000000. running mean: -7.919130\n",
      "ep 1702: ep_len:814 episode reward: total was 7.130000. running mean: -7.768639\n",
      "ep 1702: ep_len:634 episode reward: total was 1.570000. running mean: -7.675252\n",
      "ep 1702: ep_len:57 episode reward: total was 27.000000. running mean: -7.328500\n",
      "ep 1702: ep_len:65 episode reward: total was 31.000000. running mean: -6.945215\n",
      "ep 1702: ep_len:124 episode reward: total was 59.000000. running mean: -6.285763\n",
      "ep 1702: ep_len:1459 episode reward: total was 6.460000. running mean: -6.158305\n",
      "ep 1702: ep_len:2811 episode reward: total was -6.490000. running mean: -6.161622\n",
      "epsilon:0.009992 episode_count: 25647. steps_count: 27443084.000000\n",
      "ep 1703: ep_len:658 episode reward: total was -9.930000. running mean: -6.199306\n",
      "ep 1703: ep_len:652 episode reward: total was -25.420000. running mean: -6.391513\n",
      "ep 1703: ep_len:2936 episode reward: total was -41.510000. running mean: -6.742697\n",
      "ep 1703: ep_len:675 episode reward: total was 31.360000. running mean: -6.361670\n",
      "ep 1703: ep_len:86 episode reward: total was 38.500000. running mean: -5.913054\n",
      "ep 1703: ep_len:827 episode reward: total was 18.170000. running mean: -5.672223\n",
      "ep 1703: ep_len:4106 episode reward: total was -60.390000. running mean: -6.219401\n",
      "ep 1703: ep_len:868 episode reward: total was -26.840000. running mean: -6.425607\n",
      "ep 1703: ep_len:834 episode reward: total was 25.580000. running mean: -6.105551\n",
      "ep 1703: ep_len:650 episode reward: total was 7.800000. running mean: -5.966495\n",
      "ep 1703: ep_len:752 episode reward: total was -94.660000. running mean: -6.853430\n",
      "ep 1703: ep_len:2875 episode reward: total was -42.330000. running mean: -7.208196\n",
      "epsilon:0.009992 episode_count: 25659. steps_count: 27459003.000000\n",
      "ep 1704: ep_len:1112 episode reward: total was -7.230000. running mean: -7.208414\n",
      "ep 1704: ep_len:501 episode reward: total was 15.410000. running mean: -6.982230\n",
      "ep 1704: ep_len:41 episode reward: total was 17.500000. running mean: -6.737408\n",
      "ep 1704: ep_len:2984 episode reward: total was -7.560000. running mean: -6.745634\n",
      "ep 1704: ep_len:607 episode reward: total was -7.380000. running mean: -6.751977\n",
      "ep 1704: ep_len:39 episode reward: total was 16.500000. running mean: -6.519458\n",
      "ep 1704: ep_len:107 episode reward: total was 52.000000. running mean: -5.934263\n",
      "ep 1704: ep_len:889 episode reward: total was 28.920000. running mean: -5.585720\n",
      "ep 1704: ep_len:297 episode reward: total was 13.190000. running mean: -5.397963\n",
      "ep 1704: ep_len:833 episode reward: total was -39.140000. running mean: -5.735384\n",
      "ep 1704: ep_len:807 episode reward: total was 63.150000. running mean: -5.046530\n",
      "ep 1704: ep_len:1089 episode reward: total was -7.460000. running mean: -5.070664\n",
      "ep 1704: ep_len:116 episode reward: total was 55.000000. running mean: -4.469958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1704: ep_len:944 episode reward: total was -29.390000. running mean: -4.719158\n",
      "ep 1704: ep_len:2753 episode reward: total was 15.400000. running mean: -4.517967\n",
      "ep 1704: ep_len:59 episode reward: total was 26.500000. running mean: -4.207787\n",
      "epsilon:0.009992 episode_count: 25675. steps_count: 27472181.000000\n",
      "ep 1705: ep_len:1165 episode reward: total was 10.450000. running mean: -4.061209\n",
      "ep 1705: ep_len:500 episode reward: total was 13.930000. running mean: -3.881297\n",
      "ep 1705: ep_len:100 episode reward: total was 45.500000. running mean: -3.387484\n",
      "ep 1705: ep_len:500 episode reward: total was -10.620000. running mean: -3.459809\n",
      "ep 1705: ep_len:59 episode reward: total was 28.000000. running mean: -3.145211\n",
      "ep 1705: ep_len:1025 episode reward: total was -60.620000. running mean: -3.719959\n",
      "ep 1705: ep_len:4105 episode reward: total was -25.500000. running mean: -3.937759\n",
      "ep 1705: ep_len:1194 episode reward: total was -16.970000. running mean: -4.068082\n",
      "ep 1705: ep_len:760 episode reward: total was 30.380000. running mean: -3.723601\n",
      "ep 1705: ep_len:1117 episode reward: total was -30.410000. running mean: -3.990465\n",
      "ep 1705: ep_len:63 episode reward: total was 28.500000. running mean: -3.665560\n",
      "ep 1705: ep_len:194 episode reward: total was 95.500000. running mean: -2.673905\n",
      "ep 1705: ep_len:31 episode reward: total was 14.000000. running mean: -2.507166\n",
      "ep 1705: ep_len:96 episode reward: total was 46.500000. running mean: -2.017094\n",
      "ep 1705: ep_len:740 episode reward: total was -36.200000. running mean: -2.358923\n",
      "ep 1705: ep_len:2860 episode reward: total was -13.990000. running mean: -2.475234\n",
      "ep 1705: ep_len:47 episode reward: total was 19.000000. running mean: -2.260481\n",
      "epsilon:0.009992 episode_count: 25692. steps_count: 27486737.000000\n",
      "ep 1706: ep_len:1181 episode reward: total was 6.970000. running mean: -2.168177\n",
      "ep 1706: ep_len:693 episode reward: total was -37.540000. running mean: -2.521895\n",
      "ep 1706: ep_len:61 episode reward: total was 29.000000. running mean: -2.206676\n",
      "ep 1706: ep_len:2994 episode reward: total was -21.280000. running mean: -2.397409\n",
      "ep 1706: ep_len:601 episode reward: total was 1.680000. running mean: -2.356635\n",
      "ep 1706: ep_len:57 episode reward: total was 22.500000. running mean: -2.108069\n",
      "ep 1706: ep_len:101 episode reward: total was 49.000000. running mean: -1.596988\n",
      "ep 1706: ep_len:64 episode reward: total was 30.500000. running mean: -1.276018\n",
      "ep 1706: ep_len:1023 episode reward: total was -23.700000. running mean: -1.500258\n",
      "ep 1706: ep_len:3964 episode reward: total was -43.460000. running mean: -1.919855\n",
      "ep 1706: ep_len:654 episode reward: total was 3.340000. running mean: -1.867257\n",
      "ep 1706: ep_len:769 episode reward: total was 12.610000. running mean: -1.722484\n",
      "ep 1706: ep_len:999 episode reward: total was -5.080000. running mean: -1.756059\n",
      "ep 1706: ep_len:36 episode reward: total was 15.000000. running mean: -1.588499\n",
      "ep 1706: ep_len:104 episode reward: total was 49.000000. running mean: -1.082614\n",
      "ep 1706: ep_len:681 episode reward: total was -0.390000. running mean: -1.075688\n",
      "ep 1706: ep_len:2822 episode reward: total was -0.630000. running mean: -1.071231\n",
      "ep 1706: ep_len:67 episode reward: total was 32.000000. running mean: -0.740519\n",
      "epsilon:0.009992 episode_count: 25710. steps_count: 27503608.000000\n",
      "ep 1707: ep_len:1089 episode reward: total was -9.480000. running mean: -0.827913\n",
      "ep 1707: ep_len:703 episode reward: total was -24.500000. running mean: -1.064634\n",
      "ep 1707: ep_len:2903 episode reward: total was -55.670000. running mean: -1.610688\n",
      "ep 1707: ep_len:827 episode reward: total was -11.620000. running mean: -1.710781\n",
      "ep 1707: ep_len:67 episode reward: total was 30.500000. running mean: -1.388673\n",
      "ep 1707: ep_len:61 episode reward: total was 26.000000. running mean: -1.114786\n",
      "ep 1707: ep_len:987 episode reward: total was -35.750000. running mean: -1.461139\n",
      "ep 1707: ep_len:3864 episode reward: total was -49.020000. running mean: -1.936727\n",
      "ep 1707: ep_len:623 episode reward: total was 31.680000. running mean: -1.600560\n",
      "ep 1707: ep_len:814 episode reward: total was 22.060000. running mean: -1.363954\n",
      "ep 1707: ep_len:500 episode reward: total was 12.800000. running mean: -1.222315\n",
      "ep 1707: ep_len:176 episode reward: total was 85.000000. running mean: -0.360092\n",
      "ep 1707: ep_len:547 episode reward: total was 25.640000. running mean: -0.100091\n",
      "ep 1707: ep_len:2802 episode reward: total was -6.920000. running mean: -0.168290\n",
      "epsilon:0.009992 episode_count: 25724. steps_count: 27519571.000000\n",
      "ep 1708: ep_len:1105 episode reward: total was -23.890000. running mean: -0.405507\n",
      "ep 1708: ep_len:210 episode reward: total was 2.890000. running mean: -0.372552\n",
      "ep 1708: ep_len:2971 episode reward: total was -65.440000. running mean: -1.023226\n",
      "ep 1708: ep_len:645 episode reward: total was -8.810000. running mean: -1.101094\n",
      "ep 1708: ep_len:38 episode reward: total was 17.500000. running mean: -0.915083\n",
      "ep 1708: ep_len:49 episode reward: total was 23.000000. running mean: -0.675932\n",
      "ep 1708: ep_len:1380 episode reward: total was 10.780000. running mean: -0.561373\n",
      "ep 1708: ep_len:3726 episode reward: total was -150.140000. running mean: -2.057159\n",
      "ep 1708: ep_len:1174 episode reward: total was -22.250000. running mean: -2.259088\n",
      "ep 1708: ep_len:851 episode reward: total was 53.180000. running mean: -1.704697\n",
      "ep 1708: ep_len:623 episode reward: total was -6.030000. running mean: -1.747950\n",
      "ep 1708: ep_len:57 episode reward: total was 25.500000. running mean: -1.475470\n",
      "ep 1708: ep_len:673 episode reward: total was -8.590000. running mean: -1.546616\n",
      "ep 1708: ep_len:2841 episode reward: total was -34.240000. running mean: -1.873549\n",
      "epsilon:0.009992 episode_count: 25738. steps_count: 27535914.000000\n",
      "ep 1709: ep_len:603 episode reward: total was 4.320000. running mean: -1.811614\n",
      "ep 1709: ep_len:1687 episode reward: total was -143.910000. running mean: -3.232598\n",
      "ep 1709: ep_len:2885 episode reward: total was -70.790000. running mean: -3.908172\n",
      "ep 1709: ep_len:548 episode reward: total was -3.780000. running mean: -3.906890\n",
      "ep 1709: ep_len:43 episode reward: total was 20.000000. running mean: -3.667821\n",
      "ep 1709: ep_len:77 episode reward: total was 37.000000. running mean: -3.261143\n",
      "ep 1709: ep_len:591 episode reward: total was -1.330000. running mean: -3.241832\n",
      "ep 1709: ep_len:3812 episode reward: total was -114.150000. running mean: -4.350913\n",
      "ep 1709: ep_len:604 episode reward: total was -20.390000. running mean: -4.511304\n",
      "ep 1709: ep_len:7226 episode reward: total was 47.880000. running mean: -3.987391\n",
      "ep 1709: ep_len:719 episode reward: total was -12.170000. running mean: -4.069217\n",
      "ep 1709: ep_len:61 episode reward: total was 29.000000. running mean: -3.738525\n",
      "ep 1709: ep_len:50 episode reward: total was 23.500000. running mean: -3.466140\n",
      "ep 1709: ep_len:74 episode reward: total was 34.000000. running mean: -3.091478\n",
      "ep 1709: ep_len:1131 episode reward: total was -19.220000. running mean: -3.252764\n",
      "ep 1709: ep_len:2819 episode reward: total was -8.520000. running mean: -3.305436\n",
      "ep 1709: ep_len:47 episode reward: total was 22.000000. running mean: -3.052382\n",
      "epsilon:0.009992 episode_count: 25755. steps_count: 27558891.000000\n",
      "ep 1710: ep_len:869 episode reward: total was -91.070000. running mean: -3.932558\n",
      "ep 1710: ep_len:198 episode reward: total was 3.350000. running mean: -3.859732\n",
      "ep 1710: ep_len:2967 episode reward: total was -88.600000. running mean: -4.707135\n",
      "ep 1710: ep_len:630 episode reward: total was 8.220000. running mean: -4.577864\n",
      "ep 1710: ep_len:41 episode reward: total was 19.000000. running mean: -4.342085\n",
      "ep 1710: ep_len:88 episode reward: total was 41.000000. running mean: -3.888664\n",
      "ep 1710: ep_len:75 episode reward: total was 34.500000. running mean: -3.504777\n",
      "ep 1710: ep_len:52 episode reward: total was 24.500000. running mean: -3.224730\n",
      "ep 1710: ep_len:898 episode reward: total was 72.190000. running mean: -2.470582\n",
      "ep 1710: ep_len:591 episode reward: total was 23.590000. running mean: -2.209977\n",
      "ep 1710: ep_len:653 episode reward: total was -32.910000. running mean: -2.516977\n",
      "ep 1710: ep_len:752 episode reward: total was 46.740000. running mean: -2.024407\n",
      "ep 1710: ep_len:845 episode reward: total was 27.140000. running mean: -1.732763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1710: ep_len:45 episode reward: total was 18.000000. running mean: -1.535435\n",
      "ep 1710: ep_len:111 episode reward: total was 51.000000. running mean: -1.010081\n",
      "ep 1710: ep_len:1439 episode reward: total was -12.500000. running mean: -1.124980\n",
      "ep 1710: ep_len:2799 episode reward: total was 3.800000. running mean: -1.075730\n",
      "epsilon:0.009992 episode_count: 25772. steps_count: 27571944.000000\n",
      "ep 1711: ep_len:644 episode reward: total was -22.010000. running mean: -1.285073\n",
      "ep 1711: ep_len:687 episode reward: total was -17.020000. running mean: -1.442422\n",
      "ep 1711: ep_len:2955 episode reward: total was -29.580000. running mean: -1.723798\n",
      "ep 1711: ep_len:623 episode reward: total was -1.010000. running mean: -1.716660\n",
      "ep 1711: ep_len:139 episode reward: total was 66.500000. running mean: -1.034493\n",
      "ep 1711: ep_len:73 episode reward: total was 33.500000. running mean: -0.689149\n",
      "ep 1711: ep_len:50 episode reward: total was 23.500000. running mean: -0.447257\n",
      "ep 1711: ep_len:500 episode reward: total was 9.430000. running mean: -0.348484\n",
      "ep 1711: ep_len:664 episode reward: total was 23.620000. running mean: -0.108800\n",
      "ep 1711: ep_len:547 episode reward: total was -19.950000. running mean: -0.307212\n",
      "ep 1711: ep_len:745 episode reward: total was -18.370000. running mean: -0.487840\n",
      "ep 1711: ep_len:1107 episode reward: total was -11.250000. running mean: -0.595461\n",
      "ep 1711: ep_len:59 episode reward: total was 28.000000. running mean: -0.309507\n",
      "ep 1711: ep_len:209 episode reward: total was 95.500000. running mean: 0.648589\n",
      "ep 1711: ep_len:106 episode reward: total was 50.000000. running mean: 1.142103\n",
      "ep 1711: ep_len:1178 episode reward: total was -30.750000. running mean: 0.823182\n",
      "ep 1711: ep_len:2775 episode reward: total was -16.060000. running mean: 0.654350\n",
      "epsilon:0.009992 episode_count: 25789. steps_count: 27585005.000000\n",
      "ep 1712: ep_len:698 episode reward: total was -53.790000. running mean: 0.109906\n",
      "ep 1712: ep_len:1647 episode reward: total was -61.960000. running mean: -0.510793\n",
      "ep 1712: ep_len:79 episode reward: total was 38.000000. running mean: -0.125685\n",
      "ep 1712: ep_len:3021 episode reward: total was -40.610000. running mean: -0.530528\n",
      "ep 1712: ep_len:1179 episode reward: total was -47.970000. running mean: -1.004923\n",
      "ep 1712: ep_len:63 episode reward: total was 30.000000. running mean: -0.694873\n",
      "ep 1712: ep_len:879 episode reward: total was 45.940000. running mean: -0.228525\n",
      "ep 1712: ep_len:3587 episode reward: total was -128.550000. running mean: -1.511739\n",
      "ep 1712: ep_len:817 episode reward: total was -16.030000. running mean: -1.656922\n",
      "ep 1712: ep_len:7502 episode reward: total was -179.690000. running mean: -3.437253\n",
      "ep 1712: ep_len:679 episode reward: total was -2.010000. running mean: -3.422980\n",
      "ep 1712: ep_len:177 episode reward: total was 84.000000. running mean: -2.548751\n",
      "ep 1712: ep_len:109 episode reward: total was 51.500000. running mean: -2.008263\n",
      "ep 1712: ep_len:1452 episode reward: total was 15.300000. running mean: -1.835180\n",
      "ep 1712: ep_len:2792 episode reward: total was -8.940000. running mean: -1.906229\n",
      "ep 1712: ep_len:38 episode reward: total was 16.000000. running mean: -1.727166\n",
      "epsilon:0.009992 episode_count: 25805. steps_count: 27609724.000000\n",
      "ep 1713: ep_len:1063 episode reward: total was -2.840000. running mean: -1.738295\n",
      "ep 1713: ep_len:1652 episode reward: total was -80.090000. running mean: -2.521812\n",
      "ep 1713: ep_len:77 episode reward: total was 37.000000. running mean: -2.126594\n",
      "ep 1713: ep_len:3039 episode reward: total was -50.240000. running mean: -2.607728\n",
      "ep 1713: ep_len:1274 episode reward: total was -22.890000. running mean: -2.810550\n",
      "ep 1713: ep_len:895 episode reward: total was 29.320000. running mean: -2.489245\n",
      "ep 1713: ep_len:4038 episode reward: total was -131.260000. running mean: -3.776952\n",
      "ep 1713: ep_len:614 episode reward: total was -37.460000. running mean: -4.113783\n",
      "ep 1713: ep_len:828 episode reward: total was 5.170000. running mean: -4.020945\n",
      "ep 1713: ep_len:655 episode reward: total was -2.900000. running mean: -4.009736\n",
      "ep 1713: ep_len:89 episode reward: total was 43.000000. running mean: -3.539638\n",
      "ep 1713: ep_len:40 episode reward: total was 17.000000. running mean: -3.334242\n",
      "ep 1713: ep_len:758 episode reward: total was -21.880000. running mean: -3.519699\n",
      "ep 1713: ep_len:2870 episode reward: total was 5.120000. running mean: -3.433302\n",
      "epsilon:0.009992 episode_count: 25819. steps_count: 27627616.000000\n",
      "ep 1714: ep_len:1352 episode reward: total was 17.850000. running mean: -3.220469\n",
      "ep 1714: ep_len:1163 episode reward: total was -26.590000. running mean: -3.454165\n",
      "ep 1714: ep_len:62 episode reward: total was 29.500000. running mean: -3.124623\n",
      "ep 1714: ep_len:3045 episode reward: total was -54.560000. running mean: -3.638977\n",
      "ep 1714: ep_len:622 episode reward: total was -3.040000. running mean: -3.632987\n",
      "ep 1714: ep_len:49 episode reward: total was 23.000000. running mean: -3.366657\n",
      "ep 1714: ep_len:84 episode reward: total was 40.500000. running mean: -2.927991\n",
      "ep 1714: ep_len:560 episode reward: total was 40.880000. running mean: -2.489911\n",
      "ep 1714: ep_len:670 episode reward: total was 5.160000. running mean: -2.413412\n",
      "ep 1714: ep_len:1145 episode reward: total was -53.360000. running mean: -2.922878\n",
      "ep 1714: ep_len:735 episode reward: total was 44.330000. running mean: -2.450349\n",
      "ep 1714: ep_len:721 episode reward: total was 43.730000. running mean: -1.988545\n",
      "ep 1714: ep_len:144 episode reward: total was 70.500000. running mean: -1.263660\n",
      "ep 1714: ep_len:124 episode reward: total was 59.000000. running mean: -0.661023\n",
      "ep 1714: ep_len:1397 episode reward: total was -0.160000. running mean: -0.656013\n",
      "ep 1714: ep_len:2750 episode reward: total was -19.950000. running mean: -0.848953\n",
      "ep 1714: ep_len:59 episode reward: total was 26.500000. running mean: -0.575463\n",
      "epsilon:0.009992 episode_count: 25836. steps_count: 27642298.000000\n",
      "ep 1715: ep_len:1399 episode reward: total was 10.910000. running mean: -0.460609\n",
      "ep 1715: ep_len:181 episode reward: total was -1.410000. running mean: -0.470103\n",
      "ep 1715: ep_len:53 episode reward: total was 25.000000. running mean: -0.215402\n",
      "ep 1715: ep_len:2910 episode reward: total was -56.390000. running mean: -0.777148\n",
      "ep 1715: ep_len:706 episode reward: total was -8.260000. running mean: -0.851976\n",
      "ep 1715: ep_len:49 episode reward: total was 23.000000. running mean: -0.613456\n",
      "ep 1715: ep_len:106 episode reward: total was 48.500000. running mean: -0.122322\n",
      "ep 1715: ep_len:62 episode reward: total was 29.500000. running mean: 0.173901\n",
      "ep 1715: ep_len:500 episode reward: total was 8.790000. running mean: 0.260062\n",
      "ep 1715: ep_len:3952 episode reward: total was -290.540000. running mean: -2.647938\n",
      "ep 1715: ep_len:883 episode reward: total was 0.560000. running mean: -2.615859\n",
      "ep 1715: ep_len:838 episode reward: total was 48.490000. running mean: -2.104800\n",
      "ep 1715: ep_len:674 episode reward: total was -10.610000. running mean: -2.189852\n",
      "ep 1715: ep_len:500 episode reward: total was 19.870000. running mean: -1.969254\n",
      "ep 1715: ep_len:2925 episode reward: total was 2.730000. running mean: -1.922261\n",
      "epsilon:0.009992 episode_count: 25851. steps_count: 27658036.000000\n",
      "ep 1716: ep_len:868 episode reward: total was 19.680000. running mean: -1.706239\n",
      "ep 1716: ep_len:773 episode reward: total was -11.980000. running mean: -1.808976\n",
      "ep 1716: ep_len:84 episode reward: total was 40.500000. running mean: -1.385886\n",
      "ep 1716: ep_len:2994 episode reward: total was -14.780000. running mean: -1.519828\n",
      "ep 1716: ep_len:1210 episode reward: total was -27.460000. running mean: -1.779229\n",
      "ep 1716: ep_len:111 episode reward: total was 52.500000. running mean: -1.236437\n",
      "ep 1716: ep_len:60 episode reward: total was 28.500000. running mean: -0.939073\n",
      "ep 1716: ep_len:1460 episode reward: total was 9.350000. running mean: -0.836182\n",
      "ep 1716: ep_len:366 episode reward: total was 8.220000. running mean: -0.745620\n",
      "ep 1716: ep_len:1293 episode reward: total was -73.090000. running mean: -1.469064\n",
      "ep 1716: ep_len:622 episode reward: total was 6.480000. running mean: -1.389573\n",
      "ep 1716: ep_len:848 episode reward: total was 28.730000. running mean: -1.088377\n",
      "ep 1716: ep_len:113 episode reward: total was 55.000000. running mean: -0.527494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1716: ep_len:96 episode reward: total was 45.000000. running mean: -0.072219\n",
      "ep 1716: ep_len:500 episode reward: total was -20.970000. running mean: -0.281197\n",
      "ep 1716: ep_len:46 episode reward: total was 20.000000. running mean: -0.078385\n",
      "epsilon:0.009992 episode_count: 25867. steps_count: 27669480.000000\n",
      "ep 1717: ep_len:1465 episode reward: total was 14.480000. running mean: 0.067199\n",
      "ep 1717: ep_len:772 episode reward: total was -7.670000. running mean: -0.010173\n",
      "ep 1717: ep_len:34 episode reward: total was 14.000000. running mean: 0.129929\n",
      "ep 1717: ep_len:2998 episode reward: total was -84.250000. running mean: -0.713870\n",
      "ep 1717: ep_len:813 episode reward: total was 2.380000. running mean: -0.682932\n",
      "ep 1717: ep_len:851 episode reward: total was 13.090000. running mean: -0.545202\n",
      "ep 1717: ep_len:589 episode reward: total was 7.780000. running mean: -0.461950\n",
      "ep 1717: ep_len:500 episode reward: total was 29.580000. running mean: -0.161531\n",
      "ep 1717: ep_len:673 episode reward: total was 35.230000. running mean: 0.192385\n",
      "ep 1717: ep_len:1056 episode reward: total was 17.650000. running mean: 0.366961\n",
      "ep 1717: ep_len:165 episode reward: total was 79.500000. running mean: 1.158291\n",
      "ep 1717: ep_len:31 episode reward: total was 12.500000. running mean: 1.271708\n",
      "ep 1717: ep_len:596 episode reward: total was -7.340000. running mean: 1.185591\n",
      "ep 1717: ep_len:2834 episode reward: total was -42.130000. running mean: 0.752435\n",
      "ep 1717: ep_len:60 episode reward: total was 28.500000. running mean: 1.029911\n",
      "epsilon:0.009992 episode_count: 25882. steps_count: 27682917.000000\n",
      "ep 1718: ep_len:638 episode reward: total was -6.920000. running mean: 0.950412\n",
      "ep 1718: ep_len:1228 episode reward: total was -59.600000. running mean: 0.344908\n",
      "ep 1718: ep_len:3038 episode reward: total was -10.870000. running mean: 0.232759\n",
      "ep 1718: ep_len:636 episode reward: total was 8.010000. running mean: 0.310531\n",
      "ep 1718: ep_len:96 episode reward: total was 45.000000. running mean: 0.757426\n",
      "ep 1718: ep_len:54 episode reward: total was 25.500000. running mean: 1.004851\n",
      "ep 1718: ep_len:1371 episode reward: total was -139.980000. running mean: -0.404997\n",
      "ep 1718: ep_len:341 episode reward: total was 12.620000. running mean: -0.274747\n",
      "ep 1718: ep_len:802 episode reward: total was -8.070000. running mean: -0.352700\n",
      "ep 1718: ep_len:658 episode reward: total was -5.590000. running mean: -0.405073\n",
      "ep 1718: ep_len:1548 episode reward: total was 24.590000. running mean: -0.155122\n",
      "ep 1718: ep_len:1400 episode reward: total was 10.740000. running mean: -0.046171\n",
      "ep 1718: ep_len:2914 episode reward: total was 5.560000. running mean: 0.009891\n",
      "ep 1718: ep_len:56 episode reward: total was 26.500000. running mean: 0.274792\n",
      "epsilon:0.009992 episode_count: 25896. steps_count: 27697697.000000\n",
      "ep 1719: ep_len:798 episode reward: total was -22.490000. running mean: 0.047144\n",
      "ep 1719: ep_len:669 episode reward: total was -24.790000. running mean: -0.201227\n",
      "ep 1719: ep_len:50 episode reward: total was 23.500000. running mean: 0.035785\n",
      "ep 1719: ep_len:2973 episode reward: total was 3.350000. running mean: 0.068927\n",
      "ep 1719: ep_len:1734 episode reward: total was -232.710000. running mean: -2.258862\n",
      "ep 1719: ep_len:64 episode reward: total was 30.500000. running mean: -1.931274\n",
      "ep 1719: ep_len:74 episode reward: total was 32.500000. running mean: -1.586961\n",
      "ep 1719: ep_len:1400 episode reward: total was -181.620000. running mean: -3.387291\n",
      "ep 1719: ep_len:3875 episode reward: total was -449.880000. running mean: -7.852218\n",
      "ep 1719: ep_len:4276 episode reward: total was -1349.050000. running mean: -21.264196\n",
      "ep 1719: ep_len:869 episode reward: total was 55.120000. running mean: -20.500354\n",
      "ep 1719: ep_len:725 episode reward: total was -11.100000. running mean: -20.406351\n",
      "ep 1719: ep_len:69 episode reward: total was 33.000000. running mean: -19.872287\n",
      "ep 1719: ep_len:567 episode reward: total was 19.420000. running mean: -19.479364\n",
      "ep 1719: ep_len:2773 episode reward: total was -7.630000. running mean: -19.360871\n",
      "epsilon:0.009992 episode_count: 25911. steps_count: 27718613.000000\n",
      "ep 1720: ep_len:1452 episode reward: total was -43.680000. running mean: -19.604062\n",
      "ep 1720: ep_len:869 episode reward: total was 15.470000. running mean: -19.253321\n",
      "ep 1720: ep_len:3027 episode reward: total was -128.590000. running mean: -20.346688\n",
      "ep 1720: ep_len:711 episode reward: total was -51.780000. running mean: -20.661021\n",
      "ep 1720: ep_len:54 episode reward: total was 25.500000. running mean: -20.199411\n",
      "ep 1720: ep_len:106 episode reward: total was 51.500000. running mean: -19.482417\n",
      "ep 1720: ep_len:76 episode reward: total was 35.000000. running mean: -18.937593\n",
      "ep 1720: ep_len:500 episode reward: total was 58.550000. running mean: -18.162717\n",
      "ep 1720: ep_len:667 episode reward: total was 21.540000. running mean: -17.765690\n",
      "ep 1720: ep_len:587 episode reward: total was -13.920000. running mean: -17.727233\n",
      "ep 1720: ep_len:609 episode reward: total was -10.820000. running mean: -17.658160\n",
      "ep 1720: ep_len:725 episode reward: total was -22.250000. running mean: -17.704079\n",
      "ep 1720: ep_len:52 episode reward: total was 24.500000. running mean: -17.282038\n",
      "ep 1720: ep_len:160 episode reward: total was 77.000000. running mean: -16.339218\n",
      "ep 1720: ep_len:1449 episode reward: total was 22.830000. running mean: -15.947525\n",
      "ep 1720: ep_len:2803 episode reward: total was -84.080000. running mean: -16.628850\n",
      "ep 1720: ep_len:44 episode reward: total was 19.000000. running mean: -16.272562\n",
      "epsilon:0.009992 episode_count: 25928. steps_count: 27732504.000000\n",
      "ep 1721: ep_len:1497 episode reward: total was 19.730000. running mean: -15.912536\n",
      "ep 1721: ep_len:719 episode reward: total was -29.750000. running mean: -16.050911\n",
      "ep 1721: ep_len:2919 episode reward: total was -169.370000. running mean: -17.584102\n",
      "ep 1721: ep_len:717 episode reward: total was -12.630000. running mean: -17.534561\n",
      "ep 1721: ep_len:77 episode reward: total was 37.000000. running mean: -16.989215\n",
      "ep 1721: ep_len:500 episode reward: total was 45.500000. running mean: -16.364323\n",
      "ep 1721: ep_len:637 episode reward: total was 23.010000. running mean: -15.970580\n",
      "ep 1721: ep_len:633 episode reward: total was 15.250000. running mean: -15.658374\n",
      "ep 1721: ep_len:711 episode reward: total was -13.770000. running mean: -15.639490\n",
      "ep 1721: ep_len:877 episode reward: total was 24.460000. running mean: -15.238495\n",
      "ep 1721: ep_len:100 episode reward: total was 48.500000. running mean: -14.601110\n",
      "ep 1721: ep_len:1058 episode reward: total was 20.020000. running mean: -14.254899\n",
      "ep 1721: ep_len:2837 episode reward: total was -4.940000. running mean: -14.161750\n",
      "ep 1721: ep_len:40 episode reward: total was 18.500000. running mean: -13.835133\n",
      "epsilon:0.009992 episode_count: 25942. steps_count: 27745826.000000\n",
      "ep 1722: ep_len:1092 episode reward: total was 0.650000. running mean: -13.690281\n",
      "ep 1722: ep_len:733 episode reward: total was -50.640000. running mean: -14.059778\n",
      "ep 1722: ep_len:33 episode reward: total was 15.000000. running mean: -13.769181\n",
      "ep 1722: ep_len:2998 episode reward: total was -89.360000. running mean: -14.525089\n",
      "ep 1722: ep_len:916 episode reward: total was 72.320000. running mean: -13.656638\n",
      "ep 1722: ep_len:51 episode reward: total was 24.000000. running mean: -13.280072\n",
      "ep 1722: ep_len:59 episode reward: total was 26.500000. running mean: -12.882271\n",
      "ep 1722: ep_len:44 episode reward: total was 20.500000. running mean: -12.548448\n",
      "ep 1722: ep_len:669 episode reward: total was 0.310000. running mean: -12.419864\n",
      "ep 1722: ep_len:615 episode reward: total was 30.260000. running mean: -11.993065\n",
      "ep 1722: ep_len:578 episode reward: total was -41.860000. running mean: -12.291734\n",
      "ep 1722: ep_len:7334 episode reward: total was -559.430000. running mean: -17.763117\n",
      "ep 1722: ep_len:1098 episode reward: total was -17.960000. running mean: -17.765086\n",
      "ep 1722: ep_len:71 episode reward: total was 34.000000. running mean: -17.247435\n",
      "ep 1722: ep_len:958 episode reward: total was -53.250000. running mean: -17.607461\n",
      "ep 1722: ep_len:2813 episode reward: total was -23.090000. running mean: -17.662286\n",
      "epsilon:0.009992 episode_count: 25958. steps_count: 27765888.000000\n",
      "ep 1723: ep_len:1148 episode reward: total was -10.380000. running mean: -17.589463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1723: ep_len:1585 episode reward: total was -41.920000. running mean: -17.832769\n",
      "ep 1723: ep_len:3015 episode reward: total was -26.770000. running mean: -17.922141\n",
      "ep 1723: ep_len:608 episode reward: total was -75.750000. running mean: -18.500419\n",
      "ep 1723: ep_len:101 episode reward: total was 49.000000. running mean: -17.825415\n",
      "ep 1723: ep_len:54 episode reward: total was 24.000000. running mean: -17.407161\n",
      "ep 1723: ep_len:1367 episode reward: total was -91.540000. running mean: -18.148490\n",
      "ep 1723: ep_len:3812 episode reward: total was -278.790000. running mean: -20.754905\n",
      "ep 1723: ep_len:570 episode reward: total was -13.660000. running mean: -20.683956\n",
      "ep 1723: ep_len:747 episode reward: total was 1.210000. running mean: -20.465016\n",
      "ep 1723: ep_len:506 episode reward: total was 18.420000. running mean: -20.076166\n",
      "ep 1723: ep_len:1149 episode reward: total was -12.920000. running mean: -20.004604\n",
      "ep 1723: ep_len:2894 episode reward: total was 1.660000. running mean: -19.787958\n",
      "ep 1723: ep_len:45 episode reward: total was 21.000000. running mean: -19.380079\n",
      "epsilon:0.009992 episode_count: 25972. steps_count: 27783489.000000\n",
      "ep 1724: ep_len:1074 episode reward: total was -11.650000. running mean: -19.302778\n",
      "ep 1724: ep_len:500 episode reward: total was 4.900000. running mean: -19.060750\n",
      "ep 1724: ep_len:53 episode reward: total was 22.000000. running mean: -18.650143\n",
      "ep 1724: ep_len:2939 episode reward: total was -20.630000. running mean: -18.669941\n",
      "ep 1724: ep_len:827 episode reward: total was 19.580000. running mean: -18.287442\n",
      "ep 1724: ep_len:1433 episode reward: total was 9.720000. running mean: -18.007367\n",
      "ep 1724: ep_len:684 episode reward: total was 33.030000. running mean: -17.496994\n",
      "ep 1724: ep_len:1551 episode reward: total was -61.820000. running mean: -17.940224\n",
      "ep 1724: ep_len:7203 episode reward: total was -392.120000. running mean: -21.682021\n",
      "ep 1724: ep_len:733 episode reward: total was 13.050000. running mean: -21.334701\n",
      "ep 1724: ep_len:617 episode reward: total was -0.060000. running mean: -21.121954\n",
      "ep 1724: ep_len:2797 episode reward: total was -3.050000. running mean: -20.941235\n",
      "epsilon:0.009992 episode_count: 25984. steps_count: 27803900.000000\n",
      "ep 1725: ep_len:995 episode reward: total was -67.230000. running mean: -21.404122\n",
      "ep 1725: ep_len:500 episode reward: total was 3.340000. running mean: -21.156681\n",
      "ep 1725: ep_len:2935 episode reward: total was -32.570000. running mean: -21.270814\n",
      "ep 1725: ep_len:1679 episode reward: total was -49.860000. running mean: -21.556706\n",
      "ep 1725: ep_len:40 episode reward: total was 18.500000. running mean: -21.156139\n",
      "ep 1725: ep_len:1284 episode reward: total was -20.800000. running mean: -21.152578\n",
      "ep 1725: ep_len:342 episode reward: total was 24.110000. running mean: -20.699952\n",
      "ep 1725: ep_len:679 episode reward: total was -16.230000. running mean: -20.655252\n",
      "ep 1725: ep_len:826 episode reward: total was 13.540000. running mean: -20.313300\n",
      "ep 1725: ep_len:500 episode reward: total was 29.810000. running mean: -19.812067\n",
      "ep 1725: ep_len:86 episode reward: total was 41.500000. running mean: -19.198946\n",
      "ep 1725: ep_len:569 episode reward: total was -4.580000. running mean: -19.052757\n",
      "ep 1725: ep_len:2777 episode reward: total was -11.880000. running mean: -18.981029\n",
      "epsilon:0.009992 episode_count: 25997. steps_count: 27817112.000000\n",
      "ep 1726: ep_len:1124 episode reward: total was 2.530000. running mean: -18.765919\n",
      "ep 1726: ep_len:913 episode reward: total was -8.300000. running mean: -18.661260\n",
      "ep 1726: ep_len:3045 episode reward: total was -30.120000. running mean: -18.775847\n",
      "ep 1726: ep_len:531 episode reward: total was -35.720000. running mean: -18.945289\n",
      "ep 1726: ep_len:57 episode reward: total was 27.000000. running mean: -18.485836\n",
      "ep 1726: ep_len:975 episode reward: total was -19.710000. running mean: -18.498077\n",
      "ep 1726: ep_len:3808 episode reward: total was -1640.480000. running mean: -34.717897\n",
      "ep 1726: ep_len:768 episode reward: total was -13.800000. running mean: -34.508718\n",
      "ep 1726: ep_len:821 episode reward: total was 27.580000. running mean: -33.887830\n",
      "ep 1726: ep_len:905 episode reward: total was 25.690000. running mean: -33.292052\n",
      "ep 1726: ep_len:1038 episode reward: total was -33.950000. running mean: -33.298632\n",
      "ep 1726: ep_len:2925 episode reward: total was -38.900000. running mean: -33.354645\n",
      "epsilon:0.009992 episode_count: 26009. steps_count: 27834022.000000\n",
      "ep 1727: ep_len:500 episode reward: total was 1.260000. running mean: -33.008499\n",
      "ep 1727: ep_len:712 episode reward: total was -30.270000. running mean: -32.981114\n",
      "ep 1727: ep_len:3013 episode reward: total was 7.450000. running mean: -32.576803\n",
      "ep 1727: ep_len:1107 episode reward: total was -25.460000. running mean: -32.505635\n",
      "ep 1727: ep_len:60 episode reward: total was 28.500000. running mean: -31.895578\n",
      "ep 1727: ep_len:720 episode reward: total was 26.330000. running mean: -31.313323\n",
      "ep 1727: ep_len:3874 episode reward: total was -246.360000. running mean: -33.463789\n",
      "ep 1727: ep_len:1305 episode reward: total was -63.880000. running mean: -33.767951\n",
      "ep 1727: ep_len:690 episode reward: total was 4.750000. running mean: -33.382772\n",
      "ep 1727: ep_len:721 episode reward: total was -1.660000. running mean: -33.065544\n",
      "ep 1727: ep_len:93 episode reward: total was 45.000000. running mean: -32.284889\n",
      "ep 1727: ep_len:187 episode reward: total was 74.000000. running mean: -31.222040\n",
      "ep 1727: ep_len:76 episode reward: total was 35.000000. running mean: -30.559819\n",
      "ep 1727: ep_len:1515 episode reward: total was 7.850000. running mean: -30.175721\n",
      "ep 1727: ep_len:2892 episode reward: total was 3.750000. running mean: -29.836464\n",
      "epsilon:0.009992 episode_count: 26024. steps_count: 27851487.000000\n",
      "ep 1728: ep_len:1402 episode reward: total was -5.810000. running mean: -29.596199\n",
      "ep 1728: ep_len:974 episode reward: total was 18.570000. running mean: -29.114537\n",
      "ep 1728: ep_len:3087 episode reward: total was 29.070000. running mean: -28.532692\n",
      "ep 1728: ep_len:1222 episode reward: total was -30.370000. running mean: -28.551065\n",
      "ep 1728: ep_len:105 episode reward: total was 46.500000. running mean: -27.800555\n",
      "ep 1728: ep_len:51 episode reward: total was 22.500000. running mean: -27.297549\n",
      "ep 1728: ep_len:1081 episode reward: total was -0.680000. running mean: -27.031373\n",
      "ep 1728: ep_len:3833 episode reward: total was -32.160000. running mean: -27.082660\n",
      "ep 1728: ep_len:1151 episode reward: total was -46.040000. running mean: -27.272233\n",
      "ep 1728: ep_len:858 episode reward: total was 45.330000. running mean: -26.546211\n",
      "ep 1728: ep_len:1009 episode reward: total was 33.800000. running mean: -25.942749\n",
      "ep 1728: ep_len:177 episode reward: total was 82.500000. running mean: -24.858321\n",
      "ep 1728: ep_len:691 episode reward: total was -54.350000. running mean: -25.153238\n",
      "ep 1728: ep_len:2835 episode reward: total was -19.620000. running mean: -25.097906\n",
      "ep 1728: ep_len:41 episode reward: total was 19.000000. running mean: -24.656927\n",
      "epsilon:0.009992 episode_count: 26039. steps_count: 27870004.000000\n",
      "ep 1729: ep_len:631 episode reward: total was 33.320000. running mean: -24.077157\n",
      "ep 1729: ep_len:702 episode reward: total was -35.570000. running mean: -24.192086\n",
      "ep 1729: ep_len:3082 episode reward: total was 6.980000. running mean: -23.880365\n",
      "ep 1729: ep_len:618 episode reward: total was 3.630000. running mean: -23.605261\n",
      "ep 1729: ep_len:100 episode reward: total was 48.500000. running mean: -22.884209\n",
      "ep 1729: ep_len:914 episode reward: total was 57.470000. running mean: -22.080667\n",
      "ep 1729: ep_len:4120 episode reward: total was -40.600000. running mean: -22.265860\n",
      "ep 1729: ep_len:1686 episode reward: total was -134.180000. running mean: -23.385001\n",
      "ep 1729: ep_len:601 episode reward: total was -9.860000. running mean: -23.249751\n",
      "ep 1729: ep_len:496 episode reward: total was 30.460000. running mean: -22.712654\n",
      "ep 1729: ep_len:58 episode reward: total was 27.500000. running mean: -22.210527\n",
      "ep 1729: ep_len:69 episode reward: total was 33.000000. running mean: -21.658422\n",
      "ep 1729: ep_len:126 episode reward: total was 60.000000. running mean: -20.841838\n",
      "ep 1729: ep_len:1464 episode reward: total was -25.840000. running mean: -20.891819\n",
      "ep 1729: ep_len:2814 episode reward: total was -9.370000. running mean: -20.776601\n",
      "epsilon:0.009992 episode_count: 26054. steps_count: 27887485.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1730: ep_len:670 episode reward: total was 5.470000. running mean: -20.514135\n",
      "ep 1730: ep_len:684 episode reward: total was -33.730000. running mean: -20.646294\n",
      "ep 1730: ep_len:2891 episode reward: total was -34.830000. running mean: -20.788131\n",
      "ep 1730: ep_len:837 episode reward: total was 0.300000. running mean: -20.577250\n",
      "ep 1730: ep_len:56 episode reward: total was 26.500000. running mean: -20.106477\n",
      "ep 1730: ep_len:961 episode reward: total was 0.380000. running mean: -19.901612\n",
      "ep 1730: ep_len:3886 episode reward: total was -72.030000. running mean: -20.422896\n",
      "ep 1730: ep_len:1017 episode reward: total was -56.560000. running mean: -20.784267\n",
      "ep 1730: ep_len:7189 episode reward: total was -33.200000. running mean: -20.908425\n",
      "ep 1730: ep_len:624 episode reward: total was -8.170000. running mean: -20.781040\n",
      "ep 1730: ep_len:74 episode reward: total was 34.000000. running mean: -20.233230\n",
      "ep 1730: ep_len:145 episode reward: total was 68.000000. running mean: -19.350898\n",
      "ep 1730: ep_len:39 episode reward: total was 16.500000. running mean: -18.992389\n",
      "ep 1730: ep_len:889 episode reward: total was 7.500000. running mean: -18.727465\n",
      "ep 1730: ep_len:2804 episode reward: total was -27.040000. running mean: -18.810590\n",
      "ep 1730: ep_len:44 episode reward: total was 20.500000. running mean: -18.417484\n",
      "epsilon:0.009992 episode_count: 26070. steps_count: 27910295.000000\n",
      "ep 1731: ep_len:1094 episode reward: total was -15.980000. running mean: -18.393109\n",
      "ep 1731: ep_len:736 episode reward: total was -20.400000. running mean: -18.413178\n",
      "ep 1731: ep_len:3009 episode reward: total was -25.020000. running mean: -18.479246\n",
      "ep 1731: ep_len:1483 episode reward: total was -5.050000. running mean: -18.344954\n",
      "ep 1731: ep_len:34 episode reward: total was 15.500000. running mean: -18.006504\n",
      "ep 1731: ep_len:1812 episode reward: total was -48.370000. running mean: -18.310139\n",
      "ep 1731: ep_len:4000 episode reward: total was -56.610000. running mean: -18.693138\n",
      "ep 1731: ep_len:3847 episode reward: total was -1223.190000. running mean: -30.738107\n",
      "ep 1731: ep_len:862 episode reward: total was 60.060000. running mean: -29.830126\n",
      "ep 1731: ep_len:1131 episode reward: total was 3.590000. running mean: -29.495924\n",
      "ep 1731: ep_len:72 episode reward: total was 34.500000. running mean: -28.855965\n",
      "ep 1731: ep_len:1079 episode reward: total was -10.010000. running mean: -28.667505\n",
      "ep 1731: ep_len:2809 episode reward: total was -62.890000. running mean: -29.009730\n",
      "epsilon:0.009992 episode_count: 26083. steps_count: 27932263.000000\n",
      "ep 1732: ep_len:1014 episode reward: total was -418.270000. running mean: -32.902333\n",
      "ep 1732: ep_len:668 episode reward: total was -14.550000. running mean: -32.718810\n",
      "ep 1732: ep_len:96 episode reward: total was 45.000000. running mean: -31.941622\n",
      "ep 1732: ep_len:651 episode reward: total was -5.580000. running mean: -31.678005\n",
      "ep 1732: ep_len:87 episode reward: total was 40.500000. running mean: -30.956225\n",
      "ep 1732: ep_len:48 episode reward: total was 21.000000. running mean: -30.436663\n",
      "ep 1732: ep_len:500 episode reward: total was -0.610000. running mean: -30.138396\n",
      "ep 1732: ep_len:614 episode reward: total was 17.880000. running mean: -29.658212\n",
      "ep 1732: ep_len:1575 episode reward: total was -82.310000. running mean: -30.184730\n",
      "ep 1732: ep_len:796 episode reward: total was 35.580000. running mean: -29.527083\n",
      "ep 1732: ep_len:980 episode reward: total was 7.000000. running mean: -29.161812\n",
      "ep 1732: ep_len:85 episode reward: total was 39.500000. running mean: -28.475194\n",
      "ep 1732: ep_len:154 episode reward: total was 72.500000. running mean: -27.465442\n",
      "ep 1732: ep_len:1090 episode reward: total was 10.100000. running mean: -27.089788\n",
      "ep 1732: ep_len:2812 episode reward: total was -6.660000. running mean: -26.885490\n",
      "epsilon:0.009992 episode_count: 26098. steps_count: 27943433.000000\n",
      "ep 1733: ep_len:500 episode reward: total was 22.630000. running mean: -26.390335\n",
      "ep 1733: ep_len:1237 episode reward: total was -36.280000. running mean: -26.489232\n",
      "ep 1733: ep_len:2927 episode reward: total was -29.600000. running mean: -26.520339\n",
      "ep 1733: ep_len:594 episode reward: total was -2.460000. running mean: -26.279736\n",
      "ep 1733: ep_len:44 episode reward: total was 19.000000. running mean: -25.826939\n",
      "ep 1733: ep_len:95 episode reward: total was 46.000000. running mean: -25.108669\n",
      "ep 1733: ep_len:92 episode reward: total was 44.500000. running mean: -24.412582\n",
      "ep 1733: ep_len:921 episode reward: total was 60.420000. running mean: -23.564257\n",
      "ep 1733: ep_len:659 episode reward: total was 27.760000. running mean: -23.051014\n",
      "ep 1733: ep_len:1298 episode reward: total was -41.730000. running mean: -23.237804\n",
      "ep 1733: ep_len:895 episode reward: total was 13.140000. running mean: -22.874026\n",
      "ep 1733: ep_len:655 episode reward: total was 6.460000. running mean: -22.580686\n",
      "ep 1733: ep_len:172 episode reward: total was 84.500000. running mean: -21.509879\n",
      "ep 1733: ep_len:59 episode reward: total was 28.000000. running mean: -21.014780\n",
      "ep 1733: ep_len:71 episode reward: total was 34.000000. running mean: -20.464632\n",
      "ep 1733: ep_len:729 episode reward: total was -66.610000. running mean: -20.926086\n",
      "ep 1733: ep_len:2772 episode reward: total was -28.890000. running mean: -21.005725\n",
      "ep 1733: ep_len:46 episode reward: total was 21.500000. running mean: -20.580668\n",
      "epsilon:0.009992 episode_count: 26116. steps_count: 27957199.000000\n",
      "ep 1734: ep_len:725 episode reward: total was -8.180000. running mean: -20.456661\n",
      "ep 1734: ep_len:731 episode reward: total was -37.530000. running mean: -20.627394\n",
      "ep 1734: ep_len:2942 episode reward: total was -60.030000. running mean: -21.021421\n",
      "ep 1734: ep_len:606 episode reward: total was 6.740000. running mean: -20.743806\n",
      "ep 1734: ep_len:738 episode reward: total was 11.190000. running mean: -20.424468\n",
      "ep 1734: ep_len:652 episode reward: total was 13.640000. running mean: -20.083824\n",
      "ep 1734: ep_len:765 episode reward: total was -44.680000. running mean: -20.329785\n",
      "ep 1734: ep_len:808 episode reward: total was -23.090000. running mean: -20.357387\n",
      "ep 1734: ep_len:850 episode reward: total was 18.740000. running mean: -19.966414\n",
      "ep 1734: ep_len:62 episode reward: total was 29.500000. running mean: -19.471749\n",
      "ep 1734: ep_len:38 episode reward: total was 17.500000. running mean: -19.102032\n",
      "ep 1734: ep_len:100 episode reward: total was 44.000000. running mean: -18.471012\n",
      "ep 1734: ep_len:701 episode reward: total was -43.660000. running mean: -18.722902\n",
      "ep 1734: ep_len:2847 episode reward: total was -56.260000. running mean: -19.098273\n",
      "epsilon:0.009992 episode_count: 26130. steps_count: 27969764.000000\n",
      "ep 1735: ep_len:1424 episode reward: total was 27.200000. running mean: -18.635290\n",
      "ep 1735: ep_len:707 episode reward: total was -61.740000. running mean: -19.066337\n",
      "ep 1735: ep_len:49 episode reward: total was 21.500000. running mean: -18.660674\n",
      "ep 1735: ep_len:3112 episode reward: total was -30.030000. running mean: -18.774367\n",
      "ep 1735: ep_len:604 episode reward: total was 15.550000. running mean: -18.431123\n",
      "ep 1735: ep_len:83 episode reward: total was 37.000000. running mean: -17.876812\n",
      "ep 1735: ep_len:500 episode reward: total was 53.560000. running mean: -17.162444\n",
      "ep 1735: ep_len:330 episode reward: total was 7.000000. running mean: -16.920819\n",
      "ep 1735: ep_len:1250 episode reward: total was -42.210000. running mean: -17.173711\n",
      "ep 1735: ep_len:633 episode reward: total was 7.820000. running mean: -16.923774\n",
      "ep 1735: ep_len:1543 episode reward: total was -0.600000. running mean: -16.760536\n",
      "ep 1735: ep_len:82 episode reward: total was 39.500000. running mean: -16.197931\n",
      "ep 1735: ep_len:169 episode reward: total was 78.500000. running mean: -15.250952\n",
      "ep 1735: ep_len:3850 episode reward: total was -1245.240000. running mean: -27.550842\n",
      "ep 1735: ep_len:2827 episode reward: total was -32.290000. running mean: -27.598234\n",
      "ep 1735: ep_len:47 episode reward: total was 22.000000. running mean: -27.102251\n",
      "epsilon:0.009992 episode_count: 26146. steps_count: 27986974.000000\n",
      "ep 1736: ep_len:1026 episode reward: total was -15.160000. running mean: -26.982829\n",
      "ep 1736: ep_len:702 episode reward: total was -27.490000. running mean: -26.987901\n",
      "ep 1736: ep_len:39 episode reward: total was 18.000000. running mean: -26.538022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1736: ep_len:2940 episode reward: total was -21.710000. running mean: -26.489741\n",
      "ep 1736: ep_len:828 episode reward: total was 62.830000. running mean: -25.596544\n",
      "ep 1736: ep_len:49 episode reward: total was 23.000000. running mean: -25.110578\n",
      "ep 1736: ep_len:121 episode reward: total was 59.000000. running mean: -24.269473\n",
      "ep 1736: ep_len:90 episode reward: total was 42.000000. running mean: -23.606778\n",
      "ep 1736: ep_len:30 episode reward: total was 13.500000. running mean: -23.235710\n",
      "ep 1736: ep_len:1102 episode reward: total was -3.290000. running mean: -23.036253\n",
      "ep 1736: ep_len:625 episode reward: total was 13.310000. running mean: -22.672791\n",
      "ep 1736: ep_len:669 episode reward: total was -35.900000. running mean: -22.805063\n",
      "ep 1736: ep_len:799 episode reward: total was -3.740000. running mean: -22.614412\n",
      "ep 1736: ep_len:613 episode reward: total was -17.270000. running mean: -22.560968\n",
      "ep 1736: ep_len:47 episode reward: total was 20.500000. running mean: -22.130358\n",
      "ep 1736: ep_len:86 episode reward: total was 40.000000. running mean: -21.509055\n",
      "ep 1736: ep_len:1617 episode reward: total was -262.530000. running mean: -23.919264\n",
      "ep 1736: ep_len:2827 episode reward: total was -15.940000. running mean: -23.839471\n",
      "ep 1736: ep_len:68 episode reward: total was 31.000000. running mean: -23.291077\n",
      "epsilon:0.009992 episode_count: 26165. steps_count: 28001252.000000\n",
      "ep 1737: ep_len:1375 episode reward: total was -10.540000. running mean: -23.163566\n",
      "ep 1737: ep_len:500 episode reward: total was 4.900000. running mean: -22.882930\n",
      "ep 1737: ep_len:81 episode reward: total was 37.500000. running mean: -22.279101\n",
      "ep 1737: ep_len:2978 episode reward: total was -77.800000. running mean: -22.834310\n",
      "ep 1737: ep_len:825 episode reward: total was -0.520000. running mean: -22.611167\n",
      "ep 1737: ep_len:67 episode reward: total was 32.000000. running mean: -22.065055\n",
      "ep 1737: ep_len:33 episode reward: total was 13.500000. running mean: -21.709405\n",
      "ep 1737: ep_len:823 episode reward: total was 28.080000. running mean: -21.211511\n",
      "ep 1737: ep_len:655 episode reward: total was 5.620000. running mean: -20.943196\n",
      "ep 1737: ep_len:738 episode reward: total was -34.610000. running mean: -21.079864\n",
      "ep 1737: ep_len:761 episode reward: total was 29.440000. running mean: -20.574665\n",
      "ep 1737: ep_len:1131 episode reward: total was 9.190000. running mean: -20.277018\n",
      "ep 1737: ep_len:189 episode reward: total was 93.000000. running mean: -19.144248\n",
      "ep 1737: ep_len:2042 episode reward: total was -389.350000. running mean: -22.846306\n",
      "ep 1737: ep_len:2803 episode reward: total was -11.370000. running mean: -22.731543\n",
      "ep 1737: ep_len:51 episode reward: total was 22.500000. running mean: -22.279227\n",
      "epsilon:0.009992 episode_count: 26181. steps_count: 28016304.000000\n",
      "ep 1738: ep_len:1470 episode reward: total was -2.860000. running mean: -22.085035\n",
      "ep 1738: ep_len:1595 episode reward: total was -60.000000. running mean: -22.464185\n",
      "ep 1738: ep_len:44 episode reward: total was 19.000000. running mean: -22.049543\n",
      "ep 1738: ep_len:3027 episode reward: total was -0.270000. running mean: -21.831747\n",
      "ep 1738: ep_len:877 episode reward: total was 56.760000. running mean: -21.045830\n",
      "ep 1738: ep_len:54 episode reward: total was 24.000000. running mean: -20.595371\n",
      "ep 1738: ep_len:100 episode reward: total was 48.500000. running mean: -19.904418\n",
      "ep 1738: ep_len:75 episode reward: total was 36.000000. running mean: -19.345374\n",
      "ep 1738: ep_len:46 episode reward: total was 21.500000. running mean: -18.936920\n",
      "ep 1738: ep_len:500 episode reward: total was 67.950000. running mean: -18.068051\n",
      "ep 1738: ep_len:3661 episode reward: total was -323.230000. running mean: -21.119670\n",
      "ep 1738: ep_len:1194 episode reward: total was -58.470000. running mean: -21.493173\n",
      "ep 1738: ep_len:871 episode reward: total was 42.240000. running mean: -20.855842\n",
      "ep 1738: ep_len:890 episode reward: total was 15.680000. running mean: -20.490483\n",
      "ep 1738: ep_len:197 episode reward: total was 97.000000. running mean: -19.315578\n",
      "ep 1738: ep_len:1148 episode reward: total was -6.620000. running mean: -19.188623\n",
      "ep 1738: ep_len:2844 episode reward: total was -7.080000. running mean: -19.067536\n",
      "epsilon:0.009992 episode_count: 26198. steps_count: 28034897.000000\n",
      "ep 1739: ep_len:1080 episode reward: total was 0.530000. running mean: -18.871561\n",
      "ep 1739: ep_len:1029 episode reward: total was 22.830000. running mean: -18.454545\n",
      "ep 1739: ep_len:37 episode reward: total was 15.500000. running mean: -18.115000\n",
      "ep 1739: ep_len:2927 episode reward: total was -2.440000. running mean: -17.958250\n",
      "ep 1739: ep_len:645 episode reward: total was -7.290000. running mean: -17.851568\n",
      "ep 1739: ep_len:46 episode reward: total was 20.000000. running mean: -17.473052\n",
      "ep 1739: ep_len:159 episode reward: total was 78.000000. running mean: -16.518321\n",
      "ep 1739: ep_len:61 episode reward: total was 27.500000. running mean: -16.078138\n",
      "ep 1739: ep_len:63 episode reward: total was 30.000000. running mean: -15.617357\n",
      "ep 1739: ep_len:996 episode reward: total was -22.410000. running mean: -15.685283\n",
      "ep 1739: ep_len:3957 episode reward: total was -111.230000. running mean: -16.640730\n",
      "ep 1739: ep_len:1277 episode reward: total was -59.110000. running mean: -17.065423\n",
      "ep 1739: ep_len:783 episode reward: total was 18.510000. running mean: -16.709669\n",
      "ep 1739: ep_len:705 episode reward: total was -11.300000. running mean: -16.655572\n",
      "ep 1739: ep_len:61 episode reward: total was 29.000000. running mean: -16.199016\n",
      "ep 1739: ep_len:97 episode reward: total was 45.500000. running mean: -15.582026\n",
      "ep 1739: ep_len:634 episode reward: total was -12.010000. running mean: -15.546306\n",
      "ep 1739: ep_len:46 episode reward: total was 21.500000. running mean: -15.175843\n",
      "epsilon:0.009992 episode_count: 26216. steps_count: 28049500.000000\n",
      "ep 1740: ep_len:703 episode reward: total was -97.540000. running mean: -15.999484\n",
      "ep 1740: ep_len:500 episode reward: total was 20.610000. running mean: -15.633390\n",
      "ep 1740: ep_len:3048 episode reward: total was -59.970000. running mean: -16.076756\n",
      "ep 1740: ep_len:614 episode reward: total was 20.770000. running mean: -15.708288\n",
      "ep 1740: ep_len:60 episode reward: total was 27.000000. running mean: -15.281205\n",
      "ep 1740: ep_len:500 episode reward: total was 63.510000. running mean: -14.493293\n",
      "ep 1740: ep_len:3736 episode reward: total was -44.240000. running mean: -14.790760\n",
      "ep 1740: ep_len:551 episode reward: total was 2.710000. running mean: -14.615753\n",
      "ep 1740: ep_len:730 episode reward: total was 27.020000. running mean: -14.199395\n",
      "ep 1740: ep_len:682 episode reward: total was 11.540000. running mean: -13.942001\n",
      "ep 1740: ep_len:73 episode reward: total was 33.500000. running mean: -13.467581\n",
      "ep 1740: ep_len:68 episode reward: total was 29.500000. running mean: -13.037905\n",
      "ep 1740: ep_len:104 episode reward: total was 49.000000. running mean: -12.417526\n",
      "ep 1740: ep_len:870 episode reward: total was -19.660000. running mean: -12.489951\n",
      "ep 1740: ep_len:2841 episode reward: total was -22.690000. running mean: -12.591952\n",
      "ep 1740: ep_len:37 episode reward: total was 17.000000. running mean: -12.296032\n",
      "epsilon:0.009992 episode_count: 26232. steps_count: 28064617.000000\n",
      "ep 1741: ep_len:1030 episode reward: total was -13.100000. running mean: -12.304072\n",
      "ep 1741: ep_len:714 episode reward: total was -1.280000. running mean: -12.193831\n",
      "ep 1741: ep_len:2984 episode reward: total was -39.430000. running mean: -12.466193\n",
      "ep 1741: ep_len:864 episode reward: total was 17.190000. running mean: -12.169631\n",
      "ep 1741: ep_len:53 episode reward: total was 23.500000. running mean: -11.812934\n",
      "ep 1741: ep_len:98 episode reward: total was 46.000000. running mean: -11.234805\n",
      "ep 1741: ep_len:72 episode reward: total was 30.000000. running mean: -10.822457\n",
      "ep 1741: ep_len:1111 episode reward: total was -14.310000. running mean: -10.857333\n",
      "ep 1741: ep_len:335 episode reward: total was 22.020000. running mean: -10.528559\n",
      "ep 1741: ep_len:675 episode reward: total was -20.690000. running mean: -10.630174\n",
      "ep 1741: ep_len:7257 episode reward: total was -159.720000. running mean: -12.121072\n",
      "ep 1741: ep_len:673 episode reward: total was 2.830000. running mean: -11.971561\n",
      "ep 1741: ep_len:820 episode reward: total was -28.070000. running mean: -12.132546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1741: ep_len:2853 episode reward: total was -3.590000. running mean: -12.047120\n",
      "epsilon:0.009992 episode_count: 26246. steps_count: 28084156.000000\n",
      "ep 1742: ep_len:1384 episode reward: total was 7.550000. running mean: -11.851149\n",
      "ep 1742: ep_len:1249 episode reward: total was -72.520000. running mean: -12.457837\n",
      "ep 1742: ep_len:75 episode reward: total was 34.500000. running mean: -11.988259\n",
      "ep 1742: ep_len:3013 episode reward: total was -36.720000. running mean: -12.235576\n",
      "ep 1742: ep_len:794 episode reward: total was 31.000000. running mean: -11.803221\n",
      "ep 1742: ep_len:95 episode reward: total was 44.500000. running mean: -11.240188\n",
      "ep 1742: ep_len:51 episode reward: total was 24.000000. running mean: -10.887787\n",
      "ep 1742: ep_len:1056 episode reward: total was -10.820000. running mean: -10.887109\n",
      "ep 1742: ep_len:314 episode reward: total was 15.290000. running mean: -10.625338\n",
      "ep 1742: ep_len:823 episode reward: total was -66.660000. running mean: -11.185684\n",
      "ep 1742: ep_len:661 episode reward: total was 34.460000. running mean: -10.729227\n",
      "ep 1742: ep_len:961 episode reward: total was -14.800000. running mean: -10.769935\n",
      "ep 1742: ep_len:85 episode reward: total was 41.000000. running mean: -10.252236\n",
      "ep 1742: ep_len:1115 episode reward: total was -14.270000. running mean: -10.292413\n",
      "ep 1742: ep_len:2800 episode reward: total was -2.930000. running mean: -10.218789\n",
      "epsilon:0.009992 episode_count: 26261. steps_count: 28098632.000000\n",
      "ep 1743: ep_len:1153 episode reward: total was -13.890000. running mean: -10.255501\n",
      "ep 1743: ep_len:769 episode reward: total was -19.810000. running mean: -10.351046\n",
      "ep 1743: ep_len:78 episode reward: total was 37.500000. running mean: -9.872536\n",
      "ep 1743: ep_len:2951 episode reward: total was -53.600000. running mean: -10.309811\n",
      "ep 1743: ep_len:500 episode reward: total was -3.460000. running mean: -10.241312\n",
      "ep 1743: ep_len:98 episode reward: total was 47.500000. running mean: -9.663899\n",
      "ep 1743: ep_len:69 episode reward: total was 31.500000. running mean: -9.252260\n",
      "ep 1743: ep_len:1467 episode reward: total was 15.180000. running mean: -9.007938\n",
      "ep 1743: ep_len:664 episode reward: total was 24.660000. running mean: -8.671258\n",
      "ep 1743: ep_len:790 episode reward: total was -49.850000. running mean: -9.083046\n",
      "ep 1743: ep_len:669 episode reward: total was 31.330000. running mean: -8.678915\n",
      "ep 1743: ep_len:985 episode reward: total was 14.460000. running mean: -8.447526\n",
      "ep 1743: ep_len:71 episode reward: total was 34.000000. running mean: -8.023051\n",
      "ep 1743: ep_len:1002 episode reward: total was -82.060000. running mean: -8.763420\n",
      "ep 1743: ep_len:2739 episode reward: total was -5.620000. running mean: -8.731986\n",
      "epsilon:0.009992 episode_count: 26276. steps_count: 28112637.000000\n",
      "ep 1744: ep_len:1173 episode reward: total was -7.260000. running mean: -8.717266\n",
      "ep 1744: ep_len:1165 episode reward: total was 3.030000. running mean: -8.599794\n",
      "ep 1744: ep_len:50 episode reward: total was 22.000000. running mean: -8.293796\n",
      "ep 1744: ep_len:2926 episode reward: total was -26.550000. running mean: -8.476358\n",
      "ep 1744: ep_len:672 episode reward: total was -5.150000. running mean: -8.443094\n",
      "ep 1744: ep_len:138 episode reward: total was 66.000000. running mean: -7.698663\n",
      "ep 1744: ep_len:727 episode reward: total was -21.700000. running mean: -7.838677\n",
      "ep 1744: ep_len:3764 episode reward: total was -292.050000. running mean: -10.680790\n",
      "ep 1744: ep_len:595 episode reward: total was -54.390000. running mean: -11.117882\n",
      "ep 1744: ep_len:881 episode reward: total was 50.450000. running mean: -10.502203\n",
      "ep 1744: ep_len:921 episode reward: total was 22.820000. running mean: -10.168981\n",
      "ep 1744: ep_len:202 episode reward: total was 95.000000. running mean: -9.117291\n",
      "ep 1744: ep_len:60 episode reward: total was 28.500000. running mean: -8.741118\n",
      "ep 1744: ep_len:500 episode reward: total was 11.610000. running mean: -8.537607\n",
      "ep 1744: ep_len:2787 episode reward: total was -0.950000. running mean: -8.461731\n",
      "ep 1744: ep_len:72 episode reward: total was 34.500000. running mean: -8.032114\n",
      "epsilon:0.009992 episode_count: 26292. steps_count: 28129270.000000\n",
      "ep 1745: ep_len:695 episode reward: total was -3.320000. running mean: -7.984993\n",
      "ep 1745: ep_len:725 episode reward: total was -47.540000. running mean: -8.380543\n",
      "ep 1745: ep_len:2953 episode reward: total was -20.580000. running mean: -8.502537\n",
      "ep 1745: ep_len:501 episode reward: total was 12.340000. running mean: -8.294112\n",
      "ep 1745: ep_len:92 episode reward: total was -17.490000. running mean: -8.386071\n",
      "ep 1745: ep_len:54 episode reward: total was 24.000000. running mean: -8.062210\n",
      "ep 1745: ep_len:651 episode reward: total was 6.340000. running mean: -7.918188\n",
      "ep 1745: ep_len:4157 episode reward: total was -186.060000. running mean: -9.699606\n",
      "ep 1745: ep_len:1527 episode reward: total was -78.870000. running mean: -10.391310\n",
      "ep 1745: ep_len:884 episode reward: total was 68.730000. running mean: -9.600097\n",
      "ep 1745: ep_len:748 episode reward: total was 1.170000. running mean: -9.492396\n",
      "ep 1745: ep_len:56 episode reward: total was 26.500000. running mean: -9.132472\n",
      "ep 1745: ep_len:164 episode reward: total was 76.000000. running mean: -8.281147\n",
      "ep 1745: ep_len:53 episode reward: total was 25.000000. running mean: -7.948336\n",
      "ep 1745: ep_len:779 episode reward: total was -14.400000. running mean: -8.012852\n",
      "ep 1745: ep_len:2824 episode reward: total was 3.100000. running mean: -7.901724\n",
      "epsilon:0.009992 episode_count: 26308. steps_count: 28146133.000000\n",
      "ep 1746: ep_len:720 episode reward: total was -32.880000. running mean: -8.151507\n",
      "ep 1746: ep_len:948 episode reward: total was 11.120000. running mean: -7.958792\n",
      "ep 1746: ep_len:2953 episode reward: total was -22.120000. running mean: -8.100404\n",
      "ep 1746: ep_len:668 episode reward: total was -8.040000. running mean: -8.099800\n",
      "ep 1746: ep_len:137 episode reward: total was 64.000000. running mean: -7.378802\n",
      "ep 1746: ep_len:65 episode reward: total was 31.000000. running mean: -6.995014\n",
      "ep 1746: ep_len:1449 episode reward: total was -67.490000. running mean: -7.599964\n",
      "ep 1746: ep_len:4001 episode reward: total was -137.540000. running mean: -8.899364\n",
      "ep 1746: ep_len:639 episode reward: total was 10.570000. running mean: -8.704670\n",
      "ep 1746: ep_len:756 episode reward: total was 14.130000. running mean: -8.476324\n",
      "ep 1746: ep_len:597 episode reward: total was -5.310000. running mean: -8.444660\n",
      "ep 1746: ep_len:71 episode reward: total was 32.500000. running mean: -8.035214\n",
      "ep 1746: ep_len:155 episode reward: total was 76.000000. running mean: -7.194862\n",
      "ep 1746: ep_len:43 episode reward: total was 20.000000. running mean: -6.922913\n",
      "ep 1746: ep_len:743 episode reward: total was -48.290000. running mean: -7.336584\n",
      "ep 1746: ep_len:2869 episode reward: total was -45.910000. running mean: -7.722318\n",
      "epsilon:0.009992 episode_count: 26324. steps_count: 28162947.000000\n",
      "ep 1747: ep_len:1049 episode reward: total was -10.370000. running mean: -7.748795\n",
      "ep 1747: ep_len:793 episode reward: total was -15.530000. running mean: -7.826607\n",
      "ep 1747: ep_len:51 episode reward: total was 24.000000. running mean: -7.508341\n",
      "ep 1747: ep_len:2957 episode reward: total was -56.330000. running mean: -7.996557\n",
      "ep 1747: ep_len:704 episode reward: total was 20.030000. running mean: -7.716292\n",
      "ep 1747: ep_len:46 episode reward: total was 20.000000. running mean: -7.439129\n",
      "ep 1747: ep_len:937 episode reward: total was 2.130000. running mean: -7.343438\n",
      "ep 1747: ep_len:319 episode reward: total was 0.920000. running mean: -7.260803\n",
      "ep 1747: ep_len:1248 episode reward: total was -63.010000. running mean: -7.818295\n",
      "ep 1747: ep_len:649 episode reward: total was 8.560000. running mean: -7.654512\n",
      "ep 1747: ep_len:1016 episode reward: total was 13.680000. running mean: -7.441167\n",
      "ep 1747: ep_len:77 episode reward: total was 37.000000. running mean: -6.996755\n",
      "ep 1747: ep_len:157 episode reward: total was 76.510000. running mean: -6.161688\n",
      "ep 1747: ep_len:47 episode reward: total was 22.000000. running mean: -5.880071\n",
      "ep 1747: ep_len:108 episode reward: total was 51.000000. running mean: -5.311270\n",
      "ep 1747: ep_len:1167 episode reward: total was -14.760000. running mean: -5.405758\n",
      "ep 1747: ep_len:2881 episode reward: total was -46.170000. running mean: -5.813400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1747: ep_len:42 episode reward: total was 19.500000. running mean: -5.560266\n",
      "epsilon:0.009992 episode_count: 26342. steps_count: 28177195.000000\n",
      "ep 1748: ep_len:1107 episode reward: total was -12.330000. running mean: -5.627963\n",
      "ep 1748: ep_len:201 episode reward: total was 8.800000. running mean: -5.483684\n",
      "ep 1748: ep_len:71 episode reward: total was 34.000000. running mean: -5.088847\n",
      "ep 1748: ep_len:3034 episode reward: total was -23.100000. running mean: -5.268958\n",
      "ep 1748: ep_len:566 episode reward: total was -18.900000. running mean: -5.405269\n",
      "ep 1748: ep_len:500 episode reward: total was -23.350000. running mean: -5.584716\n",
      "ep 1748: ep_len:3941 episode reward: total was -99.010000. running mean: -6.518969\n",
      "ep 1748: ep_len:631 episode reward: total was -30.220000. running mean: -6.755979\n",
      "ep 1748: ep_len:646 episode reward: total was -10.330000. running mean: -6.791720\n",
      "ep 1748: ep_len:688 episode reward: total was -2.600000. running mean: -6.749802\n",
      "ep 1748: ep_len:101 episode reward: total was 49.000000. running mean: -6.192304\n",
      "ep 1748: ep_len:1083 episode reward: total was 11.040000. running mean: -6.019981\n",
      "ep 1748: ep_len:2789 episode reward: total was -6.530000. running mean: -6.025081\n",
      "ep 1748: ep_len:73 episode reward: total was 35.000000. running mean: -5.614831\n",
      "epsilon:0.009992 episode_count: 26356. steps_count: 28192626.000000\n",
      "ep 1749: ep_len:862 episode reward: total was -32.960000. running mean: -5.888282\n",
      "ep 1749: ep_len:634 episode reward: total was 17.710000. running mean: -5.652300\n",
      "ep 1749: ep_len:3102 episode reward: total was -94.990000. running mean: -6.545677\n",
      "ep 1749: ep_len:836 episode reward: total was -12.310000. running mean: -6.603320\n",
      "ep 1749: ep_len:175 episode reward: total was 84.500000. running mean: -5.692287\n",
      "ep 1749: ep_len:1066 episode reward: total was -10.720000. running mean: -5.742564\n",
      "ep 1749: ep_len:3932 episode reward: total was -33.940000. running mean: -6.024538\n",
      "ep 1749: ep_len:716 episode reward: total was -29.040000. running mean: -6.254693\n",
      "ep 1749: ep_len:908 episode reward: total was 68.020000. running mean: -5.511946\n",
      "ep 1749: ep_len:500 episode reward: total was 3.520000. running mean: -5.421626\n",
      "ep 1749: ep_len:59 episode reward: total was 28.000000. running mean: -5.087410\n",
      "ep 1749: ep_len:108 episode reward: total was 52.500000. running mean: -4.511536\n",
      "ep 1749: ep_len:859 episode reward: total was 4.200000. running mean: -4.424421\n",
      "ep 1749: ep_len:2834 episode reward: total was -58.080000. running mean: -4.960976\n",
      "epsilon:0.009992 episode_count: 26370. steps_count: 28209217.000000\n",
      "ep 1750: ep_len:1077 episode reward: total was 7.570000. running mean: -4.835667\n",
      "ep 1750: ep_len:681 episode reward: total was -17.600000. running mean: -4.963310\n",
      "ep 1750: ep_len:52 episode reward: total was 25.510000. running mean: -4.658577\n",
      "ep 1750: ep_len:85 episode reward: total was 39.500000. running mean: -4.216991\n",
      "ep 1750: ep_len:1676 episode reward: total was -41.000000. running mean: -4.584821\n",
      "ep 1750: ep_len:36 episode reward: total was 15.000000. running mean: -4.388973\n",
      "ep 1750: ep_len:500 episode reward: total was 26.830000. running mean: -4.076783\n",
      "ep 1750: ep_len:654 episode reward: total was 15.470000. running mean: -3.881315\n",
      "ep 1750: ep_len:1485 episode reward: total was -35.300000. running mean: -4.195502\n",
      "ep 1750: ep_len:806 episode reward: total was -2.020000. running mean: -4.173747\n",
      "ep 1750: ep_len:559 episode reward: total was 34.460000. running mean: -3.787410\n",
      "ep 1750: ep_len:92 episode reward: total was 44.500000. running mean: -3.304536\n",
      "ep 1750: ep_len:749 episode reward: total was -21.780000. running mean: -3.489290\n",
      "ep 1750: ep_len:2797 episode reward: total was 1.880000. running mean: -3.435597\n",
      "ep 1750: ep_len:64 episode reward: total was 29.000000. running mean: -3.111241\n",
      "epsilon:0.009992 episode_count: 26385. steps_count: 28220530.000000\n",
      "ep 1751: ep_len:1136 episode reward: total was -0.980000. running mean: -3.089929\n",
      "ep 1751: ep_len:500 episode reward: total was 10.130000. running mean: -2.957730\n",
      "ep 1751: ep_len:3065 episode reward: total was -44.350000. running mean: -3.371652\n",
      "ep 1751: ep_len:1239 episode reward: total was -19.450000. running mean: -3.532436\n",
      "ep 1751: ep_len:961 episode reward: total was 41.970000. running mean: -3.077412\n",
      "ep 1751: ep_len:302 episode reward: total was 0.750000. running mean: -3.039137\n",
      "ep 1751: ep_len:917 episode reward: total was -37.490000. running mean: -3.383646\n",
      "ep 1751: ep_len:801 episode reward: total was 49.710000. running mean: -2.852710\n",
      "ep 1751: ep_len:648 episode reward: total was -12.250000. running mean: -2.946682\n",
      "ep 1751: ep_len:199 episode reward: total was 95.000000. running mean: -1.967216\n",
      "ep 1751: ep_len:109 episode reward: total was 53.000000. running mean: -1.417543\n",
      "ep 1751: ep_len:705 episode reward: total was -30.490000. running mean: -1.708268\n",
      "ep 1751: ep_len:2841 episode reward: total was -7.720000. running mean: -1.768385\n",
      "epsilon:0.009992 episode_count: 26398. steps_count: 28233953.000000\n",
      "ep 1752: ep_len:819 episode reward: total was 15.000000. running mean: -1.600702\n",
      "ep 1752: ep_len:500 episode reward: total was 17.330000. running mean: -1.411395\n",
      "ep 1752: ep_len:2978 episode reward: total was -11.920000. running mean: -1.516481\n",
      "ep 1752: ep_len:838 episode reward: total was 46.880000. running mean: -1.032516\n",
      "ep 1752: ep_len:54 episode reward: total was 24.000000. running mean: -0.782191\n",
      "ep 1752: ep_len:145 episode reward: total was 68.000000. running mean: -0.094369\n",
      "ep 1752: ep_len:1934 episode reward: total was -78.370000. running mean: -0.877125\n",
      "ep 1752: ep_len:4151 episode reward: total was -173.080000. running mean: -2.599154\n",
      "ep 1752: ep_len:736 episode reward: total was -28.690000. running mean: -2.860062\n",
      "ep 1752: ep_len:600 episode reward: total was 0.260000. running mean: -2.828862\n",
      "ep 1752: ep_len:897 episode reward: total was 53.690000. running mean: -2.263673\n",
      "ep 1752: ep_len:45 episode reward: total was 21.000000. running mean: -2.031036\n",
      "ep 1752: ep_len:1506 episode reward: total was 13.090000. running mean: -1.879826\n",
      "ep 1752: ep_len:2715 episode reward: total was -14.460000. running mean: -2.005628\n",
      "ep 1752: ep_len:63 episode reward: total was 30.000000. running mean: -1.685571\n",
      "epsilon:0.009992 episode_count: 26413. steps_count: 28251934.000000\n",
      "ep 1753: ep_len:884 episode reward: total was -9.990000. running mean: -1.768616\n",
      "ep 1753: ep_len:1664 episode reward: total was -49.170000. running mean: -2.242629\n",
      "ep 1753: ep_len:81 episode reward: total was 39.000000. running mean: -1.830203\n",
      "ep 1753: ep_len:2944 episode reward: total was -41.760000. running mean: -2.229501\n",
      "ep 1753: ep_len:542 episode reward: total was -29.420000. running mean: -2.501406\n",
      "ep 1753: ep_len:104 episode reward: total was 49.000000. running mean: -1.986392\n",
      "ep 1753: ep_len:64 episode reward: total was 30.500000. running mean: -1.661528\n",
      "ep 1753: ep_len:834 episode reward: total was 14.410000. running mean: -1.500813\n",
      "ep 1753: ep_len:609 episode reward: total was 26.280000. running mean: -1.223005\n",
      "ep 1753: ep_len:869 episode reward: total was -23.800000. running mean: -1.448775\n",
      "ep 1753: ep_len:748 episode reward: total was 10.260000. running mean: -1.331687\n",
      "ep 1753: ep_len:538 episode reward: total was 15.570000. running mean: -1.162670\n",
      "ep 1753: ep_len:46 episode reward: total was 20.000000. running mean: -0.951043\n",
      "ep 1753: ep_len:62 episode reward: total was 29.500000. running mean: -0.646533\n",
      "ep 1753: ep_len:1459 episode reward: total was 5.020000. running mean: -0.589868\n",
      "ep 1753: ep_len:2793 episode reward: total was -13.160000. running mean: -0.715569\n",
      "epsilon:0.009992 episode_count: 26429. steps_count: 28266175.000000\n",
      "ep 1754: ep_len:1106 episode reward: total was 5.330000. running mean: -0.655113\n",
      "ep 1754: ep_len:631 episode reward: total was -8.000000. running mean: -0.728562\n",
      "ep 1754: ep_len:2907 episode reward: total was -24.600000. running mean: -0.967276\n",
      "ep 1754: ep_len:1123 episode reward: total was -18.230000. running mean: -1.139904\n",
      "ep 1754: ep_len:151 episode reward: total was 74.000000. running mean: -0.388505\n",
      "ep 1754: ep_len:42 episode reward: total was 16.500000. running mean: -0.219620\n",
      "ep 1754: ep_len:738 episode reward: total was -45.830000. running mean: -0.675723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1754: ep_len:330 episode reward: total was 17.070000. running mean: -0.498266\n",
      "ep 1754: ep_len:587 episode reward: total was -8.440000. running mean: -0.577684\n",
      "ep 1754: ep_len:638 episode reward: total was 13.960000. running mean: -0.432307\n",
      "ep 1754: ep_len:688 episode reward: total was -7.430000. running mean: -0.502284\n",
      "ep 1754: ep_len:132 episode reward: total was 64.500000. running mean: 0.147739\n",
      "ep 1754: ep_len:1158 episode reward: total was -3.240000. running mean: 0.113862\n",
      "ep 1754: ep_len:2824 episode reward: total was 4.600000. running mean: 0.158723\n",
      "ep 1754: ep_len:47 episode reward: total was 22.000000. running mean: 0.377136\n",
      "epsilon:0.009992 episode_count: 26444. steps_count: 28279277.000000\n",
      "ep 1755: ep_len:931 episode reward: total was 21.170000. running mean: 0.585065\n",
      "ep 1755: ep_len:919 episode reward: total was 19.460000. running mean: 0.773814\n",
      "ep 1755: ep_len:2812 episode reward: total was -13.520000. running mean: 0.630876\n",
      "ep 1755: ep_len:820 episode reward: total was 27.340000. running mean: 0.897967\n",
      "ep 1755: ep_len:162 episode reward: total was 76.500000. running mean: 1.653987\n",
      "ep 1755: ep_len:1464 episode reward: total was 11.990000. running mean: 1.757347\n",
      "ep 1755: ep_len:4029 episode reward: total was -2691.400000. running mean: -25.174226\n",
      "ep 1755: ep_len:815 episode reward: total was -20.980000. running mean: -25.132284\n",
      "ep 1755: ep_len:627 episode reward: total was -1.000000. running mean: -24.890961\n",
      "ep 1755: ep_len:1461 episode reward: total was 14.040000. running mean: -24.501651\n",
      "ep 1755: ep_len:98 episode reward: total was 47.500000. running mean: -23.781635\n",
      "ep 1755: ep_len:173 episode reward: total was 82.000000. running mean: -22.723818\n",
      "ep 1755: ep_len:56 episode reward: total was 26.500000. running mean: -22.231580\n",
      "ep 1755: ep_len:500 episode reward: total was 35.830000. running mean: -21.650964\n",
      "ep 1755: ep_len:2915 episode reward: total was -48.340000. running mean: -21.917855\n",
      "ep 1755: ep_len:45 episode reward: total was 21.000000. running mean: -21.488676\n",
      "epsilon:0.009992 episode_count: 26460. steps_count: 28297104.000000\n",
      "ep 1756: ep_len:5009 episode reward: total was -1299.350000. running mean: -34.267289\n",
      "ep 1756: ep_len:1600 episode reward: total was -75.440000. running mean: -34.679017\n",
      "ep 1756: ep_len:3026 episode reward: total was -37.380000. running mean: -34.706026\n",
      "ep 1756: ep_len:1132 episode reward: total was -33.290000. running mean: -34.691866\n",
      "ep 1756: ep_len:127 episode reward: total was 59.000000. running mean: -33.754947\n",
      "ep 1756: ep_len:57 episode reward: total was 27.000000. running mean: -33.147398\n",
      "ep 1756: ep_len:500 episode reward: total was 3.950000. running mean: -32.776424\n",
      "ep 1756: ep_len:4085 episode reward: total was -1041.110000. running mean: -42.859760\n",
      "ep 1756: ep_len:657 episode reward: total was 2.360000. running mean: -42.407562\n",
      "ep 1756: ep_len:666 episode reward: total was 13.140000. running mean: -41.852087\n",
      "ep 1756: ep_len:633 episode reward: total was -15.610000. running mean: -41.589666\n",
      "ep 1756: ep_len:97 episode reward: total was 47.000000. running mean: -40.703769\n",
      "ep 1756: ep_len:643 episode reward: total was 19.460000. running mean: -40.102131\n",
      "ep 1756: ep_len:2832 episode reward: total was -12.150000. running mean: -39.822610\n",
      "ep 1756: ep_len:53 episode reward: total was 23.500000. running mean: -39.189384\n",
      "epsilon:0.009992 episode_count: 26475. steps_count: 28318221.000000\n",
      "ep 1757: ep_len:1472 episode reward: total was -2.250000. running mean: -38.819990\n",
      "ep 1757: ep_len:768 episode reward: total was -3.620000. running mean: -38.467990\n",
      "ep 1757: ep_len:3045 episode reward: total was -45.210000. running mean: -38.535410\n",
      "ep 1757: ep_len:500 episode reward: total was 34.050000. running mean: -37.809556\n",
      "ep 1757: ep_len:145 episode reward: total was 71.000000. running mean: -36.721461\n",
      "ep 1757: ep_len:651 episode reward: total was 19.880000. running mean: -36.155446\n",
      "ep 1757: ep_len:636 episode reward: total was 33.010000. running mean: -35.463792\n",
      "ep 1757: ep_len:3012 episode reward: total was -745.730000. running mean: -42.566454\n",
      "ep 1757: ep_len:660 episode reward: total was -4.430000. running mean: -42.185089\n",
      "ep 1757: ep_len:647 episode reward: total was 8.120000. running mean: -41.682038\n",
      "ep 1757: ep_len:135 episode reward: total was 64.500000. running mean: -40.620218\n",
      "ep 1757: ep_len:44 episode reward: total was 20.500000. running mean: -40.009016\n",
      "ep 1757: ep_len:71 episode reward: total was 34.000000. running mean: -39.268926\n",
      "ep 1757: ep_len:631 episode reward: total was -5.590000. running mean: -38.932136\n",
      "ep 1757: ep_len:2967 episode reward: total was -16.100000. running mean: -38.703815\n",
      "ep 1757: ep_len:53 episode reward: total was 25.000000. running mean: -38.066777\n",
      "epsilon:0.009992 episode_count: 26491. steps_count: 28333658.000000\n",
      "ep 1758: ep_len:970 episode reward: total was -91.370000. running mean: -38.599809\n",
      "ep 1758: ep_len:977 episode reward: total was 1.430000. running mean: -38.199511\n",
      "ep 1758: ep_len:80 episode reward: total was 37.000000. running mean: -37.447516\n",
      "ep 1758: ep_len:2988 episode reward: total was -35.490000. running mean: -37.427941\n",
      "ep 1758: ep_len:2612 episode reward: total was -427.540000. running mean: -41.329061\n",
      "ep 1758: ep_len:66 episode reward: total was 30.000000. running mean: -40.615771\n",
      "ep 1758: ep_len:798 episode reward: total was 24.830000. running mean: -39.961313\n",
      "ep 1758: ep_len:358 episode reward: total was 21.760000. running mean: -39.344100\n",
      "ep 1758: ep_len:3017 episode reward: total was -365.360000. running mean: -42.604259\n",
      "ep 1758: ep_len:614 episode reward: total was -14.720000. running mean: -42.325416\n",
      "ep 1758: ep_len:627 episode reward: total was -3.020000. running mean: -41.932362\n",
      "ep 1758: ep_len:1523 episode reward: total was 11.730000. running mean: -41.395738\n",
      "ep 1758: ep_len:2785 episode reward: total was -6.380000. running mean: -41.045581\n",
      "epsilon:0.009992 episode_count: 26504. steps_count: 28351073.000000\n",
      "ep 1759: ep_len:1459 episode reward: total was 38.660000. running mean: -40.248525\n",
      "ep 1759: ep_len:640 episode reward: total was -21.500000. running mean: -40.061040\n",
      "ep 1759: ep_len:58 episode reward: total was 27.500000. running mean: -39.385430\n",
      "ep 1759: ep_len:2958 episode reward: total was -42.380000. running mean: -39.415375\n",
      "ep 1759: ep_len:666 episode reward: total was -8.300000. running mean: -39.104222\n",
      "ep 1759: ep_len:96 episode reward: total was 45.000000. running mean: -38.263179\n",
      "ep 1759: ep_len:656 episode reward: total was -4.660000. running mean: -37.927147\n",
      "ep 1759: ep_len:3517 episode reward: total was -494.930000. running mean: -42.497176\n",
      "ep 1759: ep_len:835 episode reward: total was -21.810000. running mean: -42.290304\n",
      "ep 1759: ep_len:688 episode reward: total was 34.890000. running mean: -41.518501\n",
      "ep 1759: ep_len:1511 episode reward: total was 10.350000. running mean: -40.999816\n",
      "ep 1759: ep_len:166 episode reward: total was 82.510000. running mean: -39.764718\n",
      "ep 1759: ep_len:1065 episode reward: total was 41.460000. running mean: -38.952471\n",
      "ep 1759: ep_len:2828 episode reward: total was -7.760000. running mean: -38.640546\n",
      "ep 1759: ep_len:65 episode reward: total was 29.500000. running mean: -37.959141\n",
      "epsilon:0.009992 episode_count: 26519. steps_count: 28368281.000000\n",
      "ep 1760: ep_len:741 episode reward: total was -70.530000. running mean: -38.284849\n",
      "ep 1760: ep_len:500 episode reward: total was 15.370000. running mean: -37.748301\n",
      "ep 1760: ep_len:48 episode reward: total was 21.000000. running mean: -37.160818\n",
      "ep 1760: ep_len:3037 episode reward: total was -18.330000. running mean: -36.972510\n",
      "ep 1760: ep_len:848 episode reward: total was 50.500000. running mean: -36.097785\n",
      "ep 1760: ep_len:38 episode reward: total was 16.000000. running mean: -35.576807\n",
      "ep 1760: ep_len:1393 episode reward: total was -30.680000. running mean: -35.527839\n",
      "ep 1760: ep_len:3907 episode reward: total was -5.230000. running mean: -35.224860\n",
      "ep 1760: ep_len:982 episode reward: total was 5.840000. running mean: -34.814212\n",
      "ep 1760: ep_len:7439 episode reward: total was -73.610000. running mean: -35.202169\n",
      "ep 1760: ep_len:883 episode reward: total was 19.930000. running mean: -34.650848\n",
      "ep 1760: ep_len:49 episode reward: total was 23.000000. running mean: -34.074339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1760: ep_len:652 episode reward: total was -8.800000. running mean: -33.821596\n",
      "ep 1760: ep_len:2834 episode reward: total was 5.160000. running mean: -33.431780\n",
      "epsilon:0.009992 episode_count: 26533. steps_count: 28391632.000000\n",
      "ep 1761: ep_len:1201 episode reward: total was -79.550000. running mean: -33.892962\n",
      "ep 1761: ep_len:1547 episode reward: total was -42.790000. running mean: -33.981933\n",
      "ep 1761: ep_len:3063 episode reward: total was -40.190000. running mean: -34.044013\n",
      "ep 1761: ep_len:785 episode reward: total was -29.690000. running mean: -34.000473\n",
      "ep 1761: ep_len:45 episode reward: total was 19.500000. running mean: -33.465468\n",
      "ep 1761: ep_len:3325 episode reward: total was -141.340000. running mean: -34.544214\n",
      "ep 1761: ep_len:3732 episode reward: total was -171.540000. running mean: -35.914172\n",
      "ep 1761: ep_len:2762 episode reward: total was -261.410000. running mean: -38.169130\n",
      "ep 1761: ep_len:7337 episode reward: total was -106.410000. running mean: -38.851539\n",
      "ep 1761: ep_len:661 episode reward: total was 5.960000. running mean: -38.403423\n",
      "ep 1761: ep_len:88 episode reward: total was 42.500000. running mean: -37.594389\n",
      "ep 1761: ep_len:137 episode reward: total was 67.000000. running mean: -36.548445\n",
      "ep 1761: ep_len:61 episode reward: total was 29.000000. running mean: -35.892961\n",
      "ep 1761: ep_len:101 episode reward: total was 47.500000. running mean: -35.059031\n",
      "ep 1761: ep_len:834 episode reward: total was -14.490000. running mean: -34.853341\n",
      "ep 1761: ep_len:2817 episode reward: total was -22.430000. running mean: -34.729107\n",
      "ep 1761: ep_len:51 episode reward: total was 24.000000. running mean: -34.141816\n",
      "epsilon:0.009992 episode_count: 26550. steps_count: 28420179.000000\n",
      "ep 1762: ep_len:1089 episode reward: total was 5.830000. running mean: -33.742098\n",
      "ep 1762: ep_len:697 episode reward: total was -20.470000. running mean: -33.609377\n",
      "ep 1762: ep_len:2952 episode reward: total was -48.370000. running mean: -33.756983\n",
      "ep 1762: ep_len:634 episode reward: total was 0.960000. running mean: -33.409813\n",
      "ep 1762: ep_len:39 episode reward: total was 16.500000. running mean: -32.910715\n",
      "ep 1762: ep_len:44 episode reward: total was 19.000000. running mean: -32.391608\n",
      "ep 1762: ep_len:839 episode reward: total was 1.000000. running mean: -32.057692\n",
      "ep 1762: ep_len:3869 episode reward: total was -156.980000. running mean: -33.306915\n",
      "ep 1762: ep_len:736 episode reward: total was -236.690000. running mean: -35.340746\n",
      "ep 1762: ep_len:678 episode reward: total was -7.130000. running mean: -35.058639\n",
      "ep 1762: ep_len:953 episode reward: total was -26.750000. running mean: -34.975552\n",
      "ep 1762: ep_len:42 episode reward: total was 19.500000. running mean: -34.430797\n",
      "ep 1762: ep_len:629 episode reward: total was -1.540000. running mean: -34.101889\n",
      "ep 1762: ep_len:2809 episode reward: total was -74.330000. running mean: -34.504170\n",
      "ep 1762: ep_len:66 episode reward: total was 31.500000. running mean: -33.844128\n",
      "epsilon:0.009992 episode_count: 26565. steps_count: 28436255.000000\n",
      "ep 1763: ep_len:671 episode reward: total was 4.530000. running mean: -33.460387\n",
      "ep 1763: ep_len:500 episode reward: total was 29.120000. running mean: -32.834583\n",
      "ep 1763: ep_len:3017 episode reward: total was -23.940000. running mean: -32.745637\n",
      "ep 1763: ep_len:633 episode reward: total was -8.050000. running mean: -32.498681\n",
      "ep 1763: ep_len:50 episode reward: total was 23.500000. running mean: -31.938694\n",
      "ep 1763: ep_len:1381 episode reward: total was -17.260000. running mean: -31.791907\n",
      "ep 1763: ep_len:313 episode reward: total was 16.780000. running mean: -31.306188\n",
      "ep 1763: ep_len:1573 episode reward: total was -93.380000. running mean: -31.926926\n",
      "ep 1763: ep_len:737 episode reward: total was -44.400000. running mean: -32.051657\n",
      "ep 1763: ep_len:1128 episode reward: total was -15.150000. running mean: -31.882640\n",
      "ep 1763: ep_len:78 episode reward: total was 37.500000. running mean: -31.188814\n",
      "ep 1763: ep_len:626 episode reward: total was -9.060000. running mean: -30.967526\n",
      "ep 1763: ep_len:2821 episode reward: total was -21.110000. running mean: -30.868950\n",
      "epsilon:0.009992 episode_count: 26578. steps_count: 28449783.000000\n",
      "ep 1764: ep_len:659 episode reward: total was 32.870000. running mean: -30.231561\n",
      "ep 1764: ep_len:1236 episode reward: total was -40.330000. running mean: -30.332545\n",
      "ep 1764: ep_len:3070 episode reward: total was -26.050000. running mean: -30.289720\n",
      "ep 1764: ep_len:605 episode reward: total was 0.160000. running mean: -29.985223\n",
      "ep 1764: ep_len:94 episode reward: total was 42.500000. running mean: -29.260370\n",
      "ep 1764: ep_len:1054 episode reward: total was -71.420000. running mean: -29.681967\n",
      "ep 1764: ep_len:3712 episode reward: total was -1.810000. running mean: -29.403247\n",
      "ep 1764: ep_len:610 episode reward: total was 9.830000. running mean: -29.010915\n",
      "ep 1764: ep_len:643 episode reward: total was 17.230000. running mean: -28.548505\n",
      "ep 1764: ep_len:500 episode reward: total was 42.900000. running mean: -27.834020\n",
      "ep 1764: ep_len:87 episode reward: total was 40.500000. running mean: -27.150680\n",
      "ep 1764: ep_len:1509 episode reward: total was 13.390000. running mean: -26.745273\n",
      "ep 1764: ep_len:46 episode reward: total was 21.500000. running mean: -26.262821\n",
      "epsilon:0.009992 episode_count: 26591. steps_count: 28463608.000000\n",
      "ep 1765: ep_len:500 episode reward: total was 18.770000. running mean: -25.812492\n",
      "ep 1765: ep_len:713 episode reward: total was -36.180000. running mean: -25.916168\n",
      "ep 1765: ep_len:2790 episode reward: total was -14.140000. running mean: -25.798406\n",
      "ep 1765: ep_len:500 episode reward: total was 19.140000. running mean: -25.349022\n",
      "ep 1765: ep_len:2451 episode reward: total was -1880.750000. running mean: -43.903032\n",
      "ep 1765: ep_len:3617 episode reward: total was -10.050000. running mean: -43.564501\n",
      "ep 1765: ep_len:904 episode reward: total was -713.130000. running mean: -50.260156\n",
      "ep 1765: ep_len:746 episode reward: total was -39.200000. running mean: -50.149555\n",
      "ep 1765: ep_len:573 episode reward: total was 5.830000. running mean: -49.589759\n",
      "ep 1765: ep_len:169 episode reward: total was 83.000000. running mean: -48.263862\n",
      "ep 1765: ep_len:1519 episode reward: total was -3.980000. running mean: -47.821023\n",
      "ep 1765: ep_len:2747 episode reward: total was -88.630000. running mean: -48.229113\n",
      "epsilon:0.009992 episode_count: 26603. steps_count: 28480837.000000\n",
      "ep 1766: ep_len:1128 episode reward: total was -19.560000. running mean: -47.942422\n",
      "ep 1766: ep_len:673 episode reward: total was -37.070000. running mean: -47.833697\n",
      "ep 1766: ep_len:64 episode reward: total was 30.500000. running mean: -47.050360\n",
      "ep 1766: ep_len:2976 episode reward: total was -17.230000. running mean: -46.752157\n",
      "ep 1766: ep_len:657 episode reward: total was 14.100000. running mean: -46.143635\n",
      "ep 1766: ep_len:87 episode reward: total was 42.000000. running mean: -45.262199\n",
      "ep 1766: ep_len:72 episode reward: total was 34.500000. running mean: -44.464577\n",
      "ep 1766: ep_len:605 episode reward: total was 14.030000. running mean: -43.879631\n",
      "ep 1766: ep_len:642 episode reward: total was 5.490000. running mean: -43.385935\n",
      "ep 1766: ep_len:891 episode reward: total was -29.640000. running mean: -43.248475\n",
      "ep 1766: ep_len:7289 episode reward: total was -788.260000. running mean: -50.698591\n",
      "ep 1766: ep_len:862 episode reward: total was 24.370000. running mean: -49.947905\n",
      "ep 1766: ep_len:701 episode reward: total was 18.260000. running mean: -49.265826\n",
      "ep 1766: ep_len:2804 episode reward: total was -6.190000. running mean: -48.835067\n",
      "epsilon:0.009992 episode_count: 26617. steps_count: 28500288.000000\n",
      "ep 1767: ep_len:1415 episode reward: total was 17.400000. running mean: -48.172717\n",
      "ep 1767: ep_len:210 episode reward: total was -8.710000. running mean: -47.778090\n",
      "ep 1767: ep_len:70 episode reward: total was 33.500000. running mean: -46.965309\n",
      "ep 1767: ep_len:2938 episode reward: total was -79.710000. running mean: -47.292756\n",
      "ep 1767: ep_len:500 episode reward: total was 23.000000. running mean: -46.589828\n",
      "ep 1767: ep_len:69 episode reward: total was 31.500000. running mean: -45.808930\n",
      "ep 1767: ep_len:1375 episode reward: total was -117.130000. running mean: -46.522140\n",
      "ep 1767: ep_len:625 episode reward: total was 28.430000. running mean: -45.772619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1767: ep_len:612 episode reward: total was -31.420000. running mean: -45.629093\n",
      "ep 1767: ep_len:7323 episode reward: total was -38.960000. running mean: -45.562402\n",
      "ep 1767: ep_len:567 episode reward: total was -16.720000. running mean: -45.273978\n",
      "ep 1767: ep_len:104 episode reward: total was 50.500000. running mean: -44.316238\n",
      "ep 1767: ep_len:1500 episode reward: total was 4.670000. running mean: -43.826376\n",
      "ep 1767: ep_len:2826 episode reward: total was -26.600000. running mean: -43.654112\n",
      "ep 1767: ep_len:49 episode reward: total was 23.000000. running mean: -42.987571\n",
      "epsilon:0.009992 episode_count: 26632. steps_count: 28520471.000000\n",
      "ep 1768: ep_len:619 episode reward: total was 7.910000. running mean: -42.478595\n",
      "ep 1768: ep_len:771 episode reward: total was -38.440000. running mean: -42.438209\n",
      "ep 1768: ep_len:3057 episode reward: total was -22.660000. running mean: -42.240427\n",
      "ep 1768: ep_len:518 episode reward: total was 6.020000. running mean: -41.757823\n",
      "ep 1768: ep_len:91 episode reward: total was 41.000000. running mean: -40.930245\n",
      "ep 1768: ep_len:50 episode reward: total was 23.500000. running mean: -40.285942\n",
      "ep 1768: ep_len:1850 episode reward: total was -76.150000. running mean: -40.644583\n",
      "ep 1768: ep_len:321 episode reward: total was 8.320000. running mean: -40.154937\n",
      "ep 1768: ep_len:746 episode reward: total was -29.480000. running mean: -40.048188\n",
      "ep 1768: ep_len:692 episode reward: total was 53.760000. running mean: -39.110106\n",
      "ep 1768: ep_len:763 episode reward: total was -15.770000. running mean: -38.876705\n",
      "ep 1768: ep_len:83 episode reward: total was 37.000000. running mean: -38.117938\n",
      "ep 1768: ep_len:1057 episode reward: total was -4.750000. running mean: -37.784258\n",
      "ep 1768: ep_len:2750 episode reward: total was -75.290000. running mean: -38.159316\n",
      "epsilon:0.009992 episode_count: 26646. steps_count: 28533839.000000\n",
      "ep 1769: ep_len:615 episode reward: total was -21.660000. running mean: -37.994322\n",
      "ep 1769: ep_len:1302 episode reward: total was -53.810000. running mean: -38.152479\n",
      "ep 1769: ep_len:25 episode reward: total was 8.000000. running mean: -37.690954\n",
      "ep 1769: ep_len:2817 episode reward: total was -33.700000. running mean: -37.651045\n",
      "ep 1769: ep_len:847 episode reward: total was 35.930000. running mean: -36.915234\n",
      "ep 1769: ep_len:38 episode reward: total was 16.000000. running mean: -36.386082\n",
      "ep 1769: ep_len:89 episode reward: total was 40.000000. running mean: -35.622221\n",
      "ep 1769: ep_len:641 episode reward: total was -11.920000. running mean: -35.385199\n",
      "ep 1769: ep_len:4029 episode reward: total was -750.400000. running mean: -42.535347\n",
      "ep 1769: ep_len:1619 episode reward: total was -19.570000. running mean: -42.305694\n",
      "ep 1769: ep_len:806 episode reward: total was -38.800000. running mean: -42.270637\n",
      "ep 1769: ep_len:1501 episode reward: total was 30.310000. running mean: -41.544830\n",
      "ep 1769: ep_len:206 episode reward: total was 98.500000. running mean: -40.144382\n",
      "ep 1769: ep_len:647 episode reward: total was 1.170000. running mean: -39.731238\n",
      "ep 1769: ep_len:2779 episode reward: total was -15.260000. running mean: -39.486526\n",
      "ep 1769: ep_len:51 episode reward: total was 22.500000. running mean: -38.866661\n",
      "epsilon:0.009992 episode_count: 26662. steps_count: 28551851.000000\n",
      "ep 1770: ep_len:1458 episode reward: total was 5.900000. running mean: -38.418994\n",
      "ep 1770: ep_len:656 episode reward: total was -34.010000. running mean: -38.374904\n",
      "ep 1770: ep_len:62 episode reward: total was 29.500000. running mean: -37.696155\n",
      "ep 1770: ep_len:2970 episode reward: total was -26.680000. running mean: -37.585993\n",
      "ep 1770: ep_len:686 episode reward: total was -6.300000. running mean: -37.273134\n",
      "ep 1770: ep_len:80 episode reward: total was 38.500000. running mean: -36.515402\n",
      "ep 1770: ep_len:500 episode reward: total was 20.580000. running mean: -35.944448\n",
      "ep 1770: ep_len:655 episode reward: total was 21.230000. running mean: -35.372704\n",
      "ep 1770: ep_len:1482 episode reward: total was -15.960000. running mean: -35.178577\n",
      "ep 1770: ep_len:7206 episode reward: total was 41.830000. running mean: -34.408491\n",
      "ep 1770: ep_len:654 episode reward: total was 14.660000. running mean: -33.917806\n",
      "ep 1770: ep_len:92 episode reward: total was 44.500000. running mean: -33.133628\n",
      "ep 1770: ep_len:75 episode reward: total was 36.000000. running mean: -32.442292\n",
      "ep 1770: ep_len:1466 episode reward: total was -7.000000. running mean: -32.187869\n",
      "ep 1770: ep_len:2856 episode reward: total was -64.830000. running mean: -32.514290\n",
      "ep 1770: ep_len:67 episode reward: total was 32.000000. running mean: -31.869147\n",
      "epsilon:0.009992 episode_count: 26678. steps_count: 28572816.000000\n",
      "ep 1771: ep_len:1201 episode reward: total was 14.850000. running mean: -31.401956\n",
      "ep 1771: ep_len:706 episode reward: total was -42.740000. running mean: -31.515336\n",
      "ep 1771: ep_len:67 episode reward: total was 30.500000. running mean: -30.895183\n",
      "ep 1771: ep_len:2941 episode reward: total was -43.590000. running mean: -31.022131\n",
      "ep 1771: ep_len:642 episode reward: total was -7.900000. running mean: -30.790910\n",
      "ep 1771: ep_len:719 episode reward: total was 19.460000. running mean: -30.288400\n",
      "ep 1771: ep_len:3679 episode reward: total was -161.450000. running mean: -31.600016\n",
      "ep 1771: ep_len:685 episode reward: total was -43.850000. running mean: -31.722516\n",
      "ep 1771: ep_len:746 episode reward: total was 30.110000. running mean: -31.104191\n",
      "ep 1771: ep_len:551 episode reward: total was 5.630000. running mean: -30.736849\n",
      "ep 1771: ep_len:1068 episode reward: total was 15.440000. running mean: -30.275081\n",
      "ep 1771: ep_len:2858 episode reward: total was -26.590000. running mean: -30.238230\n",
      "ep 1771: ep_len:31 episode reward: total was 14.000000. running mean: -29.795848\n",
      "epsilon:0.009992 episode_count: 26691. steps_count: 28588710.000000\n",
      "ep 1772: ep_len:1052 episode reward: total was 1.260000. running mean: -29.485289\n",
      "ep 1772: ep_len:789 episode reward: total was 16.580000. running mean: -29.024636\n",
      "ep 1772: ep_len:2942 episode reward: total was -77.290000. running mean: -29.507290\n",
      "ep 1772: ep_len:687 episode reward: total was 4.060000. running mean: -29.171617\n",
      "ep 1772: ep_len:59 episode reward: total was 26.500000. running mean: -28.614901\n",
      "ep 1772: ep_len:68 episode reward: total was 32.500000. running mean: -28.003752\n",
      "ep 1772: ep_len:1479 episode reward: total was 27.720000. running mean: -27.446514\n",
      "ep 1772: ep_len:348 episode reward: total was 14.130000. running mean: -27.030749\n",
      "ep 1772: ep_len:682 episode reward: total was -22.970000. running mean: -26.990142\n",
      "ep 1772: ep_len:697 episode reward: total was 35.840000. running mean: -26.361840\n",
      "ep 1772: ep_len:1014 episode reward: total was -9.520000. running mean: -26.193422\n",
      "ep 1772: ep_len:62 episode reward: total was 28.000000. running mean: -25.651488\n",
      "ep 1772: ep_len:161 episode reward: total was 76.000000. running mean: -24.634973\n",
      "ep 1772: ep_len:640 episode reward: total was 1.180000. running mean: -24.376823\n",
      "ep 1772: ep_len:2855 episode reward: total was 3.410000. running mean: -24.098955\n",
      "ep 1772: ep_len:68 episode reward: total was 31.000000. running mean: -23.547965\n",
      "epsilon:0.009992 episode_count: 26707. steps_count: 28602313.000000\n",
      "ep 1773: ep_len:1442 episode reward: total was 21.560000. running mean: -23.096886\n",
      "ep 1773: ep_len:677 episode reward: total was -41.470000. running mean: -23.280617\n",
      "ep 1773: ep_len:66 episode reward: total was 30.000000. running mean: -22.747811\n",
      "ep 1773: ep_len:2945 episode reward: total was -1.680000. running mean: -22.537132\n",
      "ep 1773: ep_len:823 episode reward: total was -19.910000. running mean: -22.510861\n",
      "ep 1773: ep_len:52 episode reward: total was 24.500000. running mean: -22.040753\n",
      "ep 1773: ep_len:74 episode reward: total was 31.000000. running mean: -21.510345\n",
      "ep 1773: ep_len:615 episode reward: total was 14.560000. running mean: -21.149642\n",
      "ep 1773: ep_len:641 episode reward: total was 27.000000. running mean: -20.668145\n",
      "ep 1773: ep_len:802 episode reward: total was -22.820000. running mean: -20.689664\n",
      "ep 1773: ep_len:715 episode reward: total was 33.140000. running mean: -20.151367\n",
      "ep 1773: ep_len:662 episode reward: total was -4.080000. running mean: -19.990653\n",
      "ep 1773: ep_len:74 episode reward: total was 35.500000. running mean: -19.435747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1773: ep_len:187 episode reward: total was 90.500000. running mean: -18.336389\n",
      "ep 1773: ep_len:50 episode reward: total was 23.500000. running mean: -17.918026\n",
      "ep 1773: ep_len:1119 episode reward: total was -4.680000. running mean: -17.785645\n",
      "ep 1773: ep_len:2751 episode reward: total was -35.770000. running mean: -17.965489\n",
      "epsilon:0.009992 episode_count: 26724. steps_count: 28616008.000000\n",
      "ep 1774: ep_len:1453 episode reward: total was 11.060000. running mean: -17.675234\n",
      "ep 1774: ep_len:1731 episode reward: total was -80.710000. running mean: -18.305582\n",
      "ep 1774: ep_len:2986 episode reward: total was 13.940000. running mean: -17.983126\n",
      "ep 1774: ep_len:520 episode reward: total was 0.990000. running mean: -17.793394\n",
      "ep 1774: ep_len:60 episode reward: total was 28.500000. running mean: -17.330461\n",
      "ep 1774: ep_len:39 episode reward: total was 18.000000. running mean: -16.977156\n",
      "ep 1774: ep_len:765 episode reward: total was -27.200000. running mean: -17.079384\n",
      "ep 1774: ep_len:3711 episode reward: total was -68.730000. running mean: -17.595891\n",
      "ep 1774: ep_len:774 episode reward: total was -31.130000. running mean: -17.731232\n",
      "ep 1774: ep_len:859 episode reward: total was 4.720000. running mean: -17.506719\n",
      "ep 1774: ep_len:1022 episode reward: total was -12.900000. running mean: -17.460652\n",
      "ep 1774: ep_len:71 episode reward: total was 32.500000. running mean: -16.961046\n",
      "ep 1774: ep_len:111 episode reward: total was 51.000000. running mean: -16.281435\n",
      "ep 1774: ep_len:500 episode reward: total was 28.510000. running mean: -15.833521\n",
      "ep 1774: ep_len:2846 episode reward: total was -28.050000. running mean: -15.955686\n",
      "epsilon:0.009992 episode_count: 26739. steps_count: 28633456.000000\n",
      "ep 1775: ep_len:1098 episode reward: total was -18.020000. running mean: -15.976329\n",
      "ep 1775: ep_len:500 episode reward: total was 1.170000. running mean: -15.804865\n",
      "ep 1775: ep_len:47 episode reward: total was 20.500000. running mean: -15.441817\n",
      "ep 1775: ep_len:3000 episode reward: total was -36.500000. running mean: -15.652399\n",
      "ep 1775: ep_len:1508 episode reward: total was 20.910000. running mean: -15.286775\n",
      "ep 1775: ep_len:63 episode reward: total was 30.000000. running mean: -14.833907\n",
      "ep 1775: ep_len:1371 episode reward: total was 7.970000. running mean: -14.605868\n",
      "ep 1775: ep_len:3962 episode reward: total was -16.990000. running mean: -14.629709\n",
      "ep 1775: ep_len:561 episode reward: total was 18.520000. running mean: -14.298212\n",
      "ep 1775: ep_len:729 episode reward: total was 3.640000. running mean: -14.118830\n",
      "ep 1775: ep_len:1098 episode reward: total was -31.820000. running mean: -14.295842\n",
      "ep 1775: ep_len:73 episode reward: total was 32.000000. running mean: -13.832883\n",
      "ep 1775: ep_len:163 episode reward: total was 75.500000. running mean: -12.939554\n",
      "ep 1775: ep_len:35 episode reward: total was 16.000000. running mean: -12.650159\n",
      "ep 1775: ep_len:90 episode reward: total was 43.500000. running mean: -12.088657\n",
      "ep 1775: ep_len:662 episode reward: total was -2.800000. running mean: -11.995771\n",
      "ep 1775: ep_len:2869 episode reward: total was -30.790000. running mean: -12.183713\n",
      "epsilon:0.009992 episode_count: 26756. steps_count: 28651285.000000\n",
      "ep 1776: ep_len:685 episode reward: total was 0.620000. running mean: -12.055676\n",
      "ep 1776: ep_len:1015 episode reward: total was 20.850000. running mean: -11.726619\n",
      "ep 1776: ep_len:70 episode reward: total was 33.500000. running mean: -11.274353\n",
      "ep 1776: ep_len:2962 episode reward: total was -85.810000. running mean: -12.019709\n",
      "ep 1776: ep_len:678 episode reward: total was -14.210000. running mean: -12.041612\n",
      "ep 1776: ep_len:135 episode reward: total was 61.500000. running mean: -11.306196\n",
      "ep 1776: ep_len:1520 episode reward: total was 3.430000. running mean: -11.158834\n",
      "ep 1776: ep_len:350 episode reward: total was 12.070000. running mean: -10.926546\n",
      "ep 1776: ep_len:677 episode reward: total was -7.390000. running mean: -10.891180\n",
      "ep 1776: ep_len:814 episode reward: total was 22.070000. running mean: -10.561569\n",
      "ep 1776: ep_len:599 episode reward: total was -48.230000. running mean: -10.938253\n",
      "ep 1776: ep_len:82 episode reward: total was 36.500000. running mean: -10.463870\n",
      "ep 1776: ep_len:115 episode reward: total was 56.000000. running mean: -9.799232\n",
      "ep 1776: ep_len:1118 episode reward: total was 6.150000. running mean: -9.639739\n",
      "ep 1776: ep_len:2839 episode reward: total was -14.100000. running mean: -9.684342\n",
      "epsilon:0.009992 episode_count: 26771. steps_count: 28664944.000000\n",
      "ep 1777: ep_len:823 episode reward: total was -18.200000. running mean: -9.769499\n",
      "ep 1777: ep_len:755 episode reward: total was -13.510000. running mean: -9.806904\n",
      "ep 1777: ep_len:2959 episode reward: total was -32.640000. running mean: -10.035235\n",
      "ep 1777: ep_len:500 episode reward: total was -16.100000. running mean: -10.095882\n",
      "ep 1777: ep_len:87 episode reward: total was 39.000000. running mean: -9.604923\n",
      "ep 1777: ep_len:1047 episode reward: total was -59.390000. running mean: -10.102774\n",
      "ep 1777: ep_len:662 episode reward: total was 12.240000. running mean: -9.879346\n",
      "ep 1777: ep_len:521 episode reward: total was -97.980000. running mean: -10.760353\n",
      "ep 1777: ep_len:686 episode reward: total was 5.320000. running mean: -10.599549\n",
      "ep 1777: ep_len:1475 episode reward: total was 20.160000. running mean: -10.291954\n",
      "ep 1777: ep_len:72 episode reward: total was 31.500000. running mean: -9.874034\n",
      "ep 1777: ep_len:137 episode reward: total was 67.000000. running mean: -9.105294\n",
      "ep 1777: ep_len:1077 episode reward: total was -15.660000. running mean: -9.170841\n",
      "ep 1777: ep_len:2858 episode reward: total was -7.610000. running mean: -9.155233\n",
      "epsilon:0.009992 episode_count: 26785. steps_count: 28678603.000000\n",
      "ep 1778: ep_len:1080 episode reward: total was -23.620000. running mean: -9.299880\n",
      "ep 1778: ep_len:748 episode reward: total was -41.020000. running mean: -9.617082\n",
      "ep 1778: ep_len:2971 episode reward: total was 1.150000. running mean: -9.509411\n",
      "ep 1778: ep_len:725 episode reward: total was -2.140000. running mean: -9.435717\n",
      "ep 1778: ep_len:102 episode reward: total was 48.000000. running mean: -8.861359\n",
      "ep 1778: ep_len:880 episode reward: total was 61.510000. running mean: -8.157646\n",
      "ep 1778: ep_len:3589 episode reward: total was -63.890000. running mean: -8.714969\n",
      "ep 1778: ep_len:669 episode reward: total was 3.490000. running mean: -8.592920\n",
      "ep 1778: ep_len:741 episode reward: total was -2.050000. running mean: -8.527490\n",
      "ep 1778: ep_len:630 episode reward: total was -7.900000. running mean: -8.521216\n",
      "ep 1778: ep_len:185 episode reward: total was 89.010000. running mean: -7.545903\n",
      "ep 1778: ep_len:919 episode reward: total was -142.480000. running mean: -8.895244\n",
      "ep 1778: ep_len:2732 episode reward: total was -3.360000. running mean: -8.839892\n",
      "epsilon:0.009992 episode_count: 26798. steps_count: 28694574.000000\n",
      "ep 1779: ep_len:1138 episode reward: total was -4.950000. running mean: -8.800993\n",
      "ep 1779: ep_len:1276 episode reward: total was -36.900000. running mean: -9.081983\n",
      "ep 1779: ep_len:2947 episode reward: total was -27.170000. running mean: -9.262863\n",
      "ep 1779: ep_len:1241 episode reward: total was -22.280000. running mean: -9.393035\n",
      "ep 1779: ep_len:59 episode reward: total was 28.000000. running mean: -9.019104\n",
      "ep 1779: ep_len:1391 episode reward: total was -119.870000. running mean: -10.127613\n",
      "ep 1779: ep_len:3725 episode reward: total was -10.650000. running mean: -10.132837\n",
      "ep 1779: ep_len:1238 episode reward: total was -38.780000. running mean: -10.419309\n",
      "ep 1779: ep_len:734 episode reward: total was 41.970000. running mean: -9.895416\n",
      "ep 1779: ep_len:552 episode reward: total was -32.020000. running mean: -10.116662\n",
      "ep 1779: ep_len:182 episode reward: total was 85.000000. running mean: -9.165495\n",
      "ep 1779: ep_len:37 episode reward: total was 17.000000. running mean: -8.903840\n",
      "ep 1779: ep_len:500 episode reward: total was -4.380000. running mean: -8.858602\n",
      "ep 1779: ep_len:2823 episode reward: total was -18.000000. running mean: -8.950016\n",
      "epsilon:0.009992 episode_count: 26812. steps_count: 28712417.000000\n",
      "ep 1780: ep_len:762 episode reward: total was -20.710000. running mean: -9.067615\n",
      "ep 1780: ep_len:879 episode reward: total was 11.280000. running mean: -8.864139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1780: ep_len:3047 episode reward: total was -27.410000. running mean: -9.049598\n",
      "ep 1780: ep_len:508 episode reward: total was -1.150000. running mean: -8.970602\n",
      "ep 1780: ep_len:63 episode reward: total was 28.500000. running mean: -8.595896\n",
      "ep 1780: ep_len:650 episode reward: total was 1.280000. running mean: -8.497137\n",
      "ep 1780: ep_len:3591 episode reward: total was -14.380000. running mean: -8.555965\n",
      "ep 1780: ep_len:568 episode reward: total was 5.760000. running mean: -8.412806\n",
      "ep 1780: ep_len:805 episode reward: total was 22.190000. running mean: -8.106778\n",
      "ep 1780: ep_len:918 episode reward: total was 34.240000. running mean: -7.683310\n",
      "ep 1780: ep_len:78 episode reward: total was 34.500000. running mean: -7.261477\n",
      "ep 1780: ep_len:55 episode reward: total was 26.000000. running mean: -6.928862\n",
      "ep 1780: ep_len:1103 episode reward: total was -6.310000. running mean: -6.922674\n",
      "ep 1780: ep_len:2911 episode reward: total was 8.840000. running mean: -6.765047\n",
      "ep 1780: ep_len:52 episode reward: total was 24.500000. running mean: -6.452396\n",
      "epsilon:0.009992 episode_count: 26827. steps_count: 28728407.000000\n",
      "ep 1781: ep_len:1104 episode reward: total was -10.340000. running mean: -6.491272\n",
      "ep 1781: ep_len:746 episode reward: total was 0.930000. running mean: -6.417060\n",
      "ep 1781: ep_len:48 episode reward: total was 21.000000. running mean: -6.142889\n",
      "ep 1781: ep_len:2913 episode reward: total was -56.310000. running mean: -6.644560\n",
      "ep 1781: ep_len:500 episode reward: total was 17.110000. running mean: -6.407015\n",
      "ep 1781: ep_len:97 episode reward: total was 44.000000. running mean: -5.902944\n",
      "ep 1781: ep_len:1385 episode reward: total was -119.640000. running mean: -7.040315\n",
      "ep 1781: ep_len:655 episode reward: total was 14.010000. running mean: -6.829812\n",
      "ep 1781: ep_len:671 episode reward: total was 5.530000. running mean: -6.706214\n",
      "ep 1781: ep_len:651 episode reward: total was -24.420000. running mean: -6.883352\n",
      "ep 1781: ep_len:1135 episode reward: total was -3.040000. running mean: -6.844918\n",
      "ep 1781: ep_len:55 episode reward: total was 24.500000. running mean: -6.531469\n",
      "ep 1781: ep_len:30 episode reward: total was 13.500000. running mean: -6.331154\n",
      "ep 1781: ep_len:91 episode reward: total was 42.500000. running mean: -5.842843\n",
      "ep 1781: ep_len:1130 episode reward: total was -15.130000. running mean: -5.935714\n",
      "ep 1781: ep_len:2824 episode reward: total was -15.390000. running mean: -6.030257\n",
      "ep 1781: ep_len:67 episode reward: total was 32.000000. running mean: -5.649954\n",
      "epsilon:0.009992 episode_count: 26844. steps_count: 28742509.000000\n",
      "ep 1782: ep_len:632 episode reward: total was -25.160000. running mean: -5.845055\n",
      "ep 1782: ep_len:782 episode reward: total was -0.640000. running mean: -5.793004\n",
      "ep 1782: ep_len:2906 episode reward: total was -28.490000. running mean: -6.019974\n",
      "ep 1782: ep_len:844 episode reward: total was -12.550000. running mean: -6.085275\n",
      "ep 1782: ep_len:640 episode reward: total was 8.680000. running mean: -5.937622\n",
      "ep 1782: ep_len:663 episode reward: total was 25.660000. running mean: -5.621646\n",
      "ep 1782: ep_len:1240 episode reward: total was -35.240000. running mean: -5.917829\n",
      "ep 1782: ep_len:743 episode reward: total was 43.100000. running mean: -5.427651\n",
      "ep 1782: ep_len:1118 episode reward: total was -9.190000. running mean: -5.465274\n",
      "ep 1782: ep_len:83 episode reward: total was 40.000000. running mean: -5.010622\n",
      "ep 1782: ep_len:104 episode reward: total was 47.500000. running mean: -4.485515\n",
      "ep 1782: ep_len:1483 episode reward: total was -41.250000. running mean: -4.853160\n",
      "ep 1782: ep_len:2834 episode reward: total was -1.210000. running mean: -4.816729\n",
      "epsilon:0.009992 episode_count: 26857. steps_count: 28756581.000000\n",
      "ep 1783: ep_len:1522 episode reward: total was 4.530000. running mean: -4.723261\n",
      "ep 1783: ep_len:215 episode reward: total was 4.930000. running mean: -4.626729\n",
      "ep 1783: ep_len:58 episode reward: total was 26.000000. running mean: -4.320461\n",
      "ep 1783: ep_len:3022 episode reward: total was -1.950000. running mean: -4.296757\n",
      "ep 1783: ep_len:791 episode reward: total was 17.100000. running mean: -4.082789\n",
      "ep 1783: ep_len:79 episode reward: total was 38.000000. running mean: -3.661961\n",
      "ep 1783: ep_len:1441 episode reward: total was -15.080000. running mean: -3.776142\n",
      "ep 1783: ep_len:363 episode reward: total was 16.240000. running mean: -3.575980\n",
      "ep 1783: ep_len:531 episode reward: total was -50.410000. running mean: -4.044321\n",
      "ep 1783: ep_len:746 episode reward: total was 1.200000. running mean: -3.991877\n",
      "ep 1783: ep_len:555 episode reward: total was 30.500000. running mean: -3.646959\n",
      "ep 1783: ep_len:78 episode reward: total was 37.500000. running mean: -3.235489\n",
      "ep 1783: ep_len:613 episode reward: total was -9.750000. running mean: -3.300634\n",
      "ep 1783: ep_len:2886 episode reward: total was -6.170000. running mean: -3.329328\n",
      "ep 1783: ep_len:75 episode reward: total was 34.500000. running mean: -2.951034\n",
      "epsilon:0.009992 episode_count: 26872. steps_count: 28769556.000000\n",
      "ep 1784: ep_len:912 episode reward: total was -176.310000. running mean: -4.684624\n",
      "ep 1784: ep_len:697 episode reward: total was -30.070000. running mean: -4.938478\n",
      "ep 1784: ep_len:2975 episode reward: total was -46.770000. running mean: -5.356793\n",
      "ep 1784: ep_len:684 episode reward: total was 12.390000. running mean: -5.179325\n",
      "ep 1784: ep_len:95 episode reward: total was 46.000000. running mean: -4.667532\n",
      "ep 1784: ep_len:1109 episode reward: total was -7.260000. running mean: -4.693457\n",
      "ep 1784: ep_len:3620 episode reward: total was -65.970000. running mean: -5.306222\n",
      "ep 1784: ep_len:666 episode reward: total was -6.110000. running mean: -5.314260\n",
      "ep 1784: ep_len:886 episode reward: total was 59.140000. running mean: -4.669717\n",
      "ep 1784: ep_len:634 episode reward: total was -95.690000. running mean: -5.579920\n",
      "ep 1784: ep_len:94 episode reward: total was 45.500000. running mean: -5.069121\n",
      "ep 1784: ep_len:1145 episode reward: total was -22.050000. running mean: -5.238930\n",
      "ep 1784: ep_len:2818 episode reward: total was -7.490000. running mean: -5.261440\n",
      "epsilon:0.009992 episode_count: 26885. steps_count: 28785891.000000\n",
      "ep 1785: ep_len:1139 episode reward: total was -12.010000. running mean: -5.328926\n",
      "ep 1785: ep_len:991 episode reward: total was 32.340000. running mean: -4.952237\n",
      "ep 1785: ep_len:2915 episode reward: total was -30.390000. running mean: -5.206614\n",
      "ep 1785: ep_len:619 episode reward: total was -8.930000. running mean: -5.243848\n",
      "ep 1785: ep_len:37 episode reward: total was 15.500000. running mean: -5.036410\n",
      "ep 1785: ep_len:121 episode reward: total was 53.000000. running mean: -4.456046\n",
      "ep 1785: ep_len:636 episode reward: total was -21.760000. running mean: -4.629085\n",
      "ep 1785: ep_len:335 episode reward: total was 16.970000. running mean: -4.413094\n",
      "ep 1785: ep_len:1479 episode reward: total was -86.370000. running mean: -5.232663\n",
      "ep 1785: ep_len:922 episode reward: total was 63.880000. running mean: -4.541537\n",
      "ep 1785: ep_len:999 episode reward: total was 17.110000. running mean: -4.325021\n",
      "ep 1785: ep_len:137 episode reward: total was 65.500000. running mean: -3.626771\n",
      "ep 1785: ep_len:527 episode reward: total was 11.510000. running mean: -3.475403\n",
      "ep 1785: ep_len:2894 episode reward: total was -3.300000. running mean: -3.473649\n",
      "ep 1785: ep_len:51 episode reward: total was 24.000000. running mean: -3.198913\n",
      "epsilon:0.009992 episode_count: 26900. steps_count: 28799693.000000\n",
      "ep 1786: ep_len:736 episode reward: total was -54.260000. running mean: -3.709524\n",
      "ep 1786: ep_len:987 episode reward: total was 10.190000. running mean: -3.570529\n",
      "ep 1786: ep_len:47 episode reward: total was 20.500000. running mean: -3.329823\n",
      "ep 1786: ep_len:2951 episode reward: total was -14.260000. running mean: -3.439125\n",
      "ep 1786: ep_len:831 episode reward: total was 34.750000. running mean: -3.057234\n",
      "ep 1786: ep_len:111 episode reward: total was 52.500000. running mean: -2.501661\n",
      "ep 1786: ep_len:874 episode reward: total was 14.230000. running mean: -2.334345\n",
      "ep 1786: ep_len:4062 episode reward: total was -2991.630000. running mean: -32.227301\n",
      "ep 1786: ep_len:775 episode reward: total was -15.570000. running mean: -32.060728\n",
      "ep 1786: ep_len:726 episode reward: total was 8.110000. running mean: -31.659021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1786: ep_len:683 episode reward: total was -9.500000. running mean: -31.437431\n",
      "ep 1786: ep_len:198 episode reward: total was 97.500000. running mean: -30.148057\n",
      "ep 1786: ep_len:92 episode reward: total was 44.500000. running mean: -29.401576\n",
      "ep 1786: ep_len:1074 episode reward: total was -14.400000. running mean: -29.251560\n",
      "ep 1786: ep_len:2858 episode reward: total was -23.860000. running mean: -29.197645\n",
      "epsilon:0.009992 episode_count: 26915. steps_count: 28816698.000000\n",
      "ep 1787: ep_len:952 episode reward: total was -78.760000. running mean: -29.693268\n",
      "ep 1787: ep_len:692 episode reward: total was -39.710000. running mean: -29.793435\n",
      "ep 1787: ep_len:3086 episode reward: total was -30.320000. running mean: -29.798701\n",
      "ep 1787: ep_len:654 episode reward: total was -1.110000. running mean: -29.511814\n",
      "ep 1787: ep_len:32 episode reward: total was 14.500000. running mean: -29.071696\n",
      "ep 1787: ep_len:92 episode reward: total was 43.000000. running mean: -28.350979\n",
      "ep 1787: ep_len:71 episode reward: total was 34.000000. running mean: -27.727469\n",
      "ep 1787: ep_len:500 episode reward: total was -29.440000. running mean: -27.744595\n",
      "ep 1787: ep_len:3853 episode reward: total was -68.360000. running mean: -28.150749\n",
      "ep 1787: ep_len:701 episode reward: total was -11.340000. running mean: -27.982641\n",
      "ep 1787: ep_len:639 episode reward: total was 21.380000. running mean: -27.489015\n",
      "ep 1787: ep_len:682 episode reward: total was -11.010000. running mean: -27.324225\n",
      "ep 1787: ep_len:821 episode reward: total was -26.190000. running mean: -27.312882\n",
      "ep 1787: ep_len:2892 episode reward: total was -4.880000. running mean: -27.088553\n",
      "ep 1787: ep_len:56 episode reward: total was 25.000000. running mean: -26.567668\n",
      "epsilon:0.009992 episode_count: 26930. steps_count: 28832421.000000\n",
      "ep 1788: ep_len:1167 episode reward: total was -21.250000. running mean: -26.514491\n",
      "ep 1788: ep_len:1306 episode reward: total was -68.920000. running mean: -26.938546\n",
      "ep 1788: ep_len:2900 episode reward: total was -24.270000. running mean: -26.911861\n",
      "ep 1788: ep_len:537 episode reward: total was 1.960000. running mean: -26.623142\n",
      "ep 1788: ep_len:124 episode reward: total was 53.000000. running mean: -25.826911\n",
      "ep 1788: ep_len:50 episode reward: total was 20.500000. running mean: -25.363642\n",
      "ep 1788: ep_len:971 episode reward: total was -33.550000. running mean: -25.445505\n",
      "ep 1788: ep_len:3950 episode reward: total was -53.210000. running mean: -25.723150\n",
      "ep 1788: ep_len:519 episode reward: total was 4.010000. running mean: -25.425819\n",
      "ep 1788: ep_len:735 episode reward: total was 1.030000. running mean: -25.161261\n",
      "ep 1788: ep_len:609 episode reward: total was 60.280000. running mean: -24.306848\n",
      "ep 1788: ep_len:89 episode reward: total was 41.500000. running mean: -23.648780\n",
      "ep 1788: ep_len:78 episode reward: total was 37.500000. running mean: -23.037292\n",
      "ep 1788: ep_len:813 episode reward: total was -15.040000. running mean: -22.957319\n",
      "ep 1788: ep_len:2916 episode reward: total was -11.370000. running mean: -22.841446\n",
      "epsilon:0.009992 episode_count: 26945. steps_count: 28849185.000000\n",
      "ep 1789: ep_len:1088 episode reward: total was -18.580000. running mean: -22.798831\n",
      "ep 1789: ep_len:616 episode reward: total was -46.530000. running mean: -23.036143\n",
      "ep 1789: ep_len:69 episode reward: total was 31.500000. running mean: -22.490781\n",
      "ep 1789: ep_len:2961 episode reward: total was 12.740000. running mean: -22.138474\n",
      "ep 1789: ep_len:616 episode reward: total was -1.080000. running mean: -21.927889\n",
      "ep 1789: ep_len:38 episode reward: total was 16.000000. running mean: -21.548610\n",
      "ep 1789: ep_len:133 episode reward: total was 65.000000. running mean: -20.683124\n",
      "ep 1789: ep_len:1383 episode reward: total was -164.220000. running mean: -22.118493\n",
      "ep 1789: ep_len:3741 episode reward: total was -42.900000. running mean: -22.326308\n",
      "ep 1789: ep_len:577 episode reward: total was -2.480000. running mean: -22.127845\n",
      "ep 1789: ep_len:816 episode reward: total was 38.620000. running mean: -21.520366\n",
      "ep 1789: ep_len:658 episode reward: total was 9.780000. running mean: -21.207363\n",
      "ep 1789: ep_len:90 episode reward: total was 42.000000. running mean: -20.575289\n",
      "ep 1789: ep_len:766 episode reward: total was -100.580000. running mean: -21.375336\n",
      "ep 1789: ep_len:2883 episode reward: total was -39.410000. running mean: -21.555683\n",
      "epsilon:0.009992 episode_count: 26960. steps_count: 28865620.000000\n",
      "ep 1790: ep_len:880 episode reward: total was 19.620000. running mean: -21.143926\n",
      "ep 1790: ep_len:725 episode reward: total was -19.830000. running mean: -21.130787\n",
      "ep 1790: ep_len:49 episode reward: total was 21.500000. running mean: -20.704479\n",
      "ep 1790: ep_len:2996 episode reward: total was 12.880000. running mean: -20.368634\n",
      "ep 1790: ep_len:871 episode reward: total was 2.810000. running mean: -20.136848\n",
      "ep 1790: ep_len:1470 episode reward: total was -13.110000. running mean: -20.066579\n",
      "ep 1790: ep_len:641 episode reward: total was 11.080000. running mean: -19.755113\n",
      "ep 1790: ep_len:782 episode reward: total was -148.610000. running mean: -21.043662\n",
      "ep 1790: ep_len:651 episode reward: total was 14.670000. running mean: -20.686526\n",
      "ep 1790: ep_len:610 episode reward: total was -11.760000. running mean: -20.597260\n",
      "ep 1790: ep_len:72 episode reward: total was 31.500000. running mean: -20.076288\n",
      "ep 1790: ep_len:685 episode reward: total was 27.080000. running mean: -19.604725\n",
      "ep 1790: ep_len:2897 episode reward: total was -43.840000. running mean: -19.847078\n",
      "epsilon:0.009992 episode_count: 26973. steps_count: 28878949.000000\n",
      "ep 1791: ep_len:1110 episode reward: total was -5.230000. running mean: -19.700907\n",
      "ep 1791: ep_len:796 episode reward: total was -2.310000. running mean: -19.526998\n",
      "ep 1791: ep_len:65 episode reward: total was 26.500000. running mean: -19.066728\n",
      "ep 1791: ep_len:2980 episode reward: total was -68.860000. running mean: -19.564660\n",
      "ep 1791: ep_len:501 episode reward: total was 1.180000. running mean: -19.357214\n",
      "ep 1791: ep_len:64 episode reward: total was 29.000000. running mean: -18.873642\n",
      "ep 1791: ep_len:158 episode reward: total was 76.000000. running mean: -17.924905\n",
      "ep 1791: ep_len:1474 episode reward: total was 17.050000. running mean: -17.575156\n",
      "ep 1791: ep_len:3662 episode reward: total was -35.890000. running mean: -17.758305\n",
      "ep 1791: ep_len:784 episode reward: total was -52.180000. running mean: -18.102522\n",
      "ep 1791: ep_len:770 episode reward: total was -5.750000. running mean: -17.978996\n",
      "ep 1791: ep_len:496 episode reward: total was 43.690000. running mean: -17.362306\n",
      "ep 1791: ep_len:132 episode reward: total was 64.500000. running mean: -16.543683\n",
      "ep 1791: ep_len:670 episode reward: total was -4.430000. running mean: -16.422547\n",
      "ep 1791: ep_len:2828 episode reward: total was 0.350000. running mean: -16.254821\n",
      "ep 1791: ep_len:33 episode reward: total was 15.000000. running mean: -15.942273\n",
      "epsilon:0.009992 episode_count: 26989. steps_count: 28895472.000000\n",
      "ep 1792: ep_len:619 episode reward: total was 4.530000. running mean: -15.737550\n",
      "ep 1792: ep_len:640 episode reward: total was -31.140000. running mean: -15.891575\n",
      "ep 1792: ep_len:2851 episode reward: total was -28.130000. running mean: -16.013959\n",
      "ep 1792: ep_len:529 episode reward: total was -14.960000. running mean: -16.003419\n",
      "ep 1792: ep_len:117 episode reward: total was 55.500000. running mean: -15.288385\n",
      "ep 1792: ep_len:74 episode reward: total was 35.500000. running mean: -14.780501\n",
      "ep 1792: ep_len:1018 episode reward: total was -2.170000. running mean: -14.654396\n",
      "ep 1792: ep_len:3765 episode reward: total was 12.300000. running mean: -14.384852\n",
      "ep 1792: ep_len:2471 episode reward: total was -183.720000. running mean: -16.078204\n",
      "ep 1792: ep_len:698 episode reward: total was 34.140000. running mean: -15.576022\n",
      "ep 1792: ep_len:608 episode reward: total was -1.160000. running mean: -15.431862\n",
      "ep 1792: ep_len:819 episode reward: total was 2.350000. running mean: -15.254043\n",
      "ep 1792: ep_len:2899 episode reward: total was -15.650000. running mean: -15.258002\n",
      "epsilon:0.009992 episode_count: 27002. steps_count: 28912580.000000\n",
      "ep 1793: ep_len:1426 episode reward: total was 18.190000. running mean: -14.923522\n",
      "ep 1793: ep_len:650 episode reward: total was 13.860000. running mean: -14.635687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1793: ep_len:3001 episode reward: total was -8.460000. running mean: -14.573930\n",
      "ep 1793: ep_len:500 episode reward: total was 23.360000. running mean: -14.194591\n",
      "ep 1793: ep_len:68 episode reward: total was 32.500000. running mean: -13.727645\n",
      "ep 1793: ep_len:112 episode reward: total was 53.000000. running mean: -13.060369\n",
      "ep 1793: ep_len:694 episode reward: total was -11.410000. running mean: -13.043865\n",
      "ep 1793: ep_len:363 episode reward: total was 24.320000. running mean: -12.670226\n",
      "ep 1793: ep_len:648 episode reward: total was -1.770000. running mean: -12.561224\n",
      "ep 1793: ep_len:783 episode reward: total was 18.690000. running mean: -12.248712\n",
      "ep 1793: ep_len:500 episode reward: total was 10.620000. running mean: -12.020025\n",
      "ep 1793: ep_len:67 episode reward: total was 32.000000. running mean: -11.579824\n",
      "ep 1793: ep_len:104 episode reward: total was 49.000000. running mean: -10.974026\n",
      "ep 1793: ep_len:88 episode reward: total was 39.500000. running mean: -10.469286\n",
      "ep 1793: ep_len:1070 episode reward: total was -11.840000. running mean: -10.482993\n",
      "ep 1793: ep_len:2901 episode reward: total was -7.150000. running mean: -10.449663\n",
      "ep 1793: ep_len:39 episode reward: total was 16.500000. running mean: -10.180167\n",
      "epsilon:0.009992 episode_count: 27019. steps_count: 28925594.000000\n",
      "ep 1794: ep_len:1392 episode reward: total was 19.230000. running mean: -9.886065\n",
      "ep 1794: ep_len:614 episode reward: total was 16.100000. running mean: -9.626204\n",
      "ep 1794: ep_len:66 episode reward: total was 31.500000. running mean: -9.214942\n",
      "ep 1794: ep_len:2935 episode reward: total was -19.240000. running mean: -9.315193\n",
      "ep 1794: ep_len:785 episode reward: total was 17.040000. running mean: -9.051641\n",
      "ep 1794: ep_len:48 episode reward: total was 21.000000. running mean: -8.751124\n",
      "ep 1794: ep_len:882 episode reward: total was 26.160000. running mean: -8.402013\n",
      "ep 1794: ep_len:3832 episode reward: total was -62.780000. running mean: -8.945793\n",
      "ep 1794: ep_len:973 episode reward: total was -41.830000. running mean: -9.274635\n",
      "ep 1794: ep_len:834 episode reward: total was 43.420000. running mean: -8.747689\n",
      "ep 1794: ep_len:1011 episode reward: total was 14.570000. running mean: -8.514512\n",
      "ep 1794: ep_len:112 episode reward: total was 54.010000. running mean: -7.889267\n",
      "ep 1794: ep_len:63 episode reward: total was 30.000000. running mean: -7.510374\n",
      "ep 1794: ep_len:998 episode reward: total was -17.820000. running mean: -7.613470\n",
      "ep 1794: ep_len:2824 episode reward: total was -16.300000. running mean: -7.700336\n",
      "epsilon:0.009992 episode_count: 27034. steps_count: 28942963.000000\n",
      "ep 1795: ep_len:725 episode reward: total was -74.730000. running mean: -8.370632\n",
      "ep 1795: ep_len:633 episode reward: total was 15.280000. running mean: -8.134126\n",
      "ep 1795: ep_len:3021 episode reward: total was -49.410000. running mean: -8.546885\n",
      "ep 1795: ep_len:609 episode reward: total was -8.220000. running mean: -8.543616\n",
      "ep 1795: ep_len:41 episode reward: total was 19.000000. running mean: -8.268180\n",
      "ep 1795: ep_len:120 episode reward: total was 58.500000. running mean: -7.600498\n",
      "ep 1795: ep_len:821 episode reward: total was 16.490000. running mean: -7.359593\n",
      "ep 1795: ep_len:646 episode reward: total was 24.050000. running mean: -7.045497\n",
      "ep 1795: ep_len:631 episode reward: total was -57.980000. running mean: -7.554842\n",
      "ep 1795: ep_len:782 episode reward: total was 26.000000. running mean: -7.219294\n",
      "ep 1795: ep_len:571 episode reward: total was 33.420000. running mean: -6.812901\n",
      "ep 1795: ep_len:96 episode reward: total was 45.000000. running mean: -6.294772\n",
      "ep 1795: ep_len:113 episode reward: total was 50.500000. running mean: -5.726824\n",
      "ep 1795: ep_len:786 episode reward: total was -63.010000. running mean: -6.299656\n",
      "ep 1795: ep_len:2872 episode reward: total was -11.050000. running mean: -6.347159\n",
      "epsilon:0.009992 episode_count: 27049. steps_count: 28955430.000000\n",
      "ep 1796: ep_len:612 episode reward: total was -11.160000. running mean: -6.395288\n",
      "ep 1796: ep_len:503 episode reward: total was -10.290000. running mean: -6.434235\n",
      "ep 1796: ep_len:61 episode reward: total was 25.510000. running mean: -6.114792\n",
      "ep 1796: ep_len:93 episode reward: total was 45.000000. running mean: -5.603644\n",
      "ep 1796: ep_len:538 episode reward: total was -17.010000. running mean: -5.717708\n",
      "ep 1796: ep_len:98 episode reward: total was 47.500000. running mean: -5.185531\n",
      "ep 1796: ep_len:82 episode reward: total was 39.500000. running mean: -4.738676\n",
      "ep 1796: ep_len:44 episode reward: total was 20.500000. running mean: -4.486289\n",
      "ep 1796: ep_len:714 episode reward: total was -36.740000. running mean: -4.808826\n",
      "ep 1796: ep_len:342 episode reward: total was 2.590000. running mean: -4.734838\n",
      "ep 1796: ep_len:672 episode reward: total was 0.490000. running mean: -4.682589\n",
      "ep 1796: ep_len:690 episode reward: total was -57.760000. running mean: -5.213363\n",
      "ep 1796: ep_len:572 episode reward: total was -9.200000. running mean: -5.253230\n",
      "ep 1796: ep_len:49 episode reward: total was 23.000000. running mean: -4.970697\n",
      "ep 1796: ep_len:1058 episode reward: total was 10.450000. running mean: -4.816491\n",
      "ep 1796: ep_len:2867 episode reward: total was -28.730000. running mean: -5.055626\n",
      "ep 1796: ep_len:58 episode reward: total was 27.500000. running mean: -4.730069\n",
      "epsilon:0.009992 episode_count: 27066. steps_count: 28964483.000000\n",
      "ep 1797: ep_len:649 episode reward: total was -24.990000. running mean: -4.932669\n",
      "ep 1797: ep_len:657 episode reward: total was -44.100000. running mean: -5.324342\n",
      "ep 1797: ep_len:3055 episode reward: total was -94.360000. running mean: -6.214699\n",
      "ep 1797: ep_len:562 episode reward: total was 0.400000. running mean: -6.148552\n",
      "ep 1797: ep_len:67 episode reward: total was 32.000000. running mean: -5.767066\n",
      "ep 1797: ep_len:69 episode reward: total was 33.000000. running mean: -5.379395\n",
      "ep 1797: ep_len:750 episode reward: total was -14.890000. running mean: -5.474501\n",
      "ep 1797: ep_len:3656 episode reward: total was -18.780000. running mean: -5.607556\n",
      "ep 1797: ep_len:601 episode reward: total was -15.370000. running mean: -5.705181\n",
      "ep 1797: ep_len:902 episode reward: total was 60.000000. running mean: -5.048129\n",
      "ep 1797: ep_len:596 episode reward: total was -3.300000. running mean: -5.030648\n",
      "ep 1797: ep_len:721 episode reward: total was 8.530000. running mean: -4.895041\n",
      "ep 1797: ep_len:2785 episode reward: total was -26.830000. running mean: -5.114391\n",
      "ep 1797: ep_len:47 episode reward: total was 22.000000. running mean: -4.843247\n",
      "epsilon:0.009992 episode_count: 27080. steps_count: 28979600.000000\n",
      "ep 1798: ep_len:1433 episode reward: total was -19.540000. running mean: -4.990214\n",
      "ep 1798: ep_len:500 episode reward: total was 13.320000. running mean: -4.807112\n",
      "ep 1798: ep_len:2893 episode reward: total was -86.500000. running mean: -5.624041\n",
      "ep 1798: ep_len:815 episode reward: total was -3.140000. running mean: -5.599201\n",
      "ep 1798: ep_len:51 episode reward: total was 22.500000. running mean: -5.318209\n",
      "ep 1798: ep_len:81 episode reward: total was 36.000000. running mean: -4.905027\n",
      "ep 1798: ep_len:1859 episode reward: total was -73.580000. running mean: -5.591776\n",
      "ep 1798: ep_len:3840 episode reward: total was 11.580000. running mean: -5.420059\n",
      "ep 1798: ep_len:1219 episode reward: total was -55.650000. running mean: -5.922358\n",
      "ep 1798: ep_len:7173 episode reward: total was -83.430000. running mean: -6.697435\n",
      "ep 1798: ep_len:500 episode reward: total was -1.380000. running mean: -6.644260\n",
      "ep 1798: ep_len:89 episode reward: total was 43.000000. running mean: -6.147818\n",
      "ep 1798: ep_len:115 episode reward: total was 56.000000. running mean: -5.526339\n",
      "ep 1798: ep_len:1457 episode reward: total was 3.750000. running mean: -5.433576\n",
      "ep 1798: ep_len:2832 episode reward: total was 1.250000. running mean: -5.366740\n",
      "epsilon:0.009992 episode_count: 27095. steps_count: 29004457.000000\n",
      "ep 1799: ep_len:764 episode reward: total was -6.600000. running mean: -5.379073\n",
      "ep 1799: ep_len:740 episode reward: total was -13.950000. running mean: -5.464782\n",
      "ep 1799: ep_len:2920 episode reward: total was -61.780000. running mean: -6.027934\n",
      "ep 1799: ep_len:556 episode reward: total was -9.140000. running mean: -6.059055\n",
      "ep 1799: ep_len:45 episode reward: total was 19.500000. running mean: -5.803464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1799: ep_len:69 episode reward: total was 33.000000. running mean: -5.415430\n",
      "ep 1799: ep_len:1089 episode reward: total was -2.100000. running mean: -5.382275\n",
      "ep 1799: ep_len:3813 episode reward: total was -3.020000. running mean: -5.358653\n",
      "ep 1799: ep_len:605 episode reward: total was -21.300000. running mean: -5.518066\n",
      "ep 1799: ep_len:775 episode reward: total was 8.110000. running mean: -5.381786\n",
      "ep 1799: ep_len:742 episode reward: total was 24.010000. running mean: -5.087868\n",
      "ep 1799: ep_len:121 episode reward: total was 59.000000. running mean: -4.446989\n",
      "ep 1799: ep_len:571 episode reward: total was -11.630000. running mean: -4.518819\n",
      "ep 1799: ep_len:2932 episode reward: total was 8.370000. running mean: -4.389931\n",
      "ep 1799: ep_len:49 episode reward: total was 23.000000. running mean: -4.116032\n",
      "epsilon:0.009992 episode_count: 27110. steps_count: 29020248.000000\n",
      "ep 1800: ep_len:996 episode reward: total was -63.490000. running mean: -4.709771\n",
      "ep 1800: ep_len:780 episode reward: total was -20.650000. running mean: -4.869174\n",
      "ep 1800: ep_len:3024 episode reward: total was -2.570000. running mean: -4.846182\n",
      "ep 1800: ep_len:1252 episode reward: total was -125.010000. running mean: -6.047820\n",
      "ep 1800: ep_len:65 episode reward: total was 31.000000. running mean: -5.677342\n",
      "ep 1800: ep_len:500 episode reward: total was 8.970000. running mean: -5.530868\n",
      "ep 1800: ep_len:630 episode reward: total was 20.860000. running mean: -5.266960\n",
      "ep 1800: ep_len:1619 episode reward: total was -87.690000. running mean: -6.091190\n",
      "ep 1800: ep_len:757 episode reward: total was 20.830000. running mean: -5.821978\n",
      "ep 1800: ep_len:500 episode reward: total was 25.840000. running mean: -5.505358\n",
      "ep 1800: ep_len:60 episode reward: total was 28.500000. running mean: -5.165305\n",
      "ep 1800: ep_len:676 episode reward: total was 27.500000. running mean: -4.838652\n",
      "ep 1800: ep_len:2868 episode reward: total was -13.910000. running mean: -4.929365\n",
      "ep 1800: ep_len:56 episode reward: total was 26.500000. running mean: -4.615072\n",
      "epsilon:0.009992 episode_count: 27124. steps_count: 29034031.000000\n",
      "ep 1801: ep_len:832 episode reward: total was 18.710000. running mean: -4.381821\n",
      "ep 1801: ep_len:691 episode reward: total was -32.130000. running mean: -4.659303\n",
      "ep 1801: ep_len:80 episode reward: total was 37.000000. running mean: -4.242710\n",
      "ep 1801: ep_len:3024 episode reward: total was -2.380000. running mean: -4.224083\n",
      "ep 1801: ep_len:666 episode reward: total was 1.290000. running mean: -4.168942\n",
      "ep 1801: ep_len:59 episode reward: total was 26.500000. running mean: -3.862252\n",
      "ep 1801: ep_len:105 episode reward: total was 51.000000. running mean: -3.313630\n",
      "ep 1801: ep_len:1397 episode reward: total was -149.210000. running mean: -4.772594\n",
      "ep 1801: ep_len:604 episode reward: total was 21.550000. running mean: -4.509368\n",
      "ep 1801: ep_len:1182 episode reward: total was -83.870000. running mean: -5.302974\n",
      "ep 1801: ep_len:752 episode reward: total was 5.700000. running mean: -5.192944\n",
      "ep 1801: ep_len:1062 episode reward: total was 22.270000. running mean: -4.918315\n",
      "ep 1801: ep_len:100 episode reward: total was 47.000000. running mean: -4.399132\n",
      "ep 1801: ep_len:45 episode reward: total was 19.500000. running mean: -4.160140\n",
      "ep 1801: ep_len:694 episode reward: total was 9.200000. running mean: -4.026539\n",
      "ep 1801: ep_len:2937 episode reward: total was -13.340000. running mean: -4.119673\n",
      "ep 1801: ep_len:61 episode reward: total was 29.000000. running mean: -3.788477\n",
      "epsilon:0.009992 episode_count: 27141. steps_count: 29048322.000000\n",
      "ep 1802: ep_len:777 episode reward: total was -70.170000. running mean: -4.452292\n",
      "ep 1802: ep_len:197 episode reward: total was 13.870000. running mean: -4.269069\n",
      "ep 1802: ep_len:2953 episode reward: total was -79.840000. running mean: -5.024778\n",
      "ep 1802: ep_len:771 episode reward: total was -28.820000. running mean: -5.262731\n",
      "ep 1802: ep_len:78 episode reward: total was 37.500000. running mean: -4.835103\n",
      "ep 1802: ep_len:1131 episode reward: total was -3.750000. running mean: -4.824252\n",
      "ep 1802: ep_len:605 episode reward: total was 28.170000. running mean: -4.494310\n",
      "ep 1802: ep_len:1228 episode reward: total was -30.950000. running mean: -4.758867\n",
      "ep 1802: ep_len:845 episode reward: total was 38.830000. running mean: -4.322978\n",
      "ep 1802: ep_len:1504 episode reward: total was -8.240000. running mean: -4.362148\n",
      "ep 1802: ep_len:121 episode reward: total was 59.000000. running mean: -3.728527\n",
      "ep 1802: ep_len:973 episode reward: total was -69.870000. running mean: -4.389941\n",
      "ep 1802: ep_len:2848 episode reward: total was 0.090000. running mean: -4.345142\n",
      "ep 1802: ep_len:35 episode reward: total was 16.000000. running mean: -4.141691\n",
      "epsilon:0.009992 episode_count: 27155. steps_count: 29062388.000000\n",
      "ep 1803: ep_len:1494 episode reward: total was 4.460000. running mean: -4.055674\n",
      "ep 1803: ep_len:736 episode reward: total was -32.860000. running mean: -4.343717\n",
      "ep 1803: ep_len:2982 episode reward: total was -40.910000. running mean: -4.709380\n",
      "ep 1803: ep_len:1117 episode reward: total was -22.330000. running mean: -4.885586\n",
      "ep 1803: ep_len:1405 episode reward: total was 2.760000. running mean: -4.809130\n",
      "ep 1803: ep_len:3594 episode reward: total was -1990.310000. running mean: -24.664139\n",
      "ep 1803: ep_len:1261 episode reward: total was -86.510000. running mean: -25.282597\n",
      "ep 1803: ep_len:772 episode reward: total was -14.150000. running mean: -25.171271\n",
      "ep 1803: ep_len:1520 episode reward: total was 11.020000. running mean: -24.809359\n",
      "ep 1803: ep_len:156 episode reward: total was 76.500000. running mean: -23.796265\n",
      "ep 1803: ep_len:500 episode reward: total was 10.320000. running mean: -23.455102\n",
      "ep 1803: ep_len:2820 episode reward: total was -19.250000. running mean: -23.413051\n",
      "epsilon:0.009992 episode_count: 27167. steps_count: 29080745.000000\n",
      "ep 1804: ep_len:649 episode reward: total was -9.840000. running mean: -23.277321\n",
      "ep 1804: ep_len:725 episode reward: total was -11.810000. running mean: -23.162648\n",
      "ep 1804: ep_len:47 episode reward: total was 22.000000. running mean: -22.711021\n",
      "ep 1804: ep_len:3022 episode reward: total was -37.480000. running mean: -22.858711\n",
      "ep 1804: ep_len:681 episode reward: total was -30.610000. running mean: -22.936224\n",
      "ep 1804: ep_len:93 episode reward: total was 45.000000. running mean: -22.256862\n",
      "ep 1804: ep_len:671 episode reward: total was -5.590000. running mean: -22.090193\n",
      "ep 1804: ep_len:336 episode reward: total was -2.460000. running mean: -21.893891\n",
      "ep 1804: ep_len:1231 episode reward: total was -72.700000. running mean: -22.401952\n",
      "ep 1804: ep_len:855 episode reward: total was 38.430000. running mean: -21.793633\n",
      "ep 1804: ep_len:607 episode reward: total was -15.340000. running mean: -21.729096\n",
      "ep 1804: ep_len:183 episode reward: total was 87.000000. running mean: -20.641805\n",
      "ep 1804: ep_len:626 episode reward: total was 9.590000. running mean: -20.339487\n",
      "ep 1804: ep_len:2750 episode reward: total was -14.750000. running mean: -20.283593\n",
      "epsilon:0.009992 episode_count: 27181. steps_count: 29093221.000000\n",
      "ep 1805: ep_len:862 episode reward: total was -88.690000. running mean: -20.967657\n",
      "ep 1805: ep_len:924 episode reward: total was 14.800000. running mean: -20.609980\n",
      "ep 1805: ep_len:2830 episode reward: total was -61.480000. running mean: -21.018680\n",
      "ep 1805: ep_len:598 episode reward: total was -61.710000. running mean: -21.425593\n",
      "ep 1805: ep_len:24 episode reward: total was 10.500000. running mean: -21.106337\n",
      "ep 1805: ep_len:93 episode reward: total was 45.000000. running mean: -20.445274\n",
      "ep 1805: ep_len:64 episode reward: total was 27.500000. running mean: -19.965821\n",
      "ep 1805: ep_len:66 episode reward: total was 31.500000. running mean: -19.451163\n",
      "ep 1805: ep_len:1493 episode reward: total was 20.830000. running mean: -19.048352\n",
      "ep 1805: ep_len:3581 episode reward: total was -191.230000. running mean: -20.770168\n",
      "ep 1805: ep_len:653 episode reward: total was -17.880000. running mean: -20.741266\n",
      "ep 1805: ep_len:849 episode reward: total was 51.780000. running mean: -20.016054\n",
      "ep 1805: ep_len:1481 episode reward: total was 7.600000. running mean: -19.739893\n",
      "ep 1805: ep_len:115 episode reward: total was 54.500000. running mean: -18.997494\n",
      "ep 1805: ep_len:37 episode reward: total was 15.500000. running mean: -18.652519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1805: ep_len:842 episode reward: total was 10.270000. running mean: -18.363294\n",
      "ep 1805: ep_len:2902 episode reward: total was -25.660000. running mean: -18.436261\n",
      "epsilon:0.009992 episode_count: 27198. steps_count: 29110635.000000\n",
      "ep 1806: ep_len:607 episode reward: total was -10.960000. running mean: -18.361499\n",
      "ep 1806: ep_len:752 episode reward: total was -10.450000. running mean: -18.282384\n",
      "ep 1806: ep_len:43 episode reward: total was 20.000000. running mean: -17.899560\n",
      "ep 1806: ep_len:2977 episode reward: total was -81.090000. running mean: -18.531464\n",
      "ep 1806: ep_len:920 episode reward: total was 58.210000. running mean: -17.764049\n",
      "ep 1806: ep_len:61 episode reward: total was 27.500000. running mean: -17.311409\n",
      "ep 1806: ep_len:115 episode reward: total was 53.000000. running mean: -16.608295\n",
      "ep 1806: ep_len:719 episode reward: total was 23.990000. running mean: -16.202312\n",
      "ep 1806: ep_len:670 episode reward: total was 18.110000. running mean: -15.859189\n",
      "ep 1806: ep_len:572 episode reward: total was -31.820000. running mean: -16.018797\n",
      "ep 1806: ep_len:625 episode reward: total was -5.980000. running mean: -15.918409\n",
      "ep 1806: ep_len:1114 episode reward: total was 25.820000. running mean: -15.501025\n",
      "ep 1806: ep_len:71 episode reward: total was 34.000000. running mean: -15.006015\n",
      "ep 1806: ep_len:79 episode reward: total was 38.000000. running mean: -14.475954\n",
      "ep 1806: ep_len:1159 episode reward: total was -24.420000. running mean: -14.575395\n",
      "ep 1806: ep_len:2829 episode reward: total was -46.990000. running mean: -14.899541\n",
      "ep 1806: ep_len:66 episode reward: total was 30.000000. running mean: -14.450546\n",
      "epsilon:0.009992 episode_count: 27215. steps_count: 29124014.000000\n",
      "ep 1807: ep_len:1152 episode reward: total was 9.190000. running mean: -14.214140\n",
      "ep 1807: ep_len:760 episode reward: total was -16.810000. running mean: -14.240099\n",
      "ep 1807: ep_len:3001 episode reward: total was -75.810000. running mean: -14.855798\n",
      "ep 1807: ep_len:931 episode reward: total was 75.280000. running mean: -13.954440\n",
      "ep 1807: ep_len:68 episode reward: total was 32.500000. running mean: -13.489895\n",
      "ep 1807: ep_len:629 episode reward: total was -6.000000. running mean: -13.414996\n",
      "ep 1807: ep_len:3728 episode reward: total was -56.440000. running mean: -13.845246\n",
      "ep 1807: ep_len:532 episode reward: total was -6.970000. running mean: -13.776494\n",
      "ep 1807: ep_len:759 episode reward: total was 23.230000. running mean: -13.406429\n",
      "ep 1807: ep_len:979 episode reward: total was 7.050000. running mean: -13.201865\n",
      "ep 1807: ep_len:113 episode reward: total was 53.500000. running mean: -12.534846\n",
      "ep 1807: ep_len:624 episode reward: total was -8.070000. running mean: -12.490198\n",
      "ep 1807: ep_len:31 episode reward: total was 14.000000. running mean: -12.225296\n",
      "ep 1807: ep_len:57 episode reward: total was 27.000000. running mean: -11.833043\n",
      "epsilon:0.009992 episode_count: 27229. steps_count: 29137378.000000\n",
      "ep 1808: ep_len:500 episode reward: total was 14.420000. running mean: -11.570512\n",
      "ep 1808: ep_len:729 episode reward: total was -20.150000. running mean: -11.656307\n",
      "ep 1808: ep_len:3105 episode reward: total was -41.610000. running mean: -11.955844\n",
      "ep 1808: ep_len:500 episode reward: total was 22.200000. running mean: -11.614286\n",
      "ep 1808: ep_len:59 episode reward: total was 28.000000. running mean: -11.218143\n",
      "ep 1808: ep_len:1003 episode reward: total was -31.430000. running mean: -11.420261\n",
      "ep 1808: ep_len:661 episode reward: total was 31.700000. running mean: -10.989059\n",
      "ep 1808: ep_len:1180 episode reward: total was -37.830000. running mean: -11.257468\n",
      "ep 1808: ep_len:807 episode reward: total was 13.940000. running mean: -11.005493\n",
      "ep 1808: ep_len:1155 episode reward: total was 7.690000. running mean: -10.818539\n",
      "ep 1808: ep_len:43 episode reward: total was 20.000000. running mean: -10.510353\n",
      "ep 1808: ep_len:1082 episode reward: total was -13.590000. running mean: -10.541150\n",
      "ep 1808: ep_len:2840 episode reward: total was -10.120000. running mean: -10.536938\n",
      "epsilon:0.009992 episode_count: 27242. steps_count: 29151042.000000\n",
      "ep 1809: ep_len:1444 episode reward: total was 21.890000. running mean: -10.212669\n",
      "ep 1809: ep_len:1036 episode reward: total was -7.980000. running mean: -10.190342\n",
      "ep 1809: ep_len:2996 episode reward: total was -6.030000. running mean: -10.148739\n",
      "ep 1809: ep_len:542 episode reward: total was -21.010000. running mean: -10.257351\n",
      "ep 1809: ep_len:41 episode reward: total was 16.000000. running mean: -9.994778\n",
      "ep 1809: ep_len:114 episode reward: total was 52.500000. running mean: -9.369830\n",
      "ep 1809: ep_len:500 episode reward: total was -15.390000. running mean: -9.430032\n",
      "ep 1809: ep_len:3725 episode reward: total was -13.040000. running mean: -9.466131\n",
      "ep 1809: ep_len:1557 episode reward: total was -140.060000. running mean: -10.772070\n",
      "ep 1809: ep_len:877 episode reward: total was 50.050000. running mean: -10.163849\n",
      "ep 1809: ep_len:556 episode reward: total was 42.150000. running mean: -9.640711\n",
      "ep 1809: ep_len:81 episode reward: total was 39.000000. running mean: -9.154304\n",
      "ep 1809: ep_len:38 episode reward: total was 17.500000. running mean: -8.887761\n",
      "ep 1809: ep_len:749 episode reward: total was -64.390000. running mean: -9.442783\n",
      "ep 1809: ep_len:2713 episode reward: total was -14.020000. running mean: -9.488555\n",
      "ep 1809: ep_len:42 episode reward: total was 19.500000. running mean: -9.198670\n",
      "epsilon:0.009992 episode_count: 27258. steps_count: 29168053.000000\n",
      "ep 1810: ep_len:1095 episode reward: total was 3.380000. running mean: -9.072883\n",
      "ep 1810: ep_len:1667 episode reward: total was -51.530000. running mean: -9.497454\n",
      "ep 1810: ep_len:3051 episode reward: total was -46.950000. running mean: -9.871980\n",
      "ep 1810: ep_len:3674 episode reward: total was -233.720000. running mean: -12.110460\n",
      "ep 1810: ep_len:62 episode reward: total was 29.500000. running mean: -11.694355\n",
      "ep 1810: ep_len:744 episode reward: total was -16.910000. running mean: -11.746512\n",
      "ep 1810: ep_len:4025 episode reward: total was -173.240000. running mean: -13.361447\n",
      "ep 1810: ep_len:1530 episode reward: total was -10.120000. running mean: -13.329032\n",
      "ep 1810: ep_len:731 episode reward: total was -24.630000. running mean: -13.442042\n",
      "ep 1810: ep_len:765 episode reward: total was 29.720000. running mean: -13.010421\n",
      "ep 1810: ep_len:92 episode reward: total was 43.000000. running mean: -12.450317\n",
      "ep 1810: ep_len:1140 episode reward: total was -16.040000. running mean: -12.486214\n",
      "ep 1810: ep_len:2760 episode reward: total was -8.440000. running mean: -12.445752\n",
      "ep 1810: ep_len:42 episode reward: total was 19.500000. running mean: -12.126294\n",
      "epsilon:0.009992 episode_count: 27272. steps_count: 29189431.000000\n",
      "ep 1811: ep_len:1147 episode reward: total was -3.330000. running mean: -12.038331\n",
      "ep 1811: ep_len:974 episode reward: total was 5.870000. running mean: -11.859248\n",
      "ep 1811: ep_len:3038 episode reward: total was -25.290000. running mean: -11.993556\n",
      "ep 1811: ep_len:849 episode reward: total was 5.990000. running mean: -11.813720\n",
      "ep 1811: ep_len:128 episode reward: total was 61.000000. running mean: -11.085583\n",
      "ep 1811: ep_len:84 episode reward: total was 40.500000. running mean: -10.569727\n",
      "ep 1811: ep_len:1446 episode reward: total was 6.730000. running mean: -10.396730\n",
      "ep 1811: ep_len:3907 episode reward: total was -90.000000. running mean: -11.192762\n",
      "ep 1811: ep_len:554 episode reward: total was 8.960000. running mean: -10.991235\n",
      "ep 1811: ep_len:709 episode reward: total was 18.420000. running mean: -10.697122\n",
      "ep 1811: ep_len:657 episode reward: total was -15.310000. running mean: -10.743251\n",
      "ep 1811: ep_len:968 episode reward: total was -59.330000. running mean: -11.229119\n",
      "ep 1811: ep_len:2825 episode reward: total was -98.000000. running mean: -12.096828\n",
      "epsilon:0.009992 episode_count: 27285. steps_count: 29206717.000000\n",
      "ep 1812: ep_len:597 episode reward: total was -19.450000. running mean: -12.170359\n",
      "ep 1812: ep_len:768 episode reward: total was -27.540000. running mean: -12.324056\n",
      "ep 1812: ep_len:2949 episode reward: total was -64.510000. running mean: -12.845915\n",
      "ep 1812: ep_len:1220 episode reward: total was -4.150000. running mean: -12.758956\n",
      "ep 1812: ep_len:49 episode reward: total was 23.000000. running mean: -12.401366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1812: ep_len:1480 episode reward: total was 24.400000. running mean: -12.033353\n",
      "ep 1812: ep_len:299 episode reward: total was 7.420000. running mean: -11.838819\n",
      "ep 1812: ep_len:1191 episode reward: total was -48.340000. running mean: -12.203831\n",
      "ep 1812: ep_len:809 episode reward: total was 33.770000. running mean: -11.744093\n",
      "ep 1812: ep_len:1722 episode reward: total was -1041.920000. running mean: -22.045852\n",
      "ep 1812: ep_len:52 episode reward: total was 24.500000. running mean: -21.580393\n",
      "ep 1812: ep_len:1483 episode reward: total was -0.370000. running mean: -21.368289\n",
      "ep 1812: ep_len:2870 episode reward: total was 1.110000. running mean: -21.143506\n",
      "ep 1812: ep_len:64 episode reward: total was 30.500000. running mean: -20.627071\n",
      "epsilon:0.009992 episode_count: 27299. steps_count: 29222270.000000\n",
      "ep 1813: ep_len:653 episode reward: total was 34.920000. running mean: -20.071601\n",
      "ep 1813: ep_len:786 episode reward: total was 14.040000. running mean: -19.730485\n",
      "ep 1813: ep_len:3108 episode reward: total was -12.470000. running mean: -19.657880\n",
      "ep 1813: ep_len:1496 episode reward: total was 24.920000. running mean: -19.212101\n",
      "ep 1813: ep_len:500 episode reward: total was 43.510000. running mean: -18.584880\n",
      "ep 1813: ep_len:3596 episode reward: total was -14.330000. running mean: -18.542331\n",
      "ep 1813: ep_len:781 episode reward: total was -13.300000. running mean: -18.489908\n",
      "ep 1813: ep_len:810 episode reward: total was 13.660000. running mean: -18.168409\n",
      "ep 1813: ep_len:556 episode reward: total was 30.450000. running mean: -17.682225\n",
      "ep 1813: ep_len:91 episode reward: total was 41.000000. running mean: -17.095402\n",
      "ep 1813: ep_len:44 episode reward: total was 19.000000. running mean: -16.734448\n",
      "ep 1813: ep_len:949 episode reward: total was -54.840000. running mean: -17.115504\n",
      "ep 1813: ep_len:2909 episode reward: total was -1.960000. running mean: -16.963949\n",
      "ep 1813: ep_len:45 episode reward: total was 21.000000. running mean: -16.584309\n",
      "epsilon:0.009992 episode_count: 27313. steps_count: 29238594.000000\n",
      "ep 1814: ep_len:741 episode reward: total was -41.570000. running mean: -16.834166\n",
      "ep 1814: ep_len:751 episode reward: total was -3.590000. running mean: -16.701725\n",
      "ep 1814: ep_len:3009 episode reward: total was -4.800000. running mean: -16.582707\n",
      "ep 1814: ep_len:543 episode reward: total was -7.870000. running mean: -16.495580\n",
      "ep 1814: ep_len:138 episode reward: total was 66.000000. running mean: -15.670625\n",
      "ep 1814: ep_len:89 episode reward: total was 41.500000. running mean: -15.098918\n",
      "ep 1814: ep_len:500 episode reward: total was 24.380000. running mean: -14.704129\n",
      "ep 1814: ep_len:3736 episode reward: total was -14.590000. running mean: -14.702988\n",
      "ep 1814: ep_len:970 episode reward: total was -13.990000. running mean: -14.695858\n",
      "ep 1814: ep_len:751 episode reward: total was 45.720000. running mean: -14.091699\n",
      "ep 1814: ep_len:566 episode reward: total was 23.570000. running mean: -13.715082\n",
      "ep 1814: ep_len:121 episode reward: total was 59.000000. running mean: -12.987932\n",
      "ep 1814: ep_len:578 episode reward: total was 8.570000. running mean: -12.772352\n",
      "ep 1814: ep_len:2801 episode reward: total was 13.890000. running mean: -12.505729\n",
      "epsilon:0.009992 episode_count: 27327. steps_count: 29253888.000000\n",
      "ep 1815: ep_len:661 episode reward: total was 44.550000. running mean: -11.935171\n",
      "ep 1815: ep_len:945 episode reward: total was 31.750000. running mean: -11.498320\n",
      "ep 1815: ep_len:70 episode reward: total was 30.500000. running mean: -11.078337\n",
      "ep 1815: ep_len:2947 episode reward: total was -21.270000. running mean: -11.180253\n",
      "ep 1815: ep_len:500 episode reward: total was 18.470000. running mean: -10.883751\n",
      "ep 1815: ep_len:63 episode reward: total was 30.000000. running mean: -10.474913\n",
      "ep 1815: ep_len:791 episode reward: total was 22.920000. running mean: -10.140964\n",
      "ep 1815: ep_len:4023 episode reward: total was -335.610000. running mean: -13.395654\n",
      "ep 1815: ep_len:684 episode reward: total was -24.350000. running mean: -13.505198\n",
      "ep 1815: ep_len:827 episode reward: total was 43.290000. running mean: -12.937246\n",
      "ep 1815: ep_len:1051 episode reward: total was 2.610000. running mean: -12.781773\n",
      "ep 1815: ep_len:59 episode reward: total was 28.000000. running mean: -12.373956\n",
      "ep 1815: ep_len:109 episode reward: total was 53.000000. running mean: -11.720216\n",
      "ep 1815: ep_len:702 episode reward: total was -84.050000. running mean: -12.443514\n",
      "ep 1815: ep_len:2950 episode reward: total was 7.790000. running mean: -12.241179\n",
      "ep 1815: ep_len:61 episode reward: total was 29.000000. running mean: -11.828767\n",
      "epsilon:0.009992 episode_count: 27343. steps_count: 29270331.000000\n",
      "ep 1816: ep_len:859 episode reward: total was 10.380000. running mean: -11.606679\n",
      "ep 1816: ep_len:748 episode reward: total was -17.620000. running mean: -11.666813\n",
      "ep 1816: ep_len:65 episode reward: total was 31.000000. running mean: -11.240144\n",
      "ep 1816: ep_len:89 episode reward: total was 43.000000. running mean: -10.697743\n",
      "ep 1816: ep_len:596 episode reward: total was -16.430000. running mean: -10.755066\n",
      "ep 1816: ep_len:62 episode reward: total was 28.000000. running mean: -10.367515\n",
      "ep 1816: ep_len:500 episode reward: total was 42.720000. running mean: -9.836640\n",
      "ep 1816: ep_len:3674 episode reward: total was -185.250000. running mean: -11.590773\n",
      "ep 1816: ep_len:511 episode reward: total was -2.130000. running mean: -11.496166\n",
      "ep 1816: ep_len:735 episode reward: total was 4.610000. running mean: -11.335104\n",
      "ep 1816: ep_len:511 episode reward: total was 2.500000. running mean: -11.196753\n",
      "ep 1816: ep_len:214 episode reward: total was 98.000000. running mean: -10.104785\n",
      "ep 1816: ep_len:113 episode reward: total was 53.500000. running mean: -9.468738\n",
      "ep 1816: ep_len:916 episode reward: total was 21.600000. running mean: -9.158050\n",
      "ep 1816: ep_len:2802 episode reward: total was -4.160000. running mean: -9.108070\n",
      "ep 1816: ep_len:56 episode reward: total was 25.000000. running mean: -8.766989\n",
      "epsilon:0.009992 episode_count: 27359. steps_count: 29282782.000000\n",
      "ep 1817: ep_len:811 episode reward: total was -25.390000. running mean: -8.933219\n",
      "ep 1817: ep_len:730 episode reward: total was -41.980000. running mean: -9.263687\n",
      "ep 1817: ep_len:3021 episode reward: total was -18.330000. running mean: -9.354350\n",
      "ep 1817: ep_len:500 episode reward: total was 20.060000. running mean: -9.060206\n",
      "ep 1817: ep_len:56 episode reward: total was 26.500000. running mean: -8.704604\n",
      "ep 1817: ep_len:52 episode reward: total was 23.000000. running mean: -8.387558\n",
      "ep 1817: ep_len:1027 episode reward: total was -19.400000. running mean: -8.497683\n",
      "ep 1817: ep_len:320 episode reward: total was 19.850000. running mean: -8.214206\n",
      "ep 1817: ep_len:536 episode reward: total was -49.350000. running mean: -8.625564\n",
      "ep 1817: ep_len:737 episode reward: total was 11.490000. running mean: -8.424408\n",
      "ep 1817: ep_len:500 episode reward: total was 44.340000. running mean: -7.896764\n",
      "ep 1817: ep_len:85 episode reward: total was 39.500000. running mean: -7.422797\n",
      "ep 1817: ep_len:45 episode reward: total was 21.000000. running mean: -7.138569\n",
      "ep 1817: ep_len:725 episode reward: total was -84.830000. running mean: -7.915483\n",
      "ep 1817: ep_len:2794 episode reward: total was 4.640000. running mean: -7.789928\n",
      "epsilon:0.009992 episode_count: 27374. steps_count: 29294721.000000\n",
      "ep 1818: ep_len:1099 episode reward: total was -1.180000. running mean: -7.723829\n",
      "ep 1818: ep_len:500 episode reward: total was -9.920000. running mean: -7.745790\n",
      "ep 1818: ep_len:2940 episode reward: total was -38.930000. running mean: -8.057633\n",
      "ep 1818: ep_len:654 episode reward: total was 22.350000. running mean: -7.753556\n",
      "ep 1818: ep_len:69 episode reward: total was 33.000000. running mean: -7.346021\n",
      "ep 1818: ep_len:147 episode reward: total was 70.500000. running mean: -6.567560\n",
      "ep 1818: ep_len:43 episode reward: total was 18.500000. running mean: -6.316885\n",
      "ep 1818: ep_len:780 episode reward: total was -36.660000. running mean: -6.620316\n",
      "ep 1818: ep_len:681 episode reward: total was 23.850000. running mean: -6.315613\n",
      "ep 1818: ep_len:583 episode reward: total was 28.920000. running mean: -5.963257\n",
      "ep 1818: ep_len:657 episode reward: total was 8.400000. running mean: -5.819624\n",
      "ep 1818: ep_len:500 episode reward: total was 23.400000. running mean: -5.527428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1818: ep_len:66 episode reward: total was 30.000000. running mean: -5.172154\n",
      "ep 1818: ep_len:602 episode reward: total was -3.540000. running mean: -5.155832\n",
      "ep 1818: ep_len:2793 episode reward: total was -11.510000. running mean: -5.219374\n",
      "ep 1818: ep_len:58 episode reward: total was 26.000000. running mean: -4.907180\n",
      "epsilon:0.009992 episode_count: 27390. steps_count: 29306893.000000\n",
      "ep 1819: ep_len:973 episode reward: total was -50.530000. running mean: -5.363408\n",
      "ep 1819: ep_len:199 episode reward: total was 4.370000. running mean: -5.266074\n",
      "ep 1819: ep_len:2939 episode reward: total was -2.620000. running mean: -5.239613\n",
      "ep 1819: ep_len:635 episode reward: total was -16.450000. running mean: -5.351717\n",
      "ep 1819: ep_len:90 episode reward: total was 43.500000. running mean: -4.863200\n",
      "ep 1819: ep_len:721 episode reward: total was -24.760000. running mean: -5.062168\n",
      "ep 1819: ep_len:334 episode reward: total was 7.870000. running mean: -4.932846\n",
      "ep 1819: ep_len:507 episode reward: total was -7.710000. running mean: -4.960618\n",
      "ep 1819: ep_len:7519 episode reward: total was 44.180000. running mean: -4.469212\n",
      "ep 1819: ep_len:1532 episode reward: total was 18.700000. running mean: -4.237520\n",
      "ep 1819: ep_len:45 episode reward: total was 21.000000. running mean: -3.985144\n",
      "ep 1819: ep_len:58 episode reward: total was 27.500000. running mean: -3.670293\n",
      "ep 1819: ep_len:98 episode reward: total was 47.500000. running mean: -3.158590\n",
      "ep 1819: ep_len:832 episode reward: total was 10.740000. running mean: -3.019604\n",
      "ep 1819: ep_len:2839 episode reward: total was -5.780000. running mean: -3.047208\n",
      "ep 1819: ep_len:57 episode reward: total was 25.500000. running mean: -2.761736\n",
      "epsilon:0.009992 episode_count: 27406. steps_count: 29326271.000000\n",
      "ep 1820: ep_len:1428 episode reward: total was 18.360000. running mean: -2.550519\n",
      "ep 1820: ep_len:1223 episode reward: total was -68.740000. running mean: -3.212414\n",
      "ep 1820: ep_len:3069 episode reward: total was -34.010000. running mean: -3.520389\n",
      "ep 1820: ep_len:608 episode reward: total was -9.240000. running mean: -3.577586\n",
      "ep 1820: ep_len:47 episode reward: total was 21.510000. running mean: -3.326710\n",
      "ep 1820: ep_len:63 episode reward: total was 30.000000. running mean: -2.993443\n",
      "ep 1820: ep_len:593 episode reward: total was 34.230000. running mean: -2.621208\n",
      "ep 1820: ep_len:331 episode reward: total was 10.620000. running mean: -2.488796\n",
      "ep 1820: ep_len:704 episode reward: total was -37.570000. running mean: -2.839608\n",
      "ep 1820: ep_len:716 episode reward: total was -13.670000. running mean: -2.947912\n",
      "ep 1820: ep_len:741 episode reward: total was 21.640000. running mean: -2.702033\n",
      "ep 1820: ep_len:76 episode reward: total was 35.000000. running mean: -2.325013\n",
      "ep 1820: ep_len:680 episode reward: total was -4.960000. running mean: -2.351362\n",
      "ep 1820: ep_len:2871 episode reward: total was -5.700000. running mean: -2.384849\n",
      "ep 1820: ep_len:60 episode reward: total was 28.500000. running mean: -2.076000\n",
      "epsilon:0.009992 episode_count: 27421. steps_count: 29339481.000000\n",
      "ep 1821: ep_len:1480 episode reward: total was 11.200000. running mean: -1.943240\n",
      "ep 1821: ep_len:664 episode reward: total was -34.940000. running mean: -2.273208\n",
      "ep 1821: ep_len:72 episode reward: total was 33.000000. running mean: -1.920476\n",
      "ep 1821: ep_len:2999 episode reward: total was 9.210000. running mean: -1.809171\n",
      "ep 1821: ep_len:693 episode reward: total was 21.350000. running mean: -1.577579\n",
      "ep 1821: ep_len:65 episode reward: total was 29.500000. running mean: -1.266804\n",
      "ep 1821: ep_len:595 episode reward: total was -24.900000. running mean: -1.503136\n",
      "ep 1821: ep_len:648 episode reward: total was 23.120000. running mean: -1.256904\n",
      "ep 1821: ep_len:568 episode reward: total was 26.460000. running mean: -0.979735\n",
      "ep 1821: ep_len:876 episode reward: total was 64.430000. running mean: -0.325638\n",
      "ep 1821: ep_len:688 episode reward: total was 3.370000. running mean: -0.288681\n",
      "ep 1821: ep_len:1098 episode reward: total was 23.740000. running mean: -0.048395\n",
      "ep 1821: ep_len:2818 episode reward: total was 3.100000. running mean: -0.016911\n",
      "ep 1821: ep_len:28 episode reward: total was 8.000000. running mean: 0.063258\n",
      "epsilon:0.009992 episode_count: 27435. steps_count: 29352773.000000\n",
      "ep 1822: ep_len:676 episode reward: total was -17.130000. running mean: -0.108674\n",
      "ep 1822: ep_len:795 episode reward: total was -18.480000. running mean: -0.292387\n",
      "ep 1822: ep_len:3050 episode reward: total was -77.970000. running mean: -1.069164\n",
      "ep 1822: ep_len:684 episode reward: total was -5.920000. running mean: -1.117672\n",
      "ep 1822: ep_len:32 episode reward: total was 14.500000. running mean: -0.961495\n",
      "ep 1822: ep_len:500 episode reward: total was 43.910000. running mean: -0.512780\n",
      "ep 1822: ep_len:3617 episode reward: total was -323.000000. running mean: -3.737652\n",
      "ep 1822: ep_len:500 episode reward: total was 14.570000. running mean: -3.554576\n",
      "ep 1822: ep_len:872 episode reward: total was 24.120000. running mean: -3.277830\n",
      "ep 1822: ep_len:1439 episode reward: total was 8.710000. running mean: -3.157952\n",
      "ep 1822: ep_len:68 episode reward: total was 32.500000. running mean: -2.801372\n",
      "ep 1822: ep_len:694 episode reward: total was 35.150000. running mean: -2.421859\n",
      "ep 1822: ep_len:2791 episode reward: total was 1.820000. running mean: -2.379440\n",
      "epsilon:0.009992 episode_count: 27448. steps_count: 29368491.000000\n",
      "ep 1823: ep_len:1134 episode reward: total was -3.980000. running mean: -2.395446\n",
      "ep 1823: ep_len:1267 episode reward: total was -68.300000. running mean: -3.054491\n",
      "ep 1823: ep_len:2939 episode reward: total was -68.280000. running mean: -3.706746\n",
      "ep 1823: ep_len:837 episode reward: total was 19.310000. running mean: -3.476579\n",
      "ep 1823: ep_len:35 episode reward: total was 16.000000. running mean: -3.281813\n",
      "ep 1823: ep_len:88 episode reward: total was 41.000000. running mean: -2.838995\n",
      "ep 1823: ep_len:66 episode reward: total was 31.500000. running mean: -2.495605\n",
      "ep 1823: ep_len:56 episode reward: total was 26.500000. running mean: -2.205649\n",
      "ep 1823: ep_len:664 episode reward: total was -13.730000. running mean: -2.320892\n",
      "ep 1823: ep_len:3595 episode reward: total was -194.610000. running mean: -4.243783\n",
      "ep 1823: ep_len:614 episode reward: total was 15.550000. running mean: -4.045846\n",
      "ep 1823: ep_len:700 episode reward: total was 46.740000. running mean: -3.537987\n",
      "ep 1823: ep_len:1093 episode reward: total was -15.160000. running mean: -3.654207\n",
      "ep 1823: ep_len:50 episode reward: total was 23.500000. running mean: -3.382665\n",
      "ep 1823: ep_len:643 episode reward: total was -1.550000. running mean: -3.364339\n",
      "ep 1823: ep_len:2794 episode reward: total was -11.500000. running mean: -3.445695\n",
      "epsilon:0.009992 episode_count: 27464. steps_count: 29385066.000000\n",
      "ep 1824: ep_len:618 episode reward: total was -15.990000. running mean: -3.571138\n",
      "ep 1824: ep_len:500 episode reward: total was -10.590000. running mean: -3.641327\n",
      "ep 1824: ep_len:2952 episode reward: total was 3.080000. running mean: -3.574114\n",
      "ep 1824: ep_len:1706 episode reward: total was -115.880000. running mean: -4.697172\n",
      "ep 1824: ep_len:36 episode reward: total was 16.500000. running mean: -4.485201\n",
      "ep 1824: ep_len:1506 episode reward: total was -135.600000. running mean: -5.796349\n",
      "ep 1824: ep_len:340 episode reward: total was 8.940000. running mean: -5.648985\n",
      "ep 1824: ep_len:1261 episode reward: total was -73.410000. running mean: -6.326595\n",
      "ep 1824: ep_len:877 episode reward: total was 33.140000. running mean: -5.931929\n",
      "ep 1824: ep_len:527 episode reward: total was 2.550000. running mean: -5.847110\n",
      "ep 1824: ep_len:68 episode reward: total was 29.500000. running mean: -5.493639\n",
      "ep 1824: ep_len:125 episode reward: total was 61.000000. running mean: -4.828703\n",
      "ep 1824: ep_len:710 episode reward: total was -81.950000. running mean: -5.599916\n",
      "ep 1824: ep_len:2772 episode reward: total was -23.250000. running mean: -5.776416\n",
      "epsilon:0.009992 episode_count: 27478. steps_count: 29399064.000000\n",
      "ep 1825: ep_len:597 episode reward: total was -25.510000. running mean: -5.973752\n",
      "ep 1825: ep_len:687 episode reward: total was -58.950000. running mean: -6.503515\n",
      "ep 1825: ep_len:51 episode reward: total was 24.000000. running mean: -6.198480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1825: ep_len:2931 episode reward: total was -0.170000. running mean: -6.138195\n",
      "ep 1825: ep_len:1633 episode reward: total was -59.160000. running mean: -6.668413\n",
      "ep 1825: ep_len:1843 episode reward: total was -44.880000. running mean: -7.050529\n",
      "ep 1825: ep_len:615 episode reward: total was 21.660000. running mean: -6.763423\n",
      "ep 1825: ep_len:689 episode reward: total was -39.740000. running mean: -7.093189\n",
      "ep 1825: ep_len:7370 episode reward: total was -2528.380000. running mean: -32.306057\n",
      "ep 1825: ep_len:1515 episode reward: total was 17.120000. running mean: -31.811797\n",
      "ep 1825: ep_len:82 episode reward: total was 38.000000. running mean: -31.113679\n",
      "ep 1825: ep_len:93 episode reward: total was 43.500000. running mean: -30.367542\n",
      "ep 1825: ep_len:641 episode reward: total was -39.700000. running mean: -30.460867\n",
      "ep 1825: ep_len:2770 episode reward: total was -16.450000. running mean: -30.320758\n",
      "epsilon:0.009992 episode_count: 27492. steps_count: 29420581.000000\n",
      "ep 1826: ep_len:802 episode reward: total was 0.930000. running mean: -30.008250\n",
      "ep 1826: ep_len:678 episode reward: total was -59.040000. running mean: -30.298568\n",
      "ep 1826: ep_len:81 episode reward: total was 37.500000. running mean: -29.620582\n",
      "ep 1826: ep_len:3008 episode reward: total was -26.210000. running mean: -29.586476\n",
      "ep 1826: ep_len:500 episode reward: total was 9.590000. running mean: -29.194712\n",
      "ep 1826: ep_len:121 episode reward: total was 54.500000. running mean: -28.357764\n",
      "ep 1826: ep_len:1631 episode reward: total was -105.010000. running mean: -29.124287\n",
      "ep 1826: ep_len:688 episode reward: total was 15.350000. running mean: -28.679544\n",
      "ep 1826: ep_len:515 episode reward: total was -3.470000. running mean: -28.427448\n",
      "ep 1826: ep_len:754 episode reward: total was 38.220000. running mean: -27.760974\n",
      "ep 1826: ep_len:520 episode reward: total was -1.850000. running mean: -27.501864\n",
      "ep 1826: ep_len:52 episode reward: total was 24.500000. running mean: -26.981846\n",
      "ep 1826: ep_len:618 episode reward: total was 7.310000. running mean: -26.638927\n",
      "ep 1826: ep_len:2876 episode reward: total was -9.390000. running mean: -26.466438\n",
      "epsilon:0.009992 episode_count: 27506. steps_count: 29433425.000000\n",
      "ep 1827: ep_len:649 episode reward: total was -7.300000. running mean: -26.274774\n",
      "ep 1827: ep_len:1591 episode reward: total was -70.020000. running mean: -26.712226\n",
      "ep 1827: ep_len:2981 episode reward: total was -34.340000. running mean: -26.788504\n",
      "ep 1827: ep_len:500 episode reward: total was 0.190000. running mean: -26.518718\n",
      "ep 1827: ep_len:46 episode reward: total was 21.500000. running mean: -26.038531\n",
      "ep 1827: ep_len:77 episode reward: total was 37.000000. running mean: -25.408146\n",
      "ep 1827: ep_len:889 episode reward: total was 38.640000. running mean: -24.767665\n",
      "ep 1827: ep_len:327 episode reward: total was 16.520000. running mean: -24.354788\n",
      "ep 1827: ep_len:596 episode reward: total was -176.620000. running mean: -25.877440\n",
      "ep 1827: ep_len:614 episode reward: total was -0.330000. running mean: -25.621966\n",
      "ep 1827: ep_len:721 episode reward: total was 8.640000. running mean: -25.279346\n",
      "ep 1827: ep_len:39 episode reward: total was 16.500000. running mean: -24.861552\n",
      "ep 1827: ep_len:500 episode reward: total was 31.630000. running mean: -24.296637\n",
      "ep 1827: ep_len:2917 episode reward: total was -563.240000. running mean: -29.686071\n",
      "ep 1827: ep_len:65 episode reward: total was 31.000000. running mean: -29.079210\n",
      "epsilon:0.009992 episode_count: 27521. steps_count: 29445937.000000\n",
      "ep 1828: ep_len:1469 episode reward: total was -13.240000. running mean: -28.920818\n",
      "ep 1828: ep_len:663 episode reward: total was -34.950000. running mean: -28.981110\n",
      "ep 1828: ep_len:2954 episode reward: total was -44.960000. running mean: -29.140899\n",
      "ep 1828: ep_len:754 episode reward: total was -18.890000. running mean: -29.038390\n",
      "ep 1828: ep_len:60 episode reward: total was 28.500000. running mean: -28.463006\n",
      "ep 1828: ep_len:910 episode reward: total was 24.130000. running mean: -27.937076\n",
      "ep 1828: ep_len:3764 episode reward: total was -387.830000. running mean: -31.536005\n",
      "ep 1828: ep_len:731 episode reward: total was -26.870000. running mean: -31.489345\n",
      "ep 1828: ep_len:893 episode reward: total was 55.810000. running mean: -30.616351\n",
      "ep 1828: ep_len:594 episode reward: total was 5.000000. running mean: -30.260188\n",
      "ep 1828: ep_len:43 episode reward: total was 18.500000. running mean: -29.772586\n",
      "ep 1828: ep_len:1186 episode reward: total was 9.900000. running mean: -29.375860\n",
      "ep 1828: ep_len:37 episode reward: total was 14.000000. running mean: -28.942101\n",
      "ep 1828: ep_len:33 episode reward: total was 15.000000. running mean: -28.502680\n",
      "epsilon:0.009992 episode_count: 27535. steps_count: 29460028.000000\n",
      "ep 1829: ep_len:761 episode reward: total was -73.360000. running mean: -28.951254\n",
      "ep 1829: ep_len:749 episode reward: total was -40.380000. running mean: -29.065541\n",
      "ep 1829: ep_len:67 episode reward: total was 29.000000. running mean: -28.484886\n",
      "ep 1829: ep_len:3005 episode reward: total was -118.220000. running mean: -29.382237\n",
      "ep 1829: ep_len:3214 episode reward: total was -256.610000. running mean: -31.654514\n",
      "ep 1829: ep_len:61 episode reward: total was 29.000000. running mean: -31.047969\n",
      "ep 1829: ep_len:500 episode reward: total was -16.250000. running mean: -30.899990\n",
      "ep 1829: ep_len:3772 episode reward: total was -91.180000. running mean: -31.502790\n",
      "ep 1829: ep_len:536 episode reward: total was -2.890000. running mean: -31.216662\n",
      "ep 1829: ep_len:7257 episode reward: total was -165.900000. running mean: -32.563495\n",
      "ep 1829: ep_len:638 episode reward: total was -20.460000. running mean: -32.442460\n",
      "ep 1829: ep_len:772 episode reward: total was -75.880000. running mean: -32.876836\n",
      "ep 1829: ep_len:2826 episode reward: total was -79.950000. running mean: -33.347567\n",
      "epsilon:0.009992 episode_count: 27548. steps_count: 29484186.000000\n",
      "ep 1830: ep_len:1086 episode reward: total was -13.550000. running mean: -33.149592\n",
      "ep 1830: ep_len:747 episode reward: total was -72.720000. running mean: -33.545296\n",
      "ep 1830: ep_len:3005 episode reward: total was -32.610000. running mean: -33.535943\n",
      "ep 1830: ep_len:1160 episode reward: total was -10.270000. running mean: -33.303283\n",
      "ep 1830: ep_len:51 episode reward: total was 22.500000. running mean: -32.745251\n",
      "ep 1830: ep_len:655 episode reward: total was 3.410000. running mean: -32.383698\n",
      "ep 1830: ep_len:3762 episode reward: total was -80.110000. running mean: -32.860961\n",
      "ep 1830: ep_len:843 episode reward: total was -40.220000. running mean: -32.934551\n",
      "ep 1830: ep_len:778 episode reward: total was -14.420000. running mean: -32.749406\n",
      "ep 1830: ep_len:980 episode reward: total was 0.300000. running mean: -32.418912\n",
      "ep 1830: ep_len:67 episode reward: total was 30.500000. running mean: -31.789723\n",
      "ep 1830: ep_len:47 episode reward: total was 22.000000. running mean: -31.251825\n",
      "ep 1830: ep_len:818 episode reward: total was -87.860000. running mean: -31.817907\n",
      "ep 1830: ep_len:2787 episode reward: total was -19.090000. running mean: -31.690628\n",
      "epsilon:0.009992 episode_count: 27562. steps_count: 29500972.000000\n",
      "ep 1831: ep_len:957 episode reward: total was -71.880000. running mean: -32.092522\n",
      "ep 1831: ep_len:1707 episode reward: total was -37.540000. running mean: -32.146997\n",
      "ep 1831: ep_len:75 episode reward: total was 33.000000. running mean: -31.495527\n",
      "ep 1831: ep_len:2874 episode reward: total was -10.110000. running mean: -31.281671\n",
      "ep 1831: ep_len:752 episode reward: total was 0.280000. running mean: -30.966055\n",
      "ep 1831: ep_len:98 episode reward: total was 47.500000. running mean: -30.181394\n",
      "ep 1831: ep_len:108 episode reward: total was 49.500000. running mean: -29.384580\n",
      "ep 1831: ep_len:666 episode reward: total was 5.120000. running mean: -29.039534\n",
      "ep 1831: ep_len:359 episode reward: total was 19.230000. running mean: -28.556839\n",
      "ep 1831: ep_len:1220 episode reward: total was -43.880000. running mean: -28.710071\n",
      "ep 1831: ep_len:776 episode reward: total was -2.530000. running mean: -28.448270\n",
      "ep 1831: ep_len:1010 episode reward: total was 2.010000. running mean: -28.143687\n",
      "ep 1831: ep_len:160 episode reward: total was 75.500000. running mean: -27.107250\n",
      "ep 1831: ep_len:750 episode reward: total was -48.220000. running mean: -27.318378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1831: ep_len:2822 episode reward: total was 7.210000. running mean: -26.973094\n",
      "ep 1831: ep_len:41 episode reward: total was 19.000000. running mean: -26.513363\n",
      "epsilon:0.009992 episode_count: 27578. steps_count: 29515347.000000\n",
      "ep 1832: ep_len:889 episode reward: total was 17.720000. running mean: -26.071030\n",
      "ep 1832: ep_len:789 episode reward: total was -30.660000. running mean: -26.116919\n",
      "ep 1832: ep_len:52 episode reward: total was 21.500000. running mean: -25.640750\n",
      "ep 1832: ep_len:3015 episode reward: total was -18.600000. running mean: -25.570343\n",
      "ep 1832: ep_len:1453 episode reward: total was 13.990000. running mean: -25.174739\n",
      "ep 1832: ep_len:64 episode reward: total was 30.500000. running mean: -24.617992\n",
      "ep 1832: ep_len:104 episode reward: total was 49.000000. running mean: -23.881812\n",
      "ep 1832: ep_len:55 episode reward: total was 24.500000. running mean: -23.397994\n",
      "ep 1832: ep_len:659 episode reward: total was -7.200000. running mean: -23.236014\n",
      "ep 1832: ep_len:3857 episode reward: total was -85.800000. running mean: -23.861654\n",
      "ep 1832: ep_len:623 episode reward: total was -8.750000. running mean: -23.710537\n",
      "ep 1832: ep_len:798 episode reward: total was 12.390000. running mean: -23.349532\n",
      "ep 1832: ep_len:626 episode reward: total was 7.680000. running mean: -23.039236\n",
      "ep 1832: ep_len:1179 episode reward: total was 19.870000. running mean: -22.610144\n",
      "ep 1832: ep_len:2833 episode reward: total was -0.390000. running mean: -22.387943\n",
      "epsilon:0.009992 episode_count: 27593. steps_count: 29532343.000000\n",
      "ep 1833: ep_len:967 episode reward: total was -127.760000. running mean: -23.441663\n",
      "ep 1833: ep_len:710 episode reward: total was -34.130000. running mean: -23.548547\n",
      "ep 1833: ep_len:56 episode reward: total was 25.000000. running mean: -23.063061\n",
      "ep 1833: ep_len:55 episode reward: total was 26.000000. running mean: -22.572430\n",
      "ep 1833: ep_len:799 episode reward: total was 10.710000. running mean: -22.239606\n",
      "ep 1833: ep_len:126 episode reward: total was 61.500000. running mean: -21.402210\n",
      "ep 1833: ep_len:1137 episode reward: total was 0.090000. running mean: -21.187288\n",
      "ep 1833: ep_len:3526 episode reward: total was -53.900000. running mean: -21.514415\n",
      "ep 1833: ep_len:1285 episode reward: total was -52.970000. running mean: -21.828971\n",
      "ep 1833: ep_len:699 episode reward: total was 30.440000. running mean: -21.306281\n",
      "ep 1833: ep_len:976 episode reward: total was 14.560000. running mean: -20.947618\n",
      "ep 1833: ep_len:38 episode reward: total was 17.500000. running mean: -20.563142\n",
      "ep 1833: ep_len:92 episode reward: total was 45.510000. running mean: -19.902411\n",
      "ep 1833: ep_len:656 episode reward: total was -11.520000. running mean: -19.818587\n",
      "ep 1833: ep_len:2816 episode reward: total was -21.620000. running mean: -19.836601\n",
      "epsilon:0.009992 episode_count: 27608. steps_count: 29546281.000000\n",
      "ep 1834: ep_len:931 episode reward: total was -67.590000. running mean: -20.314135\n",
      "ep 1834: ep_len:500 episode reward: total was -5.660000. running mean: -20.167594\n",
      "ep 1834: ep_len:2978 episode reward: total was -78.790000. running mean: -20.753818\n",
      "ep 1834: ep_len:552 episode reward: total was -32.460000. running mean: -20.870879\n",
      "ep 1834: ep_len:43 episode reward: total was 20.000000. running mean: -20.462171\n",
      "ep 1834: ep_len:1031 episode reward: total was -71.650000. running mean: -20.974049\n",
      "ep 1834: ep_len:3571 episode reward: total was -58.590000. running mean: -21.350208\n",
      "ep 1834: ep_len:948 episode reward: total was -16.570000. running mean: -21.302406\n",
      "ep 1834: ep_len:776 episode reward: total was 8.150000. running mean: -21.007882\n",
      "ep 1834: ep_len:568 episode reward: total was 25.460000. running mean: -20.543203\n",
      "ep 1834: ep_len:617 episode reward: total was 10.290000. running mean: -20.234871\n",
      "ep 1834: ep_len:2892 episode reward: total was -5.990000. running mean: -20.092423\n",
      "epsilon:0.009992 episode_count: 27620. steps_count: 29561688.000000\n",
      "ep 1835: ep_len:1407 episode reward: total was 12.550000. running mean: -19.765998\n",
      "ep 1835: ep_len:637 episode reward: total was 13.730000. running mean: -19.431038\n",
      "ep 1835: ep_len:69 episode reward: total was 31.500000. running mean: -18.921728\n",
      "ep 1835: ep_len:2868 episode reward: total was -48.400000. running mean: -19.216511\n",
      "ep 1835: ep_len:500 episode reward: total was 25.570000. running mean: -18.768646\n",
      "ep 1835: ep_len:54 episode reward: total was 25.500000. running mean: -18.325959\n",
      "ep 1835: ep_len:123 episode reward: total was 58.500000. running mean: -17.557700\n",
      "ep 1835: ep_len:56 episode reward: total was 26.500000. running mean: -17.117123\n",
      "ep 1835: ep_len:500 episode reward: total was -6.700000. running mean: -17.012951\n",
      "ep 1835: ep_len:3580 episode reward: total was -33.130000. running mean: -17.174122\n",
      "ep 1835: ep_len:752 episode reward: total was -39.740000. running mean: -17.399781\n",
      "ep 1835: ep_len:784 episode reward: total was -8.610000. running mean: -17.311883\n",
      "ep 1835: ep_len:707 episode reward: total was -11.190000. running mean: -17.250664\n",
      "ep 1835: ep_len:175 episode reward: total was 83.000000. running mean: -16.248157\n",
      "ep 1835: ep_len:765 episode reward: total was -45.040000. running mean: -16.536076\n",
      "ep 1835: ep_len:2810 episode reward: total was -4.970000. running mean: -16.420415\n",
      "epsilon:0.009992 episode_count: 27636. steps_count: 29577475.000000\n",
      "ep 1836: ep_len:2539 episode reward: total was -204.030000. running mean: -18.296511\n",
      "ep 1836: ep_len:1578 episode reward: total was -27.850000. running mean: -18.392046\n",
      "ep 1836: ep_len:81 episode reward: total was 39.000000. running mean: -17.818125\n",
      "ep 1836: ep_len:2962 episode reward: total was -83.640000. running mean: -18.476344\n",
      "ep 1836: ep_len:516 episode reward: total was 26.450000. running mean: -18.027081\n",
      "ep 1836: ep_len:26 episode reward: total was 11.500000. running mean: -17.731810\n",
      "ep 1836: ep_len:500 episode reward: total was 48.870000. running mean: -17.065792\n",
      "ep 1836: ep_len:3650 episode reward: total was -39.040000. running mean: -17.285534\n",
      "ep 1836: ep_len:577 episode reward: total was 19.080000. running mean: -16.921879\n",
      "ep 1836: ep_len:642 episode reward: total was 13.330000. running mean: -16.619360\n",
      "ep 1836: ep_len:626 episode reward: total was -7.080000. running mean: -16.523966\n",
      "ep 1836: ep_len:92 episode reward: total was 40.000000. running mean: -15.958726\n",
      "ep 1836: ep_len:600 episode reward: total was -14.430000. running mean: -15.943439\n",
      "ep 1836: ep_len:2754 episode reward: total was -12.840000. running mean: -15.912405\n",
      "ep 1836: ep_len:60 episode reward: total was 28.500000. running mean: -15.468281\n",
      "epsilon:0.009992 episode_count: 27651. steps_count: 29594678.000000\n",
      "ep 1837: ep_len:784 episode reward: total was -45.250000. running mean: -15.766098\n",
      "ep 1837: ep_len:1608 episode reward: total was -45.940000. running mean: -16.067837\n",
      "ep 1837: ep_len:2946 episode reward: total was -43.420000. running mean: -16.341359\n",
      "ep 1837: ep_len:539 episode reward: total was -34.020000. running mean: -16.518145\n",
      "ep 1837: ep_len:64 episode reward: total was 27.500000. running mean: -16.077964\n",
      "ep 1837: ep_len:1057 episode reward: total was -10.810000. running mean: -16.025284\n",
      "ep 1837: ep_len:625 episode reward: total was 17.410000. running mean: -15.690931\n",
      "ep 1837: ep_len:889 episode reward: total was -30.670000. running mean: -15.840722\n",
      "ep 1837: ep_len:7649 episode reward: total was -243.920000. running mean: -18.121515\n",
      "ep 1837: ep_len:588 episode reward: total was -11.950000. running mean: -18.059799\n",
      "ep 1837: ep_len:53 episode reward: total was 25.000000. running mean: -17.629201\n",
      "ep 1837: ep_len:985 episode reward: total was -55.730000. running mean: -18.010209\n",
      "ep 1837: ep_len:2757 episode reward: total was -34.390000. running mean: -18.174007\n",
      "epsilon:0.009992 episode_count: 27664. steps_count: 29615222.000000\n",
      "ep 1838: ep_len:641 episode reward: total was -26.080000. running mean: -18.253067\n",
      "ep 1838: ep_len:740 episode reward: total was -76.830000. running mean: -18.838837\n",
      "ep 1838: ep_len:2988 episode reward: total was -15.870000. running mean: -18.809148\n",
      "ep 1838: ep_len:500 episode reward: total was -28.040000. running mean: -18.901457\n",
      "ep 1838: ep_len:42 episode reward: total was 19.500000. running mean: -18.517442\n",
      "ep 1838: ep_len:143 episode reward: total was 70.000000. running mean: -17.632268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1838: ep_len:114 episode reward: total was 54.000000. running mean: -16.915945\n",
      "ep 1838: ep_len:1409 episode reward: total was -295.140000. running mean: -19.698186\n",
      "ep 1838: ep_len:675 episode reward: total was 24.830000. running mean: -19.252904\n",
      "ep 1838: ep_len:897 episode reward: total was -82.470000. running mean: -19.885075\n",
      "ep 1838: ep_len:769 episode reward: total was -15.510000. running mean: -19.841324\n",
      "ep 1838: ep_len:653 episode reward: total was 8.710000. running mean: -19.555811\n",
      "ep 1838: ep_len:63 episode reward: total was 30.000000. running mean: -19.060253\n",
      "ep 1838: ep_len:94 episode reward: total was 44.000000. running mean: -18.429650\n",
      "ep 1838: ep_len:860 episode reward: total was 18.350000. running mean: -18.061854\n",
      "ep 1838: ep_len:2754 episode reward: total was 5.920000. running mean: -17.822035\n",
      "epsilon:0.009992 episode_count: 27680. steps_count: 29628564.000000\n",
      "ep 1839: ep_len:791 episode reward: total was -23.570000. running mean: -17.879515\n",
      "ep 1839: ep_len:768 episode reward: total was -19.730000. running mean: -17.898020\n",
      "ep 1839: ep_len:3056 episode reward: total was -31.420000. running mean: -18.033239\n",
      "ep 1839: ep_len:806 episode reward: total was 43.400000. running mean: -17.418907\n",
      "ep 1839: ep_len:45 episode reward: total was 19.500000. running mean: -17.049718\n",
      "ep 1839: ep_len:70 episode reward: total was 33.500000. running mean: -16.544221\n",
      "ep 1839: ep_len:2945 episode reward: total was -238.460000. running mean: -18.763379\n",
      "ep 1839: ep_len:3615 episode reward: total was -53.530000. running mean: -19.111045\n",
      "ep 1839: ep_len:1601 episode reward: total was -109.290000. running mean: -20.012834\n",
      "ep 1839: ep_len:742 episode reward: total was 43.760000. running mean: -19.375106\n",
      "ep 1839: ep_len:674 episode reward: total was -2.000000. running mean: -19.201355\n",
      "ep 1839: ep_len:47 episode reward: total was 19.000000. running mean: -18.819341\n",
      "ep 1839: ep_len:4037 episode reward: total was -591.800000. running mean: -24.549148\n",
      "ep 1839: ep_len:2819 episode reward: total was -43.720000. running mean: -24.740856\n",
      "epsilon:0.009992 episode_count: 27694. steps_count: 29650580.000000\n",
      "ep 1840: ep_len:887 episode reward: total was 12.680000. running mean: -24.366648\n",
      "ep 1840: ep_len:764 episode reward: total was -13.180000. running mean: -24.254781\n",
      "ep 1840: ep_len:3030 episode reward: total was -138.960000. running mean: -25.401834\n",
      "ep 1840: ep_len:834 episode reward: total was 30.890000. running mean: -24.838915\n",
      "ep 1840: ep_len:36 episode reward: total was 15.000000. running mean: -24.440526\n",
      "ep 1840: ep_len:709 episode reward: total was -25.400000. running mean: -24.450121\n",
      "ep 1840: ep_len:4004 episode reward: total was -105.170000. running mean: -25.257320\n",
      "ep 1840: ep_len:803 episode reward: total was -29.310000. running mean: -25.297846\n",
      "ep 1840: ep_len:7369 episode reward: total was -63.870000. running mean: -25.683568\n",
      "ep 1840: ep_len:659 episode reward: total was -14.790000. running mean: -25.574632\n",
      "ep 1840: ep_len:77 episode reward: total was 37.000000. running mean: -24.948886\n",
      "ep 1840: ep_len:45 episode reward: total was 19.500000. running mean: -24.504397\n",
      "ep 1840: ep_len:1099 episode reward: total was 8.900000. running mean: -24.170353\n",
      "ep 1840: ep_len:34 episode reward: total was 15.500000. running mean: -23.773650\n",
      "ep 1840: ep_len:51 episode reward: total was 24.000000. running mean: -23.295913\n",
      "epsilon:0.009992 episode_count: 27709. steps_count: 29670981.000000\n",
      "ep 1841: ep_len:629 episode reward: total was -36.240000. running mean: -23.425354\n",
      "ep 1841: ep_len:1261 episode reward: total was -55.230000. running mean: -23.743400\n",
      "ep 1841: ep_len:53 episode reward: total was 25.000000. running mean: -23.255966\n",
      "ep 1841: ep_len:2861 episode reward: total was -94.750000. running mean: -23.970907\n",
      "ep 1841: ep_len:501 episode reward: total was -2.780000. running mean: -23.758998\n",
      "ep 1841: ep_len:108 episode reward: total was 51.000000. running mean: -23.011408\n",
      "ep 1841: ep_len:41 episode reward: total was 16.000000. running mean: -22.621294\n",
      "ep 1841: ep_len:500 episode reward: total was 12.740000. running mean: -22.267681\n",
      "ep 1841: ep_len:4220 episode reward: total was -18.380000. running mean: -22.228804\n",
      "ep 1841: ep_len:1607 episode reward: total was -28.540000. running mean: -22.291916\n",
      "ep 1841: ep_len:7416 episode reward: total was -162.670000. running mean: -23.695697\n",
      "ep 1841: ep_len:655 episode reward: total was 5.290000. running mean: -23.405840\n",
      "ep 1841: ep_len:64 episode reward: total was 30.500000. running mean: -22.866781\n",
      "ep 1841: ep_len:123 episode reward: total was 58.500000. running mean: -22.053114\n",
      "ep 1841: ep_len:609 episode reward: total was -17.310000. running mean: -22.005682\n",
      "ep 1841: ep_len:2853 episode reward: total was -379.540000. running mean: -25.581026\n",
      "epsilon:0.009992 episode_count: 27725. steps_count: 29694482.000000\n",
      "ep 1842: ep_len:684 episode reward: total was -30.700000. running mean: -25.632215\n",
      "ep 1842: ep_len:500 episode reward: total was 25.570000. running mean: -25.120193\n",
      "ep 1842: ep_len:2888 episode reward: total was -97.350000. running mean: -25.842491\n",
      "ep 1842: ep_len:915 episode reward: total was 25.200000. running mean: -25.332066\n",
      "ep 1842: ep_len:117 episode reward: total was 55.500000. running mean: -24.523746\n",
      "ep 1842: ep_len:61 episode reward: total was 26.000000. running mean: -24.018508\n",
      "ep 1842: ep_len:2898 episode reward: total was -156.920000. running mean: -25.347523\n",
      "ep 1842: ep_len:4006 episode reward: total was -64.710000. running mean: -25.741148\n",
      "ep 1842: ep_len:571 episode reward: total was -30.670000. running mean: -25.790436\n",
      "ep 1842: ep_len:870 episode reward: total was 72.290000. running mean: -24.809632\n",
      "ep 1842: ep_len:603 episode reward: total was -9.290000. running mean: -24.654436\n",
      "ep 1842: ep_len:201 episode reward: total was 97.500000. running mean: -23.432891\n",
      "ep 1842: ep_len:46 episode reward: total was 20.000000. running mean: -22.998562\n",
      "ep 1842: ep_len:1107 episode reward: total was -17.380000. running mean: -22.942377\n",
      "ep 1842: ep_len:2823 episode reward: total was -82.940000. running mean: -23.542353\n",
      "ep 1842: ep_len:36 episode reward: total was 15.000000. running mean: -23.156930\n",
      "epsilon:0.009992 episode_count: 27741. steps_count: 29712808.000000\n",
      "ep 1843: ep_len:872 episode reward: total was -18.860000. running mean: -23.113960\n",
      "ep 1843: ep_len:913 episode reward: total was 23.170000. running mean: -22.651121\n",
      "ep 1843: ep_len:80 episode reward: total was 38.500000. running mean: -22.039609\n",
      "ep 1843: ep_len:3035 episode reward: total was -10.660000. running mean: -21.925813\n",
      "ep 1843: ep_len:1728 episode reward: total was -81.810000. running mean: -22.524655\n",
      "ep 1843: ep_len:38 episode reward: total was 17.500000. running mean: -22.124409\n",
      "ep 1843: ep_len:2891 episode reward: total was -168.210000. running mean: -23.585265\n",
      "ep 1843: ep_len:500 episode reward: total was -9.190000. running mean: -23.441312\n",
      "ep 1843: ep_len:689 episode reward: total was -30.650000. running mean: -23.513399\n",
      "ep 1843: ep_len:586 episode reward: total was -12.890000. running mean: -23.407165\n",
      "ep 1843: ep_len:1485 episode reward: total was 0.570000. running mean: -23.167393\n",
      "ep 1843: ep_len:33 episode reward: total was 15.000000. running mean: -22.785719\n",
      "ep 1843: ep_len:94 episode reward: total was 42.500000. running mean: -22.132862\n",
      "ep 1843: ep_len:1043 episode reward: total was -7.920000. running mean: -21.990733\n",
      "ep 1843: ep_len:2804 episode reward: total was -30.100000. running mean: -22.071826\n",
      "epsilon:0.009992 episode_count: 27756. steps_count: 29729599.000000\n",
      "ep 1844: ep_len:870 episode reward: total was -17.140000. running mean: -22.022508\n",
      "ep 1844: ep_len:1639 episode reward: total was -37.280000. running mean: -22.175083\n",
      "ep 1844: ep_len:55 episode reward: total was 26.000000. running mean: -21.693332\n",
      "ep 1844: ep_len:2916 episode reward: total was -57.310000. running mean: -22.049499\n",
      "ep 1844: ep_len:665 episode reward: total was -14.040000. running mean: -21.969404\n",
      "ep 1844: ep_len:83 episode reward: total was 40.000000. running mean: -21.349710\n",
      "ep 1844: ep_len:39 episode reward: total was 16.500000. running mean: -20.971212\n",
      "ep 1844: ep_len:602 episode reward: total was 35.180000. running mean: -20.409700\n",
      "ep 1844: ep_len:348 episode reward: total was 6.570000. running mean: -20.139903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1844: ep_len:678 episode reward: total was -16.060000. running mean: -20.099104\n",
      "ep 1844: ep_len:824 episode reward: total was 34.050000. running mean: -19.557613\n",
      "ep 1844: ep_len:700 episode reward: total was -4.400000. running mean: -19.406037\n",
      "ep 1844: ep_len:112 episode reward: total was 54.500000. running mean: -18.666977\n",
      "ep 1844: ep_len:1057 episode reward: total was -2.200000. running mean: -18.502307\n",
      "ep 1844: ep_len:2775 episode reward: total was -63.170000. running mean: -18.948984\n",
      "ep 1844: ep_len:48 episode reward: total was 22.500000. running mean: -18.534494\n",
      "epsilon:0.009992 episode_count: 27772. steps_count: 29743010.000000\n",
      "ep 1845: ep_len:1343 episode reward: total was 8.270000. running mean: -18.266449\n",
      "ep 1845: ep_len:665 episode reward: total was -31.900000. running mean: -18.402785\n",
      "ep 1845: ep_len:83 episode reward: total was 40.000000. running mean: -17.818757\n",
      "ep 1845: ep_len:2979 episode reward: total was -41.230000. running mean: -18.052869\n",
      "ep 1845: ep_len:552 episode reward: total was -11.820000. running mean: -17.990541\n",
      "ep 1845: ep_len:111 episode reward: total was 52.500000. running mean: -17.285635\n",
      "ep 1845: ep_len:657 episode reward: total was 9.430000. running mean: -17.018479\n",
      "ep 1845: ep_len:3700 episode reward: total was -287.610000. running mean: -19.724394\n",
      "ep 1845: ep_len:539 episode reward: total was -1.850000. running mean: -19.545650\n",
      "ep 1845: ep_len:698 episode reward: total was 16.280000. running mean: -19.187394\n",
      "ep 1845: ep_len:1499 episode reward: total was 20.460000. running mean: -18.790920\n",
      "ep 1845: ep_len:70 episode reward: total was 33.500000. running mean: -18.268010\n",
      "ep 1845: ep_len:219 episode reward: total was 105.000000. running mean: -17.035330\n",
      "ep 1845: ep_len:1019 episode reward: total was -23.410000. running mean: -17.099077\n",
      "ep 1845: ep_len:2831 episode reward: total was -77.110000. running mean: -17.699186\n",
      "ep 1845: ep_len:39 episode reward: total was 18.000000. running mean: -17.342194\n",
      "epsilon:0.009992 episode_count: 27788. steps_count: 29760014.000000\n",
      "ep 1846: ep_len:976 episode reward: total was -109.640000. running mean: -18.265172\n",
      "ep 1846: ep_len:216 episode reward: total was -5.130000. running mean: -18.133821\n",
      "ep 1846: ep_len:64 episode reward: total was 30.500000. running mean: -17.647483\n",
      "ep 1846: ep_len:3112 episode reward: total was -77.420000. running mean: -18.245208\n",
      "ep 1846: ep_len:638 episode reward: total was 2.590000. running mean: -18.036856\n",
      "ep 1846: ep_len:51 episode reward: total was 24.000000. running mean: -17.616487\n",
      "ep 1846: ep_len:86 episode reward: total was 40.000000. running mean: -17.040322\n",
      "ep 1846: ep_len:81 episode reward: total was 39.000000. running mean: -16.479919\n",
      "ep 1846: ep_len:500 episode reward: total was 18.590000. running mean: -16.129220\n",
      "ep 1846: ep_len:632 episode reward: total was 9.920000. running mean: -15.868728\n",
      "ep 1846: ep_len:500 episode reward: total was -1.750000. running mean: -15.727540\n",
      "ep 1846: ep_len:850 episode reward: total was 43.470000. running mean: -15.135565\n",
      "ep 1846: ep_len:500 episode reward: total was 36.320000. running mean: -14.621009\n",
      "ep 1846: ep_len:29 episode reward: total was 12.510000. running mean: -14.349699\n",
      "ep 1846: ep_len:824 episode reward: total was -5.540000. running mean: -14.261602\n",
      "ep 1846: ep_len:2859 episode reward: total was -30.560000. running mean: -14.424586\n",
      "epsilon:0.009992 episode_count: 27804. steps_count: 29771932.000000\n",
      "ep 1847: ep_len:644 episode reward: total was -27.060000. running mean: -14.550940\n",
      "ep 1847: ep_len:1286 episode reward: total was -41.850000. running mean: -14.823931\n",
      "ep 1847: ep_len:52 episode reward: total was 23.000000. running mean: -14.445692\n",
      "ep 1847: ep_len:2886 episode reward: total was -83.200000. running mean: -15.133235\n",
      "ep 1847: ep_len:1249 episode reward: total was -7.840000. running mean: -15.060302\n",
      "ep 1847: ep_len:721 episode reward: total was -58.400000. running mean: -15.493699\n",
      "ep 1847: ep_len:3603 episode reward: total was -321.270000. running mean: -18.551462\n",
      "ep 1847: ep_len:1259 episode reward: total was -44.140000. running mean: -18.807348\n",
      "ep 1847: ep_len:7335 episode reward: total was -2984.520000. running mean: -48.464474\n",
      "ep 1847: ep_len:836 episode reward: total was -2.830000. running mean: -48.008129\n",
      "ep 1847: ep_len:55 episode reward: total was 24.500000. running mean: -47.283048\n",
      "ep 1847: ep_len:95 episode reward: total was 43.000000. running mean: -46.380218\n",
      "ep 1847: ep_len:500 episode reward: total was 22.660000. running mean: -45.689815\n",
      "ep 1847: ep_len:2821 episode reward: total was 17.610000. running mean: -45.056817\n",
      "epsilon:0.009992 episode_count: 27818. steps_count: 29795274.000000\n",
      "ep 1848: ep_len:1110 episode reward: total was 7.290000. running mean: -44.533349\n",
      "ep 1848: ep_len:4008 episode reward: total was -3063.910000. running mean: -74.727116\n",
      "ep 1848: ep_len:2944 episode reward: total was -67.200000. running mean: -74.651845\n",
      "ep 1848: ep_len:750 episode reward: total was -27.440000. running mean: -74.179726\n",
      "ep 1848: ep_len:57 episode reward: total was 27.000000. running mean: -73.167929\n",
      "ep 1848: ep_len:117 episode reward: total was 55.500000. running mean: -71.881250\n",
      "ep 1848: ep_len:677 episode reward: total was 5.590000. running mean: -71.106537\n",
      "ep 1848: ep_len:663 episode reward: total was 18.620000. running mean: -70.209272\n",
      "ep 1848: ep_len:500 episode reward: total was 42.530000. running mean: -69.081879\n",
      "ep 1848: ep_len:806 episode reward: total was 16.190000. running mean: -68.229160\n",
      "ep 1848: ep_len:1065 episode reward: total was -28.090000. running mean: -67.827769\n",
      "ep 1848: ep_len:54 episode reward: total was 25.500000. running mean: -66.894491\n",
      "ep 1848: ep_len:639 episode reward: total was 5.240000. running mean: -66.173146\n",
      "ep 1848: ep_len:2898 episode reward: total was -21.350000. running mean: -65.724914\n",
      "epsilon:0.009992 episode_count: 27832. steps_count: 29811562.000000\n",
      "ep 1849: ep_len:665 episode reward: total was 36.700000. running mean: -64.700665\n",
      "ep 1849: ep_len:500 episode reward: total was 12.310000. running mean: -63.930559\n",
      "ep 1849: ep_len:2976 episode reward: total was -92.430000. running mean: -64.215553\n",
      "ep 1849: ep_len:1447 episode reward: total was 23.330000. running mean: -63.340098\n",
      "ep 1849: ep_len:170 episode reward: total was 82.000000. running mean: -61.886697\n",
      "ep 1849: ep_len:70 episode reward: total was 30.500000. running mean: -60.962830\n",
      "ep 1849: ep_len:1110 episode reward: total was 19.270000. running mean: -60.160501\n",
      "ep 1849: ep_len:646 episode reward: total was -12.220000. running mean: -59.681096\n",
      "ep 1849: ep_len:1587 episode reward: total was -20.140000. running mean: -59.285685\n",
      "ep 1849: ep_len:804 episode reward: total was 25.330000. running mean: -58.439529\n",
      "ep 1849: ep_len:1124 episode reward: total was -20.670000. running mean: -58.061833\n",
      "ep 1849: ep_len:95 episode reward: total was 46.000000. running mean: -57.021215\n",
      "ep 1849: ep_len:97 episode reward: total was 47.000000. running mean: -55.981003\n",
      "ep 1849: ep_len:68 episode reward: total was 31.000000. running mean: -55.111193\n",
      "ep 1849: ep_len:674 episode reward: total was 5.330000. running mean: -54.506781\n",
      "ep 1849: ep_len:2804 episode reward: total was -0.960000. running mean: -53.971313\n",
      "ep 1849: ep_len:44 episode reward: total was 20.500000. running mean: -53.226600\n",
      "epsilon:0.009992 episode_count: 27849. steps_count: 29826443.000000\n",
      "ep 1850: ep_len:1023 episode reward: total was -141.930000. running mean: -54.113634\n",
      "ep 1850: ep_len:662 episode reward: total was -53.140000. running mean: -54.103898\n",
      "ep 1850: ep_len:3006 episode reward: total was -12.330000. running mean: -53.686159\n",
      "ep 1850: ep_len:819 episode reward: total was 29.750000. running mean: -52.851797\n",
      "ep 1850: ep_len:61 episode reward: total was 26.000000. running mean: -52.063279\n",
      "ep 1850: ep_len:114 episode reward: total was 52.500000. running mean: -51.017646\n",
      "ep 1850: ep_len:79 episode reward: total was 38.000000. running mean: -50.127470\n",
      "ep 1850: ep_len:35 episode reward: total was 16.000000. running mean: -49.466195\n",
      "ep 1850: ep_len:500 episode reward: total was -11.470000. running mean: -49.086233\n",
      "ep 1850: ep_len:356 episode reward: total was 7.600000. running mean: -48.519371\n",
      "ep 1850: ep_len:975 episode reward: total was -23.740000. running mean: -48.271577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1850: ep_len:800 episode reward: total was 22.930000. running mean: -47.559561\n",
      "ep 1850: ep_len:500 episode reward: total was 1.690000. running mean: -47.067066\n",
      "ep 1850: ep_len:64 episode reward: total was 16.510000. running mean: -46.431295\n",
      "ep 1850: ep_len:753 episode reward: total was -63.640000. running mean: -46.603382\n",
      "ep 1850: ep_len:2837 episode reward: total was -9.750000. running mean: -46.234848\n",
      "ep 1850: ep_len:46 episode reward: total was 20.000000. running mean: -45.572500\n",
      "epsilon:0.009992 episode_count: 27866. steps_count: 29839073.000000\n",
      "ep 1851: ep_len:1112 episode reward: total was 1.490000. running mean: -45.101875\n",
      "ep 1851: ep_len:180 episode reward: total was 1.180000. running mean: -44.639056\n",
      "ep 1851: ep_len:2989 episode reward: total was -65.570000. running mean: -44.848365\n",
      "ep 1851: ep_len:1183 episode reward: total was -20.080000. running mean: -44.600682\n",
      "ep 1851: ep_len:48 episode reward: total was 18.000000. running mean: -43.974675\n",
      "ep 1851: ep_len:142 episode reward: total was 68.000000. running mean: -42.854928\n",
      "ep 1851: ep_len:55 episode reward: total was 26.000000. running mean: -42.166379\n",
      "ep 1851: ep_len:861 episode reward: total was 19.550000. running mean: -41.549215\n",
      "ep 1851: ep_len:349 episode reward: total was 5.510000. running mean: -41.078623\n",
      "ep 1851: ep_len:683 episode reward: total was -118.550000. running mean: -41.853337\n",
      "ep 1851: ep_len:7401 episode reward: total was -173.470000. running mean: -43.169503\n",
      "ep 1851: ep_len:985 episode reward: total was 60.060000. running mean: -42.137208\n",
      "ep 1851: ep_len:50 episode reward: total was 20.500000. running mean: -41.510836\n",
      "ep 1851: ep_len:178 episode reward: total was 84.500000. running mean: -40.250728\n",
      "ep 1851: ep_len:71 episode reward: total was 29.500000. running mean: -39.553221\n",
      "ep 1851: ep_len:624 episode reward: total was -24.720000. running mean: -39.404888\n",
      "ep 1851: ep_len:2770 episode reward: total was -18.590000. running mean: -39.196740\n",
      "ep 1851: ep_len:59 episode reward: total was 26.500000. running mean: -38.539772\n",
      "epsilon:0.009992 episode_count: 27884. steps_count: 29858813.000000\n",
      "ep 1852: ep_len:1409 episode reward: total was 20.130000. running mean: -37.953074\n",
      "ep 1852: ep_len:500 episode reward: total was -8.510000. running mean: -37.658644\n",
      "ep 1852: ep_len:37 episode reward: total was 17.000000. running mean: -37.112057\n",
      "ep 1852: ep_len:2950 episode reward: total was -51.360000. running mean: -37.254537\n",
      "ep 1852: ep_len:648 episode reward: total was 2.760000. running mean: -36.854391\n",
      "ep 1852: ep_len:69 episode reward: total was 33.000000. running mean: -36.155847\n",
      "ep 1852: ep_len:632 episode reward: total was 1.100000. running mean: -35.783289\n",
      "ep 1852: ep_len:337 episode reward: total was 14.480000. running mean: -35.280656\n",
      "ep 1852: ep_len:570 episode reward: total was -167.180000. running mean: -36.599649\n",
      "ep 1852: ep_len:792 episode reward: total was 25.670000. running mean: -35.976953\n",
      "ep 1852: ep_len:545 episode reward: total was 37.720000. running mean: -35.239983\n",
      "ep 1852: ep_len:51 episode reward: total was 24.000000. running mean: -34.647584\n",
      "ep 1852: ep_len:69 episode reward: total was 31.500000. running mean: -33.986108\n",
      "ep 1852: ep_len:2321 episode reward: total was -350.230000. running mean: -37.148547\n",
      "ep 1852: ep_len:2814 episode reward: total was -40.310000. running mean: -37.180161\n",
      "ep 1852: ep_len:66 episode reward: total was 30.000000. running mean: -36.508360\n",
      "epsilon:0.009992 episode_count: 27900. steps_count: 29872623.000000\n",
      "ep 1853: ep_len:1091 episode reward: total was -4.780000. running mean: -36.191076\n",
      "ep 1853: ep_len:988 episode reward: total was 25.850000. running mean: -35.570665\n",
      "ep 1853: ep_len:2907 episode reward: total was -48.310000. running mean: -35.698059\n",
      "ep 1853: ep_len:537 episode reward: total was -27.120000. running mean: -35.612278\n",
      "ep 1853: ep_len:98 episode reward: total was 47.500000. running mean: -34.781155\n",
      "ep 1853: ep_len:663 episode reward: total was 3.060000. running mean: -34.402744\n",
      "ep 1853: ep_len:3630 episode reward: total was -278.610000. running mean: -36.844816\n",
      "ep 1853: ep_len:561 episode reward: total was -1.630000. running mean: -36.492668\n",
      "ep 1853: ep_len:830 episode reward: total was 7.440000. running mean: -36.053341\n",
      "ep 1853: ep_len:668 episode reward: total was -3.620000. running mean: -35.729008\n",
      "ep 1853: ep_len:70 episode reward: total was 33.500000. running mean: -35.036718\n",
      "ep 1853: ep_len:104 episode reward: total was 49.000000. running mean: -34.196351\n",
      "ep 1853: ep_len:1114 episode reward: total was -14.280000. running mean: -33.997187\n",
      "ep 1853: ep_len:2789 episode reward: total was 6.790000. running mean: -33.589315\n",
      "ep 1853: ep_len:67 episode reward: total was 32.000000. running mean: -32.933422\n",
      "epsilon:0.009992 episode_count: 27915. steps_count: 29888740.000000\n",
      "ep 1854: ep_len:1419 episode reward: total was 8.110000. running mean: -32.522988\n",
      "ep 1854: ep_len:710 episode reward: total was -67.210000. running mean: -32.869858\n",
      "ep 1854: ep_len:83 episode reward: total was 40.000000. running mean: -32.141160\n",
      "ep 1854: ep_len:3045 episode reward: total was -22.860000. running mean: -32.048348\n",
      "ep 1854: ep_len:777 episode reward: total was -8.280000. running mean: -31.810664\n",
      "ep 1854: ep_len:48 episode reward: total was 22.500000. running mean: -31.267558\n",
      "ep 1854: ep_len:639 episode reward: total was 1.690000. running mean: -30.937982\n",
      "ep 1854: ep_len:3707 episode reward: total was -200.560000. running mean: -32.634202\n",
      "ep 1854: ep_len:654 episode reward: total was 1.320000. running mean: -32.294660\n",
      "ep 1854: ep_len:795 episode reward: total was -5.220000. running mean: -32.023914\n",
      "ep 1854: ep_len:739 episode reward: total was 13.540000. running mean: -31.568275\n",
      "ep 1854: ep_len:110 episode reward: total was 52.000000. running mean: -30.732592\n",
      "ep 1854: ep_len:3005 episode reward: total was -680.150000. running mean: -37.226766\n",
      "ep 1854: ep_len:2819 episode reward: total was -6.840000. running mean: -36.922898\n",
      "ep 1854: ep_len:43 episode reward: total was 18.500000. running mean: -36.368669\n",
      "epsilon:0.009992 episode_count: 27930. steps_count: 29907333.000000\n",
      "ep 1855: ep_len:1453 episode reward: total was 7.410000. running mean: -35.930883\n",
      "ep 1855: ep_len:664 episode reward: total was -25.760000. running mean: -35.829174\n",
      "ep 1855: ep_len:3011 episode reward: total was -60.220000. running mean: -36.073082\n",
      "ep 1855: ep_len:1584 episode reward: total was -67.520000. running mean: -36.387551\n",
      "ep 1855: ep_len:1438 episode reward: total was -74.470000. running mean: -36.768376\n",
      "ep 1855: ep_len:618 episode reward: total was 21.780000. running mean: -36.182892\n",
      "ep 1855: ep_len:1204 episode reward: total was -72.760000. running mean: -36.548663\n",
      "ep 1855: ep_len:858 episode reward: total was 64.280000. running mean: -35.540376\n",
      "ep 1855: ep_len:982 episode reward: total was 8.800000. running mean: -35.096973\n",
      "ep 1855: ep_len:74 episode reward: total was 34.000000. running mean: -34.406003\n",
      "ep 1855: ep_len:500 episode reward: total was 6.920000. running mean: -33.992743\n",
      "ep 1855: ep_len:2834 episode reward: total was -11.920000. running mean: -33.772015\n",
      "ep 1855: ep_len:66 episode reward: total was 31.500000. running mean: -33.119295\n",
      "epsilon:0.009992 episode_count: 27943. steps_count: 29922619.000000\n",
      "ep 1856: ep_len:1147 episode reward: total was -12.540000. running mean: -32.913502\n",
      "ep 1856: ep_len:201 episode reward: total was 4.240000. running mean: -32.541967\n",
      "ep 1856: ep_len:78 episode reward: total was 36.000000. running mean: -31.856548\n",
      "ep 1856: ep_len:3019 episode reward: total was -20.130000. running mean: -31.739282\n",
      "ep 1856: ep_len:1138 episode reward: total was -18.080000. running mean: -31.602689\n",
      "ep 1856: ep_len:53 episode reward: total was 23.500000. running mean: -31.051662\n",
      "ep 1856: ep_len:80 episode reward: total was 37.000000. running mean: -30.371146\n",
      "ep 1856: ep_len:656 episode reward: total was -3.190000. running mean: -30.099334\n",
      "ep 1856: ep_len:580 episode reward: total was 23.170000. running mean: -29.566641\n",
      "ep 1856: ep_len:1571 episode reward: total was -11.180000. running mean: -29.382775\n",
      "ep 1856: ep_len:636 episode reward: total was -16.060000. running mean: -29.249547\n",
      "ep 1856: ep_len:665 episode reward: total was 17.720000. running mean: -28.779851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1856: ep_len:71 episode reward: total was 34.000000. running mean: -28.152053\n",
      "ep 1856: ep_len:25 episode reward: total was 8.000000. running mean: -27.790532\n",
      "ep 1856: ep_len:949 episode reward: total was -112.650000. running mean: -28.639127\n",
      "ep 1856: ep_len:2865 episode reward: total was -3.410000. running mean: -28.386836\n",
      "epsilon:0.009992 episode_count: 27959. steps_count: 29936353.000000\n",
      "ep 1857: ep_len:1101 episode reward: total was -0.270000. running mean: -28.105667\n",
      "ep 1857: ep_len:948 episode reward: total was 14.610000. running mean: -27.678511\n",
      "ep 1857: ep_len:3052 episode reward: total was -180.880000. running mean: -29.210526\n",
      "ep 1857: ep_len:622 episode reward: total was -4.090000. running mean: -28.959320\n",
      "ep 1857: ep_len:65 episode reward: total was 29.500000. running mean: -28.374727\n",
      "ep 1857: ep_len:45 episode reward: total was 19.500000. running mean: -27.895980\n",
      "ep 1857: ep_len:1057 episode reward: total was -55.250000. running mean: -28.169520\n",
      "ep 1857: ep_len:3752 episode reward: total was 1.400000. running mean: -27.873825\n",
      "ep 1857: ep_len:1613 episode reward: total was -71.980000. running mean: -28.314887\n",
      "ep 1857: ep_len:847 episode reward: total was 63.370000. running mean: -27.398038\n",
      "ep 1857: ep_len:595 episode reward: total was 11.040000. running mean: -27.013657\n",
      "ep 1857: ep_len:37 episode reward: total was 17.000000. running mean: -26.573521\n",
      "ep 1857: ep_len:1468 episode reward: total was 15.980000. running mean: -26.147986\n",
      "ep 1857: ep_len:2793 episode reward: total was 5.580000. running mean: -25.830706\n",
      "epsilon:0.009992 episode_count: 27973. steps_count: 29954348.000000\n",
      "ep 1858: ep_len:744 episode reward: total was -25.070000. running mean: -25.823099\n",
      "ep 1858: ep_len:1586 episode reward: total was -25.230000. running mean: -25.817168\n",
      "ep 1858: ep_len:3014 episode reward: total was -27.370000. running mean: -25.832696\n",
      "ep 1858: ep_len:507 episode reward: total was -22.400000. running mean: -25.798369\n",
      "ep 1858: ep_len:99 episode reward: total was 46.500000. running mean: -25.075385\n",
      "ep 1858: ep_len:500 episode reward: total was -5.390000. running mean: -24.878532\n",
      "ep 1858: ep_len:3528 episode reward: total was -89.630000. running mean: -25.526046\n",
      "ep 1858: ep_len:502 episode reward: total was -8.250000. running mean: -25.353286\n",
      "ep 1858: ep_len:844 episode reward: total was 42.240000. running mean: -24.677353\n",
      "ep 1858: ep_len:594 episode reward: total was -12.410000. running mean: -24.554679\n",
      "ep 1858: ep_len:58 episode reward: total was 27.500000. running mean: -24.034133\n",
      "ep 1858: ep_len:1433 episode reward: total was 25.700000. running mean: -23.536791\n",
      "ep 1858: ep_len:46 episode reward: total was 21.500000. running mean: -23.086423\n",
      "ep 1858: ep_len:58 episode reward: total was 26.000000. running mean: -22.595559\n",
      "epsilon:0.009992 episode_count: 27987. steps_count: 29967861.000000\n",
      "ep 1859: ep_len:712 episode reward: total was -49.610000. running mean: -22.865704\n",
      "ep 1859: ep_len:662 episode reward: total was -27.890000. running mean: -22.915947\n",
      "ep 1859: ep_len:2960 episode reward: total was -0.730000. running mean: -22.694087\n",
      "ep 1859: ep_len:848 episode reward: total was 44.530000. running mean: -22.021846\n",
      "ep 1859: ep_len:48 episode reward: total was 22.500000. running mean: -21.576628\n",
      "ep 1859: ep_len:59 episode reward: total was 28.000000. running mean: -21.080861\n",
      "ep 1859: ep_len:617 episode reward: total was 50.920000. running mean: -20.360853\n",
      "ep 1859: ep_len:629 episode reward: total was 17.910000. running mean: -19.978144\n",
      "ep 1859: ep_len:628 episode reward: total was -26.360000. running mean: -20.041963\n",
      "ep 1859: ep_len:745 episode reward: total was 13.140000. running mean: -19.710143\n",
      "ep 1859: ep_len:1513 episode reward: total was 22.120000. running mean: -19.291842\n",
      "ep 1859: ep_len:59 episode reward: total was 28.000000. running mean: -18.818923\n",
      "ep 1859: ep_len:107 episode reward: total was 50.500000. running mean: -18.125734\n",
      "ep 1859: ep_len:616 episode reward: total was -14.210000. running mean: -18.086577\n",
      "ep 1859: ep_len:2920 episode reward: total was -44.730000. running mean: -18.353011\n",
      "epsilon:0.009992 episode_count: 28002. steps_count: 29980984.000000\n",
      "ep 1860: ep_len:646 episode reward: total was -43.690000. running mean: -18.606381\n",
      "ep 1860: ep_len:3872 episode reward: total was -476.640000. running mean: -23.186717\n",
      "ep 1860: ep_len:2962 episode reward: total was 2.140000. running mean: -22.933450\n",
      "ep 1860: ep_len:548 episode reward: total was 8.040000. running mean: -22.623715\n",
      "ep 1860: ep_len:67 episode reward: total was 32.000000. running mean: -22.077478\n",
      "ep 1860: ep_len:70 episode reward: total was 33.500000. running mean: -21.521703\n",
      "ep 1860: ep_len:1463 episode reward: total was 15.440000. running mean: -21.152086\n",
      "ep 1860: ep_len:3698 episode reward: total was -207.720000. running mean: -23.017766\n",
      "ep 1860: ep_len:604 episode reward: total was 9.240000. running mean: -22.695188\n",
      "ep 1860: ep_len:749 episode reward: total was 19.730000. running mean: -22.270936\n",
      "ep 1860: ep_len:1011 episode reward: total was 2.670000. running mean: -22.021527\n",
      "ep 1860: ep_len:31 episode reward: total was 14.000000. running mean: -21.661311\n",
      "ep 1860: ep_len:1422 episode reward: total was 26.110000. running mean: -21.183598\n",
      "ep 1860: ep_len:2820 episode reward: total was 7.160000. running mean: -20.900162\n",
      "ep 1860: ep_len:43 episode reward: total was 18.500000. running mean: -20.506161\n",
      "epsilon:0.009992 episode_count: 28017. steps_count: 30000990.000000\n",
      "ep 1861: ep_len:674 episode reward: total was -10.600000. running mean: -20.407099\n",
      "ep 1861: ep_len:697 episode reward: total was -60.090000. running mean: -20.803928\n",
      "ep 1861: ep_len:2910 episode reward: total was -80.550000. running mean: -21.401389\n",
      "ep 1861: ep_len:729 episode reward: total was -10.050000. running mean: -21.287875\n",
      "ep 1861: ep_len:161 episode reward: total was 79.000000. running mean: -20.284996\n",
      "ep 1861: ep_len:62 episode reward: total was 28.000000. running mean: -19.802146\n",
      "ep 1861: ep_len:57 episode reward: total was 25.500000. running mean: -19.349125\n",
      "ep 1861: ep_len:644 episode reward: total was 5.710000. running mean: -19.098534\n",
      "ep 1861: ep_len:3528 episode reward: total was -40.260000. running mean: -19.310148\n",
      "ep 1861: ep_len:4177 episode reward: total was -1341.640000. running mean: -32.533447\n",
      "ep 1861: ep_len:7161 episode reward: total was -510.620000. running mean: -37.314312\n",
      "ep 1861: ep_len:947 episode reward: total was -15.030000. running mean: -37.091469\n",
      "ep 1861: ep_len:48 episode reward: total was 21.000000. running mean: -36.510554\n",
      "ep 1861: ep_len:207 episode reward: total was 99.000000. running mean: -35.155449\n",
      "ep 1861: ep_len:48 episode reward: total was 22.500000. running mean: -34.578894\n",
      "ep 1861: ep_len:100 episode reward: total was 45.500000. running mean: -33.778105\n",
      "ep 1861: ep_len:604 episode reward: total was 11.190000. running mean: -33.328424\n",
      "ep 1861: ep_len:2875 episode reward: total was -17.910000. running mean: -33.174240\n",
      "ep 1861: ep_len:60 episode reward: total was 27.000000. running mean: -32.572498\n",
      "epsilon:0.009992 episode_count: 28036. steps_count: 30026679.000000\n",
      "ep 1862: ep_len:1430 episode reward: total was 15.080000. running mean: -32.095973\n",
      "ep 1862: ep_len:648 episode reward: total was -21.970000. running mean: -31.994713\n",
      "ep 1862: ep_len:32 episode reward: total was 14.500000. running mean: -31.529766\n",
      "ep 1862: ep_len:2868 episode reward: total was -25.810000. running mean: -31.472568\n",
      "ep 1862: ep_len:1216 episode reward: total was -17.930000. running mean: -31.337143\n",
      "ep 1862: ep_len:69 episode reward: total was 31.500000. running mean: -30.708771\n",
      "ep 1862: ep_len:1080 episode reward: total was -145.920000. running mean: -31.860883\n",
      "ep 1862: ep_len:661 episode reward: total was 21.350000. running mean: -31.328775\n",
      "ep 1862: ep_len:526 episode reward: total was -35.310000. running mean: -31.368587\n",
      "ep 1862: ep_len:850 episode reward: total was 52.010000. running mean: -30.534801\n",
      "ep 1862: ep_len:2544 episode reward: total was -129.630000. running mean: -31.525753\n",
      "ep 1862: ep_len:81 episode reward: total was 34.500000. running mean: -30.865495\n",
      "ep 1862: ep_len:149 episode reward: total was 67.000000. running mean: -29.886840\n",
      "ep 1862: ep_len:68 episode reward: total was 32.500000. running mean: -29.262972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1862: ep_len:1205 episode reward: total was -6.780000. running mean: -29.038142\n",
      "ep 1862: ep_len:2780 episode reward: total was -242.900000. running mean: -31.176761\n",
      "ep 1862: ep_len:58 episode reward: total was 26.000000. running mean: -30.604993\n",
      "epsilon:0.009992 episode_count: 28053. steps_count: 30042944.000000\n",
      "ep 1863: ep_len:658 episode reward: total was -9.750000. running mean: -30.396443\n",
      "ep 1863: ep_len:1629 episode reward: total was -54.610000. running mean: -30.638579\n",
      "ep 1863: ep_len:2917 episode reward: total was -24.900000. running mean: -30.581193\n",
      "ep 1863: ep_len:500 episode reward: total was 6.220000. running mean: -30.213181\n",
      "ep 1863: ep_len:59 episode reward: total was 26.500000. running mean: -29.646049\n",
      "ep 1863: ep_len:873 episode reward: total was 54.950000. running mean: -28.800089\n",
      "ep 1863: ep_len:613 episode reward: total was 27.210000. running mean: -28.239988\n",
      "ep 1863: ep_len:824 episode reward: total was 18.570000. running mean: -27.771888\n",
      "ep 1863: ep_len:617 episode reward: total was 4.350000. running mean: -27.450669\n",
      "ep 1863: ep_len:543 episode reward: total was 14.640000. running mean: -27.029763\n",
      "ep 1863: ep_len:119 episode reward: total was 56.500000. running mean: -26.194465\n",
      "ep 1863: ep_len:803 episode reward: total was -45.120000. running mean: -26.383720\n",
      "ep 1863: ep_len:2902 episode reward: total was -31.480000. running mean: -26.434683\n",
      "epsilon:0.009992 episode_count: 28066. steps_count: 30056001.000000\n",
      "ep 1864: ep_len:750 episode reward: total was -99.730000. running mean: -27.167636\n",
      "ep 1864: ep_len:763 episode reward: total was -11.300000. running mean: -27.008960\n",
      "ep 1864: ep_len:3104 episode reward: total was -12.230000. running mean: -26.861170\n",
      "ep 1864: ep_len:1267 episode reward: total was -11.330000. running mean: -26.705859\n",
      "ep 1864: ep_len:136 episode reward: total was 65.000000. running mean: -25.788800\n",
      "ep 1864: ep_len:62 episode reward: total was 28.000000. running mean: -25.250912\n",
      "ep 1864: ep_len:685 episode reward: total was 31.980000. running mean: -24.678603\n",
      "ep 1864: ep_len:3670 episode reward: total was -369.110000. running mean: -28.122917\n",
      "ep 1864: ep_len:4214 episode reward: total was -1371.510000. running mean: -41.556788\n",
      "ep 1864: ep_len:836 episode reward: total was 36.680000. running mean: -40.774420\n",
      "ep 1864: ep_len:936 episode reward: total was 39.070000. running mean: -39.975976\n",
      "ep 1864: ep_len:62 episode reward: total was 29.500000. running mean: -39.281216\n",
      "ep 1864: ep_len:1148 episode reward: total was -31.020000. running mean: -39.198604\n",
      "ep 1864: ep_len:2816 episode reward: total was -21.410000. running mean: -39.020718\n",
      "ep 1864: ep_len:72 episode reward: total was 33.000000. running mean: -38.300511\n",
      "epsilon:0.009992 episode_count: 28081. steps_count: 30076522.000000\n",
      "ep 1865: ep_len:581 episode reward: total was 0.820000. running mean: -37.909305\n",
      "ep 1865: ep_len:792 episode reward: total was 7.800000. running mean: -37.452212\n",
      "ep 1865: ep_len:38 episode reward: total was 17.500000. running mean: -36.902690\n",
      "ep 1865: ep_len:3042 episode reward: total was -578.040000. running mean: -42.314063\n",
      "ep 1865: ep_len:500 episode reward: total was -37.220000. running mean: -42.263123\n",
      "ep 1865: ep_len:39 episode reward: total was 18.000000. running mean: -41.660491\n",
      "ep 1865: ep_len:165 episode reward: total was 79.500000. running mean: -40.448887\n",
      "ep 1865: ep_len:63 episode reward: total was 30.000000. running mean: -39.744398\n",
      "ep 1865: ep_len:1163 episode reward: total was 9.110000. running mean: -39.255854\n",
      "ep 1865: ep_len:3639 episode reward: total was -40.160000. running mean: -39.264895\n",
      "ep 1865: ep_len:541 episode reward: total was -0.270000. running mean: -38.874946\n",
      "ep 1865: ep_len:745 episode reward: total was -59.930000. running mean: -39.085497\n",
      "ep 1865: ep_len:987 episode reward: total was 8.600000. running mean: -38.608642\n",
      "ep 1865: ep_len:49 episode reward: total was 20.000000. running mean: -38.022555\n",
      "ep 1865: ep_len:118 episode reward: total was 53.000000. running mean: -37.112330\n",
      "ep 1865: ep_len:752 episode reward: total was -12.850000. running mean: -36.869707\n",
      "ep 1865: ep_len:2887 episode reward: total was -10.440000. running mean: -36.605409\n",
      "epsilon:0.009992 episode_count: 28098. steps_count: 30092623.000000\n",
      "ep 1866: ep_len:1515 episode reward: total was 22.850000. running mean: -36.010855\n",
      "ep 1866: ep_len:778 episode reward: total was -46.660000. running mean: -36.117347\n",
      "ep 1866: ep_len:50 episode reward: total was 23.500000. running mean: -35.521173\n",
      "ep 1866: ep_len:3016 episode reward: total was -109.220000. running mean: -36.258162\n",
      "ep 1866: ep_len:794 episode reward: total was -19.500000. running mean: -36.090580\n",
      "ep 1866: ep_len:88 episode reward: total was 41.000000. running mean: -35.319674\n",
      "ep 1866: ep_len:627 episode reward: total was -0.520000. running mean: -34.971677\n",
      "ep 1866: ep_len:3632 episode reward: total was -65.480000. running mean: -35.276761\n",
      "ep 1866: ep_len:990 episode reward: total was -61.460000. running mean: -35.538593\n",
      "ep 1866: ep_len:791 episode reward: total was 12.590000. running mean: -35.057307\n",
      "ep 1866: ep_len:500 episode reward: total was 1.780000. running mean: -34.688934\n",
      "ep 1866: ep_len:107 episode reward: total was 52.000000. running mean: -33.822045\n",
      "ep 1866: ep_len:980 episode reward: total was -239.320000. running mean: -35.877024\n",
      "ep 1866: ep_len:2855 episode reward: total was -5.960000. running mean: -35.577854\n",
      "ep 1866: ep_len:53 episode reward: total was 23.500000. running mean: -34.987075\n",
      "epsilon:0.009992 episode_count: 28113. steps_count: 30109399.000000\n",
      "ep 1867: ep_len:656 episode reward: total was 8.680000. running mean: -34.550405\n",
      "ep 1867: ep_len:979 episode reward: total was 11.400000. running mean: -34.090901\n",
      "ep 1867: ep_len:48 episode reward: total was 22.500000. running mean: -33.524992\n",
      "ep 1867: ep_len:2988 episode reward: total was -96.130000. running mean: -34.151042\n",
      "ep 1867: ep_len:540 episode reward: total was -0.830000. running mean: -33.817831\n",
      "ep 1867: ep_len:54 episode reward: total was 25.500000. running mean: -33.224653\n",
      "ep 1867: ep_len:177 episode reward: total was 85.500000. running mean: -32.037407\n",
      "ep 1867: ep_len:61 episode reward: total was 29.000000. running mean: -31.427032\n",
      "ep 1867: ep_len:853 episode reward: total was 22.900000. running mean: -30.883762\n",
      "ep 1867: ep_len:3678 episode reward: total was -6.010000. running mean: -30.635024\n",
      "ep 1867: ep_len:583 episode reward: total was 1.620000. running mean: -30.312474\n",
      "ep 1867: ep_len:637 episode reward: total was 6.390000. running mean: -29.945450\n",
      "ep 1867: ep_len:569 episode reward: total was 27.330000. running mean: -29.372695\n",
      "ep 1867: ep_len:159 episode reward: total was 73.500000. running mean: -28.343968\n",
      "ep 1867: ep_len:41 episode reward: total was 19.000000. running mean: -27.870528\n",
      "ep 1867: ep_len:746 episode reward: total was -62.400000. running mean: -28.215823\n",
      "ep 1867: ep_len:2903 episode reward: total was -8.930000. running mean: -28.022965\n",
      "epsilon:0.009992 episode_count: 28130. steps_count: 30125071.000000\n",
      "ep 1868: ep_len:580 episode reward: total was -23.020000. running mean: -27.972935\n",
      "ep 1868: ep_len:733 episode reward: total was -13.040000. running mean: -27.823606\n",
      "ep 1868: ep_len:59 episode reward: total was 28.000000. running mean: -27.265370\n",
      "ep 1868: ep_len:2926 episode reward: total was -22.940000. running mean: -27.222116\n",
      "ep 1868: ep_len:677 episode reward: total was 16.740000. running mean: -26.782495\n",
      "ep 1868: ep_len:500 episode reward: total was 2.700000. running mean: -26.487670\n",
      "ep 1868: ep_len:639 episode reward: total was 11.830000. running mean: -26.104493\n",
      "ep 1868: ep_len:941 episode reward: total was -68.530000. running mean: -26.528748\n",
      "ep 1868: ep_len:745 episode reward: total was 12.150000. running mean: -26.141961\n",
      "ep 1868: ep_len:527 episode reward: total was -1.260000. running mean: -25.893141\n",
      "ep 1868: ep_len:116 episode reward: total was 55.000000. running mean: -25.084210\n",
      "ep 1868: ep_len:756 episode reward: total was -28.590000. running mean: -25.119268\n",
      "ep 1868: ep_len:2879 episode reward: total was 12.250000. running mean: -24.745575\n",
      "ep 1868: ep_len:56 episode reward: total was 26.500000. running mean: -24.233119\n",
      "epsilon:0.009992 episode_count: 28144. steps_count: 30137205.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1869: ep_len:1439 episode reward: total was 7.210000. running mean: -23.918688\n",
      "ep 1869: ep_len:786 episode reward: total was -12.020000. running mean: -23.799701\n",
      "ep 1869: ep_len:82 episode reward: total was 39.500000. running mean: -23.166704\n",
      "ep 1869: ep_len:2925 episode reward: total was -5.520000. running mean: -22.990237\n",
      "ep 1869: ep_len:622 episode reward: total was -3.760000. running mean: -22.797935\n",
      "ep 1869: ep_len:69 episode reward: total was 33.000000. running mean: -22.239955\n",
      "ep 1869: ep_len:908 episode reward: total was 27.590000. running mean: -21.741656\n",
      "ep 1869: ep_len:330 episode reward: total was 6.600000. running mean: -21.458239\n",
      "ep 1869: ep_len:627 episode reward: total was -29.250000. running mean: -21.536157\n",
      "ep 1869: ep_len:685 episode reward: total was 24.420000. running mean: -21.076595\n",
      "ep 1869: ep_len:614 episode reward: total was -10.190000. running mean: -20.967729\n",
      "ep 1869: ep_len:96 episode reward: total was 46.500000. running mean: -20.293052\n",
      "ep 1869: ep_len:187 episode reward: total was 89.000000. running mean: -19.200122\n",
      "ep 1869: ep_len:40 episode reward: total was 17.000000. running mean: -18.838120\n",
      "ep 1869: ep_len:113 episode reward: total was 55.000000. running mean: -18.099739\n",
      "ep 1869: ep_len:500 episode reward: total was 24.740000. running mean: -17.671342\n",
      "ep 1869: ep_len:2897 episode reward: total was -6.920000. running mean: -17.563828\n",
      "ep 1869: ep_len:50 episode reward: total was 23.500000. running mean: -17.153190\n",
      "epsilon:0.009992 episode_count: 28162. steps_count: 30150175.000000\n",
      "ep 1870: ep_len:681 episode reward: total was 36.550000. running mean: -16.616158\n",
      "ep 1870: ep_len:746 episode reward: total was -16.950000. running mean: -16.619497\n",
      "ep 1870: ep_len:63 episode reward: total was 30.000000. running mean: -16.153302\n",
      "ep 1870: ep_len:3022 episode reward: total was -11.630000. running mean: -16.108069\n",
      "ep 1870: ep_len:592 episode reward: total was -4.350000. running mean: -15.990488\n",
      "ep 1870: ep_len:62 episode reward: total was 29.500000. running mean: -15.535583\n",
      "ep 1870: ep_len:154 episode reward: total was 75.500000. running mean: -14.625227\n",
      "ep 1870: ep_len:53 episode reward: total was 25.000000. running mean: -14.228975\n",
      "ep 1870: ep_len:59 episode reward: total was 28.000000. running mean: -13.806685\n",
      "ep 1870: ep_len:788 episode reward: total was 19.650000. running mean: -13.472118\n",
      "ep 1870: ep_len:657 episode reward: total was 25.750000. running mean: -13.079897\n",
      "ep 1870: ep_len:1332 episode reward: total was -115.730000. running mean: -14.106398\n",
      "ep 1870: ep_len:843 episode reward: total was 42.120000. running mean: -13.544134\n",
      "ep 1870: ep_len:500 episode reward: total was 10.440000. running mean: -13.304293\n",
      "ep 1870: ep_len:60 episode reward: total was 25.500000. running mean: -12.916250\n",
      "ep 1870: ep_len:1506 episode reward: total was -5.590000. running mean: -12.842987\n",
      "ep 1870: ep_len:2867 episode reward: total was -0.540000. running mean: -12.719958\n",
      "ep 1870: ep_len:38 episode reward: total was 17.500000. running mean: -12.417758\n",
      "epsilon:0.009992 episode_count: 28180. steps_count: 30164198.000000\n",
      "ep 1871: ep_len:1174 episode reward: total was -16.560000. running mean: -12.459180\n",
      "ep 1871: ep_len:500 episode reward: total was 29.270000. running mean: -12.041889\n",
      "ep 1871: ep_len:2976 episode reward: total was 1.200000. running mean: -11.909470\n",
      "ep 1871: ep_len:500 episode reward: total was -15.990000. running mean: -11.950275\n",
      "ep 1871: ep_len:96 episode reward: total was 46.500000. running mean: -11.365772\n",
      "ep 1871: ep_len:66 episode reward: total was 31.500000. running mean: -10.937115\n",
      "ep 1871: ep_len:1417 episode reward: total was 11.430000. running mean: -10.713443\n",
      "ep 1871: ep_len:663 episode reward: total was 20.060000. running mean: -10.405709\n",
      "ep 1871: ep_len:788 episode reward: total was 12.850000. running mean: -10.173152\n",
      "ep 1871: ep_len:640 episode reward: total was 6.390000. running mean: -10.007520\n",
      "ep 1871: ep_len:500 episode reward: total was 37.420000. running mean: -9.533245\n",
      "ep 1871: ep_len:77 episode reward: total was 37.000000. running mean: -9.067913\n",
      "ep 1871: ep_len:72 episode reward: total was 34.500000. running mean: -8.632234\n",
      "ep 1871: ep_len:916 episode reward: total was 18.910000. running mean: -8.356811\n",
      "ep 1871: ep_len:2807 episode reward: total was -23.480000. running mean: -8.508043\n",
      "epsilon:0.009992 episode_count: 28195. steps_count: 30177390.000000\n",
      "ep 1872: ep_len:641 episode reward: total was 12.940000. running mean: -8.293563\n",
      "ep 1872: ep_len:216 episode reward: total was 7.420000. running mean: -8.136427\n",
      "ep 1872: ep_len:2927 episode reward: total was -66.450000. running mean: -8.719563\n",
      "ep 1872: ep_len:685 episode reward: total was 0.620000. running mean: -8.626167\n",
      "ep 1872: ep_len:39 episode reward: total was 16.500000. running mean: -8.374906\n",
      "ep 1872: ep_len:51 episode reward: total was 22.500000. running mean: -8.066156\n",
      "ep 1872: ep_len:664 episode reward: total was -9.690000. running mean: -8.082395\n",
      "ep 1872: ep_len:3845 episode reward: total was -893.490000. running mean: -16.936471\n",
      "ep 1872: ep_len:663 episode reward: total was -3.630000. running mean: -16.803406\n",
      "ep 1872: ep_len:737 episode reward: total was 59.600000. running mean: -16.039372\n",
      "ep 1872: ep_len:923 episode reward: total was 49.330000. running mean: -15.385678\n",
      "ep 1872: ep_len:65 episode reward: total was 29.500000. running mean: -14.936822\n",
      "ep 1872: ep_len:53 episode reward: total was 23.500000. running mean: -14.552453\n",
      "ep 1872: ep_len:831 episode reward: total was -29.650000. running mean: -14.703429\n",
      "ep 1872: ep_len:2888 episode reward: total was -109.510000. running mean: -15.651495\n",
      "epsilon:0.009992 episode_count: 28210. steps_count: 30192618.000000\n",
      "ep 1873: ep_len:1137 episode reward: total was 18.100000. running mean: -15.313980\n",
      "ep 1873: ep_len:1037 episode reward: total was 26.130000. running mean: -14.899540\n",
      "ep 1873: ep_len:46 episode reward: total was 21.500000. running mean: -14.535544\n",
      "ep 1873: ep_len:2877 episode reward: total was -52.590000. running mean: -14.916089\n",
      "ep 1873: ep_len:625 episode reward: total was -17.680000. running mean: -14.943728\n",
      "ep 1873: ep_len:55 episode reward: total was 24.500000. running mean: -14.549291\n",
      "ep 1873: ep_len:148 episode reward: total was 71.000000. running mean: -13.693798\n",
      "ep 1873: ep_len:648 episode reward: total was -1.740000. running mean: -13.574260\n",
      "ep 1873: ep_len:659 episode reward: total was 30.180000. running mean: -13.136717\n",
      "ep 1873: ep_len:986 episode reward: total was -31.860000. running mean: -13.323950\n",
      "ep 1873: ep_len:781 episode reward: total was 42.290000. running mean: -12.767811\n",
      "ep 1873: ep_len:1100 episode reward: total was -7.350000. running mean: -12.713633\n",
      "ep 1873: ep_len:45 episode reward: total was 21.000000. running mean: -12.376496\n",
      "ep 1873: ep_len:203 episode reward: total was 98.500000. running mean: -11.267731\n",
      "ep 1873: ep_len:98 episode reward: total was 47.500000. running mean: -10.680054\n",
      "ep 1873: ep_len:1190 episode reward: total was -11.990000. running mean: -10.693153\n",
      "ep 1873: ep_len:2913 episode reward: total was -6.720000. running mean: -10.653422\n",
      "epsilon:0.009992 episode_count: 28227. steps_count: 30207166.000000\n",
      "ep 1874: ep_len:1426 episode reward: total was 2.180000. running mean: -10.525088\n",
      "ep 1874: ep_len:652 episode reward: total was -24.960000. running mean: -10.669437\n",
      "ep 1874: ep_len:2992 episode reward: total was -73.880000. running mean: -11.301542\n",
      "ep 1874: ep_len:539 episode reward: total was -21.040000. running mean: -11.398927\n",
      "ep 1874: ep_len:34 episode reward: total was 15.500000. running mean: -11.129938\n",
      "ep 1874: ep_len:121 episode reward: total was 57.500000. running mean: -10.443638\n",
      "ep 1874: ep_len:83 episode reward: total was 38.500000. running mean: -9.954202\n",
      "ep 1874: ep_len:70 episode reward: total was 33.500000. running mean: -9.519660\n",
      "ep 1874: ep_len:844 episode reward: total was 39.040000. running mean: -9.034063\n",
      "ep 1874: ep_len:341 episode reward: total was 19.050000. running mean: -8.753223\n",
      "ep 1874: ep_len:4283 episode reward: total was -1224.610000. running mean: -20.911791\n",
      "ep 1874: ep_len:641 episode reward: total was -14.450000. running mean: -20.847173\n",
      "ep 1874: ep_len:892 episode reward: total was 36.700000. running mean: -20.271701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1874: ep_len:45 episode reward: total was 21.000000. running mean: -19.858984\n",
      "ep 1874: ep_len:1152 episode reward: total was -25.990000. running mean: -19.920294\n",
      "ep 1874: ep_len:2778 episode reward: total was -6.390000. running mean: -19.784991\n",
      "epsilon:0.009992 episode_count: 28243. steps_count: 30224059.000000\n",
      "ep 1875: ep_len:660 episode reward: total was 31.040000. running mean: -19.276741\n",
      "ep 1875: ep_len:727 episode reward: total was -42.640000. running mean: -19.510374\n",
      "ep 1875: ep_len:3133 episode reward: total was -11.800000. running mean: -19.433270\n",
      "ep 1875: ep_len:638 episode reward: total was -12.810000. running mean: -19.367037\n",
      "ep 1875: ep_len:49 episode reward: total was 23.000000. running mean: -18.943367\n",
      "ep 1875: ep_len:103 episode reward: total was 50.000000. running mean: -18.253933\n",
      "ep 1875: ep_len:649 episode reward: total was 1.270000. running mean: -18.058694\n",
      "ep 1875: ep_len:3750 episode reward: total was -209.050000. running mean: -19.968607\n",
      "ep 1875: ep_len:729 episode reward: total was -89.580000. running mean: -20.664721\n",
      "ep 1875: ep_len:812 episode reward: total was 25.880000. running mean: -20.199274\n",
      "ep 1875: ep_len:867 episode reward: total was 29.840000. running mean: -19.698881\n",
      "ep 1875: ep_len:113 episode reward: total was 53.500000. running mean: -18.966892\n",
      "ep 1875: ep_len:924 episode reward: total was -76.780000. running mean: -19.545023\n",
      "ep 1875: ep_len:2844 episode reward: total was -42.640000. running mean: -19.775973\n",
      "epsilon:0.009992 episode_count: 28257. steps_count: 30240057.000000\n",
      "ep 1876: ep_len:631 episode reward: total was -6.990000. running mean: -19.648113\n",
      "ep 1876: ep_len:1653 episode reward: total was -47.300000. running mean: -19.924632\n",
      "ep 1876: ep_len:3053 episode reward: total was -20.860000. running mean: -19.933986\n",
      "ep 1876: ep_len:500 episode reward: total was 27.380000. running mean: -19.460846\n",
      "ep 1876: ep_len:158 episode reward: total was 76.000000. running mean: -18.506238\n",
      "ep 1876: ep_len:92 episode reward: total was 44.500000. running mean: -17.876175\n",
      "ep 1876: ep_len:28 episode reward: total was 12.500000. running mean: -17.572413\n",
      "ep 1876: ep_len:624 episode reward: total was 1.910000. running mean: -17.377589\n",
      "ep 1876: ep_len:640 episode reward: total was 27.450000. running mean: -16.929313\n",
      "ep 1876: ep_len:1227 episode reward: total was -57.590000. running mean: -17.335920\n",
      "ep 1876: ep_len:850 episode reward: total was 46.400000. running mean: -16.698561\n",
      "ep 1876: ep_len:576 episode reward: total was -6.530000. running mean: -16.596875\n",
      "ep 1876: ep_len:1075 episode reward: total was 32.590000. running mean: -16.105007\n",
      "ep 1876: ep_len:2826 episode reward: total was -15.740000. running mean: -16.101357\n",
      "ep 1876: ep_len:60 episode reward: total was 24.000000. running mean: -15.700343\n",
      "epsilon:0.009992 episode_count: 28272. steps_count: 30254050.000000\n",
      "ep 1877: ep_len:753 episode reward: total was -91.590000. running mean: -16.459240\n",
      "ep 1877: ep_len:1248 episode reward: total was -67.480000. running mean: -16.969447\n",
      "ep 1877: ep_len:2927 episode reward: total was -82.120000. running mean: -17.620953\n",
      "ep 1877: ep_len:640 episode reward: total was -11.070000. running mean: -17.555443\n",
      "ep 1877: ep_len:139 episode reward: total was 66.500000. running mean: -16.714889\n",
      "ep 1877: ep_len:652 episode reward: total was 16.060000. running mean: -16.387140\n",
      "ep 1877: ep_len:648 episode reward: total was 18.470000. running mean: -16.038569\n",
      "ep 1877: ep_len:838 episode reward: total was 21.980000. running mean: -15.658383\n",
      "ep 1877: ep_len:689 episode reward: total was 12.390000. running mean: -15.377899\n",
      "ep 1877: ep_len:611 episode reward: total was -4.520000. running mean: -15.269320\n",
      "ep 1877: ep_len:35 episode reward: total was 16.000000. running mean: -14.956627\n",
      "ep 1877: ep_len:653 episode reward: total was -3.740000. running mean: -14.844461\n",
      "ep 1877: ep_len:2782 episode reward: total was -31.200000. running mean: -15.008016\n",
      "ep 1877: ep_len:48 episode reward: total was 22.500000. running mean: -14.632936\n",
      "epsilon:0.009992 episode_count: 28286. steps_count: 30266713.000000\n",
      "ep 1878: ep_len:1503 episode reward: total was 7.210000. running mean: -14.414506\n",
      "ep 1878: ep_len:1223 episode reward: total was -77.830000. running mean: -15.048661\n",
      "ep 1878: ep_len:51 episode reward: total was 24.000000. running mean: -14.658175\n",
      "ep 1878: ep_len:3065 episode reward: total was -45.190000. running mean: -14.963493\n",
      "ep 1878: ep_len:854 episode reward: total was 42.970000. running mean: -14.384158\n",
      "ep 1878: ep_len:69 episode reward: total was 33.000000. running mean: -13.910316\n",
      "ep 1878: ep_len:618 episode reward: total was 9.520000. running mean: -13.676013\n",
      "ep 1878: ep_len:3789 episode reward: total was -385.340000. running mean: -17.392653\n",
      "ep 1878: ep_len:637 episode reward: total was -16.810000. running mean: -17.386827\n",
      "ep 1878: ep_len:794 episode reward: total was 7.050000. running mean: -17.142458\n",
      "ep 1878: ep_len:1455 episode reward: total was 5.470000. running mean: -16.916334\n",
      "ep 1878: ep_len:76 episode reward: total was 36.500000. running mean: -16.382170\n",
      "ep 1878: ep_len:927 episode reward: total was 18.560000. running mean: -16.032749\n",
      "ep 1878: ep_len:45 episode reward: total was 21.000000. running mean: -15.662421\n",
      "ep 1878: ep_len:42 episode reward: total was 19.500000. running mean: -15.310797\n",
      "epsilon:0.009992 episode_count: 28301. steps_count: 30281861.000000\n",
      "ep 1879: ep_len:1511 episode reward: total was 28.900000. running mean: -14.868689\n",
      "ep 1879: ep_len:500 episode reward: total was -4.010000. running mean: -14.760102\n",
      "ep 1879: ep_len:48 episode reward: total was 19.500000. running mean: -14.417501\n",
      "ep 1879: ep_len:2929 episode reward: total was -53.990000. running mean: -14.813226\n",
      "ep 1879: ep_len:846 episode reward: total was -21.010000. running mean: -14.875194\n",
      "ep 1879: ep_len:160 episode reward: total was 77.000000. running mean: -13.956442\n",
      "ep 1879: ep_len:98 episode reward: total was 47.500000. running mean: -13.341878\n",
      "ep 1879: ep_len:831 episode reward: total was -11.080000. running mean: -13.319259\n",
      "ep 1879: ep_len:654 episode reward: total was 22.230000. running mean: -12.963766\n",
      "ep 1879: ep_len:725 episode reward: total was -17.790000. running mean: -13.012029\n",
      "ep 1879: ep_len:866 episode reward: total was 71.890000. running mean: -12.163008\n",
      "ep 1879: ep_len:1450 episode reward: total was -6.210000. running mean: -12.103478\n",
      "ep 1879: ep_len:1163 episode reward: total was -0.100000. running mean: -11.983443\n",
      "ep 1879: ep_len:2829 episode reward: total was 5.350000. running mean: -11.810109\n",
      "epsilon:0.009992 episode_count: 28315. steps_count: 30296471.000000\n",
      "ep 1880: ep_len:1082 episode reward: total was -7.530000. running mean: -11.767308\n",
      "ep 1880: ep_len:675 episode reward: total was -6.090000. running mean: -11.710535\n",
      "ep 1880: ep_len:2963 episode reward: total was -84.810000. running mean: -12.441529\n",
      "ep 1880: ep_len:500 episode reward: total was 21.350000. running mean: -12.103614\n",
      "ep 1880: ep_len:123 episode reward: total was 58.500000. running mean: -11.397578\n",
      "ep 1880: ep_len:1517 episode reward: total was -545.620000. running mean: -16.739802\n",
      "ep 1880: ep_len:3682 episode reward: total was -77.910000. running mean: -17.351504\n",
      "ep 1880: ep_len:1281 episode reward: total was -79.270000. running mean: -17.970689\n",
      "ep 1880: ep_len:701 episode reward: total was 26.600000. running mean: -17.524982\n",
      "ep 1880: ep_len:626 episode reward: total was -5.550000. running mean: -17.405232\n",
      "ep 1880: ep_len:65 episode reward: total was 31.000000. running mean: -16.921180\n",
      "ep 1880: ep_len:500 episode reward: total was 0.030000. running mean: -16.751668\n",
      "ep 1880: ep_len:2874 episode reward: total was 10.550000. running mean: -16.478652\n",
      "epsilon:0.009992 episode_count: 28328. steps_count: 30313060.000000\n",
      "ep 1881: ep_len:1468 episode reward: total was 19.500000. running mean: -16.118865\n",
      "ep 1881: ep_len:216 episode reward: total was 12.470000. running mean: -15.832976\n",
      "ep 1881: ep_len:69 episode reward: total was 33.000000. running mean: -15.344647\n",
      "ep 1881: ep_len:3036 episode reward: total was -46.930000. running mean: -15.660500\n",
      "ep 1881: ep_len:684 episode reward: total was 11.750000. running mean: -15.386395\n",
      "ep 1881: ep_len:61 episode reward: total was 29.000000. running mean: -14.942531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1881: ep_len:70 episode reward: total was 32.000000. running mean: -14.473106\n",
      "ep 1881: ep_len:501 episode reward: total was -3.320000. running mean: -14.361575\n",
      "ep 1881: ep_len:646 episode reward: total was 25.030000. running mean: -13.967659\n",
      "ep 1881: ep_len:576 episode reward: total was 5.810000. running mean: -13.769883\n",
      "ep 1881: ep_len:771 episode reward: total was 8.000000. running mean: -13.552184\n",
      "ep 1881: ep_len:585 episode reward: total was -9.470000. running mean: -13.511362\n",
      "ep 1881: ep_len:87 episode reward: total was 42.000000. running mean: -12.956248\n",
      "ep 1881: ep_len:500 episode reward: total was 27.380000. running mean: -12.552886\n",
      "ep 1881: ep_len:2894 episode reward: total was -41.770000. running mean: -12.845057\n",
      "epsilon:0.009992 episode_count: 28343. steps_count: 30325224.000000\n",
      "ep 1882: ep_len:1116 episode reward: total was 0.890000. running mean: -12.707706\n",
      "ep 1882: ep_len:735 episode reward: total was -12.050000. running mean: -12.701129\n",
      "ep 1882: ep_len:67 episode reward: total was 30.500000. running mean: -12.269118\n",
      "ep 1882: ep_len:2852 episode reward: total was -117.980000. running mean: -13.326227\n",
      "ep 1882: ep_len:500 episode reward: total was 23.160000. running mean: -12.961365\n",
      "ep 1882: ep_len:63 episode reward: total was 30.000000. running mean: -12.531751\n",
      "ep 1882: ep_len:108 episode reward: total was 51.000000. running mean: -11.896433\n",
      "ep 1882: ep_len:500 episode reward: total was 25.270000. running mean: -11.524769\n",
      "ep 1882: ep_len:3607 episode reward: total was -132.390000. running mean: -12.733421\n",
      "ep 1882: ep_len:532 episode reward: total was -43.330000. running mean: -13.039387\n",
      "ep 1882: ep_len:643 episode reward: total was 7.800000. running mean: -12.830993\n",
      "ep 1882: ep_len:892 episode reward: total was 38.080000. running mean: -12.321883\n",
      "ep 1882: ep_len:184 episode reward: total was 87.500000. running mean: -11.323665\n",
      "ep 1882: ep_len:26 episode reward: total was 10.000000. running mean: -11.110428\n",
      "ep 1882: ep_len:1056 episode reward: total was -2.710000. running mean: -11.026424\n",
      "ep 1882: ep_len:2892 episode reward: total was -26.030000. running mean: -11.176459\n",
      "ep 1882: ep_len:42 episode reward: total was 19.500000. running mean: -10.869695\n",
      "epsilon:0.009992 episode_count: 28360. steps_count: 30341039.000000\n",
      "ep 1883: ep_len:1176 episode reward: total was 20.140000. running mean: -10.559598\n",
      "ep 1883: ep_len:1657 episode reward: total was -62.420000. running mean: -11.078202\n",
      "ep 1883: ep_len:3040 episode reward: total was -38.090000. running mean: -11.348320\n",
      "ep 1883: ep_len:596 episode reward: total was -10.370000. running mean: -11.338537\n",
      "ep 1883: ep_len:112 episode reward: total was 53.000000. running mean: -10.695151\n",
      "ep 1883: ep_len:67 episode reward: total was 32.000000. running mean: -10.268200\n",
      "ep 1883: ep_len:500 episode reward: total was 42.080000. running mean: -9.744718\n",
      "ep 1883: ep_len:3533 episode reward: total was -43.450000. running mean: -10.081771\n",
      "ep 1883: ep_len:1537 episode reward: total was -76.020000. running mean: -10.741153\n",
      "ep 1883: ep_len:7552 episode reward: total was -103.070000. running mean: -11.664441\n",
      "ep 1883: ep_len:524 episode reward: total was 19.040000. running mean: -11.357397\n",
      "ep 1883: ep_len:153 episode reward: total was 74.510000. running mean: -10.498723\n",
      "ep 1883: ep_len:93 episode reward: total was 43.500000. running mean: -9.958736\n",
      "ep 1883: ep_len:654 episode reward: total was -8.470000. running mean: -9.943848\n",
      "ep 1883: ep_len:2764 episode reward: total was 15.970000. running mean: -9.684710\n",
      "ep 1883: ep_len:29 episode reward: total was 13.000000. running mean: -9.457863\n",
      "epsilon:0.009992 episode_count: 28376. steps_count: 30365026.000000\n",
      "ep 1884: ep_len:500 episode reward: total was 10.620000. running mean: -9.257084\n",
      "ep 1884: ep_len:661 episode reward: total was -34.970000. running mean: -9.514213\n",
      "ep 1884: ep_len:43 episode reward: total was 18.500000. running mean: -9.234071\n",
      "ep 1884: ep_len:3114 episode reward: total was -38.150000. running mean: -9.523230\n",
      "ep 1884: ep_len:1435 episode reward: total was 24.740000. running mean: -9.180598\n",
      "ep 1884: ep_len:46 episode reward: total was 21.500000. running mean: -8.873792\n",
      "ep 1884: ep_len:916 episode reward: total was 55.750000. running mean: -8.227554\n",
      "ep 1884: ep_len:3634 episode reward: total was -79.050000. running mean: -8.935779\n",
      "ep 1884: ep_len:670 episode reward: total was -6.600000. running mean: -8.912421\n",
      "ep 1884: ep_len:789 episode reward: total was 11.930000. running mean: -8.703997\n",
      "ep 1884: ep_len:758 episode reward: total was 19.980000. running mean: -8.417157\n",
      "ep 1884: ep_len:47 episode reward: total was 20.500000. running mean: -8.127985\n",
      "ep 1884: ep_len:867 episode reward: total was 11.740000. running mean: -7.929305\n",
      "ep 1884: ep_len:2835 episode reward: total was -18.920000. running mean: -8.039212\n",
      "ep 1884: ep_len:68 episode reward: total was 32.500000. running mean: -7.633820\n",
      "epsilon:0.009992 episode_count: 28391. steps_count: 30381409.000000\n",
      "ep 1885: ep_len:940 episode reward: total was -84.730000. running mean: -8.404782\n",
      "ep 1885: ep_len:708 episode reward: total was -62.000000. running mean: -8.940734\n",
      "ep 1885: ep_len:3003 episode reward: total was -59.720000. running mean: -9.448527\n",
      "ep 1885: ep_len:500 episode reward: total was -9.460000. running mean: -9.448642\n",
      "ep 1885: ep_len:50 episode reward: total was 23.500000. running mean: -9.119155\n",
      "ep 1885: ep_len:167 episode reward: total was 82.000000. running mean: -8.207964\n",
      "ep 1885: ep_len:500 episode reward: total was 18.260000. running mean: -7.943284\n",
      "ep 1885: ep_len:649 episode reward: total was 34.120000. running mean: -7.522651\n",
      "ep 1885: ep_len:580 episode reward: total was -37.280000. running mean: -7.820225\n",
      "ep 1885: ep_len:819 episode reward: total was 3.200000. running mean: -7.710022\n",
      "ep 1885: ep_len:643 episode reward: total was -0.660000. running mean: -7.639522\n",
      "ep 1885: ep_len:69 episode reward: total was 31.500000. running mean: -7.248127\n",
      "ep 1885: ep_len:1499 episode reward: total was 14.790000. running mean: -7.027746\n",
      "ep 1885: ep_len:2891 episode reward: total was -7.980000. running mean: -7.037268\n",
      "epsilon:0.009992 episode_count: 28405. steps_count: 30394427.000000\n",
      "ep 1886: ep_len:706 episode reward: total was -19.240000. running mean: -7.159295\n",
      "ep 1886: ep_len:194 episode reward: total was 6.310000. running mean: -7.024603\n",
      "ep 1886: ep_len:3004 episode reward: total was -13.840000. running mean: -7.092757\n",
      "ep 1886: ep_len:500 episode reward: total was 30.560000. running mean: -6.716229\n",
      "ep 1886: ep_len:159 episode reward: total was 78.000000. running mean: -5.869067\n",
      "ep 1886: ep_len:1459 episode reward: total was 37.020000. running mean: -5.440176\n",
      "ep 1886: ep_len:3674 episode reward: total was -116.050000. running mean: -6.546274\n",
      "ep 1886: ep_len:737 episode reward: total was -29.940000. running mean: -6.780211\n",
      "ep 1886: ep_len:7228 episode reward: total was -266.180000. running mean: -9.374209\n",
      "ep 1886: ep_len:500 episode reward: total was 22.820000. running mean: -9.052267\n",
      "ep 1886: ep_len:93 episode reward: total was 45.000000. running mean: -8.511745\n",
      "ep 1886: ep_len:175 episode reward: total was 83.000000. running mean: -7.596627\n",
      "ep 1886: ep_len:66 episode reward: total was 30.000000. running mean: -7.220661\n",
      "ep 1886: ep_len:1079 episode reward: total was 2.540000. running mean: -7.123054\n",
      "ep 1886: ep_len:2863 episode reward: total was -87.630000. running mean: -7.928124\n",
      "epsilon:0.009992 episode_count: 28420. steps_count: 30416864.000000\n",
      "ep 1887: ep_len:1434 episode reward: total was 23.840000. running mean: -7.610442\n",
      "ep 1887: ep_len:698 episode reward: total was -26.450000. running mean: -7.798838\n",
      "ep 1887: ep_len:2969 episode reward: total was -19.370000. running mean: -7.914550\n",
      "ep 1887: ep_len:804 episode reward: total was 23.510000. running mean: -7.600304\n",
      "ep 1887: ep_len:63 episode reward: total was 30.000000. running mean: -7.224301\n",
      "ep 1887: ep_len:99 episode reward: total was 46.500000. running mean: -6.687058\n",
      "ep 1887: ep_len:865 episode reward: total was 4.530000. running mean: -6.574888\n",
      "ep 1887: ep_len:3761 episode reward: total was -10.210000. running mean: -6.611239\n",
      "ep 1887: ep_len:645 episode reward: total was -26.040000. running mean: -6.805526\n",
      "ep 1887: ep_len:7340 episode reward: total was 27.600000. running mean: -6.461471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1887: ep_len:1468 episode reward: total was 21.120000. running mean: -6.185656\n",
      "ep 1887: ep_len:52 episode reward: total was 23.000000. running mean: -5.893800\n",
      "ep 1887: ep_len:42 episode reward: total was 19.500000. running mean: -5.639862\n",
      "ep 1887: ep_len:988 episode reward: total was -48.870000. running mean: -6.072163\n",
      "ep 1887: ep_len:2779 episode reward: total was -5.520000. running mean: -6.066642\n",
      "epsilon:0.009992 episode_count: 28435. steps_count: 30440871.000000\n",
      "ep 1888: ep_len:631 episode reward: total was -11.030000. running mean: -6.116275\n",
      "ep 1888: ep_len:776 episode reward: total was -7.110000. running mean: -6.126212\n",
      "ep 1888: ep_len:52 episode reward: total was 24.500000. running mean: -5.819950\n",
      "ep 1888: ep_len:3059 episode reward: total was -31.350000. running mean: -6.075251\n",
      "ep 1888: ep_len:539 episode reward: total was -23.060000. running mean: -6.245098\n",
      "ep 1888: ep_len:1437 episode reward: total was 28.490000. running mean: -5.897747\n",
      "ep 1888: ep_len:288 episode reward: total was 20.570000. running mean: -5.633070\n",
      "ep 1888: ep_len:892 episode reward: total was 26.840000. running mean: -5.308339\n",
      "ep 1888: ep_len:7276 episode reward: total was -67.670000. running mean: -5.931956\n",
      "ep 1888: ep_len:980 episode reward: total was -7.320000. running mean: -5.945836\n",
      "ep 1888: ep_len:132 episode reward: total was 63.000000. running mean: -5.256378\n",
      "ep 1888: ep_len:1199 episode reward: total was 2.950000. running mean: -5.174314\n",
      "ep 1888: ep_len:2766 episode reward: total was 1.570000. running mean: -5.106871\n",
      "ep 1888: ep_len:39 episode reward: total was 18.000000. running mean: -4.875802\n",
      "epsilon:0.009992 episode_count: 28449. steps_count: 30460937.000000\n",
      "ep 1889: ep_len:598 episode reward: total was 8.770000. running mean: -4.739344\n",
      "ep 1889: ep_len:719 episode reward: total was -31.490000. running mean: -5.006851\n",
      "ep 1889: ep_len:54 episode reward: total was 25.500000. running mean: -4.701782\n",
      "ep 1889: ep_len:3006 episode reward: total was -20.310000. running mean: -4.857864\n",
      "ep 1889: ep_len:641 episode reward: total was 9.290000. running mean: -4.716386\n",
      "ep 1889: ep_len:130 episode reward: total was 60.500000. running mean: -4.064222\n",
      "ep 1889: ep_len:78 episode reward: total was 37.500000. running mean: -3.648580\n",
      "ep 1889: ep_len:1486 episode reward: total was -230.490000. running mean: -5.916994\n",
      "ep 1889: ep_len:3810 episode reward: total was -83.020000. running mean: -6.688024\n",
      "ep 1889: ep_len:615 episode reward: total was 2.950000. running mean: -6.591644\n",
      "ep 1889: ep_len:601 episode reward: total was -2.790000. running mean: -6.553627\n",
      "ep 1889: ep_len:587 episode reward: total was -4.890000. running mean: -6.536991\n",
      "ep 1889: ep_len:139 episode reward: total was 65.000000. running mean: -5.821621\n",
      "ep 1889: ep_len:61 episode reward: total was 29.000000. running mean: -5.473405\n",
      "ep 1889: ep_len:500 episode reward: total was 27.040000. running mean: -5.148271\n",
      "ep 1889: ep_len:2850 episode reward: total was -18.030000. running mean: -5.277088\n",
      "epsilon:0.009992 episode_count: 28465. steps_count: 30476812.000000\n",
      "ep 1890: ep_len:656 episode reward: total was 18.810000. running mean: -5.036217\n",
      "ep 1890: ep_len:509 episode reward: total was -17.900000. running mean: -5.164855\n",
      "ep 1890: ep_len:50 episode reward: total was 22.000000. running mean: -4.893206\n",
      "ep 1890: ep_len:2963 episode reward: total was -50.190000. running mean: -5.346174\n",
      "ep 1890: ep_len:721 episode reward: total was -21.240000. running mean: -5.505113\n",
      "ep 1890: ep_len:47 episode reward: total was 22.000000. running mean: -5.230062\n",
      "ep 1890: ep_len:62 episode reward: total was 25.000000. running mean: -4.927761\n",
      "ep 1890: ep_len:910 episode reward: total was 60.890000. running mean: -4.269583\n",
      "ep 1890: ep_len:3574 episode reward: total was -92.410000. running mean: -5.150987\n",
      "ep 1890: ep_len:1280 episode reward: total was -204.000000. running mean: -7.139478\n",
      "ep 1890: ep_len:733 episode reward: total was 4.480000. running mean: -7.023283\n",
      "ep 1890: ep_len:1485 episode reward: total was 19.240000. running mean: -6.760650\n",
      "ep 1890: ep_len:106 episode reward: total was 50.000000. running mean: -6.193043\n",
      "ep 1890: ep_len:100 episode reward: total was 48.500000. running mean: -5.646113\n",
      "ep 1890: ep_len:1178 episode reward: total was -4.270000. running mean: -5.632352\n",
      "ep 1890: ep_len:2798 episode reward: total was 1.190000. running mean: -5.564128\n",
      "epsilon:0.009992 episode_count: 28481. steps_count: 30493984.000000\n",
      "ep 1891: ep_len:643 episode reward: total was -3.840000. running mean: -5.546887\n",
      "ep 1891: ep_len:703 episode reward: total was -33.390000. running mean: -5.825318\n",
      "ep 1891: ep_len:75 episode reward: total was 36.000000. running mean: -5.407065\n",
      "ep 1891: ep_len:2983 episode reward: total was -22.750000. running mean: -5.580494\n",
      "ep 1891: ep_len:683 episode reward: total was 11.030000. running mean: -5.414389\n",
      "ep 1891: ep_len:41 episode reward: total was 17.500000. running mean: -5.185246\n",
      "ep 1891: ep_len:1387 episode reward: total was -227.200000. running mean: -7.405393\n",
      "ep 1891: ep_len:670 episode reward: total was 30.320000. running mean: -7.028139\n",
      "ep 1891: ep_len:784 episode reward: total was -20.930000. running mean: -7.167158\n",
      "ep 1891: ep_len:805 episode reward: total was 21.330000. running mean: -6.882186\n",
      "ep 1891: ep_len:1027 episode reward: total was 45.980000. running mean: -6.353564\n",
      "ep 1891: ep_len:96 episode reward: total was 46.500000. running mean: -5.825029\n",
      "ep 1891: ep_len:71 episode reward: total was 34.000000. running mean: -5.426778\n",
      "ep 1891: ep_len:500 episode reward: total was 19.320000. running mean: -5.179311\n",
      "ep 1891: ep_len:2871 episode reward: total was -20.580000. running mean: -5.333318\n",
      "epsilon:0.009992 episode_count: 28496. steps_count: 30507323.000000\n",
      "ep 1892: ep_len:667 episode reward: total was -2.220000. running mean: -5.302184\n",
      "ep 1892: ep_len:673 episode reward: total was -53.970000. running mean: -5.788863\n",
      "ep 1892: ep_len:66 episode reward: total was 31.500000. running mean: -5.415974\n",
      "ep 1892: ep_len:2872 episode reward: total was -33.980000. running mean: -5.701614\n",
      "ep 1892: ep_len:541 episode reward: total was -26.070000. running mean: -5.905298\n",
      "ep 1892: ep_len:40 episode reward: total was 18.500000. running mean: -5.661245\n",
      "ep 1892: ep_len:91 episode reward: total was 41.000000. running mean: -5.194633\n",
      "ep 1892: ep_len:697 episode reward: total was 30.780000. running mean: -4.834886\n",
      "ep 1892: ep_len:3592 episode reward: total was -16.390000. running mean: -4.950437\n",
      "ep 1892: ep_len:522 episode reward: total was -35.350000. running mean: -5.254433\n",
      "ep 1892: ep_len:609 episode reward: total was 4.370000. running mean: -5.158189\n",
      "ep 1892: ep_len:1097 episode reward: total was 33.180000. running mean: -4.774807\n",
      "ep 1892: ep_len:67 episode reward: total was 30.500000. running mean: -4.422059\n",
      "ep 1892: ep_len:707 episode reward: total was -110.260000. running mean: -5.480438\n",
      "ep 1892: ep_len:2767 episode reward: total was 14.560000. running mean: -5.280034\n",
      "epsilon:0.009992 episode_count: 28511. steps_count: 30522331.000000\n",
      "ep 1893: ep_len:635 episode reward: total was 5.300000. running mean: -5.174233\n",
      "ep 1893: ep_len:849 episode reward: total was 17.340000. running mean: -4.949091\n",
      "ep 1893: ep_len:2950 episode reward: total was -46.310000. running mean: -5.362700\n",
      "ep 1893: ep_len:500 episode reward: total was 4.590000. running mean: -5.263173\n",
      "ep 1893: ep_len:1457 episode reward: total was -239.250000. running mean: -7.603041\n",
      "ep 1893: ep_len:3802 episode reward: total was -9.980000. running mean: -7.626811\n",
      "ep 1893: ep_len:617 episode reward: total was 13.100000. running mean: -7.419543\n",
      "ep 1893: ep_len:796 episode reward: total was 32.760000. running mean: -7.017747\n",
      "ep 1893: ep_len:1477 episode reward: total was 5.750000. running mean: -6.890070\n",
      "ep 1893: ep_len:95 episode reward: total was 44.500000. running mean: -6.376169\n",
      "ep 1893: ep_len:142 episode reward: total was 68.000000. running mean: -5.632408\n",
      "ep 1893: ep_len:62 episode reward: total was 29.500000. running mean: -5.281084\n",
      "ep 1893: ep_len:909 episode reward: total was 12.750000. running mean: -5.100773\n",
      "ep 1893: ep_len:2905 episode reward: total was -14.210000. running mean: -5.191865\n",
      "epsilon:0.009992 episode_count: 28525. steps_count: 30539527.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1894: ep_len:746 episode reward: total was -73.510000. running mean: -5.875046\n",
      "ep 1894: ep_len:756 episode reward: total was 6.090000. running mean: -5.755396\n",
      "ep 1894: ep_len:2937 episode reward: total was -25.490000. running mean: -5.952742\n",
      "ep 1894: ep_len:677 episode reward: total was -19.660000. running mean: -6.089814\n",
      "ep 1894: ep_len:75 episode reward: total was 36.000000. running mean: -5.668916\n",
      "ep 1894: ep_len:638 episode reward: total was -12.440000. running mean: -5.736627\n",
      "ep 1894: ep_len:3680 episode reward: total was 3.680000. running mean: -5.642461\n",
      "ep 1894: ep_len:564 episode reward: total was -6.160000. running mean: -5.647636\n",
      "ep 1894: ep_len:870 episode reward: total was 43.910000. running mean: -5.152060\n",
      "ep 1894: ep_len:1084 episode reward: total was 44.040000. running mean: -4.660139\n",
      "ep 1894: ep_len:155 episode reward: total was 76.000000. running mean: -3.853538\n",
      "ep 1894: ep_len:45 episode reward: total was 21.000000. running mean: -3.605003\n",
      "ep 1894: ep_len:73 episode reward: total was 35.000000. running mean: -3.218953\n",
      "ep 1894: ep_len:755 episode reward: total was -104.730000. running mean: -4.234063\n",
      "ep 1894: ep_len:2853 episode reward: total was -85.680000. running mean: -5.048522\n",
      "epsilon:0.009992 episode_count: 28540. steps_count: 30555435.000000\n",
      "ep 1895: ep_len:1118 episode reward: total was -0.590000. running mean: -5.003937\n",
      "ep 1895: ep_len:701 episode reward: total was -16.750000. running mean: -5.121398\n",
      "ep 1895: ep_len:3051 episode reward: total was 14.410000. running mean: -4.926084\n",
      "ep 1895: ep_len:1233 episode reward: total was -0.960000. running mean: -4.886423\n",
      "ep 1895: ep_len:891 episode reward: total was 48.790000. running mean: -4.349659\n",
      "ep 1895: ep_len:3616 episode reward: total was 1.600000. running mean: -4.290162\n",
      "ep 1895: ep_len:1591 episode reward: total was -12.480000. running mean: -4.372061\n",
      "ep 1895: ep_len:642 episode reward: total was 18.840000. running mean: -4.139940\n",
      "ep 1895: ep_len:657 episode reward: total was 17.720000. running mean: -3.921341\n",
      "ep 1895: ep_len:117 episode reward: total was 55.500000. running mean: -3.327127\n",
      "ep 1895: ep_len:864 episode reward: total was 15.570000. running mean: -3.138156\n",
      "ep 1895: ep_len:2768 episode reward: total was -10.900000. running mean: -3.215774\n",
      "epsilon:0.009992 episode_count: 28552. steps_count: 30572684.000000\n",
      "ep 1896: ep_len:1002 episode reward: total was -52.770000. running mean: -3.711317\n",
      "ep 1896: ep_len:500 episode reward: total was 12.130000. running mean: -3.552903\n",
      "ep 1896: ep_len:3102 episode reward: total was 6.390000. running mean: -3.453474\n",
      "ep 1896: ep_len:529 episode reward: total was 0.040000. running mean: -3.418540\n",
      "ep 1896: ep_len:133 episode reward: total was 63.500000. running mean: -2.749354\n",
      "ep 1896: ep_len:1472 episode reward: total was 38.520000. running mean: -2.336661\n",
      "ep 1896: ep_len:653 episode reward: total was 24.610000. running mean: -2.067194\n",
      "ep 1896: ep_len:1267 episode reward: total was -48.470000. running mean: -2.531222\n",
      "ep 1896: ep_len:863 episode reward: total was 54.650000. running mean: -1.959410\n",
      "ep 1896: ep_len:500 episode reward: total was 24.840000. running mean: -1.691416\n",
      "ep 1896: ep_len:37 episode reward: total was 15.500000. running mean: -1.519502\n",
      "ep 1896: ep_len:1082 episode reward: total was 1.560000. running mean: -1.488707\n",
      "ep 1896: ep_len:2827 episode reward: total was 2.940000. running mean: -1.444420\n",
      "ep 1896: ep_len:64 episode reward: total was 30.500000. running mean: -1.124975\n",
      "epsilon:0.009992 episode_count: 28566. steps_count: 30586715.000000\n",
      "ep 1897: ep_len:883 episode reward: total was 20.230000. running mean: -0.911426\n",
      "ep 1897: ep_len:500 episode reward: total was 17.610000. running mean: -0.726211\n",
      "ep 1897: ep_len:3013 episode reward: total was -32.310000. running mean: -1.042049\n",
      "ep 1897: ep_len:2540 episode reward: total was -663.530000. running mean: -7.666929\n",
      "ep 1897: ep_len:114 episode reward: total was 54.000000. running mean: -7.050259\n",
      "ep 1897: ep_len:68 episode reward: total was 31.000000. running mean: -6.669757\n",
      "ep 1897: ep_len:1375 episode reward: total was -109.640000. running mean: -7.699459\n",
      "ep 1897: ep_len:367 episode reward: total was 23.870000. running mean: -7.383765\n",
      "ep 1897: ep_len:1461 episode reward: total was -121.860000. running mean: -8.528527\n",
      "ep 1897: ep_len:861 episode reward: total was 63.600000. running mean: -7.807242\n",
      "ep 1897: ep_len:963 episode reward: total was 26.100000. running mean: -7.468169\n",
      "ep 1897: ep_len:176 episode reward: total was 82.000000. running mean: -6.573488\n",
      "ep 1897: ep_len:48 episode reward: total was 22.500000. running mean: -6.282753\n",
      "ep 1897: ep_len:684 episode reward: total was 31.130000. running mean: -5.908625\n",
      "ep 1897: ep_len:2835 episode reward: total was -19.350000. running mean: -6.043039\n",
      "epsilon:0.009992 episode_count: 28581. steps_count: 30602603.000000\n",
      "ep 1898: ep_len:1455 episode reward: total was 14.130000. running mean: -5.841309\n",
      "ep 1898: ep_len:658 episode reward: total was -14.340000. running mean: -5.926296\n",
      "ep 1898: ep_len:2957 episode reward: total was -20.560000. running mean: -6.072633\n",
      "ep 1898: ep_len:818 episode reward: total was 22.390000. running mean: -5.788006\n",
      "ep 1898: ep_len:53 episode reward: total was 23.500000. running mean: -5.495126\n",
      "ep 1898: ep_len:1449 episode reward: total was -211.920000. running mean: -7.559375\n",
      "ep 1898: ep_len:3819 episode reward: total was 24.990000. running mean: -7.233881\n",
      "ep 1898: ep_len:2783 episode reward: total was -577.330000. running mean: -12.934842\n",
      "ep 1898: ep_len:846 episode reward: total was 41.800000. running mean: -12.387494\n",
      "ep 1898: ep_len:655 episode reward: total was 2.340000. running mean: -12.240219\n",
      "ep 1898: ep_len:121 episode reward: total was 59.000000. running mean: -11.527817\n",
      "ep 1898: ep_len:43 episode reward: total was 20.000000. running mean: -11.212539\n",
      "ep 1898: ep_len:1070 episode reward: total was -7.650000. running mean: -11.176913\n",
      "ep 1898: ep_len:2867 episode reward: total was -10.030000. running mean: -11.165444\n",
      "ep 1898: ep_len:38 episode reward: total was 17.500000. running mean: -10.878790\n",
      "epsilon:0.009992 episode_count: 28596. steps_count: 30622235.000000\n",
      "ep 1899: ep_len:717 episode reward: total was -43.900000. running mean: -11.209002\n",
      "ep 1899: ep_len:702 episode reward: total was -0.080000. running mean: -11.097712\n",
      "ep 1899: ep_len:42 episode reward: total was 19.500000. running mean: -10.791735\n",
      "ep 1899: ep_len:2986 episode reward: total was -10.660000. running mean: -10.790417\n",
      "ep 1899: ep_len:514 episode reward: total was 1.420000. running mean: -10.668313\n",
      "ep 1899: ep_len:100 episode reward: total was 48.500000. running mean: -10.076630\n",
      "ep 1899: ep_len:500 episode reward: total was -12.790000. running mean: -10.103764\n",
      "ep 1899: ep_len:635 episode reward: total was 12.000000. running mean: -9.882726\n",
      "ep 1899: ep_len:585 episode reward: total was 25.200000. running mean: -9.531899\n",
      "ep 1899: ep_len:663 episode reward: total was 36.840000. running mean: -9.068180\n",
      "ep 1899: ep_len:1127 episode reward: total was -7.080000. running mean: -9.048298\n",
      "ep 1899: ep_len:95 episode reward: total was 44.500000. running mean: -8.512815\n",
      "ep 1899: ep_len:151 episode reward: total was 72.500000. running mean: -7.702687\n",
      "ep 1899: ep_len:64 episode reward: total was 30.500000. running mean: -7.320660\n",
      "ep 1899: ep_len:1461 episode reward: total was -1.970000. running mean: -7.267153\n",
      "ep 1899: ep_len:2818 episode reward: total was 4.540000. running mean: -7.149082\n",
      "epsilon:0.009992 episode_count: 28612. steps_count: 30635395.000000\n",
      "ep 1900: ep_len:1117 episode reward: total was -3.140000. running mean: -7.108991\n",
      "ep 1900: ep_len:500 episode reward: total was 14.240000. running mean: -6.895501\n",
      "ep 1900: ep_len:74 episode reward: total was 35.500000. running mean: -6.471546\n",
      "ep 1900: ep_len:3029 episode reward: total was 2.260000. running mean: -6.384231\n",
      "ep 1900: ep_len:652 episode reward: total was -2.870000. running mean: -6.349088\n",
      "ep 1900: ep_len:111 episode reward: total was 54.000000. running mean: -5.745598\n",
      "ep 1900: ep_len:55 episode reward: total was 26.000000. running mean: -5.428142\n",
      "ep 1900: ep_len:1369 episode reward: total was -29.910000. running mean: -5.672960\n",
      "ep 1900: ep_len:351 episode reward: total was 20.710000. running mean: -5.409131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1900: ep_len:549 episode reward: total was -44.170000. running mean: -5.796739\n",
      "ep 1900: ep_len:611 episode reward: total was -17.320000. running mean: -5.911972\n",
      "ep 1900: ep_len:788 episode reward: total was -25.620000. running mean: -6.109052\n",
      "ep 1900: ep_len:138 episode reward: total was 67.500000. running mean: -5.372962\n",
      "ep 1900: ep_len:74 episode reward: total was 35.500000. running mean: -4.964232\n",
      "ep 1900: ep_len:500 episode reward: total was 1.600000. running mean: -4.898590\n",
      "ep 1900: ep_len:2787 episode reward: total was -26.320000. running mean: -5.112804\n",
      "epsilon:0.009992 episode_count: 28628. steps_count: 30648100.000000\n",
      "ep 1901: ep_len:758 episode reward: total was -118.840000. running mean: -6.250076\n",
      "ep 1901: ep_len:867 episode reward: total was -11.130000. running mean: -6.298875\n",
      "ep 1901: ep_len:2977 episode reward: total was -4.060000. running mean: -6.276486\n",
      "ep 1901: ep_len:669 episode reward: total was -7.230000. running mean: -6.286021\n",
      "ep 1901: ep_len:65 episode reward: total was 31.000000. running mean: -5.913161\n",
      "ep 1901: ep_len:160 episode reward: total was 77.000000. running mean: -5.084030\n",
      "ep 1901: ep_len:986 episode reward: total was -43.380000. running mean: -5.466989\n",
      "ep 1901: ep_len:341 episode reward: total was 18.680000. running mean: -5.225519\n",
      "ep 1901: ep_len:859 episode reward: total was -14.680000. running mean: -5.320064\n",
      "ep 1901: ep_len:827 episode reward: total was 35.360000. running mean: -4.913263\n",
      "ep 1901: ep_len:676 episode reward: total was -7.550000. running mean: -4.939631\n",
      "ep 1901: ep_len:51 episode reward: total was 21.000000. running mean: -4.680235\n",
      "ep 1901: ep_len:983 episode reward: total was -27.100000. running mean: -4.904432\n",
      "ep 1901: ep_len:2776 episode reward: total was 6.170000. running mean: -4.793688\n",
      "epsilon:0.009992 episode_count: 28642. steps_count: 30661095.000000\n",
      "ep 1902: ep_len:500 episode reward: total was -3.610000. running mean: -4.781851\n",
      "ep 1902: ep_len:500 episode reward: total was 22.750000. running mean: -4.506532\n",
      "ep 1902: ep_len:2976 episode reward: total was -15.400000. running mean: -4.615467\n",
      "ep 1902: ep_len:578 episode reward: total was -9.020000. running mean: -4.659512\n",
      "ep 1902: ep_len:1012 episode reward: total was -3.180000. running mean: -4.644717\n",
      "ep 1902: ep_len:612 episode reward: total was 25.670000. running mean: -4.341570\n",
      "ep 1902: ep_len:618 episode reward: total was -46.750000. running mean: -4.765654\n",
      "ep 1902: ep_len:7258 episode reward: total was -139.570000. running mean: -6.113698\n",
      "ep 1902: ep_len:1164 episode reward: total was -9.550000. running mean: -6.148061\n",
      "ep 1902: ep_len:37 episode reward: total was 17.000000. running mean: -5.916580\n",
      "ep 1902: ep_len:1047 episode reward: total was -0.370000. running mean: -5.861115\n",
      "ep 1902: ep_len:2817 episode reward: total was -16.190000. running mean: -5.964403\n",
      "ep 1902: ep_len:62 episode reward: total was 29.500000. running mean: -5.609759\n",
      "epsilon:0.009992 episode_count: 28655. steps_count: 30680276.000000\n",
      "ep 1903: ep_len:1413 episode reward: total was 20.200000. running mean: -5.351662\n",
      "ep 1903: ep_len:787 episode reward: total was 11.110000. running mean: -5.187045\n",
      "ep 1903: ep_len:3016 episode reward: total was -54.080000. running mean: -5.675975\n",
      "ep 1903: ep_len:689 episode reward: total was -2.260000. running mean: -5.641815\n",
      "ep 1903: ep_len:59 episode reward: total was 26.500000. running mean: -5.320397\n",
      "ep 1903: ep_len:136 episode reward: total was 63.500000. running mean: -4.632193\n",
      "ep 1903: ep_len:68 episode reward: total was 32.500000. running mean: -4.260871\n",
      "ep 1903: ep_len:59 episode reward: total was 28.000000. running mean: -3.938262\n",
      "ep 1903: ep_len:1050 episode reward: total was 1.120000. running mean: -3.887680\n",
      "ep 1903: ep_len:3893 episode reward: total was -167.520000. running mean: -5.524003\n",
      "ep 1903: ep_len:2758 episode reward: total was -483.430000. running mean: -10.303063\n",
      "ep 1903: ep_len:870 episode reward: total was 39.010000. running mean: -9.809932\n",
      "ep 1903: ep_len:950 episode reward: total was 16.740000. running mean: -9.544433\n",
      "ep 1903: ep_len:135 episode reward: total was 67.010000. running mean: -8.778888\n",
      "ep 1903: ep_len:891 episode reward: total was 29.250000. running mean: -8.398600\n",
      "ep 1903: ep_len:2791 episode reward: total was -90.400000. running mean: -9.218614\n",
      "epsilon:0.009992 episode_count: 28671. steps_count: 30699841.000000\n",
      "ep 1904: ep_len:1103 episode reward: total was -4.290000. running mean: -9.169327\n",
      "ep 1904: ep_len:765 episode reward: total was 17.230000. running mean: -8.905334\n",
      "ep 1904: ep_len:2994 episode reward: total was -0.730000. running mean: -8.823581\n",
      "ep 1904: ep_len:1486 episode reward: total was 27.730000. running mean: -8.458045\n",
      "ep 1904: ep_len:1391 episode reward: total was 10.740000. running mean: -8.266065\n",
      "ep 1904: ep_len:3890 episode reward: total was -46.140000. running mean: -8.644804\n",
      "ep 1904: ep_len:816 episode reward: total was 28.590000. running mean: -8.272456\n",
      "ep 1904: ep_len:712 episode reward: total was 32.690000. running mean: -7.862831\n",
      "ep 1904: ep_len:1477 episode reward: total was -2.540000. running mean: -7.809603\n",
      "ep 1904: ep_len:80 episode reward: total was 38.500000. running mean: -7.346507\n",
      "ep 1904: ep_len:624 episode reward: total was 10.930000. running mean: -7.163742\n",
      "ep 1904: ep_len:2787 episode reward: total was -2.870000. running mean: -7.120805\n",
      "epsilon:0.009992 episode_count: 28683. steps_count: 30717966.000000\n",
      "ep 1905: ep_len:1036 episode reward: total was -0.920000. running mean: -7.058796\n",
      "ep 1905: ep_len:720 episode reward: total was -14.900000. running mean: -7.137209\n",
      "ep 1905: ep_len:3004 episode reward: total was -65.330000. running mean: -7.719136\n",
      "ep 1905: ep_len:660 episode reward: total was 8.840000. running mean: -7.553545\n",
      "ep 1905: ep_len:1402 episode reward: total was -1.300000. running mean: -7.491010\n",
      "ep 1905: ep_len:4073 episode reward: total was -193.820000. running mean: -9.354300\n",
      "ep 1905: ep_len:1277 episode reward: total was -52.040000. running mean: -9.781157\n",
      "ep 1905: ep_len:733 episode reward: total was 49.060000. running mean: -9.192745\n",
      "ep 1905: ep_len:580 episode reward: total was -27.180000. running mean: -9.372618\n",
      "ep 1905: ep_len:49 episode reward: total was 21.500000. running mean: -9.063891\n",
      "ep 1905: ep_len:1555 episode reward: total was 14.160000. running mean: -8.831652\n",
      "ep 1905: ep_len:2695 episode reward: total was -3.760000. running mean: -8.780936\n",
      "epsilon:0.009992 episode_count: 28695. steps_count: 30735750.000000\n",
      "ep 1906: ep_len:1193 episode reward: total was -1.860000. running mean: -8.711727\n",
      "ep 1906: ep_len:500 episode reward: total was -3.820000. running mean: -8.662809\n",
      "ep 1906: ep_len:77 episode reward: total was 37.000000. running mean: -8.206181\n",
      "ep 1906: ep_len:3057 episode reward: total was -51.800000. running mean: -8.642119\n",
      "ep 1906: ep_len:1183 episode reward: total was -274.660000. running mean: -11.302298\n",
      "ep 1906: ep_len:52 episode reward: total was 23.000000. running mean: -10.959275\n",
      "ep 1906: ep_len:141 episode reward: total was 67.500000. running mean: -10.174682\n",
      "ep 1906: ep_len:68 episode reward: total was 32.500000. running mean: -9.747936\n",
      "ep 1906: ep_len:52 episode reward: total was 24.500000. running mean: -9.405456\n",
      "ep 1906: ep_len:500 episode reward: total was -2.530000. running mean: -9.336702\n",
      "ep 1906: ep_len:3535 episode reward: total was -2001.500000. running mean: -29.258335\n",
      "ep 1906: ep_len:1552 episode reward: total was -59.460000. running mean: -29.560351\n",
      "ep 1906: ep_len:718 episode reward: total was 50.500000. running mean: -28.759748\n",
      "ep 1906: ep_len:500 episode reward: total was -23.690000. running mean: -28.709050\n",
      "ep 1906: ep_len:116 episode reward: total was 53.500000. running mean: -27.886960\n",
      "ep 1906: ep_len:66 episode reward: total was 30.000000. running mean: -27.308090\n",
      "ep 1906: ep_len:1093 episode reward: total was -47.290000. running mean: -27.507909\n",
      "ep 1906: ep_len:2813 episode reward: total was -58.870000. running mean: -27.821530\n",
      "ep 1906: ep_len:55 episode reward: total was 26.000000. running mean: -27.283315\n",
      "epsilon:0.009992 episode_count: 28714. steps_count: 30753021.000000\n",
      "ep 1907: ep_len:1173 episode reward: total was 5.230000. running mean: -26.958182\n",
      "ep 1907: ep_len:1222 episode reward: total was -23.300000. running mean: -26.921600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1907: ep_len:2948 episode reward: total was -43.560000. running mean: -27.087984\n",
      "ep 1907: ep_len:801 episode reward: total was -8.560000. running mean: -26.902704\n",
      "ep 1907: ep_len:64 episode reward: total was 30.500000. running mean: -26.328677\n",
      "ep 1907: ep_len:1040 episode reward: total was -22.520000. running mean: -26.290590\n",
      "ep 1907: ep_len:327 episode reward: total was 29.530000. running mean: -25.732384\n",
      "ep 1907: ep_len:627 episode reward: total was -12.930000. running mean: -25.604361\n",
      "ep 1907: ep_len:822 episode reward: total was 69.520000. running mean: -24.653117\n",
      "ep 1907: ep_len:670 episode reward: total was 2.610000. running mean: -24.380486\n",
      "ep 1907: ep_len:60 episode reward: total was 28.500000. running mean: -23.851681\n",
      "ep 1907: ep_len:667 episode reward: total was -3.600000. running mean: -23.649164\n",
      "ep 1907: ep_len:2960 episode reward: total was -13.390000. running mean: -23.546572\n",
      "epsilon:0.009992 episode_count: 28727. steps_count: 30766402.000000\n",
      "ep 1908: ep_len:1357 episode reward: total was 15.880000. running mean: -23.152307\n",
      "ep 1908: ep_len:687 episode reward: total was -22.590000. running mean: -23.146684\n",
      "ep 1908: ep_len:75 episode reward: total was 33.000000. running mean: -22.585217\n",
      "ep 1908: ep_len:3023 episode reward: total was 1.770000. running mean: -22.341665\n",
      "ep 1908: ep_len:547 episode reward: total was -21.970000. running mean: -22.337948\n",
      "ep 1908: ep_len:66 episode reward: total was 28.500000. running mean: -21.829569\n",
      "ep 1908: ep_len:71 episode reward: total was 34.000000. running mean: -21.271273\n",
      "ep 1908: ep_len:1404 episode reward: total was -197.580000. running mean: -23.034360\n",
      "ep 1908: ep_len:3774 episode reward: total was -110.280000. running mean: -23.906817\n",
      "ep 1908: ep_len:760 episode reward: total was -20.980000. running mean: -23.877548\n",
      "ep 1908: ep_len:731 episode reward: total was -13.330000. running mean: -23.772073\n",
      "ep 1908: ep_len:889 episode reward: total was -13.830000. running mean: -23.672652\n",
      "ep 1908: ep_len:182 episode reward: total was 86.500000. running mean: -22.570926\n",
      "ep 1908: ep_len:630 episode reward: total was 5.120000. running mean: -22.294016\n",
      "ep 1908: ep_len:2789 episode reward: total was -4.260000. running mean: -22.113676\n",
      "epsilon:0.009992 episode_count: 28742. steps_count: 30783387.000000\n",
      "ep 1909: ep_len:642 episode reward: total was 12.460000. running mean: -21.767939\n",
      "ep 1909: ep_len:712 episode reward: total was 17.190000. running mean: -21.378360\n",
      "ep 1909: ep_len:2936 episode reward: total was -16.410000. running mean: -21.328676\n",
      "ep 1909: ep_len:881 episode reward: total was 24.770000. running mean: -20.867690\n",
      "ep 1909: ep_len:83 episode reward: total was 38.500000. running mean: -20.274013\n",
      "ep 1909: ep_len:1396 episode reward: total was -108.510000. running mean: -21.156373\n",
      "ep 1909: ep_len:347 episode reward: total was 13.080000. running mean: -20.814009\n",
      "ep 1909: ep_len:649 episode reward: total was -7.270000. running mean: -20.678569\n",
      "ep 1909: ep_len:805 episode reward: total was 43.040000. running mean: -20.041383\n",
      "ep 1909: ep_len:698 episode reward: total was 2.770000. running mean: -19.813269\n",
      "ep 1909: ep_len:994 episode reward: total was -37.700000. running mean: -19.992137\n",
      "ep 1909: ep_len:2779 episode reward: total was -6.900000. running mean: -19.861215\n",
      "epsilon:0.009992 episode_count: 28754. steps_count: 30796309.000000\n",
      "ep 1910: ep_len:3800 episode reward: total was -1098.830000. running mean: -30.650903\n",
      "ep 1910: ep_len:1632 episode reward: total was -51.460000. running mean: -30.858994\n",
      "ep 1910: ep_len:2943 episode reward: total was -33.630000. running mean: -30.886704\n",
      "ep 1910: ep_len:650 episode reward: total was 17.710000. running mean: -30.400737\n",
      "ep 1910: ep_len:61 episode reward: total was 26.000000. running mean: -29.836730\n",
      "ep 1910: ep_len:63 episode reward: total was 30.000000. running mean: -29.238362\n",
      "ep 1910: ep_len:981 episode reward: total was 2.570000. running mean: -28.920279\n",
      "ep 1910: ep_len:3711 episode reward: total was -98.430000. running mean: -29.615376\n",
      "ep 1910: ep_len:1266 episode reward: total was -63.260000. running mean: -29.951822\n",
      "ep 1910: ep_len:645 episode reward: total was 18.990000. running mean: -29.462404\n",
      "ep 1910: ep_len:644 episode reward: total was -18.010000. running mean: -29.347880\n",
      "ep 1910: ep_len:92 episode reward: total was 44.500000. running mean: -28.609401\n",
      "ep 1910: ep_len:70 episode reward: total was 33.500000. running mean: -27.988307\n",
      "ep 1910: ep_len:500 episode reward: total was 11.850000. running mean: -27.589924\n",
      "ep 1910: ep_len:2777 episode reward: total was 2.080000. running mean: -27.293225\n",
      "epsilon:0.009992 episode_count: 28769. steps_count: 30816144.000000\n",
      "ep 1911: ep_len:654 episode reward: total was -10.800000. running mean: -27.128293\n",
      "ep 1911: ep_len:725 episode reward: total was -9.080000. running mean: -26.947810\n",
      "ep 1911: ep_len:3031 episode reward: total was -128.840000. running mean: -27.966732\n",
      "ep 1911: ep_len:676 episode reward: total was 3.170000. running mean: -27.655364\n",
      "ep 1911: ep_len:60 episode reward: total was 28.500000. running mean: -27.093811\n",
      "ep 1911: ep_len:172 episode reward: total was 84.500000. running mean: -25.977873\n",
      "ep 1911: ep_len:738 episode reward: total was -7.420000. running mean: -25.792294\n",
      "ep 1911: ep_len:3629 episode reward: total was -33.190000. running mean: -25.866271\n",
      "ep 1911: ep_len:510 episode reward: total was 5.940000. running mean: -25.548208\n",
      "ep 1911: ep_len:821 episode reward: total was 31.930000. running mean: -24.973426\n",
      "ep 1911: ep_len:648 episode reward: total was -32.190000. running mean: -25.045592\n",
      "ep 1911: ep_len:74 episode reward: total was 35.500000. running mean: -24.440136\n",
      "ep 1911: ep_len:177 episode reward: total was 85.500000. running mean: -23.340735\n",
      "ep 1911: ep_len:1492 episode reward: total was 6.670000. running mean: -23.040627\n",
      "ep 1911: ep_len:2887 episode reward: total was -40.940000. running mean: -23.219621\n",
      "ep 1911: ep_len:62 episode reward: total was 29.500000. running mean: -22.692425\n",
      "epsilon:0.009992 episode_count: 28785. steps_count: 30832500.000000\n",
      "ep 1912: ep_len:500 episode reward: total was -23.660000. running mean: -22.702100\n",
      "ep 1912: ep_len:500 episode reward: total was 15.400000. running mean: -22.321079\n",
      "ep 1912: ep_len:70 episode reward: total was 33.500000. running mean: -21.762869\n",
      "ep 1912: ep_len:2909 episode reward: total was -111.710000. running mean: -22.662340\n",
      "ep 1912: ep_len:829 episode reward: total was 24.310000. running mean: -22.192617\n",
      "ep 1912: ep_len:48 episode reward: total was 21.000000. running mean: -21.760690\n",
      "ep 1912: ep_len:122 episode reward: total was 58.000000. running mean: -20.963084\n",
      "ep 1912: ep_len:880 episode reward: total was 25.900000. running mean: -20.494453\n",
      "ep 1912: ep_len:3744 episode reward: total was -9.980000. running mean: -20.389308\n",
      "ep 1912: ep_len:4224 episode reward: total was -958.870000. running mean: -29.774115\n",
      "ep 1912: ep_len:763 episode reward: total was 3.700000. running mean: -29.439374\n",
      "ep 1912: ep_len:1496 episode reward: total was -13.190000. running mean: -29.276880\n",
      "ep 1912: ep_len:36 episode reward: total was 16.500000. running mean: -28.819111\n",
      "ep 1912: ep_len:220 episode reward: total was 105.500000. running mean: -27.475920\n",
      "ep 1912: ep_len:43 episode reward: total was 20.000000. running mean: -27.001161\n",
      "ep 1912: ep_len:82 episode reward: total was 39.500000. running mean: -26.336149\n",
      "ep 1912: ep_len:1116 episode reward: total was -21.330000. running mean: -26.286088\n",
      "ep 1912: ep_len:2821 episode reward: total was -1.580000. running mean: -26.039027\n",
      "epsilon:0.009992 episode_count: 28803. steps_count: 30852903.000000\n",
      "ep 1913: ep_len:1126 episode reward: total was -9.720000. running mean: -25.875837\n",
      "ep 1913: ep_len:755 episode reward: total was -22.070000. running mean: -25.837778\n",
      "ep 1913: ep_len:2988 episode reward: total was -45.350000. running mean: -26.032901\n",
      "ep 1913: ep_len:622 episode reward: total was -11.120000. running mean: -25.883772\n",
      "ep 1913: ep_len:93 episode reward: total was 43.500000. running mean: -25.189934\n",
      "ep 1913: ep_len:863 episode reward: total was 10.210000. running mean: -24.835935\n",
      "ep 1913: ep_len:597 episode reward: total was 13.890000. running mean: -24.448675\n",
      "ep 1913: ep_len:1245 episode reward: total was -51.350000. running mean: -24.717689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1913: ep_len:677 episode reward: total was -4.840000. running mean: -24.518912\n",
      "ep 1913: ep_len:1502 episode reward: total was -9.240000. running mean: -24.366122\n",
      "ep 1913: ep_len:578 episode reward: total was 4.790000. running mean: -24.074561\n",
      "ep 1913: ep_len:2802 episode reward: total was -14.600000. running mean: -23.979816\n",
      "ep 1913: ep_len:49 episode reward: total was 23.000000. running mean: -23.510018\n",
      "epsilon:0.009992 episode_count: 28816. steps_count: 30866800.000000\n",
      "ep 1914: ep_len:1045 episode reward: total was -18.000000. running mean: -23.454917\n",
      "ep 1914: ep_len:709 episode reward: total was -23.220000. running mean: -23.452568\n",
      "ep 1914: ep_len:2967 episode reward: total was -66.690000. running mean: -23.884942\n",
      "ep 1914: ep_len:508 episode reward: total was 3.700000. running mean: -23.609093\n",
      "ep 1914: ep_len:60 episode reward: total was 28.500000. running mean: -23.088002\n",
      "ep 1914: ep_len:112 episode reward: total was 53.000000. running mean: -22.327122\n",
      "ep 1914: ep_len:77 episode reward: total was 26.500000. running mean: -21.838851\n",
      "ep 1914: ep_len:57 episode reward: total was 25.500000. running mean: -21.365462\n",
      "ep 1914: ep_len:658 episode reward: total was 1.730000. running mean: -21.134508\n",
      "ep 1914: ep_len:356 episode reward: total was 16.840000. running mean: -20.754763\n",
      "ep 1914: ep_len:858 episode reward: total was 7.770000. running mean: -20.469515\n",
      "ep 1914: ep_len:828 episode reward: total was 34.210000. running mean: -19.922720\n",
      "ep 1914: ep_len:638 episode reward: total was -16.480000. running mean: -19.888293\n",
      "ep 1914: ep_len:54 episode reward: total was 25.500000. running mean: -19.434410\n",
      "ep 1914: ep_len:694 episode reward: total was 1.670000. running mean: -19.223366\n",
      "ep 1914: ep_len:2828 episode reward: total was -40.920000. running mean: -19.440332\n",
      "epsilon:0.009992 episode_count: 28832. steps_count: 30879249.000000\n",
      "ep 1915: ep_len:1019 episode reward: total was -13.210000. running mean: -19.378029\n",
      "ep 1915: ep_len:937 episode reward: total was 22.910000. running mean: -18.955148\n",
      "ep 1915: ep_len:83 episode reward: total was 40.000000. running mean: -18.365597\n",
      "ep 1915: ep_len:2901 episode reward: total was -10.230000. running mean: -18.284241\n",
      "ep 1915: ep_len:515 episode reward: total was -39.460000. running mean: -18.495999\n",
      "ep 1915: ep_len:71 episode reward: total was 29.500000. running mean: -18.016039\n",
      "ep 1915: ep_len:1001 episode reward: total was -67.930000. running mean: -18.515178\n",
      "ep 1915: ep_len:3649 episode reward: total was -169.770000. running mean: -20.027726\n",
      "ep 1915: ep_len:548 episode reward: total was -11.740000. running mean: -19.944849\n",
      "ep 1915: ep_len:840 episode reward: total was 59.620000. running mean: -19.149201\n",
      "ep 1915: ep_len:912 episode reward: total was 13.150000. running mean: -18.826209\n",
      "ep 1915: ep_len:1021 episode reward: total was 1.960000. running mean: -18.618347\n",
      "ep 1915: ep_len:2931 episode reward: total was -35.660000. running mean: -18.788763\n",
      "epsilon:0.009992 episode_count: 28845. steps_count: 30895677.000000\n",
      "ep 1916: ep_len:1406 episode reward: total was 18.880000. running mean: -18.412075\n",
      "ep 1916: ep_len:500 episode reward: total was 22.720000. running mean: -18.000755\n",
      "ep 1916: ep_len:96 episode reward: total was 46.500000. running mean: -17.355747\n",
      "ep 1916: ep_len:690 episode reward: total was 3.290000. running mean: -17.149290\n",
      "ep 1916: ep_len:44 episode reward: total was 20.500000. running mean: -16.772797\n",
      "ep 1916: ep_len:155 episode reward: total was 76.000000. running mean: -15.845069\n",
      "ep 1916: ep_len:74 episode reward: total was 35.500000. running mean: -15.331618\n",
      "ep 1916: ep_len:598 episode reward: total was 0.690000. running mean: -15.171402\n",
      "ep 1916: ep_len:638 episode reward: total was 29.510000. running mean: -14.724588\n",
      "ep 1916: ep_len:538 episode reward: total was 4.170000. running mean: -14.535642\n",
      "ep 1916: ep_len:832 episode reward: total was 14.500000. running mean: -14.245286\n",
      "ep 1916: ep_len:1125 episode reward: total was 12.010000. running mean: -13.982733\n",
      "ep 1916: ep_len:73 episode reward: total was 33.500000. running mean: -13.507905\n",
      "ep 1916: ep_len:1159 episode reward: total was 7.330000. running mean: -13.299526\n",
      "ep 1916: ep_len:2776 episode reward: total was -11.920000. running mean: -13.285731\n",
      "ep 1916: ep_len:55 episode reward: total was 26.000000. running mean: -12.892874\n",
      "epsilon:0.009992 episode_count: 28861. steps_count: 30906436.000000\n",
      "ep 1917: ep_len:1134 episode reward: total was 6.160000. running mean: -12.702345\n",
      "ep 1917: ep_len:1219 episode reward: total was -36.400000. running mean: -12.939322\n",
      "ep 1917: ep_len:75 episode reward: total was 34.500000. running mean: -12.464928\n",
      "ep 1917: ep_len:3034 episode reward: total was -135.040000. running mean: -13.690679\n",
      "ep 1917: ep_len:538 episode reward: total was 1.170000. running mean: -13.542072\n",
      "ep 1917: ep_len:158 episode reward: total was 77.500000. running mean: -12.631652\n",
      "ep 1917: ep_len:65 episode reward: total was 31.000000. running mean: -12.195335\n",
      "ep 1917: ep_len:71 episode reward: total was 34.000000. running mean: -11.733382\n",
      "ep 1917: ep_len:1040 episode reward: total was -5.700000. running mean: -11.673048\n",
      "ep 1917: ep_len:627 episode reward: total was 18.870000. running mean: -11.367617\n",
      "ep 1917: ep_len:3801 episode reward: total was -816.040000. running mean: -19.414341\n",
      "ep 1917: ep_len:811 episode reward: total was 53.850000. running mean: -18.681698\n",
      "ep 1917: ep_len:500 episode reward: total was 29.710000. running mean: -18.197781\n",
      "ep 1917: ep_len:636 episode reward: total was 3.430000. running mean: -17.981503\n",
      "ep 1917: ep_len:2766 episode reward: total was 4.630000. running mean: -17.755388\n",
      "epsilon:0.009992 episode_count: 28876. steps_count: 30922911.000000\n",
      "ep 1918: ep_len:686 episode reward: total was -41.640000. running mean: -17.994234\n",
      "ep 1918: ep_len:668 episode reward: total was 5.300000. running mean: -17.761292\n",
      "ep 1918: ep_len:59 episode reward: total was 26.500000. running mean: -17.318679\n",
      "ep 1918: ep_len:3056 episode reward: total was -46.880000. running mean: -17.614292\n",
      "ep 1918: ep_len:1460 episode reward: total was 7.480000. running mean: -17.363349\n",
      "ep 1918: ep_len:154 episode reward: total was 75.500000. running mean: -16.434716\n",
      "ep 1918: ep_len:69 episode reward: total was 33.000000. running mean: -15.940369\n",
      "ep 1918: ep_len:70 episode reward: total was 32.000000. running mean: -15.460965\n",
      "ep 1918: ep_len:627 episode reward: total was 9.770000. running mean: -15.208655\n",
      "ep 1918: ep_len:320 episode reward: total was 12.530000. running mean: -14.931269\n",
      "ep 1918: ep_len:918 episode reward: total was -23.310000. running mean: -15.015056\n",
      "ep 1918: ep_len:750 episode reward: total was 5.810000. running mean: -14.806805\n",
      "ep 1918: ep_len:1169 episode reward: total was -21.810000. running mean: -14.876837\n",
      "ep 1918: ep_len:115 episode reward: total was 56.000000. running mean: -14.168069\n",
      "ep 1918: ep_len:91 episode reward: total was 41.000000. running mean: -13.616388\n",
      "ep 1918: ep_len:763 episode reward: total was -254.130000. running mean: -16.021524\n",
      "ep 1918: ep_len:2845 episode reward: total was 16.780000. running mean: -15.693509\n",
      "ep 1918: ep_len:53 episode reward: total was 25.000000. running mean: -15.286574\n",
      "epsilon:0.009992 episode_count: 28894. steps_count: 30936784.000000\n",
      "ep 1919: ep_len:500 episode reward: total was 22.260000. running mean: -14.911108\n",
      "ep 1919: ep_len:616 episode reward: total was -0.570000. running mean: -14.767697\n",
      "ep 1919: ep_len:2978 episode reward: total was -24.130000. running mean: -14.861320\n",
      "ep 1919: ep_len:1496 episode reward: total was 22.320000. running mean: -14.489507\n",
      "ep 1919: ep_len:1083 episode reward: total was -4.820000. running mean: -14.392812\n",
      "ep 1919: ep_len:677 episode reward: total was 21.850000. running mean: -14.030384\n",
      "ep 1919: ep_len:1288 episode reward: total was -76.170000. running mean: -14.651780\n",
      "ep 1919: ep_len:687 episode reward: total was 0.980000. running mean: -14.495462\n",
      "ep 1919: ep_len:534 episode reward: total was 3.930000. running mean: -14.311208\n",
      "ep 1919: ep_len:89 episode reward: total was 43.000000. running mean: -13.738096\n",
      "ep 1919: ep_len:97 episode reward: total was 45.500000. running mean: -13.145715\n",
      "ep 1919: ep_len:672 episode reward: total was 5.800000. running mean: -12.956257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1919: ep_len:2823 episode reward: total was -7.140000. running mean: -12.898095\n",
      "epsilon:0.009992 episode_count: 28907. steps_count: 30950324.000000\n",
      "ep 1920: ep_len:715 episode reward: total was -185.930000. running mean: -14.628414\n",
      "ep 1920: ep_len:728 episode reward: total was -10.060000. running mean: -14.582730\n",
      "ep 1920: ep_len:53 episode reward: total was 23.500000. running mean: -14.201902\n",
      "ep 1920: ep_len:3064 episode reward: total was -21.850000. running mean: -14.278383\n",
      "ep 1920: ep_len:1179 episode reward: total was -21.710000. running mean: -14.352700\n",
      "ep 1920: ep_len:132 episode reward: total was 63.000000. running mean: -13.579173\n",
      "ep 1920: ep_len:2961 episode reward: total was -192.240000. running mean: -15.365781\n",
      "ep 1920: ep_len:675 episode reward: total was 19.230000. running mean: -15.019823\n",
      "ep 1920: ep_len:535 episode reward: total was 5.180000. running mean: -14.817825\n",
      "ep 1920: ep_len:743 episode reward: total was 4.450000. running mean: -14.625147\n",
      "ep 1920: ep_len:976 episode reward: total was -3.100000. running mean: -14.509895\n",
      "ep 1920: ep_len:54 episode reward: total was 24.000000. running mean: -14.124796\n",
      "ep 1920: ep_len:154 episode reward: total was 72.500000. running mean: -13.258548\n",
      "ep 1920: ep_len:39 episode reward: total was 16.500000. running mean: -12.960963\n",
      "ep 1920: ep_len:69 episode reward: total was 33.000000. running mean: -12.501353\n",
      "ep 1920: ep_len:1424 episode reward: total was -4.630000. running mean: -12.422640\n",
      "ep 1920: ep_len:2751 episode reward: total was -28.170000. running mean: -12.580113\n",
      "ep 1920: ep_len:26 episode reward: total was 11.500000. running mean: -12.339312\n",
      "epsilon:0.009992 episode_count: 28925. steps_count: 30966602.000000\n",
      "ep 1921: ep_len:1004 episode reward: total was -236.790000. running mean: -14.583819\n",
      "ep 1921: ep_len:741 episode reward: total was -52.060000. running mean: -14.958581\n",
      "ep 1921: ep_len:2923 episode reward: total was -10.840000. running mean: -14.917395\n",
      "ep 1921: ep_len:549 episode reward: total was -24.060000. running mean: -15.008821\n",
      "ep 1921: ep_len:35 episode reward: total was 14.500000. running mean: -14.713733\n",
      "ep 1921: ep_len:95 episode reward: total was 44.500000. running mean: -14.121595\n",
      "ep 1921: ep_len:39 episode reward: total was 18.000000. running mean: -13.800380\n",
      "ep 1921: ep_len:922 episode reward: total was 42.410000. running mean: -13.238276\n",
      "ep 1921: ep_len:646 episode reward: total was 30.050000. running mean: -12.805393\n",
      "ep 1921: ep_len:1225 episode reward: total was -32.900000. running mean: -13.006339\n",
      "ep 1921: ep_len:788 episode reward: total was -3.360000. running mean: -12.909876\n",
      "ep 1921: ep_len:1531 episode reward: total was -10.420000. running mean: -12.884977\n",
      "ep 1921: ep_len:87 episode reward: total was 37.500000. running mean: -12.381127\n",
      "ep 1921: ep_len:1380 episode reward: total was 6.130000. running mean: -12.196016\n",
      "ep 1921: ep_len:2839 episode reward: total was -0.120000. running mean: -12.075256\n",
      "epsilon:0.009992 episode_count: 28940. steps_count: 30981406.000000\n",
      "ep 1922: ep_len:1458 episode reward: total was -17.850000. running mean: -12.133003\n",
      "ep 1922: ep_len:500 episode reward: total was 22.320000. running mean: -11.788473\n",
      "ep 1922: ep_len:2931 episode reward: total was -4.480000. running mean: -11.715388\n",
      "ep 1922: ep_len:500 episode reward: total was -3.940000. running mean: -11.637634\n",
      "ep 1922: ep_len:21 episode reward: total was 7.500000. running mean: -11.446258\n",
      "ep 1922: ep_len:1459 episode reward: total was 1.260000. running mean: -11.319196\n",
      "ep 1922: ep_len:3881 episode reward: total was -40.630000. running mean: -11.612304\n",
      "ep 1922: ep_len:632 episode reward: total was -81.660000. running mean: -12.312781\n",
      "ep 1922: ep_len:875 episode reward: total was 32.540000. running mean: -11.864253\n",
      "ep 1922: ep_len:932 episode reward: total was 48.160000. running mean: -11.264010\n",
      "ep 1922: ep_len:91 episode reward: total was 41.000000. running mean: -10.741370\n",
      "ep 1922: ep_len:150 episode reward: total was 72.000000. running mean: -9.913956\n",
      "ep 1922: ep_len:36 episode reward: total was 15.000000. running mean: -9.664817\n",
      "ep 1922: ep_len:96 episode reward: total was 45.000000. running mean: -9.118169\n",
      "ep 1922: ep_len:1112 episode reward: total was -19.230000. running mean: -9.219287\n",
      "ep 1922: ep_len:2775 episode reward: total was -18.820000. running mean: -9.315294\n",
      "ep 1922: ep_len:41 episode reward: total was 19.000000. running mean: -9.032141\n",
      "epsilon:0.009992 episode_count: 28957. steps_count: 30998896.000000\n",
      "ep 1923: ep_len:1170 episode reward: total was 4.720000. running mean: -8.894620\n",
      "ep 1923: ep_len:500 episode reward: total was 29.060000. running mean: -8.515074\n",
      "ep 1923: ep_len:29 episode reward: total was 11.500000. running mean: -8.314923\n",
      "ep 1923: ep_len:3070 episode reward: total was -20.270000. running mean: -8.434474\n",
      "ep 1923: ep_len:761 episode reward: total was 19.190000. running mean: -8.158229\n",
      "ep 1923: ep_len:56 episode reward: total was 26.500000. running mean: -7.811647\n",
      "ep 1923: ep_len:910 episode reward: total was -9.250000. running mean: -7.826030\n",
      "ep 1923: ep_len:3739 episode reward: total was -36.100000. running mean: -8.108770\n",
      "ep 1923: ep_len:952 episode reward: total was -31.050000. running mean: -8.338182\n",
      "ep 1923: ep_len:637 episode reward: total was -2.150000. running mean: -8.276300\n",
      "ep 1923: ep_len:987 episode reward: total was 19.440000. running mean: -7.999137\n",
      "ep 1923: ep_len:85 episode reward: total was 41.000000. running mean: -7.509146\n",
      "ep 1923: ep_len:137 episode reward: total was 67.000000. running mean: -6.764054\n",
      "ep 1923: ep_len:36 episode reward: total was 16.500000. running mean: -6.531414\n",
      "ep 1923: ep_len:672 episode reward: total was 10.930000. running mean: -6.356800\n",
      "ep 1923: ep_len:2882 episode reward: total was -18.110000. running mean: -6.474332\n",
      "epsilon:0.009992 episode_count: 28973. steps_count: 31015519.000000\n",
      "ep 1924: ep_len:1446 episode reward: total was 6.790000. running mean: -6.341688\n",
      "ep 1924: ep_len:1168 episode reward: total was -42.020000. running mean: -6.698472\n",
      "ep 1924: ep_len:2883 episode reward: total was -44.030000. running mean: -7.071787\n",
      "ep 1924: ep_len:672 episode reward: total was -25.660000. running mean: -7.257669\n",
      "ep 1924: ep_len:500 episode reward: total was 46.820000. running mean: -6.716892\n",
      "ep 1924: ep_len:3608 episode reward: total was -13.050000. running mean: -6.780223\n",
      "ep 1924: ep_len:547 episode reward: total was -32.070000. running mean: -7.033121\n",
      "ep 1924: ep_len:811 episode reward: total was 41.300000. running mean: -6.549790\n",
      "ep 1924: ep_len:500 episode reward: total was 5.390000. running mean: -6.430392\n",
      "ep 1924: ep_len:42 episode reward: total was 19.500000. running mean: -6.171088\n",
      "ep 1924: ep_len:1024 episode reward: total was -24.460000. running mean: -6.353977\n",
      "ep 1924: ep_len:2856 episode reward: total was -7.450000. running mean: -6.364937\n",
      "epsilon:0.009992 episode_count: 28985. steps_count: 31031576.000000\n",
      "ep 1925: ep_len:687 episode reward: total was -51.880000. running mean: -6.820088\n",
      "ep 1925: ep_len:1652 episode reward: total was -53.370000. running mean: -7.285587\n",
      "ep 1925: ep_len:66 episode reward: total was 31.500000. running mean: -6.897731\n",
      "ep 1925: ep_len:3072 episode reward: total was -17.360000. running mean: -7.002354\n",
      "ep 1925: ep_len:778 episode reward: total was 24.810000. running mean: -6.684230\n",
      "ep 1925: ep_len:67 episode reward: total was 27.500000. running mean: -6.342388\n",
      "ep 1925: ep_len:979 episode reward: total was -14.620000. running mean: -6.425164\n",
      "ep 1925: ep_len:3580 episode reward: total was -21.470000. running mean: -6.575613\n",
      "ep 1925: ep_len:913 episode reward: total was -23.820000. running mean: -6.748057\n",
      "ep 1925: ep_len:779 episode reward: total was 7.360000. running mean: -6.606976\n",
      "ep 1925: ep_len:500 episode reward: total was -3.460000. running mean: -6.575506\n",
      "ep 1925: ep_len:63 episode reward: total was 30.000000. running mean: -6.209751\n",
      "ep 1925: ep_len:714 episode reward: total was -13.230000. running mean: -6.279954\n",
      "ep 1925: ep_len:2847 episode reward: total was -9.190000. running mean: -6.309054\n",
      "epsilon:0.009992 episode_count: 28999. steps_count: 31048273.000000\n",
      "ep 1926: ep_len:1162 episode reward: total was -24.700000. running mean: -6.492964\n",
      "ep 1926: ep_len:1182 episode reward: total was -52.140000. running mean: -6.949434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1926: ep_len:2992 episode reward: total was -77.830000. running mean: -7.658240\n",
      "ep 1926: ep_len:500 episode reward: total was 12.800000. running mean: -7.453657\n",
      "ep 1926: ep_len:69 episode reward: total was 31.500000. running mean: -7.064121\n",
      "ep 1926: ep_len:80 episode reward: total was 38.500000. running mean: -6.608479\n",
      "ep 1926: ep_len:1463 episode reward: total was 9.870000. running mean: -6.443695\n",
      "ep 1926: ep_len:3549 episode reward: total was -903.460000. running mean: -15.413858\n",
      "ep 1926: ep_len:1595 episode reward: total was -97.310000. running mean: -16.232819\n",
      "ep 1926: ep_len:679 episode reward: total was 14.710000. running mean: -15.923391\n",
      "ep 1926: ep_len:500 episode reward: total was 23.820000. running mean: -15.525957\n",
      "ep 1926: ep_len:202 episode reward: total was 96.500000. running mean: -14.405697\n",
      "ep 1926: ep_len:52 episode reward: total was 21.500000. running mean: -14.046640\n",
      "ep 1926: ep_len:956 episode reward: total was -89.590000. running mean: -14.802074\n",
      "ep 1926: ep_len:2718 episode reward: total was -14.030000. running mean: -14.794353\n",
      "epsilon:0.009992 episode_count: 29014. steps_count: 31065972.000000\n",
      "ep 1927: ep_len:798 episode reward: total was -60.250000. running mean: -15.248910\n",
      "ep 1927: ep_len:911 episode reward: total was -6.390000. running mean: -15.160321\n",
      "ep 1927: ep_len:2972 episode reward: total was -39.340000. running mean: -15.402117\n",
      "ep 1927: ep_len:500 episode reward: total was 23.400000. running mean: -15.014096\n",
      "ep 1927: ep_len:697 episode reward: total was 10.760000. running mean: -14.756355\n",
      "ep 1927: ep_len:3683 episode reward: total was -1.830000. running mean: -14.627092\n",
      "ep 1927: ep_len:500 episode reward: total was 25.380000. running mean: -14.227021\n",
      "ep 1927: ep_len:872 episode reward: total was 69.530000. running mean: -13.389451\n",
      "ep 1927: ep_len:769 episode reward: total was -18.740000. running mean: -13.442956\n",
      "ep 1927: ep_len:179 episode reward: total was 88.000000. running mean: -12.428527\n",
      "ep 1927: ep_len:616 episode reward: total was 0.870000. running mean: -12.295541\n",
      "ep 1927: ep_len:2879 episode reward: total was -25.890000. running mean: -12.431486\n",
      "epsilon:0.009992 episode_count: 29026. steps_count: 31081348.000000\n",
      "ep 1928: ep_len:1094 episode reward: total was -4.500000. running mean: -12.352171\n",
      "ep 1928: ep_len:749 episode reward: total was 3.530000. running mean: -12.193349\n",
      "ep 1928: ep_len:53 episode reward: total was 22.000000. running mean: -11.851416\n",
      "ep 1928: ep_len:3096 episode reward: total was -31.450000. running mean: -12.047402\n",
      "ep 1928: ep_len:1167 episode reward: total was -24.860000. running mean: -12.175528\n",
      "ep 1928: ep_len:71 episode reward: total was 34.000000. running mean: -11.713772\n",
      "ep 1928: ep_len:1136 episode reward: total was 4.560000. running mean: -11.551035\n",
      "ep 1928: ep_len:3821 episode reward: total was -31.680000. running mean: -11.752324\n",
      "ep 1928: ep_len:500 episode reward: total was 33.680000. running mean: -11.298001\n",
      "ep 1928: ep_len:672 episode reward: total was 22.360000. running mean: -10.961421\n",
      "ep 1928: ep_len:724 episode reward: total was 31.970000. running mean: -10.532107\n",
      "ep 1928: ep_len:94 episode reward: total was 45.500000. running mean: -9.971786\n",
      "ep 1928: ep_len:102 episode reward: total was 49.500000. running mean: -9.377068\n",
      "ep 1928: ep_len:1121 episode reward: total was -5.390000. running mean: -9.337197\n",
      "ep 1928: ep_len:2810 episode reward: total was -17.030000. running mean: -9.414125\n",
      "ep 1928: ep_len:39 episode reward: total was 16.500000. running mean: -9.154984\n",
      "epsilon:0.009992 episode_count: 29042. steps_count: 31098597.000000\n",
      "ep 1929: ep_len:1447 episode reward: total was 16.780000. running mean: -8.895634\n",
      "ep 1929: ep_len:721 episode reward: total was -37.020000. running mean: -9.176878\n",
      "ep 1929: ep_len:78 episode reward: total was 36.000000. running mean: -8.725109\n",
      "ep 1929: ep_len:101 episode reward: total was 49.000000. running mean: -8.147858\n",
      "ep 1929: ep_len:524 episode reward: total was -34.320000. running mean: -8.409579\n",
      "ep 1929: ep_len:42 episode reward: total was 19.500000. running mean: -8.130484\n",
      "ep 1929: ep_len:87 episode reward: total was 42.000000. running mean: -7.629179\n",
      "ep 1929: ep_len:39 episode reward: total was 18.000000. running mean: -7.372887\n",
      "ep 1929: ep_len:896 episode reward: total was 35.390000. running mean: -6.945258\n",
      "ep 1929: ep_len:635 episode reward: total was 27.490000. running mean: -6.600906\n",
      "ep 1929: ep_len:538 episode reward: total was -0.450000. running mean: -6.539396\n",
      "ep 1929: ep_len:809 episode reward: total was 9.520000. running mean: -6.378802\n",
      "ep 1929: ep_len:591 episode reward: total was -9.900000. running mean: -6.414014\n",
      "ep 1929: ep_len:120 episode reward: total was 58.500000. running mean: -5.764874\n",
      "ep 1929: ep_len:55 episode reward: total was 26.000000. running mean: -5.447226\n",
      "ep 1929: ep_len:123 episode reward: total was 60.000000. running mean: -4.792753\n",
      "ep 1929: ep_len:915 episode reward: total was 14.340000. running mean: -4.601426\n",
      "ep 1929: ep_len:2767 episode reward: total was -7.300000. running mean: -4.628412\n",
      "epsilon:0.009992 episode_count: 29060. steps_count: 31109085.000000\n",
      "ep 1930: ep_len:1096 episode reward: total was -10.420000. running mean: -4.686327\n",
      "ep 1930: ep_len:802 episode reward: total was 10.530000. running mean: -4.534164\n",
      "ep 1930: ep_len:70 episode reward: total was 33.500000. running mean: -4.153823\n",
      "ep 1930: ep_len:2980 episode reward: total was -2363.920000. running mean: -27.751484\n",
      "ep 1930: ep_len:500 episode reward: total was 13.530000. running mean: -27.338669\n",
      "ep 1930: ep_len:57 episode reward: total was 27.000000. running mean: -26.795283\n",
      "ep 1930: ep_len:54 episode reward: total was 24.000000. running mean: -26.287330\n",
      "ep 1930: ep_len:1410 episode reward: total was -181.490000. running mean: -27.839357\n",
      "ep 1930: ep_len:3583 episode reward: total was 4.730000. running mean: -27.513663\n",
      "ep 1930: ep_len:852 episode reward: total was -20.720000. running mean: -27.445726\n",
      "ep 1930: ep_len:7385 episode reward: total was 37.410000. running mean: -26.797169\n",
      "ep 1930: ep_len:1539 episode reward: total was 4.420000. running mean: -26.484997\n",
      "ep 1930: ep_len:51 episode reward: total was 24.000000. running mean: -25.980147\n",
      "ep 1930: ep_len:109 episode reward: total was 53.000000. running mean: -25.190346\n",
      "ep 1930: ep_len:774 episode reward: total was -106.680000. running mean: -26.005243\n",
      "ep 1930: ep_len:2856 episode reward: total was -18.960000. running mean: -25.934790\n",
      "ep 1930: ep_len:50 episode reward: total was 23.500000. running mean: -25.440442\n",
      "epsilon:0.009992 episode_count: 29077. steps_count: 31133253.000000\n",
      "ep 1931: ep_len:851 episode reward: total was -15.130000. running mean: -25.337338\n",
      "ep 1931: ep_len:966 episode reward: total was 14.850000. running mean: -24.935464\n",
      "ep 1931: ep_len:3011 episode reward: total was -80.670000. running mean: -25.492810\n",
      "ep 1931: ep_len:663 episode reward: total was 9.200000. running mean: -25.145882\n",
      "ep 1931: ep_len:119 episode reward: total was 58.000000. running mean: -24.314423\n",
      "ep 1931: ep_len:59 episode reward: total was 25.000000. running mean: -23.821279\n",
      "ep 1931: ep_len:47 episode reward: total was 20.500000. running mean: -23.378066\n",
      "ep 1931: ep_len:882 episode reward: total was 21.080000. running mean: -22.933485\n",
      "ep 1931: ep_len:653 episode reward: total was 19.280000. running mean: -22.511350\n",
      "ep 1931: ep_len:853 episode reward: total was 11.050000. running mean: -22.175737\n",
      "ep 1931: ep_len:791 episode reward: total was 2.210000. running mean: -21.931879\n",
      "ep 1931: ep_len:683 episode reward: total was 4.830000. running mean: -21.664261\n",
      "ep 1931: ep_len:71 episode reward: total was 34.000000. running mean: -21.107618\n",
      "ep 1931: ep_len:128 episode reward: total was 61.000000. running mean: -20.286542\n",
      "ep 1931: ep_len:63 episode reward: total was 30.000000. running mean: -19.783676\n",
      "ep 1931: ep_len:1074 episode reward: total was -18.720000. running mean: -19.773040\n",
      "ep 1931: ep_len:2846 episode reward: total was -15.570000. running mean: -19.731009\n",
      "epsilon:0.009992 episode_count: 29094. steps_count: 31147013.000000\n",
      "ep 1932: ep_len:1139 episode reward: total was -16.940000. running mean: -19.703099\n",
      "ep 1932: ep_len:612 episode reward: total was -17.650000. running mean: -19.682568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1932: ep_len:3024 episode reward: total was -9.020000. running mean: -19.575943\n",
      "ep 1932: ep_len:807 episode reward: total was 17.230000. running mean: -19.207883\n",
      "ep 1932: ep_len:57 episode reward: total was 27.000000. running mean: -18.745804\n",
      "ep 1932: ep_len:174 episode reward: total was 84.000000. running mean: -17.718346\n",
      "ep 1932: ep_len:1453 episode reward: total was 15.980000. running mean: -17.381363\n",
      "ep 1932: ep_len:3632 episode reward: total was -16.110000. running mean: -17.368649\n",
      "ep 1932: ep_len:511 episode reward: total was -3.140000. running mean: -17.226363\n",
      "ep 1932: ep_len:723 episode reward: total was 40.510000. running mean: -16.648999\n",
      "ep 1932: ep_len:612 episode reward: total was 3.650000. running mean: -16.446009\n",
      "ep 1932: ep_len:1046 episode reward: total was 8.510000. running mean: -16.196449\n",
      "ep 1932: ep_len:2767 episode reward: total was -16.570000. running mean: -16.200184\n",
      "ep 1932: ep_len:58 episode reward: total was 26.000000. running mean: -15.778183\n",
      "epsilon:0.009992 episode_count: 29108. steps_count: 31163628.000000\n",
      "ep 1933: ep_len:1450 episode reward: total was 15.120000. running mean: -15.469201\n",
      "ep 1933: ep_len:500 episode reward: total was 16.440000. running mean: -15.150109\n",
      "ep 1933: ep_len:69 episode reward: total was 31.500000. running mean: -14.683608\n",
      "ep 1933: ep_len:3024 episode reward: total was -88.770000. running mean: -15.424472\n",
      "ep 1933: ep_len:502 episode reward: total was -4.150000. running mean: -15.311727\n",
      "ep 1933: ep_len:45 episode reward: total was 21.000000. running mean: -14.948610\n",
      "ep 1933: ep_len:53 episode reward: total was 25.000000. running mean: -14.549124\n",
      "ep 1933: ep_len:774 episode reward: total was -2.530000. running mean: -14.428932\n",
      "ep 1933: ep_len:661 episode reward: total was 27.720000. running mean: -14.007443\n",
      "ep 1933: ep_len:553 episode reward: total was -32.010000. running mean: -14.187469\n",
      "ep 1933: ep_len:657 episode reward: total was 21.500000. running mean: -13.830594\n",
      "ep 1933: ep_len:680 episode reward: total was -6.540000. running mean: -13.757688\n",
      "ep 1933: ep_len:200 episode reward: total was 94.000000. running mean: -12.680111\n",
      "ep 1933: ep_len:56 episode reward: total was 26.500000. running mean: -12.288310\n",
      "ep 1933: ep_len:500 episode reward: total was 38.520000. running mean: -11.780227\n",
      "ep 1933: ep_len:2856 episode reward: total was 12.360000. running mean: -11.538825\n",
      "ep 1933: ep_len:52 episode reward: total was 24.500000. running mean: -11.178436\n",
      "epsilon:0.009992 episode_count: 29125. steps_count: 31176260.000000\n",
      "ep 1934: ep_len:1097 episode reward: total was -8.390000. running mean: -11.150552\n",
      "ep 1934: ep_len:500 episode reward: total was -1.070000. running mean: -11.049746\n",
      "ep 1934: ep_len:22 episode reward: total was 9.500000. running mean: -10.844249\n",
      "ep 1934: ep_len:2917 episode reward: total was -23.970000. running mean: -10.975506\n",
      "ep 1934: ep_len:671 episode reward: total was -15.680000. running mean: -11.022551\n",
      "ep 1934: ep_len:63 episode reward: total was 28.500000. running mean: -10.627326\n",
      "ep 1934: ep_len:103 episode reward: total was 48.500000. running mean: -10.036053\n",
      "ep 1934: ep_len:44 episode reward: total was 20.500000. running mean: -9.730692\n",
      "ep 1934: ep_len:1814 episode reward: total was -117.560000. running mean: -10.808985\n",
      "ep 1934: ep_len:3699 episode reward: total was -39.560000. running mean: -11.096495\n",
      "ep 1934: ep_len:1188 episode reward: total was -11.540000. running mean: -11.100930\n",
      "ep 1934: ep_len:844 episode reward: total was 35.470000. running mean: -10.635221\n",
      "ep 1934: ep_len:500 episode reward: total was 18.800000. running mean: -10.340869\n",
      "ep 1934: ep_len:81 episode reward: total was 37.500000. running mean: -9.862460\n",
      "ep 1934: ep_len:148 episode reward: total was 72.500000. running mean: -9.038836\n",
      "ep 1934: ep_len:733 episode reward: total was -15.060000. running mean: -9.099047\n",
      "ep 1934: ep_len:2894 episode reward: total was -673.030000. running mean: -15.738357\n",
      "ep 1934: ep_len:51 episode reward: total was 24.000000. running mean: -15.340973\n",
      "epsilon:0.009992 episode_count: 29143. steps_count: 31193629.000000\n",
      "ep 1935: ep_len:1442 episode reward: total was -0.720000. running mean: -15.194763\n",
      "ep 1935: ep_len:972 episode reward: total was 16.900000. running mean: -14.873816\n",
      "ep 1935: ep_len:65 episode reward: total was 31.000000. running mean: -14.415078\n",
      "ep 1935: ep_len:2968 episode reward: total was -49.650000. running mean: -14.767427\n",
      "ep 1935: ep_len:1600 episode reward: total was -50.800000. running mean: -15.127753\n",
      "ep 1935: ep_len:68 episode reward: total was 32.500000. running mean: -14.651475\n",
      "ep 1935: ep_len:500 episode reward: total was -1.130000. running mean: -14.516260\n",
      "ep 1935: ep_len:3711 episode reward: total was -14.190000. running mean: -14.512998\n",
      "ep 1935: ep_len:533 episode reward: total was 1.520000. running mean: -14.352668\n",
      "ep 1935: ep_len:680 episode reward: total was 15.520000. running mean: -14.053941\n",
      "ep 1935: ep_len:947 episode reward: total was 16.160000. running mean: -13.751802\n",
      "ep 1935: ep_len:186 episode reward: total was 91.500000. running mean: -12.699284\n",
      "ep 1935: ep_len:59 episode reward: total was 28.000000. running mean: -12.292291\n",
      "ep 1935: ep_len:645 episode reward: total was 10.190000. running mean: -12.067468\n",
      "ep 1935: ep_len:2781 episode reward: total was -44.680000. running mean: -12.393593\n",
      "epsilon:0.009992 episode_count: 29158. steps_count: 31210786.000000\n",
      "ep 1936: ep_len:1456 episode reward: total was 14.020000. running mean: -12.129457\n",
      "ep 1936: ep_len:684 episode reward: total was -28.680000. running mean: -12.294963\n",
      "ep 1936: ep_len:52 episode reward: total was 23.000000. running mean: -11.942013\n",
      "ep 1936: ep_len:2944 episode reward: total was 4.320000. running mean: -11.779393\n",
      "ep 1936: ep_len:795 episode reward: total was -28.580000. running mean: -11.947399\n",
      "ep 1936: ep_len:57 episode reward: total was 27.000000. running mean: -11.557925\n",
      "ep 1936: ep_len:500 episode reward: total was 39.290000. running mean: -11.049446\n",
      "ep 1936: ep_len:606 episode reward: total was 16.880000. running mean: -10.770151\n",
      "ep 1936: ep_len:615 episode reward: total was 2.950000. running mean: -10.632950\n",
      "ep 1936: ep_len:7299 episode reward: total was -245.110000. running mean: -12.977720\n",
      "ep 1936: ep_len:500 episode reward: total was 24.070000. running mean: -12.607243\n",
      "ep 1936: ep_len:51 episode reward: total was 22.500000. running mean: -12.256171\n",
      "ep 1936: ep_len:1095 episode reward: total was -14.220000. running mean: -12.275809\n",
      "ep 1936: ep_len:2864 episode reward: total was -8.710000. running mean: -12.240151\n",
      "ep 1936: ep_len:52 episode reward: total was 24.500000. running mean: -11.872749\n",
      "epsilon:0.009992 episode_count: 29173. steps_count: 31230356.000000\n",
      "ep 1937: ep_len:730 episode reward: total was -21.640000. running mean: -11.970422\n",
      "ep 1937: ep_len:730 episode reward: total was -23.400000. running mean: -12.084718\n",
      "ep 1937: ep_len:3090 episode reward: total was -24.990000. running mean: -12.213771\n",
      "ep 1937: ep_len:609 episode reward: total was -3.080000. running mean: -12.122433\n",
      "ep 1937: ep_len:35 episode reward: total was 16.000000. running mean: -11.841208\n",
      "ep 1937: ep_len:151 episode reward: total was 74.000000. running mean: -10.982796\n",
      "ep 1937: ep_len:67 episode reward: total was 32.000000. running mean: -10.552968\n",
      "ep 1937: ep_len:746 episode reward: total was -38.470000. running mean: -10.832139\n",
      "ep 1937: ep_len:628 episode reward: total was 27.850000. running mean: -10.445317\n",
      "ep 1937: ep_len:1263 episode reward: total was -48.140000. running mean: -10.822264\n",
      "ep 1937: ep_len:734 episode reward: total was -13.820000. running mean: -10.852242\n",
      "ep 1937: ep_len:615 episode reward: total was -3.150000. running mean: -10.775219\n",
      "ep 1937: ep_len:53 episode reward: total was 25.000000. running mean: -10.417467\n",
      "ep 1937: ep_len:71 episode reward: total was 34.000000. running mean: -9.973292\n",
      "ep 1937: ep_len:500 episode reward: total was 9.370000. running mean: -9.779859\n",
      "ep 1937: ep_len:2824 episode reward: total was -5.990000. running mean: -9.741961\n",
      "epsilon:0.009992 episode_count: 29189. steps_count: 31243202.000000\n",
      "ep 1938: ep_len:1388 episode reward: total was 14.900000. running mean: -9.495541\n",
      "ep 1938: ep_len:628 episode reward: total was -9.470000. running mean: -9.495286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1938: ep_len:2953 episode reward: total was -11.070000. running mean: -9.511033\n",
      "ep 1938: ep_len:681 episode reward: total was 6.750000. running mean: -9.348423\n",
      "ep 1938: ep_len:500 episode reward: total was 27.010000. running mean: -8.984838\n",
      "ep 1938: ep_len:3728 episode reward: total was -101.800000. running mean: -9.912990\n",
      "ep 1938: ep_len:899 episode reward: total was -38.650000. running mean: -10.200360\n",
      "ep 1938: ep_len:878 episode reward: total was 39.410000. running mean: -9.704256\n",
      "ep 1938: ep_len:1066 episode reward: total was -8.700000. running mean: -9.694214\n",
      "ep 1938: ep_len:1189 episode reward: total was 7.260000. running mean: -9.524672\n",
      "ep 1938: ep_len:2913 episode reward: total was -28.000000. running mean: -9.709425\n",
      "epsilon:0.009992 episode_count: 29200. steps_count: 31260025.000000\n",
      "ep 1939: ep_len:723 episode reward: total was -31.260000. running mean: -9.924931\n",
      "ep 1939: ep_len:500 episode reward: total was 25.110000. running mean: -9.574581\n",
      "ep 1939: ep_len:3043 episode reward: total was -0.750000. running mean: -9.486336\n",
      "ep 1939: ep_len:523 episode reward: total was 8.960000. running mean: -9.301872\n",
      "ep 1939: ep_len:148 episode reward: total was 68.000000. running mean: -8.528854\n",
      "ep 1939: ep_len:80 episode reward: total was 35.500000. running mean: -8.088565\n",
      "ep 1939: ep_len:72 episode reward: total was 34.500000. running mean: -7.662679\n",
      "ep 1939: ep_len:1450 episode reward: total was 9.250000. running mean: -7.493553\n",
      "ep 1939: ep_len:3650 episode reward: total was -25.910000. running mean: -7.677717\n",
      "ep 1939: ep_len:981 episode reward: total was -9.510000. running mean: -7.696040\n",
      "ep 1939: ep_len:864 episode reward: total was 49.180000. running mean: -7.127279\n",
      "ep 1939: ep_len:597 episode reward: total was 8.550000. running mean: -6.970507\n",
      "ep 1939: ep_len:1490 episode reward: total was 6.620000. running mean: -6.834602\n",
      "ep 1939: ep_len:2750 episode reward: total was -2.480000. running mean: -6.791056\n",
      "epsilon:0.009992 episode_count: 29214. steps_count: 31276896.000000\n",
      "ep 1940: ep_len:634 episode reward: total was 16.110000. running mean: -6.562045\n",
      "ep 1940: ep_len:801 episode reward: total was -6.300000. running mean: -6.559425\n",
      "ep 1940: ep_len:61 episode reward: total was 29.000000. running mean: -6.203830\n",
      "ep 1940: ep_len:2958 episode reward: total was -1.540000. running mean: -6.157192\n",
      "ep 1940: ep_len:500 episode reward: total was -25.160000. running mean: -6.347220\n",
      "ep 1940: ep_len:48 episode reward: total was 22.500000. running mean: -6.058748\n",
      "ep 1940: ep_len:92 episode reward: total was 44.500000. running mean: -5.553160\n",
      "ep 1940: ep_len:922 episode reward: total was 68.510000. running mean: -4.812529\n",
      "ep 1940: ep_len:4421 episode reward: total was -1484.760000. running mean: -19.612004\n",
      "ep 1940: ep_len:711 episode reward: total was -40.950000. running mean: -19.825384\n",
      "ep 1940: ep_len:639 episode reward: total was -9.050000. running mean: -19.717630\n",
      "ep 1940: ep_len:626 episode reward: total was -15.120000. running mean: -19.671653\n",
      "ep 1940: ep_len:41 episode reward: total was 19.000000. running mean: -19.284937\n",
      "ep 1940: ep_len:78 episode reward: total was 36.000000. running mean: -18.732087\n",
      "ep 1940: ep_len:678 episode reward: total was -3.490000. running mean: -18.579667\n",
      "ep 1940: ep_len:2950 episode reward: total was 0.680000. running mean: -18.387070\n",
      "epsilon:0.009992 episode_count: 29230. steps_count: 31293056.000000\n",
      "ep 1941: ep_len:1021 episode reward: total was -64.950000. running mean: -18.852699\n",
      "ep 1941: ep_len:671 episode reward: total was -31.010000. running mean: -18.974272\n",
      "ep 1941: ep_len:2995 episode reward: total was -19.880000. running mean: -18.983330\n",
      "ep 1941: ep_len:604 episode reward: total was -13.320000. running mean: -18.926696\n",
      "ep 1941: ep_len:1518 episode reward: total was 9.410000. running mean: -18.643329\n",
      "ep 1941: ep_len:3649 episode reward: total was -13.800000. running mean: -18.594896\n",
      "ep 1941: ep_len:1504 episode reward: total was -116.400000. running mean: -19.572947\n",
      "ep 1941: ep_len:787 episode reward: total was 12.550000. running mean: -19.251718\n",
      "ep 1941: ep_len:500 episode reward: total was 33.070000. running mean: -18.728500\n",
      "ep 1941: ep_len:196 episode reward: total was 95.000000. running mean: -17.591215\n",
      "ep 1941: ep_len:630 episode reward: total was 9.720000. running mean: -17.318103\n",
      "ep 1941: ep_len:2874 episode reward: total was 8.310000. running mean: -17.061822\n",
      "epsilon:0.009992 episode_count: 29242. steps_count: 31310005.000000\n",
      "ep 1942: ep_len:1075 episode reward: total was -7.600000. running mean: -16.967204\n",
      "ep 1942: ep_len:199 episode reward: total was 8.290000. running mean: -16.714632\n",
      "ep 1942: ep_len:3003 episode reward: total was -13.560000. running mean: -16.683086\n",
      "ep 1942: ep_len:542 episode reward: total was -23.120000. running mean: -16.747455\n",
      "ep 1942: ep_len:64 episode reward: total was 29.000000. running mean: -16.289980\n",
      "ep 1942: ep_len:125 episode reward: total was 59.500000. running mean: -15.532080\n",
      "ep 1942: ep_len:68 episode reward: total was 32.500000. running mean: -15.051760\n",
      "ep 1942: ep_len:500 episode reward: total was -5.480000. running mean: -14.956042\n",
      "ep 1942: ep_len:4259 episode reward: total was -2982.360000. running mean: -44.630082\n",
      "ep 1942: ep_len:568 episode reward: total was -20.380000. running mean: -44.387581\n",
      "ep 1942: ep_len:7207 episode reward: total was 15.680000. running mean: -43.786905\n",
      "ep 1942: ep_len:985 episode reward: total was 5.370000. running mean: -43.295336\n",
      "ep 1942: ep_len:1120 episode reward: total was -17.740000. running mean: -43.039783\n",
      "ep 1942: ep_len:2837 episode reward: total was 2.500000. running mean: -42.584385\n",
      "epsilon:0.009992 episode_count: 29256. steps_count: 31332557.000000\n",
      "ep 1943: ep_len:1136 episode reward: total was -6.380000. running mean: -42.222341\n",
      "ep 1943: ep_len:727 episode reward: total was -9.330000. running mean: -41.893417\n",
      "ep 1943: ep_len:2924 episode reward: total was -42.120000. running mean: -41.895683\n",
      "ep 1943: ep_len:678 episode reward: total was -3.590000. running mean: -41.512626\n",
      "ep 1943: ep_len:40 episode reward: total was 18.500000. running mean: -40.912500\n",
      "ep 1943: ep_len:68 episode reward: total was 31.000000. running mean: -40.193375\n",
      "ep 1943: ep_len:1438 episode reward: total was -101.940000. running mean: -40.810841\n",
      "ep 1943: ep_len:662 episode reward: total was 15.490000. running mean: -40.247833\n",
      "ep 1943: ep_len:1201 episode reward: total was -32.600000. running mean: -40.171355\n",
      "ep 1943: ep_len:703 episode reward: total was 18.750000. running mean: -39.582141\n",
      "ep 1943: ep_len:732 episode reward: total was -4.450000. running mean: -39.230820\n",
      "ep 1943: ep_len:73 episode reward: total was 35.000000. running mean: -38.488512\n",
      "ep 1943: ep_len:1530 episode reward: total was -2.590000. running mean: -38.129526\n",
      "ep 1943: ep_len:2902 episode reward: total was -72.910000. running mean: -38.477331\n",
      "epsilon:0.009992 episode_count: 29270. steps_count: 31347371.000000\n",
      "ep 1944: ep_len:617 episode reward: total was 16.000000. running mean: -37.932558\n",
      "ep 1944: ep_len:764 episode reward: total was -17.810000. running mean: -37.731332\n",
      "ep 1944: ep_len:45 episode reward: total was 21.000000. running mean: -37.144019\n",
      "ep 1944: ep_len:2917 episode reward: total was -101.740000. running mean: -37.789979\n",
      "ep 1944: ep_len:500 episode reward: total was -4.900000. running mean: -37.461079\n",
      "ep 1944: ep_len:74 episode reward: total was 34.000000. running mean: -36.746468\n",
      "ep 1944: ep_len:766 episode reward: total was -13.720000. running mean: -36.516203\n",
      "ep 1944: ep_len:3601 episode reward: total was -7.150000. running mean: -36.222541\n",
      "ep 1944: ep_len:542 episode reward: total was -8.890000. running mean: -35.949216\n",
      "ep 1944: ep_len:740 episode reward: total was 48.060000. running mean: -35.109124\n",
      "ep 1944: ep_len:500 episode reward: total was 55.550000. running mean: -34.202533\n",
      "ep 1944: ep_len:1474 episode reward: total was 0.430000. running mean: -33.856207\n",
      "ep 1944: ep_len:2914 episode reward: total was 1.820000. running mean: -33.499445\n",
      "epsilon:0.009992 episode_count: 29283. steps_count: 31362825.000000\n",
      "ep 1945: ep_len:611 episode reward: total was 19.980000. running mean: -32.964651\n",
      "ep 1945: ep_len:698 episode reward: total was -50.410000. running mean: -33.139104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1945: ep_len:43 episode reward: total was 18.500000. running mean: -32.622713\n",
      "ep 1945: ep_len:3004 episode reward: total was -103.460000. running mean: -33.331086\n",
      "ep 1945: ep_len:501 episode reward: total was -16.520000. running mean: -33.162975\n",
      "ep 1945: ep_len:46 episode reward: total was 21.500000. running mean: -32.616345\n",
      "ep 1945: ep_len:648 episode reward: total was 22.850000. running mean: -32.061682\n",
      "ep 1945: ep_len:353 episode reward: total was 17.150000. running mean: -31.569565\n",
      "ep 1945: ep_len:671 episode reward: total was -33.480000. running mean: -31.588670\n",
      "ep 1945: ep_len:876 episode reward: total was 54.810000. running mean: -30.724683\n",
      "ep 1945: ep_len:500 episode reward: total was 42.960000. running mean: -29.987836\n",
      "ep 1945: ep_len:61 episode reward: total was 27.500000. running mean: -29.412958\n",
      "ep 1945: ep_len:672 episode reward: total was 17.690000. running mean: -28.941928\n",
      "ep 1945: ep_len:2812 episode reward: total was -5.870000. running mean: -28.711209\n",
      "epsilon:0.009992 episode_count: 29297. steps_count: 31374321.000000\n",
      "ep 1946: ep_len:1437 episode reward: total was 6.180000. running mean: -28.362297\n",
      "ep 1946: ep_len:752 episode reward: total was 7.030000. running mean: -28.008374\n",
      "ep 1946: ep_len:3041 episode reward: total was -64.500000. running mean: -28.373290\n",
      "ep 1946: ep_len:500 episode reward: total was 19.010000. running mean: -27.899457\n",
      "ep 1946: ep_len:55 episode reward: total was 26.000000. running mean: -27.360463\n",
      "ep 1946: ep_len:149 episode reward: total was 71.500000. running mean: -26.371858\n",
      "ep 1946: ep_len:62 episode reward: total was 29.500000. running mean: -25.813139\n",
      "ep 1946: ep_len:991 episode reward: total was 7.970000. running mean: -25.475308\n",
      "ep 1946: ep_len:3546 episode reward: total was -20.890000. running mean: -25.429455\n",
      "ep 1946: ep_len:947 episode reward: total was -25.070000. running mean: -25.425860\n",
      "ep 1946: ep_len:812 episode reward: total was 32.760000. running mean: -24.844002\n",
      "ep 1946: ep_len:593 episode reward: total was 70.930000. running mean: -23.886262\n",
      "ep 1946: ep_len:71 episode reward: total was 34.000000. running mean: -23.307399\n",
      "ep 1946: ep_len:209 episode reward: total was 101.500000. running mean: -22.059325\n",
      "ep 1946: ep_len:46 episode reward: total was 21.500000. running mean: -21.623732\n",
      "ep 1946: ep_len:82 episode reward: total was 36.500000. running mean: -21.042495\n",
      "ep 1946: ep_len:859 episode reward: total was -1.650000. running mean: -20.848570\n",
      "ep 1946: ep_len:30 episode reward: total was 13.500000. running mean: -20.505084\n",
      "epsilon:0.009992 episode_count: 29315. steps_count: 31388503.000000\n",
      "ep 1947: ep_len:1457 episode reward: total was 22.480000. running mean: -20.075233\n",
      "ep 1947: ep_len:942 episode reward: total was 7.480000. running mean: -19.799681\n",
      "ep 1947: ep_len:47 episode reward: total was 22.000000. running mean: -19.381684\n",
      "ep 1947: ep_len:2964 episode reward: total was -23.870000. running mean: -19.426567\n",
      "ep 1947: ep_len:516 episode reward: total was -23.750000. running mean: -19.469801\n",
      "ep 1947: ep_len:57 episode reward: total was 27.000000. running mean: -19.005103\n",
      "ep 1947: ep_len:152 episode reward: total was 73.000000. running mean: -18.085052\n",
      "ep 1947: ep_len:102 episode reward: total was 48.000000. running mean: -17.424202\n",
      "ep 1947: ep_len:1101 episode reward: total was 1.080000. running mean: -17.239160\n",
      "ep 1947: ep_len:643 episode reward: total was 27.480000. running mean: -16.791968\n",
      "ep 1947: ep_len:649 episode reward: total was 5.310000. running mean: -16.570949\n",
      "ep 1947: ep_len:717 episode reward: total was -75.670000. running mean: -17.161939\n",
      "ep 1947: ep_len:1062 episode reward: total was 44.250000. running mean: -16.547820\n",
      "ep 1947: ep_len:1067 episode reward: total was -15.660000. running mean: -16.538941\n",
      "ep 1947: ep_len:2823 episode reward: total was -8.210000. running mean: -16.455652\n",
      "ep 1947: ep_len:55 episode reward: total was 24.500000. running mean: -16.046096\n",
      "epsilon:0.009992 episode_count: 29331. steps_count: 31402857.000000\n",
      "ep 1948: ep_len:670 episode reward: total was -2.560000. running mean: -15.911235\n",
      "ep 1948: ep_len:712 episode reward: total was 8.070000. running mean: -15.671422\n",
      "ep 1948: ep_len:3040 episode reward: total was 16.050000. running mean: -15.354208\n",
      "ep 1948: ep_len:549 episode reward: total was -24.980000. running mean: -15.450466\n",
      "ep 1948: ep_len:48 episode reward: total was 22.500000. running mean: -15.070961\n",
      "ep 1948: ep_len:85 episode reward: total was 39.500000. running mean: -14.525252\n",
      "ep 1948: ep_len:67 episode reward: total was 30.500000. running mean: -14.074999\n",
      "ep 1948: ep_len:78 episode reward: total was 36.000000. running mean: -13.574249\n",
      "ep 1948: ep_len:500 episode reward: total was 21.590000. running mean: -13.222607\n",
      "ep 1948: ep_len:3686 episode reward: total was -38.680000. running mean: -13.477181\n",
      "ep 1948: ep_len:1330 episode reward: total was -83.750000. running mean: -14.179909\n",
      "ep 1948: ep_len:648 episode reward: total was -15.230000. running mean: -14.190410\n",
      "ep 1948: ep_len:500 episode reward: total was -3.430000. running mean: -14.082806\n",
      "ep 1948: ep_len:202 episode reward: total was 95.000000. running mean: -12.991978\n",
      "ep 1948: ep_len:1466 episode reward: total was 3.320000. running mean: -12.828858\n",
      "ep 1948: ep_len:2825 episode reward: total was 8.830000. running mean: -12.612269\n",
      "ep 1948: ep_len:56 episode reward: total was 26.500000. running mean: -12.221146\n",
      "epsilon:0.009992 episode_count: 29348. steps_count: 31419319.000000\n",
      "ep 1949: ep_len:1180 episode reward: total was 19.870000. running mean: -11.900235\n",
      "ep 1949: ep_len:1635 episode reward: total was -56.540000. running mean: -12.346633\n",
      "ep 1949: ep_len:43 episode reward: total was 20.000000. running mean: -12.023166\n",
      "ep 1949: ep_len:93 episode reward: total was 45.000000. running mean: -11.452935\n",
      "ep 1949: ep_len:660 episode reward: total was 16.370000. running mean: -11.174705\n",
      "ep 1949: ep_len:79 episode reward: total was 36.500000. running mean: -10.697958\n",
      "ep 1949: ep_len:899 episode reward: total was 49.940000. running mean: -10.091579\n",
      "ep 1949: ep_len:648 episode reward: total was 0.530000. running mean: -9.985363\n",
      "ep 1949: ep_len:623 episode reward: total was 11.230000. running mean: -9.773209\n",
      "ep 1949: ep_len:586 episode reward: total was -9.460000. running mean: -9.770077\n",
      "ep 1949: ep_len:1560 episode reward: total was 21.000000. running mean: -9.462376\n",
      "ep 1949: ep_len:206 episode reward: total was 101.500000. running mean: -8.352753\n",
      "ep 1949: ep_len:500 episode reward: total was 26.670000. running mean: -8.002525\n",
      "ep 1949: ep_len:2814 episode reward: total was -10.590000. running mean: -8.028400\n",
      "epsilon:0.009992 episode_count: 29362. steps_count: 31430845.000000\n",
      "ep 1950: ep_len:1162 episode reward: total was 25.530000. running mean: -7.692816\n",
      "ep 1950: ep_len:651 episode reward: total was -10.190000. running mean: -7.717788\n",
      "ep 1950: ep_len:78 episode reward: total was 36.000000. running mean: -7.280610\n",
      "ep 1950: ep_len:3044 episode reward: total was -2.220000. running mean: -7.230004\n",
      "ep 1950: ep_len:571 episode reward: total was -15.080000. running mean: -7.308504\n",
      "ep 1950: ep_len:78 episode reward: total was 37.500000. running mean: -6.860419\n",
      "ep 1950: ep_len:851 episode reward: total was 9.290000. running mean: -6.698914\n",
      "ep 1950: ep_len:3695 episode reward: total was -54.780000. running mean: -7.179725\n",
      "ep 1950: ep_len:545 episode reward: total was -6.840000. running mean: -7.176328\n",
      "ep 1950: ep_len:813 episode reward: total was 41.900000. running mean: -6.685565\n",
      "ep 1950: ep_len:585 episode reward: total was 59.190000. running mean: -6.026809\n",
      "ep 1950: ep_len:197 episode reward: total was 92.500000. running mean: -5.041541\n",
      "ep 1950: ep_len:1162 episode reward: total was -14.810000. running mean: -5.139226\n",
      "ep 1950: ep_len:2837 episode reward: total was -1.460000. running mean: -5.102433\n",
      "ep 1950: ep_len:50 episode reward: total was 23.500000. running mean: -4.816409\n",
      "epsilon:0.009992 episode_count: 29377. steps_count: 31447164.000000\n",
      "ep 1951: ep_len:1143 episode reward: total was -6.190000. running mean: -4.830145\n",
      "ep 1951: ep_len:684 episode reward: total was -17.740000. running mean: -4.959244\n",
      "ep 1951: ep_len:89 episode reward: total was 41.500000. running mean: -4.494651\n",
      "ep 1951: ep_len:500 episode reward: total was 26.310000. running mean: -4.186605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1951: ep_len:500 episode reward: total was -5.230000. running mean: -4.197039\n",
      "ep 1951: ep_len:646 episode reward: total was 0.880000. running mean: -4.146268\n",
      "ep 1951: ep_len:699 episode reward: total was -38.630000. running mean: -4.491105\n",
      "ep 1951: ep_len:673 episode reward: total was 11.370000. running mean: -4.332494\n",
      "ep 1951: ep_len:657 episode reward: total was -45.110000. running mean: -4.740269\n",
      "ep 1951: ep_len:156 episode reward: total was 73.500000. running mean: -3.957867\n",
      "ep 1951: ep_len:1514 episode reward: total was 2.480000. running mean: -3.893488\n",
      "ep 1951: ep_len:2731 episode reward: total was -20.700000. running mean: -4.061553\n",
      "epsilon:0.009992 episode_count: 29389. steps_count: 31457156.000000\n",
      "ep 1952: ep_len:660 episode reward: total was -4.370000. running mean: -4.064638\n",
      "ep 1952: ep_len:1701 episode reward: total was -61.870000. running mean: -4.642691\n",
      "ep 1952: ep_len:2846 episode reward: total was -2.680000. running mean: -4.623064\n",
      "ep 1952: ep_len:500 episode reward: total was 21.900000. running mean: -4.357834\n",
      "ep 1952: ep_len:40 episode reward: total was 17.000000. running mean: -4.144255\n",
      "ep 1952: ep_len:63 episode reward: total was 30.000000. running mean: -3.802813\n",
      "ep 1952: ep_len:606 episode reward: total was 52.730000. running mean: -3.237485\n",
      "ep 1952: ep_len:3937 episode reward: total was -114.950000. running mean: -4.354610\n",
      "ep 1952: ep_len:794 episode reward: total was -46.540000. running mean: -4.776464\n",
      "ep 1952: ep_len:796 episode reward: total was 13.560000. running mean: -4.593099\n",
      "ep 1952: ep_len:959 episode reward: total was 7.710000. running mean: -4.470068\n",
      "ep 1952: ep_len:644 episode reward: total was 1.220000. running mean: -4.413167\n",
      "ep 1952: ep_len:2846 episode reward: total was -54.590000. running mean: -4.914936\n",
      "epsilon:0.009992 episode_count: 29402. steps_count: 31473548.000000\n",
      "ep 1953: ep_len:1087 episode reward: total was -0.410000. running mean: -4.869886\n",
      "ep 1953: ep_len:688 episode reward: total was -9.020000. running mean: -4.911388\n",
      "ep 1953: ep_len:35 episode reward: total was 16.000000. running mean: -4.702274\n",
      "ep 1953: ep_len:2991 episode reward: total was -40.100000. running mean: -5.056251\n",
      "ep 1953: ep_len:663 episode reward: total was 19.440000. running mean: -4.811288\n",
      "ep 1953: ep_len:64 episode reward: total was 29.000000. running mean: -4.473176\n",
      "ep 1953: ep_len:106 episode reward: total was 50.000000. running mean: -3.928444\n",
      "ep 1953: ep_len:927 episode reward: total was 65.930000. running mean: -3.229859\n",
      "ep 1953: ep_len:3691 episode reward: total was -14.050000. running mean: -3.338061\n",
      "ep 1953: ep_len:2787 episode reward: total was -724.750000. running mean: -10.552180\n",
      "ep 1953: ep_len:598 episode reward: total was -0.310000. running mean: -10.449758\n",
      "ep 1953: ep_len:500 episode reward: total was 30.590000. running mean: -10.039361\n",
      "ep 1953: ep_len:74 episode reward: total was 35.500000. running mean: -9.583967\n",
      "ep 1953: ep_len:36 episode reward: total was 13.500000. running mean: -9.353128\n",
      "ep 1953: ep_len:1123 episode reward: total was 19.480000. running mean: -9.064796\n",
      "ep 1953: ep_len:2817 episode reward: total was -37.560000. running mean: -9.349748\n",
      "epsilon:0.009992 episode_count: 29418. steps_count: 31491735.000000\n",
      "ep 1954: ep_len:765 episode reward: total was -7.170000. running mean: -9.327951\n",
      "ep 1954: ep_len:793 episode reward: total was -13.450000. running mean: -9.369171\n",
      "ep 1954: ep_len:3044 episode reward: total was -40.780000. running mean: -9.683280\n",
      "ep 1954: ep_len:879 episode reward: total was -4.850000. running mean: -9.634947\n",
      "ep 1954: ep_len:35 episode reward: total was 16.000000. running mean: -9.378597\n",
      "ep 1954: ep_len:110 episode reward: total was 52.000000. running mean: -8.764811\n",
      "ep 1954: ep_len:500 episode reward: total was 19.780000. running mean: -8.479363\n",
      "ep 1954: ep_len:3901 episode reward: total was -79.960000. running mean: -9.194170\n",
      "ep 1954: ep_len:2197 episode reward: total was -132.580000. running mean: -10.428028\n",
      "ep 1954: ep_len:7295 episode reward: total was -7.710000. running mean: -10.400848\n",
      "ep 1954: ep_len:917 episode reward: total was 4.170000. running mean: -10.255139\n",
      "ep 1954: ep_len:176 episode reward: total was 83.500000. running mean: -9.317588\n",
      "ep 1954: ep_len:35 episode reward: total was 16.000000. running mean: -9.064412\n",
      "ep 1954: ep_len:63 episode reward: total was 27.000000. running mean: -8.703768\n",
      "ep 1954: ep_len:1057 episode reward: total was 0.550000. running mean: -8.611230\n",
      "ep 1954: ep_len:2827 episode reward: total was -17.260000. running mean: -8.697718\n",
      "epsilon:0.009992 episode_count: 29434. steps_count: 31516329.000000\n",
      "ep 1955: ep_len:1416 episode reward: total was 24.060000. running mean: -8.370141\n",
      "ep 1955: ep_len:918 episode reward: total was 23.920000. running mean: -8.047239\n",
      "ep 1955: ep_len:3000 episode reward: total was -65.130000. running mean: -8.618067\n",
      "ep 1955: ep_len:766 episode reward: total was -21.800000. running mean: -8.749886\n",
      "ep 1955: ep_len:65 episode reward: total was 31.000000. running mean: -8.352387\n",
      "ep 1955: ep_len:86 episode reward: total was 38.500000. running mean: -7.883863\n",
      "ep 1955: ep_len:48 episode reward: total was 21.000000. running mean: -7.595025\n",
      "ep 1955: ep_len:711 episode reward: total was 11.820000. running mean: -7.400875\n",
      "ep 1955: ep_len:3822 episode reward: total was -20.380000. running mean: -7.530666\n",
      "ep 1955: ep_len:552 episode reward: total was -4.750000. running mean: -7.502859\n",
      "ep 1955: ep_len:7278 episode reward: total was -102.370000. running mean: -8.451531\n",
      "ep 1955: ep_len:588 episode reward: total was -5.400000. running mean: -8.421015\n",
      "ep 1955: ep_len:145 episode reward: total was 68.000000. running mean: -7.656805\n",
      "ep 1955: ep_len:93 episode reward: total was 43.500000. running mean: -7.145237\n",
      "ep 1955: ep_len:1124 episode reward: total was -1.290000. running mean: -7.086685\n",
      "ep 1955: ep_len:2755 episode reward: total was 10.920000. running mean: -6.906618\n",
      "epsilon:0.009992 episode_count: 29450. steps_count: 31539696.000000\n",
      "ep 1956: ep_len:660 episode reward: total was 15.220000. running mean: -6.685352\n",
      "ep 1956: ep_len:198 episode reward: total was 13.300000. running mean: -6.485498\n",
      "ep 1956: ep_len:2981 episode reward: total was -27.110000. running mean: -6.691743\n",
      "ep 1956: ep_len:690 episode reward: total was -25.530000. running mean: -6.880126\n",
      "ep 1956: ep_len:970 episode reward: total was -16.300000. running mean: -6.974324\n",
      "ep 1956: ep_len:648 episode reward: total was 25.600000. running mean: -6.648581\n",
      "ep 1956: ep_len:954 episode reward: total was -18.810000. running mean: -6.770195\n",
      "ep 1956: ep_len:712 episode reward: total was 20.830000. running mean: -6.494193\n",
      "ep 1956: ep_len:1129 episode reward: total was -8.560000. running mean: -6.514851\n",
      "ep 1956: ep_len:97 episode reward: total was 45.500000. running mean: -5.994703\n",
      "ep 1956: ep_len:1195 episode reward: total was 16.540000. running mean: -5.769356\n",
      "ep 1956: ep_len:2819 episode reward: total was -8.550000. running mean: -5.797162\n",
      "ep 1956: ep_len:50 episode reward: total was 23.500000. running mean: -5.504191\n",
      "epsilon:0.009992 episode_count: 29463. steps_count: 31552799.000000\n",
      "ep 1957: ep_len:1445 episode reward: total was 22.850000. running mean: -5.220649\n",
      "ep 1957: ep_len:1119 episode reward: total was -3.000000. running mean: -5.198442\n",
      "ep 1957: ep_len:2986 episode reward: total was -10.750000. running mean: -5.253958\n",
      "ep 1957: ep_len:836 episode reward: total was 9.750000. running mean: -5.103918\n",
      "ep 1957: ep_len:112 episode reward: total was 51.500000. running mean: -4.537879\n",
      "ep 1957: ep_len:500 episode reward: total was -3.700000. running mean: -4.529500\n",
      "ep 1957: ep_len:3642 episode reward: total was -167.390000. running mean: -6.158105\n",
      "ep 1957: ep_len:812 episode reward: total was -54.440000. running mean: -6.640924\n",
      "ep 1957: ep_len:774 episode reward: total was 8.620000. running mean: -6.488315\n",
      "ep 1957: ep_len:1125 episode reward: total was 14.850000. running mean: -6.274932\n",
      "ep 1957: ep_len:78 episode reward: total was 36.000000. running mean: -5.852183\n",
      "ep 1957: ep_len:692 episode reward: total was 17.540000. running mean: -5.618261\n",
      "ep 1957: ep_len:2850 episode reward: total was 10.920000. running mean: -5.452878\n",
      "epsilon:0.009992 episode_count: 29476. steps_count: 31569770.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1958: ep_len:1085 episode reward: total was -2.450000. running mean: -5.422849\n",
      "ep 1958: ep_len:1305 episode reward: total was -49.130000. running mean: -5.859921\n",
      "ep 1958: ep_len:3007 episode reward: total was -25.210000. running mean: -6.053422\n",
      "ep 1958: ep_len:639 episode reward: total was 14.100000. running mean: -5.851887\n",
      "ep 1958: ep_len:59 episode reward: total was 26.500000. running mean: -5.528369\n",
      "ep 1958: ep_len:1377 episode reward: total was -181.330000. running mean: -7.286385\n",
      "ep 1958: ep_len:3566 episode reward: total was 2.540000. running mean: -7.188121\n",
      "ep 1958: ep_len:836 episode reward: total was -25.170000. running mean: -7.367940\n",
      "ep 1958: ep_len:820 episode reward: total was 40.230000. running mean: -6.891960\n",
      "ep 1958: ep_len:500 episode reward: total was 40.790000. running mean: -6.415141\n",
      "ep 1958: ep_len:83 episode reward: total was 40.000000. running mean: -5.950989\n",
      "ep 1958: ep_len:88 episode reward: total was 42.500000. running mean: -5.466480\n",
      "ep 1958: ep_len:1161 episode reward: total was -11.790000. running mean: -5.529715\n",
      "ep 1958: ep_len:2886 episode reward: total was -5.950000. running mean: -5.533918\n",
      "epsilon:0.009992 episode_count: 29490. steps_count: 31587182.000000\n",
      "ep 1959: ep_len:1355 episode reward: total was 28.470000. running mean: -5.193878\n",
      "ep 1959: ep_len:985 episode reward: total was 18.840000. running mean: -4.953540\n",
      "ep 1959: ep_len:2900 episode reward: total was -3.660000. running mean: -4.940604\n",
      "ep 1959: ep_len:500 episode reward: total was 29.890000. running mean: -4.592298\n",
      "ep 1959: ep_len:154 episode reward: total was 74.000000. running mean: -3.806375\n",
      "ep 1959: ep_len:641 episode reward: total was -1.840000. running mean: -3.786711\n",
      "ep 1959: ep_len:350 episode reward: total was 11.060000. running mean: -3.638244\n",
      "ep 1959: ep_len:659 episode reward: total was -74.380000. running mean: -4.345662\n",
      "ep 1959: ep_len:838 episode reward: total was 48.760000. running mean: -3.814605\n",
      "ep 1959: ep_len:601 episode reward: total was -28.260000. running mean: -4.059059\n",
      "ep 1959: ep_len:87 episode reward: total was 40.500000. running mean: -3.613469\n",
      "ep 1959: ep_len:771 episode reward: total was -72.120000. running mean: -4.298534\n",
      "ep 1959: ep_len:2891 episode reward: total was 17.300000. running mean: -4.082549\n",
      "ep 1959: ep_len:56 episode reward: total was 25.000000. running mean: -3.791723\n",
      "epsilon:0.009992 episode_count: 29504. steps_count: 31599970.000000\n",
      "ep 1960: ep_len:1493 episode reward: total was 12.980000. running mean: -3.624006\n",
      "ep 1960: ep_len:695 episode reward: total was -28.570000. running mean: -3.873466\n",
      "ep 1960: ep_len:47 episode reward: total was 22.000000. running mean: -3.614731\n",
      "ep 1960: ep_len:2930 episode reward: total was -87.010000. running mean: -4.448684\n",
      "ep 1960: ep_len:827 episode reward: total was 12.320000. running mean: -4.280997\n",
      "ep 1960: ep_len:59 episode reward: total was 28.000000. running mean: -3.958187\n",
      "ep 1960: ep_len:1422 episode reward: total was -83.510000. running mean: -4.753705\n",
      "ep 1960: ep_len:325 episode reward: total was 30.000000. running mean: -4.406168\n",
      "ep 1960: ep_len:1025 episode reward: total was -3.310000. running mean: -4.395206\n",
      "ep 1960: ep_len:784 episode reward: total was 46.110000. running mean: -3.890154\n",
      "ep 1960: ep_len:684 episode reward: total was -9.490000. running mean: -3.946153\n",
      "ep 1960: ep_len:59 episode reward: total was 28.000000. running mean: -3.626691\n",
      "ep 1960: ep_len:676 episode reward: total was -18.660000. running mean: -3.777024\n",
      "ep 1960: ep_len:2786 episode reward: total was -12.710000. running mean: -3.866354\n",
      "ep 1960: ep_len:52 episode reward: total was 24.500000. running mean: -3.582691\n",
      "epsilon:0.009992 episode_count: 29519. steps_count: 31613834.000000\n",
      "ep 1961: ep_len:651 episode reward: total was -18.910000. running mean: -3.735964\n",
      "ep 1961: ep_len:727 episode reward: total was -38.580000. running mean: -4.084404\n",
      "ep 1961: ep_len:44 episode reward: total was 20.500000. running mean: -3.838560\n",
      "ep 1961: ep_len:3070 episode reward: total was 1.650000. running mean: -3.783674\n",
      "ep 1961: ep_len:1492 episode reward: total was 14.290000. running mean: -3.602938\n",
      "ep 1961: ep_len:139 episode reward: total was 66.500000. running mean: -2.901908\n",
      "ep 1961: ep_len:500 episode reward: total was 14.370000. running mean: -2.729189\n",
      "ep 1961: ep_len:3825 episode reward: total was -448.140000. running mean: -7.183297\n",
      "ep 1961: ep_len:789 episode reward: total was -31.040000. running mean: -7.421864\n",
      "ep 1961: ep_len:787 episode reward: total was 10.070000. running mean: -7.246946\n",
      "ep 1961: ep_len:710 episode reward: total was 32.440000. running mean: -6.850076\n",
      "ep 1961: ep_len:178 episode reward: total was 84.500000. running mean: -5.936576\n",
      "ep 1961: ep_len:1148 episode reward: total was -7.540000. running mean: -5.952610\n",
      "ep 1961: ep_len:2888 episode reward: total was -376.130000. running mean: -9.654384\n",
      "ep 1961: ep_len:49 episode reward: total was 23.000000. running mean: -9.327840\n",
      "epsilon:0.009992 episode_count: 29534. steps_count: 31630831.000000\n",
      "ep 1962: ep_len:882 episode reward: total was 5.450000. running mean: -9.180061\n",
      "ep 1962: ep_len:673 episode reward: total was -19.480000. running mean: -9.283061\n",
      "ep 1962: ep_len:2869 episode reward: total was -5.730000. running mean: -9.247530\n",
      "ep 1962: ep_len:551 episode reward: total was 6.350000. running mean: -9.091555\n",
      "ep 1962: ep_len:57 episode reward: total was 27.000000. running mean: -8.730639\n",
      "ep 1962: ep_len:500 episode reward: total was 16.080000. running mean: -8.482533\n",
      "ep 1962: ep_len:340 episode reward: total was 19.590000. running mean: -8.201808\n",
      "ep 1962: ep_len:2797 episode reward: total was -321.810000. running mean: -11.337890\n",
      "ep 1962: ep_len:799 episode reward: total was 22.250000. running mean: -11.002011\n",
      "ep 1962: ep_len:713 episode reward: total was -21.390000. running mean: -11.105891\n",
      "ep 1962: ep_len:72 episode reward: total was 33.000000. running mean: -10.664832\n",
      "ep 1962: ep_len:159 episode reward: total was 73.500000. running mean: -9.823183\n",
      "ep 1962: ep_len:61 episode reward: total was 29.000000. running mean: -9.434951\n",
      "ep 1962: ep_len:937 episode reward: total was -84.890000. running mean: -10.189502\n",
      "ep 1962: ep_len:2708 episode reward: total was -2.960000. running mean: -10.117207\n",
      "ep 1962: ep_len:53 episode reward: total was 25.000000. running mean: -9.766035\n",
      "epsilon:0.009992 episode_count: 29550. steps_count: 31645002.000000\n",
      "ep 1963: ep_len:500 episode reward: total was 1.780000. running mean: -9.650575\n",
      "ep 1963: ep_len:1291 episode reward: total was -37.700000. running mean: -9.931069\n",
      "ep 1963: ep_len:61 episode reward: total was 29.000000. running mean: -9.541758\n",
      "ep 1963: ep_len:3046 episode reward: total was -57.540000. running mean: -10.021741\n",
      "ep 1963: ep_len:536 episode reward: total was -58.440000. running mean: -10.505923\n",
      "ep 1963: ep_len:60 episode reward: total was 27.000000. running mean: -10.130864\n",
      "ep 1963: ep_len:884 episode reward: total was 62.010000. running mean: -9.409455\n",
      "ep 1963: ep_len:3608 episode reward: total was -40.470000. running mean: -9.720061\n",
      "ep 1963: ep_len:816 episode reward: total was -20.670000. running mean: -9.829560\n",
      "ep 1963: ep_len:839 episode reward: total was 50.180000. running mean: -9.229464\n",
      "ep 1963: ep_len:1107 episode reward: total was -9.300000. running mean: -9.230170\n",
      "ep 1963: ep_len:74 episode reward: total was 32.500000. running mean: -8.812868\n",
      "ep 1963: ep_len:67 episode reward: total was 32.000000. running mean: -8.404739\n",
      "ep 1963: ep_len:76 episode reward: total was 35.000000. running mean: -7.970692\n",
      "ep 1963: ep_len:763 episode reward: total was -63.240000. running mean: -8.523385\n",
      "ep 1963: ep_len:2847 episode reward: total was -5.120000. running mean: -8.489351\n",
      "epsilon:0.009992 episode_count: 29566. steps_count: 31661577.000000\n",
      "ep 1964: ep_len:500 episode reward: total was 15.620000. running mean: -8.248258\n",
      "ep 1964: ep_len:660 episode reward: total was -29.930000. running mean: -8.465075\n",
      "ep 1964: ep_len:2982 episode reward: total was -69.180000. running mean: -9.072224\n",
      "ep 1964: ep_len:500 episode reward: total was -5.450000. running mean: -9.036002\n",
      "ep 1964: ep_len:73 episode reward: total was 35.000000. running mean: -8.595642\n",
      "ep 1964: ep_len:66 episode reward: total was 31.500000. running mean: -8.194686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1964: ep_len:832 episode reward: total was 21.770000. running mean: -7.895039\n",
      "ep 1964: ep_len:585 episode reward: total was 17.530000. running mean: -7.640789\n",
      "ep 1964: ep_len:545 episode reward: total was -11.030000. running mean: -7.674681\n",
      "ep 1964: ep_len:890 episode reward: total was 76.840000. running mean: -6.829534\n",
      "ep 1964: ep_len:1150 episode reward: total was -5.840000. running mean: -6.819638\n",
      "ep 1964: ep_len:823 episode reward: total was 28.080000. running mean: -6.470642\n",
      "ep 1964: ep_len:2892 episode reward: total was -13.790000. running mean: -6.543836\n",
      "ep 1964: ep_len:57 episode reward: total was 27.000000. running mean: -6.208397\n",
      "epsilon:0.009992 episode_count: 29580. steps_count: 31674132.000000\n",
      "ep 1965: ep_len:647 episode reward: total was -5.370000. running mean: -6.200013\n",
      "ep 1965: ep_len:667 episode reward: total was -45.870000. running mean: -6.596713\n",
      "ep 1965: ep_len:44 episode reward: total was 20.500000. running mean: -6.325746\n",
      "ep 1965: ep_len:3006 episode reward: total was -27.460000. running mean: -6.537089\n",
      "ep 1965: ep_len:1144 episode reward: total was -36.200000. running mean: -6.833718\n",
      "ep 1965: ep_len:113 episode reward: total was 53.500000. running mean: -6.230381\n",
      "ep 1965: ep_len:1082 episode reward: total was -7.100000. running mean: -6.239077\n",
      "ep 1965: ep_len:643 episode reward: total was 22.610000. running mean: -5.950586\n",
      "ep 1965: ep_len:793 episode reward: total was -39.360000. running mean: -6.284680\n",
      "ep 1965: ep_len:727 episode reward: total was 11.570000. running mean: -6.106133\n",
      "ep 1965: ep_len:727 episode reward: total was 21.530000. running mean: -5.829772\n",
      "ep 1965: ep_len:131 episode reward: total was 61.000000. running mean: -5.161474\n",
      "ep 1965: ep_len:652 episode reward: total was 8.890000. running mean: -5.020960\n",
      "ep 1965: ep_len:2828 episode reward: total was -9.010000. running mean: -5.060850\n",
      "epsilon:0.009992 episode_count: 29594. steps_count: 31687336.000000\n",
      "ep 1966: ep_len:500 episode reward: total was 50.370000. running mean: -4.506541\n",
      "ep 1966: ep_len:709 episode reward: total was -38.730000. running mean: -4.848776\n",
      "ep 1966: ep_len:2942 episode reward: total was -41.810000. running mean: -5.218388\n",
      "ep 1966: ep_len:500 episode reward: total was 24.500000. running mean: -4.921204\n",
      "ep 1966: ep_len:112 episode reward: total was 53.000000. running mean: -4.341992\n",
      "ep 1966: ep_len:967 episode reward: total was -135.940000. running mean: -5.657972\n",
      "ep 1966: ep_len:3565 episode reward: total was -145.450000. running mean: -7.055893\n",
      "ep 1966: ep_len:556 episode reward: total was -3.700000. running mean: -7.022334\n",
      "ep 1966: ep_len:871 episode reward: total was 66.120000. running mean: -6.290910\n",
      "ep 1966: ep_len:547 episode reward: total was 29.100000. running mean: -5.937001\n",
      "ep 1966: ep_len:160 episode reward: total was 75.500000. running mean: -5.122631\n",
      "ep 1966: ep_len:1060 episode reward: total was 23.390000. running mean: -4.837505\n",
      "ep 1966: ep_len:2906 episode reward: total was 0.460000. running mean: -4.784530\n",
      "epsilon:0.009992 episode_count: 29607. steps_count: 31702731.000000\n",
      "ep 1967: ep_len:1131 episode reward: total was -17.110000. running mean: -4.907785\n",
      "ep 1967: ep_len:936 episode reward: total was 21.590000. running mean: -4.642807\n",
      "ep 1967: ep_len:53 episode reward: total was 25.000000. running mean: -4.346379\n",
      "ep 1967: ep_len:2998 episode reward: total was -40.790000. running mean: -4.710815\n",
      "ep 1967: ep_len:858 episode reward: total was 16.000000. running mean: -4.503707\n",
      "ep 1967: ep_len:30 episode reward: total was 13.500000. running mean: -4.323670\n",
      "ep 1967: ep_len:104 episode reward: total was 49.000000. running mean: -3.790433\n",
      "ep 1967: ep_len:756 episode reward: total was -55.050000. running mean: -4.303029\n",
      "ep 1967: ep_len:3568 episode reward: total was -69.640000. running mean: -4.956398\n",
      "ep 1967: ep_len:773 episode reward: total was -34.320000. running mean: -5.250034\n",
      "ep 1967: ep_len:7434 episode reward: total was 24.340000. running mean: -4.954134\n",
      "ep 1967: ep_len:992 episode reward: total was 16.000000. running mean: -4.744593\n",
      "ep 1967: ep_len:55 episode reward: total was 26.000000. running mean: -4.437147\n",
      "ep 1967: ep_len:1065 episode reward: total was 10.420000. running mean: -4.288575\n",
      "ep 1967: ep_len:2867 episode reward: total was -104.650000. running mean: -5.292190\n",
      "epsilon:0.009992 episode_count: 29622. steps_count: 31726351.000000\n",
      "ep 1968: ep_len:796 episode reward: total was -32.700000. running mean: -5.566268\n",
      "ep 1968: ep_len:609 episode reward: total was -29.430000. running mean: -5.804905\n",
      "ep 1968: ep_len:3110 episode reward: total was -7.460000. running mean: -5.821456\n",
      "ep 1968: ep_len:679 episode reward: total was 21.400000. running mean: -5.549241\n",
      "ep 1968: ep_len:103 episode reward: total was 50.000000. running mean: -4.993749\n",
      "ep 1968: ep_len:73 episode reward: total was 35.000000. running mean: -4.593812\n",
      "ep 1968: ep_len:1130 episode reward: total was -8.060000. running mean: -4.628473\n",
      "ep 1968: ep_len:643 episode reward: total was 17.440000. running mean: -4.407789\n",
      "ep 1968: ep_len:572 episode reward: total was -47.980000. running mean: -4.843511\n",
      "ep 1968: ep_len:7637 episode reward: total was -137.710000. running mean: -6.172176\n",
      "ep 1968: ep_len:660 episode reward: total was -1.590000. running mean: -6.126354\n",
      "ep 1968: ep_len:213 episode reward: total was 103.500000. running mean: -5.030090\n",
      "ep 1968: ep_len:1073 episode reward: total was -9.020000. running mean: -5.069989\n",
      "ep 1968: ep_len:2801 episode reward: total was -754.300000. running mean: -12.562290\n",
      "epsilon:0.009992 episode_count: 29636. steps_count: 31746450.000000\n",
      "ep 1969: ep_len:1140 episode reward: total was -34.220000. running mean: -12.778867\n",
      "ep 1969: ep_len:720 episode reward: total was -18.970000. running mean: -12.840778\n",
      "ep 1969: ep_len:2941 episode reward: total was -41.210000. running mean: -13.124470\n",
      "ep 1969: ep_len:677 episode reward: total was -6.140000. running mean: -13.054626\n",
      "ep 1969: ep_len:155 episode reward: total was 76.000000. running mean: -12.164079\n",
      "ep 1969: ep_len:69 episode reward: total was 33.000000. running mean: -11.712438\n",
      "ep 1969: ep_len:1450 episode reward: total was 11.790000. running mean: -11.477414\n",
      "ep 1969: ep_len:353 episode reward: total was 24.220000. running mean: -11.120440\n",
      "ep 1969: ep_len:583 episode reward: total was -40.740000. running mean: -11.416636\n",
      "ep 1969: ep_len:764 episode reward: total was 48.420000. running mean: -10.818269\n",
      "ep 1969: ep_len:510 episode reward: total was -8.100000. running mean: -10.791087\n",
      "ep 1969: ep_len:45 episode reward: total was 21.000000. running mean: -10.473176\n",
      "ep 1969: ep_len:99 episode reward: total was 45.000000. running mean: -9.918444\n",
      "ep 1969: ep_len:744 episode reward: total was -21.070000. running mean: -10.029959\n",
      "ep 1969: ep_len:2814 episode reward: total was -10.320000. running mean: -10.032860\n",
      "ep 1969: ep_len:64 episode reward: total was 29.000000. running mean: -9.642531\n",
      "epsilon:0.009992 episode_count: 29652. steps_count: 31759578.000000\n",
      "ep 1970: ep_len:724 episode reward: total was -22.760000. running mean: -9.773706\n",
      "ep 1970: ep_len:500 episode reward: total was 9.930000. running mean: -9.576669\n",
      "ep 1970: ep_len:63 episode reward: total was 30.000000. running mean: -9.180902\n",
      "ep 1970: ep_len:3063 episode reward: total was -33.160000. running mean: -9.420693\n",
      "ep 1970: ep_len:627 episode reward: total was 6.200000. running mean: -9.264486\n",
      "ep 1970: ep_len:51 episode reward: total was 24.000000. running mean: -8.931841\n",
      "ep 1970: ep_len:69 episode reward: total was 33.000000. running mean: -8.512523\n",
      "ep 1970: ep_len:500 episode reward: total was 26.490000. running mean: -8.162498\n",
      "ep 1970: ep_len:351 episode reward: total was 13.150000. running mean: -7.949373\n",
      "ep 1970: ep_len:1305 episode reward: total was -56.550000. running mean: -8.435379\n",
      "ep 1970: ep_len:709 episode reward: total was 46.490000. running mean: -7.886125\n",
      "ep 1970: ep_len:683 episode reward: total was 8.220000. running mean: -7.725064\n",
      "ep 1970: ep_len:79 episode reward: total was 35.000000. running mean: -7.297813\n",
      "ep 1970: ep_len:37 episode reward: total was 17.000000. running mean: -7.054835\n",
      "ep 1970: ep_len:1031 episode reward: total was -3.290000. running mean: -7.017187\n",
      "ep 1970: ep_len:2780 episode reward: total was -4.290000. running mean: -6.989915\n",
      "epsilon:0.009992 episode_count: 29668. steps_count: 31772150.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1971: ep_len:684 episode reward: total was -2.350000. running mean: -6.943516\n",
      "ep 1971: ep_len:1156 episode reward: total was -10.740000. running mean: -6.981481\n",
      "ep 1971: ep_len:3028 episode reward: total was -103.020000. running mean: -7.941866\n",
      "ep 1971: ep_len:1632 episode reward: total was -143.190000. running mean: -9.294347\n",
      "ep 1971: ep_len:49 episode reward: total was 23.000000. running mean: -8.971404\n",
      "ep 1971: ep_len:875 episode reward: total was 37.290000. running mean: -8.508790\n",
      "ep 1971: ep_len:3662 episode reward: total was -41.890000. running mean: -8.842602\n",
      "ep 1971: ep_len:775 episode reward: total was -21.080000. running mean: -8.964976\n",
      "ep 1971: ep_len:798 episode reward: total was 5.730000. running mean: -8.818026\n",
      "ep 1971: ep_len:1085 episode reward: total was 44.970000. running mean: -8.280146\n",
      "ep 1971: ep_len:811 episode reward: total was -24.380000. running mean: -8.441144\n",
      "ep 1971: ep_len:2843 episode reward: total was -32.340000. running mean: -8.680133\n",
      "ep 1971: ep_len:47 episode reward: total was 22.000000. running mean: -8.373332\n",
      "epsilon:0.009992 episode_count: 29681. steps_count: 31789595.000000\n",
      "ep 1972: ep_len:820 episode reward: total was 10.170000. running mean: -8.187898\n",
      "ep 1972: ep_len:668 episode reward: total was -62.520000. running mean: -8.731219\n",
      "ep 1972: ep_len:3086 episode reward: total was -24.230000. running mean: -8.886207\n",
      "ep 1972: ep_len:840 episode reward: total was 11.430000. running mean: -8.683045\n",
      "ep 1972: ep_len:126 episode reward: total was 61.500000. running mean: -7.981215\n",
      "ep 1972: ep_len:63 episode reward: total was 30.000000. running mean: -7.601402\n",
      "ep 1972: ep_len:500 episode reward: total was -8.170000. running mean: -7.607088\n",
      "ep 1972: ep_len:3597 episode reward: total was -70.360000. running mean: -8.234617\n",
      "ep 1972: ep_len:557 episode reward: total was -8.100000. running mean: -8.233271\n",
      "ep 1972: ep_len:811 episode reward: total was 2.960000. running mean: -8.121339\n",
      "ep 1972: ep_len:683 episode reward: total was 8.940000. running mean: -7.950725\n",
      "ep 1972: ep_len:119 episode reward: total was 58.000000. running mean: -7.291218\n",
      "ep 1972: ep_len:1432 episode reward: total was 10.570000. running mean: -7.112606\n",
      "ep 1972: ep_len:2865 episode reward: total was -28.080000. running mean: -7.322280\n",
      "ep 1972: ep_len:48 episode reward: total was 21.000000. running mean: -7.039057\n",
      "epsilon:0.009992 episode_count: 29696. steps_count: 31805810.000000\n",
      "ep 1973: ep_len:997 episode reward: total was -130.590000. running mean: -8.274566\n",
      "ep 1973: ep_len:216 episode reward: total was 6.930000. running mean: -8.122521\n",
      "ep 1973: ep_len:2982 episode reward: total was 0.660000. running mean: -8.034695\n",
      "ep 1973: ep_len:1735 episode reward: total was -69.650000. running mean: -8.650849\n",
      "ep 1973: ep_len:152 episode reward: total was 70.000000. running mean: -7.864340\n",
      "ep 1973: ep_len:54 episode reward: total was 24.000000. running mean: -7.545697\n",
      "ep 1973: ep_len:1439 episode reward: total was 36.910000. running mean: -7.101140\n",
      "ep 1973: ep_len:500 episode reward: total was 21.230000. running mean: -6.817828\n",
      "ep 1973: ep_len:1156 episode reward: total was 1.390000. running mean: -6.735750\n",
      "ep 1973: ep_len:705 episode reward: total was 48.900000. running mean: -6.179392\n",
      "ep 1973: ep_len:954 episode reward: total was 60.600000. running mean: -5.511599\n",
      "ep 1973: ep_len:94 episode reward: total was 45.500000. running mean: -5.001483\n",
      "ep 1973: ep_len:1482 episode reward: total was 6.510000. running mean: -4.886368\n",
      "ep 1973: ep_len:2878 episode reward: total was 7.770000. running mean: -4.759804\n",
      "epsilon:0.009992 episode_count: 29710. steps_count: 31821154.000000\n",
      "ep 1974: ep_len:945 episode reward: total was -125.780000. running mean: -5.970006\n",
      "ep 1974: ep_len:819 episode reward: total was 13.940000. running mean: -5.770906\n",
      "ep 1974: ep_len:3011 episode reward: total was -43.050000. running mean: -6.143697\n",
      "ep 1974: ep_len:636 episode reward: total was 4.770000. running mean: -6.034560\n",
      "ep 1974: ep_len:120 episode reward: total was 58.500000. running mean: -5.389214\n",
      "ep 1974: ep_len:57 episode reward: total was 27.000000. running mean: -5.065322\n",
      "ep 1974: ep_len:1466 episode reward: total was 18.010000. running mean: -4.834569\n",
      "ep 1974: ep_len:3594 episode reward: total was -1749.500000. running mean: -22.281223\n",
      "ep 1974: ep_len:865 episode reward: total was -38.590000. running mean: -22.444311\n",
      "ep 1974: ep_len:799 episode reward: total was 5.720000. running mean: -22.162668\n",
      "ep 1974: ep_len:704 episode reward: total was -16.030000. running mean: -22.101341\n",
      "ep 1974: ep_len:37 episode reward: total was 15.500000. running mean: -21.725328\n",
      "ep 1974: ep_len:100 episode reward: total was 47.000000. running mean: -21.038075\n",
      "ep 1974: ep_len:632 episode reward: total was 9.510000. running mean: -20.732594\n",
      "ep 1974: ep_len:2817 episode reward: total was 7.380000. running mean: -20.451468\n",
      "ep 1974: ep_len:62 episode reward: total was 26.500000. running mean: -19.981953\n",
      "epsilon:0.009992 episode_count: 29726. steps_count: 31837818.000000\n",
      "ep 1975: ep_len:624 episode reward: total was 11.880000. running mean: -19.663334\n",
      "ep 1975: ep_len:699 episode reward: total was -13.610000. running mean: -19.602800\n",
      "ep 1975: ep_len:3067 episode reward: total was -69.080000. running mean: -20.097572\n",
      "ep 1975: ep_len:610 episode reward: total was -9.220000. running mean: -19.988797\n",
      "ep 1975: ep_len:61 episode reward: total was 29.000000. running mean: -19.498909\n",
      "ep 1975: ep_len:61 episode reward: total was 27.500000. running mean: -19.028920\n",
      "ep 1975: ep_len:1460 episode reward: total was -100.870000. running mean: -19.847330\n",
      "ep 1975: ep_len:343 episode reward: total was 16.560000. running mean: -19.483257\n",
      "ep 1975: ep_len:867 episode reward: total was -31.900000. running mean: -19.607424\n",
      "ep 1975: ep_len:710 episode reward: total was -0.510000. running mean: -19.416450\n",
      "ep 1975: ep_len:500 episode reward: total was 49.360000. running mean: -18.728686\n",
      "ep 1975: ep_len:84 episode reward: total was 39.000000. running mean: -18.151399\n",
      "ep 1975: ep_len:118 episode reward: total was 57.500000. running mean: -17.394885\n",
      "ep 1975: ep_len:51 episode reward: total was 24.000000. running mean: -16.980936\n",
      "ep 1975: ep_len:782 episode reward: total was -52.950000. running mean: -17.340627\n",
      "ep 1975: ep_len:2863 episode reward: total was -26.780000. running mean: -17.435020\n",
      "epsilon:0.009992 episode_count: 29742. steps_count: 31850718.000000\n",
      "ep 1976: ep_len:790 episode reward: total was -22.780000. running mean: -17.488470\n",
      "ep 1976: ep_len:793 episode reward: total was 7.500000. running mean: -17.238586\n",
      "ep 1976: ep_len:3050 episode reward: total was -25.510000. running mean: -17.321300\n",
      "ep 1976: ep_len:794 episode reward: total was 19.210000. running mean: -16.955987\n",
      "ep 1976: ep_len:49 episode reward: total was 23.000000. running mean: -16.556427\n",
      "ep 1976: ep_len:120 episode reward: total was 57.000000. running mean: -15.820863\n",
      "ep 1976: ep_len:500 episode reward: total was 8.480000. running mean: -15.577854\n",
      "ep 1976: ep_len:337 episode reward: total was 20.020000. running mean: -15.221875\n",
      "ep 1976: ep_len:657 episode reward: total was -23.410000. running mean: -15.303757\n",
      "ep 1976: ep_len:738 episode reward: total was 32.510000. running mean: -14.825619\n",
      "ep 1976: ep_len:897 episode reward: total was 11.680000. running mean: -14.560563\n",
      "ep 1976: ep_len:56 episode reward: total was 26.500000. running mean: -14.149957\n",
      "ep 1976: ep_len:166 episode reward: total was 80.000000. running mean: -13.208458\n",
      "ep 1976: ep_len:79 episode reward: total was 38.000000. running mean: -12.696373\n",
      "ep 1976: ep_len:597 episode reward: total was -8.000000. running mean: -12.649409\n",
      "ep 1976: ep_len:2824 episode reward: total was -12.330000. running mean: -12.646215\n",
      "epsilon:0.009992 episode_count: 29758. steps_count: 31863165.000000\n",
      "ep 1977: ep_len:973 episode reward: total was -82.350000. running mean: -13.343253\n",
      "ep 1977: ep_len:209 episode reward: total was 1.320000. running mean: -13.196621\n",
      "ep 1977: ep_len:3148 episode reward: total was 4.030000. running mean: -13.024354\n",
      "ep 1977: ep_len:532 episode reward: total was -21.110000. running mean: -13.105211\n",
      "ep 1977: ep_len:52 episode reward: total was 24.500000. running mean: -12.729159\n",
      "ep 1977: ep_len:85 episode reward: total was 39.500000. running mean: -12.206867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1977: ep_len:64 episode reward: total was 29.000000. running mean: -11.794798\n",
      "ep 1977: ep_len:47 episode reward: total was 22.000000. running mean: -11.456850\n",
      "ep 1977: ep_len:680 episode reward: total was 3.080000. running mean: -11.311482\n",
      "ep 1977: ep_len:673 episode reward: total was 28.330000. running mean: -10.915067\n",
      "ep 1977: ep_len:573 episode reward: total was 2.530000. running mean: -10.780616\n",
      "ep 1977: ep_len:652 episode reward: total was 13.920000. running mean: -10.533610\n",
      "ep 1977: ep_len:953 episode reward: total was 8.080000. running mean: -10.347474\n",
      "ep 1977: ep_len:87 episode reward: total was 42.000000. running mean: -9.823999\n",
      "ep 1977: ep_len:138 episode reward: total was 61.500000. running mean: -9.110759\n",
      "ep 1977: ep_len:35 episode reward: total was 16.000000. running mean: -8.859652\n",
      "ep 1977: ep_len:69 episode reward: total was 31.500000. running mean: -8.456055\n",
      "ep 1977: ep_len:1132 episode reward: total was 4.940000. running mean: -8.322095\n",
      "ep 1977: ep_len:2826 episode reward: total was -13.440000. running mean: -8.373274\n",
      "epsilon:0.009992 episode_count: 29777. steps_count: 31876093.000000\n",
      "ep 1978: ep_len:1021 episode reward: total was -7.130000. running mean: -8.360841\n",
      "ep 1978: ep_len:828 episode reward: total was 4.420000. running mean: -8.233033\n",
      "ep 1978: ep_len:67 episode reward: total was 30.500000. running mean: -7.845702\n",
      "ep 1978: ep_len:2989 episode reward: total was -38.300000. running mean: -8.150245\n",
      "ep 1978: ep_len:810 episode reward: total was 7.130000. running mean: -7.997443\n",
      "ep 1978: ep_len:60 episode reward: total was 28.500000. running mean: -7.632468\n",
      "ep 1978: ep_len:918 episode reward: total was 55.460000. running mean: -7.001544\n",
      "ep 1978: ep_len:661 episode reward: total was 19.580000. running mean: -6.735728\n",
      "ep 1978: ep_len:555 episode reward: total was -27.950000. running mean: -6.947871\n",
      "ep 1978: ep_len:684 episode reward: total was 41.310000. running mean: -6.465292\n",
      "ep 1978: ep_len:1104 episode reward: total was -7.310000. running mean: -6.473739\n",
      "ep 1978: ep_len:54 episode reward: total was 24.000000. running mean: -6.169002\n",
      "ep 1978: ep_len:500 episode reward: total was 21.950000. running mean: -5.887812\n",
      "ep 1978: ep_len:2900 episode reward: total was -24.450000. running mean: -6.073434\n",
      "ep 1978: ep_len:42 episode reward: total was 19.500000. running mean: -5.817700\n",
      "epsilon:0.009992 episode_count: 29792. steps_count: 31889286.000000\n",
      "ep 1979: ep_len:1385 episode reward: total was 7.980000. running mean: -5.679723\n",
      "ep 1979: ep_len:941 episode reward: total was 15.240000. running mean: -5.470525\n",
      "ep 1979: ep_len:47 episode reward: total was 22.000000. running mean: -5.195820\n",
      "ep 1979: ep_len:3013 episode reward: total was -6.170000. running mean: -5.205562\n",
      "ep 1979: ep_len:838 episode reward: total was 16.780000. running mean: -4.985706\n",
      "ep 1979: ep_len:114 episode reward: total was 55.500000. running mean: -4.380849\n",
      "ep 1979: ep_len:1018 episode reward: total was 2.910000. running mean: -4.307941\n",
      "ep 1979: ep_len:587 episode reward: total was 29.950000. running mean: -3.965361\n",
      "ep 1979: ep_len:917 episode reward: total was -44.530000. running mean: -4.371008\n",
      "ep 1979: ep_len:662 episode reward: total was 8.290000. running mean: -4.244398\n",
      "ep 1979: ep_len:880 episode reward: total was -472.990000. running mean: -8.931854\n",
      "ep 1979: ep_len:47 episode reward: total was 22.000000. running mean: -8.622535\n",
      "ep 1979: ep_len:91 episode reward: total was 44.000000. running mean: -8.096310\n",
      "ep 1979: ep_len:1139 episode reward: total was -15.040000. running mean: -8.165747\n",
      "ep 1979: ep_len:2707 episode reward: total was -10.130000. running mean: -8.185389\n",
      "epsilon:0.009992 episode_count: 29807. steps_count: 31903672.000000\n",
      "ep 1980: ep_len:786 episode reward: total was -6.450000. running mean: -8.168035\n",
      "ep 1980: ep_len:743 episode reward: total was -1.590000. running mean: -8.102255\n",
      "ep 1980: ep_len:80 episode reward: total was 37.000000. running mean: -7.651232\n",
      "ep 1980: ep_len:92 episode reward: total was 44.500000. running mean: -7.129720\n",
      "ep 1980: ep_len:659 episode reward: total was 3.610000. running mean: -7.022323\n",
      "ep 1980: ep_len:57 episode reward: total was 25.500000. running mean: -6.697100\n",
      "ep 1980: ep_len:114 episode reward: total was 54.000000. running mean: -6.090129\n",
      "ep 1980: ep_len:500 episode reward: total was 5.300000. running mean: -5.976227\n",
      "ep 1980: ep_len:3734 episode reward: total was -86.620000. running mean: -6.782665\n",
      "ep 1980: ep_len:1277 episode reward: total was -53.520000. running mean: -7.250038\n",
      "ep 1980: ep_len:636 episode reward: total was 34.520000. running mean: -6.832338\n",
      "ep 1980: ep_len:805 episode reward: total was 13.680000. running mean: -6.627215\n",
      "ep 1980: ep_len:189 episode reward: total was 91.500000. running mean: -5.645943\n",
      "ep 1980: ep_len:643 episode reward: total was 1.210000. running mean: -5.577383\n",
      "ep 1980: ep_len:2789 episode reward: total was -45.440000. running mean: -5.976009\n",
      "ep 1980: ep_len:48 episode reward: total was 21.000000. running mean: -5.706249\n",
      "epsilon:0.009992 episode_count: 29823. steps_count: 31916824.000000\n",
      "ep 1981: ep_len:1097 episode reward: total was -20.970000. running mean: -5.858887\n",
      "ep 1981: ep_len:1056 episode reward: total was 38.900000. running mean: -5.411298\n",
      "ep 1981: ep_len:64 episode reward: total was 30.500000. running mean: -5.052185\n",
      "ep 1981: ep_len:2858 episode reward: total was -88.660000. running mean: -5.888263\n",
      "ep 1981: ep_len:1211 episode reward: total was -8.160000. running mean: -5.910980\n",
      "ep 1981: ep_len:53 episode reward: total was 22.000000. running mean: -5.631871\n",
      "ep 1981: ep_len:93 episode reward: total was 45.000000. running mean: -5.125552\n",
      "ep 1981: ep_len:724 episode reward: total was 13.660000. running mean: -4.937696\n",
      "ep 1981: ep_len:651 episode reward: total was 23.550000. running mean: -4.652819\n",
      "ep 1981: ep_len:500 episode reward: total was 12.310000. running mean: -4.483191\n",
      "ep 1981: ep_len:751 episode reward: total was 1.810000. running mean: -4.420259\n",
      "ep 1981: ep_len:682 episode reward: total was 3.410000. running mean: -4.341957\n",
      "ep 1981: ep_len:69 episode reward: total was 31.500000. running mean: -3.983537\n",
      "ep 1981: ep_len:159 episode reward: total was 75.000000. running mean: -3.193702\n",
      "ep 1981: ep_len:61 episode reward: total was 29.000000. running mean: -2.871765\n",
      "ep 1981: ep_len:63 episode reward: total was 27.000000. running mean: -2.573047\n",
      "ep 1981: ep_len:1466 episode reward: total was 29.520000. running mean: -2.252117\n",
      "ep 1981: ep_len:2745 episode reward: total was -16.390000. running mean: -2.393495\n",
      "ep 1981: ep_len:41 episode reward: total was 19.000000. running mean: -2.179560\n",
      "epsilon:0.009992 episode_count: 29842. steps_count: 31931168.000000\n",
      "ep 1982: ep_len:627 episode reward: total was -16.240000. running mean: -2.320165\n",
      "ep 1982: ep_len:765 episode reward: total was -13.760000. running mean: -2.434563\n",
      "ep 1982: ep_len:2891 episode reward: total was -50.620000. running mean: -2.916418\n",
      "ep 1982: ep_len:700 episode reward: total was 30.320000. running mean: -2.584053\n",
      "ep 1982: ep_len:153 episode reward: total was 75.000000. running mean: -1.808213\n",
      "ep 1982: ep_len:991 episode reward: total was -13.240000. running mean: -1.922531\n",
      "ep 1982: ep_len:640 episode reward: total was 24.050000. running mean: -1.662805\n",
      "ep 1982: ep_len:901 episode reward: total was -34.070000. running mean: -1.986877\n",
      "ep 1982: ep_len:638 episode reward: total was 13.540000. running mean: -1.831609\n",
      "ep 1982: ep_len:500 episode reward: total was 1.170000. running mean: -1.801593\n",
      "ep 1982: ep_len:153 episode reward: total was 70.500000. running mean: -1.078577\n",
      "ep 1982: ep_len:1033 episode reward: total was 18.710000. running mean: -0.880691\n",
      "ep 1982: ep_len:46 episode reward: total was 21.500000. running mean: -0.656884\n",
      "ep 1982: ep_len:67 episode reward: total was 32.000000. running mean: -0.330315\n",
      "epsilon:0.009992 episode_count: 29856. steps_count: 31941273.000000\n",
      "ep 1983: ep_len:901 episode reward: total was 13.430000. running mean: -0.192712\n",
      "ep 1983: ep_len:500 episode reward: total was 20.180000. running mean: 0.011015\n",
      "ep 1983: ep_len:2987 episode reward: total was -38.790000. running mean: -0.376995\n",
      "ep 1983: ep_len:500 episode reward: total was 26.930000. running mean: -0.103925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1983: ep_len:53 episode reward: total was 25.000000. running mean: 0.147114\n",
      "ep 1983: ep_len:906 episode reward: total was 50.590000. running mean: 0.651543\n",
      "ep 1983: ep_len:363 episode reward: total was 20.800000. running mean: 0.853028\n",
      "ep 1983: ep_len:544 episode reward: total was -34.120000. running mean: 0.503297\n",
      "ep 1983: ep_len:868 episode reward: total was 37.220000. running mean: 0.870464\n",
      "ep 1983: ep_len:1023 episode reward: total was -13.440000. running mean: 0.727360\n",
      "ep 1983: ep_len:86 episode reward: total was 41.500000. running mean: 1.135086\n",
      "ep 1983: ep_len:210 episode reward: total was 99.000000. running mean: 2.113735\n",
      "ep 1983: ep_len:632 episode reward: total was 16.130000. running mean: 2.253898\n",
      "ep 1983: ep_len:2871 episode reward: total was -15.680000. running mean: 2.074559\n",
      "epsilon:0.009992 episode_count: 29870. steps_count: 31953717.000000\n",
      "ep 1984: ep_len:1439 episode reward: total was 15.900000. running mean: 2.212813\n",
      "ep 1984: ep_len:1144 episode reward: total was -11.660000. running mean: 2.074085\n",
      "ep 1984: ep_len:99 episode reward: total was 46.500000. running mean: 2.518344\n",
      "ep 1984: ep_len:918 episode reward: total was 55.890000. running mean: 3.052061\n",
      "ep 1984: ep_len:43 episode reward: total was 20.000000. running mean: 3.221540\n",
      "ep 1984: ep_len:1424 episode reward: total was -147.530000. running mean: 1.714025\n",
      "ep 1984: ep_len:329 episode reward: total was 20.550000. running mean: 1.902385\n",
      "ep 1984: ep_len:897 episode reward: total was -30.590000. running mean: 1.577461\n",
      "ep 1984: ep_len:703 episode reward: total was 16.490000. running mean: 1.726586\n",
      "ep 1984: ep_len:923 episode reward: total was 19.930000. running mean: 1.908620\n",
      "ep 1984: ep_len:171 episode reward: total was 81.000000. running mean: 2.699534\n",
      "ep 1984: ep_len:96 episode reward: total was 46.500000. running mean: 3.137539\n",
      "ep 1984: ep_len:1400 episode reward: total was 28.920000. running mean: 3.395363\n",
      "ep 1984: ep_len:2829 episode reward: total was -15.950000. running mean: 3.201910\n",
      "epsilon:0.009992 episode_count: 29884. steps_count: 31966132.000000\n",
      "ep 1985: ep_len:654 episode reward: total was -19.890000. running mean: 2.970991\n",
      "ep 1985: ep_len:1007 episode reward: total was 22.270000. running mean: 3.163981\n",
      "ep 1985: ep_len:46 episode reward: total was 18.500000. running mean: 3.317341\n",
      "ep 1985: ep_len:3014 episode reward: total was 24.730000. running mean: 3.531468\n",
      "ep 1985: ep_len:530 episode reward: total was -13.720000. running mean: 3.358953\n",
      "ep 1985: ep_len:65 episode reward: total was 29.500000. running mean: 3.620363\n",
      "ep 1985: ep_len:835 episode reward: total was 19.470000. running mean: 3.778860\n",
      "ep 1985: ep_len:500 episode reward: total was 24.200000. running mean: 3.983071\n",
      "ep 1985: ep_len:639 episode reward: total was 1.170000. running mean: 3.954940\n",
      "ep 1985: ep_len:785 episode reward: total was 8.790000. running mean: 4.003291\n",
      "ep 1985: ep_len:575 episode reward: total was -40.020000. running mean: 3.563058\n",
      "ep 1985: ep_len:189 episode reward: total was 93.000000. running mean: 4.457428\n",
      "ep 1985: ep_len:67 episode reward: total was 30.500000. running mean: 4.717853\n",
      "ep 1985: ep_len:500 episode reward: total was 36.620000. running mean: 5.036875\n",
      "ep 1985: ep_len:2888 episode reward: total was 11.790000. running mean: 5.104406\n",
      "epsilon:0.009992 episode_count: 29899. steps_count: 31978426.000000\n",
      "ep 1986: ep_len:1490 episode reward: total was 27.370000. running mean: 5.327062\n",
      "ep 1986: ep_len:500 episode reward: total was 26.330000. running mean: 5.537091\n",
      "ep 1986: ep_len:26 episode reward: total was 11.500000. running mean: 5.596720\n",
      "ep 1986: ep_len:3035 episode reward: total was -35.030000. running mean: 5.190453\n",
      "ep 1986: ep_len:500 episode reward: total was 12.120000. running mean: 5.259749\n",
      "ep 1986: ep_len:51 episode reward: total was 22.500000. running mean: 5.432151\n",
      "ep 1986: ep_len:94 episode reward: total was 44.000000. running mean: 5.817830\n",
      "ep 1986: ep_len:1006 episode reward: total was -3.480000. running mean: 5.724851\n",
      "ep 1986: ep_len:3675 episode reward: total was -1013.670000. running mean: -4.469097\n",
      "ep 1986: ep_len:1119 episode reward: total was -21.790000. running mean: -4.642306\n",
      "ep 1986: ep_len:716 episode reward: total was 34.560000. running mean: -4.250283\n",
      "ep 1986: ep_len:1080 episode reward: total was -3.510000. running mean: -4.242880\n",
      "ep 1986: ep_len:101 episode reward: total was 47.500000. running mean: -3.725451\n",
      "ep 1986: ep_len:57 episode reward: total was 27.000000. running mean: -3.418197\n",
      "ep 1986: ep_len:111 episode reward: total was 54.000000. running mean: -2.844015\n",
      "ep 1986: ep_len:1473 episode reward: total was 11.990000. running mean: -2.695675\n",
      "ep 1986: ep_len:2823 episode reward: total was -21.280000. running mean: -2.881518\n",
      "epsilon:0.009992 episode_count: 29916. steps_count: 31996283.000000\n",
      "ep 1987: ep_len:1474 episode reward: total was 15.730000. running mean: -2.695403\n",
      "ep 1987: ep_len:650 episode reward: total was -28.010000. running mean: -2.948549\n",
      "ep 1987: ep_len:75 episode reward: total was 36.000000. running mean: -2.559063\n",
      "ep 1987: ep_len:3055 episode reward: total was -4.920000. running mean: -2.582673\n",
      "ep 1987: ep_len:687 episode reward: total was -6.500000. running mean: -2.621846\n",
      "ep 1987: ep_len:43 episode reward: total was 18.500000. running mean: -2.410628\n",
      "ep 1987: ep_len:158 episode reward: total was 77.500000. running mean: -1.611521\n",
      "ep 1987: ep_len:1478 episode reward: total was 25.290000. running mean: -1.342506\n",
      "ep 1987: ep_len:3669 episode reward: total was -23.700000. running mean: -1.566081\n",
      "ep 1987: ep_len:1287 episode reward: total was -35.870000. running mean: -1.909120\n",
      "ep 1987: ep_len:852 episode reward: total was 28.720000. running mean: -1.602829\n",
      "ep 1987: ep_len:530 episode reward: total was 24.430000. running mean: -1.342501\n",
      "ep 1987: ep_len:95 episode reward: total was 43.000000. running mean: -0.899076\n",
      "ep 1987: ep_len:196 episode reward: total was 93.500000. running mean: 0.044915\n",
      "ep 1987: ep_len:66 episode reward: total was 31.500000. running mean: 0.359466\n",
      "ep 1987: ep_len:774 episode reward: total was -55.810000. running mean: -0.202229\n",
      "ep 1987: ep_len:2843 episode reward: total was -24.110000. running mean: -0.441306\n",
      "epsilon:0.009992 episode_count: 29933. steps_count: 32014215.000000\n",
      "ep 1988: ep_len:611 episode reward: total was -28.250000. running mean: -0.719393\n",
      "ep 1988: ep_len:665 episode reward: total was -13.720000. running mean: -0.849399\n",
      "ep 1988: ep_len:81 episode reward: total was 37.500000. running mean: -0.465905\n",
      "ep 1988: ep_len:2980 episode reward: total was -36.970000. running mean: -0.830946\n",
      "ep 1988: ep_len:500 episode reward: total was 19.230000. running mean: -0.630337\n",
      "ep 1988: ep_len:500 episode reward: total was -10.940000. running mean: -0.733434\n",
      "ep 1988: ep_len:656 episode reward: total was 22.620000. running mean: -0.499899\n",
      "ep 1988: ep_len:557 episode reward: total was 15.570000. running mean: -0.339200\n",
      "ep 1988: ep_len:7305 episode reward: total was -4.700000. running mean: -0.382808\n",
      "ep 1988: ep_len:500 episode reward: total was 6.400000. running mean: -0.314980\n",
      "ep 1988: ep_len:121 episode reward: total was 57.500000. running mean: 0.263170\n",
      "ep 1988: ep_len:1123 episode reward: total was -10.090000. running mean: 0.159638\n",
      "ep 1988: ep_len:2876 episode reward: total was -28.890000. running mean: -0.130858\n",
      "ep 1988: ep_len:62 episode reward: total was 29.500000. running mean: 0.165450\n",
      "epsilon:0.009992 episode_count: 29947. steps_count: 32032752.000000\n",
      "ep 1989: ep_len:1475 episode reward: total was 18.620000. running mean: 0.349996\n",
      "ep 1989: ep_len:1236 episode reward: total was -49.330000. running mean: -0.146804\n",
      "ep 1989: ep_len:2980 episode reward: total was -55.570000. running mean: -0.701036\n",
      "ep 1989: ep_len:668 episode reward: total was 0.900000. running mean: -0.685026\n",
      "ep 1989: ep_len:91 episode reward: total was 42.500000. running mean: -0.253176\n",
      "ep 1989: ep_len:624 episode reward: total was 4.970000. running mean: -0.200944\n",
      "ep 1989: ep_len:656 episode reward: total was 24.240000. running mean: 0.043466\n",
      "ep 1989: ep_len:549 episode reward: total was 3.300000. running mean: 0.076031\n",
      "ep 1989: ep_len:7350 episode reward: total was 40.980000. running mean: 0.485071\n",
      "ep 1989: ep_len:728 episode reward: total was -2.370000. running mean: 0.456520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1989: ep_len:89 episode reward: total was 43.000000. running mean: 0.881955\n",
      "ep 1989: ep_len:732 episode reward: total was -65.510000. running mean: 0.218035\n",
      "ep 1989: ep_len:2873 episode reward: total was -27.360000. running mean: -0.057745\n",
      "epsilon:0.009992 episode_count: 29960. steps_count: 32052803.000000\n",
      "ep 1990: ep_len:1150 episode reward: total was -0.790000. running mean: -0.065068\n",
      "ep 1990: ep_len:687 episode reward: total was -17.540000. running mean: -0.239817\n",
      "ep 1990: ep_len:55 episode reward: total was 26.000000. running mean: 0.022581\n",
      "ep 1990: ep_len:3027 episode reward: total was -34.750000. running mean: -0.325145\n",
      "ep 1990: ep_len:667 episode reward: total was 1.480000. running mean: -0.307093\n",
      "ep 1990: ep_len:52 episode reward: total was 23.000000. running mean: -0.074022\n",
      "ep 1990: ep_len:43 episode reward: total was 20.000000. running mean: 0.126718\n",
      "ep 1990: ep_len:858 episode reward: total was 29.130000. running mean: 0.416751\n",
      "ep 1990: ep_len:3604 episode reward: total was -16.330000. running mean: 0.249283\n",
      "ep 1990: ep_len:1100 episode reward: total was -56.200000. running mean: -0.315210\n",
      "ep 1990: ep_len:778 episode reward: total was -2.360000. running mean: -0.335658\n",
      "ep 1990: ep_len:1559 episode reward: total was 21.520000. running mean: -0.117101\n",
      "ep 1990: ep_len:143 episode reward: total was 67.000000. running mean: 0.554070\n",
      "ep 1990: ep_len:84 episode reward: total was 37.500000. running mean: 0.923529\n",
      "ep 1990: ep_len:739 episode reward: total was -4.900000. running mean: 0.865294\n",
      "ep 1990: ep_len:2860 episode reward: total was -10.620000. running mean: 0.750441\n",
      "ep 1990: ep_len:41 episode reward: total was 19.000000. running mean: 0.932937\n",
      "epsilon:0.009992 episode_count: 29977. steps_count: 32070250.000000\n",
      "ep 1991: ep_len:1155 episode reward: total was 9.950000. running mean: 1.023107\n",
      "ep 1991: ep_len:720 episode reward: total was -7.160000. running mean: 0.941276\n",
      "ep 1991: ep_len:72 episode reward: total was 34.500000. running mean: 1.276863\n",
      "ep 1991: ep_len:2950 episode reward: total was -100.190000. running mean: 0.262195\n",
      "ep 1991: ep_len:528 episode reward: total was -7.010000. running mean: 0.189473\n",
      "ep 1991: ep_len:44 episode reward: total was 20.500000. running mean: 0.392578\n",
      "ep 1991: ep_len:110 episode reward: total was 49.000000. running mean: 0.878652\n",
      "ep 1991: ep_len:708 episode reward: total was 0.910000. running mean: 0.878966\n",
      "ep 1991: ep_len:3828 episode reward: total was 4.350000. running mean: 0.913676\n",
      "ep 1991: ep_len:901 episode reward: total was -35.600000. running mean: 0.548539\n",
      "ep 1991: ep_len:617 episode reward: total was -6.790000. running mean: 0.475154\n",
      "ep 1991: ep_len:527 episode reward: total was 14.390000. running mean: 0.614303\n",
      "ep 1991: ep_len:1180 episode reward: total was 15.350000. running mean: 0.761659\n",
      "ep 1991: ep_len:2748 episode reward: total was -13.240000. running mean: 0.621643\n",
      "ep 1991: ep_len:73 episode reward: total was 35.000000. running mean: 0.965426\n",
      "epsilon:0.009992 episode_count: 29992. steps_count: 32086411.000000\n",
      "ep 1992: ep_len:677 episode reward: total was -4.510000. running mean: 0.910672\n",
      "ep 1992: ep_len:645 episode reward: total was -29.070000. running mean: 0.610865\n",
      "ep 1992: ep_len:2927 episode reward: total was 10.880000. running mean: 0.713557\n",
      "ep 1992: ep_len:500 episode reward: total was -2.110000. running mean: 0.685321\n",
      "ep 1992: ep_len:500 episode reward: total was 29.950000. running mean: 0.977968\n",
      "ep 1992: ep_len:354 episode reward: total was 20.220000. running mean: 1.170388\n",
      "ep 1992: ep_len:678 episode reward: total was -29.750000. running mean: 0.861184\n",
      "ep 1992: ep_len:832 episode reward: total was 29.410000. running mean: 1.146673\n",
      "ep 1992: ep_len:1029 episode reward: total was -2.030000. running mean: 1.114906\n",
      "ep 1992: ep_len:26 episode reward: total was 11.500000. running mean: 1.218757\n",
      "ep 1992: ep_len:88 episode reward: total was 42.500000. running mean: 1.631569\n",
      "ep 1992: ep_len:1438 episode reward: total was 5.580000. running mean: 1.671054\n",
      "ep 1992: ep_len:2761 episode reward: total was 5.590000. running mean: 1.710243\n",
      "epsilon:0.009992 episode_count: 30005. steps_count: 32098866.000000\n",
      "ep 1993: ep_len:967 episode reward: total was -68.430000. running mean: 1.008841\n",
      "ep 1993: ep_len:1712 episode reward: total was -43.680000. running mean: 0.561952\n",
      "ep 1993: ep_len:69 episode reward: total was 31.500000. running mean: 0.871333\n",
      "ep 1993: ep_len:2910 episode reward: total was -34.390000. running mean: 0.518719\n",
      "ep 1993: ep_len:500 episode reward: total was 16.050000. running mean: 0.674032\n",
      "ep 1993: ep_len:55 episode reward: total was 26.000000. running mean: 0.927292\n",
      "ep 1993: ep_len:644 episode reward: total was -4.790000. running mean: 0.870119\n",
      "ep 1993: ep_len:333 episode reward: total was 16.120000. running mean: 1.022618\n",
      "ep 1993: ep_len:657 episode reward: total was -0.150000. running mean: 1.010892\n",
      "ep 1993: ep_len:751 episode reward: total was 6.240000. running mean: 1.063183\n",
      "ep 1993: ep_len:756 episode reward: total was -9.780000. running mean: 0.954751\n",
      "ep 1993: ep_len:117 episode reward: total was 58.010000. running mean: 1.525303\n",
      "ep 1993: ep_len:38 episode reward: total was 17.500000. running mean: 1.685050\n",
      "ep 1993: ep_len:1023 episode reward: total was 14.310000. running mean: 1.811300\n",
      "ep 1993: ep_len:2945 episode reward: total was -1.750000. running mean: 1.775687\n",
      "ep 1993: ep_len:46 episode reward: total was 21.500000. running mean: 1.972930\n",
      "epsilon:0.009992 episode_count: 30021. steps_count: 32112389.000000\n",
      "ep 1994: ep_len:603 episode reward: total was -19.280000. running mean: 1.760401\n",
      "ep 1994: ep_len:780 episode reward: total was -12.760000. running mean: 1.615197\n",
      "ep 1994: ep_len:63 episode reward: total was 28.500000. running mean: 1.884045\n",
      "ep 1994: ep_len:3007 episode reward: total was -6.620000. running mean: 1.799004\n",
      "ep 1994: ep_len:788 episode reward: total was 20.010000. running mean: 1.981114\n",
      "ep 1994: ep_len:99 episode reward: total was 46.500000. running mean: 2.426303\n",
      "ep 1994: ep_len:80 episode reward: total was 38.500000. running mean: 2.787040\n",
      "ep 1994: ep_len:641 episode reward: total was -4.360000. running mean: 2.715570\n",
      "ep 1994: ep_len:500 episode reward: total was 27.600000. running mean: 2.964414\n",
      "ep 1994: ep_len:972 episode reward: total was -13.790000. running mean: 2.796870\n",
      "ep 1994: ep_len:7447 episode reward: total was -796.290000. running mean: -5.193999\n",
      "ep 1994: ep_len:612 episode reward: total was -25.430000. running mean: -5.396359\n",
      "ep 1994: ep_len:204 episode reward: total was 97.500000. running mean: -4.367395\n",
      "ep 1994: ep_len:57 episode reward: total was 25.500000. running mean: -4.068721\n",
      "ep 1994: ep_len:1212 episode reward: total was -6.230000. running mean: -4.090334\n",
      "ep 1994: ep_len:47 episode reward: total was 22.000000. running mean: -3.829431\n",
      "epsilon:0.009992 episode_count: 30037. steps_count: 32129501.000000\n",
      "ep 1995: ep_len:1173 episode reward: total was 7.130000. running mean: -3.719837\n",
      "ep 1995: ep_len:710 episode reward: total was -19.470000. running mean: -3.877338\n",
      "ep 1995: ep_len:2898 episode reward: total was -26.940000. running mean: -4.107965\n",
      "ep 1995: ep_len:584 episode reward: total was 19.920000. running mean: -3.867685\n",
      "ep 1995: ep_len:88 episode reward: total was 39.500000. running mean: -3.434008\n",
      "ep 1995: ep_len:500 episode reward: total was 1.620000. running mean: -3.383468\n",
      "ep 1995: ep_len:641 episode reward: total was 29.480000. running mean: -3.054834\n",
      "ep 1995: ep_len:3959 episode reward: total was -513.010000. running mean: -8.154385\n",
      "ep 1995: ep_len:726 episode reward: total was -4.450000. running mean: -8.117341\n",
      "ep 1995: ep_len:730 episode reward: total was -12.060000. running mean: -8.156768\n",
      "ep 1995: ep_len:57 episode reward: total was 25.500000. running mean: -7.820200\n",
      "ep 1995: ep_len:130 episode reward: total was 62.000000. running mean: -7.121998\n",
      "ep 1995: ep_len:61 episode reward: total was 29.000000. running mean: -6.760778\n",
      "ep 1995: ep_len:1024 episode reward: total was -527.100000. running mean: -11.964170\n",
      "ep 1995: ep_len:2767 episode reward: total was -30.560000. running mean: -12.150129\n",
      "ep 1995: ep_len:61 episode reward: total was 29.000000. running mean: -11.738628\n",
      "epsilon:0.009992 episode_count: 30053. steps_count: 32145610.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1996: ep_len:1083 episode reward: total was -8.900000. running mean: -11.710241\n",
      "ep 1996: ep_len:1241 episode reward: total was -40.280000. running mean: -11.995939\n",
      "ep 1996: ep_len:2969 episode reward: total was -2.760000. running mean: -11.903579\n",
      "ep 1996: ep_len:611 episode reward: total was -4.740000. running mean: -11.831944\n",
      "ep 1996: ep_len:54 episode reward: total was 21.000000. running mean: -11.503624\n",
      "ep 1996: ep_len:500 episode reward: total was -7.310000. running mean: -11.461688\n",
      "ep 1996: ep_len:672 episode reward: total was 14.730000. running mean: -11.199771\n",
      "ep 1996: ep_len:796 episode reward: total was -12.810000. running mean: -11.215873\n",
      "ep 1996: ep_len:734 episode reward: total was -3.300000. running mean: -11.136715\n",
      "ep 1996: ep_len:500 episode reward: total was 52.640000. running mean: -10.498947\n",
      "ep 1996: ep_len:37 episode reward: total was 15.500000. running mean: -10.238958\n",
      "ep 1996: ep_len:148 episode reward: total was 69.500000. running mean: -9.441568\n",
      "ep 1996: ep_len:55 episode reward: total was 26.000000. running mean: -9.087153\n",
      "ep 1996: ep_len:82 episode reward: total was 39.500000. running mean: -8.601281\n",
      "ep 1996: ep_len:655 episode reward: total was -16.480000. running mean: -8.680068\n",
      "ep 1996: ep_len:2880 episode reward: total was -4.540000. running mean: -8.638668\n",
      "epsilon:0.009992 episode_count: 30069. steps_count: 32158627.000000\n",
      "ep 1997: ep_len:663 episode reward: total was -28.340000. running mean: -8.835681\n",
      "ep 1997: ep_len:772 episode reward: total was -25.540000. running mean: -9.002724\n",
      "ep 1997: ep_len:57 episode reward: total was 27.000000. running mean: -8.642697\n",
      "ep 1997: ep_len:2891 episode reward: total was -26.530000. running mean: -8.821570\n",
      "ep 1997: ep_len:1466 episode reward: total was 7.260000. running mean: -8.660754\n",
      "ep 1997: ep_len:179 episode reward: total was 88.000000. running mean: -7.694147\n",
      "ep 1997: ep_len:640 episode reward: total was 7.970000. running mean: -7.537505\n",
      "ep 1997: ep_len:648 episode reward: total was 28.050000. running mean: -7.181630\n",
      "ep 1997: ep_len:605 episode reward: total was 14.830000. running mean: -6.961514\n",
      "ep 1997: ep_len:728 episode reward: total was -3.390000. running mean: -6.925799\n",
      "ep 1997: ep_len:615 episode reward: total was 67.630000. running mean: -6.180241\n",
      "ep 1997: ep_len:186 episode reward: total was 90.000000. running mean: -5.218438\n",
      "ep 1997: ep_len:118 episode reward: total was 57.500000. running mean: -4.591254\n",
      "ep 1997: ep_len:1097 episode reward: total was -1.320000. running mean: -4.558541\n",
      "ep 1997: ep_len:2937 episode reward: total was 9.250000. running mean: -4.420456\n",
      "ep 1997: ep_len:49 episode reward: total was 20.000000. running mean: -4.176252\n",
      "epsilon:0.009992 episode_count: 30085. steps_count: 32172278.000000\n",
      "ep 1998: ep_len:1109 episode reward: total was 10.190000. running mean: -4.032589\n",
      "ep 1998: ep_len:500 episode reward: total was 17.390000. running mean: -3.818363\n",
      "ep 1998: ep_len:51 episode reward: total was 22.500000. running mean: -3.555179\n",
      "ep 1998: ep_len:2925 episode reward: total was -68.030000. running mean: -4.199928\n",
      "ep 1998: ep_len:662 episode reward: total was -8.700000. running mean: -4.244928\n",
      "ep 1998: ep_len:122 episode reward: total was 58.000000. running mean: -3.622479\n",
      "ep 1998: ep_len:1372 episode reward: total was -152.090000. running mean: -5.107154\n",
      "ep 1998: ep_len:668 episode reward: total was 22.190000. running mean: -4.834183\n",
      "ep 1998: ep_len:734 episode reward: total was -37.710000. running mean: -5.162941\n",
      "ep 1998: ep_len:696 episode reward: total was -1.230000. running mean: -5.123612\n",
      "ep 1998: ep_len:988 episode reward: total was 17.460000. running mean: -4.897775\n",
      "ep 1998: ep_len:51 episode reward: total was 24.000000. running mean: -4.608798\n",
      "ep 1998: ep_len:100 episode reward: total was 47.000000. running mean: -4.092710\n",
      "ep 1998: ep_len:887 episode reward: total was 15.250000. running mean: -3.899283\n",
      "ep 1998: ep_len:2838 episode reward: total was -3.500000. running mean: -3.895290\n",
      "epsilon:0.009992 episode_count: 30100. steps_count: 32185981.000000\n",
      "ep 1999: ep_len:810 episode reward: total was -16.310000. running mean: -4.019437\n",
      "ep 1999: ep_len:697 episode reward: total was -26.040000. running mean: -4.239643\n",
      "ep 1999: ep_len:46 episode reward: total was 21.500000. running mean: -3.982246\n",
      "ep 1999: ep_len:3010 episode reward: total was -5.800000. running mean: -4.000424\n",
      "ep 1999: ep_len:649 episode reward: total was 10.170000. running mean: -3.858719\n",
      "ep 1999: ep_len:59 episode reward: total was 28.000000. running mean: -3.540132\n",
      "ep 1999: ep_len:998 episode reward: total was 0.480000. running mean: -3.499931\n",
      "ep 1999: ep_len:346 episode reward: total was 12.090000. running mean: -3.344032\n",
      "ep 1999: ep_len:833 episode reward: total was 17.340000. running mean: -3.137191\n",
      "ep 1999: ep_len:786 episode reward: total was 11.740000. running mean: -2.988419\n",
      "ep 1999: ep_len:1489 episode reward: total was 28.560000. running mean: -2.672935\n",
      "ep 1999: ep_len:77 episode reward: total was 35.500000. running mean: -2.291206\n",
      "ep 1999: ep_len:53 episode reward: total was 23.500000. running mean: -2.033294\n",
      "ep 1999: ep_len:119 episode reward: total was 58.000000. running mean: -1.432961\n",
      "ep 1999: ep_len:1214 episode reward: total was 12.650000. running mean: -1.292131\n",
      "ep 1999: ep_len:2805 episode reward: total was -21.270000. running mean: -1.491910\n",
      "epsilon:0.009992 episode_count: 30116. steps_count: 32199972.000000\n",
      "ep 2000: ep_len:574 episode reward: total was 14.930000. running mean: -1.327691\n",
      "ep 2000: ep_len:704 episode reward: total was -19.390000. running mean: -1.508314\n",
      "ep 2000: ep_len:50 episode reward: total was 22.000000. running mean: -1.273231\n",
      "ep 2000: ep_len:3034 episode reward: total was -44.460000. running mean: -1.705098\n",
      "ep 2000: ep_len:675 episode reward: total was 6.580000. running mean: -1.622247\n",
      "ep 2000: ep_len:60 episode reward: total was 28.500000. running mean: -1.321025\n",
      "ep 2000: ep_len:133 episode reward: total was 60.500000. running mean: -0.702815\n",
      "ep 2000: ep_len:107 episode reward: total was 49.000000. running mean: -0.205787\n",
      "ep 2000: ep_len:37 episode reward: total was 17.000000. running mean: -0.033729\n",
      "ep 2000: ep_len:1125 episode reward: total was 1.990000. running mean: -0.013491\n",
      "ep 2000: ep_len:4193 episode reward: total was -196.420000. running mean: -1.977557\n",
      "ep 2000: ep_len:709 episode reward: total was -8.860000. running mean: -2.046381\n",
      "ep 2000: ep_len:821 episode reward: total was 46.360000. running mean: -1.562317\n",
      "ep 2000: ep_len:547 episode reward: total was 4.050000. running mean: -1.506194\n",
      "ep 2000: ep_len:1123 episode reward: total was -1.000000. running mean: -1.501132\n",
      "ep 2000: ep_len:2872 episode reward: total was -17.590000. running mean: -1.662021\n",
      "epsilon:0.009992 episode_count: 30132. steps_count: 32216736.000000\n",
      "ep 2001: ep_len:1021 episode reward: total was -74.800000. running mean: -2.393400\n",
      "ep 2001: ep_len:663 episode reward: total was -23.840000. running mean: -2.607866\n",
      "ep 2001: ep_len:3027 episode reward: total was -39.440000. running mean: -2.976188\n",
      "ep 2001: ep_len:692 episode reward: total was 13.600000. running mean: -2.810426\n",
      "ep 2001: ep_len:44 episode reward: total was 20.500000. running mean: -2.577322\n",
      "ep 2001: ep_len:867 episode reward: total was 27.200000. running mean: -2.279548\n",
      "ep 2001: ep_len:3926 episode reward: total was -76.660000. running mean: -3.023353\n",
      "ep 2001: ep_len:2805 episode reward: total was -223.080000. running mean: -5.223919\n",
      "ep 2001: ep_len:661 episode reward: total was 15.200000. running mean: -5.019680\n",
      "ep 2001: ep_len:500 episode reward: total was 32.210000. running mean: -4.647383\n",
      "ep 2001: ep_len:67 episode reward: total was 32.000000. running mean: -4.280910\n",
      "ep 2001: ep_len:1062 episode reward: total was -25.780000. running mean: -4.495901\n",
      "ep 2001: ep_len:2841 episode reward: total was -14.880000. running mean: -4.599742\n",
      "epsilon:0.009992 episode_count: 30145. steps_count: 32234912.000000\n",
      "ep 2002: ep_len:1459 episode reward: total was 10.440000. running mean: -4.449344\n",
      "ep 2002: ep_len:926 episode reward: total was -20.570000. running mean: -4.610551\n",
      "ep 2002: ep_len:2989 episode reward: total was -8.730000. running mean: -4.651745\n",
      "ep 2002: ep_len:679 episode reward: total was -9.040000. running mean: -4.695628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2002: ep_len:72 episode reward: total was 33.000000. running mean: -4.318671\n",
      "ep 2002: ep_len:851 episode reward: total was 36.140000. running mean: -3.914085\n",
      "ep 2002: ep_len:4235 episode reward: total was -145.540000. running mean: -5.330344\n",
      "ep 2002: ep_len:826 episode reward: total was -18.540000. running mean: -5.462440\n",
      "ep 2002: ep_len:905 episode reward: total was 80.880000. running mean: -4.599016\n",
      "ep 2002: ep_len:500 episode reward: total was 25.780000. running mean: -4.295226\n",
      "ep 2002: ep_len:110 episode reward: total was 53.500000. running mean: -3.717274\n",
      "ep 2002: ep_len:658 episode reward: total was 8.700000. running mean: -3.593101\n",
      "ep 2002: ep_len:2815 episode reward: total was -22.000000. running mean: -3.777170\n",
      "ep 2002: ep_len:65 episode reward: total was 29.500000. running mean: -3.444398\n",
      "epsilon:0.009992 episode_count: 30159. steps_count: 32252002.000000\n",
      "ep 2003: ep_len:1131 episode reward: total was 5.200000. running mean: -3.357954\n",
      "ep 2003: ep_len:1690 episode reward: total was -47.390000. running mean: -3.798275\n",
      "ep 2003: ep_len:78 episode reward: total was 37.500000. running mean: -3.385292\n",
      "ep 2003: ep_len:2982 episode reward: total was -29.050000. running mean: -3.641939\n",
      "ep 2003: ep_len:519 episode reward: total was -61.060000. running mean: -4.216120\n",
      "ep 2003: ep_len:46 episode reward: total was 21.500000. running mean: -3.958958\n",
      "ep 2003: ep_len:106 episode reward: total was 48.500000. running mean: -3.434369\n",
      "ep 2003: ep_len:730 episode reward: total was -24.090000. running mean: -3.640925\n",
      "ep 2003: ep_len:4019 episode reward: total was -84.840000. running mean: -4.452916\n",
      "ep 2003: ep_len:837 episode reward: total was 25.740000. running mean: -4.150987\n",
      "ep 2003: ep_len:750 episode reward: total was 16.160000. running mean: -3.947877\n",
      "ep 2003: ep_len:697 episode reward: total was 9.120000. running mean: -3.817198\n",
      "ep 2003: ep_len:710 episode reward: total was -39.530000. running mean: -4.174326\n",
      "ep 2003: ep_len:2929 episode reward: total was -13.110000. running mean: -4.263683\n",
      "epsilon:0.009992 episode_count: 30173. steps_count: 32269226.000000\n",
      "ep 2004: ep_len:744 episode reward: total was -70.990000. running mean: -4.930946\n",
      "ep 2004: ep_len:987 episode reward: total was 12.640000. running mean: -4.755237\n",
      "ep 2004: ep_len:40 episode reward: total was 18.500000. running mean: -4.522684\n",
      "ep 2004: ep_len:2936 episode reward: total was -11.040000. running mean: -4.587857\n",
      "ep 2004: ep_len:684 episode reward: total was 17.210000. running mean: -4.369879\n",
      "ep 2004: ep_len:101 episode reward: total was 48.510000. running mean: -3.841080\n",
      "ep 2004: ep_len:64 episode reward: total was 29.000000. running mean: -3.512669\n",
      "ep 2004: ep_len:1373 episode reward: total was -77.340000. running mean: -4.250942\n",
      "ep 2004: ep_len:600 episode reward: total was 35.650000. running mean: -3.851933\n",
      "ep 2004: ep_len:1281 episode reward: total was -23.040000. running mean: -4.043814\n",
      "ep 2004: ep_len:734 episode reward: total was 34.310000. running mean: -3.660276\n",
      "ep 2004: ep_len:655 episode reward: total was -7.710000. running mean: -3.700773\n",
      "ep 2004: ep_len:186 episode reward: total was 85.500000. running mean: -2.808765\n",
      "ep 2004: ep_len:77 episode reward: total was 35.500000. running mean: -2.425677\n",
      "ep 2004: ep_len:1435 episode reward: total was 28.780000. running mean: -2.113621\n",
      "ep 2004: ep_len:2793 episode reward: total was -8.480000. running mean: -2.177284\n",
      "epsilon:0.009992 episode_count: 30189. steps_count: 32283916.000000\n",
      "ep 2005: ep_len:704 episode reward: total was -15.470000. running mean: -2.310212\n",
      "ep 2005: ep_len:1849 episode reward: total was -186.710000. running mean: -4.154210\n",
      "ep 2005: ep_len:80 episode reward: total was 37.000000. running mean: -3.742667\n",
      "ep 2005: ep_len:3019 episode reward: total was -52.840000. running mean: -4.233641\n",
      "ep 2005: ep_len:1218 episode reward: total was -22.330000. running mean: -4.414604\n",
      "ep 2005: ep_len:55 episode reward: total was 24.010000. running mean: -4.130358\n",
      "ep 2005: ep_len:150 episode reward: total was 72.000000. running mean: -3.369055\n",
      "ep 2005: ep_len:75 episode reward: total was 36.000000. running mean: -2.975364\n",
      "ep 2005: ep_len:1443 episode reward: total was -86.250000. running mean: -3.808111\n",
      "ep 2005: ep_len:317 episode reward: total was 23.430000. running mean: -3.535729\n",
      "ep 2005: ep_len:513 episode reward: total was -28.370000. running mean: -3.784072\n",
      "ep 2005: ep_len:829 episode reward: total was 43.750000. running mean: -3.308731\n",
      "ep 2005: ep_len:1144 episode reward: total was -4.890000. running mean: -3.324544\n",
      "ep 2005: ep_len:47 episode reward: total was 22.000000. running mean: -3.071299\n",
      "ep 2005: ep_len:195 episode reward: total was 91.500000. running mean: -2.125586\n",
      "ep 2005: ep_len:40 episode reward: total was 17.000000. running mean: -1.934330\n",
      "ep 2005: ep_len:68 episode reward: total was 28.000000. running mean: -1.634987\n",
      "ep 2005: ep_len:619 episode reward: total was -15.190000. running mean: -1.770537\n",
      "ep 2005: ep_len:2914 episode reward: total was -11.920000. running mean: -1.872031\n",
      "ep 2005: ep_len:65 episode reward: total was 31.000000. running mean: -1.543311\n",
      "epsilon:0.009992 episode_count: 30209. steps_count: 32299260.000000\n",
      "ep 2006: ep_len:1159 episode reward: total was 7.870000. running mean: -1.449178\n",
      "ep 2006: ep_len:839 episode reward: total was -32.660000. running mean: -1.761286\n",
      "ep 2006: ep_len:42 episode reward: total was 18.000000. running mean: -1.563673\n",
      "ep 2006: ep_len:78 episode reward: total was 37.500000. running mean: -1.173036\n",
      "ep 2006: ep_len:896 episode reward: total was 6.310000. running mean: -1.098206\n",
      "ep 2006: ep_len:144 episode reward: total was 69.000000. running mean: -0.397224\n",
      "ep 2006: ep_len:65 episode reward: total was 28.000000. running mean: -0.113252\n",
      "ep 2006: ep_len:1029 episode reward: total was -5.270000. running mean: -0.164819\n",
      "ep 2006: ep_len:332 episode reward: total was 7.600000. running mean: -0.087171\n",
      "ep 2006: ep_len:599 episode reward: total was -47.710000. running mean: -0.563399\n",
      "ep 2006: ep_len:732 episode reward: total was -6.520000. running mean: -0.622965\n",
      "ep 2006: ep_len:712 episode reward: total was -16.290000. running mean: -0.779636\n",
      "ep 2006: ep_len:134 episode reward: total was 65.500000. running mean: -0.116839\n",
      "ep 2006: ep_len:26 episode reward: total was 10.000000. running mean: -0.015671\n",
      "ep 2006: ep_len:1075 episode reward: total was 33.880000. running mean: 0.323286\n",
      "ep 2006: ep_len:2890 episode reward: total was -11.090000. running mean: 0.209153\n",
      "ep 2006: ep_len:59 episode reward: total was 26.500000. running mean: 0.472061\n",
      "epsilon:0.009992 episode_count: 30226. steps_count: 32310071.000000\n",
      "ep 2007: ep_len:693 episode reward: total was -12.430000. running mean: 0.343041\n",
      "ep 2007: ep_len:747 episode reward: total was -38.150000. running mean: -0.041890\n",
      "ep 2007: ep_len:68 episode reward: total was 31.000000. running mean: 0.268529\n",
      "ep 2007: ep_len:2961 episode reward: total was -3.390000. running mean: 0.231944\n",
      "ep 2007: ep_len:1195 episode reward: total was -16.560000. running mean: 0.064024\n",
      "ep 2007: ep_len:48 episode reward: total was 22.500000. running mean: 0.288384\n",
      "ep 2007: ep_len:500 episode reward: total was 32.430000. running mean: 0.609800\n",
      "ep 2007: ep_len:3764 episode reward: total was -39.560000. running mean: 0.208102\n",
      "ep 2007: ep_len:658 episode reward: total was -45.100000. running mean: -0.244979\n",
      "ep 2007: ep_len:773 episode reward: total was 18.530000. running mean: -0.057229\n",
      "ep 2007: ep_len:716 episode reward: total was -8.160000. running mean: -0.138257\n",
      "ep 2007: ep_len:199 episode reward: total was 93.500000. running mean: 0.798126\n",
      "ep 2007: ep_len:36 episode reward: total was 16.500000. running mean: 0.955145\n",
      "ep 2007: ep_len:648 episode reward: total was -8.590000. running mean: 0.859693\n",
      "ep 2007: ep_len:2888 episode reward: total was -0.390000. running mean: 0.847196\n",
      "epsilon:0.009992 episode_count: 30241. steps_count: 32325965.000000\n",
      "ep 2008: ep_len:955 episode reward: total was -54.990000. running mean: 0.288824\n",
      "ep 2008: ep_len:710 episode reward: total was -25.880000. running mean: 0.027136\n",
      "ep 2008: ep_len:79 episode reward: total was 38.000000. running mean: 0.406865\n",
      "ep 2008: ep_len:2918 episode reward: total was -25.490000. running mean: 0.147896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2008: ep_len:797 episode reward: total was -18.460000. running mean: -0.038183\n",
      "ep 2008: ep_len:48 episode reward: total was 22.500000. running mean: 0.187199\n",
      "ep 2008: ep_len:1367 episode reward: total was -274.260000. running mean: -2.557273\n",
      "ep 2008: ep_len:620 episode reward: total was 20.850000. running mean: -2.323200\n",
      "ep 2008: ep_len:576 episode reward: total was 10.370000. running mean: -2.196268\n",
      "ep 2008: ep_len:608 episode reward: total was -11.230000. running mean: -2.286606\n",
      "ep 2008: ep_len:678 episode reward: total was 4.590000. running mean: -2.217840\n",
      "ep 2008: ep_len:76 episode reward: total was 29.000000. running mean: -1.905661\n",
      "ep 2008: ep_len:138 episode reward: total was 64.500000. running mean: -1.241605\n",
      "ep 2008: ep_len:79 episode reward: total was 38.000000. running mean: -0.849188\n",
      "ep 2008: ep_len:1003 episode reward: total was -51.540000. running mean: -1.356097\n",
      "ep 2008: ep_len:2819 episode reward: total was -9.720000. running mean: -1.439736\n",
      "epsilon:0.009992 episode_count: 30257. steps_count: 32339436.000000\n",
      "ep 2009: ep_len:1100 episode reward: total was 8.900000. running mean: -1.336338\n",
      "ep 2009: ep_len:775 episode reward: total was -6.380000. running mean: -1.386775\n",
      "ep 2009: ep_len:2998 episode reward: total was -0.320000. running mean: -1.376107\n",
      "ep 2009: ep_len:948 episode reward: total was 65.380000. running mean: -0.708546\n",
      "ep 2009: ep_len:33 episode reward: total was 15.000000. running mean: -0.551461\n",
      "ep 2009: ep_len:66 episode reward: total was 31.500000. running mean: -0.230946\n",
      "ep 2009: ep_len:500 episode reward: total was 21.830000. running mean: -0.010337\n",
      "ep 2009: ep_len:3790 episode reward: total was -121.330000. running mean: -1.223533\n",
      "ep 2009: ep_len:509 episode reward: total was -5.180000. running mean: -1.263098\n",
      "ep 2009: ep_len:713 episode reward: total was 50.910000. running mean: -0.741367\n",
      "ep 2009: ep_len:604 episode reward: total was -1.670000. running mean: -0.750653\n",
      "ep 2009: ep_len:184 episode reward: total was 87.010000. running mean: 0.126953\n",
      "ep 2009: ep_len:630 episode reward: total was 32.920000. running mean: 0.454884\n",
      "ep 2009: ep_len:2855 episode reward: total was 6.070000. running mean: 0.511035\n",
      "ep 2009: ep_len:56 episode reward: total was 22.000000. running mean: 0.725925\n",
      "epsilon:0.009992 episode_count: 30272. steps_count: 32355197.000000\n",
      "ep 2010: ep_len:1419 episode reward: total was 31.620000. running mean: 1.034865\n",
      "ep 2010: ep_len:758 episode reward: total was -26.610000. running mean: 0.758417\n",
      "ep 2010: ep_len:56 episode reward: total was 26.500000. running mean: 1.015833\n",
      "ep 2010: ep_len:3056 episode reward: total was 5.280000. running mean: 1.058474\n",
      "ep 2010: ep_len:514 episode reward: total was -14.220000. running mean: 0.905689\n",
      "ep 2010: ep_len:142 episode reward: total was 68.000000. running mean: 1.576633\n",
      "ep 2010: ep_len:102 episode reward: total was 48.000000. running mean: 2.040866\n",
      "ep 2010: ep_len:1414 episode reward: total was -227.910000. running mean: -0.258642\n",
      "ep 2010: ep_len:601 episode reward: total was 20.200000. running mean: -0.054056\n",
      "ep 2010: ep_len:1583 episode reward: total was -17.030000. running mean: -0.223815\n",
      "ep 2010: ep_len:767 episode reward: total was 0.460000. running mean: -0.216977\n",
      "ep 2010: ep_len:500 episode reward: total was 12.460000. running mean: -0.090207\n",
      "ep 2010: ep_len:48 episode reward: total was 22.500000. running mean: 0.135695\n",
      "ep 2010: ep_len:102 episode reward: total was 46.500000. running mean: 0.599338\n",
      "ep 2010: ep_len:639 episode reward: total was -4.160000. running mean: 0.551744\n",
      "ep 2010: ep_len:2759 episode reward: total was -15.490000. running mean: 0.391327\n",
      "epsilon:0.009992 episode_count: 30288. steps_count: 32369657.000000\n",
      "ep 2011: ep_len:666 episode reward: total was -6.450000. running mean: 0.322914\n",
      "ep 2011: ep_len:732 episode reward: total was -23.290000. running mean: 0.086784\n",
      "ep 2011: ep_len:57 episode reward: total was 24.000000. running mean: 0.325917\n",
      "ep 2011: ep_len:79 episode reward: total was 36.500000. running mean: 0.687657\n",
      "ep 2011: ep_len:531 episode reward: total was -20.110000. running mean: 0.479681\n",
      "ep 2011: ep_len:54 episode reward: total was 25.500000. running mean: 0.729884\n",
      "ep 2011: ep_len:123 episode reward: total was 58.500000. running mean: 1.307585\n",
      "ep 2011: ep_len:103 episode reward: total was 48.500000. running mean: 1.779509\n",
      "ep 2011: ep_len:734 episode reward: total was -14.040000. running mean: 1.621314\n",
      "ep 2011: ep_len:673 episode reward: total was 23.770000. running mean: 1.842801\n",
      "ep 2011: ep_len:1273 episode reward: total was -54.590000. running mean: 1.278473\n",
      "ep 2011: ep_len:7302 episode reward: total was -4546.740000. running mean: -44.201712\n",
      "ep 2011: ep_len:720 episode reward: total was -3.280000. running mean: -43.792495\n",
      "ep 2011: ep_len:76 episode reward: total was 35.000000. running mean: -43.004570\n",
      "ep 2011: ep_len:1498 episode reward: total was 12.240000. running mean: -42.452124\n",
      "ep 2011: ep_len:2785 episode reward: total was -97.410000. running mean: -43.001703\n",
      "epsilon:0.009992 episode_count: 30304. steps_count: 32387063.000000\n",
      "ep 2012: ep_len:997 episode reward: total was -39.880000. running mean: -42.970486\n",
      "ep 2012: ep_len:147 episode reward: total was 3.330000. running mean: -42.507481\n",
      "ep 2012: ep_len:3058 episode reward: total was -11.380000. running mean: -42.196206\n",
      "ep 2012: ep_len:534 episode reward: total was -7.960000. running mean: -41.853844\n",
      "ep 2012: ep_len:78 episode reward: total was 36.000000. running mean: -41.075305\n",
      "ep 2012: ep_len:1510 episode reward: total was -60.710000. running mean: -41.271652\n",
      "ep 2012: ep_len:323 episode reward: total was 13.420000. running mean: -40.724736\n",
      "ep 2012: ep_len:1564 episode reward: total was -41.930000. running mean: -40.736789\n",
      "ep 2012: ep_len:883 episode reward: total was 47.320000. running mean: -39.856221\n",
      "ep 2012: ep_len:980 episode reward: total was -5.730000. running mean: -39.514958\n",
      "ep 2012: ep_len:51 episode reward: total was 24.000000. running mean: -38.879809\n",
      "ep 2012: ep_len:622 episode reward: total was -11.240000. running mean: -38.603411\n",
      "ep 2012: ep_len:2943 episode reward: total was -18.970000. running mean: -38.407077\n",
      "ep 2012: ep_len:40 episode reward: total was 17.000000. running mean: -37.853006\n",
      "epsilon:0.009992 episode_count: 30318. steps_count: 32400793.000000\n",
      "ep 2013: ep_len:660 episode reward: total was -29.410000. running mean: -37.768576\n",
      "ep 2013: ep_len:666 episode reward: total was -11.570000. running mean: -37.506590\n",
      "ep 2013: ep_len:2905 episode reward: total was -5.420000. running mean: -37.185724\n",
      "ep 2013: ep_len:617 episode reward: total was 5.420000. running mean: -36.759667\n",
      "ep 2013: ep_len:46 episode reward: total was 21.500000. running mean: -36.177070\n",
      "ep 2013: ep_len:110 episode reward: total was 52.000000. running mean: -35.295300\n",
      "ep 2013: ep_len:1504 episode reward: total was 15.940000. running mean: -34.782947\n",
      "ep 2013: ep_len:3642 episode reward: total was -164.270000. running mean: -36.077817\n",
      "ep 2013: ep_len:990 episode reward: total was -36.320000. running mean: -36.080239\n",
      "ep 2013: ep_len:852 episode reward: total was 63.760000. running mean: -35.081837\n",
      "ep 2013: ep_len:500 episode reward: total was 36.870000. running mean: -34.362318\n",
      "ep 2013: ep_len:94 episode reward: total was 45.500000. running mean: -33.563695\n",
      "ep 2013: ep_len:120 episode reward: total was 58.500000. running mean: -32.643058\n",
      "ep 2013: ep_len:1124 episode reward: total was 24.540000. running mean: -32.071227\n",
      "ep 2013: ep_len:2851 episode reward: total was -21.030000. running mean: -31.960815\n",
      "ep 2013: ep_len:75 episode reward: total was 34.500000. running mean: -31.296207\n",
      "epsilon:0.009992 episode_count: 30334. steps_count: 32417549.000000\n",
      "ep 2014: ep_len:1011 episode reward: total was -53.690000. running mean: -31.520145\n",
      "ep 2014: ep_len:623 episode reward: total was 4.860000. running mean: -31.156344\n",
      "ep 2014: ep_len:69 episode reward: total was 33.000000. running mean: -30.514780\n",
      "ep 2014: ep_len:3027 episode reward: total was -34.040000. running mean: -30.550032\n",
      "ep 2014: ep_len:759 episode reward: total was 31.230000. running mean: -29.932232\n",
      "ep 2014: ep_len:154 episode reward: total was 71.000000. running mean: -28.922910\n",
      "ep 2014: ep_len:63 episode reward: total was 28.500000. running mean: -28.348681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2014: ep_len:760 episode reward: total was -29.450000. running mean: -28.359694\n",
      "ep 2014: ep_len:500 episode reward: total was 30.660000. running mean: -27.769497\n",
      "ep 2014: ep_len:536 episode reward: total was 8.100000. running mean: -27.410802\n",
      "ep 2014: ep_len:851 episode reward: total was 69.130000. running mean: -26.445394\n",
      "ep 2014: ep_len:500 episode reward: total was 13.440000. running mean: -26.046540\n",
      "ep 2014: ep_len:135 episode reward: total was 66.000000. running mean: -25.126074\n",
      "ep 2014: ep_len:646 episode reward: total was -0.520000. running mean: -24.880014\n",
      "ep 2014: ep_len:2782 episode reward: total was 14.520000. running mean: -24.486014\n",
      "epsilon:0.009992 episode_count: 30349. steps_count: 32429965.000000\n",
      "ep 2015: ep_len:971 episode reward: total was -64.900000. running mean: -24.890153\n",
      "ep 2015: ep_len:1222 episode reward: total was -41.480000. running mean: -25.056052\n",
      "ep 2015: ep_len:62 episode reward: total was -60.990000. running mean: -25.415391\n",
      "ep 2015: ep_len:2962 episode reward: total was -30.140000. running mean: -25.462637\n",
      "ep 2015: ep_len:546 episode reward: total was -19.960000. running mean: -25.407611\n",
      "ep 2015: ep_len:576 episode reward: total was -96.560000. running mean: -26.119135\n",
      "ep 2015: ep_len:3998 episode reward: total was -98.180000. running mean: -26.839744\n",
      "ep 2015: ep_len:4018 episode reward: total was -1031.320000. running mean: -36.884546\n",
      "ep 2015: ep_len:7316 episode reward: total was -79.120000. running mean: -37.306901\n",
      "ep 2015: ep_len:1109 episode reward: total was -10.290000. running mean: -37.036732\n",
      "ep 2015: ep_len:59 episode reward: total was 26.500000. running mean: -36.401364\n",
      "ep 2015: ep_len:632 episode reward: total was -5.080000. running mean: -36.088151\n",
      "ep 2015: ep_len:2896 episode reward: total was -207.380000. running mean: -37.801069\n",
      "epsilon:0.009992 episode_count: 30362. steps_count: 32456332.000000\n",
      "ep 2016: ep_len:1432 episode reward: total was 34.350000. running mean: -37.079559\n",
      "ep 2016: ep_len:204 episode reward: total was -2.280000. running mean: -36.731563\n",
      "ep 2016: ep_len:2835 episode reward: total was -67.160000. running mean: -37.035847\n",
      "ep 2016: ep_len:1427 episode reward: total was 15.110000. running mean: -36.514389\n",
      "ep 2016: ep_len:53 episode reward: total was 22.000000. running mean: -35.929245\n",
      "ep 2016: ep_len:1928 episode reward: total was -63.000000. running mean: -36.199953\n",
      "ep 2016: ep_len:348 episode reward: total was 23.250000. running mean: -35.605453\n",
      "ep 2016: ep_len:772 episode reward: total was -28.580000. running mean: -35.535198\n",
      "ep 2016: ep_len:785 episode reward: total was 11.520000. running mean: -35.064647\n",
      "ep 2016: ep_len:500 episode reward: total was 53.770000. running mean: -34.176300\n",
      "ep 2016: ep_len:170 episode reward: total was 82.000000. running mean: -33.014537\n",
      "ep 2016: ep_len:110 episode reward: total was 52.000000. running mean: -32.164392\n",
      "ep 2016: ep_len:783 episode reward: total was -78.060000. running mean: -32.623348\n",
      "ep 2016: ep_len:2936 episode reward: total was -12.640000. running mean: -32.423514\n",
      "ep 2016: ep_len:45 episode reward: total was 19.500000. running mean: -31.904279\n",
      "epsilon:0.009992 episode_count: 30377. steps_count: 32470660.000000\n",
      "ep 2017: ep_len:5001 episode reward: total was -889.810000. running mean: -40.483336\n",
      "ep 2017: ep_len:191 episode reward: total was 5.180000. running mean: -40.026703\n",
      "ep 2017: ep_len:3039 episode reward: total was -29.670000. running mean: -39.923136\n",
      "ep 2017: ep_len:1235 episode reward: total was -32.260000. running mean: -39.846505\n",
      "ep 2017: ep_len:148 episode reward: total was 69.500000. running mean: -38.753040\n",
      "ep 2017: ep_len:85 episode reward: total was 39.500000. running mean: -37.970509\n",
      "ep 2017: ep_len:64 episode reward: total was 29.000000. running mean: -37.300804\n",
      "ep 2017: ep_len:870 episode reward: total was 45.730000. running mean: -36.470496\n",
      "ep 2017: ep_len:3645 episode reward: total was -78.690000. running mean: -36.892691\n",
      "ep 2017: ep_len:4029 episode reward: total was -717.810000. running mean: -43.701864\n",
      "ep 2017: ep_len:819 episode reward: total was 59.230000. running mean: -42.672546\n",
      "ep 2017: ep_len:500 episode reward: total was 4.440000. running mean: -42.201420\n",
      "ep 2017: ep_len:44 episode reward: total was 20.500000. running mean: -41.574406\n",
      "ep 2017: ep_len:691 episode reward: total was 12.630000. running mean: -41.032362\n",
      "ep 2017: ep_len:2787 episode reward: total was -9.240000. running mean: -40.714438\n",
      "ep 2017: ep_len:71 episode reward: total was 32.500000. running mean: -39.982294\n",
      "epsilon:0.009992 episode_count: 30393. steps_count: 32493879.000000\n",
      "ep 2018: ep_len:820 episode reward: total was -26.310000. running mean: -39.845571\n",
      "ep 2018: ep_len:735 episode reward: total was -36.540000. running mean: -39.812515\n",
      "ep 2018: ep_len:65 episode reward: total was 31.000000. running mean: -39.104390\n",
      "ep 2018: ep_len:3025 episode reward: total was -51.510000. running mean: -39.228446\n",
      "ep 2018: ep_len:812 episode reward: total was 1.040000. running mean: -38.825762\n",
      "ep 2018: ep_len:75 episode reward: total was 34.500000. running mean: -38.092504\n",
      "ep 2018: ep_len:1035 episode reward: total was -16.320000. running mean: -37.874779\n",
      "ep 2018: ep_len:3963 episode reward: total was -2596.010000. running mean: -63.456131\n",
      "ep 2018: ep_len:1267 episode reward: total was -66.280000. running mean: -63.484370\n",
      "ep 2018: ep_len:793 episode reward: total was -17.050000. running mean: -63.020026\n",
      "ep 2018: ep_len:3502 episode reward: total was -1178.040000. running mean: -74.170226\n",
      "ep 2018: ep_len:105 episode reward: total was 51.000000. running mean: -72.918524\n",
      "ep 2018: ep_len:74 episode reward: total was 35.500000. running mean: -71.834338\n",
      "ep 2018: ep_len:1077 episode reward: total was 13.330000. running mean: -70.982695\n",
      "ep 2018: ep_len:2809 episode reward: total was -8.620000. running mean: -70.359068\n",
      "epsilon:0.009992 episode_count: 30408. steps_count: 32514036.000000\n",
      "ep 2019: ep_len:615 episode reward: total was 1.290000. running mean: -69.642577\n",
      "ep 2019: ep_len:682 episode reward: total was -35.150000. running mean: -69.297652\n",
      "ep 2019: ep_len:2929 episode reward: total was -105.720000. running mean: -69.661875\n",
      "ep 2019: ep_len:500 episode reward: total was 24.400000. running mean: -68.721256\n",
      "ep 2019: ep_len:109 episode reward: total was 51.500000. running mean: -67.519044\n",
      "ep 2019: ep_len:945 episode reward: total was 69.420000. running mean: -66.149653\n",
      "ep 2019: ep_len:3789 episode reward: total was -172.480000. running mean: -67.212957\n",
      "ep 2019: ep_len:555 episode reward: total was -27.430000. running mean: -66.815127\n",
      "ep 2019: ep_len:780 episode reward: total was -9.520000. running mean: -66.242176\n",
      "ep 2019: ep_len:990 episode reward: total was -1.160000. running mean: -65.591354\n",
      "ep 2019: ep_len:76 episode reward: total was 32.000000. running mean: -64.615441\n",
      "ep 2019: ep_len:167 episode reward: total was 82.000000. running mean: -63.149286\n",
      "ep 2019: ep_len:1107 episode reward: total was -9.300000. running mean: -62.610793\n",
      "ep 2019: ep_len:2832 episode reward: total was -11.170000. running mean: -62.096385\n",
      "epsilon:0.009992 episode_count: 30422. steps_count: 32530112.000000\n",
      "ep 2020: ep_len:996 episode reward: total was -84.980000. running mean: -62.325222\n",
      "ep 2020: ep_len:747 episode reward: total was -43.380000. running mean: -62.135769\n",
      "ep 2020: ep_len:2913 episode reward: total was -97.690000. running mean: -62.491312\n",
      "ep 2020: ep_len:1168 episode reward: total was -30.850000. running mean: -62.174899\n",
      "ep 2020: ep_len:36 episode reward: total was 16.500000. running mean: -61.388150\n",
      "ep 2020: ep_len:67 episode reward: total was 30.500000. running mean: -60.469268\n",
      "ep 2020: ep_len:3581 episode reward: total was -770.600000. running mean: -67.570575\n",
      "ep 2020: ep_len:4227 episode reward: total was -121.560000. running mean: -68.110470\n",
      "ep 2020: ep_len:1255 episode reward: total was -42.160000. running mean: -67.850965\n",
      "ep 2020: ep_len:717 episode reward: total was -40.440000. running mean: -67.576855\n",
      "ep 2020: ep_len:651 episode reward: total was -11.940000. running mean: -67.020487\n",
      "ep 2020: ep_len:68 episode reward: total was 32.500000. running mean: -66.025282\n",
      "ep 2020: ep_len:67 episode reward: total was 32.000000. running mean: -65.045029\n",
      "ep 2020: ep_len:2068 episode reward: total was -1176.890000. running mean: -76.163479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2020: ep_len:2725 episode reward: total was -21.250000. running mean: -75.614344\n",
      "ep 2020: ep_len:40 episode reward: total was 18.500000. running mean: -74.673201\n",
      "epsilon:0.009992 episode_count: 30438. steps_count: 32551438.000000\n",
      "ep 2021: ep_len:1017 episode reward: total was -71.850000. running mean: -74.644969\n",
      "ep 2021: ep_len:805 episode reward: total was -37.200000. running mean: -74.270519\n",
      "ep 2021: ep_len:2985 episode reward: total was -106.520000. running mean: -74.593014\n",
      "ep 2021: ep_len:630 episode reward: total was -0.610000. running mean: -73.853184\n",
      "ep 2021: ep_len:90 episode reward: total was 42.000000. running mean: -72.694652\n",
      "ep 2021: ep_len:500 episode reward: total was 43.240000. running mean: -71.535305\n",
      "ep 2021: ep_len:3881 episode reward: total was -242.050000. running mean: -73.240452\n",
      "ep 2021: ep_len:601 episode reward: total was -73.950000. running mean: -73.247548\n",
      "ep 2021: ep_len:871 episode reward: total was 28.870000. running mean: -72.226372\n",
      "ep 2021: ep_len:607 episode reward: total was -9.250000. running mean: -71.596608\n",
      "ep 2021: ep_len:66 episode reward: total was 31.500000. running mean: -70.565642\n",
      "ep 2021: ep_len:159 episode reward: total was 76.500000. running mean: -69.094986\n",
      "ep 2021: ep_len:41 episode reward: total was 19.000000. running mean: -68.214036\n",
      "ep 2021: ep_len:733 episode reward: total was -53.440000. running mean: -68.066296\n",
      "ep 2021: ep_len:2825 episode reward: total was -80.260000. running mean: -68.188233\n",
      "epsilon:0.009992 episode_count: 30453. steps_count: 32567249.000000\n",
      "ep 2022: ep_len:1477 episode reward: total was -15.670000. running mean: -67.663050\n",
      "ep 2022: ep_len:1246 episode reward: total was -35.180000. running mean: -67.338220\n",
      "ep 2022: ep_len:2989 episode reward: total was -99.800000. running mean: -67.662838\n",
      "ep 2022: ep_len:500 episode reward: total was 31.330000. running mean: -66.672909\n",
      "ep 2022: ep_len:53 episode reward: total was 25.000000. running mean: -65.756180\n",
      "ep 2022: ep_len:100 episode reward: total was 48.500000. running mean: -64.613618\n",
      "ep 2022: ep_len:61 episode reward: total was 29.000000. running mean: -63.677482\n",
      "ep 2022: ep_len:500 episode reward: total was -12.730000. running mean: -63.168007\n",
      "ep 2022: ep_len:3550 episode reward: total was -91.030000. running mean: -63.446627\n",
      "ep 2022: ep_len:802 episode reward: total was -44.490000. running mean: -63.257061\n",
      "ep 2022: ep_len:7435 episode reward: total was -800.020000. running mean: -70.624690\n",
      "ep 2022: ep_len:996 episode reward: total was 37.500000. running mean: -69.543444\n",
      "ep 2022: ep_len:129 episode reward: total was 63.000000. running mean: -68.218009\n",
      "ep 2022: ep_len:40 episode reward: total was 17.000000. running mean: -67.365829\n",
      "ep 2022: ep_len:627 episode reward: total was 2.250000. running mean: -66.669671\n",
      "ep 2022: ep_len:2858 episode reward: total was -48.380000. running mean: -66.486774\n",
      "epsilon:0.009992 episode_count: 30469. steps_count: 32590612.000000\n",
      "ep 2023: ep_len:1068 episode reward: total was -6.660000. running mean: -65.888506\n",
      "ep 2023: ep_len:894 episode reward: total was -23.180000. running mean: -65.461421\n",
      "ep 2023: ep_len:98 episode reward: total was 47.500000. running mean: -64.331807\n",
      "ep 2023: ep_len:689 episode reward: total was -14.900000. running mean: -63.837489\n",
      "ep 2023: ep_len:718 episode reward: total was -8.630000. running mean: -63.285414\n",
      "ep 2023: ep_len:688 episode reward: total was 6.750000. running mean: -62.585060\n",
      "ep 2023: ep_len:992 episode reward: total was -55.220000. running mean: -62.511409\n",
      "ep 2023: ep_len:799 episode reward: total was 40.600000. running mean: -61.480295\n",
      "ep 2023: ep_len:1110 episode reward: total was -22.210000. running mean: -61.087592\n",
      "ep 2023: ep_len:182 episode reward: total was 89.500000. running mean: -59.581716\n",
      "ep 2023: ep_len:644 episode reward: total was -2.110000. running mean: -59.006999\n",
      "ep 2023: ep_len:2832 episode reward: total was -80.990000. running mean: -59.226829\n",
      "epsilon:0.009992 episode_count: 30481. steps_count: 32601326.000000\n",
      "ep 2024: ep_len:1089 episode reward: total was -14.530000. running mean: -58.779861\n",
      "ep 2024: ep_len:1204 episode reward: total was -40.650000. running mean: -58.598562\n",
      "ep 2024: ep_len:2997 episode reward: total was -127.250000. running mean: -59.285077\n",
      "ep 2024: ep_len:579 episode reward: total was 14.090000. running mean: -58.551326\n",
      "ep 2024: ep_len:137 episode reward: total was 65.500000. running mean: -57.310813\n",
      "ep 2024: ep_len:36 episode reward: total was 15.000000. running mean: -56.587705\n",
      "ep 2024: ep_len:1096 episode reward: total was -21.010000. running mean: -56.231927\n",
      "ep 2024: ep_len:3781 episode reward: total was -12.520000. running mean: -55.794808\n",
      "ep 2024: ep_len:620 episode reward: total was -58.270000. running mean: -55.819560\n",
      "ep 2024: ep_len:9045 episode reward: total was -538.890000. running mean: -60.650265\n",
      "ep 2024: ep_len:641 episode reward: total was 2.200000. running mean: -60.021762\n",
      "ep 2024: ep_len:777 episode reward: total was -62.090000. running mean: -60.042444\n",
      "ep 2024: ep_len:2894 episode reward: total was -28.890000. running mean: -59.730920\n",
      "epsilon:0.009992 episode_count: 30494. steps_count: 32626222.000000\n",
      "ep 2025: ep_len:806 episode reward: total was -85.950000. running mean: -59.993111\n",
      "ep 2025: ep_len:806 episode reward: total was -1.980000. running mean: -59.412980\n",
      "ep 2025: ep_len:51 episode reward: total was 22.500000. running mean: -58.593850\n",
      "ep 2025: ep_len:2911 episode reward: total was -122.710000. running mean: -59.235011\n",
      "ep 2025: ep_len:593 episode reward: total was 11.860000. running mean: -58.524061\n",
      "ep 2025: ep_len:38 episode reward: total was 17.500000. running mean: -57.763820\n",
      "ep 2025: ep_len:1452 episode reward: total was -3.370000. running mean: -57.219882\n",
      "ep 2025: ep_len:689 episode reward: total was -134.460000. running mean: -57.992283\n",
      "ep 2025: ep_len:1583 episode reward: total was -27.250000. running mean: -57.684861\n",
      "ep 2025: ep_len:776 episode reward: total was -9.790000. running mean: -57.205912\n",
      "ep 2025: ep_len:500 episode reward: total was 9.390000. running mean: -56.539953\n",
      "ep 2025: ep_len:1498 episode reward: total was 0.700000. running mean: -55.967553\n",
      "ep 2025: ep_len:2759 episode reward: total was -27.610000. running mean: -55.683978\n",
      "ep 2025: ep_len:48 episode reward: total was 21.000000. running mean: -54.917138\n",
      "epsilon:0.009992 episode_count: 30508. steps_count: 32640732.000000\n",
      "ep 2026: ep_len:1109 episode reward: total was 9.280000. running mean: -54.275167\n",
      "ep 2026: ep_len:1622 episode reward: total was -93.920000. running mean: -54.671615\n",
      "ep 2026: ep_len:3041 episode reward: total was -118.810000. running mean: -55.312999\n",
      "ep 2026: ep_len:500 episode reward: total was 2.240000. running mean: -54.737469\n",
      "ep 2026: ep_len:123 episode reward: total was 58.500000. running mean: -53.605094\n",
      "ep 2026: ep_len:88 episode reward: total was 39.500000. running mean: -52.674043\n",
      "ep 2026: ep_len:656 episode reward: total was -1.690000. running mean: -52.164203\n",
      "ep 2026: ep_len:3803 episode reward: total was -85.600000. running mean: -52.498561\n",
      "ep 2026: ep_len:533 episode reward: total was -5.430000. running mean: -52.027875\n",
      "ep 2026: ep_len:650 episode reward: total was -60.450000. running mean: -52.112096\n",
      "ep 2026: ep_len:780 episode reward: total was 15.910000. running mean: -51.431875\n",
      "ep 2026: ep_len:135 episode reward: total was 64.500000. running mean: -50.272557\n",
      "ep 2026: ep_len:95 episode reward: total was 30.510000. running mean: -49.464731\n",
      "ep 2026: ep_len:646 episode reward: total was 0.060000. running mean: -48.969484\n",
      "ep 2026: ep_len:2700 episode reward: total was -14.000000. running mean: -48.619789\n",
      "epsilon:0.009992 episode_count: 30523. steps_count: 32657213.000000\n",
      "ep 2027: ep_len:943 episode reward: total was -86.380000. running mean: -48.997391\n",
      "ep 2027: ep_len:727 episode reward: total was -19.090000. running mean: -48.698317\n",
      "ep 2027: ep_len:2970 episode reward: total was -108.920000. running mean: -49.300534\n",
      "ep 2027: ep_len:810 episode reward: total was -26.370000. running mean: -49.071229\n",
      "ep 2027: ep_len:44 episode reward: total was 20.500000. running mean: -48.375516\n",
      "ep 2027: ep_len:674 episode reward: total was -22.110000. running mean: -48.112861\n",
      "ep 2027: ep_len:3643 episode reward: total was -193.640000. running mean: -49.568133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2027: ep_len:968 episode reward: total was -29.320000. running mean: -49.365651\n",
      "ep 2027: ep_len:660 episode reward: total was -31.850000. running mean: -49.190495\n",
      "ep 2027: ep_len:590 episode reward: total was 28.650000. running mean: -48.412090\n",
      "ep 2027: ep_len:88 episode reward: total was 42.500000. running mean: -47.502969\n",
      "ep 2027: ep_len:127 episode reward: total was 62.000000. running mean: -46.407939\n",
      "ep 2027: ep_len:65 episode reward: total was 31.000000. running mean: -45.633860\n",
      "ep 2027: ep_len:93 episode reward: total was 42.000000. running mean: -44.757521\n",
      "ep 2027: ep_len:614 episode reward: total was -23.420000. running mean: -44.544146\n",
      "ep 2027: ep_len:2734 episode reward: total was -9.430000. running mean: -44.193005\n",
      "ep 2027: ep_len:67 episode reward: total was 30.500000. running mean: -43.446075\n",
      "epsilon:0.009992 episode_count: 30540. steps_count: 32673030.000000\n",
      "ep 2028: ep_len:831 episode reward: total was 22.160000. running mean: -42.790014\n",
      "ep 2028: ep_len:773 episode reward: total was 3.320000. running mean: -42.328914\n",
      "ep 2028: ep_len:41 episode reward: total was 19.000000. running mean: -41.715625\n",
      "ep 2028: ep_len:3034 episode reward: total was -61.660000. running mean: -41.915068\n",
      "ep 2028: ep_len:665 episode reward: total was 12.920000. running mean: -41.366718\n",
      "ep 2028: ep_len:80 episode reward: total was 38.500000. running mean: -40.568050\n",
      "ep 2028: ep_len:50 episode reward: total was 23.500000. running mean: -39.927370\n",
      "ep 2028: ep_len:705 episode reward: total was 24.830000. running mean: -39.279796\n",
      "ep 2028: ep_len:331 episode reward: total was 14.910000. running mean: -38.737898\n",
      "ep 2028: ep_len:584 episode reward: total was -45.870000. running mean: -38.809219\n",
      "ep 2028: ep_len:773 episode reward: total was 29.720000. running mean: -38.123927\n",
      "ep 2028: ep_len:966 episode reward: total was -31.430000. running mean: -38.056988\n",
      "ep 2028: ep_len:84 episode reward: total was 40.500000. running mean: -37.271418\n",
      "ep 2028: ep_len:901 episode reward: total was -163.870000. running mean: -38.537404\n",
      "ep 2028: ep_len:2881 episode reward: total was 1.160000. running mean: -38.140430\n",
      "ep 2028: ep_len:52 episode reward: total was 24.500000. running mean: -37.514025\n",
      "epsilon:0.009992 episode_count: 30556. steps_count: 32685781.000000\n",
      "ep 2029: ep_len:847 episode reward: total was 21.370000. running mean: -36.925185\n",
      "ep 2029: ep_len:1675 episode reward: total was -55.160000. running mean: -37.107533\n",
      "ep 2029: ep_len:2852 episode reward: total was -43.750000. running mean: -37.173958\n",
      "ep 2029: ep_len:824 episode reward: total was -289.360000. running mean: -39.695818\n",
      "ep 2029: ep_len:83 episode reward: total was 40.000000. running mean: -38.898860\n",
      "ep 2029: ep_len:104 episode reward: total was 49.000000. running mean: -38.019872\n",
      "ep 2029: ep_len:1442 episode reward: total was 22.330000. running mean: -37.416373\n",
      "ep 2029: ep_len:4120 episode reward: total was -152.030000. running mean: -38.562509\n",
      "ep 2029: ep_len:699 episode reward: total was -32.460000. running mean: -38.501484\n",
      "ep 2029: ep_len:904 episode reward: total was 36.920000. running mean: -37.747269\n",
      "ep 2029: ep_len:621 episode reward: total was -5.790000. running mean: -37.427697\n",
      "ep 2029: ep_len:137 episode reward: total was 62.500000. running mean: -36.428420\n",
      "ep 2029: ep_len:597 episode reward: total was -14.890000. running mean: -36.213035\n",
      "ep 2029: ep_len:37 episode reward: total was 17.000000. running mean: -35.680905\n",
      "epsilon:0.009992 episode_count: 30570. steps_count: 32700723.000000\n",
      "ep 2030: ep_len:613 episode reward: total was -84.000000. running mean: -36.164096\n",
      "ep 2030: ep_len:187 episode reward: total was 2.600000. running mean: -35.776455\n",
      "ep 2030: ep_len:2971 episode reward: total was -83.810000. running mean: -36.256790\n",
      "ep 2030: ep_len:891 episode reward: total was 13.300000. running mean: -35.761223\n",
      "ep 2030: ep_len:106 episode reward: total was 48.500000. running mean: -34.918610\n",
      "ep 2030: ep_len:1439 episode reward: total was -120.430000. running mean: -35.773724\n",
      "ep 2030: ep_len:4003 episode reward: total was -169.320000. running mean: -37.109187\n",
      "ep 2030: ep_len:1237 episode reward: total was -26.180000. running mean: -36.999895\n",
      "ep 2030: ep_len:927 episode reward: total was 72.010000. running mean: -35.909796\n",
      "ep 2030: ep_len:500 episode reward: total was 22.510000. running mean: -35.325598\n",
      "ep 2030: ep_len:618 episode reward: total was -14.760000. running mean: -35.119942\n",
      "ep 2030: ep_len:2811 episode reward: total was 8.020000. running mean: -34.688543\n",
      "epsilon:0.009992 episode_count: 30582. steps_count: 32717026.000000\n",
      "ep 2031: ep_len:621 episode reward: total was -25.270000. running mean: -34.594357\n",
      "ep 2031: ep_len:845 episode reward: total was 14.480000. running mean: -34.103614\n",
      "ep 2031: ep_len:2941 episode reward: total was -41.450000. running mean: -34.177078\n",
      "ep 2031: ep_len:545 episode reward: total was -9.890000. running mean: -33.934207\n",
      "ep 2031: ep_len:44 episode reward: total was 20.500000. running mean: -33.389865\n",
      "ep 2031: ep_len:113 episode reward: total was 55.000000. running mean: -32.505966\n",
      "ep 2031: ep_len:107 episode reward: total was 52.000000. running mean: -31.660907\n",
      "ep 2031: ep_len:850 episode reward: total was 27.860000. running mean: -31.065697\n",
      "ep 2031: ep_len:4203 episode reward: total was -1297.130000. running mean: -43.726340\n",
      "ep 2031: ep_len:536 episode reward: total was 10.000000. running mean: -43.189077\n",
      "ep 2031: ep_len:715 episode reward: total was 43.890000. running mean: -42.318286\n",
      "ep 2031: ep_len:1102 episode reward: total was -3.440000. running mean: -41.929503\n",
      "ep 2031: ep_len:59 episode reward: total was 28.000000. running mean: -41.230208\n",
      "ep 2031: ep_len:67 episode reward: total was 30.500000. running mean: -40.512906\n",
      "ep 2031: ep_len:73 episode reward: total was 33.500000. running mean: -39.772777\n",
      "ep 2031: ep_len:1435 episode reward: total was 20.060000. running mean: -39.174449\n",
      "ep 2031: ep_len:2800 episode reward: total was -23.650000. running mean: -39.019205\n",
      "epsilon:0.009992 episode_count: 30599. steps_count: 32734082.000000\n",
      "ep 2032: ep_len:1085 episode reward: total was -1.560000. running mean: -38.644613\n",
      "ep 2032: ep_len:500 episode reward: total was -9.090000. running mean: -38.349067\n",
      "ep 2032: ep_len:53 episode reward: total was 25.000000. running mean: -37.715576\n",
      "ep 2032: ep_len:3099 episode reward: total was -25.020000. running mean: -37.588620\n",
      "ep 2032: ep_len:500 episode reward: total was 9.060000. running mean: -37.122134\n",
      "ep 2032: ep_len:62 episode reward: total was 29.500000. running mean: -36.455913\n",
      "ep 2032: ep_len:106 episode reward: total was 51.500000. running mean: -35.576354\n",
      "ep 2032: ep_len:31 episode reward: total was 12.500000. running mean: -35.095590\n",
      "ep 2032: ep_len:858 episode reward: total was 28.700000. running mean: -34.457634\n",
      "ep 2032: ep_len:3725 episode reward: total was -6.840000. running mean: -34.181458\n",
      "ep 2032: ep_len:659 episode reward: total was -4.690000. running mean: -33.886543\n",
      "ep 2032: ep_len:856 episode reward: total was 20.720000. running mean: -33.340478\n",
      "ep 2032: ep_len:610 episode reward: total was -1.140000. running mean: -33.018473\n",
      "ep 2032: ep_len:46 episode reward: total was 21.500000. running mean: -32.473288\n",
      "ep 2032: ep_len:57 episode reward: total was 27.000000. running mean: -31.878556\n",
      "ep 2032: ep_len:1368 episode reward: total was 24.040000. running mean: -31.319370\n",
      "ep 2032: ep_len:2831 episode reward: total was -7.580000. running mean: -31.081976\n",
      "epsilon:0.009992 episode_count: 30616. steps_count: 32750528.000000\n",
      "ep 2033: ep_len:1148 episode reward: total was -0.810000. running mean: -30.779256\n",
      "ep 2033: ep_len:715 episode reward: total was -41.790000. running mean: -30.889364\n",
      "ep 2033: ep_len:35 episode reward: total was 16.000000. running mean: -30.420470\n",
      "ep 2033: ep_len:3029 episode reward: total was -13.940000. running mean: -30.255666\n",
      "ep 2033: ep_len:1188 episode reward: total was -26.670000. running mean: -30.219809\n",
      "ep 2033: ep_len:163 episode reward: total was 77.000000. running mean: -29.147611\n",
      "ep 2033: ep_len:79 episode reward: total was 35.000000. running mean: -28.506135\n",
      "ep 2033: ep_len:777 episode reward: total was -45.440000. running mean: -28.675473\n",
      "ep 2033: ep_len:328 episode reward: total was 2.480000. running mean: -28.363919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2033: ep_len:642 episode reward: total was -14.980000. running mean: -28.230079\n",
      "ep 2033: ep_len:746 episode reward: total was 12.110000. running mean: -27.826679\n",
      "ep 2033: ep_len:1188 episode reward: total was 1.520000. running mean: -27.533212\n",
      "ep 2033: ep_len:47 episode reward: total was 20.010000. running mean: -27.057780\n",
      "ep 2033: ep_len:1167 episode reward: total was -11.730000. running mean: -26.904502\n",
      "ep 2033: ep_len:2889 episode reward: total was 20.220000. running mean: -26.433257\n",
      "epsilon:0.009992 episode_count: 30631. steps_count: 32764669.000000\n",
      "ep 2034: ep_len:919 episode reward: total was -55.210000. running mean: -26.721024\n",
      "ep 2034: ep_len:188 episode reward: total was 0.680000. running mean: -26.447014\n",
      "ep 2034: ep_len:100 episode reward: total was 47.000000. running mean: -25.712544\n",
      "ep 2034: ep_len:1163 episode reward: total was -7.150000. running mean: -25.526919\n",
      "ep 2034: ep_len:954 episode reward: total was -13.610000. running mean: -25.407749\n",
      "ep 2034: ep_len:3972 episode reward: total was -188.330000. running mean: -27.036972\n",
      "ep 2034: ep_len:582 episode reward: total was -41.820000. running mean: -27.184802\n",
      "ep 2034: ep_len:656 episode reward: total was 3.970000. running mean: -26.873254\n",
      "ep 2034: ep_len:775 episode reward: total was 29.850000. running mean: -26.306022\n",
      "ep 2034: ep_len:52 episode reward: total was 23.000000. running mean: -25.812961\n",
      "ep 2034: ep_len:82 episode reward: total was 39.500000. running mean: -25.159832\n",
      "ep 2034: ep_len:1497 episode reward: total was 15.900000. running mean: -24.749233\n",
      "ep 2034: ep_len:2802 episode reward: total was -9.060000. running mean: -24.592341\n",
      "ep 2034: ep_len:49 episode reward: total was 23.000000. running mean: -24.116418\n",
      "epsilon:0.009992 episode_count: 30645. steps_count: 32778460.000000\n",
      "ep 2035: ep_len:986 episode reward: total was -113.600000. running mean: -25.011254\n",
      "ep 2035: ep_len:714 episode reward: total was -15.820000. running mean: -24.919341\n",
      "ep 2035: ep_len:58 episode reward: total was 26.000000. running mean: -24.410148\n",
      "ep 2035: ep_len:3083 episode reward: total was -91.670000. running mean: -25.082746\n",
      "ep 2035: ep_len:500 episode reward: total was 10.780000. running mean: -24.724119\n",
      "ep 2035: ep_len:80 episode reward: total was 38.500000. running mean: -24.091877\n",
      "ep 2035: ep_len:1456 episode reward: total was -118.930000. running mean: -25.040259\n",
      "ep 2035: ep_len:3967 episode reward: total was -161.540000. running mean: -26.405256\n",
      "ep 2035: ep_len:604 episode reward: total was 28.030000. running mean: -25.860904\n",
      "ep 2035: ep_len:7215 episode reward: total was -2960.540000. running mean: -55.207694\n",
      "ep 2035: ep_len:1464 episode reward: total was -19.760000. running mean: -54.853218\n",
      "ep 2035: ep_len:93 episode reward: total was 45.000000. running mean: -53.854685\n",
      "ep 2035: ep_len:1531 episode reward: total was 6.690000. running mean: -53.249239\n",
      "ep 2035: ep_len:2821 episode reward: total was -19.180000. running mean: -52.908546\n",
      "epsilon:0.009992 episode_count: 30659. steps_count: 32803032.000000\n",
      "ep 2036: ep_len:1103 episode reward: total was -1.720000. running mean: -52.396661\n",
      "ep 2036: ep_len:755 episode reward: total was -82.360000. running mean: -52.696294\n",
      "ep 2036: ep_len:83 episode reward: total was 38.500000. running mean: -51.784331\n",
      "ep 2036: ep_len:2943 episode reward: total was -30.300000. running mean: -51.569488\n",
      "ep 2036: ep_len:500 episode reward: total was -18.920000. running mean: -51.242993\n",
      "ep 2036: ep_len:133 episode reward: total was 65.000000. running mean: -50.080563\n",
      "ep 2036: ep_len:75 episode reward: total was 33.000000. running mean: -49.249757\n",
      "ep 2036: ep_len:36 episode reward: total was 15.000000. running mean: -48.607260\n",
      "ep 2036: ep_len:719 episode reward: total was 23.920000. running mean: -47.881987\n",
      "ep 2036: ep_len:3819 episode reward: total was -126.720000. running mean: -48.670367\n",
      "ep 2036: ep_len:851 episode reward: total was -17.920000. running mean: -48.362864\n",
      "ep 2036: ep_len:7339 episode reward: total was -333.070000. running mean: -51.209935\n",
      "ep 2036: ep_len:1453 episode reward: total was -0.950000. running mean: -50.707336\n",
      "ep 2036: ep_len:957 episode reward: total was -69.380000. running mean: -50.894062\n",
      "ep 2036: ep_len:2796 episode reward: total was -49.870000. running mean: -50.883822\n",
      "epsilon:0.009992 episode_count: 30674. steps_count: 32826594.000000\n",
      "ep 2037: ep_len:781 episode reward: total was -93.360000. running mean: -51.308583\n",
      "ep 2037: ep_len:735 episode reward: total was -25.310000. running mean: -51.048598\n",
      "ep 2037: ep_len:46 episode reward: total was 21.500000. running mean: -50.323112\n",
      "ep 2037: ep_len:2979 episode reward: total was -108.110000. running mean: -50.900981\n",
      "ep 2037: ep_len:1661 episode reward: total was -45.840000. running mean: -50.850371\n",
      "ep 2037: ep_len:150 episode reward: total was 72.000000. running mean: -49.621867\n",
      "ep 2037: ep_len:66 episode reward: total was 31.500000. running mean: -48.810648\n",
      "ep 2037: ep_len:1443 episode reward: total was -239.250000. running mean: -50.715042\n",
      "ep 2037: ep_len:650 episode reward: total was 31.590000. running mean: -49.891991\n",
      "ep 2037: ep_len:762 episode reward: total was -22.460000. running mean: -49.617672\n",
      "ep 2037: ep_len:788 episode reward: total was 18.590000. running mean: -48.935595\n",
      "ep 2037: ep_len:500 episode reward: total was 12.220000. running mean: -48.324039\n",
      "ep 2037: ep_len:955 episode reward: total was -59.300000. running mean: -48.433798\n",
      "ep 2037: ep_len:2831 episode reward: total was -34.660000. running mean: -48.296061\n",
      "ep 2037: ep_len:62 episode reward: total was 26.500000. running mean: -47.548100\n",
      "epsilon:0.009992 episode_count: 30689. steps_count: 32841003.000000\n",
      "ep 2038: ep_len:500 episode reward: total was 1.710000. running mean: -47.055519\n",
      "ep 2038: ep_len:1588 episode reward: total was -34.210000. running mean: -46.927064\n",
      "ep 2038: ep_len:2889 episode reward: total was -25.300000. running mean: -46.710793\n",
      "ep 2038: ep_len:617 episode reward: total was 12.510000. running mean: -46.118585\n",
      "ep 2038: ep_len:28 episode reward: total was 12.500000. running mean: -45.532399\n",
      "ep 2038: ep_len:143 episode reward: total was 68.500000. running mean: -44.392075\n",
      "ep 2038: ep_len:1111 episode reward: total was -5.220000. running mean: -44.000355\n",
      "ep 2038: ep_len:664 episode reward: total was 22.180000. running mean: -43.338551\n",
      "ep 2038: ep_len:853 episode reward: total was 21.520000. running mean: -42.689965\n",
      "ep 2038: ep_len:792 episode reward: total was 39.850000. running mean: -41.864566\n",
      "ep 2038: ep_len:705 episode reward: total was -18.410000. running mean: -41.630020\n",
      "ep 2038: ep_len:1088 episode reward: total was 10.190000. running mean: -41.111820\n",
      "ep 2038: ep_len:2844 episode reward: total was -11.020000. running mean: -40.810902\n",
      "ep 2038: ep_len:62 episode reward: total was 28.000000. running mean: -40.122793\n",
      "epsilon:0.009992 episode_count: 30703. steps_count: 32854887.000000\n",
      "ep 2039: ep_len:862 episode reward: total was 7.500000. running mean: -39.646565\n",
      "ep 2039: ep_len:778 episode reward: total was -18.250000. running mean: -39.432599\n",
      "ep 2039: ep_len:2970 episode reward: total was -5.220000. running mean: -39.090473\n",
      "ep 2039: ep_len:801 episode reward: total was -13.000000. running mean: -38.829568\n",
      "ep 2039: ep_len:65 episode reward: total was 29.500000. running mean: -38.146273\n",
      "ep 2039: ep_len:1009 episode reward: total was -21.050000. running mean: -37.975310\n",
      "ep 2039: ep_len:3724 episode reward: total was -80.720000. running mean: -38.402757\n",
      "ep 2039: ep_len:706 episode reward: total was -13.580000. running mean: -38.154529\n",
      "ep 2039: ep_len:725 episode reward: total was -97.810000. running mean: -38.751084\n",
      "ep 2039: ep_len:707 episode reward: total was 14.900000. running mean: -38.214573\n",
      "ep 2039: ep_len:68 episode reward: total was 32.500000. running mean: -37.507428\n",
      "ep 2039: ep_len:117 episode reward: total was 54.000000. running mean: -36.592353\n",
      "ep 2039: ep_len:105 episode reward: total was 49.500000. running mean: -35.731430\n",
      "ep 2039: ep_len:500 episode reward: total was 37.540000. running mean: -34.998715\n",
      "ep 2039: ep_len:2821 episode reward: total was -20.710000. running mean: -34.855828\n",
      "epsilon:0.009992 episode_count: 30718. steps_count: 32870845.000000\n",
      "ep 2040: ep_len:1379 episode reward: total was 23.530000. running mean: -34.271970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2040: ep_len:1575 episode reward: total was -16.860000. running mean: -34.097850\n",
      "ep 2040: ep_len:2942 episode reward: total was -18.740000. running mean: -33.944272\n",
      "ep 2040: ep_len:546 episode reward: total was -1.230000. running mean: -33.617129\n",
      "ep 2040: ep_len:69 episode reward: total was 33.000000. running mean: -32.950958\n",
      "ep 2040: ep_len:132 episode reward: total was 64.500000. running mean: -31.976448\n",
      "ep 2040: ep_len:500 episode reward: total was 21.710000. running mean: -31.439584\n",
      "ep 2040: ep_len:4153 episode reward: total was -86.330000. running mean: -31.988488\n",
      "ep 2040: ep_len:686 episode reward: total was -1.840000. running mean: -31.687003\n",
      "ep 2040: ep_len:7301 episode reward: total was -60.960000. running mean: -31.979733\n",
      "ep 2040: ep_len:1447 episode reward: total was 8.550000. running mean: -31.574436\n",
      "ep 2040: ep_len:785 episode reward: total was -17.100000. running mean: -31.429691\n",
      "ep 2040: ep_len:2751 episode reward: total was -28.640000. running mean: -31.401794\n",
      "ep 2040: ep_len:72 episode reward: total was 34.500000. running mean: -30.742776\n",
      "epsilon:0.009992 episode_count: 30732. steps_count: 32895183.000000\n",
      "ep 2041: ep_len:1152 episode reward: total was 15.490000. running mean: -30.280449\n",
      "ep 2041: ep_len:798 episode reward: total was -1.270000. running mean: -29.990344\n",
      "ep 2041: ep_len:60 episode reward: total was 28.500000. running mean: -29.405441\n",
      "ep 2041: ep_len:3008 episode reward: total was 5.450000. running mean: -29.056886\n",
      "ep 2041: ep_len:689 episode reward: total was -9.440000. running mean: -28.860717\n",
      "ep 2041: ep_len:500 episode reward: total was 33.930000. running mean: -28.232810\n",
      "ep 2041: ep_len:613 episode reward: total was 27.700000. running mean: -27.673482\n",
      "ep 2041: ep_len:500 episode reward: total was 6.060000. running mean: -27.336147\n",
      "ep 2041: ep_len:921 episode reward: total was 73.600000. running mean: -26.326786\n",
      "ep 2041: ep_len:1073 episode reward: total was 10.350000. running mean: -25.960018\n",
      "ep 2041: ep_len:117 episode reward: total was 57.000000. running mean: -25.130418\n",
      "ep 2041: ep_len:58 episode reward: total was 27.500000. running mean: -24.604114\n",
      "ep 2041: ep_len:93 episode reward: total was 43.500000. running mean: -23.923073\n",
      "ep 2041: ep_len:1076 episode reward: total was -7.610000. running mean: -23.759942\n",
      "ep 2041: ep_len:2892 episode reward: total was -24.940000. running mean: -23.771742\n",
      "ep 2041: ep_len:68 episode reward: total was 28.000000. running mean: -23.254025\n",
      "epsilon:0.009992 episode_count: 30748. steps_count: 32908801.000000\n",
      "ep 2042: ep_len:1096 episode reward: total was 12.420000. running mean: -22.897285\n",
      "ep 2042: ep_len:788 episode reward: total was -21.980000. running mean: -22.888112\n",
      "ep 2042: ep_len:2979 episode reward: total was 11.850000. running mean: -22.540731\n",
      "ep 2042: ep_len:798 episode reward: total was 44.300000. running mean: -21.872323\n",
      "ep 2042: ep_len:145 episode reward: total was 69.500000. running mean: -20.958600\n",
      "ep 2042: ep_len:55 episode reward: total was 23.000000. running mean: -20.519014\n",
      "ep 2042: ep_len:1004 episode reward: total was -59.820000. running mean: -20.912024\n",
      "ep 2042: ep_len:3767 episode reward: total was -100.470000. running mean: -21.707604\n",
      "ep 2042: ep_len:3850 episode reward: total was -1137.710000. running mean: -32.867628\n",
      "ep 2042: ep_len:734 episode reward: total was 12.780000. running mean: -32.411152\n",
      "ep 2042: ep_len:681 episode reward: total was -20.090000. running mean: -32.287940\n",
      "ep 2042: ep_len:122 episode reward: total was 59.500000. running mean: -31.370061\n",
      "ep 2042: ep_len:738 episode reward: total was -29.150000. running mean: -31.347860\n",
      "ep 2042: ep_len:2751 episode reward: total was -0.450000. running mean: -31.038881\n",
      "ep 2042: ep_len:72 episode reward: total was 33.000000. running mean: -30.398493\n",
      "epsilon:0.009992 episode_count: 30763. steps_count: 32928381.000000\n",
      "ep 2043: ep_len:976 episode reward: total was -100.900000. running mean: -31.103508\n",
      "ep 2043: ep_len:500 episode reward: total was 0.550000. running mean: -30.786973\n",
      "ep 2043: ep_len:99 episode reward: total was 46.500000. running mean: -30.014103\n",
      "ep 2043: ep_len:817 episode reward: total was -7.330000. running mean: -29.787262\n",
      "ep 2043: ep_len:120 episode reward: total was 58.500000. running mean: -28.904389\n",
      "ep 2043: ep_len:1392 episode reward: total was -268.190000. running mean: -31.297245\n",
      "ep 2043: ep_len:4134 episode reward: total was -405.800000. running mean: -35.042273\n",
      "ep 2043: ep_len:537 episode reward: total was 2.720000. running mean: -34.664650\n",
      "ep 2043: ep_len:817 episode reward: total was 11.810000. running mean: -34.199904\n",
      "ep 2043: ep_len:937 episode reward: total was -0.930000. running mean: -33.867205\n",
      "ep 2043: ep_len:87 episode reward: total was 42.000000. running mean: -33.108533\n",
      "ep 2043: ep_len:757 episode reward: total was -40.070000. running mean: -33.178147\n",
      "ep 2043: ep_len:2782 episode reward: total was -24.870000. running mean: -33.095066\n",
      "ep 2043: ep_len:55 episode reward: total was 24.500000. running mean: -32.519115\n",
      "epsilon:0.009992 episode_count: 30777. steps_count: 32942391.000000\n",
      "ep 2044: ep_len:1107 episode reward: total was 0.430000. running mean: -32.189624\n",
      "ep 2044: ep_len:1280 episode reward: total was -106.550000. running mean: -32.933228\n",
      "ep 2044: ep_len:2884 episode reward: total was -32.330000. running mean: -32.927195\n",
      "ep 2044: ep_len:1660 episode reward: total was -34.250000. running mean: -32.940423\n",
      "ep 2044: ep_len:37 episode reward: total was 17.000000. running mean: -32.441019\n",
      "ep 2044: ep_len:69 episode reward: total was 33.000000. running mean: -31.786609\n",
      "ep 2044: ep_len:81 episode reward: total was 39.000000. running mean: -31.078743\n",
      "ep 2044: ep_len:908 episode reward: total was -12.020000. running mean: -30.888156\n",
      "ep 2044: ep_len:340 episode reward: total was 22.590000. running mean: -30.353374\n",
      "ep 2044: ep_len:1558 episode reward: total was -8.740000. running mean: -30.137240\n",
      "ep 2044: ep_len:718 episode reward: total was 48.880000. running mean: -29.347068\n",
      "ep 2044: ep_len:500 episode reward: total was 26.210000. running mean: -28.791497\n",
      "ep 2044: ep_len:58 episode reward: total was 26.000000. running mean: -28.243582\n",
      "ep 2044: ep_len:570 episode reward: total was -30.570000. running mean: -28.266846\n",
      "ep 2044: ep_len:2909 episode reward: total was -628.960000. running mean: -34.273778\n",
      "epsilon:0.009992 episode_count: 30792. steps_count: 32957070.000000\n",
      "ep 2045: ep_len:1107 episode reward: total was -12.820000. running mean: -34.059240\n",
      "ep 2045: ep_len:731 episode reward: total was -4.020000. running mean: -33.758848\n",
      "ep 2045: ep_len:3050 episode reward: total was -11.190000. running mean: -33.533159\n",
      "ep 2045: ep_len:517 episode reward: total was 27.120000. running mean: -32.926628\n",
      "ep 2045: ep_len:135 episode reward: total was 63.000000. running mean: -31.967361\n",
      "ep 2045: ep_len:1508 episode reward: total was 17.390000. running mean: -31.473788\n",
      "ep 2045: ep_len:3934 episode reward: total was -202.850000. running mean: -33.187550\n",
      "ep 2045: ep_len:1206 episode reward: total was -48.710000. running mean: -33.342774\n",
      "ep 2045: ep_len:753 episode reward: total was 36.670000. running mean: -32.642647\n",
      "ep 2045: ep_len:955 episode reward: total was 11.890000. running mean: -32.197320\n",
      "ep 2045: ep_len:104 episode reward: total was 49.000000. running mean: -31.385347\n",
      "ep 2045: ep_len:1164 episode reward: total was 1.720000. running mean: -31.054293\n",
      "ep 2045: ep_len:2955 episode reward: total was -0.120000. running mean: -30.744951\n",
      "epsilon:0.009992 episode_count: 30805. steps_count: 32975189.000000\n",
      "ep 2046: ep_len:2446 episode reward: total was -189.830000. running mean: -32.335801\n",
      "ep 2046: ep_len:3671 episode reward: total was -652.370000. running mean: -38.536143\n",
      "ep 2046: ep_len:58 episode reward: total was 27.500000. running mean: -37.875782\n",
      "ep 2046: ep_len:2927 episode reward: total was -58.770000. running mean: -38.084724\n",
      "ep 2046: ep_len:577 episode reward: total was 0.030000. running mean: -37.703577\n",
      "ep 2046: ep_len:139 episode reward: total was 68.000000. running mean: -36.646541\n",
      "ep 2046: ep_len:1076 episode reward: total was -54.030000. running mean: -36.820375\n",
      "ep 2046: ep_len:327 episode reward: total was 7.310000. running mean: -36.379072\n",
      "ep 2046: ep_len:1603 episode reward: total was -7.860000. running mean: -36.093881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2046: ep_len:7350 episode reward: total was -15.800000. running mean: -35.890942\n",
      "ep 2046: ep_len:973 episode reward: total was 57.950000. running mean: -34.952533\n",
      "ep 2046: ep_len:152 episode reward: total was 73.000000. running mean: -33.873007\n",
      "ep 2046: ep_len:1119 episode reward: total was -2.110000. running mean: -33.555377\n",
      "ep 2046: ep_len:2829 episode reward: total was -16.410000. running mean: -33.383924\n",
      "epsilon:0.009992 episode_count: 30819. steps_count: 33000436.000000\n",
      "ep 2047: ep_len:1119 episode reward: total was -8.660000. running mean: -33.136684\n",
      "ep 2047: ep_len:733 episode reward: total was -15.590000. running mean: -32.961217\n",
      "ep 2047: ep_len:47 episode reward: total was 22.000000. running mean: -32.411605\n",
      "ep 2047: ep_len:3006 episode reward: total was -102.860000. running mean: -33.116089\n",
      "ep 2047: ep_len:631 episode reward: total was -21.040000. running mean: -32.995328\n",
      "ep 2047: ep_len:47 episode reward: total was 19.000000. running mean: -32.475375\n",
      "ep 2047: ep_len:82 episode reward: total was 38.000000. running mean: -31.770621\n",
      "ep 2047: ep_len:45 episode reward: total was 21.000000. running mean: -31.242915\n",
      "ep 2047: ep_len:32 episode reward: total was 14.500000. running mean: -30.785486\n",
      "ep 2047: ep_len:697 episode reward: total was 6.490000. running mean: -30.412731\n",
      "ep 2047: ep_len:352 episode reward: total was 17.200000. running mean: -29.936604\n",
      "ep 2047: ep_len:3868 episode reward: total was -977.550000. running mean: -39.412738\n",
      "ep 2047: ep_len:910 episode reward: total was 61.370000. running mean: -38.404910\n",
      "ep 2047: ep_len:713 episode reward: total was -28.400000. running mean: -38.304861\n",
      "ep 2047: ep_len:48 episode reward: total was 22.500000. running mean: -37.696813\n",
      "ep 2047: ep_len:636 episode reward: total was 8.480000. running mean: -37.235044\n",
      "ep 2047: ep_len:2876 episode reward: total was -12.240000. running mean: -36.985094\n",
      "epsilon:0.009992 episode_count: 30836. steps_count: 33016278.000000\n",
      "ep 2048: ep_len:1137 episode reward: total was 13.660000. running mean: -36.478643\n",
      "ep 2048: ep_len:1628 episode reward: total was -67.720000. running mean: -36.791057\n",
      "ep 2048: ep_len:30 episode reward: total was 12.000000. running mean: -36.303146\n",
      "ep 2048: ep_len:2976 episode reward: total was -60.480000. running mean: -36.544915\n",
      "ep 2048: ep_len:848 episode reward: total was 52.920000. running mean: -35.650265\n",
      "ep 2048: ep_len:91 episode reward: total was 42.500000. running mean: -34.868763\n",
      "ep 2048: ep_len:591 episode reward: total was 36.140000. running mean: -34.158675\n",
      "ep 2048: ep_len:3870 episode reward: total was -106.070000. running mean: -34.877788\n",
      "ep 2048: ep_len:818 episode reward: total was -56.680000. running mean: -35.095811\n",
      "ep 2048: ep_len:782 episode reward: total was 10.810000. running mean: -34.636752\n",
      "ep 2048: ep_len:500 episode reward: total was 18.130000. running mean: -34.109085\n",
      "ep 2048: ep_len:50 episode reward: total was 22.000000. running mean: -33.547994\n",
      "ep 2048: ep_len:104 episode reward: total was 50.500000. running mean: -32.707514\n",
      "ep 2048: ep_len:1107 episode reward: total was -17.380000. running mean: -32.554239\n",
      "ep 2048: ep_len:2831 episode reward: total was -10.150000. running mean: -32.330197\n",
      "epsilon:0.009992 episode_count: 30851. steps_count: 33033641.000000\n",
      "ep 2049: ep_len:656 episode reward: total was 18.940000. running mean: -31.817495\n",
      "ep 2049: ep_len:1222 episode reward: total was -39.460000. running mean: -31.893920\n",
      "ep 2049: ep_len:3096 episode reward: total was -88.680000. running mean: -32.461781\n",
      "ep 2049: ep_len:544 episode reward: total was -27.050000. running mean: -32.407663\n",
      "ep 2049: ep_len:63 episode reward: total was 28.500000. running mean: -31.798586\n",
      "ep 2049: ep_len:597 episode reward: total was 42.600000. running mean: -31.054600\n",
      "ep 2049: ep_len:3952 episode reward: total was -174.330000. running mean: -32.487354\n",
      "ep 2049: ep_len:1160 episode reward: total was -78.950000. running mean: -32.951981\n",
      "ep 2049: ep_len:783 episode reward: total was 25.980000. running mean: -32.362661\n",
      "ep 2049: ep_len:660 episode reward: total was -27.310000. running mean: -32.312134\n",
      "ep 2049: ep_len:142 episode reward: total was 66.500000. running mean: -31.324013\n",
      "ep 2049: ep_len:42 episode reward: total was 18.000000. running mean: -30.830773\n",
      "ep 2049: ep_len:97 episode reward: total was 47.000000. running mean: -30.052465\n",
      "ep 2049: ep_len:643 episode reward: total was -16.530000. running mean: -29.917240\n",
      "ep 2049: ep_len:2901 episode reward: total was -5.280000. running mean: -29.670868\n",
      "ep 2049: ep_len:55 episode reward: total was 26.000000. running mean: -29.114159\n",
      "epsilon:0.009992 episode_count: 30867. steps_count: 33050254.000000\n",
      "ep 2050: ep_len:600 episode reward: total was 24.770000. running mean: -28.575318\n",
      "ep 2050: ep_len:1226 episode reward: total was -37.400000. running mean: -28.663565\n",
      "ep 2050: ep_len:3039 episode reward: total was -46.840000. running mean: -28.845329\n",
      "ep 2050: ep_len:1274 episode reward: total was -32.880000. running mean: -28.885676\n",
      "ep 2050: ep_len:39 episode reward: total was 18.000000. running mean: -28.416819\n",
      "ep 2050: ep_len:94 episode reward: total was 45.500000. running mean: -27.677651\n",
      "ep 2050: ep_len:48 episode reward: total was 21.000000. running mean: -27.190874\n",
      "ep 2050: ep_len:747 episode reward: total was -30.900000. running mean: -27.227965\n",
      "ep 2050: ep_len:3888 episode reward: total was -149.660000. running mean: -28.452286\n",
      "ep 2050: ep_len:1284 episode reward: total was -46.920000. running mean: -28.636963\n",
      "ep 2050: ep_len:710 episode reward: total was 44.050000. running mean: -27.910093\n",
      "ep 2050: ep_len:1090 episode reward: total was -23.610000. running mean: -27.867092\n",
      "ep 2050: ep_len:1486 episode reward: total was 3.150000. running mean: -27.556921\n",
      "ep 2050: ep_len:2888 episode reward: total was -30.630000. running mean: -27.587652\n",
      "epsilon:0.009992 episode_count: 30881. steps_count: 33068667.000000\n",
      "ep 2051: ep_len:627 episode reward: total was 8.480000. running mean: -27.226976\n",
      "ep 2051: ep_len:500 episode reward: total was -62.580000. running mean: -27.580506\n",
      "ep 2051: ep_len:2975 episode reward: total was -36.820000. running mean: -27.672901\n",
      "ep 2051: ep_len:552 episode reward: total was -57.270000. running mean: -27.968872\n",
      "ep 2051: ep_len:60 episode reward: total was 25.500000. running mean: -27.434183\n",
      "ep 2051: ep_len:1002 episode reward: total was -118.420000. running mean: -28.344041\n",
      "ep 2051: ep_len:4026 episode reward: total was -117.770000. running mean: -29.238301\n",
      "ep 2051: ep_len:524 episode reward: total was -44.420000. running mean: -29.390118\n",
      "ep 2051: ep_len:7313 episode reward: total was 76.390000. running mean: -28.332317\n",
      "ep 2051: ep_len:500 episode reward: total was 33.500000. running mean: -27.713994\n",
      "ep 2051: ep_len:600 episode reward: total was -10.330000. running mean: -27.540154\n",
      "ep 2051: ep_len:2763 episode reward: total was -8.710000. running mean: -27.351852\n",
      "ep 2051: ep_len:48 episode reward: total was 21.000000. running mean: -26.868334\n",
      "epsilon:0.009992 episode_count: 30894. steps_count: 33090157.000000\n",
      "ep 2052: ep_len:595 episode reward: total was 25.640000. running mean: -26.343250\n",
      "ep 2052: ep_len:721 episode reward: total was -20.950000. running mean: -26.289318\n",
      "ep 2052: ep_len:69 episode reward: total was 30.000000. running mean: -25.726425\n",
      "ep 2052: ep_len:3051 episode reward: total was -34.100000. running mean: -25.810160\n",
      "ep 2052: ep_len:500 episode reward: total was 25.420000. running mean: -25.297859\n",
      "ep 2052: ep_len:1441 episode reward: total was -172.030000. running mean: -26.765180\n",
      "ep 2052: ep_len:3849 episode reward: total was -99.670000. running mean: -27.494228\n",
      "ep 2052: ep_len:853 episode reward: total was 23.360000. running mean: -26.985686\n",
      "ep 2052: ep_len:7260 episode reward: total was 45.430000. running mean: -26.261529\n",
      "ep 2052: ep_len:630 episode reward: total was 13.930000. running mean: -25.859614\n",
      "ep 2052: ep_len:195 episode reward: total was -55.000000. running mean: -26.151018\n",
      "ep 2052: ep_len:44 episode reward: total was 20.500000. running mean: -25.684508\n",
      "ep 2052: ep_len:1137 episode reward: total was -46.370000. running mean: -25.891362\n",
      "ep 2052: ep_len:2868 episode reward: total was -3.410000. running mean: -25.666549\n",
      "epsilon:0.009992 episode_count: 30908. steps_count: 33113370.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2053: ep_len:636 episode reward: total was 42.710000. running mean: -24.982783\n",
      "ep 2053: ep_len:738 episode reward: total was -26.610000. running mean: -24.999056\n",
      "ep 2053: ep_len:2930 episode reward: total was -92.850000. running mean: -25.677565\n",
      "ep 2053: ep_len:675 episode reward: total was 5.930000. running mean: -25.361489\n",
      "ep 2053: ep_len:92 episode reward: total was 43.000000. running mean: -24.677874\n",
      "ep 2053: ep_len:92 episode reward: total was 43.000000. running mean: -24.001096\n",
      "ep 2053: ep_len:72 episode reward: total was 34.500000. running mean: -23.416085\n",
      "ep 2053: ep_len:617 episode reward: total was -8.020000. running mean: -23.262124\n",
      "ep 2053: ep_len:332 episode reward: total was 12.560000. running mean: -22.903903\n",
      "ep 2053: ep_len:897 episode reward: total was -16.900000. running mean: -22.843864\n",
      "ep 2053: ep_len:695 episode reward: total was -1.660000. running mean: -22.632025\n",
      "ep 2053: ep_len:828 episode reward: total was -27.790000. running mean: -22.683605\n",
      "ep 2053: ep_len:64 episode reward: total was 29.000000. running mean: -22.166769\n",
      "ep 2053: ep_len:107 episode reward: total was 52.000000. running mean: -21.425101\n",
      "ep 2053: ep_len:500 episode reward: total was 40.330000. running mean: -20.807550\n",
      "ep 2053: ep_len:2776 episode reward: total was 7.060000. running mean: -20.528874\n",
      "ep 2053: ep_len:58 episode reward: total was 27.500000. running mean: -20.048586\n",
      "epsilon:0.009992 episode_count: 30925. steps_count: 33125479.000000\n",
      "ep 2054: ep_len:1019 episode reward: total was -98.370000. running mean: -20.831800\n",
      "ep 2054: ep_len:500 episode reward: total was 13.780000. running mean: -20.485682\n",
      "ep 2054: ep_len:2985 episode reward: total was -56.650000. running mean: -20.847325\n",
      "ep 2054: ep_len:816 episode reward: total was 23.660000. running mean: -20.402252\n",
      "ep 2054: ep_len:45 episode reward: total was 21.000000. running mean: -19.988229\n",
      "ep 2054: ep_len:1545 episode reward: total was 26.000000. running mean: -19.528347\n",
      "ep 2054: ep_len:3901 episode reward: total was -108.760000. running mean: -20.420664\n",
      "ep 2054: ep_len:509 episode reward: total was -4.110000. running mean: -20.257557\n",
      "ep 2054: ep_len:736 episode reward: total was 36.810000. running mean: -19.686881\n",
      "ep 2054: ep_len:657 episode reward: total was 9.440000. running mean: -19.395613\n",
      "ep 2054: ep_len:84 episode reward: total was 40.500000. running mean: -18.796656\n",
      "ep 2054: ep_len:135 episode reward: total was 63.000000. running mean: -17.978690\n",
      "ep 2054: ep_len:76 episode reward: total was 36.500000. running mean: -17.433903\n",
      "ep 2054: ep_len:901 episode reward: total was 15.970000. running mean: -17.099864\n",
      "ep 2054: ep_len:2862 episode reward: total was -13.910000. running mean: -17.067965\n",
      "epsilon:0.009992 episode_count: 30940. steps_count: 33142250.000000\n",
      "ep 2055: ep_len:1132 episode reward: total was -18.020000. running mean: -17.077486\n",
      "ep 2055: ep_len:1224 episode reward: total was -52.570000. running mean: -17.432411\n",
      "ep 2055: ep_len:2972 episode reward: total was -48.100000. running mean: -17.739087\n",
      "ep 2055: ep_len:1471 episode reward: total was 16.590000. running mean: -17.395796\n",
      "ep 2055: ep_len:54 episode reward: total was 25.500000. running mean: -16.966838\n",
      "ep 2055: ep_len:41 episode reward: total was 17.500000. running mean: -16.622169\n",
      "ep 2055: ep_len:1458 episode reward: total was 44.170000. running mean: -16.014248\n",
      "ep 2055: ep_len:3584 episode reward: total was -29.290000. running mean: -16.147005\n",
      "ep 2055: ep_len:1284 episode reward: total was -80.460000. running mean: -16.790135\n",
      "ep 2055: ep_len:731 episode reward: total was 47.110000. running mean: -16.151134\n",
      "ep 2055: ep_len:586 episode reward: total was -6.430000. running mean: -16.053923\n",
      "ep 2055: ep_len:1134 episode reward: total was -2.080000. running mean: -15.914183\n",
      "ep 2055: ep_len:2868 episode reward: total was -2.030000. running mean: -15.775341\n",
      "ep 2055: ep_len:64 episode reward: total was 30.500000. running mean: -15.312588\n",
      "epsilon:0.009992 episode_count: 30954. steps_count: 33160853.000000\n",
      "ep 2056: ep_len:627 episode reward: total was -9.300000. running mean: -15.252462\n",
      "ep 2056: ep_len:1590 episode reward: total was -53.990000. running mean: -15.639838\n",
      "ep 2056: ep_len:67 episode reward: total was 32.000000. running mean: -15.163439\n",
      "ep 2056: ep_len:3002 episode reward: total was 7.010000. running mean: -14.941705\n",
      "ep 2056: ep_len:640 episode reward: total was 26.640000. running mean: -14.525888\n",
      "ep 2056: ep_len:29 episode reward: total was 13.000000. running mean: -14.250629\n",
      "ep 2056: ep_len:762 episode reward: total was -64.660000. running mean: -14.754723\n",
      "ep 2056: ep_len:341 episode reward: total was 16.230000. running mean: -14.444875\n",
      "ep 2056: ep_len:1528 episode reward: total was -74.730000. running mean: -15.047727\n",
      "ep 2056: ep_len:859 episode reward: total was 23.900000. running mean: -14.658249\n",
      "ep 2056: ep_len:928 episode reward: total was -7.230000. running mean: -14.583967\n",
      "ep 2056: ep_len:135 episode reward: total was 64.500000. running mean: -13.793127\n",
      "ep 2056: ep_len:1114 episode reward: total was -19.330000. running mean: -13.848496\n",
      "ep 2056: ep_len:2762 episode reward: total was -1.290000. running mean: -13.722911\n",
      "ep 2056: ep_len:55 episode reward: total was 26.000000. running mean: -13.325682\n",
      "epsilon:0.009992 episode_count: 30969. steps_count: 33175292.000000\n",
      "ep 2057: ep_len:500 episode reward: total was 12.430000. running mean: -13.068125\n",
      "ep 2057: ep_len:216 episode reward: total was 10.450000. running mean: -12.832944\n",
      "ep 2057: ep_len:3027 episode reward: total was 19.350000. running mean: -12.511114\n",
      "ep 2057: ep_len:500 episode reward: total was 11.850000. running mean: -12.267503\n",
      "ep 2057: ep_len:73 episode reward: total was 35.000000. running mean: -11.794828\n",
      "ep 2057: ep_len:1007 episode reward: total was -0.260000. running mean: -11.679480\n",
      "ep 2057: ep_len:682 episode reward: total was 22.880000. running mean: -11.333885\n",
      "ep 2057: ep_len:1251 episode reward: total was -55.330000. running mean: -11.773846\n",
      "ep 2057: ep_len:636 episode reward: total was 15.380000. running mean: -11.502308\n",
      "ep 2057: ep_len:654 episode reward: total was -4.470000. running mean: -11.431985\n",
      "ep 2057: ep_len:59 episode reward: total was 28.000000. running mean: -11.037665\n",
      "ep 2057: ep_len:194 episode reward: total was 86.500000. running mean: -10.062288\n",
      "ep 2057: ep_len:48 episode reward: total was 19.500000. running mean: -9.766665\n",
      "ep 2057: ep_len:1121 episode reward: total was 9.080000. running mean: -9.578199\n",
      "ep 2057: ep_len:2915 episode reward: total was -5.020000. running mean: -9.532617\n",
      "epsilon:0.009992 episode_count: 30984. steps_count: 33188175.000000\n",
      "ep 2058: ep_len:883 episode reward: total was 20.170000. running mean: -9.235590\n",
      "ep 2058: ep_len:679 episode reward: total was -34.820000. running mean: -9.491435\n",
      "ep 2058: ep_len:2954 episode reward: total was -1.740000. running mean: -9.413920\n",
      "ep 2058: ep_len:797 episode reward: total was 9.970000. running mean: -9.220081\n",
      "ep 2058: ep_len:133 episode reward: total was 60.500000. running mean: -8.522880\n",
      "ep 2058: ep_len:90 episode reward: total was 43.500000. running mean: -8.002651\n",
      "ep 2058: ep_len:59 episode reward: total was 28.000000. running mean: -7.642625\n",
      "ep 2058: ep_len:1475 episode reward: total was 1.510000. running mean: -7.551099\n",
      "ep 2058: ep_len:645 episode reward: total was 21.960000. running mean: -7.255988\n",
      "ep 2058: ep_len:3473 episode reward: total was -681.930000. running mean: -14.002728\n",
      "ep 2058: ep_len:860 episode reward: total was 45.370000. running mean: -13.409001\n",
      "ep 2058: ep_len:956 episode reward: total was 17.810000. running mean: -13.096811\n",
      "ep 2058: ep_len:88 episode reward: total was 41.000000. running mean: -12.555842\n",
      "ep 2058: ep_len:51 episode reward: total was -1.990000. running mean: -12.450184\n",
      "ep 2058: ep_len:869 episode reward: total was 11.760000. running mean: -12.208082\n",
      "ep 2058: ep_len:2826 episode reward: total was -8.600000. running mean: -12.172001\n",
      "ep 2058: ep_len:62 episode reward: total was 29.500000. running mean: -11.755281\n",
      "epsilon:0.009992 episode_count: 31001. steps_count: 33205075.000000\n",
      "ep 2059: ep_len:697 episode reward: total was -41.560000. running mean: -12.053328\n",
      "ep 2059: ep_len:672 episode reward: total was -21.730000. running mean: -12.150095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2059: ep_len:2897 episode reward: total was -39.960000. running mean: -12.428194\n",
      "ep 2059: ep_len:553 episode reward: total was -7.770000. running mean: -12.381612\n",
      "ep 2059: ep_len:84 episode reward: total was 40.500000. running mean: -11.852796\n",
      "ep 2059: ep_len:80 episode reward: total was 38.500000. running mean: -11.349268\n",
      "ep 2059: ep_len:1070 episode reward: total was -64.210000. running mean: -11.877876\n",
      "ep 2059: ep_len:659 episode reward: total was 33.270000. running mean: -11.426397\n",
      "ep 2059: ep_len:635 episode reward: total was 22.620000. running mean: -11.085933\n",
      "ep 2059: ep_len:847 episode reward: total was 61.470000. running mean: -10.360374\n",
      "ep 2059: ep_len:1125 episode reward: total was -8.110000. running mean: -10.337870\n",
      "ep 2059: ep_len:81 episode reward: total was 39.000000. running mean: -9.844491\n",
      "ep 2059: ep_len:181 episode reward: total was 89.000000. running mean: -8.856046\n",
      "ep 2059: ep_len:851 episode reward: total was -5.350000. running mean: -8.820986\n",
      "ep 2059: ep_len:2846 episode reward: total was 2.640000. running mean: -8.706376\n",
      "epsilon:0.009992 episode_count: 31016. steps_count: 33218353.000000\n",
      "ep 2060: ep_len:1377 episode reward: total was -16.610000. running mean: -8.785412\n",
      "ep 2060: ep_len:1208 episode reward: total was -74.920000. running mean: -9.446758\n",
      "ep 2060: ep_len:2919 episode reward: total was -75.250000. running mean: -10.104790\n",
      "ep 2060: ep_len:805 episode reward: total was -18.380000. running mean: -10.187542\n",
      "ep 2060: ep_len:38 episode reward: total was 17.500000. running mean: -9.910667\n",
      "ep 2060: ep_len:122 episode reward: total was 59.500000. running mean: -9.216560\n",
      "ep 2060: ep_len:863 episode reward: total was 26.550000. running mean: -8.858895\n",
      "ep 2060: ep_len:4016 episode reward: total was -385.830000. running mean: -12.628606\n",
      "ep 2060: ep_len:1257 episode reward: total was -15.530000. running mean: -12.657620\n",
      "ep 2060: ep_len:756 episode reward: total was 8.770000. running mean: -12.443344\n",
      "ep 2060: ep_len:625 episode reward: total was 1.270000. running mean: -12.306210\n",
      "ep 2060: ep_len:81 episode reward: total was 39.000000. running mean: -11.793148\n",
      "ep 2060: ep_len:500 episode reward: total was 33.960000. running mean: -11.335617\n",
      "ep 2060: ep_len:2803 episode reward: total was -260.850000. running mean: -13.830760\n",
      "ep 2060: ep_len:56 episode reward: total was 26.500000. running mean: -13.427453\n",
      "epsilon:0.009992 episode_count: 31031. steps_count: 33235779.000000\n",
      "ep 2061: ep_len:654 episode reward: total was -1.710000. running mean: -13.310278\n",
      "ep 2061: ep_len:802 episode reward: total was -6.290000. running mean: -13.240075\n",
      "ep 2061: ep_len:79 episode reward: total was 38.000000. running mean: -12.727675\n",
      "ep 2061: ep_len:3087 episode reward: total was -8.270000. running mean: -12.683098\n",
      "ep 2061: ep_len:1436 episode reward: total was 12.810000. running mean: -12.428167\n",
      "ep 2061: ep_len:55 episode reward: total was 26.000000. running mean: -12.043885\n",
      "ep 2061: ep_len:724 episode reward: total was 22.230000. running mean: -11.701146\n",
      "ep 2061: ep_len:606 episode reward: total was 11.220000. running mean: -11.471935\n",
      "ep 2061: ep_len:809 episode reward: total was -24.280000. running mean: -11.600016\n",
      "ep 2061: ep_len:607 episode reward: total was -4.080000. running mean: -11.524815\n",
      "ep 2061: ep_len:627 episode reward: total was -23.750000. running mean: -11.647067\n",
      "ep 2061: ep_len:92 episode reward: total was 43.000000. running mean: -11.100597\n",
      "ep 2061: ep_len:929 episode reward: total was -179.540000. running mean: -12.784991\n",
      "ep 2061: ep_len:2806 episode reward: total was -5.680000. running mean: -12.713941\n",
      "epsilon:0.009992 episode_count: 31045. steps_count: 33249092.000000\n",
      "ep 2062: ep_len:978 episode reward: total was -92.130000. running mean: -13.508101\n",
      "ep 2062: ep_len:709 episode reward: total was -41.760000. running mean: -13.790620\n",
      "ep 2062: ep_len:59 episode reward: total was 28.000000. running mean: -13.372714\n",
      "ep 2062: ep_len:2856 episode reward: total was -33.250000. running mean: -13.571487\n",
      "ep 2062: ep_len:1598 episode reward: total was -32.550000. running mean: -13.761272\n",
      "ep 2062: ep_len:62 episode reward: total was 29.500000. running mean: -13.328659\n",
      "ep 2062: ep_len:3381 episode reward: total was -677.400000. running mean: -19.969373\n",
      "ep 2062: ep_len:678 episode reward: total was 20.020000. running mean: -19.569479\n",
      "ep 2062: ep_len:1193 episode reward: total was -46.300000. running mean: -19.836784\n",
      "ep 2062: ep_len:856 episode reward: total was 52.010000. running mean: -19.118316\n",
      "ep 2062: ep_len:1108 episode reward: total was -6.260000. running mean: -18.989733\n",
      "ep 2062: ep_len:79 episode reward: total was 36.500000. running mean: -18.434836\n",
      "ep 2062: ep_len:52 episode reward: total was 23.000000. running mean: -18.020488\n",
      "ep 2062: ep_len:1205 episode reward: total was 5.190000. running mean: -17.788383\n",
      "ep 2062: ep_len:2715 episode reward: total was -22.110000. running mean: -17.831599\n",
      "ep 2062: ep_len:59 episode reward: total was 28.000000. running mean: -17.373283\n",
      "epsilon:0.009992 episode_count: 31061. steps_count: 33266680.000000\n",
      "ep 2063: ep_len:681 episode reward: total was 15.220000. running mean: -17.047350\n",
      "ep 2063: ep_len:500 episode reward: total was 23.580000. running mean: -16.641077\n",
      "ep 2063: ep_len:2994 episode reward: total was -37.190000. running mean: -16.846566\n",
      "ep 2063: ep_len:698 episode reward: total was 12.980000. running mean: -16.548300\n",
      "ep 2063: ep_len:40 episode reward: total was 18.500000. running mean: -16.197817\n",
      "ep 2063: ep_len:88 episode reward: total was 42.500000. running mean: -15.610839\n",
      "ep 2063: ep_len:1519 episode reward: total was 22.550000. running mean: -15.229231\n",
      "ep 2063: ep_len:3576 episode reward: total was -250.900000. running mean: -17.585938\n",
      "ep 2063: ep_len:707 episode reward: total was -24.000000. running mean: -17.650079\n",
      "ep 2063: ep_len:822 episode reward: total was 29.740000. running mean: -17.176178\n",
      "ep 2063: ep_len:588 episode reward: total was -31.080000. running mean: -17.315216\n",
      "ep 2063: ep_len:218 episode reward: total was 104.500000. running mean: -16.097064\n",
      "ep 2063: ep_len:67 episode reward: total was 32.000000. running mean: -15.616094\n",
      "ep 2063: ep_len:1507 episode reward: total was 0.480000. running mean: -15.455133\n",
      "ep 2063: ep_len:2864 episode reward: total was -11.620000. running mean: -15.416781\n",
      "epsilon:0.009992 episode_count: 31076. steps_count: 33283549.000000\n",
      "ep 2064: ep_len:673 episode reward: total was -2.770000. running mean: -15.290313\n",
      "ep 2064: ep_len:500 episode reward: total was 18.680000. running mean: -14.950610\n",
      "ep 2064: ep_len:2855 episode reward: total was -34.750000. running mean: -15.148604\n",
      "ep 2064: ep_len:1543 episode reward: total was -178.660000. running mean: -16.783718\n",
      "ep 2064: ep_len:101 episode reward: total was 46.000000. running mean: -16.155881\n",
      "ep 2064: ep_len:50 episode reward: total was 22.000000. running mean: -15.774322\n",
      "ep 2064: ep_len:500 episode reward: total was 41.030000. running mean: -15.206279\n",
      "ep 2064: ep_len:652 episode reward: total was -74.290000. running mean: -15.797116\n",
      "ep 2064: ep_len:559 episode reward: total was -4.680000. running mean: -15.685945\n",
      "ep 2064: ep_len:752 episode reward: total was -1.310000. running mean: -15.542186\n",
      "ep 2064: ep_len:715 episode reward: total was -0.700000. running mean: -15.393764\n",
      "ep 2064: ep_len:57 episode reward: total was 27.000000. running mean: -14.969826\n",
      "ep 2064: ep_len:116 episode reward: total was 56.500000. running mean: -14.255128\n",
      "ep 2064: ep_len:1028 episode reward: total was -5.950000. running mean: -14.172077\n",
      "ep 2064: ep_len:2799 episode reward: total was -28.060000. running mean: -14.310956\n",
      "ep 2064: ep_len:48 episode reward: total was 22.500000. running mean: -13.942846\n",
      "epsilon:0.009992 episode_count: 31092. steps_count: 33296497.000000\n",
      "ep 2065: ep_len:1017 episode reward: total was -25.480000. running mean: -14.058218\n",
      "ep 2065: ep_len:751 episode reward: total was -12.860000. running mean: -14.046236\n",
      "ep 2065: ep_len:2978 episode reward: total was -61.170000. running mean: -14.517473\n",
      "ep 2065: ep_len:500 episode reward: total was 25.970000. running mean: -14.112598\n",
      "ep 2065: ep_len:57 episode reward: total was 27.000000. running mean: -13.701473\n",
      "ep 2065: ep_len:53 episode reward: total was 22.000000. running mean: -13.344458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2065: ep_len:500 episode reward: total was 44.460000. running mean: -12.766413\n",
      "ep 2065: ep_len:646 episode reward: total was 2.780000. running mean: -12.610949\n",
      "ep 2065: ep_len:1220 episode reward: total was -28.790000. running mean: -12.772740\n",
      "ep 2065: ep_len:724 episode reward: total was 51.600000. running mean: -12.129012\n",
      "ep 2065: ep_len:656 episode reward: total was -24.010000. running mean: -12.247822\n",
      "ep 2065: ep_len:597 episode reward: total was -9.410000. running mean: -12.219444\n",
      "ep 2065: ep_len:2927 episode reward: total was -8.900000. running mean: -12.186249\n",
      "epsilon:0.009992 episode_count: 31105. steps_count: 33309123.000000\n",
      "ep 2066: ep_len:650 episode reward: total was -23.970000. running mean: -12.304087\n",
      "ep 2066: ep_len:735 episode reward: total was -61.180000. running mean: -12.792846\n",
      "ep 2066: ep_len:64 episode reward: total was 30.500000. running mean: -12.359918\n",
      "ep 2066: ep_len:2878 episode reward: total was -36.670000. running mean: -12.603018\n",
      "ep 2066: ep_len:1609 episode reward: total was -58.790000. running mean: -13.064888\n",
      "ep 2066: ep_len:114 episode reward: total was 55.500000. running mean: -12.379239\n",
      "ep 2066: ep_len:1443 episode reward: total was -168.790000. running mean: -13.943347\n",
      "ep 2066: ep_len:3745 episode reward: total was -152.010000. running mean: -15.324013\n",
      "ep 2066: ep_len:786 episode reward: total was -36.580000. running mean: -15.536573\n",
      "ep 2066: ep_len:759 episode reward: total was 35.270000. running mean: -15.028508\n",
      "ep 2066: ep_len:1040 episode reward: total was 7.200000. running mean: -14.806223\n",
      "ep 2066: ep_len:82 episode reward: total was 39.500000. running mean: -14.263160\n",
      "ep 2066: ep_len:205 episode reward: total was 95.000000. running mean: -13.170529\n",
      "ep 2066: ep_len:61 episode reward: total was 29.000000. running mean: -12.748823\n",
      "ep 2066: ep_len:70 episode reward: total was 33.500000. running mean: -12.286335\n",
      "ep 2066: ep_len:1120 episode reward: total was -4.610000. running mean: -12.209572\n",
      "ep 2066: ep_len:2849 episode reward: total was -40.880000. running mean: -12.496276\n",
      "epsilon:0.009992 episode_count: 31122. steps_count: 33327333.000000\n",
      "ep 2067: ep_len:1088 episode reward: total was -1.410000. running mean: -12.385413\n",
      "ep 2067: ep_len:759 episode reward: total was -25.220000. running mean: -12.513759\n",
      "ep 2067: ep_len:2890 episode reward: total was -33.120000. running mean: -12.719822\n",
      "ep 2067: ep_len:500 episode reward: total was -6.940000. running mean: -12.662023\n",
      "ep 2067: ep_len:46 episode reward: total was 21.500000. running mean: -12.320403\n",
      "ep 2067: ep_len:500 episode reward: total was 6.560000. running mean: -12.131599\n",
      "ep 2067: ep_len:4005 episode reward: total was -494.980000. running mean: -16.960083\n",
      "ep 2067: ep_len:805 episode reward: total was 19.880000. running mean: -16.591682\n",
      "ep 2067: ep_len:787 episode reward: total was 22.710000. running mean: -16.198665\n",
      "ep 2067: ep_len:1481 episode reward: total was 18.220000. running mean: -15.854479\n",
      "ep 2067: ep_len:37 episode reward: total was 17.000000. running mean: -15.525934\n",
      "ep 2067: ep_len:717 episode reward: total was -0.070000. running mean: -15.371375\n",
      "ep 2067: ep_len:2809 episode reward: total was -2.410000. running mean: -15.241761\n",
      "epsilon:0.009992 episode_count: 31135. steps_count: 33343757.000000\n",
      "ep 2068: ep_len:671 episode reward: total was -22.750000. running mean: -15.316843\n",
      "ep 2068: ep_len:1275 episode reward: total was -41.960000. running mean: -15.583275\n",
      "ep 2068: ep_len:3036 episode reward: total was -65.900000. running mean: -16.086442\n",
      "ep 2068: ep_len:1625 episode reward: total was -31.640000. running mean: -16.241978\n",
      "ep 2068: ep_len:61 episode reward: total was 27.500000. running mean: -15.804558\n",
      "ep 2068: ep_len:65 episode reward: total was 31.000000. running mean: -15.336512\n",
      "ep 2068: ep_len:1429 episode reward: total was 13.350000. running mean: -15.049647\n",
      "ep 2068: ep_len:3587 episode reward: total was -100.270000. running mean: -15.901851\n",
      "ep 2068: ep_len:889 episode reward: total was 12.640000. running mean: -15.616432\n",
      "ep 2068: ep_len:864 episode reward: total was 66.570000. running mean: -14.794568\n",
      "ep 2068: ep_len:661 episode reward: total was 21.450000. running mean: -14.432122\n",
      "ep 2068: ep_len:169 episode reward: total was 80.000000. running mean: -13.487801\n",
      "ep 2068: ep_len:67 episode reward: total was 32.000000. running mean: -13.032923\n",
      "ep 2068: ep_len:69 episode reward: total was 33.000000. running mean: -12.572594\n",
      "ep 2068: ep_len:852 episode reward: total was -19.520000. running mean: -12.642068\n",
      "ep 2068: ep_len:2853 episode reward: total was -13.720000. running mean: -12.652847\n",
      "epsilon:0.009992 episode_count: 31151. steps_count: 33361930.000000\n",
      "ep 2069: ep_len:609 episode reward: total was -22.090000. running mean: -12.747219\n",
      "ep 2069: ep_len:746 episode reward: total was 3.010000. running mean: -12.589647\n",
      "ep 2069: ep_len:76 episode reward: total was 35.000000. running mean: -12.113750\n",
      "ep 2069: ep_len:2840 episode reward: total was 0.510000. running mean: -11.987513\n",
      "ep 2069: ep_len:504 episode reward: total was -24.850000. running mean: -12.116137\n",
      "ep 2069: ep_len:500 episode reward: total was -110.940000. running mean: -13.104376\n",
      "ep 2069: ep_len:621 episode reward: total was 14.520000. running mean: -12.828132\n",
      "ep 2069: ep_len:971 episode reward: total was -59.140000. running mean: -13.291251\n",
      "ep 2069: ep_len:708 episode reward: total was 33.620000. running mean: -12.822138\n",
      "ep 2069: ep_len:500 episode reward: total was 12.770000. running mean: -12.566217\n",
      "ep 2069: ep_len:136 episode reward: total was 65.000000. running mean: -11.790555\n",
      "ep 2069: ep_len:55 episode reward: total was 26.000000. running mean: -11.412649\n",
      "ep 2069: ep_len:581 episode reward: total was -5.470000. running mean: -11.353223\n",
      "ep 2069: ep_len:2849 episode reward: total was 6.200000. running mean: -11.177691\n",
      "ep 2069: ep_len:39 episode reward: total was 16.500000. running mean: -10.900914\n",
      "epsilon:0.009992 episode_count: 31166. steps_count: 33373665.000000\n",
      "ep 2070: ep_len:687 episode reward: total was -7.440000. running mean: -10.866305\n",
      "ep 2070: ep_len:948 episode reward: total was 23.240000. running mean: -10.525242\n",
      "ep 2070: ep_len:3032 episode reward: total was -39.310000. running mean: -10.813089\n",
      "ep 2070: ep_len:832 episode reward: total was 55.480000. running mean: -10.150158\n",
      "ep 2070: ep_len:111 episode reward: total was 54.000000. running mean: -9.508657\n",
      "ep 2070: ep_len:48 episode reward: total was 21.000000. running mean: -9.203570\n",
      "ep 2070: ep_len:632 episode reward: total was -2.880000. running mean: -9.140334\n",
      "ep 2070: ep_len:350 episode reward: total was 18.740000. running mean: -8.861531\n",
      "ep 2070: ep_len:536 episode reward: total was -9.960000. running mean: -8.872516\n",
      "ep 2070: ep_len:872 episode reward: total was 51.100000. running mean: -8.272791\n",
      "ep 2070: ep_len:1467 episode reward: total was 24.690000. running mean: -7.943163\n",
      "ep 2070: ep_len:30 episode reward: total was 12.000000. running mean: -7.743731\n",
      "ep 2070: ep_len:111 episode reward: total was 54.000000. running mean: -7.126294\n",
      "ep 2070: ep_len:1463 episode reward: total was 6.350000. running mean: -6.991531\n",
      "ep 2070: ep_len:2908 episode reward: total was 7.920000. running mean: -6.842416\n",
      "epsilon:0.009992 episode_count: 31181. steps_count: 33387692.000000\n",
      "ep 2071: ep_len:977 episode reward: total was -132.820000. running mean: -8.102191\n",
      "ep 2071: ep_len:752 episode reward: total was -7.340000. running mean: -8.094569\n",
      "ep 2071: ep_len:2895 episode reward: total was -23.220000. running mean: -8.245824\n",
      "ep 2071: ep_len:821 episode reward: total was 53.930000. running mean: -7.624066\n",
      "ep 2071: ep_len:68 episode reward: total was 32.500000. running mean: -7.222825\n",
      "ep 2071: ep_len:151 episode reward: total was 74.000000. running mean: -6.410597\n",
      "ep 2071: ep_len:79 episode reward: total was 38.000000. running mean: -5.966491\n",
      "ep 2071: ep_len:851 episode reward: total was 26.860000. running mean: -5.638226\n",
      "ep 2071: ep_len:358 episode reward: total was 17.200000. running mean: -5.409843\n",
      "ep 2071: ep_len:659 episode reward: total was -41.140000. running mean: -5.767145\n",
      "ep 2071: ep_len:816 episode reward: total was 40.670000. running mean: -5.302774\n",
      "ep 2071: ep_len:742 episode reward: total was -3.700000. running mean: -5.286746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2071: ep_len:74 episode reward: total was 35.500000. running mean: -4.878878\n",
      "ep 2071: ep_len:1146 episode reward: total was -0.830000. running mean: -4.838390\n",
      "ep 2071: ep_len:2822 episode reward: total was 8.430000. running mean: -4.705706\n",
      "epsilon:0.009992 episode_count: 31196. steps_count: 33400903.000000\n",
      "ep 2072: ep_len:1397 episode reward: total was 3.450000. running mean: -4.624149\n",
      "ep 2072: ep_len:1243 episode reward: total was -31.170000. running mean: -4.889607\n",
      "ep 2072: ep_len:2909 episode reward: total was -40.580000. running mean: -5.246511\n",
      "ep 2072: ep_len:889 episode reward: total was 49.570000. running mean: -4.698346\n",
      "ep 2072: ep_len:1574 episode reward: total was 38.870000. running mean: -4.262663\n",
      "ep 2072: ep_len:332 episode reward: total was 33.100000. running mean: -3.889036\n",
      "ep 2072: ep_len:672 episode reward: total was -22.150000. running mean: -4.071646\n",
      "ep 2072: ep_len:7505 episode reward: total was -99.120000. running mean: -5.022129\n",
      "ep 2072: ep_len:1144 episode reward: total was 14.640000. running mean: -4.825508\n",
      "ep 2072: ep_len:1454 episode reward: total was 21.500000. running mean: -4.562253\n",
      "ep 2072: ep_len:2809 episode reward: total was -94.260000. running mean: -5.459230\n",
      "epsilon:0.009992 episode_count: 31207. steps_count: 33422831.000000\n",
      "ep 2073: ep_len:1108 episode reward: total was -35.610000. running mean: -5.760738\n",
      "ep 2073: ep_len:975 episode reward: total was 3.190000. running mean: -5.671231\n",
      "ep 2073: ep_len:44 episode reward: total was 20.500000. running mean: -5.409518\n",
      "ep 2073: ep_len:2944 episode reward: total was -19.010000. running mean: -5.545523\n",
      "ep 2073: ep_len:658 episode reward: total was -10.500000. running mean: -5.595068\n",
      "ep 2073: ep_len:130 episode reward: total was 62.000000. running mean: -4.919117\n",
      "ep 2073: ep_len:92 episode reward: total was 44.500000. running mean: -4.424926\n",
      "ep 2073: ep_len:1465 episode reward: total was 4.530000. running mean: -4.335377\n",
      "ep 2073: ep_len:3521 episode reward: total was -246.740000. running mean: -6.759423\n",
      "ep 2073: ep_len:772 episode reward: total was -18.110000. running mean: -6.872929\n",
      "ep 2073: ep_len:698 episode reward: total was 23.840000. running mean: -6.565799\n",
      "ep 2073: ep_len:556 episode reward: total was -12.910000. running mean: -6.629241\n",
      "ep 2073: ep_len:63 episode reward: total was 30.000000. running mean: -6.262949\n",
      "ep 2073: ep_len:185 episode reward: total was 91.000000. running mean: -5.290320\n",
      "ep 2073: ep_len:55 episode reward: total was 24.500000. running mean: -4.992416\n",
      "ep 2073: ep_len:591 episode reward: total was -13.450000. running mean: -5.076992\n",
      "ep 2073: ep_len:2854 episode reward: total was -89.550000. running mean: -5.921722\n",
      "epsilon:0.009992 episode_count: 31224. steps_count: 33439542.000000\n",
      "ep 2074: ep_len:629 episode reward: total was 9.190000. running mean: -5.770605\n",
      "ep 2074: ep_len:818 episode reward: total was 9.710000. running mean: -5.615799\n",
      "ep 2074: ep_len:2989 episode reward: total was -28.910000. running mean: -5.848741\n",
      "ep 2074: ep_len:500 episode reward: total was 20.770000. running mean: -5.582554\n",
      "ep 2074: ep_len:116 episode reward: total was 55.000000. running mean: -4.976728\n",
      "ep 2074: ep_len:78 episode reward: total was 36.000000. running mean: -4.566961\n",
      "ep 2074: ep_len:1126 episode reward: total was 17.800000. running mean: -4.343291\n",
      "ep 2074: ep_len:3526 episode reward: total was -240.230000. running mean: -6.702158\n",
      "ep 2074: ep_len:601 episode reward: total was -5.510000. running mean: -6.690237\n",
      "ep 2074: ep_len:726 episode reward: total was 11.810000. running mean: -6.505234\n",
      "ep 2074: ep_len:605 episode reward: total was -15.370000. running mean: -6.593882\n",
      "ep 2074: ep_len:64 episode reward: total was 30.500000. running mean: -6.222943\n",
      "ep 2074: ep_len:120 episode reward: total was 57.000000. running mean: -5.590714\n",
      "ep 2074: ep_len:41 episode reward: total was 19.000000. running mean: -5.344807\n",
      "ep 2074: ep_len:806 episode reward: total was -26.110000. running mean: -5.552458\n",
      "ep 2074: ep_len:2839 episode reward: total was -6.360000. running mean: -5.560534\n",
      "ep 2074: ep_len:28 episode reward: total was 12.500000. running mean: -5.379929\n",
      "epsilon:0.009992 episode_count: 31241. steps_count: 33455154.000000\n",
      "ep 2075: ep_len:1007 episode reward: total was -154.730000. running mean: -6.873429\n",
      "ep 2075: ep_len:770 episode reward: total was -17.500000. running mean: -6.979695\n",
      "ep 2075: ep_len:78 episode reward: total was 36.000000. running mean: -6.549898\n",
      "ep 2075: ep_len:500 episode reward: total was 21.220000. running mean: -6.272199\n",
      "ep 2075: ep_len:53 episode reward: total was 25.000000. running mean: -5.959477\n",
      "ep 2075: ep_len:72 episode reward: total was 34.500000. running mean: -5.554882\n",
      "ep 2075: ep_len:500 episode reward: total was 25.390000. running mean: -5.245433\n",
      "ep 2075: ep_len:3703 episode reward: total was -129.040000. running mean: -6.483379\n",
      "ep 2075: ep_len:512 episode reward: total was -22.320000. running mean: -6.641745\n",
      "ep 2075: ep_len:629 episode reward: total was -0.950000. running mean: -6.584828\n",
      "ep 2075: ep_len:1071 episode reward: total was -5.620000. running mean: -6.575180\n",
      "ep 2075: ep_len:68 episode reward: total was 31.000000. running mean: -6.199428\n",
      "ep 2075: ep_len:44 episode reward: total was 20.500000. running mean: -5.932434\n",
      "ep 2075: ep_len:803 episode reward: total was -42.020000. running mean: -6.293309\n",
      "ep 2075: ep_len:2895 episode reward: total was -9.570000. running mean: -6.326076\n",
      "ep 2075: ep_len:66 episode reward: total was 31.500000. running mean: -5.947815\n",
      "epsilon:0.009992 episode_count: 31257. steps_count: 33467925.000000\n",
      "ep 2076: ep_len:1113 episode reward: total was -6.210000. running mean: -5.950437\n",
      "ep 2076: ep_len:796 episode reward: total was -13.050000. running mean: -6.021433\n",
      "ep 2076: ep_len:2826 episode reward: total was -9.030000. running mean: -6.051518\n",
      "ep 2076: ep_len:522 episode reward: total was -12.580000. running mean: -6.116803\n",
      "ep 2076: ep_len:1400 episode reward: total was -86.580000. running mean: -6.921435\n",
      "ep 2076: ep_len:3623 episode reward: total was -85.250000. running mean: -7.704721\n",
      "ep 2076: ep_len:2623 episode reward: total was -170.760000. running mean: -9.335274\n",
      "ep 2076: ep_len:782 episode reward: total was 27.810000. running mean: -8.963821\n",
      "ep 2076: ep_len:731 episode reward: total was -7.000000. running mean: -8.944183\n",
      "ep 2076: ep_len:778 episode reward: total was -124.700000. running mean: -10.101741\n",
      "ep 2076: ep_len:2929 episode reward: total was -49.320000. running mean: -10.493924\n",
      "epsilon:0.009992 episode_count: 31268. steps_count: 33486048.000000\n",
      "ep 2077: ep_len:1129 episode reward: total was 3.260000. running mean: -10.356384\n",
      "ep 2077: ep_len:954 episode reward: total was 22.300000. running mean: -10.029820\n",
      "ep 2077: ep_len:79 episode reward: total was 32.000000. running mean: -9.609522\n",
      "ep 2077: ep_len:2959 episode reward: total was -2259.610000. running mean: -32.109527\n",
      "ep 2077: ep_len:1244 episode reward: total was -43.280000. running mean: -32.221232\n",
      "ep 2077: ep_len:69 episode reward: total was 31.500000. running mean: -31.584019\n",
      "ep 2077: ep_len:87 episode reward: total was 42.000000. running mean: -30.848179\n",
      "ep 2077: ep_len:1428 episode reward: total was -116.630000. running mean: -31.705997\n",
      "ep 2077: ep_len:3546 episode reward: total was -15.840000. running mean: -31.547337\n",
      "ep 2077: ep_len:1240 episode reward: total was -85.740000. running mean: -32.089264\n",
      "ep 2077: ep_len:786 episode reward: total was 16.910000. running mean: -31.599271\n",
      "ep 2077: ep_len:604 episode reward: total was 13.540000. running mean: -31.147879\n",
      "ep 2077: ep_len:80 episode reward: total was 37.000000. running mean: -30.466400\n",
      "ep 2077: ep_len:635 episode reward: total was 13.380000. running mean: -30.027936\n",
      "ep 2077: ep_len:2862 episode reward: total was -10.850000. running mean: -29.836157\n",
      "epsilon:0.009992 episode_count: 31283. steps_count: 33503750.000000\n",
      "ep 2078: ep_len:959 episode reward: total was -62.830000. running mean: -30.166095\n",
      "ep 2078: ep_len:633 episode reward: total was -15.050000. running mean: -30.014934\n",
      "ep 2078: ep_len:2947 episode reward: total was -87.400000. running mean: -30.588785\n",
      "ep 2078: ep_len:1263 episode reward: total was -31.980000. running mean: -30.602697\n",
      "ep 2078: ep_len:1597 episode reward: total was -509.400000. running mean: -35.390670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2078: ep_len:319 episode reward: total was 18.830000. running mean: -34.848463\n",
      "ep 2078: ep_len:677 episode reward: total was -36.020000. running mean: -34.860179\n",
      "ep 2078: ep_len:852 episode reward: total was 50.310000. running mean: -34.008477\n",
      "ep 2078: ep_len:500 episode reward: total was 10.880000. running mean: -33.559592\n",
      "ep 2078: ep_len:52 episode reward: total was 23.000000. running mean: -32.993996\n",
      "ep 2078: ep_len:1521 episode reward: total was 23.730000. running mean: -32.426756\n",
      "ep 2078: ep_len:2915 episode reward: total was -18.790000. running mean: -32.290389\n",
      "ep 2078: ep_len:43 episode reward: total was 20.000000. running mean: -31.767485\n",
      "epsilon:0.009992 episode_count: 31296. steps_count: 33518028.000000\n",
      "ep 2079: ep_len:1058 episode reward: total was 2.390000. running mean: -31.425910\n",
      "ep 2079: ep_len:500 episode reward: total was 9.460000. running mean: -31.017051\n",
      "ep 2079: ep_len:3046 episode reward: total was -188.880000. running mean: -32.595680\n",
      "ep 2079: ep_len:832 episode reward: total was -212.890000. running mean: -34.398623\n",
      "ep 2079: ep_len:52 episode reward: total was 24.500000. running mean: -33.809637\n",
      "ep 2079: ep_len:171 episode reward: total was 84.000000. running mean: -32.631541\n",
      "ep 2079: ep_len:93 episode reward: total was 43.500000. running mean: -31.870225\n",
      "ep 2079: ep_len:500 episode reward: total was -6.730000. running mean: -31.618823\n",
      "ep 2079: ep_len:3609 episode reward: total was -193.890000. running mean: -33.241535\n",
      "ep 2079: ep_len:4180 episode reward: total was -929.010000. running mean: -42.199220\n",
      "ep 2079: ep_len:745 episode reward: total was 10.710000. running mean: -41.670127\n",
      "ep 2079: ep_len:500 episode reward: total was 29.950000. running mean: -40.953926\n",
      "ep 2079: ep_len:125 episode reward: total was 61.000000. running mean: -39.934387\n",
      "ep 2079: ep_len:500 episode reward: total was 20.000000. running mean: -39.335043\n",
      "ep 2079: ep_len:2911 episode reward: total was -55.800000. running mean: -39.499693\n",
      "ep 2079: ep_len:39 episode reward: total was 18.000000. running mean: -38.924696\n",
      "epsilon:0.009992 episode_count: 31312. steps_count: 33536889.000000\n",
      "ep 2080: ep_len:630 episode reward: total was -8.410000. running mean: -38.619549\n",
      "ep 2080: ep_len:201 episode reward: total was -10.910000. running mean: -38.342453\n",
      "ep 2080: ep_len:53 episode reward: total was 25.000000. running mean: -37.709029\n",
      "ep 2080: ep_len:2881 episode reward: total was -38.750000. running mean: -37.719438\n",
      "ep 2080: ep_len:500 episode reward: total was 21.930000. running mean: -37.122944\n",
      "ep 2080: ep_len:69 episode reward: total was 33.000000. running mean: -36.421715\n",
      "ep 2080: ep_len:104 episode reward: total was 47.500000. running mean: -35.582497\n",
      "ep 2080: ep_len:644 episode reward: total was -7.870000. running mean: -35.305372\n",
      "ep 2080: ep_len:3923 episode reward: total was -59.540000. running mean: -35.547719\n",
      "ep 2080: ep_len:511 episode reward: total was 9.810000. running mean: -35.094142\n",
      "ep 2080: ep_len:697 episode reward: total was 21.840000. running mean: -34.524800\n",
      "ep 2080: ep_len:887 episode reward: total was -0.540000. running mean: -34.184952\n",
      "ep 2080: ep_len:85 episode reward: total was 41.000000. running mean: -33.433103\n",
      "ep 2080: ep_len:1486 episode reward: total was 11.600000. running mean: -32.982772\n",
      "ep 2080: ep_len:2934 episode reward: total was -48.040000. running mean: -33.133344\n",
      "ep 2080: ep_len:38 episode reward: total was 17.500000. running mean: -32.627010\n",
      "epsilon:0.009992 episode_count: 31328. steps_count: 33552532.000000\n",
      "ep 2081: ep_len:500 episode reward: total was 7.960000. running mean: -32.221140\n",
      "ep 2081: ep_len:1655 episode reward: total was -55.670000. running mean: -32.455629\n",
      "ep 2081: ep_len:52 episode reward: total was 24.500000. running mean: -31.886073\n",
      "ep 2081: ep_len:3006 episode reward: total was -15.970000. running mean: -31.726912\n",
      "ep 2081: ep_len:852 episode reward: total was -2.150000. running mean: -31.431143\n",
      "ep 2081: ep_len:56 episode reward: total was 25.000000. running mean: -30.866831\n",
      "ep 2081: ep_len:1032 episode reward: total was -70.400000. running mean: -31.262163\n",
      "ep 2081: ep_len:4121 episode reward: total was -42.920000. running mean: -31.378741\n",
      "ep 2081: ep_len:548 episode reward: total was -27.010000. running mean: -31.335054\n",
      "ep 2081: ep_len:691 episode reward: total was 11.440000. running mean: -30.907303\n",
      "ep 2081: ep_len:739 episode reward: total was -15.000000. running mean: -30.748230\n",
      "ep 2081: ep_len:83 episode reward: total was 38.500000. running mean: -30.055748\n",
      "ep 2081: ep_len:500 episode reward: total was 12.950000. running mean: -29.625691\n",
      "ep 2081: ep_len:2831 episode reward: total was -6.260000. running mean: -29.392034\n",
      "ep 2081: ep_len:66 episode reward: total was 31.500000. running mean: -28.783113\n",
      "epsilon:0.009992 episode_count: 31343. steps_count: 33569264.000000\n",
      "ep 2082: ep_len:731 episode reward: total was -118.500000. running mean: -29.680282\n",
      "ep 2082: ep_len:984 episode reward: total was 9.030000. running mean: -29.293179\n",
      "ep 2082: ep_len:3022 episode reward: total was -67.610000. running mean: -29.676348\n",
      "ep 2082: ep_len:1686 episode reward: total was -487.380000. running mean: -34.253384\n",
      "ep 2082: ep_len:119 episode reward: total was 56.500000. running mean: -33.345850\n",
      "ep 2082: ep_len:78 episode reward: total was 36.000000. running mean: -32.652392\n",
      "ep 2082: ep_len:632 episode reward: total was -14.980000. running mean: -32.475668\n",
      "ep 2082: ep_len:4041 episode reward: total was -31.830000. running mean: -32.469211\n",
      "ep 2082: ep_len:1264 episode reward: total was -71.880000. running mean: -32.863319\n",
      "ep 2082: ep_len:739 episode reward: total was 38.860000. running mean: -32.146086\n",
      "ep 2082: ep_len:500 episode reward: total was 21.590000. running mean: -31.608725\n",
      "ep 2082: ep_len:165 episode reward: total was 79.500000. running mean: -30.497638\n",
      "ep 2082: ep_len:54 episode reward: total was 25.500000. running mean: -29.937661\n",
      "ep 2082: ep_len:97 episode reward: total was 45.500000. running mean: -29.183285\n",
      "ep 2082: ep_len:750 episode reward: total was -38.610000. running mean: -29.277552\n",
      "ep 2082: ep_len:2828 episode reward: total was -14.400000. running mean: -29.128776\n",
      "epsilon:0.009992 episode_count: 31359. steps_count: 33586954.000000\n",
      "ep 2083: ep_len:500 episode reward: total was 3.340000. running mean: -28.804089\n",
      "ep 2083: ep_len:1639 episode reward: total was -113.400000. running mean: -29.650048\n",
      "ep 2083: ep_len:2848 episode reward: total was -76.480000. running mean: -30.118347\n",
      "ep 2083: ep_len:504 episode reward: total was 25.920000. running mean: -29.557964\n",
      "ep 2083: ep_len:36 episode reward: total was 15.000000. running mean: -29.112384\n",
      "ep 2083: ep_len:1006 episode reward: total was -38.590000. running mean: -29.207160\n",
      "ep 2083: ep_len:3658 episode reward: total was -3447.570000. running mean: -63.390789\n",
      "ep 2083: ep_len:939 episode reward: total was -31.080000. running mean: -63.067681\n",
      "ep 2083: ep_len:771 episode reward: total was 42.710000. running mean: -62.009904\n",
      "ep 2083: ep_len:500 episode reward: total was -2.540000. running mean: -61.415205\n",
      "ep 2083: ep_len:59 episode reward: total was 23.010000. running mean: -60.570953\n",
      "ep 2083: ep_len:37 episode reward: total was 17.000000. running mean: -59.795243\n",
      "ep 2083: ep_len:606 episode reward: total was 6.680000. running mean: -59.130491\n",
      "ep 2083: ep_len:2812 episode reward: total was -2.530000. running mean: -58.564486\n",
      "epsilon:0.009992 episode_count: 31373. steps_count: 33602869.000000\n",
      "ep 2084: ep_len:936 episode reward: total was -46.720000. running mean: -58.446041\n",
      "ep 2084: ep_len:656 episode reward: total was -46.130000. running mean: -58.322881\n",
      "ep 2084: ep_len:69 episode reward: total was 33.000000. running mean: -57.409652\n",
      "ep 2084: ep_len:3047 episode reward: total was -40.380000. running mean: -57.239355\n",
      "ep 2084: ep_len:643 episode reward: total was 8.730000. running mean: -56.579662\n",
      "ep 2084: ep_len:56 episode reward: total was 25.000000. running mean: -55.763865\n",
      "ep 2084: ep_len:73 episode reward: total was 35.000000. running mean: -54.856227\n",
      "ep 2084: ep_len:1032 episode reward: total was -40.990000. running mean: -54.717564\n",
      "ep 2084: ep_len:3853 episode reward: total was -43.560000. running mean: -54.605989\n",
      "ep 2084: ep_len:1267 episode reward: total was -97.620000. running mean: -55.036129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2084: ep_len:786 episode reward: total was 5.560000. running mean: -54.430168\n",
      "ep 2084: ep_len:980 episode reward: total was -185.150000. running mean: -55.737366\n",
      "ep 2084: ep_len:1481 episode reward: total was 2.030000. running mean: -55.159692\n",
      "ep 2084: ep_len:46 episode reward: total was 20.000000. running mean: -54.408095\n",
      "ep 2084: ep_len:44 episode reward: total was 20.500000. running mean: -53.659014\n",
      "epsilon:0.009992 episode_count: 31388. steps_count: 33617838.000000\n",
      "ep 2085: ep_len:670 episode reward: total was -10.200000. running mean: -53.224424\n",
      "ep 2085: ep_len:945 episode reward: total was 14.240000. running mean: -52.549780\n",
      "ep 2085: ep_len:3041 episode reward: total was -47.020000. running mean: -52.494482\n",
      "ep 2085: ep_len:661 episode reward: total was -17.800000. running mean: -52.147537\n",
      "ep 2085: ep_len:66 episode reward: total was 31.500000. running mean: -51.311062\n",
      "ep 2085: ep_len:1008 episode reward: total was -85.030000. running mean: -51.648251\n",
      "ep 2085: ep_len:4010 episode reward: total was -92.920000. running mean: -52.060969\n",
      "ep 2085: ep_len:582 episode reward: total was 15.030000. running mean: -51.390059\n",
      "ep 2085: ep_len:661 episode reward: total was 6.600000. running mean: -50.810159\n",
      "ep 2085: ep_len:678 episode reward: total was -24.280000. running mean: -50.544857\n",
      "ep 2085: ep_len:145 episode reward: total was 68.000000. running mean: -49.359408\n",
      "ep 2085: ep_len:55 episode reward: total was 26.000000. running mean: -48.605814\n",
      "ep 2085: ep_len:672 episode reward: total was -12.640000. running mean: -48.246156\n",
      "ep 2085: ep_len:2877 episode reward: total was 4.060000. running mean: -47.723095\n",
      "epsilon:0.009992 episode_count: 31402. steps_count: 33633909.000000\n",
      "ep 2086: ep_len:866 episode reward: total was -4.530000. running mean: -47.291164\n",
      "ep 2086: ep_len:935 episode reward: total was 13.990000. running mean: -46.678352\n",
      "ep 2086: ep_len:3001 episode reward: total was -36.900000. running mean: -46.580569\n",
      "ep 2086: ep_len:500 episode reward: total was 37.140000. running mean: -45.743363\n",
      "ep 2086: ep_len:55 episode reward: total was 24.500000. running mean: -45.040929\n",
      "ep 2086: ep_len:125 episode reward: total was 59.500000. running mean: -43.995520\n",
      "ep 2086: ep_len:68 episode reward: total was 31.000000. running mean: -43.245565\n",
      "ep 2086: ep_len:48 episode reward: total was 21.000000. running mean: -42.603109\n",
      "ep 2086: ep_len:1449 episode reward: total was 6.090000. running mean: -42.116178\n",
      "ep 2086: ep_len:665 episode reward: total was 21.180000. running mean: -41.483216\n",
      "ep 2086: ep_len:1180 episode reward: total was -100.480000. running mean: -42.073184\n",
      "ep 2086: ep_len:895 episode reward: total was 40.710000. running mean: -41.245352\n",
      "ep 2086: ep_len:688 episode reward: total was -22.030000. running mean: -41.053199\n",
      "ep 2086: ep_len:50 episode reward: total was 23.500000. running mean: -40.407667\n",
      "ep 2086: ep_len:154 episode reward: total was 74.000000. running mean: -39.263590\n",
      "ep 2086: ep_len:54 episode reward: total was 25.500000. running mean: -38.615954\n",
      "ep 2086: ep_len:1145 episode reward: total was -6.980000. running mean: -38.299595\n",
      "ep 2086: ep_len:2795 episode reward: total was -38.020000. running mean: -38.296799\n",
      "ep 2086: ep_len:60 episode reward: total was 28.500000. running mean: -37.628831\n",
      "epsilon:0.009992 episode_count: 31421. steps_count: 33648642.000000\n",
      "ep 2087: ep_len:638 episode reward: total was -26.110000. running mean: -37.513642\n",
      "ep 2087: ep_len:1239 episode reward: total was -50.400000. running mean: -37.642506\n",
      "ep 2087: ep_len:49 episode reward: total was 21.500000. running mean: -37.051081\n",
      "ep 2087: ep_len:3039 episode reward: total was -84.310000. running mean: -37.523670\n",
      "ep 2087: ep_len:680 episode reward: total was 8.370000. running mean: -37.064733\n",
      "ep 2087: ep_len:119 episode reward: total was 58.000000. running mean: -36.114086\n",
      "ep 2087: ep_len:48 episode reward: total was 22.500000. running mean: -35.527945\n",
      "ep 2087: ep_len:1039 episode reward: total was -36.240000. running mean: -35.535066\n",
      "ep 2087: ep_len:4232 episode reward: total was -1327.770000. running mean: -48.457415\n",
      "ep 2087: ep_len:676 episode reward: total was -24.720000. running mean: -48.220041\n",
      "ep 2087: ep_len:709 episode reward: total was 25.640000. running mean: -47.481440\n",
      "ep 2087: ep_len:695 episode reward: total was -11.490000. running mean: -47.121526\n",
      "ep 2087: ep_len:89 episode reward: total was 43.000000. running mean: -46.220311\n",
      "ep 2087: ep_len:1006 episode reward: total was -51.510000. running mean: -46.273208\n",
      "ep 2087: ep_len:2909 episode reward: total was -0.770000. running mean: -45.818176\n",
      "ep 2087: ep_len:45 episode reward: total was 21.000000. running mean: -45.149994\n",
      "epsilon:0.009992 episode_count: 31437. steps_count: 33665854.000000\n",
      "ep 2088: ep_len:1116 episode reward: total was -8.930000. running mean: -44.787794\n",
      "ep 2088: ep_len:959 episode reward: total was -15.980000. running mean: -44.499716\n",
      "ep 2088: ep_len:2976 episode reward: total was -98.090000. running mean: -45.035619\n",
      "ep 2088: ep_len:500 episode reward: total was 23.360000. running mean: -44.351663\n",
      "ep 2088: ep_len:84 episode reward: total was 40.500000. running mean: -43.503146\n",
      "ep 2088: ep_len:72 episode reward: total was 30.000000. running mean: -42.768115\n",
      "ep 2088: ep_len:1081 episode reward: total was -45.920000. running mean: -42.799633\n",
      "ep 2088: ep_len:642 episode reward: total was 30.500000. running mean: -42.066637\n",
      "ep 2088: ep_len:563 episode reward: total was 15.110000. running mean: -41.494871\n",
      "ep 2088: ep_len:684 episode reward: total was 44.740000. running mean: -40.632522\n",
      "ep 2088: ep_len:606 episode reward: total was -22.340000. running mean: -40.449597\n",
      "ep 2088: ep_len:97 episode reward: total was 47.000000. running mean: -39.575101\n",
      "ep 2088: ep_len:623 episode reward: total was -16.220000. running mean: -39.341550\n",
      "ep 2088: ep_len:2845 episode reward: total was -25.740000. running mean: -39.205534\n",
      "epsilon:0.009992 episode_count: 31451. steps_count: 33678702.000000\n",
      "ep 2089: ep_len:926 episode reward: total was -41.100000. running mean: -39.224479\n",
      "ep 2089: ep_len:500 episode reward: total was -0.300000. running mean: -38.835234\n",
      "ep 2089: ep_len:2870 episode reward: total was -86.120000. running mean: -39.308082\n",
      "ep 2089: ep_len:500 episode reward: total was 11.240000. running mean: -38.802601\n",
      "ep 2089: ep_len:37 episode reward: total was 17.000000. running mean: -38.244575\n",
      "ep 2089: ep_len:159 episode reward: total was 78.000000. running mean: -37.082129\n",
      "ep 2089: ep_len:82 episode reward: total was 35.000000. running mean: -36.361308\n",
      "ep 2089: ep_len:53 episode reward: total was 25.000000. running mean: -35.747695\n",
      "ep 2089: ep_len:607 episode reward: total was -14.650000. running mean: -35.536718\n",
      "ep 2089: ep_len:676 episode reward: total was 12.470000. running mean: -35.056651\n",
      "ep 2089: ep_len:1277 episode reward: total was -61.740000. running mean: -35.323484\n",
      "ep 2089: ep_len:715 episode reward: total was 26.920000. running mean: -34.701049\n",
      "ep 2089: ep_len:944 episode reward: total was 4.870000. running mean: -34.305339\n",
      "ep 2089: ep_len:124 episode reward: total was 60.500000. running mean: -33.357286\n",
      "ep 2089: ep_len:716 episode reward: total was -6.140000. running mean: -33.085113\n",
      "ep 2089: ep_len:2780 episode reward: total was -6.860000. running mean: -32.822862\n",
      "epsilon:0.009992 episode_count: 31467. steps_count: 33691668.000000\n",
      "ep 2090: ep_len:1099 episode reward: total was -23.520000. running mean: -32.729833\n",
      "ep 2090: ep_len:1284 episode reward: total was -70.150000. running mean: -33.104035\n",
      "ep 2090: ep_len:3010 episode reward: total was -34.110000. running mean: -33.114094\n",
      "ep 2090: ep_len:722 episode reward: total was -8.940000. running mean: -32.872353\n",
      "ep 2090: ep_len:170 episode reward: total was 82.000000. running mean: -31.723630\n",
      "ep 2090: ep_len:96 episode reward: total was 43.500000. running mean: -30.971393\n",
      "ep 2090: ep_len:1728 episode reward: total was -452.450000. running mean: -35.186180\n",
      "ep 2090: ep_len:351 episode reward: total was 17.650000. running mean: -34.657818\n",
      "ep 2090: ep_len:725 episode reward: total was -22.620000. running mean: -34.537440\n",
      "ep 2090: ep_len:695 episode reward: total was 18.910000. running mean: -34.002965\n",
      "ep 2090: ep_len:726 episode reward: total was -11.090000. running mean: -33.773836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2090: ep_len:73 episode reward: total was 33.500000. running mean: -33.101097\n",
      "ep 2090: ep_len:777 episode reward: total was -4.720000. running mean: -32.817286\n",
      "ep 2090: ep_len:2807 episode reward: total was -40.200000. running mean: -32.891113\n",
      "epsilon:0.009992 episode_count: 31481. steps_count: 33705931.000000\n",
      "ep 2091: ep_len:614 episode reward: total was -6.150000. running mean: -32.623702\n",
      "ep 2091: ep_len:184 episode reward: total was 5.600000. running mean: -32.241465\n",
      "ep 2091: ep_len:44 episode reward: total was 17.500000. running mean: -31.744051\n",
      "ep 2091: ep_len:2995 episode reward: total was -130.170000. running mean: -32.728310\n",
      "ep 2091: ep_len:1218 episode reward: total was -31.570000. running mean: -32.716727\n",
      "ep 2091: ep_len:111 episode reward: total was 54.000000. running mean: -31.849560\n",
      "ep 2091: ep_len:1032 episode reward: total was -31.260000. running mean: -31.843664\n",
      "ep 2091: ep_len:3821 episode reward: total was -124.290000. running mean: -32.768127\n",
      "ep 2091: ep_len:818 episode reward: total was -21.130000. running mean: -32.651746\n",
      "ep 2091: ep_len:693 episode reward: total was 16.790000. running mean: -32.157329\n",
      "ep 2091: ep_len:1034 episode reward: total was 1.330000. running mean: -31.822455\n",
      "ep 2091: ep_len:71 episode reward: total was 34.000000. running mean: -31.164231\n",
      "ep 2091: ep_len:148 episode reward: total was 69.500000. running mean: -30.157589\n",
      "ep 2091: ep_len:39 episode reward: total was 18.000000. running mean: -29.676013\n",
      "ep 2091: ep_len:1144 episode reward: total was -15.970000. running mean: -29.538953\n",
      "ep 2091: ep_len:2812 episode reward: total was -41.660000. running mean: -29.660163\n",
      "ep 2091: ep_len:46 episode reward: total was 21.500000. running mean: -29.148561\n",
      "epsilon:0.009992 episode_count: 31498. steps_count: 33722755.000000\n",
      "ep 2092: ep_len:1366 episode reward: total was 18.600000. running mean: -28.671076\n",
      "ep 2092: ep_len:949 episode reward: total was 2.040000. running mean: -28.363965\n",
      "ep 2092: ep_len:2961 episode reward: total was -60.350000. running mean: -28.683825\n",
      "ep 2092: ep_len:644 episode reward: total was -4.420000. running mean: -28.441187\n",
      "ep 2092: ep_len:76 episode reward: total was 35.000000. running mean: -27.806775\n",
      "ep 2092: ep_len:50 episode reward: total was 23.500000. running mean: -27.293707\n",
      "ep 2092: ep_len:1495 episode reward: total was 27.400000. running mean: -26.746770\n",
      "ep 2092: ep_len:3616 episode reward: total was -74.730000. running mean: -27.226603\n",
      "ep 2092: ep_len:538 episode reward: total was 0.160000. running mean: -26.952737\n",
      "ep 2092: ep_len:666 episode reward: total was 16.910000. running mean: -26.514109\n",
      "ep 2092: ep_len:500 episode reward: total was 24.380000. running mean: -26.005168\n",
      "ep 2092: ep_len:93 episode reward: total was 43.500000. running mean: -25.310117\n",
      "ep 2092: ep_len:176 episode reward: total was 85.000000. running mean: -24.207015\n",
      "ep 2092: ep_len:53 episode reward: total was 23.500000. running mean: -23.729945\n",
      "ep 2092: ep_len:561 episode reward: total was 7.450000. running mean: -23.418146\n",
      "ep 2092: ep_len:2864 episode reward: total was 6.860000. running mean: -23.115364\n",
      "ep 2092: ep_len:69 episode reward: total was 33.000000. running mean: -22.554211\n",
      "epsilon:0.009992 episode_count: 31515. steps_count: 33739432.000000\n",
      "ep 2093: ep_len:949 episode reward: total was -42.200000. running mean: -22.750669\n",
      "ep 2093: ep_len:940 episode reward: total was 0.050000. running mean: -22.522662\n",
      "ep 2093: ep_len:3079 episode reward: total was -52.820000. running mean: -22.825635\n",
      "ep 2093: ep_len:647 episode reward: total was 26.410000. running mean: -22.333279\n",
      "ep 2093: ep_len:54 episode reward: total was 25.500000. running mean: -21.854946\n",
      "ep 2093: ep_len:500 episode reward: total was -2.080000. running mean: -21.657197\n",
      "ep 2093: ep_len:3637 episode reward: total was -28.060000. running mean: -21.721225\n",
      "ep 2093: ep_len:550 episode reward: total was 4.110000. running mean: -21.462912\n",
      "ep 2093: ep_len:893 episode reward: total was 61.200000. running mean: -20.636283\n",
      "ep 2093: ep_len:536 episode reward: total was 22.780000. running mean: -20.202120\n",
      "ep 2093: ep_len:69 episode reward: total was 33.000000. running mean: -19.670099\n",
      "ep 2093: ep_len:1525 episode reward: total was 1.150000. running mean: -19.461898\n",
      "ep 2093: ep_len:2886 episode reward: total was -10.610000. running mean: -19.373379\n",
      "epsilon:0.009992 episode_count: 31528. steps_count: 33755697.000000\n",
      "ep 2094: ep_len:1434 episode reward: total was 16.710000. running mean: -19.012545\n",
      "ep 2094: ep_len:694 episode reward: total was -39.310000. running mean: -19.215520\n",
      "ep 2094: ep_len:58 episode reward: total was 27.500000. running mean: -18.748365\n",
      "ep 2094: ep_len:101 episode reward: total was 46.000000. running mean: -18.100881\n",
      "ep 2094: ep_len:1679 episode reward: total was -86.370000. running mean: -18.783572\n",
      "ep 2094: ep_len:70 episode reward: total was 32.000000. running mean: -18.275737\n",
      "ep 2094: ep_len:599 episode reward: total was 9.010000. running mean: -18.002879\n",
      "ep 2094: ep_len:3594 episode reward: total was -13.400000. running mean: -17.956850\n",
      "ep 2094: ep_len:1658 episode reward: total was -110.160000. running mean: -18.878882\n",
      "ep 2094: ep_len:879 episode reward: total was 38.530000. running mean: -18.304793\n",
      "ep 2094: ep_len:1523 episode reward: total was 9.090000. running mean: -18.030845\n",
      "ep 2094: ep_len:89 episode reward: total was 43.000000. running mean: -17.420537\n",
      "ep 2094: ep_len:185 episode reward: total was 89.500000. running mean: -16.351331\n",
      "ep 2094: ep_len:982 episode reward: total was -22.460000. running mean: -16.412418\n",
      "ep 2094: ep_len:2816 episode reward: total was -1.020000. running mean: -16.258494\n",
      "ep 2094: ep_len:45 episode reward: total was 21.000000. running mean: -15.885909\n",
      "epsilon:0.009992 episode_count: 31544. steps_count: 33772103.000000\n",
      "ep 2095: ep_len:1158 episode reward: total was 8.020000. running mean: -15.646850\n",
      "ep 2095: ep_len:966 episode reward: total was -1.000000. running mean: -15.500381\n",
      "ep 2095: ep_len:64 episode reward: total was 30.500000. running mean: -15.040378\n",
      "ep 2095: ep_len:2895 episode reward: total was -63.800000. running mean: -15.527974\n",
      "ep 2095: ep_len:670 episode reward: total was 0.860000. running mean: -15.364094\n",
      "ep 2095: ep_len:57 episode reward: total was 27.000000. running mean: -14.940453\n",
      "ep 2095: ep_len:62 episode reward: total was 29.500000. running mean: -14.496049\n",
      "ep 2095: ep_len:1007 episode reward: total was -111.300000. running mean: -15.464088\n",
      "ep 2095: ep_len:3902 episode reward: total was -115.790000. running mean: -16.467347\n",
      "ep 2095: ep_len:649 episode reward: total was -140.990000. running mean: -17.712574\n",
      "ep 2095: ep_len:786 episode reward: total was 12.440000. running mean: -17.411048\n",
      "ep 2095: ep_len:515 episode reward: total was -1.780000. running mean: -17.254738\n",
      "ep 2095: ep_len:43 episode reward: total was 18.500000. running mean: -16.897190\n",
      "ep 2095: ep_len:90 episode reward: total was 42.000000. running mean: -16.308218\n",
      "ep 2095: ep_len:648 episode reward: total was -2.010000. running mean: -16.165236\n",
      "ep 2095: ep_len:2926 episode reward: total was 3.390000. running mean: -15.969684\n",
      "epsilon:0.009992 episode_count: 31560. steps_count: 33788541.000000\n",
      "ep 2096: ep_len:834 episode reward: total was -8.250000. running mean: -15.892487\n",
      "ep 2096: ep_len:3179 episode reward: total was -353.040000. running mean: -19.263962\n",
      "ep 2096: ep_len:55 episode reward: total was 26.000000. running mean: -18.811322\n",
      "ep 2096: ep_len:3100 episode reward: total was -21.390000. running mean: -18.837109\n",
      "ep 2096: ep_len:645 episode reward: total was -2.300000. running mean: -18.671738\n",
      "ep 2096: ep_len:103 episode reward: total was 48.500000. running mean: -18.000021\n",
      "ep 2096: ep_len:805 episode reward: total was 12.200000. running mean: -17.698020\n",
      "ep 2096: ep_len:641 episode reward: total was 17.360000. running mean: -17.347440\n",
      "ep 2096: ep_len:779 episode reward: total was -5.950000. running mean: -17.233466\n",
      "ep 2096: ep_len:864 episode reward: total was 51.600000. running mean: -16.545131\n",
      "ep 2096: ep_len:1139 episode reward: total was -8.980000. running mean: -16.469480\n",
      "ep 2096: ep_len:80 episode reward: total was 38.500000. running mean: -15.919785\n",
      "ep 2096: ep_len:134 episode reward: total was 64.000000. running mean: -15.120587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2096: ep_len:500 episode reward: total was 37.480000. running mean: -14.594581\n",
      "ep 2096: ep_len:2774 episode reward: total was -3.310000. running mean: -14.481736\n",
      "epsilon:0.009992 episode_count: 31575. steps_count: 33804173.000000\n",
      "ep 2097: ep_len:1089 episode reward: total was -8.470000. running mean: -14.421618\n",
      "ep 2097: ep_len:738 episode reward: total was -10.450000. running mean: -14.381902\n",
      "ep 2097: ep_len:2977 episode reward: total was -16.880000. running mean: -14.406883\n",
      "ep 2097: ep_len:1254 episode reward: total was -3.960000. running mean: -14.302414\n",
      "ep 2097: ep_len:44 episode reward: total was 19.000000. running mean: -13.969390\n",
      "ep 2097: ep_len:662 episode reward: total was 4.700000. running mean: -13.782696\n",
      "ep 2097: ep_len:324 episode reward: total was 10.980000. running mean: -13.535069\n",
      "ep 2097: ep_len:550 episode reward: total was -2.750000. running mean: -13.427218\n",
      "ep 2097: ep_len:701 episode reward: total was 42.370000. running mean: -12.869246\n",
      "ep 2097: ep_len:630 episode reward: total was 14.450000. running mean: -12.596054\n",
      "ep 2097: ep_len:1133 episode reward: total was -12.070000. running mean: -12.590793\n",
      "ep 2097: ep_len:2801 episode reward: total was -9.900000. running mean: -12.563885\n",
      "epsilon:0.009992 episode_count: 31587. steps_count: 33817076.000000\n",
      "ep 2098: ep_len:944 episode reward: total was -38.840000. running mean: -12.826647\n",
      "ep 2098: ep_len:500 episode reward: total was 4.380000. running mean: -12.654580\n",
      "ep 2098: ep_len:2913 episode reward: total was -16.470000. running mean: -12.692734\n",
      "ep 2098: ep_len:623 episode reward: total was -10.080000. running mean: -12.666607\n",
      "ep 2098: ep_len:51 episode reward: total was 24.000000. running mean: -12.299941\n",
      "ep 2098: ep_len:107 episode reward: total was 52.000000. running mean: -11.656941\n",
      "ep 2098: ep_len:74 episode reward: total was 34.000000. running mean: -11.200372\n",
      "ep 2098: ep_len:1397 episode reward: total was 1.000000. running mean: -11.078368\n",
      "ep 2098: ep_len:3868 episode reward: total was -41.280000. running mean: -11.380385\n",
      "ep 2098: ep_len:562 episode reward: total was 29.450000. running mean: -10.972081\n",
      "ep 2098: ep_len:686 episode reward: total was 37.590000. running mean: -10.486460\n",
      "ep 2098: ep_len:1063 episode reward: total was -16.170000. running mean: -10.543295\n",
      "ep 2098: ep_len:77 episode reward: total was 32.500000. running mean: -10.112862\n",
      "ep 2098: ep_len:1124 episode reward: total was -6.100000. running mean: -10.072734\n",
      "ep 2098: ep_len:2795 episode reward: total was -26.970000. running mean: -10.241706\n",
      "epsilon:0.009992 episode_count: 31602. steps_count: 33833860.000000\n",
      "ep 2099: ep_len:1181 episode reward: total was -10.020000. running mean: -10.239489\n",
      "ep 2099: ep_len:690 episode reward: total was -38.720000. running mean: -10.524294\n",
      "ep 2099: ep_len:2891 episode reward: total was -29.870000. running mean: -10.717752\n",
      "ep 2099: ep_len:4425 episode reward: total was -961.590000. running mean: -20.226474\n",
      "ep 2099: ep_len:74 episode reward: total was 35.500000. running mean: -19.669209\n",
      "ep 2099: ep_len:1471 episode reward: total was -257.150000. running mean: -22.044017\n",
      "ep 2099: ep_len:3928 episode reward: total was -2657.800000. running mean: -48.401577\n",
      "ep 2099: ep_len:1280 episode reward: total was -43.310000. running mean: -48.350661\n",
      "ep 2099: ep_len:782 episode reward: total was 24.530000. running mean: -47.621855\n",
      "ep 2099: ep_len:1451 episode reward: total was 17.890000. running mean: -46.966736\n",
      "ep 2099: ep_len:71 episode reward: total was 32.500000. running mean: -46.172069\n",
      "ep 2099: ep_len:208 episode reward: total was 103.510000. running mean: -44.675248\n",
      "ep 2099: ep_len:86 episode reward: total was 41.500000. running mean: -43.813496\n",
      "ep 2099: ep_len:1455 episode reward: total was 22.370000. running mean: -43.151661\n",
      "ep 2099: ep_len:40 episode reward: total was 18.500000. running mean: -42.535144\n",
      "epsilon:0.009992 episode_count: 31617. steps_count: 33853893.000000\n",
      "ep 2100: ep_len:1117 episode reward: total was -3.170000. running mean: -42.141493\n",
      "ep 2100: ep_len:500 episode reward: total was 18.310000. running mean: -41.536978\n",
      "ep 2100: ep_len:2845 episode reward: total was -26.740000. running mean: -41.389008\n",
      "ep 2100: ep_len:631 episode reward: total was -1.880000. running mean: -40.993918\n",
      "ep 2100: ep_len:154 episode reward: total was 71.000000. running mean: -39.873979\n",
      "ep 2100: ep_len:899 episode reward: total was 32.400000. running mean: -39.151239\n",
      "ep 2100: ep_len:3588 episode reward: total was -36.080000. running mean: -39.120526\n",
      "ep 2100: ep_len:806 episode reward: total was -14.910000. running mean: -38.878421\n",
      "ep 2100: ep_len:835 episode reward: total was 51.090000. running mean: -37.978737\n",
      "ep 2100: ep_len:1098 episode reward: total was 3.190000. running mean: -37.567050\n",
      "ep 2100: ep_len:164 episode reward: total was 79.000000. running mean: -36.401379\n",
      "ep 2100: ep_len:37 episode reward: total was 17.000000. running mean: -35.867365\n",
      "ep 2100: ep_len:1461 episode reward: total was -0.770000. running mean: -35.516392\n",
      "ep 2100: ep_len:2765 episode reward: total was -13.250000. running mean: -35.293728\n",
      "ep 2100: ep_len:56 episode reward: total was 25.000000. running mean: -34.690790\n",
      "epsilon:0.009992 episode_count: 31632. steps_count: 33870849.000000\n",
      "ep 2101: ep_len:1477 episode reward: total was 22.250000. running mean: -34.121383\n",
      "ep 2101: ep_len:500 episode reward: total was -8.800000. running mean: -33.868169\n",
      "ep 2101: ep_len:46 episode reward: total was 21.500000. running mean: -33.314487\n",
      "ep 2101: ep_len:2956 episode reward: total was -1.880000. running mean: -33.000142\n",
      "ep 2101: ep_len:779 episode reward: total was -8.510000. running mean: -32.755241\n",
      "ep 2101: ep_len:962 episode reward: total was -40.410000. running mean: -32.831788\n",
      "ep 2101: ep_len:326 episode reward: total was 13.850000. running mean: -32.364970\n",
      "ep 2101: ep_len:647 episode reward: total was -17.940000. running mean: -32.220721\n",
      "ep 2101: ep_len:627 episode reward: total was -9.940000. running mean: -31.997914\n",
      "ep 2101: ep_len:757 episode reward: total was 15.550000. running mean: -31.522434\n",
      "ep 2101: ep_len:54 episode reward: total was 25.500000. running mean: -30.952210\n",
      "ep 2101: ep_len:38 episode reward: total was 17.500000. running mean: -30.467688\n",
      "ep 2101: ep_len:71 episode reward: total was 34.000000. running mean: -29.823011\n",
      "ep 2101: ep_len:539 episode reward: total was -113.300000. running mean: -30.657781\n",
      "ep 2101: ep_len:2916 episode reward: total was 5.980000. running mean: -30.291403\n",
      "epsilon:0.009992 episode_count: 31647. steps_count: 33883544.000000\n",
      "ep 2102: ep_len:1451 episode reward: total was 14.770000. running mean: -29.840789\n",
      "ep 2102: ep_len:1261 episode reward: total was -61.290000. running mean: -30.155281\n",
      "ep 2102: ep_len:2847 episode reward: total was -22.320000. running mean: -30.076928\n",
      "ep 2102: ep_len:612 episode reward: total was -0.840000. running mean: -29.784559\n",
      "ep 2102: ep_len:55 episode reward: total was 24.500000. running mean: -29.241714\n",
      "ep 2102: ep_len:70 episode reward: total was 33.500000. running mean: -28.614296\n",
      "ep 2102: ep_len:1454 episode reward: total was -107.840000. running mean: -29.406553\n",
      "ep 2102: ep_len:364 episode reward: total was 9.180000. running mean: -29.020688\n",
      "ep 2102: ep_len:879 episode reward: total was 2.560000. running mean: -28.704881\n",
      "ep 2102: ep_len:776 episode reward: total was -5.220000. running mean: -28.470032\n",
      "ep 2102: ep_len:1127 episode reward: total was -6.070000. running mean: -28.246032\n",
      "ep 2102: ep_len:87 episode reward: total was 42.000000. running mean: -27.543572\n",
      "ep 2102: ep_len:35 episode reward: total was 16.000000. running mean: -27.108136\n",
      "ep 2102: ep_len:667 episode reward: total was 13.250000. running mean: -26.704555\n",
      "ep 2102: ep_len:2860 episode reward: total was -21.520000. running mean: -26.652709\n",
      "ep 2102: ep_len:39 episode reward: total was 18.000000. running mean: -26.206182\n",
      "epsilon:0.009992 episode_count: 31663. steps_count: 33898128.000000\n",
      "ep 2103: ep_len:909 episode reward: total was -311.100000. running mean: -29.055120\n",
      "ep 2103: ep_len:950 episode reward: total was 25.860000. running mean: -28.505969\n",
      "ep 2103: ep_len:2887 episode reward: total was -5.910000. running mean: -28.280009\n",
      "ep 2103: ep_len:798 episode reward: total was -11.500000. running mean: -28.112209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2103: ep_len:500 episode reward: total was 27.660000. running mean: -27.554487\n",
      "ep 2103: ep_len:653 episode reward: total was 27.580000. running mean: -27.003142\n",
      "ep 2103: ep_len:607 episode reward: total was -52.920000. running mean: -27.262311\n",
      "ep 2103: ep_len:730 episode reward: total was 0.180000. running mean: -26.987888\n",
      "ep 2103: ep_len:725 episode reward: total was -10.090000. running mean: -26.818909\n",
      "ep 2103: ep_len:55 episode reward: total was 26.000000. running mean: -26.290720\n",
      "ep 2103: ep_len:1527 episode reward: total was 10.450000. running mean: -25.923312\n",
      "ep 2103: ep_len:2872 episode reward: total was -32.140000. running mean: -25.985479\n",
      "ep 2103: ep_len:58 episode reward: total was 27.500000. running mean: -25.450625\n",
      "epsilon:0.009992 episode_count: 31676. steps_count: 33911399.000000\n",
      "ep 2104: ep_len:5014 episode reward: total was -3929.860000. running mean: -64.494718\n",
      "ep 2104: ep_len:637 episode reward: total was -1.180000. running mean: -63.861571\n",
      "ep 2104: ep_len:65 episode reward: total was 29.500000. running mean: -62.927955\n",
      "ep 2104: ep_len:2963 episode reward: total was -42.710000. running mean: -62.725776\n",
      "ep 2104: ep_len:670 episode reward: total was -8.000000. running mean: -62.178518\n",
      "ep 2104: ep_len:59 episode reward: total was 28.000000. running mean: -61.276733\n",
      "ep 2104: ep_len:500 episode reward: total was 31.360000. running mean: -60.350366\n",
      "ep 2104: ep_len:3948 episode reward: total was -658.220000. running mean: -66.329062\n",
      "ep 2104: ep_len:3905 episode reward: total was -1113.490000. running mean: -76.800671\n",
      "ep 2104: ep_len:722 episode reward: total was 9.380000. running mean: -75.938865\n",
      "ep 2104: ep_len:780 episode reward: total was -9.020000. running mean: -75.269676\n",
      "ep 2104: ep_len:164 episode reward: total was 76.000000. running mean: -73.756979\n",
      "ep 2104: ep_len:65 episode reward: total was 29.500000. running mean: -72.724409\n",
      "ep 2104: ep_len:1416 episode reward: total was 25.040000. running mean: -71.746765\n",
      "ep 2104: ep_len:2868 episode reward: total was 1.920000. running mean: -71.010098\n",
      "epsilon:0.009992 episode_count: 31691. steps_count: 33935175.000000\n",
      "ep 2105: ep_len:573 episode reward: total was -0.020000. running mean: -70.300197\n",
      "ep 2105: ep_len:923 episode reward: total was 13.380000. running mean: -69.463395\n",
      "ep 2105: ep_len:57 episode reward: total was 24.000000. running mean: -68.528761\n",
      "ep 2105: ep_len:2978 episode reward: total was -84.980000. running mean: -68.693273\n",
      "ep 2105: ep_len:634 episode reward: total was 4.880000. running mean: -67.957540\n",
      "ep 2105: ep_len:95 episode reward: total was 46.000000. running mean: -66.817965\n",
      "ep 2105: ep_len:70 episode reward: total was 32.000000. running mean: -65.829785\n",
      "ep 2105: ep_len:500 episode reward: total was 4.760000. running mean: -65.123887\n",
      "ep 2105: ep_len:634 episode reward: total was 27.450000. running mean: -64.198149\n",
      "ep 2105: ep_len:747 episode reward: total was -56.470000. running mean: -64.120867\n",
      "ep 2105: ep_len:7494 episode reward: total was -238.740000. running mean: -65.867058\n",
      "ep 2105: ep_len:1469 episode reward: total was 18.130000. running mean: -65.027088\n",
      "ep 2105: ep_len:1485 episode reward: total was 9.780000. running mean: -64.279017\n",
      "ep 2105: ep_len:2833 episode reward: total was 3.030000. running mean: -63.605927\n",
      "epsilon:0.009992 episode_count: 31705. steps_count: 33955667.000000\n",
      "ep 2106: ep_len:997 episode reward: total was -78.070000. running mean: -63.750568\n",
      "ep 2106: ep_len:789 episode reward: total was -11.100000. running mean: -63.224062\n",
      "ep 2106: ep_len:2983 episode reward: total was -41.360000. running mean: -63.005421\n",
      "ep 2106: ep_len:886 episode reward: total was 49.410000. running mean: -61.881267\n",
      "ep 2106: ep_len:146 episode reward: total was 68.500000. running mean: -60.577454\n",
      "ep 2106: ep_len:51 episode reward: total was 22.500000. running mean: -59.746680\n",
      "ep 2106: ep_len:1165 episode reward: total was 9.460000. running mean: -59.054613\n",
      "ep 2106: ep_len:3826 episode reward: total was -199.780000. running mean: -60.461867\n",
      "ep 2106: ep_len:555 episode reward: total was -54.210000. running mean: -60.399348\n",
      "ep 2106: ep_len:7262 episode reward: total was 69.400000. running mean: -59.101355\n",
      "ep 2106: ep_len:500 episode reward: total was 4.900000. running mean: -58.461341\n",
      "ep 2106: ep_len:1476 episode reward: total was 20.070000. running mean: -57.676028\n",
      "ep 2106: ep_len:2864 episode reward: total was -250.120000. running mean: -59.600468\n",
      "epsilon:0.009992 episode_count: 31718. steps_count: 33979167.000000\n",
      "ep 2107: ep_len:607 episode reward: total was 12.200000. running mean: -58.882463\n",
      "ep 2107: ep_len:500 episode reward: total was -34.150000. running mean: -58.635138\n",
      "ep 2107: ep_len:53 episode reward: total was 25.000000. running mean: -57.798787\n",
      "ep 2107: ep_len:2950 episode reward: total was -106.250000. running mean: -58.283299\n",
      "ep 2107: ep_len:561 episode reward: total was -14.760000. running mean: -57.848066\n",
      "ep 2107: ep_len:62 episode reward: total was 29.500000. running mean: -56.974585\n",
      "ep 2107: ep_len:75 episode reward: total was 36.000000. running mean: -56.044839\n",
      "ep 2107: ep_len:62 episode reward: total was 28.000000. running mean: -55.204391\n",
      "ep 2107: ep_len:1504 episode reward: total was 23.410000. running mean: -54.418247\n",
      "ep 2107: ep_len:4009 episode reward: total was -100.090000. running mean: -54.874965\n",
      "ep 2107: ep_len:1591 episode reward: total was -134.580000. running mean: -55.672015\n",
      "ep 2107: ep_len:7301 episode reward: total was 74.280000. running mean: -54.372495\n",
      "ep 2107: ep_len:854 episode reward: total was 13.790000. running mean: -53.690870\n",
      "ep 2107: ep_len:787 episode reward: total was -46.350000. running mean: -53.617461\n",
      "ep 2107: ep_len:2830 episode reward: total was -2.600000. running mean: -53.107287\n",
      "ep 2107: ep_len:72 episode reward: total was 34.500000. running mean: -52.231214\n",
      "epsilon:0.009992 episode_count: 31734. steps_count: 34002985.000000\n",
      "ep 2108: ep_len:641 episode reward: total was -8.320000. running mean: -51.792102\n",
      "ep 2108: ep_len:943 episode reward: total was 17.950000. running mean: -51.094681\n",
      "ep 2108: ep_len:3018 episode reward: total was -8.300000. running mean: -50.666734\n",
      "ep 2108: ep_len:500 episode reward: total was 23.890000. running mean: -49.921166\n",
      "ep 2108: ep_len:1062 episode reward: total was -30.960000. running mean: -49.731555\n",
      "ep 2108: ep_len:342 episode reward: total was 23.130000. running mean: -49.002939\n",
      "ep 2108: ep_len:1253 episode reward: total was -151.260000. running mean: -50.025510\n",
      "ep 2108: ep_len:792 episode reward: total was -20.420000. running mean: -49.729455\n",
      "ep 2108: ep_len:1466 episode reward: total was -13.550000. running mean: -49.367660\n",
      "ep 2108: ep_len:90 episode reward: total was 43.500000. running mean: -48.438984\n",
      "ep 2108: ep_len:1060 episode reward: total was 9.670000. running mean: -47.857894\n",
      "ep 2108: ep_len:2754 episode reward: total was -5.470000. running mean: -47.434015\n",
      "epsilon:0.009992 episode_count: 31746. steps_count: 34016906.000000\n",
      "ep 2109: ep_len:655 episode reward: total was -32.490000. running mean: -47.284575\n",
      "ep 2109: ep_len:723 episode reward: total was -65.890000. running mean: -47.470629\n",
      "ep 2109: ep_len:2988 episode reward: total was -60.440000. running mean: -47.600323\n",
      "ep 2109: ep_len:627 episode reward: total was 15.390000. running mean: -46.970419\n",
      "ep 2109: ep_len:858 episode reward: total was 29.590000. running mean: -46.204815\n",
      "ep 2109: ep_len:3881 episode reward: total was -327.820000. running mean: -49.020967\n",
      "ep 2109: ep_len:611 episode reward: total was -72.960000. running mean: -49.260357\n",
      "ep 2109: ep_len:726 episode reward: total was 3.920000. running mean: -48.728554\n",
      "ep 2109: ep_len:955 episode reward: total was -8.370000. running mean: -48.324968\n",
      "ep 2109: ep_len:713 episode reward: total was -152.620000. running mean: -49.367919\n",
      "ep 2109: ep_len:2878 episode reward: total was -31.200000. running mean: -49.186239\n",
      "ep 2109: ep_len:63 episode reward: total was -58.990000. running mean: -49.284277\n",
      "epsilon:0.009992 episode_count: 31758. steps_count: 34032584.000000\n",
      "ep 2110: ep_len:635 episode reward: total was -15.520000. running mean: -48.946634\n",
      "ep 2110: ep_len:1199 episode reward: total was -95.240000. running mean: -49.409568\n",
      "ep 2110: ep_len:83 episode reward: total was 40.000000. running mean: -48.515472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2110: ep_len:819 episode reward: total was 32.780000. running mean: -47.702518\n",
      "ep 2110: ep_len:39 episode reward: total was 16.500000. running mean: -47.060492\n",
      "ep 2110: ep_len:42 episode reward: total was 19.500000. running mean: -46.394887\n",
      "ep 2110: ep_len:1025 episode reward: total was -32.740000. running mean: -46.258339\n",
      "ep 2110: ep_len:4171 episode reward: total was -1107.790000. running mean: -56.873655\n",
      "ep 2110: ep_len:607 episode reward: total was -67.770000. running mean: -56.982619\n",
      "ep 2110: ep_len:743 episode reward: total was -6.960000. running mean: -56.482392\n",
      "ep 2110: ep_len:563 episode reward: total was -11.710000. running mean: -56.034668\n",
      "ep 2110: ep_len:67 episode reward: total was 27.500000. running mean: -55.199322\n",
      "ep 2110: ep_len:128 episode reward: total was 61.000000. running mean: -54.037329\n",
      "ep 2110: ep_len:22 episode reward: total was 9.500000. running mean: -53.401955\n",
      "ep 2110: ep_len:649 episode reward: total was 3.380000. running mean: -52.834136\n",
      "ep 2110: ep_len:2828 episode reward: total was 15.170000. running mean: -52.154094\n",
      "epsilon:0.009992 episode_count: 31774. steps_count: 34046204.000000\n",
      "ep 2111: ep_len:639 episode reward: total was -3.880000. running mean: -51.671353\n",
      "ep 2111: ep_len:767 episode reward: total was -9.670000. running mean: -51.251340\n",
      "ep 2111: ep_len:75 episode reward: total was 36.000000. running mean: -50.378827\n",
      "ep 2111: ep_len:2985 episode reward: total was -51.740000. running mean: -50.392438\n",
      "ep 2111: ep_len:668 episode reward: total was -24.900000. running mean: -50.137514\n",
      "ep 2111: ep_len:132 episode reward: total was 60.000000. running mean: -49.036139\n",
      "ep 2111: ep_len:59 episode reward: total was 26.500000. running mean: -48.280777\n",
      "ep 2111: ep_len:1653 episode reward: total was -372.330000. running mean: -51.521270\n",
      "ep 2111: ep_len:336 episode reward: total was 17.590000. running mean: -50.830157\n",
      "ep 2111: ep_len:528 episode reward: total was -11.020000. running mean: -50.432055\n",
      "ep 2111: ep_len:765 episode reward: total was 24.300000. running mean: -49.684735\n",
      "ep 2111: ep_len:500 episode reward: total was -13.190000. running mean: -49.319787\n",
      "ep 2111: ep_len:81 episode reward: total was 39.000000. running mean: -48.436590\n",
      "ep 2111: ep_len:1512 episode reward: total was 12.170000. running mean: -47.830524\n",
      "ep 2111: ep_len:2973 episode reward: total was -2266.020000. running mean: -70.012418\n",
      "ep 2111: ep_len:54 episode reward: total was 25.500000. running mean: -69.057294\n",
      "epsilon:0.009992 episode_count: 31790. steps_count: 34059931.000000\n",
      "ep 2112: ep_len:643 episode reward: total was -2.830000. running mean: -68.395021\n",
      "ep 2112: ep_len:200 episode reward: total was -26.320000. running mean: -67.974271\n",
      "ep 2112: ep_len:3013 episode reward: total was -64.810000. running mean: -67.942628\n",
      "ep 2112: ep_len:755 episode reward: total was -31.000000. running mean: -67.573202\n",
      "ep 2112: ep_len:41 episode reward: total was 19.000000. running mean: -66.707470\n",
      "ep 2112: ep_len:1850 episode reward: total was -119.610000. running mean: -67.236495\n",
      "ep 2112: ep_len:3609 episode reward: total was -111.560000. running mean: -67.679730\n",
      "ep 2112: ep_len:1235 episode reward: total was -60.750000. running mean: -67.610433\n",
      "ep 2112: ep_len:7208 episode reward: total was -163.140000. running mean: -68.565729\n",
      "ep 2112: ep_len:681 episode reward: total was 18.590000. running mean: -67.694171\n",
      "ep 2112: ep_len:134 episode reward: total was 64.000000. running mean: -66.377230\n",
      "ep 2112: ep_len:676 episode reward: total was -0.480000. running mean: -65.718257\n",
      "ep 2112: ep_len:2765 episode reward: total was -8.690000. running mean: -65.147975\n",
      "ep 2112: ep_len:40 episode reward: total was 18.500000. running mean: -64.311495\n",
      "epsilon:0.009992 episode_count: 31804. steps_count: 34082781.000000\n",
      "ep 2113: ep_len:706 episode reward: total was -22.400000. running mean: -63.892380\n",
      "ep 2113: ep_len:844 episode reward: total was 0.790000. running mean: -63.245556\n",
      "ep 2113: ep_len:73 episode reward: total was 35.000000. running mean: -62.263101\n",
      "ep 2113: ep_len:2966 episode reward: total was -109.430000. running mean: -62.734770\n",
      "ep 2113: ep_len:1676 episode reward: total was -168.180000. running mean: -63.789222\n",
      "ep 2113: ep_len:67 episode reward: total was 32.000000. running mean: -62.831330\n",
      "ep 2113: ep_len:894 episode reward: total was 53.050000. running mean: -61.672517\n",
      "ep 2113: ep_len:4082 episode reward: total was -145.280000. running mean: -62.508591\n",
      "ep 2113: ep_len:1248 episode reward: total was -45.260000. running mean: -62.336106\n",
      "ep 2113: ep_len:782 episode reward: total was -2.870000. running mean: -61.741444\n",
      "ep 2113: ep_len:582 episode reward: total was -14.580000. running mean: -61.269830\n",
      "ep 2113: ep_len:78 episode reward: total was 36.000000. running mean: -60.297132\n",
      "ep 2113: ep_len:190 episode reward: total was 93.500000. running mean: -58.759160\n",
      "ep 2113: ep_len:78 episode reward: total was 34.500000. running mean: -57.826569\n",
      "ep 2113: ep_len:886 episode reward: total was -47.720000. running mean: -57.725503\n",
      "ep 2113: ep_len:2880 episode reward: total was -14.640000. running mean: -57.294648\n",
      "epsilon:0.009992 episode_count: 31820. steps_count: 34100813.000000\n",
      "ep 2114: ep_len:1456 episode reward: total was 39.860000. running mean: -56.323102\n",
      "ep 2114: ep_len:1230 episode reward: total was -35.340000. running mean: -56.113271\n",
      "ep 2114: ep_len:46 episode reward: total was 21.500000. running mean: -55.337138\n",
      "ep 2114: ep_len:2921 episode reward: total was -86.470000. running mean: -55.648466\n",
      "ep 2114: ep_len:1195 episode reward: total was -24.270000. running mean: -55.334682\n",
      "ep 2114: ep_len:868 episode reward: total was 44.030000. running mean: -54.341035\n",
      "ep 2114: ep_len:500 episode reward: total was 20.160000. running mean: -53.596025\n",
      "ep 2114: ep_len:3929 episode reward: total was -833.520000. running mean: -61.395264\n",
      "ep 2114: ep_len:742 episode reward: total was 12.450000. running mean: -60.656812\n",
      "ep 2114: ep_len:1089 episode reward: total was -13.400000. running mean: -60.184244\n",
      "ep 2114: ep_len:122 episode reward: total was 59.500000. running mean: -58.987401\n",
      "ep 2114: ep_len:711 episode reward: total was 8.610000. running mean: -58.311427\n",
      "ep 2114: ep_len:2824 episode reward: total was -16.610000. running mean: -57.894413\n",
      "ep 2114: ep_len:53 episode reward: total was 25.000000. running mean: -57.065469\n",
      "epsilon:0.009992 episode_count: 31834. steps_count: 34118499.000000\n",
      "ep 2115: ep_len:1439 episode reward: total was -4.240000. running mean: -56.537214\n",
      "ep 2115: ep_len:1243 episode reward: total was -54.400000. running mean: -56.515842\n",
      "ep 2115: ep_len:2997 episode reward: total was -102.390000. running mean: -56.974584\n",
      "ep 2115: ep_len:503 episode reward: total was 11.300000. running mean: -56.291838\n",
      "ep 2115: ep_len:945 episode reward: total was -55.760000. running mean: -56.286519\n",
      "ep 2115: ep_len:4132 episode reward: total was -258.040000. running mean: -58.304054\n",
      "ep 2115: ep_len:736 episode reward: total was -44.400000. running mean: -58.165014\n",
      "ep 2115: ep_len:715 episode reward: total was -2.590000. running mean: -57.609263\n",
      "ep 2115: ep_len:957 episode reward: total was -15.050000. running mean: -57.183671\n",
      "ep 2115: ep_len:66 episode reward: total was 30.000000. running mean: -56.311834\n",
      "ep 2115: ep_len:25 episode reward: total was 11.000000. running mean: -55.638716\n",
      "ep 2115: ep_len:500 episode reward: total was -6.910000. running mean: -55.151429\n",
      "ep 2115: ep_len:2914 episode reward: total was -12.100000. running mean: -54.720914\n",
      "epsilon:0.009992 episode_count: 31847. steps_count: 34135671.000000\n",
      "ep 2116: ep_len:682 episode reward: total was -16.430000. running mean: -54.338005\n",
      "ep 2116: ep_len:500 episode reward: total was 14.060000. running mean: -53.654025\n",
      "ep 2116: ep_len:69 episode reward: total was 33.000000. running mean: -52.787485\n",
      "ep 2116: ep_len:2965 episode reward: total was -2463.050000. running mean: -76.890110\n",
      "ep 2116: ep_len:643 episode reward: total was 2.990000. running mean: -76.091309\n",
      "ep 2116: ep_len:69 episode reward: total was 30.000000. running mean: -75.030396\n",
      "ep 2116: ep_len:66 episode reward: total was 30.000000. running mean: -73.980092\n",
      "ep 2116: ep_len:1126 episode reward: total was -1.000000. running mean: -73.250291\n",
      "ep 2116: ep_len:3525 episode reward: total was -37.200000. running mean: -72.889788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2116: ep_len:549 episode reward: total was 1.100000. running mean: -72.149890\n",
      "ep 2116: ep_len:724 episode reward: total was 37.670000. running mean: -71.051691\n",
      "ep 2116: ep_len:2701 episode reward: total was -236.990000. running mean: -72.711074\n",
      "ep 2116: ep_len:743 episode reward: total was -92.730000. running mean: -72.911264\n",
      "ep 2116: ep_len:2838 episode reward: total was -64.430000. running mean: -72.826451\n",
      "epsilon:0.009992 episode_count: 31861. steps_count: 34152871.000000\n",
      "ep 2117: ep_len:1457 episode reward: total was 9.320000. running mean: -72.004986\n",
      "ep 2117: ep_len:964 episode reward: total was 13.480000. running mean: -71.150137\n",
      "ep 2117: ep_len:48 episode reward: total was 22.500000. running mean: -70.213635\n",
      "ep 2117: ep_len:2965 episode reward: total was -66.370000. running mean: -70.175199\n",
      "ep 2117: ep_len:500 episode reward: total was 20.150000. running mean: -69.271947\n",
      "ep 2117: ep_len:34 episode reward: total was 15.500000. running mean: -68.424227\n",
      "ep 2117: ep_len:104 episode reward: total was 50.500000. running mean: -67.234985\n",
      "ep 2117: ep_len:1085 episode reward: total was -62.530000. running mean: -67.187935\n",
      "ep 2117: ep_len:3566 episode reward: total was -57.050000. running mean: -67.086556\n",
      "ep 2117: ep_len:635 episode reward: total was -39.760000. running mean: -66.813290\n",
      "ep 2117: ep_len:7256 episode reward: total was -94.470000. running mean: -67.089857\n",
      "ep 2117: ep_len:1407 episode reward: total was 3.090000. running mean: -66.388059\n",
      "ep 2117: ep_len:63 episode reward: total was 30.000000. running mean: -65.424178\n",
      "ep 2117: ep_len:101 episode reward: total was 49.000000. running mean: -64.279937\n",
      "ep 2117: ep_len:1516 episode reward: total was 4.400000. running mean: -63.593137\n",
      "ep 2117: ep_len:2834 episode reward: total was -23.800000. running mean: -63.195206\n",
      "epsilon:0.009992 episode_count: 31877. steps_count: 34177406.000000\n",
      "ep 2118: ep_len:1447 episode reward: total was -65.770000. running mean: -63.220954\n",
      "ep 2118: ep_len:1141 episode reward: total was -17.560000. running mean: -62.764344\n",
      "ep 2118: ep_len:64 episode reward: total was 30.500000. running mean: -61.831701\n",
      "ep 2118: ep_len:2989 episode reward: total was -76.750000. running mean: -61.980884\n",
      "ep 2118: ep_len:637 episode reward: total was 19.690000. running mean: -61.164175\n",
      "ep 2118: ep_len:106 episode reward: total was 50.000000. running mean: -60.052533\n",
      "ep 2118: ep_len:1037 episode reward: total was -45.590000. running mean: -59.907908\n",
      "ep 2118: ep_len:612 episode reward: total was 22.150000. running mean: -59.087329\n",
      "ep 2118: ep_len:1510 episode reward: total was -113.840000. running mean: -59.634855\n",
      "ep 2118: ep_len:855 episode reward: total was 3.570000. running mean: -59.002807\n",
      "ep 2118: ep_len:966 episode reward: total was 20.380000. running mean: -58.208979\n",
      "ep 2118: ep_len:92 episode reward: total was 43.000000. running mean: -57.196889\n",
      "ep 2118: ep_len:88 episode reward: total was 42.500000. running mean: -56.199920\n",
      "ep 2118: ep_len:1147 episode reward: total was -3.540000. running mean: -55.673321\n",
      "ep 2118: ep_len:2959 episode reward: total was -25.330000. running mean: -55.369888\n",
      "ep 2118: ep_len:69 episode reward: total was 33.000000. running mean: -54.486189\n",
      "epsilon:0.009992 episode_count: 31893. steps_count: 34193125.000000\n",
      "ep 2119: ep_len:624 episode reward: total was 6.980000. running mean: -53.871527\n",
      "ep 2119: ep_len:500 episode reward: total was 17.210000. running mean: -53.160712\n",
      "ep 2119: ep_len:2973 episode reward: total was -112.450000. running mean: -53.753605\n",
      "ep 2119: ep_len:1473 episode reward: total was 7.950000. running mean: -53.136569\n",
      "ep 2119: ep_len:778 episode reward: total was -40.380000. running mean: -53.009003\n",
      "ep 2119: ep_len:329 episode reward: total was 7.330000. running mean: -52.405613\n",
      "ep 2119: ep_len:1484 episode reward: total was -16.610000. running mean: -52.047657\n",
      "ep 2119: ep_len:863 episode reward: total was -28.810000. running mean: -51.815280\n",
      "ep 2119: ep_len:606 episode reward: total was -8.250000. running mean: -51.379627\n",
      "ep 2119: ep_len:56 episode reward: total was 26.500000. running mean: -50.600831\n",
      "ep 2119: ep_len:35 episode reward: total was 16.000000. running mean: -49.934823\n",
      "ep 2119: ep_len:678 episode reward: total was -6.520000. running mean: -49.500675\n",
      "ep 2119: ep_len:37 episode reward: total was 17.000000. running mean: -48.835668\n",
      "epsilon:0.009992 episode_count: 31906. steps_count: 34203561.000000\n",
      "ep 2120: ep_len:650 episode reward: total was 7.890000. running mean: -48.268411\n",
      "ep 2120: ep_len:962 episode reward: total was 4.250000. running mean: -47.743227\n",
      "ep 2120: ep_len:62 episode reward: total was 29.500000. running mean: -46.970795\n",
      "ep 2120: ep_len:2921 episode reward: total was -61.770000. running mean: -47.118787\n",
      "ep 2120: ep_len:1166 episode reward: total was -13.760000. running mean: -46.785199\n",
      "ep 2120: ep_len:141 episode reward: total was 67.500000. running mean: -45.642347\n",
      "ep 2120: ep_len:1336 episode reward: total was -282.740000. running mean: -48.013323\n",
      "ep 2120: ep_len:622 episode reward: total was 22.490000. running mean: -47.308290\n",
      "ep 2120: ep_len:1017 episode reward: total was -13.030000. running mean: -46.965507\n",
      "ep 2120: ep_len:735 episode reward: total was 9.590000. running mean: -46.399952\n",
      "ep 2120: ep_len:562 episode reward: total was -29.750000. running mean: -46.233453\n",
      "ep 2120: ep_len:61 episode reward: total was 29.000000. running mean: -45.481118\n",
      "ep 2120: ep_len:1062 episode reward: total was 15.380000. running mean: -44.872507\n",
      "ep 2120: ep_len:2866 episode reward: total was -33.060000. running mean: -44.754382\n",
      "epsilon:0.009992 episode_count: 31920. steps_count: 34217724.000000\n",
      "ep 2121: ep_len:1135 episode reward: total was 8.280000. running mean: -44.224038\n",
      "ep 2121: ep_len:972 episode reward: total was 15.410000. running mean: -43.627698\n",
      "ep 2121: ep_len:2994 episode reward: total was -53.460000. running mean: -43.726021\n",
      "ep 2121: ep_len:1938 episode reward: total was -1049.220000. running mean: -53.780961\n",
      "ep 2121: ep_len:52 episode reward: total was 23.000000. running mean: -53.013151\n",
      "ep 2121: ep_len:955 episode reward: total was 78.060000. running mean: -51.702419\n",
      "ep 2121: ep_len:500 episode reward: total was 31.580000. running mean: -50.869595\n",
      "ep 2121: ep_len:4143 episode reward: total was -667.440000. running mean: -57.035299\n",
      "ep 2121: ep_len:885 episode reward: total was 39.050000. running mean: -56.074446\n",
      "ep 2121: ep_len:1080 episode reward: total was 25.330000. running mean: -55.260402\n",
      "ep 2121: ep_len:73 episode reward: total was 35.000000. running mean: -54.357798\n",
      "ep 2121: ep_len:777 episode reward: total was -36.840000. running mean: -54.182620\n",
      "ep 2121: ep_len:2853 episode reward: total was -7.200000. running mean: -53.712794\n",
      "epsilon:0.009992 episode_count: 31933. steps_count: 34236081.000000\n",
      "ep 2122: ep_len:746 episode reward: total was -9.010000. running mean: -53.265766\n",
      "ep 2122: ep_len:936 episode reward: total was 12.070000. running mean: -52.612408\n",
      "ep 2122: ep_len:84 episode reward: total was 40.500000. running mean: -51.681284\n",
      "ep 2122: ep_len:2984 episode reward: total was -80.100000. running mean: -51.965471\n",
      "ep 2122: ep_len:577 episode reward: total was -0.090000. running mean: -51.446716\n",
      "ep 2122: ep_len:58 episode reward: total was 24.500000. running mean: -50.687249\n",
      "ep 2122: ep_len:42 episode reward: total was 19.500000. running mean: -49.985377\n",
      "ep 2122: ep_len:903 episode reward: total was 39.290000. running mean: -49.092623\n",
      "ep 2122: ep_len:665 episode reward: total was 33.670000. running mean: -48.264997\n",
      "ep 2122: ep_len:1531 episode reward: total was -4.080000. running mean: -47.823147\n",
      "ep 2122: ep_len:805 episode reward: total was -18.090000. running mean: -47.525815\n",
      "ep 2122: ep_len:500 episode reward: total was 5.580000. running mean: -46.994757\n",
      "ep 2122: ep_len:60 episode reward: total was 28.500000. running mean: -46.239810\n",
      "ep 2122: ep_len:678 episode reward: total was -6.520000. running mean: -45.842612\n",
      "ep 2122: ep_len:2784 episode reward: total was -12.610000. running mean: -45.510285\n",
      "epsilon:0.009992 episode_count: 31948. steps_count: 34249434.000000\n",
      "ep 2123: ep_len:676 episode reward: total was -14.950000. running mean: -45.204683\n",
      "ep 2123: ep_len:652 episode reward: total was -18.900000. running mean: -44.941636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2123: ep_len:80 episode reward: total was 34.000000. running mean: -44.152219\n",
      "ep 2123: ep_len:2962 episode reward: total was -55.540000. running mean: -44.266097\n",
      "ep 2123: ep_len:549 episode reward: total was -14.360000. running mean: -43.967036\n",
      "ep 2123: ep_len:167 episode reward: total was 76.000000. running mean: -42.767366\n",
      "ep 2123: ep_len:1392 episode reward: total was -105.430000. running mean: -43.393992\n",
      "ep 2123: ep_len:319 episode reward: total was 25.990000. running mean: -42.700152\n",
      "ep 2123: ep_len:1112 episode reward: total was -77.840000. running mean: -43.051551\n",
      "ep 2123: ep_len:734 episode reward: total was 35.570000. running mean: -42.265335\n",
      "ep 2123: ep_len:500 episode reward: total was -17.500000. running mean: -42.017682\n",
      "ep 2123: ep_len:111 episode reward: total was 54.000000. running mean: -41.057505\n",
      "ep 2123: ep_len:1500 episode reward: total was 9.230000. running mean: -40.554630\n",
      "ep 2123: ep_len:2866 episode reward: total was -13.410000. running mean: -40.283184\n",
      "epsilon:0.009992 episode_count: 31962. steps_count: 34263054.000000\n",
      "ep 2124: ep_len:1327 episode reward: total was -2.550000. running mean: -39.905852\n",
      "ep 2124: ep_len:500 episode reward: total was 2.240000. running mean: -39.484393\n",
      "ep 2124: ep_len:69 episode reward: total was 33.000000. running mean: -38.759549\n",
      "ep 2124: ep_len:2955 episode reward: total was -35.460000. running mean: -38.726554\n",
      "ep 2124: ep_len:500 episode reward: total was 12.500000. running mean: -38.214288\n",
      "ep 2124: ep_len:88 episode reward: total was 41.000000. running mean: -37.422145\n",
      "ep 2124: ep_len:1063 episode reward: total was -0.650000. running mean: -37.054424\n",
      "ep 2124: ep_len:322 episode reward: total was 6.370000. running mean: -36.620180\n",
      "ep 2124: ep_len:541 episode reward: total was -4.860000. running mean: -36.302578\n",
      "ep 2124: ep_len:861 episode reward: total was 58.430000. running mean: -35.355252\n",
      "ep 2124: ep_len:692 episode reward: total was 9.250000. running mean: -34.909200\n",
      "ep 2124: ep_len:97 episode reward: total was 47.000000. running mean: -34.090108\n",
      "ep 2124: ep_len:1423 episode reward: total was -1.580000. running mean: -33.765007\n",
      "ep 2124: ep_len:2800 episode reward: total was -55.830000. running mean: -33.985657\n",
      "epsilon:0.009992 episode_count: 31976. steps_count: 34276292.000000\n",
      "ep 2125: ep_len:608 episode reward: total was -8.600000. running mean: -33.731800\n",
      "ep 2125: ep_len:939 episode reward: total was 37.820000. running mean: -33.016282\n",
      "ep 2125: ep_len:54 episode reward: total was 25.500000. running mean: -32.431119\n",
      "ep 2125: ep_len:2955 episode reward: total was -46.550000. running mean: -32.572308\n",
      "ep 2125: ep_len:641 episode reward: total was 4.640000. running mean: -32.200185\n",
      "ep 2125: ep_len:33 episode reward: total was 15.000000. running mean: -31.728183\n",
      "ep 2125: ep_len:69 episode reward: total was 33.000000. running mean: -31.080901\n",
      "ep 2125: ep_len:629 episode reward: total was 40.290000. running mean: -30.367192\n",
      "ep 2125: ep_len:3858 episode reward: total was -235.140000. running mean: -32.414920\n",
      "ep 2125: ep_len:788 episode reward: total was -31.350000. running mean: -32.404271\n",
      "ep 2125: ep_len:658 episode reward: total was -3.050000. running mean: -32.110728\n",
      "ep 2125: ep_len:627 episode reward: total was -6.020000. running mean: -31.849821\n",
      "ep 2125: ep_len:76 episode reward: total was 35.000000. running mean: -31.181323\n",
      "ep 2125: ep_len:174 episode reward: total was 84.000000. running mean: -30.029510\n",
      "ep 2125: ep_len:813 episode reward: total was -18.780000. running mean: -29.917015\n",
      "ep 2125: ep_len:2867 episode reward: total was -18.230000. running mean: -29.800144\n",
      "epsilon:0.009992 episode_count: 31992. steps_count: 34292081.000000\n",
      "ep 2126: ep_len:1170 episode reward: total was -14.910000. running mean: -29.651243\n",
      "ep 2126: ep_len:1234 episode reward: total was -37.320000. running mean: -29.727931\n",
      "ep 2126: ep_len:77 episode reward: total was 37.000000. running mean: -29.060651\n",
      "ep 2126: ep_len:506 episode reward: total was 16.800000. running mean: -28.602045\n",
      "ep 2126: ep_len:149 episode reward: total was 71.500000. running mean: -27.601024\n",
      "ep 2126: ep_len:694 episode reward: total was 16.660000. running mean: -27.158414\n",
      "ep 2126: ep_len:3865 episode reward: total was -2792.680000. running mean: -54.813630\n",
      "ep 2126: ep_len:808 episode reward: total was -21.110000. running mean: -54.476594\n",
      "ep 2126: ep_len:687 episode reward: total was 19.600000. running mean: -53.735828\n",
      "ep 2126: ep_len:1531 episode reward: total was -36.770000. running mean: -53.566169\n",
      "ep 2126: ep_len:59 episode reward: total was 26.500000. running mean: -52.765508\n",
      "ep 2126: ep_len:898 episode reward: total was -0.490000. running mean: -52.242753\n",
      "ep 2126: ep_len:2893 episode reward: total was -55.910000. running mean: -52.279425\n",
      "ep 2126: ep_len:44 episode reward: total was 20.500000. running mean: -51.551631\n",
      "epsilon:0.009992 episode_count: 32006. steps_count: 34306696.000000\n",
      "ep 2127: ep_len:1401 episode reward: total was -11.600000. running mean: -51.152115\n",
      "ep 2127: ep_len:759 episode reward: total was -46.970000. running mean: -51.110293\n",
      "ep 2127: ep_len:44 episode reward: total was 19.000000. running mean: -50.409190\n",
      "ep 2127: ep_len:3009 episode reward: total was -44.870000. running mean: -50.353799\n",
      "ep 2127: ep_len:819 episode reward: total was 16.860000. running mean: -49.681661\n",
      "ep 2127: ep_len:52 episode reward: total was 23.000000. running mean: -48.954844\n",
      "ep 2127: ep_len:120 episode reward: total was 57.000000. running mean: -47.895296\n",
      "ep 2127: ep_len:1419 episode reward: total was -67.790000. running mean: -48.094243\n",
      "ep 2127: ep_len:338 episode reward: total was 12.620000. running mean: -47.487100\n",
      "ep 2127: ep_len:516 episode reward: total was -47.530000. running mean: -47.487529\n",
      "ep 2127: ep_len:7628 episode reward: total was -582.840000. running mean: -52.841054\n",
      "ep 2127: ep_len:1430 episode reward: total was -1.880000. running mean: -52.331443\n",
      "ep 2127: ep_len:74 episode reward: total was 35.500000. running mean: -51.453129\n",
      "ep 2127: ep_len:115 episode reward: total was 54.500000. running mean: -50.393598\n",
      "ep 2127: ep_len:60 episode reward: total was 27.000000. running mean: -49.619662\n",
      "ep 2127: ep_len:582 episode reward: total was -8.490000. running mean: -49.208365\n",
      "ep 2127: ep_len:2864 episode reward: total was -10.800000. running mean: -48.824281\n",
      "ep 2127: ep_len:49 episode reward: total was 21.500000. running mean: -48.121039\n",
      "epsilon:0.009992 episode_count: 32024. steps_count: 34327975.000000\n",
      "ep 2128: ep_len:1120 episode reward: total was 6.630000. running mean: -47.573528\n",
      "ep 2128: ep_len:657 episode reward: total was -24.760000. running mean: -47.345393\n",
      "ep 2128: ep_len:3051 episode reward: total was -25.190000. running mean: -47.123839\n",
      "ep 2128: ep_len:501 episode reward: total was 0.630000. running mean: -46.646301\n",
      "ep 2128: ep_len:63 episode reward: total was 28.500000. running mean: -45.894838\n",
      "ep 2128: ep_len:152 episode reward: total was 68.500000. running mean: -44.750889\n",
      "ep 2128: ep_len:63 episode reward: total was 28.500000. running mean: -44.018380\n",
      "ep 2128: ep_len:829 episode reward: total was 52.670000. running mean: -43.051496\n",
      "ep 2128: ep_len:3631 episode reward: total was -105.890000. running mean: -43.679881\n",
      "ep 2128: ep_len:928 episode reward: total was -21.090000. running mean: -43.453983\n",
      "ep 2128: ep_len:782 episode reward: total was 36.690000. running mean: -42.652543\n",
      "ep 2128: ep_len:1019 episode reward: total was 10.120000. running mean: -42.124817\n",
      "ep 2128: ep_len:72 episode reward: total was 34.500000. running mean: -41.358569\n",
      "ep 2128: ep_len:500 episode reward: total was 43.820000. running mean: -40.506784\n",
      "ep 2128: ep_len:2781 episode reward: total was -45.900000. running mean: -40.560716\n",
      "epsilon:0.009992 episode_count: 32039. steps_count: 34344124.000000\n",
      "ep 2129: ep_len:1118 episode reward: total was -8.610000. running mean: -40.241209\n",
      "ep 2129: ep_len:670 episode reward: total was -15.190000. running mean: -39.990696\n",
      "ep 2129: ep_len:64 episode reward: total was 29.000000. running mean: -39.300790\n",
      "ep 2129: ep_len:2914 episode reward: total was -56.420000. running mean: -39.471982\n",
      "ep 2129: ep_len:1654 episode reward: total was -30.750000. running mean: -39.384762\n",
      "ep 2129: ep_len:95 episode reward: total was 44.500000. running mean: -38.545914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2129: ep_len:53 episode reward: total was 25.000000. running mean: -37.910455\n",
      "ep 2129: ep_len:500 episode reward: total was 14.550000. running mean: -37.385850\n",
      "ep 2129: ep_len:662 episode reward: total was 23.200000. running mean: -36.779992\n",
      "ep 2129: ep_len:586 episode reward: total was 5.480000. running mean: -36.357392\n",
      "ep 2129: ep_len:613 episode reward: total was 26.820000. running mean: -35.725618\n",
      "ep 2129: ep_len:538 episode reward: total was 19.000000. running mean: -35.178362\n",
      "ep 2129: ep_len:50 episode reward: total was 22.000000. running mean: -34.606578\n",
      "ep 2129: ep_len:1096 episode reward: total was -24.580000. running mean: -34.506313\n",
      "ep 2129: ep_len:2760 episode reward: total was -8.990000. running mean: -34.251149\n",
      "epsilon:0.009992 episode_count: 32054. steps_count: 34357497.000000\n",
      "ep 2130: ep_len:968 episode reward: total was -157.640000. running mean: -35.485038\n",
      "ep 2130: ep_len:699 episode reward: total was 1.630000. running mean: -35.113888\n",
      "ep 2130: ep_len:3029 episode reward: total was -15.040000. running mean: -34.913149\n",
      "ep 2130: ep_len:756 episode reward: total was -10.790000. running mean: -34.671917\n",
      "ep 2130: ep_len:56 episode reward: total was 26.500000. running mean: -34.060198\n",
      "ep 2130: ep_len:953 episode reward: total was 78.100000. running mean: -32.938596\n",
      "ep 2130: ep_len:653 episode reward: total was 15.640000. running mean: -32.452810\n",
      "ep 2130: ep_len:634 episode reward: total was -33.220000. running mean: -32.460482\n",
      "ep 2130: ep_len:640 episode reward: total was -9.320000. running mean: -32.229077\n",
      "ep 2130: ep_len:899 episode reward: total was 34.090000. running mean: -31.565886\n",
      "ep 2130: ep_len:180 episode reward: total was 88.500000. running mean: -30.365228\n",
      "ep 2130: ep_len:103 episode reward: total was 50.000000. running mean: -29.561575\n",
      "ep 2130: ep_len:1064 episode reward: total was -13.770000. running mean: -29.403660\n",
      "ep 2130: ep_len:2808 episode reward: total was -512.840000. running mean: -34.238023\n",
      "epsilon:0.009992 episode_count: 32068. steps_count: 34370939.000000\n",
      "ep 2131: ep_len:726 episode reward: total was 13.380000. running mean: -33.761843\n",
      "ep 2131: ep_len:977 episode reward: total was 26.500000. running mean: -33.159224\n",
      "ep 2131: ep_len:3087 episode reward: total was -166.800000. running mean: -34.495632\n",
      "ep 2131: ep_len:1487 episode reward: total was 3.680000. running mean: -34.113876\n",
      "ep 2131: ep_len:37 episode reward: total was 17.000000. running mean: -33.602737\n",
      "ep 2131: ep_len:151 episode reward: total was 72.500000. running mean: -32.541710\n",
      "ep 2131: ep_len:1716 episode reward: total was -521.340000. running mean: -37.429692\n",
      "ep 2131: ep_len:3563 episode reward: total was -70.210000. running mean: -37.757496\n",
      "ep 2131: ep_len:569 episode reward: total was 4.210000. running mean: -37.337821\n",
      "ep 2131: ep_len:673 episode reward: total was 8.530000. running mean: -36.879142\n",
      "ep 2131: ep_len:1006 episode reward: total was 5.520000. running mean: -36.455151\n",
      "ep 2131: ep_len:55 episode reward: total was 26.000000. running mean: -35.830599\n",
      "ep 2131: ep_len:79 episode reward: total was 36.500000. running mean: -35.107293\n",
      "ep 2131: ep_len:1147 episode reward: total was -1.330000. running mean: -34.769521\n",
      "ep 2131: ep_len:2770 episode reward: total was -51.380000. running mean: -34.935625\n",
      "epsilon:0.009992 episode_count: 32083. steps_count: 34388982.000000\n",
      "ep 2132: ep_len:1120 episode reward: total was -9.170000. running mean: -34.677969\n",
      "ep 2132: ep_len:704 episode reward: total was -21.410000. running mean: -34.545289\n",
      "ep 2132: ep_len:66 episode reward: total was 31.500000. running mean: -33.884836\n",
      "ep 2132: ep_len:2989 episode reward: total was -61.140000. running mean: -34.157388\n",
      "ep 2132: ep_len:659 episode reward: total was -418.640000. running mean: -38.002214\n",
      "ep 2132: ep_len:69 episode reward: total was 33.000000. running mean: -37.292192\n",
      "ep 2132: ep_len:1452 episode reward: total was -300.550000. running mean: -39.924770\n",
      "ep 2132: ep_len:656 episode reward: total was 27.210000. running mean: -39.253422\n",
      "ep 2132: ep_len:954 episode reward: total was -24.970000. running mean: -39.110588\n",
      "ep 2132: ep_len:7385 episode reward: total was -18.170000. running mean: -38.901182\n",
      "ep 2132: ep_len:1120 episode reward: total was -5.130000. running mean: -38.563471\n",
      "ep 2132: ep_len:67 episode reward: total was 32.000000. running mean: -37.857836\n",
      "ep 2132: ep_len:987 episode reward: total was -42.820000. running mean: -37.907457\n",
      "ep 2132: ep_len:2810 episode reward: total was -55.250000. running mean: -38.080883\n",
      "ep 2132: ep_len:40 episode reward: total was 18.500000. running mean: -37.515074\n",
      "epsilon:0.009992 episode_count: 32098. steps_count: 34410060.000000\n",
      "ep 2133: ep_len:940 episode reward: total was -110.550000. running mean: -38.245423\n",
      "ep 2133: ep_len:1284 episode reward: total was -57.020000. running mean: -38.433169\n",
      "ep 2133: ep_len:2914 episode reward: total was -41.530000. running mean: -38.464137\n",
      "ep 2133: ep_len:1126 episode reward: total was -11.130000. running mean: -38.190796\n",
      "ep 2133: ep_len:22 episode reward: total was 9.500000. running mean: -37.713888\n",
      "ep 2133: ep_len:500 episode reward: total was 41.890000. running mean: -36.917849\n",
      "ep 2133: ep_len:322 episode reward: total was 20.660000. running mean: -36.342071\n",
      "ep 2133: ep_len:566 episode reward: total was -69.250000. running mean: -36.671150\n",
      "ep 2133: ep_len:882 episode reward: total was 51.500000. running mean: -35.789438\n",
      "ep 2133: ep_len:900 episode reward: total was 9.660000. running mean: -35.334944\n",
      "ep 2133: ep_len:59 episode reward: total was 28.000000. running mean: -34.701595\n",
      "ep 2133: ep_len:1508 episode reward: total was -4.710000. running mean: -34.401679\n",
      "ep 2133: ep_len:2906 episode reward: total was -9.790000. running mean: -34.155562\n",
      "epsilon:0.009992 episode_count: 32111. steps_count: 34423989.000000\n",
      "ep 2134: ep_len:1065 episode reward: total was -1.120000. running mean: -33.825206\n",
      "ep 2134: ep_len:694 episode reward: total was -105.170000. running mean: -34.538654\n",
      "ep 2134: ep_len:2997 episode reward: total was -77.590000. running mean: -34.969168\n",
      "ep 2134: ep_len:647 episode reward: total was 3.350000. running mean: -34.585976\n",
      "ep 2134: ep_len:140 episode reward: total was 67.000000. running mean: -33.570116\n",
      "ep 2134: ep_len:1061 episode reward: total was -48.120000. running mean: -33.715615\n",
      "ep 2134: ep_len:3572 episode reward: total was -47.530000. running mean: -33.853759\n",
      "ep 2134: ep_len:1575 episode reward: total was -14.720000. running mean: -33.662421\n",
      "ep 2134: ep_len:7416 episode reward: total was 49.630000. running mean: -32.829497\n",
      "ep 2134: ep_len:705 episode reward: total was 5.350000. running mean: -32.447702\n",
      "ep 2134: ep_len:90 episode reward: total was 40.500000. running mean: -31.718225\n",
      "ep 2134: ep_len:720 episode reward: total was -109.120000. running mean: -32.492243\n",
      "ep 2134: ep_len:2879 episode reward: total was 1.630000. running mean: -32.151020\n",
      "epsilon:0.009992 episode_count: 32124. steps_count: 34447550.000000\n",
      "ep 2135: ep_len:1193 episode reward: total was 25.080000. running mean: -31.578710\n",
      "ep 2135: ep_len:3835 episode reward: total was -445.070000. running mean: -35.713623\n",
      "ep 2135: ep_len:2913 episode reward: total was -47.000000. running mean: -35.826487\n",
      "ep 2135: ep_len:1669 episode reward: total was -52.280000. running mean: -35.991022\n",
      "ep 2135: ep_len:47 episode reward: total was 22.000000. running mean: -35.411112\n",
      "ep 2135: ep_len:127 episode reward: total was 62.000000. running mean: -34.437001\n",
      "ep 2135: ep_len:1538 episode reward: total was -12.130000. running mean: -34.213931\n",
      "ep 2135: ep_len:3609 episode reward: total was -50.560000. running mean: -34.377391\n",
      "ep 2135: ep_len:786 episode reward: total was -25.070000. running mean: -34.284318\n",
      "ep 2135: ep_len:7453 episode reward: total was -197.310000. running mean: -35.914574\n",
      "ep 2135: ep_len:582 episode reward: total was 40.570000. running mean: -35.149729\n",
      "ep 2135: ep_len:62 episode reward: total was 29.500000. running mean: -34.503231\n",
      "ep 2135: ep_len:162 episode reward: total was 75.000000. running mean: -33.408199\n",
      "ep 2135: ep_len:82 episode reward: total was 38.000000. running mean: -32.694117\n",
      "ep 2135: ep_len:994 episode reward: total was -6.230000. running mean: -32.429476\n",
      "ep 2135: ep_len:45 episode reward: total was 21.000000. running mean: -31.895181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2135: ep_len:56 episode reward: total was 25.000000. running mean: -31.326229\n",
      "epsilon:0.009992 episode_count: 32141. steps_count: 34472703.000000\n",
      "ep 2136: ep_len:860 episode reward: total was -49.140000. running mean: -31.504367\n",
      "ep 2136: ep_len:647 episode reward: total was -6.830000. running mean: -31.257623\n",
      "ep 2136: ep_len:75 episode reward: total was 36.000000. running mean: -30.585047\n",
      "ep 2136: ep_len:684 episode reward: total was -1.020000. running mean: -30.289397\n",
      "ep 2136: ep_len:150 episode reward: total was 69.000000. running mean: -29.296503\n",
      "ep 2136: ep_len:1002 episode reward: total was -14.810000. running mean: -29.151638\n",
      "ep 2136: ep_len:680 episode reward: total was 20.290000. running mean: -28.657221\n",
      "ep 2136: ep_len:500 episode reward: total was -12.270000. running mean: -28.493349\n",
      "ep 2136: ep_len:7399 episode reward: total was 49.610000. running mean: -27.712316\n",
      "ep 2136: ep_len:654 episode reward: total was 18.600000. running mean: -27.249192\n",
      "ep 2136: ep_len:61 episode reward: total was 27.500000. running mean: -26.701700\n",
      "ep 2136: ep_len:127 episode reward: total was 62.000000. running mean: -25.814683\n",
      "ep 2136: ep_len:50 episode reward: total was 23.500000. running mean: -25.321537\n",
      "ep 2136: ep_len:1554 episode reward: total was -6.110000. running mean: -25.129421\n",
      "ep 2136: ep_len:2844 episode reward: total was -94.820000. running mean: -25.826327\n",
      "ep 2136: ep_len:42 episode reward: total was 19.500000. running mean: -25.373064\n",
      "epsilon:0.009992 episode_count: 32157. steps_count: 34490032.000000\n",
      "ep 2137: ep_len:696 episode reward: total was -21.490000. running mean: -25.334233\n",
      "ep 2137: ep_len:776 episode reward: total was -21.700000. running mean: -25.297891\n",
      "ep 2137: ep_len:59 episode reward: total was 25.000000. running mean: -24.794912\n",
      "ep 2137: ep_len:3075 episode reward: total was -114.090000. running mean: -25.687863\n",
      "ep 2137: ep_len:634 episode reward: total was 5.210000. running mean: -25.378884\n",
      "ep 2137: ep_len:500 episode reward: total was 46.420000. running mean: -24.660895\n",
      "ep 2137: ep_len:3629 episode reward: total was -103.860000. running mean: -25.452886\n",
      "ep 2137: ep_len:1174 episode reward: total was -20.290000. running mean: -25.401257\n",
      "ep 2137: ep_len:801 episode reward: total was 39.110000. running mean: -24.756145\n",
      "ep 2137: ep_len:1469 episode reward: total was 16.670000. running mean: -24.341883\n",
      "ep 2137: ep_len:129 episode reward: total was 58.500000. running mean: -23.513465\n",
      "ep 2137: ep_len:53 episode reward: total was 25.000000. running mean: -23.028330\n",
      "ep 2137: ep_len:1140 episode reward: total was -23.110000. running mean: -23.029147\n",
      "ep 2137: ep_len:2918 episode reward: total was -42.220000. running mean: -23.221055\n",
      "epsilon:0.009992 episode_count: 32171. steps_count: 34507085.000000\n",
      "ep 2138: ep_len:947 episode reward: total was -53.050000. running mean: -23.519345\n",
      "ep 2138: ep_len:668 episode reward: total was -18.450000. running mean: -23.468651\n",
      "ep 2138: ep_len:43 episode reward: total was 20.000000. running mean: -23.033965\n",
      "ep 2138: ep_len:3048 episode reward: total was -42.550000. running mean: -23.229125\n",
      "ep 2138: ep_len:539 episode reward: total was -29.120000. running mean: -23.288034\n",
      "ep 2138: ep_len:68 episode reward: total was 32.500000. running mean: -22.730153\n",
      "ep 2138: ep_len:752 episode reward: total was -10.860000. running mean: -22.611452\n",
      "ep 2138: ep_len:610 episode reward: total was 22.310000. running mean: -22.162237\n",
      "ep 2138: ep_len:672 episode reward: total was -1.550000. running mean: -21.956115\n",
      "ep 2138: ep_len:894 episode reward: total was 25.940000. running mean: -21.477154\n",
      "ep 2138: ep_len:500 episode reward: total was 7.840000. running mean: -21.183982\n",
      "ep 2138: ep_len:127 episode reward: total was 62.000000. running mean: -20.352143\n",
      "ep 2138: ep_len:63 episode reward: total was 28.500000. running mean: -19.863621\n",
      "ep 2138: ep_len:84 episode reward: total was 40.500000. running mean: -19.259985\n",
      "ep 2138: ep_len:1142 episode reward: total was -18.040000. running mean: -19.247785\n",
      "ep 2138: ep_len:2861 episode reward: total was -3.450000. running mean: -19.089807\n",
      "epsilon:0.009992 episode_count: 32187. steps_count: 34520103.000000\n",
      "ep 2139: ep_len:806 episode reward: total was -5.360000. running mean: -18.952509\n",
      "ep 2139: ep_len:500 episode reward: total was 12.430000. running mean: -18.638684\n",
      "ep 2139: ep_len:3035 episode reward: total was -5.550000. running mean: -18.507797\n",
      "ep 2139: ep_len:645 episode reward: total was -36.030000. running mean: -18.683019\n",
      "ep 2139: ep_len:51 episode reward: total was 24.000000. running mean: -18.256189\n",
      "ep 2139: ep_len:1026 episode reward: total was -33.340000. running mean: -18.407027\n",
      "ep 2139: ep_len:3940 episode reward: total was -155.730000. running mean: -19.780257\n",
      "ep 2139: ep_len:1285 episode reward: total was -71.480000. running mean: -20.297254\n",
      "ep 2139: ep_len:829 episode reward: total was 47.260000. running mean: -19.621682\n",
      "ep 2139: ep_len:962 episode reward: total was -15.740000. running mean: -19.582865\n",
      "ep 2139: ep_len:74 episode reward: total was 32.500000. running mean: -19.062036\n",
      "ep 2139: ep_len:902 episode reward: total was 12.460000. running mean: -18.746816\n",
      "ep 2139: ep_len:2866 episode reward: total was 7.560000. running mean: -18.483748\n",
      "ep 2139: ep_len:48 episode reward: total was 22.500000. running mean: -18.073910\n",
      "epsilon:0.009992 episode_count: 32201. steps_count: 34537072.000000\n",
      "ep 2140: ep_len:1149 episode reward: total was 1.290000. running mean: -17.880271\n",
      "ep 2140: ep_len:743 episode reward: total was -4.860000. running mean: -17.750068\n",
      "ep 2140: ep_len:66 episode reward: total was 31.500000. running mean: -17.257568\n",
      "ep 2140: ep_len:3001 episode reward: total was -26.160000. running mean: -17.346592\n",
      "ep 2140: ep_len:582 episode reward: total was -5.460000. running mean: -17.227726\n",
      "ep 2140: ep_len:68 episode reward: total was 31.000000. running mean: -16.745449\n",
      "ep 2140: ep_len:37 episode reward: total was 15.500000. running mean: -16.422994\n",
      "ep 2140: ep_len:1418 episode reward: total was 15.430000. running mean: -16.104464\n",
      "ep 2140: ep_len:3950 episode reward: total was -33.900000. running mean: -16.282420\n",
      "ep 2140: ep_len:642 episode reward: total was -10.800000. running mean: -16.227596\n",
      "ep 2140: ep_len:822 episode reward: total was 33.260000. running mean: -15.732720\n",
      "ep 2140: ep_len:1007 episode reward: total was 38.400000. running mean: -15.191392\n",
      "ep 2140: ep_len:141 episode reward: total was 64.500000. running mean: -14.394479\n",
      "ep 2140: ep_len:69 episode reward: total was 33.000000. running mean: -13.920534\n",
      "ep 2140: ep_len:572 episode reward: total was 3.350000. running mean: -13.747828\n",
      "ep 2140: ep_len:2799 episode reward: total was 1.160000. running mean: -13.598750\n",
      "ep 2140: ep_len:59 episode reward: total was 25.000000. running mean: -13.212763\n",
      "epsilon:0.009992 episode_count: 32218. steps_count: 34554197.000000\n",
      "ep 2141: ep_len:706 episode reward: total was -22.400000. running mean: -13.304635\n",
      "ep 2141: ep_len:737 episode reward: total was 1.240000. running mean: -13.159189\n",
      "ep 2141: ep_len:59 episode reward: total was 28.000000. running mean: -12.747597\n",
      "ep 2141: ep_len:2899 episode reward: total was -73.190000. running mean: -13.352021\n",
      "ep 2141: ep_len:625 episode reward: total was -5.980000. running mean: -13.278301\n",
      "ep 2141: ep_len:77 episode reward: total was 37.000000. running mean: -12.775518\n",
      "ep 2141: ep_len:56 episode reward: total was 26.500000. running mean: -12.382762\n",
      "ep 2141: ep_len:815 episode reward: total was 10.020000. running mean: -12.158735\n",
      "ep 2141: ep_len:631 episode reward: total was 16.060000. running mean: -11.876547\n",
      "ep 2141: ep_len:961 episode reward: total was -19.630000. running mean: -11.954082\n",
      "ep 2141: ep_len:701 episode reward: total was 44.030000. running mean: -11.394241\n",
      "ep 2141: ep_len:585 episode reward: total was 12.930000. running mean: -11.150999\n",
      "ep 2141: ep_len:160 episode reward: total was 75.500000. running mean: -10.284489\n",
      "ep 2141: ep_len:5026 episode reward: total was -1982.280000. running mean: -30.004444\n",
      "ep 2141: ep_len:2866 episode reward: total was -42.560000. running mean: -30.129999\n",
      "ep 2141: ep_len:54 episode reward: total was 25.500000. running mean: -29.573699\n",
      "epsilon:0.009992 episode_count: 32234. steps_count: 34571155.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2142: ep_len:651 episode reward: total was -0.730000. running mean: -29.285262\n",
      "ep 2142: ep_len:500 episode reward: total was 13.720000. running mean: -28.855210\n",
      "ep 2142: ep_len:3078 episode reward: total was -1765.670000. running mean: -46.223358\n",
      "ep 2142: ep_len:550 episode reward: total was -35.070000. running mean: -46.111824\n",
      "ep 2142: ep_len:111 episode reward: total was 51.000000. running mean: -45.140706\n",
      "ep 2142: ep_len:845 episode reward: total was 31.970000. running mean: -44.369599\n",
      "ep 2142: ep_len:4143 episode reward: total was -0.880000. running mean: -43.934703\n",
      "ep 2142: ep_len:621 episode reward: total was -4.060000. running mean: -43.535956\n",
      "ep 2142: ep_len:903 episode reward: total was 58.700000. running mean: -42.513596\n",
      "ep 2142: ep_len:623 episode reward: total was -23.720000. running mean: -42.325660\n",
      "ep 2142: ep_len:190 episode reward: total was 92.000000. running mean: -40.982404\n",
      "ep 2142: ep_len:1490 episode reward: total was -16.520000. running mean: -40.737780\n",
      "ep 2142: ep_len:2927 episode reward: total was -67.120000. running mean: -41.001602\n",
      "ep 2142: ep_len:64 episode reward: total was 30.500000. running mean: -40.286586\n",
      "epsilon:0.009992 episode_count: 32248. steps_count: 34587851.000000\n",
      "ep 2143: ep_len:1097 episode reward: total was -25.560000. running mean: -40.139320\n",
      "ep 2143: ep_len:722 episode reward: total was -8.100000. running mean: -39.818927\n",
      "ep 2143: ep_len:2947 episode reward: total was -31.880000. running mean: -39.739538\n",
      "ep 2143: ep_len:612 episode reward: total was 9.630000. running mean: -39.245842\n",
      "ep 2143: ep_len:72 episode reward: total was 30.000000. running mean: -38.553384\n",
      "ep 2143: ep_len:51 episode reward: total was 24.000000. running mean: -37.927850\n",
      "ep 2143: ep_len:694 episode reward: total was 5.800000. running mean: -37.490571\n",
      "ep 2143: ep_len:500 episode reward: total was -0.070000. running mean: -37.116366\n",
      "ep 2143: ep_len:1596 episode reward: total was -52.740000. running mean: -37.272602\n",
      "ep 2143: ep_len:7340 episode reward: total was -180.120000. running mean: -38.701076\n",
      "ep 2143: ep_len:500 episode reward: total was 2.420000. running mean: -38.289865\n",
      "ep 2143: ep_len:100 episode reward: total was 48.500000. running mean: -37.421967\n",
      "ep 2143: ep_len:154 episode reward: total was 74.000000. running mean: -36.307747\n",
      "ep 2143: ep_len:47 episode reward: total was 22.000000. running mean: -35.724669\n",
      "ep 2143: ep_len:635 episode reward: total was 7.700000. running mean: -35.290423\n",
      "ep 2143: ep_len:2833 episode reward: total was 3.530000. running mean: -34.902219\n",
      "epsilon:0.009992 episode_count: 32264. steps_count: 34607751.000000\n",
      "ep 2144: ep_len:646 episode reward: total was -13.360000. running mean: -34.686796\n",
      "ep 2144: ep_len:500 episode reward: total was 24.830000. running mean: -34.091628\n",
      "ep 2144: ep_len:55 episode reward: total was 26.000000. running mean: -33.490712\n",
      "ep 2144: ep_len:2962 episode reward: total was -115.410000. running mean: -34.309905\n",
      "ep 2144: ep_len:677 episode reward: total was 3.570000. running mean: -33.931106\n",
      "ep 2144: ep_len:36 episode reward: total was 15.000000. running mean: -33.441795\n",
      "ep 2144: ep_len:165 episode reward: total was 79.500000. running mean: -32.312377\n",
      "ep 2144: ep_len:68 episode reward: total was 32.500000. running mean: -31.664253\n",
      "ep 2144: ep_len:1448 episode reward: total was 2.710000. running mean: -31.320511\n",
      "ep 2144: ep_len:646 episode reward: total was 7.830000. running mean: -30.929006\n",
      "ep 2144: ep_len:774 episode reward: total was -34.920000. running mean: -30.968915\n",
      "ep 2144: ep_len:758 episode reward: total was -24.100000. running mean: -30.900226\n",
      "ep 2144: ep_len:567 episode reward: total was 25.480000. running mean: -30.336424\n",
      "ep 2144: ep_len:1117 episode reward: total was 17.490000. running mean: -29.858160\n",
      "ep 2144: ep_len:2824 episode reward: total was -13.950000. running mean: -29.699078\n",
      "epsilon:0.009992 episode_count: 32279. steps_count: 34620994.000000\n",
      "ep 2145: ep_len:1073 episode reward: total was -3.400000. running mean: -29.436087\n",
      "ep 2145: ep_len:919 episode reward: total was 25.740000. running mean: -28.884327\n",
      "ep 2145: ep_len:2986 episode reward: total was -9.440000. running mean: -28.689883\n",
      "ep 2145: ep_len:646 episode reward: total was -0.470000. running mean: -28.407684\n",
      "ep 2145: ep_len:36 episode reward: total was 16.500000. running mean: -27.958608\n",
      "ep 2145: ep_len:126 episode reward: total was 61.500000. running mean: -27.064022\n",
      "ep 2145: ep_len:46 episode reward: total was 20.000000. running mean: -26.593381\n",
      "ep 2145: ep_len:965 episode reward: total was -38.540000. running mean: -26.712848\n",
      "ep 2145: ep_len:344 episode reward: total was 6.680000. running mean: -26.378919\n",
      "ep 2145: ep_len:643 episode reward: total was -22.020000. running mean: -26.335330\n",
      "ep 2145: ep_len:750 episode reward: total was 33.790000. running mean: -25.734077\n",
      "ep 2145: ep_len:871 episode reward: total was 38.180000. running mean: -25.094936\n",
      "ep 2145: ep_len:53 episode reward: total was 25.000000. running mean: -24.593986\n",
      "ep 2145: ep_len:97 episode reward: total was 44.000000. running mean: -23.908047\n",
      "ep 2145: ep_len:1167 episode reward: total was -44.050000. running mean: -24.109466\n",
      "ep 2145: ep_len:2888 episode reward: total was -2.780000. running mean: -23.896171\n",
      "ep 2145: ep_len:64 episode reward: total was 30.500000. running mean: -23.352210\n",
      "epsilon:0.009992 episode_count: 32296. steps_count: 34634668.000000\n",
      "ep 2146: ep_len:1422 episode reward: total was 17.750000. running mean: -22.941188\n",
      "ep 2146: ep_len:1158 episode reward: total was 2.260000. running mean: -22.689176\n",
      "ep 2146: ep_len:2973 episode reward: total was -357.250000. running mean: -26.034784\n",
      "ep 2146: ep_len:892 episode reward: total was -4.080000. running mean: -25.815236\n",
      "ep 2146: ep_len:58 episode reward: total was 24.500000. running mean: -25.312084\n",
      "ep 2146: ep_len:114 episode reward: total was 52.500000. running mean: -24.533963\n",
      "ep 2146: ep_len:63 episode reward: total was 27.000000. running mean: -24.018623\n",
      "ep 2146: ep_len:767 episode reward: total was 17.690000. running mean: -23.601537\n",
      "ep 2146: ep_len:3679 episode reward: total was -118.540000. running mean: -24.550922\n",
      "ep 2146: ep_len:550 episode reward: total was -47.190000. running mean: -24.777312\n",
      "ep 2146: ep_len:7593 episode reward: total was -152.140000. running mean: -26.050939\n",
      "ep 2146: ep_len:1533 episode reward: total was 8.460000. running mean: -25.705830\n",
      "ep 2146: ep_len:56 episode reward: total was 26.500000. running mean: -25.183772\n",
      "ep 2146: ep_len:106 episode reward: total was 47.000000. running mean: -24.461934\n",
      "ep 2146: ep_len:1503 episode reward: total was -4.090000. running mean: -24.258215\n",
      "ep 2146: ep_len:2929 episode reward: total was 14.160000. running mean: -23.874032\n",
      "epsilon:0.009992 episode_count: 32312. steps_count: 34660064.000000\n",
      "ep 2147: ep_len:814 episode reward: total was -20.310000. running mean: -23.838392\n",
      "ep 2147: ep_len:1645 episode reward: total was -108.750000. running mean: -24.687508\n",
      "ep 2147: ep_len:2979 episode reward: total was -39.000000. running mean: -24.830633\n",
      "ep 2147: ep_len:4613 episode reward: total was -892.220000. running mean: -33.504527\n",
      "ep 2147: ep_len:61 episode reward: total was 27.500000. running mean: -32.894482\n",
      "ep 2147: ep_len:80 episode reward: total was 38.500000. running mean: -32.180537\n",
      "ep 2147: ep_len:69 episode reward: total was 33.000000. running mean: -31.528731\n",
      "ep 2147: ep_len:614 episode reward: total was 44.210000. running mean: -30.771344\n",
      "ep 2147: ep_len:4053 episode reward: total was -94.580000. running mean: -31.409431\n",
      "ep 2147: ep_len:2248 episode reward: total was -311.690000. running mean: -34.212236\n",
      "ep 2147: ep_len:829 episode reward: total was 32.010000. running mean: -33.550014\n",
      "ep 2147: ep_len:500 episode reward: total was 0.030000. running mean: -33.214214\n",
      "ep 2147: ep_len:152 episode reward: total was 73.000000. running mean: -32.152072\n",
      "ep 2147: ep_len:39 episode reward: total was 18.000000. running mean: -31.650551\n",
      "ep 2147: ep_len:636 episode reward: total was -0.550000. running mean: -31.339545\n",
      "ep 2147: ep_len:2813 episode reward: total was -64.070000. running mean: -31.666850\n",
      "ep 2147: ep_len:66 episode reward: total was 30.000000. running mean: -31.050181\n",
      "epsilon:0.009992 episode_count: 32329. steps_count: 34682275.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2148: ep_len:629 episode reward: total was 16.980000. running mean: -30.569880\n",
      "ep 2148: ep_len:775 episode reward: total was -9.590000. running mean: -30.360081\n",
      "ep 2148: ep_len:53 episode reward: total was 23.500000. running mean: -29.821480\n",
      "ep 2148: ep_len:3130 episode reward: total was -4.950000. running mean: -29.572765\n",
      "ep 2148: ep_len:832 episode reward: total was 24.310000. running mean: -29.033938\n",
      "ep 2148: ep_len:167 episode reward: total was 82.000000. running mean: -27.923598\n",
      "ep 2148: ep_len:769 episode reward: total was -9.650000. running mean: -27.740862\n",
      "ep 2148: ep_len:3964 episode reward: total was -192.090000. running mean: -29.384354\n",
      "ep 2148: ep_len:588 episode reward: total was 24.850000. running mean: -28.842010\n",
      "ep 2148: ep_len:790 episode reward: total was 15.210000. running mean: -28.401490\n",
      "ep 2148: ep_len:606 episode reward: total was 3.680000. running mean: -28.080675\n",
      "ep 2148: ep_len:110 episode reward: total was 50.500000. running mean: -27.294868\n",
      "ep 2148: ep_len:32 episode reward: total was 14.500000. running mean: -26.876920\n",
      "ep 2148: ep_len:122 episode reward: total was 55.000000. running mean: -26.058150\n",
      "ep 2148: ep_len:1030 episode reward: total was 4.160000. running mean: -25.755969\n",
      "ep 2148: ep_len:2787 episode reward: total was -18.240000. running mean: -25.680809\n",
      "epsilon:0.009992 episode_count: 32345. steps_count: 34698659.000000\n",
      "ep 2149: ep_len:1439 episode reward: total was 16.670000. running mean: -25.257301\n",
      "ep 2149: ep_len:760 episode reward: total was -4.690000. running mean: -25.051628\n",
      "ep 2149: ep_len:47 episode reward: total was 22.000000. running mean: -24.581112\n",
      "ep 2149: ep_len:2947 episode reward: total was -16.210000. running mean: -24.497401\n",
      "ep 2149: ep_len:789 episode reward: total was -10.460000. running mean: -24.357027\n",
      "ep 2149: ep_len:58 episode reward: total was 27.500000. running mean: -23.838456\n",
      "ep 2149: ep_len:155 episode reward: total was 76.000000. running mean: -22.840072\n",
      "ep 2149: ep_len:63 episode reward: total was 30.000000. running mean: -22.311671\n",
      "ep 2149: ep_len:987 episode reward: total was 0.610000. running mean: -22.082454\n",
      "ep 2149: ep_len:341 episode reward: total was 10.510000. running mean: -21.756530\n",
      "ep 2149: ep_len:1565 episode reward: total was -54.920000. running mean: -22.088165\n",
      "ep 2149: ep_len:834 episode reward: total was 60.210000. running mean: -21.265183\n",
      "ep 2149: ep_len:669 episode reward: total was 10.680000. running mean: -20.945731\n",
      "ep 2149: ep_len:143 episode reward: total was 71.010000. running mean: -20.026174\n",
      "ep 2149: ep_len:66 episode reward: total was 31.500000. running mean: -19.510912\n",
      "ep 2149: ep_len:799 episode reward: total was -77.200000. running mean: -20.087803\n",
      "ep 2149: ep_len:2845 episode reward: total was -41.680000. running mean: -20.303725\n",
      "ep 2149: ep_len:63 episode reward: total was 30.000000. running mean: -19.800688\n",
      "epsilon:0.009992 episode_count: 32363. steps_count: 34713229.000000\n",
      "ep 2150: ep_len:841 episode reward: total was -14.280000. running mean: -19.745481\n",
      "ep 2150: ep_len:789 episode reward: total was -12.440000. running mean: -19.672426\n",
      "ep 2150: ep_len:3062 episode reward: total was -40.510000. running mean: -19.880802\n",
      "ep 2150: ep_len:1484 episode reward: total was -2.350000. running mean: -19.705494\n",
      "ep 2150: ep_len:70 episode reward: total was 32.000000. running mean: -19.188439\n",
      "ep 2150: ep_len:77 episode reward: total was 34.000000. running mean: -18.656554\n",
      "ep 2150: ep_len:1388 episode reward: total was 8.960000. running mean: -18.380389\n",
      "ep 2150: ep_len:633 episode reward: total was 17.370000. running mean: -18.022885\n",
      "ep 2150: ep_len:528 episode reward: total was 1.070000. running mean: -17.831956\n",
      "ep 2150: ep_len:744 episode reward: total was 12.820000. running mean: -17.525437\n",
      "ep 2150: ep_len:561 episode reward: total was 22.660000. running mean: -17.123582\n",
      "ep 2150: ep_len:140 episode reward: total was 68.500000. running mean: -16.267346\n",
      "ep 2150: ep_len:40 episode reward: total was 18.500000. running mean: -15.919673\n",
      "ep 2150: ep_len:1516 episode reward: total was 0.300000. running mean: -15.757476\n",
      "ep 2150: ep_len:2841 episode reward: total was -14.270000. running mean: -15.742601\n",
      "epsilon:0.009992 episode_count: 32378. steps_count: 34727943.000000\n",
      "ep 2151: ep_len:1467 episode reward: total was 9.480000. running mean: -15.490375\n",
      "ep 2151: ep_len:709 episode reward: total was -16.240000. running mean: -15.497872\n",
      "ep 2151: ep_len:2980 episode reward: total was -78.590000. running mean: -16.128793\n",
      "ep 2151: ep_len:770 episode reward: total was 18.980000. running mean: -15.777705\n",
      "ep 2151: ep_len:65 episode reward: total was 29.500000. running mean: -15.324928\n",
      "ep 2151: ep_len:122 episode reward: total was 58.000000. running mean: -14.591679\n",
      "ep 2151: ep_len:73 episode reward: total was 33.500000. running mean: -14.110762\n",
      "ep 2151: ep_len:1464 episode reward: total was 9.850000. running mean: -13.871154\n",
      "ep 2151: ep_len:3629 episode reward: total was -419.010000. running mean: -17.922543\n",
      "ep 2151: ep_len:554 episode reward: total was -54.220000. running mean: -18.285517\n",
      "ep 2151: ep_len:668 episode reward: total was 23.510000. running mean: -17.867562\n",
      "ep 2151: ep_len:1088 episode reward: total was -69.610000. running mean: -18.384987\n",
      "ep 2151: ep_len:178 episode reward: total was 86.000000. running mean: -17.341137\n",
      "ep 2151: ep_len:68 episode reward: total was 32.500000. running mean: -16.842725\n",
      "ep 2151: ep_len:944 episode reward: total was -373.370000. running mean: -20.407998\n",
      "ep 2151: ep_len:38 episode reward: total was 16.000000. running mean: -20.043918\n",
      "ep 2151: ep_len:60 episode reward: total was 28.500000. running mean: -19.558479\n",
      "epsilon:0.009992 episode_count: 32395. steps_count: 34742820.000000\n",
      "ep 2152: ep_len:1050 episode reward: total was -95.720000. running mean: -20.320094\n",
      "ep 2152: ep_len:753 episode reward: total was -64.120000. running mean: -20.758093\n",
      "ep 2152: ep_len:66 episode reward: total was 31.500000. running mean: -20.235512\n",
      "ep 2152: ep_len:3004 episode reward: total was 10.790000. running mean: -19.925257\n",
      "ep 2152: ep_len:689 episode reward: total was -17.190000. running mean: -19.897905\n",
      "ep 2152: ep_len:41 episode reward: total was 19.000000. running mean: -19.508925\n",
      "ep 2152: ep_len:72 episode reward: total was 30.000000. running mean: -19.013836\n",
      "ep 2152: ep_len:49 episode reward: total was 23.000000. running mean: -18.593698\n",
      "ep 2152: ep_len:926 episode reward: total was 59.280000. running mean: -17.814961\n",
      "ep 2152: ep_len:3958 episode reward: total was -106.660000. running mean: -18.703411\n",
      "ep 2152: ep_len:611 episode reward: total was -18.300000. running mean: -18.699377\n",
      "ep 2152: ep_len:624 episode reward: total was 0.870000. running mean: -18.503683\n",
      "ep 2152: ep_len:952 episode reward: total was 44.720000. running mean: -17.871447\n",
      "ep 2152: ep_len:71 episode reward: total was 32.500000. running mean: -17.367732\n",
      "ep 2152: ep_len:177 episode reward: total was 84.000000. running mean: -16.354055\n",
      "ep 2152: ep_len:59 episode reward: total was 26.500000. running mean: -15.925514\n",
      "ep 2152: ep_len:1092 episode reward: total was -23.590000. running mean: -16.002159\n",
      "ep 2152: ep_len:2895 episode reward: total was -62.640000. running mean: -16.468537\n",
      "ep 2152: ep_len:47 episode reward: total was 20.500000. running mean: -16.098852\n",
      "epsilon:0.009992 episode_count: 32414. steps_count: 34759956.000000\n",
      "ep 2153: ep_len:717 episode reward: total was -45.520000. running mean: -16.393064\n",
      "ep 2153: ep_len:755 episode reward: total was -28.280000. running mean: -16.511933\n",
      "ep 2153: ep_len:3025 episode reward: total was -127.390000. running mean: -17.620714\n",
      "ep 2153: ep_len:800 episode reward: total was 43.070000. running mean: -17.013806\n",
      "ep 2153: ep_len:70 episode reward: total was 32.000000. running mean: -16.523668\n",
      "ep 2153: ep_len:1433 episode reward: total was -2.860000. running mean: -16.387032\n",
      "ep 2153: ep_len:647 episode reward: total was 9.550000. running mean: -16.127661\n",
      "ep 2153: ep_len:869 episode reward: total was 14.060000. running mean: -15.825785\n",
      "ep 2153: ep_len:684 episode reward: total was 0.740000. running mean: -15.660127\n",
      "ep 2153: ep_len:547 episode reward: total was 11.650000. running mean: -15.387026\n",
      "ep 2153: ep_len:69 episode reward: total was 33.000000. running mean: -14.903155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2153: ep_len:62 episode reward: total was 29.500000. running mean: -14.459124\n",
      "ep 2153: ep_len:89 episode reward: total was 43.000000. running mean: -13.884533\n",
      "ep 2153: ep_len:1497 episode reward: total was -4.390000. running mean: -13.789587\n",
      "ep 2153: ep_len:2861 episode reward: total was -18.390000. running mean: -13.835591\n",
      "epsilon:0.009992 episode_count: 32429. steps_count: 34774081.000000\n",
      "ep 2154: ep_len:909 episode reward: total was -47.300000. running mean: -14.170236\n",
      "ep 2154: ep_len:166 episode reward: total was -8.080000. running mean: -14.109333\n",
      "ep 2154: ep_len:65 episode reward: total was 31.000000. running mean: -13.658240\n",
      "ep 2154: ep_len:2959 episode reward: total was -181.670000. running mean: -15.338357\n",
      "ep 2154: ep_len:751 episode reward: total was 23.100000. running mean: -14.953974\n",
      "ep 2154: ep_len:111 episode reward: total was 54.000000. running mean: -14.264434\n",
      "ep 2154: ep_len:65 episode reward: total was 29.500000. running mean: -13.826790\n",
      "ep 2154: ep_len:56 episode reward: total was 25.000000. running mean: -13.438522\n",
      "ep 2154: ep_len:1497 episode reward: total was 19.460000. running mean: -13.109537\n",
      "ep 2154: ep_len:3639 episode reward: total was -28.040000. running mean: -13.258841\n",
      "ep 2154: ep_len:807 episode reward: total was -41.080000. running mean: -13.537053\n",
      "ep 2154: ep_len:735 episode reward: total was -83.570000. running mean: -14.237382\n",
      "ep 2154: ep_len:560 episode reward: total was 27.580000. running mean: -13.819209\n",
      "ep 2154: ep_len:132 episode reward: total was 58.500000. running mean: -13.096016\n",
      "ep 2154: ep_len:39 episode reward: total was 18.000000. running mean: -12.785056\n",
      "ep 2154: ep_len:118 episode reward: total was 56.000000. running mean: -12.097206\n",
      "ep 2154: ep_len:1469 episode reward: total was -2.650000. running mean: -12.002734\n",
      "ep 2154: ep_len:2855 episode reward: total was -39.200000. running mean: -12.274706\n",
      "ep 2154: ep_len:34 episode reward: total was 14.000000. running mean: -12.011959\n",
      "epsilon:0.009992 episode_count: 32448. steps_count: 34791048.000000\n",
      "ep 2155: ep_len:1391 episode reward: total was 9.390000. running mean: -11.797940\n",
      "ep 2155: ep_len:1002 episode reward: total was 15.680000. running mean: -11.523160\n",
      "ep 2155: ep_len:99 episode reward: total was 48.000000. running mean: -10.927929\n",
      "ep 2155: ep_len:887 episode reward: total was 76.370000. running mean: -10.054949\n",
      "ep 2155: ep_len:74 episode reward: total was 34.000000. running mean: -9.614400\n",
      "ep 2155: ep_len:1491 episode reward: total was 16.210000. running mean: -9.356156\n",
      "ep 2155: ep_len:3625 episode reward: total was -60.410000. running mean: -9.866694\n",
      "ep 2155: ep_len:1275 episode reward: total was -90.070000. running mean: -10.668727\n",
      "ep 2155: ep_len:659 episode reward: total was 3.820000. running mean: -10.523840\n",
      "ep 2155: ep_len:515 episode reward: total was 27.830000. running mean: -10.140302\n",
      "ep 2155: ep_len:151 episode reward: total was 69.500000. running mean: -9.343899\n",
      "ep 2155: ep_len:50 episode reward: total was 22.000000. running mean: -9.030460\n",
      "ep 2155: ep_len:785 episode reward: total was -79.670000. running mean: -9.736855\n",
      "ep 2155: ep_len:2837 episode reward: total was -3.470000. running mean: -9.674187\n",
      "ep 2155: ep_len:66 episode reward: total was 31.500000. running mean: -9.262445\n",
      "epsilon:0.009992 episode_count: 32463. steps_count: 34805955.000000\n",
      "ep 2156: ep_len:500 episode reward: total was 20.330000. running mean: -8.966520\n",
      "ep 2156: ep_len:795 episode reward: total was -33.840000. running mean: -9.215255\n",
      "ep 2156: ep_len:2954 episode reward: total was -53.110000. running mean: -9.654203\n",
      "ep 2156: ep_len:668 episode reward: total was -6.230000. running mean: -9.619960\n",
      "ep 2156: ep_len:92 episode reward: total was 43.000000. running mean: -9.093761\n",
      "ep 2156: ep_len:634 episode reward: total was 1.370000. running mean: -8.989123\n",
      "ep 2156: ep_len:4043 episode reward: total was -101.440000. running mean: -9.913632\n",
      "ep 2156: ep_len:1273 episode reward: total was -51.070000. running mean: -10.325196\n",
      "ep 2156: ep_len:744 episode reward: total was 32.510000. running mean: -9.896844\n",
      "ep 2156: ep_len:568 episode reward: total was 26.800000. running mean: -9.529875\n",
      "ep 2156: ep_len:45 episode reward: total was 21.000000. running mean: -9.224577\n",
      "ep 2156: ep_len:41 episode reward: total was 19.000000. running mean: -8.942331\n",
      "ep 2156: ep_len:749 episode reward: total was -32.070000. running mean: -9.173607\n",
      "ep 2156: ep_len:30 episode reward: total was 13.500000. running mean: -8.946871\n",
      "epsilon:0.009992 episode_count: 32477. steps_count: 34819091.000000\n",
      "ep 2157: ep_len:709 episode reward: total was -8.230000. running mean: -8.939703\n",
      "ep 2157: ep_len:707 episode reward: total was -11.850000. running mean: -8.968806\n",
      "ep 2157: ep_len:3038 episode reward: total was -60.700000. running mean: -9.486118\n",
      "ep 2157: ep_len:1167 episode reward: total was -35.380000. running mean: -9.745056\n",
      "ep 2157: ep_len:29 episode reward: total was 13.000000. running mean: -9.517606\n",
      "ep 2157: ep_len:88 episode reward: total was 41.000000. running mean: -9.012430\n",
      "ep 2157: ep_len:43 episode reward: total was 20.000000. running mean: -8.722306\n",
      "ep 2157: ep_len:910 episode reward: total was 26.350000. running mean: -8.371582\n",
      "ep 2157: ep_len:658 episode reward: total was 28.150000. running mean: -8.006367\n",
      "ep 2157: ep_len:1489 episode reward: total was -42.460000. running mean: -8.350903\n",
      "ep 2157: ep_len:757 episode reward: total was -15.150000. running mean: -8.418894\n",
      "ep 2157: ep_len:684 episode reward: total was 10.110000. running mean: -8.233605\n",
      "ep 2157: ep_len:69 episode reward: total was 31.500000. running mean: -7.836269\n",
      "ep 2157: ep_len:609 episode reward: total was -11.250000. running mean: -7.870406\n",
      "ep 2157: ep_len:2841 episode reward: total was 8.320000. running mean: -7.708502\n",
      "epsilon:0.009992 episode_count: 32492. steps_count: 34832889.000000\n",
      "ep 2158: ep_len:3389 episode reward: total was -399.930000. running mean: -11.630717\n",
      "ep 2158: ep_len:975 episode reward: total was 32.000000. running mean: -11.194410\n",
      "ep 2158: ep_len:73 episode reward: total was 33.500000. running mean: -10.747466\n",
      "ep 2158: ep_len:2977 episode reward: total was -128.770000. running mean: -11.927691\n",
      "ep 2158: ep_len:656 episode reward: total was 7.210000. running mean: -11.736314\n",
      "ep 2158: ep_len:131 episode reward: total was 62.500000. running mean: -10.993951\n",
      "ep 2158: ep_len:845 episode reward: total was 26.770000. running mean: -10.616312\n",
      "ep 2158: ep_len:667 episode reward: total was 30.380000. running mean: -10.206349\n",
      "ep 2158: ep_len:4076 episode reward: total was -1099.760000. running mean: -21.101885\n",
      "ep 2158: ep_len:685 episode reward: total was 22.090000. running mean: -20.669966\n",
      "ep 2158: ep_len:500 episode reward: total was 12.060000. running mean: -20.342667\n",
      "ep 2158: ep_len:500 episode reward: total was 1.040000. running mean: -20.128840\n",
      "ep 2158: ep_len:2869 episode reward: total was -26.750000. running mean: -20.195051\n",
      "epsilon:0.009992 episode_count: 32505. steps_count: 34851232.000000\n",
      "ep 2159: ep_len:699 episode reward: total was -45.180000. running mean: -20.444901\n",
      "ep 2159: ep_len:500 episode reward: total was 28.630000. running mean: -19.954152\n",
      "ep 2159: ep_len:3009 episode reward: total was -42.370000. running mean: -20.178310\n",
      "ep 2159: ep_len:808 episode reward: total was 23.520000. running mean: -19.741327\n",
      "ep 2159: ep_len:46 episode reward: total was 20.000000. running mean: -19.343914\n",
      "ep 2159: ep_len:155 episode reward: total was 74.500000. running mean: -18.405475\n",
      "ep 2159: ep_len:64 episode reward: total was 30.500000. running mean: -17.916420\n",
      "ep 2159: ep_len:614 episode reward: total was 49.260000. running mean: -17.244656\n",
      "ep 2159: ep_len:639 episode reward: total was 29.460000. running mean: -16.777609\n",
      "ep 2159: ep_len:1622 episode reward: total was -91.510000. running mean: -17.524933\n",
      "ep 2159: ep_len:785 episode reward: total was 12.770000. running mean: -17.221984\n",
      "ep 2159: ep_len:617 episode reward: total was 4.190000. running mean: -17.007864\n",
      "ep 2159: ep_len:79 episode reward: total was 38.000000. running mean: -16.457786\n",
      "ep 2159: ep_len:758 episode reward: total was -37.120000. running mean: -16.664408\n",
      "ep 2159: ep_len:2782 episode reward: total was -6.780000. running mean: -16.565564\n",
      "epsilon:0.009992 episode_count: 32520. steps_count: 34864409.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2160: ep_len:1127 episode reward: total was 8.450000. running mean: -16.315408\n",
      "ep 2160: ep_len:1609 episode reward: total was -66.690000. running mean: -16.819154\n",
      "ep 2160: ep_len:2919 episode reward: total was 10.890000. running mean: -16.542062\n",
      "ep 2160: ep_len:691 episode reward: total was 3.860000. running mean: -16.338042\n",
      "ep 2160: ep_len:51 episode reward: total was 24.000000. running mean: -15.934661\n",
      "ep 2160: ep_len:73 episode reward: total was 33.500000. running mean: -15.440315\n",
      "ep 2160: ep_len:1136 episode reward: total was -1.660000. running mean: -15.302512\n",
      "ep 2160: ep_len:310 episode reward: total was 8.510000. running mean: -15.064386\n",
      "ep 2160: ep_len:1259 episode reward: total was -58.280000. running mean: -15.496543\n",
      "ep 2160: ep_len:724 episode reward: total was 44.260000. running mean: -14.898977\n",
      "ep 2160: ep_len:653 episode reward: total was -3.740000. running mean: -14.787387\n",
      "ep 2160: ep_len:94 episode reward: total was 45.500000. running mean: -14.184513\n",
      "ep 2160: ep_len:647 episode reward: total was -1.190000. running mean: -14.054568\n",
      "ep 2160: ep_len:2859 episode reward: total was -14.790000. running mean: -14.061923\n",
      "ep 2160: ep_len:38 episode reward: total was 16.000000. running mean: -13.761303\n",
      "epsilon:0.009992 episode_count: 32535. steps_count: 34878599.000000\n",
      "ep 2161: ep_len:758 episode reward: total was -28.240000. running mean: -13.906090\n",
      "ep 2161: ep_len:201 episode reward: total was 8.830000. running mean: -13.678729\n",
      "ep 2161: ep_len:3011 episode reward: total was -19.170000. running mean: -13.733642\n",
      "ep 2161: ep_len:500 episode reward: total was -15.240000. running mean: -13.748706\n",
      "ep 2161: ep_len:126 episode reward: total was 60.000000. running mean: -13.011219\n",
      "ep 2161: ep_len:983 episode reward: total was -40.070000. running mean: -13.281807\n",
      "ep 2161: ep_len:3826 episode reward: total was -632.090000. running mean: -19.469888\n",
      "ep 2161: ep_len:694 episode reward: total was -9.390000. running mean: -19.369090\n",
      "ep 2161: ep_len:7496 episode reward: total was -318.490000. running mean: -22.360299\n",
      "ep 2161: ep_len:500 episode reward: total was -4.840000. running mean: -22.185096\n",
      "ep 2161: ep_len:66 episode reward: total was 27.000000. running mean: -21.693245\n",
      "ep 2161: ep_len:80 episode reward: total was 38.500000. running mean: -21.091312\n",
      "ep 2161: ep_len:500 episode reward: total was 36.070000. running mean: -20.519699\n",
      "ep 2161: ep_len:2876 episode reward: total was -18.910000. running mean: -20.503602\n",
      "epsilon:0.009992 episode_count: 32549. steps_count: 34900216.000000\n",
      "ep 2162: ep_len:1404 episode reward: total was 11.180000. running mean: -20.186766\n",
      "ep 2162: ep_len:720 episode reward: total was -15.820000. running mean: -20.143098\n",
      "ep 2162: ep_len:3006 episode reward: total was -53.890000. running mean: -20.480568\n",
      "ep 2162: ep_len:708 episode reward: total was 18.300000. running mean: -20.092762\n",
      "ep 2162: ep_len:740 episode reward: total was -14.470000. running mean: -20.036534\n",
      "ep 2162: ep_len:4173 episode reward: total was -232.940000. running mean: -22.165569\n",
      "ep 2162: ep_len:1562 episode reward: total was -27.760000. running mean: -22.221513\n",
      "ep 2162: ep_len:711 episode reward: total was 22.230000. running mean: -21.776998\n",
      "ep 2162: ep_len:616 episode reward: total was 44.280000. running mean: -21.116428\n",
      "ep 2162: ep_len:37 episode reward: total was 17.000000. running mean: -20.735264\n",
      "ep 2162: ep_len:113 episode reward: total was 53.500000. running mean: -19.992911\n",
      "ep 2162: ep_len:766 episode reward: total was -51.150000. running mean: -20.304482\n",
      "ep 2162: ep_len:2844 episode reward: total was 0.510000. running mean: -20.096337\n",
      "ep 2162: ep_len:38 episode reward: total was 17.500000. running mean: -19.720374\n",
      "epsilon:0.009992 episode_count: 32563. steps_count: 34917654.000000\n",
      "ep 2163: ep_len:1106 episode reward: total was -3.310000. running mean: -19.556270\n",
      "ep 2163: ep_len:988 episode reward: total was -6.690000. running mean: -19.427607\n",
      "ep 2163: ep_len:2908 episode reward: total was -133.910000. running mean: -20.572431\n",
      "ep 2163: ep_len:1143 episode reward: total was -66.290000. running mean: -21.029607\n",
      "ep 2163: ep_len:79 episode reward: total was 36.500000. running mean: -20.454311\n",
      "ep 2163: ep_len:1137 episode reward: total was 2.000000. running mean: -20.229768\n",
      "ep 2163: ep_len:319 episode reward: total was 24.890000. running mean: -19.778570\n",
      "ep 2163: ep_len:504 episode reward: total was -40.580000. running mean: -19.986584\n",
      "ep 2163: ep_len:687 episode reward: total was 3.580000. running mean: -19.750919\n",
      "ep 2163: ep_len:1381 episode reward: total was 5.580000. running mean: -19.497609\n",
      "ep 2163: ep_len:177 episode reward: total was 84.000000. running mean: -18.462633\n",
      "ep 2163: ep_len:58 episode reward: total was 26.000000. running mean: -18.018007\n",
      "ep 2163: ep_len:91 episode reward: total was 44.000000. running mean: -17.397827\n",
      "ep 2163: ep_len:770 episode reward: total was -37.830000. running mean: -17.602149\n",
      "ep 2163: ep_len:2792 episode reward: total was -13.320000. running mean: -17.559327\n",
      "ep 2163: ep_len:61 episode reward: total was 27.500000. running mean: -17.108734\n",
      "epsilon:0.009992 episode_count: 32579. steps_count: 34931855.000000\n",
      "ep 2164: ep_len:1129 episode reward: total was -16.150000. running mean: -17.099147\n",
      "ep 2164: ep_len:635 episode reward: total was -14.940000. running mean: -17.077555\n",
      "ep 2164: ep_len:2932 episode reward: total was -75.310000. running mean: -17.659880\n",
      "ep 2164: ep_len:1214 episode reward: total was -54.690000. running mean: -18.030181\n",
      "ep 2164: ep_len:38 episode reward: total was 17.500000. running mean: -17.674879\n",
      "ep 2164: ep_len:46 episode reward: total was 21.500000. running mean: -17.283130\n",
      "ep 2164: ep_len:58 episode reward: total was 27.500000. running mean: -16.835299\n",
      "ep 2164: ep_len:758 episode reward: total was -11.780000. running mean: -16.784746\n",
      "ep 2164: ep_len:3942 episode reward: total was -106.820000. running mean: -17.685098\n",
      "ep 2164: ep_len:748 episode reward: total was -30.740000. running mean: -17.815647\n",
      "ep 2164: ep_len:694 episode reward: total was -12.270000. running mean: -17.760191\n",
      "ep 2164: ep_len:928 episode reward: total was -8.150000. running mean: -17.664089\n",
      "ep 2164: ep_len:39 episode reward: total was 18.000000. running mean: -17.307448\n",
      "ep 2164: ep_len:664 episode reward: total was -23.250000. running mean: -17.366874\n",
      "ep 2164: ep_len:2870 episode reward: total was 0.560000. running mean: -17.187605\n",
      "epsilon:0.009992 episode_count: 32594. steps_count: 34948550.000000\n",
      "ep 2165: ep_len:1148 episode reward: total was -22.360000. running mean: -17.239329\n",
      "ep 2165: ep_len:1234 episode reward: total was -55.410000. running mean: -17.621036\n",
      "ep 2165: ep_len:3031 episode reward: total was -25.930000. running mean: -17.704125\n",
      "ep 2165: ep_len:1077 episode reward: total was -9.600000. running mean: -17.623084\n",
      "ep 2165: ep_len:500 episode reward: total was -0.230000. running mean: -17.449153\n",
      "ep 2165: ep_len:310 episode reward: total was 3.830000. running mean: -17.236362\n",
      "ep 2165: ep_len:1661 episode reward: total was -94.000000. running mean: -18.003998\n",
      "ep 2165: ep_len:781 episode reward: total was 1.490000. running mean: -17.809058\n",
      "ep 2165: ep_len:847 episode reward: total was 21.620000. running mean: -17.414767\n",
      "ep 2165: ep_len:42 episode reward: total was 19.500000. running mean: -17.045620\n",
      "ep 2165: ep_len:1131 episode reward: total was -0.980000. running mean: -16.884964\n",
      "ep 2165: ep_len:2728 episode reward: total was -59.110000. running mean: -17.307214\n",
      "ep 2165: ep_len:41 episode reward: total was 19.000000. running mean: -16.944142\n",
      "epsilon:0.009992 episode_count: 32607. steps_count: 34963081.000000\n",
      "ep 2166: ep_len:993 episode reward: total was -49.840000. running mean: -17.273100\n",
      "ep 2166: ep_len:979 episode reward: total was 1.820000. running mean: -17.082169\n",
      "ep 2166: ep_len:3077 episode reward: total was -3.570000. running mean: -16.947048\n",
      "ep 2166: ep_len:1476 episode reward: total was 19.150000. running mean: -16.586077\n",
      "ep 2166: ep_len:37 episode reward: total was 17.000000. running mean: -16.250216\n",
      "ep 2166: ep_len:1020 episode reward: total was -26.970000. running mean: -16.357414\n",
      "ep 2166: ep_len:3748 episode reward: total was -26.710000. running mean: -16.460940\n",
      "ep 2166: ep_len:888 episode reward: total was -54.800000. running mean: -16.844331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2166: ep_len:7255 episode reward: total was -5.880000. running mean: -16.734687\n",
      "ep 2166: ep_len:559 episode reward: total was 27.780000. running mean: -16.289541\n",
      "ep 2166: ep_len:74 episode reward: total was 35.500000. running mean: -15.771645\n",
      "ep 2166: ep_len:41 episode reward: total was 19.000000. running mean: -15.423929\n",
      "ep 2166: ep_len:859 episode reward: total was -8.920000. running mean: -15.358889\n",
      "ep 2166: ep_len:2899 episode reward: total was -4.200000. running mean: -15.247301\n",
      "epsilon:0.009992 episode_count: 32621. steps_count: 34986986.000000\n",
      "ep 2167: ep_len:841 episode reward: total was -8.000000. running mean: -15.174827\n",
      "ep 2167: ep_len:777 episode reward: total was 4.640000. running mean: -14.976679\n",
      "ep 2167: ep_len:3023 episode reward: total was -64.900000. running mean: -15.475912\n",
      "ep 2167: ep_len:867 episode reward: total was 7.080000. running mean: -15.250353\n",
      "ep 2167: ep_len:611 episode reward: total was 32.090000. running mean: -14.776950\n",
      "ep 2167: ep_len:331 episode reward: total was 14.910000. running mean: -14.480080\n",
      "ep 2167: ep_len:540 episode reward: total was -16.350000. running mean: -14.498779\n",
      "ep 2167: ep_len:7258 episode reward: total was 58.730000. running mean: -13.766492\n",
      "ep 2167: ep_len:583 episode reward: total was -32.720000. running mean: -13.956027\n",
      "ep 2167: ep_len:49 episode reward: total was 23.000000. running mean: -13.586466\n",
      "ep 2167: ep_len:141 episode reward: total was 66.000000. running mean: -12.790602\n",
      "ep 2167: ep_len:104 episode reward: total was 46.000000. running mean: -12.202696\n",
      "ep 2167: ep_len:1050 episode reward: total was 0.660000. running mean: -12.074069\n",
      "ep 2167: ep_len:2893 episode reward: total was 7.340000. running mean: -11.879928\n",
      "ep 2167: ep_len:74 episode reward: total was 35.500000. running mean: -11.406129\n",
      "epsilon:0.009992 episode_count: 32636. steps_count: 35006128.000000\n",
      "ep 2168: ep_len:1015 episode reward: total was -18.160000. running mean: -11.473668\n",
      "ep 2168: ep_len:683 episode reward: total was -47.880000. running mean: -11.837731\n",
      "ep 2168: ep_len:49 episode reward: total was 23.000000. running mean: -11.489354\n",
      "ep 2168: ep_len:2920 episode reward: total was -33.650000. running mean: -11.710960\n",
      "ep 2168: ep_len:1713 episode reward: total was -69.960000. running mean: -12.293450\n",
      "ep 2168: ep_len:92 episode reward: total was 44.500000. running mean: -11.725516\n",
      "ep 2168: ep_len:1051 episode reward: total was -101.770000. running mean: -12.625961\n",
      "ep 2168: ep_len:642 episode reward: total was 16.570000. running mean: -12.334001\n",
      "ep 2168: ep_len:708 episode reward: total was 2.480000. running mean: -12.185861\n",
      "ep 2168: ep_len:854 episode reward: total was 60.810000. running mean: -11.455903\n",
      "ep 2168: ep_len:561 episode reward: total was 42.930000. running mean: -10.912044\n",
      "ep 2168: ep_len:52 episode reward: total was 24.500000. running mean: -10.557923\n",
      "ep 2168: ep_len:67 episode reward: total was 32.000000. running mean: -10.132344\n",
      "ep 2168: ep_len:76 episode reward: total was 36.500000. running mean: -9.666020\n",
      "ep 2168: ep_len:602 episode reward: total was -6.270000. running mean: -9.632060\n",
      "ep 2168: ep_len:2781 episode reward: total was -1.620000. running mean: -9.551940\n",
      "epsilon:0.009992 episode_count: 32652. steps_count: 35019994.000000\n",
      "ep 2169: ep_len:724 episode reward: total was 8.430000. running mean: -9.372120\n",
      "ep 2169: ep_len:738 episode reward: total was -54.380000. running mean: -9.822199\n",
      "ep 2169: ep_len:68 episode reward: total was 31.000000. running mean: -9.413977\n",
      "ep 2169: ep_len:3029 episode reward: total was -15.250000. running mean: -9.472337\n",
      "ep 2169: ep_len:594 episode reward: total was -2.310000. running mean: -9.400714\n",
      "ep 2169: ep_len:59 episode reward: total was 28.000000. running mean: -9.026707\n",
      "ep 2169: ep_len:39 episode reward: total was 18.000000. running mean: -8.756440\n",
      "ep 2169: ep_len:500 episode reward: total was 19.330000. running mean: -8.475575\n",
      "ep 2169: ep_len:3642 episode reward: total was -1651.810000. running mean: -24.908920\n",
      "ep 2169: ep_len:560 episode reward: total was 15.940000. running mean: -24.500430\n",
      "ep 2169: ep_len:675 episode reward: total was 8.240000. running mean: -24.173026\n",
      "ep 2169: ep_len:662 episode reward: total was -5.010000. running mean: -23.981396\n",
      "ep 2169: ep_len:853 episode reward: total was 1.350000. running mean: -23.728082\n",
      "ep 2169: ep_len:2798 episode reward: total was -8.460000. running mean: -23.575401\n",
      "epsilon:0.009992 episode_count: 32666. steps_count: 35034935.000000\n",
      "ep 2170: ep_len:578 episode reward: total was 18.980000. running mean: -23.149847\n",
      "ep 2170: ep_len:709 episode reward: total was -24.880000. running mean: -23.167149\n",
      "ep 2170: ep_len:2929 episode reward: total was -39.710000. running mean: -23.332577\n",
      "ep 2170: ep_len:829 episode reward: total was 13.720000. running mean: -22.962051\n",
      "ep 2170: ep_len:37 episode reward: total was 15.500000. running mean: -22.577431\n",
      "ep 2170: ep_len:611 episode reward: total was 44.730000. running mean: -21.904356\n",
      "ep 2170: ep_len:3690 episode reward: total was -132.450000. running mean: -23.009813\n",
      "ep 2170: ep_len:831 episode reward: total was 21.150000. running mean: -22.568215\n",
      "ep 2170: ep_len:783 episode reward: total was 26.500000. running mean: -22.077533\n",
      "ep 2170: ep_len:3287 episode reward: total was -319.150000. running mean: -25.048257\n",
      "ep 2170: ep_len:177 episode reward: total was 87.000000. running mean: -23.927775\n",
      "ep 2170: ep_len:631 episode reward: total was -17.090000. running mean: -23.859397\n",
      "ep 2170: ep_len:2876 episode reward: total was 17.610000. running mean: -23.444703\n",
      "epsilon:0.009992 episode_count: 32679. steps_count: 35052903.000000\n",
      "ep 2171: ep_len:500 episode reward: total was -9.550000. running mean: -23.305756\n",
      "ep 2171: ep_len:806 episode reward: total was -45.480000. running mean: -23.527498\n",
      "ep 2171: ep_len:2999 episode reward: total was -59.470000. running mean: -23.886923\n",
      "ep 2171: ep_len:783 episode reward: total was -29.890000. running mean: -23.946954\n",
      "ep 2171: ep_len:46 episode reward: total was 18.500000. running mean: -23.522485\n",
      "ep 2171: ep_len:179 episode reward: total was 85.000000. running mean: -22.437260\n",
      "ep 2171: ep_len:80 episode reward: total was 38.500000. running mean: -21.827887\n",
      "ep 2171: ep_len:31 episode reward: total was 12.500000. running mean: -21.484608\n",
      "ep 2171: ep_len:1066 episode reward: total was -89.500000. running mean: -22.164762\n",
      "ep 2171: ep_len:655 episode reward: total was 3.020000. running mean: -21.912915\n",
      "ep 2171: ep_len:576 episode reward: total was 7.310000. running mean: -21.620685\n",
      "ep 2171: ep_len:782 episode reward: total was 29.580000. running mean: -21.108679\n",
      "ep 2171: ep_len:913 episode reward: total was 56.820000. running mean: -20.329392\n",
      "ep 2171: ep_len:164 episode reward: total was 76.000000. running mean: -19.366098\n",
      "ep 2171: ep_len:73 episode reward: total was 35.000000. running mean: -18.822437\n",
      "ep 2171: ep_len:743 episode reward: total was -88.690000. running mean: -19.521113\n",
      "ep 2171: ep_len:2832 episode reward: total was -25.040000. running mean: -19.576301\n",
      "ep 2171: ep_len:66 episode reward: total was 31.500000. running mean: -19.065538\n",
      "epsilon:0.009992 episode_count: 32697. steps_count: 35066197.000000\n",
      "ep 2172: ep_len:879 episode reward: total was -15.910000. running mean: -19.033983\n",
      "ep 2172: ep_len:1667 episode reward: total was -87.350000. running mean: -19.717143\n",
      "ep 2172: ep_len:53 episode reward: total was 25.000000. running mean: -19.269972\n",
      "ep 2172: ep_len:2991 episode reward: total was -57.080000. running mean: -19.648072\n",
      "ep 2172: ep_len:589 episode reward: total was 8.940000. running mean: -19.362191\n",
      "ep 2172: ep_len:56 episode reward: total was 25.000000. running mean: -18.918569\n",
      "ep 2172: ep_len:112 episode reward: total was 54.500000. running mean: -18.184384\n",
      "ep 2172: ep_len:2873 episode reward: total was -735.000000. running mean: -25.352540\n",
      "ep 2172: ep_len:329 episode reward: total was 16.940000. running mean: -24.929615\n",
      "ep 2172: ep_len:668 episode reward: total was -12.680000. running mean: -24.807118\n",
      "ep 2172: ep_len:832 episode reward: total was 58.960000. running mean: -23.969447\n",
      "ep 2172: ep_len:1142 episode reward: total was -16.480000. running mean: -23.894553\n",
      "ep 2172: ep_len:76 episode reward: total was 35.000000. running mean: -23.305607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2172: ep_len:47 episode reward: total was 20.500000. running mean: -22.867551\n",
      "ep 2172: ep_len:977 episode reward: total was -33.740000. running mean: -22.976276\n",
      "ep 2172: ep_len:2796 episode reward: total was -25.190000. running mean: -22.998413\n",
      "epsilon:0.009992 episode_count: 32713. steps_count: 35082284.000000\n",
      "ep 2173: ep_len:645 episode reward: total was -4.340000. running mean: -22.811829\n",
      "ep 2173: ep_len:762 episode reward: total was -24.500000. running mean: -22.828710\n",
      "ep 2173: ep_len:102 episode reward: total was 49.500000. running mean: -22.105423\n",
      "ep 2173: ep_len:536 episode reward: total was 2.070000. running mean: -21.863669\n",
      "ep 2173: ep_len:45 episode reward: total was 21.000000. running mean: -21.435032\n",
      "ep 2173: ep_len:96 episode reward: total was 45.000000. running mean: -20.770682\n",
      "ep 2173: ep_len:66 episode reward: total was 30.000000. running mean: -20.262975\n",
      "ep 2173: ep_len:769 episode reward: total was -15.190000. running mean: -20.212245\n",
      "ep 2173: ep_len:3906 episode reward: total was -56.050000. running mean: -20.570623\n",
      "ep 2173: ep_len:535 episode reward: total was -9.360000. running mean: -20.458517\n",
      "ep 2173: ep_len:712 episode reward: total was 26.100000. running mean: -19.992932\n",
      "ep 2173: ep_len:1109 episode reward: total was -7.260000. running mean: -19.865602\n",
      "ep 2173: ep_len:83 episode reward: total was 37.000000. running mean: -19.296946\n",
      "ep 2173: ep_len:152 episode reward: total was 71.010000. running mean: -18.393877\n",
      "ep 2173: ep_len:78 episode reward: total was 37.500000. running mean: -17.834938\n",
      "ep 2173: ep_len:1085 episode reward: total was -2.940000. running mean: -17.685989\n",
      "ep 2173: ep_len:2803 episode reward: total was -9.450000. running mean: -17.603629\n",
      "ep 2173: ep_len:53 episode reward: total was 23.500000. running mean: -17.192593\n",
      "epsilon:0.009992 episode_count: 32731. steps_count: 35095821.000000\n",
      "ep 2174: ep_len:717 episode reward: total was -51.570000. running mean: -17.536367\n",
      "ep 2174: ep_len:1201 episode reward: total was -176.620000. running mean: -19.127203\n",
      "ep 2174: ep_len:49 episode reward: total was 23.000000. running mean: -18.705931\n",
      "ep 2174: ep_len:3015 episode reward: total was -64.440000. running mean: -19.163272\n",
      "ep 2174: ep_len:627 episode reward: total was -2.110000. running mean: -18.992739\n",
      "ep 2174: ep_len:85 episode reward: total was 39.500000. running mean: -18.407811\n",
      "ep 2174: ep_len:2924 episode reward: total was -405.230000. running mean: -22.276033\n",
      "ep 2174: ep_len:3626 episode reward: total was -32.210000. running mean: -22.375373\n",
      "ep 2174: ep_len:546 episode reward: total was -9.860000. running mean: -22.250219\n",
      "ep 2174: ep_len:777 episode reward: total was 22.190000. running mean: -21.805817\n",
      "ep 2174: ep_len:1070 episode reward: total was 42.000000. running mean: -21.167759\n",
      "ep 2174: ep_len:99 episode reward: total was 48.000000. running mean: -20.476081\n",
      "ep 2174: ep_len:153 episode reward: total was 73.500000. running mean: -19.536321\n",
      "ep 2174: ep_len:1433 episode reward: total was 17.770000. running mean: -19.163257\n",
      "ep 2174: ep_len:2913 episode reward: total was -404.220000. running mean: -23.013825\n",
      "epsilon:0.009992 episode_count: 32746. steps_count: 35115056.000000\n",
      "ep 2175: ep_len:798 episode reward: total was -95.580000. running mean: -23.739487\n",
      "ep 2175: ep_len:778 episode reward: total was -33.800000. running mean: -23.840092\n",
      "ep 2175: ep_len:74 episode reward: total was 35.500000. running mean: -23.246691\n",
      "ep 2175: ep_len:2801 episode reward: total was -47.970000. running mean: -23.493924\n",
      "ep 2175: ep_len:837 episode reward: total was 25.730000. running mean: -23.001685\n",
      "ep 2175: ep_len:39 episode reward: total was 18.000000. running mean: -22.591668\n",
      "ep 2175: ep_len:57 episode reward: total was 25.500000. running mean: -22.110751\n",
      "ep 2175: ep_len:686 episode reward: total was -3.930000. running mean: -21.928944\n",
      "ep 2175: ep_len:500 episode reward: total was 28.090000. running mean: -21.428754\n",
      "ep 2175: ep_len:800 episode reward: total was -46.440000. running mean: -21.678867\n",
      "ep 2175: ep_len:877 episode reward: total was 59.450000. running mean: -20.867578\n",
      "ep 2175: ep_len:666 episode reward: total was -30.250000. running mean: -20.961402\n",
      "ep 2175: ep_len:82 episode reward: total was 36.500000. running mean: -20.386788\n",
      "ep 2175: ep_len:48 episode reward: total was 22.500000. running mean: -19.957920\n",
      "ep 2175: ep_len:93 episode reward: total was 45.000000. running mean: -19.308341\n",
      "ep 2175: ep_len:1468 episode reward: total was 2.850000. running mean: -19.086758\n",
      "ep 2175: ep_len:2804 episode reward: total was -45.800000. running mean: -19.353890\n",
      "ep 2175: ep_len:40 episode reward: total was 18.500000. running mean: -18.975351\n",
      "epsilon:0.009992 episode_count: 32764. steps_count: 35128504.000000\n",
      "ep 2176: ep_len:790 episode reward: total was -39.740000. running mean: -19.182998\n",
      "ep 2176: ep_len:746 episode reward: total was -14.390000. running mean: -19.135068\n",
      "ep 2176: ep_len:2897 episode reward: total was -46.370000. running mean: -19.407417\n",
      "ep 2176: ep_len:543 episode reward: total was -0.770000. running mean: -19.221043\n",
      "ep 2176: ep_len:21 episode reward: total was 9.000000. running mean: -18.938832\n",
      "ep 2176: ep_len:1494 episode reward: total was 19.270000. running mean: -18.556744\n",
      "ep 2176: ep_len:654 episode reward: total was 0.040000. running mean: -18.370777\n",
      "ep 2176: ep_len:675 episode reward: total was -11.100000. running mean: -18.298069\n",
      "ep 2176: ep_len:942 episode reward: total was 68.820000. running mean: -17.426888\n",
      "ep 2176: ep_len:720 episode reward: total was 14.600000. running mean: -17.106619\n",
      "ep 2176: ep_len:74 episode reward: total was 34.000000. running mean: -16.595553\n",
      "ep 2176: ep_len:1157 episode reward: total was -3.160000. running mean: -16.461198\n",
      "ep 2176: ep_len:2859 episode reward: total was -46.320000. running mean: -16.759786\n",
      "epsilon:0.009992 episode_count: 32777. steps_count: 35142076.000000\n",
      "ep 2177: ep_len:1081 episode reward: total was 1.550000. running mean: -16.576688\n",
      "ep 2177: ep_len:692 episode reward: total was -13.780000. running mean: -16.548721\n",
      "ep 2177: ep_len:2975 episode reward: total was -34.260000. running mean: -16.725834\n",
      "ep 2177: ep_len:1689 episode reward: total was -53.670000. running mean: -17.095275\n",
      "ep 2177: ep_len:46 episode reward: total was 21.500000. running mean: -16.709323\n",
      "ep 2177: ep_len:105 episode reward: total was 49.500000. running mean: -16.047229\n",
      "ep 2177: ep_len:42 episode reward: total was 19.500000. running mean: -15.691757\n",
      "ep 2177: ep_len:2918 episode reward: total was -266.210000. running mean: -18.196939\n",
      "ep 2177: ep_len:4105 episode reward: total was -82.680000. running mean: -18.841770\n",
      "ep 2177: ep_len:935 episode reward: total was -27.170000. running mean: -18.925052\n",
      "ep 2177: ep_len:671 episode reward: total was 19.770000. running mean: -18.538102\n",
      "ep 2177: ep_len:563 episode reward: total was 34.680000. running mean: -18.005921\n",
      "ep 2177: ep_len:48 episode reward: total was 22.500000. running mean: -17.600862\n",
      "ep 2177: ep_len:758 episode reward: total was -32.990000. running mean: -17.754753\n",
      "ep 2177: ep_len:45 episode reward: total was 19.500000. running mean: -17.382205\n",
      "ep 2177: ep_len:65 episode reward: total was 31.000000. running mean: -16.898383\n",
      "epsilon:0.009992 episode_count: 32793. steps_count: 35158814.000000\n",
      "ep 2178: ep_len:1091 episode reward: total was 1.100000. running mean: -16.718400\n",
      "ep 2178: ep_len:500 episode reward: total was -16.160000. running mean: -16.712816\n",
      "ep 2178: ep_len:2922 episode reward: total was -37.670000. running mean: -16.922387\n",
      "ep 2178: ep_len:628 episode reward: total was -13.140000. running mean: -16.884564\n",
      "ep 2178: ep_len:69 episode reward: total was 28.500000. running mean: -16.430718\n",
      "ep 2178: ep_len:95 episode reward: total was 44.500000. running mean: -15.821411\n",
      "ep 2178: ep_len:82 episode reward: total was 38.000000. running mean: -15.283197\n",
      "ep 2178: ep_len:74 episode reward: total was 35.500000. running mean: -14.775365\n",
      "ep 2178: ep_len:1416 episode reward: total was 13.220000. running mean: -14.495411\n",
      "ep 2178: ep_len:661 episode reward: total was 8.220000. running mean: -14.268257\n",
      "ep 2178: ep_len:517 episode reward: total was -5.590000. running mean: -14.181474\n",
      "ep 2178: ep_len:729 episode reward: total was 32.790000. running mean: -13.711760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2178: ep_len:500 episode reward: total was 40.800000. running mean: -13.166642\n",
      "ep 2178: ep_len:82 episode reward: total was 39.500000. running mean: -12.639976\n",
      "ep 2178: ep_len:1153 episode reward: total was -9.850000. running mean: -12.612076\n",
      "ep 2178: ep_len:2855 episode reward: total was -15.750000. running mean: -12.643455\n",
      "ep 2178: ep_len:55 episode reward: total was 24.500000. running mean: -12.272021\n",
      "epsilon:0.009992 episode_count: 32810. steps_count: 35172243.000000\n",
      "ep 2179: ep_len:1129 episode reward: total was -8.500000. running mean: -12.234300\n",
      "ep 2179: ep_len:702 episode reward: total was -106.680000. running mean: -13.178757\n",
      "ep 2179: ep_len:2956 episode reward: total was -20.120000. running mean: -13.248170\n",
      "ep 2179: ep_len:840 episode reward: total was 28.770000. running mean: -12.827988\n",
      "ep 2179: ep_len:159 episode reward: total was 76.500000. running mean: -11.934708\n",
      "ep 2179: ep_len:1088 episode reward: total was 5.090000. running mean: -11.764461\n",
      "ep 2179: ep_len:3748 episode reward: total was -64.390000. running mean: -12.290716\n",
      "ep 2179: ep_len:1299 episode reward: total was -55.860000. running mean: -12.726409\n",
      "ep 2179: ep_len:833 episode reward: total was 68.430000. running mean: -11.914845\n",
      "ep 2179: ep_len:982 episode reward: total was -258.860000. running mean: -14.384297\n",
      "ep 2179: ep_len:215 episode reward: total was 100.000000. running mean: -13.240454\n",
      "ep 2179: ep_len:1056 episode reward: total was -3.740000. running mean: -13.145449\n",
      "ep 2179: ep_len:2800 episode reward: total was -60.040000. running mean: -13.614395\n",
      "epsilon:0.009992 episode_count: 32823. steps_count: 35190050.000000\n",
      "ep 2180: ep_len:742 episode reward: total was -30.120000. running mean: -13.779451\n",
      "ep 2180: ep_len:1045 episode reward: total was 24.650000. running mean: -13.395156\n",
      "ep 2180: ep_len:3055 episode reward: total was -17.990000. running mean: -13.441105\n",
      "ep 2180: ep_len:500 episode reward: total was -11.360000. running mean: -13.420294\n",
      "ep 2180: ep_len:115 episode reward: total was 56.000000. running mean: -12.726091\n",
      "ep 2180: ep_len:47 episode reward: total was 20.500000. running mean: -12.393830\n",
      "ep 2180: ep_len:2823 episode reward: total was -264.840000. running mean: -14.918292\n",
      "ep 2180: ep_len:3763 episode reward: total was -85.450000. running mean: -15.623609\n",
      "ep 2180: ep_len:814 episode reward: total was 5.000000. running mean: -15.417373\n",
      "ep 2180: ep_len:7317 episode reward: total was -357.690000. running mean: -18.840099\n",
      "ep 2180: ep_len:1101 episode reward: total was -17.900000. running mean: -18.830698\n",
      "ep 2180: ep_len:64 episode reward: total was 30.500000. running mean: -18.337391\n",
      "ep 2180: ep_len:973 episode reward: total was -46.670000. running mean: -18.620717\n",
      "ep 2180: ep_len:2877 episode reward: total was -30.710000. running mean: -18.741610\n",
      "ep 2180: ep_len:44 episode reward: total was 19.000000. running mean: -18.364194\n",
      "epsilon:0.009992 episode_count: 32838. steps_count: 35215330.000000\n",
      "ep 2181: ep_len:1485 episode reward: total was 20.800000. running mean: -17.972552\n",
      "ep 2181: ep_len:698 episode reward: total was -39.650000. running mean: -18.189326\n",
      "ep 2181: ep_len:88 episode reward: total was 42.500000. running mean: -17.582433\n",
      "ep 2181: ep_len:1633 episode reward: total was -33.270000. running mean: -17.739309\n",
      "ep 2181: ep_len:79 episode reward: total was 35.000000. running mean: -17.211916\n",
      "ep 2181: ep_len:2962 episode reward: total was -88.180000. running mean: -17.921596\n",
      "ep 2181: ep_len:337 episode reward: total was 21.550000. running mean: -17.526880\n",
      "ep 2181: ep_len:1568 episode reward: total was -8.210000. running mean: -17.433712\n",
      "ep 2181: ep_len:806 episode reward: total was 37.080000. running mean: -16.888575\n",
      "ep 2181: ep_len:596 episode reward: total was 1.240000. running mean: -16.707289\n",
      "ep 2181: ep_len:72 episode reward: total was 31.500000. running mean: -16.225216\n",
      "ep 2181: ep_len:41 episode reward: total was 19.000000. running mean: -15.872964\n",
      "ep 2181: ep_len:1118 episode reward: total was -14.700000. running mean: -15.861234\n",
      "ep 2181: ep_len:2805 episode reward: total was -27.270000. running mean: -15.975322\n",
      "epsilon:0.009992 episode_count: 32852. steps_count: 35229618.000000\n",
      "ep 2182: ep_len:1141 episode reward: total was -1.890000. running mean: -15.834469\n",
      "ep 2182: ep_len:657 episode reward: total was -38.040000. running mean: -16.056524\n",
      "ep 2182: ep_len:66 episode reward: total was 31.500000. running mean: -15.580959\n",
      "ep 2182: ep_len:2983 episode reward: total was -139.770000. running mean: -16.822849\n",
      "ep 2182: ep_len:669 episode reward: total was -0.550000. running mean: -16.660121\n",
      "ep 2182: ep_len:1049 episode reward: total was -26.040000. running mean: -16.753919\n",
      "ep 2182: ep_len:3841 episode reward: total was -180.600000. running mean: -18.392380\n",
      "ep 2182: ep_len:1593 episode reward: total was -42.210000. running mean: -18.630556\n",
      "ep 2182: ep_len:894 episode reward: total was 74.160000. running mean: -17.702651\n",
      "ep 2182: ep_len:660 episode reward: total was 28.730000. running mean: -17.238324\n",
      "ep 2182: ep_len:187 episode reward: total was 87.500000. running mean: -16.190941\n",
      "ep 2182: ep_len:664 episode reward: total was 12.700000. running mean: -15.902032\n",
      "ep 2182: ep_len:2841 episode reward: total was -8.970000. running mean: -15.832711\n",
      "epsilon:0.009992 episode_count: 32865. steps_count: 35246863.000000\n",
      "ep 2183: ep_len:955 episode reward: total was -94.160000. running mean: -16.615984\n",
      "ep 2183: ep_len:1615 episode reward: total was -35.930000. running mean: -16.809124\n",
      "ep 2183: ep_len:3030 episode reward: total was -9.420000. running mean: -16.735233\n",
      "ep 2183: ep_len:804 episode reward: total was 2.600000. running mean: -16.541881\n",
      "ep 2183: ep_len:46 episode reward: total was 21.500000. running mean: -16.161462\n",
      "ep 2183: ep_len:88 episode reward: total was 41.000000. running mean: -15.589847\n",
      "ep 2183: ep_len:69 episode reward: total was 33.000000. running mean: -15.103949\n",
      "ep 2183: ep_len:1505 episode reward: total was 9.370000. running mean: -14.859209\n",
      "ep 2183: ep_len:3815 episode reward: total was -44.780000. running mean: -15.158417\n",
      "ep 2183: ep_len:593 episode reward: total was 21.290000. running mean: -14.793933\n",
      "ep 2183: ep_len:773 episode reward: total was 2.180000. running mean: -14.624194\n",
      "ep 2183: ep_len:500 episode reward: total was -4.990000. running mean: -14.527852\n",
      "ep 2183: ep_len:52 episode reward: total was 21.500000. running mean: -14.167573\n",
      "ep 2183: ep_len:95 episode reward: total was 46.000000. running mean: -13.565898\n",
      "ep 2183: ep_len:755 episode reward: total was -10.860000. running mean: -13.538839\n",
      "ep 2183: ep_len:2869 episode reward: total was -51.640000. running mean: -13.919850\n",
      "epsilon:0.009992 episode_count: 32881. steps_count: 35264427.000000\n",
      "ep 2184: ep_len:1137 episode reward: total was -0.920000. running mean: -13.789852\n",
      "ep 2184: ep_len:705 episode reward: total was -34.530000. running mean: -13.997253\n",
      "ep 2184: ep_len:3112 episode reward: total was -170.270000. running mean: -15.559981\n",
      "ep 2184: ep_len:722 episode reward: total was 3.610000. running mean: -15.368281\n",
      "ep 2184: ep_len:102 episode reward: total was 49.500000. running mean: -14.719598\n",
      "ep 2184: ep_len:700 episode reward: total was -15.240000. running mean: -14.724802\n",
      "ep 2184: ep_len:366 episode reward: total was 18.320000. running mean: -14.394354\n",
      "ep 2184: ep_len:557 episode reward: total was -54.370000. running mean: -14.794111\n",
      "ep 2184: ep_len:879 episode reward: total was 67.120000. running mean: -13.974969\n",
      "ep 2184: ep_len:1071 episode reward: total was 19.640000. running mean: -13.638820\n",
      "ep 2184: ep_len:50 episode reward: total was 22.000000. running mean: -13.282432\n",
      "ep 2184: ep_len:1479 episode reward: total was 27.970000. running mean: -12.869907\n",
      "ep 2184: ep_len:2780 episode reward: total was -19.650000. running mean: -12.937708\n",
      "ep 2184: ep_len:50 episode reward: total was 23.500000. running mean: -12.573331\n",
      "epsilon:0.009992 episode_count: 32895. steps_count: 35278137.000000\n",
      "ep 2185: ep_len:801 episode reward: total was -46.240000. running mean: -12.909998\n",
      "ep 2185: ep_len:944 episode reward: total was 17.080000. running mean: -12.610098\n",
      "ep 2185: ep_len:66 episode reward: total was 30.000000. running mean: -12.183997\n",
      "ep 2185: ep_len:2947 episode reward: total was -63.650000. running mean: -12.698657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2185: ep_len:688 episode reward: total was -6.390000. running mean: -12.635570\n",
      "ep 2185: ep_len:67 episode reward: total was 29.000000. running mean: -12.219215\n",
      "ep 2185: ep_len:1199 episode reward: total was 24.690000. running mean: -11.850122\n",
      "ep 2185: ep_len:3837 episode reward: total was 10.670000. running mean: -11.624921\n",
      "ep 2185: ep_len:571 episode reward: total was 16.660000. running mean: -11.342072\n",
      "ep 2185: ep_len:731 episode reward: total was 14.460000. running mean: -11.084051\n",
      "ep 2185: ep_len:500 episode reward: total was 19.420000. running mean: -10.779011\n",
      "ep 2185: ep_len:99 episode reward: total was 45.000000. running mean: -10.221221\n",
      "ep 2185: ep_len:1034 episode reward: total was -29.390000. running mean: -10.412908\n",
      "ep 2185: ep_len:2845 episode reward: total was -25.130000. running mean: -10.560079\n",
      "epsilon:0.009992 episode_count: 32909. steps_count: 35294466.000000\n",
      "ep 2186: ep_len:1134 episode reward: total was -1.960000. running mean: -10.474079\n",
      "ep 2186: ep_len:698 episode reward: total was -17.710000. running mean: -10.546438\n",
      "ep 2186: ep_len:3026 episode reward: total was -2.550000. running mean: -10.466473\n",
      "ep 2186: ep_len:1679 episode reward: total was -35.290000. running mean: -10.714709\n",
      "ep 2186: ep_len:142 episode reward: total was 66.500000. running mean: -9.942562\n",
      "ep 2186: ep_len:47 episode reward: total was 20.500000. running mean: -9.638136\n",
      "ep 2186: ep_len:992 episode reward: total was -48.370000. running mean: -10.025455\n",
      "ep 2186: ep_len:3787 episode reward: total was 17.600000. running mean: -9.749200\n",
      "ep 2186: ep_len:659 episode reward: total was -8.270000. running mean: -9.734408\n",
      "ep 2186: ep_len:823 episode reward: total was 22.860000. running mean: -9.408464\n",
      "ep 2186: ep_len:630 episode reward: total was 5.570000. running mean: -9.258679\n",
      "ep 2186: ep_len:66 episode reward: total was 31.500000. running mean: -8.851093\n",
      "ep 2186: ep_len:141 episode reward: total was 69.000000. running mean: -8.072582\n",
      "ep 2186: ep_len:1166 episode reward: total was -19.820000. running mean: -8.190056\n",
      "ep 2186: ep_len:2803 episode reward: total was -5.500000. running mean: -8.163155\n",
      "ep 2186: ep_len:56 episode reward: total was 26.500000. running mean: -7.816524\n",
      "epsilon:0.009992 episode_count: 32925. steps_count: 35312315.000000\n",
      "ep 2187: ep_len:999 episode reward: total was -36.860000. running mean: -8.106958\n",
      "ep 2187: ep_len:1569 episode reward: total was -48.540000. running mean: -8.511289\n",
      "ep 2187: ep_len:3115 episode reward: total was 4.550000. running mean: -8.380676\n",
      "ep 2187: ep_len:787 episode reward: total was -20.980000. running mean: -8.506669\n",
      "ep 2187: ep_len:67 episode reward: total was 32.000000. running mean: -8.101603\n",
      "ep 2187: ep_len:97 episode reward: total was 45.500000. running mean: -7.565586\n",
      "ep 2187: ep_len:610 episode reward: total was -10.230000. running mean: -7.592231\n",
      "ep 2187: ep_len:662 episode reward: total was 18.360000. running mean: -7.332708\n",
      "ep 2187: ep_len:580 episode reward: total was -17.600000. running mean: -7.435381\n",
      "ep 2187: ep_len:607 episode reward: total was 14.720000. running mean: -7.213827\n",
      "ep 2187: ep_len:615 episode reward: total was -38.350000. running mean: -7.525189\n",
      "ep 2187: ep_len:1178 episode reward: total was -289.370000. running mean: -10.343637\n",
      "ep 2187: ep_len:2793 episode reward: total was -47.960000. running mean: -10.719801\n",
      "ep 2187: ep_len:60 episode reward: total was 28.500000. running mean: -10.327603\n",
      "epsilon:0.009992 episode_count: 32939. steps_count: 35326054.000000\n",
      "ep 2188: ep_len:1085 episode reward: total was 1.590000. running mean: -10.208427\n",
      "ep 2188: ep_len:500 episode reward: total was -3.490000. running mean: -10.141243\n",
      "ep 2188: ep_len:2857 episode reward: total was -34.370000. running mean: -10.383530\n",
      "ep 2188: ep_len:605 episode reward: total was -87.040000. running mean: -11.150095\n",
      "ep 2188: ep_len:132 episode reward: total was 64.500000. running mean: -10.393594\n",
      "ep 2188: ep_len:731 episode reward: total was -29.620000. running mean: -10.585858\n",
      "ep 2188: ep_len:357 episode reward: total was 10.760000. running mean: -10.372399\n",
      "ep 2188: ep_len:582 episode reward: total was -10.510000. running mean: -10.373775\n",
      "ep 2188: ep_len:777 episode reward: total was 10.910000. running mean: -10.160938\n",
      "ep 2188: ep_len:933 episode reward: total was 19.140000. running mean: -9.867928\n",
      "ep 2188: ep_len:76 episode reward: total was 36.500000. running mean: -9.404249\n",
      "ep 2188: ep_len:61 episode reward: total was 29.000000. running mean: -9.020206\n",
      "ep 2188: ep_len:623 episode reward: total was -15.150000. running mean: -9.081504\n",
      "ep 2188: ep_len:2851 episode reward: total was -71.100000. running mean: -9.701689\n",
      "epsilon:0.009992 episode_count: 32953. steps_count: 35338224.000000\n",
      "ep 2189: ep_len:1068 episode reward: total was -83.420000. running mean: -10.438872\n",
      "ep 2189: ep_len:190 episode reward: total was 11.200000. running mean: -10.222484\n",
      "ep 2189: ep_len:3054 episode reward: total was -10.690000. running mean: -10.227159\n",
      "ep 2189: ep_len:585 episode reward: total was -2.400000. running mean: -10.148887\n",
      "ep 2189: ep_len:58 episode reward: total was 24.500000. running mean: -9.802398\n",
      "ep 2189: ep_len:157 episode reward: total was 77.000000. running mean: -8.934374\n",
      "ep 2189: ep_len:500 episode reward: total was 42.410000. running mean: -8.420931\n",
      "ep 2189: ep_len:3672 episode reward: total was -41.850000. running mean: -8.755221\n",
      "ep 2189: ep_len:1573 episode reward: total was -100.680000. running mean: -9.674469\n",
      "ep 2189: ep_len:829 episode reward: total was 41.200000. running mean: -9.165725\n",
      "ep 2189: ep_len:600 episode reward: total was -16.880000. running mean: -9.242867\n",
      "ep 2189: ep_len:50 episode reward: total was 23.500000. running mean: -8.915439\n",
      "ep 2189: ep_len:1024 episode reward: total was 7.630000. running mean: -8.749984\n",
      "ep 2189: ep_len:2835 episode reward: total was -9.070000. running mean: -8.753184\n",
      "epsilon:0.009992 episode_count: 32967. steps_count: 35354419.000000\n",
      "ep 2190: ep_len:1136 episode reward: total was -1.940000. running mean: -8.685053\n",
      "ep 2190: ep_len:1626 episode reward: total was -20.760000. running mean: -8.805802\n",
      "ep 2190: ep_len:3014 episode reward: total was -22.390000. running mean: -8.941644\n",
      "ep 2190: ep_len:1701 episode reward: total was -13.400000. running mean: -8.986228\n",
      "ep 2190: ep_len:44 episode reward: total was 20.500000. running mean: -8.691365\n",
      "ep 2190: ep_len:163 episode reward: total was 78.500000. running mean: -7.819452\n",
      "ep 2190: ep_len:58 episode reward: total was 26.000000. running mean: -7.481257\n",
      "ep 2190: ep_len:1507 episode reward: total was 15.880000. running mean: -7.247645\n",
      "ep 2190: ep_len:3693 episode reward: total was -187.050000. running mean: -9.045668\n",
      "ep 2190: ep_len:3906 episode reward: total was -789.310000. running mean: -16.848311\n",
      "ep 2190: ep_len:764 episode reward: total was -24.210000. running mean: -16.921928\n",
      "ep 2190: ep_len:590 episode reward: total was -22.550000. running mean: -16.978209\n",
      "ep 2190: ep_len:76 episode reward: total was 33.500000. running mean: -16.473427\n",
      "ep 2190: ep_len:37 episode reward: total was 15.500000. running mean: -16.153693\n",
      "ep 2190: ep_len:500 episode reward: total was 18.800000. running mean: -15.804156\n",
      "ep 2190: ep_len:2787 episode reward: total was -24.110000. running mean: -15.887214\n",
      "epsilon:0.009992 episode_count: 32983. steps_count: 35376021.000000\n",
      "ep 2191: ep_len:715 episode reward: total was -61.700000. running mean: -16.345342\n",
      "ep 2191: ep_len:637 episode reward: total was -24.100000. running mean: -16.422889\n",
      "ep 2191: ep_len:2976 episode reward: total was -25.060000. running mean: -16.509260\n",
      "ep 2191: ep_len:1189 episode reward: total was -7.190000. running mean: -16.416067\n",
      "ep 2191: ep_len:122 episode reward: total was 58.000000. running mean: -15.671906\n",
      "ep 2191: ep_len:500 episode reward: total was 43.240000. running mean: -15.082787\n",
      "ep 2191: ep_len:3971 episode reward: total was -96.430000. running mean: -15.896260\n",
      "ep 2191: ep_len:1324 episode reward: total was -48.470000. running mean: -16.221997\n",
      "ep 2191: ep_len:784 episode reward: total was 23.510000. running mean: -15.824677\n",
      "ep 2191: ep_len:573 episode reward: total was 39.280000. running mean: -15.273630\n",
      "ep 2191: ep_len:45 episode reward: total was 21.000000. running mean: -14.910894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2191: ep_len:762 episode reward: total was -34.390000. running mean: -15.105685\n",
      "ep 2191: ep_len:2807 episode reward: total was -7.880000. running mean: -15.033428\n",
      "epsilon:0.009992 episode_count: 32996. steps_count: 35392426.000000\n",
      "ep 2192: ep_len:1417 episode reward: total was 25.600000. running mean: -14.627094\n",
      "ep 2192: ep_len:1248 episode reward: total was -55.360000. running mean: -15.034423\n",
      "ep 2192: ep_len:2874 episode reward: total was -25.410000. running mean: -15.138179\n",
      "ep 2192: ep_len:612 episode reward: total was -5.740000. running mean: -15.044197\n",
      "ep 2192: ep_len:154 episode reward: total was 75.500000. running mean: -14.138755\n",
      "ep 2192: ep_len:500 episode reward: total was 37.910000. running mean: -13.618267\n",
      "ep 2192: ep_len:4062 episode reward: total was -2927.260000. running mean: -42.754685\n",
      "ep 2192: ep_len:889 episode reward: total was -0.830000. running mean: -42.335438\n",
      "ep 2192: ep_len:7237 episode reward: total was 67.370000. running mean: -41.238383\n",
      "ep 2192: ep_len:1071 episode reward: total was 41.890000. running mean: -40.407100\n",
      "ep 2192: ep_len:39 episode reward: total was 18.000000. running mean: -39.823029\n",
      "ep 2192: ep_len:76 episode reward: total was 35.000000. running mean: -39.074798\n",
      "ep 2192: ep_len:1145 episode reward: total was -2.150000. running mean: -38.705550\n",
      "ep 2192: ep_len:2768 episode reward: total was -6.890000. running mean: -38.387395\n",
      "epsilon:0.009992 episode_count: 33010. steps_count: 35416518.000000\n",
      "ep 2193: ep_len:978 episode reward: total was -88.360000. running mean: -38.887121\n",
      "ep 2193: ep_len:723 episode reward: total was -15.360000. running mean: -38.651850\n",
      "ep 2193: ep_len:2975 episode reward: total was -1.260000. running mean: -38.277931\n",
      "ep 2193: ep_len:515 episode reward: total was 15.410000. running mean: -37.741052\n",
      "ep 2193: ep_len:65 episode reward: total was 29.500000. running mean: -37.068641\n",
      "ep 2193: ep_len:119 episode reward: total was 58.000000. running mean: -36.117955\n",
      "ep 2193: ep_len:500 episode reward: total was 0.490000. running mean: -35.751875\n",
      "ep 2193: ep_len:637 episode reward: total was 19.920000. running mean: -35.195157\n",
      "ep 2193: ep_len:1196 episode reward: total was -68.890000. running mean: -35.532105\n",
      "ep 2193: ep_len:661 episode reward: total was -10.270000. running mean: -35.279484\n",
      "ep 2193: ep_len:669 episode reward: total was -4.590000. running mean: -34.972589\n",
      "ep 2193: ep_len:51 episode reward: total was 24.000000. running mean: -34.382863\n",
      "ep 2193: ep_len:80 episode reward: total was 38.500000. running mean: -33.654035\n",
      "ep 2193: ep_len:828 episode reward: total was 4.110000. running mean: -33.276394\n",
      "ep 2193: ep_len:2805 episode reward: total was 4.070000. running mean: -32.902930\n",
      "epsilon:0.009992 episode_count: 33025. steps_count: 35429320.000000\n",
      "ep 2194: ep_len:1440 episode reward: total was 25.830000. running mean: -32.315601\n",
      "ep 2194: ep_len:745 episode reward: total was -36.590000. running mean: -32.358345\n",
      "ep 2194: ep_len:77 episode reward: total was 37.000000. running mean: -31.664762\n",
      "ep 2194: ep_len:102 episode reward: total was 48.000000. running mean: -30.868114\n",
      "ep 2194: ep_len:701 episode reward: total was 0.800000. running mean: -30.551433\n",
      "ep 2194: ep_len:1722 episode reward: total was -282.840000. running mean: -33.074319\n",
      "ep 2194: ep_len:650 episode reward: total was 18.640000. running mean: -32.557175\n",
      "ep 2194: ep_len:2652 episode reward: total was -251.400000. running mean: -34.745604\n",
      "ep 2194: ep_len:7565 episode reward: total was -157.530000. running mean: -35.973448\n",
      "ep 2194: ep_len:689 episode reward: total was 28.010000. running mean: -35.333613\n",
      "ep 2194: ep_len:165 episode reward: total was 79.500000. running mean: -34.185277\n",
      "ep 2194: ep_len:1101 episode reward: total was -1.280000. running mean: -33.856224\n",
      "ep 2194: ep_len:2853 episode reward: total was -21.160000. running mean: -33.729262\n",
      "ep 2194: ep_len:50 episode reward: total was 23.500000. running mean: -33.156969\n",
      "epsilon:0.009992 episode_count: 33039. steps_count: 35449832.000000\n",
      "ep 2195: ep_len:3402 episode reward: total was -752.480000. running mean: -40.350200\n",
      "ep 2195: ep_len:754 episode reward: total was -1.380000. running mean: -39.960498\n",
      "ep 2195: ep_len:74 episode reward: total was 34.000000. running mean: -39.220893\n",
      "ep 2195: ep_len:2852 episode reward: total was -39.950000. running mean: -39.228184\n",
      "ep 2195: ep_len:1208 episode reward: total was -116.820000. running mean: -40.004102\n",
      "ep 2195: ep_len:54 episode reward: total was 24.000000. running mean: -39.364061\n",
      "ep 2195: ep_len:137 episode reward: total was 64.000000. running mean: -38.330420\n",
      "ep 2195: ep_len:44 episode reward: total was 20.500000. running mean: -37.742116\n",
      "ep 2195: ep_len:1050 episode reward: total was -49.690000. running mean: -37.861595\n",
      "ep 2195: ep_len:4483 episode reward: total was -403.900000. running mean: -41.521979\n",
      "ep 2195: ep_len:1208 episode reward: total was -63.840000. running mean: -41.745159\n",
      "ep 2195: ep_len:687 episode reward: total was 10.600000. running mean: -41.221708\n",
      "ep 2195: ep_len:594 episode reward: total was 15.070000. running mean: -40.658790\n",
      "ep 2195: ep_len:66 episode reward: total was 31.500000. running mean: -39.937203\n",
      "ep 2195: ep_len:40 episode reward: total was 18.500000. running mean: -39.352831\n",
      "ep 2195: ep_len:48 episode reward: total was 22.500000. running mean: -38.734302\n",
      "ep 2195: ep_len:2118 episode reward: total was -155.710000. running mean: -39.904059\n",
      "ep 2195: ep_len:2822 episode reward: total was -26.030000. running mean: -39.765319\n",
      "ep 2195: ep_len:51 episode reward: total was 24.000000. running mean: -39.127665\n",
      "epsilon:0.009992 episode_count: 33058. steps_count: 35471524.000000\n",
      "ep 2196: ep_len:837 episode reward: total was 3.780000. running mean: -38.698589\n",
      "ep 2196: ep_len:720 episode reward: total was -23.990000. running mean: -38.551503\n",
      "ep 2196: ep_len:2978 episode reward: total was -19.470000. running mean: -38.360688\n",
      "ep 2196: ep_len:601 episode reward: total was -8.670000. running mean: -38.063781\n",
      "ep 2196: ep_len:128 episode reward: total was 61.000000. running mean: -37.073143\n",
      "ep 2196: ep_len:87 episode reward: total was 42.000000. running mean: -36.282412\n",
      "ep 2196: ep_len:1099 episode reward: total was -8.800000. running mean: -36.007588\n",
      "ep 2196: ep_len:3881 episode reward: total was -134.220000. running mean: -36.989712\n",
      "ep 2196: ep_len:918 episode reward: total was -25.330000. running mean: -36.873115\n",
      "ep 2196: ep_len:681 episode reward: total was 37.510000. running mean: -36.129283\n",
      "ep 2196: ep_len:965 episode reward: total was 3.090000. running mean: -35.737091\n",
      "ep 2196: ep_len:74 episode reward: total was 34.000000. running mean: -35.039720\n",
      "ep 2196: ep_len:37 episode reward: total was 15.500000. running mean: -34.534323\n",
      "ep 2196: ep_len:99 episode reward: total was 45.000000. running mean: -33.738979\n",
      "ep 2196: ep_len:1119 episode reward: total was 4.050000. running mean: -33.361090\n",
      "ep 2196: ep_len:2899 episode reward: total was -29.690000. running mean: -33.324379\n",
      "epsilon:0.009992 episode_count: 33074. steps_count: 35488647.000000\n",
      "ep 2197: ep_len:763 episode reward: total was -29.070000. running mean: -33.281835\n",
      "ep 2197: ep_len:738 episode reward: total was -8.950000. running mean: -33.038517\n",
      "ep 2197: ep_len:3002 episode reward: total was 3.610000. running mean: -32.672031\n",
      "ep 2197: ep_len:536 episode reward: total was -57.430000. running mean: -32.919611\n",
      "ep 2197: ep_len:72 episode reward: total was 34.500000. running mean: -32.245415\n",
      "ep 2197: ep_len:53 episode reward: total was 22.000000. running mean: -31.702961\n",
      "ep 2197: ep_len:767 episode reward: total was -31.890000. running mean: -31.704831\n",
      "ep 2197: ep_len:356 episode reward: total was 12.650000. running mean: -31.261283\n",
      "ep 2197: ep_len:656 episode reward: total was -73.400000. running mean: -31.682670\n",
      "ep 2197: ep_len:7293 episode reward: total was -186.260000. running mean: -33.228443\n",
      "ep 2197: ep_len:773 episode reward: total was 2.510000. running mean: -32.871059\n",
      "ep 2197: ep_len:159 episode reward: total was 75.000000. running mean: -31.792348\n",
      "ep 2197: ep_len:33 episode reward: total was 15.000000. running mean: -31.324425\n",
      "ep 2197: ep_len:1069 episode reward: total was -8.030000. running mean: -31.091481\n",
      "ep 2197: ep_len:2850 episode reward: total was -48.920000. running mean: -31.269766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2197: ep_len:43 episode reward: total was 18.500000. running mean: -30.772068\n",
      "epsilon:0.009992 episode_count: 33090. steps_count: 35507810.000000\n",
      "ep 2198: ep_len:709 episode reward: total was -15.750000. running mean: -30.621847\n",
      "ep 2198: ep_len:193 episode reward: total was -2.610000. running mean: -30.341729\n",
      "ep 2198: ep_len:2996 episode reward: total was -68.220000. running mean: -30.720512\n",
      "ep 2198: ep_len:500 episode reward: total was 23.060000. running mean: -30.182707\n",
      "ep 2198: ep_len:659 episode reward: total was 4.400000. running mean: -29.836879\n",
      "ep 2198: ep_len:3766 episode reward: total was -25.710000. running mean: -29.795611\n",
      "ep 2198: ep_len:613 episode reward: total was 24.140000. running mean: -29.256255\n",
      "ep 2198: ep_len:760 episode reward: total was 0.890000. running mean: -28.954792\n",
      "ep 2198: ep_len:1462 episode reward: total was -15.460000. running mean: -28.819844\n",
      "ep 2198: ep_len:95 episode reward: total was 44.500000. running mean: -28.086646\n",
      "ep 2198: ep_len:46 episode reward: total was 21.500000. running mean: -27.590779\n",
      "ep 2198: ep_len:112 episode reward: total was 54.500000. running mean: -26.769871\n",
      "ep 2198: ep_len:656 episode reward: total was 13.580000. running mean: -26.366373\n",
      "ep 2198: ep_len:2848 episode reward: total was -10.530000. running mean: -26.208009\n",
      "ep 2198: ep_len:49 episode reward: total was 23.000000. running mean: -25.715929\n",
      "epsilon:0.009992 episode_count: 33105. steps_count: 35523274.000000\n",
      "ep 2199: ep_len:662 episode reward: total was -16.690000. running mean: -25.625670\n",
      "ep 2199: ep_len:789 episode reward: total was -27.810000. running mean: -25.647513\n",
      "ep 2199: ep_len:2970 episode reward: total was -128.240000. running mean: -26.673438\n",
      "ep 2199: ep_len:649 episode reward: total was -10.600000. running mean: -26.512703\n",
      "ep 2199: ep_len:59 episode reward: total was 28.000000. running mean: -25.967576\n",
      "ep 2199: ep_len:573 episode reward: total was 16.950000. running mean: -25.538401\n",
      "ep 2199: ep_len:346 episode reward: total was 20.110000. running mean: -25.081917\n",
      "ep 2199: ep_len:1170 episode reward: total was -31.440000. running mean: -25.145497\n",
      "ep 2199: ep_len:689 episode reward: total was 26.320000. running mean: -24.630842\n",
      "ep 2199: ep_len:791 episode reward: total was 3.710000. running mean: -24.347434\n",
      "ep 2199: ep_len:39 episode reward: total was 18.000000. running mean: -23.923960\n",
      "ep 2199: ep_len:792 episode reward: total was -21.480000. running mean: -23.899520\n",
      "ep 2199: ep_len:2879 episode reward: total was -22.980000. running mean: -23.890325\n",
      "ep 2199: ep_len:46 episode reward: total was 21.500000. running mean: -23.436422\n",
      "epsilon:0.009992 episode_count: 33119. steps_count: 35535728.000000\n",
      "ep 2200: ep_len:832 episode reward: total was -8.430000. running mean: -23.286357\n",
      "ep 2200: ep_len:677 episode reward: total was -19.410000. running mean: -23.247594\n",
      "ep 2200: ep_len:3004 episode reward: total was -36.630000. running mean: -23.381418\n",
      "ep 2200: ep_len:827 episode reward: total was 18.500000. running mean: -22.962604\n",
      "ep 2200: ep_len:155 episode reward: total was 74.500000. running mean: -21.987978\n",
      "ep 2200: ep_len:62 episode reward: total was 28.000000. running mean: -21.488098\n",
      "ep 2200: ep_len:1080 episode reward: total was -7.850000. running mean: -21.351717\n",
      "ep 2200: ep_len:3980 episode reward: total was -183.180000. running mean: -22.970000\n",
      "ep 2200: ep_len:538 episode reward: total was -189.570000. running mean: -24.636000\n",
      "ep 2200: ep_len:744 episode reward: total was 34.050000. running mean: -24.049140\n",
      "ep 2200: ep_len:870 episode reward: total was 16.280000. running mean: -23.645848\n",
      "ep 2200: ep_len:664 episode reward: total was 24.590000. running mean: -23.163490\n",
      "ep 2200: ep_len:2729 episode reward: total was 0.460000. running mean: -22.927255\n",
      "epsilon:0.009992 episode_count: 33132. steps_count: 35551890.000000\n",
      "ep 2201: ep_len:1090 episode reward: total was -1.390000. running mean: -22.711882\n",
      "ep 2201: ep_len:500 episode reward: total was 9.950000. running mean: -22.385264\n",
      "ep 2201: ep_len:3031 episode reward: total was -57.140000. running mean: -22.732811\n",
      "ep 2201: ep_len:824 episode reward: total was 13.210000. running mean: -22.373383\n",
      "ep 2201: ep_len:57 episode reward: total was 25.500000. running mean: -21.894649\n",
      "ep 2201: ep_len:46 episode reward: total was 22.510000. running mean: -21.450603\n",
      "ep 2201: ep_len:1018 episode reward: total was -3.240000. running mean: -21.268497\n",
      "ep 2201: ep_len:3861 episode reward: total was -109.650000. running mean: -22.152312\n",
      "ep 2201: ep_len:568 episode reward: total was -4.590000. running mean: -21.976688\n",
      "ep 2201: ep_len:785 episode reward: total was -4.860000. running mean: -21.805522\n",
      "ep 2201: ep_len:551 episode reward: total was 7.810000. running mean: -21.509366\n",
      "ep 2201: ep_len:66 episode reward: total was 30.000000. running mean: -20.994273\n",
      "ep 2201: ep_len:185 episode reward: total was 89.500000. running mean: -19.889330\n",
      "ep 2201: ep_len:86 episode reward: total was 41.500000. running mean: -19.275437\n",
      "ep 2201: ep_len:767 episode reward: total was -69.710000. running mean: -19.779782\n",
      "ep 2201: ep_len:2835 episode reward: total was -13.170000. running mean: -19.713684\n",
      "ep 2201: ep_len:67 episode reward: total was 30.500000. running mean: -19.211548\n",
      "epsilon:0.009992 episode_count: 33149. steps_count: 35568227.000000\n",
      "ep 2202: ep_len:621 episode reward: total was 39.500000. running mean: -18.624432\n",
      "ep 2202: ep_len:1607 episode reward: total was -79.500000. running mean: -19.233188\n",
      "ep 2202: ep_len:49 episode reward: total was 23.000000. running mean: -18.810856\n",
      "ep 2202: ep_len:2913 episode reward: total was -52.250000. running mean: -19.145247\n",
      "ep 2202: ep_len:902 episode reward: total was -8.970000. running mean: -19.043495\n",
      "ep 2202: ep_len:101 episode reward: total was 47.500000. running mean: -18.378060\n",
      "ep 2202: ep_len:54 episode reward: total was 25.500000. running mean: -17.939279\n",
      "ep 2202: ep_len:1458 episode reward: total was 16.580000. running mean: -17.594087\n",
      "ep 2202: ep_len:3884 episode reward: total was -28.220000. running mean: -17.700346\n",
      "ep 2202: ep_len:622 episode reward: total was -21.220000. running mean: -17.735542\n",
      "ep 2202: ep_len:860 episode reward: total was 36.160000. running mean: -17.196587\n",
      "ep 2202: ep_len:1037 episode reward: total was 35.550000. running mean: -16.669121\n",
      "ep 2202: ep_len:75 episode reward: total was 36.000000. running mean: -16.142430\n",
      "ep 2202: ep_len:140 episode reward: total was 67.000000. running mean: -15.311005\n",
      "ep 2202: ep_len:30 episode reward: total was 13.500000. running mean: -15.022895\n",
      "ep 2202: ep_len:766 episode reward: total was -133.910000. running mean: -16.211766\n",
      "ep 2202: ep_len:2855 episode reward: total was -11.100000. running mean: -16.160649\n",
      "ep 2202: ep_len:45 episode reward: total was 19.500000. running mean: -15.804042\n",
      "epsilon:0.009992 episode_count: 33167. steps_count: 35586246.000000\n",
      "ep 2203: ep_len:609 episode reward: total was -9.040000. running mean: -15.736402\n",
      "ep 2203: ep_len:984 episode reward: total was 26.940000. running mean: -15.309638\n",
      "ep 2203: ep_len:2976 episode reward: total was -83.190000. running mean: -15.988441\n",
      "ep 2203: ep_len:534 episode reward: total was -25.130000. running mean: -16.079857\n",
      "ep 2203: ep_len:65 episode reward: total was 29.500000. running mean: -15.624058\n",
      "ep 2203: ep_len:958 episode reward: total was -261.120000. running mean: -18.079018\n",
      "ep 2203: ep_len:4184 episode reward: total was -76.040000. running mean: -18.658628\n",
      "ep 2203: ep_len:930 episode reward: total was -57.530000. running mean: -19.047341\n",
      "ep 2203: ep_len:776 episode reward: total was 25.730000. running mean: -18.599568\n",
      "ep 2203: ep_len:1030 episode reward: total was 23.420000. running mean: -18.179372\n",
      "ep 2203: ep_len:66 episode reward: total was 31.500000. running mean: -17.682579\n",
      "ep 2203: ep_len:88 episode reward: total was 42.500000. running mean: -17.080753\n",
      "ep 2203: ep_len:735 episode reward: total was -42.310000. running mean: -17.333045\n",
      "ep 2203: ep_len:2841 episode reward: total was -6.650000. running mean: -17.226215\n",
      "ep 2203: ep_len:46 episode reward: total was 20.000000. running mean: -16.853953\n",
      "epsilon:0.009992 episode_count: 33182. steps_count: 35603068.000000\n",
      "ep 2204: ep_len:658 episode reward: total was -6.970000. running mean: -16.755113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2204: ep_len:701 episode reward: total was -82.490000. running mean: -17.412462\n",
      "ep 2204: ep_len:2970 episode reward: total was -23.280000. running mean: -17.471137\n",
      "ep 2204: ep_len:642 episode reward: total was -0.400000. running mean: -17.300426\n",
      "ep 2204: ep_len:103 episode reward: total was 50.000000. running mean: -16.627422\n",
      "ep 2204: ep_len:56 episode reward: total was 25.000000. running mean: -16.211148\n",
      "ep 2204: ep_len:500 episode reward: total was 16.810000. running mean: -15.880936\n",
      "ep 2204: ep_len:3933 episode reward: total was -144.710000. running mean: -17.169227\n",
      "ep 2204: ep_len:500 episode reward: total was 32.550000. running mean: -16.672034\n",
      "ep 2204: ep_len:785 episode reward: total was 30.880000. running mean: -16.196514\n",
      "ep 2204: ep_len:586 episode reward: total was 32.500000. running mean: -15.709549\n",
      "ep 2204: ep_len:61 episode reward: total was 29.000000. running mean: -15.262453\n",
      "ep 2204: ep_len:119 episode reward: total was 53.500000. running mean: -14.574829\n",
      "ep 2204: ep_len:686 episode reward: total was -4.420000. running mean: -14.473281\n",
      "ep 2204: ep_len:2872 episode reward: total was -9.190000. running mean: -14.420448\n",
      "ep 2204: ep_len:57 episode reward: total was 27.000000. running mean: -14.006243\n",
      "epsilon:0.009992 episode_count: 33198. steps_count: 35618297.000000\n",
      "ep 2205: ep_len:889 episode reward: total was -12.470000. running mean: -13.990881\n",
      "ep 2205: ep_len:1612 episode reward: total was -102.980000. running mean: -14.880772\n",
      "ep 2205: ep_len:2952 episode reward: total was -63.620000. running mean: -15.368164\n",
      "ep 2205: ep_len:667 episode reward: total was -6.210000. running mean: -15.276583\n",
      "ep 2205: ep_len:35 episode reward: total was 14.500000. running mean: -14.978817\n",
      "ep 2205: ep_len:114 episode reward: total was 54.000000. running mean: -14.289029\n",
      "ep 2205: ep_len:76 episode reward: total was 35.000000. running mean: -13.796138\n",
      "ep 2205: ep_len:80 episode reward: total was 38.500000. running mean: -13.273177\n",
      "ep 2205: ep_len:500 episode reward: total was 53.370000. running mean: -12.606745\n",
      "ep 2205: ep_len:672 episode reward: total was 15.650000. running mean: -12.324178\n",
      "ep 2205: ep_len:1226 episode reward: total was -55.980000. running mean: -12.760736\n",
      "ep 2205: ep_len:761 episode reward: total was 15.200000. running mean: -12.481129\n",
      "ep 2205: ep_len:611 episode reward: total was -6.180000. running mean: -12.418117\n",
      "ep 2205: ep_len:58 episode reward: total was 27.500000. running mean: -12.018936\n",
      "ep 2205: ep_len:904 episode reward: total was 11.930000. running mean: -11.779447\n",
      "ep 2205: ep_len:2742 episode reward: total was -16.180000. running mean: -11.823452\n",
      "epsilon:0.009992 episode_count: 33214. steps_count: 35632196.000000\n",
      "ep 2206: ep_len:1041 episode reward: total was -90.950000. running mean: -12.614718\n",
      "ep 2206: ep_len:775 episode reward: total was 14.510000. running mean: -12.343471\n",
      "ep 2206: ep_len:66 episode reward: total was 31.500000. running mean: -11.905036\n",
      "ep 2206: ep_len:3003 episode reward: total was -30.910000. running mean: -12.095086\n",
      "ep 2206: ep_len:500 episode reward: total was -83.040000. running mean: -12.804535\n",
      "ep 2206: ep_len:123 episode reward: total was 60.000000. running mean: -12.076489\n",
      "ep 2206: ep_len:105 episode reward: total was 49.500000. running mean: -11.460725\n",
      "ep 2206: ep_len:77 episode reward: total was 37.000000. running mean: -10.976117\n",
      "ep 2206: ep_len:1390 episode reward: total was 3.230000. running mean: -10.834056\n",
      "ep 2206: ep_len:643 episode reward: total was 26.470000. running mean: -10.461016\n",
      "ep 2206: ep_len:659 episode reward: total was -54.610000. running mean: -10.902505\n",
      "ep 2206: ep_len:773 episode reward: total was 11.040000. running mean: -10.683080\n",
      "ep 2206: ep_len:607 episode reward: total was 14.130000. running mean: -10.434950\n",
      "ep 2206: ep_len:76 episode reward: total was 36.500000. running mean: -9.965600\n",
      "ep 2206: ep_len:80 episode reward: total was 37.000000. running mean: -9.495944\n",
      "ep 2206: ep_len:680 episode reward: total was -2.560000. running mean: -9.426585\n",
      "ep 2206: ep_len:2795 episode reward: total was 15.940000. running mean: -9.172919\n",
      "ep 2206: ep_len:37 episode reward: total was 17.000000. running mean: -8.911190\n",
      "epsilon:0.009992 episode_count: 33232. steps_count: 35645626.000000\n",
      "ep 2207: ep_len:1110 episode reward: total was -7.740000. running mean: -8.899478\n",
      "ep 2207: ep_len:787 episode reward: total was -0.930000. running mean: -8.819783\n",
      "ep 2207: ep_len:2842 episode reward: total was -27.510000. running mean: -9.006685\n",
      "ep 2207: ep_len:793 episode reward: total was 17.460000. running mean: -8.742018\n",
      "ep 2207: ep_len:108 episode reward: total was 52.500000. running mean: -8.129598\n",
      "ep 2207: ep_len:1446 episode reward: total was 25.950000. running mean: -7.788802\n",
      "ep 2207: ep_len:354 episode reward: total was 22.210000. running mean: -7.488814\n",
      "ep 2207: ep_len:696 episode reward: total was -30.500000. running mean: -7.718926\n",
      "ep 2207: ep_len:598 episode reward: total was -8.210000. running mean: -7.723837\n",
      "ep 2207: ep_len:639 episode reward: total was 8.410000. running mean: -7.562498\n",
      "ep 2207: ep_len:68 episode reward: total was 31.000000. running mean: -7.176873\n",
      "ep 2207: ep_len:1074 episode reward: total was -32.760000. running mean: -7.432705\n",
      "ep 2207: ep_len:2754 episode reward: total was -4.610000. running mean: -7.404478\n",
      "epsilon:0.009992 episode_count: 33245. steps_count: 35658895.000000\n",
      "ep 2208: ep_len:707 episode reward: total was -30.920000. running mean: -7.639633\n",
      "ep 2208: ep_len:717 episode reward: total was -21.390000. running mean: -7.777136\n",
      "ep 2208: ep_len:60 episode reward: total was 27.000000. running mean: -7.429365\n",
      "ep 2208: ep_len:3091 episode reward: total was -2.700000. running mean: -7.382071\n",
      "ep 2208: ep_len:665 episode reward: total was -19.270000. running mean: -7.500951\n",
      "ep 2208: ep_len:1031 episode reward: total was -119.140000. running mean: -8.617341\n",
      "ep 2208: ep_len:622 episode reward: total was 18.300000. running mean: -8.348168\n",
      "ep 2208: ep_len:500 episode reward: total was 30.740000. running mean: -7.957286\n",
      "ep 2208: ep_len:7223 episode reward: total was -45.410000. running mean: -8.331813\n",
      "ep 2208: ep_len:722 episode reward: total was -5.070000. running mean: -8.299195\n",
      "ep 2208: ep_len:58 episode reward: total was 26.000000. running mean: -7.956203\n",
      "ep 2208: ep_len:157 episode reward: total was 72.500000. running mean: -7.151641\n",
      "ep 2208: ep_len:98 episode reward: total was 46.000000. running mean: -6.620125\n",
      "ep 2208: ep_len:763 episode reward: total was -41.020000. running mean: -6.964123\n",
      "ep 2208: ep_len:2736 episode reward: total was -23.430000. running mean: -7.128782\n",
      "ep 2208: ep_len:38 episode reward: total was 16.000000. running mean: -6.897494\n",
      "epsilon:0.009992 episode_count: 33261. steps_count: 35678083.000000\n",
      "ep 2209: ep_len:1065 episode reward: total was 2.400000. running mean: -6.804519\n",
      "ep 2209: ep_len:742 episode reward: total was -44.310000. running mean: -7.179574\n",
      "ep 2209: ep_len:2982 episode reward: total was -28.250000. running mean: -7.390279\n",
      "ep 2209: ep_len:592 episode reward: total was -11.420000. running mean: -7.430576\n",
      "ep 2209: ep_len:36 episode reward: total was 15.000000. running mean: -7.206270\n",
      "ep 2209: ep_len:51 episode reward: total was 24.000000. running mean: -6.894207\n",
      "ep 2209: ep_len:562 episode reward: total was 32.730000. running mean: -6.497965\n",
      "ep 2209: ep_len:318 episode reward: total was 11.810000. running mean: -6.314886\n",
      "ep 2209: ep_len:866 episode reward: total was -42.010000. running mean: -6.671837\n",
      "ep 2209: ep_len:870 episode reward: total was 61.450000. running mean: -5.990618\n",
      "ep 2209: ep_len:504 episode reward: total was 0.070000. running mean: -5.930012\n",
      "ep 2209: ep_len:133 episode reward: total was 65.000000. running mean: -5.220712\n",
      "ep 2209: ep_len:70 episode reward: total was 33.500000. running mean: -4.833505\n",
      "ep 2209: ep_len:685 episode reward: total was 0.750000. running mean: -4.777670\n",
      "ep 2209: ep_len:2727 episode reward: total was -4.640000. running mean: -4.776293\n",
      "ep 2209: ep_len:54 episode reward: total was 25.500000. running mean: -4.473530\n",
      "epsilon:0.009992 episode_count: 33277. steps_count: 35690340.000000\n",
      "ep 2210: ep_len:1129 episode reward: total was -18.140000. running mean: -4.610195\n",
      "ep 2210: ep_len:1603 episode reward: total was -72.290000. running mean: -5.286993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2210: ep_len:3006 episode reward: total was -10.580000. running mean: -5.339923\n",
      "ep 2210: ep_len:680 episode reward: total was -18.140000. running mean: -5.467924\n",
      "ep 2210: ep_len:123 episode reward: total was 57.000000. running mean: -4.843245\n",
      "ep 2210: ep_len:49 episode reward: total was 23.000000. running mean: -4.564812\n",
      "ep 2210: ep_len:1066 episode reward: total was 6.450000. running mean: -4.454664\n",
      "ep 2210: ep_len:4002 episode reward: total was -180.310000. running mean: -6.213217\n",
      "ep 2210: ep_len:591 episode reward: total was -52.840000. running mean: -6.679485\n",
      "ep 2210: ep_len:782 episode reward: total was 46.610000. running mean: -6.146590\n",
      "ep 2210: ep_len:974 episode reward: total was 23.930000. running mean: -5.845824\n",
      "ep 2210: ep_len:95 episode reward: total was 44.500000. running mean: -5.342366\n",
      "ep 2210: ep_len:750 episode reward: total was -69.430000. running mean: -5.983243\n",
      "ep 2210: ep_len:2913 episode reward: total was -4.460000. running mean: -5.968010\n",
      "epsilon:0.009992 episode_count: 33291. steps_count: 35708103.000000\n",
      "ep 2211: ep_len:1071 episode reward: total was 2.460000. running mean: -5.883730\n",
      "ep 2211: ep_len:813 episode reward: total was 5.010000. running mean: -5.774793\n",
      "ep 2211: ep_len:3063 episode reward: total was -64.160000. running mean: -6.358645\n",
      "ep 2211: ep_len:500 episode reward: total was 10.960000. running mean: -6.185458\n",
      "ep 2211: ep_len:41 episode reward: total was 19.000000. running mean: -5.933604\n",
      "ep 2211: ep_len:148 episode reward: total was 71.000000. running mean: -5.164268\n",
      "ep 2211: ep_len:843 episode reward: total was 31.040000. running mean: -4.802225\n",
      "ep 2211: ep_len:310 episode reward: total was 20.910000. running mean: -4.545103\n",
      "ep 2211: ep_len:897 episode reward: total was -1.900000. running mean: -4.518652\n",
      "ep 2211: ep_len:652 episode reward: total was 15.450000. running mean: -4.318965\n",
      "ep 2211: ep_len:900 episode reward: total was 15.570000. running mean: -4.120076\n",
      "ep 2211: ep_len:46 episode reward: total was 20.000000. running mean: -3.878875\n",
      "ep 2211: ep_len:114 episode reward: total was 55.500000. running mean: -3.285086\n",
      "ep 2211: ep_len:1081 episode reward: total was 7.490000. running mean: -3.177335\n",
      "ep 2211: ep_len:2875 episode reward: total was 5.110000. running mean: -3.094462\n",
      "epsilon:0.009992 episode_count: 33306. steps_count: 35721457.000000\n",
      "ep 2212: ep_len:1128 episode reward: total was -4.040000. running mean: -3.103917\n",
      "ep 2212: ep_len:1443 episode reward: total was -1109.720000. running mean: -14.170078\n",
      "ep 2212: ep_len:2928 episode reward: total was -15.100000. running mean: -14.179377\n",
      "ep 2212: ep_len:864 episode reward: total was 13.200000. running mean: -13.905584\n",
      "ep 2212: ep_len:69 episode reward: total was 33.000000. running mean: -13.436528\n",
      "ep 2212: ep_len:683 episode reward: total was -9.600000. running mean: -13.398162\n",
      "ep 2212: ep_len:669 episode reward: total was 28.230000. running mean: -12.981881\n",
      "ep 2212: ep_len:1233 episode reward: total was -53.490000. running mean: -13.386962\n",
      "ep 2212: ep_len:908 episode reward: total was 66.830000. running mean: -12.584792\n",
      "ep 2212: ep_len:660 episode reward: total was 23.200000. running mean: -12.226944\n",
      "ep 2212: ep_len:62 episode reward: total was 29.500000. running mean: -11.809675\n",
      "ep 2212: ep_len:598 episode reward: total was -5.300000. running mean: -11.744578\n",
      "ep 2212: ep_len:2773 episode reward: total was -1.480000. running mean: -11.641932\n",
      "ep 2212: ep_len:51 episode reward: total was 24.000000. running mean: -11.285513\n",
      "epsilon:0.009992 episode_count: 33320. steps_count: 35735526.000000\n",
      "ep 2213: ep_len:716 episode reward: total was -77.850000. running mean: -11.951158\n",
      "ep 2213: ep_len:679 episode reward: total was -35.800000. running mean: -12.189646\n",
      "ep 2213: ep_len:3087 episode reward: total was -4.390000. running mean: -12.111650\n",
      "ep 2213: ep_len:761 episode reward: total was -7.710000. running mean: -12.067633\n",
      "ep 2213: ep_len:56 episode reward: total was 26.500000. running mean: -11.681957\n",
      "ep 2213: ep_len:603 episode reward: total was 7.240000. running mean: -11.492738\n",
      "ep 2213: ep_len:3600 episode reward: total was -92.060000. running mean: -12.298410\n",
      "ep 2213: ep_len:916 episode reward: total was -56.380000. running mean: -12.739226\n",
      "ep 2213: ep_len:742 episode reward: total was 36.780000. running mean: -12.244034\n",
      "ep 2213: ep_len:1486 episode reward: total was -4.930000. running mean: -12.170893\n",
      "ep 2213: ep_len:980 episode reward: total was -16.930000. running mean: -12.218485\n",
      "ep 2213: ep_len:2917 episode reward: total was -3.840000. running mean: -12.134700\n",
      "ep 2213: ep_len:69 episode reward: total was 33.000000. running mean: -11.683353\n",
      "epsilon:0.009992 episode_count: 33333. steps_count: 35752138.000000\n",
      "ep 2214: ep_len:1479 episode reward: total was 43.030000. running mean: -11.136219\n",
      "ep 2214: ep_len:216 episode reward: total was 14.000000. running mean: -10.884857\n",
      "ep 2214: ep_len:3053 episode reward: total was -29.890000. running mean: -11.074908\n",
      "ep 2214: ep_len:1494 episode reward: total was 16.820000. running mean: -10.795959\n",
      "ep 2214: ep_len:63 episode reward: total was 30.000000. running mean: -10.388000\n",
      "ep 2214: ep_len:85 episode reward: total was 41.000000. running mean: -9.874120\n",
      "ep 2214: ep_len:1625 episode reward: total was -406.710000. running mean: -13.842479\n",
      "ep 2214: ep_len:344 episode reward: total was 21.100000. running mean: -13.493054\n",
      "ep 2214: ep_len:644 episode reward: total was 2.230000. running mean: -13.335823\n",
      "ep 2214: ep_len:7199 episode reward: total was 0.990000. running mean: -13.192565\n",
      "ep 2214: ep_len:1202 episode reward: total was -14.410000. running mean: -13.204739\n",
      "ep 2214: ep_len:28 episode reward: total was 12.500000. running mean: -12.947692\n",
      "ep 2214: ep_len:1003 episode reward: total was -26.500000. running mean: -13.083215\n",
      "ep 2214: ep_len:2743 episode reward: total was -172.570000. running mean: -14.678083\n",
      "ep 2214: ep_len:62 episode reward: total was 29.500000. running mean: -14.236302\n",
      "epsilon:0.009992 episode_count: 33348. steps_count: 35773378.000000\n",
      "ep 2215: ep_len:1138 episode reward: total was -4.950000. running mean: -14.143439\n",
      "ep 2215: ep_len:612 episode reward: total was -56.530000. running mean: -14.567305\n",
      "ep 2215: ep_len:3072 episode reward: total was -2.300000. running mean: -14.444632\n",
      "ep 2215: ep_len:500 episode reward: total was -21.710000. running mean: -14.517285\n",
      "ep 2215: ep_len:108 episode reward: total was 49.500000. running mean: -13.877112\n",
      "ep 2215: ep_len:30 episode reward: total was 13.500000. running mean: -13.603341\n",
      "ep 2215: ep_len:500 episode reward: total was 45.660000. running mean: -13.010708\n",
      "ep 2215: ep_len:608 episode reward: total was 25.200000. running mean: -12.628601\n",
      "ep 2215: ep_len:632 episode reward: total was -58.400000. running mean: -13.086315\n",
      "ep 2215: ep_len:7370 episode reward: total was -41.580000. running mean: -13.371252\n",
      "ep 2215: ep_len:1147 episode reward: total was -3.850000. running mean: -13.276039\n",
      "ep 2215: ep_len:81 episode reward: total was 39.000000. running mean: -12.753279\n",
      "ep 2215: ep_len:39 episode reward: total was 16.500000. running mean: -12.460746\n",
      "ep 2215: ep_len:500 episode reward: total was -14.600000. running mean: -12.482138\n",
      "ep 2215: ep_len:2836 episode reward: total was -19.710000. running mean: -12.554417\n",
      "epsilon:0.009992 episode_count: 33363. steps_count: 35792551.000000\n",
      "ep 2216: ep_len:635 episode reward: total was -9.580000. running mean: -12.524673\n",
      "ep 2216: ep_len:690 episode reward: total was -28.070000. running mean: -12.680126\n",
      "ep 2216: ep_len:2912 episode reward: total was -21.630000. running mean: -12.769625\n",
      "ep 2216: ep_len:820 episode reward: total was 56.250000. running mean: -12.079429\n",
      "ep 2216: ep_len:65 episode reward: total was 29.500000. running mean: -11.663634\n",
      "ep 2216: ep_len:1057 episode reward: total was -13.280000. running mean: -11.679798\n",
      "ep 2216: ep_len:3558 episode reward: total was -77.330000. running mean: -12.336300\n",
      "ep 2216: ep_len:550 episode reward: total was -4.770000. running mean: -12.260637\n",
      "ep 2216: ep_len:798 episode reward: total was 19.790000. running mean: -11.940131\n",
      "ep 2216: ep_len:516 episode reward: total was -2.900000. running mean: -11.849729\n",
      "ep 2216: ep_len:70 episode reward: total was 33.500000. running mean: -11.396232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2216: ep_len:1112 episode reward: total was 31.000000. running mean: -10.972270\n",
      "ep 2216: ep_len:2870 episode reward: total was -12.540000. running mean: -10.987947\n",
      "epsilon:0.009992 episode_count: 33376. steps_count: 35808204.000000\n",
      "ep 2217: ep_len:642 episode reward: total was 23.660000. running mean: -10.641468\n",
      "ep 2217: ep_len:1184 episode reward: total was -119.220000. running mean: -11.727253\n",
      "ep 2217: ep_len:2941 episode reward: total was -20.110000. running mean: -11.811080\n",
      "ep 2217: ep_len:676 episode reward: total was -4.200000. running mean: -11.734970\n",
      "ep 2217: ep_len:94 episode reward: total was 42.500000. running mean: -11.192620\n",
      "ep 2217: ep_len:680 episode reward: total was 12.640000. running mean: -10.954294\n",
      "ep 2217: ep_len:656 episode reward: total was 28.620000. running mean: -10.558551\n",
      "ep 2217: ep_len:523 episode reward: total was -5.470000. running mean: -10.507665\n",
      "ep 2217: ep_len:715 episode reward: total was 30.260000. running mean: -10.099989\n",
      "ep 2217: ep_len:512 episode reward: total was -16.380000. running mean: -10.162789\n",
      "ep 2217: ep_len:46 episode reward: total was 21.500000. running mean: -9.846161\n",
      "ep 2217: ep_len:665 episode reward: total was 4.070000. running mean: -9.706999\n",
      "ep 2217: ep_len:2824 episode reward: total was -9.820000. running mean: -9.708129\n",
      "ep 2217: ep_len:69 episode reward: total was 31.500000. running mean: -9.296048\n",
      "epsilon:0.009992 episode_count: 33390. steps_count: 35820431.000000\n",
      "ep 2218: ep_len:1406 episode reward: total was 30.170000. running mean: -8.901387\n",
      "ep 2218: ep_len:1679 episode reward: total was -48.450000. running mean: -9.296874\n",
      "ep 2218: ep_len:2969 episode reward: total was -34.840000. running mean: -9.552305\n",
      "ep 2218: ep_len:1126 episode reward: total was -5.070000. running mean: -9.507482\n",
      "ep 2218: ep_len:44 episode reward: total was 19.000000. running mean: -9.222407\n",
      "ep 2218: ep_len:68 episode reward: total was 32.500000. running mean: -8.805183\n",
      "ep 2218: ep_len:500 episode reward: total was 41.280000. running mean: -8.304331\n",
      "ep 2218: ep_len:332 episode reward: total was 12.160000. running mean: -8.099688\n",
      "ep 2218: ep_len:941 episode reward: total was -38.630000. running mean: -8.404991\n",
      "ep 2218: ep_len:764 episode reward: total was 1.930000. running mean: -8.301641\n",
      "ep 2218: ep_len:694 episode reward: total was -2.200000. running mean: -8.240625\n",
      "ep 2218: ep_len:146 episode reward: total was 70.000000. running mean: -7.458218\n",
      "ep 2218: ep_len:104 episode reward: total was 49.000000. running mean: -6.893636\n",
      "ep 2218: ep_len:1473 episode reward: total was 7.980000. running mean: -6.744900\n",
      "ep 2218: ep_len:2868 episode reward: total was -53.720000. running mean: -7.214651\n",
      "ep 2218: ep_len:47 episode reward: total was 20.500000. running mean: -6.937504\n",
      "epsilon:0.009992 episode_count: 33406. steps_count: 35835592.000000\n",
      "ep 2219: ep_len:825 episode reward: total was -13.130000. running mean: -6.999429\n",
      "ep 2219: ep_len:199 episode reward: total was -6.340000. running mean: -6.992835\n",
      "ep 2219: ep_len:61 episode reward: total was 29.000000. running mean: -6.632907\n",
      "ep 2219: ep_len:3058 episode reward: total was -16.030000. running mean: -6.726878\n",
      "ep 2219: ep_len:1170 episode reward: total was -10.650000. running mean: -6.766109\n",
      "ep 2219: ep_len:35 episode reward: total was 16.000000. running mean: -6.538448\n",
      "ep 2219: ep_len:1068 episode reward: total was 3.440000. running mean: -6.438663\n",
      "ep 2219: ep_len:3743 episode reward: total was -184.160000. running mean: -8.215877\n",
      "ep 2219: ep_len:539 episode reward: total was -0.840000. running mean: -8.142118\n",
      "ep 2219: ep_len:887 episode reward: total was 30.800000. running mean: -7.752697\n",
      "ep 2219: ep_len:772 episode reward: total was -8.050000. running mean: -7.755670\n",
      "ep 2219: ep_len:77 episode reward: total was 35.500000. running mean: -7.323113\n",
      "ep 2219: ep_len:38 episode reward: total was 17.500000. running mean: -7.074882\n",
      "ep 2219: ep_len:1059 episode reward: total was 8.830000. running mean: -6.915833\n",
      "ep 2219: ep_len:2835 episode reward: total was -2.730000. running mean: -6.873975\n",
      "epsilon:0.009992 episode_count: 33421. steps_count: 35851958.000000\n",
      "ep 2220: ep_len:1139 episode reward: total was 4.800000. running mean: -6.757235\n",
      "ep 2220: ep_len:783 episode reward: total was -4.330000. running mean: -6.732963\n",
      "ep 2220: ep_len:2933 episode reward: total was -10.720000. running mean: -6.772833\n",
      "ep 2220: ep_len:1106 episode reward: total was -8.300000. running mean: -6.788105\n",
      "ep 2220: ep_len:130 episode reward: total was 62.000000. running mean: -6.100224\n",
      "ep 2220: ep_len:101 episode reward: total was 46.000000. running mean: -5.579221\n",
      "ep 2220: ep_len:691 episode reward: total was 33.140000. running mean: -5.192029\n",
      "ep 2220: ep_len:4046 episode reward: total was -331.500000. running mean: -8.455109\n",
      "ep 2220: ep_len:1242 episode reward: total was -43.300000. running mean: -8.803558\n",
      "ep 2220: ep_len:7313 episode reward: total was 45.750000. running mean: -8.258022\n",
      "ep 2220: ep_len:651 episode reward: total was 1.020000. running mean: -8.165242\n",
      "ep 2220: ep_len:51 episode reward: total was 24.000000. running mean: -7.843590\n",
      "ep 2220: ep_len:1131 episode reward: total was 9.150000. running mean: -7.673654\n",
      "ep 2220: ep_len:2854 episode reward: total was -49.210000. running mean: -8.089017\n",
      "epsilon:0.009992 episode_count: 33435. steps_count: 35876129.000000\n",
      "ep 2221: ep_len:641 episode reward: total was 16.610000. running mean: -7.842027\n",
      "ep 2221: ep_len:781 episode reward: total was -10.540000. running mean: -7.869007\n",
      "ep 2221: ep_len:3066 episode reward: total was 9.970000. running mean: -7.690617\n",
      "ep 2221: ep_len:526 episode reward: total was -28.240000. running mean: -7.896110\n",
      "ep 2221: ep_len:80 episode reward: total was 38.500000. running mean: -7.432149\n",
      "ep 2221: ep_len:42 episode reward: total was 19.500000. running mean: -7.162828\n",
      "ep 2221: ep_len:500 episode reward: total was -12.720000. running mean: -7.218400\n",
      "ep 2221: ep_len:3652 episode reward: total was -293.510000. running mean: -10.081316\n",
      "ep 2221: ep_len:947 episode reward: total was -6.080000. running mean: -10.041302\n",
      "ep 2221: ep_len:774 episode reward: total was -0.510000. running mean: -9.945989\n",
      "ep 2221: ep_len:500 episode reward: total was 14.240000. running mean: -9.704129\n",
      "ep 2221: ep_len:79 episode reward: total was 36.500000. running mean: -9.242088\n",
      "ep 2221: ep_len:1083 episode reward: total was -2.470000. running mean: -9.174367\n",
      "ep 2221: ep_len:2834 episode reward: total was -8.980000. running mean: -9.172424\n",
      "epsilon:0.009992 episode_count: 33449. steps_count: 35891634.000000\n",
      "ep 2222: ep_len:1419 episode reward: total was 10.680000. running mean: -8.973899\n",
      "ep 2222: ep_len:500 episode reward: total was 27.680000. running mean: -8.607360\n",
      "ep 2222: ep_len:80 episode reward: total was 38.500000. running mean: -8.136287\n",
      "ep 2222: ep_len:2958 episode reward: total was -21.980000. running mean: -8.274724\n",
      "ep 2222: ep_len:792 episode reward: total was 6.430000. running mean: -8.127677\n",
      "ep 2222: ep_len:1474 episode reward: total was 16.250000. running mean: -7.883900\n",
      "ep 2222: ep_len:309 episode reward: total was 8.290000. running mean: -7.722161\n",
      "ep 2222: ep_len:600 episode reward: total was 13.910000. running mean: -7.505839\n",
      "ep 2222: ep_len:7272 episode reward: total was 23.060000. running mean: -7.200181\n",
      "ep 2222: ep_len:1068 episode reward: total was -10.610000. running mean: -7.234279\n",
      "ep 2222: ep_len:503 episode reward: total was -4.590000. running mean: -7.207836\n",
      "ep 2222: ep_len:2896 episode reward: total was -7.170000. running mean: -7.207458\n",
      "epsilon:0.009992 episode_count: 33461. steps_count: 35911505.000000\n",
      "ep 2223: ep_len:763 episode reward: total was -11.730000. running mean: -7.252683\n",
      "ep 2223: ep_len:749 episode reward: total was -25.680000. running mean: -7.436957\n",
      "ep 2223: ep_len:82 episode reward: total was 38.000000. running mean: -6.982587\n",
      "ep 2223: ep_len:2935 episode reward: total was -29.400000. running mean: -7.206761\n",
      "ep 2223: ep_len:665 episode reward: total was 9.000000. running mean: -7.044694\n",
      "ep 2223: ep_len:59 episode reward: total was 28.000000. running mean: -6.694247\n",
      "ep 2223: ep_len:500 episode reward: total was 4.130000. running mean: -6.586004\n",
      "ep 2223: ep_len:3705 episode reward: total was -58.000000. running mean: -7.100144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2223: ep_len:560 episode reward: total was -10.730000. running mean: -7.136443\n",
      "ep 2223: ep_len:627 episode reward: total was 10.050000. running mean: -6.964578\n",
      "ep 2223: ep_len:648 episode reward: total was 5.300000. running mean: -6.841932\n",
      "ep 2223: ep_len:47 episode reward: total was 20.500000. running mean: -6.568513\n",
      "ep 2223: ep_len:101 episode reward: total was 46.000000. running mean: -6.042828\n",
      "ep 2223: ep_len:128 episode reward: total was 61.000000. running mean: -5.372400\n",
      "ep 2223: ep_len:618 episode reward: total was 3.890000. running mean: -5.279776\n",
      "ep 2223: ep_len:2894 episode reward: total was -24.910000. running mean: -5.476078\n",
      "ep 2223: ep_len:61 episode reward: total was 29.000000. running mean: -5.131317\n",
      "epsilon:0.009992 episode_count: 33478. steps_count: 35926647.000000\n",
      "ep 2224: ep_len:500 episode reward: total was 9.370000. running mean: -4.986304\n",
      "ep 2224: ep_len:500 episode reward: total was 0.830000. running mean: -4.928141\n",
      "ep 2224: ep_len:47 episode reward: total was 22.000000. running mean: -4.658860\n",
      "ep 2224: ep_len:3049 episode reward: total was -82.740000. running mean: -5.439671\n",
      "ep 2224: ep_len:500 episode reward: total was -23.840000. running mean: -5.623674\n",
      "ep 2224: ep_len:120 episode reward: total was 57.000000. running mean: -4.997437\n",
      "ep 2224: ep_len:747 episode reward: total was -72.280000. running mean: -5.670263\n",
      "ep 2224: ep_len:3553 episode reward: total was -96.570000. running mean: -6.579260\n",
      "ep 2224: ep_len:3139 episode reward: total was -530.580000. running mean: -11.819268\n",
      "ep 2224: ep_len:7373 episode reward: total was 8.740000. running mean: -11.613675\n",
      "ep 2224: ep_len:971 episode reward: total was 26.270000. running mean: -11.234838\n",
      "ep 2224: ep_len:137 episode reward: total was 67.000000. running mean: -10.452490\n",
      "ep 2224: ep_len:61 episode reward: total was 27.500000. running mean: -10.072965\n",
      "ep 2224: ep_len:1526 episode reward: total was -10.900000. running mean: -10.081236\n",
      "ep 2224: ep_len:2895 episode reward: total was 1.640000. running mean: -9.964023\n",
      "ep 2224: ep_len:66 episode reward: total was 31.500000. running mean: -9.549383\n",
      "epsilon:0.009992 episode_count: 33494. steps_count: 35951831.000000\n",
      "ep 2225: ep_len:662 episode reward: total was -3.650000. running mean: -9.490389\n",
      "ep 2225: ep_len:675 episode reward: total was -15.050000. running mean: -9.545985\n",
      "ep 2225: ep_len:65 episode reward: total was 31.000000. running mean: -9.140525\n",
      "ep 2225: ep_len:101 episode reward: total was 49.000000. running mean: -8.559120\n",
      "ep 2225: ep_len:675 episode reward: total was -10.590000. running mean: -8.579429\n",
      "ep 2225: ep_len:33 episode reward: total was 15.000000. running mean: -8.343635\n",
      "ep 2225: ep_len:59 episode reward: total was 28.000000. running mean: -7.980198\n",
      "ep 2225: ep_len:687 episode reward: total was -0.370000. running mean: -7.904096\n",
      "ep 2225: ep_len:3676 episode reward: total was -18.580000. running mean: -8.010855\n",
      "ep 2225: ep_len:557 episode reward: total was -35.000000. running mean: -8.280747\n",
      "ep 2225: ep_len:643 episode reward: total was 7.610000. running mean: -8.121839\n",
      "ep 2225: ep_len:965 episode reward: total was 25.800000. running mean: -7.782621\n",
      "ep 2225: ep_len:1018 episode reward: total was 18.530000. running mean: -7.519495\n",
      "ep 2225: ep_len:2791 episode reward: total was 3.410000. running mean: -7.410200\n",
      "ep 2225: ep_len:45 episode reward: total was 19.500000. running mean: -7.141098\n",
      "epsilon:0.009992 episode_count: 33509. steps_count: 35964483.000000\n",
      "ep 2226: ep_len:764 episode reward: total was -3.960000. running mean: -7.109287\n",
      "ep 2226: ep_len:1657 episode reward: total was -67.370000. running mean: -7.711894\n",
      "ep 2226: ep_len:3111 episode reward: total was -3.500000. running mean: -7.669775\n",
      "ep 2226: ep_len:793 episode reward: total was 25.050000. running mean: -7.342577\n",
      "ep 2226: ep_len:49 episode reward: total was 23.000000. running mean: -7.039151\n",
      "ep 2226: ep_len:95 episode reward: total was 46.000000. running mean: -6.508760\n",
      "ep 2226: ep_len:1471 episode reward: total was 2.510000. running mean: -6.418572\n",
      "ep 2226: ep_len:4121 episode reward: total was -42.510000. running mean: -6.779487\n",
      "ep 2226: ep_len:676 episode reward: total was -7.420000. running mean: -6.785892\n",
      "ep 2226: ep_len:787 episode reward: total was 12.120000. running mean: -6.596833\n",
      "ep 2226: ep_len:731 episode reward: total was -9.020000. running mean: -6.621064\n",
      "ep 2226: ep_len:68 episode reward: total was 32.500000. running mean: -6.229854\n",
      "ep 2226: ep_len:65 episode reward: total was 28.000000. running mean: -5.887555\n",
      "ep 2226: ep_len:698 episode reward: total was -59.330000. running mean: -6.421980\n",
      "ep 2226: ep_len:2824 episode reward: total was -11.780000. running mean: -6.475560\n",
      "epsilon:0.009992 episode_count: 33524. steps_count: 35982393.000000\n",
      "ep 2227: ep_len:610 episode reward: total was 13.580000. running mean: -6.275004\n",
      "ep 2227: ep_len:977 episode reward: total was 26.720000. running mean: -5.945054\n",
      "ep 2227: ep_len:2976 episode reward: total was -14.040000. running mean: -6.026004\n",
      "ep 2227: ep_len:914 episode reward: total was 78.840000. running mean: -5.177344\n",
      "ep 2227: ep_len:51 episode reward: total was 22.500000. running mean: -4.900570\n",
      "ep 2227: ep_len:101 episode reward: total was 49.000000. running mean: -4.361565\n",
      "ep 2227: ep_len:60 episode reward: total was 28.500000. running mean: -4.032949\n",
      "ep 2227: ep_len:1205 episode reward: total was 4.670000. running mean: -3.945919\n",
      "ep 2227: ep_len:4191 episode reward: total was -68.780000. running mean: -4.594260\n",
      "ep 2227: ep_len:846 episode reward: total was -62.730000. running mean: -5.175618\n",
      "ep 2227: ep_len:654 episode reward: total was 5.550000. running mean: -5.068361\n",
      "ep 2227: ep_len:603 episode reward: total was 9.250000. running mean: -4.925178\n",
      "ep 2227: ep_len:63 episode reward: total was 30.000000. running mean: -4.575926\n",
      "ep 2227: ep_len:104 episode reward: total was 50.500000. running mean: -4.025167\n",
      "ep 2227: ep_len:638 episode reward: total was -21.430000. running mean: -4.199215\n",
      "ep 2227: ep_len:2764 episode reward: total was -2.490000. running mean: -4.182123\n",
      "epsilon:0.009992 episode_count: 33540. steps_count: 35999150.000000\n",
      "ep 2228: ep_len:909 episode reward: total was -58.710000. running mean: -4.727402\n",
      "ep 2228: ep_len:695 episode reward: total was 2.480000. running mean: -4.655328\n",
      "ep 2228: ep_len:2989 episode reward: total was -16.420000. running mean: -4.772974\n",
      "ep 2228: ep_len:662 episode reward: total was 2.320000. running mean: -4.702045\n",
      "ep 2228: ep_len:167 episode reward: total was 79.000000. running mean: -3.865024\n",
      "ep 2228: ep_len:59 episode reward: total was 28.000000. running mean: -3.546374\n",
      "ep 2228: ep_len:1568 episode reward: total was -334.560000. running mean: -6.856510\n",
      "ep 2228: ep_len:3993 episode reward: total was -38.240000. running mean: -7.170345\n",
      "ep 2228: ep_len:622 episode reward: total was 15.450000. running mean: -6.944142\n",
      "ep 2228: ep_len:7171 episode reward: total was -660.350000. running mean: -13.478200\n",
      "ep 2228: ep_len:500 episode reward: total was 13.900000. running mean: -13.204418\n",
      "ep 2228: ep_len:3628 episode reward: total was -780.540000. running mean: -20.877774\n",
      "ep 2228: ep_len:2874 episode reward: total was -5.220000. running mean: -20.721196\n",
      "ep 2228: ep_len:54 episode reward: total was 25.500000. running mean: -20.258984\n",
      "epsilon:0.009992 episode_count: 33554. steps_count: 36025041.000000\n",
      "ep 2229: ep_len:1452 episode reward: total was 10.280000. running mean: -19.953595\n",
      "ep 2229: ep_len:768 episode reward: total was -7.350000. running mean: -19.827559\n",
      "ep 2229: ep_len:2877 episode reward: total was -21.920000. running mean: -19.848483\n",
      "ep 2229: ep_len:1214 episode reward: total was -32.440000. running mean: -19.974398\n",
      "ep 2229: ep_len:55 episode reward: total was 26.000000. running mean: -19.514654\n",
      "ep 2229: ep_len:125 episode reward: total was 58.000000. running mean: -18.739508\n",
      "ep 2229: ep_len:653 episode reward: total was 1.310000. running mean: -18.539013\n",
      "ep 2229: ep_len:3885 episode reward: total was -77.090000. running mean: -19.124523\n",
      "ep 2229: ep_len:1517 episode reward: total was -207.680000. running mean: -21.010077\n",
      "ep 2229: ep_len:7615 episode reward: total was -40.320000. running mean: -21.203177\n",
      "ep 2229: ep_len:1078 episode reward: total was -3.530000. running mean: -21.026445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2229: ep_len:85 episode reward: total was 41.000000. running mean: -20.406180\n",
      "ep 2229: ep_len:1066 episode reward: total was 9.330000. running mean: -20.108818\n",
      "ep 2229: ep_len:2783 episode reward: total was -6.710000. running mean: -19.974830\n",
      "ep 2229: ep_len:62 episode reward: total was 28.000000. running mean: -19.495082\n",
      "epsilon:0.009992 episode_count: 33569. steps_count: 36050276.000000\n",
      "ep 2230: ep_len:750 episode reward: total was -23.660000. running mean: -19.536731\n",
      "ep 2230: ep_len:184 episode reward: total was -1.780000. running mean: -19.359164\n",
      "ep 2230: ep_len:2988 episode reward: total was -48.020000. running mean: -19.645772\n",
      "ep 2230: ep_len:686 episode reward: total was -3.820000. running mean: -19.487515\n",
      "ep 2230: ep_len:78 episode reward: total was 36.000000. running mean: -18.932639\n",
      "ep 2230: ep_len:757 episode reward: total was -32.510000. running mean: -19.068413\n",
      "ep 2230: ep_len:645 episode reward: total was 16.690000. running mean: -18.710829\n",
      "ep 2230: ep_len:548 episode reward: total was -5.800000. running mean: -18.581721\n",
      "ep 2230: ep_len:595 episode reward: total was 2.720000. running mean: -18.368703\n",
      "ep 2230: ep_len:1542 episode reward: total was 20.950000. running mean: -17.975516\n",
      "ep 2230: ep_len:150 episode reward: total was 73.500000. running mean: -17.060761\n",
      "ep 2230: ep_len:1499 episode reward: total was 6.190000. running mean: -16.828254\n",
      "ep 2230: ep_len:2802 episode reward: total was 1.930000. running mean: -16.640671\n",
      "ep 2230: ep_len:43 episode reward: total was 20.000000. running mean: -16.274264\n",
      "epsilon:0.009992 episode_count: 33583. steps_count: 36063543.000000\n",
      "ep 2231: ep_len:771 episode reward: total was -18.600000. running mean: -16.297522\n",
      "ep 2231: ep_len:1657 episode reward: total was -75.540000. running mean: -16.889946\n",
      "ep 2231: ep_len:2866 episode reward: total was -18.230000. running mean: -16.903347\n",
      "ep 2231: ep_len:1173 episode reward: total was -16.720000. running mean: -16.901514\n",
      "ep 2231: ep_len:53 episode reward: total was 25.000000. running mean: -16.482498\n",
      "ep 2231: ep_len:155 episode reward: total was 74.500000. running mean: -15.572673\n",
      "ep 2231: ep_len:500 episode reward: total was 31.050000. running mean: -15.106447\n",
      "ep 2231: ep_len:3737 episode reward: total was 2.170000. running mean: -14.933682\n",
      "ep 2231: ep_len:563 episode reward: total was -8.250000. running mean: -14.866845\n",
      "ep 2231: ep_len:732 episode reward: total was 22.930000. running mean: -14.488877\n",
      "ep 2231: ep_len:1136 episode reward: total was -15.070000. running mean: -14.494688\n",
      "ep 2231: ep_len:73 episode reward: total was 35.000000. running mean: -13.999741\n",
      "ep 2231: ep_len:801 episode reward: total was -24.090000. running mean: -14.100644\n",
      "ep 2231: ep_len:2830 episode reward: total was -32.010000. running mean: -14.279737\n",
      "ep 2231: ep_len:47 episode reward: total was 22.000000. running mean: -13.916940\n",
      "epsilon:0.009992 episode_count: 33598. steps_count: 36080637.000000\n",
      "ep 2232: ep_len:635 episode reward: total was -18.060000. running mean: -13.958371\n",
      "ep 2232: ep_len:692 episode reward: total was -30.620000. running mean: -14.124987\n",
      "ep 2232: ep_len:3057 episode reward: total was 2.230000. running mean: -13.961437\n",
      "ep 2232: ep_len:500 episode reward: total was 37.800000. running mean: -13.443823\n",
      "ep 2232: ep_len:49 episode reward: total was 21.500000. running mean: -13.094384\n",
      "ep 2232: ep_len:70 episode reward: total was 33.500000. running mean: -12.628441\n",
      "ep 2232: ep_len:674 episode reward: total was 2.350000. running mean: -12.478656\n",
      "ep 2232: ep_len:3936 episode reward: total was -156.280000. running mean: -13.916670\n",
      "ep 2232: ep_len:1126 episode reward: total was -56.190000. running mean: -14.339403\n",
      "ep 2232: ep_len:802 episode reward: total was 17.930000. running mean: -14.016709\n",
      "ep 2232: ep_len:879 episode reward: total was 29.500000. running mean: -13.581542\n",
      "ep 2232: ep_len:54 episode reward: total was 25.500000. running mean: -13.190726\n",
      "ep 2232: ep_len:159 episode reward: total was 75.000000. running mean: -12.308819\n",
      "ep 2232: ep_len:52 episode reward: total was 24.500000. running mean: -11.940731\n",
      "ep 2232: ep_len:1094 episode reward: total was -13.470000. running mean: -11.956024\n",
      "ep 2232: ep_len:2835 episode reward: total was 0.510000. running mean: -11.831363\n",
      "epsilon:0.009992 episode_count: 33614. steps_count: 36097251.000000\n",
      "ep 2233: ep_len:1160 episode reward: total was -1.850000. running mean: -11.731550\n",
      "ep 2233: ep_len:747 episode reward: total was -15.930000. running mean: -11.773534\n",
      "ep 2233: ep_len:80 episode reward: total was 38.500000. running mean: -11.270799\n",
      "ep 2233: ep_len:3005 episode reward: total was -24.860000. running mean: -11.406691\n",
      "ep 2233: ep_len:951 episode reward: total was 78.910000. running mean: -10.503524\n",
      "ep 2233: ep_len:68 episode reward: total was 32.500000. running mean: -10.073489\n",
      "ep 2233: ep_len:48 episode reward: total was 22.500000. running mean: -9.747754\n",
      "ep 2233: ep_len:835 episode reward: total was 22.930000. running mean: -9.420976\n",
      "ep 2233: ep_len:638 episode reward: total was 19.010000. running mean: -9.136667\n",
      "ep 2233: ep_len:3369 episode reward: total was -261.600000. running mean: -11.661300\n",
      "ep 2233: ep_len:728 episode reward: total was 31.220000. running mean: -11.232487\n",
      "ep 2233: ep_len:613 episode reward: total was -0.290000. running mean: -11.123062\n",
      "ep 2233: ep_len:61 episode reward: total was 29.000000. running mean: -10.721831\n",
      "ep 2233: ep_len:127 episode reward: total was 60.500000. running mean: -10.009613\n",
      "ep 2233: ep_len:37 episode reward: total was 14.000000. running mean: -9.769517\n",
      "ep 2233: ep_len:73 episode reward: total was 35.000000. running mean: -9.321822\n",
      "ep 2233: ep_len:905 episode reward: total was -2.840000. running mean: -9.257004\n",
      "ep 2233: ep_len:2869 episode reward: total was -54.130000. running mean: -9.705734\n",
      "ep 2233: ep_len:48 episode reward: total was 22.500000. running mean: -9.383676\n",
      "epsilon:0.009992 episode_count: 33633. steps_count: 36113613.000000\n",
      "ep 2234: ep_len:1162 episode reward: total was 18.840000. running mean: -9.101439\n",
      "ep 2234: ep_len:823 episode reward: total was -26.670000. running mean: -9.277125\n",
      "ep 2234: ep_len:2950 episode reward: total was -98.940000. running mean: -10.173754\n",
      "ep 2234: ep_len:674 episode reward: total was 0.410000. running mean: -10.067916\n",
      "ep 2234: ep_len:54 episode reward: total was 25.500000. running mean: -9.712237\n",
      "ep 2234: ep_len:138 episode reward: total was 66.000000. running mean: -8.955115\n",
      "ep 2234: ep_len:111 episode reward: total was 52.500000. running mean: -8.340564\n",
      "ep 2234: ep_len:47 episode reward: total was 22.000000. running mean: -8.037158\n",
      "ep 2234: ep_len:599 episode reward: total was -7.660000. running mean: -8.033386\n",
      "ep 2234: ep_len:500 episode reward: total was 26.160000. running mean: -7.691453\n",
      "ep 2234: ep_len:577 episode reward: total was 25.250000. running mean: -7.362038\n",
      "ep 2234: ep_len:668 episode reward: total was 21.950000. running mean: -7.068918\n",
      "ep 2234: ep_len:666 episode reward: total was 0.430000. running mean: -6.993928\n",
      "ep 2234: ep_len:94 episode reward: total was 42.500000. running mean: -6.498989\n",
      "ep 2234: ep_len:208 episode reward: total was 100.510000. running mean: -5.428899\n",
      "ep 2234: ep_len:1151 episode reward: total was -19.970000. running mean: -5.574310\n",
      "ep 2234: ep_len:2884 episode reward: total was -20.510000. running mean: -5.723667\n",
      "ep 2234: ep_len:38 episode reward: total was 17.500000. running mean: -5.491431\n",
      "epsilon:0.009992 episode_count: 33651. steps_count: 36126957.000000\n",
      "ep 2235: ep_len:681 episode reward: total was -32.590000. running mean: -5.762416\n",
      "ep 2235: ep_len:1583 episode reward: total was -49.960000. running mean: -6.204392\n",
      "ep 2235: ep_len:2946 episode reward: total was -5.800000. running mean: -6.200348\n",
      "ep 2235: ep_len:727 episode reward: total was -32.140000. running mean: -6.459745\n",
      "ep 2235: ep_len:164 episode reward: total was 77.500000. running mean: -5.620147\n",
      "ep 2235: ep_len:1498 episode reward: total was 15.970000. running mean: -5.404246\n",
      "ep 2235: ep_len:633 episode reward: total was 26.340000. running mean: -5.086803\n",
      "ep 2235: ep_len:761 episode reward: total was -17.850000. running mean: -5.214435\n",
      "ep 2235: ep_len:637 episode reward: total was -0.070000. running mean: -5.162991\n",
      "ep 2235: ep_len:579 episode reward: total was -90.300000. running mean: -6.014361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2235: ep_len:1151 episode reward: total was -10.880000. running mean: -6.063017\n",
      "ep 2235: ep_len:2848 episode reward: total was -40.640000. running mean: -6.408787\n",
      "epsilon:0.009992 episode_count: 33663. steps_count: 36141165.000000\n",
      "ep 2236: ep_len:638 episode reward: total was 6.770000. running mean: -6.276999\n",
      "ep 2236: ep_len:938 episode reward: total was 10.310000. running mean: -6.111129\n",
      "ep 2236: ep_len:65 episode reward: total was 28.000000. running mean: -5.770018\n",
      "ep 2236: ep_len:2958 episode reward: total was -37.730000. running mean: -6.089618\n",
      "ep 2236: ep_len:656 episode reward: total was 0.660000. running mean: -6.022122\n",
      "ep 2236: ep_len:115 episode reward: total was 54.500000. running mean: -5.416900\n",
      "ep 2236: ep_len:73 episode reward: total was 35.000000. running mean: -5.012731\n",
      "ep 2236: ep_len:1038 episode reward: total was -142.790000. running mean: -6.390504\n",
      "ep 2236: ep_len:3692 episode reward: total was -79.260000. running mean: -7.119199\n",
      "ep 2236: ep_len:577 episode reward: total was 1.990000. running mean: -7.028107\n",
      "ep 2236: ep_len:739 episode reward: total was -1.220000. running mean: -6.970026\n",
      "ep 2236: ep_len:1073 episode reward: total was 42.280000. running mean: -6.477526\n",
      "ep 2236: ep_len:115 episode reward: total was 54.500000. running mean: -5.867751\n",
      "ep 2236: ep_len:500 episode reward: total was 16.660000. running mean: -5.642473\n",
      "ep 2236: ep_len:2876 episode reward: total was 9.070000. running mean: -5.495348\n",
      "epsilon:0.009992 episode_count: 33678. steps_count: 36157218.000000\n",
      "ep 2237: ep_len:839 episode reward: total was -29.690000. running mean: -5.737295\n",
      "ep 2237: ep_len:821 episode reward: total was -76.320000. running mean: -6.443122\n",
      "ep 2237: ep_len:28 episode reward: total was 9.500000. running mean: -6.283691\n",
      "ep 2237: ep_len:2965 episode reward: total was -104.270000. running mean: -7.263554\n",
      "ep 2237: ep_len:584 episode reward: total was -8.470000. running mean: -7.275618\n",
      "ep 2237: ep_len:56 episode reward: total was 26.500000. running mean: -6.937862\n",
      "ep 2237: ep_len:109 episode reward: total was 51.500000. running mean: -6.353483\n",
      "ep 2237: ep_len:60 episode reward: total was 27.000000. running mean: -6.019949\n",
      "ep 2237: ep_len:584 episode reward: total was 44.120000. running mean: -5.518549\n",
      "ep 2237: ep_len:3826 episode reward: total was -87.970000. running mean: -6.343064\n",
      "ep 2237: ep_len:1512 episode reward: total was -94.560000. running mean: -7.225233\n",
      "ep 2237: ep_len:690 episode reward: total was 18.440000. running mean: -6.968581\n",
      "ep 2237: ep_len:907 episode reward: total was 12.090000. running mean: -6.777995\n",
      "ep 2237: ep_len:84 episode reward: total was 37.500000. running mean: -6.335215\n",
      "ep 2237: ep_len:81 episode reward: total was 37.500000. running mean: -5.896863\n",
      "ep 2237: ep_len:623 episode reward: total was -32.320000. running mean: -6.161094\n",
      "ep 2237: ep_len:2839 episode reward: total was -12.700000. running mean: -6.226483\n",
      "ep 2237: ep_len:35 episode reward: total was 16.000000. running mean: -6.004218\n",
      "epsilon:0.009992 episode_count: 33696. steps_count: 36173861.000000\n",
      "ep 2238: ep_len:1448 episode reward: total was 19.910000. running mean: -5.745076\n",
      "ep 2238: ep_len:1253 episode reward: total was -53.290000. running mean: -6.220525\n",
      "ep 2238: ep_len:79 episode reward: total was 36.500000. running mean: -5.793320\n",
      "ep 2238: ep_len:2937 episode reward: total was -56.770000. running mean: -6.303087\n",
      "ep 2238: ep_len:749 episode reward: total was 22.100000. running mean: -6.019056\n",
      "ep 2238: ep_len:50 episode reward: total was 23.500000. running mean: -5.723865\n",
      "ep 2238: ep_len:1456 episode reward: total was -176.190000. running mean: -7.428527\n",
      "ep 2238: ep_len:623 episode reward: total was 12.830000. running mean: -7.225942\n",
      "ep 2238: ep_len:1255 episode reward: total was -101.750000. running mean: -8.171182\n",
      "ep 2238: ep_len:704 episode reward: total was 39.830000. running mean: -7.691170\n",
      "ep 2238: ep_len:1142 episode reward: total was -4.910000. running mean: -7.663359\n",
      "ep 2238: ep_len:44 episode reward: total was 19.000000. running mean: -7.396725\n",
      "ep 2238: ep_len:971 episode reward: total was -59.880000. running mean: -7.921558\n",
      "ep 2238: ep_len:2943 episode reward: total was -1.310000. running mean: -7.855442\n",
      "epsilon:0.009992 episode_count: 33710. steps_count: 36189515.000000\n",
      "ep 2239: ep_len:1048 episode reward: total was -5.850000. running mean: -7.835388\n",
      "ep 2239: ep_len:1218 episode reward: total was -56.670000. running mean: -8.323734\n",
      "ep 2239: ep_len:2971 episode reward: total was -39.040000. running mean: -8.630897\n",
      "ep 2239: ep_len:1203 episode reward: total was -31.570000. running mean: -8.860288\n",
      "ep 2239: ep_len:885 episode reward: total was 27.720000. running mean: -8.494485\n",
      "ep 2239: ep_len:589 episode reward: total was 20.480000. running mean: -8.204740\n",
      "ep 2239: ep_len:1539 episode reward: total was -230.830000. running mean: -10.430992\n",
      "ep 2239: ep_len:7401 episode reward: total was 78.590000. running mean: -9.540783\n",
      "ep 2239: ep_len:580 episode reward: total was -5.480000. running mean: -9.500175\n",
      "ep 2239: ep_len:116 episode reward: total was 55.000000. running mean: -8.855173\n",
      "ep 2239: ep_len:795 episode reward: total was 15.930000. running mean: -8.607321\n",
      "ep 2239: ep_len:2898 episode reward: total was 3.260000. running mean: -8.488648\n",
      "epsilon:0.009992 episode_count: 33722. steps_count: 36210758.000000\n",
      "ep 2240: ep_len:676 episode reward: total was 0.940000. running mean: -8.394362\n",
      "ep 2240: ep_len:577 episode reward: total was -10.530000. running mean: -8.415718\n",
      "ep 2240: ep_len:2949 episode reward: total was -46.580000. running mean: -8.797361\n",
      "ep 2240: ep_len:666 episode reward: total was -4.470000. running mean: -8.754087\n",
      "ep 2240: ep_len:109 episode reward: total was 51.500000. running mean: -8.151546\n",
      "ep 2240: ep_len:88 episode reward: total was 41.000000. running mean: -7.660031\n",
      "ep 2240: ep_len:814 episode reward: total was -225.830000. running mean: -9.841731\n",
      "ep 2240: ep_len:3793 episode reward: total was -83.410000. running mean: -10.577413\n",
      "ep 2240: ep_len:541 episode reward: total was 13.360000. running mean: -10.338039\n",
      "ep 2240: ep_len:709 episode reward: total was 21.780000. running mean: -10.016859\n",
      "ep 2240: ep_len:500 episode reward: total was -8.320000. running mean: -9.999890\n",
      "ep 2240: ep_len:102 episode reward: total was 48.000000. running mean: -9.419891\n",
      "ep 2240: ep_len:894 episode reward: total was 16.670000. running mean: -9.158992\n",
      "ep 2240: ep_len:2907 episode reward: total was -36.670000. running mean: -9.434102\n",
      "epsilon:0.009992 episode_count: 33736. steps_count: 36226083.000000\n",
      "ep 2241: ep_len:1090 episode reward: total was 9.120000. running mean: -9.248561\n",
      "ep 2241: ep_len:1140 episode reward: total was -47.140000. running mean: -9.627476\n",
      "ep 2241: ep_len:58 episode reward: total was 27.500000. running mean: -9.256201\n",
      "ep 2241: ep_len:2888 episode reward: total was -20.900000. running mean: -9.372639\n",
      "ep 2241: ep_len:551 episode reward: total was -53.700000. running mean: -9.815913\n",
      "ep 2241: ep_len:58 episode reward: total was 26.000000. running mean: -9.457753\n",
      "ep 2241: ep_len:102 episode reward: total was 49.500000. running mean: -8.868176\n",
      "ep 2241: ep_len:42 episode reward: total was 19.500000. running mean: -8.584494\n",
      "ep 2241: ep_len:1493 episode reward: total was 12.190000. running mean: -8.376749\n",
      "ep 2241: ep_len:670 episode reward: total was 24.720000. running mean: -8.045782\n",
      "ep 2241: ep_len:1180 episode reward: total was -52.670000. running mean: -8.492024\n",
      "ep 2241: ep_len:728 episode reward: total was 54.610000. running mean: -7.861004\n",
      "ep 2241: ep_len:1584 episode reward: total was -1173.670000. running mean: -19.519094\n",
      "ep 2241: ep_len:118 episode reward: total was 54.500000. running mean: -18.778903\n",
      "ep 2241: ep_len:521 episode reward: total was 11.080000. running mean: -18.480314\n",
      "ep 2241: ep_len:46 episode reward: total was 21.500000. running mean: -18.080511\n",
      "epsilon:0.009992 episode_count: 33752. steps_count: 36238352.000000\n",
      "ep 2242: ep_len:644 episode reward: total was -8.880000. running mean: -17.988505\n",
      "ep 2242: ep_len:676 episode reward: total was -7.000000. running mean: -17.878620\n",
      "ep 2242: ep_len:56 episode reward: total was 26.500000. running mean: -17.434834\n",
      "ep 2242: ep_len:2741 episode reward: total was -25.830000. running mean: -17.518786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2242: ep_len:500 episode reward: total was -17.020000. running mean: -17.513798\n",
      "ep 2242: ep_len:95 episode reward: total was 43.000000. running mean: -16.908660\n",
      "ep 2242: ep_len:1149 episode reward: total was -18.980000. running mean: -16.929373\n",
      "ep 2242: ep_len:358 episode reward: total was 17.720000. running mean: -16.582880\n",
      "ep 2242: ep_len:579 episode reward: total was -30.740000. running mean: -16.724451\n",
      "ep 2242: ep_len:727 episode reward: total was 39.260000. running mean: -16.164606\n",
      "ep 2242: ep_len:1141 episode reward: total was -6.940000. running mean: -16.072360\n",
      "ep 2242: ep_len:84 episode reward: total was 39.000000. running mean: -15.521637\n",
      "ep 2242: ep_len:105 episode reward: total was 51.000000. running mean: -14.856420\n",
      "ep 2242: ep_len:602 episode reward: total was 7.220000. running mean: -14.635656\n",
      "ep 2242: ep_len:2942 episode reward: total was 2.780000. running mean: -14.461500\n",
      "ep 2242: ep_len:63 episode reward: total was 30.000000. running mean: -14.016885\n",
      "epsilon:0.009992 episode_count: 33768. steps_count: 36250814.000000\n",
      "ep 2243: ep_len:699 episode reward: total was -31.440000. running mean: -14.191116\n",
      "ep 2243: ep_len:749 episode reward: total was -31.260000. running mean: -14.361805\n",
      "ep 2243: ep_len:60 episode reward: total was 27.000000. running mean: -13.948186\n",
      "ep 2243: ep_len:3071 episode reward: total was 6.440000. running mean: -13.744305\n",
      "ep 2243: ep_len:1619 episode reward: total was -33.320000. running mean: -13.940062\n",
      "ep 2243: ep_len:1479 episode reward: total was 27.330000. running mean: -13.527361\n",
      "ep 2243: ep_len:3716 episode reward: total was -477.730000. running mean: -18.169387\n",
      "ep 2243: ep_len:1587 episode reward: total was -3.950000. running mean: -18.027193\n",
      "ep 2243: ep_len:813 episode reward: total was 33.720000. running mean: -17.509722\n",
      "ep 2243: ep_len:616 episode reward: total was -6.220000. running mean: -17.396824\n",
      "ep 2243: ep_len:72 episode reward: total was 34.500000. running mean: -16.877856\n",
      "ep 2243: ep_len:197 episode reward: total was 94.000000. running mean: -15.769078\n",
      "ep 2243: ep_len:598 episode reward: total was 0.760000. running mean: -15.603787\n",
      "ep 2243: ep_len:2809 episode reward: total was -3.320000. running mean: -15.480949\n",
      "ep 2243: ep_len:74 episode reward: total was 34.000000. running mean: -14.986139\n",
      "epsilon:0.009992 episode_count: 33783. steps_count: 36268973.000000\n",
      "ep 2244: ep_len:931 episode reward: total was -61.570000. running mean: -15.451978\n",
      "ep 2244: ep_len:698 episode reward: total was -29.550000. running mean: -15.592958\n",
      "ep 2244: ep_len:2938 episode reward: total was -1.300000. running mean: -15.450029\n",
      "ep 2244: ep_len:805 episode reward: total was 0.810000. running mean: -15.287428\n",
      "ep 2244: ep_len:115 episode reward: total was 56.000000. running mean: -14.574554\n",
      "ep 2244: ep_len:37 episode reward: total was 17.000000. running mean: -14.258809\n",
      "ep 2244: ep_len:593 episode reward: total was -11.880000. running mean: -14.235020\n",
      "ep 2244: ep_len:676 episode reward: total was 10.880000. running mean: -13.983870\n",
      "ep 2244: ep_len:794 episode reward: total was -21.900000. running mean: -14.063032\n",
      "ep 2244: ep_len:688 episode reward: total was 32.410000. running mean: -13.598301\n",
      "ep 2244: ep_len:500 episode reward: total was -1.440000. running mean: -13.476718\n",
      "ep 2244: ep_len:776 episode reward: total was -30.380000. running mean: -13.645751\n",
      "ep 2244: ep_len:2848 episode reward: total was 3.150000. running mean: -13.477794\n",
      "ep 2244: ep_len:48 episode reward: total was 22.500000. running mean: -13.118016\n",
      "epsilon:0.009992 episode_count: 33797. steps_count: 36281420.000000\n",
      "ep 2245: ep_len:3690 episode reward: total was -1032.860000. running mean: -23.315435\n",
      "ep 2245: ep_len:615 episode reward: total was -31.390000. running mean: -23.396181\n",
      "ep 2245: ep_len:48 episode reward: total was 22.500000. running mean: -22.937219\n",
      "ep 2245: ep_len:3023 episode reward: total was -42.430000. running mean: -23.132147\n",
      "ep 2245: ep_len:1204 episode reward: total was -28.130000. running mean: -23.182126\n",
      "ep 2245: ep_len:110 episode reward: total was 52.000000. running mean: -22.430304\n",
      "ep 2245: ep_len:80 episode reward: total was 37.000000. running mean: -21.836001\n",
      "ep 2245: ep_len:44 episode reward: total was 20.500000. running mean: -21.412641\n",
      "ep 2245: ep_len:872 episode reward: total was 7.450000. running mean: -21.124015\n",
      "ep 2245: ep_len:4127 episode reward: total was -216.910000. running mean: -23.081875\n",
      "ep 2245: ep_len:4901 episode reward: total was -1338.960000. running mean: -36.240656\n",
      "ep 2245: ep_len:7329 episode reward: total was -3.600000. running mean: -35.914249\n",
      "ep 2245: ep_len:935 episode reward: total was 27.600000. running mean: -35.279107\n",
      "ep 2245: ep_len:106 episode reward: total was 50.000000. running mean: -34.426316\n",
      "ep 2245: ep_len:652 episode reward: total was -30.500000. running mean: -34.387053\n",
      "ep 2245: ep_len:2892 episode reward: total was 5.190000. running mean: -33.991282\n",
      "epsilon:0.009992 episode_count: 33813. steps_count: 36312048.000000\n",
      "ep 2246: ep_len:663 episode reward: total was -404.260000. running mean: -37.693969\n",
      "ep 2246: ep_len:1152 episode reward: total was -7.290000. running mean: -37.389930\n",
      "ep 2246: ep_len:44 episode reward: total was 20.500000. running mean: -36.811030\n",
      "ep 2246: ep_len:2982 episode reward: total was -5.070000. running mean: -36.493620\n",
      "ep 2246: ep_len:1467 episode reward: total was 8.440000. running mean: -36.044284\n",
      "ep 2246: ep_len:78 episode reward: total was 37.500000. running mean: -35.308841\n",
      "ep 2246: ep_len:47 episode reward: total was 20.500000. running mean: -34.750753\n",
      "ep 2246: ep_len:1137 episode reward: total was 14.950000. running mean: -34.253745\n",
      "ep 2246: ep_len:324 episode reward: total was -33.740000. running mean: -34.248608\n",
      "ep 2246: ep_len:1266 episode reward: total was -66.290000. running mean: -34.569022\n",
      "ep 2246: ep_len:759 episode reward: total was 15.080000. running mean: -34.072531\n",
      "ep 2246: ep_len:1434 episode reward: total was -26.360000. running mean: -33.995406\n",
      "ep 2246: ep_len:48 episode reward: total was 22.500000. running mean: -33.430452\n",
      "ep 2246: ep_len:643 episode reward: total was 7.050000. running mean: -33.025647\n",
      "ep 2246: ep_len:2926 episode reward: total was -4.700000. running mean: -32.742391\n",
      "epsilon:0.009992 episode_count: 33828. steps_count: 36327018.000000\n",
      "ep 2247: ep_len:959 episode reward: total was -21.200000. running mean: -32.626967\n",
      "ep 2247: ep_len:678 episode reward: total was -33.790000. running mean: -32.638597\n",
      "ep 2247: ep_len:76 episode reward: total was 36.500000. running mean: -31.947211\n",
      "ep 2247: ep_len:3015 episode reward: total was -29.110000. running mean: -31.918839\n",
      "ep 2247: ep_len:797 episode reward: total was 17.740000. running mean: -31.422251\n",
      "ep 2247: ep_len:62 episode reward: total was 29.500000. running mean: -30.813028\n",
      "ep 2247: ep_len:152 episode reward: total was 71.500000. running mean: -29.789898\n",
      "ep 2247: ep_len:500 episode reward: total was 10.570000. running mean: -29.386299\n",
      "ep 2247: ep_len:3972 episode reward: total was -155.890000. running mean: -30.651336\n",
      "ep 2247: ep_len:612 episode reward: total was -31.420000. running mean: -30.659023\n",
      "ep 2247: ep_len:729 episode reward: total was 4.490000. running mean: -30.307533\n",
      "ep 2247: ep_len:3529 episode reward: total was -404.230000. running mean: -34.046757\n",
      "ep 2247: ep_len:53 episode reward: total was 25.000000. running mean: -33.456290\n",
      "ep 2247: ep_len:1514 episode reward: total was 24.280000. running mean: -32.878927\n",
      "ep 2247: ep_len:45 episode reward: total was 21.000000. running mean: -32.340137\n",
      "ep 2247: ep_len:65 episode reward: total was 29.500000. running mean: -31.721736\n",
      "epsilon:0.009992 episode_count: 33844. steps_count: 36343776.000000\n",
      "ep 2248: ep_len:1480 episode reward: total was 5.110000. running mean: -31.353419\n",
      "ep 2248: ep_len:771 episode reward: total was -15.970000. running mean: -31.199585\n",
      "ep 2248: ep_len:64 episode reward: total was 30.500000. running mean: -30.582589\n",
      "ep 2248: ep_len:3071 episode reward: total was 13.670000. running mean: -30.140063\n",
      "ep 2248: ep_len:596 episode reward: total was -2.320000. running mean: -29.861862\n",
      "ep 2248: ep_len:44 episode reward: total was 20.500000. running mean: -29.358244\n",
      "ep 2248: ep_len:59 episode reward: total was 26.500000. running mean: -28.799661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2248: ep_len:857 episode reward: total was 24.070000. running mean: -28.270965\n",
      "ep 2248: ep_len:3717 episode reward: total was -30.290000. running mean: -28.291155\n",
      "ep 2248: ep_len:1583 episode reward: total was -71.120000. running mean: -28.719443\n",
      "ep 2248: ep_len:674 episode reward: total was 8.320000. running mean: -28.349049\n",
      "ep 2248: ep_len:1087 episode reward: total was -16.570000. running mean: -28.231258\n",
      "ep 2248: ep_len:69 episode reward: total was 33.000000. running mean: -27.618946\n",
      "ep 2248: ep_len:1073 episode reward: total was 2.480000. running mean: -27.317956\n",
      "ep 2248: ep_len:2760 episode reward: total was -5.810000. running mean: -27.102877\n",
      "ep 2248: ep_len:57 episode reward: total was 27.000000. running mean: -26.561848\n",
      "epsilon:0.009992 episode_count: 33860. steps_count: 36361738.000000\n",
      "ep 2249: ep_len:1491 episode reward: total was 26.160000. running mean: -26.034630\n",
      "ep 2249: ep_len:1674 episode reward: total was -54.130000. running mean: -26.315583\n",
      "ep 2249: ep_len:64 episode reward: total was 30.500000. running mean: -25.747427\n",
      "ep 2249: ep_len:3060 episode reward: total was -41.980000. running mean: -25.909753\n",
      "ep 2249: ep_len:684 episode reward: total was 11.120000. running mean: -25.539456\n",
      "ep 2249: ep_len:54 episode reward: total was 24.000000. running mean: -25.044061\n",
      "ep 2249: ep_len:1401 episode reward: total was -169.980000. running mean: -26.493420\n",
      "ep 2249: ep_len:661 episode reward: total was 26.650000. running mean: -25.961986\n",
      "ep 2249: ep_len:949 episode reward: total was -35.120000. running mean: -26.053566\n",
      "ep 2249: ep_len:769 episode reward: total was 20.120000. running mean: -25.591831\n",
      "ep 2249: ep_len:1141 episode reward: total was -3.360000. running mean: -25.369512\n",
      "ep 2249: ep_len:98 episode reward: total was 47.500000. running mean: -24.640817\n",
      "ep 2249: ep_len:43 episode reward: total was 20.000000. running mean: -24.194409\n",
      "ep 2249: ep_len:635 episode reward: total was -21.090000. running mean: -24.163365\n",
      "ep 2249: ep_len:2801 episode reward: total was 1.740000. running mean: -23.904331\n",
      "ep 2249: ep_len:52 episode reward: total was 24.500000. running mean: -23.420288\n",
      "epsilon:0.009992 episode_count: 33876. steps_count: 36377315.000000\n",
      "ep 2250: ep_len:613 episode reward: total was 8.680000. running mean: -23.099285\n",
      "ep 2250: ep_len:1660 episode reward: total was -23.360000. running mean: -23.101892\n",
      "ep 2250: ep_len:2857 episode reward: total was -59.130000. running mean: -23.462173\n",
      "ep 2250: ep_len:796 episode reward: total was 28.510000. running mean: -22.942452\n",
      "ep 2250: ep_len:110 episode reward: total was 50.500000. running mean: -22.208027\n",
      "ep 2250: ep_len:655 episode reward: total was 16.730000. running mean: -21.818647\n",
      "ep 2250: ep_len:349 episode reward: total was 18.180000. running mean: -21.418660\n",
      "ep 2250: ep_len:1582 episode reward: total was -40.370000. running mean: -21.608174\n",
      "ep 2250: ep_len:673 episode reward: total was 24.020000. running mean: -21.151892\n",
      "ep 2250: ep_len:1105 episode reward: total was -0.230000. running mean: -20.942673\n",
      "ep 2250: ep_len:82 episode reward: total was 38.000000. running mean: -20.353246\n",
      "ep 2250: ep_len:185 episode reward: total was 89.500000. running mean: -19.254714\n",
      "ep 2250: ep_len:46 episode reward: total was 21.500000. running mean: -18.847167\n",
      "ep 2250: ep_len:102 episode reward: total was 49.500000. running mean: -18.163695\n",
      "ep 2250: ep_len:790 episode reward: total was 17.510000. running mean: -17.806958\n",
      "ep 2250: ep_len:38 episode reward: total was 17.500000. running mean: -17.453889\n",
      "epsilon:0.009992 episode_count: 33892. steps_count: 36388958.000000\n",
      "ep 2251: ep_len:611 episode reward: total was -7.780000. running mean: -17.357150\n",
      "ep 2251: ep_len:500 episode reward: total was 25.780000. running mean: -16.925778\n",
      "ep 2251: ep_len:32 episode reward: total was 14.500000. running mean: -16.611520\n",
      "ep 2251: ep_len:2977 episode reward: total was -33.480000. running mean: -16.780205\n",
      "ep 2251: ep_len:669 episode reward: total was 9.230000. running mean: -16.520103\n",
      "ep 2251: ep_len:1668 episode reward: total was -474.350000. running mean: -21.098402\n",
      "ep 2251: ep_len:353 episode reward: total was 17.700000. running mean: -20.710418\n",
      "ep 2251: ep_len:1565 episode reward: total was -3.130000. running mean: -20.534614\n",
      "ep 2251: ep_len:736 episode reward: total was 0.580000. running mean: -20.323468\n",
      "ep 2251: ep_len:1510 episode reward: total was 5.960000. running mean: -20.060633\n",
      "ep 2251: ep_len:73 episode reward: total was 35.000000. running mean: -19.510027\n",
      "ep 2251: ep_len:98 episode reward: total was 47.500000. running mean: -18.839927\n",
      "ep 2251: ep_len:106 episode reward: total was 51.500000. running mean: -18.136527\n",
      "ep 2251: ep_len:793 episode reward: total was -18.370000. running mean: -18.138862\n",
      "ep 2251: ep_len:2791 episode reward: total was -4.760000. running mean: -18.005073\n",
      "epsilon:0.009992 episode_count: 33907. steps_count: 36403440.000000\n",
      "ep 2252: ep_len:1009 episode reward: total was -67.850000. running mean: -18.503523\n",
      "ep 2252: ep_len:1001 episode reward: total was 16.980000. running mean: -18.148687\n",
      "ep 2252: ep_len:2823 episode reward: total was -0.030000. running mean: -17.967501\n",
      "ep 2252: ep_len:1417 episode reward: total was 25.570000. running mean: -17.532126\n",
      "ep 2252: ep_len:68 episode reward: total was 32.500000. running mean: -17.031804\n",
      "ep 2252: ep_len:117 episode reward: total was 57.000000. running mean: -16.291486\n",
      "ep 2252: ep_len:95 episode reward: total was 46.000000. running mean: -15.668571\n",
      "ep 2252: ep_len:702 episode reward: total was 17.390000. running mean: -15.337986\n",
      "ep 2252: ep_len:3557 episode reward: total was -69.750000. running mean: -15.882106\n",
      "ep 2252: ep_len:2802 episode reward: total was -1223.540000. running mean: -27.958685\n",
      "ep 2252: ep_len:7425 episode reward: total was -84.330000. running mean: -28.522398\n",
      "ep 2252: ep_len:669 episode reward: total was 10.980000. running mean: -28.127374\n",
      "ep 2252: ep_len:50 episode reward: total was 23.500000. running mean: -27.611100\n",
      "ep 2252: ep_len:640 episode reward: total was 2.740000. running mean: -27.307589\n",
      "ep 2252: ep_len:2869 episode reward: total was -56.560000. running mean: -27.600113\n",
      "epsilon:0.009992 episode_count: 33922. steps_count: 36428684.000000\n",
      "ep 2253: ep_len:688 episode reward: total was -9.350000. running mean: -27.417612\n",
      "ep 2253: ep_len:729 episode reward: total was -66.610000. running mean: -27.809536\n",
      "ep 2253: ep_len:78 episode reward: total was 36.000000. running mean: -27.171441\n",
      "ep 2253: ep_len:2976 episode reward: total was -98.730000. running mean: -27.887026\n",
      "ep 2253: ep_len:1497 episode reward: total was 11.770000. running mean: -27.490456\n",
      "ep 2253: ep_len:33 episode reward: total was 15.000000. running mean: -27.065551\n",
      "ep 2253: ep_len:112 episode reward: total was 54.500000. running mean: -26.249896\n",
      "ep 2253: ep_len:56 episode reward: total was 26.500000. running mean: -25.722397\n",
      "ep 2253: ep_len:500 episode reward: total was 8.940000. running mean: -25.375773\n",
      "ep 2253: ep_len:3682 episode reward: total was -4.990000. running mean: -25.171915\n",
      "ep 2253: ep_len:3033 episode reward: total was -356.670000. running mean: -28.486896\n",
      "ep 2253: ep_len:753 episode reward: total was 19.310000. running mean: -28.008927\n",
      "ep 2253: ep_len:999 episode reward: total was -38.930000. running mean: -28.118138\n",
      "ep 2253: ep_len:93 episode reward: total was 43.500000. running mean: -27.401957\n",
      "ep 2253: ep_len:40 episode reward: total was 18.500000. running mean: -26.942937\n",
      "ep 2253: ep_len:98 episode reward: total was 47.500000. running mean: -26.198508\n",
      "ep 2253: ep_len:2427 episode reward: total was -221.330000. running mean: -28.149823\n",
      "ep 2253: ep_len:2969 episode reward: total was 9.880000. running mean: -27.769524\n",
      "epsilon:0.009992 episode_count: 33940. steps_count: 36449447.000000\n",
      "ep 2254: ep_len:626 episode reward: total was 30.580000. running mean: -27.186029\n",
      "ep 2254: ep_len:1238 episode reward: total was -54.450000. running mean: -27.458669\n",
      "ep 2254: ep_len:2995 episode reward: total was -9.500000. running mean: -27.279082\n",
      "ep 2254: ep_len:500 episode reward: total was -21.700000. running mean: -27.223291\n",
      "ep 2254: ep_len:47 episode reward: total was 22.000000. running mean: -26.731058\n",
      "ep 2254: ep_len:81 episode reward: total was 37.500000. running mean: -26.088748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2254: ep_len:830 episode reward: total was 28.370000. running mean: -25.544160\n",
      "ep 2254: ep_len:3669 episode reward: total was -86.480000. running mean: -26.153519\n",
      "ep 2254: ep_len:2063 episode reward: total was -66.400000. running mean: -26.555983\n",
      "ep 2254: ep_len:681 episode reward: total was 8.730000. running mean: -26.203124\n",
      "ep 2254: ep_len:626 episode reward: total was 56.870000. running mean: -25.372392\n",
      "ep 2254: ep_len:66 episode reward: total was 31.500000. running mean: -24.803668\n",
      "ep 2254: ep_len:161 episode reward: total was 79.000000. running mean: -23.765632\n",
      "ep 2254: ep_len:953 episode reward: total was -60.600000. running mean: -24.133975\n",
      "ep 2254: ep_len:2881 episode reward: total was 2.510000. running mean: -23.867536\n",
      "epsilon:0.009992 episode_count: 33955. steps_count: 36466864.000000\n",
      "ep 2255: ep_len:1103 episode reward: total was -11.360000. running mean: -23.742460\n",
      "ep 2255: ep_len:640 episode reward: total was -18.650000. running mean: -23.691536\n",
      "ep 2255: ep_len:2956 episode reward: total was -62.390000. running mean: -24.078520\n",
      "ep 2255: ep_len:595 episode reward: total was -12.400000. running mean: -23.961735\n",
      "ep 2255: ep_len:52 episode reward: total was 24.500000. running mean: -23.477118\n",
      "ep 2255: ep_len:1422 episode reward: total was -258.030000. running mean: -25.822647\n",
      "ep 2255: ep_len:329 episode reward: total was 6.870000. running mean: -25.495720\n",
      "ep 2255: ep_len:529 episode reward: total was -118.100000. running mean: -26.421763\n",
      "ep 2255: ep_len:672 episode reward: total was 47.070000. running mean: -25.686845\n",
      "ep 2255: ep_len:743 episode reward: total was -8.900000. running mean: -25.518977\n",
      "ep 2255: ep_len:88 episode reward: total was 42.500000. running mean: -24.838787\n",
      "ep 2255: ep_len:93 episode reward: total was 45.000000. running mean: -24.140399\n",
      "ep 2255: ep_len:649 episode reward: total was 10.630000. running mean: -23.792695\n",
      "ep 2255: ep_len:2823 episode reward: total was 1.650000. running mean: -23.538268\n",
      "ep 2255: ep_len:42 episode reward: total was 19.500000. running mean: -23.107886\n",
      "epsilon:0.009992 episode_count: 33970. steps_count: 36479600.000000\n",
      "ep 2256: ep_len:689 episode reward: total was -44.670000. running mean: -23.323507\n",
      "ep 2256: ep_len:203 episode reward: total was -3.640000. running mean: -23.126672\n",
      "ep 2256: ep_len:41 episode reward: total was 19.000000. running mean: -22.705405\n",
      "ep 2256: ep_len:2961 episode reward: total was -78.190000. running mean: -23.260251\n",
      "ep 2256: ep_len:2842 episode reward: total was -194.960000. running mean: -24.977248\n",
      "ep 2256: ep_len:51 episode reward: total was 22.500000. running mean: -24.502476\n",
      "ep 2256: ep_len:774 episode reward: total was -13.640000. running mean: -24.393851\n",
      "ep 2256: ep_len:659 episode reward: total was -1.590000. running mean: -24.165813\n",
      "ep 2256: ep_len:1271 episode reward: total was -56.080000. running mean: -24.484955\n",
      "ep 2256: ep_len:701 episode reward: total was 35.600000. running mean: -23.884105\n",
      "ep 2256: ep_len:1464 episode reward: total was -655.860000. running mean: -30.203864\n",
      "ep 2256: ep_len:68 episode reward: total was 31.000000. running mean: -29.591825\n",
      "ep 2256: ep_len:177 episode reward: total was 87.000000. running mean: -28.425907\n",
      "ep 2256: ep_len:64 episode reward: total was 30.500000. running mean: -27.836648\n",
      "ep 2256: ep_len:1448 episode reward: total was 6.660000. running mean: -27.491682\n",
      "ep 2256: ep_len:2846 episode reward: total was -628.590000. running mean: -33.502665\n",
      "ep 2256: ep_len:63 episode reward: total was 27.000000. running mean: -32.897638\n",
      "epsilon:0.009992 episode_count: 33987. steps_count: 36495922.000000\n",
      "ep 2257: ep_len:1127 episode reward: total was 2.200000. running mean: -32.546662\n",
      "ep 2257: ep_len:961 episode reward: total was -4.610000. running mean: -32.267295\n",
      "ep 2257: ep_len:52 episode reward: total was 24.500000. running mean: -31.699622\n",
      "ep 2257: ep_len:2883 episode reward: total was -41.120000. running mean: -31.793826\n",
      "ep 2257: ep_len:666 episode reward: total was 3.210000. running mean: -31.443788\n",
      "ep 2257: ep_len:606 episode reward: total was -2.370000. running mean: -31.153050\n",
      "ep 2257: ep_len:655 episode reward: total was 6.660000. running mean: -30.774919\n",
      "ep 2257: ep_len:616 episode reward: total was 7.000000. running mean: -30.397170\n",
      "ep 2257: ep_len:840 episode reward: total was 44.250000. running mean: -29.650698\n",
      "ep 2257: ep_len:500 episode reward: total was -19.770000. running mean: -29.551891\n",
      "ep 2257: ep_len:645 episode reward: total was -7.860000. running mean: -29.334972\n",
      "ep 2257: ep_len:2900 episode reward: total was -37.490000. running mean: -29.416523\n",
      "epsilon:0.009992 episode_count: 33999. steps_count: 36508373.000000\n",
      "ep 2258: ep_len:690 episode reward: total was 14.760000. running mean: -28.974758\n",
      "ep 2258: ep_len:1644 episode reward: total was -71.210000. running mean: -29.397110\n",
      "ep 2258: ep_len:3149 episode reward: total was -56.710000. running mean: -29.670239\n",
      "ep 2258: ep_len:523 episode reward: total was -24.230000. running mean: -29.615836\n",
      "ep 2258: ep_len:67 episode reward: total was 32.000000. running mean: -28.999678\n",
      "ep 2258: ep_len:79 episode reward: total was 36.500000. running mean: -28.344681\n",
      "ep 2258: ep_len:1450 episode reward: total was 3.890000. running mean: -28.022334\n",
      "ep 2258: ep_len:3527 episode reward: total was -104.910000. running mean: -28.791211\n",
      "ep 2258: ep_len:906 episode reward: total was -99.670000. running mean: -29.499999\n",
      "ep 2258: ep_len:7224 episode reward: total was -286.010000. running mean: -32.065099\n",
      "ep 2258: ep_len:1013 episode reward: total was -10.510000. running mean: -31.849548\n",
      "ep 2258: ep_len:129 episode reward: total was 61.500000. running mean: -30.916053\n",
      "ep 2258: ep_len:1472 episode reward: total was 5.640000. running mean: -30.550492\n",
      "ep 2258: ep_len:2819 episode reward: total was -3.130000. running mean: -30.276287\n",
      "ep 2258: ep_len:60 episode reward: total was 27.000000. running mean: -29.703524\n",
      "epsilon:0.009992 episode_count: 34014. steps_count: 36533125.000000\n",
      "ep 2259: ep_len:863 episode reward: total was -30.870000. running mean: -29.715189\n",
      "ep 2259: ep_len:1248 episode reward: total was -53.340000. running mean: -29.951437\n",
      "ep 2259: ep_len:2950 episode reward: total was -183.380000. running mean: -31.485723\n",
      "ep 2259: ep_len:1669 episode reward: total was -52.280000. running mean: -31.693666\n",
      "ep 2259: ep_len:66 episode reward: total was 30.000000. running mean: -31.076729\n",
      "ep 2259: ep_len:1409 episode reward: total was -238.060000. running mean: -33.146562\n",
      "ep 2259: ep_len:648 episode reward: total was 23.000000. running mean: -32.585096\n",
      "ep 2259: ep_len:712 episode reward: total was -7.880000. running mean: -32.338045\n",
      "ep 2259: ep_len:630 episode reward: total was -16.090000. running mean: -32.175565\n",
      "ep 2259: ep_len:513 episode reward: total was 14.430000. running mean: -31.709509\n",
      "ep 2259: ep_len:169 episode reward: total was 81.500000. running mean: -30.577414\n",
      "ep 2259: ep_len:63 episode reward: total was 30.000000. running mean: -29.971640\n",
      "ep 2259: ep_len:1036 episode reward: total was 4.080000. running mean: -29.631123\n",
      "ep 2259: ep_len:2880 episode reward: total was 1.940000. running mean: -29.315412\n",
      "ep 2259: ep_len:67 episode reward: total was 30.500000. running mean: -28.717258\n",
      "epsilon:0.009992 episode_count: 34029. steps_count: 36548048.000000\n",
      "ep 2260: ep_len:886 episode reward: total was 12.330000. running mean: -28.306785\n",
      "ep 2260: ep_len:663 episode reward: total was -61.210000. running mean: -28.635818\n",
      "ep 2260: ep_len:59 episode reward: total was 26.500000. running mean: -28.084459\n",
      "ep 2260: ep_len:2822 episode reward: total was -93.420000. running mean: -28.737815\n",
      "ep 2260: ep_len:521 episode reward: total was -23.240000. running mean: -28.682837\n",
      "ep 2260: ep_len:38 episode reward: total was 17.500000. running mean: -28.221008\n",
      "ep 2260: ep_len:110 episode reward: total was 50.500000. running mean: -27.433798\n",
      "ep 2260: ep_len:45 episode reward: total was 21.000000. running mean: -26.949460\n",
      "ep 2260: ep_len:1429 episode reward: total was -13.770000. running mean: -26.817666\n",
      "ep 2260: ep_len:3701 episode reward: total was -157.910000. running mean: -28.128589\n",
      "ep 2260: ep_len:1191 episode reward: total was -33.710000. running mean: -28.184403\n",
      "ep 2260: ep_len:814 episode reward: total was 40.900000. running mean: -27.493559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2260: ep_len:1173 episode reward: total was -8.640000. running mean: -27.305023\n",
      "ep 2260: ep_len:500 episode reward: total was 17.360000. running mean: -26.858373\n",
      "ep 2260: ep_len:2739 episode reward: total was -5.740000. running mean: -26.647189\n",
      "epsilon:0.009992 episode_count: 34044. steps_count: 36564739.000000\n",
      "ep 2261: ep_len:1431 episode reward: total was 28.220000. running mean: -26.098518\n",
      "ep 2261: ep_len:687 episode reward: total was -42.790000. running mean: -26.265432\n",
      "ep 2261: ep_len:2969 episode reward: total was -96.530000. running mean: -26.968078\n",
      "ep 2261: ep_len:716 episode reward: total was 17.900000. running mean: -26.519397\n",
      "ep 2261: ep_len:1450 episode reward: total was -229.720000. running mean: -28.551403\n",
      "ep 2261: ep_len:3976 episode reward: total was -30.120000. running mean: -28.567089\n",
      "ep 2261: ep_len:883 episode reward: total was -9.600000. running mean: -28.377418\n",
      "ep 2261: ep_len:608 episode reward: total was 13.600000. running mean: -27.957644\n",
      "ep 2261: ep_len:751 episode reward: total was -14.880000. running mean: -27.826868\n",
      "ep 2261: ep_len:1161 episode reward: total was -16.650000. running mean: -27.715099\n",
      "ep 2261: ep_len:2904 episode reward: total was 9.350000. running mean: -27.344448\n",
      "ep 2261: ep_len:43 episode reward: total was 20.000000. running mean: -26.871004\n",
      "epsilon:0.009992 episode_count: 34056. steps_count: 36582318.000000\n",
      "ep 2262: ep_len:1469 episode reward: total was 2.340000. running mean: -26.578894\n",
      "ep 2262: ep_len:729 episode reward: total was -15.060000. running mean: -26.463705\n",
      "ep 2262: ep_len:2891 episode reward: total was -86.340000. running mean: -27.062468\n",
      "ep 2262: ep_len:860 episode reward: total was 9.490000. running mean: -26.696943\n",
      "ep 2262: ep_len:103 episode reward: total was 48.500000. running mean: -25.944973\n",
      "ep 2262: ep_len:41 episode reward: total was 19.000000. running mean: -25.495524\n",
      "ep 2262: ep_len:874 episode reward: total was 44.570000. running mean: -24.794868\n",
      "ep 2262: ep_len:3830 episode reward: total was -325.250000. running mean: -27.799420\n",
      "ep 2262: ep_len:558 episode reward: total was -6.000000. running mean: -27.581426\n",
      "ep 2262: ep_len:750 episode reward: total was 10.670000. running mean: -27.198911\n",
      "ep 2262: ep_len:640 episode reward: total was -8.390000. running mean: -27.010822\n",
      "ep 2262: ep_len:84 episode reward: total was 37.500000. running mean: -26.365714\n",
      "ep 2262: ep_len:43 episode reward: total was 20.000000. running mean: -25.902057\n",
      "ep 2262: ep_len:110 episode reward: total was 52.000000. running mean: -25.123036\n",
      "ep 2262: ep_len:871 episode reward: total was -11.880000. running mean: -24.990606\n",
      "ep 2262: ep_len:2878 episode reward: total was -14.820000. running mean: -24.888900\n",
      "epsilon:0.009992 episode_count: 34072. steps_count: 36599049.000000\n",
      "ep 2263: ep_len:1065 episode reward: total was -0.630000. running mean: -24.646311\n",
      "ep 2263: ep_len:3608 episode reward: total was -1767.850000. running mean: -42.078348\n",
      "ep 2263: ep_len:2990 episode reward: total was -29.170000. running mean: -41.949264\n",
      "ep 2263: ep_len:502 episode reward: total was -0.200000. running mean: -41.531772\n",
      "ep 2263: ep_len:1104 episode reward: total was -16.400000. running mean: -41.280454\n",
      "ep 2263: ep_len:602 episode reward: total was 23.030000. running mean: -40.637349\n",
      "ep 2263: ep_len:609 episode reward: total was -34.480000. running mean: -40.575776\n",
      "ep 2263: ep_len:688 episode reward: total was 46.070000. running mean: -39.709318\n",
      "ep 2263: ep_len:1525 episode reward: total was 12.110000. running mean: -39.191125\n",
      "ep 2263: ep_len:64 episode reward: total was 30.500000. running mean: -38.494214\n",
      "ep 2263: ep_len:56 episode reward: total was 26.500000. running mean: -37.844272\n",
      "ep 2263: ep_len:86 episode reward: total was 41.500000. running mean: -37.050829\n",
      "ep 2263: ep_len:988 episode reward: total was -116.820000. running mean: -37.848521\n",
      "ep 2263: ep_len:2778 episode reward: total was 0.590000. running mean: -37.464135\n",
      "epsilon:0.009992 episode_count: 34086. steps_count: 36615714.000000\n",
      "ep 2264: ep_len:1047 episode reward: total was -10.850000. running mean: -37.197994\n",
      "ep 2264: ep_len:733 episode reward: total was -38.150000. running mean: -37.207514\n",
      "ep 2264: ep_len:2904 episode reward: total was -59.730000. running mean: -37.432739\n",
      "ep 2264: ep_len:628 episode reward: total was 1.060000. running mean: -37.047812\n",
      "ep 2264: ep_len:138 episode reward: total was 66.000000. running mean: -36.017333\n",
      "ep 2264: ep_len:1353 episode reward: total was -214.010000. running mean: -37.797260\n",
      "ep 2264: ep_len:3787 episode reward: total was -63.910000. running mean: -38.058387\n",
      "ep 2264: ep_len:541 episode reward: total was -7.890000. running mean: -37.756704\n",
      "ep 2264: ep_len:621 episode reward: total was 27.970000. running mean: -37.099437\n",
      "ep 2264: ep_len:1433 episode reward: total was 9.320000. running mean: -36.635242\n",
      "ep 2264: ep_len:1122 episode reward: total was 8.450000. running mean: -36.184390\n",
      "ep 2264: ep_len:2815 episode reward: total was -4.430000. running mean: -35.866846\n",
      "epsilon:0.009992 episode_count: 34098. steps_count: 36632836.000000\n",
      "ep 2265: ep_len:598 episode reward: total was -34.530000. running mean: -35.853477\n",
      "ep 2265: ep_len:741 episode reward: total was -31.830000. running mean: -35.813243\n",
      "ep 2265: ep_len:74 episode reward: total was 35.500000. running mean: -35.100110\n",
      "ep 2265: ep_len:3073 episode reward: total was -57.940000. running mean: -35.328509\n",
      "ep 2265: ep_len:668 episode reward: total was -5.610000. running mean: -35.031324\n",
      "ep 2265: ep_len:42 episode reward: total was 16.500000. running mean: -34.516011\n",
      "ep 2265: ep_len:138 episode reward: total was 64.500000. running mean: -33.525851\n",
      "ep 2265: ep_len:96 episode reward: total was 45.000000. running mean: -32.740592\n",
      "ep 2265: ep_len:74 episode reward: total was 35.500000. running mean: -32.058186\n",
      "ep 2265: ep_len:671 episode reward: total was -12.200000. running mean: -31.859604\n",
      "ep 2265: ep_len:665 episode reward: total was 17.380000. running mean: -31.367208\n",
      "ep 2265: ep_len:795 episode reward: total was -35.930000. running mean: -31.412836\n",
      "ep 2265: ep_len:818 episode reward: total was 51.930000. running mean: -30.579408\n",
      "ep 2265: ep_len:1054 episode reward: total was 0.610000. running mean: -30.267514\n",
      "ep 2265: ep_len:222 episode reward: total was 3.500000. running mean: -29.929839\n",
      "ep 2265: ep_len:803 episode reward: total was -23.100000. running mean: -29.861540\n",
      "ep 2265: ep_len:2816 episode reward: total was 9.720000. running mean: -29.465725\n",
      "ep 2265: ep_len:62 episode reward: total was 29.500000. running mean: -28.876068\n",
      "epsilon:0.009992 episode_count: 34116. steps_count: 36646246.000000\n",
      "ep 2266: ep_len:1142 episode reward: total was 1.150000. running mean: -28.575807\n",
      "ep 2266: ep_len:718 episode reward: total was -17.340000. running mean: -28.463449\n",
      "ep 2266: ep_len:2919 episode reward: total was -31.910000. running mean: -28.497914\n",
      "ep 2266: ep_len:767 episode reward: total was -28.280000. running mean: -28.495735\n",
      "ep 2266: ep_len:75 episode reward: total was 34.500000. running mean: -27.865778\n",
      "ep 2266: ep_len:500 episode reward: total was -8.780000. running mean: -27.674920\n",
      "ep 2266: ep_len:349 episode reward: total was 4.590000. running mean: -27.352271\n",
      "ep 2266: ep_len:834 episode reward: total was -43.750000. running mean: -27.516248\n",
      "ep 2266: ep_len:7250 episode reward: total was 21.060000. running mean: -27.030486\n",
      "ep 2266: ep_len:500 episode reward: total was 2.730000. running mean: -26.732881\n",
      "ep 2266: ep_len:216 episode reward: total was 81.000000. running mean: -25.655552\n",
      "ep 2266: ep_len:63 episode reward: total was 30.000000. running mean: -25.098997\n",
      "ep 2266: ep_len:99 episode reward: total was 46.500000. running mean: -24.383007\n",
      "ep 2266: ep_len:1454 episode reward: total was -6.750000. running mean: -24.206677\n",
      "ep 2266: ep_len:2875 episode reward: total was -36.520000. running mean: -24.329810\n",
      "ep 2266: ep_len:48 episode reward: total was 22.500000. running mean: -23.861512\n",
      "epsilon:0.009992 episode_count: 34132. steps_count: 36666055.000000\n",
      "ep 2267: ep_len:644 episode reward: total was 2.230000. running mean: -23.600597\n",
      "ep 2267: ep_len:752 episode reward: total was -38.200000. running mean: -23.746591\n",
      "ep 2267: ep_len:2907 episode reward: total was -34.970000. running mean: -23.858825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2267: ep_len:816 episode reward: total was 17.660000. running mean: -23.443636\n",
      "ep 2267: ep_len:46 episode reward: total was 21.500000. running mean: -22.994200\n",
      "ep 2267: ep_len:663 episode reward: total was 7.470000. running mean: -22.689558\n",
      "ep 2267: ep_len:667 episode reward: total was 10.060000. running mean: -22.362062\n",
      "ep 2267: ep_len:1256 episode reward: total was -93.660000. running mean: -23.075042\n",
      "ep 2267: ep_len:7357 episode reward: total was 53.930000. running mean: -22.304991\n",
      "ep 2267: ep_len:1062 episode reward: total was 4.150000. running mean: -22.040442\n",
      "ep 2267: ep_len:99 episode reward: total was 48.000000. running mean: -21.340037\n",
      "ep 2267: ep_len:1095 episode reward: total was -6.820000. running mean: -21.194837\n",
      "ep 2267: ep_len:2792 episode reward: total was -0.500000. running mean: -20.987888\n",
      "epsilon:0.009992 episode_count: 34145. steps_count: 36686211.000000\n",
      "ep 2268: ep_len:1131 episode reward: total was -11.840000. running mean: -20.896409\n",
      "ep 2268: ep_len:1154 episode reward: total was -46.200000. running mean: -21.149445\n",
      "ep 2268: ep_len:2827 episode reward: total was -7.490000. running mean: -21.012851\n",
      "ep 2268: ep_len:658 episode reward: total was 3.170000. running mean: -20.771022\n",
      "ep 2268: ep_len:46 episode reward: total was 21.500000. running mean: -20.348312\n",
      "ep 2268: ep_len:610 episode reward: total was -35.030000. running mean: -20.495129\n",
      "ep 2268: ep_len:351 episode reward: total was 12.080000. running mean: -20.169378\n",
      "ep 2268: ep_len:581 episode reward: total was -0.900000. running mean: -19.976684\n",
      "ep 2268: ep_len:840 episode reward: total was 64.460000. running mean: -19.132317\n",
      "ep 2268: ep_len:854 episode reward: total was 18.960000. running mean: -18.751394\n",
      "ep 2268: ep_len:51 episode reward: total was 24.000000. running mean: -18.323880\n",
      "ep 2268: ep_len:116 episode reward: total was 56.500000. running mean: -17.575641\n",
      "ep 2268: ep_len:978 episode reward: total was -93.200000. running mean: -18.331885\n",
      "ep 2268: ep_len:2748 episode reward: total was -13.790000. running mean: -18.286466\n",
      "epsilon:0.009992 episode_count: 34159. steps_count: 36699156.000000\n",
      "ep 2269: ep_len:993 episode reward: total was -143.240000. running mean: -19.536001\n",
      "ep 2269: ep_len:500 episode reward: total was 31.780000. running mean: -19.022841\n",
      "ep 2269: ep_len:43 episode reward: total was 20.000000. running mean: -18.632613\n",
      "ep 2269: ep_len:3116 episode reward: total was -25.610000. running mean: -18.702387\n",
      "ep 2269: ep_len:538 episode reward: total was -18.020000. running mean: -18.695563\n",
      "ep 2269: ep_len:55 episode reward: total was 26.000000. running mean: -18.248607\n",
      "ep 2269: ep_len:60 episode reward: total was 28.500000. running mean: -17.781121\n",
      "ep 2269: ep_len:701 episode reward: total was 8.340000. running mean: -17.519910\n",
      "ep 2269: ep_len:352 episode reward: total was -2.450000. running mean: -17.369211\n",
      "ep 2269: ep_len:1148 episode reward: total was -31.280000. running mean: -17.508319\n",
      "ep 2269: ep_len:7457 episode reward: total was 22.770000. running mean: -17.105536\n",
      "ep 2269: ep_len:646 episode reward: total was 0.230000. running mean: -16.932180\n",
      "ep 2269: ep_len:115 episode reward: total was 53.000000. running mean: -16.232858\n",
      "ep 2269: ep_len:1090 episode reward: total was -10.480000. running mean: -16.175330\n",
      "ep 2269: ep_len:2844 episode reward: total was -10.410000. running mean: -16.117677\n",
      "ep 2269: ep_len:25 episode reward: total was 11.000000. running mean: -15.846500\n",
      "epsilon:0.009992 episode_count: 34175. steps_count: 36718839.000000\n",
      "ep 2270: ep_len:1102 episode reward: total was -18.440000. running mean: -15.872435\n",
      "ep 2270: ep_len:759 episode reward: total was -29.340000. running mean: -16.007110\n",
      "ep 2270: ep_len:3057 episode reward: total was 7.590000. running mean: -15.771139\n",
      "ep 2270: ep_len:512 episode reward: total was -16.260000. running mean: -15.776028\n",
      "ep 2270: ep_len:1505 episode reward: total was -26.070000. running mean: -15.878968\n",
      "ep 2270: ep_len:650 episode reward: total was -9.610000. running mean: -15.816278\n",
      "ep 2270: ep_len:1264 episode reward: total was -82.960000. running mean: -16.487715\n",
      "ep 2270: ep_len:633 episode reward: total was -4.340000. running mean: -16.366238\n",
      "ep 2270: ep_len:936 episode reward: total was -2.680000. running mean: -16.229376\n",
      "ep 2270: ep_len:70 episode reward: total was 32.000000. running mean: -15.747082\n",
      "ep 2270: ep_len:164 episode reward: total was 80.500000. running mean: -14.784611\n",
      "ep 2270: ep_len:771 episode reward: total was -118.710000. running mean: -15.823865\n",
      "ep 2270: ep_len:2863 episode reward: total was -8.880000. running mean: -15.754426\n",
      "ep 2270: ep_len:55 episode reward: total was -52.490000. running mean: -16.121782\n",
      "epsilon:0.009992 episode_count: 34189. steps_count: 36733180.000000\n",
      "ep 2271: ep_len:500 episode reward: total was 22.350000. running mean: -15.737064\n",
      "ep 2271: ep_len:752 episode reward: total was -27.390000. running mean: -15.853594\n",
      "ep 2271: ep_len:2993 episode reward: total was -2.330000. running mean: -15.718358\n",
      "ep 2271: ep_len:639 episode reward: total was -7.500000. running mean: -15.636174\n",
      "ep 2271: ep_len:60 episode reward: total was 28.500000. running mean: -15.194812\n",
      "ep 2271: ep_len:63 episode reward: total was 28.500000. running mean: -14.757864\n",
      "ep 2271: ep_len:615 episode reward: total was 2.060000. running mean: -14.589686\n",
      "ep 2271: ep_len:3784 episode reward: total was -391.000000. running mean: -18.353789\n",
      "ep 2271: ep_len:563 episode reward: total was -53.610000. running mean: -18.706351\n",
      "ep 2271: ep_len:7368 episode reward: total was 22.520000. running mean: -18.294087\n",
      "ep 2271: ep_len:900 episode reward: total was 13.150000. running mean: -17.979646\n",
      "ep 2271: ep_len:125 episode reward: total was 56.500000. running mean: -17.234850\n",
      "ep 2271: ep_len:815 episode reward: total was 31.570000. running mean: -16.746802\n",
      "ep 2271: ep_len:2785 episode reward: total was 1.640000. running mean: -16.562933\n",
      "ep 2271: ep_len:42 episode reward: total was 19.500000. running mean: -16.202304\n",
      "epsilon:0.009992 episode_count: 34204. steps_count: 36755184.000000\n",
      "ep 2272: ep_len:1425 episode reward: total was -22.310000. running mean: -16.263381\n",
      "ep 2272: ep_len:195 episode reward: total was 14.280000. running mean: -15.957947\n",
      "ep 2272: ep_len:2993 episode reward: total was -57.590000. running mean: -16.374268\n",
      "ep 2272: ep_len:682 episode reward: total was 18.590000. running mean: -16.024625\n",
      "ep 2272: ep_len:66 episode reward: total was 31.500000. running mean: -15.549379\n",
      "ep 2272: ep_len:106 episode reward: total was 51.500000. running mean: -14.878885\n",
      "ep 2272: ep_len:55 episode reward: total was 24.500000. running mean: -14.485096\n",
      "ep 2272: ep_len:599 episode reward: total was 45.100000. running mean: -13.889245\n",
      "ep 2272: ep_len:343 episode reward: total was -0.090000. running mean: -13.751253\n",
      "ep 2272: ep_len:541 episode reward: total was -11.960000. running mean: -13.733340\n",
      "ep 2272: ep_len:741 episode reward: total was 45.190000. running mean: -13.144107\n",
      "ep 2272: ep_len:1510 episode reward: total was -30.970000. running mean: -13.322366\n",
      "ep 2272: ep_len:1130 episode reward: total was -13.160000. running mean: -13.320742\n",
      "ep 2272: ep_len:2824 episode reward: total was -32.320000. running mean: -13.510735\n",
      "ep 2272: ep_len:75 episode reward: total was 36.000000. running mean: -13.015627\n",
      "epsilon:0.009992 episode_count: 34219. steps_count: 36768469.000000\n",
      "ep 2273: ep_len:500 episode reward: total was 14.180000. running mean: -12.743671\n",
      "ep 2273: ep_len:671 episode reward: total was -36.210000. running mean: -12.978334\n",
      "ep 2273: ep_len:3006 episode reward: total was -15.000000. running mean: -12.998551\n",
      "ep 2273: ep_len:627 episode reward: total was -51.960000. running mean: -13.388166\n",
      "ep 2273: ep_len:66 episode reward: total was 31.500000. running mean: -12.939284\n",
      "ep 2273: ep_len:153 episode reward: total was 75.000000. running mean: -12.059891\n",
      "ep 2273: ep_len:50 episode reward: total was 22.000000. running mean: -11.719292\n",
      "ep 2273: ep_len:713 episode reward: total was -9.200000. running mean: -11.694099\n",
      "ep 2273: ep_len:669 episode reward: total was -0.330000. running mean: -11.580458\n",
      "ep 2273: ep_len:1259 episode reward: total was -74.440000. running mean: -12.209054\n",
      "ep 2273: ep_len:867 episode reward: total was 52.360000. running mean: -11.563363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2273: ep_len:1094 episode reward: total was -2.050000. running mean: -11.468230\n",
      "ep 2273: ep_len:110 episode reward: total was 50.500000. running mean: -10.848547\n",
      "ep 2273: ep_len:76 episode reward: total was 35.000000. running mean: -10.390062\n",
      "ep 2273: ep_len:903 episode reward: total was 19.200000. running mean: -10.094161\n",
      "ep 2273: ep_len:2857 episode reward: total was -16.530000. running mean: -10.158520\n",
      "epsilon:0.009992 episode_count: 34235. steps_count: 36782090.000000\n",
      "ep 2274: ep_len:989 episode reward: total was -99.360000. running mean: -11.050534\n",
      "ep 2274: ep_len:3168 episode reward: total was -343.280000. running mean: -14.372829\n",
      "ep 2274: ep_len:2931 episode reward: total was -87.190000. running mean: -15.101001\n",
      "ep 2274: ep_len:1235 episode reward: total was -21.570000. running mean: -15.165691\n",
      "ep 2274: ep_len:94 episode reward: total was 45.500000. running mean: -14.559034\n",
      "ep 2274: ep_len:1120 episode reward: total was 14.380000. running mean: -14.269643\n",
      "ep 2274: ep_len:332 episode reward: total was 15.950000. running mean: -13.967447\n",
      "ep 2274: ep_len:861 episode reward: total was 19.610000. running mean: -13.631673\n",
      "ep 2274: ep_len:743 episode reward: total was 58.620000. running mean: -12.909156\n",
      "ep 2274: ep_len:670 episode reward: total was 18.550000. running mean: -12.594564\n",
      "ep 2274: ep_len:43 episode reward: total was 20.000000. running mean: -12.268619\n",
      "ep 2274: ep_len:173 episode reward: total was 83.500000. running mean: -11.310932\n",
      "ep 2274: ep_len:992 episode reward: total was -103.620000. running mean: -12.234023\n",
      "ep 2274: ep_len:2785 episode reward: total was 3.560000. running mean: -12.076083\n",
      "epsilon:0.009992 episode_count: 34249. steps_count: 36798226.000000\n",
      "ep 2275: ep_len:1057 episode reward: total was -8.790000. running mean: -12.043222\n",
      "ep 2275: ep_len:668 episode reward: total was -37.640000. running mean: -12.299190\n",
      "ep 2275: ep_len:2950 episode reward: total was 6.230000. running mean: -12.113898\n",
      "ep 2275: ep_len:798 episode reward: total was 32.570000. running mean: -11.667059\n",
      "ep 2275: ep_len:641 episode reward: total was -5.900000. running mean: -11.609388\n",
      "ep 2275: ep_len:344 episode reward: total was -8.990000. running mean: -11.583194\n",
      "ep 2275: ep_len:617 episode reward: total was 14.630000. running mean: -11.321063\n",
      "ep 2275: ep_len:663 episode reward: total was 12.190000. running mean: -11.085952\n",
      "ep 2275: ep_len:710 episode reward: total was -8.810000. running mean: -11.063192\n",
      "ep 2275: ep_len:140 episode reward: total was 65.500000. running mean: -10.297560\n",
      "ep 2275: ep_len:665 episode reward: total was -8.610000. running mean: -10.280685\n",
      "ep 2275: ep_len:2866 episode reward: total was -27.730000. running mean: -10.455178\n",
      "epsilon:0.009992 episode_count: 34261. steps_count: 36810345.000000\n",
      "ep 2276: ep_len:627 episode reward: total was -82.780000. running mean: -11.178426\n",
      "ep 2276: ep_len:705 episode reward: total was -56.750000. running mean: -11.634142\n",
      "ep 2276: ep_len:49 episode reward: total was 23.000000. running mean: -11.287801\n",
      "ep 2276: ep_len:2981 episode reward: total was -4.510000. running mean: -11.220023\n",
      "ep 2276: ep_len:622 episode reward: total was 4.030000. running mean: -11.067522\n",
      "ep 2276: ep_len:41 episode reward: total was 19.000000. running mean: -10.766847\n",
      "ep 2276: ep_len:500 episode reward: total was 1.440000. running mean: -10.644779\n",
      "ep 2276: ep_len:603 episode reward: total was 23.040000. running mean: -10.307931\n",
      "ep 2276: ep_len:2038 episode reward: total was -564.430000. running mean: -15.849152\n",
      "ep 2276: ep_len:875 episode reward: total was 67.970000. running mean: -15.010960\n",
      "ep 2276: ep_len:1099 episode reward: total was -25.540000. running mean: -15.116250\n",
      "ep 2276: ep_len:180 episode reward: total was 85.500000. running mean: -14.110088\n",
      "ep 2276: ep_len:64 episode reward: total was 29.000000. running mean: -13.678987\n",
      "ep 2276: ep_len:500 episode reward: total was 19.410000. running mean: -13.348097\n",
      "ep 2276: ep_len:2847 episode reward: total was -9.680000. running mean: -13.311416\n",
      "epsilon:0.009992 episode_count: 34276. steps_count: 36824076.000000\n",
      "ep 2277: ep_len:814 episode reward: total was -7.360000. running mean: -13.251902\n",
      "ep 2277: ep_len:753 episode reward: total was 4.830000. running mean: -13.071083\n",
      "ep 2277: ep_len:2985 episode reward: total was -1.120000. running mean: -12.951572\n",
      "ep 2277: ep_len:1664 episode reward: total was -120.070000. running mean: -14.022756\n",
      "ep 2277: ep_len:90 episode reward: total was 43.500000. running mean: -13.447529\n",
      "ep 2277: ep_len:613 episode reward: total was 27.730000. running mean: -13.035754\n",
      "ep 2277: ep_len:638 episode reward: total was 20.020000. running mean: -12.705196\n",
      "ep 2277: ep_len:608 episode reward: total was -29.780000. running mean: -12.875944\n",
      "ep 2277: ep_len:633 episode reward: total was 9.440000. running mean: -12.652785\n",
      "ep 2277: ep_len:961 episode reward: total was -0.590000. running mean: -12.532157\n",
      "ep 2277: ep_len:90 episode reward: total was 42.000000. running mean: -11.986835\n",
      "ep 2277: ep_len:198 episode reward: total was 94.500000. running mean: -10.921967\n",
      "ep 2277: ep_len:103 episode reward: total was 47.000000. running mean: -10.342747\n",
      "ep 2277: ep_len:1117 episode reward: total was -36.470000. running mean: -10.604020\n",
      "ep 2277: ep_len:2917 episode reward: total was 14.990000. running mean: -10.348080\n",
      "ep 2277: ep_len:70 episode reward: total was 33.500000. running mean: -9.909599\n",
      "epsilon:0.009992 episode_count: 34292. steps_count: 36838330.000000\n",
      "ep 2278: ep_len:735 episode reward: total was -30.180000. running mean: -10.112303\n",
      "ep 2278: ep_len:773 episode reward: total was -1.980000. running mean: -10.030980\n",
      "ep 2278: ep_len:56 episode reward: total was 26.500000. running mean: -9.665670\n",
      "ep 2278: ep_len:3032 episode reward: total was -29.840000. running mean: -9.867413\n",
      "ep 2278: ep_len:1653 episode reward: total was -95.410000. running mean: -10.722839\n",
      "ep 2278: ep_len:78 episode reward: total was 37.500000. running mean: -10.240611\n",
      "ep 2278: ep_len:80 episode reward: total was 38.500000. running mean: -9.753205\n",
      "ep 2278: ep_len:1443 episode reward: total was -460.440000. running mean: -14.260073\n",
      "ep 2278: ep_len:331 episode reward: total was 11.880000. running mean: -13.998672\n",
      "ep 2278: ep_len:1584 episode reward: total was -105.970000. running mean: -14.918385\n",
      "ep 2278: ep_len:640 episode reward: total was 20.380000. running mean: -14.565401\n",
      "ep 2278: ep_len:1051 episode reward: total was 40.160000. running mean: -14.018147\n",
      "ep 2278: ep_len:107 episode reward: total was 50.500000. running mean: -13.372966\n",
      "ep 2278: ep_len:37 episode reward: total was 17.000000. running mean: -13.069236\n",
      "ep 2278: ep_len:83 episode reward: total was 38.500000. running mean: -12.553544\n",
      "ep 2278: ep_len:787 episode reward: total was -46.100000. running mean: -12.889008\n",
      "ep 2278: ep_len:2885 episode reward: total was -0.540000. running mean: -12.765518\n",
      "ep 2278: ep_len:65 episode reward: total was 28.000000. running mean: -12.357863\n",
      "epsilon:0.009992 episode_count: 34310. steps_count: 36853750.000000\n",
      "ep 2279: ep_len:988 episode reward: total was -116.240000. running mean: -13.396684\n",
      "ep 2279: ep_len:1139 episode reward: total was -43.540000. running mean: -13.698118\n",
      "ep 2279: ep_len:3041 episode reward: total was -14.130000. running mean: -13.702436\n",
      "ep 2279: ep_len:639 episode reward: total was -136.190000. running mean: -14.927312\n",
      "ep 2279: ep_len:92 episode reward: total was 44.500000. running mean: -14.333039\n",
      "ep 2279: ep_len:861 episode reward: total was 53.510000. running mean: -13.654609\n",
      "ep 2279: ep_len:500 episode reward: total was 23.010000. running mean: -13.287962\n",
      "ep 2279: ep_len:779 episode reward: total was -30.710000. running mean: -13.462183\n",
      "ep 2279: ep_len:665 episode reward: total was 26.670000. running mean: -13.060861\n",
      "ep 2279: ep_len:572 episode reward: total was -5.560000. running mean: -12.985852\n",
      "ep 2279: ep_len:37 episode reward: total was 17.000000. running mean: -12.685994\n",
      "ep 2279: ep_len:975 episode reward: total was -278.120000. running mean: -15.340334\n",
      "ep 2279: ep_len:2937 episode reward: total was -8.780000. running mean: -15.274731\n",
      "epsilon:0.009992 episode_count: 34323. steps_count: 36866975.000000\n",
      "ep 2280: ep_len:959 episode reward: total was -115.060000. running mean: -16.272583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2280: ep_len:982 episode reward: total was 21.750000. running mean: -15.892357\n",
      "ep 2280: ep_len:2955 episode reward: total was -29.110000. running mean: -16.024534\n",
      "ep 2280: ep_len:755 episode reward: total was -18.270000. running mean: -16.046989\n",
      "ep 2280: ep_len:57 episode reward: total was 27.000000. running mean: -15.616519\n",
      "ep 2280: ep_len:69 episode reward: total was -24.490000. running mean: -15.705253\n",
      "ep 2280: ep_len:500 episode reward: total was 20.030000. running mean: -15.347901\n",
      "ep 2280: ep_len:683 episode reward: total was 10.490000. running mean: -15.089522\n",
      "ep 2280: ep_len:681 episode reward: total was -33.700000. running mean: -15.275627\n",
      "ep 2280: ep_len:798 episode reward: total was 49.010000. running mean: -14.632770\n",
      "ep 2280: ep_len:606 episode reward: total was 8.000000. running mean: -14.406443\n",
      "ep 2280: ep_len:39 episode reward: total was 18.000000. running mean: -14.082378\n",
      "ep 2280: ep_len:69 episode reward: total was 31.500000. running mean: -13.626555\n",
      "ep 2280: ep_len:768 episode reward: total was -42.990000. running mean: -13.920189\n",
      "ep 2280: ep_len:2833 episode reward: total was 5.760000. running mean: -13.723387\n",
      "ep 2280: ep_len:36 episode reward: total was 16.500000. running mean: -13.421153\n",
      "epsilon:0.009992 episode_count: 34339. steps_count: 36879765.000000\n",
      "ep 2281: ep_len:1055 episode reward: total was -16.890000. running mean: -13.455842\n",
      "ep 2281: ep_len:1251 episode reward: total was -67.360000. running mean: -13.994883\n",
      "ep 2281: ep_len:2931 episode reward: total was 15.990000. running mean: -13.695034\n",
      "ep 2281: ep_len:848 episode reward: total was 30.870000. running mean: -13.249384\n",
      "ep 2281: ep_len:90 episode reward: total was 42.000000. running mean: -12.696890\n",
      "ep 2281: ep_len:1144 episode reward: total was -16.000000. running mean: -12.729921\n",
      "ep 2281: ep_len:3636 episode reward: total was -600.740000. running mean: -18.610022\n",
      "ep 2281: ep_len:501 episode reward: total was -1.070000. running mean: -18.434622\n",
      "ep 2281: ep_len:802 episode reward: total was 6.970000. running mean: -18.180576\n",
      "ep 2281: ep_len:1472 episode reward: total was 2.530000. running mean: -17.973470\n",
      "ep 2281: ep_len:129 episode reward: total was 60.000000. running mean: -17.193735\n",
      "ep 2281: ep_len:735 episode reward: total was -15.040000. running mean: -17.172198\n",
      "ep 2281: ep_len:2904 episode reward: total was -23.400000. running mean: -17.234476\n",
      "ep 2281: ep_len:62 episode reward: total was 29.500000. running mean: -16.767131\n",
      "epsilon:0.009992 episode_count: 34353. steps_count: 36897325.000000\n",
      "ep 2282: ep_len:1179 episode reward: total was -17.700000. running mean: -16.776460\n",
      "ep 2282: ep_len:666 episode reward: total was -21.790000. running mean: -16.826595\n",
      "ep 2282: ep_len:2948 episode reward: total was -6.550000. running mean: -16.723829\n",
      "ep 2282: ep_len:1433 episode reward: total was 1.120000. running mean: -16.545391\n",
      "ep 2282: ep_len:68 episode reward: total was 31.000000. running mean: -16.069937\n",
      "ep 2282: ep_len:106 episode reward: total was 51.500000. running mean: -15.394238\n",
      "ep 2282: ep_len:59 episode reward: total was 28.000000. running mean: -14.960295\n",
      "ep 2282: ep_len:500 episode reward: total was 11.820000. running mean: -14.692492\n",
      "ep 2282: ep_len:3636 episode reward: total was -67.890000. running mean: -15.224467\n",
      "ep 2282: ep_len:784 episode reward: total was -64.080000. running mean: -15.713023\n",
      "ep 2282: ep_len:772 episode reward: total was 9.640000. running mean: -15.459493\n",
      "ep 2282: ep_len:665 episode reward: total was -17.370000. running mean: -15.478598\n",
      "ep 2282: ep_len:136 episode reward: total was 63.500000. running mean: -14.688812\n",
      "ep 2282: ep_len:1467 episode reward: total was -5.570000. running mean: -14.597624\n",
      "ep 2282: ep_len:2786 episode reward: total was 0.120000. running mean: -14.450447\n",
      "ep 2282: ep_len:47 episode reward: total was 20.500000. running mean: -14.100943\n",
      "epsilon:0.009992 episode_count: 34369. steps_count: 36914577.000000\n",
      "ep 2283: ep_len:707 episode reward: total was -13.050000. running mean: -14.090433\n",
      "ep 2283: ep_len:766 episode reward: total was -6.650000. running mean: -14.016029\n",
      "ep 2283: ep_len:3050 episode reward: total was -0.470000. running mean: -13.880569\n",
      "ep 2283: ep_len:656 episode reward: total was 9.420000. running mean: -13.647563\n",
      "ep 2283: ep_len:108 episode reward: total was 52.500000. running mean: -12.986087\n",
      "ep 2283: ep_len:70 episode reward: total was 30.500000. running mean: -12.551227\n",
      "ep 2283: ep_len:52 episode reward: total was 23.000000. running mean: -12.195714\n",
      "ep 2283: ep_len:500 episode reward: total was 6.750000. running mean: -12.006257\n",
      "ep 2283: ep_len:668 episode reward: total was 18.150000. running mean: -11.704695\n",
      "ep 2283: ep_len:682 episode reward: total was -26.680000. running mean: -11.854448\n",
      "ep 2283: ep_len:756 episode reward: total was 34.780000. running mean: -11.388103\n",
      "ep 2283: ep_len:1058 episode reward: total was 42.250000. running mean: -10.851722\n",
      "ep 2283: ep_len:80 episode reward: total was 38.500000. running mean: -10.358205\n",
      "ep 2283: ep_len:110 episode reward: total was 53.500000. running mean: -9.719623\n",
      "ep 2283: ep_len:1472 episode reward: total was -677.350000. running mean: -16.395927\n",
      "ep 2283: ep_len:2847 episode reward: total was -33.740000. running mean: -16.569367\n",
      "epsilon:0.009992 episode_count: 34385. steps_count: 36928159.000000\n",
      "ep 2284: ep_len:690 episode reward: total was -112.450000. running mean: -17.528174\n",
      "ep 2284: ep_len:738 episode reward: total was -23.250000. running mean: -17.585392\n",
      "ep 2284: ep_len:47 episode reward: total was 22.000000. running mean: -17.189538\n",
      "ep 2284: ep_len:3011 episode reward: total was 26.810000. running mean: -16.749543\n",
      "ep 2284: ep_len:569 episode reward: total was -42.960000. running mean: -17.011647\n",
      "ep 2284: ep_len:109 episode reward: total was 53.000000. running mean: -16.311531\n",
      "ep 2284: ep_len:631 episode reward: total was 29.380000. running mean: -15.854615\n",
      "ep 2284: ep_len:3993 episode reward: total was -754.080000. running mean: -23.236869\n",
      "ep 2284: ep_len:564 episode reward: total was -11.580000. running mean: -23.120301\n",
      "ep 2284: ep_len:776 episode reward: total was 64.920000. running mean: -22.239898\n",
      "ep 2284: ep_len:1456 episode reward: total was -13.410000. running mean: -22.151599\n",
      "ep 2284: ep_len:76 episode reward: total was 35.000000. running mean: -21.580083\n",
      "ep 2284: ep_len:145 episode reward: total was 71.000000. running mean: -20.654282\n",
      "ep 2284: ep_len:114 episode reward: total was 54.000000. running mean: -19.907739\n",
      "ep 2284: ep_len:598 episode reward: total was -8.330000. running mean: -19.791962\n",
      "ep 2284: ep_len:2901 episode reward: total was 0.080000. running mean: -19.593242\n",
      "ep 2284: ep_len:74 episode reward: total was 35.500000. running mean: -19.042310\n",
      "epsilon:0.009992 episode_count: 34402. steps_count: 36944651.000000\n",
      "ep 2285: ep_len:721 episode reward: total was -20.900000. running mean: -19.060886\n",
      "ep 2285: ep_len:187 episode reward: total was 10.830000. running mean: -18.761978\n",
      "ep 2285: ep_len:49 episode reward: total was 21.500000. running mean: -18.359358\n",
      "ep 2285: ep_len:3001 episode reward: total was -21.270000. running mean: -18.388464\n",
      "ep 2285: ep_len:1445 episode reward: total was -4.820000. running mean: -18.252780\n",
      "ep 2285: ep_len:1460 episode reward: total was -326.950000. running mean: -21.339752\n",
      "ep 2285: ep_len:352 episode reward: total was -4.350000. running mean: -21.169854\n",
      "ep 2285: ep_len:612 episode reward: total was -187.820000. running mean: -22.836356\n",
      "ep 2285: ep_len:736 episode reward: total was 29.250000. running mean: -22.315492\n",
      "ep 2285: ep_len:500 episode reward: total was 2.820000. running mean: -22.064137\n",
      "ep 2285: ep_len:55 episode reward: total was 24.500000. running mean: -21.598496\n",
      "ep 2285: ep_len:968 episode reward: total was -48.310000. running mean: -21.865611\n",
      "ep 2285: ep_len:2770 episode reward: total was -12.290000. running mean: -21.769855\n",
      "ep 2285: ep_len:52 episode reward: total was 23.000000. running mean: -21.322156\n",
      "epsilon:0.009992 episode_count: 34416. steps_count: 36957559.000000\n",
      "ep 2286: ep_len:923 episode reward: total was -51.150000. running mean: -21.620435\n",
      "ep 2286: ep_len:682 episode reward: total was -15.570000. running mean: -21.559930\n",
      "ep 2286: ep_len:3008 episode reward: total was -8.570000. running mean: -21.430031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2286: ep_len:828 episode reward: total was 22.770000. running mean: -20.988031\n",
      "ep 2286: ep_len:59 episode reward: total was 26.500000. running mean: -20.513150\n",
      "ep 2286: ep_len:43 episode reward: total was 20.000000. running mean: -20.108019\n",
      "ep 2286: ep_len:629 episode reward: total was -22.510000. running mean: -20.132039\n",
      "ep 2286: ep_len:3983 episode reward: total was -232.570000. running mean: -22.256418\n",
      "ep 2286: ep_len:830 episode reward: total was -34.730000. running mean: -22.381154\n",
      "ep 2286: ep_len:688 episode reward: total was 33.050000. running mean: -21.826843\n",
      "ep 2286: ep_len:578 episode reward: total was -2.500000. running mean: -21.633574\n",
      "ep 2286: ep_len:88 episode reward: total was 39.500000. running mean: -21.022238\n",
      "ep 2286: ep_len:61 episode reward: total was 27.500000. running mean: -20.537016\n",
      "ep 2286: ep_len:112 episode reward: total was 53.000000. running mean: -19.801646\n",
      "ep 2286: ep_len:758 episode reward: total was -25.920000. running mean: -19.862829\n",
      "ep 2286: ep_len:2838 episode reward: total was -10.570000. running mean: -19.769901\n",
      "ep 2286: ep_len:69 episode reward: total was 33.000000. running mean: -19.242202\n",
      "epsilon:0.009992 episode_count: 34433. steps_count: 36973736.000000\n",
      "ep 2287: ep_len:500 episode reward: total was 7.780000. running mean: -18.971980\n",
      "ep 2287: ep_len:500 episode reward: total was 13.780000. running mean: -18.644460\n",
      "ep 2287: ep_len:3012 episode reward: total was -21.130000. running mean: -18.669316\n",
      "ep 2287: ep_len:548 episode reward: total was -21.130000. running mean: -18.693923\n",
      "ep 2287: ep_len:168 episode reward: total was 81.000000. running mean: -17.696983\n",
      "ep 2287: ep_len:598 episode reward: total was 27.030000. running mean: -17.249714\n",
      "ep 2287: ep_len:657 episode reward: total was 21.100000. running mean: -16.866216\n",
      "ep 2287: ep_len:859 episode reward: total was -18.360000. running mean: -16.881154\n",
      "ep 2287: ep_len:823 episode reward: total was 58.840000. running mean: -16.123943\n",
      "ep 2287: ep_len:1095 episode reward: total was 7.820000. running mean: -15.884503\n",
      "ep 2287: ep_len:98 episode reward: total was 46.000000. running mean: -15.265658\n",
      "ep 2287: ep_len:40 episode reward: total was 17.000000. running mean: -14.943002\n",
      "ep 2287: ep_len:80 episode reward: total was 38.500000. running mean: -14.408572\n",
      "ep 2287: ep_len:1099 episode reward: total was -10.390000. running mean: -14.368386\n",
      "ep 2287: ep_len:45 episode reward: total was 21.000000. running mean: -14.014702\n",
      "epsilon:0.009992 episode_count: 34448. steps_count: 36983858.000000\n",
      "ep 2288: ep_len:885 episode reward: total was -104.070000. running mean: -14.915255\n",
      "ep 2288: ep_len:751 episode reward: total was 5.310000. running mean: -14.713002\n",
      "ep 2288: ep_len:46 episode reward: total was 18.500000. running mean: -14.380872\n",
      "ep 2288: ep_len:101 episode reward: total was 47.500000. running mean: -13.762064\n",
      "ep 2288: ep_len:1608 episode reward: total was -78.720000. running mean: -14.411643\n",
      "ep 2288: ep_len:500 episode reward: total was 22.110000. running mean: -14.046427\n",
      "ep 2288: ep_len:4010 episode reward: total was -1435.190000. running mean: -28.257862\n",
      "ep 2288: ep_len:578 episode reward: total was 13.760000. running mean: -27.837684\n",
      "ep 2288: ep_len:715 episode reward: total was 16.880000. running mean: -27.390507\n",
      "ep 2288: ep_len:946 episode reward: total was -2.310000. running mean: -27.139702\n",
      "ep 2288: ep_len:56 episode reward: total was 26.500000. running mean: -26.603305\n",
      "ep 2288: ep_len:131 episode reward: total was 62.500000. running mean: -25.712272\n",
      "ep 2288: ep_len:35 episode reward: total was 14.500000. running mean: -25.310149\n",
      "ep 2288: ep_len:840 episode reward: total was 15.590000. running mean: -24.901148\n",
      "ep 2288: ep_len:2906 episode reward: total was 9.770000. running mean: -24.554436\n",
      "epsilon:0.009992 episode_count: 34463. steps_count: 36997966.000000\n",
      "ep 2289: ep_len:659 episode reward: total was 26.040000. running mean: -24.048492\n",
      "ep 2289: ep_len:771 episode reward: total was 2.650000. running mean: -23.781507\n",
      "ep 2289: ep_len:3007 episode reward: total was -28.900000. running mean: -23.832692\n",
      "ep 2289: ep_len:4438 episode reward: total was -1015.690000. running mean: -33.751265\n",
      "ep 2289: ep_len:59 episode reward: total was 28.000000. running mean: -33.133752\n",
      "ep 2289: ep_len:500 episode reward: total was 15.710000. running mean: -32.645315\n",
      "ep 2289: ep_len:3644 episode reward: total was -174.920000. running mean: -34.068062\n",
      "ep 2289: ep_len:4155 episode reward: total was -1033.810000. running mean: -44.065481\n",
      "ep 2289: ep_len:645 episode reward: total was -3.970000. running mean: -43.664526\n",
      "ep 2289: ep_len:529 episode reward: total was 2.590000. running mean: -43.201981\n",
      "ep 2289: ep_len:193 episode reward: total was 93.500000. running mean: -41.834961\n",
      "ep 2289: ep_len:94 episode reward: total was 45.500000. running mean: -40.961611\n",
      "ep 2289: ep_len:1173 episode reward: total was -9.120000. running mean: -40.643195\n",
      "ep 2289: ep_len:2722 episode reward: total was -63.390000. running mean: -40.870663\n",
      "epsilon:0.009992 episode_count: 34477. steps_count: 37020555.000000\n",
      "ep 2290: ep_len:667 episode reward: total was -46.660000. running mean: -40.928557\n",
      "ep 2290: ep_len:1647 episode reward: total was -81.670000. running mean: -41.335971\n",
      "ep 2290: ep_len:3012 episode reward: total was 2.670000. running mean: -40.895911\n",
      "ep 2290: ep_len:785 episode reward: total was 54.830000. running mean: -39.938652\n",
      "ep 2290: ep_len:62 episode reward: total was 28.000000. running mean: -39.259266\n",
      "ep 2290: ep_len:150 episode reward: total was 73.500000. running mean: -38.131673\n",
      "ep 2290: ep_len:500 episode reward: total was 50.280000. running mean: -37.247556\n",
      "ep 2290: ep_len:327 episode reward: total was 2.470000. running mean: -36.850381\n",
      "ep 2290: ep_len:547 episode reward: total was -4.800000. running mean: -36.529877\n",
      "ep 2290: ep_len:687 episode reward: total was 23.950000. running mean: -35.925078\n",
      "ep 2290: ep_len:1085 episode reward: total was 39.920000. running mean: -35.166627\n",
      "ep 2290: ep_len:93 episode reward: total was 42.000000. running mean: -34.394961\n",
      "ep 2290: ep_len:123 episode reward: total was 57.000000. running mean: -33.481012\n",
      "ep 2290: ep_len:1111 episode reward: total was -6.200000. running mean: -33.208201\n",
      "ep 2290: ep_len:2792 episode reward: total was -26.660000. running mean: -33.142719\n",
      "ep 2290: ep_len:37 episode reward: total was 17.000000. running mean: -32.641292\n",
      "epsilon:0.009992 episode_count: 34493. steps_count: 37034180.000000\n",
      "ep 2291: ep_len:674 episode reward: total was 11.450000. running mean: -32.200379\n",
      "ep 2291: ep_len:1226 episode reward: total was -45.450000. running mean: -32.332876\n",
      "ep 2291: ep_len:90 episode reward: total was 43.500000. running mean: -31.574547\n",
      "ep 2291: ep_len:797 episode reward: total was 35.040000. running mean: -30.908401\n",
      "ep 2291: ep_len:58 episode reward: total was 24.500000. running mean: -30.354317\n",
      "ep 2291: ep_len:1047 episode reward: total was -151.300000. running mean: -31.563774\n",
      "ep 2291: ep_len:3750 episode reward: total was -810.690000. running mean: -39.355036\n",
      "ep 2291: ep_len:778 episode reward: total was -71.580000. running mean: -39.677286\n",
      "ep 2291: ep_len:726 episode reward: total was -41.710000. running mean: -39.697613\n",
      "ep 2291: ep_len:823 episode reward: total was 8.520000. running mean: -39.215437\n",
      "ep 2291: ep_len:127 episode reward: total was 60.500000. running mean: -38.218283\n",
      "ep 2291: ep_len:616 episode reward: total was -13.870000. running mean: -37.974800\n",
      "ep 2291: ep_len:2798 episode reward: total was -25.750000. running mean: -37.852552\n",
      "ep 2291: ep_len:41 episode reward: total was 19.000000. running mean: -37.284026\n",
      "epsilon:0.009992 episode_count: 34507. steps_count: 37047731.000000\n",
      "ep 2292: ep_len:831 episode reward: total was 8.220000. running mean: -36.828986\n",
      "ep 2292: ep_len:713 episode reward: total was -16.270000. running mean: -36.623396\n",
      "ep 2292: ep_len:41 episode reward: total was 19.000000. running mean: -36.067162\n",
      "ep 2292: ep_len:2991 episode reward: total was -19.040000. running mean: -35.896891\n",
      "ep 2292: ep_len:500 episode reward: total was 18.410000. running mean: -35.353822\n",
      "ep 2292: ep_len:112 episode reward: total was 53.000000. running mean: -34.470284\n",
      "ep 2292: ep_len:68 episode reward: total was 32.500000. running mean: -33.800581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2292: ep_len:709 episode reward: total was -2.170000. running mean: -33.484275\n",
      "ep 2292: ep_len:630 episode reward: total was 11.280000. running mean: -33.036632\n",
      "ep 2292: ep_len:922 episode reward: total was -39.430000. running mean: -33.100566\n",
      "ep 2292: ep_len:861 episode reward: total was 46.940000. running mean: -32.300160\n",
      "ep 2292: ep_len:500 episode reward: total was 4.510000. running mean: -31.932059\n",
      "ep 2292: ep_len:68 episode reward: total was 32.500000. running mean: -31.287738\n",
      "ep 2292: ep_len:27 episode reward: total was 12.000000. running mean: -30.854861\n",
      "ep 2292: ep_len:113 episode reward: total was 55.000000. running mean: -29.996312\n",
      "ep 2292: ep_len:734 episode reward: total was -77.670000. running mean: -30.473049\n",
      "ep 2292: ep_len:2806 episode reward: total was -24.230000. running mean: -30.410618\n",
      "ep 2292: ep_len:45 episode reward: total was 21.000000. running mean: -29.896512\n",
      "epsilon:0.009992 episode_count: 34525. steps_count: 37060402.000000\n",
      "ep 2293: ep_len:983 episode reward: total was -69.120000. running mean: -30.288747\n",
      "ep 2293: ep_len:670 episode reward: total was -9.140000. running mean: -30.077260\n",
      "ep 2293: ep_len:49 episode reward: total was 23.000000. running mean: -29.546487\n",
      "ep 2293: ep_len:2927 episode reward: total was -12.970000. running mean: -29.380722\n",
      "ep 2293: ep_len:676 episode reward: total was -0.480000. running mean: -29.091715\n",
      "ep 2293: ep_len:42 episode reward: total was 19.500000. running mean: -28.605798\n",
      "ep 2293: ep_len:824 episode reward: total was 38.010000. running mean: -27.939640\n",
      "ep 2293: ep_len:3791 episode reward: total was -395.480000. running mean: -31.615043\n",
      "ep 2293: ep_len:675 episode reward: total was -26.750000. running mean: -31.566393\n",
      "ep 2293: ep_len:687 episode reward: total was 11.020000. running mean: -31.140529\n",
      "ep 2293: ep_len:622 episode reward: total was 11.830000. running mean: -30.710824\n",
      "ep 2293: ep_len:1147 episode reward: total was -17.470000. running mean: -30.578415\n",
      "ep 2293: ep_len:2788 episode reward: total was -21.350000. running mean: -30.486131\n",
      "ep 2293: ep_len:54 episode reward: total was 25.500000. running mean: -29.926270\n",
      "epsilon:0.009992 episode_count: 34539. steps_count: 37076337.000000\n",
      "ep 2294: ep_len:1127 episode reward: total was -9.900000. running mean: -29.726007\n",
      "ep 2294: ep_len:204 episode reward: total was -8.340000. running mean: -29.512147\n",
      "ep 2294: ep_len:2970 episode reward: total was -13.160000. running mean: -29.348626\n",
      "ep 2294: ep_len:870 episode reward: total was 71.820000. running mean: -28.336940\n",
      "ep 2294: ep_len:42 episode reward: total was 19.500000. running mean: -27.858570\n",
      "ep 2294: ep_len:691 episode reward: total was -4.990000. running mean: -27.629884\n",
      "ep 2294: ep_len:4004 episode reward: total was -130.360000. running mean: -28.657186\n",
      "ep 2294: ep_len:1209 episode reward: total was -28.480000. running mean: -28.655414\n",
      "ep 2294: ep_len:622 episode reward: total was 10.800000. running mean: -28.260860\n",
      "ep 2294: ep_len:690 episode reward: total was -17.090000. running mean: -28.149151\n",
      "ep 2294: ep_len:108 episode reward: total was 53.510000. running mean: -27.332559\n",
      "ep 2294: ep_len:49 episode reward: total was 21.500000. running mean: -26.844234\n",
      "ep 2294: ep_len:620 episode reward: total was -0.680000. running mean: -26.582592\n",
      "ep 2294: ep_len:2781 episode reward: total was 12.530000. running mean: -26.191466\n",
      "epsilon:0.009992 episode_count: 34553. steps_count: 37092324.000000\n",
      "ep 2295: ep_len:912 episode reward: total was -71.480000. running mean: -26.644351\n",
      "ep 2295: ep_len:1632 episode reward: total was -56.600000. running mean: -26.943907\n",
      "ep 2295: ep_len:2935 episode reward: total was -9.300000. running mean: -26.767468\n",
      "ep 2295: ep_len:500 episode reward: total was 20.760000. running mean: -26.292194\n",
      "ep 2295: ep_len:53 episode reward: total was 23.500000. running mean: -25.794272\n",
      "ep 2295: ep_len:608 episode reward: total was 29.670000. running mean: -25.239629\n",
      "ep 2295: ep_len:4076 episode reward: total was -88.020000. running mean: -25.867433\n",
      "ep 2295: ep_len:1971 episode reward: total was -136.250000. running mean: -26.971258\n",
      "ep 2295: ep_len:852 episode reward: total was 28.880000. running mean: -26.412746\n",
      "ep 2295: ep_len:1013 episode reward: total was 19.730000. running mean: -25.951318\n",
      "ep 2295: ep_len:112 episode reward: total was 53.000000. running mean: -25.161805\n",
      "ep 2295: ep_len:574 episode reward: total was -4.530000. running mean: -24.955487\n",
      "ep 2295: ep_len:2848 episode reward: total was 11.480000. running mean: -24.591132\n",
      "ep 2295: ep_len:56 episode reward: total was 26.500000. running mean: -24.080221\n",
      "epsilon:0.009992 episode_count: 34567. steps_count: 37110466.000000\n",
      "ep 2296: ep_len:702 episode reward: total was -14.360000. running mean: -23.983019\n",
      "ep 2296: ep_len:619 episode reward: total was -12.160000. running mean: -23.864789\n",
      "ep 2296: ep_len:56 episode reward: total was 26.500000. running mean: -23.361141\n",
      "ep 2296: ep_len:3027 episode reward: total was -65.190000. running mean: -23.779429\n",
      "ep 2296: ep_len:580 episode reward: total was 5.540000. running mean: -23.486235\n",
      "ep 2296: ep_len:103 episode reward: total was 48.500000. running mean: -22.766373\n",
      "ep 2296: ep_len:1697 episode reward: total was -306.910000. running mean: -25.607809\n",
      "ep 2296: ep_len:4052 episode reward: total was -138.570000. running mean: -26.737431\n",
      "ep 2296: ep_len:942 episode reward: total was -33.740000. running mean: -26.807456\n",
      "ep 2296: ep_len:704 episode reward: total was 26.660000. running mean: -26.272782\n",
      "ep 2296: ep_len:500 episode reward: total was 32.400000. running mean: -25.686054\n",
      "ep 2296: ep_len:64 episode reward: total was 29.000000. running mean: -25.139194\n",
      "ep 2296: ep_len:163 episode reward: total was 77.000000. running mean: -24.117802\n",
      "ep 2296: ep_len:58 episode reward: total was 24.500000. running mean: -23.631624\n",
      "ep 2296: ep_len:748 episode reward: total was -58.740000. running mean: -23.982707\n",
      "ep 2296: ep_len:2785 episode reward: total was -12.200000. running mean: -23.864880\n",
      "ep 2296: ep_len:57 episode reward: total was 25.500000. running mean: -23.371232\n",
      "epsilon:0.009992 episode_count: 34584. steps_count: 37127323.000000\n",
      "ep 2297: ep_len:1134 episode reward: total was -1.960000. running mean: -23.157119\n",
      "ep 2297: ep_len:1635 episode reward: total was -57.980000. running mean: -23.505348\n",
      "ep 2297: ep_len:62 episode reward: total was 29.500000. running mean: -22.975295\n",
      "ep 2297: ep_len:2968 episode reward: total was -28.450000. running mean: -23.030042\n",
      "ep 2297: ep_len:848 episode reward: total was 10.850000. running mean: -22.691241\n",
      "ep 2297: ep_len:42 episode reward: total was 18.000000. running mean: -22.284329\n",
      "ep 2297: ep_len:96 episode reward: total was 43.500000. running mean: -21.626485\n",
      "ep 2297: ep_len:69 episode reward: total was 31.500000. running mean: -21.095221\n",
      "ep 2297: ep_len:839 episode reward: total was 26.220000. running mean: -20.622068\n",
      "ep 2297: ep_len:3703 episode reward: total was -310.630000. running mean: -23.522148\n",
      "ep 2297: ep_len:690 episode reward: total was -27.760000. running mean: -23.564526\n",
      "ep 2297: ep_len:838 episode reward: total was 26.810000. running mean: -23.060781\n",
      "ep 2297: ep_len:607 episode reward: total was 5.680000. running mean: -22.773373\n",
      "ep 2297: ep_len:194 episode reward: total was 92.500000. running mean: -21.620639\n",
      "ep 2297: ep_len:96 episode reward: total was 46.500000. running mean: -20.939433\n",
      "ep 2297: ep_len:658 episode reward: total was 5.840000. running mean: -20.671639\n",
      "ep 2297: ep_len:2775 episode reward: total was -11.470000. running mean: -20.579622\n",
      "epsilon:0.009992 episode_count: 34601. steps_count: 37144577.000000\n",
      "ep 2298: ep_len:766 episode reward: total was -76.340000. running mean: -21.137226\n",
      "ep 2298: ep_len:791 episode reward: total was -6.120000. running mean: -20.987054\n",
      "ep 2298: ep_len:2962 episode reward: total was -64.240000. running mean: -21.419583\n",
      "ep 2298: ep_len:1433 episode reward: total was 8.220000. running mean: -21.123187\n",
      "ep 2298: ep_len:1859 episode reward: total was -128.000000. running mean: -22.191956\n",
      "ep 2298: ep_len:3648 episode reward: total was -22.290000. running mean: -22.192936\n",
      "ep 2298: ep_len:3915 episode reward: total was -740.970000. running mean: -29.380707\n",
      "ep 2298: ep_len:7241 episode reward: total was -39.870000. running mean: -29.485600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2298: ep_len:500 episode reward: total was -7.960000. running mean: -29.270344\n",
      "ep 2298: ep_len:161 episode reward: total was 77.500000. running mean: -28.202640\n",
      "ep 2298: ep_len:59 episode reward: total was 26.500000. running mean: -27.655614\n",
      "ep 2298: ep_len:844 episode reward: total was -14.490000. running mean: -27.523958\n",
      "ep 2298: ep_len:2754 episode reward: total was -155.290000. running mean: -28.801618\n",
      "epsilon:0.009992 episode_count: 34614. steps_count: 37171510.000000\n",
      "ep 2299: ep_len:1088 episode reward: total was -13.530000. running mean: -28.648902\n",
      "ep 2299: ep_len:500 episode reward: total was 23.000000. running mean: -28.132413\n",
      "ep 2299: ep_len:2898 episode reward: total was -49.220000. running mean: -28.343289\n",
      "ep 2299: ep_len:700 episode reward: total was -25.590000. running mean: -28.315756\n",
      "ep 2299: ep_len:61 episode reward: total was 29.000000. running mean: -27.742598\n",
      "ep 2299: ep_len:108 episode reward: total was 52.500000. running mean: -26.940172\n",
      "ep 2299: ep_len:51 episode reward: total was 21.000000. running mean: -26.460771\n",
      "ep 2299: ep_len:674 episode reward: total was 4.300000. running mean: -26.153163\n",
      "ep 2299: ep_len:337 episode reward: total was 10.590000. running mean: -25.785731\n",
      "ep 2299: ep_len:618 episode reward: total was -1.060000. running mean: -25.538474\n",
      "ep 2299: ep_len:671 episode reward: total was -0.030000. running mean: -25.283389\n",
      "ep 2299: ep_len:500 episode reward: total was 45.930000. running mean: -24.571255\n",
      "ep 2299: ep_len:60 episode reward: total was 25.500000. running mean: -24.070543\n",
      "ep 2299: ep_len:108 episode reward: total was 52.500000. running mean: -23.304837\n",
      "ep 2299: ep_len:57 episode reward: total was 25.500000. running mean: -22.816789\n",
      "ep 2299: ep_len:500 episode reward: total was 42.470000. running mean: -22.163921\n",
      "ep 2299: ep_len:2797 episode reward: total was 4.570000. running mean: -21.896582\n",
      "epsilon:0.009992 episode_count: 34631. steps_count: 37183238.000000\n",
      "ep 2300: ep_len:1036 episode reward: total was -68.590000. running mean: -22.363516\n",
      "ep 2300: ep_len:661 episode reward: total was -6.900000. running mean: -22.208881\n",
      "ep 2300: ep_len:2924 episode reward: total was -44.930000. running mean: -22.436092\n",
      "ep 2300: ep_len:504 episode reward: total was -12.420000. running mean: -22.335931\n",
      "ep 2300: ep_len:97 episode reward: total was 44.000000. running mean: -21.672572\n",
      "ep 2300: ep_len:41 episode reward: total was 19.000000. running mean: -21.265846\n",
      "ep 2300: ep_len:500 episode reward: total was 9.810000. running mean: -20.955088\n",
      "ep 2300: ep_len:318 episode reward: total was 16.800000. running mean: -20.577537\n",
      "ep 2300: ep_len:2752 episode reward: total was -588.750000. running mean: -26.259261\n",
      "ep 2300: ep_len:677 episode reward: total was 32.850000. running mean: -25.668169\n",
      "ep 2300: ep_len:1527 episode reward: total was 9.500000. running mean: -25.316487\n",
      "ep 2300: ep_len:575 episode reward: total was -8.560000. running mean: -25.148922\n",
      "ep 2300: ep_len:47 episode reward: total was 20.500000. running mean: -24.692433\n",
      "epsilon:0.009992 episode_count: 34644. steps_count: 37194897.000000\n",
      "ep 2301: ep_len:1143 episode reward: total was 16.310000. running mean: -24.282409\n",
      "ep 2301: ep_len:946 episode reward: total was 13.330000. running mean: -23.906285\n",
      "ep 2301: ep_len:56 episode reward: total was 26.500000. running mean: -23.402222\n",
      "ep 2301: ep_len:3048 episode reward: total was -57.640000. running mean: -23.744600\n",
      "ep 2301: ep_len:884 episode reward: total was 21.270000. running mean: -23.294454\n",
      "ep 2301: ep_len:50 episode reward: total was 23.500000. running mean: -22.826509\n",
      "ep 2301: ep_len:1372 episode reward: total was -47.780000. running mean: -23.076044\n",
      "ep 2301: ep_len:3896 episode reward: total was -559.240000. running mean: -28.437683\n",
      "ep 2301: ep_len:993 episode reward: total was -36.080000. running mean: -28.514107\n",
      "ep 2301: ep_len:704 episode reward: total was 35.420000. running mean: -27.874766\n",
      "ep 2301: ep_len:567 episode reward: total was 32.950000. running mean: -27.266518\n",
      "ep 2301: ep_len:119 episode reward: total was 53.500000. running mean: -26.458853\n",
      "ep 2301: ep_len:46 episode reward: total was 21.500000. running mean: -25.979264\n",
      "ep 2301: ep_len:1057 episode reward: total was -47.680000. running mean: -26.196272\n",
      "ep 2301: ep_len:2807 episode reward: total was -10.880000. running mean: -26.043109\n",
      "ep 2301: ep_len:58 episode reward: total was 27.500000. running mean: -25.507678\n",
      "epsilon:0.009992 episode_count: 34660. steps_count: 37212643.000000\n",
      "ep 2302: ep_len:721 episode reward: total was 0.380000. running mean: -25.248801\n",
      "ep 2302: ep_len:763 episode reward: total was 1.360000. running mean: -24.982713\n",
      "ep 2302: ep_len:54 episode reward: total was 24.000000. running mean: -24.492886\n",
      "ep 2302: ep_len:2962 episode reward: total was -104.360000. running mean: -25.291557\n",
      "ep 2302: ep_len:782 episode reward: total was 28.800000. running mean: -24.750641\n",
      "ep 2302: ep_len:37 episode reward: total was 17.000000. running mean: -24.333135\n",
      "ep 2302: ep_len:651 episode reward: total was -21.770000. running mean: -24.307504\n",
      "ep 2302: ep_len:657 episode reward: total was 23.580000. running mean: -23.828629\n",
      "ep 2302: ep_len:1175 episode reward: total was -58.570000. running mean: -24.176042\n",
      "ep 2302: ep_len:818 episode reward: total was 56.220000. running mean: -23.372082\n",
      "ep 2302: ep_len:635 episode reward: total was -17.460000. running mean: -23.312961\n",
      "ep 2302: ep_len:48 episode reward: total was 22.500000. running mean: -22.854831\n",
      "ep 2302: ep_len:125 episode reward: total was 61.000000. running mean: -22.016283\n",
      "ep 2302: ep_len:1014 episode reward: total was -271.870000. running mean: -24.514820\n",
      "ep 2302: ep_len:2924 episode reward: total was -7.470000. running mean: -24.344372\n",
      "epsilon:0.009992 episode_count: 34675. steps_count: 37226009.000000\n",
      "ep 2303: ep_len:1461 episode reward: total was 12.180000. running mean: -23.979128\n",
      "ep 2303: ep_len:847 episode reward: total was -0.010000. running mean: -23.739437\n",
      "ep 2303: ep_len:2942 episode reward: total was -81.960000. running mean: -24.321643\n",
      "ep 2303: ep_len:643 episode reward: total was -1.300000. running mean: -24.091426\n",
      "ep 2303: ep_len:129 episode reward: total was 60.000000. running mean: -23.250512\n",
      "ep 2303: ep_len:92 episode reward: total was 43.000000. running mean: -22.588007\n",
      "ep 2303: ep_len:741 episode reward: total was -8.920000. running mean: -22.451327\n",
      "ep 2303: ep_len:677 episode reward: total was 19.370000. running mean: -22.033114\n",
      "ep 2303: ep_len:1518 episode reward: total was -9.230000. running mean: -21.905082\n",
      "ep 2303: ep_len:7312 episode reward: total was -2098.620000. running mean: -42.672232\n",
      "ep 2303: ep_len:846 episode reward: total was -84.170000. running mean: -43.087209\n",
      "ep 2303: ep_len:117 episode reward: total was 57.000000. running mean: -42.086337\n",
      "ep 2303: ep_len:597 episode reward: total was 8.000000. running mean: -41.585474\n",
      "ep 2303: ep_len:2828 episode reward: total was 14.620000. running mean: -41.023419\n",
      "ep 2303: ep_len:52 episode reward: total was 24.500000. running mean: -40.368185\n",
      "epsilon:0.009992 episode_count: 34690. steps_count: 37246811.000000\n",
      "ep 2304: ep_len:1092 episode reward: total was -1.370000. running mean: -39.978203\n",
      "ep 2304: ep_len:949 episode reward: total was 20.190000. running mean: -39.376521\n",
      "ep 2304: ep_len:44 episode reward: total was 19.000000. running mean: -38.792756\n",
      "ep 2304: ep_len:3041 episode reward: total was -57.700000. running mean: -38.981828\n",
      "ep 2304: ep_len:1453 episode reward: total was 3.770000. running mean: -38.554310\n",
      "ep 2304: ep_len:41 episode reward: total was 17.500000. running mean: -37.993767\n",
      "ep 2304: ep_len:40 episode reward: total was 17.000000. running mean: -37.443829\n",
      "ep 2304: ep_len:626 episode reward: total was -19.660000. running mean: -37.265991\n",
      "ep 2304: ep_len:3698 episode reward: total was -150.580000. running mean: -38.399131\n",
      "ep 2304: ep_len:760 episode reward: total was -88.410000. running mean: -38.899240\n",
      "ep 2304: ep_len:854 episode reward: total was 47.890000. running mean: -38.031347\n",
      "ep 2304: ep_len:600 episode reward: total was -17.590000. running mean: -37.826934\n",
      "ep 2304: ep_len:155 episode reward: total was 73.000000. running mean: -36.718665\n",
      "ep 2304: ep_len:105 episode reward: total was 49.500000. running mean: -35.856478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2304: ep_len:1047 episode reward: total was -1.600000. running mean: -35.513913\n",
      "ep 2304: ep_len:2761 episode reward: total was -11.370000. running mean: -35.272474\n",
      "epsilon:0.009992 episode_count: 34706. steps_count: 37264077.000000\n",
      "ep 2305: ep_len:771 episode reward: total was -23.770000. running mean: -35.157449\n",
      "ep 2305: ep_len:658 episode reward: total was -10.270000. running mean: -34.908575\n",
      "ep 2305: ep_len:70 episode reward: total was 30.500000. running mean: -34.254489\n",
      "ep 2305: ep_len:2974 episode reward: total was -33.930000. running mean: -34.251244\n",
      "ep 2305: ep_len:638 episode reward: total was 7.220000. running mean: -33.836532\n",
      "ep 2305: ep_len:55 episode reward: total was 24.500000. running mean: -33.253166\n",
      "ep 2305: ep_len:60 episode reward: total was 28.010000. running mean: -32.640535\n",
      "ep 2305: ep_len:680 episode reward: total was -31.540000. running mean: -32.629529\n",
      "ep 2305: ep_len:675 episode reward: total was 15.430000. running mean: -32.148934\n",
      "ep 2305: ep_len:588 episode reward: total was -69.030000. running mean: -32.517745\n",
      "ep 2305: ep_len:683 episode reward: total was 7.040000. running mean: -32.122167\n",
      "ep 2305: ep_len:500 episode reward: total was 1.990000. running mean: -31.781046\n",
      "ep 2305: ep_len:62 episode reward: total was 29.500000. running mean: -31.168235\n",
      "ep 2305: ep_len:188 episode reward: total was 86.500000. running mean: -29.991553\n",
      "ep 2305: ep_len:107 episode reward: total was 52.000000. running mean: -29.171637\n",
      "ep 2305: ep_len:985 episode reward: total was 7.850000. running mean: -28.801421\n",
      "ep 2305: ep_len:2857 episode reward: total was -26.600000. running mean: -28.779407\n",
      "epsilon:0.009992 episode_count: 34723. steps_count: 37276628.000000\n",
      "ep 2306: ep_len:823 episode reward: total was -30.320000. running mean: -28.794813\n",
      "ep 2306: ep_len:666 episode reward: total was -30.880000. running mean: -28.815664\n",
      "ep 2306: ep_len:2915 episode reward: total was -56.960000. running mean: -29.097108\n",
      "ep 2306: ep_len:553 episode reward: total was -5.750000. running mean: -28.863637\n",
      "ep 2306: ep_len:55 episode reward: total was 24.500000. running mean: -28.330000\n",
      "ep 2306: ep_len:675 episode reward: total was -21.580000. running mean: -28.262500\n",
      "ep 2306: ep_len:3962 episode reward: total was -253.340000. running mean: -30.513275\n",
      "ep 2306: ep_len:587 episode reward: total was 14.310000. running mean: -30.065043\n",
      "ep 2306: ep_len:7630 episode reward: total was -169.740000. running mean: -31.461792\n",
      "ep 2306: ep_len:500 episode reward: total was -5.470000. running mean: -31.201874\n",
      "ep 2306: ep_len:59 episode reward: total was 26.500000. running mean: -30.624856\n",
      "ep 2306: ep_len:188 episode reward: total was 91.000000. running mean: -29.408607\n",
      "ep 2306: ep_len:812 episode reward: total was -31.860000. running mean: -29.433121\n",
      "ep 2306: ep_len:2886 episode reward: total was -3.470000. running mean: -29.173490\n",
      "ep 2306: ep_len:26 episode reward: total was 11.500000. running mean: -28.766755\n",
      "epsilon:0.009992 episode_count: 34738. steps_count: 37298965.000000\n",
      "ep 2307: ep_len:644 episode reward: total was 10.180000. running mean: -28.377287\n",
      "ep 2307: ep_len:953 episode reward: total was 10.130000. running mean: -27.992214\n",
      "ep 2307: ep_len:2974 episode reward: total was -28.920000. running mean: -28.001492\n",
      "ep 2307: ep_len:882 episode reward: total was 37.070000. running mean: -27.350777\n",
      "ep 2307: ep_len:174 episode reward: total was 82.500000. running mean: -26.252270\n",
      "ep 2307: ep_len:500 episode reward: total was -14.690000. running mean: -26.136647\n",
      "ep 2307: ep_len:336 episode reward: total was 11.680000. running mean: -25.758480\n",
      "ep 2307: ep_len:617 episode reward: total was -31.370000. running mean: -25.814596\n",
      "ep 2307: ep_len:736 episode reward: total was 0.590000. running mean: -25.550550\n",
      "ep 2307: ep_len:899 episode reward: total was -13.180000. running mean: -25.426844\n",
      "ep 2307: ep_len:46 episode reward: total was 21.500000. running mean: -24.957576\n",
      "ep 2307: ep_len:1076 episode reward: total was 18.490000. running mean: -24.523100\n",
      "ep 2307: ep_len:2894 episode reward: total was 8.700000. running mean: -24.190869\n",
      "epsilon:0.009992 episode_count: 34751. steps_count: 37311696.000000\n",
      "ep 2308: ep_len:708 episode reward: total was 28.290000. running mean: -23.666060\n",
      "ep 2308: ep_len:689 episode reward: total was -3.680000. running mean: -23.466200\n",
      "ep 2308: ep_len:2906 episode reward: total was -51.630000. running mean: -23.747838\n",
      "ep 2308: ep_len:500 episode reward: total was -5.410000. running mean: -23.564459\n",
      "ep 2308: ep_len:103 episode reward: total was 47.000000. running mean: -22.858815\n",
      "ep 2308: ep_len:66 episode reward: total was 31.500000. running mean: -22.315227\n",
      "ep 2308: ep_len:1429 episode reward: total was -0.570000. running mean: -22.097774\n",
      "ep 2308: ep_len:670 episode reward: total was 15.840000. running mean: -21.718396\n",
      "ep 2308: ep_len:1187 episode reward: total was -21.840000. running mean: -21.719613\n",
      "ep 2308: ep_len:714 episode reward: total was 28.380000. running mean: -21.218616\n",
      "ep 2308: ep_len:557 episode reward: total was 37.440000. running mean: -20.632030\n",
      "ep 2308: ep_len:86 episode reward: total was 40.000000. running mean: -20.025710\n",
      "ep 2308: ep_len:1432 episode reward: total was -2.560000. running mean: -19.851053\n",
      "ep 2308: ep_len:2859 episode reward: total was -18.530000. running mean: -19.837842\n",
      "ep 2308: ep_len:42 episode reward: total was 19.500000. running mean: -19.444464\n",
      "epsilon:0.009992 episode_count: 34766. steps_count: 37325644.000000\n",
      "ep 2309: ep_len:699 episode reward: total was 0.760000. running mean: -19.242419\n",
      "ep 2309: ep_len:948 episode reward: total was 23.760000. running mean: -18.812395\n",
      "ep 2309: ep_len:2912 episode reward: total was -50.500000. running mean: -19.129271\n",
      "ep 2309: ep_len:638 episode reward: total was 6.210000. running mean: -18.875878\n",
      "ep 2309: ep_len:123 episode reward: total was 60.000000. running mean: -18.087120\n",
      "ep 2309: ep_len:44 episode reward: total was 20.500000. running mean: -17.701248\n",
      "ep 2309: ep_len:500 episode reward: total was 14.920000. running mean: -17.375036\n",
      "ep 2309: ep_len:363 episode reward: total was 11.220000. running mean: -17.089086\n",
      "ep 2309: ep_len:684 episode reward: total was -18.360000. running mean: -17.101795\n",
      "ep 2309: ep_len:748 episode reward: total was 44.340000. running mean: -16.487377\n",
      "ep 2309: ep_len:1480 episode reward: total was -68.960000. running mean: -17.012103\n",
      "ep 2309: ep_len:62 episode reward: total was 29.500000. running mean: -16.546982\n",
      "ep 2309: ep_len:96 episode reward: total was 46.500000. running mean: -15.916512\n",
      "ep 2309: ep_len:1079 episode reward: total was 8.030000. running mean: -15.677047\n",
      "ep 2309: ep_len:2910 episode reward: total was -17.430000. running mean: -15.694577\n",
      "ep 2309: ep_len:71 episode reward: total was 34.000000. running mean: -15.197631\n",
      "epsilon:0.009992 episode_count: 34782. steps_count: 37339001.000000\n",
      "ep 2310: ep_len:624 episode reward: total was -6.230000. running mean: -15.107954\n",
      "ep 2310: ep_len:500 episode reward: total was 8.150000. running mean: -14.875375\n",
      "ep 2310: ep_len:3013 episode reward: total was -70.940000. running mean: -15.436021\n",
      "ep 2310: ep_len:1092 episode reward: total was -16.520000. running mean: -15.446861\n",
      "ep 2310: ep_len:73 episode reward: total was 35.000000. running mean: -14.942392\n",
      "ep 2310: ep_len:67 episode reward: total was 32.000000. running mean: -14.472968\n",
      "ep 2310: ep_len:2354 episode reward: total was -1783.660000. running mean: -32.164839\n",
      "ep 2310: ep_len:652 episode reward: total was 26.560000. running mean: -31.577590\n",
      "ep 2310: ep_len:1552 episode reward: total was -16.330000. running mean: -31.425114\n",
      "ep 2310: ep_len:7245 episode reward: total was -326.560000. running mean: -34.376463\n",
      "ep 2310: ep_len:690 episode reward: total was 31.660000. running mean: -33.716099\n",
      "ep 2310: ep_len:159 episode reward: total was 78.000000. running mean: -32.598938\n",
      "ep 2310: ep_len:1493 episode reward: total was 0.250000. running mean: -32.270448\n",
      "ep 2310: ep_len:2894 episode reward: total was -13.800000. running mean: -32.085744\n",
      "ep 2310: ep_len:48 episode reward: total was 21.000000. running mean: -31.554886\n",
      "epsilon:0.009992 episode_count: 34797. steps_count: 37361457.000000\n",
      "ep 2311: ep_len:500 episode reward: total was 7.480000. running mean: -31.164538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2311: ep_len:500 episode reward: total was 11.360000. running mean: -30.739292\n",
      "ep 2311: ep_len:41 episode reward: total was 19.000000. running mean: -30.241899\n",
      "ep 2311: ep_len:2900 episode reward: total was -42.720000. running mean: -30.366680\n",
      "ep 2311: ep_len:542 episode reward: total was -36.160000. running mean: -30.424613\n",
      "ep 2311: ep_len:133 episode reward: total was 65.000000. running mean: -29.470367\n",
      "ep 2311: ep_len:80 episode reward: total was 38.500000. running mean: -28.790664\n",
      "ep 2311: ep_len:500 episode reward: total was 19.230000. running mean: -28.310457\n",
      "ep 2311: ep_len:3583 episode reward: total was -109.400000. running mean: -29.121352\n",
      "ep 2311: ep_len:809 episode reward: total was -28.230000. running mean: -29.112439\n",
      "ep 2311: ep_len:7387 episode reward: total was 20.570000. running mean: -28.615615\n",
      "ep 2311: ep_len:500 episode reward: total was 51.810000. running mean: -27.811358\n",
      "ep 2311: ep_len:157 episode reward: total was 77.000000. running mean: -26.763245\n",
      "ep 2311: ep_len:98 episode reward: total was 47.500000. running mean: -26.020612\n",
      "ep 2311: ep_len:966 episode reward: total was -67.370000. running mean: -26.434106\n",
      "ep 2311: ep_len:2874 episode reward: total was 2.160000. running mean: -26.148165\n",
      "ep 2311: ep_len:31 episode reward: total was 12.500000. running mean: -25.761684\n",
      "epsilon:0.009992 episode_count: 34814. steps_count: 37383058.000000\n",
      "ep 2312: ep_len:1194 episode reward: total was 6.120000. running mean: -25.442867\n",
      "ep 2312: ep_len:1281 episode reward: total was -81.290000. running mean: -26.001338\n",
      "ep 2312: ep_len:2818 episode reward: total was -105.740000. running mean: -26.798725\n",
      "ep 2312: ep_len:868 episode reward: total was 13.850000. running mean: -26.392237\n",
      "ep 2312: ep_len:53 episode reward: total was 25.000000. running mean: -25.878315\n",
      "ep 2312: ep_len:61 episode reward: total was 27.500000. running mean: -25.344532\n",
      "ep 2312: ep_len:1880 episode reward: total was -169.320000. running mean: -26.784287\n",
      "ep 2312: ep_len:500 episode reward: total was 21.230000. running mean: -26.304144\n",
      "ep 2312: ep_len:1121 episode reward: total was -33.800000. running mean: -26.379102\n",
      "ep 2312: ep_len:775 episode reward: total was 11.020000. running mean: -26.005111\n",
      "ep 2312: ep_len:1358 episode reward: total was 9.980000. running mean: -25.645260\n",
      "ep 2312: ep_len:68 episode reward: total was 32.500000. running mean: -25.063808\n",
      "ep 2312: ep_len:134 episode reward: total was 65.500000. running mean: -24.158169\n",
      "ep 2312: ep_len:1159 episode reward: total was 9.900000. running mean: -23.817588\n",
      "ep 2312: ep_len:2866 episode reward: total was -8.630000. running mean: -23.665712\n",
      "ep 2312: ep_len:30 episode reward: total was 13.500000. running mean: -23.294055\n",
      "epsilon:0.009992 episode_count: 34830. steps_count: 37399224.000000\n",
      "ep 2313: ep_len:633 episode reward: total was -7.850000. running mean: -23.139614\n",
      "ep 2313: ep_len:683 episode reward: total was -11.580000. running mean: -23.024018\n",
      "ep 2313: ep_len:51 episode reward: total was 24.000000. running mean: -22.553778\n",
      "ep 2313: ep_len:3084 episode reward: total was -100.430000. running mean: -23.332540\n",
      "ep 2313: ep_len:568 episode reward: total was 18.810000. running mean: -22.911115\n",
      "ep 2313: ep_len:53 episode reward: total was 23.500000. running mean: -22.447004\n",
      "ep 2313: ep_len:95 episode reward: total was 43.000000. running mean: -21.792534\n",
      "ep 2313: ep_len:834 episode reward: total was 21.610000. running mean: -21.358508\n",
      "ep 2313: ep_len:359 episode reward: total was 17.760000. running mean: -20.967323\n",
      "ep 2313: ep_len:603 episode reward: total was -46.110000. running mean: -21.218750\n",
      "ep 2313: ep_len:653 episode reward: total was 9.120000. running mean: -20.915362\n",
      "ep 2313: ep_len:679 episode reward: total was -12.190000. running mean: -20.828109\n",
      "ep 2313: ep_len:162 episode reward: total was 79.500000. running mean: -19.824828\n",
      "ep 2313: ep_len:43 episode reward: total was 18.500000. running mean: -19.441579\n",
      "ep 2313: ep_len:84 episode reward: total was 40.500000. running mean: -18.842164\n",
      "ep 2313: ep_len:724 episode reward: total was -12.120000. running mean: -18.774942\n",
      "ep 2313: ep_len:2740 episode reward: total was -19.660000. running mean: -18.783793\n",
      "epsilon:0.009992 episode_count: 34847. steps_count: 37411272.000000\n",
      "ep 2314: ep_len:586 episode reward: total was -9.460000. running mean: -18.690555\n",
      "ep 2314: ep_len:1634 episode reward: total was -77.790000. running mean: -19.281549\n",
      "ep 2314: ep_len:2996 episode reward: total was 12.880000. running mean: -18.959934\n",
      "ep 2314: ep_len:680 episode reward: total was 6.740000. running mean: -18.702934\n",
      "ep 2314: ep_len:114 episode reward: total was 48.000000. running mean: -18.035905\n",
      "ep 2314: ep_len:844 episode reward: total was 25.160000. running mean: -17.603946\n",
      "ep 2314: ep_len:335 episode reward: total was 16.480000. running mean: -17.263106\n",
      "ep 2314: ep_len:818 episode reward: total was -4.510000. running mean: -17.135575\n",
      "ep 2314: ep_len:7438 episode reward: total was -20.750000. running mean: -17.171720\n",
      "ep 2314: ep_len:594 episode reward: total was -26.550000. running mean: -17.265502\n",
      "ep 2314: ep_len:108 episode reward: total was 49.500000. running mean: -16.597847\n",
      "ep 2314: ep_len:500 episode reward: total was 20.330000. running mean: -16.228569\n",
      "ep 2314: ep_len:2874 episode reward: total was -7.480000. running mean: -16.141083\n",
      "epsilon:0.009992 episode_count: 34860. steps_count: 37430793.000000\n",
      "ep 2315: ep_len:779 episode reward: total was -23.220000. running mean: -16.211872\n",
      "ep 2315: ep_len:1633 episode reward: total was -48.480000. running mean: -16.534554\n",
      "ep 2315: ep_len:2978 episode reward: total was -44.450000. running mean: -16.813708\n",
      "ep 2315: ep_len:560 episode reward: total was -21.840000. running mean: -16.863971\n",
      "ep 2315: ep_len:69 episode reward: total was 33.000000. running mean: -16.365331\n",
      "ep 2315: ep_len:161 episode reward: total was 79.000000. running mean: -15.411678\n",
      "ep 2315: ep_len:56 episode reward: total was 26.500000. running mean: -14.992561\n",
      "ep 2315: ep_len:1322 episode reward: total was -104.110000. running mean: -15.883736\n",
      "ep 2315: ep_len:636 episode reward: total was 30.470000. running mean: -15.420198\n",
      "ep 2315: ep_len:646 episode reward: total was 0.230000. running mean: -15.263696\n",
      "ep 2315: ep_len:7164 episode reward: total was 15.620000. running mean: -14.954859\n",
      "ep 2315: ep_len:1526 episode reward: total was 7.160000. running mean: -14.733711\n",
      "ep 2315: ep_len:67 episode reward: total was 32.000000. running mean: -14.266374\n",
      "ep 2315: ep_len:41 episode reward: total was 17.500000. running mean: -13.948710\n",
      "ep 2315: ep_len:1475 episode reward: total was 17.610000. running mean: -13.633123\n",
      "ep 2315: ep_len:46 episode reward: total was 21.500000. running mean: -13.281792\n",
      "epsilon:0.009992 episode_count: 34876. steps_count: 37449952.000000\n",
      "ep 2316: ep_len:1038 episode reward: total was -3.410000. running mean: -13.183074\n",
      "ep 2316: ep_len:739 episode reward: total was -50.580000. running mean: -13.557043\n",
      "ep 2316: ep_len:3023 episode reward: total was 9.830000. running mean: -13.323172\n",
      "ep 2316: ep_len:537 episode reward: total was -22.070000. running mean: -13.410641\n",
      "ep 2316: ep_len:73 episode reward: total was 33.500000. running mean: -12.941534\n",
      "ep 2316: ep_len:44 episode reward: total was 20.500000. running mean: -12.607119\n",
      "ep 2316: ep_len:1069 episode reward: total was -10.920000. running mean: -12.590248\n",
      "ep 2316: ep_len:3888 episode reward: total was -40.250000. running mean: -12.866845\n",
      "ep 2316: ep_len:1131 episode reward: total was -254.960000. running mean: -15.287777\n",
      "ep 2316: ep_len:7233 episode reward: total was -9.040000. running mean: -15.225299\n",
      "ep 2316: ep_len:849 episode reward: total was 10.100000. running mean: -14.972046\n",
      "ep 2316: ep_len:165 episode reward: total was 81.000000. running mean: -14.012326\n",
      "ep 2316: ep_len:1133 episode reward: total was -9.040000. running mean: -13.962602\n",
      "ep 2316: ep_len:2715 episode reward: total was -27.500000. running mean: -14.097976\n",
      "ep 2316: ep_len:52 episode reward: total was 23.000000. running mean: -13.726997\n",
      "epsilon:0.009992 episode_count: 34891. steps_count: 37473641.000000\n",
      "ep 2317: ep_len:1096 episode reward: total was -733.430000. running mean: -20.924027\n",
      "ep 2317: ep_len:751 episode reward: total was -7.230000. running mean: -20.787086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2317: ep_len:3010 episode reward: total was -44.770000. running mean: -21.026915\n",
      "ep 2317: ep_len:500 episode reward: total was 24.930000. running mean: -20.567346\n",
      "ep 2317: ep_len:50 episode reward: total was 23.500000. running mean: -20.126673\n",
      "ep 2317: ep_len:129 episode reward: total was 63.000000. running mean: -19.295406\n",
      "ep 2317: ep_len:1402 episode reward: total was -86.630000. running mean: -19.968752\n",
      "ep 2317: ep_len:654 episode reward: total was 31.140000. running mean: -19.457665\n",
      "ep 2317: ep_len:3101 episode reward: total was -473.580000. running mean: -23.998888\n",
      "ep 2317: ep_len:853 episode reward: total was 39.820000. running mean: -23.360699\n",
      "ep 2317: ep_len:500 episode reward: total was 3.090000. running mean: -23.096192\n",
      "ep 2317: ep_len:66 episode reward: total was 31.500000. running mean: -22.550230\n",
      "ep 2317: ep_len:51 episode reward: total was 22.500000. running mean: -22.099728\n",
      "ep 2317: ep_len:52 episode reward: total was 24.500000. running mean: -21.633731\n",
      "ep 2317: ep_len:500 episode reward: total was 14.150000. running mean: -21.275893\n",
      "ep 2317: ep_len:2794 episode reward: total was -5.040000. running mean: -21.113534\n",
      "epsilon:0.009992 episode_count: 34907. steps_count: 37489150.000000\n",
      "ep 2318: ep_len:1131 episode reward: total was 2.630000. running mean: -20.876099\n",
      "ep 2318: ep_len:726 episode reward: total was -21.680000. running mean: -20.884138\n",
      "ep 2318: ep_len:2970 episode reward: total was -42.450000. running mean: -21.099797\n",
      "ep 2318: ep_len:704 episode reward: total was -8.280000. running mean: -20.971599\n",
      "ep 2318: ep_len:43 episode reward: total was 20.000000. running mean: -20.561883\n",
      "ep 2318: ep_len:51 episode reward: total was 21.000000. running mean: -20.146264\n",
      "ep 2318: ep_len:872 episode reward: total was 22.660000. running mean: -19.718201\n",
      "ep 2318: ep_len:340 episode reward: total was 12.000000. running mean: -19.401019\n",
      "ep 2318: ep_len:586 episode reward: total was 2.820000. running mean: -19.178809\n",
      "ep 2318: ep_len:778 episode reward: total was -36.490000. running mean: -19.351921\n",
      "ep 2318: ep_len:522 episode reward: total was 12.830000. running mean: -19.030102\n",
      "ep 2318: ep_len:41 episode reward: total was 19.000000. running mean: -18.649801\n",
      "ep 2318: ep_len:871 episode reward: total was 17.940000. running mean: -18.283903\n",
      "ep 2318: ep_len:2872 episode reward: total was -17.290000. running mean: -18.273964\n",
      "ep 2318: ep_len:59 episode reward: total was 26.500000. running mean: -17.826224\n",
      "epsilon:0.009992 episode_count: 34922. steps_count: 37501716.000000\n",
      "ep 2319: ep_len:2499 episode reward: total was -204.430000. running mean: -19.692262\n",
      "ep 2319: ep_len:998 episode reward: total was 28.760000. running mean: -19.207739\n",
      "ep 2319: ep_len:34 episode reward: total was 15.500000. running mean: -18.860662\n",
      "ep 2319: ep_len:3038 episode reward: total was -30.070000. running mean: -18.972755\n",
      "ep 2319: ep_len:1130 episode reward: total was -23.210000. running mean: -19.015128\n",
      "ep 2319: ep_len:39 episode reward: total was 18.000000. running mean: -18.644976\n",
      "ep 2319: ep_len:169 episode reward: total was 77.000000. running mean: -17.688527\n",
      "ep 2319: ep_len:110 episode reward: total was 53.500000. running mean: -16.976641\n",
      "ep 2319: ep_len:73 episode reward: total was 35.000000. running mean: -16.456875\n",
      "ep 2319: ep_len:940 episode reward: total was -10.510000. running mean: -16.397406\n",
      "ep 2319: ep_len:350 episode reward: total was 20.150000. running mean: -16.031932\n",
      "ep 2319: ep_len:500 episode reward: total was -4.830000. running mean: -15.919913\n",
      "ep 2319: ep_len:629 episode reward: total was 11.270000. running mean: -15.648014\n",
      "ep 2319: ep_len:1100 episode reward: total was -9.800000. running mean: -15.589533\n",
      "ep 2319: ep_len:36 episode reward: total was 16.500000. running mean: -15.268638\n",
      "ep 2319: ep_len:114 episode reward: total was 54.000000. running mean: -14.575952\n",
      "ep 2319: ep_len:1485 episode reward: total was 7.060000. running mean: -14.359592\n",
      "ep 2319: ep_len:2780 episode reward: total was -1.200000. running mean: -14.227996\n",
      "ep 2319: ep_len:58 episode reward: total was 27.500000. running mean: -13.810716\n",
      "epsilon:0.009992 episode_count: 34941. steps_count: 37517798.000000\n",
      "ep 2320: ep_len:834 episode reward: total was -3.780000. running mean: -13.710409\n",
      "ep 2320: ep_len:659 episode reward: total was -24.890000. running mean: -13.822205\n",
      "ep 2320: ep_len:2948 episode reward: total was -34.410000. running mean: -14.028083\n",
      "ep 2320: ep_len:778 episode reward: total was -14.670000. running mean: -14.034502\n",
      "ep 2320: ep_len:104 episode reward: total was 49.000000. running mean: -13.404157\n",
      "ep 2320: ep_len:81 episode reward: total was 39.000000. running mean: -12.880116\n",
      "ep 2320: ep_len:2988 episode reward: total was -146.030000. running mean: -14.211614\n",
      "ep 2320: ep_len:347 episode reward: total was 20.180000. running mean: -13.867698\n",
      "ep 2320: ep_len:578 episode reward: total was -1.920000. running mean: -13.748221\n",
      "ep 2320: ep_len:786 episode reward: total was -7.180000. running mean: -13.682539\n",
      "ep 2320: ep_len:602 episode reward: total was 63.920000. running mean: -12.906514\n",
      "ep 2320: ep_len:92 episode reward: total was 44.500000. running mean: -12.332449\n",
      "ep 2320: ep_len:190 episode reward: total was 93.500000. running mean: -11.274124\n",
      "ep 2320: ep_len:40 episode reward: total was 18.500000. running mean: -10.976383\n",
      "ep 2320: ep_len:1497 episode reward: total was 2.160000. running mean: -10.845019\n",
      "ep 2320: ep_len:2797 episode reward: total was -21.540000. running mean: -10.951969\n",
      "ep 2320: ep_len:65 episode reward: total was 29.500000. running mean: -10.547449\n",
      "epsilon:0.009992 episode_count: 34958. steps_count: 37533184.000000\n",
      "ep 2321: ep_len:620 episode reward: total was -1.040000. running mean: -10.452375\n",
      "ep 2321: ep_len:917 episode reward: total was 0.980000. running mean: -10.338051\n",
      "ep 2321: ep_len:60 episode reward: total was 28.500000. running mean: -9.949670\n",
      "ep 2321: ep_len:3108 episode reward: total was -47.950000. running mean: -10.329674\n",
      "ep 2321: ep_len:503 episode reward: total was 5.360000. running mean: -10.172777\n",
      "ep 2321: ep_len:53 episode reward: total was 25.000000. running mean: -9.821049\n",
      "ep 2321: ep_len:99 episode reward: total was 46.500000. running mean: -9.257839\n",
      "ep 2321: ep_len:73 episode reward: total was 35.000000. running mean: -8.815260\n",
      "ep 2321: ep_len:2917 episode reward: total was -161.370000. running mean: -10.340808\n",
      "ep 2321: ep_len:3649 episode reward: total was -41.070000. running mean: -10.648100\n",
      "ep 2321: ep_len:1240 episode reward: total was -58.900000. running mean: -11.130619\n",
      "ep 2321: ep_len:828 episode reward: total was 28.730000. running mean: -10.732012\n",
      "ep 2321: ep_len:760 episode reward: total was -9.740000. running mean: -10.722092\n",
      "ep 2321: ep_len:108 episode reward: total was 52.500000. running mean: -10.089871\n",
      "ep 2321: ep_len:1498 episode reward: total was 9.450000. running mean: -9.894473\n",
      "ep 2321: ep_len:2831 episode reward: total was -4.820000. running mean: -9.843728\n",
      "epsilon:0.009992 episode_count: 34974. steps_count: 37552448.000000\n",
      "ep 2322: ep_len:1028 episode reward: total was -36.350000. running mean: -10.108791\n",
      "ep 2322: ep_len:871 episode reward: total was 16.730000. running mean: -9.840403\n",
      "ep 2322: ep_len:45 episode reward: total was 21.000000. running mean: -9.531999\n",
      "ep 2322: ep_len:3010 episode reward: total was 13.300000. running mean: -9.303679\n",
      "ep 2322: ep_len:684 episode reward: total was 20.780000. running mean: -9.002842\n",
      "ep 2322: ep_len:108 episode reward: total was 52.500000. running mean: -8.387814\n",
      "ep 2322: ep_len:28 episode reward: total was 12.500000. running mean: -8.178935\n",
      "ep 2322: ep_len:865 episode reward: total was 52.630000. running mean: -7.570846\n",
      "ep 2322: ep_len:660 episode reward: total was 25.630000. running mean: -7.238838\n",
      "ep 2322: ep_len:637 episode reward: total was -26.120000. running mean: -7.427649\n",
      "ep 2322: ep_len:703 episode reward: total was -4.680000. running mean: -7.400173\n",
      "ep 2322: ep_len:724 episode reward: total was 2.020000. running mean: -7.305971\n",
      "ep 2322: ep_len:55 episode reward: total was 26.000000. running mean: -6.972911\n",
      "ep 2322: ep_len:768 episode reward: total was -55.110000. running mean: -7.454282\n",
      "ep 2322: ep_len:2849 episode reward: total was -4.790000. running mean: -7.427639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2322: ep_len:41 episode reward: total was 19.000000. running mean: -7.163363\n",
      "epsilon:0.009992 episode_count: 34990. steps_count: 37565524.000000\n",
      "ep 2323: ep_len:1026 episode reward: total was -80.810000. running mean: -7.899829\n",
      "ep 2323: ep_len:1166 episode reward: total was -115.880000. running mean: -8.979631\n",
      "ep 2323: ep_len:2999 episode reward: total was -53.970000. running mean: -9.429535\n",
      "ep 2323: ep_len:500 episode reward: total was -6.790000. running mean: -9.403139\n",
      "ep 2323: ep_len:102 episode reward: total was 48.000000. running mean: -8.829108\n",
      "ep 2323: ep_len:86 episode reward: total was 40.000000. running mean: -8.340817\n",
      "ep 2323: ep_len:58 episode reward: total was 26.000000. running mean: -7.997409\n",
      "ep 2323: ep_len:863 episode reward: total was 49.020000. running mean: -7.427235\n",
      "ep 2323: ep_len:4475 episode reward: total was -2639.230000. running mean: -33.745262\n",
      "ep 2323: ep_len:1544 episode reward: total was -45.180000. running mean: -33.859610\n",
      "ep 2323: ep_len:810 episode reward: total was 6.660000. running mean: -33.454414\n",
      "ep 2323: ep_len:1193 episode reward: total was -5.100000. running mean: -33.170869\n",
      "ep 2323: ep_len:152 episode reward: total was 73.000000. running mean: -32.109161\n",
      "ep 2323: ep_len:761 episode reward: total was -10.220000. running mean: -31.890269\n",
      "ep 2323: ep_len:2961 episode reward: total was 2.270000. running mean: -31.548666\n",
      "epsilon:0.009992 episode_count: 35005. steps_count: 37584220.000000\n",
      "ep 2324: ep_len:1417 episode reward: total was 12.560000. running mean: -31.107580\n",
      "ep 2324: ep_len:672 episode reward: total was -22.220000. running mean: -31.018704\n",
      "ep 2324: ep_len:2882 episode reward: total was -50.370000. running mean: -31.212217\n",
      "ep 2324: ep_len:575 episode reward: total was -8.050000. running mean: -30.980595\n",
      "ep 2324: ep_len:62 episode reward: total was 29.500000. running mean: -30.375789\n",
      "ep 2324: ep_len:57 episode reward: total was 25.500000. running mean: -29.817031\n",
      "ep 2324: ep_len:500 episode reward: total was 25.660000. running mean: -29.262261\n",
      "ep 2324: ep_len:3721 episode reward: total was -584.490000. running mean: -34.814538\n",
      "ep 2324: ep_len:1256 episode reward: total was -47.200000. running mean: -34.938393\n",
      "ep 2324: ep_len:634 episode reward: total was 20.110000. running mean: -34.387909\n",
      "ep 2324: ep_len:624 episode reward: total was -5.880000. running mean: -34.102830\n",
      "ep 2324: ep_len:42 episode reward: total was 19.500000. running mean: -33.566801\n",
      "ep 2324: ep_len:121 episode reward: total was 57.500000. running mean: -32.656133\n",
      "ep 2324: ep_len:1516 episode reward: total was 4.560000. running mean: -32.283972\n",
      "ep 2324: ep_len:2832 episode reward: total was -55.250000. running mean: -32.513632\n",
      "ep 2324: ep_len:46 episode reward: total was 21.500000. running mean: -31.973496\n",
      "epsilon:0.009992 episode_count: 35021. steps_count: 37601177.000000\n",
      "ep 2325: ep_len:1131 episode reward: total was 17.580000. running mean: -31.477961\n",
      "ep 2325: ep_len:723 episode reward: total was -3.940000. running mean: -31.202581\n",
      "ep 2325: ep_len:2961 episode reward: total was 10.080000. running mean: -30.789756\n",
      "ep 2325: ep_len:681 episode reward: total was 8.540000. running mean: -30.396458\n",
      "ep 2325: ep_len:79 episode reward: total was 38.000000. running mean: -29.712493\n",
      "ep 2325: ep_len:602 episode reward: total was -4.780000. running mean: -29.463169\n",
      "ep 2325: ep_len:3579 episode reward: total was -40.760000. running mean: -29.576137\n",
      "ep 2325: ep_len:1245 episode reward: total was -53.860000. running mean: -29.818975\n",
      "ep 2325: ep_len:660 episode reward: total was 2.830000. running mean: -29.492486\n",
      "ep 2325: ep_len:508 episode reward: total was 5.690000. running mean: -29.140661\n",
      "ep 2325: ep_len:60 episode reward: total was 27.000000. running mean: -28.579254\n",
      "ep 2325: ep_len:500 episode reward: total was 12.430000. running mean: -28.169162\n",
      "ep 2325: ep_len:2831 episode reward: total was -16.480000. running mean: -28.052270\n",
      "ep 2325: ep_len:65 episode reward: total was 31.000000. running mean: -27.461747\n",
      "epsilon:0.009992 episode_count: 35035. steps_count: 37616802.000000\n",
      "ep 2326: ep_len:810 episode reward: total was -27.420000. running mean: -27.461330\n",
      "ep 2326: ep_len:784 episode reward: total was 4.400000. running mean: -27.142717\n",
      "ep 2326: ep_len:2809 episode reward: total was -25.640000. running mean: -27.127689\n",
      "ep 2326: ep_len:658 episode reward: total was 3.380000. running mean: -26.822613\n",
      "ep 2326: ep_len:51 episode reward: total was 22.500000. running mean: -26.329386\n",
      "ep 2326: ep_len:500 episode reward: total was 13.270000. running mean: -25.933393\n",
      "ep 2326: ep_len:327 episode reward: total was 16.890000. running mean: -25.505159\n",
      "ep 2326: ep_len:563 episode reward: total was -2.620000. running mean: -25.276307\n",
      "ep 2326: ep_len:735 episode reward: total was -22.110000. running mean: -25.244644\n",
      "ep 2326: ep_len:1163 episode reward: total was 9.820000. running mean: -24.893998\n",
      "ep 2326: ep_len:92 episode reward: total was 44.500000. running mean: -24.200058\n",
      "ep 2326: ep_len:81 episode reward: total was 39.000000. running mean: -23.568057\n",
      "ep 2326: ep_len:839 episode reward: total was -9.000000. running mean: -23.422376\n",
      "ep 2326: ep_len:2817 episode reward: total was -11.360000. running mean: -23.301753\n",
      "epsilon:0.009992 episode_count: 35049. steps_count: 37629031.000000\n",
      "ep 2327: ep_len:1426 episode reward: total was 12.650000. running mean: -22.942235\n",
      "ep 2327: ep_len:216 episode reward: total was 9.990000. running mean: -22.612913\n",
      "ep 2327: ep_len:2991 episode reward: total was 6.400000. running mean: -22.322784\n",
      "ep 2327: ep_len:1440 episode reward: total was 12.150000. running mean: -21.978056\n",
      "ep 2327: ep_len:151 episode reward: total was 74.000000. running mean: -21.018275\n",
      "ep 2327: ep_len:1010 episode reward: total was -29.030000. running mean: -21.098392\n",
      "ep 2327: ep_len:3646 episode reward: total was -36.110000. running mean: -21.248509\n",
      "ep 2327: ep_len:585 episode reward: total was -24.250000. running mean: -21.278523\n",
      "ep 2327: ep_len:735 episode reward: total was -99.440000. running mean: -22.060138\n",
      "ep 2327: ep_len:591 episode reward: total was 2.430000. running mean: -21.815237\n",
      "ep 2327: ep_len:128 episode reward: total was 59.500000. running mean: -21.002084\n",
      "ep 2327: ep_len:67 episode reward: total was 32.000000. running mean: -20.472064\n",
      "ep 2327: ep_len:1144 episode reward: total was -13.520000. running mean: -20.402543\n",
      "ep 2327: ep_len:2819 episode reward: total was -8.430000. running mean: -20.282818\n",
      "epsilon:0.009992 episode_count: 35063. steps_count: 37645980.000000\n",
      "ep 2328: ep_len:601 episode reward: total was 19.700000. running mean: -19.882989\n",
      "ep 2328: ep_len:770 episode reward: total was -14.750000. running mean: -19.831660\n",
      "ep 2328: ep_len:2919 episode reward: total was -43.760000. running mean: -20.070943\n",
      "ep 2328: ep_len:1002 episode reward: total was -94.830000. running mean: -20.818533\n",
      "ep 2328: ep_len:62 episode reward: total was 28.000000. running mean: -20.330348\n",
      "ep 2328: ep_len:1414 episode reward: total was -196.600000. running mean: -22.093045\n",
      "ep 2328: ep_len:3825 episode reward: total was -144.350000. running mean: -23.315614\n",
      "ep 2328: ep_len:1317 episode reward: total was -48.610000. running mean: -23.568558\n",
      "ep 2328: ep_len:659 episode reward: total was 6.820000. running mean: -23.264673\n",
      "ep 2328: ep_len:670 episode reward: total was -10.190000. running mean: -23.133926\n",
      "ep 2328: ep_len:62 episode reward: total was 29.500000. running mean: -22.607587\n",
      "ep 2328: ep_len:770 episode reward: total was -47.580000. running mean: -22.857311\n",
      "ep 2328: ep_len:2820 episode reward: total was 17.660000. running mean: -22.452138\n",
      "epsilon:0.009992 episode_count: 35076. steps_count: 37662871.000000\n",
      "ep 2329: ep_len:668 episode reward: total was 1.850000. running mean: -22.209116\n",
      "ep 2329: ep_len:719 episode reward: total was -11.640000. running mean: -22.103425\n",
      "ep 2329: ep_len:3137 episode reward: total was -12.360000. running mean: -22.005991\n",
      "ep 2329: ep_len:516 episode reward: total was -12.210000. running mean: -21.908031\n",
      "ep 2329: ep_len:89 episode reward: total was 43.000000. running mean: -21.258951\n",
      "ep 2329: ep_len:881 episode reward: total was 46.880000. running mean: -20.577561\n",
      "ep 2329: ep_len:3876 episode reward: total was 1.220000. running mean: -20.359585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2329: ep_len:754 episode reward: total was -21.320000. running mean: -20.369190\n",
      "ep 2329: ep_len:786 episode reward: total was -11.980000. running mean: -20.285298\n",
      "ep 2329: ep_len:1075 episode reward: total was -16.440000. running mean: -20.246845\n",
      "ep 2329: ep_len:793 episode reward: total was -45.730000. running mean: -20.501676\n",
      "ep 2329: ep_len:2748 episode reward: total was -15.870000. running mean: -20.455359\n",
      "epsilon:0.009992 episode_count: 35088. steps_count: 37678913.000000\n",
      "ep 2330: ep_len:1410 episode reward: total was 6.210000. running mean: -20.188706\n",
      "ep 2330: ep_len:1630 episode reward: total was -45.360000. running mean: -20.440419\n",
      "ep 2330: ep_len:54 episode reward: total was 24.000000. running mean: -19.996015\n",
      "ep 2330: ep_len:2969 episode reward: total was -71.330000. running mean: -20.509355\n",
      "ep 2330: ep_len:1685 episode reward: total was -74.040000. running mean: -21.044661\n",
      "ep 2330: ep_len:85 episode reward: total was 41.000000. running mean: -20.424214\n",
      "ep 2330: ep_len:500 episode reward: total was 34.110000. running mean: -19.878872\n",
      "ep 2330: ep_len:611 episode reward: total was 4.720000. running mean: -19.632883\n",
      "ep 2330: ep_len:2646 episode reward: total was -230.250000. running mean: -21.739055\n",
      "ep 2330: ep_len:716 episode reward: total was 31.800000. running mean: -21.203664\n",
      "ep 2330: ep_len:713 episode reward: total was -34.890000. running mean: -21.340527\n",
      "ep 2330: ep_len:54 episode reward: total was 25.500000. running mean: -20.872122\n",
      "ep 2330: ep_len:127 episode reward: total was 60.500000. running mean: -20.058401\n",
      "ep 2330: ep_len:38 episode reward: total was 17.500000. running mean: -19.682817\n",
      "ep 2330: ep_len:1116 episode reward: total was -9.210000. running mean: -19.578089\n",
      "ep 2330: ep_len:2893 episode reward: total was 11.720000. running mean: -19.265108\n",
      "epsilon:0.009992 episode_count: 35104. steps_count: 37696160.000000\n",
      "ep 2331: ep_len:1445 episode reward: total was 18.500000. running mean: -18.887457\n",
      "ep 2331: ep_len:804 episode reward: total was 10.180000. running mean: -18.596782\n",
      "ep 2331: ep_len:3033 episode reward: total was -5.360000. running mean: -18.464414\n",
      "ep 2331: ep_len:663 episode reward: total was 0.790000. running mean: -18.271870\n",
      "ep 2331: ep_len:87 episode reward: total was 42.000000. running mean: -17.669152\n",
      "ep 2331: ep_len:98 episode reward: total was 46.000000. running mean: -17.032460\n",
      "ep 2331: ep_len:52 episode reward: total was 23.000000. running mean: -16.632135\n",
      "ep 2331: ep_len:500 episode reward: total was 15.680000. running mean: -16.309014\n",
      "ep 2331: ep_len:3921 episode reward: total was -2625.830000. running mean: -42.404224\n",
      "ep 2331: ep_len:920 episode reward: total was -24.300000. running mean: -42.223182\n",
      "ep 2331: ep_len:7385 episode reward: total was -212.480000. running mean: -43.925750\n",
      "ep 2331: ep_len:1131 episode reward: total was -4.010000. running mean: -43.526592\n",
      "ep 2331: ep_len:646 episode reward: total was -5.390000. running mean: -43.145226\n",
      "ep 2331: ep_len:2899 episode reward: total was -2.460000. running mean: -42.738374\n",
      "epsilon:0.009992 episode_count: 35118. steps_count: 37719744.000000\n",
      "ep 2332: ep_len:601 episode reward: total was 13.760000. running mean: -42.173390\n",
      "ep 2332: ep_len:773 episode reward: total was -1.280000. running mean: -41.764457\n",
      "ep 2332: ep_len:3004 episode reward: total was -1.830000. running mean: -41.365112\n",
      "ep 2332: ep_len:532 episode reward: total was -30.200000. running mean: -41.253461\n",
      "ep 2332: ep_len:61 episode reward: total was 29.000000. running mean: -40.550926\n",
      "ep 2332: ep_len:33 episode reward: total was 15.000000. running mean: -39.995417\n",
      "ep 2332: ep_len:790 episode reward: total was 31.510000. running mean: -39.280363\n",
      "ep 2332: ep_len:598 episode reward: total was 26.750000. running mean: -38.620059\n",
      "ep 2332: ep_len:1555 episode reward: total was -11.370000. running mean: -38.347559\n",
      "ep 2332: ep_len:756 episode reward: total was -25.080000. running mean: -38.214883\n",
      "ep 2332: ep_len:719 episode reward: total was 18.480000. running mean: -37.647934\n",
      "ep 2332: ep_len:1013 episode reward: total was -65.580000. running mean: -37.927255\n",
      "ep 2332: ep_len:2698 episode reward: total was -37.250000. running mean: -37.920482\n",
      "epsilon:0.009992 episode_count: 35131. steps_count: 37732877.000000\n",
      "ep 2333: ep_len:1508 episode reward: total was 19.010000. running mean: -37.351177\n",
      "ep 2333: ep_len:1000 episode reward: total was 44.400000. running mean: -36.533666\n",
      "ep 2333: ep_len:2986 episode reward: total was -25.420000. running mean: -36.422529\n",
      "ep 2333: ep_len:646 episode reward: total was 11.350000. running mean: -35.944804\n",
      "ep 2333: ep_len:155 episode reward: total was 76.000000. running mean: -34.825356\n",
      "ep 2333: ep_len:63 episode reward: total was 30.000000. running mean: -34.177102\n",
      "ep 2333: ep_len:1141 episode reward: total was 4.000000. running mean: -33.795331\n",
      "ep 2333: ep_len:3586 episode reward: total was -45.190000. running mean: -33.909278\n",
      "ep 2333: ep_len:876 episode reward: total was 22.670000. running mean: -33.343485\n",
      "ep 2333: ep_len:722 episode reward: total was -19.090000. running mean: -33.200950\n",
      "ep 2333: ep_len:1130 episode reward: total was -6.040000. running mean: -32.929341\n",
      "ep 2333: ep_len:37 episode reward: total was 17.000000. running mean: -32.430047\n",
      "ep 2333: ep_len:500 episode reward: total was 26.580000. running mean: -31.839947\n",
      "ep 2333: ep_len:2829 episode reward: total was -10.720000. running mean: -31.628747\n",
      "epsilon:0.009992 episode_count: 35145. steps_count: 37750056.000000\n",
      "ep 2334: ep_len:812 episode reward: total was -20.330000. running mean: -31.515760\n",
      "ep 2334: ep_len:500 episode reward: total was 12.800000. running mean: -31.072602\n",
      "ep 2334: ep_len:2891 episode reward: total was -48.880000. running mean: -31.250676\n",
      "ep 2334: ep_len:1598 episode reward: total was -47.490000. running mean: -31.413070\n",
      "ep 2334: ep_len:66 episode reward: total was 25.010000. running mean: -30.848839\n",
      "ep 2334: ep_len:1412 episode reward: total was -213.350000. running mean: -32.673850\n",
      "ep 2334: ep_len:318 episode reward: total was -56.410000. running mean: -32.911212\n",
      "ep 2334: ep_len:582 episode reward: total was 1.800000. running mean: -32.564100\n",
      "ep 2334: ep_len:798 episode reward: total was 2.100000. running mean: -32.217459\n",
      "ep 2334: ep_len:1486 episode reward: total was -7.320000. running mean: -31.968484\n",
      "ep 2334: ep_len:55 episode reward: total was 26.000000. running mean: -31.388799\n",
      "ep 2334: ep_len:49 episode reward: total was 23.000000. running mean: -30.844911\n",
      "ep 2334: ep_len:747 episode reward: total was -27.250000. running mean: -30.808962\n",
      "ep 2334: ep_len:2935 episode reward: total was -43.270000. running mean: -30.933573\n",
      "epsilon:0.009992 episode_count: 35159. steps_count: 37764305.000000\n",
      "ep 2335: ep_len:857 episode reward: total was 23.520000. running mean: -30.389037\n",
      "ep 2335: ep_len:216 episode reward: total was 7.420000. running mean: -30.010947\n",
      "ep 2335: ep_len:2887 episode reward: total was -113.190000. running mean: -30.842737\n",
      "ep 2335: ep_len:661 episode reward: total was 8.050000. running mean: -30.453810\n",
      "ep 2335: ep_len:966 episode reward: total was 0.400000. running mean: -30.145272\n",
      "ep 2335: ep_len:500 episode reward: total was 21.230000. running mean: -29.631519\n",
      "ep 2335: ep_len:967 episode reward: total was -23.080000. running mean: -29.566004\n",
      "ep 2335: ep_len:832 episode reward: total was 16.000000. running mean: -29.110344\n",
      "ep 2335: ep_len:645 episode reward: total was -1.800000. running mean: -28.837240\n",
      "ep 2335: ep_len:47 episode reward: total was 22.000000. running mean: -28.328868\n",
      "ep 2335: ep_len:83 episode reward: total was 38.500000. running mean: -27.660579\n",
      "ep 2335: ep_len:1089 episode reward: total was 7.020000. running mean: -27.313773\n",
      "ep 2335: ep_len:2811 episode reward: total was -6.980000. running mean: -27.110436\n",
      "epsilon:0.009992 episode_count: 35172. steps_count: 37776866.000000\n",
      "ep 2336: ep_len:652 episode reward: total was 25.910000. running mean: -26.580231\n",
      "ep 2336: ep_len:500 episode reward: total was 20.850000. running mean: -26.105929\n",
      "ep 2336: ep_len:2967 episode reward: total was -52.740000. running mean: -26.372270\n",
      "ep 2336: ep_len:534 episode reward: total was -5.940000. running mean: -26.167947\n",
      "ep 2336: ep_len:61 episode reward: total was 26.000000. running mean: -25.646268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2336: ep_len:70 episode reward: total was 33.500000. running mean: -25.054805\n",
      "ep 2336: ep_len:1401 episode reward: total was -180.080000. running mean: -26.605057\n",
      "ep 2336: ep_len:3639 episode reward: total was -27.370000. running mean: -26.612706\n",
      "ep 2336: ep_len:643 episode reward: total was -38.150000. running mean: -26.728079\n",
      "ep 2336: ep_len:827 episode reward: total was 11.750000. running mean: -26.343298\n",
      "ep 2336: ep_len:3341 episode reward: total was -626.780000. running mean: -32.347665\n",
      "ep 2336: ep_len:93 episode reward: total was 45.000000. running mean: -31.574189\n",
      "ep 2336: ep_len:71 episode reward: total was 34.000000. running mean: -30.918447\n",
      "ep 2336: ep_len:919 episode reward: total was 13.460000. running mean: -30.474662\n",
      "ep 2336: ep_len:2836 episode reward: total was -5.570000. running mean: -30.225616\n",
      "ep 2336: ep_len:48 episode reward: total was 22.500000. running mean: -29.698360\n",
      "epsilon:0.009992 episode_count: 35188. steps_count: 37795468.000000\n",
      "ep 2337: ep_len:894 episode reward: total was -168.010000. running mean: -31.081476\n",
      "ep 2337: ep_len:500 episode reward: total was 3.730000. running mean: -30.733361\n",
      "ep 2337: ep_len:3198 episode reward: total was -61.070000. running mean: -31.036728\n",
      "ep 2337: ep_len:500 episode reward: total was -34.230000. running mean: -31.068660\n",
      "ep 2337: ep_len:99 episode reward: total was 46.500000. running mean: -30.292974\n",
      "ep 2337: ep_len:40 episode reward: total was 18.500000. running mean: -29.805044\n",
      "ep 2337: ep_len:1000 episode reward: total was -122.880000. running mean: -30.735794\n",
      "ep 2337: ep_len:3685 episode reward: total was -268.340000. running mean: -33.111836\n",
      "ep 2337: ep_len:573 episode reward: total was 31.330000. running mean: -32.467417\n",
      "ep 2337: ep_len:841 episode reward: total was 48.400000. running mean: -31.658743\n",
      "ep 2337: ep_len:1081 episode reward: total was -9.560000. running mean: -31.437756\n",
      "ep 2337: ep_len:86 episode reward: total was 41.500000. running mean: -30.708378\n",
      "ep 2337: ep_len:144 episode reward: total was 66.000000. running mean: -29.741294\n",
      "ep 2337: ep_len:37 episode reward: total was 17.000000. running mean: -29.273881\n",
      "ep 2337: ep_len:64 episode reward: total was 27.500000. running mean: -28.706143\n",
      "ep 2337: ep_len:500 episode reward: total was -8.110000. running mean: -28.500181\n",
      "ep 2337: ep_len:2896 episode reward: total was -1.480000. running mean: -28.229979\n",
      "ep 2337: ep_len:48 episode reward: total was 22.500000. running mean: -27.722680\n",
      "epsilon:0.009992 episode_count: 35206. steps_count: 37811654.000000\n",
      "ep 2338: ep_len:1473 episode reward: total was 16.130000. running mean: -27.284153\n",
      "ep 2338: ep_len:1242 episode reward: total was -38.250000. running mean: -27.393811\n",
      "ep 2338: ep_len:2980 episode reward: total was -68.830000. running mean: -27.808173\n",
      "ep 2338: ep_len:857 episode reward: total was 12.150000. running mean: -27.408591\n",
      "ep 2338: ep_len:105 episode reward: total was 48.000000. running mean: -26.654505\n",
      "ep 2338: ep_len:82 episode reward: total was 39.500000. running mean: -25.992960\n",
      "ep 2338: ep_len:1150 episode reward: total was -7.860000. running mean: -25.811631\n",
      "ep 2338: ep_len:345 episode reward: total was 15.050000. running mean: -25.403014\n",
      "ep 2338: ep_len:1591 episode reward: total was -9.720000. running mean: -25.246184\n",
      "ep 2338: ep_len:7266 episode reward: total was 61.660000. running mean: -24.377123\n",
      "ep 2338: ep_len:547 episode reward: total was 44.320000. running mean: -23.690151\n",
      "ep 2338: ep_len:45 episode reward: total was 19.500000. running mean: -23.258250\n",
      "ep 2338: ep_len:1520 episode reward: total was 11.850000. running mean: -22.907167\n",
      "ep 2338: ep_len:2907 episode reward: total was -4.210000. running mean: -22.720196\n",
      "epsilon:0.009992 episode_count: 35220. steps_count: 37833764.000000\n",
      "ep 2339: ep_len:1168 episode reward: total was 3.010000. running mean: -22.462894\n",
      "ep 2339: ep_len:3558 episode reward: total was -325.160000. running mean: -25.489865\n",
      "ep 2339: ep_len:2993 episode reward: total was -44.420000. running mean: -25.679166\n",
      "ep 2339: ep_len:1455 episode reward: total was 18.820000. running mean: -25.234174\n",
      "ep 2339: ep_len:47 episode reward: total was 20.500000. running mean: -24.776833\n",
      "ep 2339: ep_len:1179 episode reward: total was 14.300000. running mean: -24.386064\n",
      "ep 2339: ep_len:4099 episode reward: total was -59.230000. running mean: -24.734504\n",
      "ep 2339: ep_len:1646 episode reward: total was -192.710000. running mean: -26.414259\n",
      "ep 2339: ep_len:7307 episode reward: total was -76.610000. running mean: -26.916216\n",
      "ep 2339: ep_len:629 episode reward: total was -10.280000. running mean: -26.749854\n",
      "ep 2339: ep_len:95 episode reward: total was 44.500000. running mean: -26.037355\n",
      "ep 2339: ep_len:656 episode reward: total was -21.370000. running mean: -25.990682\n",
      "ep 2339: ep_len:2765 episode reward: total was 2.170000. running mean: -25.709075\n",
      "ep 2339: ep_len:52 episode reward: total was 24.500000. running mean: -25.206984\n",
      "epsilon:0.009992 episode_count: 35234. steps_count: 37861413.000000\n",
      "ep 2340: ep_len:920 episode reward: total was -87.320000. running mean: -25.828114\n",
      "ep 2340: ep_len:698 episode reward: total was -24.030000. running mean: -25.810133\n",
      "ep 2340: ep_len:77 episode reward: total was 37.000000. running mean: -25.182032\n",
      "ep 2340: ep_len:2990 episode reward: total was -44.050000. running mean: -25.370712\n",
      "ep 2340: ep_len:1458 episode reward: total was 13.310000. running mean: -24.983904\n",
      "ep 2340: ep_len:43 episode reward: total was 20.000000. running mean: -24.534065\n",
      "ep 2340: ep_len:1442 episode reward: total was -11.340000. running mean: -24.402125\n",
      "ep 2340: ep_len:343 episode reward: total was 7.960000. running mean: -24.078504\n",
      "ep 2340: ep_len:589 episode reward: total was -35.690000. running mean: -24.194619\n",
      "ep 2340: ep_len:603 episode reward: total was -31.140000. running mean: -24.264072\n",
      "ep 2340: ep_len:1019 episode reward: total was 14.220000. running mean: -23.879232\n",
      "ep 2340: ep_len:178 episode reward: total was 86.000000. running mean: -22.780439\n",
      "ep 2340: ep_len:802 episode reward: total was 3.770000. running mean: -22.514935\n",
      "ep 2340: ep_len:2860 episode reward: total was -19.960000. running mean: -22.489386\n",
      "epsilon:0.009992 episode_count: 35248. steps_count: 37875435.000000\n",
      "ep 2341: ep_len:1056 episode reward: total was -4.210000. running mean: -22.306592\n",
      "ep 2341: ep_len:642 episode reward: total was 5.880000. running mean: -22.024726\n",
      "ep 2341: ep_len:3064 episode reward: total was -2.570000. running mean: -21.830179\n",
      "ep 2341: ep_len:1216 episode reward: total was -40.530000. running mean: -22.017177\n",
      "ep 2341: ep_len:18 episode reward: total was 7.500000. running mean: -21.722005\n",
      "ep 2341: ep_len:1140 episode reward: total was -16.470000. running mean: -21.669485\n",
      "ep 2341: ep_len:617 episode reward: total was 28.260000. running mean: -21.170190\n",
      "ep 2341: ep_len:500 episode reward: total was 0.240000. running mean: -20.956088\n",
      "ep 2341: ep_len:771 episode reward: total was 28.350000. running mean: -20.463027\n",
      "ep 2341: ep_len:1498 episode reward: total was -5.330000. running mean: -20.311697\n",
      "ep 2341: ep_len:56 episode reward: total was 23.500000. running mean: -19.873580\n",
      "ep 2341: ep_len:46 episode reward: total was 21.500000. running mean: -19.459844\n",
      "ep 2341: ep_len:70 episode reward: total was 33.500000. running mean: -18.930246\n",
      "ep 2341: ep_len:1059 episode reward: total was -60.790000. running mean: -19.348843\n",
      "ep 2341: ep_len:2848 episode reward: total was -26.810000. running mean: -19.423455\n",
      "ep 2341: ep_len:57 episode reward: total was 27.000000. running mean: -18.959220\n",
      "epsilon:0.009992 episode_count: 35264. steps_count: 37890093.000000\n",
      "ep 2342: ep_len:891 episode reward: total was 13.320000. running mean: -18.636428\n",
      "ep 2342: ep_len:1161 episode reward: total was -23.850000. running mean: -18.688564\n",
      "ep 2342: ep_len:2931 episode reward: total was -95.730000. running mean: -19.458978\n",
      "ep 2342: ep_len:678 episode reward: total was 0.320000. running mean: -19.261188\n",
      "ep 2342: ep_len:64 episode reward: total was 30.500000. running mean: -18.763577\n",
      "ep 2342: ep_len:1021 episode reward: total was -50.190000. running mean: -19.077841\n",
      "ep 2342: ep_len:3792 episode reward: total was -82.440000. running mean: -19.711462\n",
      "ep 2342: ep_len:1245 episode reward: total was -52.360000. running mean: -20.037948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2342: ep_len:7310 episode reward: total was 10.100000. running mean: -19.736568\n",
      "ep 2342: ep_len:500 episode reward: total was 14.150000. running mean: -19.397703\n",
      "ep 2342: ep_len:77 episode reward: total was 37.000000. running mean: -18.833726\n",
      "ep 2342: ep_len:1217 episode reward: total was 8.370000. running mean: -18.561688\n",
      "ep 2342: ep_len:2778 episode reward: total was 10.080000. running mean: -18.275271\n",
      "ep 2342: ep_len:59 episode reward: total was 28.000000. running mean: -17.812519\n",
      "epsilon:0.009992 episode_count: 35278. steps_count: 37913817.000000\n",
      "ep 2343: ep_len:1097 episode reward: total was -7.870000. running mean: -17.713094\n",
      "ep 2343: ep_len:755 episode reward: total was -2.440000. running mean: -17.560363\n",
      "ep 2343: ep_len:48 episode reward: total was 18.000000. running mean: -17.204759\n",
      "ep 2343: ep_len:2938 episode reward: total was -7.180000. running mean: -17.104511\n",
      "ep 2343: ep_len:598 episode reward: total was -10.350000. running mean: -17.036966\n",
      "ep 2343: ep_len:75 episode reward: total was 36.000000. running mean: -16.506597\n",
      "ep 2343: ep_len:61 episode reward: total was 27.500000. running mean: -16.066531\n",
      "ep 2343: ep_len:693 episode reward: total was 5.970000. running mean: -15.846165\n",
      "ep 2343: ep_len:3662 episode reward: total was -9.630000. running mean: -15.784004\n",
      "ep 2343: ep_len:721 episode reward: total was 0.260000. running mean: -15.623564\n",
      "ep 2343: ep_len:765 episode reward: total was 16.550000. running mean: -15.301828\n",
      "ep 2343: ep_len:1185 episode reward: total was 1.830000. running mean: -15.130510\n",
      "ep 2343: ep_len:618 episode reward: total was -8.860000. running mean: -15.067805\n",
      "ep 2343: ep_len:2789 episode reward: total was -67.070000. running mean: -15.587827\n",
      "ep 2343: ep_len:29 episode reward: total was 13.000000. running mean: -15.301948\n",
      "epsilon:0.009992 episode_count: 35293. steps_count: 37929851.000000\n",
      "ep 2344: ep_len:948 episode reward: total was -86.640000. running mean: -16.015329\n",
      "ep 2344: ep_len:958 episode reward: total was 29.860000. running mean: -15.556576\n",
      "ep 2344: ep_len:2978 episode reward: total was 3.730000. running mean: -15.363710\n",
      "ep 2344: ep_len:707 episode reward: total was 12.480000. running mean: -15.085273\n",
      "ep 2344: ep_len:120 episode reward: total was 57.000000. running mean: -14.364420\n",
      "ep 2344: ep_len:1488 episode reward: total was 3.050000. running mean: -14.190276\n",
      "ep 2344: ep_len:4108 episode reward: total was -69.330000. running mean: -14.741673\n",
      "ep 2344: ep_len:915 episode reward: total was -24.350000. running mean: -14.837756\n",
      "ep 2344: ep_len:614 episode reward: total was -10.070000. running mean: -14.790079\n",
      "ep 2344: ep_len:577 episode reward: total was 32.500000. running mean: -14.317178\n",
      "ep 2344: ep_len:95 episode reward: total was 46.000000. running mean: -13.714006\n",
      "ep 2344: ep_len:56 episode reward: total was 26.500000. running mean: -13.311866\n",
      "ep 2344: ep_len:126 episode reward: total was 57.000000. running mean: -12.608747\n",
      "ep 2344: ep_len:1449 episode reward: total was -7.260000. running mean: -12.555260\n",
      "ep 2344: ep_len:2864 episode reward: total was -19.360000. running mean: -12.623307\n",
      "epsilon:0.009992 episode_count: 35308. steps_count: 37947854.000000\n",
      "ep 2345: ep_len:655 episode reward: total was -1.700000. running mean: -12.514074\n",
      "ep 2345: ep_len:500 episode reward: total was 1.100000. running mean: -12.377934\n",
      "ep 2345: ep_len:51 episode reward: total was 24.000000. running mean: -12.014154\n",
      "ep 2345: ep_len:2929 episode reward: total was -16.240000. running mean: -12.056413\n",
      "ep 2345: ep_len:500 episode reward: total was 23.400000. running mean: -11.701849\n",
      "ep 2345: ep_len:154 episode reward: total was 75.500000. running mean: -10.829830\n",
      "ep 2345: ep_len:50 episode reward: total was 22.000000. running mean: -10.501532\n",
      "ep 2345: ep_len:1381 episode reward: total was -9.380000. running mean: -10.490316\n",
      "ep 2345: ep_len:500 episode reward: total was 18.510000. running mean: -10.200313\n",
      "ep 2345: ep_len:1177 episode reward: total was -50.930000. running mean: -10.607610\n",
      "ep 2345: ep_len:668 episode reward: total was 7.160000. running mean: -10.429934\n",
      "ep 2345: ep_len:584 episode reward: total was 43.070000. running mean: -9.894935\n",
      "ep 2345: ep_len:74 episode reward: total was 34.000000. running mean: -9.455985\n",
      "ep 2345: ep_len:44 episode reward: total was 20.500000. running mean: -9.156425\n",
      "ep 2345: ep_len:1139 episode reward: total was -17.980000. running mean: -9.244661\n",
      "ep 2345: ep_len:2847 episode reward: total was 19.860000. running mean: -8.953615\n",
      "epsilon:0.009992 episode_count: 35324. steps_count: 37961107.000000\n",
      "ep 2346: ep_len:904 episode reward: total was -64.430000. running mean: -9.508378\n",
      "ep 2346: ep_len:692 episode reward: total was -33.130000. running mean: -9.744595\n",
      "ep 2346: ep_len:2920 episode reward: total was -4.650000. running mean: -9.693649\n",
      "ep 2346: ep_len:500 episode reward: total was -7.930000. running mean: -9.676012\n",
      "ep 2346: ep_len:57 episode reward: total was 27.000000. running mean: -9.309252\n",
      "ep 2346: ep_len:1448 episode reward: total was -140.450000. running mean: -10.620660\n",
      "ep 2346: ep_len:341 episode reward: total was 29.150000. running mean: -10.222953\n",
      "ep 2346: ep_len:550 episode reward: total was -4.770000. running mean: -10.168423\n",
      "ep 2346: ep_len:740 episode reward: total was 20.740000. running mean: -9.859339\n",
      "ep 2346: ep_len:702 episode reward: total was 47.550000. running mean: -9.285246\n",
      "ep 2346: ep_len:90 episode reward: total was 40.500000. running mean: -8.787393\n",
      "ep 2346: ep_len:79 episode reward: total was 36.500000. running mean: -8.334519\n",
      "ep 2346: ep_len:1105 episode reward: total was 21.840000. running mean: -8.032774\n",
      "ep 2346: ep_len:2877 episode reward: total was 3.260000. running mean: -7.919847\n",
      "epsilon:0.009992 episode_count: 35338. steps_count: 37974112.000000\n",
      "ep 2347: ep_len:698 episode reward: total was -11.120000. running mean: -7.951848\n",
      "ep 2347: ep_len:703 episode reward: total was -13.170000. running mean: -8.004030\n",
      "ep 2347: ep_len:71 episode reward: total was 31.000000. running mean: -7.613989\n",
      "ep 2347: ep_len:2992 episode reward: total was -81.800000. running mean: -8.355849\n",
      "ep 2347: ep_len:603 episode reward: total was -10.300000. running mean: -8.375291\n",
      "ep 2347: ep_len:146 episode reward: total was 71.500000. running mean: -7.576538\n",
      "ep 2347: ep_len:875 episode reward: total was 29.800000. running mean: -7.202773\n",
      "ep 2347: ep_len:3643 episode reward: total was -86.060000. running mean: -7.991345\n",
      "ep 2347: ep_len:921 episode reward: total was -14.100000. running mean: -8.052431\n",
      "ep 2347: ep_len:721 episode reward: total was -29.170000. running mean: -8.263607\n",
      "ep 2347: ep_len:1540 episode reward: total was 32.320000. running mean: -7.857771\n",
      "ep 2347: ep_len:39 episode reward: total was 16.500000. running mean: -7.614193\n",
      "ep 2347: ep_len:137 episode reward: total was 65.500000. running mean: -6.883051\n",
      "ep 2347: ep_len:72 episode reward: total was 33.000000. running mean: -6.484221\n",
      "ep 2347: ep_len:500 episode reward: total was 29.120000. running mean: -6.128179\n",
      "ep 2347: ep_len:2823 episode reward: total was 19.280000. running mean: -5.874097\n",
      "epsilon:0.009992 episode_count: 35354. steps_count: 37990596.000000\n",
      "ep 2348: ep_len:670 episode reward: total was -13.670000. running mean: -5.952056\n",
      "ep 2348: ep_len:941 episode reward: total was 21.270000. running mean: -5.679835\n",
      "ep 2348: ep_len:68 episode reward: total was 28.000000. running mean: -5.343037\n",
      "ep 2348: ep_len:2957 episode reward: total was -15.070000. running mean: -5.440307\n",
      "ep 2348: ep_len:805 episode reward: total was 17.170000. running mean: -5.214204\n",
      "ep 2348: ep_len:112 episode reward: total was 48.500000. running mean: -4.677062\n",
      "ep 2348: ep_len:73 episode reward: total was 35.000000. running mean: -4.280291\n",
      "ep 2348: ep_len:922 episode reward: total was -22.320000. running mean: -4.460688\n",
      "ep 2348: ep_len:3830 episode reward: total was -42.390000. running mean: -4.839981\n",
      "ep 2348: ep_len:522 episode reward: total was 16.260000. running mean: -4.628981\n",
      "ep 2348: ep_len:870 episode reward: total was 57.840000. running mean: -4.004292\n",
      "ep 2348: ep_len:1108 episode reward: total was 16.430000. running mean: -3.799949\n",
      "ep 2348: ep_len:1419 episode reward: total was 15.180000. running mean: -3.610149\n",
      "ep 2348: ep_len:2854 episode reward: total was -0.980000. running mean: -3.583848\n",
      "epsilon:0.009992 episode_count: 35368. steps_count: 38007747.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2349: ep_len:609 episode reward: total was -17.720000. running mean: -3.725209\n",
      "ep 2349: ep_len:500 episode reward: total was 1.440000. running mean: -3.673557\n",
      "ep 2349: ep_len:2903 episode reward: total was -5.470000. running mean: -3.691521\n",
      "ep 2349: ep_len:691 episode reward: total was -5.270000. running mean: -3.707306\n",
      "ep 2349: ep_len:74 episode reward: total was 35.500000. running mean: -3.315233\n",
      "ep 2349: ep_len:500 episode reward: total was -33.210000. running mean: -3.614181\n",
      "ep 2349: ep_len:4138 episode reward: total was -113.780000. running mean: -4.715839\n",
      "ep 2349: ep_len:891 episode reward: total was -13.480000. running mean: -4.803481\n",
      "ep 2349: ep_len:880 episode reward: total was 38.110000. running mean: -4.374346\n",
      "ep 2349: ep_len:894 episode reward: total was 12.600000. running mean: -4.204602\n",
      "ep 2349: ep_len:173 episode reward: total was 85.000000. running mean: -3.312556\n",
      "ep 2349: ep_len:108 episode reward: total was 51.000000. running mean: -2.769431\n",
      "ep 2349: ep_len:775 episode reward: total was -15.650000. running mean: -2.898237\n",
      "ep 2349: ep_len:2847 episode reward: total was -13.600000. running mean: -3.005254\n",
      "epsilon:0.009992 episode_count: 35382. steps_count: 38023730.000000\n",
      "ep 2350: ep_len:975 episode reward: total was -153.620000. running mean: -4.511402\n",
      "ep 2350: ep_len:500 episode reward: total was 28.810000. running mean: -4.178188\n",
      "ep 2350: ep_len:2890 episode reward: total was -31.010000. running mean: -4.446506\n",
      "ep 2350: ep_len:500 episode reward: total was 8.080000. running mean: -4.321241\n",
      "ep 2350: ep_len:914 episode reward: total was 58.660000. running mean: -3.691428\n",
      "ep 2350: ep_len:3918 episode reward: total was -98.340000. running mean: -4.637914\n",
      "ep 2350: ep_len:597 episode reward: total was 5.960000. running mean: -4.531935\n",
      "ep 2350: ep_len:677 episode reward: total was 0.580000. running mean: -4.480815\n",
      "ep 2350: ep_len:500 episode reward: total was 11.490000. running mean: -4.321107\n",
      "ep 2350: ep_len:1096 episode reward: total was -17.980000. running mean: -4.457696\n",
      "ep 2350: ep_len:2884 episode reward: total was -1.690000. running mean: -4.430019\n",
      "ep 2350: ep_len:58 episode reward: total was 27.500000. running mean: -4.110719\n",
      "epsilon:0.009992 episode_count: 35394. steps_count: 38039239.000000\n",
      "ep 2351: ep_len:619 episode reward: total was 16.360000. running mean: -3.906012\n",
      "ep 2351: ep_len:733 episode reward: total was -22.330000. running mean: -4.090252\n",
      "ep 2351: ep_len:50 episode reward: total was 23.500000. running mean: -3.814349\n",
      "ep 2351: ep_len:3104 episode reward: total was -63.440000. running mean: -4.410606\n",
      "ep 2351: ep_len:700 episode reward: total was -19.470000. running mean: -4.561200\n",
      "ep 2351: ep_len:122 episode reward: total was 59.500000. running mean: -3.920588\n",
      "ep 2351: ep_len:105 episode reward: total was 51.000000. running mean: -3.371382\n",
      "ep 2351: ep_len:53 episode reward: total was 23.500000. running mean: -3.102668\n",
      "ep 2351: ep_len:620 episode reward: total was 44.570000. running mean: -2.625941\n",
      "ep 2351: ep_len:367 episode reward: total was 10.220000. running mean: -2.497482\n",
      "ep 2351: ep_len:1246 episode reward: total was -95.930000. running mean: -3.431807\n",
      "ep 2351: ep_len:717 episode reward: total was 9.030000. running mean: -3.307189\n",
      "ep 2351: ep_len:1063 episode reward: total was 33.510000. running mean: -2.939017\n",
      "ep 2351: ep_len:70 episode reward: total was 33.500000. running mean: -2.574627\n",
      "ep 2351: ep_len:621 episode reward: total was 5.570000. running mean: -2.493181\n",
      "ep 2351: ep_len:2850 episode reward: total was -32.180000. running mean: -2.790049\n",
      "ep 2351: ep_len:58 episode reward: total was 27.500000. running mean: -2.487148\n",
      "epsilon:0.009992 episode_count: 35411. steps_count: 38052337.000000\n",
      "ep 2352: ep_len:1447 episode reward: total was 14.390000. running mean: -2.318377\n",
      "ep 2352: ep_len:698 episode reward: total was -92.050000. running mean: -3.215693\n",
      "ep 2352: ep_len:68 episode reward: total was 32.500000. running mean: -2.858536\n",
      "ep 2352: ep_len:3028 episode reward: total was -58.790000. running mean: -3.417851\n",
      "ep 2352: ep_len:500 episode reward: total was -13.320000. running mean: -3.516872\n",
      "ep 2352: ep_len:159 episode reward: total was 76.500000. running mean: -2.716704\n",
      "ep 2352: ep_len:69 episode reward: total was 33.000000. running mean: -2.359537\n",
      "ep 2352: ep_len:1084 episode reward: total was 16.370000. running mean: -2.172241\n",
      "ep 2352: ep_len:679 episode reward: total was 26.860000. running mean: -1.881919\n",
      "ep 2352: ep_len:1481 episode reward: total was -47.060000. running mean: -2.333700\n",
      "ep 2352: ep_len:676 episode reward: total was 41.690000. running mean: -1.893463\n",
      "ep 2352: ep_len:623 episode reward: total was -17.170000. running mean: -2.046228\n",
      "ep 2352: ep_len:157 episode reward: total was 77.000000. running mean: -1.255766\n",
      "ep 2352: ep_len:36 episode reward: total was 16.500000. running mean: -1.078208\n",
      "ep 2352: ep_len:1202 episode reward: total was 0.810000. running mean: -1.059326\n",
      "ep 2352: ep_len:2880 episode reward: total was -5.370000. running mean: -1.102433\n",
      "ep 2352: ep_len:56 episode reward: total was 26.500000. running mean: -0.826408\n",
      "epsilon:0.009992 episode_count: 35428. steps_count: 38067180.000000\n",
      "ep 2353: ep_len:500 episode reward: total was 10.260000. running mean: -0.715544\n",
      "ep 2353: ep_len:965 episode reward: total was 22.490000. running mean: -0.483489\n",
      "ep 2353: ep_len:45 episode reward: total was 21.000000. running mean: -0.268654\n",
      "ep 2353: ep_len:2980 episode reward: total was -111.700000. running mean: -1.382967\n",
      "ep 2353: ep_len:500 episode reward: total was 2.610000. running mean: -1.343038\n",
      "ep 2353: ep_len:71 episode reward: total was 34.000000. running mean: -0.989607\n",
      "ep 2353: ep_len:1018 episode reward: total was -29.380000. running mean: -1.273511\n",
      "ep 2353: ep_len:3540 episode reward: total was -43.110000. running mean: -1.691876\n",
      "ep 2353: ep_len:780 episode reward: total was -47.010000. running mean: -2.145057\n",
      "ep 2353: ep_len:842 episode reward: total was 52.880000. running mean: -1.594807\n",
      "ep 2353: ep_len:732 episode reward: total was -25.660000. running mean: -1.835459\n",
      "ep 2353: ep_len:190 episode reward: total was 92.000000. running mean: -0.897104\n",
      "ep 2353: ep_len:123 episode reward: total was 60.000000. running mean: -0.288133\n",
      "ep 2353: ep_len:1164 episode reward: total was 13.350000. running mean: -0.151752\n",
      "ep 2353: ep_len:2858 episode reward: total was 4.480000. running mean: -0.105434\n",
      "epsilon:0.009992 episode_count: 35443. steps_count: 38083488.000000\n",
      "ep 2354: ep_len:671 episode reward: total was -28.350000. running mean: -0.387880\n",
      "ep 2354: ep_len:500 episode reward: total was -2.050000. running mean: -0.404501\n",
      "ep 2354: ep_len:2957 episode reward: total was -11.240000. running mean: -0.512856\n",
      "ep 2354: ep_len:674 episode reward: total was -27.780000. running mean: -0.785528\n",
      "ep 2354: ep_len:79 episode reward: total was 36.500000. running mean: -0.412672\n",
      "ep 2354: ep_len:1661 episode reward: total was -567.340000. running mean: -6.081946\n",
      "ep 2354: ep_len:4065 episode reward: total was -71.260000. running mean: -6.733726\n",
      "ep 2354: ep_len:2852 episode reward: total was -883.000000. running mean: -15.496389\n",
      "ep 2354: ep_len:892 episode reward: total was 64.800000. running mean: -14.693425\n",
      "ep 2354: ep_len:591 episode reward: total was -7.390000. running mean: -14.620391\n",
      "ep 2354: ep_len:75 episode reward: total was 34.500000. running mean: -14.129187\n",
      "ep 2354: ep_len:1482 episode reward: total was 29.410000. running mean: -13.693795\n",
      "ep 2354: ep_len:2805 episode reward: total was 17.540000. running mean: -13.381457\n",
      "ep 2354: ep_len:55 episode reward: total was 24.500000. running mean: -13.002642\n",
      "epsilon:0.009992 episode_count: 35457. steps_count: 38102847.000000\n",
      "ep 2355: ep_len:500 episode reward: total was 42.070000. running mean: -12.451916\n",
      "ep 2355: ep_len:500 episode reward: total was 33.650000. running mean: -11.990897\n",
      "ep 2355: ep_len:2927 episode reward: total was -3.140000. running mean: -11.902388\n",
      "ep 2355: ep_len:650 episode reward: total was 6.330000. running mean: -11.720064\n",
      "ep 2355: ep_len:45 episode reward: total was 19.500000. running mean: -11.407863\n",
      "ep 2355: ep_len:74 episode reward: total was 34.000000. running mean: -10.953785\n",
      "ep 2355: ep_len:854 episode reward: total was 35.980000. running mean: -10.484447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2355: ep_len:500 episode reward: total was 19.210000. running mean: -10.187502\n",
      "ep 2355: ep_len:950 episode reward: total was -29.050000. running mean: -10.376127\n",
      "ep 2355: ep_len:708 episode reward: total was 41.710000. running mean: -9.855266\n",
      "ep 2355: ep_len:622 episode reward: total was 11.280000. running mean: -9.643913\n",
      "ep 2355: ep_len:39 episode reward: total was 18.000000. running mean: -9.367474\n",
      "ep 2355: ep_len:61 episode reward: total was 27.500000. running mean: -8.998800\n",
      "ep 2355: ep_len:61 episode reward: total was 29.000000. running mean: -8.618812\n",
      "ep 2355: ep_len:636 episode reward: total was -16.030000. running mean: -8.692923\n",
      "ep 2355: ep_len:2817 episode reward: total was 3.460000. running mean: -8.571394\n",
      "epsilon:0.009992 episode_count: 35473. steps_count: 38114791.000000\n",
      "ep 2356: ep_len:1087 episode reward: total was -4.940000. running mean: -8.535080\n",
      "ep 2356: ep_len:718 episode reward: total was -22.940000. running mean: -8.679130\n",
      "ep 2356: ep_len:3058 episode reward: total was -189.620000. running mean: -10.488538\n",
      "ep 2356: ep_len:669 episode reward: total was -16.320000. running mean: -10.546853\n",
      "ep 2356: ep_len:112 episode reward: total was 53.000000. running mean: -9.911384\n",
      "ep 2356: ep_len:96 episode reward: total was 45.000000. running mean: -9.362270\n",
      "ep 2356: ep_len:1021 episode reward: total was -50.160000. running mean: -9.770248\n",
      "ep 2356: ep_len:4054 episode reward: total was -96.620000. running mean: -10.638745\n",
      "ep 2356: ep_len:541 episode reward: total was -57.380000. running mean: -11.106158\n",
      "ep 2356: ep_len:751 episode reward: total was 18.650000. running mean: -10.808596\n",
      "ep 2356: ep_len:684 episode reward: total was 0.030000. running mean: -10.700210\n",
      "ep 2356: ep_len:683 episode reward: total was -0.270000. running mean: -10.595908\n",
      "ep 2356: ep_len:2815 episode reward: total was -19.330000. running mean: -10.683249\n",
      "epsilon:0.009992 episode_count: 35486. steps_count: 38131080.000000\n",
      "ep 2357: ep_len:758 episode reward: total was -90.560000. running mean: -11.482017\n",
      "ep 2357: ep_len:696 episode reward: total was 6.680000. running mean: -11.300396\n",
      "ep 2357: ep_len:64 episode reward: total was 27.500000. running mean: -10.912392\n",
      "ep 2357: ep_len:2847 episode reward: total was -70.160000. running mean: -11.504869\n",
      "ep 2357: ep_len:576 episode reward: total was -7.540000. running mean: -11.465220\n",
      "ep 2357: ep_len:156 episode reward: total was 75.000000. running mean: -10.600568\n",
      "ep 2357: ep_len:87 episode reward: total was 42.000000. running mean: -10.074562\n",
      "ep 2357: ep_len:1452 episode reward: total was -285.620000. running mean: -12.830016\n",
      "ep 2357: ep_len:308 episode reward: total was 14.340000. running mean: -12.558316\n",
      "ep 2357: ep_len:633 episode reward: total was -26.160000. running mean: -12.694333\n",
      "ep 2357: ep_len:829 episode reward: total was 34.930000. running mean: -12.218090\n",
      "ep 2357: ep_len:626 episode reward: total was 4.920000. running mean: -12.046709\n",
      "ep 2357: ep_len:48 episode reward: total was 22.500000. running mean: -11.701242\n",
      "ep 2357: ep_len:1525 episode reward: total was 11.990000. running mean: -11.464329\n",
      "ep 2357: ep_len:2775 episode reward: total was 4.930000. running mean: -11.300386\n",
      "epsilon:0.009992 episode_count: 35501. steps_count: 38144460.000000\n",
      "ep 2358: ep_len:957 episode reward: total was -60.020000. running mean: -11.787582\n",
      "ep 2358: ep_len:768 episode reward: total was -47.200000. running mean: -12.141706\n",
      "ep 2358: ep_len:30 episode reward: total was 12.000000. running mean: -11.900289\n",
      "ep 2358: ep_len:2967 episode reward: total was -50.660000. running mean: -12.287886\n",
      "ep 2358: ep_len:675 episode reward: total was -5.540000. running mean: -12.220408\n",
      "ep 2358: ep_len:132 episode reward: total was 64.500000. running mean: -11.453203\n",
      "ep 2358: ep_len:115 episode reward: total was 27.500000. running mean: -11.063671\n",
      "ep 2358: ep_len:40 episode reward: total was 17.000000. running mean: -10.783035\n",
      "ep 2358: ep_len:1423 episode reward: total was -85.810000. running mean: -11.533304\n",
      "ep 2358: ep_len:3624 episode reward: total was -38.720000. running mean: -11.805171\n",
      "ep 2358: ep_len:810 episode reward: total was -30.030000. running mean: -11.987420\n",
      "ep 2358: ep_len:721 episode reward: total was 25.800000. running mean: -11.609545\n",
      "ep 2358: ep_len:885 episode reward: total was 9.570000. running mean: -11.397750\n",
      "ep 2358: ep_len:173 episode reward: total was 64.000000. running mean: -10.643772\n",
      "ep 2358: ep_len:77 episode reward: total was 35.500000. running mean: -10.182335\n",
      "ep 2358: ep_len:722 episode reward: total was -37.390000. running mean: -10.454411\n",
      "ep 2358: ep_len:2889 episode reward: total was 10.760000. running mean: -10.242267\n",
      "ep 2358: ep_len:39 episode reward: total was 18.000000. running mean: -9.959845\n",
      "epsilon:0.009992 episode_count: 35519. steps_count: 38161507.000000\n",
      "ep 2359: ep_len:956 episode reward: total was -55.250000. running mean: -10.412746\n",
      "ep 2359: ep_len:793 episode reward: total was 2.260000. running mean: -10.286019\n",
      "ep 2359: ep_len:2965 episode reward: total was -65.670000. running mean: -10.839858\n",
      "ep 2359: ep_len:500 episode reward: total was 2.210000. running mean: -10.709360\n",
      "ep 2359: ep_len:51 episode reward: total was 24.000000. running mean: -10.362266\n",
      "ep 2359: ep_len:1166 episode reward: total was 12.180000. running mean: -10.136844\n",
      "ep 2359: ep_len:4048 episode reward: total was -18.820000. running mean: -10.223675\n",
      "ep 2359: ep_len:990 episode reward: total was -18.720000. running mean: -10.308638\n",
      "ep 2359: ep_len:751 episode reward: total was 33.260000. running mean: -9.872952\n",
      "ep 2359: ep_len:616 episode reward: total was -12.740000. running mean: -9.901623\n",
      "ep 2359: ep_len:83 episode reward: total was 40.000000. running mean: -9.402606\n",
      "ep 2359: ep_len:63 episode reward: total was 30.000000. running mean: -9.008580\n",
      "ep 2359: ep_len:1124 episode reward: total was -14.180000. running mean: -9.060294\n",
      "ep 2359: ep_len:2791 episode reward: total was -23.860000. running mean: -9.208292\n",
      "epsilon:0.009992 episode_count: 35533. steps_count: 38178404.000000\n",
      "ep 2360: ep_len:983 episode reward: total was -67.600000. running mean: -9.792209\n",
      "ep 2360: ep_len:729 episode reward: total was -16.740000. running mean: -9.861687\n",
      "ep 2360: ep_len:3007 episode reward: total was 10.420000. running mean: -9.658870\n",
      "ep 2360: ep_len:759 episode reward: total was -3.690000. running mean: -9.599181\n",
      "ep 2360: ep_len:55 episode reward: total was 26.000000. running mean: -9.243189\n",
      "ep 2360: ep_len:93 episode reward: total was 43.500000. running mean: -8.715757\n",
      "ep 2360: ep_len:72 episode reward: total was 34.500000. running mean: -8.283600\n",
      "ep 2360: ep_len:500 episode reward: total was 29.670000. running mean: -7.904064\n",
      "ep 2360: ep_len:650 episode reward: total was 35.170000. running mean: -7.473323\n",
      "ep 2360: ep_len:2859 episode reward: total was -182.670000. running mean: -9.225290\n",
      "ep 2360: ep_len:721 episode reward: total was 11.950000. running mean: -9.013537\n",
      "ep 2360: ep_len:987 episode reward: total was 15.060000. running mean: -8.772802\n",
      "ep 2360: ep_len:48 episode reward: total was 22.500000. running mean: -8.460074\n",
      "ep 2360: ep_len:85 episode reward: total was 41.000000. running mean: -7.965473\n",
      "ep 2360: ep_len:553 episode reward: total was 27.720000. running mean: -7.608618\n",
      "ep 2360: ep_len:2919 episode reward: total was -17.860000. running mean: -7.711132\n",
      "ep 2360: ep_len:46 episode reward: total was 18.500000. running mean: -7.449021\n",
      "epsilon:0.009992 episode_count: 35550. steps_count: 38193470.000000\n",
      "ep 2361: ep_len:890 episode reward: total was 27.860000. running mean: -7.095930\n",
      "ep 2361: ep_len:739 episode reward: total was -34.390000. running mean: -7.368871\n",
      "ep 2361: ep_len:2890 episode reward: total was -17.640000. running mean: -7.471582\n",
      "ep 2361: ep_len:500 episode reward: total was 36.630000. running mean: -7.030567\n",
      "ep 2361: ep_len:151 episode reward: total was 72.500000. running mean: -6.235261\n",
      "ep 2361: ep_len:46 episode reward: total was 21.500000. running mean: -5.957908\n",
      "ep 2361: ep_len:1093 episode reward: total was -11.890000. running mean: -6.017229\n",
      "ep 2361: ep_len:3666 episode reward: total was -12.070000. running mean: -6.077757\n",
      "ep 2361: ep_len:556 episode reward: total was 4.320000. running mean: -5.973779\n",
      "ep 2361: ep_len:650 episode reward: total was 12.980000. running mean: -5.784242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2361: ep_len:552 episode reward: total was -6.770000. running mean: -5.794099\n",
      "ep 2361: ep_len:95 episode reward: total was 44.500000. running mean: -5.291158\n",
      "ep 2361: ep_len:165 episode reward: total was 79.500000. running mean: -4.443247\n",
      "ep 2361: ep_len:49 episode reward: total was 23.000000. running mean: -4.168814\n",
      "ep 2361: ep_len:739 episode reward: total was -15.000000. running mean: -4.277126\n",
      "ep 2361: ep_len:2745 episode reward: total was -1.580000. running mean: -4.250155\n",
      "epsilon:0.009992 episode_count: 35566. steps_count: 38208996.000000\n",
      "ep 2362: ep_len:1423 episode reward: total was -0.940000. running mean: -4.217053\n",
      "ep 2362: ep_len:706 episode reward: total was -0.720000. running mean: -4.182083\n",
      "ep 2362: ep_len:2934 episode reward: total was -18.270000. running mean: -4.322962\n",
      "ep 2362: ep_len:500 episode reward: total was 53.650000. running mean: -3.743232\n",
      "ep 2362: ep_len:114 episode reward: total was 55.500000. running mean: -3.150800\n",
      "ep 2362: ep_len:1038 episode reward: total was -14.030000. running mean: -3.259592\n",
      "ep 2362: ep_len:319 episode reward: total was 27.520000. running mean: -2.951796\n",
      "ep 2362: ep_len:1293 episode reward: total was -149.790000. running mean: -4.420178\n",
      "ep 2362: ep_len:798 episode reward: total was 22.610000. running mean: -4.149876\n",
      "ep 2362: ep_len:923 episode reward: total was 21.430000. running mean: -3.894077\n",
      "ep 2362: ep_len:185 episode reward: total was 89.500000. running mean: -2.960137\n",
      "ep 2362: ep_len:62 episode reward: total was 29.500000. running mean: -2.635535\n",
      "ep 2362: ep_len:1047 episode reward: total was 23.490000. running mean: -2.374280\n",
      "ep 2362: ep_len:2871 episode reward: total was -32.910000. running mean: -2.679637\n",
      "epsilon:0.009992 episode_count: 35580. steps_count: 38223209.000000\n",
      "ep 2363: ep_len:2554 episode reward: total was -178.630000. running mean: -4.439141\n",
      "ep 2363: ep_len:663 episode reward: total was -12.610000. running mean: -4.520849\n",
      "ep 2363: ep_len:2928 episode reward: total was 3.380000. running mean: -4.441841\n",
      "ep 2363: ep_len:619 episode reward: total was -7.110000. running mean: -4.468522\n",
      "ep 2363: ep_len:59 episode reward: total was 28.000000. running mean: -4.143837\n",
      "ep 2363: ep_len:116 episode reward: total was 55.000000. running mean: -3.552399\n",
      "ep 2363: ep_len:59 episode reward: total was 28.000000. running mean: -3.236875\n",
      "ep 2363: ep_len:1429 episode reward: total was -274.740000. running mean: -5.951906\n",
      "ep 2363: ep_len:664 episode reward: total was 35.890000. running mean: -5.533487\n",
      "ep 2363: ep_len:1208 episode reward: total was -46.670000. running mean: -5.944852\n",
      "ep 2363: ep_len:879 episode reward: total was 62.400000. running mean: -5.261404\n",
      "ep 2363: ep_len:980 episode reward: total was 9.480000. running mean: -5.113990\n",
      "ep 2363: ep_len:45 episode reward: total was 19.500000. running mean: -4.867850\n",
      "ep 2363: ep_len:82 episode reward: total was 38.000000. running mean: -4.439171\n",
      "ep 2363: ep_len:1089 episode reward: total was 8.240000. running mean: -4.312380\n",
      "ep 2363: ep_len:2864 episode reward: total was -5.250000. running mean: -4.321756\n",
      "epsilon:0.009992 episode_count: 35596. steps_count: 38239447.000000\n",
      "ep 2364: ep_len:1085 episode reward: total was -5.480000. running mean: -4.333338\n",
      "ep 2364: ep_len:946 episode reward: total was 10.360000. running mean: -4.186405\n",
      "ep 2364: ep_len:87 episode reward: total was 40.500000. running mean: -3.739541\n",
      "ep 2364: ep_len:575 episode reward: total was 19.980000. running mean: -3.502345\n",
      "ep 2364: ep_len:1175 episode reward: total was 22.180000. running mean: -3.245522\n",
      "ep 2364: ep_len:343 episode reward: total was 22.100000. running mean: -2.992067\n",
      "ep 2364: ep_len:1556 episode reward: total was -19.350000. running mean: -3.155646\n",
      "ep 2364: ep_len:693 episode reward: total was 37.700000. running mean: -2.747090\n",
      "ep 2364: ep_len:591 episode reward: total was -15.470000. running mean: -2.874319\n",
      "ep 2364: ep_len:43 episode reward: total was 20.000000. running mean: -2.645575\n",
      "ep 2364: ep_len:1095 episode reward: total was 25.840000. running mean: -2.360720\n",
      "ep 2364: ep_len:2829 episode reward: total was -10.780000. running mean: -2.444912\n",
      "ep 2364: ep_len:46 episode reward: total was 21.500000. running mean: -2.205463\n",
      "epsilon:0.009992 episode_count: 35609. steps_count: 38250511.000000\n",
      "ep 2365: ep_len:2552 episode reward: total was -190.770000. running mean: -4.091109\n",
      "ep 2365: ep_len:1291 episode reward: total was -45.840000. running mean: -4.508598\n",
      "ep 2365: ep_len:2855 episode reward: total was -25.240000. running mean: -4.715912\n",
      "ep 2365: ep_len:500 episode reward: total was -36.000000. running mean: -5.028753\n",
      "ep 2365: ep_len:121 episode reward: total was 59.000000. running mean: -4.388465\n",
      "ep 2365: ep_len:976 episode reward: total was 1.110000. running mean: -4.333480\n",
      "ep 2365: ep_len:649 episode reward: total was 14.010000. running mean: -4.150046\n",
      "ep 2365: ep_len:753 episode reward: total was -68.220000. running mean: -4.790745\n",
      "ep 2365: ep_len:7265 episode reward: total was -21.600000. running mean: -4.958838\n",
      "ep 2365: ep_len:706 episode reward: total was -8.260000. running mean: -4.991849\n",
      "ep 2365: ep_len:114 episode reward: total was 54.000000. running mean: -4.401931\n",
      "ep 2365: ep_len:1045 episode reward: total was 11.240000. running mean: -4.245511\n",
      "ep 2365: ep_len:2838 episode reward: total was -12.310000. running mean: -4.326156\n",
      "epsilon:0.009992 episode_count: 35622. steps_count: 38272176.000000\n",
      "ep 2366: ep_len:1462 episode reward: total was 1.960000. running mean: -4.263295\n",
      "ep 2366: ep_len:1575 episode reward: total was -68.250000. running mean: -4.903162\n",
      "ep 2366: ep_len:2953 episode reward: total was -22.540000. running mean: -5.079530\n",
      "ep 2366: ep_len:723 episode reward: total was -7.080000. running mean: -5.099535\n",
      "ep 2366: ep_len:114 episode reward: total was 54.000000. running mean: -4.508540\n",
      "ep 2366: ep_len:45 episode reward: total was 21.000000. running mean: -4.253454\n",
      "ep 2366: ep_len:2885 episode reward: total was -194.530000. running mean: -6.156220\n",
      "ep 2366: ep_len:4002 episode reward: total was -507.560000. running mean: -11.170257\n",
      "ep 2366: ep_len:1212 episode reward: total was -51.680000. running mean: -11.575355\n",
      "ep 2366: ep_len:831 episode reward: total was 53.530000. running mean: -10.924301\n",
      "ep 2366: ep_len:1471 episode reward: total was 17.170000. running mean: -10.643358\n",
      "ep 2366: ep_len:500 episode reward: total was 21.400000. running mean: -10.322925\n",
      "ep 2366: ep_len:2802 episode reward: total was -44.700000. running mean: -10.666695\n",
      "epsilon:0.009992 episode_count: 35635. steps_count: 38292751.000000\n",
      "ep 2367: ep_len:953 episode reward: total was -30.650000. running mean: -10.866529\n",
      "ep 2367: ep_len:968 episode reward: total was 15.390000. running mean: -10.603963\n",
      "ep 2367: ep_len:2933 episode reward: total was -71.200000. running mean: -11.209924\n",
      "ep 2367: ep_len:693 episode reward: total was 1.090000. running mean: -11.086924\n",
      "ep 2367: ep_len:78 episode reward: total was 36.000000. running mean: -10.616055\n",
      "ep 2367: ep_len:58 episode reward: total was 27.500000. running mean: -10.234895\n",
      "ep 2367: ep_len:673 episode reward: total was 3.260000. running mean: -10.099946\n",
      "ep 2367: ep_len:3645 episode reward: total was -109.180000. running mean: -11.090746\n",
      "ep 2367: ep_len:1592 episode reward: total was -19.450000. running mean: -11.174339\n",
      "ep 2367: ep_len:7246 episode reward: total was -18.030000. running mean: -11.242895\n",
      "ep 2367: ep_len:500 episode reward: total was 25.440000. running mean: -10.876066\n",
      "ep 2367: ep_len:113 episode reward: total was 53.500000. running mean: -10.232306\n",
      "ep 2367: ep_len:1192 episode reward: total was -3.910000. running mean: -10.169083\n",
      "ep 2367: ep_len:2778 episode reward: total was -15.730000. running mean: -10.224692\n",
      "epsilon:0.009992 episode_count: 35649. steps_count: 38316173.000000\n",
      "ep 2368: ep_len:587 episode reward: total was 19.160000. running mean: -9.930845\n",
      "ep 2368: ep_len:1304 episode reward: total was -56.300000. running mean: -10.394536\n",
      "ep 2368: ep_len:60 episode reward: total was 28.500000. running mean: -10.005591\n",
      "ep 2368: ep_len:2971 episode reward: total was -46.010000. running mean: -10.365635\n",
      "ep 2368: ep_len:707 episode reward: total was -9.260000. running mean: -10.354579\n",
      "ep 2368: ep_len:54 episode reward: total was 24.000000. running mean: -10.011033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2368: ep_len:59 episode reward: total was 28.000000. running mean: -9.630923\n",
      "ep 2368: ep_len:929 episode reward: total was 75.990000. running mean: -8.774713\n",
      "ep 2368: ep_len:592 episode reward: total was 14.570000. running mean: -8.541266\n",
      "ep 2368: ep_len:571 episode reward: total was 14.300000. running mean: -8.312854\n",
      "ep 2368: ep_len:814 episode reward: total was 14.540000. running mean: -8.084325\n",
      "ep 2368: ep_len:720 episode reward: total was 36.860000. running mean: -7.634882\n",
      "ep 2368: ep_len:1102 episode reward: total was 1.760000. running mean: -7.540933\n",
      "ep 2368: ep_len:2829 episode reward: total was 0.820000. running mean: -7.457324\n",
      "ep 2368: ep_len:60 episode reward: total was 28.500000. running mean: -7.097751\n",
      "epsilon:0.009992 episode_count: 35664. steps_count: 38329532.000000\n",
      "ep 2369: ep_len:605 episode reward: total was 18.760000. running mean: -6.839173\n",
      "ep 2369: ep_len:989 episode reward: total was 22.640000. running mean: -6.544381\n",
      "ep 2369: ep_len:2990 episode reward: total was 1.780000. running mean: -6.461137\n",
      "ep 2369: ep_len:1631 episode reward: total was -98.360000. running mean: -7.380126\n",
      "ep 2369: ep_len:136 episode reward: total was 66.500000. running mean: -6.641325\n",
      "ep 2369: ep_len:55 episode reward: total was 26.000000. running mean: -6.314912\n",
      "ep 2369: ep_len:500 episode reward: total was -14.350000. running mean: -6.395262\n",
      "ep 2369: ep_len:684 episode reward: total was 26.270000. running mean: -6.068610\n",
      "ep 2369: ep_len:1528 episode reward: total was -27.500000. running mean: -6.282924\n",
      "ep 2369: ep_len:829 episode reward: total was 48.430000. running mean: -5.735795\n",
      "ep 2369: ep_len:1043 episode reward: total was -24.780000. running mean: -5.926237\n",
      "ep 2369: ep_len:49 episode reward: total was 21.500000. running mean: -5.651974\n",
      "ep 2369: ep_len:76 episode reward: total was 36.500000. running mean: -5.230454\n",
      "ep 2369: ep_len:790 episode reward: total was 31.990000. running mean: -4.858250\n",
      "ep 2369: ep_len:2883 episode reward: total was -0.500000. running mean: -4.814667\n",
      "epsilon:0.009992 episode_count: 35679. steps_count: 38344320.000000\n",
      "ep 2370: ep_len:1065 episode reward: total was -11.130000. running mean: -4.877821\n",
      "ep 2370: ep_len:728 episode reward: total was -70.550000. running mean: -5.534543\n",
      "ep 2370: ep_len:42 episode reward: total was 19.500000. running mean: -5.284197\n",
      "ep 2370: ep_len:3048 episode reward: total was 13.070000. running mean: -5.100655\n",
      "ep 2370: ep_len:705 episode reward: total was -19.660000. running mean: -5.246249\n",
      "ep 2370: ep_len:127 episode reward: total was 59.000000. running mean: -4.603786\n",
      "ep 2370: ep_len:1437 episode reward: total was 6.300000. running mean: -4.494748\n",
      "ep 2370: ep_len:3661 episode reward: total was -221.070000. running mean: -6.660501\n",
      "ep 2370: ep_len:1294 episode reward: total was -556.070000. running mean: -12.154596\n",
      "ep 2370: ep_len:767 episode reward: total was 1.660000. running mean: -12.016450\n",
      "ep 2370: ep_len:1480 episode reward: total was -114.380000. running mean: -13.040085\n",
      "ep 2370: ep_len:67 episode reward: total was 30.500000. running mean: -12.604684\n",
      "ep 2370: ep_len:1483 episode reward: total was 6.330000. running mean: -12.415338\n",
      "ep 2370: ep_len:2744 episode reward: total was -10.710000. running mean: -12.398284\n",
      "ep 2370: ep_len:56 episode reward: total was 26.500000. running mean: -12.009301\n",
      "epsilon:0.009992 episode_count: 35694. steps_count: 38363024.000000\n",
      "ep 2371: ep_len:778 episode reward: total was -41.880000. running mean: -12.308008\n",
      "ep 2371: ep_len:971 episode reward: total was 2.930000. running mean: -12.155628\n",
      "ep 2371: ep_len:3032 episode reward: total was -95.820000. running mean: -12.992272\n",
      "ep 2371: ep_len:577 episode reward: total was -5.510000. running mean: -12.917449\n",
      "ep 2371: ep_len:644 episode reward: total was 2.230000. running mean: -12.765975\n",
      "ep 2371: ep_len:323 episode reward: total was 27.100000. running mean: -12.367315\n",
      "ep 2371: ep_len:587 episode reward: total was 8.250000. running mean: -12.161142\n",
      "ep 2371: ep_len:663 episode reward: total was -5.380000. running mean: -12.093330\n",
      "ep 2371: ep_len:686 episode reward: total was -25.240000. running mean: -12.224797\n",
      "ep 2371: ep_len:35 episode reward: total was 14.500000. running mean: -11.957549\n",
      "ep 2371: ep_len:741 episode reward: total was -55.380000. running mean: -12.391774\n",
      "ep 2371: ep_len:2856 episode reward: total was -14.580000. running mean: -12.413656\n",
      "epsilon:0.009992 episode_count: 35706. steps_count: 38374917.000000\n",
      "ep 2372: ep_len:647 episode reward: total was -61.770000. running mean: -12.907219\n",
      "ep 2372: ep_len:1617 episode reward: total was -126.380000. running mean: -14.041947\n",
      "ep 2372: ep_len:3006 episode reward: total was -39.920000. running mean: -14.300728\n",
      "ep 2372: ep_len:500 episode reward: total was 25.260000. running mean: -13.905120\n",
      "ep 2372: ep_len:87 episode reward: total was 42.000000. running mean: -13.346069\n",
      "ep 2372: ep_len:993 episode reward: total was 79.360000. running mean: -12.419009\n",
      "ep 2372: ep_len:604 episode reward: total was 28.160000. running mean: -12.013218\n",
      "ep 2372: ep_len:1556 episode reward: total was -21.770000. running mean: -12.110786\n",
      "ep 2372: ep_len:768 episode reward: total was -4.110000. running mean: -12.030778\n",
      "ep 2372: ep_len:4188 episode reward: total was -1769.800000. running mean: -29.608471\n",
      "ep 2372: ep_len:57 episode reward: total was 27.000000. running mean: -29.042386\n",
      "ep 2372: ep_len:131 episode reward: total was 64.000000. running mean: -28.111962\n",
      "ep 2372: ep_len:1061 episode reward: total was 30.620000. running mean: -27.524642\n",
      "ep 2372: ep_len:2887 episode reward: total was -8.760000. running mean: -27.336996\n",
      "epsilon:0.009992 episode_count: 35720. steps_count: 38393019.000000\n",
      "ep 2373: ep_len:829 episode reward: total was 2.800000. running mean: -27.035626\n",
      "ep 2373: ep_len:670 episode reward: total was -67.200000. running mean: -27.437270\n",
      "ep 2373: ep_len:2956 episode reward: total was -4.880000. running mean: -27.211697\n",
      "ep 2373: ep_len:651 episode reward: total was 1.290000. running mean: -26.926680\n",
      "ep 2373: ep_len:93 episode reward: total was 43.500000. running mean: -26.222413\n",
      "ep 2373: ep_len:50 episode reward: total was 23.500000. running mean: -25.725189\n",
      "ep 2373: ep_len:500 episode reward: total was 9.590000. running mean: -25.372037\n",
      "ep 2373: ep_len:298 episode reward: total was 0.040000. running mean: -25.117917\n",
      "ep 2373: ep_len:1316 episode reward: total was -103.130000. running mean: -25.898038\n",
      "ep 2373: ep_len:876 episode reward: total was 68.710000. running mean: -24.951957\n",
      "ep 2373: ep_len:1093 episode reward: total was -10.530000. running mean: -24.807738\n",
      "ep 2373: ep_len:197 episode reward: total was 95.500000. running mean: -23.604660\n",
      "ep 2373: ep_len:1209 episode reward: total was -8.330000. running mean: -23.451914\n",
      "ep 2373: ep_len:2864 episode reward: total was -1.920000. running mean: -23.236595\n",
      "epsilon:0.009992 episode_count: 35734. steps_count: 38406621.000000\n",
      "ep 2374: ep_len:698 episode reward: total was -49.750000. running mean: -23.501729\n",
      "ep 2374: ep_len:697 episode reward: total was -72.990000. running mean: -23.996611\n",
      "ep 2374: ep_len:68 episode reward: total was 32.500000. running mean: -23.431645\n",
      "ep 2374: ep_len:2996 episode reward: total was 10.760000. running mean: -23.089729\n",
      "ep 2374: ep_len:780 episode reward: total was 16.070000. running mean: -22.698132\n",
      "ep 2374: ep_len:50 episode reward: total was 23.500000. running mean: -22.236150\n",
      "ep 2374: ep_len:94 episode reward: total was 44.000000. running mean: -21.573789\n",
      "ep 2374: ep_len:69 episode reward: total was 33.000000. running mean: -21.028051\n",
      "ep 2374: ep_len:1367 episode reward: total was -169.310000. running mean: -22.510870\n",
      "ep 2374: ep_len:3367 episode reward: total was -126.540000. running mean: -23.551162\n",
      "ep 2374: ep_len:4418 episode reward: total was -1022.490000. running mean: -33.540550\n",
      "ep 2374: ep_len:7174 episode reward: total was 29.700000. running mean: -32.908145\n",
      "ep 2374: ep_len:1060 episode reward: total was 6.340000. running mean: -32.515663\n",
      "ep 2374: ep_len:127 episode reward: total was 57.500000. running mean: -31.615506\n",
      "ep 2374: ep_len:1079 episode reward: total was -25.100000. running mean: -31.550351\n",
      "ep 2374: ep_len:2815 episode reward: total was -58.940000. running mean: -31.824248\n",
      "epsilon:0.009992 episode_count: 35750. steps_count: 38433480.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2375: ep_len:1160 episode reward: total was -8.280000. running mean: -31.588805\n",
      "ep 2375: ep_len:500 episode reward: total was 11.270000. running mean: -31.160217\n",
      "ep 2375: ep_len:49 episode reward: total was 21.500000. running mean: -30.633615\n",
      "ep 2375: ep_len:3054 episode reward: total was -8.550000. running mean: -30.412779\n",
      "ep 2375: ep_len:500 episode reward: total was 9.780000. running mean: -30.010851\n",
      "ep 2375: ep_len:105 episode reward: total was 51.000000. running mean: -29.200743\n",
      "ep 2375: ep_len:34 episode reward: total was 15.500000. running mean: -28.753735\n",
      "ep 2375: ep_len:500 episode reward: total was 27.470000. running mean: -28.191498\n",
      "ep 2375: ep_len:356 episode reward: total was 14.150000. running mean: -27.768083\n",
      "ep 2375: ep_len:1546 episode reward: total was -26.550000. running mean: -27.755902\n",
      "ep 2375: ep_len:749 episode reward: total was 17.580000. running mean: -27.302543\n",
      "ep 2375: ep_len:500 episode reward: total was 32.490000. running mean: -26.704618\n",
      "ep 2375: ep_len:74 episode reward: total was 35.500000. running mean: -26.082572\n",
      "ep 2375: ep_len:183 episode reward: total was 90.000000. running mean: -24.921746\n",
      "ep 2375: ep_len:114 episode reward: total was 52.500000. running mean: -24.147528\n",
      "ep 2375: ep_len:776 episode reward: total was -55.790000. running mean: -24.463953\n",
      "ep 2375: ep_len:2798 episode reward: total was -8.850000. running mean: -24.307814\n",
      "ep 2375: ep_len:38 episode reward: total was 17.500000. running mean: -23.889735\n",
      "epsilon:0.009992 episode_count: 35768. steps_count: 38446516.000000\n",
      "ep 2376: ep_len:965 episode reward: total was -90.240000. running mean: -24.553238\n",
      "ep 2376: ep_len:1227 episode reward: total was -78.800000. running mean: -25.095706\n",
      "ep 2376: ep_len:68 episode reward: total was 32.500000. running mean: -24.519749\n",
      "ep 2376: ep_len:3009 episode reward: total was 1.010000. running mean: -24.264451\n",
      "ep 2376: ep_len:516 episode reward: total was 5.230000. running mean: -23.969507\n",
      "ep 2376: ep_len:126 episode reward: total was 58.500000. running mean: -23.144812\n",
      "ep 2376: ep_len:81 episode reward: total was 37.500000. running mean: -22.538363\n",
      "ep 2376: ep_len:896 episode reward: total was 40.020000. running mean: -21.912780\n",
      "ep 2376: ep_len:3684 episode reward: total was -55.410000. running mean: -22.247752\n",
      "ep 2376: ep_len:564 episode reward: total was 16.030000. running mean: -21.864974\n",
      "ep 2376: ep_len:692 episode reward: total was 26.080000. running mean: -21.385525\n",
      "ep 2376: ep_len:755 episode reward: total was -13.830000. running mean: -21.309969\n",
      "ep 2376: ep_len:114 episode reward: total was 55.500000. running mean: -20.541870\n",
      "ep 2376: ep_len:55 episode reward: total was 26.000000. running mean: -20.076451\n",
      "ep 2376: ep_len:664 episode reward: total was -5.760000. running mean: -19.933287\n",
      "ep 2376: ep_len:2822 episode reward: total was -67.340000. running mean: -20.407354\n",
      "epsilon:0.009992 episode_count: 35784. steps_count: 38462754.000000\n",
      "ep 2377: ep_len:1434 episode reward: total was 4.740000. running mean: -20.155880\n",
      "ep 2377: ep_len:632 episode reward: total was 8.660000. running mean: -19.867721\n",
      "ep 2377: ep_len:2984 episode reward: total was -110.530000. running mean: -20.774344\n",
      "ep 2377: ep_len:833 episode reward: total was 25.430000. running mean: -20.312301\n",
      "ep 2377: ep_len:70 episode reward: total was 33.500000. running mean: -19.774178\n",
      "ep 2377: ep_len:787 episode reward: total was 9.070000. running mean: -19.485736\n",
      "ep 2377: ep_len:645 episode reward: total was 20.980000. running mean: -19.081079\n",
      "ep 2377: ep_len:795 episode reward: total was -24.910000. running mean: -19.139368\n",
      "ep 2377: ep_len:7266 episode reward: total was -225.400000. running mean: -21.201974\n",
      "ep 2377: ep_len:1407 episode reward: total was 11.510000. running mean: -20.874854\n",
      "ep 2377: ep_len:127 episode reward: total was 62.000000. running mean: -20.046106\n",
      "ep 2377: ep_len:38 episode reward: total was 16.000000. running mean: -19.685645\n",
      "ep 2377: ep_len:1167 episode reward: total was 8.450000. running mean: -19.404288\n",
      "ep 2377: ep_len:2849 episode reward: total was -9.630000. running mean: -19.306545\n",
      "ep 2377: ep_len:47 episode reward: total was 22.000000. running mean: -18.893480\n",
      "epsilon:0.009992 episode_count: 35799. steps_count: 38483835.000000\n",
      "ep 2378: ep_len:1411 episode reward: total was 9.410000. running mean: -18.610445\n",
      "ep 2378: ep_len:751 episode reward: total was -53.580000. running mean: -18.960141\n",
      "ep 2378: ep_len:47 episode reward: total was 20.500000. running mean: -18.565539\n",
      "ep 2378: ep_len:2800 episode reward: total was -77.660000. running mean: -19.156484\n",
      "ep 2378: ep_len:3117 episode reward: total was -2499.970000. running mean: -43.964619\n",
      "ep 2378: ep_len:43 episode reward: total was 20.000000. running mean: -43.324973\n",
      "ep 2378: ep_len:97 episode reward: total was 44.000000. running mean: -42.451723\n",
      "ep 2378: ep_len:94 episode reward: total was 45.500000. running mean: -41.572206\n",
      "ep 2378: ep_len:624 episode reward: total was 4.970000. running mean: -41.106784\n",
      "ep 2378: ep_len:3660 episode reward: total was -387.880000. running mean: -44.574516\n",
      "ep 2378: ep_len:766 episode reward: total was -53.700000. running mean: -44.665771\n",
      "ep 2378: ep_len:620 episode reward: total was -4.440000. running mean: -44.263513\n",
      "ep 2378: ep_len:578 episode reward: total was 25.720000. running mean: -43.563678\n",
      "ep 2378: ep_len:181 episode reward: total was 89.000000. running mean: -42.238041\n",
      "ep 2378: ep_len:85 episode reward: total was 39.500000. running mean: -41.420661\n",
      "ep 2378: ep_len:500 episode reward: total was 17.180000. running mean: -40.834654\n",
      "ep 2378: ep_len:2803 episode reward: total was -26.220000. running mean: -40.688508\n",
      "ep 2378: ep_len:43 episode reward: total was 20.000000. running mean: -40.081623\n",
      "epsilon:0.009992 episode_count: 35817. steps_count: 38502055.000000\n",
      "ep 2379: ep_len:1208 episode reward: total was 20.890000. running mean: -39.471906\n",
      "ep 2379: ep_len:632 episode reward: total was -27.730000. running mean: -39.354487\n",
      "ep 2379: ep_len:2999 episode reward: total was 8.080000. running mean: -38.880142\n",
      "ep 2379: ep_len:671 episode reward: total was 0.480000. running mean: -38.486541\n",
      "ep 2379: ep_len:67 episode reward: total was 32.000000. running mean: -37.781676\n",
      "ep 2379: ep_len:1485 episode reward: total was 5.920000. running mean: -37.344659\n",
      "ep 2379: ep_len:640 episode reward: total was 7.310000. running mean: -36.898112\n",
      "ep 2379: ep_len:511 episode reward: total was -43.540000. running mean: -36.964531\n",
      "ep 2379: ep_len:7266 episode reward: total was 38.760000. running mean: -36.207286\n",
      "ep 2379: ep_len:656 episode reward: total was 1.340000. running mean: -35.831813\n",
      "ep 2379: ep_len:500 episode reward: total was 19.390000. running mean: -35.279595\n",
      "ep 2379: ep_len:2790 episode reward: total was -64.180000. running mean: -35.568599\n",
      "epsilon:0.009992 episode_count: 35829. steps_count: 38521480.000000\n",
      "ep 2380: ep_len:1491 episode reward: total was 16.970000. running mean: -35.043213\n",
      "ep 2380: ep_len:726 episode reward: total was -45.660000. running mean: -35.149381\n",
      "ep 2380: ep_len:2964 episode reward: total was -33.360000. running mean: -35.131487\n",
      "ep 2380: ep_len:901 episode reward: total was 38.910000. running mean: -34.391072\n",
      "ep 2380: ep_len:56 episode reward: total was 26.500000. running mean: -33.782161\n",
      "ep 2380: ep_len:70 episode reward: total was 33.500000. running mean: -33.109340\n",
      "ep 2380: ep_len:704 episode reward: total was -16.360000. running mean: -32.941846\n",
      "ep 2380: ep_len:339 episode reward: total was -0.290000. running mean: -32.615328\n",
      "ep 2380: ep_len:587 episode reward: total was -27.630000. running mean: -32.565475\n",
      "ep 2380: ep_len:801 episode reward: total was 30.630000. running mean: -31.933520\n",
      "ep 2380: ep_len:623 episode reward: total was 14.350000. running mean: -31.470685\n",
      "ep 2380: ep_len:77 episode reward: total was 37.000000. running mean: -30.785978\n",
      "ep 2380: ep_len:200 episode reward: total was -183.990000. running mean: -32.318018\n",
      "ep 2380: ep_len:44 episode reward: total was 17.500000. running mean: -31.819838\n",
      "ep 2380: ep_len:61 episode reward: total was 29.000000. running mean: -31.211640\n",
      "ep 2380: ep_len:1054 episode reward: total was -45.780000. running mean: -31.357323\n",
      "ep 2380: ep_len:2762 episode reward: total was -81.020000. running mean: -31.853950\n",
      "epsilon:0.009992 episode_count: 35846. steps_count: 38534940.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2381: ep_len:1081 episode reward: total was 0.540000. running mean: -31.530010\n",
      "ep 2381: ep_len:690 episode reward: total was -30.640000. running mean: -31.521110\n",
      "ep 2381: ep_len:2971 episode reward: total was -149.140000. running mean: -32.697299\n",
      "ep 2381: ep_len:561 episode reward: total was -30.920000. running mean: -32.679526\n",
      "ep 2381: ep_len:100 episode reward: total was 44.000000. running mean: -31.912731\n",
      "ep 2381: ep_len:84 episode reward: total was 39.000000. running mean: -31.203604\n",
      "ep 2381: ep_len:1082 episode reward: total was -5.510000. running mean: -30.946668\n",
      "ep 2381: ep_len:3896 episode reward: total was -2533.660000. running mean: -55.973801\n",
      "ep 2381: ep_len:802 episode reward: total was -49.330000. running mean: -55.907363\n",
      "ep 2381: ep_len:675 episode reward: total was 18.770000. running mean: -55.160589\n",
      "ep 2381: ep_len:500 episode reward: total was 18.680000. running mean: -54.422183\n",
      "ep 2381: ep_len:93 episode reward: total was 45.000000. running mean: -53.427962\n",
      "ep 2381: ep_len:60 episode reward: total was 25.500000. running mean: -52.638682\n",
      "ep 2381: ep_len:901 episode reward: total was -12.400000. running mean: -52.236295\n",
      "ep 2381: ep_len:2815 episode reward: total was -8.840000. running mean: -51.802332\n",
      "epsilon:0.009992 episode_count: 35861. steps_count: 38551251.000000\n",
      "ep 2382: ep_len:1111 episode reward: total was 1.850000. running mean: -51.265809\n",
      "ep 2382: ep_len:1589 episode reward: total was -61.930000. running mean: -51.372451\n",
      "ep 2382: ep_len:77 episode reward: total was 37.000000. running mean: -50.488726\n",
      "ep 2382: ep_len:3005 episode reward: total was -46.710000. running mean: -50.450939\n",
      "ep 2382: ep_len:603 episode reward: total was 4.520000. running mean: -49.901230\n",
      "ep 2382: ep_len:94 episode reward: total was 45.500000. running mean: -48.947217\n",
      "ep 2382: ep_len:32 episode reward: total was 13.000000. running mean: -48.327745\n",
      "ep 2382: ep_len:747 episode reward: total was -58.870000. running mean: -48.433168\n",
      "ep 2382: ep_len:641 episode reward: total was -26.990000. running mean: -48.218736\n",
      "ep 2382: ep_len:1230 episode reward: total was -55.540000. running mean: -48.291949\n",
      "ep 2382: ep_len:7171 episode reward: total was -97.380000. running mean: -48.782829\n",
      "ep 2382: ep_len:500 episode reward: total was -13.310000. running mean: -48.428101\n",
      "ep 2382: ep_len:131 episode reward: total was 61.000000. running mean: -47.333820\n",
      "ep 2382: ep_len:76 episode reward: total was 35.000000. running mean: -46.510482\n",
      "ep 2382: ep_len:1159 episode reward: total was -4.550000. running mean: -46.090877\n",
      "ep 2382: ep_len:2810 episode reward: total was -32.610000. running mean: -45.956068\n",
      "ep 2382: ep_len:46 episode reward: total was 21.500000. running mean: -45.281507\n",
      "epsilon:0.009992 episode_count: 35878. steps_count: 38572273.000000\n",
      "ep 2383: ep_len:838 episode reward: total was -54.410000. running mean: -45.372792\n",
      "ep 2383: ep_len:663 episode reward: total was -35.960000. running mean: -45.278664\n",
      "ep 2383: ep_len:3039 episode reward: total was -45.180000. running mean: -45.277678\n",
      "ep 2383: ep_len:1062 episode reward: total was -15.810000. running mean: -44.983001\n",
      "ep 2383: ep_len:54 episode reward: total was 25.500000. running mean: -44.278171\n",
      "ep 2383: ep_len:500 episode reward: total was 5.440000. running mean: -43.780989\n",
      "ep 2383: ep_len:3779 episode reward: total was -104.240000. running mean: -44.385579\n",
      "ep 2383: ep_len:840 episode reward: total was 23.410000. running mean: -43.707624\n",
      "ep 2383: ep_len:694 episode reward: total was 44.630000. running mean: -42.824247\n",
      "ep 2383: ep_len:641 episode reward: total was -6.810000. running mean: -42.464105\n",
      "ep 2383: ep_len:1491 episode reward: total was -11.580000. running mean: -42.155264\n",
      "ep 2383: ep_len:2888 episode reward: total was -38.410000. running mean: -42.117811\n",
      "ep 2383: ep_len:40 episode reward: total was 18.500000. running mean: -41.511633\n",
      "epsilon:0.009992 episode_count: 35891. steps_count: 38588802.000000\n",
      "ep 2384: ep_len:869 episode reward: total was 6.560000. running mean: -41.030917\n",
      "ep 2384: ep_len:665 episode reward: total was -23.820000. running mean: -40.858808\n",
      "ep 2384: ep_len:2974 episode reward: total was -39.090000. running mean: -40.841119\n",
      "ep 2384: ep_len:1099 episode reward: total was -3.320000. running mean: -40.465908\n",
      "ep 2384: ep_len:48 episode reward: total was 21.000000. running mean: -39.851249\n",
      "ep 2384: ep_len:884 episode reward: total was 35.800000. running mean: -39.094737\n",
      "ep 2384: ep_len:500 episode reward: total was -19.200000. running mean: -38.895789\n",
      "ep 2384: ep_len:531 episode reward: total was -28.650000. running mean: -38.793331\n",
      "ep 2384: ep_len:819 episode reward: total was 31.760000. running mean: -38.087798\n",
      "ep 2384: ep_len:564 episode reward: total was -13.410000. running mean: -37.841020\n",
      "ep 2384: ep_len:1061 episode reward: total was -47.670000. running mean: -37.939310\n",
      "ep 2384: ep_len:2854 episode reward: total was -39.670000. running mean: -37.956617\n",
      "epsilon:0.009992 episode_count: 35903. steps_count: 38601670.000000\n",
      "ep 2385: ep_len:1124 episode reward: total was -7.110000. running mean: -37.648151\n",
      "ep 2385: ep_len:648 episode reward: total was -17.920000. running mean: -37.450869\n",
      "ep 2385: ep_len:2999 episode reward: total was -30.580000. running mean: -37.382160\n",
      "ep 2385: ep_len:670 episode reward: total was 17.470000. running mean: -36.833639\n",
      "ep 2385: ep_len:108 episode reward: total was 52.500000. running mean: -35.940302\n",
      "ep 2385: ep_len:1533 episode reward: total was -412.070000. running mean: -39.701599\n",
      "ep 2385: ep_len:680 episode reward: total was 0.640000. running mean: -39.298183\n",
      "ep 2385: ep_len:603 episode reward: total was 2.010000. running mean: -38.885102\n",
      "ep 2385: ep_len:796 episode reward: total was 56.280000. running mean: -37.933451\n",
      "ep 2385: ep_len:1046 episode reward: total was -50.340000. running mean: -38.057516\n",
      "ep 2385: ep_len:65 episode reward: total was 31.000000. running mean: -37.366941\n",
      "ep 2385: ep_len:57 episode reward: total was 25.500000. running mean: -36.738272\n",
      "ep 2385: ep_len:98 episode reward: total was 44.500000. running mean: -35.925889\n",
      "ep 2385: ep_len:1124 episode reward: total was -3.070000. running mean: -35.597330\n",
      "ep 2385: ep_len:2833 episode reward: total was -24.910000. running mean: -35.490457\n",
      "epsilon:0.009992 episode_count: 35918. steps_count: 38616054.000000\n",
      "ep 2386: ep_len:652 episode reward: total was -52.660000. running mean: -35.662152\n",
      "ep 2386: ep_len:719 episode reward: total was -19.410000. running mean: -35.499631\n",
      "ep 2386: ep_len:78 episode reward: total was 36.000000. running mean: -34.784634\n",
      "ep 2386: ep_len:3044 episode reward: total was -7.940000. running mean: -34.516188\n",
      "ep 2386: ep_len:1265 episode reward: total was -15.390000. running mean: -34.324926\n",
      "ep 2386: ep_len:65 episode reward: total was 31.000000. running mean: -33.671677\n",
      "ep 2386: ep_len:1916 episode reward: total was -92.840000. running mean: -34.263360\n",
      "ep 2386: ep_len:344 episode reward: total was 1.910000. running mean: -33.901626\n",
      "ep 2386: ep_len:569 episode reward: total was -1.550000. running mean: -33.578110\n",
      "ep 2386: ep_len:781 episode reward: total was 54.350000. running mean: -32.698829\n",
      "ep 2386: ep_len:500 episode reward: total was 17.610000. running mean: -32.195741\n",
      "ep 2386: ep_len:158 episode reward: total was 76.000000. running mean: -31.113783\n",
      "ep 2386: ep_len:57 episode reward: total was 27.000000. running mean: -30.532645\n",
      "ep 2386: ep_len:1066 episode reward: total was 38.690000. running mean: -29.840419\n",
      "ep 2386: ep_len:2856 episode reward: total was -17.820000. running mean: -29.720215\n",
      "ep 2386: ep_len:53 episode reward: total was 25.000000. running mean: -29.173013\n",
      "epsilon:0.009992 episode_count: 35934. steps_count: 38630177.000000\n",
      "ep 2387: ep_len:812 episode reward: total was -3.070000. running mean: -28.911983\n",
      "ep 2387: ep_len:1626 episode reward: total was -84.940000. running mean: -29.472263\n",
      "ep 2387: ep_len:28 episode reward: total was 12.500000. running mean: -29.052540\n",
      "ep 2387: ep_len:3071 episode reward: total was -69.860000. running mean: -29.460615\n",
      "ep 2387: ep_len:728 episode reward: total was -36.320000. running mean: -29.529209\n",
      "ep 2387: ep_len:24 episode reward: total was 10.500000. running mean: -29.128916\n",
      "ep 2387: ep_len:86 episode reward: total was 40.000000. running mean: -28.437627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2387: ep_len:593 episode reward: total was 26.950000. running mean: -27.883751\n",
      "ep 2387: ep_len:3660 episode reward: total was -349.010000. running mean: -31.095014\n",
      "ep 2387: ep_len:3727 episode reward: total was -2126.470000. running mean: -52.048763\n",
      "ep 2387: ep_len:861 episode reward: total was 40.820000. running mean: -51.120076\n",
      "ep 2387: ep_len:761 episode reward: total was 12.930000. running mean: -50.479575\n",
      "ep 2387: ep_len:199 episode reward: total was 94.510000. running mean: -49.029679\n",
      "ep 2387: ep_len:47 episode reward: total was 20.500000. running mean: -48.334382\n",
      "ep 2387: ep_len:597 episode reward: total was -12.380000. running mean: -47.974839\n",
      "ep 2387: ep_len:2802 episode reward: total was -19.370000. running mean: -47.688790\n",
      "ep 2387: ep_len:47 episode reward: total was 22.000000. running mean: -46.991902\n",
      "epsilon:0.009992 episode_count: 35951. steps_count: 38649846.000000\n",
      "ep 2388: ep_len:1433 episode reward: total was 14.190000. running mean: -46.380083\n",
      "ep 2388: ep_len:981 episode reward: total was 2.690000. running mean: -45.889383\n",
      "ep 2388: ep_len:79 episode reward: total was 35.000000. running mean: -45.080489\n",
      "ep 2388: ep_len:2923 episode reward: total was -37.510000. running mean: -45.004784\n",
      "ep 2388: ep_len:848 episode reward: total was 47.740000. running mean: -44.077336\n",
      "ep 2388: ep_len:29 episode reward: total was 13.000000. running mean: -43.506563\n",
      "ep 2388: ep_len:59 episode reward: total was 28.000000. running mean: -42.791497\n",
      "ep 2388: ep_len:652 episode reward: total was -17.380000. running mean: -42.537382\n",
      "ep 2388: ep_len:3968 episode reward: total was -72.220000. running mean: -42.834208\n",
      "ep 2388: ep_len:784 episode reward: total was -21.540000. running mean: -42.621266\n",
      "ep 2388: ep_len:671 episode reward: total was 28.500000. running mean: -41.910053\n",
      "ep 2388: ep_len:1039 episode reward: total was 24.120000. running mean: -41.249753\n",
      "ep 2388: ep_len:65 episode reward: total was 31.000000. running mean: -40.527255\n",
      "ep 2388: ep_len:200 episode reward: total was 97.000000. running mean: -39.151983\n",
      "ep 2388: ep_len:41 episode reward: total was 19.000000. running mean: -38.570463\n",
      "ep 2388: ep_len:805 episode reward: total was -9.950000. running mean: -38.284258\n",
      "ep 2388: ep_len:2819 episode reward: total was -23.000000. running mean: -38.131416\n",
      "epsilon:0.009992 episode_count: 35968. steps_count: 38667242.000000\n",
      "ep 2389: ep_len:598 episode reward: total was 10.180000. running mean: -37.648302\n",
      "ep 2389: ep_len:969 episode reward: total was -6.980000. running mean: -37.341619\n",
      "ep 2389: ep_len:48 episode reward: total was 22.500000. running mean: -36.743202\n",
      "ep 2389: ep_len:2959 episode reward: total was -49.510000. running mean: -36.870870\n",
      "ep 2389: ep_len:500 episode reward: total was 12.160000. running mean: -36.380562\n",
      "ep 2389: ep_len:85 episode reward: total was 41.000000. running mean: -35.606756\n",
      "ep 2389: ep_len:500 episode reward: total was 35.800000. running mean: -34.892689\n",
      "ep 2389: ep_len:619 episode reward: total was 0.670000. running mean: -34.537062\n",
      "ep 2389: ep_len:636 episode reward: total was -3.910000. running mean: -34.230791\n",
      "ep 2389: ep_len:761 episode reward: total was 16.080000. running mean: -33.727683\n",
      "ep 2389: ep_len:704 episode reward: total was 19.920000. running mean: -33.191206\n",
      "ep 2389: ep_len:80 episode reward: total was 38.500000. running mean: -32.474294\n",
      "ep 2389: ep_len:592 episode reward: total was 1.100000. running mean: -32.138551\n",
      "ep 2389: ep_len:2798 episode reward: total was -30.370000. running mean: -32.120866\n",
      "ep 2389: ep_len:63 episode reward: total was 30.000000. running mean: -31.499657\n",
      "epsilon:0.009992 episode_count: 35983. steps_count: 38679154.000000\n",
      "ep 2390: ep_len:840 episode reward: total was -2.240000. running mean: -31.207061\n",
      "ep 2390: ep_len:774 episode reward: total was -86.210000. running mean: -31.757090\n",
      "ep 2390: ep_len:2876 episode reward: total was 4.490000. running mean: -31.394619\n",
      "ep 2390: ep_len:598 episode reward: total was 3.640000. running mean: -31.044273\n",
      "ep 2390: ep_len:94 episode reward: total was 45.500000. running mean: -30.278830\n",
      "ep 2390: ep_len:500 episode reward: total was 33.010000. running mean: -29.645942\n",
      "ep 2390: ep_len:3997 episode reward: total was -34.190000. running mean: -29.691382\n",
      "ep 2390: ep_len:657 episode reward: total was -44.890000. running mean: -29.843369\n",
      "ep 2390: ep_len:801 episode reward: total was 41.840000. running mean: -29.126535\n",
      "ep 2390: ep_len:744 episode reward: total was 25.160000. running mean: -28.583670\n",
      "ep 2390: ep_len:671 episode reward: total was 7.920000. running mean: -28.218633\n",
      "ep 2390: ep_len:2877 episode reward: total was -3.260000. running mean: -27.969046\n",
      "ep 2390: ep_len:55 episode reward: total was 26.000000. running mean: -27.429356\n",
      "epsilon:0.009992 episode_count: 35996. steps_count: 38694638.000000\n",
      "ep 2391: ep_len:714 episode reward: total was -17.660000. running mean: -27.331662\n",
      "ep 2391: ep_len:711 episode reward: total was -27.140000. running mean: -27.329746\n",
      "ep 2391: ep_len:2961 episode reward: total was -23.030000. running mean: -27.286748\n",
      "ep 2391: ep_len:1462 episode reward: total was -83.520000. running mean: -27.849081\n",
      "ep 2391: ep_len:93 episode reward: total was 45.000000. running mean: -27.120590\n",
      "ep 2391: ep_len:49 episode reward: total was 23.000000. running mean: -26.619384\n",
      "ep 2391: ep_len:500 episode reward: total was -7.520000. running mean: -26.428390\n",
      "ep 2391: ep_len:325 episode reward: total was -19.980000. running mean: -26.363906\n",
      "ep 2391: ep_len:4212 episode reward: total was -1225.360000. running mean: -38.353867\n",
      "ep 2391: ep_len:886 episode reward: total was 35.530000. running mean: -37.615029\n",
      "ep 2391: ep_len:1080 episode reward: total was -16.640000. running mean: -37.405278\n",
      "ep 2391: ep_len:76 episode reward: total was 36.500000. running mean: -36.666226\n",
      "ep 2391: ep_len:734 episode reward: total was -67.930000. running mean: -36.978863\n",
      "ep 2391: ep_len:2883 episode reward: total was -21.380000. running mean: -36.822875\n",
      "epsilon:0.009992 episode_count: 36010. steps_count: 38711324.000000\n",
      "ep 2392: ep_len:802 episode reward: total was -18.710000. running mean: -36.641746\n",
      "ep 2392: ep_len:500 episode reward: total was 24.680000. running mean: -36.028529\n",
      "ep 2392: ep_len:53 episode reward: total was 23.500000. running mean: -35.433243\n",
      "ep 2392: ep_len:3039 episode reward: total was 5.110000. running mean: -35.027811\n",
      "ep 2392: ep_len:500 episode reward: total was -7.380000. running mean: -34.751333\n",
      "ep 2392: ep_len:125 episode reward: total was 58.000000. running mean: -33.823819\n",
      "ep 2392: ep_len:622 episode reward: total was 39.390000. running mean: -33.091681\n",
      "ep 2392: ep_len:3593 episode reward: total was -20.570000. running mean: -32.966464\n",
      "ep 2392: ep_len:1567 episode reward: total was -35.460000. running mean: -32.991400\n",
      "ep 2392: ep_len:659 episode reward: total was -9.190000. running mean: -32.753386\n",
      "ep 2392: ep_len:781 episode reward: total was -3.960000. running mean: -32.465452\n",
      "ep 2392: ep_len:92 episode reward: total was 43.000000. running mean: -31.710797\n",
      "ep 2392: ep_len:41 episode reward: total was 19.000000. running mean: -31.203689\n",
      "ep 2392: ep_len:593 episode reward: total was -4.340000. running mean: -30.935052\n",
      "ep 2392: ep_len:2785 episode reward: total was -17.340000. running mean: -30.799102\n",
      "ep 2392: ep_len:32 episode reward: total was 14.500000. running mean: -30.346111\n",
      "epsilon:0.009992 episode_count: 36026. steps_count: 38727108.000000\n",
      "ep 2393: ep_len:1445 episode reward: total was -10.240000. running mean: -30.145050\n",
      "ep 2393: ep_len:1273 episode reward: total was -82.380000. running mean: -30.667399\n",
      "ep 2393: ep_len:64 episode reward: total was 30.500000. running mean: -30.055725\n",
      "ep 2393: ep_len:2952 episode reward: total was -40.770000. running mean: -30.162868\n",
      "ep 2393: ep_len:504 episode reward: total was 19.260000. running mean: -29.668639\n",
      "ep 2393: ep_len:43 episode reward: total was 20.000000. running mean: -29.171953\n",
      "ep 2393: ep_len:114 episode reward: total was 55.500000. running mean: -28.325233\n",
      "ep 2393: ep_len:64 episode reward: total was 30.500000. running mean: -27.736981\n",
      "ep 2393: ep_len:60 episode reward: total was 28.500000. running mean: -27.174611\n",
      "ep 2393: ep_len:1468 episode reward: total was -2.260000. running mean: -26.925465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2393: ep_len:3665 episode reward: total was -774.670000. running mean: -34.402911\n",
      "ep 2393: ep_len:511 episode reward: total was -29.400000. running mean: -34.352881\n",
      "ep 2393: ep_len:779 episode reward: total was 0.800000. running mean: -34.001353\n",
      "ep 2393: ep_len:1083 episode reward: total was -32.160000. running mean: -33.982939\n",
      "ep 2393: ep_len:46 episode reward: total was 21.500000. running mean: -33.428110\n",
      "ep 2393: ep_len:88 episode reward: total was 42.500000. running mean: -32.668829\n",
      "ep 2393: ep_len:1472 episode reward: total was 0.470000. running mean: -32.337440\n",
      "ep 2393: ep_len:2893 episode reward: total was -11.120000. running mean: -32.125266\n",
      "ep 2393: ep_len:45 episode reward: total was 21.000000. running mean: -31.594013\n",
      "epsilon:0.009992 episode_count: 36045. steps_count: 38745677.000000\n",
      "ep 2394: ep_len:1084 episode reward: total was -0.440000. running mean: -31.282473\n",
      "ep 2394: ep_len:785 episode reward: total was -3.750000. running mean: -31.007148\n",
      "ep 2394: ep_len:3157 episode reward: total was -54.800000. running mean: -31.245077\n",
      "ep 2394: ep_len:776 episode reward: total was -15.640000. running mean: -31.089026\n",
      "ep 2394: ep_len:56 episode reward: total was 26.500000. running mean: -30.513136\n",
      "ep 2394: ep_len:100 episode reward: total was 47.000000. running mean: -29.738005\n",
      "ep 2394: ep_len:59 episode reward: total was 28.000000. running mean: -29.160625\n",
      "ep 2394: ep_len:789 episode reward: total was -39.750000. running mean: -29.266518\n",
      "ep 2394: ep_len:618 episode reward: total was 21.350000. running mean: -28.760353\n",
      "ep 2394: ep_len:543 episode reward: total was -1.810000. running mean: -28.490850\n",
      "ep 2394: ep_len:760 episode reward: total was 35.300000. running mean: -27.852941\n",
      "ep 2394: ep_len:500 episode reward: total was -12.030000. running mean: -27.694712\n",
      "ep 2394: ep_len:55 episode reward: total was 26.000000. running mean: -27.157765\n",
      "ep 2394: ep_len:756 episode reward: total was -71.880000. running mean: -27.604987\n",
      "ep 2394: ep_len:2793 episode reward: total was 10.440000. running mean: -27.224537\n",
      "ep 2394: ep_len:57 episode reward: total was 27.000000. running mean: -26.682292\n",
      "epsilon:0.009992 episode_count: 36061. steps_count: 38758565.000000\n",
      "ep 2395: ep_len:3646 episode reward: total was -636.760000. running mean: -32.783069\n",
      "ep 2395: ep_len:1614 episode reward: total was -94.150000. running mean: -33.396738\n",
      "ep 2395: ep_len:2938 episode reward: total was -20.190000. running mean: -33.264671\n",
      "ep 2395: ep_len:650 episode reward: total was 19.610000. running mean: -32.735924\n",
      "ep 2395: ep_len:1080 episode reward: total was -8.440000. running mean: -32.492965\n",
      "ep 2395: ep_len:675 episode reward: total was 25.840000. running mean: -31.909635\n",
      "ep 2395: ep_len:1813 episode reward: total was -117.380000. running mean: -32.764339\n",
      "ep 2395: ep_len:864 episode reward: total was 60.780000. running mean: -31.828895\n",
      "ep 2395: ep_len:1465 episode reward: total was -170.230000. running mean: -33.212906\n",
      "ep 2395: ep_len:61 episode reward: total was 29.000000. running mean: -32.590777\n",
      "ep 2395: ep_len:1107 episode reward: total was -15.360000. running mean: -32.418470\n",
      "ep 2395: ep_len:2934 episode reward: total was -39.610000. running mean: -32.490385\n",
      "epsilon:0.009992 episode_count: 36073. steps_count: 38777412.000000\n",
      "ep 2396: ep_len:1478 episode reward: total was 14.060000. running mean: -32.024881\n",
      "ep 2396: ep_len:918 episode reward: total was -1.880000. running mean: -31.723432\n",
      "ep 2396: ep_len:52 episode reward: total was 24.500000. running mean: -31.161198\n",
      "ep 2396: ep_len:3023 episode reward: total was -32.120000. running mean: -31.170786\n",
      "ep 2396: ep_len:678 episode reward: total was 5.280000. running mean: -30.806278\n",
      "ep 2396: ep_len:62 episode reward: total was 28.000000. running mean: -30.218215\n",
      "ep 2396: ep_len:102 episode reward: total was 49.500000. running mean: -29.421033\n",
      "ep 2396: ep_len:1502 episode reward: total was 12.340000. running mean: -29.003423\n",
      "ep 2396: ep_len:318 episode reward: total was 11.500000. running mean: -28.598389\n",
      "ep 2396: ep_len:662 episode reward: total was -54.150000. running mean: -28.853905\n",
      "ep 2396: ep_len:801 episode reward: total was 29.320000. running mean: -28.272166\n",
      "ep 2396: ep_len:571 episode reward: total was 32.810000. running mean: -27.661344\n",
      "ep 2396: ep_len:55 episode reward: total was 26.000000. running mean: -27.124731\n",
      "ep 2396: ep_len:51 episode reward: total was 24.000000. running mean: -26.613483\n",
      "ep 2396: ep_len:99 episode reward: total was 48.000000. running mean: -25.867348\n",
      "ep 2396: ep_len:971 episode reward: total was -61.410000. running mean: -26.222775\n",
      "ep 2396: ep_len:2917 episode reward: total was -1989.860000. running mean: -45.859147\n",
      "epsilon:0.009992 episode_count: 36090. steps_count: 38791672.000000\n",
      "ep 2397: ep_len:905 episode reward: total was 13.410000. running mean: -45.266456\n",
      "ep 2397: ep_len:968 episode reward: total was -6.320000. running mean: -44.876991\n",
      "ep 2397: ep_len:3032 episode reward: total was -33.680000. running mean: -44.765021\n",
      "ep 2397: ep_len:608 episode reward: total was 3.890000. running mean: -44.278471\n",
      "ep 2397: ep_len:51 episode reward: total was 24.000000. running mean: -43.595686\n",
      "ep 2397: ep_len:57 episode reward: total was 25.500000. running mean: -42.904729\n",
      "ep 2397: ep_len:500 episode reward: total was -9.850000. running mean: -42.574182\n",
      "ep 2397: ep_len:356 episode reward: total was 5.730000. running mean: -42.091140\n",
      "ep 2397: ep_len:604 episode reward: total was 9.060000. running mean: -41.579629\n",
      "ep 2397: ep_len:882 episode reward: total was 54.840000. running mean: -40.615433\n",
      "ep 2397: ep_len:636 episode reward: total was 9.430000. running mean: -40.114978\n",
      "ep 2397: ep_len:67 episode reward: total was 32.000000. running mean: -39.393828\n",
      "ep 2397: ep_len:186 episode reward: total was 91.500000. running mean: -38.084890\n",
      "ep 2397: ep_len:69 episode reward: total was 33.000000. running mean: -37.374041\n",
      "ep 2397: ep_len:820 episode reward: total was -16.210000. running mean: -37.162401\n",
      "ep 2397: ep_len:2778 episode reward: total was -10.890000. running mean: -36.899677\n",
      "ep 2397: ep_len:41 episode reward: total was 17.500000. running mean: -36.355680\n",
      "epsilon:0.009992 episode_count: 36107. steps_count: 38804232.000000\n",
      "ep 2398: ep_len:1140 episode reward: total was 11.690000. running mean: -35.875223\n",
      "ep 2398: ep_len:616 episode reward: total was -2.300000. running mean: -35.539471\n",
      "ep 2398: ep_len:2970 episode reward: total was -7.000000. running mean: -35.254076\n",
      "ep 2398: ep_len:605 episode reward: total was 9.650000. running mean: -34.805036\n",
      "ep 2398: ep_len:46 episode reward: total was 21.500000. running mean: -34.241985\n",
      "ep 2398: ep_len:685 episode reward: total was -6.330000. running mean: -33.962865\n",
      "ep 2398: ep_len:3568 episode reward: total was -28.840000. running mean: -33.911637\n",
      "ep 2398: ep_len:1235 episode reward: total was -70.640000. running mean: -34.278920\n",
      "ep 2398: ep_len:7261 episode reward: total was -20.910000. running mean: -34.145231\n",
      "ep 2398: ep_len:565 episode reward: total was -34.920000. running mean: -34.152979\n",
      "ep 2398: ep_len:828 episode reward: total was 27.810000. running mean: -33.533349\n",
      "ep 2398: ep_len:2772 episode reward: total was -29.600000. running mean: -33.494016\n",
      "epsilon:0.009992 episode_count: 36119. steps_count: 38826523.000000\n",
      "ep 2399: ep_len:1424 episode reward: total was 12.630000. running mean: -33.032775\n",
      "ep 2399: ep_len:500 episode reward: total was -1.350000. running mean: -32.715948\n",
      "ep 2399: ep_len:38 episode reward: total was 17.500000. running mean: -32.213788\n",
      "ep 2399: ep_len:3014 episode reward: total was -73.720000. running mean: -32.628850\n",
      "ep 2399: ep_len:508 episode reward: total was -5.370000. running mean: -32.356262\n",
      "ep 2399: ep_len:57 episode reward: total was 27.000000. running mean: -31.762699\n",
      "ep 2399: ep_len:76 episode reward: total was 35.000000. running mean: -31.095072\n",
      "ep 2399: ep_len:915 episode reward: total was 38.650000. running mean: -30.397621\n",
      "ep 2399: ep_len:633 episode reward: total was 25.660000. running mean: -29.837045\n",
      "ep 2399: ep_len:1302 episode reward: total was -76.940000. running mean: -30.308075\n",
      "ep 2399: ep_len:7411 episode reward: total was 36.480000. running mean: -29.640194\n",
      "ep 2399: ep_len:691 episode reward: total was -18.050000. running mean: -29.524292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2399: ep_len:218 episode reward: total was 106.000000. running mean: -28.169049\n",
      "ep 2399: ep_len:46 episode reward: total was 20.000000. running mean: -27.687359\n",
      "ep 2399: ep_len:765 episode reward: total was -113.680000. running mean: -28.547285\n",
      "ep 2399: ep_len:2763 episode reward: total was -6.850000. running mean: -28.330312\n",
      "epsilon:0.009992 episode_count: 36135. steps_count: 38846884.000000\n",
      "ep 2400: ep_len:641 episode reward: total was -23.050000. running mean: -28.277509\n",
      "ep 2400: ep_len:830 episode reward: total was 12.130000. running mean: -27.873434\n",
      "ep 2400: ep_len:2921 episode reward: total was -7.410000. running mean: -27.668800\n",
      "ep 2400: ep_len:1228 episode reward: total was -11.540000. running mean: -27.507512\n",
      "ep 2400: ep_len:33 episode reward: total was 13.500000. running mean: -27.097437\n",
      "ep 2400: ep_len:77 episode reward: total was 35.500000. running mean: -26.471462\n",
      "ep 2400: ep_len:59 episode reward: total was 28.000000. running mean: -25.926748\n",
      "ep 2400: ep_len:732 episode reward: total was -14.060000. running mean: -25.808080\n",
      "ep 2400: ep_len:3658 episode reward: total was -112.170000. running mean: -26.671699\n",
      "ep 2400: ep_len:880 episode reward: total was 5.970000. running mean: -26.345282\n",
      "ep 2400: ep_len:653 episode reward: total was 21.920000. running mean: -25.862630\n",
      "ep 2400: ep_len:627 episode reward: total was -8.040000. running mean: -25.684403\n",
      "ep 2400: ep_len:208 episode reward: total was 101.000000. running mean: -24.417559\n",
      "ep 2400: ep_len:818 episode reward: total was -50.570000. running mean: -24.679084\n",
      "ep 2400: ep_len:2854 episode reward: total was -28.770000. running mean: -24.719993\n",
      "ep 2400: ep_len:70 episode reward: total was 30.500000. running mean: -24.167793\n",
      "epsilon:0.009992 episode_count: 36151. steps_count: 38863173.000000\n",
      "ep 2401: ep_len:1157 episode reward: total was -7.760000. running mean: -24.003715\n",
      "ep 2401: ep_len:500 episode reward: total was 16.440000. running mean: -23.599278\n",
      "ep 2401: ep_len:2963 episode reward: total was -44.890000. running mean: -23.812185\n",
      "ep 2401: ep_len:659 episode reward: total was -5.280000. running mean: -23.626863\n",
      "ep 2401: ep_len:38 episode reward: total was 17.500000. running mean: -23.215595\n",
      "ep 2401: ep_len:500 episode reward: total was 44.250000. running mean: -22.540939\n",
      "ep 2401: ep_len:4061 episode reward: total was -60.470000. running mean: -22.920229\n",
      "ep 2401: ep_len:545 episode reward: total was -31.080000. running mean: -23.001827\n",
      "ep 2401: ep_len:7224 episode reward: total was 41.250000. running mean: -22.359309\n",
      "ep 2401: ep_len:1491 episode reward: total was 9.200000. running mean: -22.043716\n",
      "ep 2401: ep_len:1470 episode reward: total was 3.850000. running mean: -21.784778\n",
      "ep 2401: ep_len:2794 episode reward: total was -5.560000. running mean: -21.622531\n",
      "epsilon:0.009992 episode_count: 36163. steps_count: 38886575.000000\n",
      "ep 2402: ep_len:687 episode reward: total was 13.750000. running mean: -21.268805\n",
      "ep 2402: ep_len:677 episode reward: total was -5.520000. running mean: -21.111317\n",
      "ep 2402: ep_len:3029 episode reward: total was -43.230000. running mean: -21.332504\n",
      "ep 2402: ep_len:500 episode reward: total was -4.530000. running mean: -21.164479\n",
      "ep 2402: ep_len:840 episode reward: total was 22.250000. running mean: -20.730334\n",
      "ep 2402: ep_len:3914 episode reward: total was -23.160000. running mean: -20.754631\n",
      "ep 2402: ep_len:879 episode reward: total was -42.560000. running mean: -20.972685\n",
      "ep 2402: ep_len:7329 episode reward: total was 36.340000. running mean: -20.399558\n",
      "ep 2402: ep_len:929 episode reward: total was 23.220000. running mean: -19.963362\n",
      "ep 2402: ep_len:71 episode reward: total was 34.000000. running mean: -19.423729\n",
      "ep 2402: ep_len:1135 episode reward: total was 0.840000. running mean: -19.221091\n",
      "ep 2402: ep_len:2909 episode reward: total was -7.250000. running mean: -19.101380\n",
      "epsilon:0.009992 episode_count: 36175. steps_count: 38909474.000000\n",
      "ep 2403: ep_len:642 episode reward: total was 15.740000. running mean: -18.752967\n",
      "ep 2403: ep_len:778 episode reward: total was 23.270000. running mean: -18.332737\n",
      "ep 2403: ep_len:56 episode reward: total was 26.500000. running mean: -17.884409\n",
      "ep 2403: ep_len:2895 episode reward: total was -46.810000. running mean: -18.173665\n",
      "ep 2403: ep_len:614 episode reward: total was 6.300000. running mean: -17.928929\n",
      "ep 2403: ep_len:50 episode reward: total was 23.500000. running mean: -17.514639\n",
      "ep 2403: ep_len:121 episode reward: total was 54.500000. running mean: -16.794493\n",
      "ep 2403: ep_len:1449 episode reward: total was -287.640000. running mean: -19.502948\n",
      "ep 2403: ep_len:4056 episode reward: total was -94.150000. running mean: -20.249419\n",
      "ep 2403: ep_len:1245 episode reward: total was -40.240000. running mean: -20.449324\n",
      "ep 2403: ep_len:763 episode reward: total was 16.440000. running mean: -20.080431\n",
      "ep 2403: ep_len:574 episode reward: total was 20.900000. running mean: -19.670627\n",
      "ep 2403: ep_len:171 episode reward: total was 84.000000. running mean: -18.633921\n",
      "ep 2403: ep_len:120 episode reward: total was 54.000000. running mean: -17.907581\n",
      "ep 2403: ep_len:616 episode reward: total was -18.740000. running mean: -17.915906\n",
      "ep 2403: ep_len:2818 episode reward: total was -30.510000. running mean: -18.041847\n",
      "epsilon:0.009992 episode_count: 36191. steps_count: 38926442.000000\n",
      "ep 2404: ep_len:844 episode reward: total was -16.970000. running mean: -18.031128\n",
      "ep 2404: ep_len:1612 episode reward: total was -73.820000. running mean: -18.589017\n",
      "ep 2404: ep_len:52 episode reward: total was 24.500000. running mean: -18.158127\n",
      "ep 2404: ep_len:3017 episode reward: total was -18.720000. running mean: -18.163745\n",
      "ep 2404: ep_len:654 episode reward: total was -11.810000. running mean: -18.100208\n",
      "ep 2404: ep_len:87 episode reward: total was 42.000000. running mean: -17.499206\n",
      "ep 2404: ep_len:688 episode reward: total was 27.570000. running mean: -17.048514\n",
      "ep 2404: ep_len:3685 episode reward: total was 3.730000. running mean: -16.840729\n",
      "ep 2404: ep_len:1266 episode reward: total was -38.620000. running mean: -17.058521\n",
      "ep 2404: ep_len:706 episode reward: total was 5.700000. running mean: -16.830936\n",
      "ep 2404: ep_len:1395 episode reward: total was -1.380000. running mean: -16.676427\n",
      "ep 2404: ep_len:78 episode reward: total was 37.500000. running mean: -16.134663\n",
      "ep 2404: ep_len:170 episode reward: total was 80.500000. running mean: -15.168316\n",
      "ep 2404: ep_len:649 episode reward: total was -10.060000. running mean: -15.117233\n",
      "ep 2404: ep_len:2831 episode reward: total was -19.880000. running mean: -15.164860\n",
      "epsilon:0.009992 episode_count: 36206. steps_count: 38944176.000000\n",
      "ep 2405: ep_len:1108 episode reward: total was -1.210000. running mean: -15.025312\n",
      "ep 2405: ep_len:977 episode reward: total was 21.570000. running mean: -14.659359\n",
      "ep 2405: ep_len:2908 episode reward: total was -21.010000. running mean: -14.722865\n",
      "ep 2405: ep_len:657 episode reward: total was 24.490000. running mean: -14.330736\n",
      "ep 2405: ep_len:166 episode reward: total was 78.500000. running mean: -13.402429\n",
      "ep 2405: ep_len:60 episode reward: total was 28.500000. running mean: -12.983405\n",
      "ep 2405: ep_len:799 episode reward: total was -31.080000. running mean: -13.164371\n",
      "ep 2405: ep_len:3580 episode reward: total was -94.280000. running mean: -13.975527\n",
      "ep 2405: ep_len:552 episode reward: total was 8.780000. running mean: -13.747972\n",
      "ep 2405: ep_len:757 episode reward: total was -1.690000. running mean: -13.627392\n",
      "ep 2405: ep_len:585 episode reward: total was -9.040000. running mean: -13.581518\n",
      "ep 2405: ep_len:168 episode reward: total was 79.500000. running mean: -12.650703\n",
      "ep 2405: ep_len:58 episode reward: total was 27.500000. running mean: -12.249196\n",
      "ep 2405: ep_len:1145 episode reward: total was -19.020000. running mean: -12.316904\n",
      "ep 2405: ep_len:2898 episode reward: total was -11.340000. running mean: -12.307135\n",
      "epsilon:0.009992 episode_count: 36221. steps_count: 38960594.000000\n",
      "ep 2406: ep_len:1449 episode reward: total was -6.070000. running mean: -12.244764\n",
      "ep 2406: ep_len:183 episode reward: total was 4.060000. running mean: -12.081716\n",
      "ep 2406: ep_len:2968 episode reward: total was -72.490000. running mean: -12.685799\n",
      "ep 2406: ep_len:624 episode reward: total was 3.720000. running mean: -12.521741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2406: ep_len:46 episode reward: total was 21.500000. running mean: -12.181523\n",
      "ep 2406: ep_len:958 episode reward: total was -53.790000. running mean: -12.597608\n",
      "ep 2406: ep_len:3924 episode reward: total was -7.850000. running mean: -12.550132\n",
      "ep 2406: ep_len:539 episode reward: total was -4.880000. running mean: -12.473431\n",
      "ep 2406: ep_len:804 episode reward: total was 54.090000. running mean: -11.807796\n",
      "ep 2406: ep_len:673 episode reward: total was -20.190000. running mean: -11.891618\n",
      "ep 2406: ep_len:87 episode reward: total was 39.000000. running mean: -11.382702\n",
      "ep 2406: ep_len:84 episode reward: total was 39.000000. running mean: -10.878875\n",
      "ep 2406: ep_len:676 episode reward: total was -17.490000. running mean: -10.944987\n",
      "ep 2406: ep_len:2814 episode reward: total was -27.150000. running mean: -11.107037\n",
      "epsilon:0.009992 episode_count: 36235. steps_count: 38976423.000000\n",
      "ep 2407: ep_len:1422 episode reward: total was 15.670000. running mean: -10.839266\n",
      "ep 2407: ep_len:184 episode reward: total was 7.620000. running mean: -10.654674\n",
      "ep 2407: ep_len:100 episode reward: total was 48.500000. running mean: -10.063127\n",
      "ep 2407: ep_len:828 episode reward: total was 31.860000. running mean: -9.643896\n",
      "ep 2407: ep_len:96 episode reward: total was 45.000000. running mean: -9.097457\n",
      "ep 2407: ep_len:500 episode reward: total was 20.700000. running mean: -8.799482\n",
      "ep 2407: ep_len:3591 episode reward: total was -12.850000. running mean: -8.839987\n",
      "ep 2407: ep_len:532 episode reward: total was -32.220000. running mean: -9.073787\n",
      "ep 2407: ep_len:7411 episode reward: total was -36.490000. running mean: -9.347950\n",
      "ep 2407: ep_len:867 episode reward: total was 12.390000. running mean: -9.130570\n",
      "ep 2407: ep_len:116 episode reward: total was 53.500000. running mean: -8.504264\n",
      "ep 2407: ep_len:796 episode reward: total was -58.870000. running mean: -9.007922\n",
      "ep 2407: ep_len:2856 episode reward: total was -24.190000. running mean: -9.159742\n",
      "ep 2407: ep_len:59 episode reward: total was 28.000000. running mean: -8.788145\n",
      "epsilon:0.009992 episode_count: 36249. steps_count: 38995781.000000\n",
      "ep 2408: ep_len:806 episode reward: total was -37.370000. running mean: -9.073964\n",
      "ep 2408: ep_len:1628 episode reward: total was -85.840000. running mean: -9.841624\n",
      "ep 2408: ep_len:2956 episode reward: total was -67.210000. running mean: -10.415308\n",
      "ep 2408: ep_len:545 episode reward: total was -60.370000. running mean: -10.914855\n",
      "ep 2408: ep_len:120 episode reward: total was 57.000000. running mean: -10.235706\n",
      "ep 2408: ep_len:1075 episode reward: total was -56.080000. running mean: -10.694149\n",
      "ep 2408: ep_len:664 episode reward: total was 23.340000. running mean: -10.353808\n",
      "ep 2408: ep_len:1286 episode reward: total was -59.020000. running mean: -10.840469\n",
      "ep 2408: ep_len:881 episode reward: total was 53.150000. running mean: -10.200565\n",
      "ep 2408: ep_len:1504 episode reward: total was 3.390000. running mean: -10.064659\n",
      "ep 2408: ep_len:138 episode reward: total was 66.000000. running mean: -9.304013\n",
      "ep 2408: ep_len:1196 episode reward: total was 13.520000. running mean: -9.075772\n",
      "ep 2408: ep_len:2834 episode reward: total was -11.340000. running mean: -9.098415\n",
      "epsilon:0.009992 episode_count: 36262. steps_count: 39011414.000000\n",
      "ep 2409: ep_len:689 episode reward: total was 51.570000. running mean: -8.491731\n",
      "ep 2409: ep_len:798 episode reward: total was 2.170000. running mean: -8.385113\n",
      "ep 2409: ep_len:57 episode reward: total was 24.000000. running mean: -8.061262\n",
      "ep 2409: ep_len:3042 episode reward: total was -26.080000. running mean: -8.241449\n",
      "ep 2409: ep_len:627 episode reward: total was 17.720000. running mean: -7.981835\n",
      "ep 2409: ep_len:1139 episode reward: total was -1.910000. running mean: -7.921117\n",
      "ep 2409: ep_len:3461 episode reward: total was -16.690000. running mean: -8.008805\n",
      "ep 2409: ep_len:540 episode reward: total was -39.210000. running mean: -8.320817\n",
      "ep 2409: ep_len:849 episode reward: total was 58.430000. running mean: -7.653309\n",
      "ep 2409: ep_len:979 episode reward: total was 4.540000. running mean: -7.531376\n",
      "ep 2409: ep_len:77 episode reward: total was 35.500000. running mean: -7.101062\n",
      "ep 2409: ep_len:617 episode reward: total was -10.100000. running mean: -7.131052\n",
      "ep 2409: ep_len:2832 episode reward: total was -1.010000. running mean: -7.069841\n",
      "ep 2409: ep_len:54 episode reward: total was 22.500000. running mean: -6.774143\n",
      "epsilon:0.009992 episode_count: 36276. steps_count: 39027175.000000\n",
      "ep 2410: ep_len:1463 episode reward: total was 29.180000. running mean: -6.414601\n",
      "ep 2410: ep_len:732 episode reward: total was -28.570000. running mean: -6.636155\n",
      "ep 2410: ep_len:64 episode reward: total was 30.500000. running mean: -6.264794\n",
      "ep 2410: ep_len:2938 episode reward: total was -17.900000. running mean: -6.381146\n",
      "ep 2410: ep_len:500 episode reward: total was -17.540000. running mean: -6.492734\n",
      "ep 2410: ep_len:49 episode reward: total was 21.500000. running mean: -6.212807\n",
      "ep 2410: ep_len:115 episode reward: total was 56.000000. running mean: -5.590679\n",
      "ep 2410: ep_len:85 episode reward: total was 41.000000. running mean: -5.124772\n",
      "ep 2410: ep_len:3366 episode reward: total was -173.190000. running mean: -6.805425\n",
      "ep 2410: ep_len:3733 episode reward: total was 2.070000. running mean: -6.716670\n",
      "ep 2410: ep_len:1210 episode reward: total was -48.670000. running mean: -7.136204\n",
      "ep 2410: ep_len:892 episode reward: total was 66.390000. running mean: -6.400942\n",
      "ep 2410: ep_len:605 episode reward: total was -4.650000. running mean: -6.383432\n",
      "ep 2410: ep_len:99 episode reward: total was 46.500000. running mean: -5.854598\n",
      "ep 2410: ep_len:1054 episode reward: total was -80.810000. running mean: -6.604152\n",
      "ep 2410: ep_len:2882 episode reward: total was -8.440000. running mean: -6.622510\n",
      "epsilon:0.009992 episode_count: 36292. steps_count: 39046962.000000\n",
      "ep 2411: ep_len:676 episode reward: total was -8.560000. running mean: -6.641885\n",
      "ep 2411: ep_len:211 episode reward: total was 13.460000. running mean: -6.440866\n",
      "ep 2411: ep_len:2950 episode reward: total was -30.210000. running mean: -6.678558\n",
      "ep 2411: ep_len:761 episode reward: total was 18.550000. running mean: -6.426272\n",
      "ep 2411: ep_len:49 episode reward: total was 23.000000. running mean: -6.132009\n",
      "ep 2411: ep_len:67 episode reward: total was 32.000000. running mean: -5.750689\n",
      "ep 2411: ep_len:890 episode reward: total was 38.580000. running mean: -5.307382\n",
      "ep 2411: ep_len:3700 episode reward: total was -109.910000. running mean: -6.353409\n",
      "ep 2411: ep_len:1202 episode reward: total was -36.570000. running mean: -6.655574\n",
      "ep 2411: ep_len:677 episode reward: total was 18.520000. running mean: -6.403819\n",
      "ep 2411: ep_len:617 episode reward: total was 7.140000. running mean: -6.268381\n",
      "ep 2411: ep_len:711 episode reward: total was 27.620000. running mean: -5.929497\n",
      "ep 2411: ep_len:2846 episode reward: total was 1.270000. running mean: -5.857502\n",
      "ep 2411: ep_len:74 episode reward: total was 34.000000. running mean: -5.458927\n",
      "epsilon:0.009992 episode_count: 36306. steps_count: 39062393.000000\n",
      "ep 2412: ep_len:1789 episode reward: total was -917.200000. running mean: -14.576337\n",
      "ep 2412: ep_len:679 episode reward: total was -24.600000. running mean: -14.676574\n",
      "ep 2412: ep_len:3021 episode reward: total was -34.710000. running mean: -14.876908\n",
      "ep 2412: ep_len:500 episode reward: total was -31.100000. running mean: -15.039139\n",
      "ep 2412: ep_len:50 episode reward: total was 23.500000. running mean: -14.653748\n",
      "ep 2412: ep_len:106 episode reward: total was 48.500000. running mean: -14.022210\n",
      "ep 2412: ep_len:1515 episode reward: total was -278.590000. running mean: -16.667888\n",
      "ep 2412: ep_len:3642 episode reward: total was -48.210000. running mean: -16.983309\n",
      "ep 2412: ep_len:1564 episode reward: total was -16.980000. running mean: -16.983276\n",
      "ep 2412: ep_len:606 episode reward: total was 0.380000. running mean: -16.809644\n",
      "ep 2412: ep_len:606 episode reward: total was -8.740000. running mean: -16.728947\n",
      "ep 2412: ep_len:189 episode reward: total was 90.000000. running mean: -15.661658\n",
      "ep 2412: ep_len:43 episode reward: total was 17.000000. running mean: -15.335041\n",
      "ep 2412: ep_len:615 episode reward: total was -8.190000. running mean: -15.263591\n",
      "ep 2412: ep_len:2928 episode reward: total was -56.460000. running mean: -15.675555\n",
      "epsilon:0.009992 episode_count: 36321. steps_count: 39080246.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2413: ep_len:1463 episode reward: total was 30.160000. running mean: -15.217199\n",
      "ep 2413: ep_len:681 episode reward: total was -13.560000. running mean: -15.200627\n",
      "ep 2413: ep_len:2839 episode reward: total was -47.070000. running mean: -15.519321\n",
      "ep 2413: ep_len:706 episode reward: total was -7.880000. running mean: -15.442928\n",
      "ep 2413: ep_len:174 episode reward: total was 81.000000. running mean: -14.478498\n",
      "ep 2413: ep_len:1515 episode reward: total was -18.820000. running mean: -14.521913\n",
      "ep 2413: ep_len:3928 episode reward: total was -86.760000. running mean: -15.244294\n",
      "ep 2413: ep_len:838 episode reward: total was -20.620000. running mean: -15.298051\n",
      "ep 2413: ep_len:900 episode reward: total was 50.580000. running mean: -14.639271\n",
      "ep 2413: ep_len:995 episode reward: total was 13.090000. running mean: -14.361978\n",
      "ep 2413: ep_len:83 episode reward: total was 40.000000. running mean: -13.818358\n",
      "ep 2413: ep_len:170 episode reward: total was 83.500000. running mean: -12.845175\n",
      "ep 2413: ep_len:58 episode reward: total was 26.000000. running mean: -12.456723\n",
      "ep 2413: ep_len:648 episode reward: total was -7.520000. running mean: -12.407356\n",
      "ep 2413: ep_len:2793 episode reward: total was -14.840000. running mean: -12.431682\n",
      "ep 2413: ep_len:60 episode reward: total was 27.000000. running mean: -12.037365\n",
      "epsilon:0.009992 episode_count: 36337. steps_count: 39098097.000000\n",
      "ep 2414: ep_len:1438 episode reward: total was 21.000000. running mean: -11.706992\n",
      "ep 2414: ep_len:764 episode reward: total was -13.710000. running mean: -11.727022\n",
      "ep 2414: ep_len:66 episode reward: total was 30.000000. running mean: -11.309752\n",
      "ep 2414: ep_len:3069 episode reward: total was -41.730000. running mean: -11.613954\n",
      "ep 2414: ep_len:503 episode reward: total was -20.360000. running mean: -11.701415\n",
      "ep 2414: ep_len:37 episode reward: total was 17.000000. running mean: -11.414400\n",
      "ep 2414: ep_len:94 episode reward: total was 44.000000. running mean: -10.860256\n",
      "ep 2414: ep_len:50 episode reward: total was 23.500000. running mean: -10.516654\n",
      "ep 2414: ep_len:1499 episode reward: total was 21.860000. running mean: -10.192887\n",
      "ep 2414: ep_len:659 episode reward: total was 21.240000. running mean: -9.878558\n",
      "ep 2414: ep_len:836 episode reward: total was 22.700000. running mean: -9.552773\n",
      "ep 2414: ep_len:854 episode reward: total was 59.360000. running mean: -8.863645\n",
      "ep 2414: ep_len:556 episode reward: total was 34.430000. running mean: -8.430709\n",
      "ep 2414: ep_len:69 episode reward: total was 33.000000. running mean: -8.016402\n",
      "ep 2414: ep_len:35 episode reward: total was 16.000000. running mean: -7.776238\n",
      "ep 2414: ep_len:519 episode reward: total was -194.400000. running mean: -9.642475\n",
      "ep 2414: ep_len:2813 episode reward: total was -19.970000. running mean: -9.745750\n",
      "ep 2414: ep_len:55 episode reward: total was 26.000000. running mean: -9.388293\n",
      "epsilon:0.009992 episode_count: 36355. steps_count: 39112013.000000\n",
      "ep 2415: ep_len:922 episode reward: total was 16.060000. running mean: -9.133810\n",
      "ep 2415: ep_len:758 episode reward: total was -33.040000. running mean: -9.372872\n",
      "ep 2415: ep_len:3022 episode reward: total was -29.370000. running mean: -9.572843\n",
      "ep 2415: ep_len:622 episode reward: total was -0.010000. running mean: -9.477215\n",
      "ep 2415: ep_len:68 episode reward: total was 32.500000. running mean: -9.057443\n",
      "ep 2415: ep_len:150 episode reward: total was 73.500000. running mean: -8.231868\n",
      "ep 2415: ep_len:83 episode reward: total was 40.000000. running mean: -7.749550\n",
      "ep 2415: ep_len:500 episode reward: total was -2.660000. running mean: -7.698654\n",
      "ep 2415: ep_len:670 episode reward: total was 23.830000. running mean: -7.383368\n",
      "ep 2415: ep_len:4188 episode reward: total was -496.090000. running mean: -12.270434\n",
      "ep 2415: ep_len:7638 episode reward: total was -548.190000. running mean: -17.629629\n",
      "ep 2415: ep_len:1070 episode reward: total was -14.200000. running mean: -17.595333\n",
      "ep 2415: ep_len:38 episode reward: total was 17.500000. running mean: -17.244380\n",
      "ep 2415: ep_len:702 episode reward: total was -11.930000. running mean: -17.191236\n",
      "ep 2415: ep_len:2920 episode reward: total was -17.000000. running mean: -17.189324\n",
      "epsilon:0.009992 episode_count: 36370. steps_count: 39135364.000000\n",
      "ep 2416: ep_len:1439 episode reward: total was 14.220000. running mean: -16.875230\n",
      "ep 2416: ep_len:774 episode reward: total was -3.710000. running mean: -16.743578\n",
      "ep 2416: ep_len:2993 episode reward: total was -38.940000. running mean: -16.965542\n",
      "ep 2416: ep_len:664 episode reward: total was 6.960000. running mean: -16.726287\n",
      "ep 2416: ep_len:107 episode reward: total was 50.500000. running mean: -16.054024\n",
      "ep 2416: ep_len:1533 episode reward: total was 34.210000. running mean: -15.551384\n",
      "ep 2416: ep_len:333 episode reward: total was 27.050000. running mean: -15.125370\n",
      "ep 2416: ep_len:613 episode reward: total was -42.520000. running mean: -15.399316\n",
      "ep 2416: ep_len:856 episode reward: total was 58.470000. running mean: -14.660623\n",
      "ep 2416: ep_len:933 episode reward: total was 32.710000. running mean: -14.186917\n",
      "ep 2416: ep_len:75 episode reward: total was 36.000000. running mean: -13.685048\n",
      "ep 2416: ep_len:841 episode reward: total was -41.450000. running mean: -13.962697\n",
      "ep 2416: ep_len:2886 episode reward: total was -8.920000. running mean: -13.912270\n",
      "ep 2416: ep_len:61 episode reward: total was 27.500000. running mean: -13.498148\n",
      "epsilon:0.009992 episode_count: 36384. steps_count: 39149472.000000\n",
      "ep 2417: ep_len:680 episode reward: total was 10.650000. running mean: -13.256666\n",
      "ep 2417: ep_len:775 episode reward: total was -16.660000. running mean: -13.290699\n",
      "ep 2417: ep_len:99 episode reward: total was 46.500000. running mean: -12.692792\n",
      "ep 2417: ep_len:500 episode reward: total was 13.080000. running mean: -12.435065\n",
      "ep 2417: ep_len:500 episode reward: total was 15.380000. running mean: -12.156914\n",
      "ep 2417: ep_len:3970 episode reward: total was -698.800000. running mean: -19.023345\n",
      "ep 2417: ep_len:934 episode reward: total was -39.800000. running mean: -19.231111\n",
      "ep 2417: ep_len:773 episode reward: total was -3.210000. running mean: -19.070900\n",
      "ep 2417: ep_len:1490 episode reward: total was 4.690000. running mean: -18.833291\n",
      "ep 2417: ep_len:1487 episode reward: total was 25.360000. running mean: -18.391358\n",
      "ep 2417: ep_len:2879 episode reward: total was -19.370000. running mean: -18.401145\n",
      "epsilon:0.009992 episode_count: 36395. steps_count: 39163559.000000\n",
      "ep 2418: ep_len:1179 episode reward: total was 6.580000. running mean: -18.151333\n",
      "ep 2418: ep_len:770 episode reward: total was -34.610000. running mean: -18.315920\n",
      "ep 2418: ep_len:3038 episode reward: total was -98.780000. running mean: -19.120561\n",
      "ep 2418: ep_len:639 episode reward: total was -5.950000. running mean: -18.988855\n",
      "ep 2418: ep_len:57 episode reward: total was 27.000000. running mean: -18.528967\n",
      "ep 2418: ep_len:69 episode reward: total was 31.500000. running mean: -18.028677\n",
      "ep 2418: ep_len:1050 episode reward: total was -49.260000. running mean: -18.340990\n",
      "ep 2418: ep_len:660 episode reward: total was 19.660000. running mean: -17.960980\n",
      "ep 2418: ep_len:500 episode reward: total was 16.230000. running mean: -17.619070\n",
      "ep 2418: ep_len:7356 episode reward: total was -249.870000. running mean: -19.941580\n",
      "ep 2418: ep_len:683 episode reward: total was -13.480000. running mean: -19.876964\n",
      "ep 2418: ep_len:61 episode reward: total was 29.000000. running mean: -19.388194\n",
      "ep 2418: ep_len:1020 episode reward: total was 36.170000. running mean: -18.832612\n",
      "ep 2418: ep_len:2854 episode reward: total was 14.910000. running mean: -18.495186\n",
      "ep 2418: ep_len:59 episode reward: total was 28.000000. running mean: -18.030234\n",
      "epsilon:0.009992 episode_count: 36410. steps_count: 39183554.000000\n",
      "ep 2419: ep_len:1382 episode reward: total was -4.570000. running mean: -17.895632\n",
      "ep 2419: ep_len:209 episode reward: total was 7.900000. running mean: -17.637676\n",
      "ep 2419: ep_len:2961 episode reward: total was -42.590000. running mean: -17.887199\n",
      "ep 2419: ep_len:1157 episode reward: total was -34.040000. running mean: -18.048727\n",
      "ep 2419: ep_len:122 episode reward: total was 58.000000. running mean: -17.288240\n",
      "ep 2419: ep_len:43 episode reward: total was 18.500000. running mean: -16.930357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2419: ep_len:1390 episode reward: total was -192.580000. running mean: -18.686854\n",
      "ep 2419: ep_len:635 episode reward: total was 18.060000. running mean: -18.319385\n",
      "ep 2419: ep_len:1537 episode reward: total was -36.320000. running mean: -18.499391\n",
      "ep 2419: ep_len:877 episode reward: total was 39.150000. running mean: -17.922897\n",
      "ep 2419: ep_len:661 episode reward: total was 32.350000. running mean: -17.420168\n",
      "ep 2419: ep_len:43 episode reward: total was 20.000000. running mean: -17.045967\n",
      "ep 2419: ep_len:1068 episode reward: total was 4.490000. running mean: -16.830607\n",
      "ep 2419: ep_len:2930 episode reward: total was -14.600000. running mean: -16.808301\n",
      "epsilon:0.009992 episode_count: 36424. steps_count: 39198569.000000\n",
      "ep 2420: ep_len:633 episode reward: total was 21.340000. running mean: -16.426818\n",
      "ep 2420: ep_len:211 episode reward: total was 7.400000. running mean: -16.188550\n",
      "ep 2420: ep_len:2940 episode reward: total was -42.940000. running mean: -16.456064\n",
      "ep 2420: ep_len:1232 episode reward: total was -150.310000. running mean: -17.794604\n",
      "ep 2420: ep_len:43 episode reward: total was 18.500000. running mean: -17.431658\n",
      "ep 2420: ep_len:674 episode reward: total was -6.070000. running mean: -17.318041\n",
      "ep 2420: ep_len:627 episode reward: total was 27.930000. running mean: -16.865561\n",
      "ep 2420: ep_len:896 episode reward: total was 30.860000. running mean: -16.388305\n",
      "ep 2420: ep_len:861 episode reward: total was 77.380000. running mean: -15.450622\n",
      "ep 2420: ep_len:519 episode reward: total was 49.520000. running mean: -14.800916\n",
      "ep 2420: ep_len:189 episode reward: total was 91.500000. running mean: -13.737907\n",
      "ep 2420: ep_len:781 episode reward: total was -3.370000. running mean: -13.634228\n",
      "ep 2420: ep_len:2808 episode reward: total was -5.750000. running mean: -13.555385\n",
      "ep 2420: ep_len:22 episode reward: total was 9.500000. running mean: -13.324831\n",
      "epsilon:0.009992 episode_count: 36438. steps_count: 39211005.000000\n",
      "ep 2421: ep_len:642 episode reward: total was 0.930000. running mean: -13.182283\n",
      "ep 2421: ep_len:736 episode reward: total was -37.620000. running mean: -13.426660\n",
      "ep 2421: ep_len:3030 episode reward: total was -6.240000. running mean: -13.354794\n",
      "ep 2421: ep_len:831 episode reward: total was 3.780000. running mean: -13.183446\n",
      "ep 2421: ep_len:115 episode reward: total was 54.500000. running mean: -12.506611\n",
      "ep 2421: ep_len:1124 episode reward: total was -7.110000. running mean: -12.452645\n",
      "ep 2421: ep_len:3643 episode reward: total was -60.320000. running mean: -12.931319\n",
      "ep 2421: ep_len:1565 episode reward: total was -7.010000. running mean: -12.872106\n",
      "ep 2421: ep_len:864 episode reward: total was 50.950000. running mean: -12.233884\n",
      "ep 2421: ep_len:691 episode reward: total was 3.740000. running mean: -12.074146\n",
      "ep 2421: ep_len:91 episode reward: total was 41.000000. running mean: -11.543404\n",
      "ep 2421: ep_len:59 episode reward: total was 28.000000. running mean: -11.147970\n",
      "ep 2421: ep_len:785 episode reward: total was 22.300000. running mean: -10.813490\n",
      "ep 2421: ep_len:2798 episode reward: total was -35.790000. running mean: -11.063256\n",
      "ep 2421: ep_len:68 episode reward: total was 31.000000. running mean: -10.642623\n",
      "epsilon:0.009992 episode_count: 36453. steps_count: 39228047.000000\n",
      "ep 2422: ep_len:791 episode reward: total was -19.530000. running mean: -10.731497\n",
      "ep 2422: ep_len:708 episode reward: total was -30.140000. running mean: -10.925582\n",
      "ep 2422: ep_len:3029 episode reward: total was -8.030000. running mean: -10.896626\n",
      "ep 2422: ep_len:726 episode reward: total was -11.090000. running mean: -10.898560\n",
      "ep 2422: ep_len:102 episode reward: total was 45.000000. running mean: -10.339574\n",
      "ep 2422: ep_len:59 episode reward: total was 25.000000. running mean: -9.986178\n",
      "ep 2422: ep_len:693 episode reward: total was -47.510000. running mean: -10.361417\n",
      "ep 2422: ep_len:3697 episode reward: total was -16.150000. running mean: -10.419302\n",
      "ep 2422: ep_len:865 episode reward: total was -10.890000. running mean: -10.424009\n",
      "ep 2422: ep_len:829 episode reward: total was 1.920000. running mean: -10.300569\n",
      "ep 2422: ep_len:500 episode reward: total was -1.160000. running mean: -10.209164\n",
      "ep 2422: ep_len:58 episode reward: total was 27.500000. running mean: -9.832072\n",
      "ep 2422: ep_len:37 episode reward: total was 17.000000. running mean: -9.563751\n",
      "ep 2422: ep_len:1433 episode reward: total was -37.690000. running mean: -9.845014\n",
      "ep 2422: ep_len:2923 episode reward: total was -1.630000. running mean: -9.762864\n",
      "epsilon:0.009992 episode_count: 36468. steps_count: 39244497.000000\n",
      "ep 2423: ep_len:656 episode reward: total was -8.200000. running mean: -9.747235\n",
      "ep 2423: ep_len:757 episode reward: total was -13.300000. running mean: -9.782763\n",
      "ep 2423: ep_len:87 episode reward: total was 42.000000. running mean: -9.264935\n",
      "ep 2423: ep_len:4449 episode reward: total was -1007.090000. running mean: -19.243186\n",
      "ep 2423: ep_len:58 episode reward: total was 27.500000. running mean: -18.775754\n",
      "ep 2423: ep_len:1098 episode reward: total was -10.280000. running mean: -18.690796\n",
      "ep 2423: ep_len:3851 episode reward: total was -33.430000. running mean: -18.838188\n",
      "ep 2423: ep_len:889 episode reward: total was -26.750000. running mean: -18.917306\n",
      "ep 2423: ep_len:844 episode reward: total was 47.540000. running mean: -18.252733\n",
      "ep 2423: ep_len:612 episode reward: total was 6.960000. running mean: -18.000606\n",
      "ep 2423: ep_len:76 episode reward: total was 36.500000. running mean: -17.455600\n",
      "ep 2423: ep_len:128 episode reward: total was 62.500000. running mean: -16.656044\n",
      "ep 2423: ep_len:84 episode reward: total was 39.000000. running mean: -16.099483\n",
      "ep 2423: ep_len:761 episode reward: total was -50.280000. running mean: -16.441289\n",
      "ep 2423: ep_len:2893 episode reward: total was -44.870000. running mean: -16.725576\n",
      "ep 2423: ep_len:44 episode reward: total was 20.500000. running mean: -16.353320\n",
      "epsilon:0.009992 episode_count: 36484. steps_count: 39261784.000000\n",
      "ep 2424: ep_len:861 episode reward: total was 17.830000. running mean: -16.011487\n",
      "ep 2424: ep_len:636 episode reward: total was -16.510000. running mean: -16.016472\n",
      "ep 2424: ep_len:61 episode reward: total was 26.000000. running mean: -15.596307\n",
      "ep 2424: ep_len:2898 episode reward: total was -75.270000. running mean: -16.193044\n",
      "ep 2424: ep_len:774 episode reward: total was -20.150000. running mean: -16.232614\n",
      "ep 2424: ep_len:39 episode reward: total was 18.000000. running mean: -15.890288\n",
      "ep 2424: ep_len:89 episode reward: total was 43.000000. running mean: -15.301385\n",
      "ep 2424: ep_len:98 episode reward: total was 44.500000. running mean: -14.703371\n",
      "ep 2424: ep_len:79 episode reward: total was 36.500000. running mean: -14.191337\n",
      "ep 2424: ep_len:500 episode reward: total was 9.650000. running mean: -13.952924\n",
      "ep 2424: ep_len:3520 episode reward: total was -14.570000. running mean: -13.959095\n",
      "ep 2424: ep_len:1318 episode reward: total was -97.080000. running mean: -14.790304\n",
      "ep 2424: ep_len:726 episode reward: total was 3.050000. running mean: -14.611901\n",
      "ep 2424: ep_len:500 episode reward: total was 5.940000. running mean: -14.406382\n",
      "ep 2424: ep_len:69 episode reward: total was 33.000000. running mean: -13.932318\n",
      "ep 2424: ep_len:190 episode reward: total was 93.500000. running mean: -12.857995\n",
      "ep 2424: ep_len:1110 episode reward: total was 11.970000. running mean: -12.609715\n",
      "ep 2424: ep_len:2711 episode reward: total was -8.870000. running mean: -12.572317\n",
      "epsilon:0.009992 episode_count: 36502. steps_count: 39277963.000000\n",
      "ep 2425: ep_len:1490 episode reward: total was -24.540000. running mean: -12.691994\n",
      "ep 2425: ep_len:751 episode reward: total was -13.870000. running mean: -12.703774\n",
      "ep 2425: ep_len:61 episode reward: total was 29.000000. running mean: -12.286737\n",
      "ep 2425: ep_len:2998 episode reward: total was -23.340000. running mean: -12.397269\n",
      "ep 2425: ep_len:501 episode reward: total was 1.760000. running mean: -12.255697\n",
      "ep 2425: ep_len:54 episode reward: total was 24.000000. running mean: -11.893140\n",
      "ep 2425: ep_len:61 episode reward: total was 27.500000. running mean: -11.499208\n",
      "ep 2425: ep_len:663 episode reward: total was -1.620000. running mean: -11.400416\n",
      "ep 2425: ep_len:3764 episode reward: total was -11.870000. running mean: -11.405112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2425: ep_len:867 episode reward: total was -59.980000. running mean: -11.890861\n",
      "ep 2425: ep_len:882 episode reward: total was 47.280000. running mean: -11.299152\n",
      "ep 2425: ep_len:500 episode reward: total was 36.380000. running mean: -10.822361\n",
      "ep 2425: ep_len:191 episode reward: total was 91.000000. running mean: -9.804137\n",
      "ep 2425: ep_len:1522 episode reward: total was -15.620000. running mean: -9.862296\n",
      "ep 2425: ep_len:2911 episode reward: total was -20.180000. running mean: -9.965473\n",
      "epsilon:0.009992 episode_count: 36517. steps_count: 39295179.000000\n",
      "ep 2426: ep_len:1060 episode reward: total was -16.840000. running mean: -10.034218\n",
      "ep 2426: ep_len:971 episode reward: total was 12.630000. running mean: -9.807576\n",
      "ep 2426: ep_len:63 episode reward: total was 27.000000. running mean: -9.439500\n",
      "ep 2426: ep_len:2993 episode reward: total was -52.340000. running mean: -9.868505\n",
      "ep 2426: ep_len:656 episode reward: total was -1.690000. running mean: -9.786720\n",
      "ep 2426: ep_len:41 episode reward: total was 16.000000. running mean: -9.528853\n",
      "ep 2426: ep_len:138 episode reward: total was 66.000000. running mean: -8.773564\n",
      "ep 2426: ep_len:102 episode reward: total was 48.000000. running mean: -8.205829\n",
      "ep 2426: ep_len:67 episode reward: total was 30.500000. running mean: -7.818770\n",
      "ep 2426: ep_len:810 episode reward: total was 34.010000. running mean: -7.400483\n",
      "ep 2426: ep_len:3748 episode reward: total was -11.280000. running mean: -7.439278\n",
      "ep 2426: ep_len:2883 episode reward: total was -1026.600000. running mean: -17.630885\n",
      "ep 2426: ep_len:652 episode reward: total was 13.030000. running mean: -17.324276\n",
      "ep 2426: ep_len:902 episode reward: total was 66.660000. running mean: -16.484433\n",
      "ep 2426: ep_len:89 episode reward: total was 43.000000. running mean: -15.889589\n",
      "ep 2426: ep_len:1175 episode reward: total was -4.810000. running mean: -15.778793\n",
      "ep 2426: ep_len:2775 episode reward: total was -10.950000. running mean: -15.730505\n",
      "ep 2426: ep_len:45 episode reward: total was 21.000000. running mean: -15.363200\n",
      "epsilon:0.009992 episode_count: 36535. steps_count: 39314349.000000\n",
      "ep 2427: ep_len:1141 episode reward: total was -19.060000. running mean: -15.400168\n",
      "ep 2427: ep_len:802 episode reward: total was 7.830000. running mean: -15.167867\n",
      "ep 2427: ep_len:61 episode reward: total was 27.500000. running mean: -14.741188\n",
      "ep 2427: ep_len:2993 episode reward: total was -21.310000. running mean: -14.806876\n",
      "ep 2427: ep_len:546 episode reward: total was -25.010000. running mean: -14.908907\n",
      "ep 2427: ep_len:63 episode reward: total was 30.000000. running mean: -14.459818\n",
      "ep 2427: ep_len:73 episode reward: total was 33.500000. running mean: -13.980220\n",
      "ep 2427: ep_len:32 episode reward: total was 14.500000. running mean: -13.695418\n",
      "ep 2427: ep_len:622 episode reward: total was 47.960000. running mean: -13.078864\n",
      "ep 2427: ep_len:4064 episode reward: total was -81.980000. running mean: -13.767875\n",
      "ep 2427: ep_len:655 episode reward: total was 6.380000. running mean: -13.566396\n",
      "ep 2427: ep_len:784 episode reward: total was 11.930000. running mean: -13.311432\n",
      "ep 2427: ep_len:1119 episode reward: total was -17.260000. running mean: -13.350918\n",
      "ep 2427: ep_len:130 episode reward: total was 62.000000. running mean: -12.597409\n",
      "ep 2427: ep_len:49 episode reward: total was 23.000000. running mean: -12.241435\n",
      "ep 2427: ep_len:904 episode reward: total was -9.100000. running mean: -12.210020\n",
      "ep 2427: ep_len:2888 episode reward: total was -21.080000. running mean: -12.298720\n",
      "ep 2427: ep_len:53 episode reward: total was 23.500000. running mean: -11.940733\n",
      "epsilon:0.009992 episode_count: 36553. steps_count: 39331328.000000\n",
      "ep 2428: ep_len:1157 episode reward: total was -4.760000. running mean: -11.868926\n",
      "ep 2428: ep_len:743 episode reward: total was -13.830000. running mean: -11.888536\n",
      "ep 2428: ep_len:66 episode reward: total was 31.500000. running mean: -11.454651\n",
      "ep 2428: ep_len:2959 episode reward: total was -34.440000. running mean: -11.684504\n",
      "ep 2428: ep_len:834 episode reward: total was 11.980000. running mean: -11.447859\n",
      "ep 2428: ep_len:104 episode reward: total was 49.000000. running mean: -10.843381\n",
      "ep 2428: ep_len:849 episode reward: total was 45.760000. running mean: -10.277347\n",
      "ep 2428: ep_len:340 episode reward: total was 16.040000. running mean: -10.014174\n",
      "ep 2428: ep_len:771 episode reward: total was -36.370000. running mean: -10.277732\n",
      "ep 2428: ep_len:7349 episode reward: total was 51.990000. running mean: -9.655055\n",
      "ep 2428: ep_len:990 episode reward: total was 43.060000. running mean: -9.127904\n",
      "ep 2428: ep_len:500 episode reward: total was 29.640000. running mean: -8.740225\n",
      "ep 2428: ep_len:2743 episode reward: total was -19.140000. running mean: -8.844223\n",
      "epsilon:0.009992 episode_count: 36566. steps_count: 39350733.000000\n",
      "ep 2429: ep_len:1141 episode reward: total was -7.610000. running mean: -8.831880\n",
      "ep 2429: ep_len:186 episode reward: total was 20.250000. running mean: -8.541062\n",
      "ep 2429: ep_len:59 episode reward: total was 28.000000. running mean: -8.175651\n",
      "ep 2429: ep_len:2983 episode reward: total was -76.890000. running mean: -8.862795\n",
      "ep 2429: ep_len:500 episode reward: total was 0.370000. running mean: -8.770467\n",
      "ep 2429: ep_len:156 episode reward: total was 76.500000. running mean: -7.917762\n",
      "ep 2429: ep_len:1428 episode reward: total was -166.270000. running mean: -9.501284\n",
      "ep 2429: ep_len:671 episode reward: total was 25.250000. running mean: -9.153771\n",
      "ep 2429: ep_len:663 episode reward: total was -38.990000. running mean: -9.452134\n",
      "ep 2429: ep_len:649 episode reward: total was -22.770000. running mean: -9.585312\n",
      "ep 2429: ep_len:631 episode reward: total was 11.920000. running mean: -9.370259\n",
      "ep 2429: ep_len:53 episode reward: total was 23.500000. running mean: -9.041557\n",
      "ep 2429: ep_len:1067 episode reward: total was -36.390000. running mean: -9.315041\n",
      "ep 2429: ep_len:2790 episode reward: total was -7.770000. running mean: -9.299591\n",
      "ep 2429: ep_len:55 episode reward: total was 26.000000. running mean: -8.946595\n",
      "epsilon:0.009992 episode_count: 36581. steps_count: 39363765.000000\n",
      "ep 2430: ep_len:659 episode reward: total was 0.360000. running mean: -8.853529\n",
      "ep 2430: ep_len:812 episode reward: total was -14.270000. running mean: -8.907694\n",
      "ep 2430: ep_len:37 episode reward: total was 15.500000. running mean: -8.663617\n",
      "ep 2430: ep_len:3029 episode reward: total was -0.720000. running mean: -8.584180\n",
      "ep 2430: ep_len:741 episode reward: total was 31.100000. running mean: -8.187339\n",
      "ep 2430: ep_len:38 episode reward: total was 16.000000. running mean: -7.945465\n",
      "ep 2430: ep_len:78 episode reward: total was 37.500000. running mean: -7.491011\n",
      "ep 2430: ep_len:54 episode reward: total was 25.500000. running mean: -7.161100\n",
      "ep 2430: ep_len:677 episode reward: total was 31.580000. running mean: -6.773689\n",
      "ep 2430: ep_len:3576 episode reward: total was -54.440000. running mean: -7.250353\n",
      "ep 2430: ep_len:1256 episode reward: total was -55.280000. running mean: -7.730649\n",
      "ep 2430: ep_len:841 episode reward: total was 42.360000. running mean: -7.229743\n",
      "ep 2430: ep_len:516 episode reward: total was 19.970000. running mean: -6.957745\n",
      "ep 2430: ep_len:165 episode reward: total was 81.000000. running mean: -6.078168\n",
      "ep 2430: ep_len:500 episode reward: total was 11.880000. running mean: -5.898586\n",
      "ep 2430: ep_len:2783 episode reward: total was 8.960000. running mean: -5.750000\n",
      "ep 2430: ep_len:62 episode reward: total was 29.500000. running mean: -5.397500\n",
      "epsilon:0.009992 episode_count: 36598. steps_count: 39379589.000000\n",
      "ep 2431: ep_len:1426 episode reward: total was 0.310000. running mean: -5.340425\n",
      "ep 2431: ep_len:733 episode reward: total was -17.770000. running mean: -5.464721\n",
      "ep 2431: ep_len:2976 episode reward: total was -21.450000. running mean: -5.624574\n",
      "ep 2431: ep_len:628 episode reward: total was 3.080000. running mean: -5.537528\n",
      "ep 2431: ep_len:146 episode reward: total was 68.500000. running mean: -4.797153\n",
      "ep 2431: ep_len:1463 episode reward: total was -253.550000. running mean: -7.284681\n",
      "ep 2431: ep_len:3794 episode reward: total was -6.060000. running mean: -7.272434\n",
      "ep 2431: ep_len:689 episode reward: total was -30.620000. running mean: -7.505910\n",
      "ep 2431: ep_len:7263 episode reward: total was 24.990000. running mean: -7.180951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2431: ep_len:661 episode reward: total was 20.900000. running mean: -6.900141\n",
      "ep 2431: ep_len:144 episode reward: total was 69.000000. running mean: -6.141140\n",
      "ep 2431: ep_len:76 episode reward: total was 36.500000. running mean: -5.714729\n",
      "ep 2431: ep_len:1176 episode reward: total was -14.810000. running mean: -5.805681\n",
      "ep 2431: ep_len:2828 episode reward: total was 7.730000. running mean: -5.670324\n",
      "epsilon:0.009992 episode_count: 36612. steps_count: 39403592.000000\n",
      "ep 2432: ep_len:1084 episode reward: total was -12.350000. running mean: -5.737121\n",
      "ep 2432: ep_len:691 episode reward: total was -25.110000. running mean: -5.930850\n",
      "ep 2432: ep_len:3023 episode reward: total was -55.330000. running mean: -6.424842\n",
      "ep 2432: ep_len:501 episode reward: total was -21.420000. running mean: -6.574793\n",
      "ep 2432: ep_len:44 episode reward: total was 19.000000. running mean: -6.319045\n",
      "ep 2432: ep_len:86 episode reward: total was 40.000000. running mean: -5.855855\n",
      "ep 2432: ep_len:58 episode reward: total was 26.000000. running mean: -5.537296\n",
      "ep 2432: ep_len:1870 episode reward: total was -76.220000. running mean: -6.244123\n",
      "ep 2432: ep_len:3908 episode reward: total was -94.550000. running mean: -7.127182\n",
      "ep 2432: ep_len:532 episode reward: total was 16.940000. running mean: -6.886510\n",
      "ep 2432: ep_len:634 episode reward: total was -0.350000. running mean: -6.821145\n",
      "ep 2432: ep_len:500 episode reward: total was 8.330000. running mean: -6.669634\n",
      "ep 2432: ep_len:128 episode reward: total was 58.000000. running mean: -6.022937\n",
      "ep 2432: ep_len:1126 episode reward: total was -35.370000. running mean: -6.316408\n",
      "ep 2432: ep_len:2875 episode reward: total was -4.690000. running mean: -6.300144\n",
      "ep 2432: ep_len:75 episode reward: total was 36.000000. running mean: -5.877142\n",
      "epsilon:0.009992 episode_count: 36628. steps_count: 39420727.000000\n",
      "ep 2433: ep_len:729 episode reward: total was -64.560000. running mean: -6.463971\n",
      "ep 2433: ep_len:500 episode reward: total was 36.650000. running mean: -6.032831\n",
      "ep 2433: ep_len:36 episode reward: total was 16.500000. running mean: -5.807503\n",
      "ep 2433: ep_len:2858 episode reward: total was 0.330000. running mean: -5.746128\n",
      "ep 2433: ep_len:514 episode reward: total was 20.050000. running mean: -5.488167\n",
      "ep 2433: ep_len:1125 episode reward: total was -4.400000. running mean: -5.477285\n",
      "ep 2433: ep_len:660 episode reward: total was 23.120000. running mean: -5.191312\n",
      "ep 2433: ep_len:803 episode reward: total was -14.120000. running mean: -5.280599\n",
      "ep 2433: ep_len:764 episode reward: total was 48.300000. running mean: -4.744793\n",
      "ep 2433: ep_len:933 episode reward: total was 65.530000. running mean: -4.042045\n",
      "ep 2433: ep_len:1503 episode reward: total was -11.950000. running mean: -4.121125\n",
      "ep 2433: ep_len:2873 episode reward: total was 0.070000. running mean: -4.079213\n",
      "ep 2433: ep_len:47 episode reward: total was 22.000000. running mean: -3.818421\n",
      "epsilon:0.009992 episode_count: 36641. steps_count: 39434072.000000\n",
      "ep 2434: ep_len:687 episode reward: total was -11.480000. running mean: -3.895037\n",
      "ep 2434: ep_len:201 episode reward: total was 6.290000. running mean: -3.793187\n",
      "ep 2434: ep_len:67 episode reward: total was 32.000000. running mean: -3.435255\n",
      "ep 2434: ep_len:3030 episode reward: total was -31.960000. running mean: -3.720502\n",
      "ep 2434: ep_len:508 episode reward: total was 17.630000. running mean: -3.506997\n",
      "ep 2434: ep_len:57 episode reward: total was 27.000000. running mean: -3.201927\n",
      "ep 2434: ep_len:163 episode reward: total was 77.000000. running mean: -2.399908\n",
      "ep 2434: ep_len:97 episode reward: total was 47.000000. running mean: -1.905909\n",
      "ep 2434: ep_len:59 episode reward: total was 26.500000. running mean: -1.621850\n",
      "ep 2434: ep_len:1343 episode reward: total was -16.770000. running mean: -1.773331\n",
      "ep 2434: ep_len:662 episode reward: total was 19.220000. running mean: -1.563398\n",
      "ep 2434: ep_len:949 episode reward: total was -54.180000. running mean: -2.089564\n",
      "ep 2434: ep_len:771 episode reward: total was 41.170000. running mean: -1.656968\n",
      "ep 2434: ep_len:580 episode reward: total was 60.970000. running mean: -1.030699\n",
      "ep 2434: ep_len:67 episode reward: total was 32.000000. running mean: -0.700392\n",
      "ep 2434: ep_len:117 episode reward: total was 55.500000. running mean: -0.138388\n",
      "ep 2434: ep_len:105 episode reward: total was 51.000000. running mean: 0.372996\n",
      "ep 2434: ep_len:1479 episode reward: total was 1.090000. running mean: 0.380166\n",
      "ep 2434: ep_len:2878 episode reward: total was -1.070000. running mean: 0.365664\n",
      "epsilon:0.009992 episode_count: 36660. steps_count: 39447892.000000\n",
      "ep 2435: ep_len:1418 episode reward: total was 17.740000. running mean: 0.539408\n",
      "ep 2435: ep_len:746 episode reward: total was -10.370000. running mean: 0.430314\n",
      "ep 2435: ep_len:92 episode reward: total was 43.000000. running mean: 0.856011\n",
      "ep 2435: ep_len:500 episode reward: total was 29.670000. running mean: 1.144150\n",
      "ep 2435: ep_len:56 episode reward: total was 26.500000. running mean: 1.397709\n",
      "ep 2435: ep_len:102 episode reward: total was 49.500000. running mean: 1.878732\n",
      "ep 2435: ep_len:64 episode reward: total was 30.500000. running mean: 2.164945\n",
      "ep 2435: ep_len:78 episode reward: total was 37.500000. running mean: 2.518295\n",
      "ep 2435: ep_len:988 episode reward: total was 1.480000. running mean: 2.507912\n",
      "ep 2435: ep_len:3676 episode reward: total was -169.130000. running mean: 0.791533\n",
      "ep 2435: ep_len:1173 episode reward: total was -21.550000. running mean: 0.568118\n",
      "ep 2435: ep_len:710 episode reward: total was 4.560000. running mean: 0.608037\n",
      "ep 2435: ep_len:648 episode reward: total was 1.260000. running mean: 0.614556\n",
      "ep 2435: ep_len:137 episode reward: total was 65.500000. running mean: 1.263411\n",
      "ep 2435: ep_len:42 episode reward: total was 18.000000. running mean: 1.430777\n",
      "ep 2435: ep_len:743 episode reward: total was -43.240000. running mean: 0.984069\n",
      "ep 2435: ep_len:2813 episode reward: total was 3.750000. running mean: 1.011728\n",
      "epsilon:0.009992 episode_count: 36677. steps_count: 39461878.000000\n",
      "ep 2436: ep_len:1069 episode reward: total was -7.660000. running mean: 0.925011\n",
      "ep 2436: ep_len:500 episode reward: total was 16.290000. running mean: 1.078661\n",
      "ep 2436: ep_len:56 episode reward: total was 26.500000. running mean: 1.332874\n",
      "ep 2436: ep_len:2908 episode reward: total was -16.740000. running mean: 1.152145\n",
      "ep 2436: ep_len:831 episode reward: total was -16.240000. running mean: 0.978224\n",
      "ep 2436: ep_len:167 episode reward: total was 82.000000. running mean: 1.788442\n",
      "ep 2436: ep_len:54 episode reward: total was 25.500000. running mean: 2.025557\n",
      "ep 2436: ep_len:1095 episode reward: total was -3.690000. running mean: 1.968402\n",
      "ep 2436: ep_len:500 episode reward: total was 26.710000. running mean: 2.215818\n",
      "ep 2436: ep_len:682 episode reward: total was -16.480000. running mean: 2.028859\n",
      "ep 2436: ep_len:738 episode reward: total was 3.660000. running mean: 2.045171\n",
      "ep 2436: ep_len:755 episode reward: total was 7.940000. running mean: 2.104119\n",
      "ep 2436: ep_len:90 episode reward: total was 43.500000. running mean: 2.518078\n",
      "ep 2436: ep_len:137 episode reward: total was 65.500000. running mean: 3.147897\n",
      "ep 2436: ep_len:1439 episode reward: total was 11.620000. running mean: 3.232618\n",
      "ep 2436: ep_len:2867 episode reward: total was -21.970000. running mean: 2.980592\n",
      "ep 2436: ep_len:57 episode reward: total was 27.000000. running mean: 3.220786\n",
      "epsilon:0.009992 episode_count: 36694. steps_count: 39475823.000000\n",
      "ep 2437: ep_len:1441 episode reward: total was 18.030000. running mean: 3.368878\n",
      "ep 2437: ep_len:209 episode reward: total was 10.990000. running mean: 3.445089\n",
      "ep 2437: ep_len:72 episode reward: total was 34.500000. running mean: 3.755639\n",
      "ep 2437: ep_len:83 episode reward: total was 38.500000. running mean: 4.103082\n",
      "ep 2437: ep_len:497 episode reward: total was 40.760000. running mean: 4.469651\n",
      "ep 2437: ep_len:642 episode reward: total was 3.220000. running mean: 4.457155\n",
      "ep 2437: ep_len:4491 episode reward: total was -570.410000. running mean: -1.291517\n",
      "ep 2437: ep_len:830 episode reward: total was 30.720000. running mean: -0.971402\n",
      "ep 2437: ep_len:872 episode reward: total was 65.210000. running mean: -0.309588\n",
      "ep 2437: ep_len:764 episode reward: total was -2.630000. running mean: -0.332792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2437: ep_len:50 episode reward: total was 23.500000. running mean: -0.094464\n",
      "ep 2437: ep_len:190 episode reward: total was 92.000000. running mean: 0.826481\n",
      "ep 2437: ep_len:85 episode reward: total was 41.000000. running mean: 1.228216\n",
      "ep 2437: ep_len:1087 episode reward: total was -5.680000. running mean: 1.159134\n",
      "ep 2437: ep_len:2835 episode reward: total was -37.010000. running mean: 0.777443\n",
      "epsilon:0.009992 episode_count: 36709. steps_count: 39489971.000000\n",
      "ep 2438: ep_len:1122 episode reward: total was 9.160000. running mean: 0.861268\n",
      "ep 2438: ep_len:500 episode reward: total was 26.240000. running mean: 1.115055\n",
      "ep 2438: ep_len:3047 episode reward: total was -20.190000. running mean: 0.902005\n",
      "ep 2438: ep_len:616 episode reward: total was -4.110000. running mean: 0.851885\n",
      "ep 2438: ep_len:47 episode reward: total was 22.000000. running mean: 1.063366\n",
      "ep 2438: ep_len:62 episode reward: total was 29.500000. running mean: 1.347732\n",
      "ep 2438: ep_len:1409 episode reward: total was 20.600000. running mean: 1.540255\n",
      "ep 2438: ep_len:303 episode reward: total was 11.290000. running mean: 1.637753\n",
      "ep 2438: ep_len:868 episode reward: total was -38.400000. running mean: 1.237375\n",
      "ep 2438: ep_len:696 episode reward: total was 15.370000. running mean: 1.378701\n",
      "ep 2438: ep_len:862 episode reward: total was 20.510000. running mean: 1.570014\n",
      "ep 2438: ep_len:89 episode reward: total was 41.500000. running mean: 1.969314\n",
      "ep 2438: ep_len:621 episode reward: total was 9.750000. running mean: 2.047121\n",
      "ep 2438: ep_len:2791 episode reward: total was -14.040000. running mean: 1.886250\n",
      "ep 2438: ep_len:68 episode reward: total was 31.000000. running mean: 2.177387\n",
      "epsilon:0.009992 episode_count: 36724. steps_count: 39503072.000000\n",
      "ep 2439: ep_len:3499 episode reward: total was -1970.150000. running mean: -17.545887\n",
      "ep 2439: ep_len:1146 episode reward: total was 7.190000. running mean: -17.298528\n",
      "ep 2439: ep_len:3037 episode reward: total was -2.720000. running mean: -17.152742\n",
      "ep 2439: ep_len:702 episode reward: total was 0.380000. running mean: -16.977415\n",
      "ep 2439: ep_len:61 episode reward: total was 29.000000. running mean: -16.517641\n",
      "ep 2439: ep_len:111 episode reward: total was 54.000000. running mean: -15.812465\n",
      "ep 2439: ep_len:593 episode reward: total was 26.240000. running mean: -15.391940\n",
      "ep 2439: ep_len:598 episode reward: total was 25.770000. running mean: -14.980320\n",
      "ep 2439: ep_len:637 episode reward: total was -4.910000. running mean: -14.879617\n",
      "ep 2439: ep_len:807 episode reward: total was 44.560000. running mean: -14.285221\n",
      "ep 2439: ep_len:937 episode reward: total was 63.980000. running mean: -13.502569\n",
      "ep 2439: ep_len:136 episode reward: total was 66.500000. running mean: -12.702543\n",
      "ep 2439: ep_len:500 episode reward: total was 21.280000. running mean: -12.362718\n",
      "ep 2439: ep_len:2804 episode reward: total was -8.090000. running mean: -12.319991\n",
      "epsilon:0.009992 episode_count: 36738. steps_count: 39518640.000000\n",
      "ep 2440: ep_len:996 episode reward: total was -101.260000. running mean: -13.209391\n",
      "ep 2440: ep_len:666 episode reward: total was -21.760000. running mean: -13.294897\n",
      "ep 2440: ep_len:2949 episode reward: total was -51.510000. running mean: -13.677048\n",
      "ep 2440: ep_len:788 episode reward: total was -26.780000. running mean: -13.808077\n",
      "ep 2440: ep_len:38 episode reward: total was 16.000000. running mean: -13.509997\n",
      "ep 2440: ep_len:73 episode reward: total was 35.000000. running mean: -13.024897\n",
      "ep 2440: ep_len:500 episode reward: total was 19.440000. running mean: -12.700248\n",
      "ep 2440: ep_len:3646 episode reward: total was -197.130000. running mean: -14.544545\n",
      "ep 2440: ep_len:558 episode reward: total was -4.660000. running mean: -14.445700\n",
      "ep 2440: ep_len:795 episode reward: total was 33.660000. running mean: -13.964643\n",
      "ep 2440: ep_len:714 episode reward: total was 19.760000. running mean: -13.627396\n",
      "ep 2440: ep_len:141 episode reward: total was 69.000000. running mean: -12.801122\n",
      "ep 2440: ep_len:62 episode reward: total was 29.500000. running mean: -12.378111\n",
      "ep 2440: ep_len:617 episode reward: total was -14.110000. running mean: -12.395430\n",
      "ep 2440: ep_len:2894 episode reward: total was -8.840000. running mean: -12.359876\n",
      "ep 2440: ep_len:48 episode reward: total was 17.510000. running mean: -12.061177\n",
      "epsilon:0.009992 episode_count: 36754. steps_count: 39534125.000000\n",
      "ep 2441: ep_len:1124 episode reward: total was 1.610000. running mean: -11.924465\n",
      "ep 2441: ep_len:637 episode reward: total was 26.340000. running mean: -11.541820\n",
      "ep 2441: ep_len:2898 episode reward: total was -47.510000. running mean: -11.901502\n",
      "ep 2441: ep_len:648 episode reward: total was 14.230000. running mean: -11.640187\n",
      "ep 2441: ep_len:35 episode reward: total was 16.000000. running mean: -11.363785\n",
      "ep 2441: ep_len:63 episode reward: total was 27.000000. running mean: -10.980148\n",
      "ep 2441: ep_len:1397 episode reward: total was 7.030000. running mean: -10.800046\n",
      "ep 2441: ep_len:3560 episode reward: total was -78.320000. running mean: -11.475246\n",
      "ep 2441: ep_len:855 episode reward: total was 18.540000. running mean: -11.175093\n",
      "ep 2441: ep_len:685 episode reward: total was 2.000000. running mean: -11.043342\n",
      "ep 2441: ep_len:898 episode reward: total was 29.720000. running mean: -10.635709\n",
      "ep 2441: ep_len:95 episode reward: total was 46.000000. running mean: -10.069352\n",
      "ep 2441: ep_len:112 episode reward: total was 54.500000. running mean: -9.423658\n",
      "ep 2441: ep_len:1146 episode reward: total was -29.260000. running mean: -9.622022\n",
      "ep 2441: ep_len:2855 episode reward: total was 0.990000. running mean: -9.515901\n",
      "ep 2441: ep_len:59 episode reward: total was 28.000000. running mean: -9.140742\n",
      "epsilon:0.009992 episode_count: 36770. steps_count: 39551192.000000\n",
      "ep 2442: ep_len:845 episode reward: total was -68.770000. running mean: -9.737035\n",
      "ep 2442: ep_len:732 episode reward: total was -16.770000. running mean: -9.807365\n",
      "ep 2442: ep_len:58 episode reward: total was 26.000000. running mean: -9.449291\n",
      "ep 2442: ep_len:3049 episode reward: total was -14.080000. running mean: -9.495598\n",
      "ep 2442: ep_len:500 episode reward: total was 20.000000. running mean: -9.200642\n",
      "ep 2442: ep_len:56 episode reward: total was 26.500000. running mean: -8.843636\n",
      "ep 2442: ep_len:1405 episode reward: total was -159.840000. running mean: -10.353599\n",
      "ep 2442: ep_len:337 episode reward: total was 17.080000. running mean: -10.079263\n",
      "ep 2442: ep_len:663 episode reward: total was 5.450000. running mean: -9.923971\n",
      "ep 2442: ep_len:780 episode reward: total was 9.210000. running mean: -9.732631\n",
      "ep 2442: ep_len:606 episode reward: total was -6.230000. running mean: -9.697605\n",
      "ep 2442: ep_len:62 episode reward: total was 29.500000. running mean: -9.305629\n",
      "ep 2442: ep_len:187 episode reward: total was 90.500000. running mean: -8.307572\n",
      "ep 2442: ep_len:49 episode reward: total was 21.500000. running mean: -8.009497\n",
      "ep 2442: ep_len:970 episode reward: total was 14.250000. running mean: -7.786902\n",
      "ep 2442: ep_len:2794 episode reward: total was -11.160000. running mean: -7.820633\n",
      "epsilon:0.009992 episode_count: 36786. steps_count: 39564285.000000\n",
      "ep 2443: ep_len:727 episode reward: total was 0.350000. running mean: -7.738926\n",
      "ep 2443: ep_len:686 episode reward: total was -20.580000. running mean: -7.867337\n",
      "ep 2443: ep_len:70 episode reward: total was 30.500000. running mean: -7.483664\n",
      "ep 2443: ep_len:3055 episode reward: total was 17.940000. running mean: -7.229427\n",
      "ep 2443: ep_len:505 episode reward: total was 24.520000. running mean: -6.911933\n",
      "ep 2443: ep_len:44 episode reward: total was 20.500000. running mean: -6.637813\n",
      "ep 2443: ep_len:92 episode reward: total was 40.000000. running mean: -6.171435\n",
      "ep 2443: ep_len:51 episode reward: total was 24.000000. running mean: -5.869721\n",
      "ep 2443: ep_len:810 episode reward: total was 29.850000. running mean: -5.512524\n",
      "ep 2443: ep_len:3836 episode reward: total was -57.700000. running mean: -6.034398\n",
      "ep 2443: ep_len:3506 episode reward: total was -813.480000. running mean: -14.108854\n",
      "ep 2443: ep_len:884 episode reward: total was 63.250000. running mean: -13.335266\n",
      "ep 2443: ep_len:500 episode reward: total was 13.660000. running mean: -13.065313\n",
      "ep 2443: ep_len:65 episode reward: total was 29.500000. running mean: -12.639660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2443: ep_len:29 episode reward: total was 13.000000. running mean: -12.383264\n",
      "ep 2443: ep_len:82 episode reward: total was 39.500000. running mean: -11.864431\n",
      "ep 2443: ep_len:660 episode reward: total was -10.270000. running mean: -11.848487\n",
      "ep 2443: ep_len:2772 episode reward: total was 7.050000. running mean: -11.659502\n",
      "epsilon:0.009992 episode_count: 36804. steps_count: 39582659.000000\n",
      "ep 2444: ep_len:598 episode reward: total was -3.950000. running mean: -11.582407\n",
      "ep 2444: ep_len:637 episode reward: total was 21.260000. running mean: -11.253983\n",
      "ep 2444: ep_len:3094 episode reward: total was -8.920000. running mean: -11.230643\n",
      "ep 2444: ep_len:659 episode reward: total was 23.430000. running mean: -10.884036\n",
      "ep 2444: ep_len:51 episode reward: total was 22.500000. running mean: -10.550196\n",
      "ep 2444: ep_len:500 episode reward: total was 49.240000. running mean: -9.952294\n",
      "ep 2444: ep_len:3543 episode reward: total was -80.480000. running mean: -10.657571\n",
      "ep 2444: ep_len:710 episode reward: total was -13.400000. running mean: -10.684995\n",
      "ep 2444: ep_len:722 episode reward: total was 4.970000. running mean: -10.528445\n",
      "ep 2444: ep_len:681 episode reward: total was 15.530000. running mean: -10.267861\n",
      "ep 2444: ep_len:56 episode reward: total was 26.500000. running mean: -9.900182\n",
      "ep 2444: ep_len:182 episode reward: total was 86.500000. running mean: -8.936181\n",
      "ep 2444: ep_len:1561 episode reward: total was 14.040000. running mean: -8.706419\n",
      "ep 2444: ep_len:2933 episode reward: total was -4.840000. running mean: -8.667755\n",
      "ep 2444: ep_len:49 episode reward: total was 23.000000. running mean: -8.351077\n",
      "epsilon:0.009992 episode_count: 36819. steps_count: 39598635.000000\n",
      "ep 2445: ep_len:961 episode reward: total was -53.370000. running mean: -8.801266\n",
      "ep 2445: ep_len:675 episode reward: total was -23.690000. running mean: -8.950154\n",
      "ep 2445: ep_len:51 episode reward: total was 22.500000. running mean: -8.635652\n",
      "ep 2445: ep_len:3076 episode reward: total was -16.110000. running mean: -8.710396\n",
      "ep 2445: ep_len:760 episode reward: total was 12.320000. running mean: -8.500092\n",
      "ep 2445: ep_len:38 episode reward: total was 17.500000. running mean: -8.240091\n",
      "ep 2445: ep_len:604 episode reward: total was -9.750000. running mean: -8.255190\n",
      "ep 2445: ep_len:339 episode reward: total was 17.710000. running mean: -7.995538\n",
      "ep 2445: ep_len:1580 episode reward: total was -58.840000. running mean: -8.503983\n",
      "ep 2445: ep_len:754 episode reward: total was 16.980000. running mean: -8.249143\n",
      "ep 2445: ep_len:576 episode reward: total was 14.490000. running mean: -8.021751\n",
      "ep 2445: ep_len:155 episode reward: total was 74.500000. running mean: -7.196534\n",
      "ep 2445: ep_len:813 episode reward: total was 19.420000. running mean: -6.930368\n",
      "ep 2445: ep_len:2862 episode reward: total was -7.360000. running mean: -6.934665\n",
      "ep 2445: ep_len:28 episode reward: total was 12.500000. running mean: -6.740318\n",
      "epsilon:0.009992 episode_count: 36834. steps_count: 39611907.000000\n",
      "ep 2446: ep_len:1467 episode reward: total was 44.960000. running mean: -6.223315\n",
      "ep 2446: ep_len:757 episode reward: total was -32.910000. running mean: -6.490182\n",
      "ep 2446: ep_len:3021 episode reward: total was -11.870000. running mean: -6.543980\n",
      "ep 2446: ep_len:1168 episode reward: total was -38.990000. running mean: -6.868440\n",
      "ep 2446: ep_len:46 episode reward: total was 20.000000. running mean: -6.599756\n",
      "ep 2446: ep_len:723 episode reward: total was -24.040000. running mean: -6.774158\n",
      "ep 2446: ep_len:3579 episode reward: total was -145.280000. running mean: -8.159217\n",
      "ep 2446: ep_len:1597 episode reward: total was -135.290000. running mean: -9.430524\n",
      "ep 2446: ep_len:783 episode reward: total was 70.260000. running mean: -8.633619\n",
      "ep 2446: ep_len:1541 episode reward: total was 23.350000. running mean: -8.313783\n",
      "ep 2446: ep_len:92 episode reward: total was 43.000000. running mean: -7.800645\n",
      "ep 2446: ep_len:741 episode reward: total was -63.950000. running mean: -8.362139\n",
      "ep 2446: ep_len:2891 episode reward: total was 2.630000. running mean: -8.252217\n",
      "epsilon:0.009992 episode_count: 36847. steps_count: 39630313.000000\n",
      "ep 2447: ep_len:603 episode reward: total was -97.010000. running mean: -9.139795\n",
      "ep 2447: ep_len:700 episode reward: total was -8.550000. running mean: -9.133897\n",
      "ep 2447: ep_len:75 episode reward: total was 36.000000. running mean: -8.682558\n",
      "ep 2447: ep_len:2954 episode reward: total was -5.120000. running mean: -8.646933\n",
      "ep 2447: ep_len:686 episode reward: total was 39.230000. running mean: -8.168163\n",
      "ep 2447: ep_len:142 episode reward: total was 68.000000. running mean: -7.406482\n",
      "ep 2447: ep_len:79 episode reward: total was 36.500000. running mean: -6.967417\n",
      "ep 2447: ep_len:51 episode reward: total was 24.000000. running mean: -6.657743\n",
      "ep 2447: ep_len:600 episode reward: total was 32.590000. running mean: -6.265265\n",
      "ep 2447: ep_len:680 episode reward: total was 30.940000. running mean: -5.893213\n",
      "ep 2447: ep_len:1754 episode reward: total was -121.490000. running mean: -7.049180\n",
      "ep 2447: ep_len:831 episode reward: total was 55.550000. running mean: -6.423189\n",
      "ep 2447: ep_len:597 episode reward: total was -15.350000. running mean: -6.512457\n",
      "ep 2447: ep_len:33 episode reward: total was 10.010000. running mean: -6.347232\n",
      "ep 2447: ep_len:1026 episode reward: total was 23.720000. running mean: -6.046560\n",
      "ep 2447: ep_len:2904 episode reward: total was -23.090000. running mean: -6.216994\n",
      "ep 2447: ep_len:56 episode reward: total was 25.000000. running mean: -5.904824\n",
      "epsilon:0.009992 episode_count: 36864. steps_count: 39644084.000000\n",
      "ep 2448: ep_len:699 episode reward: total was 1.050000. running mean: -5.835276\n",
      "ep 2448: ep_len:764 episode reward: total was -13.740000. running mean: -5.914323\n",
      "ep 2448: ep_len:56 episode reward: total was 26.500000. running mean: -5.590180\n",
      "ep 2448: ep_len:96 episode reward: total was 43.500000. running mean: -5.099278\n",
      "ep 2448: ep_len:511 episode reward: total was -30.380000. running mean: -5.352086\n",
      "ep 2448: ep_len:41 episode reward: total was 17.500000. running mean: -5.123565\n",
      "ep 2448: ep_len:62 episode reward: total was 29.500000. running mean: -4.777329\n",
      "ep 2448: ep_len:628 episode reward: total was 2.070000. running mean: -4.708856\n",
      "ep 2448: ep_len:3530 episode reward: total was -79.630000. running mean: -5.458067\n",
      "ep 2448: ep_len:617 episode reward: total was 0.950000. running mean: -5.393987\n",
      "ep 2448: ep_len:855 episode reward: total was 30.660000. running mean: -5.033447\n",
      "ep 2448: ep_len:721 episode reward: total was -9.120000. running mean: -5.074312\n",
      "ep 2448: ep_len:51 episode reward: total was 22.500000. running mean: -4.798569\n",
      "ep 2448: ep_len:66 episode reward: total was 31.500000. running mean: -4.435583\n",
      "ep 2448: ep_len:981 episode reward: total was -107.000000. running mean: -5.461228\n",
      "ep 2448: ep_len:2740 episode reward: total was -12.190000. running mean: -5.528515\n",
      "ep 2448: ep_len:41 episode reward: total was 19.000000. running mean: -5.283230\n",
      "epsilon:0.009992 episode_count: 36881. steps_count: 39656543.000000\n",
      "ep 2449: ep_len:713 episode reward: total was -0.040000. running mean: -5.230798\n",
      "ep 2449: ep_len:1633 episode reward: total was -60.390000. running mean: -5.782390\n",
      "ep 2449: ep_len:78 episode reward: total was 37.500000. running mean: -5.349566\n",
      "ep 2449: ep_len:3025 episode reward: total was -8.560000. running mean: -5.381670\n",
      "ep 2449: ep_len:500 episode reward: total was 29.460000. running mean: -5.033254\n",
      "ep 2449: ep_len:56 episode reward: total was 26.500000. running mean: -4.717921\n",
      "ep 2449: ep_len:1457 episode reward: total was -111.570000. running mean: -5.786442\n",
      "ep 2449: ep_len:341 episode reward: total was 15.530000. running mean: -5.573277\n",
      "ep 2449: ep_len:830 episode reward: total was 33.780000. running mean: -5.179745\n",
      "ep 2449: ep_len:596 episode reward: total was 4.910000. running mean: -5.078847\n",
      "ep 2449: ep_len:511 episode reward: total was -9.040000. running mean: -5.118459\n",
      "ep 2449: ep_len:81 episode reward: total was 36.000000. running mean: -4.707274\n",
      "ep 2449: ep_len:70 episode reward: total was 33.500000. running mean: -4.325201\n",
      "ep 2449: ep_len:1159 episode reward: total was -7.770000. running mean: -4.359649\n",
      "ep 2449: ep_len:2834 episode reward: total was -6.870000. running mean: -4.384753\n",
      "epsilon:0.009992 episode_count: 36896. steps_count: 39670427.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2450: ep_len:1425 episode reward: total was 23.350000. running mean: -4.107405\n",
      "ep 2450: ep_len:678 episode reward: total was -24.700000. running mean: -4.313331\n",
      "ep 2450: ep_len:57 episode reward: total was 27.000000. running mean: -4.000198\n",
      "ep 2450: ep_len:2940 episode reward: total was 5.830000. running mean: -3.901896\n",
      "ep 2450: ep_len:1660 episode reward: total was -60.880000. running mean: -4.471677\n",
      "ep 2450: ep_len:60 episode reward: total was 27.000000. running mean: -4.156960\n",
      "ep 2450: ep_len:97 episode reward: total was 47.000000. running mean: -3.645391\n",
      "ep 2450: ep_len:867 episode reward: total was 47.900000. running mean: -3.129937\n",
      "ep 2450: ep_len:648 episode reward: total was 13.970000. running mean: -2.958937\n",
      "ep 2450: ep_len:892 episode reward: total was -35.690000. running mean: -3.286248\n",
      "ep 2450: ep_len:724 episode reward: total was -11.200000. running mean: -3.365386\n",
      "ep 2450: ep_len:500 episode reward: total was 26.580000. running mean: -3.065932\n",
      "ep 2450: ep_len:55 episode reward: total was 26.000000. running mean: -2.775272\n",
      "ep 2450: ep_len:42 episode reward: total was 18.000000. running mean: -2.567520\n",
      "ep 2450: ep_len:81 episode reward: total was 39.000000. running mean: -2.151844\n",
      "ep 2450: ep_len:1513 episode reward: total was 8.350000. running mean: -2.046826\n",
      "ep 2450: ep_len:2821 episode reward: total was -7.430000. running mean: -2.100658\n",
      "epsilon:0.009992 episode_count: 36913. steps_count: 39685487.000000\n",
      "ep 2451: ep_len:1130 episode reward: total was 16.160000. running mean: -1.918051\n",
      "ep 2451: ep_len:783 episode reward: total was -3.440000. running mean: -1.933271\n",
      "ep 2451: ep_len:66 episode reward: total was 31.500000. running mean: -1.598938\n",
      "ep 2451: ep_len:2951 episode reward: total was -6.740000. running mean: -1.650349\n",
      "ep 2451: ep_len:825 episode reward: total was 60.680000. running mean: -1.027045\n",
      "ep 2451: ep_len:40 episode reward: total was 18.500000. running mean: -0.831775\n",
      "ep 2451: ep_len:126 episode reward: total was 61.500000. running mean: -0.208457\n",
      "ep 2451: ep_len:75 episode reward: total was 36.000000. running mean: 0.153628\n",
      "ep 2451: ep_len:1478 episode reward: total was 13.140000. running mean: 0.283491\n",
      "ep 2451: ep_len:304 episode reward: total was 4.930000. running mean: 0.329956\n",
      "ep 2451: ep_len:731 episode reward: total was -15.760000. running mean: 0.169057\n",
      "ep 2451: ep_len:766 episode reward: total was 20.270000. running mean: 0.370066\n",
      "ep 2451: ep_len:713 episode reward: total was 28.370000. running mean: 0.650066\n",
      "ep 2451: ep_len:170 episode reward: total was 83.500000. running mean: 1.478565\n",
      "ep 2451: ep_len:55 episode reward: total was 24.500000. running mean: 1.708779\n",
      "ep 2451: ep_len:632 episode reward: total was 21.880000. running mean: 1.910492\n",
      "ep 2451: ep_len:2884 episode reward: total was -60.760000. running mean: 1.283787\n",
      "epsilon:0.009992 episode_count: 36930. steps_count: 39699216.000000\n",
      "ep 2452: ep_len:1491 episode reward: total was 27.470000. running mean: 1.545649\n",
      "ep 2452: ep_len:783 episode reward: total was -33.330000. running mean: 1.196892\n",
      "ep 2452: ep_len:59 episode reward: total was 25.000000. running mean: 1.434923\n",
      "ep 2452: ep_len:2997 episode reward: total was -25.130000. running mean: 1.169274\n",
      "ep 2452: ep_len:500 episode reward: total was 24.130000. running mean: 1.398881\n",
      "ep 2452: ep_len:111 episode reward: total was 49.500000. running mean: 1.879893\n",
      "ep 2452: ep_len:65 episode reward: total was 28.000000. running mean: 2.141094\n",
      "ep 2452: ep_len:851 episode reward: total was 29.890000. running mean: 2.418583\n",
      "ep 2452: ep_len:3661 episode reward: total was -782.170000. running mean: -5.427303\n",
      "ep 2452: ep_len:948 episode reward: total was -7.860000. running mean: -5.451630\n",
      "ep 2452: ep_len:7365 episode reward: total was -132.220000. running mean: -6.719314\n",
      "ep 2452: ep_len:1546 episode reward: total was 10.640000. running mean: -6.545721\n",
      "ep 2452: ep_len:219 episode reward: total was 106.500000. running mean: -5.415263\n",
      "ep 2452: ep_len:98 episode reward: total was 47.500000. running mean: -4.886111\n",
      "ep 2452: ep_len:587 episode reward: total was -28.090000. running mean: -5.118150\n",
      "ep 2452: ep_len:2918 episode reward: total was -151.900000. running mean: -6.585968\n",
      "epsilon:0.009992 episode_count: 36946. steps_count: 39723415.000000\n",
      "ep 2453: ep_len:1056 episode reward: total was -6.780000. running mean: -6.587908\n",
      "ep 2453: ep_len:785 episode reward: total was 0.310000. running mean: -6.518929\n",
      "ep 2453: ep_len:3010 episode reward: total was 2.050000. running mean: -6.433240\n",
      "ep 2453: ep_len:500 episode reward: total was 18.100000. running mean: -6.187908\n",
      "ep 2453: ep_len:53 episode reward: total was 25.000000. running mean: -5.876029\n",
      "ep 2453: ep_len:500 episode reward: total was 57.780000. running mean: -5.239468\n",
      "ep 2453: ep_len:3660 episode reward: total was -56.110000. running mean: -5.748174\n",
      "ep 2453: ep_len:1503 episode reward: total was -70.080000. running mean: -6.391492\n",
      "ep 2453: ep_len:7563 episode reward: total was 26.620000. running mean: -6.061377\n",
      "ep 2453: ep_len:1062 episode reward: total was 36.690000. running mean: -5.633863\n",
      "ep 2453: ep_len:181 episode reward: total was 86.000000. running mean: -4.717525\n",
      "ep 2453: ep_len:1050 episode reward: total was 8.110000. running mean: -4.589249\n",
      "ep 2453: ep_len:2925 episode reward: total was 4.510000. running mean: -4.498257\n",
      "epsilon:0.009992 episode_count: 36959. steps_count: 39747263.000000\n",
      "ep 2454: ep_len:1433 episode reward: total was 24.200000. running mean: -4.211274\n",
      "ep 2454: ep_len:953 episode reward: total was 16.340000. running mean: -4.005762\n",
      "ep 2454: ep_len:2915 episode reward: total was 13.530000. running mean: -3.830404\n",
      "ep 2454: ep_len:505 episode reward: total was -31.480000. running mean: -4.106900\n",
      "ep 2454: ep_len:1412 episode reward: total was -300.160000. running mean: -7.067431\n",
      "ep 2454: ep_len:619 episode reward: total was 30.300000. running mean: -6.693757\n",
      "ep 2454: ep_len:1180 episode reward: total was -68.710000. running mean: -7.313919\n",
      "ep 2454: ep_len:858 episode reward: total was 50.960000. running mean: -6.731180\n",
      "ep 2454: ep_len:500 episode reward: total was -13.440000. running mean: -6.798268\n",
      "ep 2454: ep_len:151 episode reward: total was 74.000000. running mean: -5.990285\n",
      "ep 2454: ep_len:129 episode reward: total was 61.500000. running mean: -5.315382\n",
      "ep 2454: ep_len:635 episode reward: total was -24.120000. running mean: -5.503429\n",
      "ep 2454: ep_len:2872 episode reward: total was -31.310000. running mean: -5.761494\n",
      "ep 2454: ep_len:29 episode reward: total was 13.000000. running mean: -5.573879\n",
      "epsilon:0.009992 episode_count: 36973. steps_count: 39761454.000000\n",
      "ep 2455: ep_len:1153 episode reward: total was -8.870000. running mean: -5.606841\n",
      "ep 2455: ep_len:781 episode reward: total was -4.480000. running mean: -5.595572\n",
      "ep 2455: ep_len:57 episode reward: total was 25.500000. running mean: -5.284617\n",
      "ep 2455: ep_len:78 episode reward: total was 36.000000. running mean: -4.871770\n",
      "ep 2455: ep_len:1628 episode reward: total was -107.300000. running mean: -5.896053\n",
      "ep 2455: ep_len:500 episode reward: total was 19.200000. running mean: -5.645092\n",
      "ep 2455: ep_len:659 episode reward: total was 31.160000. running mean: -5.277041\n",
      "ep 2455: ep_len:560 episode reward: total was 13.730000. running mean: -5.086971\n",
      "ep 2455: ep_len:781 episode reward: total was 9.610000. running mean: -4.940001\n",
      "ep 2455: ep_len:581 episode reward: total was 1.780000. running mean: -4.872801\n",
      "ep 2455: ep_len:89 episode reward: total was 41.500000. running mean: -4.409073\n",
      "ep 2455: ep_len:150 episode reward: total was 70.500000. running mean: -3.659982\n",
      "ep 2455: ep_len:745 episode reward: total was -59.380000. running mean: -4.217183\n",
      "ep 2455: ep_len:2815 episode reward: total was -8.900000. running mean: -4.264011\n",
      "epsilon:0.009992 episode_count: 36987. steps_count: 39772031.000000\n",
      "ep 2456: ep_len:1000 episode reward: total was -106.320000. running mean: -5.284571\n",
      "ep 2456: ep_len:500 episode reward: total was 23.580000. running mean: -4.995925\n",
      "ep 2456: ep_len:2982 episode reward: total was 13.570000. running mean: -4.810266\n",
      "ep 2456: ep_len:1667 episode reward: total was -46.370000. running mean: -5.225863\n",
      "ep 2456: ep_len:75 episode reward: total was 33.000000. running mean: -4.843604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2456: ep_len:61 episode reward: total was 27.500000. running mean: -4.520168\n",
      "ep 2456: ep_len:828 episode reward: total was 27.180000. running mean: -4.203167\n",
      "ep 2456: ep_len:584 episode reward: total was 27.930000. running mean: -3.881835\n",
      "ep 2456: ep_len:731 episode reward: total was -18.790000. running mean: -4.030917\n",
      "ep 2456: ep_len:692 episode reward: total was 4.190000. running mean: -3.948707\n",
      "ep 2456: ep_len:500 episode reward: total was -4.160000. running mean: -3.950820\n",
      "ep 2456: ep_len:53 episode reward: total was 25.000000. running mean: -3.661312\n",
      "ep 2456: ep_len:1449 episode reward: total was 28.430000. running mean: -3.340399\n",
      "ep 2456: ep_len:2818 episode reward: total was 6.370000. running mean: -3.243295\n",
      "ep 2456: ep_len:59 episode reward: total was 28.000000. running mean: -2.930862\n",
      "epsilon:0.009992 episode_count: 37002. steps_count: 39786030.000000\n",
      "ep 2457: ep_len:1035 episode reward: total was -62.210000. running mean: -3.523653\n",
      "ep 2457: ep_len:718 episode reward: total was -14.010000. running mean: -3.628517\n",
      "ep 2457: ep_len:70 episode reward: total was 32.000000. running mean: -3.272232\n",
      "ep 2457: ep_len:3007 episode reward: total was -11.870000. running mean: -3.358209\n",
      "ep 2457: ep_len:500 episode reward: total was 10.590000. running mean: -3.218727\n",
      "ep 2457: ep_len:54 episode reward: total was 25.500000. running mean: -2.931540\n",
      "ep 2457: ep_len:91 episode reward: total was 42.500000. running mean: -2.477225\n",
      "ep 2457: ep_len:80 episode reward: total was 37.000000. running mean: -2.082452\n",
      "ep 2457: ep_len:1389 episode reward: total was -257.450000. running mean: -4.636128\n",
      "ep 2457: ep_len:3639 episode reward: total was -45.360000. running mean: -5.043367\n",
      "ep 2457: ep_len:597 episode reward: total was -29.550000. running mean: -5.288433\n",
      "ep 2457: ep_len:697 episode reward: total was 32.380000. running mean: -4.911749\n",
      "ep 2457: ep_len:932 episode reward: total was 45.040000. running mean: -4.412231\n",
      "ep 2457: ep_len:51 episode reward: total was 24.000000. running mean: -4.128109\n",
      "ep 2457: ep_len:1119 episode reward: total was 19.810000. running mean: -3.888728\n",
      "ep 2457: ep_len:47 episode reward: total was 22.000000. running mean: -3.629840\n",
      "epsilon:0.009992 episode_count: 37018. steps_count: 39800056.000000\n",
      "ep 2458: ep_len:2071 episode reward: total was -353.700000. running mean: -7.130542\n",
      "ep 2458: ep_len:763 episode reward: total was -8.700000. running mean: -7.146237\n",
      "ep 2458: ep_len:3007 episode reward: total was -27.040000. running mean: -7.345174\n",
      "ep 2458: ep_len:809 episode reward: total was 29.280000. running mean: -6.978923\n",
      "ep 2458: ep_len:61 episode reward: total was 29.000000. running mean: -6.619133\n",
      "ep 2458: ep_len:1397 episode reward: total was -90.680000. running mean: -7.459742\n",
      "ep 2458: ep_len:3645 episode reward: total was -128.370000. running mean: -8.668845\n",
      "ep 2458: ep_len:1308 episode reward: total was -86.070000. running mean: -9.442856\n",
      "ep 2458: ep_len:866 episode reward: total was 54.800000. running mean: -8.800428\n",
      "ep 2458: ep_len:1053 episode reward: total was -40.580000. running mean: -9.118223\n",
      "ep 2458: ep_len:144 episode reward: total was 66.000000. running mean: -8.367041\n",
      "ep 2458: ep_len:52 episode reward: total was 24.500000. running mean: -8.038371\n",
      "ep 2458: ep_len:1141 episode reward: total was -1.370000. running mean: -7.971687\n",
      "ep 2458: ep_len:2799 episode reward: total was -53.440000. running mean: -8.426370\n",
      "ep 2458: ep_len:46 episode reward: total was 18.500000. running mean: -8.157106\n",
      "epsilon:0.009992 episode_count: 37033. steps_count: 39819218.000000\n",
      "ep 2459: ep_len:1109 episode reward: total was 0.820000. running mean: -8.067335\n",
      "ep 2459: ep_len:961 episode reward: total was 25.240000. running mean: -7.734262\n",
      "ep 2459: ep_len:2986 episode reward: total was -10.490000. running mean: -7.761819\n",
      "ep 2459: ep_len:642 episode reward: total was -9.920000. running mean: -7.783401\n",
      "ep 2459: ep_len:110 episode reward: total was 53.500000. running mean: -7.170567\n",
      "ep 2459: ep_len:68 episode reward: total was 32.500000. running mean: -6.773861\n",
      "ep 2459: ep_len:977 episode reward: total was 53.820000. running mean: -6.167923\n",
      "ep 2459: ep_len:3681 episode reward: total was -67.500000. running mean: -6.781244\n",
      "ep 2459: ep_len:573 episode reward: total was -17.390000. running mean: -6.887331\n",
      "ep 2459: ep_len:764 episode reward: total was -43.890000. running mean: -7.257358\n",
      "ep 2459: ep_len:1080 episode reward: total was 30.010000. running mean: -6.884684\n",
      "ep 2459: ep_len:136 episode reward: total was 65.000000. running mean: -6.165837\n",
      "ep 2459: ep_len:37 episode reward: total was 15.500000. running mean: -5.949179\n",
      "ep 2459: ep_len:132 episode reward: total was 63.000000. running mean: -5.259687\n",
      "ep 2459: ep_len:511 episode reward: total was 4.310000. running mean: -5.163990\n",
      "ep 2459: ep_len:2904 episode reward: total was -10.730000. running mean: -5.219650\n",
      "ep 2459: ep_len:73 episode reward: total was 33.500000. running mean: -4.832454\n",
      "epsilon:0.009992 episode_count: 37050. steps_count: 39835962.000000\n",
      "ep 2460: ep_len:688 episode reward: total was -39.230000. running mean: -5.176429\n",
      "ep 2460: ep_len:195 episode reward: total was 15.380000. running mean: -4.970865\n",
      "ep 2460: ep_len:2969 episode reward: total was -28.420000. running mean: -5.205356\n",
      "ep 2460: ep_len:1085 episode reward: total was -2.450000. running mean: -5.177803\n",
      "ep 2460: ep_len:132 episode reward: total was 64.500000. running mean: -4.481025\n",
      "ep 2460: ep_len:93 episode reward: total was 45.000000. running mean: -3.986215\n",
      "ep 2460: ep_len:4354 episode reward: total was -1543.000000. running mean: -19.376353\n",
      "ep 2460: ep_len:619 episode reward: total was 14.690000. running mean: -19.035689\n",
      "ep 2460: ep_len:1549 episode reward: total was -161.620000. running mean: -20.461532\n",
      "ep 2460: ep_len:749 episode reward: total was -14.750000. running mean: -20.404417\n",
      "ep 2460: ep_len:622 episode reward: total was 15.840000. running mean: -20.041973\n",
      "ep 2460: ep_len:1073 episode reward: total was -16.260000. running mean: -20.004153\n",
      "ep 2460: ep_len:2817 episode reward: total was 9.430000. running mean: -19.709811\n",
      "epsilon:0.009992 episode_count: 37063. steps_count: 39852907.000000\n",
      "ep 2461: ep_len:652 episode reward: total was -29.000000. running mean: -19.802713\n",
      "ep 2461: ep_len:963 episode reward: total was 19.380000. running mean: -19.410886\n",
      "ep 2461: ep_len:82 episode reward: total was 38.000000. running mean: -18.836777\n",
      "ep 2461: ep_len:2962 episode reward: total was -105.120000. running mean: -19.699609\n",
      "ep 2461: ep_len:1090 episode reward: total was -8.460000. running mean: -19.587213\n",
      "ep 2461: ep_len:624 episode reward: total was 12.000000. running mean: -19.271341\n",
      "ep 2461: ep_len:324 episode reward: total was 7.460000. running mean: -19.004028\n",
      "ep 2461: ep_len:542 episode reward: total was 1.210000. running mean: -18.801888\n",
      "ep 2461: ep_len:854 episode reward: total was 35.580000. running mean: -18.258069\n",
      "ep 2461: ep_len:985 episode reward: total was -38.740000. running mean: -18.462888\n",
      "ep 2461: ep_len:85 episode reward: total was 39.500000. running mean: -17.883259\n",
      "ep 2461: ep_len:152 episode reward: total was 72.510000. running mean: -16.979327\n",
      "ep 2461: ep_len:74 episode reward: total was 34.000000. running mean: -16.469533\n",
      "ep 2461: ep_len:500 episode reward: total was 20.910000. running mean: -16.095738\n",
      "ep 2461: ep_len:2773 episode reward: total was -8.710000. running mean: -16.021881\n",
      "epsilon:0.009992 episode_count: 37078. steps_count: 39865569.000000\n",
      "ep 2462: ep_len:1492 episode reward: total was 13.650000. running mean: -15.725162\n",
      "ep 2462: ep_len:193 episode reward: total was 7.740000. running mean: -15.490510\n",
      "ep 2462: ep_len:2967 episode reward: total was -29.400000. running mean: -15.629605\n",
      "ep 2462: ep_len:712 episode reward: total was -45.020000. running mean: -15.923509\n",
      "ep 2462: ep_len:41 episode reward: total was 19.000000. running mean: -15.574274\n",
      "ep 2462: ep_len:1377 episode reward: total was -6.200000. running mean: -15.480531\n",
      "ep 2462: ep_len:3629 episode reward: total was -32.240000. running mean: -15.648126\n",
      "ep 2462: ep_len:906 episode reward: total was 14.750000. running mean: -15.344145\n",
      "ep 2462: ep_len:645 episode reward: total was -2.960000. running mean: -15.220303\n",
      "ep 2462: ep_len:735 episode reward: total was -1.230000. running mean: -15.080400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2462: ep_len:931 episode reward: total was -21.350000. running mean: -15.143096\n",
      "ep 2462: ep_len:2903 episode reward: total was 0.670000. running mean: -14.984965\n",
      "epsilon:0.009992 episode_count: 37090. steps_count: 39882100.000000\n",
      "ep 2463: ep_len:1087 episode reward: total was -5.460000. running mean: -14.889715\n",
      "ep 2463: ep_len:980 episode reward: total was 38.410000. running mean: -14.356718\n",
      "ep 2463: ep_len:2792 episode reward: total was -25.260000. running mean: -14.465751\n",
      "ep 2463: ep_len:1482 episode reward: total was 25.700000. running mean: -14.064094\n",
      "ep 2463: ep_len:46 episode reward: total was 20.000000. running mean: -13.723453\n",
      "ep 2463: ep_len:703 episode reward: total was 13.420000. running mean: -13.452018\n",
      "ep 2463: ep_len:641 episode reward: total was 27.030000. running mean: -13.047198\n",
      "ep 2463: ep_len:630 episode reward: total was -31.730000. running mean: -13.234026\n",
      "ep 2463: ep_len:701 episode reward: total was 20.630000. running mean: -12.895386\n",
      "ep 2463: ep_len:627 episode reward: total was 8.860000. running mean: -12.677832\n",
      "ep 2463: ep_len:83 episode reward: total was 38.500000. running mean: -12.166054\n",
      "ep 2463: ep_len:1068 episode reward: total was 24.410000. running mean: -11.800293\n",
      "ep 2463: ep_len:2930 episode reward: total was -2.510000. running mean: -11.707390\n",
      "ep 2463: ep_len:44 episode reward: total was 19.000000. running mean: -11.400316\n",
      "epsilon:0.009992 episode_count: 37104. steps_count: 39895914.000000\n",
      "ep 2464: ep_len:1098 episode reward: total was -11.900000. running mean: -11.405313\n",
      "ep 2464: ep_len:500 episode reward: total was 2.210000. running mean: -11.269160\n",
      "ep 2464: ep_len:3016 episode reward: total was -9.590000. running mean: -11.252368\n",
      "ep 2464: ep_len:509 episode reward: total was -43.560000. running mean: -11.575445\n",
      "ep 2464: ep_len:88 episode reward: total was 41.000000. running mean: -11.049690\n",
      "ep 2464: ep_len:30 episode reward: total was 13.500000. running mean: -10.804193\n",
      "ep 2464: ep_len:994 episode reward: total was -13.670000. running mean: -10.832851\n",
      "ep 2464: ep_len:323 episode reward: total was 15.380000. running mean: -10.570723\n",
      "ep 2464: ep_len:1571 episode reward: total was -837.410000. running mean: -18.839116\n",
      "ep 2464: ep_len:7384 episode reward: total was -88.450000. running mean: -19.535224\n",
      "ep 2464: ep_len:500 episode reward: total was 44.030000. running mean: -18.899572\n",
      "ep 2464: ep_len:1462 episode reward: total was -730.400000. running mean: -26.014576\n",
      "ep 2464: ep_len:2817 episode reward: total was 2.790000. running mean: -25.726531\n",
      "ep 2464: ep_len:57 episode reward: total was 25.500000. running mean: -25.214265\n",
      "epsilon:0.009992 episode_count: 37118. steps_count: 39916263.000000\n",
      "ep 2465: ep_len:3560 episode reward: total was -543.300000. running mean: -30.395123\n",
      "ep 2465: ep_len:782 episode reward: total was -1.920000. running mean: -30.110372\n",
      "ep 2465: ep_len:3042 episode reward: total was -12.000000. running mean: -29.929268\n",
      "ep 2465: ep_len:640 episode reward: total was 4.210000. running mean: -29.587875\n",
      "ep 2465: ep_len:593 episode reward: total was -9.390000. running mean: -29.385896\n",
      "ep 2465: ep_len:326 episode reward: total was 16.480000. running mean: -28.927237\n",
      "ep 2465: ep_len:583 episode reward: total was 16.810000. running mean: -28.469865\n",
      "ep 2465: ep_len:682 episode reward: total was 15.540000. running mean: -28.029766\n",
      "ep 2465: ep_len:1061 episode reward: total was -6.730000. running mean: -27.816769\n",
      "ep 2465: ep_len:40 episode reward: total was 18.500000. running mean: -27.353601\n",
      "ep 2465: ep_len:175 episode reward: total was 84.500000. running mean: -26.235065\n",
      "ep 2465: ep_len:77 episode reward: total was 37.000000. running mean: -25.602714\n",
      "ep 2465: ep_len:780 episode reward: total was -34.790000. running mean: -25.694587\n",
      "ep 2465: ep_len:2860 episode reward: total was -6.700000. running mean: -25.504641\n",
      "ep 2465: ep_len:42 episode reward: total was 19.500000. running mean: -25.054595\n",
      "epsilon:0.009992 episode_count: 37133. steps_count: 39931506.000000\n",
      "ep 2466: ep_len:621 episode reward: total was 12.860000. running mean: -24.675449\n",
      "ep 2466: ep_len:631 episode reward: total was 16.670000. running mean: -24.261995\n",
      "ep 2466: ep_len:2991 episode reward: total was -13.230000. running mean: -24.151675\n",
      "ep 2466: ep_len:727 episode reward: total was -8.670000. running mean: -23.996858\n",
      "ep 2466: ep_len:59 episode reward: total was 28.000000. running mean: -23.476889\n",
      "ep 2466: ep_len:1468 episode reward: total was 12.210000. running mean: -23.120020\n",
      "ep 2466: ep_len:3607 episode reward: total was -5.130000. running mean: -22.940120\n",
      "ep 2466: ep_len:1208 episode reward: total was -35.900000. running mean: -23.069719\n",
      "ep 2466: ep_len:680 episode reward: total was 45.340000. running mean: -22.385622\n",
      "ep 2466: ep_len:1159 episode reward: total was 2.310000. running mean: -22.138666\n",
      "ep 2466: ep_len:48 episode reward: total was 22.500000. running mean: -21.692279\n",
      "ep 2466: ep_len:110 episode reward: total was 52.000000. running mean: -20.955356\n",
      "ep 2466: ep_len:1501 episode reward: total was 13.310000. running mean: -20.612703\n",
      "ep 2466: ep_len:2921 episode reward: total was -6.400000. running mean: -20.470576\n",
      "ep 2466: ep_len:72 episode reward: total was 33.000000. running mean: -19.935870\n",
      "epsilon:0.009992 episode_count: 37148. steps_count: 39949309.000000\n",
      "ep 2467: ep_len:1141 episode reward: total was -2.900000. running mean: -19.765511\n",
      "ep 2467: ep_len:628 episode reward: total was -21.070000. running mean: -19.778556\n",
      "ep 2467: ep_len:2873 episode reward: total was -22.980000. running mean: -19.810570\n",
      "ep 2467: ep_len:578 episode reward: total was -3.480000. running mean: -19.647265\n",
      "ep 2467: ep_len:55 episode reward: total was 26.000000. running mean: -19.190792\n",
      "ep 2467: ep_len:175 episode reward: total was 85.510000. running mean: -18.143784\n",
      "ep 2467: ep_len:44 episode reward: total was 20.500000. running mean: -17.757346\n",
      "ep 2467: ep_len:883 episode reward: total was 66.070000. running mean: -16.919073\n",
      "ep 2467: ep_len:605 episode reward: total was 30.160000. running mean: -16.448282\n",
      "ep 2467: ep_len:1250 episode reward: total was -81.600000. running mean: -17.099799\n",
      "ep 2467: ep_len:717 episode reward: total was -5.590000. running mean: -16.984701\n",
      "ep 2467: ep_len:1141 episode reward: total was -10.980000. running mean: -16.924654\n",
      "ep 2467: ep_len:102 episode reward: total was 46.500000. running mean: -16.290408\n",
      "ep 2467: ep_len:747 episode reward: total was -6.840000. running mean: -16.195904\n",
      "ep 2467: ep_len:2734 episode reward: total was -5.580000. running mean: -16.089745\n",
      "epsilon:0.009992 episode_count: 37163. steps_count: 39962982.000000\n",
      "ep 2468: ep_len:1063 episode reward: total was -5.700000. running mean: -15.985847\n",
      "ep 2468: ep_len:813 episode reward: total was -3.970000. running mean: -15.865689\n",
      "ep 2468: ep_len:56 episode reward: total was 26.500000. running mean: -15.442032\n",
      "ep 2468: ep_len:2977 episode reward: total was -17.910000. running mean: -15.466711\n",
      "ep 2468: ep_len:824 episode reward: total was 21.050000. running mean: -15.101544\n",
      "ep 2468: ep_len:162 episode reward: total was 79.500000. running mean: -14.155529\n",
      "ep 2468: ep_len:67 episode reward: total was 30.500000. running mean: -13.708974\n",
      "ep 2468: ep_len:54 episode reward: total was 25.500000. running mean: -13.316884\n",
      "ep 2468: ep_len:825 episode reward: total was 31.340000. running mean: -12.870315\n",
      "ep 2468: ep_len:367 episode reward: total was 16.800000. running mean: -12.573612\n",
      "ep 2468: ep_len:674 episode reward: total was -16.040000. running mean: -12.608276\n",
      "ep 2468: ep_len:718 episode reward: total was 42.080000. running mean: -12.061393\n",
      "ep 2468: ep_len:1105 episode reward: total was -4.270000. running mean: -11.983479\n",
      "ep 2468: ep_len:99 episode reward: total was 46.500000. running mean: -11.398644\n",
      "ep 2468: ep_len:181 episode reward: total was 90.010000. running mean: -10.384558\n",
      "ep 2468: ep_len:39 episode reward: total was 16.500000. running mean: -10.115712\n",
      "ep 2468: ep_len:121 episode reward: total was 57.500000. running mean: -9.439555\n",
      "ep 2468: ep_len:740 episode reward: total was -23.070000. running mean: -9.575860\n",
      "ep 2468: ep_len:2824 episode reward: total was 1.570000. running mean: -9.464401\n",
      "epsilon:0.009992 episode_count: 37182. steps_count: 39976691.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2469: ep_len:890 episode reward: total was 22.010000. running mean: -9.149657\n",
      "ep 2469: ep_len:707 episode reward: total was -51.880000. running mean: -9.576960\n",
      "ep 2469: ep_len:3078 episode reward: total was -20.050000. running mean: -9.681691\n",
      "ep 2469: ep_len:752 episode reward: total was 22.190000. running mean: -9.362974\n",
      "ep 2469: ep_len:58 episode reward: total was 26.000000. running mean: -9.009344\n",
      "ep 2469: ep_len:127 episode reward: total was 59.000000. running mean: -8.329251\n",
      "ep 2469: ep_len:72 episode reward: total was 34.500000. running mean: -7.900958\n",
      "ep 2469: ep_len:745 episode reward: total was -25.040000. running mean: -8.072349\n",
      "ep 2469: ep_len:628 episode reward: total was 30.050000. running mean: -7.691125\n",
      "ep 2469: ep_len:793 episode reward: total was 26.860000. running mean: -7.345614\n",
      "ep 2469: ep_len:612 episode reward: total was 3.290000. running mean: -7.239258\n",
      "ep 2469: ep_len:1034 episode reward: total was 15.960000. running mean: -7.007265\n",
      "ep 2469: ep_len:56 episode reward: total was 25.000000. running mean: -6.687193\n",
      "ep 2469: ep_len:46 episode reward: total was 20.000000. running mean: -6.420321\n",
      "ep 2469: ep_len:1466 episode reward: total was 2.830000. running mean: -6.327817\n",
      "ep 2469: ep_len:2882 episode reward: total was 5.270000. running mean: -6.211839\n",
      "ep 2469: ep_len:74 episode reward: total was 34.000000. running mean: -5.809721\n",
      "epsilon:0.009992 episode_count: 37199. steps_count: 39990711.000000\n",
      "ep 2470: ep_len:845 episode reward: total was -22.470000. running mean: -5.976324\n",
      "ep 2470: ep_len:960 episode reward: total was 27.860000. running mean: -5.637960\n",
      "ep 2470: ep_len:2957 episode reward: total was -14.880000. running mean: -5.730381\n",
      "ep 2470: ep_len:798 episode reward: total was 33.150000. running mean: -5.341577\n",
      "ep 2470: ep_len:67 episode reward: total was 30.500000. running mean: -4.983161\n",
      "ep 2470: ep_len:44 episode reward: total was 20.500000. running mean: -4.728330\n",
      "ep 2470: ep_len:698 episode reward: total was 2.730000. running mean: -4.653746\n",
      "ep 2470: ep_len:306 episode reward: total was 14.780000. running mean: -4.459409\n",
      "ep 2470: ep_len:1282 episode reward: total was -65.000000. running mean: -5.064815\n",
      "ep 2470: ep_len:663 episode reward: total was 25.480000. running mean: -4.759367\n",
      "ep 2470: ep_len:644 episode reward: total was 5.010000. running mean: -4.661673\n",
      "ep 2470: ep_len:58 episode reward: total was 26.000000. running mean: -4.355056\n",
      "ep 2470: ep_len:59 episode reward: total was 28.000000. running mean: -4.031506\n",
      "ep 2470: ep_len:110 episode reward: total was 53.500000. running mean: -3.456191\n",
      "ep 2470: ep_len:1484 episode reward: total was 8.640000. running mean: -3.335229\n",
      "ep 2470: ep_len:2886 episode reward: total was -20.830000. running mean: -3.510176\n",
      "ep 2470: ep_len:60 episode reward: total was 27.000000. running mean: -3.205075\n",
      "epsilon:0.009992 episode_count: 37216. steps_count: 40004632.000000\n",
      "ep 2471: ep_len:1072 episode reward: total was -17.640000. running mean: -3.349424\n",
      "ep 2471: ep_len:500 episode reward: total was 23.300000. running mean: -3.082930\n",
      "ep 2471: ep_len:3019 episode reward: total was -59.990000. running mean: -3.652000\n",
      "ep 2471: ep_len:600 episode reward: total was -3.810000. running mean: -3.653580\n",
      "ep 2471: ep_len:43 episode reward: total was 20.000000. running mean: -3.417045\n",
      "ep 2471: ep_len:1416 episode reward: total was -226.390000. running mean: -5.646774\n",
      "ep 2471: ep_len:622 episode reward: total was 23.780000. running mean: -5.352506\n",
      "ep 2471: ep_len:1524 episode reward: total was -79.180000. running mean: -6.090781\n",
      "ep 2471: ep_len:878 episode reward: total was 54.830000. running mean: -5.481573\n",
      "ep 2471: ep_len:500 episode reward: total was 13.290000. running mean: -5.293858\n",
      "ep 2471: ep_len:117 episode reward: total was 54.000000. running mean: -4.700919\n",
      "ep 2471: ep_len:1076 episode reward: total was -14.570000. running mean: -4.799610\n",
      "ep 2471: ep_len:2851 episode reward: total was 1.900000. running mean: -4.732614\n",
      "ep 2471: ep_len:45 episode reward: total was 21.000000. running mean: -4.475288\n",
      "epsilon:0.009992 episode_count: 37230. steps_count: 40018895.000000\n",
      "ep 2472: ep_len:1443 episode reward: total was 19.550000. running mean: -4.235035\n",
      "ep 2472: ep_len:774 episode reward: total was -17.970000. running mean: -4.372385\n",
      "ep 2472: ep_len:78 episode reward: total was 34.500000. running mean: -3.983661\n",
      "ep 2472: ep_len:2925 episode reward: total was -81.710000. running mean: -4.760924\n",
      "ep 2472: ep_len:765 episode reward: total was -45.040000. running mean: -5.163715\n",
      "ep 2472: ep_len:31 episode reward: total was 14.000000. running mean: -4.972078\n",
      "ep 2472: ep_len:100 episode reward: total was 47.000000. running mean: -4.452357\n",
      "ep 2472: ep_len:45 episode reward: total was 19.500000. running mean: -4.212833\n",
      "ep 2472: ep_len:740 episode reward: total was -18.020000. running mean: -4.350905\n",
      "ep 2472: ep_len:3684 episode reward: total was -43.750000. running mean: -4.744896\n",
      "ep 2472: ep_len:787 episode reward: total was -70.510000. running mean: -5.402547\n",
      "ep 2472: ep_len:741 episode reward: total was 7.000000. running mean: -5.278522\n",
      "ep 2472: ep_len:622 episode reward: total was -2.010000. running mean: -5.245836\n",
      "ep 2472: ep_len:72 episode reward: total was 31.500000. running mean: -4.878378\n",
      "ep 2472: ep_len:97 episode reward: total was 47.000000. running mean: -4.359594\n",
      "ep 2472: ep_len:781 episode reward: total was -67.100000. running mean: -4.986998\n",
      "ep 2472: ep_len:2751 episode reward: total was -10.890000. running mean: -5.046028\n",
      "epsilon:0.009992 episode_count: 37247. steps_count: 40035331.000000\n",
      "ep 2473: ep_len:820 episode reward: total was -17.180000. running mean: -5.167368\n",
      "ep 2473: ep_len:656 episode reward: total was -27.430000. running mean: -5.389994\n",
      "ep 2473: ep_len:2957 episode reward: total was -81.630000. running mean: -6.152394\n",
      "ep 2473: ep_len:652 episode reward: total was 10.210000. running mean: -5.988770\n",
      "ep 2473: ep_len:46 episode reward: total was 21.500000. running mean: -5.713883\n",
      "ep 2473: ep_len:500 episode reward: total was 28.080000. running mean: -5.375944\n",
      "ep 2473: ep_len:347 episode reward: total was 16.600000. running mean: -5.156184\n",
      "ep 2473: ep_len:563 episode reward: total was -18.780000. running mean: -5.292423\n",
      "ep 2473: ep_len:631 episode reward: total was -6.840000. running mean: -5.307898\n",
      "ep 2473: ep_len:565 episode reward: total was 38.590000. running mean: -4.868919\n",
      "ep 2473: ep_len:79 episode reward: total was 38.000000. running mean: -4.440230\n",
      "ep 2473: ep_len:166 episode reward: total was 81.500000. running mean: -3.580828\n",
      "ep 2473: ep_len:43 episode reward: total was 20.000000. running mean: -3.345020\n",
      "ep 2473: ep_len:1408 episode reward: total was -4.030000. running mean: -3.351869\n",
      "ep 2473: ep_len:2789 episode reward: total was -15.160000. running mean: -3.469951\n",
      "epsilon:0.009992 episode_count: 37262. steps_count: 40047553.000000\n",
      "ep 2474: ep_len:713 episode reward: total was -14.250000. running mean: -3.577751\n",
      "ep 2474: ep_len:687 episode reward: total was -24.610000. running mean: -3.788074\n",
      "ep 2474: ep_len:50 episode reward: total was 23.500000. running mean: -3.515193\n",
      "ep 2474: ep_len:3005 episode reward: total was -19.930000. running mean: -3.679341\n",
      "ep 2474: ep_len:500 episode reward: total was 28.290000. running mean: -3.359648\n",
      "ep 2474: ep_len:110 episode reward: total was 46.000000. running mean: -2.866051\n",
      "ep 2474: ep_len:581 episode reward: total was -12.910000. running mean: -2.966491\n",
      "ep 2474: ep_len:3699 episode reward: total was -35.520000. running mean: -3.292026\n",
      "ep 2474: ep_len:1550 episode reward: total was -25.990000. running mean: -3.519005\n",
      "ep 2474: ep_len:7402 episode reward: total was -63.780000. running mean: -4.121615\n",
      "ep 2474: ep_len:500 episode reward: total was -5.330000. running mean: -4.133699\n",
      "ep 2474: ep_len:51 episode reward: total was 24.000000. running mean: -3.852362\n",
      "ep 2474: ep_len:1018 episode reward: total was -12.110000. running mean: -3.934939\n",
      "ep 2474: ep_len:2812 episode reward: total was 9.990000. running mean: -3.795689\n",
      "ep 2474: ep_len:49 episode reward: total was 23.000000. running mean: -3.527732\n",
      "epsilon:0.009992 episode_count: 37277. steps_count: 40070280.000000\n",
      "ep 2475: ep_len:738 episode reward: total was -25.960000. running mean: -3.752055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2475: ep_len:3935 episode reward: total was -389.580000. running mean: -7.610334\n",
      "ep 2475: ep_len:2980 episode reward: total was -64.270000. running mean: -8.176931\n",
      "ep 2475: ep_len:530 episode reward: total was -3.960000. running mean: -8.134762\n",
      "ep 2475: ep_len:145 episode reward: total was 69.500000. running mean: -7.358414\n",
      "ep 2475: ep_len:76 episode reward: total was 36.500000. running mean: -6.919830\n",
      "ep 2475: ep_len:1413 episode reward: total was -19.010000. running mean: -7.040732\n",
      "ep 2475: ep_len:682 episode reward: total was 20.920000. running mean: -6.761124\n",
      "ep 2475: ep_len:762 episode reward: total was -63.050000. running mean: -7.324013\n",
      "ep 2475: ep_len:714 episode reward: total was 25.900000. running mean: -6.991773\n",
      "ep 2475: ep_len:928 episode reward: total was 1.750000. running mean: -6.904355\n",
      "ep 2475: ep_len:189 episode reward: total was 87.000000. running mean: -5.965312\n",
      "ep 2475: ep_len:71 episode reward: total was 31.000000. running mean: -5.595659\n",
      "ep 2475: ep_len:1436 episode reward: total was -0.470000. running mean: -5.544402\n",
      "ep 2475: ep_len:2856 episode reward: total was 2.750000. running mean: -5.461458\n",
      "epsilon:0.009992 episode_count: 37292. steps_count: 40087735.000000\n",
      "ep 2476: ep_len:845 episode reward: total was -46.220000. running mean: -5.869043\n",
      "ep 2476: ep_len:693 episode reward: total was -3.790000. running mean: -5.848253\n",
      "ep 2476: ep_len:3007 episode reward: total was -35.060000. running mean: -6.140371\n",
      "ep 2476: ep_len:620 episode reward: total was -3.270000. running mean: -6.111667\n",
      "ep 2476: ep_len:89 episode reward: total was 41.500000. running mean: -5.635550\n",
      "ep 2476: ep_len:500 episode reward: total was 11.640000. running mean: -5.462795\n",
      "ep 2476: ep_len:3822 episode reward: total was -1404.330000. running mean: -19.451467\n",
      "ep 2476: ep_len:637 episode reward: total was 6.200000. running mean: -19.194952\n",
      "ep 2476: ep_len:867 episode reward: total was 59.590000. running mean: -18.407103\n",
      "ep 2476: ep_len:688 episode reward: total was -30.540000. running mean: -18.528431\n",
      "ep 2476: ep_len:135 episode reward: total was 66.000000. running mean: -17.683147\n",
      "ep 2476: ep_len:40 episode reward: total was 18.500000. running mean: -17.321316\n",
      "ep 2476: ep_len:1094 episode reward: total was -27.610000. running mean: -17.424203\n",
      "ep 2476: ep_len:2887 episode reward: total was 0.240000. running mean: -17.247561\n",
      "ep 2476: ep_len:43 episode reward: total was 20.000000. running mean: -16.875085\n",
      "epsilon:0.009992 episode_count: 37307. steps_count: 40103702.000000\n",
      "ep 2477: ep_len:1378 episode reward: total was 4.210000. running mean: -16.664234\n",
      "ep 2477: ep_len:3803 episode reward: total was -350.560000. running mean: -20.003192\n",
      "ep 2477: ep_len:70 episode reward: total was 33.500000. running mean: -19.468160\n",
      "ep 2477: ep_len:3000 episode reward: total was -24.900000. running mean: -19.522478\n",
      "ep 2477: ep_len:618 episode reward: total was -15.200000. running mean: -19.479253\n",
      "ep 2477: ep_len:71 episode reward: total was 32.500000. running mean: -18.959461\n",
      "ep 2477: ep_len:688 episode reward: total was 1.660000. running mean: -18.753266\n",
      "ep 2477: ep_len:658 episode reward: total was 25.120000. running mean: -18.314534\n",
      "ep 2477: ep_len:505 episode reward: total was 2.860000. running mean: -18.102788\n",
      "ep 2477: ep_len:853 episode reward: total was 36.300000. running mean: -17.558760\n",
      "ep 2477: ep_len:500 episode reward: total was 21.470000. running mean: -17.168473\n",
      "ep 2477: ep_len:59 episode reward: total was 23.500000. running mean: -16.761788\n",
      "ep 2477: ep_len:54 episode reward: total was 25.500000. running mean: -16.339170\n",
      "ep 2477: ep_len:1138 episode reward: total was -7.390000. running mean: -16.249678\n",
      "ep 2477: ep_len:2938 episode reward: total was -7.020000. running mean: -16.157382\n",
      "epsilon:0.009992 episode_count: 37322. steps_count: 40120035.000000\n",
      "ep 2478: ep_len:2505 episode reward: total was -608.430000. running mean: -22.080108\n",
      "ep 2478: ep_len:3736 episode reward: total was -380.380000. running mean: -25.663107\n",
      "ep 2478: ep_len:2982 episode reward: total was -19.520000. running mean: -25.601676\n",
      "ep 2478: ep_len:719 episode reward: total was 1.970000. running mean: -25.325959\n",
      "ep 2478: ep_len:44 episode reward: total was 20.500000. running mean: -24.867699\n",
      "ep 2478: ep_len:1423 episode reward: total was -3.390000. running mean: -24.652922\n",
      "ep 2478: ep_len:349 episode reward: total was 17.110000. running mean: -24.235293\n",
      "ep 2478: ep_len:1601 episode reward: total was -80.640000. running mean: -24.799340\n",
      "ep 2478: ep_len:710 episode reward: total was -5.680000. running mean: -24.608147\n",
      "ep 2478: ep_len:544 episode reward: total was 20.560000. running mean: -24.156465\n",
      "ep 2478: ep_len:116 episode reward: total was 55.000000. running mean: -23.364901\n",
      "ep 2478: ep_len:1120 episode reward: total was -3.020000. running mean: -23.161452\n",
      "ep 2478: ep_len:2726 episode reward: total was -7.740000. running mean: -23.007237\n",
      "ep 2478: ep_len:57 episode reward: total was 27.000000. running mean: -22.507165\n",
      "epsilon:0.009992 episode_count: 37336. steps_count: 40138667.000000\n",
      "ep 2479: ep_len:1070 episode reward: total was -22.800000. running mean: -22.510093\n",
      "ep 2479: ep_len:205 episode reward: total was -2.790000. running mean: -22.312892\n",
      "ep 2479: ep_len:2943 episode reward: total was -35.960000. running mean: -22.449363\n",
      "ep 2479: ep_len:871 episode reward: total was 54.660000. running mean: -21.678270\n",
      "ep 2479: ep_len:133 episode reward: total was 63.500000. running mean: -20.826487\n",
      "ep 2479: ep_len:1455 episode reward: total was -1.810000. running mean: -20.636322\n",
      "ep 2479: ep_len:3618 episode reward: total was -52.090000. running mean: -20.950859\n",
      "ep 2479: ep_len:765 episode reward: total was -40.830000. running mean: -21.149650\n",
      "ep 2479: ep_len:673 episode reward: total was -16.150000. running mean: -21.099654\n",
      "ep 2479: ep_len:701 episode reward: total was -9.900000. running mean: -20.987657\n",
      "ep 2479: ep_len:219 episode reward: total was 102.000000. running mean: -19.757781\n",
      "ep 2479: ep_len:117 episode reward: total was 54.000000. running mean: -19.020203\n",
      "ep 2479: ep_len:649 episode reward: total was 0.850000. running mean: -18.821501\n",
      "ep 2479: ep_len:2799 episode reward: total was 6.370000. running mean: -18.569586\n",
      "ep 2479: ep_len:51 episode reward: total was 22.500000. running mean: -18.158890\n",
      "epsilon:0.009992 episode_count: 37351. steps_count: 40154936.000000\n",
      "ep 2480: ep_len:819 episode reward: total was -39.450000. running mean: -18.371801\n",
      "ep 2480: ep_len:708 episode reward: total was -8.970000. running mean: -18.277783\n",
      "ep 2480: ep_len:3018 episode reward: total was -34.030000. running mean: -18.435305\n",
      "ep 2480: ep_len:610 episode reward: total was -7.720000. running mean: -18.328152\n",
      "ep 2480: ep_len:113 episode reward: total was 53.500000. running mean: -17.609871\n",
      "ep 2480: ep_len:47 episode reward: total was 22.000000. running mean: -17.213772\n",
      "ep 2480: ep_len:585 episode reward: total was -30.560000. running mean: -17.347234\n",
      "ep 2480: ep_len:661 episode reward: total was 6.540000. running mean: -17.108362\n",
      "ep 2480: ep_len:672 episode reward: total was -12.910000. running mean: -17.066378\n",
      "ep 2480: ep_len:688 episode reward: total was 58.650000. running mean: -16.309215\n",
      "ep 2480: ep_len:759 episode reward: total was -23.890000. running mean: -16.385022\n",
      "ep 2480: ep_len:63 episode reward: total was 30.000000. running mean: -15.921172\n",
      "ep 2480: ep_len:1100 episode reward: total was -2.300000. running mean: -15.784960\n",
      "ep 2480: ep_len:2800 episode reward: total was -29.740000. running mean: -15.924511\n",
      "epsilon:0.009992 episode_count: 37365. steps_count: 40167579.000000\n",
      "ep 2481: ep_len:773 episode reward: total was 1.700000. running mean: -15.748266\n",
      "ep 2481: ep_len:965 episode reward: total was 4.160000. running mean: -15.549183\n",
      "ep 2481: ep_len:2947 episode reward: total was -14.730000. running mean: -15.540991\n",
      "ep 2481: ep_len:500 episode reward: total was 14.450000. running mean: -15.241081\n",
      "ep 2481: ep_len:160 episode reward: total was 75.500000. running mean: -14.333670\n",
      "ep 2481: ep_len:68 episode reward: total was 32.500000. running mean: -13.865334\n",
      "ep 2481: ep_len:1069 episode reward: total was -67.250000. running mean: -14.399180\n",
      "ep 2481: ep_len:346 episode reward: total was 9.060000. running mean: -14.164589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2481: ep_len:746 episode reward: total was -12.580000. running mean: -14.148743\n",
      "ep 2481: ep_len:660 episode reward: total was -27.600000. running mean: -14.283255\n",
      "ep 2481: ep_len:500 episode reward: total was 16.540000. running mean: -13.975023\n",
      "ep 2481: ep_len:114 episode reward: total was 55.500000. running mean: -13.280273\n",
      "ep 2481: ep_len:1523 episode reward: total was 9.640000. running mean: -13.051070\n",
      "ep 2481: ep_len:2807 episode reward: total was -5.800000. running mean: -12.978559\n",
      "ep 2481: ep_len:61 episode reward: total was 29.000000. running mean: -12.558774\n",
      "epsilon:0.009992 episode_count: 37380. steps_count: 40180818.000000\n",
      "ep 2482: ep_len:1543 episode reward: total was 19.000000. running mean: -12.243186\n",
      "ep 2482: ep_len:796 episode reward: total was -30.590000. running mean: -12.426654\n",
      "ep 2482: ep_len:3039 episode reward: total was -73.100000. running mean: -13.033387\n",
      "ep 2482: ep_len:1447 episode reward: total was 9.890000. running mean: -12.804154\n",
      "ep 2482: ep_len:40 episode reward: total was 18.500000. running mean: -12.491112\n",
      "ep 2482: ep_len:89 episode reward: total was 43.000000. running mean: -11.936201\n",
      "ep 2482: ep_len:44 episode reward: total was 20.500000. running mean: -11.611839\n",
      "ep 2482: ep_len:1431 episode reward: total was -24.250000. running mean: -11.738220\n",
      "ep 2482: ep_len:651 episode reward: total was 15.280000. running mean: -11.468038\n",
      "ep 2482: ep_len:574 episode reward: total was -36.850000. running mean: -11.721858\n",
      "ep 2482: ep_len:7263 episode reward: total was -90.120000. running mean: -12.505839\n",
      "ep 2482: ep_len:659 episode reward: total was 6.770000. running mean: -12.313081\n",
      "ep 2482: ep_len:89 episode reward: total was 41.500000. running mean: -11.774950\n",
      "ep 2482: ep_len:769 episode reward: total was -83.380000. running mean: -12.491001\n",
      "ep 2482: ep_len:2901 episode reward: total was -14.100000. running mean: -12.507091\n",
      "ep 2482: ep_len:57 episode reward: total was 27.000000. running mean: -12.112020\n",
      "epsilon:0.009992 episode_count: 37396. steps_count: 40202210.000000\n",
      "ep 2483: ep_len:1086 episode reward: total was -129.560000. running mean: -13.286499\n",
      "ep 2483: ep_len:768 episode reward: total was -8.290000. running mean: -13.236534\n",
      "ep 2483: ep_len:2850 episode reward: total was -22.280000. running mean: -13.326969\n",
      "ep 2483: ep_len:1081 episode reward: total was -5.520000. running mean: -13.248899\n",
      "ep 2483: ep_len:815 episode reward: total was -26.880000. running mean: -13.385210\n",
      "ep 2483: ep_len:3575 episode reward: total was -386.590000. running mean: -17.117258\n",
      "ep 2483: ep_len:1270 episode reward: total was -65.240000. running mean: -17.598486\n",
      "ep 2483: ep_len:774 episode reward: total was 10.620000. running mean: -17.316301\n",
      "ep 2483: ep_len:675 episode reward: total was -0.490000. running mean: -17.148038\n",
      "ep 2483: ep_len:53 episode reward: total was 23.500000. running mean: -16.741558\n",
      "ep 2483: ep_len:500 episode reward: total was 25.080000. running mean: -16.323342\n",
      "ep 2483: ep_len:2922 episode reward: total was 4.080000. running mean: -16.119309\n",
      "ep 2483: ep_len:57 episode reward: total was 27.000000. running mean: -15.688115\n",
      "epsilon:0.009992 episode_count: 37409. steps_count: 40218636.000000\n",
      "ep 2484: ep_len:634 episode reward: total was 9.440000. running mean: -15.436834\n",
      "ep 2484: ep_len:1626 episode reward: total was -92.900000. running mean: -16.211466\n",
      "ep 2484: ep_len:49 episode reward: total was 23.000000. running mean: -15.819351\n",
      "ep 2484: ep_len:2998 episode reward: total was -14.030000. running mean: -15.801458\n",
      "ep 2484: ep_len:531 episode reward: total was -18.030000. running mean: -15.823743\n",
      "ep 2484: ep_len:141 episode reward: total was 67.500000. running mean: -14.990506\n",
      "ep 2484: ep_len:1424 episode reward: total was 15.690000. running mean: -14.683701\n",
      "ep 2484: ep_len:3660 episode reward: total was -21.770000. running mean: -14.754564\n",
      "ep 2484: ep_len:999 episode reward: total was -38.680000. running mean: -14.993818\n",
      "ep 2484: ep_len:743 episode reward: total was 23.680000. running mean: -14.607080\n",
      "ep 2484: ep_len:603 episode reward: total was -6.260000. running mean: -14.523609\n",
      "ep 2484: ep_len:70 episode reward: total was 31.510000. running mean: -14.063273\n",
      "ep 2484: ep_len:48 episode reward: total was 22.500000. running mean: -13.697640\n",
      "ep 2484: ep_len:74 episode reward: total was 35.500000. running mean: -13.205664\n",
      "ep 2484: ep_len:650 episode reward: total was -10.840000. running mean: -13.182007\n",
      "ep 2484: ep_len:2676 episode reward: total was -10.350000. running mean: -13.153687\n",
      "epsilon:0.009992 episode_count: 37425. steps_count: 40235562.000000\n",
      "ep 2485: ep_len:917 episode reward: total was -60.270000. running mean: -13.624850\n",
      "ep 2485: ep_len:692 episode reward: total was -1.720000. running mean: -13.505802\n",
      "ep 2485: ep_len:60 episode reward: total was 28.500000. running mean: -13.085744\n",
      "ep 2485: ep_len:3009 episode reward: total was -36.940000. running mean: -13.324286\n",
      "ep 2485: ep_len:706 episode reward: total was -21.980000. running mean: -13.410843\n",
      "ep 2485: ep_len:60 episode reward: total was 28.500000. running mean: -12.991735\n",
      "ep 2485: ep_len:167 episode reward: total was 82.000000. running mean: -12.041818\n",
      "ep 2485: ep_len:1351 episode reward: total was -32.130000. running mean: -12.242700\n",
      "ep 2485: ep_len:3829 episode reward: total was -814.700000. running mean: -20.267273\n",
      "ep 2485: ep_len:549 episode reward: total was -35.080000. running mean: -20.415400\n",
      "ep 2485: ep_len:804 episode reward: total was -14.190000. running mean: -20.353146\n",
      "ep 2485: ep_len:1122 episode reward: total was -11.170000. running mean: -20.261314\n",
      "ep 2485: ep_len:80 episode reward: total was 37.000000. running mean: -19.688701\n",
      "ep 2485: ep_len:1102 episode reward: total was 35.620000. running mean: -19.135614\n",
      "ep 2485: ep_len:2768 episode reward: total was -0.920000. running mean: -18.953458\n",
      "ep 2485: ep_len:50 episode reward: total was 22.000000. running mean: -18.543923\n",
      "epsilon:0.009992 episode_count: 37441. steps_count: 40252828.000000\n",
      "ep 2486: ep_len:1110 episode reward: total was 7.140000. running mean: -18.287084\n",
      "ep 2486: ep_len:816 episode reward: total was 19.730000. running mean: -17.906913\n",
      "ep 2486: ep_len:2959 episode reward: total was -17.100000. running mean: -17.898844\n",
      "ep 2486: ep_len:692 episode reward: total was 25.040000. running mean: -17.469456\n",
      "ep 2486: ep_len:84 episode reward: total was 40.500000. running mean: -16.889761\n",
      "ep 2486: ep_len:650 episode reward: total was -0.740000. running mean: -16.728264\n",
      "ep 2486: ep_len:335 episode reward: total was 10.420000. running mean: -16.456781\n",
      "ep 2486: ep_len:790 episode reward: total was -3.780000. running mean: -16.330013\n",
      "ep 2486: ep_len:7254 episode reward: total was 55.230000. running mean: -15.614413\n",
      "ep 2486: ep_len:627 episode reward: total was 13.990000. running mean: -15.318369\n",
      "ep 2486: ep_len:111 episode reward: total was 54.000000. running mean: -14.625185\n",
      "ep 2486: ep_len:103 episode reward: total was 44.000000. running mean: -14.038933\n",
      "ep 2486: ep_len:647 episode reward: total was 15.160000. running mean: -13.746944\n",
      "ep 2486: ep_len:2746 episode reward: total was -0.350000. running mean: -13.612975\n",
      "epsilon:0.009992 episode_count: 37455. steps_count: 40271752.000000\n",
      "ep 2487: ep_len:708 episode reward: total was -17.330000. running mean: -13.650145\n",
      "ep 2487: ep_len:747 episode reward: total was -8.860000. running mean: -13.602243\n",
      "ep 2487: ep_len:3008 episode reward: total was 0.580000. running mean: -13.460421\n",
      "ep 2487: ep_len:763 episode reward: total was 17.220000. running mean: -13.153617\n",
      "ep 2487: ep_len:70 episode reward: total was 33.500000. running mean: -12.687081\n",
      "ep 2487: ep_len:94 episode reward: total was 45.500000. running mean: -12.105210\n",
      "ep 2487: ep_len:50 episode reward: total was 23.500000. running mean: -11.749158\n",
      "ep 2487: ep_len:1390 episode reward: total was -112.400000. running mean: -12.755666\n",
      "ep 2487: ep_len:673 episode reward: total was 28.820000. running mean: -12.339909\n",
      "ep 2487: ep_len:570 episode reward: total was -6.590000. running mean: -12.282410\n",
      "ep 2487: ep_len:814 episode reward: total was -1.290000. running mean: -12.172486\n",
      "ep 2487: ep_len:864 episode reward: total was 7.740000. running mean: -11.973361\n",
      "ep 2487: ep_len:137 episode reward: total was 67.000000. running mean: -11.183628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2487: ep_len:42 episode reward: total was 18.000000. running mean: -10.891792\n",
      "ep 2487: ep_len:85 episode reward: total was 41.000000. running mean: -10.372874\n",
      "ep 2487: ep_len:1412 episode reward: total was -7.050000. running mean: -10.339645\n",
      "ep 2487: ep_len:2872 episode reward: total was -13.620000. running mean: -10.372448\n",
      "ep 2487: ep_len:48 episode reward: total was 22.500000. running mean: -10.043724\n",
      "epsilon:0.009992 episode_count: 37473. steps_count: 40286099.000000\n",
      "ep 2488: ep_len:1096 episode reward: total was -13.450000. running mean: -10.077787\n",
      "ep 2488: ep_len:928 episode reward: total was 14.530000. running mean: -9.831709\n",
      "ep 2488: ep_len:92 episode reward: total was 44.500000. running mean: -9.288392\n",
      "ep 2488: ep_len:1247 episode reward: total was -26.190000. running mean: -9.457408\n",
      "ep 2488: ep_len:602 episode reward: total was 2.820000. running mean: -9.334634\n",
      "ep 2488: ep_len:634 episode reward: total was 24.970000. running mean: -8.991587\n",
      "ep 2488: ep_len:1220 episode reward: total was -14.870000. running mean: -9.050372\n",
      "ep 2488: ep_len:664 episode reward: total was 1.240000. running mean: -8.947468\n",
      "ep 2488: ep_len:500 episode reward: total was 4.070000. running mean: -8.817293\n",
      "ep 2488: ep_len:53 episode reward: total was 23.500000. running mean: -8.494120\n",
      "ep 2488: ep_len:1526 episode reward: total was -0.150000. running mean: -8.410679\n",
      "ep 2488: ep_len:2832 episode reward: total was 1.590000. running mean: -8.310672\n",
      "ep 2488: ep_len:51 episode reward: total was 24.000000. running mean: -7.987565\n",
      "epsilon:0.009992 episode_count: 37486. steps_count: 40297544.000000\n",
      "ep 2489: ep_len:645 episode reward: total was 12.730000. running mean: -7.780390\n",
      "ep 2489: ep_len:939 episode reward: total was -4.550000. running mean: -7.748086\n",
      "ep 2489: ep_len:48 episode reward: total was 21.000000. running mean: -7.460605\n",
      "ep 2489: ep_len:2987 episode reward: total was -31.620000. running mean: -7.702199\n",
      "ep 2489: ep_len:1211 episode reward: total was -24.010000. running mean: -7.865277\n",
      "ep 2489: ep_len:1397 episode reward: total was 11.470000. running mean: -7.671924\n",
      "ep 2489: ep_len:3724 episode reward: total was -397.100000. running mean: -11.566205\n",
      "ep 2489: ep_len:600 episode reward: total was -22.330000. running mean: -11.673843\n",
      "ep 2489: ep_len:688 episode reward: total was 49.830000. running mean: -11.058805\n",
      "ep 2489: ep_len:500 episode reward: total was 18.430000. running mean: -10.763916\n",
      "ep 2489: ep_len:48 episode reward: total was 22.500000. running mean: -10.431277\n",
      "ep 2489: ep_len:1515 episode reward: total was -9.630000. running mean: -10.423265\n",
      "ep 2489: ep_len:2889 episode reward: total was -19.820000. running mean: -10.517232\n",
      "epsilon:0.009992 episode_count: 37499. steps_count: 40314735.000000\n",
      "ep 2490: ep_len:1370 episode reward: total was 6.020000. running mean: -10.351860\n",
      "ep 2490: ep_len:661 episode reward: total was -20.830000. running mean: -10.456641\n",
      "ep 2490: ep_len:3026 episode reward: total was -25.200000. running mean: -10.604075\n",
      "ep 2490: ep_len:875 episode reward: total was 67.610000. running mean: -9.821934\n",
      "ep 2490: ep_len:930 episode reward: total was 60.020000. running mean: -9.123514\n",
      "ep 2490: ep_len:646 episode reward: total was 20.990000. running mean: -8.822379\n",
      "ep 2490: ep_len:554 episode reward: total was 3.110000. running mean: -8.703056\n",
      "ep 2490: ep_len:813 episode reward: total was -6.660000. running mean: -8.682625\n",
      "ep 2490: ep_len:783 episode reward: total was -16.580000. running mean: -8.761599\n",
      "ep 2490: ep_len:212 episode reward: total was 103.000000. running mean: -7.643983\n",
      "ep 2490: ep_len:39 episode reward: total was 16.500000. running mean: -7.402543\n",
      "ep 2490: ep_len:59 episode reward: total was 28.000000. running mean: -7.048518\n",
      "ep 2490: ep_len:1467 episode reward: total was 20.590000. running mean: -6.772132\n",
      "ep 2490: ep_len:2844 episode reward: total was -0.740000. running mean: -6.711811\n",
      "epsilon:0.009992 episode_count: 37513. steps_count: 40329014.000000\n",
      "ep 2491: ep_len:621 episode reward: total was 4.780000. running mean: -6.596893\n",
      "ep 2491: ep_len:802 episode reward: total was -36.590000. running mean: -6.896824\n",
      "ep 2491: ep_len:3130 episode reward: total was -18.840000. running mean: -7.016256\n",
      "ep 2491: ep_len:799 episode reward: total was 21.250000. running mean: -6.733593\n",
      "ep 2491: ep_len:107 episode reward: total was 50.500000. running mean: -6.161257\n",
      "ep 2491: ep_len:500 episode reward: total was 21.350000. running mean: -5.886145\n",
      "ep 2491: ep_len:3878 episode reward: total was -800.550000. running mean: -13.832783\n",
      "ep 2491: ep_len:845 episode reward: total was -6.900000. running mean: -13.763455\n",
      "ep 2491: ep_len:645 episode reward: total was -11.900000. running mean: -13.744821\n",
      "ep 2491: ep_len:560 episode reward: total was 0.380000. running mean: -13.603573\n",
      "ep 2491: ep_len:50 episode reward: total was 23.500000. running mean: -13.232537\n",
      "ep 2491: ep_len:141 episode reward: total was 64.500000. running mean: -12.455212\n",
      "ep 2491: ep_len:500 episode reward: total was -7.590000. running mean: -12.406559\n",
      "ep 2491: ep_len:2865 episode reward: total was -3.350000. running mean: -12.315994\n",
      "ep 2491: ep_len:43 episode reward: total was 18.500000. running mean: -12.007834\n",
      "epsilon:0.009992 episode_count: 37528. steps_count: 40344500.000000\n",
      "ep 2492: ep_len:646 episode reward: total was 16.880000. running mean: -11.718956\n",
      "ep 2492: ep_len:2063 episode reward: total was -209.710000. running mean: -13.698866\n",
      "ep 2492: ep_len:58 episode reward: total was 26.000000. running mean: -13.301877\n",
      "ep 2492: ep_len:3038 episode reward: total was -31.540000. running mean: -13.484259\n",
      "ep 2492: ep_len:753 episode reward: total was -0.720000. running mean: -13.356616\n",
      "ep 2492: ep_len:175 episode reward: total was 86.000000. running mean: -12.363050\n",
      "ep 2492: ep_len:63 episode reward: total was 28.500000. running mean: -11.954419\n",
      "ep 2492: ep_len:56 episode reward: total was 26.500000. running mean: -11.569875\n",
      "ep 2492: ep_len:1470 episode reward: total was -30.380000. running mean: -11.757976\n",
      "ep 2492: ep_len:4117 episode reward: total was -317.490000. running mean: -14.815297\n",
      "ep 2492: ep_len:563 episode reward: total was 12.630000. running mean: -14.540844\n",
      "ep 2492: ep_len:703 episode reward: total was 23.220000. running mean: -14.163235\n",
      "ep 2492: ep_len:934 episode reward: total was 22.860000. running mean: -13.793003\n",
      "ep 2492: ep_len:100 episode reward: total was 48.500000. running mean: -13.170073\n",
      "ep 2492: ep_len:1091 episode reward: total was -29.660000. running mean: -13.334972\n",
      "ep 2492: ep_len:2820 episode reward: total was -14.020000. running mean: -13.341822\n",
      "epsilon:0.009992 episode_count: 37544. steps_count: 40363150.000000\n",
      "ep 2493: ep_len:696 episode reward: total was -67.950000. running mean: -13.887904\n",
      "ep 2493: ep_len:1618 episode reward: total was -87.350000. running mean: -14.622525\n",
      "ep 2493: ep_len:3045 episode reward: total was -59.210000. running mean: -15.068400\n",
      "ep 2493: ep_len:1667 episode reward: total was -98.060000. running mean: -15.898316\n",
      "ep 2493: ep_len:66 episode reward: total was 31.500000. running mean: -15.424333\n",
      "ep 2493: ep_len:500 episode reward: total was 9.250000. running mean: -15.177589\n",
      "ep 2493: ep_len:359 episode reward: total was 20.240000. running mean: -14.823413\n",
      "ep 2493: ep_len:609 episode reward: total was -52.660000. running mean: -15.201779\n",
      "ep 2493: ep_len:874 episode reward: total was 69.610000. running mean: -14.353662\n",
      "ep 2493: ep_len:1148 episode reward: total was 10.710000. running mean: -14.103025\n",
      "ep 2493: ep_len:44 episode reward: total was 19.000000. running mean: -13.771995\n",
      "ep 2493: ep_len:1437 episode reward: total was 11.690000. running mean: -13.517375\n",
      "ep 2493: ep_len:2828 episode reward: total was -3.870000. running mean: -13.420901\n",
      "epsilon:0.009992 episode_count: 37557. steps_count: 40378041.000000\n",
      "ep 2494: ep_len:1093 episode reward: total was 3.690000. running mean: -13.249792\n",
      "ep 2494: ep_len:700 episode reward: total was 3.060000. running mean: -13.086694\n",
      "ep 2494: ep_len:74 episode reward: total was 34.000000. running mean: -12.615827\n",
      "ep 2494: ep_len:3001 episode reward: total was -10.940000. running mean: -12.599069\n",
      "ep 2494: ep_len:859 episode reward: total was 38.970000. running mean: -12.083378\n",
      "ep 2494: ep_len:49 episode reward: total was 23.000000. running mean: -11.732544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2494: ep_len:500 episode reward: total was -19.520000. running mean: -11.810419\n",
      "ep 2494: ep_len:619 episode reward: total was 22.890000. running mean: -11.463415\n",
      "ep 2494: ep_len:1303 episode reward: total was -72.100000. running mean: -12.069781\n",
      "ep 2494: ep_len:787 episode reward: total was 7.770000. running mean: -11.871383\n",
      "ep 2494: ep_len:645 episode reward: total was -15.370000. running mean: -11.906369\n",
      "ep 2494: ep_len:68 episode reward: total was 32.500000. running mean: -11.462305\n",
      "ep 2494: ep_len:103 episode reward: total was 47.000000. running mean: -10.877682\n",
      "ep 2494: ep_len:851 episode reward: total was 5.540000. running mean: -10.713505\n",
      "ep 2494: ep_len:2821 episode reward: total was -30.290000. running mean: -10.909270\n",
      "epsilon:0.009992 episode_count: 37572. steps_count: 40391514.000000\n",
      "ep 2495: ep_len:802 episode reward: total was -6.080000. running mean: -10.860978\n",
      "ep 2495: ep_len:1643 episode reward: total was -111.000000. running mean: -11.862368\n",
      "ep 2495: ep_len:70 episode reward: total was 33.500000. running mean: -11.408744\n",
      "ep 2495: ep_len:3108 episode reward: total was -53.920000. running mean: -11.833857\n",
      "ep 2495: ep_len:823 episode reward: total was 20.700000. running mean: -11.508518\n",
      "ep 2495: ep_len:102 episode reward: total was 48.000000. running mean: -10.913433\n",
      "ep 2495: ep_len:91 episode reward: total was 44.000000. running mean: -10.364299\n",
      "ep 2495: ep_len:999 episode reward: total was -43.710000. running mean: -10.697756\n",
      "ep 2495: ep_len:3832 episode reward: total was -15.500000. running mean: -10.745778\n",
      "ep 2495: ep_len:559 episode reward: total was -6.180000. running mean: -10.700120\n",
      "ep 2495: ep_len:790 episode reward: total was 31.590000. running mean: -10.277219\n",
      "ep 2495: ep_len:821 episode reward: total was 17.800000. running mean: -9.996447\n",
      "ep 2495: ep_len:68 episode reward: total was 32.500000. running mean: -9.571482\n",
      "ep 2495: ep_len:99 episode reward: total was 46.500000. running mean: -9.010768\n",
      "ep 2495: ep_len:500 episode reward: total was -17.230000. running mean: -9.092960\n",
      "ep 2495: ep_len:2878 episode reward: total was -18.180000. running mean: -9.183830\n",
      "ep 2495: ep_len:32 episode reward: total was 13.000000. running mean: -8.961992\n",
      "epsilon:0.009992 episode_count: 37589. steps_count: 40408731.000000\n",
      "ep 2496: ep_len:1078 episode reward: total was 9.600000. running mean: -8.776372\n",
      "ep 2496: ep_len:1609 episode reward: total was -53.190000. running mean: -9.220508\n",
      "ep 2496: ep_len:2972 episode reward: total was -20.340000. running mean: -9.331703\n",
      "ep 2496: ep_len:1418 episode reward: total was 18.320000. running mean: -9.055186\n",
      "ep 2496: ep_len:42 episode reward: total was 19.500000. running mean: -8.769634\n",
      "ep 2496: ep_len:69 episode reward: total was 33.000000. running mean: -8.351938\n",
      "ep 2496: ep_len:1504 episode reward: total was 15.450000. running mean: -8.113919\n",
      "ep 2496: ep_len:682 episode reward: total was 32.340000. running mean: -7.709380\n",
      "ep 2496: ep_len:1199 episode reward: total was -82.110000. running mean: -8.453386\n",
      "ep 2496: ep_len:788 episode reward: total was 17.670000. running mean: -8.192152\n",
      "ep 2496: ep_len:718 episode reward: total was -22.650000. running mean: -8.336730\n",
      "ep 2496: ep_len:71 episode reward: total was 34.000000. running mean: -7.913363\n",
      "ep 2496: ep_len:64 episode reward: total was 30.500000. running mean: -7.529229\n",
      "ep 2496: ep_len:101 episode reward: total was 47.500000. running mean: -6.978937\n",
      "ep 2496: ep_len:1028 episode reward: total was 11.190000. running mean: -6.797248\n",
      "ep 2496: ep_len:2821 episode reward: total was -28.150000. running mean: -7.010775\n",
      "epsilon:0.009992 episode_count: 37605. steps_count: 40424895.000000\n",
      "ep 2497: ep_len:653 episode reward: total was 15.510000. running mean: -6.785568\n",
      "ep 2497: ep_len:754 episode reward: total was -20.070000. running mean: -6.918412\n",
      "ep 2497: ep_len:2960 episode reward: total was -24.680000. running mean: -7.096028\n",
      "ep 2497: ep_len:2582 episode reward: total was -201.020000. running mean: -9.035267\n",
      "ep 2497: ep_len:142 episode reward: total was 65.000000. running mean: -8.294915\n",
      "ep 2497: ep_len:1395 episode reward: total was -197.310000. running mean: -10.185066\n",
      "ep 2497: ep_len:638 episode reward: total was 6.120000. running mean: -10.022015\n",
      "ep 2497: ep_len:542 episode reward: total was 7.430000. running mean: -9.847495\n",
      "ep 2497: ep_len:703 episode reward: total was 0.890000. running mean: -9.740120\n",
      "ep 2497: ep_len:500 episode reward: total was 4.590000. running mean: -9.596819\n",
      "ep 2497: ep_len:112 episode reward: total was 54.500000. running mean: -8.955851\n",
      "ep 2497: ep_len:47 episode reward: total was 20.500000. running mean: -8.661292\n",
      "ep 2497: ep_len:115 episode reward: total was 56.000000. running mean: -8.014679\n",
      "ep 2497: ep_len:500 episode reward: total was -16.380000. running mean: -8.098332\n",
      "ep 2497: ep_len:2882 episode reward: total was -46.130000. running mean: -8.478649\n",
      "epsilon:0.009992 episode_count: 37620. steps_count: 40439420.000000\n",
      "ep 2498: ep_len:688 episode reward: total was 4.610000. running mean: -8.347762\n",
      "ep 2498: ep_len:1665 episode reward: total was -48.070000. running mean: -8.744985\n",
      "ep 2498: ep_len:3038 episode reward: total was -54.370000. running mean: -9.201235\n",
      "ep 2498: ep_len:790 episode reward: total was 27.500000. running mean: -8.834223\n",
      "ep 2498: ep_len:70 episode reward: total was 33.500000. running mean: -8.410880\n",
      "ep 2498: ep_len:1013 episode reward: total was -3.170000. running mean: -8.358472\n",
      "ep 2498: ep_len:3627 episode reward: total was -159.340000. running mean: -9.868287\n",
      "ep 2498: ep_len:2123 episode reward: total was -586.040000. running mean: -15.630004\n",
      "ep 2498: ep_len:813 episode reward: total was 10.080000. running mean: -15.372904\n",
      "ep 2498: ep_len:1023 episode reward: total was 31.460000. running mean: -14.904575\n",
      "ep 2498: ep_len:1165 episode reward: total was 5.060000. running mean: -14.704929\n",
      "ep 2498: ep_len:2892 episode reward: total was -4.940000. running mean: -14.607280\n",
      "epsilon:0.009992 episode_count: 37632. steps_count: 40458327.000000\n",
      "ep 2499: ep_len:916 episode reward: total was -248.980000. running mean: -16.951007\n",
      "ep 2499: ep_len:673 episode reward: total was -5.770000. running mean: -16.839197\n",
      "ep 2499: ep_len:2881 episode reward: total was -84.490000. running mean: -17.515705\n",
      "ep 2499: ep_len:605 episode reward: total was 3.270000. running mean: -17.307848\n",
      "ep 2499: ep_len:63 episode reward: total was 25.500000. running mean: -16.879770\n",
      "ep 2499: ep_len:43 episode reward: total was 20.000000. running mean: -16.510972\n",
      "ep 2499: ep_len:1424 episode reward: total was -241.830000. running mean: -18.764162\n",
      "ep 2499: ep_len:3831 episode reward: total was -36.070000. running mean: -18.937221\n",
      "ep 2499: ep_len:1270 episode reward: total was -86.810000. running mean: -19.615948\n",
      "ep 2499: ep_len:846 episode reward: total was 9.850000. running mean: -19.321289\n",
      "ep 2499: ep_len:500 episode reward: total was 6.460000. running mean: -19.063476\n",
      "ep 2499: ep_len:147 episode reward: total was 67.500000. running mean: -18.197841\n",
      "ep 2499: ep_len:1073 episode reward: total was -27.240000. running mean: -18.288263\n",
      "ep 2499: ep_len:2858 episode reward: total was -11.500000. running mean: -18.220380\n",
      "epsilon:0.009992 episode_count: 37646. steps_count: 40475457.000000\n",
      "ep 2500: ep_len:863 episode reward: total was -39.920000. running mean: -18.437376\n",
      "ep 2500: ep_len:964 episode reward: total was 11.740000. running mean: -18.135603\n",
      "ep 2500: ep_len:72 episode reward: total was 34.500000. running mean: -17.609247\n",
      "ep 2500: ep_len:3007 episode reward: total was -28.480000. running mean: -17.717954\n",
      "ep 2500: ep_len:500 episode reward: total was 7.410000. running mean: -17.466675\n",
      "ep 2500: ep_len:53 episode reward: total was 25.000000. running mean: -17.042008\n",
      "ep 2500: ep_len:1140 episode reward: total was 1.620000. running mean: -16.855388\n",
      "ep 2500: ep_len:4046 episode reward: total was -28.850000. running mean: -16.975334\n",
      "ep 2500: ep_len:973 episode reward: total was -38.920000. running mean: -17.194780\n",
      "ep 2500: ep_len:644 episode reward: total was -6.520000. running mean: -17.088033\n",
      "ep 2500: ep_len:741 episode reward: total was -1.850000. running mean: -16.935652\n",
      "ep 2500: ep_len:617 episode reward: total was -18.240000. running mean: -16.948696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2500: ep_len:2858 episode reward: total was -7.820000. running mean: -16.857409\n",
      "epsilon:0.009992 episode_count: 37659. steps_count: 40491935.000000\n",
      "ep 2501: ep_len:624 episode reward: total was -10.580000. running mean: -16.794635\n",
      "ep 2501: ep_len:665 episode reward: total was -33.920000. running mean: -16.965888\n",
      "ep 2501: ep_len:2994 episode reward: total was -1.920000. running mean: -16.815430\n",
      "ep 2501: ep_len:779 episode reward: total was 40.160000. running mean: -16.245675\n",
      "ep 2501: ep_len:79 episode reward: total was 35.000000. running mean: -15.733219\n",
      "ep 2501: ep_len:763 episode reward: total was -4.660000. running mean: -15.622486\n",
      "ep 2501: ep_len:3649 episode reward: total was -951.460000. running mean: -24.980861\n",
      "ep 2501: ep_len:816 episode reward: total was 13.410000. running mean: -24.596953\n",
      "ep 2501: ep_len:721 episode reward: total was 17.150000. running mean: -24.179483\n",
      "ep 2501: ep_len:1042 episode reward: total was 3.180000. running mean: -23.905888\n",
      "ep 2501: ep_len:45 episode reward: total was 19.500000. running mean: -23.471830\n",
      "ep 2501: ep_len:849 episode reward: total was -2.620000. running mean: -23.263311\n",
      "ep 2501: ep_len:2841 episode reward: total was -14.050000. running mean: -23.171178\n",
      "ep 2501: ep_len:42 episode reward: total was 19.500000. running mean: -22.744466\n",
      "epsilon:0.009992 episode_count: 37673. steps_count: 40507844.000000\n",
      "ep 2502: ep_len:1430 episode reward: total was 31.830000. running mean: -22.198722\n",
      "ep 2502: ep_len:691 episode reward: total was -27.680000. running mean: -22.253535\n",
      "ep 2502: ep_len:59 episode reward: total was 28.000000. running mean: -21.750999\n",
      "ep 2502: ep_len:2930 episode reward: total was -25.310000. running mean: -21.786589\n",
      "ep 2502: ep_len:648 episode reward: total was 15.590000. running mean: -21.412823\n",
      "ep 2502: ep_len:46 episode reward: total was 21.500000. running mean: -20.983695\n",
      "ep 2502: ep_len:841 episode reward: total was 22.440000. running mean: -20.549458\n",
      "ep 2502: ep_len:349 episode reward: total was 7.070000. running mean: -20.273264\n",
      "ep 2502: ep_len:607 episode reward: total was -36.520000. running mean: -20.435731\n",
      "ep 2502: ep_len:604 episode reward: total was 9.090000. running mean: -20.140474\n",
      "ep 2502: ep_len:1041 episode reward: total was -9.150000. running mean: -20.030569\n",
      "ep 2502: ep_len:66 episode reward: total was 31.500000. running mean: -19.515263\n",
      "ep 2502: ep_len:529 episode reward: total was 17.070000. running mean: -19.149411\n",
      "ep 2502: ep_len:2830 episode reward: total was -8.630000. running mean: -19.044216\n",
      "epsilon:0.009992 episode_count: 37687. steps_count: 40520515.000000\n",
      "ep 2503: ep_len:735 episode reward: total was -77.660000. running mean: -19.630374\n",
      "ep 2503: ep_len:635 episode reward: total was 15.270000. running mean: -19.281371\n",
      "ep 2503: ep_len:3040 episode reward: total was -34.760000. running mean: -19.436157\n",
      "ep 2503: ep_len:818 episode reward: total was 29.190000. running mean: -18.949895\n",
      "ep 2503: ep_len:42 episode reward: total was 19.500000. running mean: -18.565396\n",
      "ep 2503: ep_len:109 episode reward: total was 53.000000. running mean: -17.849742\n",
      "ep 2503: ep_len:74 episode reward: total was 34.000000. running mean: -17.331245\n",
      "ep 2503: ep_len:501 episode reward: total was 26.840000. running mean: -16.889532\n",
      "ep 2503: ep_len:354 episode reward: total was 13.120000. running mean: -16.589437\n",
      "ep 2503: ep_len:1169 episode reward: total was -10.970000. running mean: -16.533243\n",
      "ep 2503: ep_len:7488 episode reward: total was -156.280000. running mean: -17.930710\n",
      "ep 2503: ep_len:703 episode reward: total was -19.010000. running mean: -17.941503\n",
      "ep 2503: ep_len:65 episode reward: total was 29.500000. running mean: -17.467088\n",
      "ep 2503: ep_len:1056 episode reward: total was 3.360000. running mean: -17.258817\n",
      "ep 2503: ep_len:2902 episode reward: total was 0.880000. running mean: -17.077429\n",
      "ep 2503: ep_len:62 episode reward: total was 28.000000. running mean: -16.626655\n",
      "epsilon:0.009992 episode_count: 37703. steps_count: 40540268.000000\n",
      "ep 2504: ep_len:705 episode reward: total was -17.360000. running mean: -16.633988\n",
      "ep 2504: ep_len:500 episode reward: total was 13.660000. running mean: -16.331048\n",
      "ep 2504: ep_len:43 episode reward: total was 20.000000. running mean: -15.967738\n",
      "ep 2504: ep_len:3022 episode reward: total was -52.920000. running mean: -16.337261\n",
      "ep 2504: ep_len:588 episode reward: total was -15.500000. running mean: -16.328888\n",
      "ep 2504: ep_len:60 episode reward: total was 28.500000. running mean: -15.880599\n",
      "ep 2504: ep_len:78 episode reward: total was 37.500000. running mean: -15.346793\n",
      "ep 2504: ep_len:97 episode reward: total was 44.000000. running mean: -14.753325\n",
      "ep 2504: ep_len:727 episode reward: total was -8.450000. running mean: -14.690292\n",
      "ep 2504: ep_len:3912 episode reward: total was -316.160000. running mean: -17.704989\n",
      "ep 2504: ep_len:696 episode reward: total was -30.580000. running mean: -17.833739\n",
      "ep 2504: ep_len:798 episode reward: total was 8.250000. running mean: -17.572902\n",
      "ep 2504: ep_len:633 episode reward: total was 56.360000. running mean: -16.833573\n",
      "ep 2504: ep_len:962 episode reward: total was -42.980000. running mean: -17.095037\n",
      "ep 2504: ep_len:2873 episode reward: total was -33.020000. running mean: -17.254287\n",
      "epsilon:0.009992 episode_count: 37718. steps_count: 40555962.000000\n",
      "ep 2505: ep_len:1047 episode reward: total was -24.610000. running mean: -17.327844\n",
      "ep 2505: ep_len:918 episode reward: total was 6.350000. running mean: -17.091065\n",
      "ep 2505: ep_len:76 episode reward: total was 36.500000. running mean: -16.555155\n",
      "ep 2505: ep_len:2942 episode reward: total was 3.830000. running mean: -16.351303\n",
      "ep 2505: ep_len:679 episode reward: total was -14.630000. running mean: -16.334090\n",
      "ep 2505: ep_len:887 episode reward: total was 45.780000. running mean: -15.712949\n",
      "ep 2505: ep_len:324 episode reward: total was 11.500000. running mean: -15.440820\n",
      "ep 2505: ep_len:776 episode reward: total was -20.580000. running mean: -15.492211\n",
      "ep 2505: ep_len:830 episode reward: total was 41.950000. running mean: -14.917789\n",
      "ep 2505: ep_len:1122 episode reward: total was -30.360000. running mean: -15.072211\n",
      "ep 2505: ep_len:38 episode reward: total was 16.000000. running mean: -14.761489\n",
      "ep 2505: ep_len:1463 episode reward: total was -23.950000. running mean: -14.853374\n",
      "ep 2505: ep_len:2805 episode reward: total was -22.470000. running mean: -14.929541\n",
      "ep 2505: ep_len:56 episode reward: total was 26.500000. running mean: -14.515245\n",
      "epsilon:0.009992 episode_count: 37732. steps_count: 40569925.000000\n",
      "ep 2506: ep_len:500 episode reward: total was 24.310000. running mean: -14.126993\n",
      "ep 2506: ep_len:731 episode reward: total was -11.040000. running mean: -14.096123\n",
      "ep 2506: ep_len:3013 episode reward: total was -1.820000. running mean: -13.973362\n",
      "ep 2506: ep_len:500 episode reward: total was -9.600000. running mean: -13.929628\n",
      "ep 2506: ep_len:41 episode reward: total was 19.000000. running mean: -13.600332\n",
      "ep 2506: ep_len:96 episode reward: total was 17.510000. running mean: -13.289228\n",
      "ep 2506: ep_len:49 episode reward: total was 23.000000. running mean: -12.926336\n",
      "ep 2506: ep_len:1471 episode reward: total was -27.880000. running mean: -13.075873\n",
      "ep 2506: ep_len:3796 episode reward: total was -99.190000. running mean: -13.937014\n",
      "ep 2506: ep_len:653 episode reward: total was 5.350000. running mean: -13.744144\n",
      "ep 2506: ep_len:713 episode reward: total was 21.850000. running mean: -13.388203\n",
      "ep 2506: ep_len:1041 episode reward: total was 25.090000. running mean: -13.003420\n",
      "ep 2506: ep_len:192 episode reward: total was 93.000000. running mean: -11.943386\n",
      "ep 2506: ep_len:40 episode reward: total was 18.500000. running mean: -11.638952\n",
      "ep 2506: ep_len:1178 episode reward: total was -30.810000. running mean: -11.830663\n",
      "ep 2506: ep_len:2829 episode reward: total was -51.590000. running mean: -12.228256\n",
      "epsilon:0.009992 episode_count: 37748. steps_count: 40586768.000000\n",
      "ep 2507: ep_len:1402 episode reward: total was -12.630000. running mean: -12.232274\n",
      "ep 2507: ep_len:666 episode reward: total was -7.650000. running mean: -12.186451\n",
      "ep 2507: ep_len:3036 episode reward: total was -52.860000. running mean: -12.593186\n",
      "ep 2507: ep_len:500 episode reward: total was 13.750000. running mean: -12.329755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2507: ep_len:79 episode reward: total was 36.500000. running mean: -11.841457\n",
      "ep 2507: ep_len:35 episode reward: total was 16.000000. running mean: -11.563042\n",
      "ep 2507: ep_len:500 episode reward: total was 1.070000. running mean: -11.436712\n",
      "ep 2507: ep_len:3849 episode reward: total was -70.530000. running mean: -12.027645\n",
      "ep 2507: ep_len:540 episode reward: total was -28.100000. running mean: -12.188368\n",
      "ep 2507: ep_len:730 episode reward: total was -9.490000. running mean: -12.161385\n",
      "ep 2507: ep_len:1068 episode reward: total was 13.610000. running mean: -11.903671\n",
      "ep 2507: ep_len:61 episode reward: total was 27.500000. running mean: -11.509634\n",
      "ep 2507: ep_len:89 episode reward: total was 41.500000. running mean: -10.979538\n",
      "ep 2507: ep_len:1119 episode reward: total was 15.490000. running mean: -10.714843\n",
      "ep 2507: ep_len:2867 episode reward: total was -9.630000. running mean: -10.703994\n",
      "epsilon:0.009992 episode_count: 37763. steps_count: 40603309.000000\n",
      "ep 2508: ep_len:689 episode reward: total was 43.210000. running mean: -10.164854\n",
      "ep 2508: ep_len:179 episode reward: total was -5.840000. running mean: -10.121606\n",
      "ep 2508: ep_len:68 episode reward: total was 31.000000. running mean: -9.710390\n",
      "ep 2508: ep_len:2948 episode reward: total was -106.390000. running mean: -10.677186\n",
      "ep 2508: ep_len:517 episode reward: total was -35.400000. running mean: -10.924414\n",
      "ep 2508: ep_len:127 episode reward: total was 62.000000. running mean: -10.195170\n",
      "ep 2508: ep_len:38 episode reward: total was 16.000000. running mean: -9.933218\n",
      "ep 2508: ep_len:928 episode reward: total was 71.630000. running mean: -9.117586\n",
      "ep 2508: ep_len:3924 episode reward: total was -35.290000. running mean: -9.379310\n",
      "ep 2508: ep_len:613 episode reward: total was -26.360000. running mean: -9.549117\n",
      "ep 2508: ep_len:836 episode reward: total was 34.480000. running mean: -9.108826\n",
      "ep 2508: ep_len:971 episode reward: total was 45.280000. running mean: -8.564937\n",
      "ep 2508: ep_len:54 episode reward: total was 24.000000. running mean: -8.239288\n",
      "ep 2508: ep_len:36 episode reward: total was 16.500000. running mean: -7.991895\n",
      "ep 2508: ep_len:1173 episode reward: total was -19.890000. running mean: -8.110876\n",
      "ep 2508: ep_len:2748 episode reward: total was -40.730000. running mean: -8.437067\n",
      "ep 2508: ep_len:64 episode reward: total was 29.000000. running mean: -8.062697\n",
      "epsilon:0.009992 episode_count: 37780. steps_count: 40619222.000000\n",
      "ep 2509: ep_len:1376 episode reward: total was -6.030000. running mean: -8.042370\n",
      "ep 2509: ep_len:216 episode reward: total was 14.000000. running mean: -7.821946\n",
      "ep 2509: ep_len:2889 episode reward: total was -27.780000. running mean: -8.021527\n",
      "ep 2509: ep_len:832 episode reward: total was 23.300000. running mean: -7.708311\n",
      "ep 2509: ep_len:44 episode reward: total was 20.500000. running mean: -7.426228\n",
      "ep 2509: ep_len:1053 episode reward: total was -26.000000. running mean: -7.611966\n",
      "ep 2509: ep_len:339 episode reward: total was 26.620000. running mean: -7.269646\n",
      "ep 2509: ep_len:1258 episode reward: total was -126.600000. running mean: -8.462950\n",
      "ep 2509: ep_len:690 episode reward: total was 24.280000. running mean: -8.135520\n",
      "ep 2509: ep_len:1148 episode reward: total was -53.330000. running mean: -8.587465\n",
      "ep 2509: ep_len:1493 episode reward: total was -25.360000. running mean: -8.755191\n",
      "ep 2509: ep_len:2854 episode reward: total was 9.400000. running mean: -8.573639\n",
      "epsilon:0.009992 episode_count: 37792. steps_count: 40633414.000000\n",
      "ep 2510: ep_len:864 episode reward: total was -72.820000. running mean: -9.216102\n",
      "ep 2510: ep_len:749 episode reward: total was -103.750000. running mean: -10.161441\n",
      "ep 2510: ep_len:2943 episode reward: total was -42.850000. running mean: -10.488327\n",
      "ep 2510: ep_len:500 episode reward: total was 7.290000. running mean: -10.310544\n",
      "ep 2510: ep_len:107 episode reward: total was 52.000000. running mean: -9.687438\n",
      "ep 2510: ep_len:869 episode reward: total was 37.110000. running mean: -9.219464\n",
      "ep 2510: ep_len:4111 episode reward: total was -89.620000. running mean: -10.023469\n",
      "ep 2510: ep_len:636 episode reward: total was -22.090000. running mean: -10.144134\n",
      "ep 2510: ep_len:861 episode reward: total was 61.460000. running mean: -9.428093\n",
      "ep 2510: ep_len:1495 episode reward: total was -10.720000. running mean: -9.441012\n",
      "ep 2510: ep_len:117 episode reward: total was 55.500000. running mean: -8.791602\n",
      "ep 2510: ep_len:500 episode reward: total was 42.560000. running mean: -8.278086\n",
      "ep 2510: ep_len:2808 episode reward: total was -4.130000. running mean: -8.236605\n",
      "ep 2510: ep_len:42 episode reward: total was 19.500000. running mean: -7.959239\n",
      "epsilon:0.009992 episode_count: 37806. steps_count: 40650016.000000\n",
      "ep 2511: ep_len:1060 episode reward: total was -24.310000. running mean: -8.122747\n",
      "ep 2511: ep_len:1602 episode reward: total was -112.730000. running mean: -9.168819\n",
      "ep 2511: ep_len:77 episode reward: total was 34.000000. running mean: -8.737131\n",
      "ep 2511: ep_len:875 episode reward: total was 46.610000. running mean: -8.183660\n",
      "ep 2511: ep_len:66 episode reward: total was 30.000000. running mean: -7.801823\n",
      "ep 2511: ep_len:51 episode reward: total was 22.500000. running mean: -7.498805\n",
      "ep 2511: ep_len:815 episode reward: total was 16.430000. running mean: -7.259517\n",
      "ep 2511: ep_len:326 episode reward: total was 25.110000. running mean: -6.935822\n",
      "ep 2511: ep_len:948 episode reward: total was -20.990000. running mean: -7.076363\n",
      "ep 2511: ep_len:844 episode reward: total was 64.440000. running mean: -6.361200\n",
      "ep 2511: ep_len:1490 episode reward: total was -7.890000. running mean: -6.376488\n",
      "ep 2511: ep_len:749 episode reward: total was -68.310000. running mean: -6.995823\n",
      "ep 2511: ep_len:2818 episode reward: total was -29.130000. running mean: -7.217165\n",
      "epsilon:0.009992 episode_count: 37819. steps_count: 40661737.000000\n",
      "ep 2512: ep_len:971 episode reward: total was -28.050000. running mean: -7.425493\n",
      "ep 2512: ep_len:766 episode reward: total was -27.570000. running mean: -7.626938\n",
      "ep 2512: ep_len:3028 episode reward: total was -34.130000. running mean: -7.891969\n",
      "ep 2512: ep_len:1162 episode reward: total was -52.570000. running mean: -8.338749\n",
      "ep 2512: ep_len:52 episode reward: total was 23.000000. running mean: -8.025362\n",
      "ep 2512: ep_len:1797 episode reward: total was -46.230000. running mean: -8.407408\n",
      "ep 2512: ep_len:615 episode reward: total was 24.290000. running mean: -8.080434\n",
      "ep 2512: ep_len:891 episode reward: total was -17.520000. running mean: -8.174830\n",
      "ep 2512: ep_len:680 episode reward: total was 25.040000. running mean: -7.842681\n",
      "ep 2512: ep_len:1534 episode reward: total was -207.600000. running mean: -9.840254\n",
      "ep 2512: ep_len:48 episode reward: total was 21.000000. running mean: -9.531852\n",
      "ep 2512: ep_len:911 episode reward: total was 13.380000. running mean: -9.302733\n",
      "ep 2512: ep_len:2887 episode reward: total was 1.650000. running mean: -9.193206\n",
      "epsilon:0.009992 episode_count: 37832. steps_count: 40677079.000000\n",
      "ep 2513: ep_len:1124 episode reward: total was -13.170000. running mean: -9.232974\n",
      "ep 2513: ep_len:791 episode reward: total was -34.680000. running mean: -9.487444\n",
      "ep 2513: ep_len:62 episode reward: total was 28.000000. running mean: -9.112570\n",
      "ep 2513: ep_len:3014 episode reward: total was -49.160000. running mean: -9.513044\n",
      "ep 2513: ep_len:536 episode reward: total was -24.100000. running mean: -9.658914\n",
      "ep 2513: ep_len:40 episode reward: total was 18.500000. running mean: -9.377324\n",
      "ep 2513: ep_len:90 episode reward: total was 43.500000. running mean: -8.848551\n",
      "ep 2513: ep_len:99 episode reward: total was 48.000000. running mean: -8.280066\n",
      "ep 2513: ep_len:37 episode reward: total was 17.000000. running mean: -8.027265\n",
      "ep 2513: ep_len:1066 episode reward: total was -11.730000. running mean: -8.064292\n",
      "ep 2513: ep_len:3890 episode reward: total was -24.620000. running mean: -8.229850\n",
      "ep 2513: ep_len:807 episode reward: total was -1.540000. running mean: -8.162951\n",
      "ep 2513: ep_len:7264 episode reward: total was 8.450000. running mean: -7.996821\n",
      "ep 2513: ep_len:500 episode reward: total was 9.030000. running mean: -7.826553\n",
      "ep 2513: ep_len:66 episode reward: total was 31.500000. running mean: -7.433288\n",
      "ep 2513: ep_len:134 episode reward: total was 64.000000. running mean: -6.718955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2513: ep_len:128 episode reward: total was 59.500000. running mean: -6.056765\n",
      "ep 2513: ep_len:560 episode reward: total was 2.190000. running mean: -5.974298\n",
      "ep 2513: ep_len:2708 episode reward: total was -19.240000. running mean: -6.106955\n",
      "epsilon:0.009992 episode_count: 37851. steps_count: 40699995.000000\n",
      "ep 2514: ep_len:653 episode reward: total was 40.740000. running mean: -5.638485\n",
      "ep 2514: ep_len:3749 episode reward: total was -329.910000. running mean: -8.881200\n",
      "ep 2514: ep_len:2982 episode reward: total was -16.550000. running mean: -8.957888\n",
      "ep 2514: ep_len:803 episode reward: total was -23.540000. running mean: -9.103709\n",
      "ep 2514: ep_len:57 episode reward: total was 27.000000. running mean: -8.742672\n",
      "ep 2514: ep_len:62 episode reward: total was 29.500000. running mean: -8.360246\n",
      "ep 2514: ep_len:857 episode reward: total was 28.540000. running mean: -7.991243\n",
      "ep 2514: ep_len:359 episode reward: total was 17.760000. running mean: -7.733731\n",
      "ep 2514: ep_len:602 episode reward: total was -37.060000. running mean: -8.026993\n",
      "ep 2514: ep_len:846 episode reward: total was 56.650000. running mean: -7.380223\n",
      "ep 2514: ep_len:699 episode reward: total was 5.950000. running mean: -7.246921\n",
      "ep 2514: ep_len:185 episode reward: total was -168.990000. running mean: -8.864352\n",
      "ep 2514: ep_len:89 episode reward: total was 43.000000. running mean: -8.345708\n",
      "ep 2514: ep_len:500 episode reward: total was 17.330000. running mean: -8.088951\n",
      "ep 2514: ep_len:2930 episode reward: total was -28.100000. running mean: -8.289062\n",
      "epsilon:0.009992 episode_count: 37866. steps_count: 40715368.000000\n",
      "ep 2515: ep_len:665 episode reward: total was 9.400000. running mean: -8.112171\n",
      "ep 2515: ep_len:1230 episode reward: total was -134.340000. running mean: -9.374450\n",
      "ep 2515: ep_len:2938 episode reward: total was -73.500000. running mean: -10.015705\n",
      "ep 2515: ep_len:1636 episode reward: total was -113.650000. running mean: -11.052048\n",
      "ep 2515: ep_len:47 episode reward: total was 22.000000. running mean: -10.721528\n",
      "ep 2515: ep_len:148 episode reward: total was 69.500000. running mean: -9.919312\n",
      "ep 2515: ep_len:103 episode reward: total was 48.500000. running mean: -9.335119\n",
      "ep 2515: ep_len:500 episode reward: total was 14.740000. running mean: -9.094368\n",
      "ep 2515: ep_len:4012 episode reward: total was -474.460000. running mean: -13.748024\n",
      "ep 2515: ep_len:627 episode reward: total was -24.200000. running mean: -13.852544\n",
      "ep 2515: ep_len:703 episode reward: total was -12.210000. running mean: -13.836119\n",
      "ep 2515: ep_len:929 episode reward: total was 10.500000. running mean: -13.592757\n",
      "ep 2515: ep_len:46 episode reward: total was 20.000000. running mean: -13.256830\n",
      "ep 2515: ep_len:95 episode reward: total was 44.500000. running mean: -12.679262\n",
      "ep 2515: ep_len:645 episode reward: total was -6.860000. running mean: -12.621069\n",
      "ep 2515: ep_len:2834 episode reward: total was -19.940000. running mean: -12.694258\n",
      "ep 2515: ep_len:58 episode reward: total was 27.500000. running mean: -12.292316\n",
      "epsilon:0.009992 episode_count: 37883. steps_count: 40732584.000000\n",
      "ep 2516: ep_len:713 episode reward: total was -49.600000. running mean: -12.665392\n",
      "ep 2516: ep_len:1716 episode reward: total was -77.390000. running mean: -13.312639\n",
      "ep 2516: ep_len:48 episode reward: total was 22.500000. running mean: -12.954512\n",
      "ep 2516: ep_len:2890 episode reward: total was -60.030000. running mean: -13.425267\n",
      "ep 2516: ep_len:654 episode reward: total was 7.900000. running mean: -13.212014\n",
      "ep 2516: ep_len:93 episode reward: total was 45.000000. running mean: -12.629894\n",
      "ep 2516: ep_len:51 episode reward: total was 24.000000. running mean: -12.263595\n",
      "ep 2516: ep_len:1042 episode reward: total was -35.200000. running mean: -12.492959\n",
      "ep 2516: ep_len:3643 episode reward: total was -31.030000. running mean: -12.678330\n",
      "ep 2516: ep_len:1287 episode reward: total was -89.310000. running mean: -13.444646\n",
      "ep 2516: ep_len:860 episode reward: total was 65.860000. running mean: -12.651600\n",
      "ep 2516: ep_len:517 episode reward: total was 27.270000. running mean: -12.252384\n",
      "ep 2516: ep_len:123 episode reward: total was 60.000000. running mean: -11.529860\n",
      "ep 2516: ep_len:45 episode reward: total was 21.000000. running mean: -11.204562\n",
      "ep 2516: ep_len:1510 episode reward: total was -241.140000. running mean: -13.503916\n",
      "ep 2516: ep_len:2836 episode reward: total was -16.000000. running mean: -13.528877\n",
      "epsilon:0.009992 episode_count: 37899. steps_count: 40750612.000000\n",
      "ep 2517: ep_len:600 episode reward: total was 25.260000. running mean: -13.140988\n",
      "ep 2517: ep_len:204 episode reward: total was 3.780000. running mean: -12.971778\n",
      "ep 2517: ep_len:56 episode reward: total was 26.500000. running mean: -12.577060\n",
      "ep 2517: ep_len:2872 episode reward: total was -33.640000. running mean: -12.787690\n",
      "ep 2517: ep_len:625 episode reward: total was -2.000000. running mean: -12.679813\n",
      "ep 2517: ep_len:108 episode reward: total was 49.500000. running mean: -12.058015\n",
      "ep 2517: ep_len:639 episode reward: total was -7.900000. running mean: -12.016435\n",
      "ep 2517: ep_len:646 episode reward: total was 18.970000. running mean: -11.706570\n",
      "ep 2517: ep_len:1207 episode reward: total was -9.940000. running mean: -11.688905\n",
      "ep 2517: ep_len:734 episode reward: total was -11.930000. running mean: -11.691315\n",
      "ep 2517: ep_len:1505 episode reward: total was -24.200000. running mean: -11.816402\n",
      "ep 2517: ep_len:50 episode reward: total was 23.500000. running mean: -11.463238\n",
      "ep 2517: ep_len:74 episode reward: total was 35.500000. running mean: -10.993606\n",
      "ep 2517: ep_len:1194 episode reward: total was -16.680000. running mean: -11.050470\n",
      "ep 2517: ep_len:2896 episode reward: total was -58.500000. running mean: -11.524965\n",
      "epsilon:0.009992 episode_count: 37914. steps_count: 40764022.000000\n",
      "ep 2518: ep_len:1396 episode reward: total was -6.780000. running mean: -11.477516\n",
      "ep 2518: ep_len:1616 episode reward: total was -87.490000. running mean: -12.237640\n",
      "ep 2518: ep_len:2985 episode reward: total was -69.970000. running mean: -12.814964\n",
      "ep 2518: ep_len:826 episode reward: total was 37.910000. running mean: -12.307714\n",
      "ep 2518: ep_len:33 episode reward: total was 15.000000. running mean: -12.034637\n",
      "ep 2518: ep_len:1407 episode reward: total was -74.090000. running mean: -12.655191\n",
      "ep 2518: ep_len:673 episode reward: total was 6.750000. running mean: -12.461139\n",
      "ep 2518: ep_len:864 episode reward: total was 12.510000. running mean: -12.211427\n",
      "ep 2518: ep_len:882 episode reward: total was 49.150000. running mean: -11.597813\n",
      "ep 2518: ep_len:637 episode reward: total was -4.020000. running mean: -11.522035\n",
      "ep 2518: ep_len:166 episode reward: total was -149.990000. running mean: -12.906715\n",
      "ep 2518: ep_len:55 episode reward: total was 26.000000. running mean: -12.517648\n",
      "ep 2518: ep_len:50 episode reward: total was 20.500000. running mean: -12.187471\n",
      "ep 2518: ep_len:651 episode reward: total was -6.390000. running mean: -12.129496\n",
      "ep 2518: ep_len:2927 episode reward: total was -6.220000. running mean: -12.070401\n",
      "epsilon:0.009992 episode_count: 37929. steps_count: 40779190.000000\n",
      "ep 2519: ep_len:500 episode reward: total was 28.750000. running mean: -11.662197\n",
      "ep 2519: ep_len:1113 episode reward: total was -22.800000. running mean: -11.773575\n",
      "ep 2519: ep_len:3016 episode reward: total was -6.450000. running mean: -11.720340\n",
      "ep 2519: ep_len:843 episode reward: total was 20.840000. running mean: -11.394736\n",
      "ep 2519: ep_len:32 episode reward: total was 13.000000. running mean: -11.150789\n",
      "ep 2519: ep_len:88 episode reward: total was 42.500000. running mean: -10.614281\n",
      "ep 2519: ep_len:62 episode reward: total was 26.500000. running mean: -10.243138\n",
      "ep 2519: ep_len:1032 episode reward: total was -19.140000. running mean: -10.332107\n",
      "ep 2519: ep_len:664 episode reward: total was 17.130000. running mean: -10.057486\n",
      "ep 2519: ep_len:947 episode reward: total was -8.880000. running mean: -10.045711\n",
      "ep 2519: ep_len:7359 episode reward: total was 27.790000. running mean: -9.667354\n",
      "ep 2519: ep_len:1458 episode reward: total was -30.470000. running mean: -9.875380\n",
      "ep 2519: ep_len:78 episode reward: total was 34.500000. running mean: -9.431626\n",
      "ep 2519: ep_len:635 episode reward: total was -5.940000. running mean: -9.396710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2519: ep_len:46 episode reward: total was 21.500000. running mean: -9.087743\n",
      "ep 2519: ep_len:46 episode reward: total was 21.500000. running mean: -8.781866\n",
      "epsilon:0.009992 episode_count: 37945. steps_count: 40797109.000000\n",
      "ep 2520: ep_len:741 episode reward: total was -12.490000. running mean: -8.818947\n",
      "ep 2520: ep_len:1668 episode reward: total was -98.660000. running mean: -9.717358\n",
      "ep 2520: ep_len:3078 episode reward: total was -49.260000. running mean: -10.112784\n",
      "ep 2520: ep_len:1653 episode reward: total was -84.270000. running mean: -10.854356\n",
      "ep 2520: ep_len:37 episode reward: total was 17.000000. running mean: -10.575813\n",
      "ep 2520: ep_len:60 episode reward: total was 28.500000. running mean: -10.185054\n",
      "ep 2520: ep_len:1500 episode reward: total was -46.300000. running mean: -10.546204\n",
      "ep 2520: ep_len:3884 episode reward: total was -567.650000. running mean: -16.117242\n",
      "ep 2520: ep_len:869 episode reward: total was 17.240000. running mean: -15.783669\n",
      "ep 2520: ep_len:7362 episode reward: total was 5.760000. running mean: -15.568233\n",
      "ep 2520: ep_len:676 episode reward: total was -23.780000. running mean: -15.650350\n",
      "ep 2520: ep_len:79 episode reward: total was 38.000000. running mean: -15.113847\n",
      "ep 2520: ep_len:166 episode reward: total was 80.000000. running mean: -14.162708\n",
      "ep 2520: ep_len:87 episode reward: total was 42.000000. running mean: -13.601081\n",
      "ep 2520: ep_len:1080 episode reward: total was -41.890000. running mean: -13.883971\n",
      "ep 2520: ep_len:2792 episode reward: total was -83.770000. running mean: -14.582831\n",
      "epsilon:0.009992 episode_count: 37961. steps_count: 40822841.000000\n",
      "ep 2521: ep_len:669 episode reward: total was 0.900000. running mean: -14.428003\n",
      "ep 2521: ep_len:711 episode reward: total was -21.540000. running mean: -14.499123\n",
      "ep 2521: ep_len:2955 episode reward: total was -2.340000. running mean: -14.377531\n",
      "ep 2521: ep_len:617 episode reward: total was 6.960000. running mean: -14.164156\n",
      "ep 2521: ep_len:31 episode reward: total was 14.000000. running mean: -13.882514\n",
      "ep 2521: ep_len:702 episode reward: total was 12.430000. running mean: -13.619389\n",
      "ep 2521: ep_len:3639 episode reward: total was -29.050000. running mean: -13.773695\n",
      "ep 2521: ep_len:531 episode reward: total was -5.450000. running mean: -13.690458\n",
      "ep 2521: ep_len:805 episode reward: total was 24.730000. running mean: -13.306254\n",
      "ep 2521: ep_len:516 episode reward: total was -6.120000. running mean: -13.234391\n",
      "ep 2521: ep_len:31 episode reward: total was 14.000000. running mean: -12.962047\n",
      "ep 2521: ep_len:741 episode reward: total was -25.080000. running mean: -13.083227\n",
      "ep 2521: ep_len:2778 episode reward: total was -8.200000. running mean: -13.034395\n",
      "epsilon:0.009992 episode_count: 37974. steps_count: 40837567.000000\n",
      "ep 2522: ep_len:1379 episode reward: total was -22.800000. running mean: -13.132051\n",
      "ep 2522: ep_len:752 episode reward: total was -24.370000. running mean: -13.244430\n",
      "ep 2522: ep_len:54 episode reward: total was 24.000000. running mean: -12.871986\n",
      "ep 2522: ep_len:2995 episode reward: total was -2.480000. running mean: -12.768066\n",
      "ep 2522: ep_len:531 episode reward: total was -95.340000. running mean: -13.593785\n",
      "ep 2522: ep_len:121 episode reward: total was 59.000000. running mean: -12.867848\n",
      "ep 2522: ep_len:500 episode reward: total was 7.320000. running mean: -12.665969\n",
      "ep 2522: ep_len:604 episode reward: total was -51.970000. running mean: -13.059009\n",
      "ep 2522: ep_len:739 episode reward: total was -15.160000. running mean: -13.080019\n",
      "ep 2522: ep_len:853 episode reward: total was 56.600000. running mean: -12.383219\n",
      "ep 2522: ep_len:592 episode reward: total was -9.400000. running mean: -12.353387\n",
      "ep 2522: ep_len:183 episode reward: total was 88.500000. running mean: -11.344853\n",
      "ep 2522: ep_len:101 episode reward: total was 49.000000. running mean: -10.741404\n",
      "ep 2522: ep_len:1461 episode reward: total was -29.020000. running mean: -10.924190\n",
      "ep 2522: ep_len:2756 episode reward: total was -46.890000. running mean: -11.283849\n",
      "epsilon:0.009992 episode_count: 37989. steps_count: 40851188.000000\n",
      "ep 2523: ep_len:644 episode reward: total was 13.670000. running mean: -11.034310\n",
      "ep 2523: ep_len:796 episode reward: total was -18.470000. running mean: -11.108667\n",
      "ep 2523: ep_len:2892 episode reward: total was -13.270000. running mean: -11.130280\n",
      "ep 2523: ep_len:612 episode reward: total was 0.340000. running mean: -11.015577\n",
      "ep 2523: ep_len:151 episode reward: total was 72.500000. running mean: -10.180422\n",
      "ep 2523: ep_len:58 episode reward: total was 27.500000. running mean: -9.803617\n",
      "ep 2523: ep_len:703 episode reward: total was 3.040000. running mean: -9.675181\n",
      "ep 2523: ep_len:636 episode reward: total was -5.860000. running mean: -9.637029\n",
      "ep 2523: ep_len:592 episode reward: total was -56.350000. running mean: -10.104159\n",
      "ep 2523: ep_len:815 episode reward: total was 6.380000. running mean: -9.939318\n",
      "ep 2523: ep_len:972 episode reward: total was -7.830000. running mean: -9.918224\n",
      "ep 2523: ep_len:903 episode reward: total was 12.720000. running mean: -9.691842\n",
      "ep 2523: ep_len:2847 episode reward: total was 4.130000. running mean: -9.553624\n",
      "ep 2523: ep_len:71 episode reward: total was 32.500000. running mean: -9.133088\n",
      "epsilon:0.009992 episode_count: 38003. steps_count: 40863880.000000\n",
      "ep 2524: ep_len:589 episode reward: total was 8.320000. running mean: -8.958557\n",
      "ep 2524: ep_len:756 episode reward: total was -6.230000. running mean: -8.931271\n",
      "ep 2524: ep_len:78 episode reward: total was 37.500000. running mean: -8.466958\n",
      "ep 2524: ep_len:2939 episode reward: total was -31.730000. running mean: -8.699589\n",
      "ep 2524: ep_len:1187 episode reward: total was -87.370000. running mean: -9.486293\n",
      "ep 2524: ep_len:48 episode reward: total was 22.500000. running mean: -9.166430\n",
      "ep 2524: ep_len:108 episode reward: total was 48.000000. running mean: -8.594766\n",
      "ep 2524: ep_len:76 episode reward: total was 36.500000. running mean: -8.143818\n",
      "ep 2524: ep_len:59 episode reward: total was 28.000000. running mean: -7.782380\n",
      "ep 2524: ep_len:1107 episode reward: total was -3.600000. running mean: -7.740556\n",
      "ep 2524: ep_len:647 episode reward: total was 11.880000. running mean: -7.544350\n",
      "ep 2524: ep_len:552 episode reward: total was 2.320000. running mean: -7.445707\n",
      "ep 2524: ep_len:685 episode reward: total was 25.270000. running mean: -7.118550\n",
      "ep 2524: ep_len:678 episode reward: total was -12.160000. running mean: -7.168964\n",
      "ep 2524: ep_len:93 episode reward: total was 45.000000. running mean: -6.647275\n",
      "ep 2524: ep_len:1059 episode reward: total was -8.980000. running mean: -6.670602\n",
      "ep 2524: ep_len:2850 episode reward: total was -11.090000. running mean: -6.714796\n",
      "epsilon:0.009992 episode_count: 38020. steps_count: 40877391.000000\n",
      "ep 2525: ep_len:1489 episode reward: total was -24.090000. running mean: -6.888548\n",
      "ep 2525: ep_len:738 episode reward: total was -8.380000. running mean: -6.903463\n",
      "ep 2525: ep_len:3042 episode reward: total was 14.600000. running mean: -6.688428\n",
      "ep 2525: ep_len:615 episode reward: total was 28.680000. running mean: -6.334744\n",
      "ep 2525: ep_len:625 episode reward: total was -56.740000. running mean: -6.838796\n",
      "ep 2525: ep_len:3921 episode reward: total was -406.170000. running mean: -10.832108\n",
      "ep 2525: ep_len:4229 episode reward: total was -1149.590000. running mean: -22.219687\n",
      "ep 2525: ep_len:851 episode reward: total was 56.000000. running mean: -21.437490\n",
      "ep 2525: ep_len:976 episode reward: total was 13.450000. running mean: -21.088615\n",
      "ep 2525: ep_len:770 episode reward: total was -107.540000. running mean: -21.953129\n",
      "ep 2525: ep_len:2842 episode reward: total was -1.280000. running mean: -21.746398\n",
      "epsilon:0.009992 episode_count: 38031. steps_count: 40897489.000000\n",
      "ep 2526: ep_len:1152 episode reward: total was -7.840000. running mean: -21.607334\n",
      "ep 2526: ep_len:705 episode reward: total was -24.490000. running mean: -21.636161\n",
      "ep 2526: ep_len:2982 episode reward: total was -12.780000. running mean: -21.547599\n",
      "ep 2526: ep_len:1095 episode reward: total was -28.610000. running mean: -21.618223\n",
      "ep 2526: ep_len:44 episode reward: total was 20.500000. running mean: -21.197041\n",
      "ep 2526: ep_len:60 episode reward: total was 28.500000. running mean: -20.700070\n",
      "ep 2526: ep_len:858 episode reward: total was 28.000000. running mean: -20.213070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2526: ep_len:336 episode reward: total was -58.040000. running mean: -20.591339\n",
      "ep 2526: ep_len:1581 episode reward: total was -148.900000. running mean: -21.874426\n",
      "ep 2526: ep_len:839 episode reward: total was 40.240000. running mean: -21.253281\n",
      "ep 2526: ep_len:740 episode reward: total was -8.830000. running mean: -21.129049\n",
      "ep 2526: ep_len:152 episode reward: total was 73.000000. running mean: -20.187758\n",
      "ep 2526: ep_len:822 episode reward: total was -4.600000. running mean: -20.031880\n",
      "ep 2526: ep_len:2851 episode reward: total was -22.620000. running mean: -20.057762\n",
      "epsilon:0.009992 episode_count: 38045. steps_count: 40911706.000000\n",
      "ep 2527: ep_len:1450 episode reward: total was -44.710000. running mean: -20.304284\n",
      "ep 2527: ep_len:216 episode reward: total was 6.930000. running mean: -20.031941\n",
      "ep 2527: ep_len:3008 episode reward: total was 4.520000. running mean: -19.786422\n",
      "ep 2527: ep_len:1447 episode reward: total was -48.450000. running mean: -20.073058\n",
      "ep 2527: ep_len:134 episode reward: total was 64.000000. running mean: -19.232327\n",
      "ep 2527: ep_len:70 episode reward: total was 33.500000. running mean: -18.705004\n",
      "ep 2527: ep_len:500 episode reward: total was 13.410000. running mean: -18.383854\n",
      "ep 2527: ep_len:3690 episode reward: total was -87.550000. running mean: -19.075515\n",
      "ep 2527: ep_len:820 episode reward: total was -62.530000. running mean: -19.510060\n",
      "ep 2527: ep_len:773 episode reward: total was 29.520000. running mean: -19.019759\n",
      "ep 2527: ep_len:3620 episode reward: total was -578.010000. running mean: -24.609662\n",
      "ep 2527: ep_len:143 episode reward: total was 70.000000. running mean: -23.663565\n",
      "ep 2527: ep_len:755 episode reward: total was -50.190000. running mean: -23.928830\n",
      "ep 2527: ep_len:2863 episode reward: total was 6.090000. running mean: -23.628641\n",
      "epsilon:0.009992 episode_count: 38059. steps_count: 40931195.000000\n",
      "ep 2528: ep_len:1192 episode reward: total was 5.670000. running mean: -23.335655\n",
      "ep 2528: ep_len:709 episode reward: total was -15.480000. running mean: -23.257098\n",
      "ep 2528: ep_len:41 episode reward: total was 16.000000. running mean: -22.864527\n",
      "ep 2528: ep_len:2965 episode reward: total was -17.650000. running mean: -22.812382\n",
      "ep 2528: ep_len:500 episode reward: total was 14.480000. running mean: -22.439458\n",
      "ep 2528: ep_len:160 episode reward: total was 75.500000. running mean: -21.460064\n",
      "ep 2528: ep_len:65 episode reward: total was 31.000000. running mean: -20.935463\n",
      "ep 2528: ep_len:1438 episode reward: total was -23.190000. running mean: -20.958008\n",
      "ep 2528: ep_len:362 episode reward: total was -27.690000. running mean: -21.025328\n",
      "ep 2528: ep_len:794 episode reward: total was -27.010000. running mean: -21.085175\n",
      "ep 2528: ep_len:732 episode reward: total was 15.420000. running mean: -20.720123\n",
      "ep 2528: ep_len:988 episode reward: total was 5.950000. running mean: -20.453422\n",
      "ep 2528: ep_len:69 episode reward: total was 30.000000. running mean: -19.948888\n",
      "ep 2528: ep_len:690 episode reward: total was -51.970000. running mean: -20.269099\n",
      "ep 2528: ep_len:2845 episode reward: total was -5.600000. running mean: -20.122408\n",
      "ep 2528: ep_len:47 episode reward: total was 22.000000. running mean: -19.701184\n",
      "epsilon:0.009992 episode_count: 38075. steps_count: 40944792.000000\n",
      "ep 2529: ep_len:653 episode reward: total was 27.390000. running mean: -19.230272\n",
      "ep 2529: ep_len:966 episode reward: total was 24.400000. running mean: -18.793969\n",
      "ep 2529: ep_len:76 episode reward: total was 36.500000. running mean: -18.241030\n",
      "ep 2529: ep_len:3062 episode reward: total was -11.320000. running mean: -18.171819\n",
      "ep 2529: ep_len:718 episode reward: total was -1.560000. running mean: -18.005701\n",
      "ep 2529: ep_len:1388 episode reward: total was -93.350000. running mean: -18.759144\n",
      "ep 2529: ep_len:636 episode reward: total was 8.770000. running mean: -18.483853\n",
      "ep 2529: ep_len:1158 episode reward: total was -64.280000. running mean: -18.941814\n",
      "ep 2529: ep_len:687 episode reward: total was 40.270000. running mean: -18.349696\n",
      "ep 2529: ep_len:1054 episode reward: total was 19.380000. running mean: -17.972399\n",
      "ep 2529: ep_len:87 episode reward: total was 39.000000. running mean: -17.402675\n",
      "ep 2529: ep_len:627 episode reward: total was 6.920000. running mean: -17.159448\n",
      "ep 2529: ep_len:2913 episode reward: total was 0.440000. running mean: -16.983454\n",
      "ep 2529: ep_len:51 episode reward: total was 24.000000. running mean: -16.573619\n",
      "epsilon:0.009992 episode_count: 38089. steps_count: 40958868.000000\n",
      "ep 2530: ep_len:500 episode reward: total was 34.510000. running mean: -16.062783\n",
      "ep 2530: ep_len:933 episode reward: total was 33.280000. running mean: -15.569355\n",
      "ep 2530: ep_len:67 episode reward: total was 32.000000. running mean: -15.093662\n",
      "ep 2530: ep_len:3030 episode reward: total was 14.360000. running mean: -14.799125\n",
      "ep 2530: ep_len:691 episode reward: total was -4.500000. running mean: -14.696134\n",
      "ep 2530: ep_len:155 episode reward: total was 74.500000. running mean: -13.804172\n",
      "ep 2530: ep_len:604 episode reward: total was 0.820000. running mean: -13.657931\n",
      "ep 2530: ep_len:350 episode reward: total was -8.100000. running mean: -13.602351\n",
      "ep 2530: ep_len:516 episode reward: total was -29.840000. running mean: -13.764728\n",
      "ep 2530: ep_len:852 episode reward: total was 38.370000. running mean: -13.243381\n",
      "ep 2530: ep_len:727 episode reward: total was 24.630000. running mean: -12.864647\n",
      "ep 2530: ep_len:92 episode reward: total was 43.000000. running mean: -12.306000\n",
      "ep 2530: ep_len:118 episode reward: total was 57.500000. running mean: -11.607940\n",
      "ep 2530: ep_len:622 episode reward: total was -10.110000. running mean: -11.592961\n",
      "ep 2530: ep_len:2816 episode reward: total was -5.770000. running mean: -11.534731\n",
      "ep 2530: ep_len:61 episode reward: total was 27.500000. running mean: -11.144384\n",
      "epsilon:0.009992 episode_count: 38105. steps_count: 40971002.000000\n",
      "ep 2531: ep_len:2553 episode reward: total was -216.330000. running mean: -13.196240\n",
      "ep 2531: ep_len:1472 episode reward: total was -532.870000. running mean: -18.392978\n",
      "ep 2531: ep_len:2976 episode reward: total was -6.610000. running mean: -18.275148\n",
      "ep 2531: ep_len:500 episode reward: total was 11.190000. running mean: -17.980497\n",
      "ep 2531: ep_len:44 episode reward: total was 17.500000. running mean: -17.625692\n",
      "ep 2531: ep_len:41 episode reward: total was 19.000000. running mean: -17.259435\n",
      "ep 2531: ep_len:500 episode reward: total was 23.640000. running mean: -16.850440\n",
      "ep 2531: ep_len:616 episode reward: total was -15.000000. running mean: -16.831936\n",
      "ep 2531: ep_len:1548 episode reward: total was -42.750000. running mean: -17.091117\n",
      "ep 2531: ep_len:681 episode reward: total was 14.300000. running mean: -16.777205\n",
      "ep 2531: ep_len:719 episode reward: total was -3.020000. running mean: -16.639633\n",
      "ep 2531: ep_len:164 episode reward: total was 79.000000. running mean: -15.683237\n",
      "ep 2531: ep_len:767 episode reward: total was -55.760000. running mean: -16.084005\n",
      "ep 2531: ep_len:2851 episode reward: total was 2.300000. running mean: -15.900165\n",
      "epsilon:0.009992 episode_count: 38119. steps_count: 40986434.000000\n",
      "ep 2532: ep_len:1470 episode reward: total was -26.020000. running mean: -16.001363\n",
      "ep 2532: ep_len:692 episode reward: total was 5.810000. running mean: -15.783249\n",
      "ep 2532: ep_len:2963 episode reward: total was -34.960000. running mean: -15.975017\n",
      "ep 2532: ep_len:1227 episode reward: total was -5.760000. running mean: -15.872867\n",
      "ep 2532: ep_len:57 episode reward: total was 27.000000. running mean: -15.444138\n",
      "ep 2532: ep_len:71 episode reward: total was 34.000000. running mean: -14.949697\n",
      "ep 2532: ep_len:1018 episode reward: total was -42.510000. running mean: -15.225300\n",
      "ep 2532: ep_len:632 episode reward: total was 16.410000. running mean: -14.908947\n",
      "ep 2532: ep_len:652 episode reward: total was -14.860000. running mean: -14.908457\n",
      "ep 2532: ep_len:644 episode reward: total was 6.040000. running mean: -14.698973\n",
      "ep 2532: ep_len:676 episode reward: total was 22.670000. running mean: -14.325283\n",
      "ep 2532: ep_len:59 episode reward: total was 28.000000. running mean: -13.902030\n",
      "ep 2532: ep_len:32 episode reward: total was 14.500000. running mean: -13.618010\n",
      "ep 2532: ep_len:1073 episode reward: total was 15.010000. running mean: -13.331730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2532: ep_len:2672 episode reward: total was -6.380000. running mean: -13.262212\n",
      "epsilon:0.009992 episode_count: 38134. steps_count: 41000372.000000\n",
      "ep 2533: ep_len:1169 episode reward: total was -6.600000. running mean: -13.195590\n",
      "ep 2533: ep_len:699 episode reward: total was -17.560000. running mean: -13.239234\n",
      "ep 2533: ep_len:54 episode reward: total was 25.500000. running mean: -12.851842\n",
      "ep 2533: ep_len:2957 episode reward: total was 5.150000. running mean: -12.671824\n",
      "ep 2533: ep_len:760 episode reward: total was -3.460000. running mean: -12.579705\n",
      "ep 2533: ep_len:41 episode reward: total was 19.000000. running mean: -12.263908\n",
      "ep 2533: ep_len:80 episode reward: total was 38.500000. running mean: -11.756269\n",
      "ep 2533: ep_len:54 episode reward: total was 24.000000. running mean: -11.398707\n",
      "ep 2533: ep_len:602 episode reward: total was 31.260000. running mean: -10.972119\n",
      "ep 2533: ep_len:624 episode reward: total was -10.820000. running mean: -10.970598\n",
      "ep 2533: ep_len:2841 episode reward: total was -820.700000. running mean: -19.067892\n",
      "ep 2533: ep_len:754 episode reward: total was -6.460000. running mean: -18.941813\n",
      "ep 2533: ep_len:3902 episode reward: total was -426.020000. running mean: -23.012595\n",
      "ep 2533: ep_len:49 episode reward: total was 23.000000. running mean: -22.552469\n",
      "ep 2533: ep_len:659 episode reward: total was -2.670000. running mean: -22.353645\n",
      "ep 2533: ep_len:2725 episode reward: total was 4.890000. running mean: -22.081208\n",
      "epsilon:0.009992 episode_count: 38150. steps_count: 41018342.000000\n",
      "ep 2534: ep_len:1104 episode reward: total was -3.760000. running mean: -21.897996\n",
      "ep 2534: ep_len:739 episode reward: total was -47.490000. running mean: -22.153916\n",
      "ep 2534: ep_len:3004 episode reward: total was -34.670000. running mean: -22.279077\n",
      "ep 2534: ep_len:671 episode reward: total was -11.250000. running mean: -22.168786\n",
      "ep 2534: ep_len:40 episode reward: total was 13.510000. running mean: -21.811998\n",
      "ep 2534: ep_len:68 episode reward: total was 29.500000. running mean: -21.298878\n",
      "ep 2534: ep_len:500 episode reward: total was 23.920000. running mean: -20.846690\n",
      "ep 2534: ep_len:3548 episode reward: total was -653.130000. running mean: -27.169523\n",
      "ep 2534: ep_len:638 episode reward: total was -19.040000. running mean: -27.088227\n",
      "ep 2534: ep_len:758 episode reward: total was -22.280000. running mean: -27.040145\n",
      "ep 2534: ep_len:1511 episode reward: total was -51.450000. running mean: -27.284244\n",
      "ep 2534: ep_len:153 episode reward: total was 75.000000. running mean: -26.261401\n",
      "ep 2534: ep_len:685 episode reward: total was 3.230000. running mean: -25.966487\n",
      "ep 2534: ep_len:2809 episode reward: total was -8.500000. running mean: -25.791822\n",
      "ep 2534: ep_len:40 episode reward: total was 17.000000. running mean: -25.363904\n",
      "epsilon:0.009992 episode_count: 38165. steps_count: 41034610.000000\n",
      "ep 2535: ep_len:500 episode reward: total was 18.680000. running mean: -24.923465\n",
      "ep 2535: ep_len:709 episode reward: total was -14.980000. running mean: -24.824030\n",
      "ep 2535: ep_len:2943 episode reward: total was -87.080000. running mean: -25.446590\n",
      "ep 2535: ep_len:1432 episode reward: total was -29.220000. running mean: -25.484324\n",
      "ep 2535: ep_len:85 episode reward: total was 41.000000. running mean: -24.819481\n",
      "ep 2535: ep_len:50 episode reward: total was 22.000000. running mean: -24.351286\n",
      "ep 2535: ep_len:1130 episode reward: total was 3.700000. running mean: -24.070773\n",
      "ep 2535: ep_len:3650 episode reward: total was -2800.610000. running mean: -51.836166\n",
      "ep 2535: ep_len:1088 episode reward: total was -32.410000. running mean: -51.641904\n",
      "ep 2535: ep_len:686 episode reward: total was -6.360000. running mean: -51.189085\n",
      "ep 2535: ep_len:915 episode reward: total was 15.230000. running mean: -50.524894\n",
      "ep 2535: ep_len:60 episode reward: total was 25.500000. running mean: -49.764645\n",
      "ep 2535: ep_len:676 episode reward: total was -12.620000. running mean: -49.393199\n",
      "ep 2535: ep_len:2752 episode reward: total was -16.140000. running mean: -49.060667\n",
      "epsilon:0.009992 episode_count: 38179. steps_count: 41051286.000000\n",
      "ep 2536: ep_len:765 episode reward: total was -8.480000. running mean: -48.654860\n",
      "ep 2536: ep_len:629 episode reward: total was -54.420000. running mean: -48.712511\n",
      "ep 2536: ep_len:2979 episode reward: total was -42.550000. running mean: -48.650886\n",
      "ep 2536: ep_len:661 episode reward: total was -18.730000. running mean: -48.351677\n",
      "ep 2536: ep_len:606 episode reward: total was 53.190000. running mean: -47.336261\n",
      "ep 2536: ep_len:608 episode reward: total was 2.490000. running mean: -46.837998\n",
      "ep 2536: ep_len:521 episode reward: total was -89.320000. running mean: -47.262818\n",
      "ep 2536: ep_len:785 episode reward: total was 32.470000. running mean: -46.465490\n",
      "ep 2536: ep_len:500 episode reward: total was 5.550000. running mean: -45.945335\n",
      "ep 2536: ep_len:650 episode reward: total was -6.800000. running mean: -45.553882\n",
      "ep 2536: ep_len:2832 episode reward: total was -16.500000. running mean: -45.263343\n",
      "ep 2536: ep_len:45 episode reward: total was 21.000000. running mean: -44.600709\n",
      "epsilon:0.009992 episode_count: 38191. steps_count: 41062867.000000\n",
      "ep 2537: ep_len:865 episode reward: total was 10.470000. running mean: -44.050002\n",
      "ep 2537: ep_len:758 episode reward: total was -2.570000. running mean: -43.635202\n",
      "ep 2537: ep_len:67 episode reward: total was 30.500000. running mean: -42.893850\n",
      "ep 2537: ep_len:2972 episode reward: total was -82.740000. running mean: -43.292312\n",
      "ep 2537: ep_len:642 episode reward: total was 1.200000. running mean: -42.847389\n",
      "ep 2537: ep_len:89 episode reward: total was 40.000000. running mean: -42.018915\n",
      "ep 2537: ep_len:74 episode reward: total was 34.000000. running mean: -41.258726\n",
      "ep 2537: ep_len:1403 episode reward: total was -24.430000. running mean: -41.090438\n",
      "ep 2537: ep_len:673 episode reward: total was 2.590000. running mean: -40.653634\n",
      "ep 2537: ep_len:543 episode reward: total was 5.880000. running mean: -40.188298\n",
      "ep 2537: ep_len:829 episode reward: total was 61.110000. running mean: -39.175315\n",
      "ep 2537: ep_len:1541 episode reward: total was -20.080000. running mean: -38.984361\n",
      "ep 2537: ep_len:65 episode reward: total was 29.500000. running mean: -38.299518\n",
      "ep 2537: ep_len:132 episode reward: total was 64.500000. running mean: -37.271523\n",
      "ep 2537: ep_len:1107 episode reward: total was -8.560000. running mean: -36.984407\n",
      "ep 2537: ep_len:2894 episode reward: total was -10.430000. running mean: -36.718863\n",
      "epsilon:0.009992 episode_count: 38207. steps_count: 41077521.000000\n",
      "ep 2538: ep_len:1034 episode reward: total was -9.020000. running mean: -36.441875\n",
      "ep 2538: ep_len:630 episode reward: total was -22.150000. running mean: -36.298956\n",
      "ep 2538: ep_len:3044 episode reward: total was -43.340000. running mean: -36.369366\n",
      "ep 2538: ep_len:1624 episode reward: total was -87.600000. running mean: -36.881673\n",
      "ep 2538: ep_len:41 episode reward: total was 19.000000. running mean: -36.322856\n",
      "ep 2538: ep_len:93 episode reward: total was 43.500000. running mean: -35.524627\n",
      "ep 2538: ep_len:1851 episode reward: total was -16.980000. running mean: -35.339181\n",
      "ep 2538: ep_len:614 episode reward: total was 7.020000. running mean: -34.915589\n",
      "ep 2538: ep_len:1351 episode reward: total was -101.090000. running mean: -35.577334\n",
      "ep 2538: ep_len:7282 episode reward: total was 53.030000. running mean: -34.691260\n",
      "ep 2538: ep_len:630 episode reward: total was -4.960000. running mean: -34.393948\n",
      "ep 2538: ep_len:36 episode reward: total was 16.500000. running mean: -33.885008\n",
      "ep 2538: ep_len:608 episode reward: total was -2.390000. running mean: -33.570058\n",
      "ep 2538: ep_len:2843 episode reward: total was -3.170000. running mean: -33.266057\n",
      "epsilon:0.009992 episode_count: 38221. steps_count: 41099202.000000\n",
      "ep 2539: ep_len:1105 episode reward: total was -4.270000. running mean: -32.976097\n",
      "ep 2539: ep_len:1649 episode reward: total was -94.780000. running mean: -33.594136\n",
      "ep 2539: ep_len:2957 episode reward: total was -105.300000. running mean: -34.311195\n",
      "ep 2539: ep_len:678 episode reward: total was -5.510000. running mean: -34.023183\n",
      "ep 2539: ep_len:62 episode reward: total was 29.500000. running mean: -33.387951\n",
      "ep 2539: ep_len:62 episode reward: total was 26.500000. running mean: -32.789071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2539: ep_len:1012 episode reward: total was -23.100000. running mean: -32.692181\n",
      "ep 2539: ep_len:3987 episode reward: total was -197.250000. running mean: -34.337759\n",
      "ep 2539: ep_len:666 episode reward: total was 0.430000. running mean: -33.990081\n",
      "ep 2539: ep_len:681 episode reward: total was 3.830000. running mean: -33.611880\n",
      "ep 2539: ep_len:902 episode reward: total was 37.570000. running mean: -32.900062\n",
      "ep 2539: ep_len:808 episode reward: total was -16.330000. running mean: -32.734361\n",
      "ep 2539: ep_len:2792 episode reward: total was -6.990000. running mean: -32.476917\n",
      "epsilon:0.009992 episode_count: 38234. steps_count: 41116563.000000\n",
      "ep 2540: ep_len:663 episode reward: total was -7.680000. running mean: -32.228948\n",
      "ep 2540: ep_len:680 episode reward: total was -78.580000. running mean: -32.692459\n",
      "ep 2540: ep_len:2852 episode reward: total was -132.330000. running mean: -33.688834\n",
      "ep 2540: ep_len:844 episode reward: total was 31.990000. running mean: -33.032046\n",
      "ep 2540: ep_len:133 episode reward: total was 65.000000. running mean: -32.051725\n",
      "ep 2540: ep_len:47 episode reward: total was 17.500000. running mean: -31.556208\n",
      "ep 2540: ep_len:769 episode reward: total was -47.540000. running mean: -31.716046\n",
      "ep 2540: ep_len:658 episode reward: total was -1.660000. running mean: -31.415485\n",
      "ep 2540: ep_len:579 episode reward: total was -1.450000. running mean: -31.115831\n",
      "ep 2540: ep_len:916 episode reward: total was 44.870000. running mean: -30.355972\n",
      "ep 2540: ep_len:655 episode reward: total was -5.290000. running mean: -30.105313\n",
      "ep 2540: ep_len:90 episode reward: total was 40.500000. running mean: -29.399259\n",
      "ep 2540: ep_len:107 episode reward: total was 50.500000. running mean: -28.600267\n",
      "ep 2540: ep_len:46 episode reward: total was 18.500000. running mean: -28.129264\n",
      "ep 2540: ep_len:983 episode reward: total was -78.000000. running mean: -28.627972\n",
      "ep 2540: ep_len:2918 episode reward: total was -1.140000. running mean: -28.353092\n",
      "ep 2540: ep_len:43 episode reward: total was 18.500000. running mean: -27.884561\n",
      "epsilon:0.009992 episode_count: 38251. steps_count: 41129546.000000\n",
      "ep 2541: ep_len:500 episode reward: total was 7.840000. running mean: -27.527315\n",
      "ep 2541: ep_len:196 episode reward: total was 7.460000. running mean: -27.177442\n",
      "ep 2541: ep_len:45 episode reward: total was 21.000000. running mean: -26.695668\n",
      "ep 2541: ep_len:2935 episode reward: total was -59.160000. running mean: -27.020311\n",
      "ep 2541: ep_len:664 episode reward: total was -10.160000. running mean: -26.851708\n",
      "ep 2541: ep_len:81 episode reward: total was 37.500000. running mean: -26.208191\n",
      "ep 2541: ep_len:30 episode reward: total was 13.500000. running mean: -25.811109\n",
      "ep 2541: ep_len:911 episode reward: total was 80.620000. running mean: -24.746798\n",
      "ep 2541: ep_len:3807 episode reward: total was -351.450000. running mean: -28.013830\n",
      "ep 2541: ep_len:651 episode reward: total was 6.340000. running mean: -27.670292\n",
      "ep 2541: ep_len:7304 episode reward: total was -82.640000. running mean: -28.219989\n",
      "ep 2541: ep_len:961 episode reward: total was 59.540000. running mean: -27.342389\n",
      "ep 2541: ep_len:157 episode reward: total was 75.500000. running mean: -26.313965\n",
      "ep 2541: ep_len:771 episode reward: total was -13.140000. running mean: -26.182225\n",
      "ep 2541: ep_len:2788 episode reward: total was 1.020000. running mean: -25.910203\n",
      "epsilon:0.009992 episode_count: 38266. steps_count: 41151347.000000\n",
      "ep 2542: ep_len:1056 episode reward: total was -4.700000. running mean: -25.698101\n",
      "ep 2542: ep_len:751 episode reward: total was -49.100000. running mean: -25.932120\n",
      "ep 2542: ep_len:3034 episode reward: total was -211.060000. running mean: -27.783399\n",
      "ep 2542: ep_len:915 episode reward: total was 76.430000. running mean: -26.741265\n",
      "ep 2542: ep_len:79 episode reward: total was 38.000000. running mean: -26.093852\n",
      "ep 2542: ep_len:500 episode reward: total was 42.900000. running mean: -25.403914\n",
      "ep 2542: ep_len:335 episode reward: total was -20.340000. running mean: -25.353274\n",
      "ep 2542: ep_len:532 episode reward: total was -45.350000. running mean: -25.553242\n",
      "ep 2542: ep_len:7520 episode reward: total was 9.170000. running mean: -25.206009\n",
      "ep 2542: ep_len:1048 episode reward: total was 23.170000. running mean: -24.722249\n",
      "ep 2542: ep_len:162 episode reward: total was 75.000000. running mean: -23.725027\n",
      "ep 2542: ep_len:63 episode reward: total was 30.000000. running mean: -23.187776\n",
      "ep 2542: ep_len:500 episode reward: total was 34.140000. running mean: -22.614499\n",
      "ep 2542: ep_len:2821 episode reward: total was -16.250000. running mean: -22.550854\n",
      "ep 2542: ep_len:58 episode reward: total was 26.000000. running mean: -22.065345\n",
      "epsilon:0.009992 episode_count: 38281. steps_count: 41170721.000000\n",
      "ep 2543: ep_len:1408 episode reward: total was -6.500000. running mean: -21.909692\n",
      "ep 2543: ep_len:673 episode reward: total was 2.410000. running mean: -21.666495\n",
      "ep 2543: ep_len:56 episode reward: total was 25.000000. running mean: -21.199830\n",
      "ep 2543: ep_len:2979 episode reward: total was -177.380000. running mean: -22.761632\n",
      "ep 2543: ep_len:788 episode reward: total was -20.450000. running mean: -22.738515\n",
      "ep 2543: ep_len:116 episode reward: total was 56.500000. running mean: -21.946130\n",
      "ep 2543: ep_len:668 episode reward: total was 5.500000. running mean: -21.671669\n",
      "ep 2543: ep_len:3730 episode reward: total was -85.540000. running mean: -22.310352\n",
      "ep 2543: ep_len:657 episode reward: total was -65.400000. running mean: -22.741249\n",
      "ep 2543: ep_len:771 episode reward: total was 1.640000. running mean: -22.497436\n",
      "ep 2543: ep_len:979 episode reward: total was 63.300000. running mean: -21.639462\n",
      "ep 2543: ep_len:81 episode reward: total was 36.000000. running mean: -21.063067\n",
      "ep 2543: ep_len:905 episode reward: total was 20.330000. running mean: -20.649136\n",
      "ep 2543: ep_len:2746 episode reward: total was 15.390000. running mean: -20.288745\n",
      "ep 2543: ep_len:55 episode reward: total was 26.000000. running mean: -19.825858\n",
      "epsilon:0.009992 episode_count: 38296. steps_count: 41187333.000000\n",
      "ep 2544: ep_len:1104 episode reward: total was 2.240000. running mean: -19.605199\n",
      "ep 2544: ep_len:1599 episode reward: total was -94.270000. running mean: -20.351847\n",
      "ep 2544: ep_len:83 episode reward: total was 38.500000. running mean: -19.763329\n",
      "ep 2544: ep_len:2771 episode reward: total was -187.190000. running mean: -21.437595\n",
      "ep 2544: ep_len:1181 episode reward: total was -31.790000. running mean: -21.541119\n",
      "ep 2544: ep_len:43 episode reward: total was 20.000000. running mean: -21.125708\n",
      "ep 2544: ep_len:71 episode reward: total was 32.500000. running mean: -20.589451\n",
      "ep 2544: ep_len:68 episode reward: total was 32.500000. running mean: -20.058557\n",
      "ep 2544: ep_len:1066 episode reward: total was -9.710000. running mean: -19.955071\n",
      "ep 2544: ep_len:4221 episode reward: total was -103.810000. running mean: -20.793620\n",
      "ep 2544: ep_len:1256 episode reward: total was -60.540000. running mean: -21.191084\n",
      "ep 2544: ep_len:646 episode reward: total was 5.680000. running mean: -20.922373\n",
      "ep 2544: ep_len:610 episode reward: total was 6.960000. running mean: -20.643550\n",
      "ep 2544: ep_len:29 episode reward: total was 13.000000. running mean: -20.307114\n",
      "ep 2544: ep_len:181 episode reward: total was 87.500000. running mean: -19.229043\n",
      "ep 2544: ep_len:65 episode reward: total was 31.000000. running mean: -18.726752\n",
      "ep 2544: ep_len:108 episode reward: total was 51.000000. running mean: -18.029485\n",
      "ep 2544: ep_len:500 episode reward: total was 31.260000. running mean: -17.536590\n",
      "ep 2544: ep_len:2809 episode reward: total was -7.920000. running mean: -17.440424\n",
      "ep 2544: ep_len:38 episode reward: total was 17.500000. running mean: -17.091020\n",
      "epsilon:0.009992 episode_count: 38316. steps_count: 41205782.000000\n",
      "ep 2545: ep_len:1111 episode reward: total was 0.350000. running mean: -16.916610\n",
      "ep 2545: ep_len:764 episode reward: total was -23.060000. running mean: -16.978044\n",
      "ep 2545: ep_len:2885 episode reward: total was -338.220000. running mean: -20.190463\n",
      "ep 2545: ep_len:555 episode reward: total was 13.480000. running mean: -19.853759\n",
      "ep 2545: ep_len:56 episode reward: total was 25.000000. running mean: -19.405221\n",
      "ep 2545: ep_len:623 episode reward: total was -9.240000. running mean: -19.303569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2545: ep_len:320 episode reward: total was 1.270000. running mean: -19.097833\n",
      "ep 2545: ep_len:594 episode reward: total was 17.780000. running mean: -18.729055\n",
      "ep 2545: ep_len:799 episode reward: total was 31.350000. running mean: -18.228264\n",
      "ep 2545: ep_len:1458 episode reward: total was -16.750000. running mean: -18.213482\n",
      "ep 2545: ep_len:66 episode reward: total was 31.500000. running mean: -17.716347\n",
      "ep 2545: ep_len:156 episode reward: total was 73.500000. running mean: -16.804183\n",
      "ep 2545: ep_len:45 episode reward: total was 21.000000. running mean: -16.426141\n",
      "ep 2545: ep_len:71 episode reward: total was 31.000000. running mean: -15.951880\n",
      "ep 2545: ep_len:607 episode reward: total was 10.210000. running mean: -15.690261\n",
      "ep 2545: ep_len:2877 episode reward: total was -4.050000. running mean: -15.573859\n",
      "epsilon:0.009992 episode_count: 38332. steps_count: 41218769.000000\n",
      "ep 2546: ep_len:721 episode reward: total was -2.950000. running mean: -15.447620\n",
      "ep 2546: ep_len:200 episode reward: total was 0.280000. running mean: -15.290344\n",
      "ep 2546: ep_len:32 episode reward: total was 14.500000. running mean: -14.992440\n",
      "ep 2546: ep_len:101 episode reward: total was 47.500000. running mean: -14.367516\n",
      "ep 2546: ep_len:526 episode reward: total was -43.390000. running mean: -14.657741\n",
      "ep 2546: ep_len:63 episode reward: total was 28.500000. running mean: -14.226163\n",
      "ep 2546: ep_len:105 episode reward: total was 51.000000. running mean: -13.573902\n",
      "ep 2546: ep_len:1444 episode reward: total was -72.590000. running mean: -14.164063\n",
      "ep 2546: ep_len:610 episode reward: total was 13.160000. running mean: -13.890822\n",
      "ep 2546: ep_len:3391 episode reward: total was -484.850000. running mean: -18.600414\n",
      "ep 2546: ep_len:869 episode reward: total was 71.000000. running mean: -17.704410\n",
      "ep 2546: ep_len:882 episode reward: total was 34.640000. running mean: -17.180966\n",
      "ep 2546: ep_len:165 episode reward: total was 79.500000. running mean: -16.214156\n",
      "ep 2546: ep_len:132 episode reward: total was 60.000000. running mean: -15.452014\n",
      "ep 2546: ep_len:889 episode reward: total was -261.620000. running mean: -17.913694\n",
      "ep 2546: ep_len:2786 episode reward: total was -1.810000. running mean: -17.752657\n",
      "ep 2546: ep_len:64 episode reward: total was 27.500000. running mean: -17.300131\n",
      "epsilon:0.009992 episode_count: 38349. steps_count: 41231749.000000\n",
      "ep 2547: ep_len:900 episode reward: total was -90.580000. running mean: -18.032930\n",
      "ep 2547: ep_len:988 episode reward: total was 33.930000. running mean: -17.513300\n",
      "ep 2547: ep_len:45 episode reward: total was 21.000000. running mean: -17.128167\n",
      "ep 2547: ep_len:3021 episode reward: total was -308.580000. running mean: -20.042686\n",
      "ep 2547: ep_len:570 episode reward: total was 2.930000. running mean: -19.812959\n",
      "ep 2547: ep_len:131 episode reward: total was 64.000000. running mean: -18.974829\n",
      "ep 2547: ep_len:73 episode reward: total was 33.500000. running mean: -18.450081\n",
      "ep 2547: ep_len:66 episode reward: total was 30.000000. running mean: -17.965580\n",
      "ep 2547: ep_len:1427 episode reward: total was -3.460000. running mean: -17.820524\n",
      "ep 2547: ep_len:650 episode reward: total was 25.040000. running mean: -17.391919\n",
      "ep 2547: ep_len:554 episode reward: total was -60.190000. running mean: -17.819900\n",
      "ep 2547: ep_len:7313 episode reward: total was 2.070000. running mean: -17.621001\n",
      "ep 2547: ep_len:933 episode reward: total was 51.720000. running mean: -16.927591\n",
      "ep 2547: ep_len:161 episode reward: total was 62.500000. running mean: -16.133315\n",
      "ep 2547: ep_len:61 episode reward: total was 29.000000. running mean: -15.681982\n",
      "ep 2547: ep_len:1102 episode reward: total was -19.110000. running mean: -15.716262\n",
      "ep 2547: ep_len:2901 episode reward: total was -4.120000. running mean: -15.600299\n",
      "epsilon:0.009992 episode_count: 38366. steps_count: 41252645.000000\n",
      "ep 2548: ep_len:1071 episode reward: total was -7.640000. running mean: -15.520696\n",
      "ep 2548: ep_len:1143 episode reward: total was 1.310000. running mean: -15.352389\n",
      "ep 2548: ep_len:49 episode reward: total was 23.000000. running mean: -14.968865\n",
      "ep 2548: ep_len:3012 episode reward: total was -248.350000. running mean: -17.302677\n",
      "ep 2548: ep_len:670 episode reward: total was -6.600000. running mean: -17.195650\n",
      "ep 2548: ep_len:41 episode reward: total was 19.000000. running mean: -16.833693\n",
      "ep 2548: ep_len:1026 episode reward: total was -36.340000. running mean: -17.028757\n",
      "ep 2548: ep_len:3683 episode reward: total was -2561.060000. running mean: -42.469069\n",
      "ep 2548: ep_len:540 episode reward: total was -57.390000. running mean: -42.618278\n",
      "ep 2548: ep_len:7242 episode reward: total was -21.040000. running mean: -42.402496\n",
      "ep 2548: ep_len:600 episode reward: total was -4.240000. running mean: -42.020871\n",
      "ep 2548: ep_len:41 episode reward: total was 19.000000. running mean: -41.410662\n",
      "ep 2548: ep_len:1456 episode reward: total was -20.500000. running mean: -41.201555\n",
      "ep 2548: ep_len:2897 episode reward: total was 20.880000. running mean: -40.580740\n",
      "epsilon:0.009992 episode_count: 38380. steps_count: 41276116.000000\n",
      "ep 2549: ep_len:1142 episode reward: total was -5.000000. running mean: -40.224932\n",
      "ep 2549: ep_len:947 episode reward: total was 19.190000. running mean: -39.630783\n",
      "ep 2549: ep_len:57 episode reward: total was 27.000000. running mean: -38.964475\n",
      "ep 2549: ep_len:2892 episode reward: total was -245.360000. running mean: -41.028430\n",
      "ep 2549: ep_len:1132 episode reward: total was -20.160000. running mean: -40.819746\n",
      "ep 2549: ep_len:43 episode reward: total was 20.000000. running mean: -40.211549\n",
      "ep 2549: ep_len:118 episode reward: total was 54.500000. running mean: -39.264433\n",
      "ep 2549: ep_len:64 episode reward: total was 30.500000. running mean: -38.566789\n",
      "ep 2549: ep_len:824 episode reward: total was 27.600000. running mean: -37.905121\n",
      "ep 2549: ep_len:614 episode reward: total was -0.640000. running mean: -37.532470\n",
      "ep 2549: ep_len:1264 episode reward: total was -70.350000. running mean: -37.860645\n",
      "ep 2549: ep_len:650 episode reward: total was -7.720000. running mean: -37.559239\n",
      "ep 2549: ep_len:500 episode reward: total was 27.780000. running mean: -36.905846\n",
      "ep 2549: ep_len:99 episode reward: total was 46.500000. running mean: -36.071788\n",
      "ep 2549: ep_len:155 episode reward: total was 74.500000. running mean: -34.966070\n",
      "ep 2549: ep_len:59 episode reward: total was 28.000000. running mean: -34.336409\n",
      "ep 2549: ep_len:928 episode reward: total was -81.060000. running mean: -34.803645\n",
      "ep 2549: ep_len:2763 episode reward: total was -5.750000. running mean: -34.513109\n",
      "ep 2549: ep_len:66 episode reward: total was 30.000000. running mean: -33.867978\n",
      "epsilon:0.009992 episode_count: 38399. steps_count: 41290433.000000\n",
      "ep 2550: ep_len:1149 episode reward: total was -4.840000. running mean: -33.577698\n",
      "ep 2550: ep_len:1198 episode reward: total was -119.600000. running mean: -34.437921\n",
      "ep 2550: ep_len:89 episode reward: total was 41.500000. running mean: -33.678542\n",
      "ep 2550: ep_len:500 episode reward: total was 14.770000. running mean: -33.194056\n",
      "ep 2550: ep_len:153 episode reward: total was 73.500000. running mean: -32.127116\n",
      "ep 2550: ep_len:78 episode reward: total was 37.500000. running mean: -31.430844\n",
      "ep 2550: ep_len:34 episode reward: total was 14.000000. running mean: -30.976536\n",
      "ep 2550: ep_len:500 episode reward: total was 24.990000. running mean: -30.416871\n",
      "ep 2550: ep_len:345 episode reward: total was 22.120000. running mean: -29.891502\n",
      "ep 2550: ep_len:1267 episode reward: total was -97.070000. running mean: -30.563287\n",
      "ep 2550: ep_len:719 episode reward: total was -18.780000. running mean: -30.445454\n",
      "ep 2550: ep_len:1497 episode reward: total was -28.570000. running mean: -30.426699\n",
      "ep 2550: ep_len:95 episode reward: total was 46.000000. running mean: -29.662432\n",
      "ep 2550: ep_len:1410 episode reward: total was 3.250000. running mean: -29.333308\n",
      "ep 2550: ep_len:2806 episode reward: total was -17.130000. running mean: -29.211275\n",
      "epsilon:0.009992 episode_count: 38414. steps_count: 41302273.000000\n",
      "ep 2551: ep_len:620 episode reward: total was -4.770000. running mean: -28.966862\n",
      "ep 2551: ep_len:657 episode reward: total was -16.830000. running mean: -28.845494\n",
      "ep 2551: ep_len:2953 episode reward: total was -172.560000. running mean: -30.282639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2551: ep_len:1115 episode reward: total was -4.170000. running mean: -30.021512\n",
      "ep 2551: ep_len:39 episode reward: total was 18.000000. running mean: -29.541297\n",
      "ep 2551: ep_len:99 episode reward: total was 46.500000. running mean: -28.780884\n",
      "ep 2551: ep_len:59 episode reward: total was 26.500000. running mean: -28.228075\n",
      "ep 2551: ep_len:64 episode reward: total was 30.500000. running mean: -27.640795\n",
      "ep 2551: ep_len:755 episode reward: total was -6.760000. running mean: -27.431987\n",
      "ep 2551: ep_len:330 episode reward: total was 14.560000. running mean: -27.012067\n",
      "ep 2551: ep_len:762 episode reward: total was -39.660000. running mean: -27.138546\n",
      "ep 2551: ep_len:860 episode reward: total was 63.590000. running mean: -26.231261\n",
      "ep 2551: ep_len:740 episode reward: total was -5.900000. running mean: -26.027948\n",
      "ep 2551: ep_len:70 episode reward: total was 33.500000. running mean: -25.432669\n",
      "ep 2551: ep_len:45 episode reward: total was 19.500000. running mean: -24.983342\n",
      "ep 2551: ep_len:1082 episode reward: total was -10.740000. running mean: -24.840909\n",
      "ep 2551: ep_len:2948 episode reward: total was -26.600000. running mean: -24.858499\n",
      "epsilon:0.009992 episode_count: 38431. steps_count: 41315471.000000\n",
      "ep 2552: ep_len:672 episode reward: total was -0.150000. running mean: -24.611414\n",
      "ep 2552: ep_len:786 episode reward: total was -1.100000. running mean: -24.376300\n",
      "ep 2552: ep_len:3044 episode reward: total was -154.080000. running mean: -25.673337\n",
      "ep 2552: ep_len:610 episode reward: total was -14.270000. running mean: -25.559304\n",
      "ep 2552: ep_len:714 episode reward: total was -19.660000. running mean: -25.500311\n",
      "ep 2552: ep_len:366 episode reward: total was 18.290000. running mean: -25.062408\n",
      "ep 2552: ep_len:518 episode reward: total was -0.040000. running mean: -24.812184\n",
      "ep 2552: ep_len:625 episode reward: total was 7.680000. running mean: -24.487262\n",
      "ep 2552: ep_len:620 episode reward: total was 42.640000. running mean: -23.815989\n",
      "ep 2552: ep_len:101 episode reward: total was 49.000000. running mean: -23.087829\n",
      "ep 2552: ep_len:201 episode reward: total was 97.500000. running mean: -21.881951\n",
      "ep 2552: ep_len:58 episode reward: total was 27.500000. running mean: -21.388132\n",
      "ep 2552: ep_len:64 episode reward: total was 30.500000. running mean: -20.869250\n",
      "ep 2552: ep_len:781 episode reward: total was 16.620000. running mean: -20.494358\n",
      "ep 2552: ep_len:2814 episode reward: total was -28.650000. running mean: -20.575914\n",
      "ep 2552: ep_len:40 episode reward: total was 18.500000. running mean: -20.185155\n",
      "epsilon:0.009992 episode_count: 38447. steps_count: 41327485.000000\n",
      "ep 2553: ep_len:500 episode reward: total was 41.770000. running mean: -19.565603\n",
      "ep 2553: ep_len:941 episode reward: total was 1.710000. running mean: -19.352847\n",
      "ep 2553: ep_len:74 episode reward: total was 34.000000. running mean: -18.819319\n",
      "ep 2553: ep_len:3001 episode reward: total was -572.980000. running mean: -24.360926\n",
      "ep 2553: ep_len:555 episode reward: total was -22.770000. running mean: -24.345017\n",
      "ep 2553: ep_len:827 episode reward: total was 34.950000. running mean: -23.752066\n",
      "ep 2553: ep_len:4001 episode reward: total was -292.530000. running mean: -26.439846\n",
      "ep 2553: ep_len:1522 episode reward: total was -86.940000. running mean: -27.044847\n",
      "ep 2553: ep_len:635 episode reward: total was -22.960000. running mean: -27.003999\n",
      "ep 2553: ep_len:1039 episode reward: total was 39.030000. running mean: -26.343659\n",
      "ep 2553: ep_len:35 episode reward: total was 16.000000. running mean: -25.920222\n",
      "ep 2553: ep_len:500 episode reward: total was 47.340000. running mean: -25.187620\n",
      "ep 2553: ep_len:2915 episode reward: total was -5.110000. running mean: -24.986844\n",
      "ep 2553: ep_len:49 episode reward: total was 23.000000. running mean: -24.506975\n",
      "epsilon:0.009992 episode_count: 38461. steps_count: 41344079.000000\n",
      "ep 2554: ep_len:1397 episode reward: total was -6.590000. running mean: -24.327806\n",
      "ep 2554: ep_len:706 episode reward: total was -4.300000. running mean: -24.127528\n",
      "ep 2554: ep_len:37 episode reward: total was 17.000000. running mean: -23.716252\n",
      "ep 2554: ep_len:3008 episode reward: total was -251.270000. running mean: -25.991790\n",
      "ep 2554: ep_len:513 episode reward: total was 12.320000. running mean: -25.608672\n",
      "ep 2554: ep_len:67 episode reward: total was 32.000000. running mean: -25.032585\n",
      "ep 2554: ep_len:97 episode reward: total was 47.000000. running mean: -24.312259\n",
      "ep 2554: ep_len:73 episode reward: total was 33.500000. running mean: -23.734137\n",
      "ep 2554: ep_len:49 episode reward: total was 23.000000. running mean: -23.266795\n",
      "ep 2554: ep_len:1468 episode reward: total was 38.940000. running mean: -22.644727\n",
      "ep 2554: ep_len:3677 episode reward: total was -43.820000. running mean: -22.856480\n",
      "ep 2554: ep_len:1286 episode reward: total was -38.820000. running mean: -23.016115\n",
      "ep 2554: ep_len:667 episode reward: total was 23.560000. running mean: -22.550354\n",
      "ep 2554: ep_len:500 episode reward: total was 10.760000. running mean: -22.217251\n",
      "ep 2554: ep_len:150 episode reward: total was 70.500000. running mean: -21.290078\n",
      "ep 2554: ep_len:500 episode reward: total was 43.480000. running mean: -20.642377\n",
      "ep 2554: ep_len:2795 episode reward: total was -56.540000. running mean: -21.001354\n",
      "epsilon:0.009992 episode_count: 38478. steps_count: 41361069.000000\n",
      "ep 2555: ep_len:697 episode reward: total was -13.150000. running mean: -20.922840\n",
      "ep 2555: ep_len:500 episode reward: total was 12.920000. running mean: -20.584412\n",
      "ep 2555: ep_len:75 episode reward: total was 36.000000. running mean: -20.018567\n",
      "ep 2555: ep_len:2928 episode reward: total was -31.350000. running mean: -20.131882\n",
      "ep 2555: ep_len:699 episode reward: total was 22.730000. running mean: -19.703263\n",
      "ep 2555: ep_len:12 episode reward: total was 4.500000. running mean: -19.461230\n",
      "ep 2555: ep_len:105 episode reward: total was 49.500000. running mean: -18.771618\n",
      "ep 2555: ep_len:635 episode reward: total was -5.580000. running mean: -18.639702\n",
      "ep 2555: ep_len:332 episode reward: total was 19.750000. running mean: -18.255805\n",
      "ep 2555: ep_len:1183 episode reward: total was -63.020000. running mean: -18.703447\n",
      "ep 2555: ep_len:884 episode reward: total was 61.840000. running mean: -17.898012\n",
      "ep 2555: ep_len:717 episode reward: total was -5.610000. running mean: -17.775132\n",
      "ep 2555: ep_len:500 episode reward: total was 22.750000. running mean: -17.369881\n",
      "ep 2555: ep_len:46 episode reward: total was 20.000000. running mean: -16.996182\n",
      "epsilon:0.009992 episode_count: 38492. steps_count: 41370382.000000\n",
      "ep 2556: ep_len:940 episode reward: total was -135.380000. running mean: -18.180020\n",
      "ep 2556: ep_len:763 episode reward: total was -10.720000. running mean: -18.105420\n",
      "ep 2556: ep_len:56 episode reward: total was 26.500000. running mean: -17.659366\n",
      "ep 2556: ep_len:3043 episode reward: total was -25.860000. running mean: -17.741372\n",
      "ep 2556: ep_len:524 episode reward: total was -32.300000. running mean: -17.886958\n",
      "ep 2556: ep_len:48 episode reward: total was 22.500000. running mean: -17.483089\n",
      "ep 2556: ep_len:588 episode reward: total was 47.620000. running mean: -16.832058\n",
      "ep 2556: ep_len:3679 episode reward: total was -53.130000. running mean: -17.195037\n",
      "ep 2556: ep_len:1266 episode reward: total was -50.620000. running mean: -17.529287\n",
      "ep 2556: ep_len:865 episode reward: total was 53.930000. running mean: -16.814694\n",
      "ep 2556: ep_len:881 episode reward: total was 34.050000. running mean: -16.306047\n",
      "ep 2556: ep_len:134 episode reward: total was 64.000000. running mean: -15.502987\n",
      "ep 2556: ep_len:1053 episode reward: total was -10.720000. running mean: -15.455157\n",
      "ep 2556: ep_len:2836 episode reward: total was -22.210000. running mean: -15.522705\n",
      "ep 2556: ep_len:41 episode reward: total was 19.000000. running mean: -15.177478\n",
      "epsilon:0.009992 episode_count: 38507. steps_count: 41387099.000000\n",
      "ep 2557: ep_len:801 episode reward: total was -92.640000. running mean: -15.952103\n",
      "ep 2557: ep_len:500 episode reward: total was 15.060000. running mean: -15.641982\n",
      "ep 2557: ep_len:2960 episode reward: total was -198.680000. running mean: -17.472363\n",
      "ep 2557: ep_len:3899 episode reward: total was -520.210000. running mean: -22.499739\n",
      "ep 2557: ep_len:52 episode reward: total was 23.000000. running mean: -22.044742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2557: ep_len:181 episode reward: total was 89.000000. running mean: -20.934294\n",
      "ep 2557: ep_len:98 episode reward: total was 41.500000. running mean: -20.309951\n",
      "ep 2557: ep_len:1001 episode reward: total was -56.820000. running mean: -20.675052\n",
      "ep 2557: ep_len:308 episode reward: total was 14.370000. running mean: -20.324601\n",
      "ep 2557: ep_len:500 episode reward: total was 29.180000. running mean: -19.829555\n",
      "ep 2557: ep_len:914 episode reward: total was 69.920000. running mean: -18.932060\n",
      "ep 2557: ep_len:1168 episode reward: total was -1.620000. running mean: -18.758939\n",
      "ep 2557: ep_len:138 episode reward: total was 67.500000. running mean: -17.896350\n",
      "ep 2557: ep_len:63 episode reward: total was 30.000000. running mean: -17.417386\n",
      "ep 2557: ep_len:658 episode reward: total was 23.730000. running mean: -17.005912\n",
      "ep 2557: ep_len:2798 episode reward: total was -5.150000. running mean: -16.887353\n",
      "epsilon:0.009992 episode_count: 38523. steps_count: 41403138.000000\n",
      "ep 2558: ep_len:671 episode reward: total was -18.710000. running mean: -16.905580\n",
      "ep 2558: ep_len:637 episode reward: total was 11.060000. running mean: -16.625924\n",
      "ep 2558: ep_len:64 episode reward: total was 29.000000. running mean: -16.169665\n",
      "ep 2558: ep_len:3019 episode reward: total was 1.730000. running mean: -15.990668\n",
      "ep 2558: ep_len:764 episode reward: total was 16.190000. running mean: -15.668861\n",
      "ep 2558: ep_len:65 episode reward: total was 31.000000. running mean: -15.202173\n",
      "ep 2558: ep_len:500 episode reward: total was 8.550000. running mean: -14.964651\n",
      "ep 2558: ep_len:676 episode reward: total was 27.320000. running mean: -14.541804\n",
      "ep 2558: ep_len:641 episode reward: total was -24.060000. running mean: -14.636986\n",
      "ep 2558: ep_len:717 episode reward: total was -0.710000. running mean: -14.497717\n",
      "ep 2558: ep_len:1110 episode reward: total was -10.310000. running mean: -14.455839\n",
      "ep 2558: ep_len:733 episode reward: total was -17.080000. running mean: -14.482081\n",
      "ep 2558: ep_len:2904 episode reward: total was 20.340000. running mean: -14.133860\n",
      "epsilon:0.009992 episode_count: 38536. steps_count: 41415639.000000\n",
      "ep 2559: ep_len:772 episode reward: total was -9.020000. running mean: -14.082722\n",
      "ep 2559: ep_len:945 episode reward: total was 12.400000. running mean: -13.817894\n",
      "ep 2559: ep_len:3063 episode reward: total was 20.160000. running mean: -13.478115\n",
      "ep 2559: ep_len:649 episode reward: total was 0.420000. running mean: -13.339134\n",
      "ep 2559: ep_len:62 episode reward: total was 26.500000. running mean: -12.940743\n",
      "ep 2559: ep_len:661 episode reward: total was 4.940000. running mean: -12.761935\n",
      "ep 2559: ep_len:3673 episode reward: total was -78.200000. running mean: -13.416316\n",
      "ep 2559: ep_len:583 episode reward: total was 19.830000. running mean: -13.083853\n",
      "ep 2559: ep_len:7455 episode reward: total was -832.230000. running mean: -21.275314\n",
      "ep 2559: ep_len:1055 episode reward: total was 27.350000. running mean: -20.789061\n",
      "ep 2559: ep_len:170 episode reward: total was 83.010000. running mean: -19.751071\n",
      "ep 2559: ep_len:79 episode reward: total was 38.000000. running mean: -19.173560\n",
      "ep 2559: ep_len:500 episode reward: total was 3.370000. running mean: -18.948124\n",
      "ep 2559: ep_len:2785 episode reward: total was -9.260000. running mean: -18.851243\n",
      "ep 2559: ep_len:69 episode reward: total was 33.000000. running mean: -18.332731\n",
      "epsilon:0.009992 episode_count: 38551. steps_count: 41438160.000000\n",
      "ep 2560: ep_len:717 episode reward: total was -63.700000. running mean: -18.786403\n",
      "ep 2560: ep_len:1001 episode reward: total was 37.030000. running mean: -18.228239\n",
      "ep 2560: ep_len:59 episode reward: total was 28.000000. running mean: -17.765957\n",
      "ep 2560: ep_len:3076 episode reward: total was -10.750000. running mean: -17.695797\n",
      "ep 2560: ep_len:598 episode reward: total was -14.390000. running mean: -17.662739\n",
      "ep 2560: ep_len:1049 episode reward: total was -24.020000. running mean: -17.726312\n",
      "ep 2560: ep_len:3698 episode reward: total was -1.680000. running mean: -17.565849\n",
      "ep 2560: ep_len:1995 episode reward: total was -128.260000. running mean: -18.672790\n",
      "ep 2560: ep_len:752 episode reward: total was 1.230000. running mean: -18.473763\n",
      "ep 2560: ep_len:540 episode reward: total was 27.870000. running mean: -18.010325\n",
      "ep 2560: ep_len:56 episode reward: total was 26.500000. running mean: -17.565222\n",
      "ep 2560: ep_len:178 episode reward: total was 86.000000. running mean: -16.529569\n",
      "ep 2560: ep_len:1041 episode reward: total was 11.750000. running mean: -16.246774\n",
      "ep 2560: ep_len:2768 episode reward: total was 3.270000. running mean: -16.051606\n",
      "epsilon:0.009992 episode_count: 38565. steps_count: 41455688.000000\n",
      "ep 2561: ep_len:978 episode reward: total was -146.940000. running mean: -17.360490\n",
      "ep 2561: ep_len:682 episode reward: total was -16.780000. running mean: -17.354685\n",
      "ep 2561: ep_len:66 episode reward: total was 30.000000. running mean: -16.881138\n",
      "ep 2561: ep_len:2988 episode reward: total was 9.740000. running mean: -16.614927\n",
      "ep 2561: ep_len:821 episode reward: total was 10.210000. running mean: -16.346678\n",
      "ep 2561: ep_len:1396 episode reward: total was -244.770000. running mean: -18.630911\n",
      "ep 2561: ep_len:647 episode reward: total was 11.570000. running mean: -18.328902\n",
      "ep 2561: ep_len:1986 episode reward: total was -117.730000. running mean: -19.322913\n",
      "ep 2561: ep_len:687 episode reward: total was 6.280000. running mean: -19.066884\n",
      "ep 2561: ep_len:1000 episode reward: total was 9.040000. running mean: -18.785815\n",
      "ep 2561: ep_len:70 episode reward: total was 33.500000. running mean: -18.262957\n",
      "ep 2561: ep_len:83 episode reward: total was 38.500000. running mean: -17.695327\n",
      "ep 2561: ep_len:1491 episode reward: total was -12.530000. running mean: -17.643674\n",
      "ep 2561: ep_len:2832 episode reward: total was 18.240000. running mean: -17.284837\n",
      "epsilon:0.009992 episode_count: 38579. steps_count: 41471415.000000\n",
      "ep 2562: ep_len:1038 episode reward: total was -112.000000. running mean: -18.231989\n",
      "ep 2562: ep_len:1264 episode reward: total was -58.230000. running mean: -18.631969\n",
      "ep 2562: ep_len:2924 episode reward: total was 29.520000. running mean: -18.150449\n",
      "ep 2562: ep_len:1177 episode reward: total was -49.000000. running mean: -18.458945\n",
      "ep 2562: ep_len:57 episode reward: total was 27.000000. running mean: -18.004355\n",
      "ep 2562: ep_len:82 episode reward: total was 39.500000. running mean: -17.429312\n",
      "ep 2562: ep_len:613 episode reward: total was 33.760000. running mean: -16.917418\n",
      "ep 2562: ep_len:3751 episode reward: total was -7.430000. running mean: -16.822544\n",
      "ep 2562: ep_len:679 episode reward: total was -12.620000. running mean: -16.780519\n",
      "ep 2562: ep_len:695 episode reward: total was 27.210000. running mean: -16.340614\n",
      "ep 2562: ep_len:655 episode reward: total was 21.400000. running mean: -15.963207\n",
      "ep 2562: ep_len:43 episode reward: total was 17.000000. running mean: -15.633575\n",
      "ep 2562: ep_len:788 episode reward: total was -33.320000. running mean: -15.810440\n",
      "ep 2562: ep_len:2842 episode reward: total was 5.150000. running mean: -15.600835\n",
      "epsilon:0.009992 episode_count: 38593. steps_count: 41488023.000000\n",
      "ep 2563: ep_len:697 episode reward: total was 25.060000. running mean: -15.194227\n",
      "ep 2563: ep_len:960 episode reward: total was 26.510000. running mean: -14.777185\n",
      "ep 2563: ep_len:3079 episode reward: total was 2.050000. running mean: -14.608913\n",
      "ep 2563: ep_len:706 episode reward: total was 26.770000. running mean: -14.195124\n",
      "ep 2563: ep_len:116 episode reward: total was 52.000000. running mean: -13.533172\n",
      "ep 2563: ep_len:66 episode reward: total was 31.500000. running mean: -13.082841\n",
      "ep 2563: ep_len:541 episode reward: total was 40.810000. running mean: -12.543912\n",
      "ep 2563: ep_len:3651 episode reward: total was -6.190000. running mean: -12.480373\n",
      "ep 2563: ep_len:4255 episode reward: total was -419.490000. running mean: -16.550469\n",
      "ep 2563: ep_len:617 episode reward: total was -3.490000. running mean: -16.419865\n",
      "ep 2563: ep_len:1102 episode reward: total was 34.090000. running mean: -15.914766\n",
      "ep 2563: ep_len:80 episode reward: total was 35.500000. running mean: -15.400618\n",
      "ep 2563: ep_len:43 episode reward: total was 18.500000. running mean: -15.061612\n",
      "ep 2563: ep_len:838 episode reward: total was -17.420000. running mean: -15.085196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2563: ep_len:2818 episode reward: total was -5.010000. running mean: -14.984444\n",
      "ep 2563: ep_len:72 episode reward: total was 34.500000. running mean: -14.489600\n",
      "epsilon:0.009992 episode_count: 38609. steps_count: 41507664.000000\n",
      "ep 2564: ep_len:625 episode reward: total was 20.980000. running mean: -14.134904\n",
      "ep 2564: ep_len:999 episode reward: total was 8.480000. running mean: -13.908755\n",
      "ep 2564: ep_len:71 episode reward: total was 32.500000. running mean: -13.444667\n",
      "ep 2564: ep_len:2962 episode reward: total was -13.130000. running mean: -13.441520\n",
      "ep 2564: ep_len:663 episode reward: total was 18.640000. running mean: -13.120705\n",
      "ep 2564: ep_len:65 episode reward: total was 31.000000. running mean: -12.679498\n",
      "ep 2564: ep_len:42 episode reward: total was 18.000000. running mean: -12.372703\n",
      "ep 2564: ep_len:4334 episode reward: total was -742.610000. running mean: -19.675076\n",
      "ep 2564: ep_len:346 episode reward: total was 20.630000. running mean: -19.272025\n",
      "ep 2564: ep_len:3957 episode reward: total was -383.760000. running mean: -22.916905\n",
      "ep 2564: ep_len:823 episode reward: total was 33.910000. running mean: -22.348636\n",
      "ep 2564: ep_len:659 episode reward: total was 12.830000. running mean: -21.996850\n",
      "ep 2564: ep_len:66 episode reward: total was 30.000000. running mean: -21.476881\n",
      "ep 2564: ep_len:500 episode reward: total was 3.830000. running mean: -21.223812\n",
      "ep 2564: ep_len:2789 episode reward: total was -15.160000. running mean: -21.163174\n",
      "epsilon:0.009992 episode_count: 38624. steps_count: 41526565.000000\n",
      "ep 2565: ep_len:618 episode reward: total was 13.120000. running mean: -20.820343\n",
      "ep 2565: ep_len:3128 episode reward: total was -745.490000. running mean: -28.067039\n",
      "ep 2565: ep_len:55 episode reward: total was 26.000000. running mean: -27.526369\n",
      "ep 2565: ep_len:74 episode reward: total was 34.000000. running mean: -26.911105\n",
      "ep 2565: ep_len:644 episode reward: total was -5.760000. running mean: -26.699594\n",
      "ep 2565: ep_len:24 episode reward: total was 10.500000. running mean: -26.327598\n",
      "ep 2565: ep_len:642 episode reward: total was 2.790000. running mean: -26.036422\n",
      "ep 2565: ep_len:3580 episode reward: total was -54.280000. running mean: -26.318858\n",
      "ep 2565: ep_len:561 episode reward: total was -36.610000. running mean: -26.421769\n",
      "ep 2565: ep_len:7298 episode reward: total was -49.100000. running mean: -26.648552\n",
      "ep 2565: ep_len:1463 episode reward: total was -44.000000. running mean: -26.822066\n",
      "ep 2565: ep_len:212 episode reward: total was 104.010000. running mean: -25.513745\n",
      "ep 2565: ep_len:68 episode reward: total was 32.500000. running mean: -24.933608\n",
      "ep 2565: ep_len:1506 episode reward: total was -14.950000. running mean: -24.833772\n",
      "ep 2565: ep_len:2773 episode reward: total was -8.160000. running mean: -24.667034\n",
      "ep 2565: ep_len:71 episode reward: total was 34.000000. running mean: -24.080364\n",
      "epsilon:0.009992 episode_count: 38640. steps_count: 41549282.000000\n",
      "ep 2566: ep_len:1126 episode reward: total was -22.630000. running mean: -24.065860\n",
      "ep 2566: ep_len:1645 episode reward: total was -103.910000. running mean: -24.864302\n",
      "ep 2566: ep_len:67 episode reward: total was 32.000000. running mean: -24.295659\n",
      "ep 2566: ep_len:2945 episode reward: total was -335.220000. running mean: -27.404902\n",
      "ep 2566: ep_len:681 episode reward: total was -24.670000. running mean: -27.377553\n",
      "ep 2566: ep_len:95 episode reward: total was 44.500000. running mean: -26.658777\n",
      "ep 2566: ep_len:59 episode reward: total was 28.000000. running mean: -26.112190\n",
      "ep 2566: ep_len:787 episode reward: total was 3.470000. running mean: -25.816368\n",
      "ep 2566: ep_len:3487 episode reward: total was -9.360000. running mean: -25.651804\n",
      "ep 2566: ep_len:781 episode reward: total was -37.090000. running mean: -25.766186\n",
      "ep 2566: ep_len:814 episode reward: total was 37.620000. running mean: -25.132324\n",
      "ep 2566: ep_len:563 episode reward: total was 17.110000. running mean: -24.709901\n",
      "ep 2566: ep_len:76 episode reward: total was 35.000000. running mean: -24.112802\n",
      "ep 2566: ep_len:625 episode reward: total was -6.110000. running mean: -23.932774\n",
      "ep 2566: ep_len:2778 episode reward: total was 4.840000. running mean: -23.645046\n",
      "ep 2566: ep_len:51 episode reward: total was 22.500000. running mean: -23.183596\n",
      "epsilon:0.009992 episode_count: 38656. steps_count: 41565862.000000\n",
      "ep 2567: ep_len:1108 episode reward: total was -8.280000. running mean: -23.034560\n",
      "ep 2567: ep_len:956 episode reward: total was -3.190000. running mean: -22.836114\n",
      "ep 2567: ep_len:43 episode reward: total was 20.000000. running mean: -22.407753\n",
      "ep 2567: ep_len:2947 episode reward: total was -230.440000. running mean: -24.488075\n",
      "ep 2567: ep_len:585 episode reward: total was 12.950000. running mean: -24.113695\n",
      "ep 2567: ep_len:58 episode reward: total was 27.500000. running mean: -23.597558\n",
      "ep 2567: ep_len:675 episode reward: total was 6.770000. running mean: -23.293882\n",
      "ep 2567: ep_len:3774 episode reward: total was 6.170000. running mean: -22.999243\n",
      "ep 2567: ep_len:564 episode reward: total was -31.900000. running mean: -23.088251\n",
      "ep 2567: ep_len:826 episode reward: total was 28.590000. running mean: -22.571468\n",
      "ep 2567: ep_len:1055 episode reward: total was -1.240000. running mean: -22.358154\n",
      "ep 2567: ep_len:75 episode reward: total was 36.000000. running mean: -21.774572\n",
      "ep 2567: ep_len:1164 episode reward: total was -22.840000. running mean: -21.785227\n",
      "ep 2567: ep_len:47 episode reward: total was 22.000000. running mean: -21.347374\n",
      "ep 2567: ep_len:65 episode reward: total was 31.000000. running mean: -20.823900\n",
      "epsilon:0.009992 episode_count: 38671. steps_count: 41579804.000000\n",
      "ep 2568: ep_len:711 episode reward: total was -33.020000. running mean: -20.945861\n",
      "ep 2568: ep_len:1536 episode reward: total was -61.850000. running mean: -21.354903\n",
      "ep 2568: ep_len:56 episode reward: total was 26.500000. running mean: -20.876354\n",
      "ep 2568: ep_len:2969 episode reward: total was -224.740000. running mean: -22.914990\n",
      "ep 2568: ep_len:500 episode reward: total was 12.710000. running mean: -22.558740\n",
      "ep 2568: ep_len:110 episode reward: total was 53.500000. running mean: -21.798153\n",
      "ep 2568: ep_len:606 episode reward: total was 34.820000. running mean: -21.231971\n",
      "ep 2568: ep_len:654 episode reward: total was 28.140000. running mean: -20.738252\n",
      "ep 2568: ep_len:629 episode reward: total was -23.170000. running mean: -20.762569\n",
      "ep 2568: ep_len:778 episode reward: total was 3.240000. running mean: -20.522544\n",
      "ep 2568: ep_len:526 episode reward: total was -7.210000. running mean: -20.389418\n",
      "ep 2568: ep_len:66 episode reward: total was 31.500000. running mean: -19.870524\n",
      "ep 2568: ep_len:3200 episode reward: total was -206.010000. running mean: -21.731919\n",
      "ep 2568: ep_len:2906 episode reward: total was -34.160000. running mean: -21.856200\n",
      "ep 2568: ep_len:62 episode reward: total was 26.500000. running mean: -21.372638\n",
      "epsilon:0.009992 episode_count: 38686. steps_count: 41595113.000000\n",
      "ep 2569: ep_len:722 episode reward: total was -59.610000. running mean: -21.755011\n",
      "ep 2569: ep_len:1658 episode reward: total was -59.270000. running mean: -22.130161\n",
      "ep 2569: ep_len:45 episode reward: total was 19.500000. running mean: -21.713859\n",
      "ep 2569: ep_len:2926 episode reward: total was -282.390000. running mean: -24.320621\n",
      "ep 2569: ep_len:602 episode reward: total was 4.840000. running mean: -24.029015\n",
      "ep 2569: ep_len:37 episode reward: total was 15.500000. running mean: -23.633724\n",
      "ep 2569: ep_len:165 episode reward: total was 76.500000. running mean: -22.632387\n",
      "ep 2569: ep_len:94 episode reward: total was 45.500000. running mean: -21.951063\n",
      "ep 2569: ep_len:1428 episode reward: total was -103.770000. running mean: -22.769253\n",
      "ep 2569: ep_len:3996 episode reward: total was -101.230000. running mean: -23.553860\n",
      "ep 2569: ep_len:1542 episode reward: total was -37.490000. running mean: -23.693222\n",
      "ep 2569: ep_len:755 episode reward: total was -2.870000. running mean: -23.484989\n",
      "ep 2569: ep_len:670 episode reward: total was -13.250000. running mean: -23.382639\n",
      "ep 2569: ep_len:1433 episode reward: total was 2.500000. running mean: -23.123813\n",
      "ep 2569: ep_len:2757 episode reward: total was -6.910000. running mean: -22.961675\n",
      "epsilon:0.009992 episode_count: 38701. steps_count: 41613943.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2570: ep_len:1469 episode reward: total was 33.990000. running mean: -22.392158\n",
      "ep 2570: ep_len:749 episode reward: total was -22.100000. running mean: -22.389237\n",
      "ep 2570: ep_len:3081 episode reward: total was -274.240000. running mean: -24.907744\n",
      "ep 2570: ep_len:586 episode reward: total was -1.870000. running mean: -24.677367\n",
      "ep 2570: ep_len:44 episode reward: total was 20.500000. running mean: -24.225593\n",
      "ep 2570: ep_len:85 episode reward: total was 38.000000. running mean: -23.603337\n",
      "ep 2570: ep_len:615 episode reward: total was 45.690000. running mean: -22.910404\n",
      "ep 2570: ep_len:3728 episode reward: total was -1489.000000. running mean: -37.571300\n",
      "ep 2570: ep_len:2909 episode reward: total was -789.910000. running mean: -45.094687\n",
      "ep 2570: ep_len:789 episode reward: total was 42.850000. running mean: -44.215240\n",
      "ep 2570: ep_len:878 episode reward: total was -4.240000. running mean: -43.815488\n",
      "ep 2570: ep_len:93 episode reward: total was 45.000000. running mean: -42.927333\n",
      "ep 2570: ep_len:1456 episode reward: total was 15.130000. running mean: -42.346759\n",
      "ep 2570: ep_len:2827 episode reward: total was -19.950000. running mean: -42.122792\n",
      "ep 2570: ep_len:70 episode reward: total was 32.000000. running mean: -41.381564\n",
      "epsilon:0.009992 episode_count: 38716. steps_count: 41633322.000000\n",
      "ep 2571: ep_len:747 episode reward: total was -54.310000. running mean: -41.510848\n",
      "ep 2571: ep_len:780 episode reward: total was -2.240000. running mean: -41.118140\n",
      "ep 2571: ep_len:47 episode reward: total was 22.000000. running mean: -40.486958\n",
      "ep 2571: ep_len:2979 episode reward: total was -298.840000. running mean: -43.070489\n",
      "ep 2571: ep_len:650 episode reward: total was -1.300000. running mean: -42.652784\n",
      "ep 2571: ep_len:46 episode reward: total was 20.000000. running mean: -42.026256\n",
      "ep 2571: ep_len:76 episode reward: total was 35.000000. running mean: -41.255993\n",
      "ep 2571: ep_len:62 episode reward: total was 29.500000. running mean: -40.548434\n",
      "ep 2571: ep_len:1410 episode reward: total was -7.280000. running mean: -40.215749\n",
      "ep 2571: ep_len:652 episode reward: total was 32.160000. running mean: -39.491992\n",
      "ep 2571: ep_len:963 episode reward: total was -46.450000. running mean: -39.561572\n",
      "ep 2571: ep_len:7291 episode reward: total was 36.470000. running mean: -38.801256\n",
      "ep 2571: ep_len:500 episode reward: total was 30.410000. running mean: -38.109143\n",
      "ep 2571: ep_len:670 episode reward: total was 6.230000. running mean: -37.665752\n",
      "ep 2571: ep_len:2843 episode reward: total was -3.410000. running mean: -37.323195\n",
      "ep 2571: ep_len:48 episode reward: total was 21.000000. running mean: -36.739963\n",
      "epsilon:0.009992 episode_count: 38732. steps_count: 41653086.000000\n",
      "ep 2572: ep_len:1472 episode reward: total was 16.450000. running mean: -36.208063\n",
      "ep 2572: ep_len:670 episode reward: total was -33.870000. running mean: -36.184682\n",
      "ep 2572: ep_len:72 episode reward: total was 34.500000. running mean: -35.477836\n",
      "ep 2572: ep_len:2846 episode reward: total was -275.110000. running mean: -37.874157\n",
      "ep 2572: ep_len:572 episode reward: total was -9.130000. running mean: -37.586716\n",
      "ep 2572: ep_len:73 episode reward: total was 33.500000. running mean: -36.875848\n",
      "ep 2572: ep_len:1813 episode reward: total was 8.530000. running mean: -36.421790\n",
      "ep 2572: ep_len:3804 episode reward: total was -143.200000. running mean: -37.489572\n",
      "ep 2572: ep_len:1607 episode reward: total was -95.090000. running mean: -38.065576\n",
      "ep 2572: ep_len:727 episode reward: total was -10.010000. running mean: -37.785021\n",
      "ep 2572: ep_len:1066 episode reward: total was 41.440000. running mean: -36.992770\n",
      "ep 2572: ep_len:99 episode reward: total was 48.000000. running mean: -36.142843\n",
      "ep 2572: ep_len:148 episode reward: total was 72.500000. running mean: -35.056414\n",
      "ep 2572: ep_len:37 episode reward: total was 17.000000. running mean: -34.535850\n",
      "ep 2572: ep_len:111 episode reward: total was 52.500000. running mean: -33.665492\n",
      "ep 2572: ep_len:795 episode reward: total was -23.700000. running mean: -33.565837\n",
      "ep 2572: ep_len:2823 episode reward: total was -33.460000. running mean: -33.564778\n",
      "epsilon:0.009992 episode_count: 38749. steps_count: 41671821.000000\n",
      "ep 2573: ep_len:805 episode reward: total was 27.470000. running mean: -32.954431\n",
      "ep 2573: ep_len:992 episode reward: total was 11.260000. running mean: -32.512286\n",
      "ep 2573: ep_len:2964 episode reward: total was -270.520000. running mean: -34.892363\n",
      "ep 2573: ep_len:1088 episode reward: total was -22.620000. running mean: -34.769640\n",
      "ep 2573: ep_len:96 episode reward: total was 46.500000. running mean: -33.956943\n",
      "ep 2573: ep_len:93 episode reward: total was 45.000000. running mean: -33.167374\n",
      "ep 2573: ep_len:52 episode reward: total was 23.000000. running mean: -32.605700\n",
      "ep 2573: ep_len:761 episode reward: total was -7.710000. running mean: -32.356743\n",
      "ep 2573: ep_len:669 episode reward: total was 25.960000. running mean: -31.773576\n",
      "ep 2573: ep_len:633 episode reward: total was -13.950000. running mean: -31.595340\n",
      "ep 2573: ep_len:840 episode reward: total was 22.910000. running mean: -31.050287\n",
      "ep 2573: ep_len:1131 episode reward: total was -24.210000. running mean: -30.981884\n",
      "ep 2573: ep_len:70 episode reward: total was 33.500000. running mean: -30.337065\n",
      "ep 2573: ep_len:39 episode reward: total was 18.000000. running mean: -29.853694\n",
      "ep 2573: ep_len:1501 episode reward: total was 6.950000. running mean: -29.485657\n",
      "ep 2573: ep_len:2824 episode reward: total was 3.460000. running mean: -29.156201\n",
      "ep 2573: ep_len:54 episode reward: total was 24.000000. running mean: -28.624639\n",
      "epsilon:0.009992 episode_count: 38766. steps_count: 41686433.000000\n",
      "ep 2574: ep_len:1063 episode reward: total was -11.760000. running mean: -28.455992\n",
      "ep 2574: ep_len:691 episode reward: total was -41.970000. running mean: -28.591132\n",
      "ep 2574: ep_len:66 episode reward: total was 31.500000. running mean: -27.990221\n",
      "ep 2574: ep_len:92 episode reward: total was 44.500000. running mean: -27.265319\n",
      "ep 2574: ep_len:684 episode reward: total was 2.630000. running mean: -26.966366\n",
      "ep 2574: ep_len:38 episode reward: total was 16.000000. running mean: -26.536702\n",
      "ep 2574: ep_len:87 episode reward: total was 42.000000. running mean: -25.851335\n",
      "ep 2574: ep_len:52 episode reward: total was 24.500000. running mean: -25.347822\n",
      "ep 2574: ep_len:765 episode reward: total was -13.730000. running mean: -25.231643\n",
      "ep 2574: ep_len:313 episode reward: total was 3.620000. running mean: -24.943127\n",
      "ep 2574: ep_len:1166 episode reward: total was -53.180000. running mean: -25.225496\n",
      "ep 2574: ep_len:755 episode reward: total was -5.870000. running mean: -25.031941\n",
      "ep 2574: ep_len:1002 episode reward: total was 10.130000. running mean: -24.680321\n",
      "ep 2574: ep_len:1077 episode reward: total was -13.120000. running mean: -24.564718\n",
      "ep 2574: ep_len:2857 episode reward: total was -6.180000. running mean: -24.380871\n",
      "epsilon:0.009992 episode_count: 38781. steps_count: 41697141.000000\n",
      "ep 2575: ep_len:1469 episode reward: total was 11.890000. running mean: -24.018162\n",
      "ep 2575: ep_len:974 episode reward: total was 3.880000. running mean: -23.739181\n",
      "ep 2575: ep_len:2967 episode reward: total was -282.310000. running mean: -26.324889\n",
      "ep 2575: ep_len:815 episode reward: total was 34.710000. running mean: -25.714540\n",
      "ep 2575: ep_len:130 episode reward: total was 60.500000. running mean: -24.852395\n",
      "ep 2575: ep_len:80 episode reward: total was 37.000000. running mean: -24.233871\n",
      "ep 2575: ep_len:54 episode reward: total was 22.500000. running mean: -23.766532\n",
      "ep 2575: ep_len:1007 episode reward: total was -63.690000. running mean: -24.165767\n",
      "ep 2575: ep_len:674 episode reward: total was 18.760000. running mean: -23.736509\n",
      "ep 2575: ep_len:804 episode reward: total was -39.120000. running mean: -23.890344\n",
      "ep 2575: ep_len:7302 episode reward: total was -51.470000. running mean: -24.166140\n",
      "ep 2575: ep_len:668 episode reward: total was -25.330000. running mean: -24.177779\n",
      "ep 2575: ep_len:68 episode reward: total was 32.500000. running mean: -23.611001\n",
      "ep 2575: ep_len:39 episode reward: total was 18.000000. running mean: -23.194891\n",
      "ep 2575: ep_len:787 episode reward: total was -8.210000. running mean: -23.045042\n",
      "ep 2575: ep_len:2846 episode reward: total was -5.710000. running mean: -22.871692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2575: ep_len:57 episode reward: total was 25.500000. running mean: -22.387975\n",
      "epsilon:0.009992 episode_count: 38798. steps_count: 41717882.000000\n",
      "ep 2576: ep_len:677 episode reward: total was -31.320000. running mean: -22.477295\n",
      "ep 2576: ep_len:701 episode reward: total was -58.810000. running mean: -22.840622\n",
      "ep 2576: ep_len:66 episode reward: total was 31.500000. running mean: -22.297216\n",
      "ep 2576: ep_len:3059 episode reward: total was -269.170000. running mean: -24.765944\n",
      "ep 2576: ep_len:4519 episode reward: total was -1867.980000. running mean: -43.198084\n",
      "ep 2576: ep_len:65 episode reward: total was 31.000000. running mean: -42.456104\n",
      "ep 2576: ep_len:82 episode reward: total was 39.500000. running mean: -41.636542\n",
      "ep 2576: ep_len:70 episode reward: total was 33.500000. running mean: -40.885177\n",
      "ep 2576: ep_len:1024 episode reward: total was -18.880000. running mean: -40.665125\n",
      "ep 2576: ep_len:3640 episode reward: total was -489.600000. running mean: -45.154474\n",
      "ep 2576: ep_len:1169 episode reward: total was -41.000000. running mean: -45.112929\n",
      "ep 2576: ep_len:887 episode reward: total was 46.020000. running mean: -44.201600\n",
      "ep 2576: ep_len:686 episode reward: total was 11.780000. running mean: -43.641784\n",
      "ep 2576: ep_len:56 episode reward: total was 25.000000. running mean: -42.955366\n",
      "ep 2576: ep_len:106 episode reward: total was 51.500000. running mean: -42.010813\n",
      "ep 2576: ep_len:500 episode reward: total was -5.190000. running mean: -41.642604\n",
      "ep 2576: ep_len:2865 episode reward: total was -13.350000. running mean: -41.359678\n",
      "epsilon:0.009992 episode_count: 38815. steps_count: 41738054.000000\n",
      "ep 2577: ep_len:963 episode reward: total was -43.800000. running mean: -41.384082\n",
      "ep 2577: ep_len:1148 episode reward: total was -6.870000. running mean: -41.038941\n",
      "ep 2577: ep_len:2889 episode reward: total was -264.250000. running mean: -43.271051\n",
      "ep 2577: ep_len:508 episode reward: total was -20.340000. running mean: -43.041741\n",
      "ep 2577: ep_len:41 episode reward: total was 17.500000. running mean: -42.436323\n",
      "ep 2577: ep_len:1447 episode reward: total was 1.720000. running mean: -41.994760\n",
      "ep 2577: ep_len:3641 episode reward: total was -79.250000. running mean: -42.367313\n",
      "ep 2577: ep_len:564 episode reward: total was -17.760000. running mean: -42.121239\n",
      "ep 2577: ep_len:696 episode reward: total was 6.860000. running mean: -41.631427\n",
      "ep 2577: ep_len:648 episode reward: total was 16.650000. running mean: -41.048613\n",
      "ep 2577: ep_len:628 episode reward: total was -5.890000. running mean: -40.697027\n",
      "ep 2577: ep_len:2769 episode reward: total was 0.500000. running mean: -40.285056\n",
      "ep 2577: ep_len:51 episode reward: total was 24.000000. running mean: -39.642206\n",
      "epsilon:0.009992 episode_count: 38828. steps_count: 41754047.000000\n",
      "ep 2578: ep_len:646 episode reward: total was -16.940000. running mean: -39.415184\n",
      "ep 2578: ep_len:1282 episode reward: total was -134.810000. running mean: -40.369132\n",
      "ep 2578: ep_len:77 episode reward: total was 35.500000. running mean: -39.610441\n",
      "ep 2578: ep_len:3047 episode reward: total was -281.010000. running mean: -42.024436\n",
      "ep 2578: ep_len:4430 episode reward: total was -1049.010000. running mean: -52.094292\n",
      "ep 2578: ep_len:51 episode reward: total was 22.500000. running mean: -51.348349\n",
      "ep 2578: ep_len:74 episode reward: total was 34.000000. running mean: -50.494865\n",
      "ep 2578: ep_len:825 episode reward: total was 42.270000. running mean: -49.567217\n",
      "ep 2578: ep_len:3556 episode reward: total was -26.330000. running mean: -49.334845\n",
      "ep 2578: ep_len:1139 episode reward: total was -13.080000. running mean: -48.972296\n",
      "ep 2578: ep_len:678 episode reward: total was 14.820000. running mean: -48.334373\n",
      "ep 2578: ep_len:1485 episode reward: total was 4.330000. running mean: -47.807729\n",
      "ep 2578: ep_len:66 episode reward: total was 31.500000. running mean: -47.014652\n",
      "ep 2578: ep_len:78 episode reward: total was 36.000000. running mean: -46.184506\n",
      "ep 2578: ep_len:1118 episode reward: total was -0.240000. running mean: -45.725061\n",
      "ep 2578: ep_len:2872 episode reward: total was -36.580000. running mean: -45.633610\n",
      "epsilon:0.009992 episode_count: 38844. steps_count: 41775471.000000\n",
      "ep 2579: ep_len:616 episode reward: total was -6.130000. running mean: -45.238574\n",
      "ep 2579: ep_len:1272 episode reward: total was -81.870000. running mean: -45.604888\n",
      "ep 2579: ep_len:3048 episode reward: total was -299.890000. running mean: -48.147739\n",
      "ep 2579: ep_len:717 episode reward: total was -25.390000. running mean: -47.920162\n",
      "ep 2579: ep_len:48 episode reward: total was 22.500000. running mean: -47.215960\n",
      "ep 2579: ep_len:110 episode reward: total was 52.000000. running mean: -46.223801\n",
      "ep 2579: ep_len:68 episode reward: total was 31.000000. running mean: -45.451563\n",
      "ep 2579: ep_len:1508 episode reward: total was 6.950000. running mean: -44.927547\n",
      "ep 2579: ep_len:672 episode reward: total was 28.720000. running mean: -44.191072\n",
      "ep 2579: ep_len:686 episode reward: total was 2.650000. running mean: -43.722661\n",
      "ep 2579: ep_len:782 episode reward: total was 37.270000. running mean: -42.912734\n",
      "ep 2579: ep_len:616 episode reward: total was 3.970000. running mean: -42.443907\n",
      "ep 2579: ep_len:41 episode reward: total was 19.000000. running mean: -41.829468\n",
      "ep 2579: ep_len:1410 episode reward: total was -3.300000. running mean: -41.444173\n",
      "ep 2579: ep_len:2800 episode reward: total was -18.320000. running mean: -41.212931\n",
      "ep 2579: ep_len:57 episode reward: total was 27.000000. running mean: -40.530802\n",
      "epsilon:0.009992 episode_count: 38860. steps_count: 41789922.000000\n",
      "ep 2580: ep_len:993 episode reward: total was -135.420000. running mean: -41.479694\n",
      "ep 2580: ep_len:981 episode reward: total was 25.560000. running mean: -40.809297\n",
      "ep 2580: ep_len:2964 episode reward: total was -249.670000. running mean: -42.897904\n",
      "ep 2580: ep_len:1674 episode reward: total was -57.280000. running mean: -43.041725\n",
      "ep 2580: ep_len:118 episode reward: total was 56.000000. running mean: -42.051308\n",
      "ep 2580: ep_len:1082 episode reward: total was -6.000000. running mean: -41.690795\n",
      "ep 2580: ep_len:628 episode reward: total was 17.380000. running mean: -41.100087\n",
      "ep 2580: ep_len:735 episode reward: total was -43.940000. running mean: -41.128486\n",
      "ep 2580: ep_len:833 episode reward: total was 67.240000. running mean: -40.044801\n",
      "ep 2580: ep_len:594 episode reward: total was -16.910000. running mean: -39.813453\n",
      "ep 2580: ep_len:91 episode reward: total was 42.500000. running mean: -38.990319\n",
      "ep 2580: ep_len:155 episode reward: total was 76.000000. running mean: -37.840415\n",
      "ep 2580: ep_len:91 episode reward: total was 42.500000. running mean: -37.037011\n",
      "ep 2580: ep_len:2620 episode reward: total was -465.690000. running mean: -41.323541\n",
      "ep 2580: ep_len:2920 episode reward: total was -2.180000. running mean: -40.932106\n",
      "epsilon:0.009992 episode_count: 38875. steps_count: 41806401.000000\n",
      "ep 2581: ep_len:1131 episode reward: total was -14.600000. running mean: -40.668785\n",
      "ep 2581: ep_len:729 episode reward: total was -3.990000. running mean: -40.301997\n",
      "ep 2581: ep_len:72 episode reward: total was 34.500000. running mean: -39.553977\n",
      "ep 2581: ep_len:3031 episode reward: total was -187.730000. running mean: -41.035737\n",
      "ep 2581: ep_len:745 episode reward: total was -18.400000. running mean: -40.809380\n",
      "ep 2581: ep_len:30 episode reward: total was 13.500000. running mean: -40.266286\n",
      "ep 2581: ep_len:63 episode reward: total was 30.000000. running mean: -39.563623\n",
      "ep 2581: ep_len:52 episode reward: total was 24.500000. running mean: -38.922987\n",
      "ep 2581: ep_len:588 episode reward: total was 47.440000. running mean: -38.059357\n",
      "ep 2581: ep_len:636 episode reward: total was 22.910000. running mean: -37.449663\n",
      "ep 2581: ep_len:646 episode reward: total was -21.770000. running mean: -37.292867\n",
      "ep 2581: ep_len:677 episode reward: total was 15.210000. running mean: -36.767838\n",
      "ep 2581: ep_len:500 episode reward: total was 12.290000. running mean: -36.277260\n",
      "ep 2581: ep_len:69 episode reward: total was 33.000000. running mean: -35.584487\n",
      "ep 2581: ep_len:739 episode reward: total was -41.660000. running mean: -35.645242\n",
      "ep 2581: ep_len:2837 episode reward: total was -11.740000. running mean: -35.406190\n",
      "ep 2581: ep_len:65 episode reward: total was 31.000000. running mean: -34.742128\n",
      "epsilon:0.009992 episode_count: 38892. steps_count: 41819011.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2582: ep_len:1123 episode reward: total was -6.110000. running mean: -34.455807\n",
      "ep 2582: ep_len:216 episode reward: total was 0.900000. running mean: -34.102249\n",
      "ep 2582: ep_len:3044 episode reward: total was -178.580000. running mean: -35.547026\n",
      "ep 2582: ep_len:3725 episode reward: total was -1162.410000. running mean: -46.815656\n",
      "ep 2582: ep_len:54 episode reward: total was 25.500000. running mean: -46.092499\n",
      "ep 2582: ep_len:501 episode reward: total was 44.720000. running mean: -45.184374\n",
      "ep 2582: ep_len:351 episode reward: total was 21.690000. running mean: -44.515631\n",
      "ep 2582: ep_len:588 episode reward: total was -37.720000. running mean: -44.447674\n",
      "ep 2582: ep_len:7252 episode reward: total was -28.040000. running mean: -44.283597\n",
      "ep 2582: ep_len:754 episode reward: total was -12.830000. running mean: -43.969062\n",
      "ep 2582: ep_len:76 episode reward: total was 36.500000. running mean: -43.164371\n",
      "ep 2582: ep_len:152 episode reward: total was 73.000000. running mean: -42.002727\n",
      "ep 2582: ep_len:701 episode reward: total was -60.830000. running mean: -42.191000\n",
      "ep 2582: ep_len:2762 episode reward: total was -10.680000. running mean: -41.875890\n",
      "epsilon:0.009992 episode_count: 38906. steps_count: 41840310.000000\n",
      "ep 2583: ep_len:1502 episode reward: total was -147.570000. running mean: -42.932831\n",
      "ep 2583: ep_len:979 episode reward: total was 24.320000. running mean: -42.260303\n",
      "ep 2583: ep_len:2974 episode reward: total was -42.020000. running mean: -42.257900\n",
      "ep 2583: ep_len:905 episode reward: total was 75.050000. running mean: -41.084821\n",
      "ep 2583: ep_len:90 episode reward: total was 43.500000. running mean: -40.238972\n",
      "ep 2583: ep_len:44 episode reward: total was 20.500000. running mean: -39.631583\n",
      "ep 2583: ep_len:1534 episode reward: total was -213.190000. running mean: -41.367167\n",
      "ep 2583: ep_len:326 episode reward: total was 19.910000. running mean: -40.754395\n",
      "ep 2583: ep_len:753 episode reward: total was -29.840000. running mean: -40.645251\n",
      "ep 2583: ep_len:653 episode reward: total was 12.430000. running mean: -40.114499\n",
      "ep 2583: ep_len:801 episode reward: total was 16.830000. running mean: -39.545054\n",
      "ep 2583: ep_len:79 episode reward: total was 36.500000. running mean: -38.784603\n",
      "ep 2583: ep_len:59 episode reward: total was 28.000000. running mean: -38.116757\n",
      "ep 2583: ep_len:612 episode reward: total was 5.290000. running mean: -37.682690\n",
      "ep 2583: ep_len:2853 episode reward: total was -17.510000. running mean: -37.480963\n",
      "ep 2583: ep_len:71 episode reward: total was 34.000000. running mean: -36.766153\n",
      "epsilon:0.009992 episode_count: 38922. steps_count: 41854545.000000\n",
      "ep 2584: ep_len:675 episode reward: total was -81.200000. running mean: -37.210492\n",
      "ep 2584: ep_len:1562 episode reward: total was -37.590000. running mean: -37.214287\n",
      "ep 2584: ep_len:77 episode reward: total was 37.000000. running mean: -36.472144\n",
      "ep 2584: ep_len:2988 episode reward: total was -37.770000. running mean: -36.485122\n",
      "ep 2584: ep_len:504 episode reward: total was -41.590000. running mean: -36.536171\n",
      "ep 2584: ep_len:115 episode reward: total was 56.000000. running mean: -35.610809\n",
      "ep 2584: ep_len:61 episode reward: total was 29.000000. running mean: -34.964701\n",
      "ep 2584: ep_len:500 episode reward: total was 37.330000. running mean: -34.241754\n",
      "ep 2584: ep_len:340 episode reward: total was 9.150000. running mean: -33.807837\n",
      "ep 2584: ep_len:1537 episode reward: total was -52.930000. running mean: -33.999058\n",
      "ep 2584: ep_len:689 episode reward: total was 3.360000. running mean: -33.625468\n",
      "ep 2584: ep_len:602 episode reward: total was 46.160000. running mean: -32.827613\n",
      "ep 2584: ep_len:97 episode reward: total was 47.000000. running mean: -32.029337\n",
      "ep 2584: ep_len:610 episode reward: total was -10.230000. running mean: -31.811344\n",
      "ep 2584: ep_len:2919 episode reward: total was 10.110000. running mean: -31.392130\n",
      "epsilon:0.009992 episode_count: 38937. steps_count: 41867821.000000\n",
      "ep 2585: ep_len:3871 episode reward: total was -899.570000. running mean: -40.073909\n",
      "ep 2585: ep_len:774 episode reward: total was -15.660000. running mean: -39.829770\n",
      "ep 2585: ep_len:3030 episode reward: total was -193.940000. running mean: -41.370872\n",
      "ep 2585: ep_len:835 episode reward: total was 3.050000. running mean: -40.926663\n",
      "ep 2585: ep_len:58 episode reward: total was 26.000000. running mean: -40.257397\n",
      "ep 2585: ep_len:61 episode reward: total was 29.000000. running mean: -39.564823\n",
      "ep 2585: ep_len:1337 episode reward: total was -228.130000. running mean: -41.450475\n",
      "ep 2585: ep_len:3673 episode reward: total was -700.760000. running mean: -48.043570\n",
      "ep 2585: ep_len:1177 episode reward: total was -10.160000. running mean: -47.664734\n",
      "ep 2585: ep_len:718 episode reward: total was 44.560000. running mean: -46.742487\n",
      "ep 2585: ep_len:620 episode reward: total was -4.070000. running mean: -46.315762\n",
      "ep 2585: ep_len:72 episode reward: total was 34.500000. running mean: -45.507604\n",
      "ep 2585: ep_len:123 episode reward: total was 60.000000. running mean: -44.452528\n",
      "ep 2585: ep_len:1001 episode reward: total was -39.810000. running mean: -44.406103\n",
      "ep 2585: ep_len:2765 episode reward: total was -15.830000. running mean: -44.120342\n",
      "ep 2585: ep_len:47 episode reward: total was 19.000000. running mean: -43.489139\n",
      "epsilon:0.009992 episode_count: 38953. steps_count: 41887983.000000\n",
      "ep 2586: ep_len:1435 episode reward: total was 14.210000. running mean: -42.912147\n",
      "ep 2586: ep_len:785 episode reward: total was -12.520000. running mean: -42.608226\n",
      "ep 2586: ep_len:43 episode reward: total was 20.000000. running mean: -41.982143\n",
      "ep 2586: ep_len:2955 episode reward: total was -47.680000. running mean: -42.039122\n",
      "ep 2586: ep_len:500 episode reward: total was 11.080000. running mean: -41.507931\n",
      "ep 2586: ep_len:122 episode reward: total was 58.000000. running mean: -40.512851\n",
      "ep 2586: ep_len:52 episode reward: total was 24.500000. running mean: -39.862723\n",
      "ep 2586: ep_len:931 episode reward: total was 45.640000. running mean: -39.007696\n",
      "ep 2586: ep_len:3713 episode reward: total was -66.690000. running mean: -39.284519\n",
      "ep 2586: ep_len:913 episode reward: total was -25.410000. running mean: -39.145774\n",
      "ep 2586: ep_len:856 episode reward: total was 44.780000. running mean: -38.306516\n",
      "ep 2586: ep_len:647 episode reward: total was 11.560000. running mean: -37.807851\n",
      "ep 2586: ep_len:83 episode reward: total was 40.000000. running mean: -37.029772\n",
      "ep 2586: ep_len:1451 episode reward: total was -18.200000. running mean: -36.841474\n",
      "ep 2586: ep_len:2823 episode reward: total was -27.730000. running mean: -36.750360\n",
      "ep 2586: ep_len:48 episode reward: total was 21.000000. running mean: -36.172856\n",
      "epsilon:0.009992 episode_count: 38969. steps_count: 41905340.000000\n",
      "ep 2587: ep_len:1453 episode reward: total was 29.570000. running mean: -35.515428\n",
      "ep 2587: ep_len:800 episode reward: total was 23.300000. running mean: -34.927273\n",
      "ep 2587: ep_len:2968 episode reward: total was -20.120000. running mean: -34.779201\n",
      "ep 2587: ep_len:500 episode reward: total was 8.250000. running mean: -34.348909\n",
      "ep 2587: ep_len:1420 episode reward: total was -226.350000. running mean: -36.268919\n",
      "ep 2587: ep_len:332 episode reward: total was 11.920000. running mean: -35.787030\n",
      "ep 2587: ep_len:575 episode reward: total was -19.670000. running mean: -35.625860\n",
      "ep 2587: ep_len:718 episode reward: total was 4.580000. running mean: -35.223801\n",
      "ep 2587: ep_len:1453 episode reward: total was 13.990000. running mean: -34.731663\n",
      "ep 2587: ep_len:93 episode reward: total was 42.000000. running mean: -33.964347\n",
      "ep 2587: ep_len:203 episode reward: total was 95.500000. running mean: -32.669703\n",
      "ep 2587: ep_len:82 episode reward: total was 39.500000. running mean: -31.948006\n",
      "ep 2587: ep_len:1151 episode reward: total was -18.440000. running mean: -31.812926\n",
      "ep 2587: ep_len:2830 episode reward: total was -23.830000. running mean: -31.733097\n",
      "ep 2587: ep_len:49 episode reward: total was 21.500000. running mean: -31.200766\n",
      "epsilon:0.009992 episode_count: 38984. steps_count: 41919967.000000\n",
      "ep 2588: ep_len:1013 episode reward: total was -80.280000. running mean: -31.691558\n",
      "ep 2588: ep_len:500 episode reward: total was 22.320000. running mean: -31.151443\n",
      "ep 2588: ep_len:3028 episode reward: total was -8.070000. running mean: -30.920628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2588: ep_len:675 episode reward: total was 2.930000. running mean: -30.582122\n",
      "ep 2588: ep_len:49 episode reward: total was 23.000000. running mean: -30.046301\n",
      "ep 2588: ep_len:1507 episode reward: total was 30.270000. running mean: -29.443138\n",
      "ep 2588: ep_len:3721 episode reward: total was -59.020000. running mean: -29.738906\n",
      "ep 2588: ep_len:796 episode reward: total was -0.080000. running mean: -29.442317\n",
      "ep 2588: ep_len:814 episode reward: total was -12.880000. running mean: -29.276694\n",
      "ep 2588: ep_len:955 episode reward: total was 49.340000. running mean: -28.490527\n",
      "ep 2588: ep_len:55 episode reward: total was 24.500000. running mean: -27.960622\n",
      "ep 2588: ep_len:189 episode reward: total was 90.000000. running mean: -26.781016\n",
      "ep 2588: ep_len:500 episode reward: total was 25.440000. running mean: -26.258806\n",
      "ep 2588: ep_len:2802 episode reward: total was 0.890000. running mean: -25.987317\n",
      "ep 2588: ep_len:35 episode reward: total was 14.500000. running mean: -25.582444\n",
      "epsilon:0.009992 episode_count: 38999. steps_count: 41936606.000000\n",
      "ep 2589: ep_len:969 episode reward: total was -162.180000. running mean: -26.948420\n",
      "ep 2589: ep_len:188 episode reward: total was 7.140000. running mean: -26.607536\n",
      "ep 2589: ep_len:71 episode reward: total was 31.000000. running mean: -26.031460\n",
      "ep 2589: ep_len:3081 episode reward: total was -1.660000. running mean: -25.787746\n",
      "ep 2589: ep_len:631 episode reward: total was 11.080000. running mean: -25.419068\n",
      "ep 2589: ep_len:99 episode reward: total was 46.500000. running mean: -24.699878\n",
      "ep 2589: ep_len:822 episode reward: total was 22.500000. running mean: -24.227879\n",
      "ep 2589: ep_len:3492 episode reward: total was -6.280000. running mean: -24.048400\n",
      "ep 2589: ep_len:656 episode reward: total was 4.370000. running mean: -23.764216\n",
      "ep 2589: ep_len:784 episode reward: total was -8.270000. running mean: -23.609274\n",
      "ep 2589: ep_len:628 episode reward: total was 0.050000. running mean: -23.372681\n",
      "ep 2589: ep_len:68 episode reward: total was 31.000000. running mean: -22.828954\n",
      "ep 2589: ep_len:151 episode reward: total was 74.000000. running mean: -21.860665\n",
      "ep 2589: ep_len:46 episode reward: total was 21.500000. running mean: -21.427058\n",
      "ep 2589: ep_len:814 episode reward: total was -19.360000. running mean: -21.406388\n",
      "ep 2589: ep_len:2819 episode reward: total was -13.140000. running mean: -21.323724\n",
      "epsilon:0.009992 episode_count: 39015. steps_count: 41951925.000000\n",
      "ep 2590: ep_len:1463 episode reward: total was 12.680000. running mean: -20.983686\n",
      "ep 2590: ep_len:776 episode reward: total was 15.810000. running mean: -20.615750\n",
      "ep 2590: ep_len:3013 episode reward: total was 1.000000. running mean: -20.399592\n",
      "ep 2590: ep_len:500 episode reward: total was -1.010000. running mean: -20.205696\n",
      "ep 2590: ep_len:39 episode reward: total was 16.500000. running mean: -19.838639\n",
      "ep 2590: ep_len:105 episode reward: total was 49.500000. running mean: -19.145253\n",
      "ep 2590: ep_len:50 episode reward: total was 22.000000. running mean: -18.733800\n",
      "ep 2590: ep_len:500 episode reward: total was 41.680000. running mean: -18.129662\n",
      "ep 2590: ep_len:346 episode reward: total was 21.120000. running mean: -17.737166\n",
      "ep 2590: ep_len:868 episode reward: total was -138.800000. running mean: -18.947794\n",
      "ep 2590: ep_len:747 episode reward: total was 7.400000. running mean: -18.684316\n",
      "ep 2590: ep_len:911 episode reward: total was 31.320000. running mean: -18.184273\n",
      "ep 2590: ep_len:80 episode reward: total was 38.500000. running mean: -17.617430\n",
      "ep 2590: ep_len:36 episode reward: total was 15.000000. running mean: -17.291256\n",
      "ep 2590: ep_len:1050 episode reward: total was -25.120000. running mean: -17.369543\n",
      "ep 2590: ep_len:2842 episode reward: total was -6.980000. running mean: -17.265648\n",
      "epsilon:0.009992 episode_count: 39031. steps_count: 41965251.000000\n",
      "ep 2591: ep_len:689 episode reward: total was 4.370000. running mean: -17.049291\n",
      "ep 2591: ep_len:807 episode reward: total was 21.540000. running mean: -16.663398\n",
      "ep 2591: ep_len:58 episode reward: total was 27.500000. running mean: -16.221764\n",
      "ep 2591: ep_len:3002 episode reward: total was -28.190000. running mean: -16.341447\n",
      "ep 2591: ep_len:1432 episode reward: total was -7.340000. running mean: -16.251432\n",
      "ep 2591: ep_len:88 episode reward: total was 41.000000. running mean: -15.678918\n",
      "ep 2591: ep_len:1410 episode reward: total was -183.290000. running mean: -17.355029\n",
      "ep 2591: ep_len:660 episode reward: total was 21.650000. running mean: -16.964979\n",
      "ep 2591: ep_len:619 episode reward: total was -1.050000. running mean: -16.805829\n",
      "ep 2591: ep_len:825 episode reward: total was 6.540000. running mean: -16.572370\n",
      "ep 2591: ep_len:655 episode reward: total was -34.950000. running mean: -16.756147\n",
      "ep 2591: ep_len:100 episode reward: total was 45.500000. running mean: -16.133585\n",
      "ep 2591: ep_len:660 episode reward: total was 7.300000. running mean: -15.899249\n",
      "ep 2591: ep_len:2780 episode reward: total was -13.660000. running mean: -15.876857\n",
      "epsilon:0.009992 episode_count: 39045. steps_count: 41979036.000000\n",
      "ep 2592: ep_len:841 episode reward: total was -2.460000. running mean: -15.742688\n",
      "ep 2592: ep_len:760 episode reward: total was -1.410000. running mean: -15.599362\n",
      "ep 2592: ep_len:2993 episode reward: total was -24.740000. running mean: -15.690768\n",
      "ep 2592: ep_len:576 episode reward: total was 24.800000. running mean: -15.285860\n",
      "ep 2592: ep_len:67 episode reward: total was 30.500000. running mean: -14.828002\n",
      "ep 2592: ep_len:108 episode reward: total was 51.000000. running mean: -14.169722\n",
      "ep 2592: ep_len:1384 episode reward: total was -130.970000. running mean: -15.337724\n",
      "ep 2592: ep_len:3724 episode reward: total was -105.250000. running mean: -16.236847\n",
      "ep 2592: ep_len:1278 episode reward: total was -72.440000. running mean: -16.798879\n",
      "ep 2592: ep_len:678 episode reward: total was 22.320000. running mean: -16.407690\n",
      "ep 2592: ep_len:510 episode reward: total was 20.430000. running mean: -16.039313\n",
      "ep 2592: ep_len:77 episode reward: total was 37.000000. running mean: -15.508920\n",
      "ep 2592: ep_len:120 episode reward: total was 57.000000. running mean: -14.783831\n",
      "ep 2592: ep_len:594 episode reward: total was -6.350000. running mean: -14.699492\n",
      "ep 2592: ep_len:2813 episode reward: total was -2.580000. running mean: -14.578297\n",
      "epsilon:0.009992 episode_count: 39060. steps_count: 41995559.000000\n",
      "ep 2593: ep_len:722 episode reward: total was -47.490000. running mean: -14.907414\n",
      "ep 2593: ep_len:500 episode reward: total was 18.950000. running mean: -14.568840\n",
      "ep 2593: ep_len:68 episode reward: total was 29.500000. running mean: -14.128152\n",
      "ep 2593: ep_len:3049 episode reward: total was -51.050000. running mean: -14.497370\n",
      "ep 2593: ep_len:884 episode reward: total was 67.030000. running mean: -13.682097\n",
      "ep 2593: ep_len:60 episode reward: total was 27.000000. running mean: -13.275276\n",
      "ep 2593: ep_len:795 episode reward: total was 21.610000. running mean: -12.926423\n",
      "ep 2593: ep_len:343 episode reward: total was 22.100000. running mean: -12.576159\n",
      "ep 2593: ep_len:1528 episode reward: total was -42.160000. running mean: -12.871997\n",
      "ep 2593: ep_len:841 episode reward: total was 16.250000. running mean: -12.580777\n",
      "ep 2593: ep_len:743 episode reward: total was 38.990000. running mean: -12.065069\n",
      "ep 2593: ep_len:91 episode reward: total was 44.000000. running mean: -11.504419\n",
      "ep 2593: ep_len:480 episode reward: total was 47.080000. running mean: -10.918575\n",
      "ep 2593: ep_len:2674 episode reward: total was -10.160000. running mean: -10.910989\n",
      "epsilon:0.009992 episode_count: 39074. steps_count: 42008337.000000\n",
      "ep 2594: ep_len:781 episode reward: total was -5.940000. running mean: -10.861279\n",
      "ep 2594: ep_len:500 episode reward: total was -25.220000. running mean: -11.004866\n",
      "ep 2594: ep_len:103 episode reward: total was 50.000000. running mean: -10.394817\n",
      "ep 2594: ep_len:877 episode reward: total was 67.940000. running mean: -9.611469\n",
      "ep 2594: ep_len:55 episode reward: total was 26.000000. running mean: -9.255355\n",
      "ep 2594: ep_len:151 episode reward: total was 74.000000. running mean: -8.422801\n",
      "ep 2594: ep_len:1087 episode reward: total was -62.020000. running mean: -8.958773\n",
      "ep 2594: ep_len:367 episode reward: total was 13.770000. running mean: -8.731485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2594: ep_len:637 episode reward: total was -35.210000. running mean: -8.996270\n",
      "ep 2594: ep_len:805 episode reward: total was 44.760000. running mean: -8.458708\n",
      "ep 2594: ep_len:2741 episode reward: total was -171.550000. running mean: -10.089621\n",
      "ep 2594: ep_len:52 episode reward: total was 24.500000. running mean: -9.743724\n",
      "ep 2594: ep_len:122 episode reward: total was 56.500000. running mean: -9.081287\n",
      "ep 2594: ep_len:46 episode reward: total was 20.000000. running mean: -8.790474\n",
      "ep 2594: ep_len:741 episode reward: total was -28.200000. running mean: -8.984570\n",
      "ep 2594: ep_len:2832 episode reward: total was 1.310000. running mean: -8.881624\n",
      "epsilon:0.009992 episode_count: 39090. steps_count: 42020234.000000\n",
      "ep 2595: ep_len:907 episode reward: total was -39.540000. running mean: -9.188208\n",
      "ep 2595: ep_len:716 episode reward: total was -21.290000. running mean: -9.309226\n",
      "ep 2595: ep_len:3027 episode reward: total was -43.290000. running mean: -9.649033\n",
      "ep 2595: ep_len:630 episode reward: total was 4.110000. running mean: -9.511443\n",
      "ep 2595: ep_len:49 episode reward: total was 23.000000. running mean: -9.186329\n",
      "ep 2595: ep_len:901 episode reward: total was 55.750000. running mean: -8.536965\n",
      "ep 2595: ep_len:3953 episode reward: total was -426.300000. running mean: -12.714596\n",
      "ep 2595: ep_len:816 episode reward: total was -13.070000. running mean: -12.718150\n",
      "ep 2595: ep_len:778 episode reward: total was -2.270000. running mean: -12.613668\n",
      "ep 2595: ep_len:649 episode reward: total was 4.300000. running mean: -12.444531\n",
      "ep 2595: ep_len:93 episode reward: total was 43.500000. running mean: -11.885086\n",
      "ep 2595: ep_len:136 episode reward: total was 62.000000. running mean: -11.146235\n",
      "ep 2595: ep_len:46 episode reward: total was 20.000000. running mean: -10.834773\n",
      "ep 2595: ep_len:101 episode reward: total was 49.000000. running mean: -10.236425\n",
      "ep 2595: ep_len:891 episode reward: total was 9.230000. running mean: -10.041761\n",
      "ep 2595: ep_len:2889 episode reward: total was -1.730000. running mean: -9.958643\n",
      "epsilon:0.009992 episode_count: 39106. steps_count: 42036816.000000\n",
      "ep 2596: ep_len:1112 episode reward: total was 12.980000. running mean: -9.729257\n",
      "ep 2596: ep_len:752 episode reward: total was -36.800000. running mean: -9.999964\n",
      "ep 2596: ep_len:2897 episode reward: total was -30.240000. running mean: -10.202365\n",
      "ep 2596: ep_len:500 episode reward: total was 14.030000. running mean: -9.960041\n",
      "ep 2596: ep_len:166 episode reward: total was 81.500000. running mean: -9.045441\n",
      "ep 2596: ep_len:44 episode reward: total was 19.000000. running mean: -8.764986\n",
      "ep 2596: ep_len:1103 episode reward: total was -5.300000. running mean: -8.730336\n",
      "ep 2596: ep_len:337 episode reward: total was 16.500000. running mean: -8.478033\n",
      "ep 2596: ep_len:1497 episode reward: total was -25.360000. running mean: -8.646853\n",
      "ep 2596: ep_len:7475 episode reward: total was -204.150000. running mean: -10.601884\n",
      "ep 2596: ep_len:829 episode reward: total was 5.580000. running mean: -10.440065\n",
      "ep 2596: ep_len:95 episode reward: total was 43.000000. running mean: -9.905665\n",
      "ep 2596: ep_len:696 episode reward: total was -1.290000. running mean: -9.819508\n",
      "ep 2596: ep_len:2824 episode reward: total was 0.900000. running mean: -9.712313\n",
      "epsilon:0.009992 episode_count: 39120. steps_count: 42057143.000000\n",
      "ep 2597: ep_len:1354 episode reward: total was -0.100000. running mean: -9.616190\n",
      "ep 2597: ep_len:1598 episode reward: total was -61.320000. running mean: -10.133228\n",
      "ep 2597: ep_len:3056 episode reward: total was -57.540000. running mean: -10.607296\n",
      "ep 2597: ep_len:501 episode reward: total was 10.300000. running mean: -10.398223\n",
      "ep 2597: ep_len:44 episode reward: total was 20.500000. running mean: -10.089240\n",
      "ep 2597: ep_len:500 episode reward: total was 32.460000. running mean: -9.663748\n",
      "ep 2597: ep_len:3679 episode reward: total was -23.600000. running mean: -9.803111\n",
      "ep 2597: ep_len:1279 episode reward: total was -46.450000. running mean: -10.169579\n",
      "ep 2597: ep_len:7576 episode reward: total was -20.050000. running mean: -10.268384\n",
      "ep 2597: ep_len:574 episode reward: total was -18.670000. running mean: -10.352400\n",
      "ep 2597: ep_len:221 episode reward: total was -201.990000. running mean: -12.268776\n",
      "ep 2597: ep_len:1438 episode reward: total was 11.360000. running mean: -12.032488\n",
      "ep 2597: ep_len:2781 episode reward: total was 5.090000. running mean: -11.861263\n",
      "epsilon:0.009992 episode_count: 39133. steps_count: 42081744.000000\n",
      "ep 2598: ep_len:698 episode reward: total was -18.440000. running mean: -11.927051\n",
      "ep 2598: ep_len:801 episode reward: total was -6.710000. running mean: -11.874880\n",
      "ep 2598: ep_len:91 episode reward: total was 39.500000. running mean: -11.361131\n",
      "ep 2598: ep_len:695 episode reward: total was 27.200000. running mean: -10.975520\n",
      "ep 2598: ep_len:167 episode reward: total was 80.500000. running mean: -10.060765\n",
      "ep 2598: ep_len:79 episode reward: total was 36.500000. running mean: -9.595157\n",
      "ep 2598: ep_len:1432 episode reward: total was -12.390000. running mean: -9.623106\n",
      "ep 2598: ep_len:3622 episode reward: total was -10.030000. running mean: -9.627174\n",
      "ep 2598: ep_len:986 episode reward: total was -24.520000. running mean: -9.776103\n",
      "ep 2598: ep_len:810 episode reward: total was 43.460000. running mean: -9.243742\n",
      "ep 2598: ep_len:701 episode reward: total was -28.550000. running mean: -9.436804\n",
      "ep 2598: ep_len:41 episode reward: total was 19.000000. running mean: -9.152436\n",
      "ep 2598: ep_len:643 episode reward: total was 1.940000. running mean: -9.041512\n",
      "ep 2598: ep_len:2947 episode reward: total was -9.840000. running mean: -9.049497\n",
      "ep 2598: ep_len:48 episode reward: total was 22.500000. running mean: -8.734002\n",
      "epsilon:0.009992 episode_count: 39148. steps_count: 42095505.000000\n",
      "ep 2599: ep_len:742 episode reward: total was -69.510000. running mean: -9.341762\n",
      "ep 2599: ep_len:500 episode reward: total was -2.910000. running mean: -9.277444\n",
      "ep 2599: ep_len:3071 episode reward: total was -36.440000. running mean: -9.549070\n",
      "ep 2599: ep_len:822 episode reward: total was -27.300000. running mean: -9.726579\n",
      "ep 2599: ep_len:68 episode reward: total was 32.500000. running mean: -9.304313\n",
      "ep 2599: ep_len:722 episode reward: total was -34.760000. running mean: -9.558870\n",
      "ep 2599: ep_len:350 episode reward: total was 20.180000. running mean: -9.261481\n",
      "ep 2599: ep_len:952 episode reward: total was -22.380000. running mean: -9.392667\n",
      "ep 2599: ep_len:874 episode reward: total was 33.300000. running mean: -8.965740\n",
      "ep 2599: ep_len:1477 episode reward: total was 31.630000. running mean: -8.559783\n",
      "ep 2599: ep_len:51 episode reward: total was 22.500000. running mean: -8.249185\n",
      "ep 2599: ep_len:1435 episode reward: total was 12.590000. running mean: -8.040793\n",
      "ep 2599: ep_len:2820 episode reward: total was 1.740000. running mean: -7.942985\n",
      "epsilon:0.009992 episode_count: 39161. steps_count: 42109389.000000\n",
      "ep 2600: ep_len:702 episode reward: total was -34.560000. running mean: -8.209155\n",
      "ep 2600: ep_len:982 episode reward: total was 3.870000. running mean: -8.088364\n",
      "ep 2600: ep_len:3024 episode reward: total was -86.280000. running mean: -8.870280\n",
      "ep 2600: ep_len:500 episode reward: total was 8.670000. running mean: -8.694877\n",
      "ep 2600: ep_len:127 episode reward: total was 59.000000. running mean: -8.017928\n",
      "ep 2600: ep_len:85 episode reward: total was 39.500000. running mean: -7.542749\n",
      "ep 2600: ep_len:500 episode reward: total was 56.220000. running mean: -6.905122\n",
      "ep 2600: ep_len:668 episode reward: total was 6.030000. running mean: -6.775770\n",
      "ep 2600: ep_len:827 episode reward: total was -6.100000. running mean: -6.769013\n",
      "ep 2600: ep_len:678 episode reward: total was 25.120000. running mean: -6.450122\n",
      "ep 2600: ep_len:673 episode reward: total was -0.510000. running mean: -6.390721\n",
      "ep 2600: ep_len:92 episode reward: total was 44.500000. running mean: -5.881814\n",
      "ep 2600: ep_len:95 episode reward: total was 46.000000. running mean: -5.362996\n",
      "ep 2600: ep_len:1167 episode reward: total was -23.450000. running mean: -5.543866\n",
      "ep 2600: ep_len:2787 episode reward: total was -25.410000. running mean: -5.742527\n",
      "ep 2600: ep_len:68 episode reward: total was 32.500000. running mean: -5.360102\n",
      "epsilon:0.009992 episode_count: 39177. steps_count: 42122364.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2601: ep_len:1451 episode reward: total was 18.470000. running mean: -5.121801\n",
      "ep 2601: ep_len:500 episode reward: total was 18.280000. running mean: -4.887783\n",
      "ep 2601: ep_len:68 episode reward: total was 32.500000. running mean: -4.513905\n",
      "ep 2601: ep_len:2996 episode reward: total was -52.390000. running mean: -4.992666\n",
      "ep 2601: ep_len:500 episode reward: total was 3.710000. running mean: -4.905639\n",
      "ep 2601: ep_len:71 episode reward: total was 34.000000. running mean: -4.516583\n",
      "ep 2601: ep_len:1070 episode reward: total was -3.610000. running mean: -4.507517\n",
      "ep 2601: ep_len:3594 episode reward: total was -95.520000. running mean: -5.417642\n",
      "ep 2601: ep_len:869 episode reward: total was 2.520000. running mean: -5.338266\n",
      "ep 2601: ep_len:850 episode reward: total was 50.930000. running mean: -4.775583\n",
      "ep 2601: ep_len:608 episode reward: total was 14.720000. running mean: -4.580627\n",
      "ep 2601: ep_len:36 episode reward: total was 16.500000. running mean: -4.369821\n",
      "ep 2601: ep_len:176 episode reward: total was 80.500000. running mean: -3.521123\n",
      "ep 2601: ep_len:91 episode reward: total was 42.500000. running mean: -3.060911\n",
      "ep 2601: ep_len:1142 episode reward: total was -35.700000. running mean: -3.387302\n",
      "ep 2601: ep_len:2844 episode reward: total was -13.410000. running mean: -3.487529\n",
      "epsilon:0.009992 episode_count: 39193. steps_count: 42139230.000000\n",
      "ep 2602: ep_len:610 episode reward: total was 8.740000. running mean: -3.365254\n",
      "ep 2602: ep_len:967 episode reward: total was 20.160000. running mean: -3.130001\n",
      "ep 2602: ep_len:2926 episode reward: total was 10.250000. running mean: -2.996201\n",
      "ep 2602: ep_len:544 episode reward: total was -26.040000. running mean: -3.226639\n",
      "ep 2602: ep_len:37 episode reward: total was 17.000000. running mean: -3.024373\n",
      "ep 2602: ep_len:66 episode reward: total was 30.000000. running mean: -2.694129\n",
      "ep 2602: ep_len:500 episode reward: total was 13.140000. running mean: -2.535788\n",
      "ep 2602: ep_len:629 episode reward: total was 15.830000. running mean: -2.352130\n",
      "ep 2602: ep_len:810 episode reward: total was -30.800000. running mean: -2.636609\n",
      "ep 2602: ep_len:861 episode reward: total was 73.830000. running mean: -1.871943\n",
      "ep 2602: ep_len:587 episode reward: total was -1.370000. running mean: -1.866923\n",
      "ep 2602: ep_len:203 episode reward: total was 98.500000. running mean: -0.863254\n",
      "ep 2602: ep_len:51 episode reward: total was 24.000000. running mean: -0.614622\n",
      "ep 2602: ep_len:102 episode reward: total was 49.500000. running mean: -0.113475\n",
      "ep 2602: ep_len:1505 episode reward: total was -38.350000. running mean: -0.495841\n",
      "ep 2602: ep_len:2898 episode reward: total was -11.370000. running mean: -0.604582\n",
      "epsilon:0.009992 episode_count: 39209. steps_count: 42152526.000000\n",
      "ep 2603: ep_len:924 episode reward: total was -36.270000. running mean: -0.961236\n",
      "ep 2603: ep_len:675 episode reward: total was -22.830000. running mean: -1.179924\n",
      "ep 2603: ep_len:3033 episode reward: total was -23.910000. running mean: -1.407225\n",
      "ep 2603: ep_len:1253 episode reward: total was -47.230000. running mean: -1.865452\n",
      "ep 2603: ep_len:129 episode reward: total was 61.500000. running mean: -1.231798\n",
      "ep 2603: ep_len:500 episode reward: total was 15.840000. running mean: -1.061080\n",
      "ep 2603: ep_len:627 episode reward: total was -53.330000. running mean: -1.583769\n",
      "ep 2603: ep_len:552 episode reward: total was -10.470000. running mean: -1.672632\n",
      "ep 2603: ep_len:744 episode reward: total was 51.190000. running mean: -1.144005\n",
      "ep 2603: ep_len:646 episode reward: total was 11.130000. running mean: -1.021265\n",
      "ep 2603: ep_len:74 episode reward: total was 32.500000. running mean: -0.686052\n",
      "ep 2603: ep_len:48 episode reward: total was 22.500000. running mean: -0.454192\n",
      "ep 2603: ep_len:99 episode reward: total was 48.000000. running mean: 0.030350\n",
      "ep 2603: ep_len:1143 episode reward: total was -5.420000. running mean: -0.024154\n",
      "ep 2603: ep_len:2878 episode reward: total was -50.760000. running mean: -0.531512\n",
      "ep 2603: ep_len:46 episode reward: total was 21.500000. running mean: -0.311197\n",
      "epsilon:0.009992 episode_count: 39225. steps_count: 42165897.000000\n",
      "ep 2604: ep_len:595 episode reward: total was 3.020000. running mean: -0.277885\n",
      "ep 2604: ep_len:708 episode reward: total was -22.790000. running mean: -0.503006\n",
      "ep 2604: ep_len:2958 episode reward: total was -54.630000. running mean: -1.044276\n",
      "ep 2604: ep_len:724 episode reward: total was -17.170000. running mean: -1.205533\n",
      "ep 2604: ep_len:57 episode reward: total was 27.000000. running mean: -0.923478\n",
      "ep 2604: ep_len:1400 episode reward: total was -184.130000. running mean: -2.755543\n",
      "ep 2604: ep_len:3636 episode reward: total was -80.800000. running mean: -3.535988\n",
      "ep 2604: ep_len:1150 episode reward: total was -26.990000. running mean: -3.770528\n",
      "ep 2604: ep_len:771 episode reward: total was 10.710000. running mean: -3.625723\n",
      "ep 2604: ep_len:701 episode reward: total was -17.500000. running mean: -3.764465\n",
      "ep 2604: ep_len:47 episode reward: total was 20.500000. running mean: -3.521821\n",
      "ep 2604: ep_len:132 episode reward: total was 64.500000. running mean: -2.841602\n",
      "ep 2604: ep_len:782 episode reward: total was -19.100000. running mean: -3.004186\n",
      "ep 2604: ep_len:2803 episode reward: total was -43.830000. running mean: -3.412445\n",
      "epsilon:0.009992 episode_count: 39239. steps_count: 42182361.000000\n",
      "ep 2605: ep_len:658 episode reward: total was -4.600000. running mean: -3.424320\n",
      "ep 2605: ep_len:963 episode reward: total was 26.910000. running mean: -3.120977\n",
      "ep 2605: ep_len:2925 episode reward: total was -2.580000. running mean: -3.115567\n",
      "ep 2605: ep_len:691 episode reward: total was 6.580000. running mean: -3.018611\n",
      "ep 2605: ep_len:41 episode reward: total was 17.500000. running mean: -2.813425\n",
      "ep 2605: ep_len:133 episode reward: total was 62.000000. running mean: -2.165291\n",
      "ep 2605: ep_len:1114 episode reward: total was 0.870000. running mean: -2.134938\n",
      "ep 2605: ep_len:4007 episode reward: total was -570.290000. running mean: -7.816489\n",
      "ep 2605: ep_len:1558 episode reward: total was -150.150000. running mean: -9.239824\n",
      "ep 2605: ep_len:826 episode reward: total was 36.120000. running mean: -8.786226\n",
      "ep 2605: ep_len:500 episode reward: total was 40.020000. running mean: -8.298163\n",
      "ep 2605: ep_len:1084 episode reward: total was -20.640000. running mean: -8.421582\n",
      "ep 2605: ep_len:2808 episode reward: total was -32.050000. running mean: -8.657866\n",
      "epsilon:0.009992 episode_count: 39252. steps_count: 42199669.000000\n",
      "ep 2606: ep_len:500 episode reward: total was 10.960000. running mean: -8.461687\n",
      "ep 2606: ep_len:745 episode reward: total was 1.470000. running mean: -8.362370\n",
      "ep 2606: ep_len:3017 episode reward: total was 3.880000. running mean: -8.239947\n",
      "ep 2606: ep_len:646 episode reward: total was 18.300000. running mean: -7.974547\n",
      "ep 2606: ep_len:1069 episode reward: total was -10.780000. running mean: -8.002602\n",
      "ep 2606: ep_len:675 episode reward: total was 23.790000. running mean: -7.684676\n",
      "ep 2606: ep_len:4287 episode reward: total was -786.240000. running mean: -15.470229\n",
      "ep 2606: ep_len:871 episode reward: total was 49.460000. running mean: -14.820927\n",
      "ep 2606: ep_len:1518 episode reward: total was -27.560000. running mean: -14.948317\n",
      "ep 2606: ep_len:98 episode reward: total was 43.000000. running mean: -14.368834\n",
      "ep 2606: ep_len:49 episode reward: total was 23.000000. running mean: -13.995146\n",
      "ep 2606: ep_len:75 episode reward: total was 33.000000. running mean: -13.525194\n",
      "ep 2606: ep_len:1147 episode reward: total was -21.420000. running mean: -13.604143\n",
      "ep 2606: ep_len:2825 episode reward: total was -940.050000. running mean: -22.868601\n",
      "epsilon:0.009992 episode_count: 39266. steps_count: 42217191.000000\n",
      "ep 2607: ep_len:1474 episode reward: total was -47.750000. running mean: -23.117415\n",
      "ep 2607: ep_len:655 episode reward: total was -41.520000. running mean: -23.301441\n",
      "ep 2607: ep_len:71 episode reward: total was 32.500000. running mean: -22.743427\n",
      "ep 2607: ep_len:3090 episode reward: total was -8.530000. running mean: -22.601292\n",
      "ep 2607: ep_len:783 episode reward: total was -14.500000. running mean: -22.520279\n",
      "ep 2607: ep_len:110 episode reward: total was 52.000000. running mean: -21.775077\n",
      "ep 2607: ep_len:1493 episode reward: total was -103.060000. running mean: -22.587926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2607: ep_len:666 episode reward: total was 28.110000. running mean: -22.080947\n",
      "ep 2607: ep_len:1254 episode reward: total was -53.280000. running mean: -22.392937\n",
      "ep 2607: ep_len:7389 episode reward: total was -34.070000. running mean: -22.509708\n",
      "ep 2607: ep_len:609 episode reward: total was -7.270000. running mean: -22.357311\n",
      "ep 2607: ep_len:94 episode reward: total was 45.500000. running mean: -21.678738\n",
      "ep 2607: ep_len:500 episode reward: total was 32.490000. running mean: -21.137050\n",
      "ep 2607: ep_len:2851 episode reward: total was 14.300000. running mean: -20.782680\n",
      "ep 2607: ep_len:73 episode reward: total was 32.000000. running mean: -20.254853\n",
      "epsilon:0.009992 episode_count: 39281. steps_count: 42238303.000000\n",
      "ep 2608: ep_len:748 episode reward: total was 10.510000. running mean: -19.947204\n",
      "ep 2608: ep_len:953 episode reward: total was 17.760000. running mean: -19.570132\n",
      "ep 2608: ep_len:2952 episode reward: total was -14.610000. running mean: -19.520531\n",
      "ep 2608: ep_len:1114 episode reward: total was -25.900000. running mean: -19.584326\n",
      "ep 2608: ep_len:49 episode reward: total was 23.000000. running mean: -19.158482\n",
      "ep 2608: ep_len:105 episode reward: total was 51.000000. running mean: -18.456898\n",
      "ep 2608: ep_len:1384 episode reward: total was -47.940000. running mean: -18.751729\n",
      "ep 2608: ep_len:608 episode reward: total was -0.300000. running mean: -18.567211\n",
      "ep 2608: ep_len:685 episode reward: total was -33.890000. running mean: -18.720439\n",
      "ep 2608: ep_len:771 episode reward: total was 8.560000. running mean: -18.447635\n",
      "ep 2608: ep_len:553 episode reward: total was 26.650000. running mean: -17.996658\n",
      "ep 2608: ep_len:196 episode reward: total was 89.000000. running mean: -16.926692\n",
      "ep 2608: ep_len:60 episode reward: total was 28.500000. running mean: -16.472425\n",
      "ep 2608: ep_len:663 episode reward: total was 12.180000. running mean: -16.185901\n",
      "ep 2608: ep_len:2943 episode reward: total was -6.880000. running mean: -16.092842\n",
      "epsilon:0.009992 episode_count: 39296. steps_count: 42252087.000000\n",
      "ep 2609: ep_len:1122 episode reward: total was 0.000000. running mean: -15.931913\n",
      "ep 2609: ep_len:641 episode reward: total was -20.600000. running mean: -15.978594\n",
      "ep 2609: ep_len:3013 episode reward: total was -12.780000. running mean: -15.946608\n",
      "ep 2609: ep_len:576 episode reward: total was 11.510000. running mean: -15.672042\n",
      "ep 2609: ep_len:84 episode reward: total was 39.000000. running mean: -15.125322\n",
      "ep 2609: ep_len:1042 episode reward: total was -5.010000. running mean: -15.024168\n",
      "ep 2609: ep_len:664 episode reward: total was -7.630000. running mean: -14.950227\n",
      "ep 2609: ep_len:887 episode reward: total was -21.600000. running mean: -15.016725\n",
      "ep 2609: ep_len:911 episode reward: total was 42.890000. running mean: -14.437657\n",
      "ep 2609: ep_len:635 episode reward: total was 13.690000. running mean: -14.156381\n",
      "ep 2609: ep_len:56 episode reward: total was 26.500000. running mean: -13.749817\n",
      "ep 2609: ep_len:91 episode reward: total was 44.000000. running mean: -13.172319\n",
      "ep 2609: ep_len:1503 episode reward: total was -410.920000. running mean: -17.149796\n",
      "ep 2609: ep_len:47 episode reward: total was 22.000000. running mean: -16.758298\n",
      "epsilon:0.009992 episode_count: 39310. steps_count: 42263359.000000\n",
      "ep 2610: ep_len:625 episode reward: total was 7.770000. running mean: -16.513015\n",
      "ep 2610: ep_len:988 episode reward: total was -6.130000. running mean: -16.409184\n",
      "ep 2610: ep_len:2893 episode reward: total was -47.970000. running mean: -16.724793\n",
      "ep 2610: ep_len:1211 episode reward: total was -41.490000. running mean: -16.972445\n",
      "ep 2610: ep_len:26 episode reward: total was 11.500000. running mean: -16.687720\n",
      "ep 2610: ep_len:108 episode reward: total was 52.500000. running mean: -15.995843\n",
      "ep 2610: ep_len:500 episode reward: total was 13.970000. running mean: -15.696185\n",
      "ep 2610: ep_len:351 episode reward: total was 1.490000. running mean: -15.524323\n",
      "ep 2610: ep_len:731 episode reward: total was -34.650000. running mean: -15.715580\n",
      "ep 2610: ep_len:666 episode reward: total was 9.990000. running mean: -15.458524\n",
      "ep 2610: ep_len:838 episode reward: total was 13.200000. running mean: -15.171939\n",
      "ep 2610: ep_len:82 episode reward: total was 39.500000. running mean: -14.625219\n",
      "ep 2610: ep_len:20 episode reward: total was 7.000000. running mean: -14.408967\n",
      "ep 2610: ep_len:677 episode reward: total was 30.080000. running mean: -13.964077\n",
      "ep 2610: ep_len:2850 episode reward: total was -3.160000. running mean: -13.856036\n",
      "ep 2610: ep_len:50 episode reward: total was 22.000000. running mean: -13.497476\n",
      "epsilon:0.009992 episode_count: 39326. steps_count: 42275975.000000\n",
      "ep 2611: ep_len:658 episode reward: total was -1.670000. running mean: -13.379201\n",
      "ep 2611: ep_len:636 episode reward: total was -11.990000. running mean: -13.365309\n",
      "ep 2611: ep_len:2916 episode reward: total was -15.960000. running mean: -13.391256\n",
      "ep 2611: ep_len:695 episode reward: total was 26.420000. running mean: -12.993144\n",
      "ep 2611: ep_len:34 episode reward: total was 15.500000. running mean: -12.708212\n",
      "ep 2611: ep_len:621 episode reward: total was -3.510000. running mean: -12.616230\n",
      "ep 2611: ep_len:3530 episode reward: total was -916.290000. running mean: -21.652968\n",
      "ep 2611: ep_len:542 episode reward: total was -12.410000. running mean: -21.560538\n",
      "ep 2611: ep_len:796 episode reward: total was 3.460000. running mean: -21.310333\n",
      "ep 2611: ep_len:538 episode reward: total was 26.100000. running mean: -20.836229\n",
      "ep 2611: ep_len:671 episode reward: total was 11.900000. running mean: -20.508867\n",
      "ep 2611: ep_len:2828 episode reward: total was -37.510000. running mean: -20.678878\n",
      "epsilon:0.009992 episode_count: 39338. steps_count: 42290440.000000\n",
      "ep 2612: ep_len:822 episode reward: total was -10.220000. running mean: -20.574290\n",
      "ep 2612: ep_len:917 episode reward: total was 30.460000. running mean: -20.063947\n",
      "ep 2612: ep_len:3057 episode reward: total was 42.140000. running mean: -19.441907\n",
      "ep 2612: ep_len:500 episode reward: total was 21.710000. running mean: -19.030388\n",
      "ep 2612: ep_len:52 episode reward: total was 24.500000. running mean: -18.595084\n",
      "ep 2612: ep_len:105 episode reward: total was 49.500000. running mean: -17.914134\n",
      "ep 2612: ep_len:27 episode reward: total was 12.000000. running mean: -17.614992\n",
      "ep 2612: ep_len:1832 episode reward: total was -114.860000. running mean: -18.587442\n",
      "ep 2612: ep_len:352 episode reward: total was -10.990000. running mean: -18.511468\n",
      "ep 2612: ep_len:1257 episode reward: total was -65.370000. running mean: -18.980053\n",
      "ep 2612: ep_len:839 episode reward: total was 30.980000. running mean: -18.480453\n",
      "ep 2612: ep_len:683 episode reward: total was -15.660000. running mean: -18.452248\n",
      "ep 2612: ep_len:179 episode reward: total was 85.000000. running mean: -17.417726\n",
      "ep 2612: ep_len:39 episode reward: total was 18.000000. running mean: -17.063548\n",
      "ep 2612: ep_len:783 episode reward: total was 0.840000. running mean: -16.884513\n",
      "ep 2612: ep_len:2796 episode reward: total was -20.170000. running mean: -16.917368\n",
      "epsilon:0.009992 episode_count: 39354. steps_count: 42304680.000000\n",
      "ep 2613: ep_len:1123 episode reward: total was -18.230000. running mean: -16.930494\n",
      "ep 2613: ep_len:708 episode reward: total was -28.350000. running mean: -17.044689\n",
      "ep 2613: ep_len:3008 episode reward: total was -51.010000. running mean: -17.384342\n",
      "ep 2613: ep_len:1140 episode reward: total was -30.150000. running mean: -17.511999\n",
      "ep 2613: ep_len:123 episode reward: total was 60.000000. running mean: -16.736879\n",
      "ep 2613: ep_len:994 episode reward: total was -73.180000. running mean: -17.301310\n",
      "ep 2613: ep_len:3934 episode reward: total was -628.270000. running mean: -23.410997\n",
      "ep 2613: ep_len:1574 episode reward: total was -54.680000. running mean: -23.723687\n",
      "ep 2613: ep_len:703 episode reward: total was 27.690000. running mean: -23.209550\n",
      "ep 2613: ep_len:1505 episode reward: total was -34.390000. running mean: -23.321355\n",
      "ep 2613: ep_len:99 episode reward: total was 48.000000. running mean: -22.608141\n",
      "ep 2613: ep_len:919 episode reward: total was -39.580000. running mean: -22.777860\n",
      "ep 2613: ep_len:2960 episode reward: total was -411.310000. running mean: -26.663181\n",
      "ep 2613: ep_len:73 episode reward: total was 35.000000. running mean: -26.046549\n",
      "epsilon:0.009992 episode_count: 39368. steps_count: 42323543.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2614: ep_len:1447 episode reward: total was -18.050000. running mean: -25.966584\n",
      "ep 2614: ep_len:952 episode reward: total was 22.090000. running mean: -25.486018\n",
      "ep 2614: ep_len:3001 episode reward: total was -41.950000. running mean: -25.650658\n",
      "ep 2614: ep_len:500 episode reward: total was -13.950000. running mean: -25.533651\n",
      "ep 2614: ep_len:91 episode reward: total was 42.500000. running mean: -24.853315\n",
      "ep 2614: ep_len:969 episode reward: total was -14.720000. running mean: -24.751982\n",
      "ep 2614: ep_len:3635 episode reward: total was -241.680000. running mean: -26.921262\n",
      "ep 2614: ep_len:3906 episode reward: total was -999.390000. running mean: -36.645949\n",
      "ep 2614: ep_len:824 episode reward: total was 55.940000. running mean: -35.720090\n",
      "ep 2614: ep_len:575 episode reward: total was -11.500000. running mean: -35.477889\n",
      "ep 2614: ep_len:141 episode reward: total was 64.500000. running mean: -34.478110\n",
      "ep 2614: ep_len:79 episode reward: total was 36.500000. running mean: -33.768329\n",
      "ep 2614: ep_len:1450 episode reward: total was -20.740000. running mean: -33.638045\n",
      "ep 2614: ep_len:2811 episode reward: total was -28.100000. running mean: -33.582665\n",
      "ep 2614: ep_len:73 episode reward: total was 33.500000. running mean: -32.911838\n",
      "epsilon:0.009992 episode_count: 39383. steps_count: 42343997.000000\n",
      "ep 2615: ep_len:1152 episode reward: total was 7.110000. running mean: -32.511620\n",
      "ep 2615: ep_len:985 episode reward: total was 22.660000. running mean: -31.959904\n",
      "ep 2615: ep_len:2931 episode reward: total was -23.470000. running mean: -31.875005\n",
      "ep 2615: ep_len:869 episode reward: total was 57.290000. running mean: -30.983355\n",
      "ep 2615: ep_len:58 episode reward: total was 27.500000. running mean: -30.398521\n",
      "ep 2615: ep_len:57 episode reward: total was 27.000000. running mean: -29.824536\n",
      "ep 2615: ep_len:703 episode reward: total was -72.110000. running mean: -30.247391\n",
      "ep 2615: ep_len:3616 episode reward: total was -41.830000. running mean: -30.363217\n",
      "ep 2615: ep_len:1580 episode reward: total was -81.210000. running mean: -30.871684\n",
      "ep 2615: ep_len:635 episode reward: total was 0.180000. running mean: -30.561168\n",
      "ep 2615: ep_len:1006 episode reward: total was 14.670000. running mean: -30.108856\n",
      "ep 2615: ep_len:3256 episode reward: total was -292.680000. running mean: -32.734567\n",
      "ep 2615: ep_len:2724 episode reward: total was -49.780000. running mean: -32.905022\n",
      "epsilon:0.009992 episode_count: 39396. steps_count: 42363569.000000\n",
      "ep 2616: ep_len:1132 episode reward: total was -15.110000. running mean: -32.727071\n",
      "ep 2616: ep_len:800 episode reward: total was -13.870000. running mean: -32.538501\n",
      "ep 2616: ep_len:3104 episode reward: total was 20.640000. running mean: -32.006716\n",
      "ep 2616: ep_len:640 episode reward: total was -17.750000. running mean: -31.864149\n",
      "ep 2616: ep_len:63 episode reward: total was 30.000000. running mean: -31.245507\n",
      "ep 2616: ep_len:137 episode reward: total was 65.500000. running mean: -30.278052\n",
      "ep 2616: ep_len:76 episode reward: total was 36.500000. running mean: -29.610272\n",
      "ep 2616: ep_len:79 episode reward: total was 38.000000. running mean: -28.934169\n",
      "ep 2616: ep_len:500 episode reward: total was 15.890000. running mean: -28.485927\n",
      "ep 2616: ep_len:322 episode reward: total was -31.060000. running mean: -28.511668\n",
      "ep 2616: ep_len:633 episode reward: total was 3.130000. running mean: -28.195251\n",
      "ep 2616: ep_len:682 episode reward: total was 33.510000. running mean: -27.578199\n",
      "ep 2616: ep_len:968 episode reward: total was -12.590000. running mean: -27.428317\n",
      "ep 2616: ep_len:835 episode reward: total was 21.540000. running mean: -26.938634\n",
      "ep 2616: ep_len:2906 episode reward: total was -120.220000. running mean: -27.871447\n",
      "epsilon:0.009992 episode_count: 39411. steps_count: 42376446.000000\n",
      "ep 2617: ep_len:1450 episode reward: total was -33.330000. running mean: -27.926033\n",
      "ep 2617: ep_len:763 episode reward: total was 14.150000. running mean: -27.505272\n",
      "ep 2617: ep_len:81 episode reward: total was 37.500000. running mean: -26.855220\n",
      "ep 2617: ep_len:821 episode reward: total was -38.900000. running mean: -26.975667\n",
      "ep 2617: ep_len:1511 episode reward: total was -21.140000. running mean: -26.917311\n",
      "ep 2617: ep_len:649 episode reward: total was -5.580000. running mean: -26.703938\n",
      "ep 2617: ep_len:624 episode reward: total was 27.650000. running mean: -26.160398\n",
      "ep 2617: ep_len:728 episode reward: total was 5.710000. running mean: -25.841694\n",
      "ep 2617: ep_len:869 episode reward: total was 30.930000. running mean: -25.273977\n",
      "ep 2617: ep_len:57 episode reward: total was 27.000000. running mean: -24.751238\n",
      "ep 2617: ep_len:129 episode reward: total was 63.000000. running mean: -23.873725\n",
      "ep 2617: ep_len:648 episode reward: total was -22.980000. running mean: -23.864788\n",
      "ep 2617: ep_len:47 episode reward: total was 20.500000. running mean: -23.421140\n",
      "ep 2617: ep_len:57 episode reward: total was 27.000000. running mean: -22.916929\n",
      "epsilon:0.009992 episode_count: 39425. steps_count: 42384880.000000\n",
      "ep 2618: ep_len:1124 episode reward: total was 9.030000. running mean: -22.597459\n",
      "ep 2618: ep_len:1123 episode reward: total was -37.640000. running mean: -22.747885\n",
      "ep 2618: ep_len:44 episode reward: total was 20.500000. running mean: -22.315406\n",
      "ep 2618: ep_len:3053 episode reward: total was -19.360000. running mean: -22.285852\n",
      "ep 2618: ep_len:664 episode reward: total was -11.600000. running mean: -22.178993\n",
      "ep 2618: ep_len:93 episode reward: total was 45.000000. running mean: -21.507203\n",
      "ep 2618: ep_len:25 episode reward: total was 11.000000. running mean: -21.182131\n",
      "ep 2618: ep_len:500 episode reward: total was 50.770000. running mean: -20.462610\n",
      "ep 2618: ep_len:649 episode reward: total was 1.370000. running mean: -20.244284\n",
      "ep 2618: ep_len:837 episode reward: total was -103.400000. running mean: -21.075841\n",
      "ep 2618: ep_len:657 episode reward: total was 21.720000. running mean: -20.647883\n",
      "ep 2618: ep_len:994 episode reward: total was 13.760000. running mean: -20.303804\n",
      "ep 2618: ep_len:74 episode reward: total was 35.500000. running mean: -19.745766\n",
      "ep 2618: ep_len:110 episode reward: total was 52.000000. running mean: -19.028308\n",
      "ep 2618: ep_len:68 episode reward: total was 32.500000. running mean: -18.513025\n",
      "ep 2618: ep_len:1040 episode reward: total was -10.980000. running mean: -18.437695\n",
      "ep 2618: ep_len:2762 episode reward: total was -77.100000. running mean: -19.024318\n",
      "epsilon:0.009992 episode_count: 39442. steps_count: 42398697.000000\n",
      "ep 2619: ep_len:1501 episode reward: total was -42.240000. running mean: -19.256475\n",
      "ep 2619: ep_len:1007 episode reward: total was 34.980000. running mean: -18.714110\n",
      "ep 2619: ep_len:63 episode reward: total was 30.000000. running mean: -18.226969\n",
      "ep 2619: ep_len:2905 episode reward: total was 3.760000. running mean: -18.007099\n",
      "ep 2619: ep_len:1672 episode reward: total was -113.040000. running mean: -18.957428\n",
      "ep 2619: ep_len:176 episode reward: total was 83.500000. running mean: -17.932854\n",
      "ep 2619: ep_len:72 episode reward: total was 33.000000. running mean: -17.423525\n",
      "ep 2619: ep_len:1078 episode reward: total was -56.050000. running mean: -17.809790\n",
      "ep 2619: ep_len:331 episode reward: total was -2.260000. running mean: -17.654292\n",
      "ep 2619: ep_len:585 episode reward: total was -2.800000. running mean: -17.505749\n",
      "ep 2619: ep_len:821 episode reward: total was 30.770000. running mean: -17.022992\n",
      "ep 2619: ep_len:1561 episode reward: total was -25.560000. running mean: -17.108362\n",
      "ep 2619: ep_len:67 episode reward: total was 32.000000. running mean: -16.617278\n",
      "ep 2619: ep_len:172 episode reward: total was 81.500000. running mean: -15.636106\n",
      "ep 2619: ep_len:500 episode reward: total was 0.890000. running mean: -15.470844\n",
      "ep 2619: ep_len:2814 episode reward: total was -62.680000. running mean: -15.942936\n",
      "ep 2619: ep_len:46 episode reward: total was 21.500000. running mean: -15.568507\n",
      "epsilon:0.009992 episode_count: 39459. steps_count: 42414068.000000\n",
      "ep 2620: ep_len:500 episode reward: total was 11.760000. running mean: -15.295222\n",
      "ep 2620: ep_len:196 episode reward: total was 2.230000. running mean: -15.119969\n",
      "ep 2620: ep_len:2997 episode reward: total was -5.750000. running mean: -15.026270\n",
      "ep 2620: ep_len:783 episode reward: total was -2.310000. running mean: -14.899107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2620: ep_len:174 episode reward: total was 82.500000. running mean: -13.925116\n",
      "ep 2620: ep_len:1032 episode reward: total was -5.150000. running mean: -13.837365\n",
      "ep 2620: ep_len:338 episode reward: total was -13.790000. running mean: -13.836891\n",
      "ep 2620: ep_len:636 episode reward: total was 34.200000. running mean: -13.356522\n",
      "ep 2620: ep_len:826 episode reward: total was 39.030000. running mean: -12.832657\n",
      "ep 2620: ep_len:1530 episode reward: total was -43.090000. running mean: -13.135230\n",
      "ep 2620: ep_len:59 episode reward: total was 28.000000. running mean: -12.723878\n",
      "ep 2620: ep_len:1044 episode reward: total was 18.850000. running mean: -12.408139\n",
      "ep 2620: ep_len:2815 episode reward: total was -32.460000. running mean: -12.608658\n",
      "epsilon:0.009992 episode_count: 39472. steps_count: 42426998.000000\n",
      "ep 2621: ep_len:961 episode reward: total was -211.620000. running mean: -14.598771\n",
      "ep 2621: ep_len:500 episode reward: total was 20.820000. running mean: -14.244584\n",
      "ep 2621: ep_len:3100 episode reward: total was 3.950000. running mean: -14.062638\n",
      "ep 2621: ep_len:1617 episode reward: total was -97.980000. running mean: -14.901811\n",
      "ep 2621: ep_len:86 episode reward: total was 40.000000. running mean: -14.352793\n",
      "ep 2621: ep_len:60 episode reward: total was 28.500000. running mean: -13.924265\n",
      "ep 2621: ep_len:62 episode reward: total was 28.000000. running mean: -13.505023\n",
      "ep 2621: ep_len:1418 episode reward: total was -74.870000. running mean: -14.118672\n",
      "ep 2621: ep_len:630 episode reward: total was 8.710000. running mean: -13.890386\n",
      "ep 2621: ep_len:555 episode reward: total was -0.680000. running mean: -13.758282\n",
      "ep 2621: ep_len:679 episode reward: total was 6.870000. running mean: -13.551999\n",
      "ep 2621: ep_len:587 episode reward: total was -5.350000. running mean: -13.469979\n",
      "ep 2621: ep_len:54 episode reward: total was 25.500000. running mean: -13.080279\n",
      "ep 2621: ep_len:2418 episode reward: total was -218.390000. running mean: -15.133377\n",
      "ep 2621: ep_len:2782 episode reward: total was -5.460000. running mean: -15.036643\n",
      "epsilon:0.009992 episode_count: 39487. steps_count: 42442507.000000\n",
      "ep 2622: ep_len:1450 episode reward: total was -0.730000. running mean: -14.893576\n",
      "ep 2622: ep_len:722 episode reward: total was -9.090000. running mean: -14.835541\n",
      "ep 2622: ep_len:44 episode reward: total was 20.500000. running mean: -14.482185\n",
      "ep 2622: ep_len:3090 episode reward: total was -52.700000. running mean: -14.864363\n",
      "ep 2622: ep_len:860 episode reward: total was 11.730000. running mean: -14.598420\n",
      "ep 2622: ep_len:68 episode reward: total was 32.500000. running mean: -14.127435\n",
      "ep 2622: ep_len:104 episode reward: total was 46.000000. running mean: -13.526161\n",
      "ep 2622: ep_len:745 episode reward: total was 0.210000. running mean: -13.388800\n",
      "ep 2622: ep_len:3611 episode reward: total was -536.350000. running mean: -18.618412\n",
      "ep 2622: ep_len:727 episode reward: total was -64.250000. running mean: -19.074727\n",
      "ep 2622: ep_len:642 episode reward: total was 12.040000. running mean: -18.763580\n",
      "ep 2622: ep_len:715 episode reward: total was 4.960000. running mean: -18.526344\n",
      "ep 2622: ep_len:500 episode reward: total was 50.310000. running mean: -17.837981\n",
      "ep 2622: ep_len:2921 episode reward: total was -6.400000. running mean: -17.723601\n",
      "ep 2622: ep_len:65 episode reward: total was 29.500000. running mean: -17.251365\n",
      "epsilon:0.009992 episode_count: 39502. steps_count: 42458771.000000\n",
      "ep 2623: ep_len:698 episode reward: total was -9.370000. running mean: -17.172551\n",
      "ep 2623: ep_len:925 episode reward: total was 26.530000. running mean: -16.735526\n",
      "ep 2623: ep_len:36 episode reward: total was 16.500000. running mean: -16.403171\n",
      "ep 2623: ep_len:3017 episode reward: total was -26.890000. running mean: -16.508039\n",
      "ep 2623: ep_len:1047 episode reward: total was -9.900000. running mean: -16.441959\n",
      "ep 2623: ep_len:115 episode reward: total was 56.000000. running mean: -15.717539\n",
      "ep 2623: ep_len:595 episode reward: total was 1.740000. running mean: -15.542964\n",
      "ep 2623: ep_len:285 episode reward: total was 14.020000. running mean: -15.247334\n",
      "ep 2623: ep_len:856 episode reward: total was -32.010000. running mean: -15.414961\n",
      "ep 2623: ep_len:7369 episode reward: total was -184.050000. running mean: -17.101311\n",
      "ep 2623: ep_len:983 episode reward: total was 21.360000. running mean: -16.716698\n",
      "ep 2623: ep_len:82 episode reward: total was 38.000000. running mean: -16.169531\n",
      "ep 2623: ep_len:72 episode reward: total was 34.500000. running mean: -15.662836\n",
      "ep 2623: ep_len:574 episode reward: total was -9.580000. running mean: -15.602007\n",
      "ep 2623: ep_len:2839 episode reward: total was -13.310000. running mean: -15.579087\n",
      "epsilon:0.009992 episode_count: 39517. steps_count: 42478264.000000\n",
      "ep 2624: ep_len:1501 episode reward: total was 0.670000. running mean: -15.416596\n",
      "ep 2624: ep_len:500 episode reward: total was -3.060000. running mean: -15.293030\n",
      "ep 2624: ep_len:75 episode reward: total was 34.500000. running mean: -14.795100\n",
      "ep 2624: ep_len:2992 episode reward: total was 0.650000. running mean: -14.640649\n",
      "ep 2624: ep_len:812 episode reward: total was 8.560000. running mean: -14.408643\n",
      "ep 2624: ep_len:52 episode reward: total was 24.500000. running mean: -14.019556\n",
      "ep 2624: ep_len:744 episode reward: total was -43.720000. running mean: -14.316561\n",
      "ep 2624: ep_len:3572 episode reward: total was -218.070000. running mean: -16.354095\n",
      "ep 2624: ep_len:908 episode reward: total was -34.520000. running mean: -16.535754\n",
      "ep 2624: ep_len:836 episode reward: total was 25.720000. running mean: -16.113196\n",
      "ep 2624: ep_len:1162 episode reward: total was 5.680000. running mean: -15.895264\n",
      "ep 2624: ep_len:64 episode reward: total was 30.500000. running mean: -15.431312\n",
      "ep 2624: ep_len:1001 episode reward: total was -80.850000. running mean: -16.085499\n",
      "ep 2624: ep_len:2908 episode reward: total was -3.830000. running mean: -15.962944\n",
      "ep 2624: ep_len:47 episode reward: total was 22.000000. running mean: -15.583314\n",
      "epsilon:0.009992 episode_count: 39532. steps_count: 42495438.000000\n",
      "ep 2625: ep_len:526 episode reward: total was -12.520000. running mean: -15.552681\n",
      "ep 2625: ep_len:1237 episode reward: total was -59.510000. running mean: -15.992254\n",
      "ep 2625: ep_len:51 episode reward: total was 24.000000. running mean: -15.592332\n",
      "ep 2625: ep_len:2899 episode reward: total was 3.550000. running mean: -15.400908\n",
      "ep 2625: ep_len:555 episode reward: total was -3.710000. running mean: -15.283999\n",
      "ep 2625: ep_len:167 episode reward: total was 77.500000. running mean: -14.356159\n",
      "ep 2625: ep_len:51 episode reward: total was 22.500000. running mean: -13.987598\n",
      "ep 2625: ep_len:1028 episode reward: total was -34.330000. running mean: -14.191022\n",
      "ep 2625: ep_len:586 episode reward: total was 0.770000. running mean: -14.041412\n",
      "ep 2625: ep_len:615 episode reward: total was 17.120000. running mean: -13.729797\n",
      "ep 2625: ep_len:703 episode reward: total was -3.610000. running mean: -13.628600\n",
      "ep 2625: ep_len:500 episode reward: total was 30.900000. running mean: -13.183314\n",
      "ep 2625: ep_len:144 episode reward: total was 70.500000. running mean: -12.346480\n",
      "ep 2625: ep_len:640 episode reward: total was 0.130000. running mean: -12.221716\n",
      "ep 2625: ep_len:2839 episode reward: total was -10.400000. running mean: -12.203498\n",
      "ep 2625: ep_len:45 episode reward: total was 21.000000. running mean: -11.871463\n",
      "epsilon:0.009992 episode_count: 39548. steps_count: 42508024.000000\n",
      "ep 2626: ep_len:1188 episode reward: total was 19.070000. running mean: -11.562049\n",
      "ep 2626: ep_len:500 episode reward: total was 16.780000. running mean: -11.278628\n",
      "ep 2626: ep_len:2973 episode reward: total was -53.140000. running mean: -11.697242\n",
      "ep 2626: ep_len:506 episode reward: total was -78.940000. running mean: -12.369670\n",
      "ep 2626: ep_len:131 episode reward: total was 62.010000. running mean: -11.625873\n",
      "ep 2626: ep_len:500 episode reward: total was 34.300000. running mean: -11.166614\n",
      "ep 2626: ep_len:655 episode reward: total was 13.000000. running mean: -10.924948\n",
      "ep 2626: ep_len:3871 episode reward: total was -746.230000. running mean: -18.277999\n",
      "ep 2626: ep_len:747 episode reward: total was 7.610000. running mean: -18.019119\n",
      "ep 2626: ep_len:500 episode reward: total was -15.080000. running mean: -17.989727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2626: ep_len:88 episode reward: total was 42.500000. running mean: -17.384830\n",
      "ep 2626: ep_len:121 episode reward: total was 54.500000. running mean: -16.665982\n",
      "ep 2626: ep_len:36 episode reward: total was 16.500000. running mean: -16.334322\n",
      "ep 2626: ep_len:82 episode reward: total was 39.500000. running mean: -15.775979\n",
      "ep 2626: ep_len:825 episode reward: total was -16.580000. running mean: -15.784019\n",
      "ep 2626: ep_len:2898 episode reward: total was -1.790000. running mean: -15.644079\n",
      "epsilon:0.009992 episode_count: 39564. steps_count: 42523645.000000\n",
      "ep 2627: ep_len:710 episode reward: total was -25.660000. running mean: -15.744238\n",
      "ep 2627: ep_len:762 episode reward: total was 3.620000. running mean: -15.550596\n",
      "ep 2627: ep_len:2931 episode reward: total was -44.620000. running mean: -15.841290\n",
      "ep 2627: ep_len:509 episode reward: total was -25.380000. running mean: -15.936677\n",
      "ep 2627: ep_len:58 episode reward: total was 27.500000. running mean: -15.502310\n",
      "ep 2627: ep_len:79 episode reward: total was 38.000000. running mean: -14.967287\n",
      "ep 2627: ep_len:71 episode reward: total was 34.000000. running mean: -14.477614\n",
      "ep 2627: ep_len:1410 episode reward: total was -202.090000. running mean: -16.353738\n",
      "ep 2627: ep_len:3570 episode reward: total was -227.180000. running mean: -18.462001\n",
      "ep 2627: ep_len:1176 episode reward: total was -2.210000. running mean: -18.299481\n",
      "ep 2627: ep_len:7506 episode reward: total was 27.880000. running mean: -17.837686\n",
      "ep 2627: ep_len:1135 episode reward: total was 16.030000. running mean: -17.499009\n",
      "ep 2627: ep_len:1134 episode reward: total was -16.100000. running mean: -17.485019\n",
      "ep 2627: ep_len:2822 episode reward: total was 1.150000. running mean: -17.298669\n",
      "epsilon:0.009992 episode_count: 39578. steps_count: 42547518.000000\n",
      "ep 2628: ep_len:637 episode reward: total was -9.140000. running mean: -17.217082\n",
      "ep 2628: ep_len:215 episode reward: total was 4.410000. running mean: -17.000811\n",
      "ep 2628: ep_len:3022 episode reward: total was 9.840000. running mean: -16.732403\n",
      "ep 2628: ep_len:637 episode reward: total was -2.470000. running mean: -16.589779\n",
      "ep 2628: ep_len:55 episode reward: total was 26.000000. running mean: -16.163881\n",
      "ep 2628: ep_len:105 episode reward: total was 51.000000. running mean: -15.492242\n",
      "ep 2628: ep_len:73 episode reward: total was 35.000000. running mean: -14.987320\n",
      "ep 2628: ep_len:1160 episode reward: total was 13.550000. running mean: -14.701947\n",
      "ep 2628: ep_len:362 episode reward: total was 0.100000. running mean: -14.553927\n",
      "ep 2628: ep_len:903 episode reward: total was -25.960000. running mean: -14.667988\n",
      "ep 2628: ep_len:687 episode reward: total was 13.260000. running mean: -14.388708\n",
      "ep 2628: ep_len:971 episode reward: total was 11.350000. running mean: -14.131321\n",
      "ep 2628: ep_len:170 episode reward: total was 82.000000. running mean: -13.170008\n",
      "ep 2628: ep_len:118 episode reward: total was 57.500000. running mean: -12.463308\n",
      "ep 2628: ep_len:1105 episode reward: total was 0.780000. running mean: -12.330875\n",
      "ep 2628: ep_len:2878 episode reward: total was 8.140000. running mean: -12.126166\n",
      "epsilon:0.009992 episode_count: 39594. steps_count: 42560616.000000\n",
      "ep 2629: ep_len:644 episode reward: total was 6.020000. running mean: -11.944704\n",
      "ep 2629: ep_len:768 episode reward: total was -12.690000. running mean: -11.952157\n",
      "ep 2629: ep_len:84 episode reward: total was 40.500000. running mean: -11.427636\n",
      "ep 2629: ep_len:2936 episode reward: total was -9.450000. running mean: -11.407859\n",
      "ep 2629: ep_len:616 episode reward: total was -16.600000. running mean: -11.459781\n",
      "ep 2629: ep_len:92 episode reward: total was 44.500000. running mean: -10.900183\n",
      "ep 2629: ep_len:78 episode reward: total was 37.500000. running mean: -10.416181\n",
      "ep 2629: ep_len:683 episode reward: total was -1.320000. running mean: -10.325219\n",
      "ep 2629: ep_len:3767 episode reward: total was -185.340000. running mean: -12.075367\n",
      "ep 2629: ep_len:714 episode reward: total was -96.580000. running mean: -12.920413\n",
      "ep 2629: ep_len:7336 episode reward: total was 71.940000. running mean: -12.071809\n",
      "ep 2629: ep_len:623 episode reward: total was -18.180000. running mean: -12.132891\n",
      "ep 2629: ep_len:37 episode reward: total was 17.000000. running mean: -11.841562\n",
      "ep 2629: ep_len:73 episode reward: total was 35.000000. running mean: -11.373147\n",
      "ep 2629: ep_len:1105 episode reward: total was -1.240000. running mean: -11.271815\n",
      "ep 2629: ep_len:2858 episode reward: total was -2.100000. running mean: -11.180097\n",
      "epsilon:0.009992 episode_count: 39610. steps_count: 42583030.000000\n",
      "ep 2630: ep_len:1459 episode reward: total was -3.090000. running mean: -11.099196\n",
      "ep 2630: ep_len:647 episode reward: total was 11.930000. running mean: -10.868904\n",
      "ep 2630: ep_len:80 episode reward: total was 38.500000. running mean: -10.375215\n",
      "ep 2630: ep_len:2918 episode reward: total was -32.840000. running mean: -10.599863\n",
      "ep 2630: ep_len:873 episode reward: total was 0.950000. running mean: -10.484364\n",
      "ep 2630: ep_len:98 episode reward: total was 46.000000. running mean: -9.919521\n",
      "ep 2630: ep_len:1478 episode reward: total was 15.750000. running mean: -9.662825\n",
      "ep 2630: ep_len:632 episode reward: total was 12.030000. running mean: -9.445897\n",
      "ep 2630: ep_len:618 episode reward: total was 28.480000. running mean: -9.066638\n",
      "ep 2630: ep_len:874 episode reward: total was 62.200000. running mean: -8.353972\n",
      "ep 2630: ep_len:660 episode reward: total was 11.210000. running mean: -8.158332\n",
      "ep 2630: ep_len:709 episode reward: total was -19.050000. running mean: -8.267249\n",
      "ep 2630: ep_len:2830 episode reward: total was -3.240000. running mean: -8.216976\n",
      "epsilon:0.009992 episode_count: 39623. steps_count: 42596906.000000\n",
      "ep 2631: ep_len:618 episode reward: total was -31.360000. running mean: -8.448407\n",
      "ep 2631: ep_len:764 episode reward: total was -6.670000. running mean: -8.430622\n",
      "ep 2631: ep_len:2931 episode reward: total was -0.470000. running mean: -8.351016\n",
      "ep 2631: ep_len:641 episode reward: total was 29.310000. running mean: -7.974406\n",
      "ep 2631: ep_len:165 episode reward: total was 75.000000. running mean: -7.144662\n",
      "ep 2631: ep_len:903 episode reward: total was 55.920000. running mean: -6.514015\n",
      "ep 2631: ep_len:3670 episode reward: total was -42.880000. running mean: -6.877675\n",
      "ep 2631: ep_len:1192 episode reward: total was -9.580000. running mean: -6.904698\n",
      "ep 2631: ep_len:884 episode reward: total was 61.200000. running mean: -6.223651\n",
      "ep 2631: ep_len:1093 episode reward: total was 0.480000. running mean: -6.156615\n",
      "ep 2631: ep_len:82 episode reward: total was 39.500000. running mean: -5.700049\n",
      "ep 2631: ep_len:98 episode reward: total was 44.500000. running mean: -5.198048\n",
      "ep 2631: ep_len:595 episode reward: total was -10.040000. running mean: -5.246468\n",
      "ep 2631: ep_len:45 episode reward: total was 21.000000. running mean: -4.984003\n",
      "epsilon:0.009992 episode_count: 39637. steps_count: 42610587.000000\n",
      "ep 2632: ep_len:1419 episode reward: total was 0.730000. running mean: -4.926863\n",
      "ep 2632: ep_len:752 episode reward: total was 0.590000. running mean: -4.871695\n",
      "ep 2632: ep_len:3006 episode reward: total was -0.670000. running mean: -4.829678\n",
      "ep 2632: ep_len:558 episode reward: total was 4.630000. running mean: -4.735081\n",
      "ep 2632: ep_len:62 episode reward: total was 28.000000. running mean: -4.407730\n",
      "ep 2632: ep_len:75 episode reward: total was 36.000000. running mean: -4.003653\n",
      "ep 2632: ep_len:748 episode reward: total was -3.800000. running mean: -4.001616\n",
      "ep 2632: ep_len:3678 episode reward: total was -22.570000. running mean: -4.187300\n",
      "ep 2632: ep_len:1253 episode reward: total was -53.290000. running mean: -4.678327\n",
      "ep 2632: ep_len:710 episode reward: total was 34.130000. running mean: -4.290244\n",
      "ep 2632: ep_len:616 episode reward: total was -0.290000. running mean: -4.250241\n",
      "ep 2632: ep_len:152 episode reward: total was 71.500000. running mean: -3.492739\n",
      "ep 2632: ep_len:71 episode reward: total was 34.000000. running mean: -3.117811\n",
      "ep 2632: ep_len:1107 episode reward: total was -5.260000. running mean: -3.139233\n",
      "ep 2632: ep_len:2839 episode reward: total was -66.600000. running mean: -3.773841\n",
      "epsilon:0.009992 episode_count: 39652. steps_count: 42627633.000000\n",
      "ep 2633: ep_len:853 episode reward: total was 17.190000. running mean: -3.564203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2633: ep_len:675 episode reward: total was -17.370000. running mean: -3.702261\n",
      "ep 2633: ep_len:61 episode reward: total was 29.000000. running mean: -3.375238\n",
      "ep 2633: ep_len:3039 episode reward: total was -13.840000. running mean: -3.479886\n",
      "ep 2633: ep_len:675 episode reward: total was -7.410000. running mean: -3.519187\n",
      "ep 2633: ep_len:46 episode reward: total was 21.500000. running mean: -3.268995\n",
      "ep 2633: ep_len:153 episode reward: total was 75.000000. running mean: -2.486305\n",
      "ep 2633: ep_len:984 episode reward: total was -31.740000. running mean: -2.778842\n",
      "ep 2633: ep_len:3625 episode reward: total was -25.090000. running mean: -3.001953\n",
      "ep 2633: ep_len:1224 episode reward: total was 17.070000. running mean: -2.801234\n",
      "ep 2633: ep_len:781 episode reward: total was 27.580000. running mean: -2.497422\n",
      "ep 2633: ep_len:938 episode reward: total was -98.860000. running mean: -3.461047\n",
      "ep 2633: ep_len:629 episode reward: total was -22.160000. running mean: -3.648037\n",
      "ep 2633: ep_len:2898 episode reward: total was -15.020000. running mean: -3.761757\n",
      "epsilon:0.009992 episode_count: 39666. steps_count: 42644214.000000\n",
      "ep 2634: ep_len:579 episode reward: total was -27.710000. running mean: -4.001239\n",
      "ep 2634: ep_len:1622 episode reward: total was -50.820000. running mean: -4.469427\n",
      "ep 2634: ep_len:37 episode reward: total was 17.000000. running mean: -4.254732\n",
      "ep 2634: ep_len:2999 episode reward: total was -21.640000. running mean: -4.428585\n",
      "ep 2634: ep_len:621 episode reward: total was -3.050000. running mean: -4.414799\n",
      "ep 2634: ep_len:31 episode reward: total was 14.000000. running mean: -4.230651\n",
      "ep 2634: ep_len:1446 episode reward: total was -244.270000. running mean: -6.631045\n",
      "ep 2634: ep_len:621 episode reward: total was 27.380000. running mean: -6.290934\n",
      "ep 2634: ep_len:928 episode reward: total was -25.720000. running mean: -6.485225\n",
      "ep 2634: ep_len:592 episode reward: total was 1.220000. running mean: -6.408173\n",
      "ep 2634: ep_len:961 episode reward: total was -34.510000. running mean: -6.689191\n",
      "ep 2634: ep_len:500 episode reward: total was 7.750000. running mean: -6.544799\n",
      "ep 2634: ep_len:2743 episode reward: total was -11.880000. running mean: -6.598151\n",
      "epsilon:0.009992 episode_count: 39679. steps_count: 42657894.000000\n",
      "ep 2635: ep_len:1420 episode reward: total was 26.800000. running mean: -6.264169\n",
      "ep 2635: ep_len:777 episode reward: total was -9.110000. running mean: -6.292628\n",
      "ep 2635: ep_len:3041 episode reward: total was -23.520000. running mean: -6.464901\n",
      "ep 2635: ep_len:1224 episode reward: total was -50.550000. running mean: -6.905752\n",
      "ep 2635: ep_len:66 episode reward: total was 31.500000. running mean: -6.521695\n",
      "ep 2635: ep_len:72 episode reward: total was 31.500000. running mean: -6.141478\n",
      "ep 2635: ep_len:626 episode reward: total was 15.250000. running mean: -5.927563\n",
      "ep 2635: ep_len:3852 episode reward: total was -3.090000. running mean: -5.899188\n",
      "ep 2635: ep_len:912 episode reward: total was -3.570000. running mean: -5.875896\n",
      "ep 2635: ep_len:648 episode reward: total was 10.600000. running mean: -5.711137\n",
      "ep 2635: ep_len:627 episode reward: total was 3.460000. running mean: -5.619425\n",
      "ep 2635: ep_len:64 episode reward: total was 27.500000. running mean: -5.288231\n",
      "ep 2635: ep_len:110 episode reward: total was 53.500000. running mean: -4.700349\n",
      "ep 2635: ep_len:45 episode reward: total was 21.000000. running mean: -4.443345\n",
      "ep 2635: ep_len:784 episode reward: total was -29.700000. running mean: -4.695912\n",
      "ep 2635: ep_len:2825 episode reward: total was -8.460000. running mean: -4.733553\n",
      "epsilon:0.009992 episode_count: 39695. steps_count: 42674987.000000\n",
      "ep 2636: ep_len:895 episode reward: total was -2.280000. running mean: -4.709017\n",
      "ep 2636: ep_len:927 episode reward: total was -113.830000. running mean: -5.800227\n",
      "ep 2636: ep_len:2939 episode reward: total was -17.730000. running mean: -5.919525\n",
      "ep 2636: ep_len:699 episode reward: total was -9.500000. running mean: -5.955330\n",
      "ep 2636: ep_len:100 episode reward: total was 48.500000. running mean: -5.410776\n",
      "ep 2636: ep_len:68 episode reward: total was 32.500000. running mean: -5.031668\n",
      "ep 2636: ep_len:825 episode reward: total was 16.070000. running mean: -4.820652\n",
      "ep 2636: ep_len:3812 episode reward: total was -100.510000. running mean: -5.777545\n",
      "ep 2636: ep_len:1257 episode reward: total was -49.450000. running mean: -6.214270\n",
      "ep 2636: ep_len:657 episode reward: total was -3.670000. running mean: -6.188827\n",
      "ep 2636: ep_len:1542 episode reward: total was -12.390000. running mean: -6.250839\n",
      "ep 2636: ep_len:85 episode reward: total was 39.500000. running mean: -5.793330\n",
      "ep 2636: ep_len:1012 episode reward: total was -14.900000. running mean: -5.884397\n",
      "ep 2636: ep_len:2814 episode reward: total was -2.700000. running mean: -5.852553\n",
      "ep 2636: ep_len:40 episode reward: total was 18.500000. running mean: -5.609028\n",
      "epsilon:0.009992 episode_count: 39710. steps_count: 42692659.000000\n",
      "ep 2637: ep_len:1162 episode reward: total was -25.490000. running mean: -5.807837\n",
      "ep 2637: ep_len:1599 episode reward: total was -49.250000. running mean: -6.242259\n",
      "ep 2637: ep_len:84 episode reward: total was 39.000000. running mean: -5.789836\n",
      "ep 2637: ep_len:2946 episode reward: total was -28.280000. running mean: -6.014738\n",
      "ep 2637: ep_len:557 episode reward: total was -12.780000. running mean: -6.082391\n",
      "ep 2637: ep_len:145 episode reward: total was 69.500000. running mean: -5.326567\n",
      "ep 2637: ep_len:500 episode reward: total was 41.460000. running mean: -4.858701\n",
      "ep 2637: ep_len:4176 episode reward: total was -95.560000. running mean: -5.765714\n",
      "ep 2637: ep_len:2254 episode reward: total was -712.920000. running mean: -12.837257\n",
      "ep 2637: ep_len:673 episode reward: total was 19.730000. running mean: -12.511584\n",
      "ep 2637: ep_len:818 episode reward: total was 16.490000. running mean: -12.221569\n",
      "ep 2637: ep_len:37 episode reward: total was 17.000000. running mean: -11.929353\n",
      "ep 2637: ep_len:707 episode reward: total was -66.830000. running mean: -12.478359\n",
      "ep 2637: ep_len:2784 episode reward: total was -12.700000. running mean: -12.480576\n",
      "epsilon:0.009992 episode_count: 39724. steps_count: 42711101.000000\n",
      "ep 2638: ep_len:931 episode reward: total was -138.160000. running mean: -13.737370\n",
      "ep 2638: ep_len:805 episode reward: total was -7.510000. running mean: -13.675096\n",
      "ep 2638: ep_len:2996 episode reward: total was -42.400000. running mean: -13.962345\n",
      "ep 2638: ep_len:896 episode reward: total was 22.340000. running mean: -13.599322\n",
      "ep 2638: ep_len:114 episode reward: total was 55.500000. running mean: -12.908329\n",
      "ep 2638: ep_len:110 episode reward: total was 53.500000. running mean: -12.244245\n",
      "ep 2638: ep_len:46 episode reward: total was 21.500000. running mean: -11.906803\n",
      "ep 2638: ep_len:1861 episode reward: total was -59.170000. running mean: -12.379435\n",
      "ep 2638: ep_len:323 episode reward: total was 15.870000. running mean: -12.096941\n",
      "ep 2638: ep_len:1170 episode reward: total was -39.310000. running mean: -12.369071\n",
      "ep 2638: ep_len:824 episode reward: total was 52.570000. running mean: -11.719680\n",
      "ep 2638: ep_len:653 episode reward: total was -21.300000. running mean: -11.815484\n",
      "ep 2638: ep_len:56 episode reward: total was 25.000000. running mean: -11.447329\n",
      "ep 2638: ep_len:182 episode reward: total was 82.000000. running mean: -10.512855\n",
      "ep 2638: ep_len:1099 episode reward: total was 1.430000. running mean: -10.393427\n",
      "ep 2638: ep_len:38 episode reward: total was 17.500000. running mean: -10.114493\n",
      "ep 2638: ep_len:45 episode reward: total was 18.000000. running mean: -9.833348\n",
      "epsilon:0.009992 episode_count: 39741. steps_count: 42723250.000000\n",
      "ep 2639: ep_len:500 episode reward: total was 21.160000. running mean: -9.523414\n",
      "ep 2639: ep_len:1007 episode reward: total was 32.890000. running mean: -9.099280\n",
      "ep 2639: ep_len:3018 episode reward: total was 4.990000. running mean: -8.958387\n",
      "ep 2639: ep_len:626 episode reward: total was -29.670000. running mean: -9.165503\n",
      "ep 2639: ep_len:46 episode reward: total was 20.000000. running mean: -8.873848\n",
      "ep 2639: ep_len:1031 episode reward: total was -41.800000. running mean: -9.203110\n",
      "ep 2639: ep_len:3968 episode reward: total was -82.300000. running mean: -9.934079\n",
      "ep 2639: ep_len:500 episode reward: total was 35.150000. running mean: -9.483238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2639: ep_len:786 episode reward: total was -63.940000. running mean: -10.027806\n",
      "ep 2639: ep_len:622 episode reward: total was 0.010000. running mean: -9.927428\n",
      "ep 2639: ep_len:99 episode reward: total was 46.500000. running mean: -9.363153\n",
      "ep 2639: ep_len:495 episode reward: total was 44.810000. running mean: -8.821422\n",
      "ep 2639: ep_len:2863 episode reward: total was -0.490000. running mean: -8.738108\n",
      "epsilon:0.009992 episode_count: 39754. steps_count: 42738811.000000\n",
      "ep 2640: ep_len:1453 episode reward: total was 11.790000. running mean: -8.532826\n",
      "ep 2640: ep_len:1644 episode reward: total was -46.290000. running mean: -8.910398\n",
      "ep 2640: ep_len:3036 episode reward: total was -8.720000. running mean: -8.908494\n",
      "ep 2640: ep_len:1201 episode reward: total was -46.740000. running mean: -9.286809\n",
      "ep 2640: ep_len:158 episode reward: total was 74.500000. running mean: -8.448941\n",
      "ep 2640: ep_len:43 episode reward: total was 20.000000. running mean: -8.164452\n",
      "ep 2640: ep_len:749 episode reward: total was 30.600000. running mean: -7.776807\n",
      "ep 2640: ep_len:4144 episode reward: total was -261.420000. running mean: -10.313239\n",
      "ep 2640: ep_len:967 episode reward: total was -20.210000. running mean: -10.412207\n",
      "ep 2640: ep_len:7374 episode reward: total was -36.030000. running mean: -10.668385\n",
      "ep 2640: ep_len:705 episode reward: total was -2.210000. running mean: -10.583801\n",
      "ep 2640: ep_len:181 episode reward: total was 87.500000. running mean: -9.602963\n",
      "ep 2640: ep_len:67 episode reward: total was 29.000000. running mean: -9.216933\n",
      "ep 2640: ep_len:658 episode reward: total was -22.880000. running mean: -9.353564\n",
      "ep 2640: ep_len:2812 episode reward: total was 7.330000. running mean: -9.186728\n",
      "epsilon:0.009992 episode_count: 39769. steps_count: 42764003.000000\n",
      "ep 2641: ep_len:942 episode reward: total was -41.980000. running mean: -9.514661\n",
      "ep 2641: ep_len:1619 episode reward: total was -45.590000. running mean: -9.875414\n",
      "ep 2641: ep_len:2854 episode reward: total was -77.130000. running mean: -10.547960\n",
      "ep 2641: ep_len:626 episode reward: total was 7.640000. running mean: -10.366081\n",
      "ep 2641: ep_len:84 episode reward: total was 39.000000. running mean: -9.872420\n",
      "ep 2641: ep_len:106 episode reward: total was 51.500000. running mean: -9.258696\n",
      "ep 2641: ep_len:46 episode reward: total was 21.500000. running mean: -8.951109\n",
      "ep 2641: ep_len:641 episode reward: total was 1.950000. running mean: -8.842098\n",
      "ep 2641: ep_len:330 episode reward: total was 17.680000. running mean: -8.576877\n",
      "ep 2641: ep_len:939 episode reward: total was -8.250000. running mean: -8.573608\n",
      "ep 2641: ep_len:827 episode reward: total was 28.050000. running mean: -8.207372\n",
      "ep 2641: ep_len:618 episode reward: total was 27.430000. running mean: -7.850998\n",
      "ep 2641: ep_len:81 episode reward: total was 39.000000. running mean: -7.382488\n",
      "ep 2641: ep_len:500 episode reward: total was 36.930000. running mean: -6.939363\n",
      "ep 2641: ep_len:2867 episode reward: total was -0.450000. running mean: -6.874470\n",
      "epsilon:0.009992 episode_count: 39784. steps_count: 42777083.000000\n",
      "ep 2642: ep_len:647 episode reward: total was -13.130000. running mean: -6.937025\n",
      "ep 2642: ep_len:500 episode reward: total was 14.480000. running mean: -6.722855\n",
      "ep 2642: ep_len:3075 episode reward: total was -19.600000. running mean: -6.851626\n",
      "ep 2642: ep_len:500 episode reward: total was 1.350000. running mean: -6.769610\n",
      "ep 2642: ep_len:862 episode reward: total was 35.570000. running mean: -6.346214\n",
      "ep 2642: ep_len:3650 episode reward: total was -31.450000. running mean: -6.597252\n",
      "ep 2642: ep_len:1231 episode reward: total was -0.490000. running mean: -6.536179\n",
      "ep 2642: ep_len:836 episode reward: total was 59.890000. running mean: -5.871917\n",
      "ep 2642: ep_len:603 episode reward: total was -14.340000. running mean: -5.956598\n",
      "ep 2642: ep_len:35 episode reward: total was 14.500000. running mean: -5.752032\n",
      "ep 2642: ep_len:502 episode reward: total was 17.600000. running mean: -5.518512\n",
      "ep 2642: ep_len:2757 episode reward: total was -12.320000. running mean: -5.586527\n",
      "epsilon:0.009992 episode_count: 39796. steps_count: 42792281.000000\n",
      "ep 2643: ep_len:1008 episode reward: total was -41.850000. running mean: -5.949161\n",
      "ep 2643: ep_len:181 episode reward: total was 14.690000. running mean: -5.742770\n",
      "ep 2643: ep_len:3040 episode reward: total was -4.330000. running mean: -5.728642\n",
      "ep 2643: ep_len:652 episode reward: total was 9.490000. running mean: -5.576456\n",
      "ep 2643: ep_len:58 episode reward: total was 27.500000. running mean: -5.245691\n",
      "ep 2643: ep_len:1454 episode reward: total was 11.860000. running mean: -5.074634\n",
      "ep 2643: ep_len:327 episode reward: total was 29.010000. running mean: -4.733788\n",
      "ep 2643: ep_len:543 episode reward: total was 17.880000. running mean: -4.507650\n",
      "ep 2643: ep_len:708 episode reward: total was 8.170000. running mean: -4.380873\n",
      "ep 2643: ep_len:1502 episode reward: total was 8.050000. running mean: -4.256565\n",
      "ep 2643: ep_len:150 episode reward: total was 73.500000. running mean: -3.478999\n",
      "ep 2643: ep_len:66 episode reward: total was 30.000000. running mean: -3.144209\n",
      "ep 2643: ep_len:1076 episode reward: total was 12.950000. running mean: -2.983267\n",
      "ep 2643: ep_len:2824 episode reward: total was -113.180000. running mean: -4.085234\n",
      "ep 2643: ep_len:53 episode reward: total was 20.500000. running mean: -3.839382\n",
      "epsilon:0.009992 episode_count: 39811. steps_count: 42805923.000000\n",
      "ep 2644: ep_len:1435 episode reward: total was 14.110000. running mean: -3.659888\n",
      "ep 2644: ep_len:3517 episode reward: total was -333.050000. running mean: -6.953789\n",
      "ep 2644: ep_len:69 episode reward: total was 31.500000. running mean: -6.569251\n",
      "ep 2644: ep_len:2979 episode reward: total was -22.360000. running mean: -6.727159\n",
      "ep 2644: ep_len:669 episode reward: total was 4.310000. running mean: -6.616787\n",
      "ep 2644: ep_len:147 episode reward: total was 70.500000. running mean: -5.845619\n",
      "ep 2644: ep_len:99 episode reward: total was 46.500000. running mean: -5.322163\n",
      "ep 2644: ep_len:697 episode reward: total was -8.840000. running mean: -5.357342\n",
      "ep 2644: ep_len:586 episode reward: total was 23.540000. running mean: -5.068368\n",
      "ep 2644: ep_len:947 episode reward: total was -28.070000. running mean: -5.298385\n",
      "ep 2644: ep_len:754 episode reward: total was -13.100000. running mean: -5.376401\n",
      "ep 2644: ep_len:1063 episode reward: total was 6.730000. running mean: -5.255337\n",
      "ep 2644: ep_len:1132 episode reward: total was -4.000000. running mean: -5.242783\n",
      "ep 2644: ep_len:2802 episode reward: total was -16.830000. running mean: -5.358655\n",
      "epsilon:0.009992 episode_count: 39825. steps_count: 42822819.000000\n",
      "ep 2645: ep_len:1127 episode reward: total was -13.170000. running mean: -5.436769\n",
      "ep 2645: ep_len:807 episode reward: total was 1.140000. running mean: -5.371001\n",
      "ep 2645: ep_len:2998 episode reward: total was -14.010000. running mean: -5.457391\n",
      "ep 2645: ep_len:566 episode reward: total was -5.620000. running mean: -5.459017\n",
      "ep 2645: ep_len:108 episode reward: total was 49.500000. running mean: -4.909427\n",
      "ep 2645: ep_len:625 episode reward: total was 45.570000. running mean: -4.404633\n",
      "ep 2645: ep_len:690 episode reward: total was 38.050000. running mean: -3.980087\n",
      "ep 2645: ep_len:861 episode reward: total was 33.570000. running mean: -3.604586\n",
      "ep 2645: ep_len:705 episode reward: total was 54.290000. running mean: -3.025640\n",
      "ep 2645: ep_len:640 episode reward: total was 11.000000. running mean: -2.885383\n",
      "ep 2645: ep_len:67 episode reward: total was 32.000000. running mean: -2.536530\n",
      "ep 2645: ep_len:501 episode reward: total was 5.120000. running mean: -2.459964\n",
      "ep 2645: ep_len:2832 episode reward: total was -37.330000. running mean: -2.808665\n",
      "ep 2645: ep_len:67 episode reward: total was 32.000000. running mean: -2.460578\n",
      "epsilon:0.009992 episode_count: 39839. steps_count: 42835413.000000\n",
      "ep 2646: ep_len:1527 episode reward: total was 5.000000. running mean: -2.385972\n",
      "ep 2646: ep_len:651 episode reward: total was -48.200000. running mean: -2.844112\n",
      "ep 2646: ep_len:55 episode reward: total was 26.000000. running mean: -2.555671\n",
      "ep 2646: ep_len:2992 episode reward: total was -50.370000. running mean: -3.033815\n",
      "ep 2646: ep_len:2729 episode reward: total was -210.790000. running mean: -5.111376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2646: ep_len:55 episode reward: total was 26.000000. running mean: -4.800263\n",
      "ep 2646: ep_len:148 episode reward: total was 72.500000. running mean: -4.027260\n",
      "ep 2646: ep_len:699 episode reward: total was -3.280000. running mean: -4.019788\n",
      "ep 2646: ep_len:3662 episode reward: total was -65.180000. running mean: -4.631390\n",
      "ep 2646: ep_len:500 episode reward: total was 36.160000. running mean: -4.223476\n",
      "ep 2646: ep_len:7311 episode reward: total was -49.700000. running mean: -4.678241\n",
      "ep 2646: ep_len:929 episode reward: total was 56.310000. running mean: -4.068359\n",
      "ep 2646: ep_len:69 episode reward: total was 33.000000. running mean: -3.697675\n",
      "ep 2646: ep_len:37 episode reward: total was 15.010000. running mean: -3.510598\n",
      "ep 2646: ep_len:674 episode reward: total was 6.620000. running mean: -3.409292\n",
      "ep 2646: ep_len:2793 episode reward: total was -26.590000. running mean: -3.641099\n",
      "ep 2646: ep_len:53 episode reward: total was 25.000000. running mean: -3.354688\n",
      "epsilon:0.009992 episode_count: 39856. steps_count: 42860297.000000\n",
      "ep 2647: ep_len:793 episode reward: total was -68.480000. running mean: -4.005941\n",
      "ep 2647: ep_len:784 episode reward: total was -12.530000. running mean: -4.091182\n",
      "ep 2647: ep_len:2915 episode reward: total was -55.460000. running mean: -4.604870\n",
      "ep 2647: ep_len:593 episode reward: total was -2.320000. running mean: -4.582022\n",
      "ep 2647: ep_len:107 episode reward: total was 50.500000. running mean: -4.031201\n",
      "ep 2647: ep_len:72 episode reward: total was 34.500000. running mean: -3.645889\n",
      "ep 2647: ep_len:1035 episode reward: total was -24.420000. running mean: -3.853630\n",
      "ep 2647: ep_len:3547 episode reward: total was -2.700000. running mean: -3.842094\n",
      "ep 2647: ep_len:1262 episode reward: total was -30.980000. running mean: -4.113473\n",
      "ep 2647: ep_len:809 episode reward: total was 24.800000. running mean: -3.824338\n",
      "ep 2647: ep_len:940 episode reward: total was 10.550000. running mean: -3.680595\n",
      "ep 2647: ep_len:743 episode reward: total was -16.950000. running mean: -3.813289\n",
      "ep 2647: ep_len:2800 episode reward: total was -27.380000. running mean: -4.048956\n",
      "ep 2647: ep_len:67 episode reward: total was 30.500000. running mean: -3.703467\n",
      "epsilon:0.009992 episode_count: 39870. steps_count: 42876764.000000\n",
      "ep 2648: ep_len:1069 episode reward: total was -19.780000. running mean: -3.864232\n",
      "ep 2648: ep_len:745 episode reward: total was 2.050000. running mean: -3.805090\n",
      "ep 2648: ep_len:100 episode reward: total was 47.000000. running mean: -3.297039\n",
      "ep 2648: ep_len:500 episode reward: total was 11.790000. running mean: -3.146168\n",
      "ep 2648: ep_len:500 episode reward: total was 11.490000. running mean: -2.999807\n",
      "ep 2648: ep_len:317 episode reward: total was 19.050000. running mean: -2.779309\n",
      "ep 2648: ep_len:543 episode reward: total was 3.670000. running mean: -2.714816\n",
      "ep 2648: ep_len:7234 episode reward: total was -49.300000. running mean: -3.180667\n",
      "ep 2648: ep_len:631 episode reward: total was -3.430000. running mean: -3.183161\n",
      "ep 2648: ep_len:49 episode reward: total was 21.500000. running mean: -2.936329\n",
      "ep 2648: ep_len:683 episode reward: total was -38.360000. running mean: -3.290566\n",
      "ep 2648: ep_len:2906 episode reward: total was -6.820000. running mean: -3.325860\n",
      "ep 2648: ep_len:58 episode reward: total was 27.500000. running mean: -3.017602\n",
      "epsilon:0.009992 episode_count: 39883. steps_count: 42892099.000000\n",
      "ep 2649: ep_len:1439 episode reward: total was -9.230000. running mean: -3.079726\n",
      "ep 2649: ep_len:1807 episode reward: total was -285.120000. running mean: -5.900128\n",
      "ep 2649: ep_len:59 episode reward: total was 26.500000. running mean: -5.576127\n",
      "ep 2649: ep_len:2871 episode reward: total was -66.770000. running mean: -6.188066\n",
      "ep 2649: ep_len:500 episode reward: total was -70.200000. running mean: -6.828185\n",
      "ep 2649: ep_len:59 episode reward: total was 26.500000. running mean: -6.494903\n",
      "ep 2649: ep_len:73 episode reward: total was 35.000000. running mean: -6.079954\n",
      "ep 2649: ep_len:921 episode reward: total was 49.490000. running mean: -5.524255\n",
      "ep 2649: ep_len:4017 episode reward: total was -442.200000. running mean: -9.891012\n",
      "ep 2649: ep_len:611 episode reward: total was 36.580000. running mean: -9.426302\n",
      "ep 2649: ep_len:845 episode reward: total was 21.090000. running mean: -9.121139\n",
      "ep 2649: ep_len:500 episode reward: total was 28.230000. running mean: -8.747628\n",
      "ep 2649: ep_len:56 episode reward: total was 26.500000. running mean: -8.395151\n",
      "ep 2649: ep_len:46 episode reward: total was 21.500000. running mean: -8.096200\n",
      "ep 2649: ep_len:806 episode reward: total was -17.320000. running mean: -8.188438\n",
      "ep 2649: ep_len:2854 episode reward: total was -28.370000. running mean: -8.390253\n",
      "epsilon:0.009992 episode_count: 39899. steps_count: 42909563.000000\n",
      "ep 2650: ep_len:600 episode reward: total was 14.270000. running mean: -8.163651\n",
      "ep 2650: ep_len:1238 episode reward: total was -53.440000. running mean: -8.616414\n",
      "ep 2650: ep_len:51 episode reward: total was 24.000000. running mean: -8.290250\n",
      "ep 2650: ep_len:2936 episode reward: total was -27.340000. running mean: -8.480748\n",
      "ep 2650: ep_len:500 episode reward: total was -16.130000. running mean: -8.557240\n",
      "ep 2650: ep_len:500 episode reward: total was 17.730000. running mean: -8.294368\n",
      "ep 2650: ep_len:3628 episode reward: total was -85.080000. running mean: -9.062224\n",
      "ep 2650: ep_len:780 episode reward: total was -31.310000. running mean: -9.284702\n",
      "ep 2650: ep_len:775 episode reward: total was 4.160000. running mean: -9.150255\n",
      "ep 2650: ep_len:1135 episode reward: total was -15.080000. running mean: -9.209552\n",
      "ep 2650: ep_len:97 episode reward: total was 45.500000. running mean: -8.662457\n",
      "ep 2650: ep_len:609 episode reward: total was 6.710000. running mean: -8.508732\n",
      "ep 2650: ep_len:2792 episode reward: total was -2.920000. running mean: -8.452845\n",
      "ep 2650: ep_len:54 episode reward: total was 25.500000. running mean: -8.113316\n",
      "epsilon:0.009992 episode_count: 39913. steps_count: 42925258.000000\n",
      "ep 2651: ep_len:1421 episode reward: total was -2.950000. running mean: -8.061683\n",
      "ep 2651: ep_len:697 episode reward: total was -37.140000. running mean: -8.352466\n",
      "ep 2651: ep_len:3072 episode reward: total was -116.070000. running mean: -9.429642\n",
      "ep 2651: ep_len:670 episode reward: total was 5.630000. running mean: -9.279045\n",
      "ep 2651: ep_len:76 episode reward: total was 35.000000. running mean: -8.836255\n",
      "ep 2651: ep_len:93 episode reward: total was 43.500000. running mean: -8.312892\n",
      "ep 2651: ep_len:897 episode reward: total was 51.240000. running mean: -7.717363\n",
      "ep 2651: ep_len:338 episode reward: total was 21.070000. running mean: -7.429490\n",
      "ep 2651: ep_len:762 episode reward: total was -12.180000. running mean: -7.476995\n",
      "ep 2651: ep_len:773 episode reward: total was 29.090000. running mean: -7.111325\n",
      "ep 2651: ep_len:500 episode reward: total was 17.240000. running mean: -6.867812\n",
      "ep 2651: ep_len:143 episode reward: total was 67.000000. running mean: -6.129134\n",
      "ep 2651: ep_len:615 episode reward: total was -19.330000. running mean: -6.261142\n",
      "ep 2651: ep_len:2856 episode reward: total was -39.340000. running mean: -6.591931\n",
      "ep 2651: ep_len:42 episode reward: total was 18.000000. running mean: -6.346012\n",
      "epsilon:0.009992 episode_count: 39928. steps_count: 42938213.000000\n",
      "ep 2652: ep_len:654 episode reward: total was -35.040000. running mean: -6.632951\n",
      "ep 2652: ep_len:999 episode reward: total was 30.490000. running mean: -6.261722\n",
      "ep 2652: ep_len:47 episode reward: total was 22.000000. running mean: -5.979105\n",
      "ep 2652: ep_len:2941 episode reward: total was -55.110000. running mean: -6.470414\n",
      "ep 2652: ep_len:535 episode reward: total was -22.090000. running mean: -6.626610\n",
      "ep 2652: ep_len:64 episode reward: total was 27.500000. running mean: -6.285343\n",
      "ep 2652: ep_len:500 episode reward: total was 8.880000. running mean: -6.133690\n",
      "ep 2652: ep_len:658 episode reward: total was 23.250000. running mean: -5.839853\n",
      "ep 2652: ep_len:640 episode reward: total was 25.760000. running mean: -5.523855\n",
      "ep 2652: ep_len:651 episode reward: total was 29.890000. running mean: -5.169716\n",
      "ep 2652: ep_len:801 episode reward: total was -19.430000. running mean: -5.312319\n",
      "ep 2652: ep_len:91 episode reward: total was 44.000000. running mean: -4.819196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2652: ep_len:172 episode reward: total was 83.000000. running mean: -3.941004\n",
      "ep 2652: ep_len:813 episode reward: total was -18.760000. running mean: -4.089194\n",
      "ep 2652: ep_len:2852 episode reward: total was 5.770000. running mean: -3.990602\n",
      "epsilon:0.009992 episode_count: 39943. steps_count: 42950631.000000\n",
      "ep 2653: ep_len:624 episode reward: total was -1.220000. running mean: -3.962896\n",
      "ep 2653: ep_len:672 episode reward: total was -42.940000. running mean: -4.352667\n",
      "ep 2653: ep_len:2960 episode reward: total was -86.290000. running mean: -5.172040\n",
      "ep 2653: ep_len:1428 episode reward: total was -6.430000. running mean: -5.184620\n",
      "ep 2653: ep_len:51 episode reward: total was 24.000000. running mean: -4.892773\n",
      "ep 2653: ep_len:79 episode reward: total was 36.500000. running mean: -4.478846\n",
      "ep 2653: ep_len:854 episode reward: total was 28.570000. running mean: -4.148357\n",
      "ep 2653: ep_len:3673 episode reward: total was -84.260000. running mean: -4.949474\n",
      "ep 2653: ep_len:1253 episode reward: total was -73.090000. running mean: -5.630879\n",
      "ep 2653: ep_len:561 episode reward: total was 5.750000. running mean: -5.517070\n",
      "ep 2653: ep_len:1406 episode reward: total was 3.480000. running mean: -5.427099\n",
      "ep 2653: ep_len:951 episode reward: total was -96.710000. running mean: -6.339929\n",
      "ep 2653: ep_len:2917 episode reward: total was 2.040000. running mean: -6.256129\n",
      "epsilon:0.009992 episode_count: 39956. steps_count: 42968060.000000\n",
      "ep 2654: ep_len:1402 episode reward: total was 20.920000. running mean: -5.984368\n",
      "ep 2654: ep_len:183 episode reward: total was 6.690000. running mean: -5.857624\n",
      "ep 2654: ep_len:2916 episode reward: total was -34.610000. running mean: -6.145148\n",
      "ep 2654: ep_len:685 episode reward: total was -7.460000. running mean: -6.158297\n",
      "ep 2654: ep_len:144 episode reward: total was 69.000000. running mean: -5.406714\n",
      "ep 2654: ep_len:724 episode reward: total was 2.020000. running mean: -5.332446\n",
      "ep 2654: ep_len:3636 episode reward: total was -27.060000. running mean: -5.549722\n",
      "ep 2654: ep_len:1268 episode reward: total was -70.310000. running mean: -6.197325\n",
      "ep 2654: ep_len:810 episode reward: total was 22.850000. running mean: -5.906851\n",
      "ep 2654: ep_len:803 episode reward: total was 16.410000. running mean: -5.683683\n",
      "ep 2654: ep_len:201 episode reward: total was 97.500000. running mean: -4.651846\n",
      "ep 2654: ep_len:47 episode reward: total was 20.500000. running mean: -4.400328\n",
      "ep 2654: ep_len:500 episode reward: total was 26.120000. running mean: -4.095124\n",
      "ep 2654: ep_len:2858 episode reward: total was -13.030000. running mean: -4.184473\n",
      "epsilon:0.009992 episode_count: 39970. steps_count: 42984237.000000\n",
      "ep 2655: ep_len:672 episode reward: total was 20.930000. running mean: -3.933328\n",
      "ep 2655: ep_len:731 episode reward: total was -17.300000. running mean: -4.066995\n",
      "ep 2655: ep_len:3044 episode reward: total was -34.840000. running mean: -4.374725\n",
      "ep 2655: ep_len:614 episode reward: total was -5.140000. running mean: -4.382378\n",
      "ep 2655: ep_len:127 episode reward: total was 57.500000. running mean: -3.763554\n",
      "ep 2655: ep_len:57 episode reward: total was 27.000000. running mean: -3.455919\n",
      "ep 2655: ep_len:500 episode reward: total was 25.420000. running mean: -3.167159\n",
      "ep 2655: ep_len:349 episode reward: total was 16.710000. running mean: -2.968388\n",
      "ep 2655: ep_len:1288 episode reward: total was -81.220000. running mean: -3.750904\n",
      "ep 2655: ep_len:764 episode reward: total was 41.290000. running mean: -3.300495\n",
      "ep 2655: ep_len:1458 episode reward: total was 1.370000. running mean: -3.253790\n",
      "ep 2655: ep_len:101 episode reward: total was 49.000000. running mean: -2.731252\n",
      "ep 2655: ep_len:36 episode reward: total was 16.500000. running mean: -2.538940\n",
      "ep 2655: ep_len:760 episode reward: total was -30.950000. running mean: -2.823050\n",
      "ep 2655: ep_len:2850 episode reward: total was -14.090000. running mean: -2.935720\n",
      "ep 2655: ep_len:46 episode reward: total was 20.000000. running mean: -2.706362\n",
      "epsilon:0.009992 episode_count: 39986. steps_count: 42997634.000000\n",
      "ep 2656: ep_len:1123 episode reward: total was -17.220000. running mean: -2.851499\n",
      "ep 2656: ep_len:899 episode reward: total was 21.250000. running mean: -2.610484\n",
      "ep 2656: ep_len:59 episode reward: total was 16.000000. running mean: -2.424379\n",
      "ep 2656: ep_len:3044 episode reward: total was -39.970000. running mean: -2.799835\n",
      "ep 2656: ep_len:827 episode reward: total was 49.170000. running mean: -2.280137\n",
      "ep 2656: ep_len:57 episode reward: total was 24.000000. running mean: -2.017335\n",
      "ep 2656: ep_len:71 episode reward: total was 32.500000. running mean: -1.672162\n",
      "ep 2656: ep_len:78 episode reward: total was 37.500000. running mean: -1.280441\n",
      "ep 2656: ep_len:1497 episode reward: total was -207.990000. running mean: -3.347536\n",
      "ep 2656: ep_len:630 episode reward: total was 35.950000. running mean: -2.954561\n",
      "ep 2656: ep_len:690 episode reward: total was -41.750000. running mean: -3.342515\n",
      "ep 2656: ep_len:762 episode reward: total was -11.280000. running mean: -3.421890\n",
      "ep 2656: ep_len:772 episode reward: total was 27.860000. running mean: -3.109071\n",
      "ep 2656: ep_len:40 episode reward: total was 18.500000. running mean: -2.892980\n",
      "ep 2656: ep_len:137 episode reward: total was 65.500000. running mean: -2.209051\n",
      "ep 2656: ep_len:66 episode reward: total was 31.500000. running mean: -1.871960\n",
      "ep 2656: ep_len:1090 episode reward: total was -9.470000. running mean: -1.947940\n",
      "ep 2656: ep_len:2818 episode reward: total was -15.110000. running mean: -2.079561\n",
      "epsilon:0.009992 episode_count: 40004. steps_count: 43012294.000000\n",
      "ep 2657: ep_len:1049 episode reward: total was -9.880000. running mean: -2.157565\n",
      "ep 2657: ep_len:714 episode reward: total was -32.560000. running mean: -2.461590\n",
      "ep 2657: ep_len:2979 episode reward: total was -54.780000. running mean: -2.984774\n",
      "ep 2657: ep_len:864 episode reward: total was -14.430000. running mean: -3.099226\n",
      "ep 2657: ep_len:120 episode reward: total was 57.000000. running mean: -2.498234\n",
      "ep 2657: ep_len:87 episode reward: total was 40.500000. running mean: -2.068252\n",
      "ep 2657: ep_len:41 episode reward: total was 16.000000. running mean: -1.887569\n",
      "ep 2657: ep_len:1362 episode reward: total was -114.820000. running mean: -3.016893\n",
      "ep 2657: ep_len:366 episode reward: total was 15.780000. running mean: -2.828924\n",
      "ep 2657: ep_len:561 episode reward: total was 0.390000. running mean: -2.796735\n",
      "ep 2657: ep_len:7367 episode reward: total was -262.470000. running mean: -5.393468\n",
      "ep 2657: ep_len:989 episode reward: total was 59.940000. running mean: -4.740133\n",
      "ep 2657: ep_len:88 episode reward: total was 41.000000. running mean: -4.282732\n",
      "ep 2657: ep_len:1150 episode reward: total was -23.010000. running mean: -4.470005\n",
      "ep 2657: ep_len:2861 episode reward: total was -11.650000. running mean: -4.541804\n",
      "ep 2657: ep_len:55 episode reward: total was 26.000000. running mean: -4.236386\n",
      "epsilon:0.009992 episode_count: 40020. steps_count: 43032947.000000\n",
      "ep 2658: ep_len:684 episode reward: total was -30.700000. running mean: -4.501023\n",
      "ep 2658: ep_len:500 episode reward: total was 9.980000. running mean: -4.356212\n",
      "ep 2658: ep_len:2975 episode reward: total was -40.870000. running mean: -4.721350\n",
      "ep 2658: ep_len:518 episode reward: total was -4.470000. running mean: -4.718837\n",
      "ep 2658: ep_len:41 episode reward: total was 17.500000. running mean: -4.496648\n",
      "ep 2658: ep_len:172 episode reward: total was 84.500000. running mean: -3.606682\n",
      "ep 2658: ep_len:51 episode reward: total was 24.000000. running mean: -3.330615\n",
      "ep 2658: ep_len:732 episode reward: total was 21.270000. running mean: -3.084609\n",
      "ep 2658: ep_len:317 episode reward: total was 23.890000. running mean: -2.814863\n",
      "ep 2658: ep_len:731 episode reward: total was -17.080000. running mean: -2.957514\n",
      "ep 2658: ep_len:631 episode reward: total was 27.100000. running mean: -2.656939\n",
      "ep 2658: ep_len:704 episode reward: total was -13.760000. running mean: -2.767970\n",
      "ep 2658: ep_len:72 episode reward: total was 33.000000. running mean: -2.410290\n",
      "ep 2658: ep_len:685 episode reward: total was -1.400000. running mean: -2.400187\n",
      "ep 2658: ep_len:2859 episode reward: total was -2.610000. running mean: -2.402285\n",
      "ep 2658: ep_len:53 episode reward: total was 23.500000. running mean: -2.143262\n",
      "epsilon:0.009992 episode_count: 40036. steps_count: 43044672.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2659: ep_len:1001 episode reward: total was -84.310000. running mean: -2.964930\n",
      "ep 2659: ep_len:679 episode reward: total was -36.780000. running mean: -3.303080\n",
      "ep 2659: ep_len:77 episode reward: total was 37.000000. running mean: -2.900050\n",
      "ep 2659: ep_len:2978 episode reward: total was -25.230000. running mean: -3.123349\n",
      "ep 2659: ep_len:500 episode reward: total was 20.330000. running mean: -2.888816\n",
      "ep 2659: ep_len:102 episode reward: total was 46.500000. running mean: -2.394927\n",
      "ep 2659: ep_len:65 episode reward: total was 29.500000. running mean: -2.075978\n",
      "ep 2659: ep_len:697 episode reward: total was -47.380000. running mean: -2.529018\n",
      "ep 2659: ep_len:659 episode reward: total was 11.720000. running mean: -2.386528\n",
      "ep 2659: ep_len:2194 episode reward: total was -230.290000. running mean: -4.665563\n",
      "ep 2659: ep_len:7326 episode reward: total was -131.980000. running mean: -5.938707\n",
      "ep 2659: ep_len:760 episode reward: total was -4.690000. running mean: -5.926220\n",
      "ep 2659: ep_len:216 episode reward: total was 106.010000. running mean: -4.806858\n",
      "ep 2659: ep_len:41 episode reward: total was 19.000000. running mean: -4.568789\n",
      "ep 2659: ep_len:116 episode reward: total was 56.500000. running mean: -3.958102\n",
      "ep 2659: ep_len:1133 episode reward: total was -3.370000. running mean: -3.952221\n",
      "ep 2659: ep_len:2783 episode reward: total was -2.670000. running mean: -3.939398\n",
      "epsilon:0.009992 episode_count: 40053. steps_count: 43065999.000000\n",
      "ep 2660: ep_len:1128 episode reward: total was 9.060000. running mean: -3.809404\n",
      "ep 2660: ep_len:1622 episode reward: total was -39.470000. running mean: -4.166010\n",
      "ep 2660: ep_len:3041 episode reward: total was -44.100000. running mean: -4.565350\n",
      "ep 2660: ep_len:854 episode reward: total was 14.510000. running mean: -4.374597\n",
      "ep 2660: ep_len:131 episode reward: total was 62.500000. running mean: -3.705851\n",
      "ep 2660: ep_len:52 episode reward: total was 24.500000. running mean: -3.423792\n",
      "ep 2660: ep_len:1004 episode reward: total was -36.990000. running mean: -3.759454\n",
      "ep 2660: ep_len:3794 episode reward: total was -66.720000. running mean: -4.389060\n",
      "ep 2660: ep_len:1530 episode reward: total was -61.210000. running mean: -4.957269\n",
      "ep 2660: ep_len:598 episode reward: total was 11.110000. running mean: -4.796596\n",
      "ep 2660: ep_len:695 episode reward: total was -9.380000. running mean: -4.842431\n",
      "ep 2660: ep_len:90 episode reward: total was 43.500000. running mean: -4.359006\n",
      "ep 2660: ep_len:1526 episode reward: total was -7.400000. running mean: -4.389416\n",
      "ep 2660: ep_len:2870 episode reward: total was 9.160000. running mean: -4.253922\n",
      "ep 2660: ep_len:60 episode reward: total was 25.500000. running mean: -3.956383\n",
      "epsilon:0.009992 episode_count: 40068. steps_count: 43084994.000000\n",
      "ep 2661: ep_len:854 episode reward: total was -20.600000. running mean: -4.122819\n",
      "ep 2661: ep_len:949 episode reward: total was 0.140000. running mean: -4.080191\n",
      "ep 2661: ep_len:40 episode reward: total was 18.500000. running mean: -3.854389\n",
      "ep 2661: ep_len:3020 episode reward: total was -43.450000. running mean: -4.250345\n",
      "ep 2661: ep_len:1748 episode reward: total was -42.180000. running mean: -4.629641\n",
      "ep 2661: ep_len:66 episode reward: total was 30.000000. running mean: -4.283345\n",
      "ep 2661: ep_len:57 episode reward: total was 25.500000. running mean: -3.985512\n",
      "ep 2661: ep_len:500 episode reward: total was 18.160000. running mean: -3.764057\n",
      "ep 2661: ep_len:3690 episode reward: total was -410.550000. running mean: -7.831916\n",
      "ep 2661: ep_len:1289 episode reward: total was -60.820000. running mean: -8.361797\n",
      "ep 2661: ep_len:793 episode reward: total was 33.730000. running mean: -7.940879\n",
      "ep 2661: ep_len:902 episode reward: total was 39.500000. running mean: -7.466470\n",
      "ep 2661: ep_len:51 episode reward: total was 24.000000. running mean: -7.151805\n",
      "ep 2661: ep_len:1441 episode reward: total was 25.810000. running mean: -6.822187\n",
      "ep 2661: ep_len:2817 episode reward: total was 1.470000. running mean: -6.739265\n",
      "epsilon:0.009992 episode_count: 40083. steps_count: 43103211.000000\n",
      "ep 2662: ep_len:1151 episode reward: total was -15.930000. running mean: -6.831173\n",
      "ep 2662: ep_len:758 episode reward: total was -20.320000. running mean: -6.966061\n",
      "ep 2662: ep_len:35 episode reward: total was 16.000000. running mean: -6.736400\n",
      "ep 2662: ep_len:2910 episode reward: total was -60.380000. running mean: -7.272836\n",
      "ep 2662: ep_len:500 episode reward: total was -6.910000. running mean: -7.269208\n",
      "ep 2662: ep_len:113 episode reward: total was 52.000000. running mean: -6.676516\n",
      "ep 2662: ep_len:500 episode reward: total was 7.230000. running mean: -6.537451\n",
      "ep 2662: ep_len:625 episode reward: total was 15.910000. running mean: -6.312976\n",
      "ep 2662: ep_len:4170 episode reward: total was -476.140000. running mean: -11.011247\n",
      "ep 2662: ep_len:788 episode reward: total was 21.500000. running mean: -10.686134\n",
      "ep 2662: ep_len:1073 episode reward: total was 32.720000. running mean: -10.252073\n",
      "ep 2662: ep_len:135 episode reward: total was 66.000000. running mean: -9.489552\n",
      "ep 2662: ep_len:668 episode reward: total was 7.440000. running mean: -9.320256\n",
      "ep 2662: ep_len:2826 episode reward: total was -1.960000. running mean: -9.246654\n",
      "ep 2662: ep_len:44 episode reward: total was 20.500000. running mean: -8.949187\n",
      "epsilon:0.009992 episode_count: 40098. steps_count: 43119507.000000\n",
      "ep 2663: ep_len:1399 episode reward: total was 15.840000. running mean: -8.701296\n",
      "ep 2663: ep_len:768 episode reward: total was -21.780000. running mean: -8.832083\n",
      "ep 2663: ep_len:99 episode reward: total was 48.000000. running mean: -8.263762\n",
      "ep 2663: ep_len:680 episode reward: total was -17.470000. running mean: -8.355824\n",
      "ep 2663: ep_len:41 episode reward: total was 19.000000. running mean: -8.082266\n",
      "ep 2663: ep_len:784 episode reward: total was -56.180000. running mean: -8.563243\n",
      "ep 2663: ep_len:364 episode reward: total was 20.840000. running mean: -8.269211\n",
      "ep 2663: ep_len:620 episode reward: total was 3.000000. running mean: -8.156519\n",
      "ep 2663: ep_len:7389 episode reward: total was -386.720000. running mean: -11.942153\n",
      "ep 2663: ep_len:1074 episode reward: total was 5.470000. running mean: -11.768032\n",
      "ep 2663: ep_len:75 episode reward: total was 34.500000. running mean: -11.305352\n",
      "ep 2663: ep_len:646 episode reward: total was -1.320000. running mean: -11.205498\n",
      "ep 2663: ep_len:2787 episode reward: total was -16.770000. running mean: -11.261143\n",
      "epsilon:0.009992 episode_count: 40111. steps_count: 43136233.000000\n",
      "ep 2664: ep_len:658 episode reward: total was 19.510000. running mean: -10.953432\n",
      "ep 2664: ep_len:720 episode reward: total was -38.130000. running mean: -11.225197\n",
      "ep 2664: ep_len:3019 episode reward: total was -20.890000. running mean: -11.321845\n",
      "ep 2664: ep_len:693 episode reward: total was -3.830000. running mean: -11.246927\n",
      "ep 2664: ep_len:136 episode reward: total was 65.000000. running mean: -10.484458\n",
      "ep 2664: ep_len:97 episode reward: total was 47.000000. running mean: -9.909613\n",
      "ep 2664: ep_len:646 episode reward: total was 1.240000. running mean: -9.798117\n",
      "ep 2664: ep_len:647 episode reward: total was 23.050000. running mean: -9.469636\n",
      "ep 2664: ep_len:2040 episode reward: total was -350.440000. running mean: -12.879339\n",
      "ep 2664: ep_len:710 episode reward: total was 29.960000. running mean: -12.450946\n",
      "ep 2664: ep_len:1140 episode reward: total was -10.900000. running mean: -12.435437\n",
      "ep 2664: ep_len:53 episode reward: total was 25.000000. running mean: -12.061082\n",
      "ep 2664: ep_len:217 episode reward: total was 103.510000. running mean: -10.905371\n",
      "ep 2664: ep_len:37 episode reward: total was 17.000000. running mean: -10.626318\n",
      "ep 2664: ep_len:1070 episode reward: total was 14.030000. running mean: -10.379755\n",
      "ep 2664: ep_len:2814 episode reward: total was -26.110000. running mean: -10.537057\n",
      "epsilon:0.009992 episode_count: 40127. steps_count: 43150930.000000\n",
      "ep 2665: ep_len:1099 episode reward: total was -16.450000. running mean: -10.596186\n",
      "ep 2665: ep_len:724 episode reward: total was -0.040000. running mean: -10.490625\n",
      "ep 2665: ep_len:68 episode reward: total was 32.500000. running mean: -10.060718\n",
      "ep 2665: ep_len:2920 episode reward: total was -74.450000. running mean: -10.704611\n",
      "ep 2665: ep_len:1455 episode reward: total was -0.990000. running mean: -10.607465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2665: ep_len:20 episode reward: total was -2.000000. running mean: -10.521390\n",
      "ep 2665: ep_len:500 episode reward: total was -15.720000. running mean: -10.573376\n",
      "ep 2665: ep_len:3627 episode reward: total was -1045.720000. running mean: -20.924843\n",
      "ep 2665: ep_len:537 episode reward: total was 19.810000. running mean: -20.517494\n",
      "ep 2665: ep_len:654 episode reward: total was 23.190000. running mean: -20.080419\n",
      "ep 2665: ep_len:754 episode reward: total was -18.370000. running mean: -20.063315\n",
      "ep 2665: ep_len:122 episode reward: total was 59.500000. running mean: -19.267682\n",
      "ep 2665: ep_len:104 episode reward: total was 50.500000. running mean: -18.570005\n",
      "ep 2665: ep_len:2454 episode reward: total was -314.990000. running mean: -21.534205\n",
      "ep 2665: ep_len:2863 episode reward: total was -6.670000. running mean: -21.385563\n",
      "epsilon:0.009992 episode_count: 40142. steps_count: 43168831.000000\n",
      "ep 2666: ep_len:1846 episode reward: total was -1312.740000. running mean: -34.299107\n",
      "ep 2666: ep_len:930 episode reward: total was 22.660000. running mean: -33.729516\n",
      "ep 2666: ep_len:2969 episode reward: total was -6.090000. running mean: -33.453121\n",
      "ep 2666: ep_len:530 episode reward: total was -35.270000. running mean: -33.471290\n",
      "ep 2666: ep_len:118 episode reward: total was 57.500000. running mean: -32.561577\n",
      "ep 2666: ep_len:68 episode reward: total was 32.500000. running mean: -31.910961\n",
      "ep 2666: ep_len:1114 episode reward: total was -43.570000. running mean: -32.027552\n",
      "ep 2666: ep_len:326 episode reward: total was 3.500000. running mean: -31.672276\n",
      "ep 2666: ep_len:2178 episode reward: total was -126.800000. running mean: -32.623553\n",
      "ep 2666: ep_len:734 episode reward: total was -3.480000. running mean: -32.332118\n",
      "ep 2666: ep_len:886 episode reward: total was 19.100000. running mean: -31.817797\n",
      "ep 2666: ep_len:1407 episode reward: total was -4.350000. running mean: -31.543119\n",
      "ep 2666: ep_len:2861 episode reward: total was -5.680000. running mean: -31.284488\n",
      "epsilon:0.009992 episode_count: 40155. steps_count: 43184798.000000\n",
      "ep 2667: ep_len:655 episode reward: total was -1.320000. running mean: -30.984843\n",
      "ep 2667: ep_len:710 episode reward: total was -51.120000. running mean: -31.186194\n",
      "ep 2667: ep_len:72 episode reward: total was 34.500000. running mean: -30.529332\n",
      "ep 2667: ep_len:2861 episode reward: total was -36.660000. running mean: -30.590639\n",
      "ep 2667: ep_len:684 episode reward: total was -20.000000. running mean: -30.484733\n",
      "ep 2667: ep_len:91 episode reward: total was 42.500000. running mean: -29.754885\n",
      "ep 2667: ep_len:1017 episode reward: total was -39.490000. running mean: -29.852236\n",
      "ep 2667: ep_len:3547 episode reward: total was -623.000000. running mean: -35.783714\n",
      "ep 2667: ep_len:1670 episode reward: total was -230.960000. running mean: -37.735477\n",
      "ep 2667: ep_len:871 episode reward: total was 57.210000. running mean: -36.786022\n",
      "ep 2667: ep_len:641 episode reward: total was 0.760000. running mean: -36.410562\n",
      "ep 2667: ep_len:108 episode reward: total was 51.000000. running mean: -35.536456\n",
      "ep 2667: ep_len:673 episode reward: total was -11.520000. running mean: -35.296292\n",
      "ep 2667: ep_len:2796 episode reward: total was 15.770000. running mean: -34.785629\n",
      "epsilon:0.009992 episode_count: 40169. steps_count: 43201194.000000\n",
      "ep 2668: ep_len:1515 episode reward: total was -123.440000. running mean: -35.672173\n",
      "ep 2668: ep_len:918 episode reward: total was 18.620000. running mean: -35.129251\n",
      "ep 2668: ep_len:79 episode reward: total was 38.000000. running mean: -34.397958\n",
      "ep 2668: ep_len:3044 episode reward: total was -120.050000. running mean: -35.254479\n",
      "ep 2668: ep_len:795 episode reward: total was 28.100000. running mean: -34.620934\n",
      "ep 2668: ep_len:73 episode reward: total was 35.000000. running mean: -33.924725\n",
      "ep 2668: ep_len:1496 episode reward: total was 15.770000. running mean: -33.427777\n",
      "ep 2668: ep_len:303 episode reward: total was 23.720000. running mean: -32.856300\n",
      "ep 2668: ep_len:1223 episode reward: total was -58.640000. running mean: -33.114137\n",
      "ep 2668: ep_len:698 episode reward: total was -15.440000. running mean: -32.937395\n",
      "ep 2668: ep_len:611 episode reward: total was 3.210000. running mean: -32.575921\n",
      "ep 2668: ep_len:1137 episode reward: total was -30.480000. running mean: -32.554962\n",
      "ep 2668: ep_len:2927 episode reward: total was -5.940000. running mean: -32.288812\n",
      "epsilon:0.009992 episode_count: 40182. steps_count: 43216013.000000\n",
      "ep 2669: ep_len:1497 episode reward: total was 29.590000. running mean: -31.670024\n",
      "ep 2669: ep_len:1664 episode reward: total was -36.810000. running mean: -31.721424\n",
      "ep 2669: ep_len:47 episode reward: total was 22.000000. running mean: -31.184210\n",
      "ep 2669: ep_len:2918 episode reward: total was -12.230000. running mean: -30.994668\n",
      "ep 2669: ep_len:500 episode reward: total was -6.490000. running mean: -30.749621\n",
      "ep 2669: ep_len:62 episode reward: total was 29.500000. running mean: -30.147125\n",
      "ep 2669: ep_len:500 episode reward: total was 25.080000. running mean: -29.594854\n",
      "ep 2669: ep_len:624 episode reward: total was 12.780000. running mean: -29.171105\n",
      "ep 2669: ep_len:1408 episode reward: total was -292.490000. running mean: -31.804294\n",
      "ep 2669: ep_len:849 episode reward: total was 44.500000. running mean: -31.041251\n",
      "ep 2669: ep_len:606 episode reward: total was -11.280000. running mean: -30.843639\n",
      "ep 2669: ep_len:780 episode reward: total was -5.000000. running mean: -30.585202\n",
      "ep 2669: ep_len:2793 episode reward: total was -18.060000. running mean: -30.459950\n",
      "epsilon:0.009992 episode_count: 40195. steps_count: 43230261.000000\n",
      "ep 2670: ep_len:989 episode reward: total was -107.440000. running mean: -31.229751\n",
      "ep 2670: ep_len:1597 episode reward: total was -32.710000. running mean: -31.244553\n",
      "ep 2670: ep_len:2881 episode reward: total was -13.590000. running mean: -31.068008\n",
      "ep 2670: ep_len:885 episode reward: total was 51.640000. running mean: -30.240928\n",
      "ep 2670: ep_len:85 episode reward: total was 41.000000. running mean: -29.528518\n",
      "ep 2670: ep_len:76 episode reward: total was 35.000000. running mean: -28.883233\n",
      "ep 2670: ep_len:53 episode reward: total was 25.000000. running mean: -28.344401\n",
      "ep 2670: ep_len:581 episode reward: total was 16.360000. running mean: -27.897357\n",
      "ep 2670: ep_len:634 episode reward: total was 7.340000. running mean: -27.544983\n",
      "ep 2670: ep_len:517 episode reward: total was -6.110000. running mean: -27.330633\n",
      "ep 2670: ep_len:916 episode reward: total was 75.110000. running mean: -26.306227\n",
      "ep 2670: ep_len:732 episode reward: total was -26.280000. running mean: -26.305965\n",
      "ep 2670: ep_len:1440 episode reward: total was 16.770000. running mean: -25.875205\n",
      "ep 2670: ep_len:2895 episode reward: total was -19.150000. running mean: -25.807953\n",
      "ep 2670: ep_len:60 episode reward: total was 28.500000. running mean: -25.264873\n",
      "epsilon:0.009992 episode_count: 40210. steps_count: 43244602.000000\n",
      "ep 2671: ep_len:1160 episode reward: total was 11.990000. running mean: -24.892325\n",
      "ep 2671: ep_len:751 episode reward: total was -24.930000. running mean: -24.892702\n",
      "ep 2671: ep_len:46 episode reward: total was 21.500000. running mean: -24.428774\n",
      "ep 2671: ep_len:3012 episode reward: total was -40.230000. running mean: -24.586787\n",
      "ep 2671: ep_len:525 episode reward: total was -24.210000. running mean: -24.583019\n",
      "ep 2671: ep_len:57 episode reward: total was 27.000000. running mean: -24.067189\n",
      "ep 2671: ep_len:174 episode reward: total was 81.000000. running mean: -23.016517\n",
      "ep 2671: ep_len:71 episode reward: total was 34.000000. running mean: -22.446352\n",
      "ep 2671: ep_len:70 episode reward: total was 32.000000. running mean: -21.901888\n",
      "ep 2671: ep_len:873 episode reward: total was 37.460000. running mean: -21.308269\n",
      "ep 2671: ep_len:4051 episode reward: total was -79.460000. running mean: -21.889787\n",
      "ep 2671: ep_len:1251 episode reward: total was -55.330000. running mean: -22.224189\n",
      "ep 2671: ep_len:771 episode reward: total was 43.250000. running mean: -21.569447\n",
      "ep 2671: ep_len:500 episode reward: total was 27.100000. running mean: -21.082752\n",
      "ep 2671: ep_len:38 episode reward: total was 17.500000. running mean: -20.696925\n",
      "ep 2671: ep_len:83 episode reward: total was 40.000000. running mean: -20.089956\n",
      "ep 2671: ep_len:681 episode reward: total was -6.980000. running mean: -19.958856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2671: ep_len:2835 episode reward: total was -3.830000. running mean: -19.797567\n",
      "epsilon:0.009992 episode_count: 40228. steps_count: 43261551.000000\n",
      "ep 2672: ep_len:1490 episode reward: total was 38.700000. running mean: -19.212592\n",
      "ep 2672: ep_len:744 episode reward: total was -9.380000. running mean: -19.114266\n",
      "ep 2672: ep_len:3001 episode reward: total was -20.800000. running mean: -19.131123\n",
      "ep 2672: ep_len:1647 episode reward: total was -58.020000. running mean: -19.520012\n",
      "ep 2672: ep_len:64 episode reward: total was 30.500000. running mean: -19.019812\n",
      "ep 2672: ep_len:718 episode reward: total was -20.140000. running mean: -19.031014\n",
      "ep 2672: ep_len:684 episode reward: total was 17.820000. running mean: -18.662504\n",
      "ep 2672: ep_len:636 episode reward: total was -17.040000. running mean: -18.646279\n",
      "ep 2672: ep_len:746 episode reward: total was 8.950000. running mean: -18.370316\n",
      "ep 2672: ep_len:912 episode reward: total was 25.250000. running mean: -17.934113\n",
      "ep 2672: ep_len:52 episode reward: total was 23.000000. running mean: -17.524771\n",
      "ep 2672: ep_len:62 episode reward: total was 29.500000. running mean: -17.054524\n",
      "ep 2672: ep_len:729 episode reward: total was -32.270000. running mean: -17.206679\n",
      "ep 2672: ep_len:2881 episode reward: total was -0.520000. running mean: -17.039812\n",
      "epsilon:0.009992 episode_count: 40242. steps_count: 43275917.000000\n",
      "ep 2673: ep_len:1152 episode reward: total was 12.160000. running mean: -16.747814\n",
      "ep 2673: ep_len:755 episode reward: total was -17.870000. running mean: -16.759035\n",
      "ep 2673: ep_len:2925 episode reward: total was -39.600000. running mean: -16.987445\n",
      "ep 2673: ep_len:575 episode reward: total was 21.880000. running mean: -16.598771\n",
      "ep 2673: ep_len:116 episode reward: total was 53.500000. running mean: -15.897783\n",
      "ep 2673: ep_len:677 episode reward: total was -15.620000. running mean: -15.895005\n",
      "ep 2673: ep_len:288 episode reward: total was 18.700000. running mean: -15.549055\n",
      "ep 2673: ep_len:4220 episode reward: total was -472.460000. running mean: -20.118165\n",
      "ep 2673: ep_len:842 episode reward: total was 51.900000. running mean: -19.397983\n",
      "ep 2673: ep_len:500 episode reward: total was 18.840000. running mean: -19.015603\n",
      "ep 2673: ep_len:188 episode reward: total was 89.500000. running mean: -17.930447\n",
      "ep 2673: ep_len:1459 episode reward: total was 17.940000. running mean: -17.571743\n",
      "ep 2673: ep_len:2840 episode reward: total was 17.310000. running mean: -17.222925\n",
      "epsilon:0.009992 episode_count: 40255. steps_count: 43292454.000000\n",
      "ep 2674: ep_len:940 episode reward: total was -44.230000. running mean: -17.492996\n",
      "ep 2674: ep_len:775 episode reward: total was -13.510000. running mean: -17.453166\n",
      "ep 2674: ep_len:55 episode reward: total was 24.500000. running mean: -17.033634\n",
      "ep 2674: ep_len:3047 episode reward: total was -19.200000. running mean: -17.055298\n",
      "ep 2674: ep_len:556 episode reward: total was 2.360000. running mean: -16.861145\n",
      "ep 2674: ep_len:657 episode reward: total was -1.980000. running mean: -16.712333\n",
      "ep 2674: ep_len:646 episode reward: total was 26.530000. running mean: -16.279910\n",
      "ep 2674: ep_len:672 episode reward: total was -8.600000. running mean: -16.203111\n",
      "ep 2674: ep_len:7298 episode reward: total was 17.880000. running mean: -15.862280\n",
      "ep 2674: ep_len:1065 episode reward: total was -24.860000. running mean: -15.952257\n",
      "ep 2674: ep_len:89 episode reward: total was 43.000000. running mean: -15.362735\n",
      "ep 2674: ep_len:637 episode reward: total was 11.270000. running mean: -15.096407\n",
      "ep 2674: ep_len:2865 episode reward: total was -35.760000. running mean: -15.303043\n",
      "ep 2674: ep_len:30 episode reward: total was 12.000000. running mean: -15.030013\n",
      "epsilon:0.009992 episode_count: 40269. steps_count: 43311786.000000\n",
      "ep 2675: ep_len:1138 episode reward: total was 4.700000. running mean: -14.832713\n",
      "ep 2675: ep_len:729 episode reward: total was -76.670000. running mean: -15.451085\n",
      "ep 2675: ep_len:2824 episode reward: total was -65.280000. running mean: -15.949375\n",
      "ep 2675: ep_len:663 episode reward: total was 8.800000. running mean: -15.701881\n",
      "ep 2675: ep_len:51 episode reward: total was 24.000000. running mean: -15.304862\n",
      "ep 2675: ep_len:27 episode reward: total was 12.000000. running mean: -15.031813\n",
      "ep 2675: ep_len:1372 episode reward: total was -53.540000. running mean: -15.416895\n",
      "ep 2675: ep_len:3481 episode reward: total was -465.940000. running mean: -19.922126\n",
      "ep 2675: ep_len:802 episode reward: total was -57.350000. running mean: -20.296405\n",
      "ep 2675: ep_len:762 episode reward: total was 8.190000. running mean: -20.011541\n",
      "ep 2675: ep_len:708 episode reward: total was -11.270000. running mean: -19.924126\n",
      "ep 2675: ep_len:53 episode reward: total was 25.000000. running mean: -19.474884\n",
      "ep 2675: ep_len:80 episode reward: total was 38.500000. running mean: -18.895136\n",
      "ep 2675: ep_len:912 episode reward: total was -97.100000. running mean: -19.677184\n",
      "ep 2675: ep_len:2728 episode reward: total was -24.150000. running mean: -19.721912\n",
      "ep 2675: ep_len:63 episode reward: total was 30.000000. running mean: -19.224693\n",
      "epsilon:0.009992 episode_count: 40285. steps_count: 43328179.000000\n",
      "ep 2676: ep_len:1159 episode reward: total was -2.220000. running mean: -19.054646\n",
      "ep 2676: ep_len:770 episode reward: total was -9.730000. running mean: -18.961400\n",
      "ep 2676: ep_len:2956 episode reward: total was -93.340000. running mean: -19.705186\n",
      "ep 2676: ep_len:654 episode reward: total was 17.590000. running mean: -19.332234\n",
      "ep 2676: ep_len:66 episode reward: total was 31.500000. running mean: -18.823912\n",
      "ep 2676: ep_len:66 episode reward: total was 28.500000. running mean: -18.350673\n",
      "ep 2676: ep_len:748 episode reward: total was -11.330000. running mean: -18.280466\n",
      "ep 2676: ep_len:333 episode reward: total was 22.000000. running mean: -17.877661\n",
      "ep 2676: ep_len:621 episode reward: total was 0.990000. running mean: -17.688985\n",
      "ep 2676: ep_len:665 episode reward: total was -6.250000. running mean: -17.574595\n",
      "ep 2676: ep_len:579 episode reward: total was -6.500000. running mean: -17.463849\n",
      "ep 2676: ep_len:117 episode reward: total was 57.000000. running mean: -16.719210\n",
      "ep 2676: ep_len:686 episode reward: total was 34.090000. running mean: -16.211118\n",
      "ep 2676: ep_len:40 episode reward: total was 18.500000. running mean: -15.864007\n",
      "epsilon:0.009992 episode_count: 40299. steps_count: 43337639.000000\n",
      "ep 2677: ep_len:1470 episode reward: total was 18.050000. running mean: -15.524867\n",
      "ep 2677: ep_len:956 episode reward: total was 7.070000. running mean: -15.298918\n",
      "ep 2677: ep_len:2951 episode reward: total was -38.590000. running mean: -15.531829\n",
      "ep 2677: ep_len:500 episode reward: total was 9.000000. running mean: -15.286511\n",
      "ep 2677: ep_len:126 episode reward: total was 33.500000. running mean: -14.798646\n",
      "ep 2677: ep_len:62 episode reward: total was 28.000000. running mean: -14.370659\n",
      "ep 2677: ep_len:1097 episode reward: total was -2.300000. running mean: -14.249953\n",
      "ep 2677: ep_len:3957 episode reward: total was -287.820000. running mean: -16.985653\n",
      "ep 2677: ep_len:711 episode reward: total was -26.490000. running mean: -17.080697\n",
      "ep 2677: ep_len:744 episode reward: total was 47.300000. running mean: -16.436890\n",
      "ep 2677: ep_len:623 episode reward: total was 3.030000. running mean: -16.242221\n",
      "ep 2677: ep_len:66 episode reward: total was 31.500000. running mean: -15.764798\n",
      "ep 2677: ep_len:97 episode reward: total was 45.500000. running mean: -15.152150\n",
      "ep 2677: ep_len:126 episode reward: total was 57.000000. running mean: -14.430629\n",
      "ep 2677: ep_len:1426 episode reward: total was 10.290000. running mean: -14.183423\n",
      "ep 2677: ep_len:37 episode reward: total was 17.000000. running mean: -13.871588\n",
      "ep 2677: ep_len:55 episode reward: total was 24.500000. running mean: -13.487873\n",
      "epsilon:0.009992 episode_count: 40316. steps_count: 43352643.000000\n",
      "ep 2678: ep_len:1093 episode reward: total was -7.910000. running mean: -13.432094\n",
      "ep 2678: ep_len:668 episode reward: total was -10.860000. running mean: -13.406373\n",
      "ep 2678: ep_len:77 episode reward: total was 37.000000. running mean: -12.902309\n",
      "ep 2678: ep_len:2920 episode reward: total was -77.600000. running mean: -13.549286\n",
      "ep 2678: ep_len:879 episode reward: total was 71.570000. running mean: -12.698093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2678: ep_len:866 episode reward: total was 27.010000. running mean: -12.301012\n",
      "ep 2678: ep_len:3991 episode reward: total was -144.690000. running mean: -13.624902\n",
      "ep 2678: ep_len:532 episode reward: total was -74.640000. running mean: -14.235053\n",
      "ep 2678: ep_len:831 episode reward: total was 57.450000. running mean: -13.518203\n",
      "ep 2678: ep_len:635 episode reward: total was -7.110000. running mean: -13.454121\n",
      "ep 2678: ep_len:73 episode reward: total was 35.000000. running mean: -12.969579\n",
      "ep 2678: ep_len:67 episode reward: total was 29.000000. running mean: -12.549884\n",
      "ep 2678: ep_len:661 episode reward: total was 1.610000. running mean: -12.408285\n",
      "ep 2678: ep_len:2807 episode reward: total was -39.250000. running mean: -12.676702\n",
      "ep 2678: ep_len:37 episode reward: total was 15.500000. running mean: -12.394935\n",
      "epsilon:0.009992 episode_count: 40331. steps_count: 43368780.000000\n",
      "ep 2679: ep_len:643 episode reward: total was -2.080000. running mean: -12.291786\n",
      "ep 2679: ep_len:500 episode reward: total was 19.050000. running mean: -11.978368\n",
      "ep 2679: ep_len:3008 episode reward: total was -41.390000. running mean: -12.272484\n",
      "ep 2679: ep_len:500 episode reward: total was 9.220000. running mean: -12.057559\n",
      "ep 2679: ep_len:57 episode reward: total was 27.000000. running mean: -11.666984\n",
      "ep 2679: ep_len:916 episode reward: total was 60.340000. running mean: -10.946914\n",
      "ep 2679: ep_len:643 episode reward: total was 24.910000. running mean: -10.588345\n",
      "ep 2679: ep_len:564 episode reward: total was 1.830000. running mean: -10.464161\n",
      "ep 2679: ep_len:663 episode reward: total was 6.440000. running mean: -10.295120\n",
      "ep 2679: ep_len:983 episode reward: total was 2.410000. running mean: -10.168068\n",
      "ep 2679: ep_len:172 episode reward: total was 83.000000. running mean: -9.236388\n",
      "ep 2679: ep_len:93 episode reward: total was 42.000000. running mean: -8.724024\n",
      "ep 2679: ep_len:635 episode reward: total was 10.250000. running mean: -8.534284\n",
      "ep 2679: ep_len:2848 episode reward: total was -36.760000. running mean: -8.816541\n",
      "epsilon:0.009992 episode_count: 40345. steps_count: 43381005.000000\n",
      "ep 2680: ep_len:591 episode reward: total was -40.660000. running mean: -9.134975\n",
      "ep 2680: ep_len:741 episode reward: total was -51.340000. running mean: -9.557026\n",
      "ep 2680: ep_len:2898 episode reward: total was -60.490000. running mean: -10.066355\n",
      "ep 2680: ep_len:1680 episode reward: total was -42.410000. running mean: -10.389792\n",
      "ep 2680: ep_len:59 episode reward: total was 26.500000. running mean: -10.020894\n",
      "ep 2680: ep_len:679 episode reward: total was 17.020000. running mean: -9.750485\n",
      "ep 2680: ep_len:673 episode reward: total was 30.810000. running mean: -9.344880\n",
      "ep 2680: ep_len:911 episode reward: total was -50.070000. running mean: -9.752131\n",
      "ep 2680: ep_len:779 episode reward: total was 14.730000. running mean: -9.507310\n",
      "ep 2680: ep_len:805 episode reward: total was 9.560000. running mean: -9.316637\n",
      "ep 2680: ep_len:1474 episode reward: total was 19.070000. running mean: -9.032770\n",
      "ep 2680: ep_len:2678 episode reward: total was -212.610000. running mean: -11.068543\n",
      "epsilon:0.009992 episode_count: 40357. steps_count: 43394973.000000\n",
      "ep 2681: ep_len:636 episode reward: total was 6.030000. running mean: -10.897557\n",
      "ep 2681: ep_len:703 episode reward: total was -11.030000. running mean: -10.898882\n",
      "ep 2681: ep_len:3047 episode reward: total was -13.750000. running mean: -10.927393\n",
      "ep 2681: ep_len:1236 episode reward: total was -16.630000. running mean: -10.984419\n",
      "ep 2681: ep_len:68 episode reward: total was 32.500000. running mean: -10.549575\n",
      "ep 2681: ep_len:107 episode reward: total was 49.000000. running mean: -9.954079\n",
      "ep 2681: ep_len:1107 episode reward: total was -13.340000. running mean: -9.987938\n",
      "ep 2681: ep_len:3985 episode reward: total was -134.650000. running mean: -11.234559\n",
      "ep 2681: ep_len:1254 episode reward: total was -74.490000. running mean: -11.867113\n",
      "ep 2681: ep_len:716 episode reward: total was 30.060000. running mean: -11.447842\n",
      "ep 2681: ep_len:725 episode reward: total was 6.380000. running mean: -11.269564\n",
      "ep 2681: ep_len:62 episode reward: total was 29.500000. running mean: -10.861868\n",
      "ep 2681: ep_len:717 episode reward: total was -86.930000. running mean: -11.622549\n",
      "ep 2681: ep_len:2884 episode reward: total was -1327.650000. running mean: -24.782824\n",
      "ep 2681: ep_len:45 episode reward: total was 19.500000. running mean: -24.339996\n",
      "epsilon:0.009992 episode_count: 40372. steps_count: 43412265.000000\n",
      "ep 2682: ep_len:1355 episode reward: total was 28.470000. running mean: -23.811896\n",
      "ep 2682: ep_len:687 episode reward: total was -31.740000. running mean: -23.891177\n",
      "ep 2682: ep_len:42 episode reward: total was 19.500000. running mean: -23.457265\n",
      "ep 2682: ep_len:2879 episode reward: total was -66.930000. running mean: -23.891992\n",
      "ep 2682: ep_len:630 episode reward: total was -111.550000. running mean: -24.768572\n",
      "ep 2682: ep_len:1381 episode reward: total was -148.970000. running mean: -26.010587\n",
      "ep 2682: ep_len:669 episode reward: total was 13.410000. running mean: -25.616381\n",
      "ep 2682: ep_len:4729 episode reward: total was -620.110000. running mean: -31.561317\n",
      "ep 2682: ep_len:770 episode reward: total was 6.040000. running mean: -31.185304\n",
      "ep 2682: ep_len:683 episode reward: total was -5.460000. running mean: -30.928051\n",
      "ep 2682: ep_len:57 episode reward: total was 27.000000. running mean: -30.348770\n",
      "ep 2682: ep_len:50 episode reward: total was 23.500000. running mean: -29.810283\n",
      "ep 2682: ep_len:116 episode reward: total was 55.000000. running mean: -28.962180\n",
      "ep 2682: ep_len:1206 episode reward: total was 5.230000. running mean: -28.620258\n",
      "ep 2682: ep_len:2788 episode reward: total was -43.840000. running mean: -28.772455\n",
      "ep 2682: ep_len:39 episode reward: total was 18.000000. running mean: -28.304731\n",
      "epsilon:0.009992 episode_count: 40388. steps_count: 43430346.000000\n",
      "ep 2683: ep_len:612 episode reward: total was 27.620000. running mean: -27.745484\n",
      "ep 2683: ep_len:788 episode reward: total was -21.580000. running mean: -27.683829\n",
      "ep 2683: ep_len:65 episode reward: total was 31.000000. running mean: -27.096990\n",
      "ep 2683: ep_len:3036 episode reward: total was -38.340000. running mean: -27.209421\n",
      "ep 2683: ep_len:704 episode reward: total was 1.200000. running mean: -26.925326\n",
      "ep 2683: ep_len:2901 episode reward: total was -433.740000. running mean: -30.993473\n",
      "ep 2683: ep_len:4006 episode reward: total was -173.830000. running mean: -32.421838\n",
      "ep 2683: ep_len:926 episode reward: total was -59.590000. running mean: -32.693520\n",
      "ep 2683: ep_len:7304 episode reward: total was -660.090000. running mean: -38.967485\n",
      "ep 2683: ep_len:500 episode reward: total was 54.840000. running mean: -38.029410\n",
      "ep 2683: ep_len:615 episode reward: total was 8.330000. running mean: -37.565816\n",
      "ep 2683: ep_len:2767 episode reward: total was -0.590000. running mean: -37.196058\n",
      "epsilon:0.009992 episode_count: 40400. steps_count: 43454570.000000\n",
      "ep 2684: ep_len:735 episode reward: total was -41.750000. running mean: -37.241597\n",
      "ep 2684: ep_len:500 episode reward: total was 17.330000. running mean: -36.695881\n",
      "ep 2684: ep_len:58 episode reward: total was 27.500000. running mean: -36.053922\n",
      "ep 2684: ep_len:3004 episode reward: total was -16.440000. running mean: -35.857783\n",
      "ep 2684: ep_len:500 episode reward: total was 5.640000. running mean: -35.442805\n",
      "ep 2684: ep_len:39 episode reward: total was 18.000000. running mean: -34.908377\n",
      "ep 2684: ep_len:142 episode reward: total was 69.500000. running mean: -33.864293\n",
      "ep 2684: ep_len:1429 episode reward: total was -74.040000. running mean: -34.266050\n",
      "ep 2684: ep_len:672 episode reward: total was 28.780000. running mean: -33.635590\n",
      "ep 2684: ep_len:1222 episode reward: total was -81.880000. running mean: -34.118034\n",
      "ep 2684: ep_len:768 episode reward: total was -11.030000. running mean: -33.887154\n",
      "ep 2684: ep_len:608 episode reward: total was -18.330000. running mean: -33.731582\n",
      "ep 2684: ep_len:778 episode reward: total was -48.460000. running mean: -33.878866\n",
      "ep 2684: ep_len:2819 episode reward: total was -24.370000. running mean: -33.783778\n",
      "epsilon:0.009992 episode_count: 40414. steps_count: 43467844.000000\n",
      "ep 2685: ep_len:1156 episode reward: total was 9.350000. running mean: -33.352440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2685: ep_len:726 episode reward: total was -18.880000. running mean: -33.207716\n",
      "ep 2685: ep_len:49 episode reward: total was 23.000000. running mean: -32.645638\n",
      "ep 2685: ep_len:3063 episode reward: total was -172.590000. running mean: -34.045082\n",
      "ep 2685: ep_len:1628 episode reward: total was -85.870000. running mean: -34.563331\n",
      "ep 2685: ep_len:50 episode reward: total was 23.500000. running mean: -33.982698\n",
      "ep 2685: ep_len:81 episode reward: total was 39.000000. running mean: -33.252871\n",
      "ep 2685: ep_len:1611 episode reward: total was -367.860000. running mean: -36.598942\n",
      "ep 2685: ep_len:4084 episode reward: total was -75.150000. running mean: -36.984453\n",
      "ep 2685: ep_len:1249 episode reward: total was -56.360000. running mean: -37.178208\n",
      "ep 2685: ep_len:7543 episode reward: total was -1.710000. running mean: -36.823526\n",
      "ep 2685: ep_len:731 episode reward: total was 30.020000. running mean: -36.155091\n",
      "ep 2685: ep_len:1048 episode reward: total was -12.700000. running mean: -35.920540\n",
      "ep 2685: ep_len:2911 episode reward: total was -47.020000. running mean: -36.031535\n",
      "ep 2685: ep_len:58 episode reward: total was 27.500000. running mean: -35.396219\n",
      "epsilon:0.009992 episode_count: 40429. steps_count: 43493832.000000\n",
      "ep 2686: ep_len:1026 episode reward: total was -141.410000. running mean: -36.456357\n",
      "ep 2686: ep_len:2311 episode reward: total was -248.130000. running mean: -38.573093\n",
      "ep 2686: ep_len:87 episode reward: total was 40.500000. running mean: -37.782363\n",
      "ep 2686: ep_len:1222 episode reward: total was -52.860000. running mean: -37.933139\n",
      "ep 2686: ep_len:38 episode reward: total was 16.000000. running mean: -37.393808\n",
      "ep 2686: ep_len:39 episode reward: total was 16.500000. running mean: -36.854869\n",
      "ep 2686: ep_len:1877 episode reward: total was -127.940000. running mean: -37.765721\n",
      "ep 2686: ep_len:4038 episode reward: total was -43.330000. running mean: -37.821364\n",
      "ep 2686: ep_len:607 episode reward: total was 8.930000. running mean: -37.353850\n",
      "ep 2686: ep_len:691 episode reward: total was 15.170000. running mean: -36.828611\n",
      "ep 2686: ep_len:616 episode reward: total was 7.120000. running mean: -36.389125\n",
      "ep 2686: ep_len:60 episode reward: total was 27.000000. running mean: -35.755234\n",
      "ep 2686: ep_len:708 episode reward: total was -11.270000. running mean: -35.510382\n",
      "ep 2686: ep_len:2859 episode reward: total was -27.250000. running mean: -35.427778\n",
      "epsilon:0.009992 episode_count: 40443. steps_count: 43510011.000000\n",
      "ep 2687: ep_len:1466 episode reward: total was 13.970000. running mean: -34.933800\n",
      "ep 2687: ep_len:965 episode reward: total was -6.740000. running mean: -34.651862\n",
      "ep 2687: ep_len:3037 episode reward: total was -280.010000. running mean: -37.105443\n",
      "ep 2687: ep_len:1175 episode reward: total was -51.040000. running mean: -37.244789\n",
      "ep 2687: ep_len:55 episode reward: total was 24.500000. running mean: -36.627341\n",
      "ep 2687: ep_len:986 episode reward: total was -52.040000. running mean: -36.781468\n",
      "ep 2687: ep_len:3944 episode reward: total was -33.560000. running mean: -36.749253\n",
      "ep 2687: ep_len:998 episode reward: total was -41.080000. running mean: -36.792561\n",
      "ep 2687: ep_len:841 episode reward: total was 37.530000. running mean: -36.049335\n",
      "ep 2687: ep_len:500 episode reward: total was -0.670000. running mean: -35.695542\n",
      "ep 2687: ep_len:149 episode reward: total was 70.000000. running mean: -34.638586\n",
      "ep 2687: ep_len:83 episode reward: total was 38.500000. running mean: -33.907200\n",
      "ep 2687: ep_len:627 episode reward: total was -25.210000. running mean: -33.820228\n",
      "ep 2687: ep_len:2823 episode reward: total was -34.980000. running mean: -33.831826\n",
      "epsilon:0.009992 episode_count: 40457. steps_count: 43527660.000000\n",
      "ep 2688: ep_len:600 episode reward: total was 10.140000. running mean: -33.392108\n",
      "ep 2688: ep_len:500 episode reward: total was 12.950000. running mean: -32.928687\n",
      "ep 2688: ep_len:2873 episode reward: total was -170.840000. running mean: -34.307800\n",
      "ep 2688: ep_len:906 episode reward: total was 75.150000. running mean: -33.213222\n",
      "ep 2688: ep_len:36 episode reward: total was 16.500000. running mean: -32.716090\n",
      "ep 2688: ep_len:40 episode reward: total was 18.500000. running mean: -32.203929\n",
      "ep 2688: ep_len:1396 episode reward: total was -179.120000. running mean: -33.673089\n",
      "ep 2688: ep_len:3945 episode reward: total was -49.220000. running mean: -33.828559\n",
      "ep 2688: ep_len:636 episode reward: total was 19.000000. running mean: -33.300273\n",
      "ep 2688: ep_len:859 episode reward: total was 60.120000. running mean: -32.366070\n",
      "ep 2688: ep_len:1437 episode reward: total was 2.720000. running mean: -32.015209\n",
      "ep 2688: ep_len:63 episode reward: total was 30.000000. running mean: -31.395057\n",
      "ep 2688: ep_len:1156 episode reward: total was -26.990000. running mean: -31.351007\n",
      "ep 2688: ep_len:2862 episode reward: total was -24.520000. running mean: -31.282697\n",
      "ep 2688: ep_len:61 episode reward: total was 24.500000. running mean: -30.724870\n",
      "epsilon:0.009992 episode_count: 40472. steps_count: 43545030.000000\n",
      "ep 2689: ep_len:1449 episode reward: total was 5.260000. running mean: -30.365021\n",
      "ep 2689: ep_len:1248 episode reward: total was -86.670000. running mean: -30.928071\n",
      "ep 2689: ep_len:49 episode reward: total was 23.000000. running mean: -30.388790\n",
      "ep 2689: ep_len:2976 episode reward: total was -85.550000. running mean: -30.940402\n",
      "ep 2689: ep_len:500 episode reward: total was 14.640000. running mean: -30.484598\n",
      "ep 2689: ep_len:64 episode reward: total was 30.500000. running mean: -29.874752\n",
      "ep 2689: ep_len:141 episode reward: total was 64.500000. running mean: -28.931005\n",
      "ep 2689: ep_len:1454 episode reward: total was -120.860000. running mean: -29.850295\n",
      "ep 2689: ep_len:321 episode reward: total was 18.390000. running mean: -29.367892\n",
      "ep 2689: ep_len:833 episode reward: total was 8.560000. running mean: -28.988613\n",
      "ep 2689: ep_len:689 episode reward: total was 9.940000. running mean: -28.599327\n",
      "ep 2689: ep_len:605 episode reward: total was 7.830000. running mean: -28.235033\n",
      "ep 2689: ep_len:56 episode reward: total was 26.500000. running mean: -27.687683\n",
      "ep 2689: ep_len:102 episode reward: total was 49.500000. running mean: -26.915806\n",
      "ep 2689: ep_len:1189 episode reward: total was 7.600000. running mean: -26.570648\n",
      "ep 2689: ep_len:2877 episode reward: total was -22.510000. running mean: -26.530042\n",
      "epsilon:0.009992 episode_count: 40488. steps_count: 43559583.000000\n",
      "ep 2690: ep_len:664 episode reward: total was -5.650000. running mean: -26.321241\n",
      "ep 2690: ep_len:689 episode reward: total was -15.800000. running mean: -26.216029\n",
      "ep 2690: ep_len:2981 episode reward: total was -255.020000. running mean: -28.504069\n",
      "ep 2690: ep_len:770 episode reward: total was -44.410000. running mean: -28.663128\n",
      "ep 2690: ep_len:24 episode reward: total was 10.500000. running mean: -28.271497\n",
      "ep 2690: ep_len:116 episode reward: total was 56.500000. running mean: -27.423782\n",
      "ep 2690: ep_len:53 episode reward: total was 23.500000. running mean: -26.914544\n",
      "ep 2690: ep_len:1436 episode reward: total was -151.450000. running mean: -28.159898\n",
      "ep 2690: ep_len:347 episode reward: total was 7.630000. running mean: -27.801999\n",
      "ep 2690: ep_len:783 episode reward: total was -97.510000. running mean: -28.499079\n",
      "ep 2690: ep_len:865 episode reward: total was 53.040000. running mean: -27.683689\n",
      "ep 2690: ep_len:830 episode reward: total was 25.210000. running mean: -27.154752\n",
      "ep 2690: ep_len:69 episode reward: total was 33.000000. running mean: -26.553204\n",
      "ep 2690: ep_len:675 episode reward: total was -3.520000. running mean: -26.322872\n",
      "ep 2690: ep_len:2921 episode reward: total was -19.340000. running mean: -26.253043\n",
      "epsilon:0.009992 episode_count: 40503. steps_count: 43572806.000000\n",
      "ep 2691: ep_len:659 episode reward: total was -46.740000. running mean: -26.457913\n",
      "ep 2691: ep_len:716 episode reward: total was -18.590000. running mean: -26.379234\n",
      "ep 2691: ep_len:69 episode reward: total was 33.000000. running mean: -25.785442\n",
      "ep 2691: ep_len:3029 episode reward: total was -120.430000. running mean: -26.731887\n",
      "ep 2691: ep_len:794 episode reward: total was 37.980000. running mean: -26.084768\n",
      "ep 2691: ep_len:109 episode reward: total was 53.000000. running mean: -25.293921\n",
      "ep 2691: ep_len:76 episode reward: total was 36.500000. running mean: -24.675981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2691: ep_len:500 episode reward: total was 2.210000. running mean: -24.407122\n",
      "ep 2691: ep_len:355 episode reward: total was 15.150000. running mean: -24.011550\n",
      "ep 2691: ep_len:552 episode reward: total was -55.250000. running mean: -24.323935\n",
      "ep 2691: ep_len:844 episode reward: total was 64.440000. running mean: -23.436296\n",
      "ep 2691: ep_len:671 episode reward: total was -0.200000. running mean: -23.203933\n",
      "ep 2691: ep_len:59 episode reward: total was 26.500000. running mean: -22.706893\n",
      "ep 2691: ep_len:81 episode reward: total was 39.000000. running mean: -22.089824\n",
      "ep 2691: ep_len:501 episode reward: total was -2.070000. running mean: -21.889626\n",
      "ep 2691: ep_len:2911 episode reward: total was -13.140000. running mean: -21.802130\n",
      "epsilon:0.009992 episode_count: 40519. steps_count: 43584732.000000\n",
      "ep 2692: ep_len:1427 episode reward: total was 23.770000. running mean: -21.346409\n",
      "ep 2692: ep_len:823 episode reward: total was 11.870000. running mean: -21.014244\n",
      "ep 2692: ep_len:3099 episode reward: total was -33.000000. running mean: -21.134102\n",
      "ep 2692: ep_len:1694 episode reward: total was -80.280000. running mean: -21.725561\n",
      "ep 2692: ep_len:41 episode reward: total was 19.000000. running mean: -21.318305\n",
      "ep 2692: ep_len:108 episode reward: total was 52.500000. running mean: -20.580122\n",
      "ep 2692: ep_len:101 episode reward: total was 47.500000. running mean: -19.899321\n",
      "ep 2692: ep_len:500 episode reward: total was 39.450000. running mean: -19.305828\n",
      "ep 2692: ep_len:3545 episode reward: total was -2749.800000. running mean: -46.610770\n",
      "ep 2692: ep_len:955 episode reward: total was -43.260000. running mean: -46.577262\n",
      "ep 2692: ep_len:784 episode reward: total was 15.610000. running mean: -45.955389\n",
      "ep 2692: ep_len:899 episode reward: total was -6.200000. running mean: -45.557835\n",
      "ep 2692: ep_len:176 episode reward: total was 83.500000. running mean: -44.267257\n",
      "ep 2692: ep_len:110 episode reward: total was 52.000000. running mean: -43.304584\n",
      "ep 2692: ep_len:491 episode reward: total was 46.730000. running mean: -42.404239\n",
      "ep 2692: ep_len:2847 episode reward: total was 7.220000. running mean: -41.907996\n",
      "epsilon:0.009992 episode_count: 40535. steps_count: 43602332.000000\n",
      "ep 2693: ep_len:635 episode reward: total was 28.980000. running mean: -41.199116\n",
      "ep 2693: ep_len:639 episode reward: total was 3.160000. running mean: -40.755525\n",
      "ep 2693: ep_len:3071 episode reward: total was -67.400000. running mean: -41.021970\n",
      "ep 2693: ep_len:764 episode reward: total was -18.790000. running mean: -40.799650\n",
      "ep 2693: ep_len:71 episode reward: total was 34.000000. running mean: -40.051654\n",
      "ep 2693: ep_len:1504 episode reward: total was 15.360000. running mean: -39.497537\n",
      "ep 2693: ep_len:341 episode reward: total was 15.530000. running mean: -38.947262\n",
      "ep 2693: ep_len:1630 episode reward: total was -23.230000. running mean: -38.790089\n",
      "ep 2693: ep_len:863 episode reward: total was 27.580000. running mean: -38.126388\n",
      "ep 2693: ep_len:500 episode reward: total was 11.210000. running mean: -37.633024\n",
      "ep 2693: ep_len:81 episode reward: total was 39.000000. running mean: -36.866694\n",
      "ep 2693: ep_len:32 episode reward: total was 14.500000. running mean: -36.353027\n",
      "ep 2693: ep_len:1544 episode reward: total was 14.230000. running mean: -35.847197\n",
      "ep 2693: ep_len:2756 episode reward: total was -3.030000. running mean: -35.519025\n",
      "ep 2693: ep_len:46 episode reward: total was 18.500000. running mean: -34.978835\n",
      "epsilon:0.009992 episode_count: 40550. steps_count: 43616809.000000\n",
      "ep 2694: ep_len:649 episode reward: total was 9.780000. running mean: -34.531246\n",
      "ep 2694: ep_len:683 episode reward: total was -28.690000. running mean: -34.472834\n",
      "ep 2694: ep_len:3046 episode reward: total was -44.520000. running mean: -34.573306\n",
      "ep 2694: ep_len:506 episode reward: total was -19.720000. running mean: -34.424772\n",
      "ep 2694: ep_len:33 episode reward: total was 13.500000. running mean: -33.945525\n",
      "ep 2694: ep_len:131 episode reward: total was 58.000000. running mean: -33.026069\n",
      "ep 2694: ep_len:65 episode reward: total was 31.000000. running mean: -32.385809\n",
      "ep 2694: ep_len:610 episode reward: total was 32.170000. running mean: -31.740251\n",
      "ep 2694: ep_len:4297 episode reward: total was -156.150000. running mean: -32.984348\n",
      "ep 2694: ep_len:1221 episode reward: total was -57.650000. running mean: -33.231005\n",
      "ep 2694: ep_len:7252 episode reward: total was 28.890000. running mean: -32.609795\n",
      "ep 2694: ep_len:662 episode reward: total was -6.500000. running mean: -32.348697\n",
      "ep 2694: ep_len:657 episode reward: total was 8.200000. running mean: -31.943210\n",
      "ep 2694: ep_len:2848 episode reward: total was 9.760000. running mean: -31.526178\n",
      "epsilon:0.009992 episode_count: 40564. steps_count: 43639469.000000\n",
      "ep 2695: ep_len:842 episode reward: total was -26.080000. running mean: -31.471716\n",
      "ep 2695: ep_len:817 episode reward: total was -50.020000. running mean: -31.657199\n",
      "ep 2695: ep_len:44 episode reward: total was 20.500000. running mean: -31.135627\n",
      "ep 2695: ep_len:3111 episode reward: total was -99.670000. running mean: -31.820970\n",
      "ep 2695: ep_len:738 episode reward: total was -11.980000. running mean: -31.622561\n",
      "ep 2695: ep_len:35 episode reward: total was 13.000000. running mean: -31.176335\n",
      "ep 2695: ep_len:990 episode reward: total was -37.580000. running mean: -31.240372\n",
      "ep 2695: ep_len:353 episode reward: total was 17.210000. running mean: -30.755868\n",
      "ep 2695: ep_len:930 episode reward: total was -26.190000. running mean: -30.710209\n",
      "ep 2695: ep_len:766 episode reward: total was 2.470000. running mean: -30.378407\n",
      "ep 2695: ep_len:984 episode reward: total was 0.090000. running mean: -30.073723\n",
      "ep 2695: ep_len:61 episode reward: total was 27.500000. running mean: -29.497986\n",
      "ep 2695: ep_len:154 episode reward: total was 72.500000. running mean: -28.478006\n",
      "ep 2695: ep_len:83 episode reward: total was 40.000000. running mean: -27.793226\n",
      "ep 2695: ep_len:909 episode reward: total was -64.450000. running mean: -28.159794\n",
      "ep 2695: ep_len:2879 episode reward: total was 2.460000. running mean: -27.853596\n",
      "epsilon:0.009992 episode_count: 40580. steps_count: 43653165.000000\n",
      "ep 2696: ep_len:734 episode reward: total was -101.910000. running mean: -28.594160\n",
      "ep 2696: ep_len:743 episode reward: total was -7.990000. running mean: -28.388118\n",
      "ep 2696: ep_len:3022 episode reward: total was -112.780000. running mean: -29.232037\n",
      "ep 2696: ep_len:698 episode reward: total was 1.350000. running mean: -28.926217\n",
      "ep 2696: ep_len:82 episode reward: total was 39.500000. running mean: -28.241955\n",
      "ep 2696: ep_len:1003 episode reward: total was -57.690000. running mean: -28.536435\n",
      "ep 2696: ep_len:3724 episode reward: total was -459.120000. running mean: -32.842271\n",
      "ep 2696: ep_len:544 episode reward: total was -35.130000. running mean: -32.865148\n",
      "ep 2696: ep_len:7304 episode reward: total was -51.640000. running mean: -33.052897\n",
      "ep 2696: ep_len:1424 episode reward: total was 12.720000. running mean: -32.595168\n",
      "ep 2696: ep_len:56 episode reward: total was 26.500000. running mean: -32.004216\n",
      "ep 2696: ep_len:91 episode reward: total was 44.000000. running mean: -31.244174\n",
      "ep 2696: ep_len:1135 episode reward: total was -3.110000. running mean: -30.962832\n",
      "ep 2696: ep_len:2863 episode reward: total was -4.650000. running mean: -30.699704\n",
      "epsilon:0.009992 episode_count: 40594. steps_count: 43676588.000000\n",
      "ep 2697: ep_len:819 episode reward: total was -18.820000. running mean: -30.580907\n",
      "ep 2697: ep_len:682 episode reward: total was -28.700000. running mean: -30.562098\n",
      "ep 2697: ep_len:47 episode reward: total was 22.000000. running mean: -30.036477\n",
      "ep 2697: ep_len:2968 episode reward: total was -115.340000. running mean: -30.889512\n",
      "ep 2697: ep_len:862 episode reward: total was 7.620000. running mean: -30.504417\n",
      "ep 2697: ep_len:38 episode reward: total was 16.000000. running mean: -30.039373\n",
      "ep 2697: ep_len:119 episode reward: total was 55.000000. running mean: -29.188979\n",
      "ep 2697: ep_len:42 episode reward: total was 19.500000. running mean: -28.702089\n",
      "ep 2697: ep_len:992 episode reward: total was -3.520000. running mean: -28.450268\n",
      "ep 2697: ep_len:615 episode reward: total was 16.360000. running mean: -28.002165\n",
      "ep 2697: ep_len:1156 episode reward: total was -57.900000. running mean: -28.301144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2697: ep_len:826 episode reward: total was 4.060000. running mean: -27.977532\n",
      "ep 2697: ep_len:1469 episode reward: total was -15.420000. running mean: -27.851957\n",
      "ep 2697: ep_len:500 episode reward: total was 35.890000. running mean: -27.214537\n",
      "ep 2697: ep_len:2792 episode reward: total was 7.160000. running mean: -26.870792\n",
      "epsilon:0.009992 episode_count: 40609. steps_count: 43690515.000000\n",
      "ep 2698: ep_len:1137 episode reward: total was -3.950000. running mean: -26.641584\n",
      "ep 2698: ep_len:681 episode reward: total was -16.310000. running mean: -26.538268\n",
      "ep 2698: ep_len:2892 episode reward: total was -216.310000. running mean: -28.435986\n",
      "ep 2698: ep_len:500 episode reward: total was -3.400000. running mean: -28.185626\n",
      "ep 2698: ep_len:60 episode reward: total was 28.500000. running mean: -27.618770\n",
      "ep 2698: ep_len:500 episode reward: total was -5.960000. running mean: -27.402182\n",
      "ep 2698: ep_len:3957 episode reward: total was -98.710000. running mean: -28.115260\n",
      "ep 2698: ep_len:540 episode reward: total was 7.990000. running mean: -27.754207\n",
      "ep 2698: ep_len:743 episode reward: total was 37.830000. running mean: -27.098365\n",
      "ep 2698: ep_len:1436 episode reward: total was -32.370000. running mean: -27.151082\n",
      "ep 2698: ep_len:186 episode reward: total was 90.000000. running mean: -25.979571\n",
      "ep 2698: ep_len:104 episode reward: total was 50.500000. running mean: -25.214775\n",
      "ep 2698: ep_len:1203 episode reward: total was -10.660000. running mean: -25.069227\n",
      "ep 2698: ep_len:2849 episode reward: total was 4.210000. running mean: -24.776435\n",
      "epsilon:0.009992 episode_count: 40623. steps_count: 43707303.000000\n",
      "ep 2699: ep_len:1444 episode reward: total was 30.370000. running mean: -24.224971\n",
      "ep 2699: ep_len:841 episode reward: total was 16.310000. running mean: -23.819621\n",
      "ep 2699: ep_len:31 episode reward: total was 12.500000. running mean: -23.456425\n",
      "ep 2699: ep_len:2990 episode reward: total was -96.720000. running mean: -24.189061\n",
      "ep 2699: ep_len:734 episode reward: total was -3.940000. running mean: -23.986570\n",
      "ep 2699: ep_len:28 episode reward: total was 12.500000. running mean: -23.621704\n",
      "ep 2699: ep_len:173 episode reward: total was 82.000000. running mean: -22.565487\n",
      "ep 2699: ep_len:62 episode reward: total was 28.000000. running mean: -22.059832\n",
      "ep 2699: ep_len:645 episode reward: total was -122.230000. running mean: -23.061534\n",
      "ep 2699: ep_len:642 episode reward: total was 23.670000. running mean: -22.594219\n",
      "ep 2699: ep_len:898 episode reward: total was -33.610000. running mean: -22.704377\n",
      "ep 2699: ep_len:751 episode reward: total was 1.710000. running mean: -22.460233\n",
      "ep 2699: ep_len:730 episode reward: total was 6.160000. running mean: -22.174030\n",
      "ep 2699: ep_len:68 episode reward: total was 32.500000. running mean: -21.627290\n",
      "ep 2699: ep_len:855 episode reward: total was 2.250000. running mean: -21.388517\n",
      "ep 2699: ep_len:2838 episode reward: total was -49.930000. running mean: -21.673932\n",
      "epsilon:0.009992 episode_count: 40639. steps_count: 43721033.000000\n",
      "ep 2700: ep_len:1131 episode reward: total was -6.030000. running mean: -21.517493\n",
      "ep 2700: ep_len:204 episode reward: total was -4.820000. running mean: -21.350518\n",
      "ep 2700: ep_len:67 episode reward: total was 30.500000. running mean: -20.832013\n",
      "ep 2700: ep_len:3061 episode reward: total was -54.600000. running mean: -21.169693\n",
      "ep 2700: ep_len:635 episode reward: total was -0.410000. running mean: -20.962096\n",
      "ep 2700: ep_len:62 episode reward: total was 29.500000. running mean: -20.457475\n",
      "ep 2700: ep_len:113 episode reward: total was 55.000000. running mean: -19.702900\n",
      "ep 2700: ep_len:101 episode reward: total was 49.000000. running mean: -19.015871\n",
      "ep 2700: ep_len:1132 episode reward: total was 5.770000. running mean: -18.768012\n",
      "ep 2700: ep_len:4007 episode reward: total was -78.380000. running mean: -19.364132\n",
      "ep 2700: ep_len:4242 episode reward: total was -520.460000. running mean: -24.375091\n",
      "ep 2700: ep_len:744 episode reward: total was -4.450000. running mean: -24.175840\n",
      "ep 2700: ep_len:1486 episode reward: total was -4.350000. running mean: -23.977581\n",
      "ep 2700: ep_len:85 episode reward: total was 39.500000. running mean: -23.342806\n",
      "ep 2700: ep_len:45 episode reward: total was 19.500000. running mean: -22.914378\n",
      "ep 2700: ep_len:1128 episode reward: total was -32.530000. running mean: -23.010534\n",
      "ep 2700: ep_len:2863 episode reward: total was -5.940000. running mean: -22.839828\n",
      "epsilon:0.009992 episode_count: 40656. steps_count: 43742139.000000\n",
      "ep 2701: ep_len:730 episode reward: total was -31.700000. running mean: -22.928430\n",
      "ep 2701: ep_len:972 episode reward: total was 33.800000. running mean: -22.361146\n",
      "ep 2701: ep_len:2941 episode reward: total was -41.270000. running mean: -22.550234\n",
      "ep 2701: ep_len:526 episode reward: total was -31.720000. running mean: -22.641932\n",
      "ep 2701: ep_len:42 episode reward: total was 18.000000. running mean: -22.235513\n",
      "ep 2701: ep_len:163 episode reward: total was 80.000000. running mean: -21.213158\n",
      "ep 2701: ep_len:59 episode reward: total was 25.000000. running mean: -20.751026\n",
      "ep 2701: ep_len:500 episode reward: total was -27.630000. running mean: -20.819816\n",
      "ep 2701: ep_len:348 episode reward: total was 13.270000. running mean: -20.478918\n",
      "ep 2701: ep_len:560 episode reward: total was 3.510000. running mean: -20.239028\n",
      "ep 2701: ep_len:771 episode reward: total was 41.390000. running mean: -19.622738\n",
      "ep 2701: ep_len:1040 episode reward: total was 34.050000. running mean: -19.086011\n",
      "ep 2701: ep_len:1481 episode reward: total was -14.410000. running mean: -19.039251\n",
      "ep 2701: ep_len:2794 episode reward: total was -8.560000. running mean: -18.934458\n",
      "ep 2701: ep_len:59 episode reward: total was 26.500000. running mean: -18.480114\n",
      "epsilon:0.009992 episode_count: 40671. steps_count: 43755125.000000\n",
      "ep 2702: ep_len:1032 episode reward: total was -126.690000. running mean: -19.562212\n",
      "ep 2702: ep_len:782 episode reward: total was 13.490000. running mean: -19.231690\n",
      "ep 2702: ep_len:3047 episode reward: total was -25.760000. running mean: -19.296973\n",
      "ep 2702: ep_len:653 episode reward: total was -6.930000. running mean: -19.173304\n",
      "ep 2702: ep_len:57 episode reward: total was 27.000000. running mean: -18.711571\n",
      "ep 2702: ep_len:1419 episode reward: total was -90.530000. running mean: -19.429755\n",
      "ep 2702: ep_len:322 episode reward: total was 16.500000. running mean: -19.070457\n",
      "ep 2702: ep_len:914 episode reward: total was -16.520000. running mean: -19.044953\n",
      "ep 2702: ep_len:694 episode reward: total was 54.580000. running mean: -18.308703\n",
      "ep 2702: ep_len:725 episode reward: total was 13.980000. running mean: -17.985816\n",
      "ep 2702: ep_len:157 episode reward: total was 74.000000. running mean: -17.065958\n",
      "ep 2702: ep_len:36 episode reward: total was 16.500000. running mean: -16.730299\n",
      "ep 2702: ep_len:87 episode reward: total was 39.000000. running mean: -16.172996\n",
      "ep 2702: ep_len:1085 episode reward: total was -27.700000. running mean: -16.288266\n",
      "ep 2702: ep_len:2681 episode reward: total was -5.370000. running mean: -16.179083\n",
      "epsilon:0.009992 episode_count: 40686. steps_count: 43768816.000000\n",
      "ep 2703: ep_len:765 episode reward: total was -79.380000. running mean: -16.811092\n",
      "ep 2703: ep_len:695 episode reward: total was -14.600000. running mean: -16.788981\n",
      "ep 2703: ep_len:2861 episode reward: total was -30.620000. running mean: -16.927291\n",
      "ep 2703: ep_len:500 episode reward: total was 23.860000. running mean: -16.519418\n",
      "ep 2703: ep_len:53 episode reward: total was 23.500000. running mean: -16.119224\n",
      "ep 2703: ep_len:500 episode reward: total was 19.790000. running mean: -15.760132\n",
      "ep 2703: ep_len:353 episode reward: total was 16.140000. running mean: -15.441131\n",
      "ep 2703: ep_len:528 episode reward: total was -3.490000. running mean: -15.321619\n",
      "ep 2703: ep_len:790 episode reward: total was 0.390000. running mean: -15.164503\n",
      "ep 2703: ep_len:737 episode reward: total was 17.830000. running mean: -14.834558\n",
      "ep 2703: ep_len:71 episode reward: total was 32.500000. running mean: -14.361213\n",
      "ep 2703: ep_len:932 episode reward: total was 19.040000. running mean: -14.027200\n",
      "ep 2703: ep_len:2751 episode reward: total was 7.480000. running mean: -13.812128\n",
      "epsilon:0.009992 episode_count: 40699. steps_count: 43780352.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2704: ep_len:1012 episode reward: total was -39.540000. running mean: -14.069407\n",
      "ep 2704: ep_len:211 episode reward: total was 15.450000. running mean: -13.774213\n",
      "ep 2704: ep_len:57 episode reward: total was 27.000000. running mean: -13.366471\n",
      "ep 2704: ep_len:3020 episode reward: total was -9.160000. running mean: -13.324406\n",
      "ep 2704: ep_len:653 episode reward: total was 13.140000. running mean: -13.059762\n",
      "ep 2704: ep_len:159 episode reward: total was 72.000000. running mean: -12.209165\n",
      "ep 2704: ep_len:65 episode reward: total was 29.500000. running mean: -11.792073\n",
      "ep 2704: ep_len:593 episode reward: total was 28.630000. running mean: -11.387852\n",
      "ep 2704: ep_len:4167 episode reward: total was -96.870000. running mean: -12.242674\n",
      "ep 2704: ep_len:1279 episode reward: total was -110.600000. running mean: -13.226247\n",
      "ep 2704: ep_len:784 episode reward: total was 16.220000. running mean: -12.931784\n",
      "ep 2704: ep_len:500 episode reward: total was 18.500000. running mean: -12.617467\n",
      "ep 2704: ep_len:94 episode reward: total was 44.000000. running mean: -12.051292\n",
      "ep 2704: ep_len:154 episode reward: total was 75.500000. running mean: -11.175779\n",
      "ep 2704: ep_len:101 episode reward: total was 46.000000. running mean: -10.604021\n",
      "ep 2704: ep_len:1110 episode reward: total was -26.440000. running mean: -10.762381\n",
      "ep 2704: ep_len:2822 episode reward: total was -33.410000. running mean: -10.988857\n",
      "epsilon:0.009992 episode_count: 40716. steps_count: 43797133.000000\n",
      "ep 2705: ep_len:670 episode reward: total was 6.530000. running mean: -10.813669\n",
      "ep 2705: ep_len:1520 episode reward: total was -103.170000. running mean: -11.737232\n",
      "ep 2705: ep_len:44 episode reward: total was 19.000000. running mean: -11.429860\n",
      "ep 2705: ep_len:3094 episode reward: total was -6.490000. running mean: -11.380461\n",
      "ep 2705: ep_len:1245 episode reward: total was -35.360000. running mean: -11.620256\n",
      "ep 2705: ep_len:107 episode reward: total was 50.500000. running mean: -10.999054\n",
      "ep 2705: ep_len:38 episode reward: total was 16.000000. running mean: -10.729063\n",
      "ep 2705: ep_len:1443 episode reward: total was -67.550000. running mean: -11.297273\n",
      "ep 2705: ep_len:3897 episode reward: total was -38.190000. running mean: -11.566200\n",
      "ep 2705: ep_len:790 episode reward: total was -25.980000. running mean: -11.710338\n",
      "ep 2705: ep_len:739 episode reward: total was 18.190000. running mean: -11.411335\n",
      "ep 2705: ep_len:701 episode reward: total was -29.960000. running mean: -11.596821\n",
      "ep 2705: ep_len:89 episode reward: total was 41.500000. running mean: -11.065853\n",
      "ep 2705: ep_len:2449 episode reward: total was -624.100000. running mean: -17.196195\n",
      "ep 2705: ep_len:2847 episode reward: total was 14.960000. running mean: -16.874633\n",
      "epsilon:0.009992 episode_count: 40731. steps_count: 43816806.000000\n",
      "ep 2706: ep_len:1415 episode reward: total was 24.570000. running mean: -16.460186\n",
      "ep 2706: ep_len:678 episode reward: total was -14.600000. running mean: -16.441584\n",
      "ep 2706: ep_len:2971 episode reward: total was 23.320000. running mean: -16.043969\n",
      "ep 2706: ep_len:1240 episode reward: total was -10.650000. running mean: -15.990029\n",
      "ep 2706: ep_len:50 episode reward: total was 23.500000. running mean: -15.595129\n",
      "ep 2706: ep_len:48 episode reward: total was 22.500000. running mean: -15.214177\n",
      "ep 2706: ep_len:1028 episode reward: total was 15.450000. running mean: -14.907535\n",
      "ep 2706: ep_len:327 episode reward: total was 24.630000. running mean: -14.512160\n",
      "ep 2706: ep_len:783 episode reward: total was -32.230000. running mean: -14.689339\n",
      "ep 2706: ep_len:666 episode reward: total was -8.010000. running mean: -14.622545\n",
      "ep 2706: ep_len:633 episode reward: total was 5.020000. running mean: -14.426120\n",
      "ep 2706: ep_len:64 episode reward: total was 29.000000. running mean: -13.991859\n",
      "ep 2706: ep_len:105 episode reward: total was 49.500000. running mean: -13.356940\n",
      "ep 2706: ep_len:64 episode reward: total was 30.500000. running mean: -12.918371\n",
      "ep 2706: ep_len:713 episode reward: total was 1.910000. running mean: -12.770087\n",
      "ep 2706: ep_len:2749 episode reward: total was 8.220000. running mean: -12.560186\n",
      "epsilon:0.009992 episode_count: 40747. steps_count: 43830340.000000\n",
      "ep 2707: ep_len:669 episode reward: total was 3.490000. running mean: -12.399684\n",
      "ep 2707: ep_len:680 episode reward: total was -28.720000. running mean: -12.562887\n",
      "ep 2707: ep_len:71 episode reward: total was 34.000000. running mean: -12.097258\n",
      "ep 2707: ep_len:3070 episode reward: total was 9.770000. running mean: -11.878586\n",
      "ep 2707: ep_len:1650 episode reward: total was -79.410000. running mean: -12.553900\n",
      "ep 2707: ep_len:55 episode reward: total was 24.500000. running mean: -12.183361\n",
      "ep 2707: ep_len:128 episode reward: total was 59.500000. running mean: -11.466527\n",
      "ep 2707: ep_len:41 episode reward: total was 19.000000. running mean: -11.161862\n",
      "ep 2707: ep_len:876 episode reward: total was 3.880000. running mean: -11.011443\n",
      "ep 2707: ep_len:299 episode reward: total was 14.130000. running mean: -10.760029\n",
      "ep 2707: ep_len:4186 episode reward: total was -603.400000. running mean: -16.686429\n",
      "ep 2707: ep_len:698 episode reward: total was 24.360000. running mean: -16.275964\n",
      "ep 2707: ep_len:913 episode reward: total was 3.190000. running mean: -16.081305\n",
      "ep 2707: ep_len:621 episode reward: total was 14.680000. running mean: -15.773692\n",
      "ep 2707: ep_len:2854 episode reward: total was 11.850000. running mean: -15.497455\n",
      "epsilon:0.009992 episode_count: 40762. steps_count: 43847151.000000\n",
      "ep 2708: ep_len:1137 episode reward: total was 4.590000. running mean: -15.296580\n",
      "ep 2708: ep_len:745 episode reward: total was -0.800000. running mean: -15.151614\n",
      "ep 2708: ep_len:2962 episode reward: total was 8.930000. running mean: -14.910798\n",
      "ep 2708: ep_len:897 episode reward: total was 78.770000. running mean: -13.973990\n",
      "ep 2708: ep_len:112 episode reward: total was 48.500000. running mean: -13.349250\n",
      "ep 2708: ep_len:102 episode reward: total was 45.000000. running mean: -12.765758\n",
      "ep 2708: ep_len:1003 episode reward: total was -1.250000. running mean: -12.650600\n",
      "ep 2708: ep_len:688 episode reward: total was 21.530000. running mean: -12.308794\n",
      "ep 2708: ep_len:1604 episode reward: total was -101.390000. running mean: -13.199606\n",
      "ep 2708: ep_len:708 episode reward: total was 25.900000. running mean: -12.808610\n",
      "ep 2708: ep_len:582 episode reward: total was 32.400000. running mean: -12.356524\n",
      "ep 2708: ep_len:689 episode reward: total was -3.340000. running mean: -12.266359\n",
      "ep 2708: ep_len:2770 episode reward: total was 5.740000. running mean: -12.086295\n",
      "epsilon:0.009992 episode_count: 40775. steps_count: 43861150.000000\n",
      "ep 2709: ep_len:833 episode reward: total was 20.710000. running mean: -11.758332\n",
      "ep 2709: ep_len:203 episode reward: total was 5.270000. running mean: -11.588049\n",
      "ep 2709: ep_len:66 episode reward: total was 30.000000. running mean: -11.172169\n",
      "ep 2709: ep_len:3122 episode reward: total was 18.430000. running mean: -10.876147\n",
      "ep 2709: ep_len:500 episode reward: total was 20.610000. running mean: -10.561285\n",
      "ep 2709: ep_len:60 episode reward: total was 27.000000. running mean: -10.185673\n",
      "ep 2709: ep_len:500 episode reward: total was 25.510000. running mean: -9.828716\n",
      "ep 2709: ep_len:622 episode reward: total was 17.410000. running mean: -9.556329\n",
      "ep 2709: ep_len:1556 episode reward: total was -105.050000. running mean: -10.511265\n",
      "ep 2709: ep_len:791 episode reward: total was 32.550000. running mean: -10.080653\n",
      "ep 2709: ep_len:500 episode reward: total was -8.010000. running mean: -10.059946\n",
      "ep 2709: ep_len:86 episode reward: total was 41.500000. running mean: -9.544347\n",
      "ep 2709: ep_len:148 episode reward: total was 71.000000. running mean: -8.738903\n",
      "ep 2709: ep_len:95 episode reward: total was 46.000000. running mean: -8.191514\n",
      "ep 2709: ep_len:500 episode reward: total was 30.350000. running mean: -7.806099\n",
      "ep 2709: ep_len:2865 episode reward: total was -39.190000. running mean: -8.119938\n",
      "ep 2709: ep_len:64 episode reward: total was 29.000000. running mean: -7.748739\n",
      "epsilon:0.009992 episode_count: 40792. steps_count: 43873661.000000\n",
      "ep 2710: ep_len:628 episode reward: total was -23.180000. running mean: -7.903051\n",
      "ep 2710: ep_len:500 episode reward: total was 11.390000. running mean: -7.710121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2710: ep_len:50 episode reward: total was 23.500000. running mean: -7.398020\n",
      "ep 2710: ep_len:3040 episode reward: total was 19.230000. running mean: -7.131739\n",
      "ep 2710: ep_len:515 episode reward: total was -33.400000. running mean: -7.394422\n",
      "ep 2710: ep_len:68 episode reward: total was 32.500000. running mean: -6.995478\n",
      "ep 2710: ep_len:137 episode reward: total was 64.000000. running mean: -6.285523\n",
      "ep 2710: ep_len:1376 episode reward: total was -78.840000. running mean: -7.011068\n",
      "ep 2710: ep_len:3824 episode reward: total was -552.340000. running mean: -12.464357\n",
      "ep 2710: ep_len:1318 episode reward: total was -65.770000. running mean: -12.997414\n",
      "ep 2710: ep_len:7261 episode reward: total was 39.420000. running mean: -12.473239\n",
      "ep 2710: ep_len:981 episode reward: total was 26.690000. running mean: -12.081607\n",
      "ep 2710: ep_len:62 episode reward: total was 29.500000. running mean: -11.665791\n",
      "ep 2710: ep_len:128 episode reward: total was 62.500000. running mean: -10.924133\n",
      "ep 2710: ep_len:642 episode reward: total was 0.680000. running mean: -10.808092\n",
      "ep 2710: ep_len:2820 episode reward: total was 1.900000. running mean: -10.681011\n",
      "epsilon:0.009992 episode_count: 40808. steps_count: 43897011.000000\n",
      "ep 2711: ep_len:1118 episode reward: total was -8.180000. running mean: -10.656001\n",
      "ep 2711: ep_len:820 episode reward: total was -14.090000. running mean: -10.690341\n",
      "ep 2711: ep_len:3023 episode reward: total was -22.940000. running mean: -10.812837\n",
      "ep 2711: ep_len:777 episode reward: total was 14.790000. running mean: -10.556809\n",
      "ep 2711: ep_len:73 episode reward: total was 35.000000. running mean: -10.101241\n",
      "ep 2711: ep_len:58 episode reward: total was 26.000000. running mean: -9.740228\n",
      "ep 2711: ep_len:797 episode reward: total was 23.780000. running mean: -9.405026\n",
      "ep 2711: ep_len:4005 episode reward: total was -63.680000. running mean: -9.947776\n",
      "ep 2711: ep_len:1311 episode reward: total was -50.530000. running mean: -10.353598\n",
      "ep 2711: ep_len:755 episode reward: total was 54.820000. running mean: -9.701862\n",
      "ep 2711: ep_len:589 episode reward: total was -9.830000. running mean: -9.703144\n",
      "ep 2711: ep_len:68 episode reward: total was 31.000000. running mean: -9.296112\n",
      "ep 2711: ep_len:174 episode reward: total was 82.500000. running mean: -8.378151\n",
      "ep 2711: ep_len:45 episode reward: total was 21.000000. running mean: -8.084369\n",
      "ep 2711: ep_len:97 episode reward: total was 45.500000. running mean: -7.548526\n",
      "ep 2711: ep_len:1081 episode reward: total was 10.120000. running mean: -7.371841\n",
      "ep 2711: ep_len:2805 episode reward: total was 3.740000. running mean: -7.260722\n",
      "epsilon:0.009992 episode_count: 40825. steps_count: 43914607.000000\n",
      "ep 2712: ep_len:1098 episode reward: total was -0.300000. running mean: -7.191115\n",
      "ep 2712: ep_len:804 episode reward: total was -18.390000. running mean: -7.303104\n",
      "ep 2712: ep_len:47 episode reward: total was 20.500000. running mean: -7.025073\n",
      "ep 2712: ep_len:2923 episode reward: total was -27.380000. running mean: -7.228622\n",
      "ep 2712: ep_len:502 episode reward: total was -0.690000. running mean: -7.163236\n",
      "ep 2712: ep_len:62 episode reward: total was 29.500000. running mean: -6.796603\n",
      "ep 2712: ep_len:40 episode reward: total was 18.500000. running mean: -6.543637\n",
      "ep 2712: ep_len:1045 episode reward: total was -65.300000. running mean: -7.131201\n",
      "ep 2712: ep_len:3941 episode reward: total was -79.470000. running mean: -7.854589\n",
      "ep 2712: ep_len:553 episode reward: total was -11.810000. running mean: -7.894143\n",
      "ep 2712: ep_len:625 episode reward: total was -5.430000. running mean: -7.869502\n",
      "ep 2712: ep_len:660 episode reward: total was -37.860000. running mean: -8.169407\n",
      "ep 2712: ep_len:90 episode reward: total was 43.500000. running mean: -7.652713\n",
      "ep 2712: ep_len:1191 episode reward: total was 2.480000. running mean: -7.551385\n",
      "ep 2712: ep_len:2920 episode reward: total was -11.950000. running mean: -7.595372\n",
      "epsilon:0.009992 episode_count: 40840. steps_count: 43931108.000000\n",
      "ep 2713: ep_len:3593 episode reward: total was -403.590000. running mean: -11.555318\n",
      "ep 2713: ep_len:500 episode reward: total was -0.150000. running mean: -11.441265\n",
      "ep 2713: ep_len:49 episode reward: total was 23.000000. running mean: -11.096852\n",
      "ep 2713: ep_len:3008 episode reward: total was -16.940000. running mean: -11.155284\n",
      "ep 2713: ep_len:634 episode reward: total was 4.890000. running mean: -10.994831\n",
      "ep 2713: ep_len:37 episode reward: total was 17.000000. running mean: -10.714882\n",
      "ep 2713: ep_len:86 episode reward: total was 41.500000. running mean: -10.192734\n",
      "ep 2713: ep_len:806 episode reward: total was 21.780000. running mean: -9.873006\n",
      "ep 2713: ep_len:3884 episode reward: total was -45.790000. running mean: -10.232176\n",
      "ep 2713: ep_len:802 episode reward: total was -14.500000. running mean: -10.274854\n",
      "ep 2713: ep_len:7294 episode reward: total was 23.320000. running mean: -9.938906\n",
      "ep 2713: ep_len:1002 episode reward: total was -1.440000. running mean: -9.853917\n",
      "ep 2713: ep_len:136 episode reward: total was 65.000000. running mean: -9.105378\n",
      "ep 2713: ep_len:45 episode reward: total was 21.000000. running mean: -8.804324\n",
      "ep 2713: ep_len:618 episode reward: total was -0.090000. running mean: -8.717181\n",
      "ep 2713: ep_len:2872 episode reward: total was 14.050000. running mean: -8.489509\n",
      "epsilon:0.009992 episode_count: 40856. steps_count: 43956474.000000\n",
      "ep 2714: ep_len:668 episode reward: total was -0.560000. running mean: -8.410214\n",
      "ep 2714: ep_len:675 episode reward: total was -21.780000. running mean: -8.543912\n",
      "ep 2714: ep_len:3035 episode reward: total was -73.510000. running mean: -9.193572\n",
      "ep 2714: ep_len:1067 episode reward: total was -27.880000. running mean: -9.380437\n",
      "ep 2714: ep_len:129 episode reward: total was 63.000000. running mean: -8.656632\n",
      "ep 2714: ep_len:617 episode reward: total was 39.680000. running mean: -8.173266\n",
      "ep 2714: ep_len:4058 episode reward: total was -51.720000. running mean: -8.608733\n",
      "ep 2714: ep_len:1567 episode reward: total was -63.040000. running mean: -9.153046\n",
      "ep 2714: ep_len:680 episode reward: total was 21.820000. running mean: -8.843316\n",
      "ep 2714: ep_len:605 episode reward: total was -14.070000. running mean: -8.895582\n",
      "ep 2714: ep_len:188 episode reward: total was 83.500000. running mean: -7.971627\n",
      "ep 2714: ep_len:37 episode reward: total was 17.000000. running mean: -7.721910\n",
      "ep 2714: ep_len:76 episode reward: total was 36.500000. running mean: -7.279691\n",
      "ep 2714: ep_len:1209 episode reward: total was -10.480000. running mean: -7.311694\n",
      "ep 2714: ep_len:2788 episode reward: total was -4.820000. running mean: -7.286777\n",
      "epsilon:0.009992 episode_count: 40871. steps_count: 43973873.000000\n",
      "ep 2715: ep_len:654 episode reward: total was -13.830000. running mean: -7.352210\n",
      "ep 2715: ep_len:718 episode reward: total was -33.100000. running mean: -7.609688\n",
      "ep 2715: ep_len:2950 episode reward: total was -51.650000. running mean: -8.050091\n",
      "ep 2715: ep_len:1671 episode reward: total was -44.360000. running mean: -8.413190\n",
      "ep 2715: ep_len:53 episode reward: total was 25.000000. running mean: -8.079058\n",
      "ep 2715: ep_len:87 episode reward: total was 40.500000. running mean: -7.593267\n",
      "ep 2715: ep_len:72 episode reward: total was 34.500000. running mean: -7.172335\n",
      "ep 2715: ep_len:30 episode reward: total was 13.500000. running mean: -6.965611\n",
      "ep 2715: ep_len:715 episode reward: total was -2.110000. running mean: -6.917055\n",
      "ep 2715: ep_len:646 episode reward: total was -29.850000. running mean: -7.146385\n",
      "ep 2715: ep_len:570 episode reward: total was 16.680000. running mean: -6.908121\n",
      "ep 2715: ep_len:633 episode reward: total was -10.370000. running mean: -6.942740\n",
      "ep 2715: ep_len:665 episode reward: total was 4.540000. running mean: -6.827912\n",
      "ep 2715: ep_len:43 episode reward: total was 20.000000. running mean: -6.559633\n",
      "ep 2715: ep_len:87 episode reward: total was 40.500000. running mean: -6.089037\n",
      "ep 2715: ep_len:500 episode reward: total was 21.650000. running mean: -5.811646\n",
      "ep 2715: ep_len:2756 episode reward: total was -33.760000. running mean: -6.091130\n",
      "epsilon:0.009992 episode_count: 40888. steps_count: 43986723.000000\n",
      "ep 2716: ep_len:1119 episode reward: total was -24.150000. running mean: -6.271719\n",
      "ep 2716: ep_len:671 episode reward: total was -44.200000. running mean: -6.651001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2716: ep_len:43 episode reward: total was 18.500000. running mean: -6.399491\n",
      "ep 2716: ep_len:3088 episode reward: total was -13.710000. running mean: -6.472596\n",
      "ep 2716: ep_len:880 episode reward: total was 36.250000. running mean: -6.045370\n",
      "ep 2716: ep_len:500 episode reward: total was 4.720000. running mean: -5.937717\n",
      "ep 2716: ep_len:3948 episode reward: total was -125.430000. running mean: -7.132640\n",
      "ep 2716: ep_len:530 episode reward: total was 1.490000. running mean: -7.046413\n",
      "ep 2716: ep_len:767 episode reward: total was 24.790000. running mean: -6.728049\n",
      "ep 2716: ep_len:947 episode reward: total was -5.110000. running mean: -6.711869\n",
      "ep 2716: ep_len:63 episode reward: total was 28.500000. running mean: -6.359750\n",
      "ep 2716: ep_len:44 episode reward: total was 20.500000. running mean: -6.091152\n",
      "ep 2716: ep_len:627 episode reward: total was -9.050000. running mean: -6.120741\n",
      "ep 2716: ep_len:2845 episode reward: total was -15.670000. running mean: -6.216233\n",
      "ep 2716: ep_len:47 episode reward: total was 21.510000. running mean: -5.938971\n",
      "epsilon:0.009992 episode_count: 40903. steps_count: 44002842.000000\n",
      "ep 2717: ep_len:713 episode reward: total was 38.860000. running mean: -5.490981\n",
      "ep 2717: ep_len:1620 episode reward: total was -79.280000. running mean: -6.228872\n",
      "ep 2717: ep_len:2962 episode reward: total was -95.570000. running mean: -7.122283\n",
      "ep 2717: ep_len:643 episode reward: total was -42.220000. running mean: -7.473260\n",
      "ep 2717: ep_len:68 episode reward: total was 32.500000. running mean: -7.073527\n",
      "ep 2717: ep_len:743 episode reward: total was -18.020000. running mean: -7.182992\n",
      "ep 2717: ep_len:3963 episode reward: total was -35.390000. running mean: -7.465062\n",
      "ep 2717: ep_len:1299 episode reward: total was -57.080000. running mean: -7.961212\n",
      "ep 2717: ep_len:712 episode reward: total was 15.170000. running mean: -7.729900\n",
      "ep 2717: ep_len:1121 episode reward: total was -17.700000. running mean: -7.829601\n",
      "ep 2717: ep_len:771 episode reward: total was -76.290000. running mean: -8.514205\n",
      "ep 2717: ep_len:2844 episode reward: total was -40.470000. running mean: -8.833762\n",
      "ep 2717: ep_len:52 episode reward: total was 24.500000. running mean: -8.500425\n",
      "epsilon:0.009992 episode_count: 40916. steps_count: 44020353.000000\n",
      "ep 2718: ep_len:673 episode reward: total was -22.450000. running mean: -8.639921\n",
      "ep 2718: ep_len:786 episode reward: total was -17.560000. running mean: -8.729121\n",
      "ep 2718: ep_len:3033 episode reward: total was -29.170000. running mean: -8.933530\n",
      "ep 2718: ep_len:1154 episode reward: total was -42.100000. running mean: -9.265195\n",
      "ep 2718: ep_len:97 episode reward: total was 44.000000. running mean: -8.732543\n",
      "ep 2718: ep_len:948 episode reward: total was 51.570000. running mean: -8.129518\n",
      "ep 2718: ep_len:361 episode reward: total was 5.140000. running mean: -7.996822\n",
      "ep 2718: ep_len:1493 episode reward: total was -76.430000. running mean: -8.681154\n",
      "ep 2718: ep_len:669 episode reward: total was -7.830000. running mean: -8.672643\n",
      "ep 2718: ep_len:1457 episode reward: total was 6.740000. running mean: -8.518516\n",
      "ep 2718: ep_len:63 episode reward: total was 28.500000. running mean: -8.148331\n",
      "ep 2718: ep_len:167 episode reward: total was 79.000000. running mean: -7.276848\n",
      "ep 2718: ep_len:70 episode reward: total was 33.500000. running mean: -6.869079\n",
      "ep 2718: ep_len:609 episode reward: total was -39.440000. running mean: -7.194788\n",
      "ep 2718: ep_len:2722 episode reward: total was -68.260000. running mean: -7.805441\n",
      "ep 2718: ep_len:72 episode reward: total was 33.000000. running mean: -7.397386\n",
      "epsilon:0.009992 episode_count: 40932. steps_count: 44034727.000000\n",
      "ep 2719: ep_len:500 episode reward: total was 5.330000. running mean: -7.270112\n",
      "ep 2719: ep_len:1785 episode reward: total was -455.540000. running mean: -11.752811\n",
      "ep 2719: ep_len:2994 episode reward: total was -5.290000. running mean: -11.688183\n",
      "ep 2719: ep_len:809 episode reward: total was 20.590000. running mean: -11.365401\n",
      "ep 2719: ep_len:42 episode reward: total was 19.500000. running mean: -11.056747\n",
      "ep 2719: ep_len:51 episode reward: total was 24.000000. running mean: -10.706180\n",
      "ep 2719: ep_len:500 episode reward: total was 24.930000. running mean: -10.349818\n",
      "ep 2719: ep_len:367 episode reward: total was 19.310000. running mean: -10.053220\n",
      "ep 2719: ep_len:1146 episode reward: total was -20.570000. running mean: -10.158388\n",
      "ep 2719: ep_len:773 episode reward: total was 5.000000. running mean: -10.006804\n",
      "ep 2719: ep_len:500 episode reward: total was 1.810000. running mean: -9.888636\n",
      "ep 2719: ep_len:63 episode reward: total was 30.000000. running mean: -9.489749\n",
      "ep 2719: ep_len:160 episode reward: total was 74.000000. running mean: -8.654852\n",
      "ep 2719: ep_len:500 episode reward: total was 26.820000. running mean: -8.300103\n",
      "ep 2719: ep_len:2824 episode reward: total was -12.390000. running mean: -8.341002\n",
      "epsilon:0.009992 episode_count: 40947. steps_count: 44047741.000000\n",
      "ep 2720: ep_len:1020 episode reward: total was -97.040000. running mean: -9.227992\n",
      "ep 2720: ep_len:1684 episode reward: total was -37.400000. running mean: -9.509712\n",
      "ep 2720: ep_len:2950 episode reward: total was -260.390000. running mean: -12.018515\n",
      "ep 2720: ep_len:628 episode reward: total was 6.110000. running mean: -11.837230\n",
      "ep 2720: ep_len:959 episode reward: total was -7.990000. running mean: -11.798758\n",
      "ep 2720: ep_len:337 episode reward: total was 11.940000. running mean: -11.561370\n",
      "ep 2720: ep_len:953 episode reward: total was 3.130000. running mean: -11.414456\n",
      "ep 2720: ep_len:845 episode reward: total was 42.040000. running mean: -10.879912\n",
      "ep 2720: ep_len:1159 episode reward: total was -4.740000. running mean: -10.818513\n",
      "ep 2720: ep_len:181 episode reward: total was 86.000000. running mean: -9.850328\n",
      "ep 2720: ep_len:1179 episode reward: total was -144.290000. running mean: -11.194724\n",
      "ep 2720: ep_len:2810 episode reward: total was -85.070000. running mean: -11.933477\n",
      "epsilon:0.009992 episode_count: 40959. steps_count: 44062446.000000\n",
      "ep 2721: ep_len:1466 episode reward: total was 23.980000. running mean: -11.574342\n",
      "ep 2721: ep_len:216 episode reward: total was 11.460000. running mean: -11.343999\n",
      "ep 2721: ep_len:2908 episode reward: total was -42.120000. running mean: -11.651759\n",
      "ep 2721: ep_len:1116 episode reward: total was -42.140000. running mean: -11.956641\n",
      "ep 2721: ep_len:132 episode reward: total was 61.500000. running mean: -11.222075\n",
      "ep 2721: ep_len:50 episode reward: total was 23.500000. running mean: -10.874854\n",
      "ep 2721: ep_len:1014 episode reward: total was -11.240000. running mean: -10.878506\n",
      "ep 2721: ep_len:3894 episode reward: total was -205.210000. running mean: -12.821821\n",
      "ep 2721: ep_len:718 episode reward: total was -50.580000. running mean: -13.199402\n",
      "ep 2721: ep_len:809 episode reward: total was 25.170000. running mean: -12.815708\n",
      "ep 2721: ep_len:1007 episode reward: total was 26.340000. running mean: -12.424151\n",
      "ep 2721: ep_len:42 episode reward: total was 18.000000. running mean: -12.119910\n",
      "ep 2721: ep_len:115 episode reward: total was 56.000000. running mean: -11.438711\n",
      "ep 2721: ep_len:617 episode reward: total was 3.140000. running mean: -11.292924\n",
      "ep 2721: ep_len:2853 episode reward: total was -19.600000. running mean: -11.375994\n",
      "ep 2721: ep_len:51 episode reward: total was 24.000000. running mean: -11.022234\n",
      "epsilon:0.009992 episode_count: 40975. steps_count: 44079454.000000\n",
      "ep 2722: ep_len:847 episode reward: total was -30.570000. running mean: -11.217712\n",
      "ep 2722: ep_len:679 episode reward: total was -47.370000. running mean: -11.579235\n",
      "ep 2722: ep_len:2919 episode reward: total was -37.600000. running mean: -11.839443\n",
      "ep 2722: ep_len:740 episode reward: total was -8.930000. running mean: -11.810348\n",
      "ep 2722: ep_len:171 episode reward: total was 81.000000. running mean: -10.882245\n",
      "ep 2722: ep_len:1104 episode reward: total was -103.260000. running mean: -11.806022\n",
      "ep 2722: ep_len:3636 episode reward: total was -42.180000. running mean: -12.109762\n",
      "ep 2722: ep_len:795 episode reward: total was -40.070000. running mean: -12.389364\n",
      "ep 2722: ep_len:617 episode reward: total was -11.050000. running mean: -12.375971\n",
      "ep 2722: ep_len:898 episode reward: total was 8.480000. running mean: -12.167411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2722: ep_len:86 episode reward: total was 41.500000. running mean: -11.630737\n",
      "ep 2722: ep_len:140 episode reward: total was 62.500000. running mean: -10.889430\n",
      "ep 2722: ep_len:65 episode reward: total was 29.500000. running mean: -10.485535\n",
      "ep 2722: ep_len:68 episode reward: total was 32.500000. running mean: -10.055680\n",
      "ep 2722: ep_len:602 episode reward: total was -4.250000. running mean: -9.997623\n",
      "ep 2722: ep_len:2808 episode reward: total was -31.510000. running mean: -10.212747\n",
      "epsilon:0.009992 episode_count: 40991. steps_count: 44095629.000000\n",
      "ep 2723: ep_len:994 episode reward: total was -75.520000. running mean: -10.865819\n",
      "ep 2723: ep_len:753 episode reward: total was 1.430000. running mean: -10.742861\n",
      "ep 2723: ep_len:2997 episode reward: total was -83.290000. running mean: -11.468333\n",
      "ep 2723: ep_len:636 episode reward: total was 32.410000. running mean: -11.029549\n",
      "ep 2723: ep_len:109 episode reward: total was 53.000000. running mean: -10.389254\n",
      "ep 2723: ep_len:77 episode reward: total was 37.000000. running mean: -9.915361\n",
      "ep 2723: ep_len:500 episode reward: total was 70.520000. running mean: -9.111008\n",
      "ep 2723: ep_len:661 episode reward: total was 2.650000. running mean: -8.993398\n",
      "ep 2723: ep_len:575 episode reward: total was 0.840000. running mean: -8.895064\n",
      "ep 2723: ep_len:720 episode reward: total was 44.890000. running mean: -8.357213\n",
      "ep 2723: ep_len:973 episode reward: total was 22.240000. running mean: -8.051241\n",
      "ep 2723: ep_len:38 episode reward: total was 17.500000. running mean: -7.795728\n",
      "ep 2723: ep_len:1114 episode reward: total was -10.240000. running mean: -7.820171\n",
      "ep 2723: ep_len:2821 episode reward: total was -2.690000. running mean: -7.768869\n",
      "epsilon:0.009992 episode_count: 41005. steps_count: 44108597.000000\n",
      "ep 2724: ep_len:1127 episode reward: total was -14.150000. running mean: -7.832681\n",
      "ep 2724: ep_len:1586 episode reward: total was -84.330000. running mean: -8.597654\n",
      "ep 2724: ep_len:2939 episode reward: total was -49.280000. running mean: -9.004477\n",
      "ep 2724: ep_len:641 episode reward: total was -2.470000. running mean: -8.939133\n",
      "ep 2724: ep_len:664 episode reward: total was -2.630000. running mean: -8.876041\n",
      "ep 2724: ep_len:332 episode reward: total was 21.530000. running mean: -8.571981\n",
      "ep 2724: ep_len:996 episode reward: total was -44.250000. running mean: -8.928761\n",
      "ep 2724: ep_len:798 episode reward: total was 21.350000. running mean: -8.625973\n",
      "ep 2724: ep_len:706 episode reward: total was 18.230000. running mean: -8.357414\n",
      "ep 2724: ep_len:222 episode reward: total was 108.000000. running mean: -7.193840\n",
      "ep 2724: ep_len:57 episode reward: total was 27.000000. running mean: -6.851901\n",
      "ep 2724: ep_len:513 episode reward: total was 16.420000. running mean: -6.619182\n",
      "ep 2724: ep_len:2729 episode reward: total was -0.610000. running mean: -6.559090\n",
      "epsilon:0.009992 episode_count: 41018. steps_count: 44121907.000000\n",
      "ep 2725: ep_len:996 episode reward: total was -185.570000. running mean: -8.349199\n",
      "ep 2725: ep_len:858 episode reward: total was 9.690000. running mean: -8.168807\n",
      "ep 2725: ep_len:3063 episode reward: total was -5.180000. running mean: -8.138919\n",
      "ep 2725: ep_len:778 episode reward: total was -6.530000. running mean: -8.122830\n",
      "ep 2725: ep_len:88 episode reward: total was 42.500000. running mean: -7.616602\n",
      "ep 2725: ep_len:659 episode reward: total was 0.320000. running mean: -7.537236\n",
      "ep 2725: ep_len:3689 episode reward: total was -47.160000. running mean: -7.933463\n",
      "ep 2725: ep_len:725 episode reward: total was -11.060000. running mean: -7.964729\n",
      "ep 2725: ep_len:639 episode reward: total was 5.060000. running mean: -7.834482\n",
      "ep 2725: ep_len:571 episode reward: total was -7.590000. running mean: -7.832037\n",
      "ep 2725: ep_len:67 episode reward: total was 32.000000. running mean: -7.433716\n",
      "ep 2725: ep_len:86 episode reward: total was 40.000000. running mean: -6.959379\n",
      "ep 2725: ep_len:1479 episode reward: total was 0.330000. running mean: -6.886485\n",
      "ep 2725: ep_len:2776 episode reward: total was -33.380000. running mean: -7.151421\n",
      "ep 2725: ep_len:58 episode reward: total was 27.500000. running mean: -6.804906\n",
      "epsilon:0.009992 episode_count: 41033. steps_count: 44138439.000000\n",
      "ep 2726: ep_len:1067 episode reward: total was 7.750000. running mean: -6.659357\n",
      "ep 2726: ep_len:648 episode reward: total was -12.300000. running mean: -6.715764\n",
      "ep 2726: ep_len:3033 episode reward: total was -7.990000. running mean: -6.728506\n",
      "ep 2726: ep_len:1198 episode reward: total was -10.650000. running mean: -6.767721\n",
      "ep 2726: ep_len:113 episode reward: total was 55.000000. running mean: -6.150044\n",
      "ep 2726: ep_len:47 episode reward: total was 22.000000. running mean: -5.868543\n",
      "ep 2726: ep_len:500 episode reward: total was 21.070000. running mean: -5.599158\n",
      "ep 2726: ep_len:667 episode reward: total was 12.630000. running mean: -5.416866\n",
      "ep 2726: ep_len:962 episode reward: total was 20.180000. running mean: -5.160898\n",
      "ep 2726: ep_len:763 episode reward: total was 14.880000. running mean: -4.960489\n",
      "ep 2726: ep_len:1097 episode reward: total was -17.480000. running mean: -5.085684\n",
      "ep 2726: ep_len:60 episode reward: total was 27.000000. running mean: -4.764827\n",
      "ep 2726: ep_len:194 episode reward: total was 94.000000. running mean: -3.777179\n",
      "ep 2726: ep_len:64 episode reward: total was 29.000000. running mean: -3.449407\n",
      "ep 2726: ep_len:1520 episode reward: total was -8.050000. running mean: -3.495413\n",
      "ep 2726: ep_len:2984 episode reward: total was -1467.500000. running mean: -18.135459\n",
      "epsilon:0.009992 episode_count: 41049. steps_count: 44153356.000000\n",
      "ep 2727: ep_len:1073 episode reward: total was -5.110000. running mean: -18.005204\n",
      "ep 2727: ep_len:500 episode reward: total was -8.630000. running mean: -17.911452\n",
      "ep 2727: ep_len:58 episode reward: total was 27.500000. running mean: -17.457338\n",
      "ep 2727: ep_len:2990 episode reward: total was -16.230000. running mean: -17.445064\n",
      "ep 2727: ep_len:500 episode reward: total was 20.450000. running mean: -17.066114\n",
      "ep 2727: ep_len:71 episode reward: total was 32.500000. running mean: -16.570452\n",
      "ep 2727: ep_len:1003 episode reward: total was -35.590000. running mean: -16.760648\n",
      "ep 2727: ep_len:3737 episode reward: total was -63.420000. running mean: -17.227241\n",
      "ep 2727: ep_len:854 episode reward: total was -23.190000. running mean: -17.286869\n",
      "ep 2727: ep_len:684 episode reward: total was 22.290000. running mean: -16.891100\n",
      "ep 2727: ep_len:985 episode reward: total was -15.380000. running mean: -16.875989\n",
      "ep 2727: ep_len:797 episode reward: total was -6.330000. running mean: -16.770529\n",
      "ep 2727: ep_len:2795 episode reward: total was -14.490000. running mean: -16.747724\n",
      "ep 2727: ep_len:50 episode reward: total was 23.500000. running mean: -16.345247\n",
      "epsilon:0.009992 episode_count: 41063. steps_count: 44169453.000000\n",
      "ep 2728: ep_len:625 episode reward: total was -29.270000. running mean: -16.474494\n",
      "ep 2728: ep_len:702 episode reward: total was -11.530000. running mean: -16.425049\n",
      "ep 2728: ep_len:2912 episode reward: total was -11.690000. running mean: -16.377699\n",
      "ep 2728: ep_len:510 episode reward: total was -31.430000. running mean: -16.528222\n",
      "ep 2728: ep_len:145 episode reward: total was 65.000000. running mean: -15.712940\n",
      "ep 2728: ep_len:80 episode reward: total was 38.500000. running mean: -15.170810\n",
      "ep 2728: ep_len:774 episode reward: total was -1.520000. running mean: -15.034302\n",
      "ep 2728: ep_len:657 episode reward: total was 14.150000. running mean: -14.742459\n",
      "ep 2728: ep_len:1520 episode reward: total was -57.150000. running mean: -15.166535\n",
      "ep 2728: ep_len:816 episode reward: total was 18.320000. running mean: -14.831669\n",
      "ep 2728: ep_len:1111 episode reward: total was 2.410000. running mean: -14.659253\n",
      "ep 2728: ep_len:57 episode reward: total was 27.000000. running mean: -14.242660\n",
      "ep 2728: ep_len:37 episode reward: total was 15.500000. running mean: -13.945233\n",
      "ep 2728: ep_len:741 episode reward: total was -35.150000. running mean: -14.157281\n",
      "ep 2728: ep_len:2891 episode reward: total was -78.010000. running mean: -14.795808\n",
      "epsilon:0.009992 episode_count: 41078. steps_count: 44183031.000000\n",
      "ep 2729: ep_len:1122 episode reward: total was 0.950000. running mean: -14.638350\n",
      "ep 2729: ep_len:968 episode reward: total was 17.840000. running mean: -14.313567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2729: ep_len:74 episode reward: total was 32.500000. running mean: -13.845431\n",
      "ep 2729: ep_len:3017 episode reward: total was -52.930000. running mean: -14.236277\n",
      "ep 2729: ep_len:1085 episode reward: total was -12.550000. running mean: -14.219414\n",
      "ep 2729: ep_len:93 episode reward: total was 43.500000. running mean: -13.642220\n",
      "ep 2729: ep_len:53 episode reward: total was 23.500000. running mean: -13.270798\n",
      "ep 2729: ep_len:832 episode reward: total was 38.150000. running mean: -12.756590\n",
      "ep 2729: ep_len:327 episode reward: total was 21.940000. running mean: -12.409624\n",
      "ep 2729: ep_len:3720 episode reward: total was -443.620000. running mean: -16.721728\n",
      "ep 2729: ep_len:859 episode reward: total was 72.980000. running mean: -15.824710\n",
      "ep 2729: ep_len:500 episode reward: total was 5.940000. running mean: -15.607063\n",
      "ep 2729: ep_len:65 episode reward: total was 29.500000. running mean: -15.155993\n",
      "ep 2729: ep_len:41 episode reward: total was 17.500000. running mean: -14.829433\n",
      "ep 2729: ep_len:89 episode reward: total was 43.000000. running mean: -14.251138\n",
      "ep 2729: ep_len:589 episode reward: total was -10.850000. running mean: -14.217127\n",
      "ep 2729: ep_len:2894 episode reward: total was 2.910000. running mean: -14.045856\n",
      "epsilon:0.009992 episode_count: 41095. steps_count: 44199359.000000\n",
      "ep 2730: ep_len:611 episode reward: total was -5.140000. running mean: -13.956797\n",
      "ep 2730: ep_len:674 episode reward: total was -9.590000. running mean: -13.913129\n",
      "ep 2730: ep_len:43 episode reward: total was 20.000000. running mean: -13.573998\n",
      "ep 2730: ep_len:3058 episode reward: total was -32.260000. running mean: -13.760858\n",
      "ep 2730: ep_len:804 episode reward: total was 18.700000. running mean: -13.436249\n",
      "ep 2730: ep_len:57 episode reward: total was 27.000000. running mean: -13.031887\n",
      "ep 2730: ep_len:63 episode reward: total was 27.000000. running mean: -12.631568\n",
      "ep 2730: ep_len:500 episode reward: total was 24.130000. running mean: -12.263952\n",
      "ep 2730: ep_len:3598 episode reward: total was -16.330000. running mean: -12.304613\n",
      "ep 2730: ep_len:4215 episode reward: total was -1208.340000. running mean: -24.264967\n",
      "ep 2730: ep_len:863 episode reward: total was 19.750000. running mean: -23.824817\n",
      "ep 2730: ep_len:901 episode reward: total was 6.520000. running mean: -23.521369\n",
      "ep 2730: ep_len:54 episode reward: total was 25.500000. running mean: -23.031155\n",
      "ep 2730: ep_len:500 episode reward: total was 26.670000. running mean: -22.534144\n",
      "ep 2730: ep_len:2862 episode reward: total was -12.990000. running mean: -22.438702\n",
      "epsilon:0.009992 episode_count: 41110. steps_count: 44218162.000000\n",
      "ep 2731: ep_len:1401 episode reward: total was 23.110000. running mean: -21.983215\n",
      "ep 2731: ep_len:1267 episode reward: total was -50.120000. running mean: -22.264583\n",
      "ep 2731: ep_len:3044 episode reward: total was -26.950000. running mean: -22.311437\n",
      "ep 2731: ep_len:647 episode reward: total was 0.240000. running mean: -22.085923\n",
      "ep 2731: ep_len:107 episode reward: total was 50.500000. running mean: -21.360063\n",
      "ep 2731: ep_len:56 episode reward: total was 25.000000. running mean: -20.896463\n",
      "ep 2731: ep_len:852 episode reward: total was 30.760000. running mean: -20.379898\n",
      "ep 2731: ep_len:3759 episode reward: total was -22.770000. running mean: -20.403799\n",
      "ep 2731: ep_len:666 episode reward: total was -32.040000. running mean: -20.520161\n",
      "ep 2731: ep_len:634 episode reward: total was 20.200000. running mean: -20.112960\n",
      "ep 2731: ep_len:500 episode reward: total was -11.330000. running mean: -20.025130\n",
      "ep 2731: ep_len:34 episode reward: total was 14.000000. running mean: -19.684879\n",
      "ep 2731: ep_len:752 episode reward: total was -113.330000. running mean: -20.621330\n",
      "ep 2731: ep_len:2755 episode reward: total was 6.260000. running mean: -20.352517\n",
      "epsilon:0.009992 episode_count: 41124. steps_count: 44234636.000000\n",
      "ep 2732: ep_len:635 episode reward: total was -10.990000. running mean: -20.258891\n",
      "ep 2732: ep_len:190 episode reward: total was 12.210000. running mean: -19.934203\n",
      "ep 2732: ep_len:2884 episode reward: total was -70.680000. running mean: -20.441661\n",
      "ep 2732: ep_len:642 episode reward: total was -4.440000. running mean: -20.281644\n",
      "ep 2732: ep_len:42 episode reward: total was 19.500000. running mean: -19.883827\n",
      "ep 2732: ep_len:500 episode reward: total was 35.890000. running mean: -19.326089\n",
      "ep 2732: ep_len:653 episode reward: total was 26.260000. running mean: -18.870228\n",
      "ep 2732: ep_len:1218 episode reward: total was -37.480000. running mean: -19.056326\n",
      "ep 2732: ep_len:641 episode reward: total was 6.730000. running mean: -18.798463\n",
      "ep 2732: ep_len:500 episode reward: total was 51.780000. running mean: -18.092678\n",
      "ep 2732: ep_len:67 episode reward: total was 29.000000. running mean: -17.621751\n",
      "ep 2732: ep_len:72 episode reward: total was 34.500000. running mean: -17.100534\n",
      "ep 2732: ep_len:648 episode reward: total was -8.540000. running mean: -17.014929\n",
      "ep 2732: ep_len:2747 episode reward: total was -39.760000. running mean: -17.242379\n",
      "ep 2732: ep_len:62 episode reward: total was 29.500000. running mean: -16.774955\n",
      "epsilon:0.009992 episode_count: 41139. steps_count: 44246137.000000\n",
      "ep 2733: ep_len:1319 episode reward: total was 23.610000. running mean: -16.371106\n",
      "ep 2733: ep_len:809 episode reward: total was 26.270000. running mean: -15.944695\n",
      "ep 2733: ep_len:60 episode reward: total was 28.500000. running mean: -15.500248\n",
      "ep 2733: ep_len:2939 episode reward: total was 2.760000. running mean: -15.317645\n",
      "ep 2733: ep_len:788 episode reward: total was 28.030000. running mean: -14.884169\n",
      "ep 2733: ep_len:1140 episode reward: total was -9.550000. running mean: -14.830827\n",
      "ep 2733: ep_len:330 episode reward: total was 10.980000. running mean: -14.572719\n",
      "ep 2733: ep_len:664 episode reward: total was 3.440000. running mean: -14.392592\n",
      "ep 2733: ep_len:7337 episode reward: total was -354.030000. running mean: -17.788966\n",
      "ep 2733: ep_len:593 episode reward: total was 48.030000. running mean: -17.130776\n",
      "ep 2733: ep_len:61 episode reward: total was 27.500000. running mean: -16.684468\n",
      "ep 2733: ep_len:783 episode reward: total was -24.660000. running mean: -16.764224\n",
      "ep 2733: ep_len:2871 episode reward: total was -0.410000. running mean: -16.600682\n",
      "epsilon:0.009992 episode_count: 41152. steps_count: 44265831.000000\n",
      "ep 2734: ep_len:1031 episode reward: total was -89.330000. running mean: -17.327975\n",
      "ep 2734: ep_len:751 episode reward: total was -12.950000. running mean: -17.284195\n",
      "ep 2734: ep_len:3019 episode reward: total was -41.560000. running mean: -17.526953\n",
      "ep 2734: ep_len:500 episode reward: total was 16.230000. running mean: -17.189383\n",
      "ep 2734: ep_len:40 episode reward: total was 18.500000. running mean: -16.832490\n",
      "ep 2734: ep_len:113 episode reward: total was 55.000000. running mean: -16.114165\n",
      "ep 2734: ep_len:47 episode reward: total was 22.000000. running mean: -15.733023\n",
      "ep 2734: ep_len:500 episode reward: total was 40.420000. running mean: -15.171493\n",
      "ep 2734: ep_len:614 episode reward: total was 17.700000. running mean: -14.842778\n",
      "ep 2734: ep_len:554 episode reward: total was -6.260000. running mean: -14.756950\n",
      "ep 2734: ep_len:674 episode reward: total was 7.500000. running mean: -14.534381\n",
      "ep 2734: ep_len:683 episode reward: total was 9.540000. running mean: -14.293637\n",
      "ep 2734: ep_len:31 episode reward: total was 14.000000. running mean: -14.010700\n",
      "ep 2734: ep_len:500 episode reward: total was 4.990000. running mean: -13.820693\n",
      "ep 2734: ep_len:2783 episode reward: total was -3.490000. running mean: -13.717387\n",
      "epsilon:0.009992 episode_count: 41167. steps_count: 44277671.000000\n",
      "ep 2735: ep_len:692 episode reward: total was -1.330000. running mean: -13.593513\n",
      "ep 2735: ep_len:878 episode reward: total was 10.480000. running mean: -13.352778\n",
      "ep 2735: ep_len:3090 episode reward: total was -0.170000. running mean: -13.220950\n",
      "ep 2735: ep_len:1208 episode reward: total was -40.610000. running mean: -13.494840\n",
      "ep 2735: ep_len:1445 episode reward: total was 21.050000. running mean: -13.149392\n",
      "ep 2735: ep_len:353 episode reward: total was 15.650000. running mean: -12.861398\n",
      "ep 2735: ep_len:831 episode reward: total was -50.930000. running mean: -13.242084\n",
      "ep 2735: ep_len:778 episode reward: total was -3.550000. running mean: -13.145163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2735: ep_len:587 episode reward: total was 2.110000. running mean: -12.992612\n",
      "ep 2735: ep_len:74 episode reward: total was 35.500000. running mean: -12.507685\n",
      "ep 2735: ep_len:61 episode reward: total was 29.000000. running mean: -12.092609\n",
      "ep 2735: ep_len:1039 episode reward: total was 18.090000. running mean: -11.790782\n",
      "ep 2735: ep_len:2849 episode reward: total was -50.340000. running mean: -12.176275\n",
      "epsilon:0.009992 episode_count: 41180. steps_count: 44291556.000000\n",
      "ep 2736: ep_len:577 episode reward: total was -6.280000. running mean: -12.117312\n",
      "ep 2736: ep_len:740 episode reward: total was -4.890000. running mean: -12.045039\n",
      "ep 2736: ep_len:3063 episode reward: total was -124.130000. running mean: -13.165888\n",
      "ep 2736: ep_len:500 episode reward: total was 19.130000. running mean: -12.842930\n",
      "ep 2736: ep_len:121 episode reward: total was 59.000000. running mean: -12.124500\n",
      "ep 2736: ep_len:1119 episode reward: total was -10.190000. running mean: -12.105155\n",
      "ep 2736: ep_len:331 episode reward: total was 6.920000. running mean: -11.914904\n",
      "ep 2736: ep_len:861 episode reward: total was -28.510000. running mean: -12.080855\n",
      "ep 2736: ep_len:707 episode reward: total was 25.410000. running mean: -11.705946\n",
      "ep 2736: ep_len:625 episode reward: total was 28.980000. running mean: -11.299087\n",
      "ep 2736: ep_len:79 episode reward: total was 38.000000. running mean: -10.806096\n",
      "ep 2736: ep_len:67 episode reward: total was 32.000000. running mean: -10.378035\n",
      "ep 2736: ep_len:1078 episode reward: total was -3.530000. running mean: -10.309554\n",
      "ep 2736: ep_len:44 episode reward: total was 20.500000. running mean: -10.001459\n",
      "ep 2736: ep_len:41 episode reward: total was 19.000000. running mean: -9.711444\n",
      "epsilon:0.009992 episode_count: 41195. steps_count: 44301509.000000\n",
      "ep 2737: ep_len:589 episode reward: total was 10.030000. running mean: -9.514030\n",
      "ep 2737: ep_len:500 episode reward: total was -18.420000. running mean: -9.603090\n",
      "ep 2737: ep_len:2988 episode reward: total was 3.070000. running mean: -9.476359\n",
      "ep 2737: ep_len:1241 episode reward: total was -26.140000. running mean: -9.642995\n",
      "ep 2737: ep_len:169 episode reward: total was 81.500000. running mean: -8.731565\n",
      "ep 2737: ep_len:32 episode reward: total was 14.500000. running mean: -8.499249\n",
      "ep 2737: ep_len:1027 episode reward: total was -66.140000. running mean: -9.075657\n",
      "ep 2737: ep_len:644 episode reward: total was 23.140000. running mean: -8.753500\n",
      "ep 2737: ep_len:537 episode reward: total was -9.460000. running mean: -8.760565\n",
      "ep 2737: ep_len:701 episode reward: total was 20.080000. running mean: -8.472160\n",
      "ep 2737: ep_len:644 episode reward: total was 9.400000. running mean: -8.293438\n",
      "ep 2737: ep_len:99 episode reward: total was 46.010000. running mean: -7.750404\n",
      "ep 2737: ep_len:66 episode reward: total was 31.500000. running mean: -7.357900\n",
      "ep 2737: ep_len:1170 episode reward: total was -15.620000. running mean: -7.440521\n",
      "ep 2737: ep_len:2816 episode reward: total was -8.980000. running mean: -7.455916\n",
      "epsilon:0.009992 episode_count: 41210. steps_count: 44314732.000000\n",
      "ep 2738: ep_len:987 episode reward: total was -80.680000. running mean: -8.188156\n",
      "ep 2738: ep_len:761 episode reward: total was -0.300000. running mean: -8.109275\n",
      "ep 2738: ep_len:2945 episode reward: total was -24.800000. running mean: -8.276182\n",
      "ep 2738: ep_len:510 episode reward: total was -52.640000. running mean: -8.719820\n",
      "ep 2738: ep_len:35 episode reward: total was 16.000000. running mean: -8.472622\n",
      "ep 2738: ep_len:1406 episode reward: total was -199.220000. running mean: -10.380096\n",
      "ep 2738: ep_len:3991 episode reward: total was -729.480000. running mean: -17.571095\n",
      "ep 2738: ep_len:651 episode reward: total was 5.330000. running mean: -17.342084\n",
      "ep 2738: ep_len:624 episode reward: total was 2.000000. running mean: -17.148663\n",
      "ep 2738: ep_len:590 episode reward: total was 0.620000. running mean: -16.970976\n",
      "ep 2738: ep_len:58 episode reward: total was 26.000000. running mean: -16.541267\n",
      "ep 2738: ep_len:94 episode reward: total was 44.000000. running mean: -15.935854\n",
      "ep 2738: ep_len:905 episode reward: total was -9.790000. running mean: -15.874395\n",
      "ep 2738: ep_len:2807 episode reward: total was 3.080000. running mean: -15.684852\n",
      "ep 2738: ep_len:63 episode reward: total was 30.000000. running mean: -15.228003\n",
      "epsilon:0.009992 episode_count: 41225. steps_count: 44331159.000000\n",
      "ep 2739: ep_len:664 episode reward: total was -20.370000. running mean: -15.279423\n",
      "ep 2739: ep_len:711 episode reward: total was -23.010000. running mean: -15.356729\n",
      "ep 2739: ep_len:65 episode reward: total was 28.000000. running mean: -14.923161\n",
      "ep 2739: ep_len:3050 episode reward: total was -48.840000. running mean: -15.262330\n",
      "ep 2739: ep_len:604 episode reward: total was 0.820000. running mean: -15.101507\n",
      "ep 2739: ep_len:16 episode reward: total was 6.500000. running mean: -14.885491\n",
      "ep 2739: ep_len:90 episode reward: total was 42.000000. running mean: -14.316637\n",
      "ep 2739: ep_len:41 episode reward: total was 19.000000. running mean: -13.983470\n",
      "ep 2739: ep_len:1473 episode reward: total was -143.590000. running mean: -15.279535\n",
      "ep 2739: ep_len:3649 episode reward: total was -107.210000. running mean: -16.198840\n",
      "ep 2739: ep_len:917 episode reward: total was -28.370000. running mean: -16.320552\n",
      "ep 2739: ep_len:759 episode reward: total was 13.400000. running mean: -16.023346\n",
      "ep 2739: ep_len:682 episode reward: total was 19.760000. running mean: -15.665513\n",
      "ep 2739: ep_len:121 episode reward: total was 57.500000. running mean: -14.933858\n",
      "ep 2739: ep_len:63 episode reward: total was 28.500000. running mean: -14.499519\n",
      "ep 2739: ep_len:500 episode reward: total was 10.840000. running mean: -14.246124\n",
      "ep 2739: ep_len:46 episode reward: total was 20.000000. running mean: -13.903663\n",
      "epsilon:0.009992 episode_count: 41242. steps_count: 44344610.000000\n",
      "ep 2740: ep_len:1121 episode reward: total was 0.940000. running mean: -13.755226\n",
      "ep 2740: ep_len:667 episode reward: total was -21.200000. running mean: -13.829674\n",
      "ep 2740: ep_len:43 episode reward: total was 20.000000. running mean: -13.491377\n",
      "ep 2740: ep_len:2951 episode reward: total was -4.410000. running mean: -13.400563\n",
      "ep 2740: ep_len:516 episode reward: total was 0.310000. running mean: -13.263458\n",
      "ep 2740: ep_len:148 episode reward: total was 72.500000. running mean: -12.405823\n",
      "ep 2740: ep_len:50 episode reward: total was 23.500000. running mean: -12.046765\n",
      "ep 2740: ep_len:667 episode reward: total was 16.640000. running mean: -11.759897\n",
      "ep 2740: ep_len:3577 episode reward: total was -59.970000. running mean: -12.241998\n",
      "ep 2740: ep_len:941 episode reward: total was -24.090000. running mean: -12.360478\n",
      "ep 2740: ep_len:663 episode reward: total was 25.390000. running mean: -11.982973\n",
      "ep 2740: ep_len:806 episode reward: total was 27.100000. running mean: -11.592144\n",
      "ep 2740: ep_len:120 episode reward: total was 54.000000. running mean: -10.936222\n",
      "ep 2740: ep_len:978 episode reward: total was -71.470000. running mean: -11.541560\n",
      "ep 2740: ep_len:2706 episode reward: total was -4.720000. running mean: -11.473344\n",
      "epsilon:0.009992 episode_count: 41257. steps_count: 44360564.000000\n",
      "ep 2741: ep_len:602 episode reward: total was -0.760000. running mean: -11.366211\n",
      "ep 2741: ep_len:742 episode reward: total was -22.040000. running mean: -11.472949\n",
      "ep 2741: ep_len:2999 episode reward: total was -59.670000. running mean: -11.954919\n",
      "ep 2741: ep_len:653 episode reward: total was -27.920000. running mean: -12.114570\n",
      "ep 2741: ep_len:33 episode reward: total was 15.000000. running mean: -11.843424\n",
      "ep 2741: ep_len:62 episode reward: total was 28.000000. running mean: -11.444990\n",
      "ep 2741: ep_len:26 episode reward: total was 11.500000. running mean: -11.215540\n",
      "ep 2741: ep_len:1335 episode reward: total was -177.710000. running mean: -12.880485\n",
      "ep 2741: ep_len:3559 episode reward: total was -76.310000. running mean: -13.514780\n",
      "ep 2741: ep_len:1493 episode reward: total was -17.040000. running mean: -13.550032\n",
      "ep 2741: ep_len:800 episode reward: total was 28.380000. running mean: -13.130732\n",
      "ep 2741: ep_len:921 episode reward: total was 44.570000. running mean: -12.553725\n",
      "ep 2741: ep_len:41 episode reward: total was 19.000000. running mean: -12.238187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2741: ep_len:1416 episode reward: total was 8.570000. running mean: -12.030106\n",
      "ep 2741: ep_len:2793 episode reward: total was -1.560000. running mean: -11.925404\n",
      "epsilon:0.009992 episode_count: 41272. steps_count: 44378039.000000\n",
      "ep 2742: ep_len:1078 episode reward: total was -1.510000. running mean: -11.821250\n",
      "ep 2742: ep_len:500 episode reward: total was 12.000000. running mean: -11.583038\n",
      "ep 2742: ep_len:44 episode reward: total was 20.500000. running mean: -11.262208\n",
      "ep 2742: ep_len:2999 episode reward: total was 5.230000. running mean: -11.097285\n",
      "ep 2742: ep_len:865 episode reward: total was 53.580000. running mean: -10.450513\n",
      "ep 2742: ep_len:66 episode reward: total was 31.500000. running mean: -10.031007\n",
      "ep 2742: ep_len:109 episode reward: total was 50.000000. running mean: -9.430697\n",
      "ep 2742: ep_len:886 episode reward: total was 34.590000. running mean: -8.990490\n",
      "ep 2742: ep_len:3650 episode reward: total was -47.150000. running mean: -9.372086\n",
      "ep 2742: ep_len:1206 episode reward: total was -25.230000. running mean: -9.530665\n",
      "ep 2742: ep_len:750 episode reward: total was -5.800000. running mean: -9.493358\n",
      "ep 2742: ep_len:730 episode reward: total was -187.330000. running mean: -11.271724\n",
      "ep 2742: ep_len:47 episode reward: total was 20.500000. running mean: -10.954007\n",
      "ep 2742: ep_len:77 episode reward: total was 37.000000. running mean: -10.474467\n",
      "ep 2742: ep_len:577 episode reward: total was -29.120000. running mean: -10.660922\n",
      "ep 2742: ep_len:2900 episode reward: total was -20.440000. running mean: -10.758713\n",
      "ep 2742: ep_len:63 episode reward: total was 30.000000. running mean: -10.351126\n",
      "epsilon:0.009992 episode_count: 41289. steps_count: 44394586.000000\n",
      "ep 2743: ep_len:785 episode reward: total was -33.730000. running mean: -10.584915\n",
      "ep 2743: ep_len:500 episode reward: total was 11.300000. running mean: -10.366066\n",
      "ep 2743: ep_len:2987 episode reward: total was -249.200000. running mean: -12.754405\n",
      "ep 2743: ep_len:802 episode reward: total was 44.950000. running mean: -12.177361\n",
      "ep 2743: ep_len:38 episode reward: total was 16.000000. running mean: -11.895587\n",
      "ep 2743: ep_len:74 episode reward: total was 34.000000. running mean: -11.436631\n",
      "ep 2743: ep_len:500 episode reward: total was 29.800000. running mean: -11.024265\n",
      "ep 2743: ep_len:649 episode reward: total was 23.530000. running mean: -10.678723\n",
      "ep 2743: ep_len:635 episode reward: total was -57.720000. running mean: -11.149135\n",
      "ep 2743: ep_len:775 episode reward: total was 33.800000. running mean: -10.699644\n",
      "ep 2743: ep_len:783 episode reward: total was 10.750000. running mean: -10.485148\n",
      "ep 2743: ep_len:1387 episode reward: total was 16.300000. running mean: -10.217296\n",
      "ep 2743: ep_len:2922 episode reward: total was 6.010000. running mean: -10.055023\n",
      "epsilon:0.009992 episode_count: 41302. steps_count: 44407423.000000\n",
      "ep 2744: ep_len:646 episode reward: total was 35.130000. running mean: -9.603173\n",
      "ep 2744: ep_len:667 episode reward: total was -21.780000. running mean: -9.724941\n",
      "ep 2744: ep_len:2984 episode reward: total was 7.250000. running mean: -9.555192\n",
      "ep 2744: ep_len:657 episode reward: total was 8.260000. running mean: -9.377040\n",
      "ep 2744: ep_len:128 episode reward: total was 62.500000. running mean: -8.658269\n",
      "ep 2744: ep_len:60 episode reward: total was 28.500000. running mean: -8.286687\n",
      "ep 2744: ep_len:500 episode reward: total was 30.470000. running mean: -7.899120\n",
      "ep 2744: ep_len:318 episode reward: total was 15.330000. running mean: -7.666829\n",
      "ep 2744: ep_len:1284 episode reward: total was -96.410000. running mean: -8.554260\n",
      "ep 2744: ep_len:775 episode reward: total was 12.910000. running mean: -8.339618\n",
      "ep 2744: ep_len:789 episode reward: total was 8.160000. running mean: -8.174622\n",
      "ep 2744: ep_len:67 episode reward: total was 30.500000. running mean: -7.787875\n",
      "ep 2744: ep_len:188 episode reward: total was 89.500000. running mean: -6.814997\n",
      "ep 2744: ep_len:500 episode reward: total was 30.560000. running mean: -6.441247\n",
      "ep 2744: ep_len:2806 episode reward: total was -12.200000. running mean: -6.498834\n",
      "epsilon:0.009992 episode_count: 41317. steps_count: 44419792.000000\n",
      "ep 2745: ep_len:1034 episode reward: total was -11.040000. running mean: -6.544246\n",
      "ep 2745: ep_len:186 episode reward: total was 3.780000. running mean: -6.441003\n",
      "ep 2745: ep_len:54 episode reward: total was 25.500000. running mean: -6.121593\n",
      "ep 2745: ep_len:2970 episode reward: total was -44.280000. running mean: -6.503177\n",
      "ep 2745: ep_len:500 episode reward: total was 15.570000. running mean: -6.282446\n",
      "ep 2745: ep_len:43 episode reward: total was 20.000000. running mean: -6.019621\n",
      "ep 2745: ep_len:62 episode reward: total was 29.500000. running mean: -5.664425\n",
      "ep 2745: ep_len:1445 episode reward: total was -265.820000. running mean: -8.265981\n",
      "ep 2745: ep_len:4007 episode reward: total was -3131.110000. running mean: -39.494421\n",
      "ep 2745: ep_len:1275 episode reward: total was -84.380000. running mean: -39.943277\n",
      "ep 2745: ep_len:683 episode reward: total was 13.250000. running mean: -39.411344\n",
      "ep 2745: ep_len:539 episode reward: total was 38.580000. running mean: -38.631430\n",
      "ep 2745: ep_len:62 episode reward: total was 29.500000. running mean: -37.950116\n",
      "ep 2745: ep_len:45 episode reward: total was 19.500000. running mean: -37.375615\n",
      "ep 2745: ep_len:744 episode reward: total was -43.600000. running mean: -37.437859\n",
      "ep 2745: ep_len:2841 episode reward: total was -179.670000. running mean: -38.860180\n",
      "ep 2745: ep_len:62 episode reward: total was 29.500000. running mean: -38.176578\n",
      "epsilon:0.009992 episode_count: 41334. steps_count: 44436344.000000\n",
      "ep 2746: ep_len:1105 episode reward: total was 3.260000. running mean: -37.762213\n",
      "ep 2746: ep_len:943 episode reward: total was -0.840000. running mean: -37.392991\n",
      "ep 2746: ep_len:94 episode reward: total was 45.500000. running mean: -36.564061\n",
      "ep 2746: ep_len:745 episode reward: total was -11.910000. running mean: -36.317520\n",
      "ep 2746: ep_len:116 episode reward: total was 52.000000. running mean: -35.434345\n",
      "ep 2746: ep_len:500 episode reward: total was 1.480000. running mean: -35.065201\n",
      "ep 2746: ep_len:634 episode reward: total was 22.670000. running mean: -34.487849\n",
      "ep 2746: ep_len:660 episode reward: total was -32.960000. running mean: -34.472571\n",
      "ep 2746: ep_len:7328 episode reward: total was -52.800000. running mean: -34.655845\n",
      "ep 2746: ep_len:627 episode reward: total was 17.420000. running mean: -34.135087\n",
      "ep 2746: ep_len:117 episode reward: total was 55.500000. running mean: -33.238736\n",
      "ep 2746: ep_len:32 episode reward: total was 14.500000. running mean: -32.761349\n",
      "ep 2746: ep_len:95 episode reward: total was 46.000000. running mean: -31.973735\n",
      "ep 2746: ep_len:1499 episode reward: total was 6.830000. running mean: -31.585698\n",
      "ep 2746: ep_len:2723 episode reward: total was 7.600000. running mean: -31.193841\n",
      "epsilon:0.009992 episode_count: 41349. steps_count: 44453562.000000\n",
      "ep 2747: ep_len:612 episode reward: total was 15.620000. running mean: -30.725702\n",
      "ep 2747: ep_len:697 episode reward: total was -1.280000. running mean: -30.431245\n",
      "ep 2747: ep_len:79 episode reward: total was 38.000000. running mean: -29.746933\n",
      "ep 2747: ep_len:2972 episode reward: total was -41.270000. running mean: -29.862163\n",
      "ep 2747: ep_len:500 episode reward: total was 15.870000. running mean: -29.404842\n",
      "ep 2747: ep_len:54 episode reward: total was 25.500000. running mean: -28.855793\n",
      "ep 2747: ep_len:45 episode reward: total was 21.000000. running mean: -28.357235\n",
      "ep 2747: ep_len:834 episode reward: total was 9.670000. running mean: -27.976963\n",
      "ep 2747: ep_len:648 episode reward: total was 24.010000. running mean: -27.457094\n",
      "ep 2747: ep_len:603 episode reward: total was -4.450000. running mean: -27.227023\n",
      "ep 2747: ep_len:749 episode reward: total was -2.030000. running mean: -26.975052\n",
      "ep 2747: ep_len:613 episode reward: total was 7.390000. running mean: -26.631402\n",
      "ep 2747: ep_len:67 episode reward: total was 32.000000. running mean: -26.045088\n",
      "ep 2747: ep_len:845 episode reward: total was 6.240000. running mean: -25.722237\n",
      "ep 2747: ep_len:2785 episode reward: total was -12.380000. running mean: -25.588815\n",
      "epsilon:0.009992 episode_count: 41364. steps_count: 44465665.000000\n",
      "ep 2748: ep_len:968 episode reward: total was -79.370000. running mean: -26.126626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2748: ep_len:682 episode reward: total was -12.540000. running mean: -25.990760\n",
      "ep 2748: ep_len:2995 episode reward: total was -60.190000. running mean: -26.332753\n",
      "ep 2748: ep_len:602 episode reward: total was -5.260000. running mean: -26.122025\n",
      "ep 2748: ep_len:34 episode reward: total was 15.500000. running mean: -25.705805\n",
      "ep 2748: ep_len:100 episode reward: total was 45.500000. running mean: -24.993747\n",
      "ep 2748: ep_len:77 episode reward: total was 37.000000. running mean: -24.373809\n",
      "ep 2748: ep_len:500 episode reward: total was 19.030000. running mean: -23.939771\n",
      "ep 2748: ep_len:643 episode reward: total was 22.460000. running mean: -23.475773\n",
      "ep 2748: ep_len:615 episode reward: total was 3.960000. running mean: -23.201416\n",
      "ep 2748: ep_len:915 episode reward: total was 45.350000. running mean: -22.515902\n",
      "ep 2748: ep_len:689 episode reward: total was -33.750000. running mean: -22.628243\n",
      "ep 2748: ep_len:62 episode reward: total was 28.000000. running mean: -22.121960\n",
      "ep 2748: ep_len:124 episode reward: total was 57.500000. running mean: -21.325741\n",
      "ep 2748: ep_len:514 episode reward: total was 18.050000. running mean: -20.931983\n",
      "ep 2748: ep_len:2850 episode reward: total was -2.210000. running mean: -20.744763\n",
      "epsilon:0.009992 episode_count: 41380. steps_count: 44478035.000000\n",
      "ep 2749: ep_len:847 episode reward: total was 2.330000. running mean: -20.514016\n",
      "ep 2749: ep_len:650 episode reward: total was -22.960000. running mean: -20.538475\n",
      "ep 2749: ep_len:68 episode reward: total was 32.500000. running mean: -20.008091\n",
      "ep 2749: ep_len:786 episode reward: total was 28.840000. running mean: -19.519610\n",
      "ep 2749: ep_len:37 episode reward: total was 17.000000. running mean: -19.154414\n",
      "ep 2749: ep_len:93 episode reward: total was 45.000000. running mean: -18.512870\n",
      "ep 2749: ep_len:41 episode reward: total was 17.500000. running mean: -18.152741\n",
      "ep 2749: ep_len:500 episode reward: total was 8.850000. running mean: -17.882713\n",
      "ep 2749: ep_len:317 episode reward: total was 9.720000. running mean: -17.606686\n",
      "ep 2749: ep_len:580 episode reward: total was 13.230000. running mean: -17.298319\n",
      "ep 2749: ep_len:689 episode reward: total was -4.350000. running mean: -17.168836\n",
      "ep 2749: ep_len:1083 episode reward: total was 44.950000. running mean: -16.547648\n",
      "ep 2749: ep_len:682 episode reward: total was -1.480000. running mean: -16.396971\n",
      "ep 2749: ep_len:2866 episode reward: total was 1.320000. running mean: -16.219802\n",
      "ep 2749: ep_len:56 episode reward: total was 26.500000. running mean: -15.792604\n",
      "epsilon:0.009992 episode_count: 41395. steps_count: 44487330.000000\n",
      "ep 2750: ep_len:1159 episode reward: total was 17.680000. running mean: -15.457878\n",
      "ep 2750: ep_len:903 episode reward: total was 2.950000. running mean: -15.273799\n",
      "ep 2750: ep_len:3000 episode reward: total was 20.510000. running mean: -14.915961\n",
      "ep 2750: ep_len:794 episode reward: total was -12.430000. running mean: -14.891101\n",
      "ep 2750: ep_len:68 episode reward: total was 32.500000. running mean: -14.417190\n",
      "ep 2750: ep_len:152 episode reward: total was 70.000000. running mean: -13.573018\n",
      "ep 2750: ep_len:50 episode reward: total was 19.000000. running mean: -13.247288\n",
      "ep 2750: ep_len:1111 episode reward: total was -13.300000. running mean: -13.247815\n",
      "ep 2750: ep_len:3476 episode reward: total was -1041.060000. running mean: -23.525937\n",
      "ep 2750: ep_len:920 episode reward: total was -31.800000. running mean: -23.608678\n",
      "ep 2750: ep_len:849 episode reward: total was 40.180000. running mean: -22.970791\n",
      "ep 2750: ep_len:983 episode reward: total was 21.450000. running mean: -22.526583\n",
      "ep 2750: ep_len:146 episode reward: total was 70.000000. running mean: -21.601317\n",
      "ep 2750: ep_len:68 episode reward: total was 32.500000. running mean: -21.060304\n",
      "ep 2750: ep_len:837 episode reward: total was 4.180000. running mean: -20.807901\n",
      "ep 2750: ep_len:2849 episode reward: total was -20.160000. running mean: -20.801422\n",
      "ep 2750: ep_len:40 episode reward: total was 18.500000. running mean: -20.408408\n",
      "epsilon:0.009992 episode_count: 41412. steps_count: 44504735.000000\n",
      "ep 2751: ep_len:1120 episode reward: total was -5.130000. running mean: -20.255624\n",
      "ep 2751: ep_len:761 episode reward: total was -16.800000. running mean: -20.221068\n",
      "ep 2751: ep_len:3041 episode reward: total was 18.470000. running mean: -19.834157\n",
      "ep 2751: ep_len:906 episode reward: total was 80.750000. running mean: -18.828315\n",
      "ep 2751: ep_len:101 episode reward: total was 46.000000. running mean: -18.180032\n",
      "ep 2751: ep_len:80 episode reward: total was 38.500000. running mean: -17.613232\n",
      "ep 2751: ep_len:1047 episode reward: total was -83.630000. running mean: -18.273399\n",
      "ep 2751: ep_len:679 episode reward: total was 17.770000. running mean: -17.912965\n",
      "ep 2751: ep_len:1258 episode reward: total was -65.050000. running mean: -18.384336\n",
      "ep 2751: ep_len:653 episode reward: total was 13.380000. running mean: -18.066692\n",
      "ep 2751: ep_len:700 episode reward: total was -28.320000. running mean: -18.169226\n",
      "ep 2751: ep_len:869 episode reward: total was 13.960000. running mean: -17.847933\n",
      "ep 2751: ep_len:2933 episode reward: total was -13.190000. running mean: -17.801354\n",
      "ep 2751: ep_len:40 episode reward: total was 17.000000. running mean: -17.453340\n",
      "epsilon:0.009992 episode_count: 41426. steps_count: 44518923.000000\n",
      "ep 2752: ep_len:804 episode reward: total was -155.750000. running mean: -18.836307\n",
      "ep 2752: ep_len:1619 episode reward: total was -38.970000. running mean: -19.037644\n",
      "ep 2752: ep_len:44 episode reward: total was 20.500000. running mean: -18.642268\n",
      "ep 2752: ep_len:2935 episode reward: total was -10.180000. running mean: -18.557645\n",
      "ep 2752: ep_len:665 episode reward: total was 9.410000. running mean: -18.277968\n",
      "ep 2752: ep_len:47 episode reward: total was 20.500000. running mean: -17.890189\n",
      "ep 2752: ep_len:77 episode reward: total was 35.500000. running mean: -17.356287\n",
      "ep 2752: ep_len:544 episode reward: total was 27.950000. running mean: -16.903224\n",
      "ep 2752: ep_len:3597 episode reward: total was -358.610000. running mean: -20.320292\n",
      "ep 2752: ep_len:1537 episode reward: total was -21.560000. running mean: -20.332689\n",
      "ep 2752: ep_len:655 episode reward: total was 6.260000. running mean: -20.066762\n",
      "ep 2752: ep_len:500 episode reward: total was 14.910000. running mean: -19.716994\n",
      "ep 2752: ep_len:112 episode reward: total was 54.500000. running mean: -18.974824\n",
      "ep 2752: ep_len:760 episode reward: total was -132.960000. running mean: -20.114676\n",
      "ep 2752: ep_len:2847 episode reward: total was 2.810000. running mean: -19.885429\n",
      "epsilon:0.009992 episode_count: 41441. steps_count: 44535666.000000\n",
      "ep 2753: ep_len:727 episode reward: total was -72.910000. running mean: -20.415675\n",
      "ep 2753: ep_len:500 episode reward: total was 20.820000. running mean: -20.003318\n",
      "ep 2753: ep_len:48 episode reward: total was 22.500000. running mean: -19.578285\n",
      "ep 2753: ep_len:2994 episode reward: total was -157.340000. running mean: -20.955902\n",
      "ep 2753: ep_len:658 episode reward: total was -80.450000. running mean: -21.550843\n",
      "ep 2753: ep_len:500 episode reward: total was 24.960000. running mean: -21.085735\n",
      "ep 2753: ep_len:337 episode reward: total was 22.040000. running mean: -20.654477\n",
      "ep 2753: ep_len:783 episode reward: total was -24.830000. running mean: -20.696233\n",
      "ep 2753: ep_len:889 episode reward: total was 57.420000. running mean: -19.915070\n",
      "ep 2753: ep_len:905 episode reward: total was 18.920000. running mean: -19.526720\n",
      "ep 2753: ep_len:73 episode reward: total was 35.000000. running mean: -18.981452\n",
      "ep 2753: ep_len:789 episode reward: total was -10.400000. running mean: -18.895638\n",
      "ep 2753: ep_len:2777 episode reward: total was -31.900000. running mean: -19.025682\n",
      "epsilon:0.009992 episode_count: 41454. steps_count: 44547646.000000\n",
      "ep 2754: ep_len:1454 episode reward: total was -6.380000. running mean: -18.899225\n",
      "ep 2754: ep_len:1241 episode reward: total was -58.460000. running mean: -19.294832\n",
      "ep 2754: ep_len:3063 episode reward: total was -47.400000. running mean: -19.575884\n",
      "ep 2754: ep_len:640 episode reward: total was -164.830000. running mean: -21.028425\n",
      "ep 2754: ep_len:500 episode reward: total was 48.170000. running mean: -20.336441\n",
      "ep 2754: ep_len:3692 episode reward: total was -69.320000. running mean: -20.826277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2754: ep_len:655 episode reward: total was -26.950000. running mean: -20.887514\n",
      "ep 2754: ep_len:737 episode reward: total was 27.880000. running mean: -20.399839\n",
      "ep 2754: ep_len:784 episode reward: total was 0.860000. running mean: -20.187240\n",
      "ep 2754: ep_len:91 episode reward: total was 42.500000. running mean: -19.560368\n",
      "ep 2754: ep_len:110 episode reward: total was 52.000000. running mean: -18.844764\n",
      "ep 2754: ep_len:61 episode reward: total was 27.500000. running mean: -18.381317\n",
      "ep 2754: ep_len:62 episode reward: total was 29.500000. running mean: -17.902503\n",
      "ep 2754: ep_len:586 episode reward: total was 10.640000. running mean: -17.617078\n",
      "ep 2754: ep_len:2871 episode reward: total was 12.510000. running mean: -17.315808\n",
      "epsilon:0.009992 episode_count: 41469. steps_count: 44564193.000000\n",
      "ep 2755: ep_len:667 episode reward: total was 30.680000. running mean: -16.835850\n",
      "ep 2755: ep_len:799 episode reward: total was -3.740000. running mean: -16.704891\n",
      "ep 2755: ep_len:69 episode reward: total was 31.500000. running mean: -16.222842\n",
      "ep 2755: ep_len:2998 episode reward: total was -10.080000. running mean: -16.161414\n",
      "ep 2755: ep_len:824 episode reward: total was 24.780000. running mean: -15.752000\n",
      "ep 2755: ep_len:75 episode reward: total was 36.000000. running mean: -15.234480\n",
      "ep 2755: ep_len:49 episode reward: total was 23.000000. running mean: -14.852135\n",
      "ep 2755: ep_len:923 episode reward: total was 78.200000. running mean: -13.921613\n",
      "ep 2755: ep_len:342 episode reward: total was 19.180000. running mean: -13.590597\n",
      "ep 2755: ep_len:571 episode reward: total was -53.040000. running mean: -13.985091\n",
      "ep 2755: ep_len:812 episode reward: total was 36.290000. running mean: -13.482340\n",
      "ep 2755: ep_len:500 episode reward: total was 16.820000. running mean: -13.179317\n",
      "ep 2755: ep_len:89 episode reward: total was 40.000000. running mean: -12.647524\n",
      "ep 2755: ep_len:80 episode reward: total was 37.000000. running mean: -12.151049\n",
      "ep 2755: ep_len:747 episode reward: total was -30.440000. running mean: -12.333938\n",
      "ep 2755: ep_len:2892 episode reward: total was -13.030000. running mean: -12.340899\n",
      "epsilon:0.009992 episode_count: 41485. steps_count: 44576630.000000\n",
      "ep 2756: ep_len:1508 episode reward: total was 5.760000. running mean: -12.159890\n",
      "ep 2756: ep_len:192 episode reward: total was -118.920000. running mean: -13.227491\n",
      "ep 2756: ep_len:2885 episode reward: total was -14.590000. running mean: -13.241116\n",
      "ep 2756: ep_len:1467 episode reward: total was -4.660000. running mean: -13.155305\n",
      "ep 2756: ep_len:59 episode reward: total was 28.000000. running mean: -12.743752\n",
      "ep 2756: ep_len:137 episode reward: total was 65.500000. running mean: -11.961314\n",
      "ep 2756: ep_len:51 episode reward: total was 24.000000. running mean: -11.601701\n",
      "ep 2756: ep_len:628 episode reward: total was -19.640000. running mean: -11.682084\n",
      "ep 2756: ep_len:3571 episode reward: total was -113.990000. running mean: -12.705163\n",
      "ep 2756: ep_len:842 episode reward: total was 6.440000. running mean: -12.513712\n",
      "ep 2756: ep_len:888 episode reward: total was 54.970000. running mean: -11.838875\n",
      "ep 2756: ep_len:686 episode reward: total was -6.440000. running mean: -11.784886\n",
      "ep 2756: ep_len:64 episode reward: total was 29.000000. running mean: -11.377037\n",
      "ep 2756: ep_len:1102 episode reward: total was 15.780000. running mean: -11.105467\n",
      "ep 2756: ep_len:2825 episode reward: total was -4.760000. running mean: -11.042012\n",
      "epsilon:0.009992 episode_count: 41500. steps_count: 44593535.000000\n",
      "ep 2757: ep_len:968 episode reward: total was -36.160000. running mean: -11.293192\n",
      "ep 2757: ep_len:784 episode reward: total was 1.070000. running mean: -11.169560\n",
      "ep 2757: ep_len:58 episode reward: total was 27.500000. running mean: -10.782864\n",
      "ep 2757: ep_len:3049 episode reward: total was 1.600000. running mean: -10.659036\n",
      "ep 2757: ep_len:697 episode reward: total was -18.450000. running mean: -10.736945\n",
      "ep 2757: ep_len:85 episode reward: total was 41.000000. running mean: -10.219576\n",
      "ep 2757: ep_len:827 episode reward: total was 23.310000. running mean: -9.884280\n",
      "ep 2757: ep_len:3653 episode reward: total was -4.670000. running mean: -9.832137\n",
      "ep 2757: ep_len:529 episode reward: total was -24.170000. running mean: -9.975516\n",
      "ep 2757: ep_len:809 episode reward: total was 48.900000. running mean: -9.386761\n",
      "ep 2757: ep_len:1091 episode reward: total was -10.470000. running mean: -9.397593\n",
      "ep 2757: ep_len:179 episode reward: total was 86.500000. running mean: -8.438617\n",
      "ep 2757: ep_len:1022 episode reward: total was 13.020000. running mean: -8.224031\n",
      "ep 2757: ep_len:2839 episode reward: total was 4.630000. running mean: -8.095491\n",
      "epsilon:0.009992 episode_count: 41514. steps_count: 44610125.000000\n",
      "ep 2758: ep_len:1477 episode reward: total was 26.660000. running mean: -7.747936\n",
      "ep 2758: ep_len:189 episode reward: total was 5.220000. running mean: -7.618256\n",
      "ep 2758: ep_len:2954 episode reward: total was -11.330000. running mean: -7.655374\n",
      "ep 2758: ep_len:500 episode reward: total was 17.030000. running mean: -7.408520\n",
      "ep 2758: ep_len:165 episode reward: total was 76.500000. running mean: -6.569435\n",
      "ep 2758: ep_len:77 episode reward: total was 35.500000. running mean: -6.148741\n",
      "ep 2758: ep_len:897 episode reward: total was 44.070000. running mean: -5.646553\n",
      "ep 2758: ep_len:3938 episode reward: total was -237.580000. running mean: -7.965888\n",
      "ep 2758: ep_len:3893 episode reward: total was -706.620000. running mean: -14.952429\n",
      "ep 2758: ep_len:826 episode reward: total was 18.030000. running mean: -14.622604\n",
      "ep 2758: ep_len:919 episode reward: total was 14.660000. running mean: -14.329778\n",
      "ep 2758: ep_len:163 episode reward: total was 75.500000. running mean: -13.431481\n",
      "ep 2758: ep_len:56 episode reward: total was 23.500000. running mean: -13.062166\n",
      "ep 2758: ep_len:1160 episode reward: total was -12.810000. running mean: -13.059644\n",
      "ep 2758: ep_len:2775 episode reward: total was -3.700000. running mean: -12.966048\n",
      "epsilon:0.009992 episode_count: 41529. steps_count: 44630114.000000\n",
      "ep 2759: ep_len:678 episode reward: total was -37.400000. running mean: -13.210387\n",
      "ep 2759: ep_len:962 episode reward: total was 22.130000. running mean: -12.856983\n",
      "ep 2759: ep_len:2899 episode reward: total was -12.040000. running mean: -12.848814\n",
      "ep 2759: ep_len:626 episode reward: total was -2.510000. running mean: -12.745425\n",
      "ep 2759: ep_len:36 episode reward: total was 15.000000. running mean: -12.467971\n",
      "ep 2759: ep_len:84 episode reward: total was 36.000000. running mean: -11.983291\n",
      "ep 2759: ep_len:67 episode reward: total was 32.000000. running mean: -11.543459\n",
      "ep 2759: ep_len:667 episode reward: total was 10.700000. running mean: -11.321024\n",
      "ep 2759: ep_len:3910 episode reward: total was -232.760000. running mean: -13.535414\n",
      "ep 2759: ep_len:554 episode reward: total was 1.330000. running mean: -13.386760\n",
      "ep 2759: ep_len:707 episode reward: total was -92.940000. running mean: -14.182292\n",
      "ep 2759: ep_len:618 episode reward: total was 9.310000. running mean: -13.947369\n",
      "ep 2759: ep_len:80 episode reward: total was 23.500000. running mean: -13.572895\n",
      "ep 2759: ep_len:73 episode reward: total was 32.000000. running mean: -13.117166\n",
      "ep 2759: ep_len:1173 episode reward: total was -20.760000. running mean: -13.193595\n",
      "ep 2759: ep_len:47 episode reward: total was 20.500000. running mean: -12.856659\n",
      "ep 2759: ep_len:61 episode reward: total was 29.000000. running mean: -12.438092\n",
      "epsilon:0.009992 episode_count: 41546. steps_count: 44643356.000000\n",
      "ep 2760: ep_len:1428 episode reward: total was 6.490000. running mean: -12.248811\n",
      "ep 2760: ep_len:1169 episode reward: total was -6.230000. running mean: -12.188623\n",
      "ep 2760: ep_len:42 episode reward: total was 19.500000. running mean: -11.871737\n",
      "ep 2760: ep_len:3105 episode reward: total was 6.940000. running mean: -11.683620\n",
      "ep 2760: ep_len:500 episode reward: total was -18.150000. running mean: -11.748283\n",
      "ep 2760: ep_len:1063 episode reward: total was -6.710000. running mean: -11.697901\n",
      "ep 2760: ep_len:327 episode reward: total was 21.940000. running mean: -11.361522\n",
      "ep 2760: ep_len:569 episode reward: total was 26.590000. running mean: -10.982006\n",
      "ep 2760: ep_len:790 episode reward: total was 22.930000. running mean: -10.642886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2760: ep_len:957 episode reward: total was -28.030000. running mean: -10.816757\n",
      "ep 2760: ep_len:66 episode reward: total was 31.500000. running mean: -10.393590\n",
      "ep 2760: ep_len:1039 episode reward: total was 7.080000. running mean: -10.218854\n",
      "ep 2760: ep_len:2956 episode reward: total was -15.200000. running mean: -10.268665\n",
      "ep 2760: ep_len:32 episode reward: total was 14.500000. running mean: -10.020979\n",
      "epsilon:0.009992 episode_count: 41560. steps_count: 44657399.000000\n",
      "ep 2761: ep_len:997 episode reward: total was -85.140000. running mean: -10.772169\n",
      "ep 2761: ep_len:1623 episode reward: total was -7.590000. running mean: -10.740347\n",
      "ep 2761: ep_len:61 episode reward: total was 27.500000. running mean: -10.357944\n",
      "ep 2761: ep_len:2965 episode reward: total was -35.560000. running mean: -10.609964\n",
      "ep 2761: ep_len:505 episode reward: total was -21.380000. running mean: -10.717665\n",
      "ep 2761: ep_len:62 episode reward: total was 28.000000. running mean: -10.330488\n",
      "ep 2761: ep_len:680 episode reward: total was 17.720000. running mean: -10.049983\n",
      "ep 2761: ep_len:4088 episode reward: total was -191.780000. running mean: -11.867283\n",
      "ep 2761: ep_len:2632 episode reward: total was -170.830000. running mean: -13.456911\n",
      "ep 2761: ep_len:676 episode reward: total was 11.040000. running mean: -13.211941\n",
      "ep 2761: ep_len:500 episode reward: total was -11.050000. running mean: -13.190322\n",
      "ep 2761: ep_len:71 episode reward: total was 34.000000. running mean: -12.718419\n",
      "ep 2761: ep_len:169 episode reward: total was 77.000000. running mean: -11.821235\n",
      "ep 2761: ep_len:40 episode reward: total was 18.500000. running mean: -11.518022\n",
      "ep 2761: ep_len:1160 episode reward: total was -32.000000. running mean: -11.722842\n",
      "ep 2761: ep_len:2839 episode reward: total was 14.150000. running mean: -11.464114\n",
      "epsilon:0.009992 episode_count: 41576. steps_count: 44676467.000000\n",
      "ep 2762: ep_len:1455 episode reward: total was 1.000000. running mean: -11.339472\n",
      "ep 2762: ep_len:500 episode reward: total was 27.220000. running mean: -10.953878\n",
      "ep 2762: ep_len:2976 episode reward: total was -42.240000. running mean: -11.266739\n",
      "ep 2762: ep_len:866 episode reward: total was 14.110000. running mean: -11.012972\n",
      "ep 2762: ep_len:54 episode reward: total was 25.500000. running mean: -10.647842\n",
      "ep 2762: ep_len:1073 episode reward: total was -1.560000. running mean: -10.556963\n",
      "ep 2762: ep_len:4060 episode reward: total was -40.410000. running mean: -10.855494\n",
      "ep 2762: ep_len:649 episode reward: total was -62.300000. running mean: -11.369939\n",
      "ep 2762: ep_len:804 episode reward: total was 21.630000. running mean: -11.039939\n",
      "ep 2762: ep_len:610 episode reward: total was -5.800000. running mean: -10.987540\n",
      "ep 2762: ep_len:83 episode reward: total was 40.000000. running mean: -10.477665\n",
      "ep 2762: ep_len:131 episode reward: total was 64.000000. running mean: -9.732888\n",
      "ep 2762: ep_len:1131 episode reward: total was -23.200000. running mean: -9.867559\n",
      "ep 2762: ep_len:2834 episode reward: total was 10.580000. running mean: -9.663084\n",
      "ep 2762: ep_len:55 episode reward: total was 23.000000. running mean: -9.336453\n",
      "epsilon:0.009992 episode_count: 41591. steps_count: 44693748.000000\n",
      "ep 2763: ep_len:500 episode reward: total was 23.970000. running mean: -9.003388\n",
      "ep 2763: ep_len:183 episode reward: total was -12.110000. running mean: -9.034454\n",
      "ep 2763: ep_len:46 episode reward: total was 21.500000. running mean: -8.729110\n",
      "ep 2763: ep_len:2998 episode reward: total was -17.530000. running mean: -8.817119\n",
      "ep 2763: ep_len:641 episode reward: total was -25.720000. running mean: -8.986147\n",
      "ep 2763: ep_len:41 episode reward: total was 19.000000. running mean: -8.706286\n",
      "ep 2763: ep_len:57 episode reward: total was 25.500000. running mean: -8.364223\n",
      "ep 2763: ep_len:567 episode reward: total was 45.940000. running mean: -7.821181\n",
      "ep 2763: ep_len:652 episode reward: total was 29.590000. running mean: -7.447069\n",
      "ep 2763: ep_len:536 episode reward: total was -12.870000. running mean: -7.501298\n",
      "ep 2763: ep_len:7390 episode reward: total was -147.070000. running mean: -8.896985\n",
      "ep 2763: ep_len:590 episode reward: total was -19.000000. running mean: -8.998016\n",
      "ep 2763: ep_len:65 episode reward: total was 31.000000. running mean: -8.598035\n",
      "ep 2763: ep_len:27 episode reward: total was 10.500000. running mean: -8.407055\n",
      "ep 2763: ep_len:611 episode reward: total was -12.240000. running mean: -8.445385\n",
      "ep 2763: ep_len:2879 episode reward: total was -25.700000. running mean: -8.617931\n",
      "ep 2763: ep_len:32 episode reward: total was 14.500000. running mean: -8.386751\n",
      "epsilon:0.009992 episode_count: 41608. steps_count: 44711563.000000\n",
      "ep 2764: ep_len:1436 episode reward: total was 6.600000. running mean: -8.236884\n",
      "ep 2764: ep_len:1166 episode reward: total was 13.480000. running mean: -8.019715\n",
      "ep 2764: ep_len:3007 episode reward: total was -20.670000. running mean: -8.146218\n",
      "ep 2764: ep_len:500 episode reward: total was -42.310000. running mean: -8.487856\n",
      "ep 2764: ep_len:774 episode reward: total was -34.700000. running mean: -8.749977\n",
      "ep 2764: ep_len:592 episode reward: total was 15.640000. running mean: -8.506077\n",
      "ep 2764: ep_len:2764 episode reward: total was -699.610000. running mean: -15.417117\n",
      "ep 2764: ep_len:761 episode reward: total was 5.150000. running mean: -15.211445\n",
      "ep 2764: ep_len:1511 episode reward: total was -20.200000. running mean: -15.261331\n",
      "ep 2764: ep_len:55 episode reward: total was 23.000000. running mean: -14.878718\n",
      "ep 2764: ep_len:120 episode reward: total was 57.000000. running mean: -14.159930\n",
      "ep 2764: ep_len:1190 episode reward: total was -28.670000. running mean: -14.305031\n",
      "ep 2764: ep_len:2865 episode reward: total was -17.300000. running mean: -14.334981\n",
      "epsilon:0.009992 episode_count: 41621. steps_count: 44728304.000000\n",
      "ep 2765: ep_len:500 episode reward: total was 12.560000. running mean: -14.066031\n",
      "ep 2765: ep_len:1685 episode reward: total was -80.310000. running mean: -14.728471\n",
      "ep 2765: ep_len:48 episode reward: total was 22.500000. running mean: -14.356186\n",
      "ep 2765: ep_len:3056 episode reward: total was 1.760000. running mean: -14.195024\n",
      "ep 2765: ep_len:808 episode reward: total was 40.170000. running mean: -13.651374\n",
      "ep 2765: ep_len:39 episode reward: total was 18.000000. running mean: -13.334860\n",
      "ep 2765: ep_len:649 episode reward: total was 0.260000. running mean: -13.198912\n",
      "ep 2765: ep_len:3811 episode reward: total was -242.260000. running mean: -15.489522\n",
      "ep 2765: ep_len:530 episode reward: total was -1.940000. running mean: -15.354027\n",
      "ep 2765: ep_len:681 episode reward: total was 5.950000. running mean: -15.140987\n",
      "ep 2765: ep_len:776 episode reward: total was 11.370000. running mean: -14.875877\n",
      "ep 2765: ep_len:80 episode reward: total was 38.500000. running mean: -14.342118\n",
      "ep 2765: ep_len:1168 episode reward: total was -22.310000. running mean: -14.421797\n",
      "ep 2765: ep_len:45 episode reward: total was 21.000000. running mean: -14.067579\n",
      "epsilon:0.009992 episode_count: 41635. steps_count: 44742180.000000\n",
      "ep 2766: ep_len:518 episode reward: total was -173.990000. running mean: -15.666803\n",
      "ep 2766: ep_len:647 episode reward: total was -35.230000. running mean: -15.862435\n",
      "ep 2766: ep_len:2935 episode reward: total was 15.360000. running mean: -15.550211\n",
      "ep 2766: ep_len:3570 episode reward: total was -305.250000. running mean: -18.447209\n",
      "ep 2766: ep_len:30 episode reward: total was 13.500000. running mean: -18.127737\n",
      "ep 2766: ep_len:116 episode reward: total was 52.000000. running mean: -17.426459\n",
      "ep 2766: ep_len:71 episode reward: total was 31.000000. running mean: -16.942195\n",
      "ep 2766: ep_len:1885 episode reward: total was -193.020000. running mean: -18.702973\n",
      "ep 2766: ep_len:3927 episode reward: total was -169.310000. running mean: -20.209043\n",
      "ep 2766: ep_len:615 episode reward: total was 15.070000. running mean: -19.856253\n",
      "ep 2766: ep_len:683 episode reward: total was 41.730000. running mean: -19.240390\n",
      "ep 2766: ep_len:617 episode reward: total was 10.870000. running mean: -18.939286\n",
      "ep 2766: ep_len:500 episode reward: total was 35.740000. running mean: -18.392493\n",
      "ep 2766: ep_len:2889 episode reward: total was -44.710000. running mean: -18.655669\n",
      "ep 2766: ep_len:59 episode reward: total was 28.000000. running mean: -18.189112\n",
      "epsilon:0.009992 episode_count: 41650. steps_count: 44761242.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2767: ep_len:700 episode reward: total was -46.700000. running mean: -18.474221\n",
      "ep 2767: ep_len:3536 episode reward: total was -264.190000. running mean: -20.931378\n",
      "ep 2767: ep_len:2955 episode reward: total was -11.440000. running mean: -20.836465\n",
      "ep 2767: ep_len:500 episode reward: total was 14.020000. running mean: -20.487900\n",
      "ep 2767: ep_len:64 episode reward: total was 29.000000. running mean: -19.993021\n",
      "ep 2767: ep_len:797 episode reward: total was -13.440000. running mean: -19.927491\n",
      "ep 2767: ep_len:317 episode reward: total was 26.950000. running mean: -19.458716\n",
      "ep 2767: ep_len:556 episode reward: total was -23.900000. running mean: -19.503129\n",
      "ep 2767: ep_len:893 episode reward: total was 54.950000. running mean: -18.758597\n",
      "ep 2767: ep_len:725 episode reward: total was -1.000000. running mean: -18.581012\n",
      "ep 2767: ep_len:68 episode reward: total was 31.000000. running mean: -18.085201\n",
      "ep 2767: ep_len:762 episode reward: total was 0.540000. running mean: -17.898949\n",
      "ep 2767: ep_len:2802 episode reward: total was -9.430000. running mean: -17.814260\n",
      "epsilon:0.009992 episode_count: 41663. steps_count: 44775917.000000\n",
      "ep 2768: ep_len:1143 episode reward: total was -3.950000. running mean: -17.675617\n",
      "ep 2768: ep_len:702 episode reward: total was 7.600000. running mean: -17.422861\n",
      "ep 2768: ep_len:79 episode reward: total was 38.000000. running mean: -16.868633\n",
      "ep 2768: ep_len:3032 episode reward: total was -69.060000. running mean: -17.390546\n",
      "ep 2768: ep_len:1148 episode reward: total was -25.050000. running mean: -17.467141\n",
      "ep 2768: ep_len:46 episode reward: total was 20.000000. running mean: -17.092469\n",
      "ep 2768: ep_len:500 episode reward: total was 32.890000. running mean: -16.592645\n",
      "ep 2768: ep_len:3856 episode reward: total was -151.720000. running mean: -17.943918\n",
      "ep 2768: ep_len:1287 episode reward: total was -54.970000. running mean: -18.314179\n",
      "ep 2768: ep_len:7206 episode reward: total was -305.060000. running mean: -21.181637\n",
      "ep 2768: ep_len:525 episode reward: total was -20.810000. running mean: -21.177921\n",
      "ep 2768: ep_len:67 episode reward: total was 30.500000. running mean: -20.661142\n",
      "ep 2768: ep_len:100 episode reward: total was 47.000000. running mean: -19.984530\n",
      "ep 2768: ep_len:736 episode reward: total was -57.450000. running mean: -20.359185\n",
      "ep 2768: ep_len:2948 episode reward: total was -3.800000. running mean: -20.193593\n",
      "ep 2768: ep_len:39 episode reward: total was 18.000000. running mean: -19.811657\n",
      "epsilon:0.009992 episode_count: 41679. steps_count: 44799331.000000\n",
      "ep 2769: ep_len:656 episode reward: total was -16.840000. running mean: -19.781941\n",
      "ep 2769: ep_len:500 episode reward: total was 24.340000. running mean: -19.340721\n",
      "ep 2769: ep_len:64 episode reward: total was 30.500000. running mean: -18.842314\n",
      "ep 2769: ep_len:2976 episode reward: total was -40.300000. running mean: -19.056891\n",
      "ep 2769: ep_len:839 episode reward: total was 23.520000. running mean: -18.631122\n",
      "ep 2769: ep_len:150 episode reward: total was 73.500000. running mean: -17.709811\n",
      "ep 2769: ep_len:57 episode reward: total was 27.000000. running mean: -17.262713\n",
      "ep 2769: ep_len:1495 episode reward: total was -203.610000. running mean: -19.126185\n",
      "ep 2769: ep_len:662 episode reward: total was 23.630000. running mean: -18.698624\n",
      "ep 2769: ep_len:680 episode reward: total was -32.880000. running mean: -18.840437\n",
      "ep 2769: ep_len:7237 episode reward: total was 2.240000. running mean: -18.629633\n",
      "ep 2769: ep_len:900 episode reward: total was -4.080000. running mean: -18.484137\n",
      "ep 2769: ep_len:37 episode reward: total was 17.000000. running mean: -18.129295\n",
      "ep 2769: ep_len:651 episode reward: total was 13.610000. running mean: -17.811902\n",
      "ep 2769: ep_len:46 episode reward: total was 21.500000. running mean: -17.418783\n",
      "epsilon:0.009992 episode_count: 41694. steps_count: 44816281.000000\n",
      "ep 2770: ep_len:703 episode reward: total was -23.440000. running mean: -17.478995\n",
      "ep 2770: ep_len:951 episode reward: total was -16.980000. running mean: -17.474006\n",
      "ep 2770: ep_len:3022 episode reward: total was -48.710000. running mean: -17.786365\n",
      "ep 2770: ep_len:579 episode reward: total was -11.030000. running mean: -17.718802\n",
      "ep 2770: ep_len:51 episode reward: total was 24.000000. running mean: -17.301614\n",
      "ep 2770: ep_len:1399 episode reward: total was -124.520000. running mean: -18.373798\n",
      "ep 2770: ep_len:4004 episode reward: total was -387.780000. running mean: -22.067860\n",
      "ep 2770: ep_len:1558 episode reward: total was -185.410000. running mean: -23.701281\n",
      "ep 2770: ep_len:870 episode reward: total was 43.680000. running mean: -23.027468\n",
      "ep 2770: ep_len:1448 episode reward: total was -28.290000. running mean: -23.080094\n",
      "ep 2770: ep_len:2139 episode reward: total was -107.660000. running mean: -23.925893\n",
      "ep 2770: ep_len:2798 episode reward: total was -10.780000. running mean: -23.794434\n",
      "epsilon:0.009992 episode_count: 41706. steps_count: 44835803.000000\n",
      "ep 2771: ep_len:749 episode reward: total was 1.640000. running mean: -23.540089\n",
      "ep 2771: ep_len:1003 episode reward: total was -23.340000. running mean: -23.538088\n",
      "ep 2771: ep_len:63 episode reward: total was 28.500000. running mean: -23.017708\n",
      "ep 2771: ep_len:2918 episode reward: total was -69.780000. running mean: -23.485331\n",
      "ep 2771: ep_len:774 episode reward: total was 1.040000. running mean: -23.240077\n",
      "ep 2771: ep_len:134 episode reward: total was 61.000000. running mean: -22.397676\n",
      "ep 2771: ep_len:898 episode reward: total was 62.270000. running mean: -21.551000\n",
      "ep 2771: ep_len:644 episode reward: total was 20.480000. running mean: -21.130690\n",
      "ep 2771: ep_len:608 episode reward: total was 0.950000. running mean: -20.909883\n",
      "ep 2771: ep_len:796 episode reward: total was 16.800000. running mean: -20.532784\n",
      "ep 2771: ep_len:1065 episode reward: total was 48.840000. running mean: -19.839056\n",
      "ep 2771: ep_len:71 episode reward: total was 34.000000. running mean: -19.300666\n",
      "ep 2771: ep_len:124 episode reward: total was 60.500000. running mean: -18.502659\n",
      "ep 2771: ep_len:38 episode reward: total was 16.000000. running mean: -18.157632\n",
      "ep 2771: ep_len:1098 episode reward: total was -18.940000. running mean: -18.165456\n",
      "ep 2771: ep_len:2869 episode reward: total was -33.180000. running mean: -18.315601\n",
      "epsilon:0.009992 episode_count: 41722. steps_count: 44849655.000000\n",
      "ep 2772: ep_len:1145 episode reward: total was 9.240000. running mean: -18.040045\n",
      "ep 2772: ep_len:1593 episode reward: total was -54.900000. running mean: -18.408645\n",
      "ep 2772: ep_len:3008 episode reward: total was -33.280000. running mean: -18.557359\n",
      "ep 2772: ep_len:695 episode reward: total was -6.450000. running mean: -18.436285\n",
      "ep 2772: ep_len:44 episode reward: total was 17.500000. running mean: -18.076922\n",
      "ep 2772: ep_len:838 episode reward: total was 29.940000. running mean: -17.596753\n",
      "ep 2772: ep_len:3640 episode reward: total was -444.060000. running mean: -21.861385\n",
      "ep 2772: ep_len:500 episode reward: total was 28.140000. running mean: -21.361371\n",
      "ep 2772: ep_len:768 episode reward: total was 13.920000. running mean: -21.008558\n",
      "ep 2772: ep_len:1386 episode reward: total was -32.650000. running mean: -21.124972\n",
      "ep 2772: ep_len:190 episode reward: total was 92.000000. running mean: -19.993722\n",
      "ep 2772: ep_len:35 episode reward: total was 16.000000. running mean: -19.633785\n",
      "ep 2772: ep_len:75 episode reward: total was 36.000000. running mean: -19.077447\n",
      "ep 2772: ep_len:789 episode reward: total was -36.720000. running mean: -19.253873\n",
      "ep 2772: ep_len:2868 episode reward: total was -37.750000. running mean: -19.438834\n",
      "epsilon:0.009992 episode_count: 41737. steps_count: 44867229.000000\n",
      "ep 2773: ep_len:861 episode reward: total was -5.620000. running mean: -19.300646\n",
      "ep 2773: ep_len:682 episode reward: total was 1.090000. running mean: -19.096739\n",
      "ep 2773: ep_len:2907 episode reward: total was -40.150000. running mean: -19.307272\n",
      "ep 2773: ep_len:703 episode reward: total was 29.280000. running mean: -18.821399\n",
      "ep 2773: ep_len:69 episode reward: total was 33.000000. running mean: -18.303185\n",
      "ep 2773: ep_len:706 episode reward: total was -37.090000. running mean: -18.491053\n",
      "ep 2773: ep_len:3628 episode reward: total was -35.830000. running mean: -18.664443\n",
      "ep 2773: ep_len:543 episode reward: total was -26.050000. running mean: -18.738298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2773: ep_len:853 episode reward: total was 22.190000. running mean: -18.329015\n",
      "ep 2773: ep_len:500 episode reward: total was -16.870000. running mean: -18.314425\n",
      "ep 2773: ep_len:94 episode reward: total was 45.500000. running mean: -17.676281\n",
      "ep 2773: ep_len:150 episode reward: total was 73.500000. running mean: -16.764518\n",
      "ep 2773: ep_len:617 episode reward: total was -4.100000. running mean: -16.637873\n",
      "ep 2773: ep_len:2870 episode reward: total was 3.410000. running mean: -16.437394\n",
      "ep 2773: ep_len:46 episode reward: total was 21.500000. running mean: -16.058020\n",
      "epsilon:0.009992 episode_count: 41752. steps_count: 44882458.000000\n",
      "ep 2774: ep_len:500 episode reward: total was 9.830000. running mean: -15.799140\n",
      "ep 2774: ep_len:914 episode reward: total was 7.590000. running mean: -15.565249\n",
      "ep 2774: ep_len:2982 episode reward: total was -25.960000. running mean: -15.669196\n",
      "ep 2774: ep_len:1665 episode reward: total was -79.810000. running mean: -16.310604\n",
      "ep 2774: ep_len:43 episode reward: total was 18.500000. running mean: -15.962498\n",
      "ep 2774: ep_len:1395 episode reward: total was -119.870000. running mean: -17.001573\n",
      "ep 2774: ep_len:337 episode reward: total was 11.200000. running mean: -16.719558\n",
      "ep 2774: ep_len:1546 episode reward: total was -34.910000. running mean: -16.901462\n",
      "ep 2774: ep_len:827 episode reward: total was 66.440000. running mean: -16.068047\n",
      "ep 2774: ep_len:866 episode reward: total was 14.190000. running mean: -15.765467\n",
      "ep 2774: ep_len:739 episode reward: total was -99.100000. running mean: -16.598812\n",
      "ep 2774: ep_len:2848 episode reward: total was -1.130000. running mean: -16.444124\n",
      "ep 2774: ep_len:52 episode reward: total was 23.000000. running mean: -16.049683\n",
      "epsilon:0.009992 episode_count: 41765. steps_count: 44897172.000000\n",
      "ep 2775: ep_len:711 episode reward: total was -15.430000. running mean: -16.043486\n",
      "ep 2775: ep_len:500 episode reward: total was 26.420000. running mean: -15.618851\n",
      "ep 2775: ep_len:65 episode reward: total was 31.000000. running mean: -15.152663\n",
      "ep 2775: ep_len:2954 episode reward: total was 3.740000. running mean: -14.963736\n",
      "ep 2775: ep_len:806 episode reward: total was -13.320000. running mean: -14.947299\n",
      "ep 2775: ep_len:103 episode reward: total was 50.000000. running mean: -14.297826\n",
      "ep 2775: ep_len:95 episode reward: total was 43.000000. running mean: -13.724847\n",
      "ep 2775: ep_len:70 episode reward: total was 33.500000. running mean: -13.252599\n",
      "ep 2775: ep_len:500 episode reward: total was 30.930000. running mean: -12.810773\n",
      "ep 2775: ep_len:307 episode reward: total was 12.430000. running mean: -12.558365\n",
      "ep 2775: ep_len:1494 episode reward: total was -41.540000. running mean: -12.848182\n",
      "ep 2775: ep_len:873 episode reward: total was 52.180000. running mean: -12.197900\n",
      "ep 2775: ep_len:1099 episode reward: total was 0.720000. running mean: -12.068721\n",
      "ep 2775: ep_len:99 episode reward: total was 46.500000. running mean: -11.483034\n",
      "ep 2775: ep_len:47 episode reward: total was 22.000000. running mean: -11.148203\n",
      "ep 2775: ep_len:701 episode reward: total was -4.270000. running mean: -11.079421\n",
      "ep 2775: ep_len:2979 episode reward: total was -588.880000. running mean: -16.857427\n",
      "ep 2775: ep_len:54 episode reward: total was 25.500000. running mean: -16.433853\n",
      "epsilon:0.009992 episode_count: 41783. steps_count: 44910629.000000\n",
      "ep 2776: ep_len:826 episode reward: total was -9.140000. running mean: -16.360914\n",
      "ep 2776: ep_len:216 episode reward: total was -7.730000. running mean: -16.274605\n",
      "ep 2776: ep_len:2961 episode reward: total was -12.320000. running mean: -16.235059\n",
      "ep 2776: ep_len:661 episode reward: total was -14.660000. running mean: -16.219308\n",
      "ep 2776: ep_len:37 episode reward: total was 17.000000. running mean: -15.887115\n",
      "ep 2776: ep_len:73 episode reward: total was 33.500000. running mean: -15.393244\n",
      "ep 2776: ep_len:1868 episode reward: total was -26.230000. running mean: -15.501612\n",
      "ep 2776: ep_len:677 episode reward: total was 28.830000. running mean: -15.058296\n",
      "ep 2776: ep_len:1188 episode reward: total was -59.050000. running mean: -15.498213\n",
      "ep 2776: ep_len:779 episode reward: total was 13.330000. running mean: -15.209931\n",
      "ep 2776: ep_len:761 episode reward: total was -17.290000. running mean: -15.230731\n",
      "ep 2776: ep_len:187 episode reward: total was -170.990000. running mean: -16.788324\n",
      "ep 2776: ep_len:1461 episode reward: total was 20.470000. running mean: -16.415741\n",
      "ep 2776: ep_len:2865 episode reward: total was -37.630000. running mean: -16.627883\n",
      "epsilon:0.009992 episode_count: 41797. steps_count: 44925189.000000\n",
      "ep 2777: ep_len:1084 episode reward: total was 3.910000. running mean: -16.422504\n",
      "ep 2777: ep_len:795 episode reward: total was 0.620000. running mean: -16.252079\n",
      "ep 2777: ep_len:3066 episode reward: total was -33.490000. running mean: -16.424459\n",
      "ep 2777: ep_len:843 episode reward: total was -18.710000. running mean: -16.447314\n",
      "ep 2777: ep_len:141 episode reward: total was 66.000000. running mean: -15.622841\n",
      "ep 2777: ep_len:68 episode reward: total was 31.000000. running mean: -15.156612\n",
      "ep 2777: ep_len:769 episode reward: total was -24.310000. running mean: -15.248146\n",
      "ep 2777: ep_len:3589 episode reward: total was -273.970000. running mean: -17.835365\n",
      "ep 2777: ep_len:1165 episode reward: total was -32.290000. running mean: -17.979911\n",
      "ep 2777: ep_len:851 episode reward: total was 43.050000. running mean: -17.369612\n",
      "ep 2777: ep_len:940 episode reward: total was 27.210000. running mean: -16.923816\n",
      "ep 2777: ep_len:79 episode reward: total was 35.000000. running mean: -16.404578\n",
      "ep 2777: ep_len:648 episode reward: total was 8.330000. running mean: -16.157232\n",
      "ep 2777: ep_len:2826 episode reward: total was -0.130000. running mean: -15.996960\n",
      "epsilon:0.009992 episode_count: 41811. steps_count: 44942053.000000\n",
      "ep 2778: ep_len:700 episode reward: total was -5.190000. running mean: -15.888890\n",
      "ep 2778: ep_len:1232 episode reward: total was -48.450000. running mean: -16.214501\n",
      "ep 2778: ep_len:59 episode reward: total was 25.000000. running mean: -15.802356\n",
      "ep 2778: ep_len:2993 episode reward: total was -61.230000. running mean: -16.256633\n",
      "ep 2778: ep_len:500 episode reward: total was 12.530000. running mean: -15.968766\n",
      "ep 2778: ep_len:158 episode reward: total was 76.000000. running mean: -15.049079\n",
      "ep 2778: ep_len:500 episode reward: total was 60.050000. running mean: -14.298088\n",
      "ep 2778: ep_len:3648 episode reward: total was -98.650000. running mean: -15.141607\n",
      "ep 2778: ep_len:608 episode reward: total was 20.340000. running mean: -14.786791\n",
      "ep 2778: ep_len:668 episode reward: total was 2.520000. running mean: -14.613723\n",
      "ep 2778: ep_len:1061 episode reward: total was 28.110000. running mean: -14.186486\n",
      "ep 2778: ep_len:100 episode reward: total was 45.500000. running mean: -13.589621\n",
      "ep 2778: ep_len:761 episode reward: total was -23.870000. running mean: -13.692425\n",
      "ep 2778: ep_len:2824 episode reward: total was -13.250000. running mean: -13.688000\n",
      "ep 2778: ep_len:63 episode reward: total was 28.500000. running mean: -13.266120\n",
      "epsilon:0.009992 episode_count: 41826. steps_count: 44957928.000000\n",
      "ep 2779: ep_len:656 episode reward: total was 14.130000. running mean: -12.992159\n",
      "ep 2779: ep_len:615 episode reward: total was -1.250000. running mean: -12.874738\n",
      "ep 2779: ep_len:103 episode reward: total was 50.000000. running mean: -12.245990\n",
      "ep 2779: ep_len:500 episode reward: total was 5.090000. running mean: -12.072630\n",
      "ep 2779: ep_len:80 episode reward: total was 38.500000. running mean: -11.566904\n",
      "ep 2779: ep_len:814 episode reward: total was 28.050000. running mean: -11.170735\n",
      "ep 2779: ep_len:3556 episode reward: total was -29.360000. running mean: -11.352628\n",
      "ep 2779: ep_len:1601 episode reward: total was -122.960000. running mean: -12.468701\n",
      "ep 2779: ep_len:753 episode reward: total was 48.280000. running mean: -11.861214\n",
      "ep 2779: ep_len:1047 episode reward: total was 11.960000. running mean: -11.623002\n",
      "ep 2779: ep_len:75 episode reward: total was 34.500000. running mean: -11.161772\n",
      "ep 2779: ep_len:77 episode reward: total was 37.000000. running mean: -10.680155\n",
      "ep 2779: ep_len:784 episode reward: total was -9.500000. running mean: -10.668353\n",
      "ep 2779: ep_len:2761 episode reward: total was -38.060000. running mean: -10.942269\n",
      "epsilon:0.009992 episode_count: 41840. steps_count: 44971350.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2780: ep_len:1451 episode reward: total was -41.940000. running mean: -11.252247\n",
      "ep 2780: ep_len:500 episode reward: total was 19.320000. running mean: -10.946524\n",
      "ep 2780: ep_len:79 episode reward: total was 36.500000. running mean: -10.472059\n",
      "ep 2780: ep_len:2980 episode reward: total was -43.600000. running mean: -10.803338\n",
      "ep 2780: ep_len:800 episode reward: total was 27.080000. running mean: -10.424505\n",
      "ep 2780: ep_len:145 episode reward: total was 71.000000. running mean: -9.610260\n",
      "ep 2780: ep_len:72 episode reward: total was 33.000000. running mean: -9.184157\n",
      "ep 2780: ep_len:66 episode reward: total was 31.500000. running mean: -8.777316\n",
      "ep 2780: ep_len:904 episode reward: total was 54.400000. running mean: -8.145543\n",
      "ep 2780: ep_len:639 episode reward: total was -27.990000. running mean: -8.343987\n",
      "ep 2780: ep_len:505 episode reward: total was -6.230000. running mean: -8.322847\n",
      "ep 2780: ep_len:7309 episode reward: total was -211.440000. running mean: -10.354019\n",
      "ep 2780: ep_len:633 episode reward: total was 16.990000. running mean: -10.080579\n",
      "ep 2780: ep_len:54 episode reward: total was 24.000000. running mean: -9.739773\n",
      "ep 2780: ep_len:1432 episode reward: total was -28.610000. running mean: -9.928475\n",
      "ep 2780: ep_len:2860 episode reward: total was -12.890000. running mean: -9.958090\n",
      "ep 2780: ep_len:54 episode reward: total was 25.500000. running mean: -9.603510\n",
      "epsilon:0.009992 episode_count: 41857. steps_count: 44991833.000000\n",
      "ep 2781: ep_len:1118 episode reward: total was 7.280000. running mean: -9.434674\n",
      "ep 2781: ep_len:1165 episode reward: total was -21.640000. running mean: -9.556728\n",
      "ep 2781: ep_len:2964 episode reward: total was -36.970000. running mean: -9.830860\n",
      "ep 2781: ep_len:1139 episode reward: total was -13.510000. running mean: -9.867652\n",
      "ep 2781: ep_len:53 episode reward: total was 25.000000. running mean: -9.518975\n",
      "ep 2781: ep_len:103 episode reward: total was 50.000000. running mean: -8.923786\n",
      "ep 2781: ep_len:25 episode reward: total was 11.000000. running mean: -8.724548\n",
      "ep 2781: ep_len:1435 episode reward: total was -238.220000. running mean: -11.019502\n",
      "ep 2781: ep_len:3886 episode reward: total was -75.930000. running mean: -11.668607\n",
      "ep 2781: ep_len:736 episode reward: total was -50.910000. running mean: -12.061021\n",
      "ep 2781: ep_len:663 episode reward: total was 7.780000. running mean: -11.862611\n",
      "ep 2781: ep_len:500 episode reward: total was 8.700000. running mean: -11.656985\n",
      "ep 2781: ep_len:1406 episode reward: total was -9.400000. running mean: -11.634415\n",
      "ep 2781: ep_len:2854 episode reward: total was -16.430000. running mean: -11.682371\n",
      "epsilon:0.009992 episode_count: 41871. steps_count: 45009880.000000\n",
      "ep 2782: ep_len:617 episode reward: total was -14.200000. running mean: -11.707547\n",
      "ep 2782: ep_len:1279 episode reward: total was -28.790000. running mean: -11.878372\n",
      "ep 2782: ep_len:2971 episode reward: total was 5.380000. running mean: -11.705788\n",
      "ep 2782: ep_len:758 episode reward: total was 14.540000. running mean: -11.443330\n",
      "ep 2782: ep_len:62 episode reward: total was 29.500000. running mean: -11.033897\n",
      "ep 2782: ep_len:39 episode reward: total was 18.000000. running mean: -10.743558\n",
      "ep 2782: ep_len:711 episode reward: total was -23.360000. running mean: -10.869722\n",
      "ep 2782: ep_len:3533 episode reward: total was -53.340000. running mean: -11.294425\n",
      "ep 2782: ep_len:948 episode reward: total was -19.980000. running mean: -11.381281\n",
      "ep 2782: ep_len:762 episode reward: total was 14.070000. running mean: -11.126768\n",
      "ep 2782: ep_len:944 episode reward: total was 2.080000. running mean: -10.994700\n",
      "ep 2782: ep_len:191 episode reward: total was 92.500000. running mean: -9.959753\n",
      "ep 2782: ep_len:1055 episode reward: total was -4.770000. running mean: -9.907856\n",
      "ep 2782: ep_len:2848 episode reward: total was 8.480000. running mean: -9.723977\n",
      "epsilon:0.009992 episode_count: 41885. steps_count: 45026598.000000\n",
      "ep 2783: ep_len:926 episode reward: total was -149.440000. running mean: -11.121137\n",
      "ep 2783: ep_len:821 episode reward: total was 15.220000. running mean: -10.857726\n",
      "ep 2783: ep_len:2949 episode reward: total was -69.990000. running mean: -11.449049\n",
      "ep 2783: ep_len:512 episode reward: total was 5.320000. running mean: -11.281358\n",
      "ep 2783: ep_len:102 episode reward: total was 48.000000. running mean: -10.688545\n",
      "ep 2783: ep_len:1417 episode reward: total was -144.570000. running mean: -12.027359\n",
      "ep 2783: ep_len:3718 episode reward: total was 0.510000. running mean: -11.901986\n",
      "ep 2783: ep_len:559 episode reward: total was 1.380000. running mean: -11.769166\n",
      "ep 2783: ep_len:7354 episode reward: total was 50.540000. running mean: -11.146074\n",
      "ep 2783: ep_len:636 episode reward: total was -21.090000. running mean: -11.245513\n",
      "ep 2783: ep_len:67 episode reward: total was 32.000000. running mean: -10.813058\n",
      "ep 2783: ep_len:94 episode reward: total was 45.500000. running mean: -10.249928\n",
      "ep 2783: ep_len:729 episode reward: total was -85.800000. running mean: -11.005428\n",
      "ep 2783: ep_len:2635 episode reward: total was -7.240000. running mean: -10.967774\n",
      "epsilon:0.009992 episode_count: 41899. steps_count: 45049117.000000\n",
      "ep 2784: ep_len:946 episode reward: total was -92.970000. running mean: -11.787796\n",
      "ep 2784: ep_len:672 episode reward: total was -20.110000. running mean: -11.871018\n",
      "ep 2784: ep_len:2940 episode reward: total was 8.740000. running mean: -11.664908\n",
      "ep 2784: ep_len:500 episode reward: total was 10.170000. running mean: -11.446559\n",
      "ep 2784: ep_len:122 episode reward: total was 59.500000. running mean: -10.737094\n",
      "ep 2784: ep_len:500 episode reward: total was -9.460000. running mean: -10.724323\n",
      "ep 2784: ep_len:3642 episode reward: total was -24.980000. running mean: -10.866879\n",
      "ep 2784: ep_len:502 episode reward: total was -28.480000. running mean: -11.043011\n",
      "ep 2784: ep_len:750 episode reward: total was 29.600000. running mean: -10.636580\n",
      "ep 2784: ep_len:1169 episode reward: total was 1.940000. running mean: -10.510815\n",
      "ep 2784: ep_len:80 episode reward: total was 38.500000. running mean: -10.020707\n",
      "ep 2784: ep_len:160 episode reward: total was 56.500000. running mean: -9.355499\n",
      "ep 2784: ep_len:792 episode reward: total was 2.470000. running mean: -9.237244\n",
      "ep 2784: ep_len:2857 episode reward: total was -25.890000. running mean: -9.403772\n",
      "ep 2784: ep_len:51 episode reward: total was 22.500000. running mean: -9.084734\n",
      "epsilon:0.009992 episode_count: 41914. steps_count: 45064800.000000\n",
      "ep 2785: ep_len:834 episode reward: total was -0.720000. running mean: -9.001087\n",
      "ep 2785: ep_len:748 episode reward: total was 3.450000. running mean: -8.876576\n",
      "ep 2785: ep_len:2921 episode reward: total was -184.180000. running mean: -10.629610\n",
      "ep 2785: ep_len:648 episode reward: total was 14.720000. running mean: -10.376114\n",
      "ep 2785: ep_len:62 episode reward: total was 28.000000. running mean: -9.992353\n",
      "ep 2785: ep_len:47 episode reward: total was 20.500000. running mean: -9.687430\n",
      "ep 2785: ep_len:865 episode reward: total was 33.210000. running mean: -9.258455\n",
      "ep 2785: ep_len:3834 episode reward: total was -95.090000. running mean: -10.116771\n",
      "ep 2785: ep_len:1272 episode reward: total was -87.440000. running mean: -10.890003\n",
      "ep 2785: ep_len:649 episode reward: total was 23.870000. running mean: -10.542403\n",
      "ep 2785: ep_len:681 episode reward: total was -7.900000. running mean: -10.515979\n",
      "ep 2785: ep_len:1172 episode reward: total was -16.760000. running mean: -10.578419\n",
      "ep 2785: ep_len:2885 episode reward: total was -2.440000. running mean: -10.497035\n",
      "epsilon:0.009992 episode_count: 41927. steps_count: 45081418.000000\n",
      "ep 2786: ep_len:1483 episode reward: total was -51.880000. running mean: -10.910865\n",
      "ep 2786: ep_len:500 episode reward: total was 16.780000. running mean: -10.633956\n",
      "ep 2786: ep_len:71 episode reward: total was 32.500000. running mean: -10.202616\n",
      "ep 2786: ep_len:2969 episode reward: total was -52.060000. running mean: -10.621190\n",
      "ep 2786: ep_len:500 episode reward: total was -87.390000. running mean: -11.388878\n",
      "ep 2786: ep_len:64 episode reward: total was 30.500000. running mean: -10.969990\n",
      "ep 2786: ep_len:722 episode reward: total was -7.580000. running mean: -10.936090\n",
      "ep 2786: ep_len:637 episode reward: total was -15.090000. running mean: -10.977629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2786: ep_len:797 episode reward: total was -60.100000. running mean: -11.468852\n",
      "ep 2786: ep_len:676 episode reward: total was 19.820000. running mean: -11.155964\n",
      "ep 2786: ep_len:500 episode reward: total was 4.040000. running mean: -11.004004\n",
      "ep 2786: ep_len:791 episode reward: total was 2.690000. running mean: -10.867064\n",
      "ep 2786: ep_len:2859 episode reward: total was -22.780000. running mean: -10.986194\n",
      "epsilon:0.009992 episode_count: 41940. steps_count: 45093987.000000\n",
      "ep 2787: ep_len:821 episode reward: total was -26.700000. running mean: -11.143332\n",
      "ep 2787: ep_len:670 episode reward: total was -25.270000. running mean: -11.284598\n",
      "ep 2787: ep_len:3035 episode reward: total was -6.720000. running mean: -11.238952\n",
      "ep 2787: ep_len:1193 episode reward: total was -19.550000. running mean: -11.322063\n",
      "ep 2787: ep_len:568 episode reward: total was 25.190000. running mean: -10.956942\n",
      "ep 2787: ep_len:3720 episode reward: total was -41.860000. running mean: -11.265973\n",
      "ep 2787: ep_len:895 episode reward: total was -71.500000. running mean: -11.868313\n",
      "ep 2787: ep_len:658 episode reward: total was 17.530000. running mean: -11.574330\n",
      "ep 2787: ep_len:500 episode reward: total was 17.980000. running mean: -11.278787\n",
      "ep 2787: ep_len:1478 episode reward: total was -14.710000. running mean: -11.313099\n",
      "ep 2787: ep_len:2772 episode reward: total was -6.180000. running mean: -11.261768\n",
      "ep 2787: ep_len:41 episode reward: total was 17.500000. running mean: -10.974150\n",
      "epsilon:0.009992 episode_count: 41952. steps_count: 45110338.000000\n",
      "ep 2788: ep_len:883 episode reward: total was 11.080000. running mean: -10.753609\n",
      "ep 2788: ep_len:788 episode reward: total was -12.490000. running mean: -10.770973\n",
      "ep 2788: ep_len:56 episode reward: total was 22.000000. running mean: -10.443263\n",
      "ep 2788: ep_len:2895 episode reward: total was -135.270000. running mean: -11.691530\n",
      "ep 2788: ep_len:1203 episode reward: total was -40.660000. running mean: -11.981215\n",
      "ep 2788: ep_len:93 episode reward: total was 42.000000. running mean: -11.441403\n",
      "ep 2788: ep_len:92 episode reward: total was 44.500000. running mean: -10.881989\n",
      "ep 2788: ep_len:786 episode reward: total was -8.990000. running mean: -10.863069\n",
      "ep 2788: ep_len:3787 episode reward: total was -42.030000. running mean: -11.174738\n",
      "ep 2788: ep_len:658 episode reward: total was -35.000000. running mean: -11.412991\n",
      "ep 2788: ep_len:633 episode reward: total was 1.020000. running mean: -11.288661\n",
      "ep 2788: ep_len:797 episode reward: total was 14.610000. running mean: -11.029674\n",
      "ep 2788: ep_len:935 episode reward: total was -53.440000. running mean: -11.453777\n",
      "ep 2788: ep_len:38 episode reward: total was 16.000000. running mean: -11.179240\n",
      "ep 2788: ep_len:74 episode reward: total was 34.000000. running mean: -10.727447\n",
      "epsilon:0.009992 episode_count: 41967. steps_count: 45124056.000000\n",
      "ep 2789: ep_len:783 episode reward: total was 9.020000. running mean: -10.529973\n",
      "ep 2789: ep_len:699 episode reward: total was -44.400000. running mean: -10.868673\n",
      "ep 2789: ep_len:3023 episode reward: total was -29.040000. running mean: -11.050386\n",
      "ep 2789: ep_len:629 episode reward: total was 2.010000. running mean: -10.919783\n",
      "ep 2789: ep_len:956 episode reward: total was -16.410000. running mean: -10.974685\n",
      "ep 2789: ep_len:3685 episode reward: total was -3.800000. running mean: -10.902938\n",
      "ep 2789: ep_len:1140 episode reward: total was -24.700000. running mean: -11.040908\n",
      "ep 2789: ep_len:876 episode reward: total was 46.950000. running mean: -10.460999\n",
      "ep 2789: ep_len:500 episode reward: total was 14.330000. running mean: -10.213089\n",
      "ep 2789: ep_len:131 episode reward: total was 62.500000. running mean: -9.485958\n",
      "ep 2789: ep_len:53 episode reward: total was 23.500000. running mean: -9.156099\n",
      "ep 2789: ep_len:500 episode reward: total was 25.050000. running mean: -8.814038\n",
      "ep 2789: ep_len:2839 episode reward: total was 2.330000. running mean: -8.702598\n",
      "ep 2789: ep_len:45 episode reward: total was 19.500000. running mean: -8.420572\n",
      "epsilon:0.009992 episode_count: 41981. steps_count: 45139915.000000\n",
      "ep 2790: ep_len:1341 episode reward: total was -28.020000. running mean: -8.616566\n",
      "ep 2790: ep_len:625 episode reward: total was -21.190000. running mean: -8.742300\n",
      "ep 2790: ep_len:2988 episode reward: total was -24.510000. running mean: -8.899977\n",
      "ep 2790: ep_len:825 episode reward: total was 3.820000. running mean: -8.772777\n",
      "ep 2790: ep_len:44 episode reward: total was 19.000000. running mean: -8.495050\n",
      "ep 2790: ep_len:500 episode reward: total was 7.840000. running mean: -8.331699\n",
      "ep 2790: ep_len:4177 episode reward: total was -37.730000. running mean: -8.625682\n",
      "ep 2790: ep_len:567 episode reward: total was -8.640000. running mean: -8.625825\n",
      "ep 2790: ep_len:800 episode reward: total was 38.100000. running mean: -8.158567\n",
      "ep 2790: ep_len:990 episode reward: total was 27.010000. running mean: -7.806881\n",
      "ep 2790: ep_len:49 episode reward: total was 23.000000. running mean: -7.498813\n",
      "ep 2790: ep_len:619 episode reward: total was -32.850000. running mean: -7.752324\n",
      "ep 2790: ep_len:2891 episode reward: total was -7.560000. running mean: -7.750401\n",
      "epsilon:0.009992 episode_count: 41994. steps_count: 45156331.000000\n",
      "ep 2791: ep_len:890 episode reward: total was 24.890000. running mean: -7.423997\n",
      "ep 2791: ep_len:1000 episode reward: total was 19.720000. running mean: -7.152557\n",
      "ep 2791: ep_len:58 episode reward: total was 27.500000. running mean: -6.806032\n",
      "ep 2791: ep_len:3040 episode reward: total was -59.950000. running mean: -7.337471\n",
      "ep 2791: ep_len:1230 episode reward: total was -39.380000. running mean: -7.657897\n",
      "ep 2791: ep_len:107 episode reward: total was 44.500000. running mean: -7.136318\n",
      "ep 2791: ep_len:75 episode reward: total was 36.000000. running mean: -6.704954\n",
      "ep 2791: ep_len:695 episode reward: total was -1.640000. running mean: -6.654305\n",
      "ep 2791: ep_len:361 episode reward: total was -15.090000. running mean: -6.738662\n",
      "ep 2791: ep_len:957 episode reward: total was -50.030000. running mean: -7.171575\n",
      "ep 2791: ep_len:737 episode reward: total was -3.310000. running mean: -7.132960\n",
      "ep 2791: ep_len:640 episode reward: total was 0.170000. running mean: -7.059930\n",
      "ep 2791: ep_len:84 episode reward: total was 40.500000. running mean: -6.584331\n",
      "ep 2791: ep_len:500 episode reward: total was -8.940000. running mean: -6.607887\n",
      "ep 2791: ep_len:2773 episode reward: total was 9.480000. running mean: -6.447008\n",
      "epsilon:0.009992 episode_count: 42009. steps_count: 45169478.000000\n",
      "ep 2792: ep_len:1026 episode reward: total was -67.680000. running mean: -7.059338\n",
      "ep 2792: ep_len:769 episode reward: total was -4.600000. running mean: -7.034745\n",
      "ep 2792: ep_len:66 episode reward: total was 31.500000. running mean: -6.649398\n",
      "ep 2792: ep_len:3068 episode reward: total was -30.510000. running mean: -6.888004\n",
      "ep 2792: ep_len:1434 episode reward: total was -33.790000. running mean: -7.157024\n",
      "ep 2792: ep_len:73 episode reward: total was 35.000000. running mean: -6.735453\n",
      "ep 2792: ep_len:1429 episode reward: total was -218.880000. running mean: -8.856899\n",
      "ep 2792: ep_len:624 episode reward: total was -7.420000. running mean: -8.842530\n",
      "ep 2792: ep_len:664 episode reward: total was -29.890000. running mean: -9.053004\n",
      "ep 2792: ep_len:759 episode reward: total was -3.140000. running mean: -8.993874\n",
      "ep 2792: ep_len:738 episode reward: total was 24.060000. running mean: -8.663336\n",
      "ep 2792: ep_len:173 episode reward: total was 85.000000. running mean: -7.726702\n",
      "ep 2792: ep_len:1156 episode reward: total was 16.780000. running mean: -7.481635\n",
      "ep 2792: ep_len:2825 episode reward: total was 6.050000. running mean: -7.346319\n",
      "ep 2792: ep_len:63 episode reward: total was 28.500000. running mean: -6.987856\n",
      "epsilon:0.009992 episode_count: 42024. steps_count: 45184345.000000\n",
      "ep 2793: ep_len:729 episode reward: total was -7.130000. running mean: -6.989277\n",
      "ep 2793: ep_len:977 episode reward: total was 4.400000. running mean: -6.875384\n",
      "ep 2793: ep_len:3035 episode reward: total was -61.820000. running mean: -7.424831\n",
      "ep 2793: ep_len:859 episode reward: total was 66.990000. running mean: -6.680682\n",
      "ep 2793: ep_len:50 episode reward: total was 17.500000. running mean: -6.438875\n",
      "ep 2793: ep_len:849 episode reward: total was -23.940000. running mean: -6.613887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2793: ep_len:3699 episode reward: total was -73.620000. running mean: -7.283948\n",
      "ep 2793: ep_len:4149 episode reward: total was -707.460000. running mean: -14.285708\n",
      "ep 2793: ep_len:821 episode reward: total was 31.530000. running mean: -13.827551\n",
      "ep 2793: ep_len:1018 episode reward: total was -0.940000. running mean: -13.698676\n",
      "ep 2793: ep_len:70 episode reward: total was 32.000000. running mean: -13.241689\n",
      "ep 2793: ep_len:1138 episode reward: total was -11.010000. running mean: -13.219372\n",
      "ep 2793: ep_len:2880 episode reward: total was -13.880000. running mean: -13.225978\n",
      "ep 2793: ep_len:42 episode reward: total was 19.500000. running mean: -12.898719\n",
      "epsilon:0.009992 episode_count: 42038. steps_count: 45204661.000000\n",
      "ep 2794: ep_len:1437 episode reward: total was 7.190000. running mean: -12.697831\n",
      "ep 2794: ep_len:1659 episode reward: total was -77.270000. running mean: -13.343553\n",
      "ep 2794: ep_len:3073 episode reward: total was -21.850000. running mean: -13.428618\n",
      "ep 2794: ep_len:1666 episode reward: total was -74.720000. running mean: -14.041531\n",
      "ep 2794: ep_len:1425 episode reward: total was -174.570000. running mean: -15.646816\n",
      "ep 2794: ep_len:3725 episode reward: total was -26.670000. running mean: -15.757048\n",
      "ep 2794: ep_len:549 episode reward: total was -14.760000. running mean: -15.747077\n",
      "ep 2794: ep_len:679 episode reward: total was -12.530000. running mean: -15.714907\n",
      "ep 2794: ep_len:687 episode reward: total was 26.640000. running mean: -15.291358\n",
      "ep 2794: ep_len:159 episode reward: total was 75.000000. running mean: -14.388444\n",
      "ep 2794: ep_len:44 episode reward: total was 20.500000. running mean: -14.039560\n",
      "ep 2794: ep_len:997 episode reward: total was -21.200000. running mean: -14.111164\n",
      "ep 2794: ep_len:30 episode reward: total was 13.500000. running mean: -13.835052\n",
      "epsilon:0.009992 episode_count: 42051. steps_count: 45220791.000000\n",
      "ep 2795: ep_len:1023 episode reward: total was -112.150000. running mean: -14.818202\n",
      "ep 2795: ep_len:500 episode reward: total was 9.340000. running mean: -14.576620\n",
      "ep 2795: ep_len:3007 episode reward: total was -58.880000. running mean: -15.019654\n",
      "ep 2795: ep_len:507 episode reward: total was -13.950000. running mean: -15.008957\n",
      "ep 2795: ep_len:134 episode reward: total was 64.000000. running mean: -14.218868\n",
      "ep 2795: ep_len:109 episode reward: total was 53.000000. running mean: -13.546679\n",
      "ep 2795: ep_len:61 episode reward: total was 27.500000. running mean: -13.136212\n",
      "ep 2795: ep_len:1457 episode reward: total was -115.020000. running mean: -14.155050\n",
      "ep 2795: ep_len:341 episode reward: total was -3.660000. running mean: -14.050099\n",
      "ep 2795: ep_len:658 episode reward: total was -49.570000. running mean: -14.405298\n",
      "ep 2795: ep_len:807 episode reward: total was 51.790000. running mean: -13.743345\n",
      "ep 2795: ep_len:708 episode reward: total was -0.800000. running mean: -13.613912\n",
      "ep 2795: ep_len:44 episode reward: total was 20.500000. running mean: -13.272773\n",
      "ep 2795: ep_len:677 episode reward: total was 38.810000. running mean: -12.751945\n",
      "ep 2795: ep_len:2819 episode reward: total was -50.700000. running mean: -13.131426\n",
      "epsilon:0.009992 episode_count: 42066. steps_count: 45233643.000000\n",
      "ep 2796: ep_len:1100 episode reward: total was -0.190000. running mean: -13.002011\n",
      "ep 2796: ep_len:756 episode reward: total was -4.980000. running mean: -12.921791\n",
      "ep 2796: ep_len:2934 episode reward: total was -54.410000. running mean: -13.336673\n",
      "ep 2796: ep_len:1723 episode reward: total was -62.400000. running mean: -13.827307\n",
      "ep 2796: ep_len:36 episode reward: total was 16.500000. running mean: -13.524034\n",
      "ep 2796: ep_len:153 episode reward: total was 70.500000. running mean: -12.683793\n",
      "ep 2796: ep_len:87 episode reward: total was 40.500000. running mean: -12.151955\n",
      "ep 2796: ep_len:50 episode reward: total was 22.000000. running mean: -11.810436\n",
      "ep 2796: ep_len:1509 episode reward: total was -24.080000. running mean: -11.933131\n",
      "ep 2796: ep_len:620 episode reward: total was 8.790000. running mean: -11.725900\n",
      "ep 2796: ep_len:898 episode reward: total was -48.120000. running mean: -12.089841\n",
      "ep 2796: ep_len:667 episode reward: total was 29.200000. running mean: -11.676943\n",
      "ep 2796: ep_len:899 episode reward: total was 12.100000. running mean: -11.439173\n",
      "ep 2796: ep_len:32 episode reward: total was 14.500000. running mean: -11.179782\n",
      "ep 2796: ep_len:586 episode reward: total was -14.940000. running mean: -11.217384\n",
      "ep 2796: ep_len:2844 episode reward: total was 8.410000. running mean: -11.021110\n",
      "epsilon:0.009992 episode_count: 42082. steps_count: 45248537.000000\n",
      "ep 2797: ep_len:1011 episode reward: total was -46.620000. running mean: -11.377099\n",
      "ep 2797: ep_len:1619 episode reward: total was -62.270000. running mean: -11.886028\n",
      "ep 2797: ep_len:42 episode reward: total was 18.000000. running mean: -11.587168\n",
      "ep 2797: ep_len:102 episode reward: total was 48.000000. running mean: -10.991296\n",
      "ep 2797: ep_len:843 episode reward: total was 18.420000. running mean: -10.697183\n",
      "ep 2797: ep_len:1149 episode reward: total was -17.970000. running mean: -10.769911\n",
      "ep 2797: ep_len:3628 episode reward: total was -124.100000. running mean: -11.903212\n",
      "ep 2797: ep_len:543 episode reward: total was 2.750000. running mean: -11.756680\n",
      "ep 2797: ep_len:693 episode reward: total was 5.240000. running mean: -11.586713\n",
      "ep 2797: ep_len:1134 episode reward: total was -7.010000. running mean: -11.540946\n",
      "ep 2797: ep_len:71 episode reward: total was 32.500000. running mean: -11.100536\n",
      "ep 2797: ep_len:125 episode reward: total was 61.000000. running mean: -10.379531\n",
      "ep 2797: ep_len:81 episode reward: total was 37.500000. running mean: -9.900736\n",
      "ep 2797: ep_len:622 episode reward: total was -23.710000. running mean: -10.038828\n",
      "ep 2797: ep_len:2926 episode reward: total was -3.010000. running mean: -9.968540\n",
      "epsilon:0.009992 episode_count: 42097. steps_count: 45263126.000000\n",
      "ep 2798: ep_len:1501 episode reward: total was 24.580000. running mean: -9.623055\n",
      "ep 2798: ep_len:1256 episode reward: total was -24.980000. running mean: -9.776624\n",
      "ep 2798: ep_len:2945 episode reward: total was -19.100000. running mean: -9.869858\n",
      "ep 2798: ep_len:500 episode reward: total was -12.110000. running mean: -9.892259\n",
      "ep 2798: ep_len:43 episode reward: total was 20.000000. running mean: -9.593337\n",
      "ep 2798: ep_len:1362 episode reward: total was -137.930000. running mean: -10.876703\n",
      "ep 2798: ep_len:625 episode reward: total was 7.430000. running mean: -10.693636\n",
      "ep 2798: ep_len:863 episode reward: total was 10.480000. running mean: -10.481900\n",
      "ep 2798: ep_len:717 episode reward: total was 30.740000. running mean: -10.069681\n",
      "ep 2798: ep_len:500 episode reward: total was 23.850000. running mean: -9.730484\n",
      "ep 2798: ep_len:44 episode reward: total was 19.000000. running mean: -9.443179\n",
      "ep 2798: ep_len:610 episode reward: total was 10.130000. running mean: -9.247448\n",
      "ep 2798: ep_len:2761 episode reward: total was 4.430000. running mean: -9.110673\n",
      "ep 2798: ep_len:55 episode reward: total was 26.000000. running mean: -8.759566\n",
      "epsilon:0.009992 episode_count: 42111. steps_count: 45276908.000000\n",
      "ep 2799: ep_len:1113 episode reward: total was -7.220000. running mean: -8.744171\n",
      "ep 2799: ep_len:764 episode reward: total was -16.150000. running mean: -8.818229\n",
      "ep 2799: ep_len:51 episode reward: total was 22.500000. running mean: -8.505047\n",
      "ep 2799: ep_len:3038 episode reward: total was -10.750000. running mean: -8.527496\n",
      "ep 2799: ep_len:857 episode reward: total was 32.800000. running mean: -8.114221\n",
      "ep 2799: ep_len:43 episode reward: total was 20.000000. running mean: -7.833079\n",
      "ep 2799: ep_len:57 episode reward: total was 27.000000. running mean: -7.484748\n",
      "ep 2799: ep_len:815 episode reward: total was 29.310000. running mean: -7.116801\n",
      "ep 2799: ep_len:3676 episode reward: total was -1378.910000. running mean: -20.834733\n",
      "ep 2799: ep_len:1551 episode reward: total was -17.410000. running mean: -20.800485\n",
      "ep 2799: ep_len:662 episode reward: total was 26.970000. running mean: -20.322781\n",
      "ep 2799: ep_len:507 episode reward: total was 6.470000. running mean: -20.054853\n",
      "ep 2799: ep_len:25 episode reward: total was 11.000000. running mean: -19.744304\n",
      "ep 2799: ep_len:672 episode reward: total was 0.490000. running mean: -19.541961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2799: ep_len:2811 episode reward: total was 10.590000. running mean: -19.240642\n",
      "epsilon:0.009992 episode_count: 42126. steps_count: 45293550.000000\n",
      "ep 2800: ep_len:1135 episode reward: total was 26.650000. running mean: -18.781735\n",
      "ep 2800: ep_len:685 episode reward: total was -12.880000. running mean: -18.722718\n",
      "ep 2800: ep_len:2894 episode reward: total was -48.720000. running mean: -19.022691\n",
      "ep 2800: ep_len:1474 episode reward: total was -3.120000. running mean: -18.863664\n",
      "ep 2800: ep_len:37 episode reward: total was 17.000000. running mean: -18.505027\n",
      "ep 2800: ep_len:39 episode reward: total was 16.500000. running mean: -18.154977\n",
      "ep 2800: ep_len:610 episode reward: total was -76.140000. running mean: -18.734827\n",
      "ep 2800: ep_len:3730 episode reward: total was -99.600000. running mean: -19.543479\n",
      "ep 2800: ep_len:566 episode reward: total was -13.060000. running mean: -19.478644\n",
      "ep 2800: ep_len:823 episode reward: total was 10.590000. running mean: -19.177958\n",
      "ep 2800: ep_len:680 episode reward: total was -15.040000. running mean: -19.136578\n",
      "ep 2800: ep_len:133 episode reward: total was 65.000000. running mean: -18.295212\n",
      "ep 2800: ep_len:54 episode reward: total was 24.000000. running mean: -17.872260\n",
      "ep 2800: ep_len:1105 episode reward: total was -10.330000. running mean: -17.796837\n",
      "ep 2800: ep_len:2860 episode reward: total was -37.070000. running mean: -17.989569\n",
      "ep 2800: ep_len:43 episode reward: total was 20.000000. running mean: -17.609673\n",
      "epsilon:0.009992 episode_count: 42142. steps_count: 45310418.000000\n",
      "ep 2801: ep_len:880 episode reward: total was -10.840000. running mean: -17.541977\n",
      "ep 2801: ep_len:842 episode reward: total was 15.830000. running mean: -17.208257\n",
      "ep 2801: ep_len:3006 episode reward: total was -16.400000. running mean: -17.200174\n",
      "ep 2801: ep_len:1169 episode reward: total was -128.840000. running mean: -18.316573\n",
      "ep 2801: ep_len:1458 episode reward: total was -1.260000. running mean: -18.146007\n",
      "ep 2801: ep_len:603 episode reward: total was 20.010000. running mean: -17.764447\n",
      "ep 2801: ep_len:630 episode reward: total was 1.080000. running mean: -17.576002\n",
      "ep 2801: ep_len:7332 episode reward: total was -119.120000. running mean: -18.591442\n",
      "ep 2801: ep_len:1483 episode reward: total was -9.950000. running mean: -18.505028\n",
      "ep 2801: ep_len:166 episode reward: total was 75.500000. running mean: -17.564978\n",
      "ep 2801: ep_len:36 episode reward: total was 16.500000. running mean: -17.224328\n",
      "ep 2801: ep_len:500 episode reward: total was 48.810000. running mean: -16.563985\n",
      "ep 2801: ep_len:2888 episode reward: total was -10.830000. running mean: -16.506645\n",
      "epsilon:0.009992 episode_count: 42155. steps_count: 45331411.000000\n",
      "ep 2802: ep_len:632 episode reward: total was 12.970000. running mean: -16.211878\n",
      "ep 2802: ep_len:650 episode reward: total was -14.880000. running mean: -16.198559\n",
      "ep 2802: ep_len:44 episode reward: total was 20.500000. running mean: -15.831574\n",
      "ep 2802: ep_len:2847 episode reward: total was -67.250000. running mean: -16.345758\n",
      "ep 2802: ep_len:732 episode reward: total was -27.190000. running mean: -16.454201\n",
      "ep 2802: ep_len:73 episode reward: total was 35.000000. running mean: -15.939659\n",
      "ep 2802: ep_len:1868 episode reward: total was -108.040000. running mean: -16.860662\n",
      "ep 2802: ep_len:647 episode reward: total was 22.470000. running mean: -16.467355\n",
      "ep 2802: ep_len:788 episode reward: total was -36.650000. running mean: -16.669182\n",
      "ep 2802: ep_len:7553 episode reward: total was -27.840000. running mean: -16.780890\n",
      "ep 2802: ep_len:578 episode reward: total was 25.400000. running mean: -16.359081\n",
      "ep 2802: ep_len:684 episode reward: total was -2.420000. running mean: -16.219690\n",
      "ep 2802: ep_len:2829 episode reward: total was -2.940000. running mean: -16.086893\n",
      "ep 2802: ep_len:40 episode reward: total was 18.500000. running mean: -15.741024\n",
      "epsilon:0.009992 episode_count: 42169. steps_count: 45351376.000000\n",
      "ep 2803: ep_len:1494 episode reward: total was -25.840000. running mean: -15.842014\n",
      "ep 2803: ep_len:713 episode reward: total was -3.440000. running mean: -15.717994\n",
      "ep 2803: ep_len:65 episode reward: total was 31.000000. running mean: -15.250814\n",
      "ep 2803: ep_len:3023 episode reward: total was -28.690000. running mean: -15.385206\n",
      "ep 2803: ep_len:644 episode reward: total was 5.260000. running mean: -15.178754\n",
      "ep 2803: ep_len:69 episode reward: total was 33.000000. running mean: -14.696966\n",
      "ep 2803: ep_len:793 episode reward: total was 21.040000. running mean: -14.339597\n",
      "ep 2803: ep_len:651 episode reward: total was 6.930000. running mean: -14.126901\n",
      "ep 2803: ep_len:881 episode reward: total was 14.550000. running mean: -13.840132\n",
      "ep 2803: ep_len:822 episode reward: total was 27.690000. running mean: -13.424830\n",
      "ep 2803: ep_len:1186 episode reward: total was -13.560000. running mean: -13.426182\n",
      "ep 2803: ep_len:72 episode reward: total was 33.000000. running mean: -12.961920\n",
      "ep 2803: ep_len:53 episode reward: total was 25.000000. running mean: -12.582301\n",
      "ep 2803: ep_len:91 episode reward: total was 42.500000. running mean: -12.031478\n",
      "ep 2803: ep_len:1128 episode reward: total was -17.170000. running mean: -12.082863\n",
      "ep 2803: ep_len:2860 episode reward: total was 2.880000. running mean: -11.933235\n",
      "epsilon:0.009992 episode_count: 42185. steps_count: 45365921.000000\n",
      "ep 2804: ep_len:1491 episode reward: total was -4.420000. running mean: -11.858102\n",
      "ep 2804: ep_len:1115 episode reward: total was -25.730000. running mean: -11.996821\n",
      "ep 2804: ep_len:3030 episode reward: total was -0.890000. running mean: -11.885753\n",
      "ep 2804: ep_len:1245 episode reward: total was -55.360000. running mean: -12.320496\n",
      "ep 2804: ep_len:74 episode reward: total was 34.000000. running mean: -11.857291\n",
      "ep 2804: ep_len:1382 episode reward: total was -80.280000. running mean: -12.541518\n",
      "ep 2804: ep_len:602 episode reward: total was 8.890000. running mean: -12.327203\n",
      "ep 2804: ep_len:758 episode reward: total was -25.560000. running mean: -12.459530\n",
      "ep 2804: ep_len:657 episode reward: total was 31.420000. running mean: -12.020735\n",
      "ep 2804: ep_len:985 episode reward: total was 13.880000. running mean: -11.761728\n",
      "ep 2804: ep_len:204 episode reward: total was 97.500000. running mean: -10.669111\n",
      "ep 2804: ep_len:117 episode reward: total was 55.500000. running mean: -10.007419\n",
      "ep 2804: ep_len:630 episode reward: total was -0.800000. running mean: -9.915345\n",
      "ep 2804: ep_len:2841 episode reward: total was 3.030000. running mean: -9.785892\n",
      "epsilon:0.009992 episode_count: 42199. steps_count: 45381052.000000\n",
      "ep 2805: ep_len:726 episode reward: total was 4.620000. running mean: -9.641833\n",
      "ep 2805: ep_len:1259 episode reward: total was -42.120000. running mean: -9.966615\n",
      "ep 2805: ep_len:73 episode reward: total was 32.000000. running mean: -9.546948\n",
      "ep 2805: ep_len:2949 episode reward: total was -59.340000. running mean: -10.044879\n",
      "ep 2805: ep_len:673 episode reward: total was 22.670000. running mean: -9.717730\n",
      "ep 2805: ep_len:38 episode reward: total was 16.000000. running mean: -9.460553\n",
      "ep 2805: ep_len:128 episode reward: total was 59.500000. running mean: -8.770947\n",
      "ep 2805: ep_len:76 episode reward: total was 36.500000. running mean: -8.318238\n",
      "ep 2805: ep_len:1897 episode reward: total was -32.000000. running mean: -8.555055\n",
      "ep 2805: ep_len:367 episode reward: total was 11.230000. running mean: -8.357205\n",
      "ep 2805: ep_len:639 episode reward: total was 2.180000. running mean: -8.251833\n",
      "ep 2805: ep_len:638 episode reward: total was 31.940000. running mean: -7.849915\n",
      "ep 2805: ep_len:1154 episode reward: total was -8.310000. running mean: -7.854515\n",
      "ep 2805: ep_len:66 episode reward: total was 31.500000. running mean: -7.460970\n",
      "ep 2805: ep_len:192 episode reward: total was 93.000000. running mean: -6.456361\n",
      "ep 2805: ep_len:39 episode reward: total was 18.000000. running mean: -6.211797\n",
      "ep 2805: ep_len:77 episode reward: total was 35.500000. running mean: -5.794679\n",
      "ep 2805: ep_len:1409 episode reward: total was 5.260000. running mean: -5.684132\n",
      "ep 2805: ep_len:2771 episode reward: total was 8.480000. running mean: -5.542491\n",
      "ep 2805: ep_len:69 episode reward: total was 33.000000. running mean: -5.157066\n",
      "epsilon:0.009992 episode_count: 42219. steps_count: 45396292.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2806: ep_len:1089 episode reward: total was -1.370000. running mean: -5.119195\n",
      "ep 2806: ep_len:963 episode reward: total was 22.690000. running mean: -4.841103\n",
      "ep 2806: ep_len:50 episode reward: total was 22.000000. running mean: -4.572692\n",
      "ep 2806: ep_len:3032 episode reward: total was 4.890000. running mean: -4.478065\n",
      "ep 2806: ep_len:1703 episode reward: total was -61.890000. running mean: -5.052185\n",
      "ep 2806: ep_len:51 episode reward: total was 22.500000. running mean: -4.776663\n",
      "ep 2806: ep_len:63 episode reward: total was 30.000000. running mean: -4.428896\n",
      "ep 2806: ep_len:1430 episode reward: total was -138.290000. running mean: -5.767507\n",
      "ep 2806: ep_len:342 episode reward: total was 5.930000. running mean: -5.650532\n",
      "ep 2806: ep_len:646 episode reward: total was -34.110000. running mean: -5.935127\n",
      "ep 2806: ep_len:856 episode reward: total was 42.360000. running mean: -5.452176\n",
      "ep 2806: ep_len:947 episode reward: total was 8.720000. running mean: -5.310454\n",
      "ep 2806: ep_len:150 episode reward: total was 72.000000. running mean: -4.537349\n",
      "ep 2806: ep_len:68 episode reward: total was 32.500000. running mean: -4.166976\n",
      "ep 2806: ep_len:780 episode reward: total was -11.550000. running mean: -4.240806\n",
      "ep 2806: ep_len:2799 episode reward: total was 2.700000. running mean: -4.171398\n",
      "epsilon:0.009992 episode_count: 42235. steps_count: 45411261.000000\n",
      "ep 2807: ep_len:914 episode reward: total was -49.830000. running mean: -4.627984\n",
      "ep 2807: ep_len:770 episode reward: total was -5.130000. running mean: -4.633004\n",
      "ep 2807: ep_len:3069 episode reward: total was -107.830000. running mean: -5.664974\n",
      "ep 2807: ep_len:629 episode reward: total was 8.470000. running mean: -5.523624\n",
      "ep 2807: ep_len:82 episode reward: total was 38.000000. running mean: -5.088388\n",
      "ep 2807: ep_len:46 episode reward: total was 21.500000. running mean: -4.822504\n",
      "ep 2807: ep_len:500 episode reward: total was 24.830000. running mean: -4.525979\n",
      "ep 2807: ep_len:335 episode reward: total was 23.640000. running mean: -4.244319\n",
      "ep 2807: ep_len:1645 episode reward: total was -54.210000. running mean: -4.743976\n",
      "ep 2807: ep_len:779 episode reward: total was -83.580000. running mean: -5.532336\n",
      "ep 2807: ep_len:648 episode reward: total was -9.890000. running mean: -5.575913\n",
      "ep 2807: ep_len:131 episode reward: total was 62.500000. running mean: -4.895154\n",
      "ep 2807: ep_len:47 episode reward: total was 19.000000. running mean: -4.656202\n",
      "ep 2807: ep_len:70 episode reward: total was 32.000000. running mean: -4.289640\n",
      "ep 2807: ep_len:990 episode reward: total was -17.540000. running mean: -4.422144\n",
      "ep 2807: ep_len:2911 episode reward: total was -30.370000. running mean: -4.681623\n",
      "epsilon:0.009992 episode_count: 42251. steps_count: 45424827.000000\n",
      "ep 2808: ep_len:712 episode reward: total was -14.350000. running mean: -4.778306\n",
      "ep 2808: ep_len:699 episode reward: total was -22.670000. running mean: -4.957223\n",
      "ep 2808: ep_len:3018 episode reward: total was -55.410000. running mean: -5.461751\n",
      "ep 2808: ep_len:705 episode reward: total was 0.650000. running mean: -5.400634\n",
      "ep 2808: ep_len:91 episode reward: total was 41.000000. running mean: -4.936627\n",
      "ep 2808: ep_len:1350 episode reward: total was -102.330000. running mean: -5.910561\n",
      "ep 2808: ep_len:339 episode reward: total was 23.070000. running mean: -5.620755\n",
      "ep 2808: ep_len:712 episode reward: total was -10.900000. running mean: -5.673548\n",
      "ep 2808: ep_len:7384 episode reward: total was -32.080000. running mean: -5.937612\n",
      "ep 2808: ep_len:604 episode reward: total was -6.250000. running mean: -5.940736\n",
      "ep 2808: ep_len:56 episode reward: total was 26.500000. running mean: -5.616329\n",
      "ep 2808: ep_len:186 episode reward: total was 91.500000. running mean: -4.645166\n",
      "ep 2808: ep_len:1440 episode reward: total was -3.710000. running mean: -4.635814\n",
      "ep 2808: ep_len:2756 episode reward: total was -0.950000. running mean: -4.598956\n",
      "epsilon:0.009992 episode_count: 42265. steps_count: 45444879.000000\n",
      "ep 2809: ep_len:965 episode reward: total was -44.240000. running mean: -4.995366\n",
      "ep 2809: ep_len:1589 episode reward: total was -83.020000. running mean: -5.775613\n",
      "ep 2809: ep_len:31 episode reward: total was 14.000000. running mean: -5.577856\n",
      "ep 2809: ep_len:2927 episode reward: total was -8.900000. running mean: -5.611078\n",
      "ep 2809: ep_len:4489 episode reward: total was -715.700000. running mean: -12.711967\n",
      "ep 2809: ep_len:62 episode reward: total was 29.500000. running mean: -12.289847\n",
      "ep 2809: ep_len:839 episode reward: total was 38.770000. running mean: -11.779249\n",
      "ep 2809: ep_len:348 episode reward: total was 12.260000. running mean: -11.538856\n",
      "ep 2809: ep_len:794 episode reward: total was -10.080000. running mean: -11.524268\n",
      "ep 2809: ep_len:883 episode reward: total was 50.380000. running mean: -10.905225\n",
      "ep 2809: ep_len:649 episode reward: total was 10.690000. running mean: -10.689273\n",
      "ep 2809: ep_len:133 episode reward: total was 64.510000. running mean: -9.937280\n",
      "ep 2809: ep_len:791 episode reward: total was 2.430000. running mean: -9.813607\n",
      "ep 2809: ep_len:2889 episode reward: total was -21.630000. running mean: -9.931771\n",
      "epsilon:0.009992 episode_count: 42279. steps_count: 45462268.000000\n",
      "ep 2810: ep_len:635 episode reward: total was 23.870000. running mean: -9.593754\n",
      "ep 2810: ep_len:979 episode reward: total was 9.410000. running mean: -9.403716\n",
      "ep 2810: ep_len:2927 episode reward: total was -28.560000. running mean: -9.595279\n",
      "ep 2810: ep_len:774 episode reward: total was 22.350000. running mean: -9.275826\n",
      "ep 2810: ep_len:55 episode reward: total was 26.000000. running mean: -8.923068\n",
      "ep 2810: ep_len:116 episode reward: total was 55.000000. running mean: -8.283837\n",
      "ep 2810: ep_len:99 episode reward: total was 48.000000. running mean: -7.720999\n",
      "ep 2810: ep_len:47 episode reward: total was 22.000000. running mean: -7.423789\n",
      "ep 2810: ep_len:1084 episode reward: total was -13.570000. running mean: -7.485251\n",
      "ep 2810: ep_len:3694 episode reward: total was 8.520000. running mean: -7.325198\n",
      "ep 2810: ep_len:629 episode reward: total was -42.360000. running mean: -7.675546\n",
      "ep 2810: ep_len:806 episode reward: total was 28.810000. running mean: -7.310691\n",
      "ep 2810: ep_len:500 episode reward: total was 10.260000. running mean: -7.134984\n",
      "ep 2810: ep_len:199 episode reward: total was 98.000000. running mean: -6.083634\n",
      "ep 2810: ep_len:58 episode reward: total was 26.000000. running mean: -5.762798\n",
      "ep 2810: ep_len:742 episode reward: total was -50.320000. running mean: -6.208370\n",
      "ep 2810: ep_len:2873 episode reward: total was -36.170000. running mean: -6.507986\n",
      "ep 2810: ep_len:62 episode reward: total was 29.500000. running mean: -6.147906\n",
      "epsilon:0.009992 episode_count: 42297. steps_count: 45478547.000000\n",
      "ep 2811: ep_len:623 episode reward: total was -20.200000. running mean: -6.288427\n",
      "ep 2811: ep_len:674 episode reward: total was -36.830000. running mean: -6.593843\n",
      "ep 2811: ep_len:3036 episode reward: total was -41.180000. running mean: -6.939705\n",
      "ep 2811: ep_len:534 episode reward: total was -34.220000. running mean: -7.212508\n",
      "ep 2811: ep_len:40 episode reward: total was 18.500000. running mean: -6.955382\n",
      "ep 2811: ep_len:500 episode reward: total was 22.690000. running mean: -6.658929\n",
      "ep 2811: ep_len:572 episode reward: total was 30.410000. running mean: -6.288239\n",
      "ep 2811: ep_len:1247 episode reward: total was -104.860000. running mean: -7.273957\n",
      "ep 2811: ep_len:777 episode reward: total was 13.340000. running mean: -7.067817\n",
      "ep 2811: ep_len:1520 episode reward: total was -6.370000. running mean: -7.060839\n",
      "ep 2811: ep_len:93 episode reward: total was 43.500000. running mean: -6.555231\n",
      "ep 2811: ep_len:639 episode reward: total was 5.310000. running mean: -6.436578\n",
      "ep 2811: ep_len:2745 episode reward: total was -6.940000. running mean: -6.441613\n",
      "epsilon:0.009992 episode_count: 42310. steps_count: 45491547.000000\n",
      "ep 2812: ep_len:1087 episode reward: total was 4.640000. running mean: -6.330797\n",
      "ep 2812: ep_len:686 episode reward: total was -1.390000. running mean: -6.281389\n",
      "ep 2812: ep_len:2983 episode reward: total was -52.820000. running mean: -6.746775\n",
      "ep 2812: ep_len:644 episode reward: total was -12.210000. running mean: -6.801407\n",
      "ep 2812: ep_len:161 episode reward: total was 76.000000. running mean: -5.973393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2812: ep_len:74 episode reward: total was 35.500000. running mean: -5.558659\n",
      "ep 2812: ep_len:793 episode reward: total was 34.880000. running mean: -5.154272\n",
      "ep 2812: ep_len:279 episode reward: total was 17.690000. running mean: -4.925830\n",
      "ep 2812: ep_len:505 episode reward: total was -30.470000. running mean: -5.181271\n",
      "ep 2812: ep_len:7349 episode reward: total was 36.740000. running mean: -4.762059\n",
      "ep 2812: ep_len:657 episode reward: total was 15.530000. running mean: -4.559138\n",
      "ep 2812: ep_len:90 episode reward: total was 42.000000. running mean: -4.093547\n",
      "ep 2812: ep_len:629 episode reward: total was 18.120000. running mean: -3.871411\n",
      "ep 2812: ep_len:37 episode reward: total was 17.000000. running mean: -3.662697\n",
      "ep 2812: ep_len:40 episode reward: total was 18.500000. running mean: -3.441070\n",
      "epsilon:0.009992 episode_count: 42325. steps_count: 45507561.000000\n",
      "ep 2813: ep_len:654 episode reward: total was -19.340000. running mean: -3.600059\n",
      "ep 2813: ep_len:699 episode reward: total was 6.920000. running mean: -3.494859\n",
      "ep 2813: ep_len:3036 episode reward: total was 3.890000. running mean: -3.421010\n",
      "ep 2813: ep_len:500 episode reward: total was 19.200000. running mean: -3.194800\n",
      "ep 2813: ep_len:47 episode reward: total was 22.000000. running mean: -2.942852\n",
      "ep 2813: ep_len:109 episode reward: total was 53.000000. running mean: -2.383424\n",
      "ep 2813: ep_len:500 episode reward: total was 7.780000. running mean: -2.281789\n",
      "ep 2813: ep_len:4427 episode reward: total was -286.480000. running mean: -5.123772\n",
      "ep 2813: ep_len:590 episode reward: total was -36.780000. running mean: -5.440334\n",
      "ep 2813: ep_len:652 episode reward: total was 3.050000. running mean: -5.355430\n",
      "ep 2813: ep_len:500 episode reward: total was 16.660000. running mean: -5.135276\n",
      "ep 2813: ep_len:54 episode reward: total was 25.500000. running mean: -4.828923\n",
      "ep 2813: ep_len:111 episode reward: total was 52.500000. running mean: -4.255634\n",
      "ep 2813: ep_len:500 episode reward: total was -5.510000. running mean: -4.268178\n",
      "ep 2813: ep_len:2762 episode reward: total was 5.510000. running mean: -4.170396\n",
      "ep 2813: ep_len:44 episode reward: total was 20.500000. running mean: -3.923692\n",
      "epsilon:0.009992 episode_count: 42341. steps_count: 45522746.000000\n",
      "ep 2814: ep_len:685 episode reward: total was -24.630000. running mean: -4.130755\n",
      "ep 2814: ep_len:746 episode reward: total was 7.090000. running mean: -4.018548\n",
      "ep 2814: ep_len:2947 episode reward: total was -27.260000. running mean: -4.250962\n",
      "ep 2814: ep_len:1221 episode reward: total was -50.060000. running mean: -4.709053\n",
      "ep 2814: ep_len:81 episode reward: total was 37.500000. running mean: -4.286962\n",
      "ep 2814: ep_len:67 episode reward: total was 30.500000. running mean: -3.939092\n",
      "ep 2814: ep_len:645 episode reward: total was -4.900000. running mean: -3.948701\n",
      "ep 2814: ep_len:3637 episode reward: total was -76.020000. running mean: -4.669414\n",
      "ep 2814: ep_len:1533 episode reward: total was -12.540000. running mean: -4.748120\n",
      "ep 2814: ep_len:637 episode reward: total was 1.610000. running mean: -4.684539\n",
      "ep 2814: ep_len:720 episode reward: total was -8.740000. running mean: -4.725094\n",
      "ep 2814: ep_len:154 episode reward: total was 74.000000. running mean: -3.937843\n",
      "ep 2814: ep_len:49 episode reward: total was 23.000000. running mean: -3.668464\n",
      "ep 2814: ep_len:86 episode reward: total was 41.500000. running mean: -3.216780\n",
      "ep 2814: ep_len:676 episode reward: total was 18.650000. running mean: -2.998112\n",
      "ep 2814: ep_len:2793 episode reward: total was -55.120000. running mean: -3.519331\n",
      "ep 2814: ep_len:56 episode reward: total was 26.500000. running mean: -3.219137\n",
      "epsilon:0.009992 episode_count: 42358. steps_count: 45539479.000000\n",
      "ep 2815: ep_len:653 episode reward: total was 0.300000. running mean: -3.183946\n",
      "ep 2815: ep_len:3156 episode reward: total was -229.070000. running mean: -5.442807\n",
      "ep 2815: ep_len:2921 episode reward: total was -10.750000. running mean: -5.495879\n",
      "ep 2815: ep_len:868 episode reward: total was 21.210000. running mean: -5.228820\n",
      "ep 2815: ep_len:747 episode reward: total was -7.850000. running mean: -5.255032\n",
      "ep 2815: ep_len:3825 episode reward: total was -48.220000. running mean: -5.684681\n",
      "ep 2815: ep_len:672 episode reward: total was -2.020000. running mean: -5.648034\n",
      "ep 2815: ep_len:802 episode reward: total was 29.080000. running mean: -5.300754\n",
      "ep 2815: ep_len:534 episode reward: total was 11.180000. running mean: -5.135947\n",
      "ep 2815: ep_len:73 episode reward: total was 33.500000. running mean: -4.749587\n",
      "ep 2815: ep_len:114 episode reward: total was 54.000000. running mean: -4.162091\n",
      "ep 2815: ep_len:500 episode reward: total was 5.090000. running mean: -4.069570\n",
      "ep 2815: ep_len:2849 episode reward: total was -9.660000. running mean: -4.125475\n",
      "ep 2815: ep_len:73 episode reward: total was 35.000000. running mean: -3.734220\n",
      "epsilon:0.009992 episode_count: 42372. steps_count: 45557266.000000\n",
      "ep 2816: ep_len:1499 episode reward: total was 24.500000. running mean: -3.451878\n",
      "ep 2816: ep_len:955 episode reward: total was 9.360000. running mean: -3.323759\n",
      "ep 2816: ep_len:74 episode reward: total was 35.500000. running mean: -2.935521\n",
      "ep 2816: ep_len:3079 episode reward: total was -40.560000. running mean: -3.311766\n",
      "ep 2816: ep_len:737 episode reward: total was -17.040000. running mean: -3.449048\n",
      "ep 2816: ep_len:1073 episode reward: total was -17.720000. running mean: -3.591758\n",
      "ep 2816: ep_len:358 episode reward: total was 17.200000. running mean: -3.383840\n",
      "ep 2816: ep_len:811 episode reward: total was -24.760000. running mean: -3.597602\n",
      "ep 2816: ep_len:755 episode reward: total was 11.000000. running mean: -3.451626\n",
      "ep 2816: ep_len:598 episode reward: total was 18.140000. running mean: -3.235710\n",
      "ep 2816: ep_len:47 episode reward: total was 22.000000. running mean: -2.983353\n",
      "ep 2816: ep_len:135 episode reward: total was 64.500000. running mean: -2.308519\n",
      "ep 2816: ep_len:74 episode reward: total was 35.500000. running mean: -1.930434\n",
      "ep 2816: ep_len:980 episode reward: total was -11.580000. running mean: -2.026930\n",
      "ep 2816: ep_len:2737 episode reward: total was -81.240000. running mean: -2.819060\n",
      "epsilon:0.009992 episode_count: 42387. steps_count: 45571178.000000\n",
      "ep 2817: ep_len:659 episode reward: total was -3.060000. running mean: -2.821470\n",
      "ep 2817: ep_len:996 episode reward: total was 0.430000. running mean: -2.788955\n",
      "ep 2817: ep_len:2922 episode reward: total was -5.100000. running mean: -2.812065\n",
      "ep 2817: ep_len:680 episode reward: total was -0.790000. running mean: -2.791845\n",
      "ep 2817: ep_len:983 episode reward: total was -15.960000. running mean: -2.923526\n",
      "ep 2817: ep_len:4040 episode reward: total was -130.060000. running mean: -4.194891\n",
      "ep 2817: ep_len:1253 episode reward: total was -128.700000. running mean: -5.439942\n",
      "ep 2817: ep_len:836 episode reward: total was 24.740000. running mean: -5.138143\n",
      "ep 2817: ep_len:500 episode reward: total was -1.560000. running mean: -5.102361\n",
      "ep 2817: ep_len:95 episode reward: total was 44.500000. running mean: -4.606338\n",
      "ep 2817: ep_len:824 episode reward: total was 14.850000. running mean: -4.411774\n",
      "ep 2817: ep_len:2835 episode reward: total was -10.500000. running mean: -4.472657\n",
      "ep 2817: ep_len:40 episode reward: total was 17.000000. running mean: -4.257930\n",
      "epsilon:0.009992 episode_count: 42400. steps_count: 45587841.000000\n",
      "ep 2818: ep_len:1163 episode reward: total was 7.710000. running mean: -4.138251\n",
      "ep 2818: ep_len:754 episode reward: total was -15.590000. running mean: -4.252768\n",
      "ep 2818: ep_len:44 episode reward: total was 20.500000. running mean: -4.005240\n",
      "ep 2818: ep_len:2968 episode reward: total was -32.680000. running mean: -4.291988\n",
      "ep 2818: ep_len:1673 episode reward: total was -62.190000. running mean: -4.870968\n",
      "ep 2818: ep_len:32 episode reward: total was 14.500000. running mean: -4.677259\n",
      "ep 2818: ep_len:124 episode reward: total was 57.500000. running mean: -4.055486\n",
      "ep 2818: ep_len:58 episode reward: total was 26.000000. running mean: -3.754931\n",
      "ep 2818: ep_len:500 episode reward: total was 30.810000. running mean: -3.409282\n",
      "ep 2818: ep_len:663 episode reward: total was 20.150000. running mean: -3.173689\n",
      "ep 2818: ep_len:574 episode reward: total was -50.990000. running mean: -3.651852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2818: ep_len:742 episode reward: total was -5.270000. running mean: -3.668034\n",
      "ep 2818: ep_len:661 episode reward: total was 10.830000. running mean: -3.523053\n",
      "ep 2818: ep_len:52 episode reward: total was 24.500000. running mean: -3.242823\n",
      "ep 2818: ep_len:150 episode reward: total was 72.000000. running mean: -2.490394\n",
      "ep 2818: ep_len:123 episode reward: total was 58.500000. running mean: -1.880490\n",
      "ep 2818: ep_len:1139 episode reward: total was -101.380000. running mean: -2.875486\n",
      "ep 2818: ep_len:2728 episode reward: total was -9.610000. running mean: -2.942831\n",
      "epsilon:0.009992 episode_count: 42418. steps_count: 45601989.000000\n",
      "ep 2819: ep_len:939 episode reward: total was -17.450000. running mean: -3.087902\n",
      "ep 2819: ep_len:716 episode reward: total was -24.490000. running mean: -3.301923\n",
      "ep 2819: ep_len:51 episode reward: total was 22.500000. running mean: -3.043904\n",
      "ep 2819: ep_len:2911 episode reward: total was 11.810000. running mean: -2.895365\n",
      "ep 2819: ep_len:2559 episode reward: total was -547.250000. running mean: -8.338911\n",
      "ep 2819: ep_len:68 episode reward: total was 32.500000. running mean: -7.930522\n",
      "ep 2819: ep_len:1052 episode reward: total was -17.410000. running mean: -8.025317\n",
      "ep 2819: ep_len:624 episode reward: total was -9.050000. running mean: -8.035564\n",
      "ep 2819: ep_len:2155 episode reward: total was -492.710000. running mean: -12.882308\n",
      "ep 2819: ep_len:720 episode reward: total was 33.070000. running mean: -12.422785\n",
      "ep 2819: ep_len:1063 episode reward: total was 44.900000. running mean: -11.849557\n",
      "ep 2819: ep_len:755 episode reward: total was 2.330000. running mean: -11.707762\n",
      "ep 2819: ep_len:2895 episode reward: total was -1.490000. running mean: -11.605584\n",
      "epsilon:0.009992 episode_count: 42431. steps_count: 45618497.000000\n",
      "ep 2820: ep_len:634 episode reward: total was 4.150000. running mean: -11.448028\n",
      "ep 2820: ep_len:709 episode reward: total was 6.190000. running mean: -11.271648\n",
      "ep 2820: ep_len:3039 episode reward: total was -25.000000. running mean: -11.408932\n",
      "ep 2820: ep_len:664 episode reward: total was -5.720000. running mean: -11.352042\n",
      "ep 2820: ep_len:73 episode reward: total was 35.000000. running mean: -10.888522\n",
      "ep 2820: ep_len:1399 episode reward: total was 16.850000. running mean: -10.611137\n",
      "ep 2820: ep_len:651 episode reward: total was 28.690000. running mean: -10.218125\n",
      "ep 2820: ep_len:2871 episode reward: total was -250.430000. running mean: -12.620244\n",
      "ep 2820: ep_len:678 episode reward: total was 13.170000. running mean: -12.362342\n",
      "ep 2820: ep_len:715 episode reward: total was -7.640000. running mean: -12.315118\n",
      "ep 2820: ep_len:39 episode reward: total was 15.000000. running mean: -12.041967\n",
      "ep 2820: ep_len:500 episode reward: total was 15.340000. running mean: -11.768147\n",
      "ep 2820: ep_len:2976 episode reward: total was -5.110000. running mean: -11.701566\n",
      "epsilon:0.009992 episode_count: 42444. steps_count: 45633445.000000\n",
      "ep 2821: ep_len:1176 episode reward: total was -17.700000. running mean: -11.761550\n",
      "ep 2821: ep_len:664 episode reward: total was 7.540000. running mean: -11.568535\n",
      "ep 2821: ep_len:44 episode reward: total was 20.500000. running mean: -11.247849\n",
      "ep 2821: ep_len:3020 episode reward: total was 9.640000. running mean: -11.038971\n",
      "ep 2821: ep_len:826 episode reward: total was -19.290000. running mean: -11.121481\n",
      "ep 2821: ep_len:115 episode reward: total was 56.000000. running mean: -10.450266\n",
      "ep 2821: ep_len:2422 episode reward: total was -1869.260000. running mean: -29.038364\n",
      "ep 2821: ep_len:317 episode reward: total was 11.950000. running mean: -28.628480\n",
      "ep 2821: ep_len:2166 episode reward: total was -178.300000. running mean: -30.125195\n",
      "ep 2821: ep_len:830 episode reward: total was 52.510000. running mean: -29.298843\n",
      "ep 2821: ep_len:916 episode reward: total was 26.870000. running mean: -28.737155\n",
      "ep 2821: ep_len:115 episode reward: total was 56.000000. running mean: -27.889783\n",
      "ep 2821: ep_len:21 episode reward: total was 9.000000. running mean: -27.520885\n",
      "ep 2821: ep_len:1140 episode reward: total was -19.990000. running mean: -27.445577\n",
      "ep 2821: ep_len:2884 episode reward: total was 4.160000. running mean: -27.129521\n",
      "ep 2821: ep_len:75 episode reward: total was 36.000000. running mean: -26.498226\n",
      "epsilon:0.009992 episode_count: 42460. steps_count: 45650176.000000\n",
      "ep 2822: ep_len:1207 episode reward: total was -8.410000. running mean: -26.317343\n",
      "ep 2822: ep_len:2836 episode reward: total was -461.130000. running mean: -30.665470\n",
      "ep 2822: ep_len:2951 episode reward: total was -7.350000. running mean: -30.432315\n",
      "ep 2822: ep_len:2048 episode reward: total was -317.030000. running mean: -33.298292\n",
      "ep 2822: ep_len:60 episode reward: total was 28.500000. running mean: -32.680309\n",
      "ep 2822: ep_len:1022 episode reward: total was -27.320000. running mean: -32.626706\n",
      "ep 2822: ep_len:649 episode reward: total was 18.690000. running mean: -32.113539\n",
      "ep 2822: ep_len:643 episode reward: total was -31.110000. running mean: -32.103504\n",
      "ep 2822: ep_len:728 episode reward: total was 35.260000. running mean: -31.429869\n",
      "ep 2822: ep_len:679 episode reward: total was -33.790000. running mean: -31.453470\n",
      "ep 2822: ep_len:62 episode reward: total was 28.000000. running mean: -30.858935\n",
      "ep 2822: ep_len:116 episode reward: total was 53.500000. running mean: -30.015346\n",
      "ep 2822: ep_len:50 episode reward: total was 20.500000. running mean: -29.510192\n",
      "ep 2822: ep_len:87 episode reward: total was 40.500000. running mean: -28.810090\n",
      "ep 2822: ep_len:574 episode reward: total was 5.080000. running mean: -28.471190\n",
      "ep 2822: ep_len:2814 episode reward: total was -29.350000. running mean: -28.479978\n",
      "epsilon:0.009992 episode_count: 42476. steps_count: 45666702.000000\n",
      "ep 2823: ep_len:500 episode reward: total was -0.280000. running mean: -28.197978\n",
      "ep 2823: ep_len:723 episode reward: total was -38.620000. running mean: -28.302198\n",
      "ep 2823: ep_len:3027 episode reward: total was -21.440000. running mean: -28.233576\n",
      "ep 2823: ep_len:720 episode reward: total was 33.430000. running mean: -27.616940\n",
      "ep 2823: ep_len:118 episode reward: total was 54.500000. running mean: -26.795771\n",
      "ep 2823: ep_len:54 episode reward: total was 22.500000. running mean: -26.302813\n",
      "ep 2823: ep_len:53 episode reward: total was 25.000000. running mean: -25.789785\n",
      "ep 2823: ep_len:1481 episode reward: total was -237.330000. running mean: -27.905187\n",
      "ep 2823: ep_len:671 episode reward: total was 20.720000. running mean: -27.418935\n",
      "ep 2823: ep_len:4220 episode reward: total was -1235.650000. running mean: -39.501246\n",
      "ep 2823: ep_len:701 episode reward: total was 51.280000. running mean: -38.593434\n",
      "ep 2823: ep_len:685 episode reward: total was -22.220000. running mean: -38.429699\n",
      "ep 2823: ep_len:119 episode reward: total was 56.500000. running mean: -37.480402\n",
      "ep 2823: ep_len:39 episode reward: total was 18.000000. running mean: -36.925598\n",
      "ep 2823: ep_len:75 episode reward: total was 34.500000. running mean: -36.211342\n",
      "ep 2823: ep_len:670 episode reward: total was -13.670000. running mean: -35.985929\n",
      "ep 2823: ep_len:2816 episode reward: total was 7.270000. running mean: -35.553370\n",
      "ep 2823: ep_len:68 episode reward: total was 32.500000. running mean: -34.872836\n",
      "epsilon:0.009992 episode_count: 42494. steps_count: 45683442.000000\n",
      "ep 2824: ep_len:1091 episode reward: total was -5.420000. running mean: -34.578307\n",
      "ep 2824: ep_len:500 episode reward: total was 25.810000. running mean: -33.974424\n",
      "ep 2824: ep_len:3004 episode reward: total was -16.770000. running mean: -33.802380\n",
      "ep 2824: ep_len:1535 episode reward: total was 16.130000. running mean: -33.303056\n",
      "ep 2824: ep_len:113 episode reward: total was 52.000000. running mean: -32.450026\n",
      "ep 2824: ep_len:1081 episode reward: total was -41.880000. running mean: -32.544326\n",
      "ep 2824: ep_len:3767 episode reward: total was -606.050000. running mean: -38.279382\n",
      "ep 2824: ep_len:1187 episode reward: total was -85.260000. running mean: -38.749188\n",
      "ep 2824: ep_len:642 episode reward: total was 20.310000. running mean: -38.158597\n",
      "ep 2824: ep_len:500 episode reward: total was 20.330000. running mean: -37.573711\n",
      "ep 2824: ep_len:163 episode reward: total was 77.000000. running mean: -36.427974\n",
      "ep 2824: ep_len:46 episode reward: total was 21.500000. running mean: -35.848694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2824: ep_len:119 episode reward: total was 58.000000. running mean: -34.910207\n",
      "ep 2824: ep_len:549 episode reward: total was -8.820000. running mean: -34.649305\n",
      "ep 2824: ep_len:2882 episode reward: total was -14.160000. running mean: -34.444412\n",
      "ep 2824: ep_len:49 episode reward: total was 23.000000. running mean: -33.869968\n",
      "epsilon:0.009992 episode_count: 42510. steps_count: 45700670.000000\n",
      "ep 2825: ep_len:684 episode reward: total was -41.470000. running mean: -33.945968\n",
      "ep 2825: ep_len:500 episode reward: total was 8.270000. running mean: -33.523808\n",
      "ep 2825: ep_len:73 episode reward: total was 32.000000. running mean: -32.868570\n",
      "ep 2825: ep_len:3006 episode reward: total was -12.100000. running mean: -32.660884\n",
      "ep 2825: ep_len:810 episode reward: total was 20.290000. running mean: -32.131376\n",
      "ep 2825: ep_len:37 episode reward: total was 14.000000. running mean: -31.670062\n",
      "ep 2825: ep_len:90 episode reward: total was 42.000000. running mean: -30.933361\n",
      "ep 2825: ep_len:1528 episode reward: total was 25.220000. running mean: -30.371828\n",
      "ep 2825: ep_len:685 episode reward: total was 20.860000. running mean: -29.859509\n",
      "ep 2825: ep_len:3842 episode reward: total was -642.490000. running mean: -35.985814\n",
      "ep 2825: ep_len:744 episode reward: total was 8.620000. running mean: -35.539756\n",
      "ep 2825: ep_len:612 episode reward: total was 14.360000. running mean: -35.040759\n",
      "ep 2825: ep_len:169 episode reward: total was 80.000000. running mean: -33.890351\n",
      "ep 2825: ep_len:42 episode reward: total was 18.000000. running mean: -33.371447\n",
      "ep 2825: ep_len:96 episode reward: total was 45.000000. running mean: -32.587733\n",
      "ep 2825: ep_len:1421 episode reward: total was 10.180000. running mean: -32.160056\n",
      "ep 2825: ep_len:2931 episode reward: total was -10.340000. running mean: -31.941855\n",
      "ep 2825: ep_len:29 episode reward: total was 13.000000. running mean: -31.492437\n",
      "epsilon:0.009992 episode_count: 42528. steps_count: 45717969.000000\n",
      "ep 2826: ep_len:1013 episode reward: total was -76.140000. running mean: -31.938912\n",
      "ep 2826: ep_len:775 episode reward: total was 4.870000. running mean: -31.570823\n",
      "ep 2826: ep_len:3127 episode reward: total was 15.730000. running mean: -31.097815\n",
      "ep 2826: ep_len:500 episode reward: total was -0.580000. running mean: -30.792637\n",
      "ep 2826: ep_len:55 episode reward: total was 26.000000. running mean: -30.224710\n",
      "ep 2826: ep_len:43 episode reward: total was 17.000000. running mean: -29.752463\n",
      "ep 2826: ep_len:962 episode reward: total was -19.840000. running mean: -29.653339\n",
      "ep 2826: ep_len:3743 episode reward: total was -36.130000. running mean: -29.718105\n",
      "ep 2826: ep_len:791 episode reward: total was -29.280000. running mean: -29.713724\n",
      "ep 2826: ep_len:826 episode reward: total was 23.600000. running mean: -29.180587\n",
      "ep 2826: ep_len:657 episode reward: total was -2.390000. running mean: -28.912681\n",
      "ep 2826: ep_len:132 episode reward: total was 57.000000. running mean: -28.053554\n",
      "ep 2826: ep_len:42 episode reward: total was 18.000000. running mean: -27.593019\n",
      "ep 2826: ep_len:1176 episode reward: total was -15.680000. running mean: -27.473888\n",
      "ep 2826: ep_len:2937 episode reward: total was 7.350000. running mean: -27.125650\n",
      "ep 2826: ep_len:73 episode reward: total was 35.000000. running mean: -26.504393\n",
      "epsilon:0.009992 episode_count: 42544. steps_count: 45734821.000000\n",
      "ep 2827: ep_len:718 episode reward: total was -56.620000. running mean: -26.805549\n",
      "ep 2827: ep_len:663 episode reward: total was -50.100000. running mean: -27.038494\n",
      "ep 2827: ep_len:64 episode reward: total was 29.000000. running mean: -26.478109\n",
      "ep 2827: ep_len:2944 episode reward: total was -55.560000. running mean: -26.768928\n",
      "ep 2827: ep_len:821 episode reward: total was -6.410000. running mean: -26.565338\n",
      "ep 2827: ep_len:90 episode reward: total was 42.000000. running mean: -25.879685\n",
      "ep 2827: ep_len:40 episode reward: total was 17.000000. running mean: -25.450888\n",
      "ep 2827: ep_len:500 episode reward: total was 34.480000. running mean: -24.851579\n",
      "ep 2827: ep_len:500 episode reward: total was 34.210000. running mean: -24.260963\n",
      "ep 2827: ep_len:4142 episode reward: total was -547.240000. running mean: -29.490754\n",
      "ep 2827: ep_len:683 episode reward: total was -1.990000. running mean: -29.215746\n",
      "ep 2827: ep_len:1784 episode reward: total was -84.180000. running mean: -29.765389\n",
      "ep 2827: ep_len:59 episode reward: total was 28.000000. running mean: -29.187735\n",
      "ep 2827: ep_len:160 episode reward: total was 75.500000. running mean: -28.140858\n",
      "ep 2827: ep_len:767 episode reward: total was -1.670000. running mean: -27.876149\n",
      "ep 2827: ep_len:2729 episode reward: total was -1.250000. running mean: -27.609888\n",
      "epsilon:0.009992 episode_count: 42560. steps_count: 45751485.000000\n",
      "ep 2828: ep_len:960 episode reward: total was -54.220000. running mean: -27.875989\n",
      "ep 2828: ep_len:500 episode reward: total was 24.590000. running mean: -27.351329\n",
      "ep 2828: ep_len:2872 episode reward: total was -16.350000. running mean: -27.241315\n",
      "ep 2828: ep_len:643 episode reward: total was -14.320000. running mean: -27.112102\n",
      "ep 2828: ep_len:86 episode reward: total was 41.500000. running mean: -26.425981\n",
      "ep 2828: ep_len:3540 episode reward: total was -2034.880000. running mean: -46.510521\n",
      "ep 2828: ep_len:3896 episode reward: total was -114.640000. running mean: -47.191816\n",
      "ep 2828: ep_len:1491 episode reward: total was -129.340000. running mean: -48.013298\n",
      "ep 2828: ep_len:750 episode reward: total was 7.030000. running mean: -47.462865\n",
      "ep 2828: ep_len:1444 episode reward: total was 0.860000. running mean: -46.979636\n",
      "ep 2828: ep_len:58 episode reward: total was 27.500000. running mean: -46.234840\n",
      "ep 2828: ep_len:632 episode reward: total was -11.600000. running mean: -45.888492\n",
      "ep 2828: ep_len:2786 episode reward: total was 1.130000. running mean: -45.418307\n",
      "epsilon:0.009992 episode_count: 42573. steps_count: 45771143.000000\n",
      "ep 2829: ep_len:500 episode reward: total was 6.920000. running mean: -44.894924\n",
      "ep 2829: ep_len:683 episode reward: total was -26.550000. running mean: -44.711474\n",
      "ep 2829: ep_len:2935 episode reward: total was -52.510000. running mean: -44.789460\n",
      "ep 2829: ep_len:803 episode reward: total was 22.580000. running mean: -44.115765\n",
      "ep 2829: ep_len:66 episode reward: total was 30.000000. running mean: -43.374608\n",
      "ep 2829: ep_len:724 episode reward: total was -0.000000. running mean: -42.940861\n",
      "ep 2829: ep_len:646 episode reward: total was 23.440000. running mean: -42.277053\n",
      "ep 2829: ep_len:712 episode reward: total was -30.420000. running mean: -42.158482\n",
      "ep 2829: ep_len:735 episode reward: total was 10.550000. running mean: -41.631397\n",
      "ep 2829: ep_len:785 episode reward: total was -8.970000. running mean: -41.304783\n",
      "ep 2829: ep_len:151 episode reward: total was 69.500000. running mean: -40.196736\n",
      "ep 2829: ep_len:84 episode reward: total was 40.500000. running mean: -39.389768\n",
      "ep 2829: ep_len:585 episode reward: total was -10.950000. running mean: -39.105371\n",
      "ep 2829: ep_len:2838 episode reward: total was -5.120000. running mean: -38.765517\n",
      "ep 2829: ep_len:68 episode reward: total was 32.500000. running mean: -38.052862\n",
      "epsilon:0.009992 episode_count: 42588. steps_count: 45783458.000000\n",
      "ep 2830: ep_len:875 episode reward: total was -55.020000. running mean: -38.222533\n",
      "ep 2830: ep_len:500 episode reward: total was 9.430000. running mean: -37.746008\n",
      "ep 2830: ep_len:2886 episode reward: total was -161.190000. running mean: -38.980448\n",
      "ep 2830: ep_len:668 episode reward: total was 7.340000. running mean: -38.517243\n",
      "ep 2830: ep_len:114 episode reward: total was 55.500000. running mean: -37.577071\n",
      "ep 2830: ep_len:71 episode reward: total was 34.000000. running mean: -36.861300\n",
      "ep 2830: ep_len:50 episode reward: total was 22.000000. running mean: -36.272687\n",
      "ep 2830: ep_len:855 episode reward: total was 11.380000. running mean: -35.796160\n",
      "ep 2830: ep_len:642 episode reward: total was 26.640000. running mean: -35.171799\n",
      "ep 2830: ep_len:1521 episode reward: total was -40.630000. running mean: -35.226381\n",
      "ep 2830: ep_len:721 episode reward: total was 42.170000. running mean: -34.452417\n",
      "ep 2830: ep_len:546 episode reward: total was -11.880000. running mean: -34.226693\n",
      "ep 2830: ep_len:191 episode reward: total was 88.000000. running mean: -33.004426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2830: ep_len:713 episode reward: total was -15.260000. running mean: -32.826981\n",
      "ep 2830: ep_len:2809 episode reward: total was -18.420000. running mean: -32.682912\n",
      "epsilon:0.009992 episode_count: 42603. steps_count: 45796620.000000\n",
      "ep 2831: ep_len:1128 episode reward: total was -36.360000. running mean: -32.719683\n",
      "ep 2831: ep_len:1606 episode reward: total was -156.600000. running mean: -33.958486\n",
      "ep 2831: ep_len:2905 episode reward: total was -40.690000. running mean: -34.025801\n",
      "ep 2831: ep_len:874 episode reward: total was 9.390000. running mean: -33.591643\n",
      "ep 2831: ep_len:1540 episode reward: total was -196.210000. running mean: -35.217826\n",
      "ep 2831: ep_len:3639 episode reward: total was -131.150000. running mean: -36.177148\n",
      "ep 2831: ep_len:923 episode reward: total was -30.850000. running mean: -36.123877\n",
      "ep 2831: ep_len:838 episode reward: total was 28.190000. running mean: -35.480738\n",
      "ep 2831: ep_len:755 episode reward: total was -12.820000. running mean: -35.254131\n",
      "ep 2831: ep_len:62 episode reward: total was 29.500000. running mean: -34.606589\n",
      "ep 2831: ep_len:51 episode reward: total was 22.500000. running mean: -34.035523\n",
      "ep 2831: ep_len:1097 episode reward: total was 20.200000. running mean: -33.493168\n",
      "ep 2831: ep_len:2925 episode reward: total was 2.150000. running mean: -33.136736\n",
      "ep 2831: ep_len:45 episode reward: total was 21.000000. running mean: -32.595369\n",
      "epsilon:0.009992 episode_count: 42617. steps_count: 45815008.000000\n",
      "ep 2832: ep_len:637 episode reward: total was -20.030000. running mean: -32.469715\n",
      "ep 2832: ep_len:1271 episode reward: total was -60.180000. running mean: -32.746818\n",
      "ep 2832: ep_len:3036 episode reward: total was -104.860000. running mean: -33.467950\n",
      "ep 2832: ep_len:753 episode reward: total was -22.940000. running mean: -33.362671\n",
      "ep 2832: ep_len:43 episode reward: total was 20.000000. running mean: -32.829044\n",
      "ep 2832: ep_len:34 episode reward: total was 15.500000. running mean: -32.345753\n",
      "ep 2832: ep_len:881 episode reward: total was 50.310000. running mean: -31.519196\n",
      "ep 2832: ep_len:3681 episode reward: total was -66.000000. running mean: -31.864004\n",
      "ep 2832: ep_len:1263 episode reward: total was -80.340000. running mean: -32.348764\n",
      "ep 2832: ep_len:7307 episode reward: total was -1399.190000. running mean: -46.017176\n",
      "ep 2832: ep_len:4400 episode reward: total was -3440.440000. running mean: -79.961404\n",
      "ep 2832: ep_len:143 episode reward: total was 70.000000. running mean: -78.461790\n",
      "ep 2832: ep_len:53 episode reward: total was 23.500000. running mean: -77.442173\n",
      "ep 2832: ep_len:1502 episode reward: total was -0.850000. running mean: -76.676251\n",
      "ep 2832: ep_len:2807 episode reward: total was -27.860000. running mean: -76.188088\n",
      "epsilon:0.009992 episode_count: 42632. steps_count: 45842819.000000\n",
      "ep 2833: ep_len:811 episode reward: total was -34.750000. running mean: -75.773707\n",
      "ep 2833: ep_len:867 episode reward: total was 8.950000. running mean: -74.926470\n",
      "ep 2833: ep_len:43 episode reward: total was 17.000000. running mean: -74.007206\n",
      "ep 2833: ep_len:3002 episode reward: total was -103.400000. running mean: -74.301134\n",
      "ep 2833: ep_len:4777 episode reward: total was -1889.630000. running mean: -92.454422\n",
      "ep 2833: ep_len:42 episode reward: total was 16.500000. running mean: -91.364878\n",
      "ep 2833: ep_len:117 episode reward: total was 55.500000. running mean: -89.896229\n",
      "ep 2833: ep_len:1139 episode reward: total was -22.110000. running mean: -89.218367\n",
      "ep 2833: ep_len:4017 episode reward: total was -835.800000. running mean: -96.684183\n",
      "ep 2833: ep_len:956 episode reward: total was -71.350000. running mean: -96.430841\n",
      "ep 2833: ep_len:746 episode reward: total was -37.760000. running mean: -95.844133\n",
      "ep 2833: ep_len:750 episode reward: total was -11.860000. running mean: -95.004292\n",
      "ep 2833: ep_len:66 episode reward: total was 30.000000. running mean: -93.754249\n",
      "ep 2833: ep_len:113 episode reward: total was 53.500000. running mean: -92.281706\n",
      "ep 2833: ep_len:692 episode reward: total was -23.550000. running mean: -91.594389\n",
      "ep 2833: ep_len:2842 episode reward: total was -22.490000. running mean: -90.903345\n",
      "ep 2833: ep_len:59 episode reward: total was 28.000000. running mean: -89.714312\n",
      "epsilon:0.009992 episode_count: 42649. steps_count: 45863858.000000\n",
      "ep 2834: ep_len:671 episode reward: total was -7.600000. running mean: -88.893169\n",
      "ep 2834: ep_len:500 episode reward: total was -1.340000. running mean: -88.017637\n",
      "ep 2834: ep_len:2959 episode reward: total was -78.240000. running mean: -87.919861\n",
      "ep 2834: ep_len:500 episode reward: total was -2.540000. running mean: -87.066062\n",
      "ep 2834: ep_len:57 episode reward: total was 27.000000. running mean: -85.925401\n",
      "ep 2834: ep_len:129 episode reward: total was 63.000000. running mean: -84.436147\n",
      "ep 2834: ep_len:1438 episode reward: total was -296.870000. running mean: -86.560486\n",
      "ep 2834: ep_len:333 episode reward: total was 16.980000. running mean: -85.525081\n",
      "ep 2834: ep_len:3214 episode reward: total was -236.660000. running mean: -87.036430\n",
      "ep 2834: ep_len:795 episode reward: total was 16.520000. running mean: -86.000866\n",
      "ep 2834: ep_len:1172 episode reward: total was -45.010000. running mean: -85.590957\n",
      "ep 2834: ep_len:76 episode reward: total was 36.500000. running mean: -84.370048\n",
      "ep 2834: ep_len:56 episode reward: total was 25.000000. running mean: -83.276347\n",
      "ep 2834: ep_len:610 episode reward: total was -19.320000. running mean: -82.636784\n",
      "ep 2834: ep_len:2832 episode reward: total was -46.590000. running mean: -82.276316\n",
      "ep 2834: ep_len:53 episode reward: total was 23.500000. running mean: -81.218553\n",
      "epsilon:0.009992 episode_count: 42665. steps_count: 45879253.000000\n",
      "ep 2835: ep_len:1155 episode reward: total was 24.680000. running mean: -80.159567\n",
      "ep 2835: ep_len:701 episode reward: total was -37.740000. running mean: -79.735372\n",
      "ep 2835: ep_len:2893 episode reward: total was -191.790000. running mean: -80.855918\n",
      "ep 2835: ep_len:723 episode reward: total was -133.960000. running mean: -81.386959\n",
      "ep 2835: ep_len:38 episode reward: total was 17.500000. running mean: -80.398089\n",
      "ep 2835: ep_len:178 episode reward: total was 87.500000. running mean: -78.719108\n",
      "ep 2835: ep_len:69 episode reward: total was 33.000000. running mean: -77.601917\n",
      "ep 2835: ep_len:35 episode reward: total was 14.500000. running mean: -76.680898\n",
      "ep 2835: ep_len:600 episode reward: total was 33.170000. running mean: -75.582389\n",
      "ep 2835: ep_len:343 episode reward: total was 13.220000. running mean: -74.694365\n",
      "ep 2835: ep_len:556 episode reward: total was -102.680000. running mean: -74.974221\n",
      "ep 2835: ep_len:832 episode reward: total was 22.280000. running mean: -74.001679\n",
      "ep 2835: ep_len:833 episode reward: total was 15.940000. running mean: -73.102262\n",
      "ep 2835: ep_len:203 episode reward: total was 100.000000. running mean: -71.371240\n",
      "ep 2835: ep_len:31 episode reward: total was 14.000000. running mean: -70.517527\n",
      "ep 2835: ep_len:1091 episode reward: total was -26.170000. running mean: -70.074052\n",
      "ep 2835: ep_len:2753 episode reward: total was -75.780000. running mean: -70.131112\n",
      "ep 2835: ep_len:53 episode reward: total was 25.000000. running mean: -69.179801\n",
      "epsilon:0.009992 episode_count: 42683. steps_count: 45892340.000000\n",
      "ep 2836: ep_len:662 episode reward: total was -6.680000. running mean: -68.554803\n",
      "ep 2836: ep_len:771 episode reward: total was -14.960000. running mean: -68.018854\n",
      "ep 2836: ep_len:78 episode reward: total was 36.000000. running mean: -66.978666\n",
      "ep 2836: ep_len:2950 episode reward: total was -77.900000. running mean: -67.087879\n",
      "ep 2836: ep_len:801 episode reward: total was 36.890000. running mean: -66.048100\n",
      "ep 2836: ep_len:68 episode reward: total was 29.500000. running mean: -65.092619\n",
      "ep 2836: ep_len:68 episode reward: total was 32.500000. running mean: -64.116693\n",
      "ep 2836: ep_len:864 episode reward: total was 20.410000. running mean: -63.271426\n",
      "ep 2836: ep_len:3979 episode reward: total was -275.040000. running mean: -65.389112\n",
      "ep 2836: ep_len:865 episode reward: total was 18.980000. running mean: -64.545421\n",
      "ep 2836: ep_len:642 episode reward: total was 6.680000. running mean: -63.833167\n",
      "ep 2836: ep_len:667 episode reward: total was 2.460000. running mean: -63.170235\n",
      "ep 2836: ep_len:159 episode reward: total was 78.000000. running mean: -61.758533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2836: ep_len:80 episode reward: total was 37.000000. running mean: -60.770947\n",
      "ep 2836: ep_len:838 episode reward: total was 6.210000. running mean: -60.101138\n",
      "ep 2836: ep_len:2887 episode reward: total was -17.020000. running mean: -59.670327\n",
      "ep 2836: ep_len:72 episode reward: total was 33.000000. running mean: -58.743623\n",
      "epsilon:0.009992 episode_count: 42700. steps_count: 45908791.000000\n",
      "ep 2837: ep_len:1390 episode reward: total was 0.080000. running mean: -58.155387\n",
      "ep 2837: ep_len:720 episode reward: total was -53.800000. running mean: -58.111833\n",
      "ep 2837: ep_len:2969 episode reward: total was -56.840000. running mean: -58.099115\n",
      "ep 2837: ep_len:1615 episode reward: total was -65.150000. running mean: -58.169624\n",
      "ep 2837: ep_len:88 episode reward: total was 41.000000. running mean: -57.177927\n",
      "ep 2837: ep_len:1092 episode reward: total was -134.690000. running mean: -57.953048\n",
      "ep 2837: ep_len:3653 episode reward: total was -4.790000. running mean: -57.421418\n",
      "ep 2837: ep_len:1176 episode reward: total was -47.850000. running mean: -57.325704\n",
      "ep 2837: ep_len:7318 episode reward: total was -194.210000. running mean: -58.694547\n",
      "ep 2837: ep_len:680 episode reward: total was -7.810000. running mean: -58.185701\n",
      "ep 2837: ep_len:1102 episode reward: total was -23.490000. running mean: -57.838744\n",
      "ep 2837: ep_len:2801 episode reward: total was -21.770000. running mean: -57.478057\n",
      "ep 2837: ep_len:72 episode reward: total was 34.500000. running mean: -56.558276\n",
      "epsilon:0.009992 episode_count: 42713. steps_count: 45933467.000000\n",
      "ep 2838: ep_len:1396 episode reward: total was 22.820000. running mean: -55.764493\n",
      "ep 2838: ep_len:964 episode reward: total was 15.200000. running mean: -55.054848\n",
      "ep 2838: ep_len:69 episode reward: total was 33.000000. running mean: -54.174300\n",
      "ep 2838: ep_len:3011 episode reward: total was -93.020000. running mean: -54.562757\n",
      "ep 2838: ep_len:619 episode reward: total was -5.090000. running mean: -54.068029\n",
      "ep 2838: ep_len:124 episode reward: total was 59.000000. running mean: -52.937349\n",
      "ep 2838: ep_len:55 episode reward: total was 26.000000. running mean: -52.147976\n",
      "ep 2838: ep_len:52 episode reward: total was 24.500000. running mean: -51.381496\n",
      "ep 2838: ep_len:833 episode reward: total was 28.670000. running mean: -50.580981\n",
      "ep 2838: ep_len:3797 episode reward: total was -60.110000. running mean: -50.676271\n",
      "ep 2838: ep_len:1303 episode reward: total was -63.070000. running mean: -50.800208\n",
      "ep 2838: ep_len:643 episode reward: total was 1.210000. running mean: -50.280106\n",
      "ep 2838: ep_len:729 episode reward: total was -6.010000. running mean: -49.837405\n",
      "ep 2838: ep_len:57 episode reward: total was 25.500000. running mean: -49.084031\n",
      "ep 2838: ep_len:79 episode reward: total was 35.000000. running mean: -48.243191\n",
      "ep 2838: ep_len:1126 episode reward: total was 6.080000. running mean: -47.699959\n",
      "ep 2838: ep_len:2780 episode reward: total was -57.730000. running mean: -47.800259\n",
      "ep 2838: ep_len:54 episode reward: total was 25.500000. running mean: -47.067257\n",
      "epsilon:0.009992 episode_count: 42731. steps_count: 45951158.000000\n",
      "ep 2839: ep_len:749 episode reward: total was -117.920000. running mean: -47.775784\n",
      "ep 2839: ep_len:603 episode reward: total was -39.590000. running mean: -47.693926\n",
      "ep 2839: ep_len:76 episode reward: total was 36.500000. running mean: -46.851987\n",
      "ep 2839: ep_len:2985 episode reward: total was -46.020000. running mean: -46.843667\n",
      "ep 2839: ep_len:1459 episode reward: total was 12.890000. running mean: -46.246330\n",
      "ep 2839: ep_len:115 episode reward: total was 56.000000. running mean: -45.223867\n",
      "ep 2839: ep_len:1040 episode reward: total was -10.950000. running mean: -44.881128\n",
      "ep 2839: ep_len:3728 episode reward: total was -26.630000. running mean: -44.698617\n",
      "ep 2839: ep_len:1275 episode reward: total was -62.590000. running mean: -44.877531\n",
      "ep 2839: ep_len:707 episode reward: total was 40.290000. running mean: -44.025856\n",
      "ep 2839: ep_len:500 episode reward: total was 18.320000. running mean: -43.402397\n",
      "ep 2839: ep_len:500 episode reward: total was 17.480000. running mean: -42.793573\n",
      "ep 2839: ep_len:46 episode reward: total was 21.500000. running mean: -42.150637\n",
      "epsilon:0.009992 episode_count: 42744. steps_count: 45964941.000000\n",
      "ep 2840: ep_len:662 episode reward: total was 0.390000. running mean: -41.725231\n",
      "ep 2840: ep_len:500 episode reward: total was 13.320000. running mean: -41.174779\n",
      "ep 2840: ep_len:67 episode reward: total was 27.500000. running mean: -40.488031\n",
      "ep 2840: ep_len:3016 episode reward: total was -60.500000. running mean: -40.688151\n",
      "ep 2840: ep_len:562 episode reward: total was -44.040000. running mean: -40.721669\n",
      "ep 2840: ep_len:65 episode reward: total was 28.000000. running mean: -40.034452\n",
      "ep 2840: ep_len:743 episode reward: total was 27.810000. running mean: -39.356008\n",
      "ep 2840: ep_len:3830 episode reward: total was -16.040000. running mean: -39.122848\n",
      "ep 2840: ep_len:620 episode reward: total was 24.180000. running mean: -38.489819\n",
      "ep 2840: ep_len:766 episode reward: total was -17.450000. running mean: -38.279421\n",
      "ep 2840: ep_len:588 episode reward: total was -9.440000. running mean: -37.991027\n",
      "ep 2840: ep_len:193 episode reward: total was 92.000000. running mean: -36.691117\n",
      "ep 2840: ep_len:1128 episode reward: total was -8.570000. running mean: -36.409906\n",
      "ep 2840: ep_len:2740 episode reward: total was -61.680000. running mean: -36.662606\n",
      "ep 2840: ep_len:59 episode reward: total was 28.000000. running mean: -36.015980\n",
      "epsilon:0.009992 episode_count: 42759. steps_count: 45980480.000000\n",
      "ep 2841: ep_len:598 episode reward: total was 10.090000. running mean: -35.554921\n",
      "ep 2841: ep_len:707 episode reward: total was -19.630000. running mean: -35.395671\n",
      "ep 2841: ep_len:72 episode reward: total was 33.000000. running mean: -34.711715\n",
      "ep 2841: ep_len:3020 episode reward: total was -29.730000. running mean: -34.661898\n",
      "ep 2841: ep_len:656 episode reward: total was 4.480000. running mean: -34.270479\n",
      "ep 2841: ep_len:69 episode reward: total was 33.000000. running mean: -33.597774\n",
      "ep 2841: ep_len:42 episode reward: total was 19.500000. running mean: -33.066796\n",
      "ep 2841: ep_len:861 episode reward: total was 29.960000. running mean: -32.436528\n",
      "ep 2841: ep_len:328 episode reward: total was -4.900000. running mean: -32.161163\n",
      "ep 2841: ep_len:1146 episode reward: total was -50.600000. running mean: -32.345551\n",
      "ep 2841: ep_len:644 episode reward: total was 5.630000. running mean: -31.965796\n",
      "ep 2841: ep_len:976 episode reward: total was 2.190000. running mean: -31.624238\n",
      "ep 2841: ep_len:36 episode reward: total was 16.500000. running mean: -31.142995\n",
      "ep 2841: ep_len:648 episode reward: total was 14.020000. running mean: -30.691365\n",
      "ep 2841: ep_len:2863 episode reward: total was -20.840000. running mean: -30.592852\n",
      "epsilon:0.009992 episode_count: 42774. steps_count: 45993146.000000\n",
      "ep 2842: ep_len:645 episode reward: total was 10.070000. running mean: -30.186223\n",
      "ep 2842: ep_len:1287 episode reward: total was -52.950000. running mean: -30.413861\n",
      "ep 2842: ep_len:2971 episode reward: total was -73.720000. running mean: -30.846922\n",
      "ep 2842: ep_len:581 episode reward: total was -16.950000. running mean: -30.707953\n",
      "ep 2842: ep_len:1093 episode reward: total was -3.380000. running mean: -30.434674\n",
      "ep 2842: ep_len:343 episode reward: total was 11.140000. running mean: -30.018927\n",
      "ep 2842: ep_len:1136 episode reward: total was -41.790000. running mean: -30.136638\n",
      "ep 2842: ep_len:773 episode reward: total was 11.850000. running mean: -29.716771\n",
      "ep 2842: ep_len:682 episode reward: total was -14.560000. running mean: -29.565204\n",
      "ep 2842: ep_len:37 episode reward: total was 15.500000. running mean: -29.114551\n",
      "ep 2842: ep_len:1103 episode reward: total was -2.170000. running mean: -28.845106\n",
      "ep 2842: ep_len:2821 episode reward: total was -32.190000. running mean: -28.878555\n",
      "ep 2842: ep_len:29 episode reward: total was 13.000000. running mean: -28.459769\n",
      "epsilon:0.009992 episode_count: 42787. steps_count: 46006647.000000\n",
      "ep 2843: ep_len:1449 episode reward: total was 9.480000. running mean: -28.080372\n",
      "ep 2843: ep_len:721 episode reward: total was -26.240000. running mean: -28.061968\n",
      "ep 2843: ep_len:2957 episode reward: total was -64.650000. running mean: -28.427848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2843: ep_len:1390 episode reward: total was 18.720000. running mean: -27.956370\n",
      "ep 2843: ep_len:79 episode reward: total was 35.000000. running mean: -27.326806\n",
      "ep 2843: ep_len:1469 episode reward: total was -184.980000. running mean: -28.903338\n",
      "ep 2843: ep_len:605 episode reward: total was 17.210000. running mean: -28.442205\n",
      "ep 2843: ep_len:756 episode reward: total was -34.400000. running mean: -28.501783\n",
      "ep 2843: ep_len:7448 episode reward: total was 49.920000. running mean: -27.717565\n",
      "ep 2843: ep_len:835 episode reward: total was -2.380000. running mean: -27.464189\n",
      "ep 2843: ep_len:162 episode reward: total was 76.500000. running mean: -26.424547\n",
      "ep 2843: ep_len:48 episode reward: total was 19.500000. running mean: -25.965302\n",
      "ep 2843: ep_len:71 episode reward: total was 34.000000. running mean: -25.365649\n",
      "ep 2843: ep_len:627 episode reward: total was -3.480000. running mean: -25.146792\n",
      "ep 2843: ep_len:2890 episode reward: total was -0.860000. running mean: -24.903924\n",
      "epsilon:0.009992 episode_count: 42802. steps_count: 46028154.000000\n",
      "ep 2844: ep_len:682 episode reward: total was -6.480000. running mean: -24.719685\n",
      "ep 2844: ep_len:977 episode reward: total was 5.810000. running mean: -24.414388\n",
      "ep 2844: ep_len:49 episode reward: total was 23.000000. running mean: -23.940244\n",
      "ep 2844: ep_len:2896 episode reward: total was -25.770000. running mean: -23.958542\n",
      "ep 2844: ep_len:500 episode reward: total was 14.460000. running mean: -23.574357\n",
      "ep 2844: ep_len:64 episode reward: total was 30.500000. running mean: -23.033613\n",
      "ep 2844: ep_len:42 episode reward: total was 19.500000. running mean: -22.608277\n",
      "ep 2844: ep_len:668 episode reward: total was 14.290000. running mean: -22.239294\n",
      "ep 2844: ep_len:3574 episode reward: total was -47.660000. running mean: -22.493501\n",
      "ep 2844: ep_len:531 episode reward: total was -1.930000. running mean: -22.287866\n",
      "ep 2844: ep_len:7286 episode reward: total was 52.550000. running mean: -21.539487\n",
      "ep 2844: ep_len:1110 episode reward: total was -11.750000. running mean: -21.441593\n",
      "ep 2844: ep_len:77 episode reward: total was 37.000000. running mean: -20.857177\n",
      "ep 2844: ep_len:616 episode reward: total was 1.070000. running mean: -20.637905\n",
      "ep 2844: ep_len:2897 episode reward: total was -23.290000. running mean: -20.664426\n",
      "epsilon:0.009992 episode_count: 42817. steps_count: 46050123.000000\n",
      "ep 2845: ep_len:1426 episode reward: total was 13.170000. running mean: -20.326082\n",
      "ep 2845: ep_len:696 episode reward: total was -24.520000. running mean: -20.368021\n",
      "ep 2845: ep_len:3024 episode reward: total was -12.640000. running mean: -20.290741\n",
      "ep 2845: ep_len:846 episode reward: total was 50.660000. running mean: -19.581233\n",
      "ep 2845: ep_len:45 episode reward: total was 19.500000. running mean: -19.190421\n",
      "ep 2845: ep_len:1039 episode reward: total was -13.770000. running mean: -19.136217\n",
      "ep 2845: ep_len:653 episode reward: total was 14.200000. running mean: -18.802854\n",
      "ep 2845: ep_len:736 episode reward: total was -57.060000. running mean: -19.185426\n",
      "ep 2845: ep_len:828 episode reward: total was 64.250000. running mean: -18.351072\n",
      "ep 2845: ep_len:1519 episode reward: total was 2.110000. running mean: -18.146461\n",
      "ep 2845: ep_len:193 episode reward: total was 95.000000. running mean: -17.014996\n",
      "ep 2845: ep_len:583 episode reward: total was -12.130000. running mean: -16.966146\n",
      "ep 2845: ep_len:2850 episode reward: total was -34.900000. running mean: -17.145485\n",
      "ep 2845: ep_len:45 episode reward: total was 19.500000. running mean: -16.779030\n",
      "epsilon:0.009992 episode_count: 42831. steps_count: 46064606.000000\n",
      "ep 2846: ep_len:675 episode reward: total was -31.710000. running mean: -16.928340\n",
      "ep 2846: ep_len:709 episode reward: total was -22.370000. running mean: -16.982756\n",
      "ep 2846: ep_len:51 episode reward: total was 24.000000. running mean: -16.572929\n",
      "ep 2846: ep_len:3101 episode reward: total was -54.170000. running mean: -16.948899\n",
      "ep 2846: ep_len:842 episode reward: total was 64.710000. running mean: -16.132310\n",
      "ep 2846: ep_len:1025 episode reward: total was -60.620000. running mean: -16.577187\n",
      "ep 2846: ep_len:3739 episode reward: total was -155.310000. running mean: -17.964516\n",
      "ep 2846: ep_len:568 episode reward: total was -2.480000. running mean: -17.809670\n",
      "ep 2846: ep_len:767 episode reward: total was 14.520000. running mean: -17.486374\n",
      "ep 2846: ep_len:1508 episode reward: total was 15.550000. running mean: -17.156010\n",
      "ep 2846: ep_len:72 episode reward: total was 33.000000. running mean: -16.654450\n",
      "ep 2846: ep_len:178 episode reward: total was 87.500000. running mean: -15.612905\n",
      "ep 2846: ep_len:70 episode reward: total was 32.000000. running mean: -15.136776\n",
      "ep 2846: ep_len:1106 episode reward: total was -26.910000. running mean: -15.254508\n",
      "ep 2846: ep_len:2910 episode reward: total was -37.820000. running mean: -15.480163\n",
      "epsilon:0.009992 episode_count: 42846. steps_count: 46081927.000000\n",
      "ep 2847: ep_len:653 episode reward: total was -5.760000. running mean: -15.382962\n",
      "ep 2847: ep_len:714 episode reward: total was -5.230000. running mean: -15.281432\n",
      "ep 2847: ep_len:71 episode reward: total was 34.000000. running mean: -14.788618\n",
      "ep 2847: ep_len:3072 episode reward: total was -29.030000. running mean: -14.931032\n",
      "ep 2847: ep_len:819 episode reward: total was -9.360000. running mean: -14.875321\n",
      "ep 2847: ep_len:135 episode reward: total was 66.000000. running mean: -14.066568\n",
      "ep 2847: ep_len:1397 episode reward: total was 27.910000. running mean: -13.646802\n",
      "ep 2847: ep_len:3549 episode reward: total was -92.570000. running mean: -14.436034\n",
      "ep 2847: ep_len:1243 episode reward: total was -55.410000. running mean: -14.845774\n",
      "ep 2847: ep_len:888 episode reward: total was 62.920000. running mean: -14.068116\n",
      "ep 2847: ep_len:710 episode reward: total was -25.430000. running mean: -14.181735\n",
      "ep 2847: ep_len:1511 episode reward: total was 3.310000. running mean: -14.006818\n",
      "ep 2847: ep_len:2787 episode reward: total was -9.360000. running mean: -13.960350\n",
      "ep 2847: ep_len:38 episode reward: total was 17.500000. running mean: -13.645746\n",
      "epsilon:0.009992 episode_count: 42860. steps_count: 46099514.000000\n",
      "ep 2848: ep_len:943 episode reward: total was -141.110000. running mean: -14.920389\n",
      "ep 2848: ep_len:204 episode reward: total was 17.400000. running mean: -14.597185\n",
      "ep 2848: ep_len:75 episode reward: total was 36.000000. running mean: -14.091213\n",
      "ep 2848: ep_len:99 episode reward: total was 48.000000. running mean: -13.470301\n",
      "ep 2848: ep_len:766 episode reward: total was -16.750000. running mean: -13.503098\n",
      "ep 2848: ep_len:92 episode reward: total was 44.500000. running mean: -12.923067\n",
      "ep 2848: ep_len:70 episode reward: total was 32.000000. running mean: -12.473836\n",
      "ep 2848: ep_len:500 episode reward: total was -20.800000. running mean: -12.557098\n",
      "ep 2848: ep_len:662 episode reward: total was 16.620000. running mean: -12.265327\n",
      "ep 2848: ep_len:3898 episode reward: total was -1217.840000. running mean: -24.321074\n",
      "ep 2848: ep_len:704 episode reward: total was 1.390000. running mean: -24.063963\n",
      "ep 2848: ep_len:1043 episode reward: total was 13.570000. running mean: -23.687623\n",
      "ep 2848: ep_len:77 episode reward: total was 35.500000. running mean: -23.095747\n",
      "ep 2848: ep_len:116 episode reward: total was 53.500000. running mean: -22.329789\n",
      "ep 2848: ep_len:41 episode reward: total was 19.000000. running mean: -21.916492\n",
      "ep 2848: ep_len:684 episode reward: total was 13.860000. running mean: -21.558727\n",
      "ep 2848: ep_len:2833 episode reward: total was -8.560000. running mean: -21.428739\n",
      "epsilon:0.009992 episode_count: 42877. steps_count: 46112321.000000\n",
      "ep 2849: ep_len:1115 episode reward: total was -4.780000. running mean: -21.262252\n",
      "ep 2849: ep_len:1092 episode reward: total was -63.870000. running mean: -21.688329\n",
      "ep 2849: ep_len:3016 episode reward: total was -67.110000. running mean: -22.142546\n",
      "ep 2849: ep_len:1157 episode reward: total was -32.030000. running mean: -22.241421\n",
      "ep 2849: ep_len:51 episode reward: total was 24.000000. running mean: -21.779007\n",
      "ep 2849: ep_len:154 episode reward: total was 74.000000. running mean: -20.821216\n",
      "ep 2849: ep_len:1396 episode reward: total was 9.870000. running mean: -20.514304\n",
      "ep 2849: ep_len:627 episode reward: total was 30.870000. running mean: -20.000461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2849: ep_len:500 episode reward: total was -42.970000. running mean: -20.230157\n",
      "ep 2849: ep_len:758 episode reward: total was -2.200000. running mean: -20.049855\n",
      "ep 2849: ep_len:721 episode reward: total was -8.110000. running mean: -19.930457\n",
      "ep 2849: ep_len:37 episode reward: total was 17.000000. running mean: -19.561152\n",
      "ep 2849: ep_len:148 episode reward: total was 68.000000. running mean: -18.685540\n",
      "ep 2849: ep_len:129 episode reward: total was 61.500000. running mean: -17.883685\n",
      "ep 2849: ep_len:1082 episode reward: total was 6.640000. running mean: -17.638448\n",
      "ep 2849: ep_len:2731 episode reward: total was -22.440000. running mean: -17.686464\n",
      "epsilon:0.009992 episode_count: 42893. steps_count: 46127035.000000\n",
      "ep 2850: ep_len:663 episode reward: total was -3.230000. running mean: -17.541899\n",
      "ep 2850: ep_len:744 episode reward: total was -12.930000. running mean: -17.495780\n",
      "ep 2850: ep_len:51 episode reward: total was 22.500000. running mean: -17.095822\n",
      "ep 2850: ep_len:3039 episode reward: total was -59.410000. running mean: -17.518964\n",
      "ep 2850: ep_len:793 episode reward: total was -20.520000. running mean: -17.548974\n",
      "ep 2850: ep_len:94 episode reward: total was 44.000000. running mean: -16.933485\n",
      "ep 2850: ep_len:59 episode reward: total was 28.000000. running mean: -16.484150\n",
      "ep 2850: ep_len:678 episode reward: total was -1.470000. running mean: -16.334008\n",
      "ep 2850: ep_len:3776 episode reward: total was -93.650000. running mean: -17.107168\n",
      "ep 2850: ep_len:831 episode reward: total was -24.800000. running mean: -17.184097\n",
      "ep 2850: ep_len:825 episode reward: total was 56.500000. running mean: -16.447256\n",
      "ep 2850: ep_len:584 episode reward: total was -13.700000. running mean: -16.419783\n",
      "ep 2850: ep_len:65 episode reward: total was 29.500000. running mean: -15.960585\n",
      "ep 2850: ep_len:1137 episode reward: total was -7.990000. running mean: -15.880879\n",
      "ep 2850: ep_len:2817 episode reward: total was -11.840000. running mean: -15.840471\n",
      "ep 2850: ep_len:56 episode reward: total was 26.500000. running mean: -15.417066\n",
      "epsilon:0.009992 episode_count: 42909. steps_count: 46143247.000000\n",
      "ep 2851: ep_len:777 episode reward: total was -4.350000. running mean: -15.306395\n",
      "ep 2851: ep_len:946 episode reward: total was 20.680000. running mean: -14.946531\n",
      "ep 2851: ep_len:65 episode reward: total was 28.000000. running mean: -14.517066\n",
      "ep 2851: ep_len:2931 episode reward: total was -39.690000. running mean: -14.768795\n",
      "ep 2851: ep_len:694 episode reward: total was -11.210000. running mean: -14.733207\n",
      "ep 2851: ep_len:64 episode reward: total was 29.000000. running mean: -14.295875\n",
      "ep 2851: ep_len:868 episode reward: total was 50.400000. running mean: -13.648916\n",
      "ep 2851: ep_len:3610 episode reward: total was -10.150000. running mean: -13.613927\n",
      "ep 2851: ep_len:1247 episode reward: total was -75.570000. running mean: -14.233488\n",
      "ep 2851: ep_len:611 episode reward: total was 4.420000. running mean: -14.046953\n",
      "ep 2851: ep_len:500 episode reward: total was -17.690000. running mean: -14.083384\n",
      "ep 2851: ep_len:84 episode reward: total was 40.500000. running mean: -13.537550\n",
      "ep 2851: ep_len:819 episode reward: total was 14.620000. running mean: -13.255974\n",
      "ep 2851: ep_len:2857 episode reward: total was -0.240000. running mean: -13.125815\n",
      "epsilon:0.009992 episode_count: 42923. steps_count: 46159320.000000\n",
      "ep 2852: ep_len:572 episode reward: total was -24.230000. running mean: -13.236856\n",
      "ep 2852: ep_len:734 episode reward: total was -28.880000. running mean: -13.393288\n",
      "ep 2852: ep_len:3017 episode reward: total was -53.800000. running mean: -13.797355\n",
      "ep 2852: ep_len:657 episode reward: total was 3.370000. running mean: -13.625681\n",
      "ep 2852: ep_len:129 episode reward: total was 63.000000. running mean: -12.859425\n",
      "ep 2852: ep_len:987 episode reward: total was -17.170000. running mean: -12.902530\n",
      "ep 2852: ep_len:3595 episode reward: total was 3.630000. running mean: -12.737205\n",
      "ep 2852: ep_len:664 episode reward: total was -32.930000. running mean: -12.939133\n",
      "ep 2852: ep_len:737 episode reward: total was 9.780000. running mean: -12.711942\n",
      "ep 2852: ep_len:500 episode reward: total was 2.800000. running mean: -12.556822\n",
      "ep 2852: ep_len:169 episode reward: total was 77.000000. running mean: -11.661254\n",
      "ep 2852: ep_len:78 episode reward: total was 34.500000. running mean: -11.199642\n",
      "ep 2852: ep_len:3622 episode reward: total was -873.770000. running mean: -19.825345\n",
      "ep 2852: ep_len:2878 episode reward: total was 9.330000. running mean: -19.533792\n",
      "ep 2852: ep_len:58 episode reward: total was 26.000000. running mean: -19.078454\n",
      "epsilon:0.009992 episode_count: 42938. steps_count: 46177717.000000\n",
      "ep 2853: ep_len:1504 episode reward: total was 21.910000. running mean: -18.668569\n",
      "ep 2853: ep_len:989 episode reward: total was 12.790000. running mean: -18.353983\n",
      "ep 2853: ep_len:2938 episode reward: total was -69.370000. running mean: -18.864144\n",
      "ep 2853: ep_len:536 episode reward: total was 1.150000. running mean: -18.664002\n",
      "ep 2853: ep_len:638 episode reward: total was 23.420000. running mean: -18.243162\n",
      "ep 2853: ep_len:3706 episode reward: total was -4.140000. running mean: -18.102131\n",
      "ep 2853: ep_len:1223 episode reward: total was -145.500000. running mean: -19.376109\n",
      "ep 2853: ep_len:750 episode reward: total was 45.530000. running mean: -18.727048\n",
      "ep 2853: ep_len:1033 episode reward: total was 45.060000. running mean: -18.089178\n",
      "ep 2853: ep_len:81 episode reward: total was 39.000000. running mean: -17.518286\n",
      "ep 2853: ep_len:165 episode reward: total was 81.000000. running mean: -16.533103\n",
      "ep 2853: ep_len:22 episode reward: total was 9.500000. running mean: -16.272772\n",
      "ep 2853: ep_len:124 episode reward: total was 59.000000. running mean: -15.520044\n",
      "ep 2853: ep_len:820 episode reward: total was -49.800000. running mean: -15.862844\n",
      "ep 2853: ep_len:2880 episode reward: total was 4.490000. running mean: -15.659315\n",
      "epsilon:0.009992 episode_count: 42953. steps_count: 46195126.000000\n",
      "ep 2854: ep_len:1489 episode reward: total was 33.700000. running mean: -15.165722\n",
      "ep 2854: ep_len:766 episode reward: total was -15.740000. running mean: -15.171465\n",
      "ep 2854: ep_len:2997 episode reward: total was -13.770000. running mean: -15.157450\n",
      "ep 2854: ep_len:700 episode reward: total was -22.950000. running mean: -15.235376\n",
      "ep 2854: ep_len:132 episode reward: total was 63.000000. running mean: -14.453022\n",
      "ep 2854: ep_len:55 episode reward: total was 26.000000. running mean: -14.048492\n",
      "ep 2854: ep_len:500 episode reward: total was 25.690000. running mean: -13.651107\n",
      "ep 2854: ep_len:3746 episode reward: total was -8.310000. running mean: -13.597696\n",
      "ep 2854: ep_len:698 episode reward: total was -38.640000. running mean: -13.848119\n",
      "ep 2854: ep_len:7285 episode reward: total was 42.010000. running mean: -13.289538\n",
      "ep 2854: ep_len:717 episode reward: total was -6.130000. running mean: -13.217942\n",
      "ep 2854: ep_len:69 episode reward: total was 33.000000. running mean: -12.755763\n",
      "ep 2854: ep_len:951 episode reward: total was -56.310000. running mean: -13.191305\n",
      "ep 2854: ep_len:2830 episode reward: total was -3.910000. running mean: -13.098492\n",
      "epsilon:0.009992 episode_count: 42967. steps_count: 46218061.000000\n",
      "ep 2855: ep_len:1418 episode reward: total was 8.740000. running mean: -12.880107\n",
      "ep 2855: ep_len:961 episode reward: total was 21.850000. running mean: -12.532806\n",
      "ep 2855: ep_len:51 episode reward: total was 24.000000. running mean: -12.167478\n",
      "ep 2855: ep_len:2970 episode reward: total was -29.600000. running mean: -12.341803\n",
      "ep 2855: ep_len:683 episode reward: total was 14.450000. running mean: -12.073885\n",
      "ep 2855: ep_len:32 episode reward: total was 14.500000. running mean: -11.808147\n",
      "ep 2855: ep_len:123 episode reward: total was 58.500000. running mean: -11.105065\n",
      "ep 2855: ep_len:65 episode reward: total was 29.500000. running mean: -10.699014\n",
      "ep 2855: ep_len:500 episode reward: total was 8.760000. running mean: -10.504424\n",
      "ep 2855: ep_len:328 episode reward: total was 23.570000. running mean: -10.163680\n",
      "ep 2855: ep_len:550 episode reward: total was 13.570000. running mean: -9.926343\n",
      "ep 2855: ep_len:906 episode reward: total was 67.270000. running mean: -9.154380\n",
      "ep 2855: ep_len:1009 episode reward: total was 13.080000. running mean: -8.932036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2855: ep_len:106 episode reward: total was 50.000000. running mean: -8.342716\n",
      "ep 2855: ep_len:1163 episode reward: total was -10.720000. running mean: -8.366489\n",
      "ep 2855: ep_len:2857 episode reward: total was -6.120000. running mean: -8.344024\n",
      "epsilon:0.009992 episode_count: 42983. steps_count: 46231783.000000\n",
      "ep 2856: ep_len:616 episode reward: total was -31.870000. running mean: -8.579283\n",
      "ep 2856: ep_len:940 episode reward: total was 19.240000. running mean: -8.301091\n",
      "ep 2856: ep_len:2971 episode reward: total was -28.480000. running mean: -8.502880\n",
      "ep 2856: ep_len:700 episode reward: total was -0.330000. running mean: -8.421151\n",
      "ep 2856: ep_len:105 episode reward: total was 46.500000. running mean: -7.871939\n",
      "ep 2856: ep_len:1076 episode reward: total was -41.930000. running mean: -8.212520\n",
      "ep 2856: ep_len:308 episode reward: total was 32.370000. running mean: -7.806695\n",
      "ep 2856: ep_len:681 episode reward: total was -35.780000. running mean: -8.086428\n",
      "ep 2856: ep_len:796 episode reward: total was 40.990000. running mean: -7.595664\n",
      "ep 2856: ep_len:1027 episode reward: total was 13.100000. running mean: -7.388707\n",
      "ep 2856: ep_len:174 episode reward: total was 82.500000. running mean: -6.489820\n",
      "ep 2856: ep_len:52 episode reward: total was 24.500000. running mean: -6.179922\n",
      "ep 2856: ep_len:762 episode reward: total was -12.750000. running mean: -6.245622\n",
      "ep 2856: ep_len:2855 episode reward: total was 13.850000. running mean: -6.044666\n",
      "epsilon:0.009992 episode_count: 42997. steps_count: 46244846.000000\n",
      "ep 2857: ep_len:706 episode reward: total was 2.620000. running mean: -5.958020\n",
      "ep 2857: ep_len:754 episode reward: total was -34.270000. running mean: -6.241139\n",
      "ep 2857: ep_len:3104 episode reward: total was 21.430000. running mean: -5.964428\n",
      "ep 2857: ep_len:687 episode reward: total was 15.730000. running mean: -5.747484\n",
      "ep 2857: ep_len:752 episode reward: total was -7.800000. running mean: -5.768009\n",
      "ep 2857: ep_len:3695 episode reward: total was -9.760000. running mean: -5.807929\n",
      "ep 2857: ep_len:1307 episode reward: total was -102.940000. running mean: -6.779249\n",
      "ep 2857: ep_len:819 episode reward: total was 35.310000. running mean: -6.358357\n",
      "ep 2857: ep_len:729 episode reward: total was 35.880000. running mean: -5.935973\n",
      "ep 2857: ep_len:158 episode reward: total was 74.500000. running mean: -5.131614\n",
      "ep 2857: ep_len:771 episode reward: total was -10.640000. running mean: -5.186698\n",
      "ep 2857: ep_len:2871 episode reward: total was -4.700000. running mean: -5.181831\n",
      "epsilon:0.009992 episode_count: 43009. steps_count: 46261199.000000\n",
      "ep 2858: ep_len:981 episode reward: total was -68.130000. running mean: -5.811312\n",
      "ep 2858: ep_len:687 episode reward: total was -3.730000. running mean: -5.790499\n",
      "ep 2858: ep_len:2987 episode reward: total was -46.200000. running mean: -6.194594\n",
      "ep 2858: ep_len:505 episode reward: total was 28.360000. running mean: -5.849048\n",
      "ep 2858: ep_len:928 episode reward: total was 3.570000. running mean: -5.754858\n",
      "ep 2858: ep_len:3964 episode reward: total was -236.100000. running mean: -8.058309\n",
      "ep 2858: ep_len:558 episode reward: total was -31.960000. running mean: -8.297326\n",
      "ep 2858: ep_len:7273 episode reward: total was -56.910000. running mean: -8.783453\n",
      "ep 2858: ep_len:591 episode reward: total was 1.340000. running mean: -8.682218\n",
      "ep 2858: ep_len:67 episode reward: total was 30.500000. running mean: -8.290396\n",
      "ep 2858: ep_len:114 episode reward: total was -28.990000. running mean: -8.497392\n",
      "ep 2858: ep_len:1088 episode reward: total was -7.470000. running mean: -8.487118\n",
      "ep 2858: ep_len:2839 episode reward: total was 2.240000. running mean: -8.379847\n",
      "ep 2858: ep_len:74 episode reward: total was 35.500000. running mean: -7.941049\n",
      "epsilon:0.009992 episode_count: 43023. steps_count: 46283855.000000\n",
      "ep 2859: ep_len:653 episode reward: total was -4.750000. running mean: -7.909138\n",
      "ep 2859: ep_len:500 episode reward: total was 12.490000. running mean: -7.705147\n",
      "ep 2859: ep_len:3056 episode reward: total was -33.290000. running mean: -7.960995\n",
      "ep 2859: ep_len:602 episode reward: total was -40.610000. running mean: -8.287485\n",
      "ep 2859: ep_len:153 episode reward: total was 73.500000. running mean: -7.469610\n",
      "ep 2859: ep_len:52 episode reward: total was 23.000000. running mean: -7.164914\n",
      "ep 2859: ep_len:1017 episode reward: total was -45.550000. running mean: -7.548765\n",
      "ep 2859: ep_len:3794 episode reward: total was -107.180000. running mean: -8.545077\n",
      "ep 2859: ep_len:4159 episode reward: total was -793.620000. running mean: -16.395827\n",
      "ep 2859: ep_len:621 episode reward: total was -13.730000. running mean: -16.369168\n",
      "ep 2859: ep_len:500 episode reward: total was 38.430000. running mean: -15.821177\n",
      "ep 2859: ep_len:844 episode reward: total was 11.630000. running mean: -15.546665\n",
      "ep 2859: ep_len:2799 episode reward: total was -21.580000. running mean: -15.606998\n",
      "epsilon:0.009992 episode_count: 43036. steps_count: 46302605.000000\n",
      "ep 2860: ep_len:1058 episode reward: total was -3.640000. running mean: -15.487328\n",
      "ep 2860: ep_len:1228 episode reward: total was -55.560000. running mean: -15.888055\n",
      "ep 2860: ep_len:2900 episode reward: total was -22.040000. running mean: -15.949575\n",
      "ep 2860: ep_len:559 episode reward: total was -2.040000. running mean: -15.810479\n",
      "ep 2860: ep_len:96 episode reward: total was 45.000000. running mean: -15.202374\n",
      "ep 2860: ep_len:108 episode reward: total was 49.500000. running mean: -14.555350\n",
      "ep 2860: ep_len:625 episode reward: total was 10.890000. running mean: -14.300897\n",
      "ep 2860: ep_len:648 episode reward: total was 20.640000. running mean: -13.951488\n",
      "ep 2860: ep_len:536 episode reward: total was -40.260000. running mean: -14.214573\n",
      "ep 2860: ep_len:687 episode reward: total was 8.080000. running mean: -13.991627\n",
      "ep 2860: ep_len:1534 episode reward: total was -12.990000. running mean: -13.981611\n",
      "ep 2860: ep_len:140 episode reward: total was 67.000000. running mean: -13.171795\n",
      "ep 2860: ep_len:770 episode reward: total was -30.850000. running mean: -13.348577\n",
      "ep 2860: ep_len:2741 episode reward: total was -27.210000. running mean: -13.487191\n",
      "ep 2860: ep_len:50 episode reward: total was 23.500000. running mean: -13.117319\n",
      "epsilon:0.009992 episode_count: 43051. steps_count: 46316285.000000\n",
      "ep 2861: ep_len:1004 episode reward: total was -67.350000. running mean: -13.659646\n",
      "ep 2861: ep_len:216 episode reward: total was -1.520000. running mean: -13.538250\n",
      "ep 2861: ep_len:3075 episode reward: total was 0.030000. running mean: -13.402567\n",
      "ep 2861: ep_len:594 episode reward: total was -2.310000. running mean: -13.291641\n",
      "ep 2861: ep_len:97 episode reward: total was 44.000000. running mean: -12.718725\n",
      "ep 2861: ep_len:36 episode reward: total was 16.500000. running mean: -12.426538\n",
      "ep 2861: ep_len:500 episode reward: total was 29.240000. running mean: -12.009872\n",
      "ep 2861: ep_len:3869 episode reward: total was -160.970000. running mean: -13.499474\n",
      "ep 2861: ep_len:855 episode reward: total was 12.480000. running mean: -13.239679\n",
      "ep 2861: ep_len:841 episode reward: total was 38.480000. running mean: -12.722482\n",
      "ep 2861: ep_len:867 episode reward: total was 8.960000. running mean: -12.505657\n",
      "ep 2861: ep_len:118 episode reward: total was 57.500000. running mean: -11.805601\n",
      "ep 2861: ep_len:65 episode reward: total was 28.000000. running mean: -11.407545\n",
      "ep 2861: ep_len:72 episode reward: total was 33.000000. running mean: -10.963469\n",
      "ep 2861: ep_len:701 episode reward: total was 31.550000. running mean: -10.538335\n",
      "ep 2861: ep_len:23 episode reward: total was 10.000000. running mean: -10.332951\n",
      "ep 2861: ep_len:74 episode reward: total was 32.500000. running mean: -9.904622\n",
      "epsilon:0.009992 episode_count: 43068. steps_count: 46329292.000000\n",
      "ep 2862: ep_len:802 episode reward: total was -6.210000. running mean: -9.867675\n",
      "ep 2862: ep_len:988 episode reward: total was 1.630000. running mean: -9.752699\n",
      "ep 2862: ep_len:37 episode reward: total was 17.000000. running mean: -9.485172\n",
      "ep 2862: ep_len:2944 episode reward: total was -46.970000. running mean: -9.860020\n",
      "ep 2862: ep_len:1188 episode reward: total was -23.640000. running mean: -9.997820\n",
      "ep 2862: ep_len:61 episode reward: total was 27.500000. running mean: -9.622842\n",
      "ep 2862: ep_len:157 episode reward: total was 77.000000. running mean: -8.756613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2862: ep_len:60 episode reward: total was 28.500000. running mean: -8.384047\n",
      "ep 2862: ep_len:1006 episode reward: total was -12.330000. running mean: -8.423507\n",
      "ep 2862: ep_len:330 episode reward: total was 26.530000. running mean: -8.073972\n",
      "ep 2862: ep_len:525 episode reward: total was 2.050000. running mean: -7.972732\n",
      "ep 2862: ep_len:712 episode reward: total was 1.870000. running mean: -7.874304\n",
      "ep 2862: ep_len:718 episode reward: total was -28.440000. running mean: -8.079961\n",
      "ep 2862: ep_len:75 episode reward: total was 36.000000. running mean: -7.639162\n",
      "ep 2862: ep_len:222 episode reward: total was 106.500000. running mean: -6.497770\n",
      "ep 2862: ep_len:109 episode reward: total was 50.000000. running mean: -5.932792\n",
      "ep 2862: ep_len:634 episode reward: total was -3.140000. running mean: -5.904865\n",
      "ep 2862: ep_len:2778 episode reward: total was -17.010000. running mean: -6.015916\n",
      "epsilon:0.009992 episode_count: 43086. steps_count: 46342638.000000\n",
      "ep 2863: ep_len:865 episode reward: total was 6.560000. running mean: -5.890157\n",
      "ep 2863: ep_len:761 episode reward: total was 7.720000. running mean: -5.754055\n",
      "ep 2863: ep_len:3022 episode reward: total was -394.000000. running mean: -9.636515\n",
      "ep 2863: ep_len:770 episode reward: total was -1.560000. running mean: -9.555750\n",
      "ep 2863: ep_len:66 episode reward: total was 31.500000. running mean: -9.145192\n",
      "ep 2863: ep_len:100 episode reward: total was 47.000000. running mean: -8.583740\n",
      "ep 2863: ep_len:990 episode reward: total was 60.780000. running mean: -7.890103\n",
      "ep 2863: ep_len:3801 episode reward: total was -39.560000. running mean: -8.206802\n",
      "ep 2863: ep_len:633 episode reward: total was 36.710000. running mean: -7.757634\n",
      "ep 2863: ep_len:912 episode reward: total was 64.730000. running mean: -7.032757\n",
      "ep 2863: ep_len:545 episode reward: total was 17.570000. running mean: -6.786730\n",
      "ep 2863: ep_len:113 episode reward: total was 53.500000. running mean: -6.183862\n",
      "ep 2863: ep_len:109 episode reward: total was 50.000000. running mean: -5.622024\n",
      "ep 2863: ep_len:1535 episode reward: total was 0.730000. running mean: -5.558504\n",
      "ep 2863: ep_len:2871 episode reward: total was -22.450000. running mean: -5.727419\n",
      "epsilon:0.009992 episode_count: 43101. steps_count: 46359731.000000\n",
      "ep 2864: ep_len:836 episode reward: total was -16.050000. running mean: -5.830644\n",
      "ep 2864: ep_len:938 episode reward: total was 22.280000. running mean: -5.549538\n",
      "ep 2864: ep_len:50 episode reward: total was 23.500000. running mean: -5.259043\n",
      "ep 2864: ep_len:3096 episode reward: total was 8.900000. running mean: -5.117452\n",
      "ep 2864: ep_len:1235 episode reward: total was -44.380000. running mean: -5.510078\n",
      "ep 2864: ep_len:960 episode reward: total was -14.840000. running mean: -5.603377\n",
      "ep 2864: ep_len:325 episode reward: total was 9.520000. running mean: -5.452143\n",
      "ep 2864: ep_len:511 episode reward: total was -21.320000. running mean: -5.610822\n",
      "ep 2864: ep_len:855 episode reward: total was 18.010000. running mean: -5.374613\n",
      "ep 2864: ep_len:1460 episode reward: total was 9.990000. running mean: -5.220967\n",
      "ep 2864: ep_len:79 episode reward: total was 36.500000. running mean: -4.803758\n",
      "ep 2864: ep_len:109 episode reward: total was 51.010000. running mean: -4.245620\n",
      "ep 2864: ep_len:754 episode reward: total was -79.490000. running mean: -4.998064\n",
      "ep 2864: ep_len:2787 episode reward: total was -10.280000. running mean: -5.050883\n",
      "epsilon:0.009992 episode_count: 43115. steps_count: 46373726.000000\n",
      "ep 2865: ep_len:789 episode reward: total was -32.680000. running mean: -5.327174\n",
      "ep 2865: ep_len:680 episode reward: total was -10.500000. running mean: -5.378903\n",
      "ep 2865: ep_len:2901 episode reward: total was -31.640000. running mean: -5.641514\n",
      "ep 2865: ep_len:857 episode reward: total was 23.640000. running mean: -5.348698\n",
      "ep 2865: ep_len:66 episode reward: total was 30.000000. running mean: -4.995211\n",
      "ep 2865: ep_len:979 episode reward: total was -25.390000. running mean: -5.199159\n",
      "ep 2865: ep_len:641 episode reward: total was 31.010000. running mean: -4.837068\n",
      "ep 2865: ep_len:933 episode reward: total was -23.650000. running mean: -5.025197\n",
      "ep 2865: ep_len:583 episode reward: total was 4.130000. running mean: -4.933645\n",
      "ep 2865: ep_len:636 episode reward: total was 8.050000. running mean: -4.803809\n",
      "ep 2865: ep_len:95 episode reward: total was 44.500000. running mean: -4.310771\n",
      "ep 2865: ep_len:206 episode reward: total was 94.000000. running mean: -3.327663\n",
      "ep 2865: ep_len:64 episode reward: total was 29.000000. running mean: -3.004386\n",
      "ep 2865: ep_len:95 episode reward: total was 43.000000. running mean: -2.544342\n",
      "ep 2865: ep_len:1039 episode reward: total was -45.120000. running mean: -2.970099\n",
      "ep 2865: ep_len:2800 episode reward: total was -13.730000. running mean: -3.077698\n",
      "ep 2865: ep_len:61 episode reward: total was 26.000000. running mean: -2.786921\n",
      "epsilon:0.009992 episode_count: 43132. steps_count: 46387151.000000\n",
      "ep 2866: ep_len:625 episode reward: total was 25.510000. running mean: -2.503952\n",
      "ep 2866: ep_len:754 episode reward: total was 4.470000. running mean: -2.434212\n",
      "ep 2866: ep_len:78 episode reward: total was 37.500000. running mean: -2.034870\n",
      "ep 2866: ep_len:3055 episode reward: total was -57.180000. running mean: -2.586321\n",
      "ep 2866: ep_len:936 episode reward: total was 80.440000. running mean: -1.756058\n",
      "ep 2866: ep_len:54 episode reward: total was 25.500000. running mean: -1.483498\n",
      "ep 2866: ep_len:148 episode reward: total was 72.500000. running mean: -0.743663\n",
      "ep 2866: ep_len:44 episode reward: total was 19.000000. running mean: -0.546226\n",
      "ep 2866: ep_len:630 episode reward: total was 3.620000. running mean: -0.504564\n",
      "ep 2866: ep_len:663 episode reward: total was 25.780000. running mean: -0.241718\n",
      "ep 2866: ep_len:671 episode reward: total was -23.820000. running mean: -0.477501\n",
      "ep 2866: ep_len:737 episode reward: total was 50.690000. running mean: 0.034174\n",
      "ep 2866: ep_len:660 episode reward: total was -8.720000. running mean: -0.053368\n",
      "ep 2866: ep_len:44 episode reward: total was 20.500000. running mean: 0.152166\n",
      "ep 2866: ep_len:1012 episode reward: total was 8.730000. running mean: 0.237944\n",
      "ep 2866: ep_len:2799 episode reward: total was -2.880000. running mean: 0.206765\n",
      "ep 2866: ep_len:38 episode reward: total was 17.500000. running mean: 0.379697\n",
      "epsilon:0.009992 episode_count: 43149. steps_count: 46400099.000000\n",
      "ep 2867: ep_len:705 episode reward: total was -30.920000. running mean: 0.066700\n",
      "ep 2867: ep_len:743 episode reward: total was -35.330000. running mean: -0.287267\n",
      "ep 2867: ep_len:2987 episode reward: total was 21.980000. running mean: -0.064594\n",
      "ep 2867: ep_len:636 episode reward: total was 6.110000. running mean: -0.002848\n",
      "ep 2867: ep_len:77 episode reward: total was 35.500000. running mean: 0.352180\n",
      "ep 2867: ep_len:992 episode reward: total was -51.800000. running mean: -0.169341\n",
      "ep 2867: ep_len:367 episode reward: total was 19.310000. running mean: 0.025452\n",
      "ep 2867: ep_len:4168 episode reward: total was -670.570000. running mean: -6.680503\n",
      "ep 2867: ep_len:802 episode reward: total was -0.240000. running mean: -6.616098\n",
      "ep 2867: ep_len:1125 episode reward: total was -28.190000. running mean: -6.831837\n",
      "ep 2867: ep_len:68 episode reward: total was 32.500000. running mean: -6.438518\n",
      "ep 2867: ep_len:83 episode reward: total was 40.000000. running mean: -5.974133\n",
      "ep 2867: ep_len:687 episode reward: total was -6.430000. running mean: -5.978692\n",
      "ep 2867: ep_len:2809 episode reward: total was -8.070000. running mean: -5.999605\n",
      "epsilon:0.009992 episode_count: 43163. steps_count: 46416348.000000\n",
      "ep 2868: ep_len:986 episode reward: total was -99.740000. running mean: -6.937009\n",
      "ep 2868: ep_len:965 episode reward: total was 27.630000. running mean: -6.591339\n",
      "ep 2868: ep_len:3066 episode reward: total was -115.870000. running mean: -7.684125\n",
      "ep 2868: ep_len:671 episode reward: total was -11.500000. running mean: -7.722284\n",
      "ep 2868: ep_len:121 episode reward: total was 53.000000. running mean: -7.115061\n",
      "ep 2868: ep_len:599 episode reward: total was -5.380000. running mean: -7.097711\n",
      "ep 2868: ep_len:3717 episode reward: total was -61.600000. running mean: -7.642733\n",
      "ep 2868: ep_len:519 episode reward: total was 3.980000. running mean: -7.526506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2868: ep_len:806 episode reward: total was 10.780000. running mean: -7.343441\n",
      "ep 2868: ep_len:500 episode reward: total was 29.460000. running mean: -6.975407\n",
      "ep 2868: ep_len:60 episode reward: total was 27.000000. running mean: -6.635653\n",
      "ep 2868: ep_len:673 episode reward: total was 7.520000. running mean: -6.494096\n",
      "ep 2868: ep_len:2912 episode reward: total was -8.290000. running mean: -6.512055\n",
      "epsilon:0.009992 episode_count: 43176. steps_count: 46431943.000000\n",
      "ep 2869: ep_len:670 episode reward: total was 18.500000. running mean: -6.261935\n",
      "ep 2869: ep_len:656 episode reward: total was -20.880000. running mean: -6.408115\n",
      "ep 2869: ep_len:2954 episode reward: total was -28.890000. running mean: -6.632934\n",
      "ep 2869: ep_len:500 episode reward: total was 29.430000. running mean: -6.272305\n",
      "ep 2869: ep_len:57 episode reward: total was 27.000000. running mean: -5.939582\n",
      "ep 2869: ep_len:910 episode reward: total was 44.870000. running mean: -5.431486\n",
      "ep 2869: ep_len:346 episode reward: total was 11.110000. running mean: -5.266071\n",
      "ep 2869: ep_len:841 episode reward: total was 21.250000. running mean: -5.000910\n",
      "ep 2869: ep_len:7348 episode reward: total was 41.580000. running mean: -4.535101\n",
      "ep 2869: ep_len:711 episode reward: total was -4.470000. running mean: -4.534450\n",
      "ep 2869: ep_len:176 episode reward: total was 82.000000. running mean: -3.669106\n",
      "ep 2869: ep_len:1504 episode reward: total was -17.970000. running mean: -3.812115\n",
      "ep 2869: ep_len:2832 episode reward: total was -0.010000. running mean: -3.774093\n",
      "epsilon:0.009992 episode_count: 43189. steps_count: 46451448.000000\n",
      "ep 2870: ep_len:1103 episode reward: total was -34.560000. running mean: -4.081952\n",
      "ep 2870: ep_len:771 episode reward: total was -15.690000. running mean: -4.198033\n",
      "ep 2870: ep_len:37 episode reward: total was 15.500000. running mean: -4.001053\n",
      "ep 2870: ep_len:3049 episode reward: total was -43.580000. running mean: -4.396842\n",
      "ep 2870: ep_len:500 episode reward: total was 13.230000. running mean: -4.220574\n",
      "ep 2870: ep_len:622 episode reward: total was -15.560000. running mean: -4.333968\n",
      "ep 2870: ep_len:646 episode reward: total was 0.750000. running mean: -4.283128\n",
      "ep 2870: ep_len:1545 episode reward: total was -70.880000. running mean: -4.949097\n",
      "ep 2870: ep_len:704 episode reward: total was 45.310000. running mean: -4.446506\n",
      "ep 2870: ep_len:1583 episode reward: total was -19.020000. running mean: -4.592241\n",
      "ep 2870: ep_len:52 episode reward: total was 23.000000. running mean: -4.316319\n",
      "ep 2870: ep_len:975 episode reward: total was -390.620000. running mean: -8.179355\n",
      "ep 2870: ep_len:2843 episode reward: total was -1392.670000. running mean: -22.024262\n",
      "ep 2870: ep_len:54 episode reward: total was 25.500000. running mean: -21.549019\n",
      "epsilon:0.009992 episode_count: 43203. steps_count: 46465932.000000\n",
      "ep 2871: ep_len:1039 episode reward: total was -3.920000. running mean: -21.372729\n",
      "ep 2871: ep_len:737 episode reward: total was 2.140000. running mean: -21.137602\n",
      "ep 2871: ep_len:2870 episode reward: total was -13.310000. running mean: -21.059326\n",
      "ep 2871: ep_len:758 episode reward: total was -11.690000. running mean: -20.965632\n",
      "ep 2871: ep_len:1070 episode reward: total was -28.770000. running mean: -21.043676\n",
      "ep 2871: ep_len:3628 episode reward: total was -52.390000. running mean: -21.357139\n",
      "ep 2871: ep_len:1280 episode reward: total was -95.990000. running mean: -22.103468\n",
      "ep 2871: ep_len:711 episode reward: total was 31.660000. running mean: -21.565833\n",
      "ep 2871: ep_len:654 episode reward: total was 14.450000. running mean: -21.205675\n",
      "ep 2871: ep_len:106 episode reward: total was 50.000000. running mean: -20.493618\n",
      "ep 2871: ep_len:983 episode reward: total was -68.110000. running mean: -20.969782\n",
      "ep 2871: ep_len:2928 episode reward: total was -37.310000. running mean: -21.133184\n",
      "epsilon:0.009992 episode_count: 43215. steps_count: 46482696.000000\n",
      "ep 2872: ep_len:955 episode reward: total was -72.330000. running mean: -21.645152\n",
      "ep 2872: ep_len:721 episode reward: total was -7.100000. running mean: -21.499701\n",
      "ep 2872: ep_len:2972 episode reward: total was -24.280000. running mean: -21.527504\n",
      "ep 2872: ep_len:842 episode reward: total was 12.940000. running mean: -21.182829\n",
      "ep 2872: ep_len:66 episode reward: total was 31.500000. running mean: -20.656000\n",
      "ep 2872: ep_len:137 episode reward: total was 64.000000. running mean: -19.809440\n",
      "ep 2872: ep_len:93 episode reward: total was 45.000000. running mean: -19.161346\n",
      "ep 2872: ep_len:58 episode reward: total was 27.500000. running mean: -18.694733\n",
      "ep 2872: ep_len:958 episode reward: total was -13.820000. running mean: -18.645985\n",
      "ep 2872: ep_len:3629 episode reward: total was -66.520000. running mean: -19.124725\n",
      "ep 2872: ep_len:719 episode reward: total was -5.090000. running mean: -18.984378\n",
      "ep 2872: ep_len:771 episode reward: total was 53.790000. running mean: -18.256634\n",
      "ep 2872: ep_len:500 episode reward: total was 15.920000. running mean: -17.914868\n",
      "ep 2872: ep_len:129 episode reward: total was 63.000000. running mean: -17.105719\n",
      "ep 2872: ep_len:679 episode reward: total was 16.000000. running mean: -16.774662\n",
      "ep 2872: ep_len:2821 episode reward: total was -19.790000. running mean: -16.804816\n",
      "epsilon:0.009992 episode_count: 43231. steps_count: 46498746.000000\n",
      "ep 2873: ep_len:1093 episode reward: total was -12.470000. running mean: -16.761467\n",
      "ep 2873: ep_len:1020 episode reward: total was 35.110000. running mean: -16.242753\n",
      "ep 2873: ep_len:2930 episode reward: total was 4.630000. running mean: -16.034025\n",
      "ep 2873: ep_len:719 episode reward: total was -165.400000. running mean: -17.527685\n",
      "ep 2873: ep_len:92 episode reward: total was 43.000000. running mean: -16.922408\n",
      "ep 2873: ep_len:74 episode reward: total was 35.500000. running mean: -16.398184\n",
      "ep 2873: ep_len:602 episode reward: total was -0.220000. running mean: -16.236402\n",
      "ep 2873: ep_len:349 episode reward: total was 4.100000. running mean: -16.033038\n",
      "ep 2873: ep_len:851 episode reward: total was 2.340000. running mean: -15.849308\n",
      "ep 2873: ep_len:715 episode reward: total was -20.290000. running mean: -15.893715\n",
      "ep 2873: ep_len:500 episode reward: total was 9.340000. running mean: -15.641378\n",
      "ep 2873: ep_len:112 episode reward: total was 54.500000. running mean: -14.939964\n",
      "ep 2873: ep_len:35 episode reward: total was 16.000000. running mean: -14.630564\n",
      "ep 2873: ep_len:1102 episode reward: total was -24.990000. running mean: -14.734159\n",
      "ep 2873: ep_len:2700 episode reward: total was -8.490000. running mean: -14.671717\n",
      "ep 2873: ep_len:67 episode reward: total was 29.000000. running mean: -14.235000\n",
      "epsilon:0.009992 episode_count: 43247. steps_count: 46511707.000000\n",
      "ep 2874: ep_len:601 episode reward: total was -11.330000. running mean: -14.205950\n",
      "ep 2874: ep_len:569 episode reward: total was -0.970000. running mean: -14.073590\n",
      "ep 2874: ep_len:3075 episode reward: total was -7.940000. running mean: -14.012254\n",
      "ep 2874: ep_len:546 episode reward: total was -20.970000. running mean: -14.081832\n",
      "ep 2874: ep_len:95 episode reward: total was 46.000000. running mean: -13.481013\n",
      "ep 2874: ep_len:729 episode reward: total was -0.960000. running mean: -13.355803\n",
      "ep 2874: ep_len:343 episode reward: total was 25.650000. running mean: -12.965745\n",
      "ep 2874: ep_len:749 episode reward: total was -33.150000. running mean: -13.167588\n",
      "ep 2874: ep_len:706 episode reward: total was 15.810000. running mean: -12.877812\n",
      "ep 2874: ep_len:917 episode reward: total was 16.000000. running mean: -12.589034\n",
      "ep 2874: ep_len:37 episode reward: total was 17.000000. running mean: -12.293144\n",
      "ep 2874: ep_len:60 episode reward: total was 27.000000. running mean: -11.900212\n",
      "ep 2874: ep_len:1039 episode reward: total was 26.360000. running mean: -11.517610\n",
      "ep 2874: ep_len:2783 episode reward: total was 9.240000. running mean: -11.310034\n",
      "epsilon:0.009992 episode_count: 43261. steps_count: 46523956.000000\n",
      "ep 2875: ep_len:1131 episode reward: total was -8.050000. running mean: -11.277434\n",
      "ep 2875: ep_len:925 episode reward: total was 30.840000. running mean: -10.856259\n",
      "ep 2875: ep_len:51 episode reward: total was 22.500000. running mean: -10.522697\n",
      "ep 2875: ep_len:89 episode reward: total was 43.000000. running mean: -9.987470\n",
      "ep 2875: ep_len:885 episode reward: total was -9.200000. running mean: -9.979595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2875: ep_len:61 episode reward: total was 27.500000. running mean: -9.604799\n",
      "ep 2875: ep_len:1461 episode reward: total was -85.360000. running mean: -10.362351\n",
      "ep 2875: ep_len:315 episode reward: total was 19.890000. running mean: -10.059827\n",
      "ep 2875: ep_len:939 episode reward: total was -112.980000. running mean: -11.089029\n",
      "ep 2875: ep_len:691 episode reward: total was 15.080000. running mean: -10.827339\n",
      "ep 2875: ep_len:500 episode reward: total was 43.420000. running mean: -10.284866\n",
      "ep 2875: ep_len:94 episode reward: total was 45.500000. running mean: -9.727017\n",
      "ep 2875: ep_len:1192 episode reward: total was -3.690000. running mean: -9.666647\n",
      "ep 2875: ep_len:2801 episode reward: total was 9.270000. running mean: -9.477280\n",
      "epsilon:0.009992 episode_count: 43275. steps_count: 46535091.000000\n",
      "ep 2876: ep_len:1151 episode reward: total was -68.920000. running mean: -10.071707\n",
      "ep 2876: ep_len:1634 episode reward: total was -56.450000. running mean: -10.535490\n",
      "ep 2876: ep_len:2995 episode reward: total was -3.690000. running mean: -10.467035\n",
      "ep 2876: ep_len:2773 episode reward: total was -209.790000. running mean: -12.460265\n",
      "ep 2876: ep_len:59 episode reward: total was 26.500000. running mean: -12.070662\n",
      "ep 2876: ep_len:69 episode reward: total was 31.500000. running mean: -11.634956\n",
      "ep 2876: ep_len:58 episode reward: total was 27.500000. running mean: -11.243606\n",
      "ep 2876: ep_len:1042 episode reward: total was 18.860000. running mean: -10.942570\n",
      "ep 2876: ep_len:639 episode reward: total was 18.560000. running mean: -10.647545\n",
      "ep 2876: ep_len:575 episode reward: total was -5.500000. running mean: -10.596069\n",
      "ep 2876: ep_len:688 episode reward: total was 24.850000. running mean: -10.241608\n",
      "ep 2876: ep_len:1510 episode reward: total was -3.680000. running mean: -10.175992\n",
      "ep 2876: ep_len:73 episode reward: total was 33.500000. running mean: -9.739232\n",
      "ep 2876: ep_len:103 episode reward: total was 48.500000. running mean: -9.156840\n",
      "ep 2876: ep_len:38 episode reward: total was 17.500000. running mean: -8.890272\n",
      "ep 2876: ep_len:1481 episode reward: total was -8.380000. running mean: -8.885169\n",
      "ep 2876: ep_len:2829 episode reward: total was 7.770000. running mean: -8.718617\n",
      "ep 2876: ep_len:48 episode reward: total was 21.000000. running mean: -8.421431\n",
      "epsilon:0.009992 episode_count: 43293. steps_count: 46552856.000000\n",
      "ep 2877: ep_len:847 episode reward: total was -27.650000. running mean: -8.613717\n",
      "ep 2877: ep_len:1632 episode reward: total was -86.900000. running mean: -9.396580\n",
      "ep 2877: ep_len:3051 episode reward: total was -20.670000. running mean: -9.509314\n",
      "ep 2877: ep_len:718 episode reward: total was 13.670000. running mean: -9.277521\n",
      "ep 2877: ep_len:114 episode reward: total was 55.500000. running mean: -8.629745\n",
      "ep 2877: ep_len:65 episode reward: total was 31.000000. running mean: -8.233448\n",
      "ep 2877: ep_len:500 episode reward: total was 21.470000. running mean: -7.936414\n",
      "ep 2877: ep_len:338 episode reward: total was 15.040000. running mean: -7.706649\n",
      "ep 2877: ep_len:670 episode reward: total was -13.150000. running mean: -7.761083\n",
      "ep 2877: ep_len:860 episode reward: total was 43.720000. running mean: -7.246272\n",
      "ep 2877: ep_len:1118 episode reward: total was -22.320000. running mean: -7.397009\n",
      "ep 2877: ep_len:82 episode reward: total was 39.500000. running mean: -6.928039\n",
      "ep 2877: ep_len:153 episode reward: total was 70.500000. running mean: -6.153759\n",
      "ep 2877: ep_len:111 episode reward: total was 52.500000. running mean: -5.567221\n",
      "ep 2877: ep_len:1003 episode reward: total was 19.910000. running mean: -5.312449\n",
      "ep 2877: ep_len:2805 episode reward: total was 7.320000. running mean: -5.186125\n",
      "ep 2877: ep_len:52 episode reward: total was 23.000000. running mean: -4.904263\n",
      "epsilon:0.009992 episode_count: 43310. steps_count: 46566975.000000\n",
      "ep 2878: ep_len:1107 episode reward: total was -5.900000. running mean: -4.914221\n",
      "ep 2878: ep_len:500 episode reward: total was 7.140000. running mean: -4.793678\n",
      "ep 2878: ep_len:73 episode reward: total was 33.500000. running mean: -4.410742\n",
      "ep 2878: ep_len:3011 episode reward: total was -22.780000. running mean: -4.594434\n",
      "ep 2878: ep_len:509 episode reward: total was -24.370000. running mean: -4.792190\n",
      "ep 2878: ep_len:80 episode reward: total was 37.000000. running mean: -4.374268\n",
      "ep 2878: ep_len:66 episode reward: total was 31.500000. running mean: -4.015525\n",
      "ep 2878: ep_len:1411 episode reward: total was -4.790000. running mean: -4.023270\n",
      "ep 2878: ep_len:3707 episode reward: total was -17.260000. running mean: -4.155637\n",
      "ep 2878: ep_len:1343 episode reward: total was -75.620000. running mean: -4.870281\n",
      "ep 2878: ep_len:7302 episode reward: total was 48.420000. running mean: -4.337378\n",
      "ep 2878: ep_len:576 episode reward: total was -19.460000. running mean: -4.488604\n",
      "ep 2878: ep_len:95 episode reward: total was 46.000000. running mean: -3.983718\n",
      "ep 2878: ep_len:1117 episode reward: total was -8.190000. running mean: -4.025781\n",
      "ep 2878: ep_len:2871 episode reward: total was -4.970000. running mean: -4.035223\n",
      "epsilon:0.009992 episode_count: 43325. steps_count: 46590743.000000\n",
      "ep 2879: ep_len:1460 episode reward: total was 4.820000. running mean: -3.946671\n",
      "ep 2879: ep_len:500 episode reward: total was 8.790000. running mean: -3.819304\n",
      "ep 2879: ep_len:2951 episode reward: total was -21.160000. running mean: -3.992711\n",
      "ep 2879: ep_len:595 episode reward: total was -1.290000. running mean: -3.965684\n",
      "ep 2879: ep_len:36 episode reward: total was 16.500000. running mean: -3.761027\n",
      "ep 2879: ep_len:61 episode reward: total was 29.000000. running mean: -3.433417\n",
      "ep 2879: ep_len:881 episode reward: total was 0.560000. running mean: -3.393483\n",
      "ep 2879: ep_len:3983 episode reward: total was -169.390000. running mean: -5.053448\n",
      "ep 2879: ep_len:562 episode reward: total was -3.760000. running mean: -5.040514\n",
      "ep 2879: ep_len:688 episode reward: total was 16.670000. running mean: -4.823409\n",
      "ep 2879: ep_len:705 episode reward: total was -20.820000. running mean: -4.983374\n",
      "ep 2879: ep_len:881 episode reward: total was -22.830000. running mean: -5.161841\n",
      "ep 2879: ep_len:2820 episode reward: total was -0.340000. running mean: -5.113622\n",
      "ep 2879: ep_len:55 episode reward: total was 24.500000. running mean: -4.817486\n",
      "epsilon:0.009992 episode_count: 43339. steps_count: 46606921.000000\n",
      "ep 2880: ep_len:666 episode reward: total was 0.430000. running mean: -4.765011\n",
      "ep 2880: ep_len:682 episode reward: total was -21.630000. running mean: -4.933661\n",
      "ep 2880: ep_len:70 episode reward: total was 33.500000. running mean: -4.549325\n",
      "ep 2880: ep_len:3015 episode reward: total was -21.660000. running mean: -4.720431\n",
      "ep 2880: ep_len:645 episode reward: total was 13.100000. running mean: -4.542227\n",
      "ep 2880: ep_len:35 episode reward: total was 16.000000. running mean: -4.336805\n",
      "ep 2880: ep_len:86 episode reward: total was 38.500000. running mean: -3.908437\n",
      "ep 2880: ep_len:1006 episode reward: total was -58.790000. running mean: -4.457252\n",
      "ep 2880: ep_len:3544 episode reward: total was -184.440000. running mean: -6.257080\n",
      "ep 2880: ep_len:568 episode reward: total was -10.650000. running mean: -6.301009\n",
      "ep 2880: ep_len:849 episode reward: total was 34.860000. running mean: -5.889399\n",
      "ep 2880: ep_len:653 episode reward: total was -16.550000. running mean: -5.996005\n",
      "ep 2880: ep_len:68 episode reward: total was 32.500000. running mean: -5.611045\n",
      "ep 2880: ep_len:1001 episode reward: total was -52.100000. running mean: -6.075934\n",
      "ep 2880: ep_len:2825 episode reward: total was 10.150000. running mean: -5.913675\n",
      "ep 2880: ep_len:45 episode reward: total was 21.000000. running mean: -5.644538\n",
      "epsilon:0.009992 episode_count: 43355. steps_count: 46622679.000000\n",
      "ep 2881: ep_len:719 episode reward: total was -86.910000. running mean: -6.457193\n",
      "ep 2881: ep_len:1276 episode reward: total was -205.420000. running mean: -8.446821\n",
      "ep 2881: ep_len:2996 episode reward: total was -18.410000. running mean: -8.546453\n",
      "ep 2881: ep_len:500 episode reward: total was -4.710000. running mean: -8.508088\n",
      "ep 2881: ep_len:61 episode reward: total was 29.000000. running mean: -8.133007\n",
      "ep 2881: ep_len:99 episode reward: total was 46.500000. running mean: -7.586677\n",
      "ep 2881: ep_len:68 episode reward: total was 31.000000. running mean: -7.200810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2881: ep_len:1452 episode reward: total was -4.410000. running mean: -7.172902\n",
      "ep 2881: ep_len:3698 episode reward: total was -11.290000. running mean: -7.214073\n",
      "ep 2881: ep_len:669 episode reward: total was -29.840000. running mean: -7.440333\n",
      "ep 2881: ep_len:863 episode reward: total was 37.900000. running mean: -6.986929\n",
      "ep 2881: ep_len:3912 episode reward: total was -1106.320000. running mean: -17.980260\n",
      "ep 2881: ep_len:68 episode reward: total was 31.000000. running mean: -17.490457\n",
      "ep 2881: ep_len:594 episode reward: total was 1.730000. running mean: -17.298253\n",
      "ep 2881: ep_len:2916 episode reward: total was -38.010000. running mean: -17.505370\n",
      "epsilon:0.009992 episode_count: 43370. steps_count: 46642570.000000\n",
      "ep 2882: ep_len:1150 episode reward: total was 11.620000. running mean: -17.214117\n",
      "ep 2882: ep_len:500 episode reward: total was 9.430000. running mean: -16.947675\n",
      "ep 2882: ep_len:3002 episode reward: total was -77.990000. running mean: -17.558099\n",
      "ep 2882: ep_len:638 episode reward: total was 0.050000. running mean: -17.382018\n",
      "ep 2882: ep_len:50 episode reward: total was 23.500000. running mean: -16.973198\n",
      "ep 2882: ep_len:932 episode reward: total was 60.040000. running mean: -16.203066\n",
      "ep 2882: ep_len:3645 episode reward: total was -24.310000. running mean: -16.284135\n",
      "ep 2882: ep_len:565 episode reward: total was -58.150000. running mean: -16.702794\n",
      "ep 2882: ep_len:7360 episode reward: total was -187.360000. running mean: -18.409366\n",
      "ep 2882: ep_len:1572 episode reward: total was 7.910000. running mean: -18.146172\n",
      "ep 2882: ep_len:36 episode reward: total was 16.500000. running mean: -17.799710\n",
      "ep 2882: ep_len:770 episode reward: total was -53.500000. running mean: -18.156713\n",
      "ep 2882: ep_len:2774 episode reward: total was -33.470000. running mean: -18.309846\n",
      "epsilon:0.009992 episode_count: 43383. steps_count: 46665564.000000\n",
      "ep 2883: ep_len:640 episode reward: total was -34.170000. running mean: -18.468448\n",
      "ep 2883: ep_len:977 episode reward: total was -2.150000. running mean: -18.305263\n",
      "ep 2883: ep_len:3007 episode reward: total was -66.900000. running mean: -18.791210\n",
      "ep 2883: ep_len:534 episode reward: total was -14.020000. running mean: -18.743498\n",
      "ep 2883: ep_len:107 episode reward: total was 50.500000. running mean: -18.051063\n",
      "ep 2883: ep_len:73 episode reward: total was 32.000000. running mean: -17.550553\n",
      "ep 2883: ep_len:1030 episode reward: total was -31.000000. running mean: -17.685047\n",
      "ep 2883: ep_len:353 episode reward: total was 23.210000. running mean: -17.276097\n",
      "ep 2883: ep_len:1273 episode reward: total was -49.410000. running mean: -17.597436\n",
      "ep 2883: ep_len:756 episode reward: total was 31.500000. running mean: -17.106461\n",
      "ep 2883: ep_len:603 episode reward: total was -8.280000. running mean: -17.018197\n",
      "ep 2883: ep_len:76 episode reward: total was 36.500000. running mean: -16.483015\n",
      "ep 2883: ep_len:1090 episode reward: total was 4.210000. running mean: -16.276085\n",
      "ep 2883: ep_len:2797 episode reward: total was -302.620000. running mean: -19.139524\n",
      "ep 2883: ep_len:50 episode reward: total was 22.000000. running mean: -18.728129\n",
      "epsilon:0.009992 episode_count: 43398. steps_count: 46678930.000000\n",
      "ep 2884: ep_len:932 episode reward: total was -65.280000. running mean: -19.193647\n",
      "ep 2884: ep_len:732 episode reward: total was 3.810000. running mean: -18.963611\n",
      "ep 2884: ep_len:41 episode reward: total was 17.500000. running mean: -18.598975\n",
      "ep 2884: ep_len:3152 episode reward: total was -16.160000. running mean: -18.574585\n",
      "ep 2884: ep_len:1674 episode reward: total was -59.000000. running mean: -18.978839\n",
      "ep 2884: ep_len:75 episode reward: total was 36.000000. running mean: -18.429051\n",
      "ep 2884: ep_len:1060 episode reward: total was -60.270000. running mean: -18.847460\n",
      "ep 2884: ep_len:658 episode reward: total was 17.070000. running mean: -18.488286\n",
      "ep 2884: ep_len:1223 episode reward: total was -13.250000. running mean: -18.435903\n",
      "ep 2884: ep_len:781 episode reward: total was 40.600000. running mean: -17.845544\n",
      "ep 2884: ep_len:1121 episode reward: total was -36.430000. running mean: -18.031388\n",
      "ep 2884: ep_len:99 episode reward: total was 46.500000. running mean: -17.386074\n",
      "ep 2884: ep_len:1466 episode reward: total was -23.370000. running mean: -17.445914\n",
      "ep 2884: ep_len:2879 episode reward: total was -28.610000. running mean: -17.557555\n",
      "epsilon:0.009992 episode_count: 43412. steps_count: 46694823.000000\n",
      "ep 2885: ep_len:631 episode reward: total was -5.980000. running mean: -17.441779\n",
      "ep 2885: ep_len:1272 episode reward: total was -44.500000. running mean: -17.712361\n",
      "ep 2885: ep_len:75 episode reward: total was 36.000000. running mean: -17.175238\n",
      "ep 2885: ep_len:3004 episode reward: total was -42.720000. running mean: -17.430685\n",
      "ep 2885: ep_len:1103 episode reward: total was -13.380000. running mean: -17.390178\n",
      "ep 2885: ep_len:39 episode reward: total was 18.000000. running mean: -17.036277\n",
      "ep 2885: ep_len:74 episode reward: total was 35.500000. running mean: -16.510914\n",
      "ep 2885: ep_len:500 episode reward: total was 44.010000. running mean: -15.905705\n",
      "ep 2885: ep_len:659 episode reward: total was 20.290000. running mean: -15.543748\n",
      "ep 2885: ep_len:1585 episode reward: total was -28.330000. running mean: -15.671610\n",
      "ep 2885: ep_len:763 episode reward: total was 34.780000. running mean: -15.167094\n",
      "ep 2885: ep_len:605 episode reward: total was -3.680000. running mean: -15.052223\n",
      "ep 2885: ep_len:147 episode reward: total was 67.500000. running mean: -14.226701\n",
      "ep 2885: ep_len:95 episode reward: total was 44.500000. running mean: -13.639434\n",
      "ep 2885: ep_len:719 episode reward: total was -40.260000. running mean: -13.905640\n",
      "ep 2885: ep_len:2815 episode reward: total was -62.090000. running mean: -14.387483\n",
      "epsilon:0.009992 episode_count: 43428. steps_count: 46708909.000000\n",
      "ep 2886: ep_len:665 episode reward: total was -37.960000. running mean: -14.623208\n",
      "ep 2886: ep_len:1664 episode reward: total was -80.980000. running mean: -15.286776\n",
      "ep 2886: ep_len:3013 episode reward: total was 0.660000. running mean: -15.127308\n",
      "ep 2886: ep_len:819 episode reward: total was -8.660000. running mean: -15.062635\n",
      "ep 2886: ep_len:39 episode reward: total was 18.000000. running mean: -14.732009\n",
      "ep 2886: ep_len:134 episode reward: total was 64.000000. running mean: -13.944689\n",
      "ep 2886: ep_len:1025 episode reward: total was -36.380000. running mean: -14.169042\n",
      "ep 2886: ep_len:660 episode reward: total was 29.730000. running mean: -13.730052\n",
      "ep 2886: ep_len:806 episode reward: total was -50.150000. running mean: -14.094251\n",
      "ep 2886: ep_len:913 episode reward: total was 65.040000. running mean: -13.302909\n",
      "ep 2886: ep_len:641 episode reward: total was 13.050000. running mean: -13.039379\n",
      "ep 2886: ep_len:77 episode reward: total was 37.000000. running mean: -12.538986\n",
      "ep 2886: ep_len:1038 episode reward: total was 15.050000. running mean: -12.263096\n",
      "ep 2886: ep_len:2894 episode reward: total was -39.020000. running mean: -12.530665\n",
      "epsilon:0.009992 episode_count: 43442. steps_count: 46723297.000000\n",
      "ep 2887: ep_len:1125 episode reward: total was 9.840000. running mean: -12.306958\n",
      "ep 2887: ep_len:657 episode reward: total was -30.160000. running mean: -12.485489\n",
      "ep 2887: ep_len:2993 episode reward: total was 7.280000. running mean: -12.287834\n",
      "ep 2887: ep_len:805 episode reward: total was -475.760000. running mean: -16.922555\n",
      "ep 2887: ep_len:103 episode reward: total was 50.000000. running mean: -16.253330\n",
      "ep 2887: ep_len:59 episode reward: total was 28.000000. running mean: -15.810797\n",
      "ep 2887: ep_len:1878 episode reward: total was -61.420000. running mean: -16.266889\n",
      "ep 2887: ep_len:684 episode reward: total was 37.870000. running mean: -15.725520\n",
      "ep 2887: ep_len:656 episode reward: total was -3.090000. running mean: -15.599165\n",
      "ep 2887: ep_len:795 episode reward: total was 34.920000. running mean: -15.093973\n",
      "ep 2887: ep_len:712 episode reward: total was 6.190000. running mean: -14.881133\n",
      "ep 2887: ep_len:187 episode reward: total was 92.000000. running mean: -13.812322\n",
      "ep 2887: ep_len:83 episode reward: total was 38.500000. running mean: -13.289199\n",
      "ep 2887: ep_len:1104 episode reward: total was -13.370000. running mean: -13.290007\n",
      "ep 2887: ep_len:2832 episode reward: total was -14.510000. running mean: -13.302207\n",
      "epsilon:0.009992 episode_count: 43457. steps_count: 46737970.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2888: ep_len:500 episode reward: total was 11.910000. running mean: -13.050084\n",
      "ep 2888: ep_len:1505 episode reward: total was -112.810000. running mean: -14.047684\n",
      "ep 2888: ep_len:3045 episode reward: total was -70.900000. running mean: -14.616207\n",
      "ep 2888: ep_len:669 episode reward: total was 0.570000. running mean: -14.464345\n",
      "ep 2888: ep_len:57 episode reward: total was 25.500000. running mean: -14.064701\n",
      "ep 2888: ep_len:68 episode reward: total was 32.500000. running mean: -13.599054\n",
      "ep 2888: ep_len:1855 episode reward: total was -36.980000. running mean: -13.832864\n",
      "ep 2888: ep_len:343 episode reward: total was 13.040000. running mean: -13.564135\n",
      "ep 2888: ep_len:855 episode reward: total was 8.040000. running mean: -13.348094\n",
      "ep 2888: ep_len:743 episode reward: total was 39.390000. running mean: -12.820713\n",
      "ep 2888: ep_len:500 episode reward: total was 11.480000. running mean: -12.577706\n",
      "ep 2888: ep_len:76 episode reward: total was 36.500000. running mean: -12.086929\n",
      "ep 2888: ep_len:38 episode reward: total was 16.000000. running mean: -11.806059\n",
      "ep 2888: ep_len:583 episode reward: total was 10.540000. running mean: -11.582599\n",
      "ep 2888: ep_len:2855 episode reward: total was -14.680000. running mean: -11.613573\n",
      "ep 2888: ep_len:58 episode reward: total was 24.500000. running mean: -11.252437\n",
      "epsilon:0.009992 episode_count: 43473. steps_count: 46751720.000000\n",
      "ep 2889: ep_len:1488 episode reward: total was 0.230000. running mean: -11.137613\n",
      "ep 2889: ep_len:500 episode reward: total was -111.470000. running mean: -12.140937\n",
      "ep 2889: ep_len:2928 episode reward: total was -81.310000. running mean: -12.832627\n",
      "ep 2889: ep_len:1077 episode reward: total was -12.630000. running mean: -12.830601\n",
      "ep 2889: ep_len:966 episode reward: total was 11.510000. running mean: -12.587195\n",
      "ep 2889: ep_len:354 episode reward: total was 14.280000. running mean: -12.318523\n",
      "ep 2889: ep_len:1158 episode reward: total was -25.040000. running mean: -12.445738\n",
      "ep 2889: ep_len:712 episode reward: total was 52.430000. running mean: -11.796980\n",
      "ep 2889: ep_len:580 episode reward: total was -3.460000. running mean: -11.713611\n",
      "ep 2889: ep_len:80 episode reward: total was 37.000000. running mean: -11.226474\n",
      "ep 2889: ep_len:103 episode reward: total was 48.500000. running mean: -10.629210\n",
      "ep 2889: ep_len:642 episode reward: total was 19.990000. running mean: -10.323018\n",
      "ep 2889: ep_len:2832 episode reward: total was -14.670000. running mean: -10.366487\n",
      "epsilon:0.009992 episode_count: 43486. steps_count: 46765140.000000\n",
      "ep 2890: ep_len:1126 episode reward: total was -8.900000. running mean: -10.351823\n",
      "ep 2890: ep_len:761 episode reward: total was -48.110000. running mean: -10.729404\n",
      "ep 2890: ep_len:3112 episode reward: total was -39.610000. running mean: -11.018210\n",
      "ep 2890: ep_len:500 episode reward: total was 10.780000. running mean: -10.800228\n",
      "ep 2890: ep_len:69 episode reward: total was 33.000000. running mean: -10.362226\n",
      "ep 2890: ep_len:64 episode reward: total was 11.000000. running mean: -10.148604\n",
      "ep 2890: ep_len:79 episode reward: total was 38.000000. running mean: -9.667118\n",
      "ep 2890: ep_len:695 episode reward: total was 13.980000. running mean: -9.430646\n",
      "ep 2890: ep_len:342 episode reward: total was 20.650000. running mean: -9.129840\n",
      "ep 2890: ep_len:1249 episode reward: total was -66.460000. running mean: -9.703142\n",
      "ep 2890: ep_len:807 episode reward: total was 28.120000. running mean: -9.324910\n",
      "ep 2890: ep_len:1514 episode reward: total was -11.050000. running mean: -9.342161\n",
      "ep 2890: ep_len:61 episode reward: total was 29.000000. running mean: -8.958739\n",
      "ep 2890: ep_len:661 episode reward: total was 16.320000. running mean: -8.705952\n",
      "ep 2890: ep_len:2873 episode reward: total was -45.660000. running mean: -9.075493\n",
      "epsilon:0.009992 episode_count: 43501. steps_count: 46779053.000000\n",
      "ep 2891: ep_len:1171 episode reward: total was 10.300000. running mean: -8.881738\n",
      "ep 2891: ep_len:776 episode reward: total was 1.890000. running mean: -8.774020\n",
      "ep 2891: ep_len:3060 episode reward: total was -11.170000. running mean: -8.797980\n",
      "ep 2891: ep_len:646 episode reward: total was -9.300000. running mean: -8.803000\n",
      "ep 2891: ep_len:53 episode reward: total was 23.500000. running mean: -8.479970\n",
      "ep 2891: ep_len:88 episode reward: total was 41.000000. running mean: -7.985170\n",
      "ep 2891: ep_len:60 episode reward: total was 28.500000. running mean: -7.620319\n",
      "ep 2891: ep_len:617 episode reward: total was -11.580000. running mean: -7.659916\n",
      "ep 2891: ep_len:607 episode reward: total was 14.440000. running mean: -7.438916\n",
      "ep 2891: ep_len:591 episode reward: total was -59.390000. running mean: -7.958427\n",
      "ep 2891: ep_len:827 episode reward: total was 26.230000. running mean: -7.616543\n",
      "ep 2891: ep_len:648 episode reward: total was 3.400000. running mean: -7.506378\n",
      "ep 2891: ep_len:500 episode reward: total was 33.010000. running mean: -7.101214\n",
      "ep 2891: ep_len:2786 episode reward: total was -12.280000. running mean: -7.153002\n",
      "ep 2891: ep_len:67 episode reward: total was 29.000000. running mean: -6.791472\n",
      "epsilon:0.009992 episode_count: 43516. steps_count: 46791550.000000\n",
      "ep 2892: ep_len:1109 episode reward: total was -6.250000. running mean: -6.786057\n",
      "ep 2892: ep_len:500 episode reward: total was 9.860000. running mean: -6.619596\n",
      "ep 2892: ep_len:3051 episode reward: total was -20.230000. running mean: -6.755700\n",
      "ep 2892: ep_len:500 episode reward: total was 9.090000. running mean: -6.597243\n",
      "ep 2892: ep_len:52 episode reward: total was 23.000000. running mean: -6.301271\n",
      "ep 2892: ep_len:1028 episode reward: total was 2.130000. running mean: -6.216958\n",
      "ep 2892: ep_len:344 episode reward: total was 20.610000. running mean: -5.948689\n",
      "ep 2892: ep_len:563 episode reward: total was -20.160000. running mean: -6.090802\n",
      "ep 2892: ep_len:852 episode reward: total was 53.290000. running mean: -5.496994\n",
      "ep 2892: ep_len:529 episode reward: total was -6.840000. running mean: -5.510424\n",
      "ep 2892: ep_len:80 episode reward: total was 38.500000. running mean: -5.070320\n",
      "ep 2892: ep_len:778 episode reward: total was -21.030000. running mean: -5.229916\n",
      "ep 2892: ep_len:2817 episode reward: total was -9.640000. running mean: -5.274017\n",
      "ep 2892: ep_len:52 episode reward: total was 24.500000. running mean: -4.976277\n",
      "epsilon:0.009992 episode_count: 43530. steps_count: 46803805.000000\n",
      "ep 2893: ep_len:1122 episode reward: total was -0.780000. running mean: -4.934314\n",
      "ep 2893: ep_len:753 episode reward: total was -1.030000. running mean: -4.895271\n",
      "ep 2893: ep_len:63 episode reward: total was 30.000000. running mean: -4.546318\n",
      "ep 2893: ep_len:3085 episode reward: total was -19.280000. running mean: -4.693655\n",
      "ep 2893: ep_len:714 episode reward: total was -45.450000. running mean: -5.101219\n",
      "ep 2893: ep_len:60 episode reward: total was 28.500000. running mean: -4.765207\n",
      "ep 2893: ep_len:915 episode reward: total was 6.290000. running mean: -4.654654\n",
      "ep 2893: ep_len:3609 episode reward: total was -333.330000. running mean: -7.941408\n",
      "ep 2893: ep_len:743 episode reward: total was -50.020000. running mean: -8.362194\n",
      "ep 2893: ep_len:1598 episode reward: total was -201.210000. running mean: -10.290672\n",
      "ep 2893: ep_len:727 episode reward: total was -24.150000. running mean: -10.429265\n",
      "ep 2893: ep_len:1065 episode reward: total was -12.160000. running mean: -10.446573\n",
      "ep 2893: ep_len:2811 episode reward: total was -33.330000. running mean: -10.675407\n",
      "epsilon:0.009992 episode_count: 43543. steps_count: 46821070.000000\n",
      "ep 2894: ep_len:1404 episode reward: total was 14.390000. running mean: -10.424753\n",
      "ep 2894: ep_len:1186 episode reward: total was -47.900000. running mean: -10.799505\n",
      "ep 2894: ep_len:3027 episode reward: total was -27.860000. running mean: -10.970110\n",
      "ep 2894: ep_len:500 episode reward: total was 16.290000. running mean: -10.697509\n",
      "ep 2894: ep_len:132 episode reward: total was 64.500000. running mean: -9.945534\n",
      "ep 2894: ep_len:1153 episode reward: total was -14.380000. running mean: -9.989879\n",
      "ep 2894: ep_len:3630 episode reward: total was -82.670000. running mean: -10.716680\n",
      "ep 2894: ep_len:602 episode reward: total was 8.830000. running mean: -10.521213\n",
      "ep 2894: ep_len:661 episode reward: total was 0.410000. running mean: -10.411901\n",
      "ep 2894: ep_len:652 episode reward: total was -3.750000. running mean: -10.345282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2894: ep_len:39 episode reward: total was 18.000000. running mean: -10.061829\n",
      "ep 2894: ep_len:784 episode reward: total was -19.900000. running mean: -10.160211\n",
      "ep 2894: ep_len:2845 episode reward: total was -22.220000. running mean: -10.280809\n",
      "ep 2894: ep_len:41 episode reward: total was 19.000000. running mean: -9.988001\n",
      "epsilon:0.009992 episode_count: 43557. steps_count: 46837726.000000\n",
      "ep 2895: ep_len:500 episode reward: total was -5.080000. running mean: -9.938921\n",
      "ep 2895: ep_len:731 episode reward: total was -10.530000. running mean: -9.944831\n",
      "ep 2895: ep_len:2946 episode reward: total was -69.810000. running mean: -10.543483\n",
      "ep 2895: ep_len:558 episode reward: total was -32.970000. running mean: -10.767748\n",
      "ep 2895: ep_len:55 episode reward: total was 26.000000. running mean: -10.400071\n",
      "ep 2895: ep_len:620 episode reward: total was 21.400000. running mean: -10.082070\n",
      "ep 2895: ep_len:648 episode reward: total was 17.670000. running mean: -9.804549\n",
      "ep 2895: ep_len:931 episode reward: total was -56.510000. running mean: -10.271604\n",
      "ep 2895: ep_len:667 episode reward: total was 8.460000. running mean: -10.084288\n",
      "ep 2895: ep_len:964 episode reward: total was -7.730000. running mean: -10.060745\n",
      "ep 2895: ep_len:59 episode reward: total was 28.000000. running mean: -9.680137\n",
      "ep 2895: ep_len:123 episode reward: total was 57.000000. running mean: -9.013336\n",
      "ep 2895: ep_len:596 episode reward: total was -10.370000. running mean: -9.026903\n",
      "ep 2895: ep_len:2831 episode reward: total was -28.810000. running mean: -9.224734\n",
      "ep 2895: ep_len:40 episode reward: total was 18.500000. running mean: -8.947486\n",
      "epsilon:0.009992 episode_count: 43572. steps_count: 46849995.000000\n",
      "ep 2896: ep_len:674 episode reward: total was 5.700000. running mean: -8.801012\n",
      "ep 2896: ep_len:682 episode reward: total was -24.700000. running mean: -8.960001\n",
      "ep 2896: ep_len:71 episode reward: total was 32.500000. running mean: -8.545401\n",
      "ep 2896: ep_len:3010 episode reward: total was -17.130000. running mean: -8.631247\n",
      "ep 2896: ep_len:606 episode reward: total was -5.710000. running mean: -8.602035\n",
      "ep 2896: ep_len:98 episode reward: total was 46.000000. running mean: -8.056015\n",
      "ep 2896: ep_len:66 episode reward: total was 30.000000. running mean: -7.675454\n",
      "ep 2896: ep_len:1384 episode reward: total was -30.770000. running mean: -7.906400\n",
      "ep 2896: ep_len:3750 episode reward: total was -36.020000. running mean: -8.187536\n",
      "ep 2896: ep_len:556 episode reward: total was -2.690000. running mean: -8.132561\n",
      "ep 2896: ep_len:640 episode reward: total was 27.830000. running mean: -7.772935\n",
      "ep 2896: ep_len:1152 episode reward: total was 2.940000. running mean: -7.665806\n",
      "ep 2896: ep_len:42 episode reward: total was 18.000000. running mean: -7.409147\n",
      "ep 2896: ep_len:74 episode reward: total was 31.000000. running mean: -7.025056\n",
      "ep 2896: ep_len:694 episode reward: total was 15.500000. running mean: -6.799805\n",
      "ep 2896: ep_len:2941 episode reward: total was -6.560000. running mean: -6.797407\n",
      "epsilon:0.009992 episode_count: 43588. steps_count: 46866435.000000\n",
      "ep 2897: ep_len:1132 episode reward: total was -13.090000. running mean: -6.860333\n",
      "ep 2897: ep_len:690 episode reward: total was -12.170000. running mean: -6.913430\n",
      "ep 2897: ep_len:2969 episode reward: total was -5.110000. running mean: -6.895396\n",
      "ep 2897: ep_len:667 episode reward: total was 19.080000. running mean: -6.635642\n",
      "ep 2897: ep_len:82 episode reward: total was 39.500000. running mean: -6.174285\n",
      "ep 2897: ep_len:75 episode reward: total was 36.000000. running mean: -5.752542\n",
      "ep 2897: ep_len:1401 episode reward: total was -31.550000. running mean: -6.010517\n",
      "ep 2897: ep_len:656 episode reward: total was 23.660000. running mean: -5.713812\n",
      "ep 2897: ep_len:579 episode reward: total was -26.700000. running mean: -5.923674\n",
      "ep 2897: ep_len:7339 episode reward: total was -122.640000. running mean: -7.090837\n",
      "ep 2897: ep_len:714 episode reward: total was -15.250000. running mean: -7.172429\n",
      "ep 2897: ep_len:1090 episode reward: total was -76.640000. running mean: -7.867104\n",
      "ep 2897: ep_len:2811 episode reward: total was -50.010000. running mean: -8.288533\n",
      "ep 2897: ep_len:69 episode reward: total was 31.500000. running mean: -7.890648\n",
      "epsilon:0.009992 episode_count: 43602. steps_count: 46886709.000000\n",
      "ep 2898: ep_len:660 episode reward: total was -11.750000. running mean: -7.929242\n",
      "ep 2898: ep_len:1265 episode reward: total was -39.030000. running mean: -8.240249\n",
      "ep 2898: ep_len:70 episode reward: total was 33.500000. running mean: -7.822847\n",
      "ep 2898: ep_len:83 episode reward: total was 40.000000. running mean: -7.344618\n",
      "ep 2898: ep_len:806 episode reward: total was -6.800000. running mean: -7.339172\n",
      "ep 2898: ep_len:80 episode reward: total was 35.500000. running mean: -6.910780\n",
      "ep 2898: ep_len:636 episode reward: total was -9.450000. running mean: -6.936172\n",
      "ep 2898: ep_len:3630 episode reward: total was -1167.790000. running mean: -18.544711\n",
      "ep 2898: ep_len:1242 episode reward: total was -19.020000. running mean: -18.549464\n",
      "ep 2898: ep_len:672 episode reward: total was 28.080000. running mean: -18.083169\n",
      "ep 2898: ep_len:910 episode reward: total was -15.210000. running mean: -18.054437\n",
      "ep 2898: ep_len:641 episode reward: total was -22.590000. running mean: -18.099793\n",
      "ep 2898: ep_len:2941 episode reward: total was -6.870000. running mean: -17.987495\n",
      "ep 2898: ep_len:50 episode reward: total was 23.500000. running mean: -17.572620\n",
      "epsilon:0.009992 episode_count: 43616. steps_count: 46900395.000000\n",
      "ep 2899: ep_len:1112 episode reward: total was -16.320000. running mean: -17.560094\n",
      "ep 2899: ep_len:749 episode reward: total was -17.120000. running mean: -17.555693\n",
      "ep 2899: ep_len:39 episode reward: total was 18.000000. running mean: -17.200136\n",
      "ep 2899: ep_len:2992 episode reward: total was 3.990000. running mean: -16.988235\n",
      "ep 2899: ep_len:820 episode reward: total was 10.720000. running mean: -16.711152\n",
      "ep 2899: ep_len:49 episode reward: total was 23.000000. running mean: -16.314041\n",
      "ep 2899: ep_len:64 episode reward: total was 29.000000. running mean: -15.860900\n",
      "ep 2899: ep_len:55 episode reward: total was 26.000000. running mean: -15.442291\n",
      "ep 2899: ep_len:761 episode reward: total was -44.960000. running mean: -15.737468\n",
      "ep 2899: ep_len:3776 episode reward: total was -70.200000. running mean: -16.282094\n",
      "ep 2899: ep_len:2096 episode reward: total was -413.570000. running mean: -20.254973\n",
      "ep 2899: ep_len:818 episode reward: total was 18.530000. running mean: -19.867123\n",
      "ep 2899: ep_len:500 episode reward: total was 8.120000. running mean: -19.587252\n",
      "ep 2899: ep_len:70 episode reward: total was 33.500000. running mean: -19.056379\n",
      "ep 2899: ep_len:904 episode reward: total was 23.890000. running mean: -18.626916\n",
      "ep 2899: ep_len:2806 episode reward: total was 0.380000. running mean: -18.436846\n",
      "ep 2899: ep_len:39 episode reward: total was 16.500000. running mean: -18.087478\n",
      "epsilon:0.009992 episode_count: 43633. steps_count: 46918045.000000\n",
      "ep 2900: ep_len:1439 episode reward: total was -11.710000. running mean: -18.023703\n",
      "ep 2900: ep_len:628 episode reward: total was -49.840000. running mean: -18.341866\n",
      "ep 2900: ep_len:43 episode reward: total was 18.010000. running mean: -17.978347\n",
      "ep 2900: ep_len:3033 episode reward: total was -6.180000. running mean: -17.860364\n",
      "ep 2900: ep_len:1487 episode reward: total was 7.600000. running mean: -17.605760\n",
      "ep 2900: ep_len:72 episode reward: total was 33.000000. running mean: -17.099703\n",
      "ep 2900: ep_len:1118 episode reward: total was -16.260000. running mean: -17.091306\n",
      "ep 2900: ep_len:3848 episode reward: total was -7.010000. running mean: -16.990493\n",
      "ep 2900: ep_len:653 episode reward: total was -32.020000. running mean: -17.140788\n",
      "ep 2900: ep_len:694 episode reward: total was 9.350000. running mean: -16.875880\n",
      "ep 2900: ep_len:3602 episode reward: total was -582.760000. running mean: -22.534721\n",
      "ep 2900: ep_len:76 episode reward: total was 36.500000. running mean: -21.944374\n",
      "ep 2900: ep_len:692 episode reward: total was 8.970000. running mean: -21.635230\n",
      "ep 2900: ep_len:2905 episode reward: total was 0.150000. running mean: -21.417378\n",
      "ep 2900: ep_len:43 episode reward: total was 20.000000. running mean: -21.003204\n",
      "epsilon:0.009992 episode_count: 43648. steps_count: 46938378.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2901: ep_len:672 episode reward: total was -6.480000. running mean: -20.857972\n",
      "ep 2901: ep_len:1583 episode reward: total was -35.120000. running mean: -21.000592\n",
      "ep 2901: ep_len:53 episode reward: total was 25.000000. running mean: -20.540586\n",
      "ep 2901: ep_len:3055 episode reward: total was -35.900000. running mean: -20.694180\n",
      "ep 2901: ep_len:570 episode reward: total was -9.370000. running mean: -20.580939\n",
      "ep 2901: ep_len:124 episode reward: total was 59.000000. running mean: -19.785129\n",
      "ep 2901: ep_len:98 episode reward: total was 47.500000. running mean: -19.112278\n",
      "ep 2901: ep_len:500 episode reward: total was -2.320000. running mean: -18.944355\n",
      "ep 2901: ep_len:658 episode reward: total was 33.690000. running mean: -18.418012\n",
      "ep 2901: ep_len:662 episode reward: total was -4.660000. running mean: -18.280432\n",
      "ep 2901: ep_len:7254 episode reward: total was -23.550000. running mean: -18.333127\n",
      "ep 2901: ep_len:1462 episode reward: total was -10.470000. running mean: -18.254496\n",
      "ep 2901: ep_len:156 episode reward: total was 76.500000. running mean: -17.306951\n",
      "ep 2901: ep_len:600 episode reward: total was 19.700000. running mean: -16.936881\n",
      "ep 2901: ep_len:2808 episode reward: total was 4.900000. running mean: -16.718513\n",
      "ep 2901: ep_len:64 episode reward: total was 27.500000. running mean: -16.276328\n",
      "epsilon:0.009992 episode_count: 43664. steps_count: 46958697.000000\n",
      "ep 2902: ep_len:640 episode reward: total was -9.930000. running mean: -16.212864\n",
      "ep 2902: ep_len:757 episode reward: total was 8.790000. running mean: -15.962836\n",
      "ep 2902: ep_len:3002 episode reward: total was -32.670000. running mean: -16.129907\n",
      "ep 2902: ep_len:550 episode reward: total was 1.290000. running mean: -15.955708\n",
      "ep 2902: ep_len:59 episode reward: total was 26.500000. running mean: -15.531151\n",
      "ep 2902: ep_len:1380 episode reward: total was -89.390000. running mean: -16.269740\n",
      "ep 2902: ep_len:296 episode reward: total was 19.240000. running mean: -15.914642\n",
      "ep 2902: ep_len:825 episode reward: total was -40.960000. running mean: -16.165096\n",
      "ep 2902: ep_len:709 episode reward: total was 43.550000. running mean: -15.567945\n",
      "ep 2902: ep_len:619 episode reward: total was -21.250000. running mean: -15.624765\n",
      "ep 2902: ep_len:62 episode reward: total was 26.500000. running mean: -15.203518\n",
      "ep 2902: ep_len:1046 episode reward: total was 27.410000. running mean: -14.777383\n",
      "ep 2902: ep_len:2925 episode reward: total was -1.980000. running mean: -14.649409\n",
      "ep 2902: ep_len:57 episode reward: total was 27.000000. running mean: -14.232915\n",
      "epsilon:0.009992 episode_count: 43678. steps_count: 46971624.000000\n",
      "ep 2903: ep_len:778 episode reward: total was -77.230000. running mean: -14.862885\n",
      "ep 2903: ep_len:1646 episode reward: total was -44.830000. running mean: -15.162557\n",
      "ep 2903: ep_len:71 episode reward: total was 34.000000. running mean: -14.670931\n",
      "ep 2903: ep_len:3000 episode reward: total was -51.580000. running mean: -15.040022\n",
      "ep 2903: ep_len:545 episode reward: total was -13.110000. running mean: -15.020722\n",
      "ep 2903: ep_len:147 episode reward: total was 72.000000. running mean: -14.150514\n",
      "ep 2903: ep_len:68 episode reward: total was 32.500000. running mean: -13.684009\n",
      "ep 2903: ep_len:500 episode reward: total was 29.370000. running mean: -13.253469\n",
      "ep 2903: ep_len:618 episode reward: total was 25.730000. running mean: -12.863634\n",
      "ep 2903: ep_len:1290 episode reward: total was -60.320000. running mean: -13.338198\n",
      "ep 2903: ep_len:851 episode reward: total was 40.420000. running mean: -12.800616\n",
      "ep 2903: ep_len:661 episode reward: total was -10.610000. running mean: -12.778710\n",
      "ep 2903: ep_len:72 episode reward: total was 34.500000. running mean: -12.305923\n",
      "ep 2903: ep_len:701 episode reward: total was -5.770000. running mean: -12.240564\n",
      "ep 2903: ep_len:2862 episode reward: total was -14.370000. running mean: -12.261858\n",
      "ep 2903: ep_len:71 episode reward: total was 32.500000. running mean: -11.814239\n",
      "epsilon:0.009992 episode_count: 43694. steps_count: 46985505.000000\n",
      "ep 2904: ep_len:783 episode reward: total was -25.670000. running mean: -11.952797\n",
      "ep 2904: ep_len:1602 episode reward: total was -78.940000. running mean: -12.622669\n",
      "ep 2904: ep_len:2961 episode reward: total was -20.020000. running mean: -12.696642\n",
      "ep 2904: ep_len:500 episode reward: total was 22.660000. running mean: -12.343076\n",
      "ep 2904: ep_len:54 episode reward: total was 25.500000. running mean: -11.964645\n",
      "ep 2904: ep_len:1852 episode reward: total was -22.520000. running mean: -12.070199\n",
      "ep 2904: ep_len:3763 episode reward: total was -215.370000. running mean: -14.103197\n",
      "ep 2904: ep_len:643 episode reward: total was -239.020000. running mean: -16.352365\n",
      "ep 2904: ep_len:942 episode reward: total was -86.920000. running mean: -17.058041\n",
      "ep 2904: ep_len:664 episode reward: total was 15.230000. running mean: -16.735161\n",
      "ep 2904: ep_len:65 episode reward: total was 31.000000. running mean: -16.257809\n",
      "ep 2904: ep_len:785 episode reward: total was 17.150000. running mean: -15.923731\n",
      "ep 2904: ep_len:2714 episode reward: total was -4.180000. running mean: -15.806294\n",
      "epsilon:0.009992 episode_count: 43707. steps_count: 47002833.000000\n",
      "ep 2905: ep_len:1143 episode reward: total was -32.390000. running mean: -15.972131\n",
      "ep 2905: ep_len:500 episode reward: total was 18.190000. running mean: -15.630509\n",
      "ep 2905: ep_len:71 episode reward: total was 31.000000. running mean: -15.164204\n",
      "ep 2905: ep_len:2955 episode reward: total was -21.020000. running mean: -15.222762\n",
      "ep 2905: ep_len:1147 episode reward: total was -21.020000. running mean: -15.280735\n",
      "ep 2905: ep_len:80 episode reward: total was 38.500000. running mean: -14.742927\n",
      "ep 2905: ep_len:680 episode reward: total was 2.590000. running mean: -14.569598\n",
      "ep 2905: ep_len:3635 episode reward: total was -71.230000. running mean: -15.136202\n",
      "ep 2905: ep_len:604 episode reward: total was -28.440000. running mean: -15.269240\n",
      "ep 2905: ep_len:819 episode reward: total was 35.590000. running mean: -14.760648\n",
      "ep 2905: ep_len:686 episode reward: total was -34.150000. running mean: -14.954541\n",
      "ep 2905: ep_len:58 episode reward: total was 27.500000. running mean: -14.529996\n",
      "ep 2905: ep_len:58 episode reward: total was 27.500000. running mean: -14.109696\n",
      "ep 2905: ep_len:617 episode reward: total was -12.180000. running mean: -14.090399\n",
      "ep 2905: ep_len:46 episode reward: total was 21.500000. running mean: -13.734495\n",
      "epsilon:0.009992 episode_count: 43722. steps_count: 47015932.000000\n",
      "ep 2906: ep_len:1104 episode reward: total was -3.270000. running mean: -13.629850\n",
      "ep 2906: ep_len:1595 episode reward: total was -66.120000. running mean: -14.154751\n",
      "ep 2906: ep_len:3027 episode reward: total was -59.770000. running mean: -14.610904\n",
      "ep 2906: ep_len:658 episode reward: total was 15.310000. running mean: -14.311695\n",
      "ep 2906: ep_len:125 episode reward: total was 58.000000. running mean: -13.588578\n",
      "ep 2906: ep_len:115 episode reward: total was 56.000000. running mean: -12.892692\n",
      "ep 2906: ep_len:71 episode reward: total was 34.000000. running mean: -12.423765\n",
      "ep 2906: ep_len:671 episode reward: total was -1.450000. running mean: -12.314028\n",
      "ep 2906: ep_len:623 episode reward: total was 1.810000. running mean: -12.172787\n",
      "ep 2906: ep_len:678 episode reward: total was -6.880000. running mean: -12.119859\n",
      "ep 2906: ep_len:733 episode reward: total was 45.230000. running mean: -11.546361\n",
      "ep 2906: ep_len:500 episode reward: total was 37.420000. running mean: -11.056697\n",
      "ep 2906: ep_len:85 episode reward: total was 39.500000. running mean: -10.551130\n",
      "ep 2906: ep_len:769 episode reward: total was -39.950000. running mean: -10.845119\n",
      "ep 2906: ep_len:2811 episode reward: total was -10.190000. running mean: -10.838568\n",
      "ep 2906: ep_len:55 episode reward: total was 26.000000. running mean: -10.470182\n",
      "epsilon:0.009992 episode_count: 43738. steps_count: 47029552.000000\n",
      "ep 2907: ep_len:1392 episode reward: total was -309.800000. running mean: -13.463480\n",
      "ep 2907: ep_len:918 episode reward: total was -10.000000. running mean: -13.428845\n",
      "ep 2907: ep_len:3056 episode reward: total was 2.710000. running mean: -13.267457\n",
      "ep 2907: ep_len:641 episode reward: total was 2.070000. running mean: -13.114082\n",
      "ep 2907: ep_len:53 episode reward: total was 25.000000. running mean: -12.732942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2907: ep_len:60 episode reward: total was 28.500000. running mean: -12.320612\n",
      "ep 2907: ep_len:1102 episode reward: total was -8.340000. running mean: -12.280806\n",
      "ep 2907: ep_len:3795 episode reward: total was 4.730000. running mean: -12.110698\n",
      "ep 2907: ep_len:564 episode reward: total was 5.840000. running mean: -11.931191\n",
      "ep 2907: ep_len:771 episode reward: total was -8.070000. running mean: -11.892579\n",
      "ep 2907: ep_len:732 episode reward: total was -6.540000. running mean: -11.839053\n",
      "ep 2907: ep_len:1512 episode reward: total was 9.780000. running mean: -11.622863\n",
      "ep 2907: ep_len:2853 episode reward: total was -84.670000. running mean: -12.353334\n",
      "epsilon:0.009992 episode_count: 43751. steps_count: 47047001.000000\n",
      "ep 2908: ep_len:836 episode reward: total was -39.650000. running mean: -12.626301\n",
      "ep 2908: ep_len:191 episode reward: total was 8.300000. running mean: -12.417038\n",
      "ep 2908: ep_len:3026 episode reward: total was -18.350000. running mean: -12.476367\n",
      "ep 2908: ep_len:619 episode reward: total was 2.150000. running mean: -12.330104\n",
      "ep 2908: ep_len:66 episode reward: total was 31.500000. running mean: -11.891803\n",
      "ep 2908: ep_len:136 episode reward: total was 65.000000. running mean: -11.122885\n",
      "ep 2908: ep_len:981 episode reward: total was -9.550000. running mean: -11.107156\n",
      "ep 2908: ep_len:3999 episode reward: total was -74.490000. running mean: -11.740984\n",
      "ep 2908: ep_len:500 episode reward: total was -18.000000. running mean: -11.803574\n",
      "ep 2908: ep_len:893 episode reward: total was 66.740000. running mean: -11.018139\n",
      "ep 2908: ep_len:718 episode reward: total was -14.200000. running mean: -11.049957\n",
      "ep 2908: ep_len:45 episode reward: total was 18.000000. running mean: -10.759458\n",
      "ep 2908: ep_len:1164 episode reward: total was -19.410000. running mean: -10.845963\n",
      "ep 2908: ep_len:2843 episode reward: total was 8.830000. running mean: -10.649204\n",
      "ep 2908: ep_len:53 episode reward: total was 25.000000. running mean: -10.292711\n",
      "epsilon:0.009992 episode_count: 43766. steps_count: 47063071.000000\n",
      "ep 2909: ep_len:645 episode reward: total was -20.990000. running mean: -10.399684\n",
      "ep 2909: ep_len:772 episode reward: total was -14.150000. running mean: -10.437188\n",
      "ep 2909: ep_len:70 episode reward: total was 29.000000. running mean: -10.042816\n",
      "ep 2909: ep_len:2989 episode reward: total was -58.090000. running mean: -10.523287\n",
      "ep 2909: ep_len:698 episode reward: total was -16.490000. running mean: -10.582955\n",
      "ep 2909: ep_len:56 episode reward: total was 25.000000. running mean: -10.227125\n",
      "ep 2909: ep_len:102 episode reward: total was 48.000000. running mean: -9.644854\n",
      "ep 2909: ep_len:734 episode reward: total was -47.130000. running mean: -10.019705\n",
      "ep 2909: ep_len:3644 episode reward: total was -51.650000. running mean: -10.436008\n",
      "ep 2909: ep_len:1590 episode reward: total was -19.720000. running mean: -10.528848\n",
      "ep 2909: ep_len:837 episode reward: total was 17.780000. running mean: -10.245760\n",
      "ep 2909: ep_len:1510 episode reward: total was -34.780000. running mean: -10.491102\n",
      "ep 2909: ep_len:164 episode reward: total was 80.500000. running mean: -9.581191\n",
      "ep 2909: ep_len:66 episode reward: total was 31.500000. running mean: -9.170379\n",
      "ep 2909: ep_len:96 episode reward: total was 46.500000. running mean: -8.613675\n",
      "ep 2909: ep_len:500 episode reward: total was 15.830000. running mean: -8.369239\n",
      "ep 2909: ep_len:2802 episode reward: total was -12.000000. running mean: -8.405546\n",
      "ep 2909: ep_len:46 episode reward: total was 21.500000. running mean: -8.106491\n",
      "epsilon:0.009992 episode_count: 43784. steps_count: 47080392.000000\n",
      "ep 2910: ep_len:1435 episode reward: total was 1.320000. running mean: -8.012226\n",
      "ep 2910: ep_len:629 episode reward: total was -47.410000. running mean: -8.406204\n",
      "ep 2910: ep_len:2964 episode reward: total was -34.300000. running mean: -8.665142\n",
      "ep 2910: ep_len:656 episode reward: total was 18.490000. running mean: -8.393590\n",
      "ep 2910: ep_len:34 episode reward: total was 15.500000. running mean: -8.154654\n",
      "ep 2910: ep_len:138 episode reward: total was 66.000000. running mean: -7.413108\n",
      "ep 2910: ep_len:63 episode reward: total was 30.000000. running mean: -7.038977\n",
      "ep 2910: ep_len:1008 episode reward: total was -13.410000. running mean: -7.102687\n",
      "ep 2910: ep_len:3953 episode reward: total was -753.470000. running mean: -14.566360\n",
      "ep 2910: ep_len:774 episode reward: total was -15.180000. running mean: -14.572496\n",
      "ep 2910: ep_len:619 episode reward: total was -41.540000. running mean: -14.842171\n",
      "ep 2910: ep_len:612 episode reward: total was -19.300000. running mean: -14.886750\n",
      "ep 2910: ep_len:55 episode reward: total was 26.000000. running mean: -14.477882\n",
      "ep 2910: ep_len:106 episode reward: total was 50.000000. running mean: -13.833103\n",
      "ep 2910: ep_len:69 episode reward: total was 31.500000. running mean: -13.379772\n",
      "ep 2910: ep_len:1440 episode reward: total was 12.670000. running mean: -13.119275\n",
      "ep 2910: ep_len:2954 episode reward: total was -1.260000. running mean: -13.000682\n",
      "ep 2910: ep_len:45 episode reward: total was 21.000000. running mean: -12.660675\n",
      "epsilon:0.009992 episode_count: 43802. steps_count: 47097946.000000\n",
      "ep 2911: ep_len:705 episode reward: total was -38.160000. running mean: -12.915668\n",
      "ep 2911: ep_len:201 episode reward: total was 15.380000. running mean: -12.632712\n",
      "ep 2911: ep_len:71 episode reward: total was 34.000000. running mean: -12.166384\n",
      "ep 2911: ep_len:849 episode reward: total was -9.560000. running mean: -12.140321\n",
      "ep 2911: ep_len:102 episode reward: total was 49.500000. running mean: -11.523917\n",
      "ep 2911: ep_len:1543 episode reward: total was 21.780000. running mean: -11.190878\n",
      "ep 2911: ep_len:676 episode reward: total was 27.990000. running mean: -10.799069\n",
      "ep 2911: ep_len:991 episode reward: total was -15.220000. running mean: -10.843279\n",
      "ep 2911: ep_len:7197 episode reward: total was 16.500000. running mean: -10.569846\n",
      "ep 2911: ep_len:618 episode reward: total was -32.860000. running mean: -10.792748\n",
      "ep 2911: ep_len:77 episode reward: total was 35.500000. running mean: -10.329820\n",
      "ep 2911: ep_len:80 episode reward: total was 37.000000. running mean: -9.856522\n",
      "ep 2911: ep_len:1138 episode reward: total was 1.110000. running mean: -9.746857\n",
      "ep 2911: ep_len:2842 episode reward: total was -2.470000. running mean: -9.674088\n",
      "epsilon:0.009992 episode_count: 43816. steps_count: 47115036.000000\n",
      "ep 2912: ep_len:1135 episode reward: total was -42.380000. running mean: -10.001147\n",
      "ep 2912: ep_len:633 episode reward: total was 15.160000. running mean: -9.749536\n",
      "ep 2912: ep_len:3015 episode reward: total was -34.670000. running mean: -9.998740\n",
      "ep 2912: ep_len:521 episode reward: total was 1.880000. running mean: -9.879953\n",
      "ep 2912: ep_len:38 episode reward: total was 17.500000. running mean: -9.606153\n",
      "ep 2912: ep_len:47 episode reward: total was 20.500000. running mean: -9.305092\n",
      "ep 2912: ep_len:1473 episode reward: total was -77.400000. running mean: -9.986041\n",
      "ep 2912: ep_len:3702 episode reward: total was -80.940000. running mean: -10.695581\n",
      "ep 2912: ep_len:1218 episode reward: total was -47.580000. running mean: -11.064425\n",
      "ep 2912: ep_len:840 episode reward: total was -20.060000. running mean: -11.154381\n",
      "ep 2912: ep_len:1012 episode reward: total was 5.670000. running mean: -10.986137\n",
      "ep 2912: ep_len:47 episode reward: total was 22.000000. running mean: -10.656275\n",
      "ep 2912: ep_len:182 episode reward: total was 89.500000. running mean: -9.654713\n",
      "ep 2912: ep_len:628 episode reward: total was 3.870000. running mean: -9.519465\n",
      "ep 2912: ep_len:2843 episode reward: total was 2.160000. running mean: -9.402671\n",
      "ep 2912: ep_len:45 episode reward: total was 21.000000. running mean: -9.098644\n",
      "epsilon:0.009992 episode_count: 43832. steps_count: 47132415.000000\n",
      "ep 2913: ep_len:1061 episode reward: total was -15.820000. running mean: -9.165858\n",
      "ep 2913: ep_len:764 episode reward: total was -32.240000. running mean: -9.396599\n",
      "ep 2913: ep_len:2983 episode reward: total was -37.670000. running mean: -9.679333\n",
      "ep 2913: ep_len:582 episode reward: total was -14.550000. running mean: -9.728040\n",
      "ep 2913: ep_len:111 episode reward: total was 52.500000. running mean: -9.105759\n",
      "ep 2913: ep_len:28 episode reward: total was 12.500000. running mean: -8.889702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2913: ep_len:500 episode reward: total was -5.170000. running mean: -8.852505\n",
      "ep 2913: ep_len:350 episode reward: total was 15.100000. running mean: -8.612980\n",
      "ep 2913: ep_len:538 episode reward: total was -17.010000. running mean: -8.696950\n",
      "ep 2913: ep_len:686 episode reward: total was -2.030000. running mean: -8.630280\n",
      "ep 2913: ep_len:604 episode reward: total was -4.230000. running mean: -8.586278\n",
      "ep 2913: ep_len:132 episode reward: total was 64.500000. running mean: -7.855415\n",
      "ep 2913: ep_len:1486 episode reward: total was 0.490000. running mean: -7.771961\n",
      "ep 2913: ep_len:2796 episode reward: total was -9.460000. running mean: -7.788841\n",
      "epsilon:0.009992 episode_count: 43846. steps_count: 47145036.000000\n",
      "ep 2914: ep_len:773 episode reward: total was -69.690000. running mean: -8.407853\n",
      "ep 2914: ep_len:754 episode reward: total was -21.920000. running mean: -8.542974\n",
      "ep 2914: ep_len:60 episode reward: total was 28.500000. running mean: -8.172544\n",
      "ep 2914: ep_len:2854 episode reward: total was -47.010000. running mean: -8.560919\n",
      "ep 2914: ep_len:857 episode reward: total was 13.410000. running mean: -8.341210\n",
      "ep 2914: ep_len:126 episode reward: total was 55.500000. running mean: -7.702798\n",
      "ep 2914: ep_len:1421 episode reward: total was 29.850000. running mean: -7.327270\n",
      "ep 2914: ep_len:3691 episode reward: total was -119.260000. running mean: -8.446597\n",
      "ep 2914: ep_len:534 episode reward: total was -9.980000. running mean: -8.461931\n",
      "ep 2914: ep_len:7304 episode reward: total was 17.140000. running mean: -8.205912\n",
      "ep 2914: ep_len:593 episode reward: total was -9.390000. running mean: -8.217753\n",
      "ep 2914: ep_len:110 episode reward: total was 50.500000. running mean: -7.630575\n",
      "ep 2914: ep_len:2073 episode reward: total was -160.230000. running mean: -9.156569\n",
      "ep 2914: ep_len:2851 episode reward: total was -27.670000. running mean: -9.341704\n",
      "epsilon:0.009992 episode_count: 43860. steps_count: 47169037.000000\n",
      "ep 2915: ep_len:719 episode reward: total was -66.710000. running mean: -9.915387\n",
      "ep 2915: ep_len:500 episode reward: total was 32.610000. running mean: -9.490133\n",
      "ep 2915: ep_len:49 episode reward: total was 23.000000. running mean: -9.165231\n",
      "ep 2915: ep_len:2810 episode reward: total was -140.310000. running mean: -10.476679\n",
      "ep 2915: ep_len:1389 episode reward: total was 10.380000. running mean: -10.268112\n",
      "ep 2915: ep_len:43 episode reward: total was 20.000000. running mean: -9.965431\n",
      "ep 2915: ep_len:500 episode reward: total was 20.700000. running mean: -9.658777\n",
      "ep 2915: ep_len:3619 episode reward: total was -23.190000. running mean: -9.794089\n",
      "ep 2915: ep_len:577 episode reward: total was -10.560000. running mean: -9.801748\n",
      "ep 2915: ep_len:689 episode reward: total was 36.650000. running mean: -9.337231\n",
      "ep 2915: ep_len:732 episode reward: total was -13.540000. running mean: -9.379258\n",
      "ep 2915: ep_len:49 episode reward: total was 23.000000. running mean: -9.055466\n",
      "ep 2915: ep_len:173 episode reward: total was 83.500000. running mean: -8.129911\n",
      "ep 2915: ep_len:86 episode reward: total was 41.500000. running mean: -7.633612\n",
      "ep 2915: ep_len:713 episode reward: total was 32.770000. running mean: -7.229576\n",
      "ep 2915: ep_len:2790 episode reward: total was -8.870000. running mean: -7.245980\n",
      "ep 2915: ep_len:65 episode reward: total was 31.000000. running mean: -6.863520\n",
      "epsilon:0.009992 episode_count: 43877. steps_count: 47184540.000000\n",
      "ep 2916: ep_len:1459 episode reward: total was 15.950000. running mean: -6.635385\n",
      "ep 2916: ep_len:733 episode reward: total was -15.060000. running mean: -6.719631\n",
      "ep 2916: ep_len:2976 episode reward: total was -49.590000. running mean: -7.148335\n",
      "ep 2916: ep_len:784 episode reward: total was -20.610000. running mean: -7.282952\n",
      "ep 2916: ep_len:65 episode reward: total was 28.000000. running mean: -6.930122\n",
      "ep 2916: ep_len:908 episode reward: total was 64.700000. running mean: -6.213821\n",
      "ep 2916: ep_len:3919 episode reward: total was -129.330000. running mean: -7.444983\n",
      "ep 2916: ep_len:592 episode reward: total was -22.530000. running mean: -7.595833\n",
      "ep 2916: ep_len:745 episode reward: total was 6.760000. running mean: -7.452275\n",
      "ep 2916: ep_len:927 episode reward: total was 10.850000. running mean: -7.269252\n",
      "ep 2916: ep_len:101 episode reward: total was 46.000000. running mean: -6.736559\n",
      "ep 2916: ep_len:650 episode reward: total was 2.390000. running mean: -6.645294\n",
      "ep 2916: ep_len:2955 episode reward: total was -35.130000. running mean: -6.930141\n",
      "epsilon:0.009992 episode_count: 43890. steps_count: 47201354.000000\n",
      "ep 2917: ep_len:1111 episode reward: total was 3.480000. running mean: -6.826039\n",
      "ep 2917: ep_len:624 episode reward: total was -38.890000. running mean: -7.146679\n",
      "ep 2917: ep_len:3017 episode reward: total was -45.380000. running mean: -7.529012\n",
      "ep 2917: ep_len:683 episode reward: total was 8.550000. running mean: -7.368222\n",
      "ep 2917: ep_len:751 episode reward: total was 0.270000. running mean: -7.291840\n",
      "ep 2917: ep_len:347 episode reward: total was 19.110000. running mean: -7.027821\n",
      "ep 2917: ep_len:603 episode reward: total was -11.330000. running mean: -7.070843\n",
      "ep 2917: ep_len:743 episode reward: total was 11.090000. running mean: -6.889235\n",
      "ep 2917: ep_len:511 episode reward: total was 32.570000. running mean: -6.494642\n",
      "ep 2917: ep_len:683 episode reward: total was -4.450000. running mean: -6.474196\n",
      "ep 2917: ep_len:2776 episode reward: total was -5.650000. running mean: -6.465954\n",
      "ep 2917: ep_len:67 episode reward: total was 32.000000. running mean: -6.081295\n",
      "epsilon:0.009992 episode_count: 43902. steps_count: 47213270.000000\n",
      "ep 2918: ep_len:684 episode reward: total was -52.100000. running mean: -6.541482\n",
      "ep 2918: ep_len:759 episode reward: total was -3.380000. running mean: -6.509867\n",
      "ep 2918: ep_len:3044 episode reward: total was -23.250000. running mean: -6.677268\n",
      "ep 2918: ep_len:500 episode reward: total was -10.070000. running mean: -6.711195\n",
      "ep 2918: ep_len:69 episode reward: total was 31.500000. running mean: -6.329083\n",
      "ep 2918: ep_len:1131 episode reward: total was -52.470000. running mean: -6.790493\n",
      "ep 2918: ep_len:3778 episode reward: total was -189.120000. running mean: -8.613788\n",
      "ep 2918: ep_len:767 episode reward: total was -25.260000. running mean: -8.780250\n",
      "ep 2918: ep_len:806 episode reward: total was 35.270000. running mean: -8.339747\n",
      "ep 2918: ep_len:1072 episode reward: total was 0.450000. running mean: -8.251850\n",
      "ep 2918: ep_len:174 episode reward: total was 84.000000. running mean: -7.329331\n",
      "ep 2918: ep_len:74 episode reward: total was 25.000000. running mean: -7.006038\n",
      "ep 2918: ep_len:622 episode reward: total was 13.910000. running mean: -6.796878\n",
      "ep 2918: ep_len:2905 episode reward: total was -8.400000. running mean: -6.812909\n",
      "ep 2918: ep_len:49 episode reward: total was 21.500000. running mean: -6.529780\n",
      "epsilon:0.009992 episode_count: 43917. steps_count: 47229704.000000\n",
      "ep 2919: ep_len:1416 episode reward: total was 14.630000. running mean: -6.318182\n",
      "ep 2919: ep_len:1216 episode reward: total was -66.790000. running mean: -6.922900\n",
      "ep 2919: ep_len:77 episode reward: total was 35.500000. running mean: -6.498671\n",
      "ep 2919: ep_len:2964 episode reward: total was -28.500000. running mean: -6.718684\n",
      "ep 2919: ep_len:738 episode reward: total was 24.070000. running mean: -6.410798\n",
      "ep 2919: ep_len:46 episode reward: total was 20.000000. running mean: -6.146690\n",
      "ep 2919: ep_len:99 episode reward: total was 45.000000. running mean: -5.635223\n",
      "ep 2919: ep_len:500 episode reward: total was 51.630000. running mean: -5.062571\n",
      "ep 2919: ep_len:649 episode reward: total was 22.670000. running mean: -4.785245\n",
      "ep 2919: ep_len:574 episode reward: total was 9.100000. running mean: -4.646392\n",
      "ep 2919: ep_len:826 episode reward: total was 41.970000. running mean: -4.180228\n",
      "ep 2919: ep_len:1049 episode reward: total was 23.730000. running mean: -3.901126\n",
      "ep 2919: ep_len:80 episode reward: total was 38.500000. running mean: -3.477115\n",
      "ep 2919: ep_len:83 episode reward: total was 40.000000. running mean: -3.042344\n",
      "ep 2919: ep_len:1199 episode reward: total was 1.490000. running mean: -2.997020\n",
      "ep 2919: ep_len:2964 episode reward: total was -51.160000. running mean: -3.478650\n",
      "epsilon:0.009992 episode_count: 43933. steps_count: 47244184.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2920: ep_len:1135 episode reward: total was 0.070000. running mean: -3.443164\n",
      "ep 2920: ep_len:1614 episode reward: total was -47.110000. running mean: -3.879832\n",
      "ep 2920: ep_len:3044 episode reward: total was -7.300000. running mean: -3.914034\n",
      "ep 2920: ep_len:644 episode reward: total was 15.200000. running mean: -3.722893\n",
      "ep 2920: ep_len:45 episode reward: total was 21.000000. running mean: -3.475664\n",
      "ep 2920: ep_len:45 episode reward: total was 21.000000. running mean: -3.230908\n",
      "ep 2920: ep_len:676 episode reward: total was 4.730000. running mean: -3.151299\n",
      "ep 2920: ep_len:340 episode reward: total was 11.140000. running mean: -3.008386\n",
      "ep 2920: ep_len:1185 episode reward: total was 1.280000. running mean: -2.965502\n",
      "ep 2920: ep_len:843 episode reward: total was -101.160000. running mean: -3.947447\n",
      "ep 2920: ep_len:743 episode reward: total was -0.820000. running mean: -3.916172\n",
      "ep 2920: ep_len:207 episode reward: total was 97.500000. running mean: -2.902011\n",
      "ep 2920: ep_len:34 episode reward: total was 15.500000. running mean: -2.717990\n",
      "ep 2920: ep_len:65 episode reward: total was 29.500000. running mean: -2.395811\n",
      "ep 2920: ep_len:887 episode reward: total was -12.330000. running mean: -2.495152\n",
      "ep 2920: ep_len:2790 episode reward: total was 1.840000. running mean: -2.451801\n",
      "epsilon:0.009992 episode_count: 43949. steps_count: 47258481.000000\n",
      "ep 2921: ep_len:1332 episode reward: total was 16.210000. running mean: -2.265183\n",
      "ep 2921: ep_len:784 episode reward: total was -11.520000. running mean: -2.357731\n",
      "ep 2921: ep_len:2950 episode reward: total was -30.280000. running mean: -2.636954\n",
      "ep 2921: ep_len:648 episode reward: total was 5.300000. running mean: -2.557584\n",
      "ep 2921: ep_len:171 episode reward: total was 81.000000. running mean: -1.722008\n",
      "ep 2921: ep_len:56 episode reward: total was 26.500000. running mean: -1.439788\n",
      "ep 2921: ep_len:668 episode reward: total was -64.140000. running mean: -2.066790\n",
      "ep 2921: ep_len:318 episode reward: total was 22.000000. running mean: -1.826123\n",
      "ep 2921: ep_len:526 episode reward: total was -10.550000. running mean: -1.913361\n",
      "ep 2921: ep_len:896 episode reward: total was 69.650000. running mean: -1.197728\n",
      "ep 2921: ep_len:987 episode reward: total was 32.080000. running mean: -0.864950\n",
      "ep 2921: ep_len:74 episode reward: total was 34.000000. running mean: -0.516301\n",
      "ep 2921: ep_len:191 episode reward: total was 94.000000. running mean: 0.428862\n",
      "ep 2921: ep_len:2088 episode reward: total was -370.130000. running mean: -3.276727\n",
      "ep 2921: ep_len:2881 episode reward: total was -96.050000. running mean: -4.204459\n",
      "ep 2921: ep_len:71 episode reward: total was 32.500000. running mean: -3.837415\n",
      "epsilon:0.009992 episode_count: 43965. steps_count: 47273122.000000\n",
      "ep 2922: ep_len:633 episode reward: total was 20.170000. running mean: -3.597341\n",
      "ep 2922: ep_len:1714 episode reward: total was -30.950000. running mean: -3.870867\n",
      "ep 2922: ep_len:2969 episode reward: total was -40.840000. running mean: -4.240558\n",
      "ep 2922: ep_len:500 episode reward: total was 29.280000. running mean: -3.905353\n",
      "ep 2922: ep_len:48 episode reward: total was 22.500000. running mean: -3.641299\n",
      "ep 2922: ep_len:53 episode reward: total was 25.000000. running mean: -3.354886\n",
      "ep 2922: ep_len:1123 episode reward: total was -6.110000. running mean: -3.382437\n",
      "ep 2922: ep_len:665 episode reward: total was 14.140000. running mean: -3.207213\n",
      "ep 2922: ep_len:732 episode reward: total was -21.940000. running mean: -3.394541\n",
      "ep 2922: ep_len:769 episode reward: total was -20.290000. running mean: -3.563496\n",
      "ep 2922: ep_len:979 episode reward: total was -4.790000. running mean: -3.575761\n",
      "ep 2922: ep_len:81 episode reward: total was 39.000000. running mean: -3.150003\n",
      "ep 2922: ep_len:104 episode reward: total was 49.000000. running mean: -2.628503\n",
      "ep 2922: ep_len:69 episode reward: total was 33.000000. running mean: -2.272218\n",
      "ep 2922: ep_len:500 episode reward: total was 33.590000. running mean: -1.913596\n",
      "ep 2922: ep_len:2825 episode reward: total was -4.510000. running mean: -1.939560\n",
      "epsilon:0.009992 episode_count: 43981. steps_count: 47286886.000000\n",
      "ep 2923: ep_len:856 episode reward: total was -38.070000. running mean: -2.300864\n",
      "ep 2923: ep_len:715 episode reward: total was -62.760000. running mean: -2.905456\n",
      "ep 2923: ep_len:2980 episode reward: total was -46.120000. running mean: -3.337601\n",
      "ep 2923: ep_len:1024 episode reward: total was 17.800000. running mean: -3.126225\n",
      "ep 2923: ep_len:76 episode reward: total was 36.500000. running mean: -2.729963\n",
      "ep 2923: ep_len:1014 episode reward: total was -0.370000. running mean: -2.706363\n",
      "ep 2923: ep_len:3752 episode reward: total was -36.280000. running mean: -3.042100\n",
      "ep 2923: ep_len:3943 episode reward: total was -721.470000. running mean: -10.226379\n",
      "ep 2923: ep_len:7207 episode reward: total was 21.160000. running mean: -9.912515\n",
      "ep 2923: ep_len:1026 episode reward: total was 24.880000. running mean: -9.564590\n",
      "ep 2923: ep_len:96 episode reward: total was 46.500000. running mean: -9.003944\n",
      "ep 2923: ep_len:1546 episode reward: total was 32.010000. running mean: -8.593804\n",
      "ep 2923: ep_len:2888 episode reward: total was -6.910000. running mean: -8.576966\n",
      "ep 2923: ep_len:53 episode reward: total was 25.000000. running mean: -8.241197\n",
      "epsilon:0.009992 episode_count: 43995. steps_count: 47314062.000000\n",
      "ep 2924: ep_len:1126 episode reward: total was -1.030000. running mean: -8.169085\n",
      "ep 2924: ep_len:697 episode reward: total was -29.390000. running mean: -8.381294\n",
      "ep 2924: ep_len:81 episode reward: total was 39.000000. running mean: -7.907481\n",
      "ep 2924: ep_len:3141 episode reward: total was 9.320000. running mean: -7.735206\n",
      "ep 2924: ep_len:599 episode reward: total was -5.440000. running mean: -7.712254\n",
      "ep 2924: ep_len:42 episode reward: total was 19.500000. running mean: -7.440131\n",
      "ep 2924: ep_len:72 episode reward: total was 33.000000. running mean: -7.035730\n",
      "ep 2924: ep_len:66 episode reward: total was 31.500000. running mean: -6.650373\n",
      "ep 2924: ep_len:1118 episode reward: total was 2.480000. running mean: -6.559069\n",
      "ep 2924: ep_len:3644 episode reward: total was -14.310000. running mean: -6.636578\n",
      "ep 2924: ep_len:870 episode reward: total was 11.310000. running mean: -6.457113\n",
      "ep 2924: ep_len:724 episode reward: total was 33.630000. running mean: -6.056241\n",
      "ep 2924: ep_len:500 episode reward: total was 11.860000. running mean: -5.877079\n",
      "ep 2924: ep_len:115 episode reward: total was 56.000000. running mean: -5.258308\n",
      "ep 2924: ep_len:659 episode reward: total was -29.990000. running mean: -5.505625\n",
      "ep 2924: ep_len:2868 episode reward: total was -18.500000. running mean: -5.635569\n",
      "epsilon:0.009992 episode_count: 44011. steps_count: 47330384.000000\n",
      "ep 2925: ep_len:735 episode reward: total was -66.550000. running mean: -6.244713\n",
      "ep 2925: ep_len:1590 episode reward: total was -34.530000. running mean: -6.527566\n",
      "ep 2925: ep_len:32 episode reward: total was 13.000000. running mean: -6.332290\n",
      "ep 2925: ep_len:3027 episode reward: total was -20.390000. running mean: -6.472868\n",
      "ep 2925: ep_len:1672 episode reward: total was -58.960000. running mean: -6.997739\n",
      "ep 2925: ep_len:37 episode reward: total was 15.500000. running mean: -6.772761\n",
      "ep 2925: ep_len:125 episode reward: total was 59.500000. running mean: -6.110034\n",
      "ep 2925: ep_len:50 episode reward: total was 22.000000. running mean: -5.828933\n",
      "ep 2925: ep_len:1493 episode reward: total was 3.620000. running mean: -5.734444\n",
      "ep 2925: ep_len:3663 episode reward: total was -21.950000. running mean: -5.896600\n",
      "ep 2925: ep_len:1288 episode reward: total was -55.970000. running mean: -6.397334\n",
      "ep 2925: ep_len:753 episode reward: total was -20.800000. running mean: -6.541360\n",
      "ep 2925: ep_len:602 episode reward: total was -16.370000. running mean: -6.639647\n",
      "ep 2925: ep_len:67 episode reward: total was 29.000000. running mean: -6.283250\n",
      "ep 2925: ep_len:54 episode reward: total was 25.500000. running mean: -5.965418\n",
      "ep 2925: ep_len:500 episode reward: total was 50.400000. running mean: -5.401764\n",
      "ep 2925: ep_len:2791 episode reward: total was -65.150000. running mean: -5.999246\n",
      "epsilon:0.009992 episode_count: 44028. steps_count: 47348863.000000\n",
      "ep 2926: ep_len:1158 episode reward: total was -3.740000. running mean: -5.976654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2926: ep_len:698 episode reward: total was -52.780000. running mean: -6.444687\n",
      "ep 2926: ep_len:82 episode reward: total was 39.500000. running mean: -5.985240\n",
      "ep 2926: ep_len:2882 episode reward: total was -100.230000. running mean: -6.927688\n",
      "ep 2926: ep_len:693 episode reward: total was 20.330000. running mean: -6.655111\n",
      "ep 2926: ep_len:67 episode reward: total was 29.000000. running mean: -6.298560\n",
      "ep 2926: ep_len:62 episode reward: total was 28.000000. running mean: -5.955574\n",
      "ep 2926: ep_len:500 episode reward: total was 14.920000. running mean: -5.746818\n",
      "ep 2926: ep_len:3492 episode reward: total was -13.290000. running mean: -5.822250\n",
      "ep 2926: ep_len:585 episode reward: total was -34.840000. running mean: -6.112428\n",
      "ep 2926: ep_len:7268 episode reward: total was 12.130000. running mean: -5.930003\n",
      "ep 2926: ep_len:788 episode reward: total was -16.530000. running mean: -6.036003\n",
      "ep 2926: ep_len:67 episode reward: total was 30.500000. running mean: -5.670643\n",
      "ep 2926: ep_len:52 episode reward: total was 24.500000. running mean: -5.368937\n",
      "ep 2926: ep_len:101 episode reward: total was 47.500000. running mean: -4.840248\n",
      "ep 2926: ep_len:500 episode reward: total was 11.910000. running mean: -4.672745\n",
      "ep 2926: ep_len:2812 episode reward: total was -35.550000. running mean: -4.981518\n",
      "epsilon:0.009992 episode_count: 44045. steps_count: 47370670.000000\n",
      "ep 2927: ep_len:1074 episode reward: total was -22.670000. running mean: -5.158402\n",
      "ep 2927: ep_len:997 episode reward: total was -0.970000. running mean: -5.116518\n",
      "ep 2927: ep_len:3017 episode reward: total was -46.050000. running mean: -5.525853\n",
      "ep 2927: ep_len:601 episode reward: total was -1.230000. running mean: -5.482895\n",
      "ep 2927: ep_len:46 episode reward: total was 20.000000. running mean: -5.228066\n",
      "ep 2927: ep_len:90 episode reward: total was 40.500000. running mean: -4.770785\n",
      "ep 2927: ep_len:500 episode reward: total was 13.070000. running mean: -4.592377\n",
      "ep 2927: ep_len:3841 episode reward: total was -20.920000. running mean: -4.755653\n",
      "ep 2927: ep_len:681 episode reward: total was -48.170000. running mean: -5.189797\n",
      "ep 2927: ep_len:651 episode reward: total was -4.770000. running mean: -5.185599\n",
      "ep 2927: ep_len:1102 episode reward: total was 3.480000. running mean: -5.098943\n",
      "ep 2927: ep_len:145 episode reward: total was 69.500000. running mean: -4.352954\n",
      "ep 2927: ep_len:1118 episode reward: total was -16.260000. running mean: -4.472024\n",
      "ep 2927: ep_len:2823 episode reward: total was -2.510000. running mean: -4.452404\n",
      "epsilon:0.009992 episode_count: 44059. steps_count: 47387356.000000\n",
      "ep 2928: ep_len:1149 episode reward: total was -9.890000. running mean: -4.506780\n",
      "ep 2928: ep_len:789 episode reward: total was -16.520000. running mean: -4.626912\n",
      "ep 2928: ep_len:47 episode reward: total was 22.000000. running mean: -4.360643\n",
      "ep 2928: ep_len:2979 episode reward: total was -28.740000. running mean: -4.604436\n",
      "ep 2928: ep_len:500 episode reward: total was -17.390000. running mean: -4.732292\n",
      "ep 2928: ep_len:65 episode reward: total was 31.000000. running mean: -4.374969\n",
      "ep 2928: ep_len:1066 episode reward: total was -5.910000. running mean: -4.390319\n",
      "ep 2928: ep_len:3794 episode reward: total was -2.260000. running mean: -4.369016\n",
      "ep 2928: ep_len:642 episode reward: total was -7.770000. running mean: -4.403026\n",
      "ep 2928: ep_len:722 episode reward: total was -35.030000. running mean: -4.709296\n",
      "ep 2928: ep_len:819 episode reward: total was -12.180000. running mean: -4.784003\n",
      "ep 2928: ep_len:764 episode reward: total was 5.770000. running mean: -4.678463\n",
      "ep 2928: ep_len:2891 episode reward: total was -18.790000. running mean: -4.819578\n",
      "ep 2928: ep_len:46 episode reward: total was 21.500000. running mean: -4.556382\n",
      "epsilon:0.009992 episode_count: 44073. steps_count: 47403629.000000\n",
      "ep 2929: ep_len:635 episode reward: total was 10.120000. running mean: -4.409619\n",
      "ep 2929: ep_len:779 episode reward: total was -10.680000. running mean: -4.472322\n",
      "ep 2929: ep_len:43 episode reward: total was 20.000000. running mean: -4.227599\n",
      "ep 2929: ep_len:2901 episode reward: total was -88.100000. running mean: -5.066323\n",
      "ep 2929: ep_len:661 episode reward: total was 10.090000. running mean: -4.914760\n",
      "ep 2929: ep_len:50 episode reward: total was 23.500000. running mean: -4.630612\n",
      "ep 2929: ep_len:572 episode reward: total was -24.600000. running mean: -4.830306\n",
      "ep 2929: ep_len:340 episode reward: total was 27.120000. running mean: -4.510803\n",
      "ep 2929: ep_len:662 episode reward: total was -41.020000. running mean: -4.875895\n",
      "ep 2929: ep_len:731 episode reward: total was -6.450000. running mean: -4.891636\n",
      "ep 2929: ep_len:667 episode reward: total was -27.060000. running mean: -5.113320\n",
      "ep 2929: ep_len:165 episode reward: total was 78.000000. running mean: -4.282187\n",
      "ep 2929: ep_len:86 episode reward: total was 41.500000. running mean: -3.824365\n",
      "ep 2929: ep_len:622 episode reward: total was 9.500000. running mean: -3.691121\n",
      "ep 2929: ep_len:2861 episode reward: total was -8.470000. running mean: -3.738910\n",
      "epsilon:0.009992 episode_count: 44088. steps_count: 47415404.000000\n",
      "ep 2930: ep_len:1160 episode reward: total was -21.920000. running mean: -3.920721\n",
      "ep 2930: ep_len:1615 episode reward: total was -28.890000. running mean: -4.170414\n",
      "ep 2930: ep_len:2892 episode reward: total was -31.050000. running mean: -4.439209\n",
      "ep 2930: ep_len:654 episode reward: total was 14.930000. running mean: -4.245517\n",
      "ep 2930: ep_len:43 episode reward: total was 20.000000. running mean: -4.003062\n",
      "ep 2930: ep_len:1035 episode reward: total was 4.610000. running mean: -3.916932\n",
      "ep 2930: ep_len:4052 episode reward: total was -172.380000. running mean: -5.601562\n",
      "ep 2930: ep_len:1563 episode reward: total was -27.910000. running mean: -5.824647\n",
      "ep 2930: ep_len:612 episode reward: total was 6.870000. running mean: -5.697700\n",
      "ep 2930: ep_len:1095 episode reward: total was -9.910000. running mean: -5.739823\n",
      "ep 2930: ep_len:68 episode reward: total was 28.000000. running mean: -5.402425\n",
      "ep 2930: ep_len:205 episode reward: total was 98.000000. running mean: -4.368401\n",
      "ep 2930: ep_len:87 episode reward: total was 39.000000. running mean: -3.934717\n",
      "ep 2930: ep_len:1117 episode reward: total was -22.330000. running mean: -4.118670\n",
      "ep 2930: ep_len:2821 episode reward: total was -27.510000. running mean: -4.352583\n",
      "epsilon:0.009992 episode_count: 44103. steps_count: 47434423.000000\n",
      "ep 2931: ep_len:1464 episode reward: total was 21.050000. running mean: -4.098557\n",
      "ep 2931: ep_len:193 episode reward: total was -1.380000. running mean: -4.071371\n",
      "ep 2931: ep_len:2970 episode reward: total was -65.280000. running mean: -4.683458\n",
      "ep 2931: ep_len:1423 episode reward: total was 21.530000. running mean: -4.421323\n",
      "ep 2931: ep_len:38 episode reward: total was 17.500000. running mean: -4.202110\n",
      "ep 2931: ep_len:120 episode reward: total was 57.000000. running mean: -3.590089\n",
      "ep 2931: ep_len:86 episode reward: total was 38.500000. running mean: -3.169188\n",
      "ep 2931: ep_len:55 episode reward: total was 26.000000. running mean: -2.877496\n",
      "ep 2931: ep_len:714 episode reward: total was 17.910000. running mean: -2.669621\n",
      "ep 2931: ep_len:345 episode reward: total was 21.140000. running mean: -2.431525\n",
      "ep 2931: ep_len:4144 episode reward: total was -420.850000. running mean: -6.615710\n",
      "ep 2931: ep_len:613 episode reward: total was 5.260000. running mean: -6.496953\n",
      "ep 2931: ep_len:1048 episode reward: total was -1.070000. running mean: -6.442683\n",
      "ep 2931: ep_len:126 episode reward: total was 62.510000. running mean: -5.753156\n",
      "ep 2931: ep_len:500 episode reward: total was 27.310000. running mean: -5.422525\n",
      "ep 2931: ep_len:2861 episode reward: total was -8.070000. running mean: -5.448999\n",
      "ep 2931: ep_len:70 episode reward: total was 33.500000. running mean: -5.059509\n",
      "epsilon:0.009992 episode_count: 44120. steps_count: 47451193.000000\n",
      "ep 2932: ep_len:701 episode reward: total was 1.790000. running mean: -4.991014\n",
      "ep 2932: ep_len:1650 episode reward: total was -34.690000. running mean: -5.288004\n",
      "ep 2932: ep_len:2967 episode reward: total was -25.310000. running mean: -5.488224\n",
      "ep 2932: ep_len:625 episode reward: total was 14.810000. running mean: -5.285242\n",
      "ep 2932: ep_len:130 episode reward: total was 62.000000. running mean: -4.612389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2932: ep_len:19 episode reward: total was 8.000000. running mean: -4.486266\n",
      "ep 2932: ep_len:689 episode reward: total was 2.680000. running mean: -4.414603\n",
      "ep 2932: ep_len:3576 episode reward: total was -217.090000. running mean: -6.541357\n",
      "ep 2932: ep_len:638 episode reward: total was -27.120000. running mean: -6.747143\n",
      "ep 2932: ep_len:774 episode reward: total was -5.310000. running mean: -6.732772\n",
      "ep 2932: ep_len:687 episode reward: total was -5.860000. running mean: -6.724044\n",
      "ep 2932: ep_len:89 episode reward: total was 41.500000. running mean: -6.241804\n",
      "ep 2932: ep_len:1469 episode reward: total was 19.540000. running mean: -5.983986\n",
      "ep 2932: ep_len:2736 episode reward: total was -18.990000. running mean: -6.114046\n",
      "ep 2932: ep_len:52 episode reward: total was 23.000000. running mean: -5.822905\n",
      "epsilon:0.009992 episode_count: 44135. steps_count: 47467995.000000\n",
      "ep 2933: ep_len:810 episode reward: total was -42.570000. running mean: -6.190376\n",
      "ep 2933: ep_len:918 episode reward: total was 6.320000. running mean: -6.065273\n",
      "ep 2933: ep_len:77 episode reward: total was 34.000000. running mean: -5.664620\n",
      "ep 2933: ep_len:3008 episode reward: total was -54.860000. running mean: -6.156574\n",
      "ep 2933: ep_len:671 episode reward: total was 22.080000. running mean: -5.874208\n",
      "ep 2933: ep_len:38 episode reward: total was 17.500000. running mean: -5.640466\n",
      "ep 2933: ep_len:98 episode reward: total was 47.500000. running mean: -5.109061\n",
      "ep 2933: ep_len:83 episode reward: total was 38.500000. running mean: -4.672971\n",
      "ep 2933: ep_len:500 episode reward: total was 33.500000. running mean: -4.291241\n",
      "ep 2933: ep_len:642 episode reward: total was 21.100000. running mean: -4.037328\n",
      "ep 2933: ep_len:548 episode reward: total was -6.780000. running mean: -4.064755\n",
      "ep 2933: ep_len:833 episode reward: total was 25.460000. running mean: -3.769508\n",
      "ep 2933: ep_len:1081 episode reward: total was -15.620000. running mean: -3.888012\n",
      "ep 2933: ep_len:50 episode reward: total was 23.500000. running mean: -3.614132\n",
      "ep 2933: ep_len:928 episode reward: total was -48.460000. running mean: -4.062591\n",
      "ep 2933: ep_len:2855 episode reward: total was -27.750000. running mean: -4.299465\n",
      "ep 2933: ep_len:41 episode reward: total was 19.000000. running mean: -4.066470\n",
      "epsilon:0.009992 episode_count: 44152. steps_count: 47481176.000000\n",
      "ep 2934: ep_len:1077 episode reward: total was -6.080000. running mean: -4.086606\n",
      "ep 2934: ep_len:1224 episode reward: total was -36.410000. running mean: -4.409840\n",
      "ep 2934: ep_len:49 episode reward: total was 23.000000. running mean: -4.135741\n",
      "ep 2934: ep_len:3024 episode reward: total was -28.800000. running mean: -4.382384\n",
      "ep 2934: ep_len:1645 episode reward: total was -15.180000. running mean: -4.490360\n",
      "ep 2934: ep_len:874 episode reward: total was 35.690000. running mean: -4.088556\n",
      "ep 2934: ep_len:3815 episode reward: total was -52.720000. running mean: -4.574871\n",
      "ep 2934: ep_len:1281 episode reward: total was -43.920000. running mean: -4.968322\n",
      "ep 2934: ep_len:715 episode reward: total was 39.690000. running mean: -4.521739\n",
      "ep 2934: ep_len:757 episode reward: total was -15.830000. running mean: -4.634822\n",
      "ep 2934: ep_len:68 episode reward: total was 29.500000. running mean: -4.293473\n",
      "ep 2934: ep_len:500 episode reward: total was 18.410000. running mean: -4.066439\n",
      "ep 2934: ep_len:2810 episode reward: total was 0.080000. running mean: -4.024974\n",
      "epsilon:0.009992 episode_count: 44165. steps_count: 47499015.000000\n",
      "ep 2935: ep_len:628 episode reward: total was -4.240000. running mean: -4.027125\n",
      "ep 2935: ep_len:1241 episode reward: total was -45.330000. running mean: -4.440153\n",
      "ep 2935: ep_len:102 episode reward: total was 49.500000. running mean: -3.900752\n",
      "ep 2935: ep_len:500 episode reward: total was 25.320000. running mean: -3.608544\n",
      "ep 2935: ep_len:77 episode reward: total was 37.000000. running mean: -3.202459\n",
      "ep 2935: ep_len:41 episode reward: total was 19.000000. running mean: -2.980434\n",
      "ep 2935: ep_len:1877 episode reward: total was -22.070000. running mean: -3.171330\n",
      "ep 2935: ep_len:3659 episode reward: total was -11.160000. running mean: -3.251217\n",
      "ep 2935: ep_len:1598 episode reward: total was -64.510000. running mean: -3.863804\n",
      "ep 2935: ep_len:756 episode reward: total was -9.650000. running mean: -3.921666\n",
      "ep 2935: ep_len:735 episode reward: total was 7.620000. running mean: -3.806250\n",
      "ep 2935: ep_len:127 episode reward: total was 62.000000. running mean: -3.148187\n",
      "ep 2935: ep_len:1062 episode reward: total was -1.640000. running mean: -3.133105\n",
      "ep 2935: ep_len:2871 episode reward: total was 0.600000. running mean: -3.095774\n",
      "epsilon:0.009992 episode_count: 44179. steps_count: 47514289.000000\n",
      "ep 2936: ep_len:1142 episode reward: total was -1.140000. running mean: -3.076217\n",
      "ep 2936: ep_len:205 episode reward: total was 8.470000. running mean: -2.960754\n",
      "ep 2936: ep_len:2988 episode reward: total was -54.170000. running mean: -3.472847\n",
      "ep 2936: ep_len:1164 episode reward: total was -23.880000. running mean: -3.676918\n",
      "ep 2936: ep_len:28 episode reward: total was 12.500000. running mean: -3.515149\n",
      "ep 2936: ep_len:83 episode reward: total was 40.000000. running mean: -3.079998\n",
      "ep 2936: ep_len:647 episode reward: total was 2.770000. running mean: -3.021498\n",
      "ep 2936: ep_len:657 episode reward: total was 22.720000. running mean: -2.764083\n",
      "ep 2936: ep_len:581 episode reward: total was -0.420000. running mean: -2.740642\n",
      "ep 2936: ep_len:798 episode reward: total was -19.540000. running mean: -2.908635\n",
      "ep 2936: ep_len:944 episode reward: total was -12.030000. running mean: -2.999849\n",
      "ep 2936: ep_len:45 episode reward: total was 21.000000. running mean: -2.759851\n",
      "ep 2936: ep_len:111 episode reward: total was 54.000000. running mean: -2.192252\n",
      "ep 2936: ep_len:1006 episode reward: total was -19.880000. running mean: -2.369130\n",
      "ep 2936: ep_len:2833 episode reward: total was -25.770000. running mean: -2.603138\n",
      "epsilon:0.009992 episode_count: 44194. steps_count: 47527521.000000\n",
      "ep 2937: ep_len:883 episode reward: total was 24.730000. running mean: -2.329807\n",
      "ep 2937: ep_len:500 episode reward: total was 10.780000. running mean: -2.198709\n",
      "ep 2937: ep_len:3020 episode reward: total was -75.580000. running mean: -2.932522\n",
      "ep 2937: ep_len:614 episode reward: total was -1.040000. running mean: -2.913597\n",
      "ep 2937: ep_len:86 episode reward: total was 41.500000. running mean: -2.469461\n",
      "ep 2937: ep_len:500 episode reward: total was 7.050000. running mean: -2.374266\n",
      "ep 2937: ep_len:3638 episode reward: total was -57.680000. running mean: -2.927323\n",
      "ep 2937: ep_len:1502 episode reward: total was -13.160000. running mean: -3.029650\n",
      "ep 2937: ep_len:7258 episode reward: total was -89.400000. running mean: -3.893354\n",
      "ep 2937: ep_len:552 episode reward: total was -10.810000. running mean: -3.962520\n",
      "ep 2937: ep_len:92 episode reward: total was 43.000000. running mean: -3.492895\n",
      "ep 2937: ep_len:1418 episode reward: total was 17.100000. running mean: -3.286966\n",
      "ep 2937: ep_len:2856 episode reward: total was -1.450000. running mean: -3.268596\n",
      "ep 2937: ep_len:67 episode reward: total was 29.000000. running mean: -2.945910\n",
      "epsilon:0.009992 episode_count: 44208. steps_count: 47550507.000000\n",
      "ep 2938: ep_len:1174 episode reward: total was 17.550000. running mean: -2.740951\n",
      "ep 2938: ep_len:1262 episode reward: total was -46.130000. running mean: -3.174842\n",
      "ep 2938: ep_len:2974 episode reward: total was -29.930000. running mean: -3.442393\n",
      "ep 2938: ep_len:1424 episode reward: total was 23.740000. running mean: -3.170569\n",
      "ep 2938: ep_len:82 episode reward: total was 36.500000. running mean: -2.773864\n",
      "ep 2938: ep_len:57 episode reward: total was 27.000000. running mean: -2.476125\n",
      "ep 2938: ep_len:888 episode reward: total was 59.720000. running mean: -1.854164\n",
      "ep 2938: ep_len:654 episode reward: total was 30.250000. running mean: -1.533122\n",
      "ep 2938: ep_len:1252 episode reward: total was -38.150000. running mean: -1.899291\n",
      "ep 2938: ep_len:713 episode reward: total was 40.440000. running mean: -1.475898\n",
      "ep 2938: ep_len:787 episode reward: total was 6.460000. running mean: -1.396539\n",
      "ep 2938: ep_len:191 episode reward: total was 95.010000. running mean: -0.432474\n",
      "ep 2938: ep_len:36 episode reward: total was 16.500000. running mean: -0.263149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2938: ep_len:64 episode reward: total was 29.000000. running mean: 0.029483\n",
      "ep 2938: ep_len:716 episode reward: total was -48.560000. running mean: -0.456412\n",
      "ep 2938: ep_len:2848 episode reward: total was 5.300000. running mean: -0.398848\n",
      "epsilon:0.009992 episode_count: 44224. steps_count: 47565629.000000\n",
      "ep 2939: ep_len:993 episode reward: total was -29.360000. running mean: -0.688460\n",
      "ep 2939: ep_len:714 episode reward: total was -21.240000. running mean: -0.893975\n",
      "ep 2939: ep_len:73 episode reward: total was 35.000000. running mean: -0.535035\n",
      "ep 2939: ep_len:3041 episode reward: total was -47.330000. running mean: -1.002985\n",
      "ep 2939: ep_len:798 episode reward: total was 29.510000. running mean: -0.697855\n",
      "ep 2939: ep_len:144 episode reward: total was 69.000000. running mean: -0.000876\n",
      "ep 2939: ep_len:1851 episode reward: total was -52.940000. running mean: -0.530268\n",
      "ep 2939: ep_len:3640 episode reward: total was -28.030000. running mean: -0.805265\n",
      "ep 2939: ep_len:1268 episode reward: total was -43.040000. running mean: -1.227612\n",
      "ep 2939: ep_len:902 episode reward: total was 40.200000. running mean: -0.813336\n",
      "ep 2939: ep_len:662 episode reward: total was -8.860000. running mean: -0.893803\n",
      "ep 2939: ep_len:57 episode reward: total was 25.500000. running mean: -0.629865\n",
      "ep 2939: ep_len:93 episode reward: total was 45.000000. running mean: -0.173566\n",
      "ep 2939: ep_len:873 episode reward: total was -56.100000. running mean: -0.732831\n",
      "ep 2939: ep_len:2842 episode reward: total was 0.160000. running mean: -0.723902\n",
      "epsilon:0.009992 episode_count: 44239. steps_count: 47583580.000000\n",
      "ep 2940: ep_len:2145 episode reward: total was -289.160000. running mean: -3.608263\n",
      "ep 2940: ep_len:733 episode reward: total was 9.620000. running mean: -3.475981\n",
      "ep 2940: ep_len:70 episode reward: total was 30.500000. running mean: -3.136221\n",
      "ep 2940: ep_len:2964 episode reward: total was -83.890000. running mean: -3.943759\n",
      "ep 2940: ep_len:500 episode reward: total was -85.790000. running mean: -4.762221\n",
      "ep 2940: ep_len:108 episode reward: total was 51.000000. running mean: -4.204599\n",
      "ep 2940: ep_len:113 episode reward: total was 52.000000. running mean: -3.642553\n",
      "ep 2940: ep_len:1435 episode reward: total was 31.140000. running mean: -3.294727\n",
      "ep 2940: ep_len:657 episode reward: total was 25.630000. running mean: -3.005480\n",
      "ep 2940: ep_len:549 episode reward: total was -7.810000. running mean: -3.053525\n",
      "ep 2940: ep_len:826 episode reward: total was 17.840000. running mean: -2.844590\n",
      "ep 2940: ep_len:500 episode reward: total was 31.480000. running mean: -2.501344\n",
      "ep 2940: ep_len:215 episode reward: total was 100.000000. running mean: -1.476331\n",
      "ep 2940: ep_len:37 episode reward: total was 17.000000. running mean: -1.291567\n",
      "ep 2940: ep_len:107 episode reward: total was 52.000000. running mean: -0.758652\n",
      "ep 2940: ep_len:722 episode reward: total was -112.130000. running mean: -1.872365\n",
      "ep 2940: ep_len:2920 episode reward: total was -11.790000. running mean: -1.971541\n",
      "ep 2940: ep_len:46 episode reward: total was 21.500000. running mean: -1.736826\n",
      "epsilon:0.009992 episode_count: 44257. steps_count: 47598227.000000\n",
      "ep 2941: ep_len:617 episode reward: total was 23.450000. running mean: -1.484958\n",
      "ep 2941: ep_len:667 episode reward: total was -21.400000. running mean: -1.684108\n",
      "ep 2941: ep_len:82 episode reward: total was 38.000000. running mean: -1.287267\n",
      "ep 2941: ep_len:2992 episode reward: total was -26.240000. running mean: -1.536794\n",
      "ep 2941: ep_len:697 episode reward: total was 9.530000. running mean: -1.426127\n",
      "ep 2941: ep_len:111 episode reward: total was 52.500000. running mean: -0.886865\n",
      "ep 2941: ep_len:52 episode reward: total was 24.500000. running mean: -0.632997\n",
      "ep 2941: ep_len:771 episode reward: total was -23.220000. running mean: -0.858867\n",
      "ep 2941: ep_len:3566 episode reward: total was -10.530000. running mean: -0.955578\n",
      "ep 2941: ep_len:622 episode reward: total was -29.700000. running mean: -1.243022\n",
      "ep 2941: ep_len:788 episode reward: total was 8.420000. running mean: -1.146392\n",
      "ep 2941: ep_len:617 episode reward: total was 13.630000. running mean: -0.998628\n",
      "ep 2941: ep_len:141 episode reward: total was 67.500000. running mean: -0.313642\n",
      "ep 2941: ep_len:500 episode reward: total was 43.330000. running mean: 0.122795\n",
      "ep 2941: ep_len:2879 episode reward: total was 1.080000. running mean: 0.132367\n",
      "epsilon:0.009992 episode_count: 44272. steps_count: 47613329.000000\n",
      "ep 2942: ep_len:754 episode reward: total was -45.640000. running mean: -0.325357\n",
      "ep 2942: ep_len:706 episode reward: total was -38.160000. running mean: -0.703703\n",
      "ep 2942: ep_len:49 episode reward: total was 21.500000. running mean: -0.481666\n",
      "ep 2942: ep_len:2951 episode reward: total was -130.850000. running mean: -1.785350\n",
      "ep 2942: ep_len:639 episode reward: total was -3.010000. running mean: -1.797596\n",
      "ep 2942: ep_len:42 episode reward: total was 19.500000. running mean: -1.584620\n",
      "ep 2942: ep_len:645 episode reward: total was 6.280000. running mean: -1.505974\n",
      "ep 2942: ep_len:3601 episode reward: total was -14.280000. running mean: -1.633714\n",
      "ep 2942: ep_len:2197 episode reward: total was -217.830000. running mean: -3.795677\n",
      "ep 2942: ep_len:799 episode reward: total was 29.230000. running mean: -3.465420\n",
      "ep 2942: ep_len:500 episode reward: total was 11.820000. running mean: -3.312566\n",
      "ep 2942: ep_len:127 episode reward: total was 57.500000. running mean: -2.704441\n",
      "ep 2942: ep_len:115 episode reward: total was 56.000000. running mean: -2.117396\n",
      "ep 2942: ep_len:759 episode reward: total was -13.790000. running mean: -2.234122\n",
      "ep 2942: ep_len:2768 episode reward: total was -5.970000. running mean: -2.271481\n",
      "ep 2942: ep_len:40 episode reward: total was 17.000000. running mean: -2.078766\n",
      "epsilon:0.009992 episode_count: 44288. steps_count: 47630021.000000\n",
      "ep 2943: ep_len:947 episode reward: total was -6.560000. running mean: -2.123578\n",
      "ep 2943: ep_len:674 episode reward: total was -9.590000. running mean: -2.198243\n",
      "ep 2943: ep_len:2988 episode reward: total was -54.750000. running mean: -2.723760\n",
      "ep 2943: ep_len:673 episode reward: total was 12.340000. running mean: -2.573123\n",
      "ep 2943: ep_len:47 episode reward: total was 22.000000. running mean: -2.327391\n",
      "ep 2943: ep_len:170 episode reward: total was 80.500000. running mean: -1.499117\n",
      "ep 2943: ep_len:48 episode reward: total was 22.500000. running mean: -1.259126\n",
      "ep 2943: ep_len:500 episode reward: total was 5.390000. running mean: -1.192635\n",
      "ep 2943: ep_len:342 episode reward: total was 18.750000. running mean: -0.993209\n",
      "ep 2943: ep_len:774 episode reward: total was -11.750000. running mean: -1.100777\n",
      "ep 2943: ep_len:892 episode reward: total was 60.730000. running mean: -0.482469\n",
      "ep 2943: ep_len:616 episode reward: total was 7.360000. running mean: -0.404044\n",
      "ep 2943: ep_len:1111 episode reward: total was -14.310000. running mean: -0.543104\n",
      "ep 2943: ep_len:2942 episode reward: total was 1.190000. running mean: -0.525773\n",
      "epsilon:0.009992 episode_count: 44302. steps_count: 47642745.000000\n",
      "ep 2944: ep_len:1384 episode reward: total was 19.690000. running mean: -0.323615\n",
      "ep 2944: ep_len:826 episode reward: total was -22.660000. running mean: -0.546979\n",
      "ep 2944: ep_len:3036 episode reward: total was -97.940000. running mean: -1.520909\n",
      "ep 2944: ep_len:690 episode reward: total was -14.480000. running mean: -1.650500\n",
      "ep 2944: ep_len:37 episode reward: total was 17.000000. running mean: -1.463995\n",
      "ep 2944: ep_len:1371 episode reward: total was 20.210000. running mean: -1.247255\n",
      "ep 2944: ep_len:3594 episode reward: total was -22.640000. running mean: -1.461182\n",
      "ep 2944: ep_len:3978 episode reward: total was -530.910000. running mean: -6.755671\n",
      "ep 2944: ep_len:782 episode reward: total was 15.770000. running mean: -6.530414\n",
      "ep 2944: ep_len:500 episode reward: total was 16.660000. running mean: -6.298510\n",
      "ep 2944: ep_len:94 episode reward: total was 44.000000. running mean: -5.795525\n",
      "ep 2944: ep_len:175 episode reward: total was 80.000000. running mean: -4.937569\n",
      "ep 2944: ep_len:62 episode reward: total was 29.500000. running mean: -4.593194\n",
      "ep 2944: ep_len:813 episode reward: total was -142.390000. running mean: -5.971162\n",
      "ep 2944: ep_len:2788 episode reward: total was -16.760000. running mean: -6.079050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2944: ep_len:53 episode reward: total was 23.500000. running mean: -5.783260\n",
      "epsilon:0.009992 episode_count: 44318. steps_count: 47662928.000000\n",
      "ep 2945: ep_len:742 episode reward: total was -17.100000. running mean: -5.896427\n",
      "ep 2945: ep_len:500 episode reward: total was 18.860000. running mean: -5.648863\n",
      "ep 2945: ep_len:3044 episode reward: total was 17.890000. running mean: -5.413474\n",
      "ep 2945: ep_len:582 episode reward: total was 16.400000. running mean: -5.195339\n",
      "ep 2945: ep_len:56 episode reward: total was 26.500000. running mean: -4.878386\n",
      "ep 2945: ep_len:178 episode reward: total was 87.500000. running mean: -3.954602\n",
      "ep 2945: ep_len:100 episode reward: total was 47.000000. running mean: -3.445056\n",
      "ep 2945: ep_len:1865 episode reward: total was -106.330000. running mean: -4.473906\n",
      "ep 2945: ep_len:3609 episode reward: total was -24.480000. running mean: -4.673967\n",
      "ep 2945: ep_len:680 episode reward: total was 2.050000. running mean: -4.606727\n",
      "ep 2945: ep_len:652 episode reward: total was 8.070000. running mean: -4.479960\n",
      "ep 2945: ep_len:760 episode reward: total was 28.720000. running mean: -4.147960\n",
      "ep 2945: ep_len:155 episode reward: total was 74.500000. running mean: -3.361480\n",
      "ep 2945: ep_len:73 episode reward: total was 35.000000. running mean: -2.977866\n",
      "ep 2945: ep_len:1101 episode reward: total was -2.290000. running mean: -2.970987\n",
      "ep 2945: ep_len:2844 episode reward: total was -21.400000. running mean: -3.155277\n",
      "epsilon:0.009992 episode_count: 44334. steps_count: 47679869.000000\n",
      "ep 2946: ep_len:585 episode reward: total was 12.190000. running mean: -3.001824\n",
      "ep 2946: ep_len:707 episode reward: total was -7.780000. running mean: -3.049606\n",
      "ep 2946: ep_len:70 episode reward: total was 32.000000. running mean: -2.699110\n",
      "ep 2946: ep_len:3047 episode reward: total was -27.190000. running mean: -2.944019\n",
      "ep 2946: ep_len:883 episode reward: total was 70.940000. running mean: -2.205179\n",
      "ep 2946: ep_len:87 episode reward: total was 39.000000. running mean: -1.793127\n",
      "ep 2946: ep_len:45 episode reward: total was 21.000000. running mean: -1.565196\n",
      "ep 2946: ep_len:500 episode reward: total was -44.570000. running mean: -1.995244\n",
      "ep 2946: ep_len:283 episode reward: total was 12.770000. running mean: -1.847591\n",
      "ep 2946: ep_len:808 episode reward: total was -16.680000. running mean: -1.995915\n",
      "ep 2946: ep_len:902 episode reward: total was 83.240000. running mean: -1.143556\n",
      "ep 2946: ep_len:500 episode reward: total was 33.840000. running mean: -0.793721\n",
      "ep 2946: ep_len:23 episode reward: total was 8.500000. running mean: -0.700783\n",
      "ep 2946: ep_len:125 episode reward: total was 61.000000. running mean: -0.083776\n",
      "ep 2946: ep_len:700 episode reward: total was 10.180000. running mean: 0.018862\n",
      "ep 2946: ep_len:46 episode reward: total was 21.500000. running mean: 0.233674\n",
      "epsilon:0.009992 episode_count: 44350. steps_count: 47689180.000000\n",
      "ep 2947: ep_len:814 episode reward: total was -102.780000. running mean: -0.796463\n",
      "ep 2947: ep_len:500 episode reward: total was 12.100000. running mean: -0.667499\n",
      "ep 2947: ep_len:2926 episode reward: total was -38.360000. running mean: -1.044424\n",
      "ep 2947: ep_len:1572 episode reward: total was -17.510000. running mean: -1.209079\n",
      "ep 2947: ep_len:39 episode reward: total was 18.000000. running mean: -1.016989\n",
      "ep 2947: ep_len:500 episode reward: total was 57.260000. running mean: -0.434219\n",
      "ep 2947: ep_len:603 episode reward: total was 22.790000. running mean: -0.201976\n",
      "ep 2947: ep_len:809 episode reward: total was -36.220000. running mean: -0.562157\n",
      "ep 2947: ep_len:833 episode reward: total was 55.170000. running mean: -0.004835\n",
      "ep 2947: ep_len:500 episode reward: total was -7.410000. running mean: -0.078887\n",
      "ep 2947: ep_len:766 episode reward: total was -54.120000. running mean: -0.619298\n",
      "ep 2947: ep_len:2895 episode reward: total was 8.000000. running mean: -0.533105\n",
      "epsilon:0.009992 episode_count: 44362. steps_count: 47701937.000000\n",
      "ep 2948: ep_len:693 episode reward: total was -54.850000. running mean: -1.076274\n",
      "ep 2948: ep_len:685 episode reward: total was -14.530000. running mean: -1.210811\n",
      "ep 2948: ep_len:41 episode reward: total was 17.500000. running mean: -1.023703\n",
      "ep 2948: ep_len:2971 episode reward: total was -34.300000. running mean: -1.356466\n",
      "ep 2948: ep_len:695 episode reward: total was -12.410000. running mean: -1.467001\n",
      "ep 2948: ep_len:51 episode reward: total was 24.000000. running mean: -1.212331\n",
      "ep 2948: ep_len:659 episode reward: total was -0.750000. running mean: -1.207708\n",
      "ep 2948: ep_len:3585 episode reward: total was -52.940000. running mean: -1.725031\n",
      "ep 2948: ep_len:633 episode reward: total was -7.980000. running mean: -1.787581\n",
      "ep 2948: ep_len:757 episode reward: total was -2.150000. running mean: -1.791205\n",
      "ep 2948: ep_len:1485 episode reward: total was 27.690000. running mean: -1.496393\n",
      "ep 2948: ep_len:174 episode reward: total was 86.510000. running mean: -0.616329\n",
      "ep 2948: ep_len:100 episode reward: total was 48.500000. running mean: -0.125166\n",
      "ep 2948: ep_len:1491 episode reward: total was 17.740000. running mean: 0.053486\n",
      "ep 2948: ep_len:2785 episode reward: total was -5.770000. running mean: -0.004749\n",
      "epsilon:0.009992 episode_count: 44377. steps_count: 47718742.000000\n",
      "ep 2949: ep_len:963 episode reward: total was -79.960000. running mean: -0.804301\n",
      "ep 2949: ep_len:918 episode reward: total was 16.230000. running mean: -0.633958\n",
      "ep 2949: ep_len:74 episode reward: total was 35.500000. running mean: -0.272619\n",
      "ep 2949: ep_len:3047 episode reward: total was -5.070000. running mean: -0.320593\n",
      "ep 2949: ep_len:902 episode reward: total was 78.450000. running mean: 0.467113\n",
      "ep 2949: ep_len:50 episode reward: total was 23.500000. running mean: 0.697442\n",
      "ep 2949: ep_len:62 episode reward: total was 26.500000. running mean: 0.955468\n",
      "ep 2949: ep_len:79 episode reward: total was 36.500000. running mean: 1.310913\n",
      "ep 2949: ep_len:1020 episode reward: total was -103.090000. running mean: 0.266904\n",
      "ep 2949: ep_len:3599 episode reward: total was -16.920000. running mean: 0.095035\n",
      "ep 2949: ep_len:1225 episode reward: total was -6.010000. running mean: 0.033985\n",
      "ep 2949: ep_len:794 episode reward: total was 41.400000. running mean: 0.447645\n",
      "ep 2949: ep_len:768 episode reward: total was 30.150000. running mean: 0.744668\n",
      "ep 2949: ep_len:45 episode reward: total was 21.000000. running mean: 0.947222\n",
      "ep 2949: ep_len:135 episode reward: total was 66.000000. running mean: 1.597749\n",
      "ep 2949: ep_len:48 episode reward: total was 22.500000. running mean: 1.806772\n",
      "ep 2949: ep_len:1523 episode reward: total was 27.830000. running mean: 2.067004\n",
      "ep 2949: ep_len:2773 episode reward: total was -14.580000. running mean: 1.900534\n",
      "epsilon:0.009992 episode_count: 44395. steps_count: 47736767.000000\n",
      "ep 2950: ep_len:701 episode reward: total was -20.910000. running mean: 1.672429\n",
      "ep 2950: ep_len:821 episode reward: total was -9.450000. running mean: 1.561205\n",
      "ep 2950: ep_len:2951 episode reward: total was -34.780000. running mean: 1.197793\n",
      "ep 2950: ep_len:833 episode reward: total was 50.720000. running mean: 1.693015\n",
      "ep 2950: ep_len:108 episode reward: total was 52.500000. running mean: 2.201084\n",
      "ep 2950: ep_len:500 episode reward: total was 53.710000. running mean: 2.716174\n",
      "ep 2950: ep_len:3657 episode reward: total was -17.300000. running mean: 2.516012\n",
      "ep 2950: ep_len:650 episode reward: total was 3.340000. running mean: 2.524252\n",
      "ep 2950: ep_len:7309 episode reward: total was -5.920000. running mean: 2.439809\n",
      "ep 2950: ep_len:578 episode reward: total was -12.450000. running mean: 2.290911\n",
      "ep 2950: ep_len:188 episode reward: total was 92.500000. running mean: 3.193002\n",
      "ep 2950: ep_len:36 episode reward: total was 16.500000. running mean: 3.326072\n",
      "ep 2950: ep_len:659 episode reward: total was 10.630000. running mean: 3.399111\n",
      "ep 2950: ep_len:2877 episode reward: total was -0.440000. running mean: 3.360720\n",
      "epsilon:0.009992 episode_count: 44409. steps_count: 47758635.000000\n",
      "ep 2951: ep_len:652 episode reward: total was -31.020000. running mean: 3.016913\n",
      "ep 2951: ep_len:190 episode reward: total was 9.420000. running mean: 3.080944\n",
      "ep 2951: ep_len:2998 episode reward: total was -48.070000. running mean: 2.569434\n",
      "ep 2951: ep_len:1493 episode reward: total was 18.800000. running mean: 2.731740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2951: ep_len:140 episode reward: total was 67.000000. running mean: 3.374423\n",
      "ep 2951: ep_len:501 episode reward: total was 7.790000. running mean: 3.418578\n",
      "ep 2951: ep_len:641 episode reward: total was 18.430000. running mean: 3.568693\n",
      "ep 2951: ep_len:1618 episode reward: total was -47.170000. running mean: 3.061306\n",
      "ep 2951: ep_len:781 episode reward: total was 28.100000. running mean: 3.311693\n",
      "ep 2951: ep_len:599 episode reward: total was -30.420000. running mean: 2.974376\n",
      "ep 2951: ep_len:53 episode reward: total was 25.000000. running mean: 3.194632\n",
      "ep 2951: ep_len:718 episode reward: total was -46.520000. running mean: 2.697486\n",
      "ep 2951: ep_len:2816 episode reward: total was -45.460000. running mean: 2.215911\n",
      "ep 2951: ep_len:69 episode reward: total was 33.000000. running mean: 2.523752\n",
      "epsilon:0.009992 episode_count: 44423. steps_count: 47771904.000000\n",
      "ep 2952: ep_len:500 episode reward: total was 22.820000. running mean: 2.726714\n",
      "ep 2952: ep_len:1557 episode reward: total was 0.220000. running mean: 2.701647\n",
      "ep 2952: ep_len:3020 episode reward: total was -13.540000. running mean: 2.539231\n",
      "ep 2952: ep_len:648 episode reward: total was 2.270000. running mean: 2.536538\n",
      "ep 2952: ep_len:109 episode reward: total was 53.000000. running mean: 3.041173\n",
      "ep 2952: ep_len:989 episode reward: total was -2.610000. running mean: 2.984661\n",
      "ep 2952: ep_len:663 episode reward: total was 25.170000. running mean: 3.206515\n",
      "ep 2952: ep_len:3469 episode reward: total was -596.730000. running mean: -2.792851\n",
      "ep 2952: ep_len:777 episode reward: total was 43.710000. running mean: -2.327822\n",
      "ep 2952: ep_len:500 episode reward: total was 50.920000. running mean: -1.795344\n",
      "ep 2952: ep_len:46 episode reward: total was 20.000000. running mean: -1.577390\n",
      "ep 2952: ep_len:855 episode reward: total was 14.990000. running mean: -1.411717\n",
      "ep 2952: ep_len:2859 episode reward: total was -19.940000. running mean: -1.596999\n",
      "epsilon:0.009992 episode_count: 44436. steps_count: 47787896.000000\n",
      "ep 2953: ep_len:698 episode reward: total was -30.560000. running mean: -1.886629\n",
      "ep 2953: ep_len:702 episode reward: total was -17.070000. running mean: -2.038463\n",
      "ep 2953: ep_len:2919 episode reward: total was -119.630000. running mean: -3.214378\n",
      "ep 2953: ep_len:500 episode reward: total was 13.600000. running mean: -3.046235\n",
      "ep 2953: ep_len:620 episode reward: total was -5.020000. running mean: -3.065972\n",
      "ep 2953: ep_len:669 episode reward: total was 29.760000. running mean: -2.737713\n",
      "ep 2953: ep_len:1238 episode reward: total was -17.780000. running mean: -2.888135\n",
      "ep 2953: ep_len:668 episode reward: total was 16.680000. running mean: -2.692454\n",
      "ep 2953: ep_len:685 episode reward: total was 0.960000. running mean: -2.655930\n",
      "ep 2953: ep_len:127 episode reward: total was 62.000000. running mean: -2.009370\n",
      "ep 2953: ep_len:1052 episode reward: total was 19.970000. running mean: -1.789577\n",
      "ep 2953: ep_len:2882 episode reward: total was -8.010000. running mean: -1.851781\n",
      "epsilon:0.009992 episode_count: 44448. steps_count: 47800656.000000\n",
      "ep 2954: ep_len:1002 episode reward: total was -55.660000. running mean: -2.389863\n",
      "ep 2954: ep_len:675 episode reward: total was -20.690000. running mean: -2.572864\n",
      "ep 2954: ep_len:46 episode reward: total was 21.500000. running mean: -2.332136\n",
      "ep 2954: ep_len:3005 episode reward: total was -21.280000. running mean: -2.521614\n",
      "ep 2954: ep_len:685 episode reward: total was 9.240000. running mean: -2.403998\n",
      "ep 2954: ep_len:50 episode reward: total was 23.500000. running mean: -2.144958\n",
      "ep 2954: ep_len:159 episode reward: total was 78.000000. running mean: -1.343509\n",
      "ep 2954: ep_len:67 episode reward: total was 30.500000. running mean: -1.025074\n",
      "ep 2954: ep_len:1503 episode reward: total was -271.980000. running mean: -3.734623\n",
      "ep 2954: ep_len:624 episode reward: total was 30.900000. running mean: -3.388277\n",
      "ep 2954: ep_len:611 episode reward: total was 5.450000. running mean: -3.299894\n",
      "ep 2954: ep_len:702 episode reward: total was 45.320000. running mean: -2.813695\n",
      "ep 2954: ep_len:640 episode reward: total was 20.930000. running mean: -2.576258\n",
      "ep 2954: ep_len:500 episode reward: total was 13.260000. running mean: -2.417895\n",
      "ep 2954: ep_len:2847 episode reward: total was -497.300000. running mean: -7.366716\n",
      "epsilon:0.009992 episode_count: 44463. steps_count: 47813772.000000\n",
      "ep 2955: ep_len:631 episode reward: total was 12.960000. running mean: -7.163449\n",
      "ep 2955: ep_len:789 episode reward: total was -2.520000. running mean: -7.117015\n",
      "ep 2955: ep_len:3030 episode reward: total was -17.380000. running mean: -7.219645\n",
      "ep 2955: ep_len:561 episode reward: total was -35.970000. running mean: -7.507148\n",
      "ep 2955: ep_len:110 episode reward: total was 53.500000. running mean: -6.897077\n",
      "ep 2955: ep_len:1478 episode reward: total was -108.610000. running mean: -7.914206\n",
      "ep 2955: ep_len:652 episode reward: total was 20.800000. running mean: -7.627064\n",
      "ep 2955: ep_len:691 episode reward: total was 9.930000. running mean: -7.451493\n",
      "ep 2955: ep_len:780 episode reward: total was 21.420000. running mean: -7.162778\n",
      "ep 2955: ep_len:1385 episode reward: total was -670.090000. running mean: -13.792051\n",
      "ep 2955: ep_len:44 episode reward: total was 19.000000. running mean: -13.464130\n",
      "ep 2955: ep_len:625 episode reward: total was 15.840000. running mean: -13.171089\n",
      "ep 2955: ep_len:2837 episode reward: total was -7.640000. running mean: -13.115778\n",
      "ep 2955: ep_len:47 episode reward: total was 22.000000. running mean: -12.764620\n",
      "epsilon:0.009992 episode_count: 44477. steps_count: 47827432.000000\n",
      "ep 2956: ep_len:633 episode reward: total was 0.780000. running mean: -12.629174\n",
      "ep 2956: ep_len:692 episode reward: total was -14.460000. running mean: -12.647482\n",
      "ep 2956: ep_len:3014 episode reward: total was 0.550000. running mean: -12.515507\n",
      "ep 2956: ep_len:500 episode reward: total was -20.810000. running mean: -12.598452\n",
      "ep 2956: ep_len:109 episode reward: total was 51.500000. running mean: -11.957468\n",
      "ep 2956: ep_len:1456 episode reward: total was -171.560000. running mean: -13.553493\n",
      "ep 2956: ep_len:4017 episode reward: total was -643.360000. running mean: -19.851558\n",
      "ep 2956: ep_len:1149 episode reward: total was -6.860000. running mean: -19.721642\n",
      "ep 2956: ep_len:803 episode reward: total was 37.080000. running mean: -19.153626\n",
      "ep 2956: ep_len:500 episode reward: total was 2.510000. running mean: -18.936990\n",
      "ep 2956: ep_len:66 episode reward: total was 31.500000. running mean: -18.432620\n",
      "ep 2956: ep_len:579 episode reward: total was -3.930000. running mean: -18.287594\n",
      "ep 2956: ep_len:2883 episode reward: total was -20.000000. running mean: -18.304718\n",
      "ep 2956: ep_len:67 episode reward: total was 32.000000. running mean: -17.801671\n",
      "epsilon:0.009992 episode_count: 44491. steps_count: 47843900.000000\n",
      "ep 2957: ep_len:686 episode reward: total was -11.490000. running mean: -17.738554\n",
      "ep 2957: ep_len:663 episode reward: total was -12.730000. running mean: -17.688468\n",
      "ep 2957: ep_len:71 episode reward: total was 32.500000. running mean: -17.186584\n",
      "ep 2957: ep_len:2941 episode reward: total was -74.630000. running mean: -17.761018\n",
      "ep 2957: ep_len:500 episode reward: total was -0.630000. running mean: -17.589708\n",
      "ep 2957: ep_len:67 episode reward: total was 32.000000. running mean: -17.093811\n",
      "ep 2957: ep_len:1465 episode reward: total was 18.520000. running mean: -16.737672\n",
      "ep 2957: ep_len:4039 episode reward: total was -179.560000. running mean: -18.365896\n",
      "ep 2957: ep_len:1269 episode reward: total was -34.950000. running mean: -18.531737\n",
      "ep 2957: ep_len:665 episode reward: total was 11.850000. running mean: -18.227919\n",
      "ep 2957: ep_len:630 episode reward: total was 6.070000. running mean: -17.984940\n",
      "ep 2957: ep_len:47 episode reward: total was 22.000000. running mean: -17.585091\n",
      "ep 2957: ep_len:1453 episode reward: total was -0.670000. running mean: -17.415940\n",
      "ep 2957: ep_len:2885 episode reward: total was -35.990000. running mean: -17.601681\n",
      "epsilon:0.009992 episode_count: 44505. steps_count: 47861281.000000\n",
      "ep 2958: ep_len:618 episode reward: total was -28.210000. running mean: -17.707764\n",
      "ep 2958: ep_len:209 episode reward: total was 1.990000. running mean: -17.510786\n",
      "ep 2958: ep_len:2998 episode reward: total was -6.530000. running mean: -17.400978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2958: ep_len:783 episode reward: total was 14.330000. running mean: -17.083668\n",
      "ep 2958: ep_len:51 episode reward: total was 22.500000. running mean: -16.687832\n",
      "ep 2958: ep_len:147 episode reward: total was 70.500000. running mean: -15.815953\n",
      "ep 2958: ep_len:72 episode reward: total was 33.000000. running mean: -15.327794\n",
      "ep 2958: ep_len:617 episode reward: total was 55.110000. running mean: -14.623416\n",
      "ep 2958: ep_len:3928 episode reward: total was -91.810000. running mean: -15.395282\n",
      "ep 2958: ep_len:2911 episode reward: total was -185.040000. running mean: -17.091729\n",
      "ep 2958: ep_len:709 episode reward: total was 40.610000. running mean: -16.514712\n",
      "ep 2958: ep_len:770 episode reward: total was 37.240000. running mean: -15.977165\n",
      "ep 2958: ep_len:56 episode reward: total was 26.500000. running mean: -15.552393\n",
      "ep 2958: ep_len:104 episode reward: total was 49.000000. running mean: -14.906869\n",
      "ep 2958: ep_len:132 episode reward: total was 63.000000. running mean: -14.127800\n",
      "ep 2958: ep_len:667 episode reward: total was 3.330000. running mean: -13.953222\n",
      "ep 2958: ep_len:2871 episode reward: total was -19.080000. running mean: -14.004490\n",
      "epsilon:0.009992 episode_count: 44522. steps_count: 47878924.000000\n",
      "ep 2959: ep_len:752 episode reward: total was -86.580000. running mean: -14.730245\n",
      "ep 2959: ep_len:994 episode reward: total was 47.460000. running mean: -14.108343\n",
      "ep 2959: ep_len:2926 episode reward: total was -15.000000. running mean: -14.117259\n",
      "ep 2959: ep_len:1726 episode reward: total was -27.320000. running mean: -14.249287\n",
      "ep 2959: ep_len:113 episode reward: total was 55.000000. running mean: -13.556794\n",
      "ep 2959: ep_len:57 episode reward: total was 25.500000. running mean: -13.166226\n",
      "ep 2959: ep_len:805 episode reward: total was 23.120000. running mean: -12.803364\n",
      "ep 2959: ep_len:3953 episode reward: total was -70.820000. running mean: -13.383530\n",
      "ep 2959: ep_len:752 episode reward: total was -30.670000. running mean: -13.556395\n",
      "ep 2959: ep_len:664 episode reward: total was 18.050000. running mean: -13.240331\n",
      "ep 2959: ep_len:608 episode reward: total was -4.190000. running mean: -13.149827\n",
      "ep 2959: ep_len:109 episode reward: total was 53.000000. running mean: -12.488329\n",
      "ep 2959: ep_len:1109 episode reward: total was -10.290000. running mean: -12.466346\n",
      "ep 2959: ep_len:2719 episode reward: total was -25.930000. running mean: -12.600982\n",
      "epsilon:0.009992 episode_count: 44536. steps_count: 47896211.000000\n",
      "ep 2960: ep_len:643 episode reward: total was -0.680000. running mean: -12.481773\n",
      "ep 2960: ep_len:715 episode reward: total was -27.040000. running mean: -12.627355\n",
      "ep 2960: ep_len:2981 episode reward: total was -20.730000. running mean: -12.708381\n",
      "ep 2960: ep_len:554 episode reward: total was -8.550000. running mean: -12.666798\n",
      "ep 2960: ep_len:135 episode reward: total was 61.500000. running mean: -11.925130\n",
      "ep 2960: ep_len:111 episode reward: total was 52.500000. running mean: -11.280878\n",
      "ep 2960: ep_len:782 episode reward: total was -34.800000. running mean: -11.516069\n",
      "ep 2960: ep_len:4058 episode reward: total was -165.470000. running mean: -13.055609\n",
      "ep 2960: ep_len:1601 episode reward: total was -90.740000. running mean: -13.832453\n",
      "ep 2960: ep_len:681 episode reward: total was 32.550000. running mean: -13.368628\n",
      "ep 2960: ep_len:1048 episode reward: total was 47.870000. running mean: -12.756242\n",
      "ep 2960: ep_len:52 episode reward: total was 24.500000. running mean: -12.383679\n",
      "ep 2960: ep_len:30 episode reward: total was 13.500000. running mean: -12.124843\n",
      "ep 2960: ep_len:749 episode reward: total was -77.490000. running mean: -12.778494\n",
      "ep 2960: ep_len:2897 episode reward: total was 7.870000. running mean: -12.572009\n",
      "ep 2960: ep_len:45 episode reward: total was 21.000000. running mean: -12.236289\n",
      "epsilon:0.009992 episode_count: 44552. steps_count: 47913293.000000\n",
      "ep 2961: ep_len:655 episode reward: total was -11.800000. running mean: -12.231926\n",
      "ep 2961: ep_len:783 episode reward: total was 0.570000. running mean: -12.103907\n",
      "ep 2961: ep_len:56 episode reward: total was 26.500000. running mean: -11.717868\n",
      "ep 2961: ep_len:96 episode reward: total was 46.500000. running mean: -11.135689\n",
      "ep 2961: ep_len:667 episode reward: total was -2.070000. running mean: -11.045032\n",
      "ep 2961: ep_len:1484 episode reward: total was 7.660000. running mean: -10.857982\n",
      "ep 2961: ep_len:4080 episode reward: total was -55.100000. running mean: -11.300402\n",
      "ep 2961: ep_len:918 episode reward: total was -28.360000. running mean: -11.470998\n",
      "ep 2961: ep_len:860 episode reward: total was 75.560000. running mean: -10.600688\n",
      "ep 2961: ep_len:1112 episode reward: total was -268.670000. running mean: -13.181381\n",
      "ep 2961: ep_len:141 episode reward: total was 67.500000. running mean: -12.374568\n",
      "ep 2961: ep_len:47 episode reward: total was 22.000000. running mean: -12.030822\n",
      "ep 2961: ep_len:75 episode reward: total was 36.000000. running mean: -11.550514\n",
      "ep 2961: ep_len:1474 episode reward: total was 24.420000. running mean: -11.190809\n",
      "ep 2961: ep_len:2853 episode reward: total was 2.710000. running mean: -11.051800\n",
      "epsilon:0.009992 episode_count: 44567. steps_count: 47928594.000000\n",
      "ep 2962: ep_len:1061 episode reward: total was -110.760000. running mean: -12.048882\n",
      "ep 2962: ep_len:500 episode reward: total was 11.880000. running mean: -11.809594\n",
      "ep 2962: ep_len:3053 episode reward: total was -41.360000. running mean: -12.105098\n",
      "ep 2962: ep_len:1701 episode reward: total was -23.920000. running mean: -12.223247\n",
      "ep 2962: ep_len:36 episode reward: total was 16.500000. running mean: -11.936014\n",
      "ep 2962: ep_len:64 episode reward: total was 30.500000. running mean: -11.511654\n",
      "ep 2962: ep_len:1429 episode reward: total was -141.570000. running mean: -12.812238\n",
      "ep 2962: ep_len:4150 episode reward: total was -49.610000. running mean: -13.180215\n",
      "ep 2962: ep_len:789 episode reward: total was -17.910000. running mean: -13.227513\n",
      "ep 2962: ep_len:811 episode reward: total was 49.440000. running mean: -12.600838\n",
      "ep 2962: ep_len:500 episode reward: total was -0.400000. running mean: -12.478830\n",
      "ep 2962: ep_len:143 episode reward: total was 65.500000. running mean: -11.699041\n",
      "ep 2962: ep_len:63 episode reward: total was 28.500000. running mean: -11.297051\n",
      "ep 2962: ep_len:500 episode reward: total was 21.590000. running mean: -10.968180\n",
      "ep 2962: ep_len:2853 episode reward: total was -21.100000. running mean: -11.069498\n",
      "epsilon:0.009992 episode_count: 44582. steps_count: 47946247.000000\n",
      "ep 2963: ep_len:1160 episode reward: total was 0.320000. running mean: -10.955604\n",
      "ep 2963: ep_len:687 episode reward: total was -11.190000. running mean: -10.957947\n",
      "ep 2963: ep_len:2989 episode reward: total was -32.060000. running mean: -11.168968\n",
      "ep 2963: ep_len:1189 episode reward: total was -10.500000. running mean: -11.162278\n",
      "ep 2963: ep_len:50 episode reward: total was 23.500000. running mean: -10.815656\n",
      "ep 2963: ep_len:80 episode reward: total was 34.000000. running mean: -10.367499\n",
      "ep 2963: ep_len:630 episode reward: total was -28.220000. running mean: -10.546024\n",
      "ep 2963: ep_len:3858 episode reward: total was -92.940000. running mean: -11.369964\n",
      "ep 2963: ep_len:592 episode reward: total was 4.720000. running mean: -11.209064\n",
      "ep 2963: ep_len:929 episode reward: total was 30.520000. running mean: -10.791773\n",
      "ep 2963: ep_len:1468 episode reward: total was 19.770000. running mean: -10.486156\n",
      "ep 2963: ep_len:89 episode reward: total was 43.000000. running mean: -9.951294\n",
      "ep 2963: ep_len:1156 episode reward: total was -6.790000. running mean: -9.919681\n",
      "ep 2963: ep_len:2922 episode reward: total was -23.410000. running mean: -10.054584\n",
      "epsilon:0.009992 episode_count: 44596. steps_count: 47964046.000000\n",
      "ep 2964: ep_len:724 episode reward: total was -58.580000. running mean: -10.539839\n",
      "ep 2964: ep_len:709 episode reward: total was 6.060000. running mean: -10.373840\n",
      "ep 2964: ep_len:65 episode reward: total was 26.500000. running mean: -10.005102\n",
      "ep 2964: ep_len:2993 episode reward: total was -19.810000. running mean: -10.103151\n",
      "ep 2964: ep_len:500 episode reward: total was 13.360000. running mean: -9.868519\n",
      "ep 2964: ep_len:65 episode reward: total was 31.000000. running mean: -9.459834\n",
      "ep 2964: ep_len:54 episode reward: total was 25.500000. running mean: -9.110236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2964: ep_len:1112 episode reward: total was 1.860000. running mean: -9.000533\n",
      "ep 2964: ep_len:3651 episode reward: total was -11.880000. running mean: -9.029328\n",
      "ep 2964: ep_len:591 episode reward: total was -24.960000. running mean: -9.188635\n",
      "ep 2964: ep_len:7412 episode reward: total was -36.970000. running mean: -9.466448\n",
      "ep 2964: ep_len:2828 episode reward: total was -243.330000. running mean: -11.805084\n",
      "ep 2964: ep_len:133 episode reward: total was 65.000000. running mean: -11.037033\n",
      "ep 2964: ep_len:91 episode reward: total was 42.500000. running mean: -10.501663\n",
      "ep 2964: ep_len:799 episode reward: total was 7.340000. running mean: -10.323246\n",
      "ep 2964: ep_len:2876 episode reward: total was -28.880000. running mean: -10.508814\n",
      "ep 2964: ep_len:70 episode reward: total was 30.500000. running mean: -10.098726\n",
      "epsilon:0.009992 episode_count: 44613. steps_count: 47988719.000000\n",
      "ep 2965: ep_len:712 episode reward: total was -6.690000. running mean: -10.064638\n",
      "ep 2965: ep_len:950 episode reward: total was 10.990000. running mean: -9.854092\n",
      "ep 2965: ep_len:2934 episode reward: total was -73.720000. running mean: -10.492751\n",
      "ep 2965: ep_len:500 episode reward: total was 13.960000. running mean: -10.248223\n",
      "ep 2965: ep_len:168 episode reward: total was 81.000000. running mean: -9.335741\n",
      "ep 2965: ep_len:708 episode reward: total was -7.720000. running mean: -9.319584\n",
      "ep 2965: ep_len:3680 episode reward: total was -148.490000. running mean: -10.711288\n",
      "ep 2965: ep_len:582 episode reward: total was -0.710000. running mean: -10.611275\n",
      "ep 2965: ep_len:730 episode reward: total was 37.760000. running mean: -10.127562\n",
      "ep 2965: ep_len:656 episode reward: total was -32.520000. running mean: -10.351487\n",
      "ep 2965: ep_len:1038 episode reward: total was -4.330000. running mean: -10.291272\n",
      "ep 2965: ep_len:2866 episode reward: total was -11.850000. running mean: -10.306859\n",
      "ep 2965: ep_len:60 episode reward: total was 27.000000. running mean: -9.933791\n",
      "epsilon:0.009992 episode_count: 44626. steps_count: 48004303.000000\n",
      "ep 2966: ep_len:1049 episode reward: total was -85.630000. running mean: -10.690753\n",
      "ep 2966: ep_len:702 episode reward: total was -26.290000. running mean: -10.846745\n",
      "ep 2966: ep_len:2983 episode reward: total was 5.070000. running mean: -10.687578\n",
      "ep 2966: ep_len:729 episode reward: total was 32.540000. running mean: -10.255302\n",
      "ep 2966: ep_len:112 episode reward: total was 54.500000. running mean: -9.607749\n",
      "ep 2966: ep_len:500 episode reward: total was 2.820000. running mean: -9.483471\n",
      "ep 2966: ep_len:3862 episode reward: total was -37.930000. running mean: -9.767937\n",
      "ep 2966: ep_len:725 episode reward: total was -8.690000. running mean: -9.757157\n",
      "ep 2966: ep_len:717 episode reward: total was -10.540000. running mean: -9.764986\n",
      "ep 2966: ep_len:612 episode reward: total was 1.910000. running mean: -9.648236\n",
      "ep 2966: ep_len:105 episode reward: total was 51.000000. running mean: -9.041754\n",
      "ep 2966: ep_len:839 episode reward: total was 34.810000. running mean: -8.603236\n",
      "ep 2966: ep_len:2823 episode reward: total was -9.920000. running mean: -8.616404\n",
      "epsilon:0.009992 episode_count: 44639. steps_count: 48020061.000000\n",
      "ep 2967: ep_len:990 episode reward: total was -55.920000. running mean: -9.089440\n",
      "ep 2967: ep_len:631 episode reward: total was 13.210000. running mean: -8.866445\n",
      "ep 2967: ep_len:64 episode reward: total was 30.500000. running mean: -8.472781\n",
      "ep 2967: ep_len:3030 episode reward: total was -140.010000. running mean: -9.788153\n",
      "ep 2967: ep_len:902 episode reward: total was 85.460000. running mean: -8.835671\n",
      "ep 2967: ep_len:59 episode reward: total was 25.000000. running mean: -8.497315\n",
      "ep 2967: ep_len:64 episode reward: total was 29.000000. running mean: -8.122342\n",
      "ep 2967: ep_len:650 episode reward: total was 4.450000. running mean: -7.996618\n",
      "ep 2967: ep_len:3891 episode reward: total was -97.780000. running mean: -8.894452\n",
      "ep 2967: ep_len:521 episode reward: total was -33.250000. running mean: -9.138007\n",
      "ep 2967: ep_len:658 episode reward: total was -1.880000. running mean: -9.065427\n",
      "ep 2967: ep_len:897 episode reward: total was 12.570000. running mean: -8.849073\n",
      "ep 2967: ep_len:57 episode reward: total was 27.000000. running mean: -8.490582\n",
      "ep 2967: ep_len:184 episode reward: total was 89.000000. running mean: -7.515677\n",
      "ep 2967: ep_len:653 episode reward: total was -10.810000. running mean: -7.548620\n",
      "ep 2967: ep_len:2875 episode reward: total was -13.560000. running mean: -7.608734\n",
      "epsilon:0.009992 episode_count: 44655. steps_count: 48036187.000000\n",
      "ep 2968: ep_len:1026 episode reward: total was -100.000000. running mean: -8.532646\n",
      "ep 2968: ep_len:957 episode reward: total was 37.320000. running mean: -8.074120\n",
      "ep 2968: ep_len:76 episode reward: total was 35.000000. running mean: -7.643379\n",
      "ep 2968: ep_len:2924 episode reward: total was 9.290000. running mean: -7.474045\n",
      "ep 2968: ep_len:500 episode reward: total was 15.890000. running mean: -7.240404\n",
      "ep 2968: ep_len:48 episode reward: total was 22.500000. running mean: -6.943000\n",
      "ep 2968: ep_len:779 episode reward: total was -18.950000. running mean: -7.063070\n",
      "ep 2968: ep_len:675 episode reward: total was 12.190000. running mean: -6.870540\n",
      "ep 2968: ep_len:602 episode reward: total was -18.550000. running mean: -6.987334\n",
      "ep 2968: ep_len:791 episode reward: total was 9.290000. running mean: -6.824561\n",
      "ep 2968: ep_len:598 episode reward: total was -7.320000. running mean: -6.829515\n",
      "ep 2968: ep_len:117 episode reward: total was 57.000000. running mean: -6.191220\n",
      "ep 2968: ep_len:65 episode reward: total was 31.000000. running mean: -5.819308\n",
      "ep 2968: ep_len:1441 episode reward: total was -63.350000. running mean: -6.394615\n",
      "ep 2968: ep_len:2889 episode reward: total was 0.050000. running mean: -6.330169\n",
      "ep 2968: ep_len:66 episode reward: total was 30.000000. running mean: -5.966867\n",
      "epsilon:0.009992 episode_count: 44671. steps_count: 48049741.000000\n",
      "ep 2969: ep_len:773 episode reward: total was -95.460000. running mean: -6.861798\n",
      "ep 2969: ep_len:1602 episode reward: total was -102.540000. running mean: -7.818580\n",
      "ep 2969: ep_len:2956 episode reward: total was -17.980000. running mean: -7.920195\n",
      "ep 2969: ep_len:1666 episode reward: total was -82.520000. running mean: -8.666193\n",
      "ep 2969: ep_len:101 episode reward: total was 49.000000. running mean: -8.089531\n",
      "ep 2969: ep_len:1448 episode reward: total was -114.940000. running mean: -9.158035\n",
      "ep 2969: ep_len:298 episode reward: total was 8.520000. running mean: -8.981255\n",
      "ep 2969: ep_len:545 episode reward: total was -33.590000. running mean: -9.227342\n",
      "ep 2969: ep_len:656 episode reward: total was 17.480000. running mean: -8.960269\n",
      "ep 2969: ep_len:802 episode reward: total was -17.890000. running mean: -9.049566\n",
      "ep 2969: ep_len:60 episode reward: total was 27.000000. running mean: -8.689071\n",
      "ep 2969: ep_len:187 episode reward: total was 90.500000. running mean: -7.697180\n",
      "ep 2969: ep_len:1129 episode reward: total was -27.260000. running mean: -7.892808\n",
      "ep 2969: ep_len:2808 episode reward: total was -7.440000. running mean: -7.888280\n",
      "epsilon:0.009992 episode_count: 44685. steps_count: 48064772.000000\n",
      "ep 2970: ep_len:640 episode reward: total was 2.190000. running mean: -7.787497\n",
      "ep 2970: ep_len:750 episode reward: total was -6.040000. running mean: -7.770022\n",
      "ep 2970: ep_len:45 episode reward: total was 21.000000. running mean: -7.482322\n",
      "ep 2970: ep_len:2990 episode reward: total was 10.130000. running mean: -7.306199\n",
      "ep 2970: ep_len:500 episode reward: total was 38.950000. running mean: -6.843637\n",
      "ep 2970: ep_len:47 episode reward: total was 22.000000. running mean: -6.555200\n",
      "ep 2970: ep_len:1440 episode reward: total was -118.570000. running mean: -7.675348\n",
      "ep 2970: ep_len:626 episode reward: total was 25.350000. running mean: -7.345095\n",
      "ep 2970: ep_len:667 episode reward: total was -86.940000. running mean: -8.141044\n",
      "ep 2970: ep_len:817 episode reward: total was 41.970000. running mean: -7.639934\n",
      "ep 2970: ep_len:1034 episode reward: total was 42.010000. running mean: -7.143434\n",
      "ep 2970: ep_len:38 episode reward: total was 16.000000. running mean: -6.912000\n",
      "ep 2970: ep_len:85 episode reward: total was 41.000000. running mean: -6.432880\n",
      "ep 2970: ep_len:652 episode reward: total was -8.800000. running mean: -6.456551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2970: ep_len:2821 episode reward: total was 10.480000. running mean: -6.287186\n",
      "epsilon:0.009992 episode_count: 44700. steps_count: 48077924.000000\n",
      "ep 2971: ep_len:1084 episode reward: total was -207.690000. running mean: -8.301214\n",
      "ep 2971: ep_len:213 episode reward: total was 9.470000. running mean: -8.123502\n",
      "ep 2971: ep_len:3074 episode reward: total was 1.450000. running mean: -8.027767\n",
      "ep 2971: ep_len:513 episode reward: total was -14.230000. running mean: -8.089789\n",
      "ep 2971: ep_len:1010 episode reward: total was -53.090000. running mean: -8.539791\n",
      "ep 2971: ep_len:3781 episode reward: total was -204.270000. running mean: -10.497093\n",
      "ep 2971: ep_len:947 episode reward: total was -18.460000. running mean: -10.576722\n",
      "ep 2971: ep_len:653 episode reward: total was 4.100000. running mean: -10.429955\n",
      "ep 2971: ep_len:500 episode reward: total was 10.930000. running mean: -10.216355\n",
      "ep 2971: ep_len:76 episode reward: total was 36.500000. running mean: -9.749192\n",
      "ep 2971: ep_len:58 episode reward: total was 26.000000. running mean: -9.391700\n",
      "ep 2971: ep_len:675 episode reward: total was 6.010000. running mean: -9.237683\n",
      "ep 2971: ep_len:2820 episode reward: total was 5.020000. running mean: -9.095106\n",
      "ep 2971: ep_len:54 episode reward: total was 25.500000. running mean: -8.749155\n",
      "epsilon:0.009992 episode_count: 44714. steps_count: 48093382.000000\n",
      "ep 2972: ep_len:986 episode reward: total was -112.490000. running mean: -9.786564\n",
      "ep 2972: ep_len:1133 episode reward: total was 10.300000. running mean: -9.585698\n",
      "ep 2972: ep_len:52 episode reward: total was 24.500000. running mean: -9.244841\n",
      "ep 2972: ep_len:2957 episode reward: total was -46.040000. running mean: -9.612793\n",
      "ep 2972: ep_len:1160 episode reward: total was -127.920000. running mean: -10.795865\n",
      "ep 2972: ep_len:69 episode reward: total was 30.000000. running mean: -10.387906\n",
      "ep 2972: ep_len:1410 episode reward: total was -54.300000. running mean: -10.827027\n",
      "ep 2972: ep_len:500 episode reward: total was 12.480000. running mean: -10.593957\n",
      "ep 2972: ep_len:1582 episode reward: total was -75.260000. running mean: -11.240617\n",
      "ep 2972: ep_len:682 episode reward: total was 16.360000. running mean: -10.964611\n",
      "ep 2972: ep_len:950 episode reward: total was 41.330000. running mean: -10.441665\n",
      "ep 2972: ep_len:62 episode reward: total was 29.500000. running mean: -10.042248\n",
      "ep 2972: ep_len:970 episode reward: total was -19.760000. running mean: -10.139426\n",
      "ep 2972: ep_len:47 episode reward: total was 22.000000. running mean: -9.818031\n",
      "ep 2972: ep_len:52 episode reward: total was 23.000000. running mean: -9.489851\n",
      "epsilon:0.009992 episode_count: 44729. steps_count: 48105994.000000\n",
      "ep 2973: ep_len:742 episode reward: total was -69.510000. running mean: -10.090053\n",
      "ep 2973: ep_len:500 episode reward: total was 8.480000. running mean: -9.904352\n",
      "ep 2973: ep_len:2960 episode reward: total was -864.270000. running mean: -18.448009\n",
      "ep 2973: ep_len:1614 episode reward: total was -21.520000. running mean: -18.478728\n",
      "ep 2973: ep_len:60 episode reward: total was 28.500000. running mean: -18.008941\n",
      "ep 2973: ep_len:106 episode reward: total was 51.500000. running mean: -17.313852\n",
      "ep 2973: ep_len:1019 episode reward: total was -15.230000. running mean: -17.293013\n",
      "ep 2973: ep_len:621 episode reward: total was 22.330000. running mean: -16.896783\n",
      "ep 2973: ep_len:786 episode reward: total was -8.080000. running mean: -16.808615\n",
      "ep 2973: ep_len:785 episode reward: total was 9.100000. running mean: -16.549529\n",
      "ep 2973: ep_len:505 episode reward: total was 26.660000. running mean: -16.117434\n",
      "ep 2973: ep_len:47 episode reward: total was 20.500000. running mean: -15.751259\n",
      "ep 2973: ep_len:47 episode reward: total was 22.000000. running mean: -15.373747\n",
      "ep 2973: ep_len:96 episode reward: total was 45.000000. running mean: -14.770009\n",
      "ep 2973: ep_len:1484 episode reward: total was 2.000000. running mean: -14.602309\n",
      "ep 2973: ep_len:2866 episode reward: total was -14.840000. running mean: -14.604686\n",
      "epsilon:0.009992 episode_count: 44745. steps_count: 48120232.000000\n",
      "ep 2974: ep_len:719 episode reward: total was -51.800000. running mean: -14.976639\n",
      "ep 2974: ep_len:500 episode reward: total was 27.770000. running mean: -14.549173\n",
      "ep 2974: ep_len:3043 episode reward: total was 13.780000. running mean: -14.265881\n",
      "ep 2974: ep_len:833 episode reward: total was 42.570000. running mean: -13.697522\n",
      "ep 2974: ep_len:53 episode reward: total was 25.000000. running mean: -13.310547\n",
      "ep 2974: ep_len:58 episode reward: total was 27.500000. running mean: -12.902442\n",
      "ep 2974: ep_len:1413 episode reward: total was 9.650000. running mean: -12.676917\n",
      "ep 2974: ep_len:367 episode reward: total was 10.740000. running mean: -12.442748\n",
      "ep 2974: ep_len:1163 episode reward: total was -32.460000. running mean: -12.642921\n",
      "ep 2974: ep_len:7383 episode reward: total was 41.370000. running mean: -12.102791\n",
      "ep 2974: ep_len:1165 episode reward: total was 0.370000. running mean: -11.978064\n",
      "ep 2974: ep_len:55 episode reward: total was 26.000000. running mean: -11.598283\n",
      "ep 2974: ep_len:159 episode reward: total was 76.500000. running mean: -10.717300\n",
      "ep 2974: ep_len:76 episode reward: total was 35.000000. running mean: -10.260127\n",
      "ep 2974: ep_len:780 episode reward: total was -16.610000. running mean: -10.323626\n",
      "ep 2974: ep_len:2907 episode reward: total was -52.910000. running mean: -10.749490\n",
      "ep 2974: ep_len:50 episode reward: total was 23.500000. running mean: -10.406995\n",
      "epsilon:0.009992 episode_count: 44762. steps_count: 48140956.000000\n",
      "ep 2975: ep_len:1107 episode reward: total was -11.040000. running mean: -10.413325\n",
      "ep 2975: ep_len:500 episode reward: total was 16.320000. running mean: -10.145991\n",
      "ep 2975: ep_len:2914 episode reward: total was -28.720000. running mean: -10.331732\n",
      "ep 2975: ep_len:1167 episode reward: total was -14.760000. running mean: -10.376014\n",
      "ep 2975: ep_len:1436 episode reward: total was 21.570000. running mean: -10.056554\n",
      "ep 2975: ep_len:3829 episode reward: total was -247.800000. running mean: -12.433989\n",
      "ep 2975: ep_len:656 episode reward: total was -57.240000. running mean: -12.882049\n",
      "ep 2975: ep_len:680 episode reward: total was 39.950000. running mean: -12.353728\n",
      "ep 2975: ep_len:1064 episode reward: total was 16.470000. running mean: -12.065491\n",
      "ep 2975: ep_len:69 episode reward: total was 31.500000. running mean: -11.629836\n",
      "ep 2975: ep_len:44 episode reward: total was 20.500000. running mean: -11.308538\n",
      "ep 2975: ep_len:1473 episode reward: total was -0.320000. running mean: -11.198652\n",
      "ep 2975: ep_len:2874 episode reward: total was -13.600000. running mean: -11.222666\n",
      "epsilon:0.009992 episode_count: 44775. steps_count: 48158769.000000\n",
      "ep 2976: ep_len:643 episode reward: total was 11.160000. running mean: -10.998839\n",
      "ep 2976: ep_len:742 episode reward: total was -5.590000. running mean: -10.944751\n",
      "ep 2976: ep_len:79 episode reward: total was 38.000000. running mean: -10.455303\n",
      "ep 2976: ep_len:92 episode reward: total was 44.500000. running mean: -9.905750\n",
      "ep 2976: ep_len:811 episode reward: total was 36.280000. running mean: -9.443893\n",
      "ep 2976: ep_len:55 episode reward: total was 24.500000. running mean: -9.104454\n",
      "ep 2976: ep_len:705 episode reward: total was 1.010000. running mean: -9.003309\n",
      "ep 2976: ep_len:675 episode reward: total was 13.750000. running mean: -8.775776\n",
      "ep 2976: ep_len:500 episode reward: total was 26.180000. running mean: -8.426218\n",
      "ep 2976: ep_len:7251 episode reward: total was 60.070000. running mean: -7.741256\n",
      "ep 2976: ep_len:638 episode reward: total was -6.590000. running mean: -7.729744\n",
      "ep 2976: ep_len:58 episode reward: total was 26.000000. running mean: -7.392446\n",
      "ep 2976: ep_len:35 episode reward: total was 14.500000. running mean: -7.173522\n",
      "ep 2976: ep_len:122 episode reward: total was 56.500000. running mean: -6.536786\n",
      "ep 2976: ep_len:602 episode reward: total was -12.330000. running mean: -6.594719\n",
      "ep 2976: ep_len:2868 episode reward: total was -18.100000. running mean: -6.709771\n",
      "ep 2976: ep_len:31 episode reward: total was 14.000000. running mean: -6.502674\n",
      "epsilon:0.009992 episode_count: 44792. steps_count: 48174676.000000\n",
      "ep 2977: ep_len:1508 episode reward: total was 41.910000. running mean: -6.018547\n",
      "ep 2977: ep_len:1209 episode reward: total was -41.610000. running mean: -6.374461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2977: ep_len:2958 episode reward: total was -14.910000. running mean: -6.459817\n",
      "ep 2977: ep_len:627 episode reward: total was -4.000000. running mean: -6.435219\n",
      "ep 2977: ep_len:48 episode reward: total was 22.500000. running mean: -6.145867\n",
      "ep 2977: ep_len:64 episode reward: total was 27.500000. running mean: -5.809408\n",
      "ep 2977: ep_len:1002 episode reward: total was -3.650000. running mean: -5.787814\n",
      "ep 2977: ep_len:3657 episode reward: total was -395.380000. running mean: -9.683736\n",
      "ep 2977: ep_len:921 episode reward: total was -5.100000. running mean: -9.637898\n",
      "ep 2977: ep_len:848 episode reward: total was 26.280000. running mean: -9.278719\n",
      "ep 2977: ep_len:1743 episode reward: total was -143.640000. running mean: -10.622332\n",
      "ep 2977: ep_len:203 episode reward: total was 94.000000. running mean: -9.576109\n",
      "ep 2977: ep_len:1438 episode reward: total was 2.270000. running mean: -9.457648\n",
      "ep 2977: ep_len:45 episode reward: total was 21.000000. running mean: -9.153071\n",
      "ep 2977: ep_len:60 episode reward: total was 28.500000. running mean: -8.776540\n",
      "epsilon:0.009992 episode_count: 44807. steps_count: 48191007.000000\n",
      "ep 2978: ep_len:924 episode reward: total was 10.570000. running mean: -8.583075\n",
      "ep 2978: ep_len:1231 episode reward: total was -31.290000. running mean: -8.810144\n",
      "ep 2978: ep_len:2932 episode reward: total was -33.500000. running mean: -9.057043\n",
      "ep 2978: ep_len:500 episode reward: total was 24.620000. running mean: -8.720272\n",
      "ep 2978: ep_len:58 episode reward: total was 27.500000. running mean: -8.358070\n",
      "ep 2978: ep_len:63 episode reward: total was 28.500000. running mean: -7.989489\n",
      "ep 2978: ep_len:736 episode reward: total was -14.420000. running mean: -8.053794\n",
      "ep 2978: ep_len:643 episode reward: total was 28.860000. running mean: -7.684656\n",
      "ep 2978: ep_len:920 episode reward: total was -42.080000. running mean: -8.028610\n",
      "ep 2978: ep_len:824 episode reward: total was 12.500000. running mean: -7.823324\n",
      "ep 2978: ep_len:1141 episode reward: total was -1.890000. running mean: -7.763990\n",
      "ep 2978: ep_len:81 episode reward: total was 39.000000. running mean: -7.296350\n",
      "ep 2978: ep_len:1139 episode reward: total was -6.960000. running mean: -7.292987\n",
      "ep 2978: ep_len:2891 episode reward: total was -9.610000. running mean: -7.316157\n",
      "epsilon:0.009992 episode_count: 44821. steps_count: 48205090.000000\n",
      "ep 2979: ep_len:742 episode reward: total was -59.410000. running mean: -7.837095\n",
      "ep 2979: ep_len:1647 episode reward: total was -38.110000. running mean: -8.139825\n",
      "ep 2979: ep_len:2984 episode reward: total was 4.800000. running mean: -8.010426\n",
      "ep 2979: ep_len:713 episode reward: total was -18.290000. running mean: -8.113222\n",
      "ep 2979: ep_len:79 episode reward: total was 38.000000. running mean: -7.652090\n",
      "ep 2979: ep_len:48 episode reward: total was 18.000000. running mean: -7.395569\n",
      "ep 2979: ep_len:891 episode reward: total was 35.530000. running mean: -6.966313\n",
      "ep 2979: ep_len:616 episode reward: total was 13.130000. running mean: -6.765350\n",
      "ep 2979: ep_len:1172 episode reward: total was -10.060000. running mean: -6.798297\n",
      "ep 2979: ep_len:783 episode reward: total was 15.170000. running mean: -6.578614\n",
      "ep 2979: ep_len:1172 episode reward: total was -11.680000. running mean: -6.629627\n",
      "ep 2979: ep_len:889 episode reward: total was 45.320000. running mean: -6.110131\n",
      "ep 2979: ep_len:2911 episode reward: total was -24.990000. running mean: -6.298930\n",
      "epsilon:0.009992 episode_count: 44834. steps_count: 48219737.000000\n",
      "ep 2980: ep_len:1138 episode reward: total was -6.480000. running mean: -6.300741\n",
      "ep 2980: ep_len:757 episode reward: total was 9.090000. running mean: -6.146833\n",
      "ep 2980: ep_len:2898 episode reward: total was 5.040000. running mean: -6.034965\n",
      "ep 2980: ep_len:500 episode reward: total was 16.810000. running mean: -5.806515\n",
      "ep 2980: ep_len:35 episode reward: total was 14.500000. running mean: -5.603450\n",
      "ep 2980: ep_len:1478 episode reward: total was -6.450000. running mean: -5.611916\n",
      "ep 2980: ep_len:3644 episode reward: total was -292.610000. running mean: -8.481896\n",
      "ep 2980: ep_len:729 episode reward: total was -15.260000. running mean: -8.549677\n",
      "ep 2980: ep_len:793 episode reward: total was 10.100000. running mean: -8.363181\n",
      "ep 2980: ep_len:900 episode reward: total was 18.710000. running mean: -8.092449\n",
      "ep 2980: ep_len:120 episode reward: total was 57.000000. running mean: -7.441524\n",
      "ep 2980: ep_len:65 episode reward: total was 31.000000. running mean: -7.057109\n",
      "ep 2980: ep_len:1102 episode reward: total was -20.310000. running mean: -7.189638\n",
      "ep 2980: ep_len:2886 episode reward: total was -12.560000. running mean: -7.243342\n",
      "epsilon:0.009992 episode_count: 44848. steps_count: 48236782.000000\n",
      "ep 2981: ep_len:1111 episode reward: total was -2.190000. running mean: -7.192808\n",
      "ep 2981: ep_len:712 episode reward: total was -34.460000. running mean: -7.465480\n",
      "ep 2981: ep_len:3049 episode reward: total was 11.910000. running mean: -7.271725\n",
      "ep 2981: ep_len:686 episode reward: total was 28.270000. running mean: -6.916308\n",
      "ep 2981: ep_len:67 episode reward: total was 32.000000. running mean: -6.527145\n",
      "ep 2981: ep_len:49 episode reward: total was 21.500000. running mean: -6.246874\n",
      "ep 2981: ep_len:1388 episode reward: total was -86.280000. running mean: -7.047205\n",
      "ep 2981: ep_len:4196 episode reward: total was -67.040000. running mean: -7.647133\n",
      "ep 2981: ep_len:589 episode reward: total was -42.760000. running mean: -7.998261\n",
      "ep 2981: ep_len:845 episode reward: total was 70.940000. running mean: -7.208879\n",
      "ep 2981: ep_len:1460 episode reward: total was 2.980000. running mean: -7.106990\n",
      "ep 2981: ep_len:162 episode reward: total was 76.500000. running mean: -6.270920\n",
      "ep 2981: ep_len:39 episode reward: total was 16.500000. running mean: -6.043211\n",
      "ep 2981: ep_len:913 episode reward: total was 4.220000. running mean: -5.940579\n",
      "ep 2981: ep_len:2910 episode reward: total was -19.210000. running mean: -6.073273\n",
      "epsilon:0.009992 episode_count: 44863. steps_count: 48254958.000000\n",
      "ep 2982: ep_len:695 episode reward: total was -13.420000. running mean: -6.146740\n",
      "ep 2982: ep_len:1153 episode reward: total was 15.830000. running mean: -5.926973\n",
      "ep 2982: ep_len:3089 episode reward: total was 27.990000. running mean: -5.587803\n",
      "ep 2982: ep_len:557 episode reward: total was -22.880000. running mean: -5.760725\n",
      "ep 2982: ep_len:59 episode reward: total was 28.000000. running mean: -5.423118\n",
      "ep 2982: ep_len:653 episode reward: total was -1.720000. running mean: -5.386087\n",
      "ep 2982: ep_len:3965 episode reward: total was -76.780000. running mean: -6.100026\n",
      "ep 2982: ep_len:755 episode reward: total was 1.650000. running mean: -6.022526\n",
      "ep 2982: ep_len:697 episode reward: total was 45.970000. running mean: -5.502600\n",
      "ep 2982: ep_len:526 episode reward: total was 15.820000. running mean: -5.289374\n",
      "ep 2982: ep_len:32 episode reward: total was 14.500000. running mean: -5.091481\n",
      "ep 2982: ep_len:986 episode reward: total was 12.210000. running mean: -4.918466\n",
      "ep 2982: ep_len:2852 episode reward: total was -39.200000. running mean: -5.261281\n",
      "ep 2982: ep_len:46 episode reward: total was 17.000000. running mean: -5.038668\n",
      "epsilon:0.009992 episode_count: 44877. steps_count: 48271023.000000\n",
      "ep 2983: ep_len:1430 episode reward: total was 19.210000. running mean: -4.796182\n",
      "ep 2983: ep_len:749 episode reward: total was -3.270000. running mean: -4.780920\n",
      "ep 2983: ep_len:2988 episode reward: total was -3.250000. running mean: -4.765611\n",
      "ep 2983: ep_len:746 episode reward: total was -10.430000. running mean: -4.822255\n",
      "ep 2983: ep_len:113 episode reward: total was 55.000000. running mean: -4.224032\n",
      "ep 2983: ep_len:500 episode reward: total was 7.480000. running mean: -4.106992\n",
      "ep 2983: ep_len:3779 episode reward: total was -1.340000. running mean: -4.079322\n",
      "ep 2983: ep_len:985 episode reward: total was -14.790000. running mean: -4.186429\n",
      "ep 2983: ep_len:811 episode reward: total was 17.380000. running mean: -3.970764\n",
      "ep 2983: ep_len:920 episode reward: total was 27.400000. running mean: -3.657057\n",
      "ep 2983: ep_len:45 episode reward: total was 21.000000. running mean: -3.410486\n",
      "ep 2983: ep_len:1152 episode reward: total was -16.930000. running mean: -3.545681\n",
      "ep 2983: ep_len:2792 episode reward: total was -46.010000. running mean: -3.970324\n",
      "epsilon:0.009992 episode_count: 44890. steps_count: 48288033.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2984: ep_len:665 episode reward: total was -1.930000. running mean: -3.949921\n",
      "ep 2984: ep_len:500 episode reward: total was -3.460000. running mean: -3.945022\n",
      "ep 2984: ep_len:58 episode reward: total was 27.500000. running mean: -3.630572\n",
      "ep 2984: ep_len:2900 episode reward: total was 18.470000. running mean: -3.409566\n",
      "ep 2984: ep_len:1429 episode reward: total was 16.690000. running mean: -3.208570\n",
      "ep 2984: ep_len:49 episode reward: total was 21.500000. running mean: -2.961485\n",
      "ep 2984: ep_len:804 episode reward: total was 30.210000. running mean: -2.629770\n",
      "ep 2984: ep_len:4225 episode reward: total was -93.410000. running mean: -3.537572\n",
      "ep 2984: ep_len:1247 episode reward: total was -52.340000. running mean: -4.025596\n",
      "ep 2984: ep_len:662 episode reward: total was 7.010000. running mean: -3.915240\n",
      "ep 2984: ep_len:1042 episode reward: total was 33.790000. running mean: -3.538188\n",
      "ep 2984: ep_len:1135 episode reward: total was -12.050000. running mean: -3.623306\n",
      "ep 2984: ep_len:2952 episode reward: total was -5.870000. running mean: -3.645773\n",
      "epsilon:0.009992 episode_count: 44903. steps_count: 48305701.000000\n",
      "ep 2985: ep_len:707 episode reward: total was -50.630000. running mean: -4.115615\n",
      "ep 2985: ep_len:1592 episode reward: total was -30.650000. running mean: -4.380959\n",
      "ep 2985: ep_len:2878 episode reward: total was -15.610000. running mean: -4.493250\n",
      "ep 2985: ep_len:500 episode reward: total was -48.300000. running mean: -4.931317\n",
      "ep 2985: ep_len:60 episode reward: total was 28.500000. running mean: -4.597004\n",
      "ep 2985: ep_len:41 episode reward: total was 17.500000. running mean: -4.376034\n",
      "ep 2985: ep_len:500 episode reward: total was 19.640000. running mean: -4.135874\n",
      "ep 2985: ep_len:342 episode reward: total was 16.030000. running mean: -3.934215\n",
      "ep 2985: ep_len:613 episode reward: total was 0.910000. running mean: -3.885773\n",
      "ep 2985: ep_len:7262 episode reward: total was -7.700000. running mean: -3.923915\n",
      "ep 2985: ep_len:956 episode reward: total was 69.590000. running mean: -3.188776\n",
      "ep 2985: ep_len:164 episode reward: total was 76.000000. running mean: -2.396888\n",
      "ep 2985: ep_len:1487 episode reward: total was 15.650000. running mean: -2.216419\n",
      "ep 2985: ep_len:2822 episode reward: total was 12.050000. running mean: -2.073755\n",
      "epsilon:0.009992 episode_count: 44917. steps_count: 48325625.000000\n",
      "ep 2986: ep_len:1193 episode reward: total was 21.510000. running mean: -1.837917\n",
      "ep 2986: ep_len:677 episode reward: total was -25.200000. running mean: -2.071538\n",
      "ep 2986: ep_len:3063 episode reward: total was 19.160000. running mean: -1.859223\n",
      "ep 2986: ep_len:3690 episode reward: total was -301.020000. running mean: -4.850831\n",
      "ep 2986: ep_len:667 episode reward: total was 6.500000. running mean: -4.737322\n",
      "ep 2986: ep_len:316 episode reward: total was 10.350000. running mean: -4.586449\n",
      "ep 2986: ep_len:842 episode reward: total was -23.200000. running mean: -4.772585\n",
      "ep 2986: ep_len:695 episode reward: total was 22.160000. running mean: -4.503259\n",
      "ep 2986: ep_len:4435 episode reward: total was -1843.380000. running mean: -22.892026\n",
      "ep 2986: ep_len:181 episode reward: total was 87.500000. running mean: -21.788106\n",
      "ep 2986: ep_len:66 episode reward: total was 31.500000. running mean: -21.255225\n",
      "ep 2986: ep_len:121 episode reward: total was 59.000000. running mean: -20.452673\n",
      "ep 2986: ep_len:1465 episode reward: total was 14.450000. running mean: -20.103646\n",
      "ep 2986: ep_len:2806 episode reward: total was -7.700000. running mean: -19.979609\n",
      "ep 2986: ep_len:57 episode reward: total was 27.000000. running mean: -19.509813\n",
      "epsilon:0.009992 episode_count: 44932. steps_count: 48345899.000000\n",
      "ep 2987: ep_len:1346 episode reward: total was 11.510000. running mean: -19.199615\n",
      "ep 2987: ep_len:717 episode reward: total was -24.310000. running mean: -19.250719\n",
      "ep 2987: ep_len:57 episode reward: total was 27.000000. running mean: -18.788212\n",
      "ep 2987: ep_len:2832 episode reward: total was -26.350000. running mean: -18.863830\n",
      "ep 2987: ep_len:1283 episode reward: total was 1.380000. running mean: -18.661391\n",
      "ep 2987: ep_len:106 episode reward: total was 50.000000. running mean: -17.974777\n",
      "ep 2987: ep_len:53 episode reward: total was 25.000000. running mean: -17.545030\n",
      "ep 2987: ep_len:1465 episode reward: total was -146.530000. running mean: -18.834879\n",
      "ep 2987: ep_len:3586 episode reward: total was -46.230000. running mean: -19.108831\n",
      "ep 2987: ep_len:579 episode reward: total was -44.880000. running mean: -19.366542\n",
      "ep 2987: ep_len:7347 episode reward: total was -8.210000. running mean: -19.254977\n",
      "ep 2987: ep_len:1171 episode reward: total was 24.620000. running mean: -18.816227\n",
      "ep 2987: ep_len:78 episode reward: total was 36.000000. running mean: -18.268065\n",
      "ep 2987: ep_len:223 episode reward: total was 108.500000. running mean: -17.000384\n",
      "ep 2987: ep_len:39 episode reward: total was 18.000000. running mean: -16.650380\n",
      "ep 2987: ep_len:126 episode reward: total was 58.500000. running mean: -15.898877\n",
      "ep 2987: ep_len:732 episode reward: total was -26.180000. running mean: -16.001688\n",
      "ep 2987: ep_len:2885 episode reward: total was 11.210000. running mean: -15.729571\n",
      "ep 2987: ep_len:43 episode reward: total was 20.000000. running mean: -15.372275\n",
      "epsilon:0.009992 episode_count: 44951. steps_count: 48370567.000000\n",
      "ep 2988: ep_len:989 episode reward: total was -80.630000. running mean: -16.024852\n",
      "ep 2988: ep_len:741 episode reward: total was -37.250000. running mean: -16.237104\n",
      "ep 2988: ep_len:70 episode reward: total was 30.500000. running mean: -15.769733\n",
      "ep 2988: ep_len:2902 episode reward: total was -13.570000. running mean: -15.747736\n",
      "ep 2988: ep_len:749 episode reward: total was -14.900000. running mean: -15.739258\n",
      "ep 2988: ep_len:45 episode reward: total was 21.000000. running mean: -15.371866\n",
      "ep 2988: ep_len:75 episode reward: total was 36.000000. running mean: -14.858147\n",
      "ep 2988: ep_len:1092 episode reward: total was 6.530000. running mean: -14.644265\n",
      "ep 2988: ep_len:300 episode reward: total was 8.840000. running mean: -14.409423\n",
      "ep 2988: ep_len:942 episode reward: total was -24.190000. running mean: -14.507229\n",
      "ep 2988: ep_len:717 episode reward: total was -3.770000. running mean: -14.399856\n",
      "ep 2988: ep_len:1059 episode reward: total was 35.650000. running mean: -13.899358\n",
      "ep 2988: ep_len:165 episode reward: total was 81.000000. running mean: -12.950364\n",
      "ep 2988: ep_len:98 episode reward: total was 47.500000. running mean: -12.345861\n",
      "ep 2988: ep_len:752 episode reward: total was -82.540000. running mean: -13.047802\n",
      "ep 2988: ep_len:2903 episode reward: total was -5.380000. running mean: -12.971124\n",
      "epsilon:0.009992 episode_count: 44967. steps_count: 48384166.000000\n",
      "ep 2989: ep_len:968 episode reward: total was -230.640000. running mean: -15.147813\n",
      "ep 2989: ep_len:500 episode reward: total was 12.460000. running mean: -14.871735\n",
      "ep 2989: ep_len:2927 episode reward: total was 7.440000. running mean: -14.648617\n",
      "ep 2989: ep_len:552 episode reward: total was -24.950000. running mean: -14.751631\n",
      "ep 2989: ep_len:35 episode reward: total was 14.500000. running mean: -14.459115\n",
      "ep 2989: ep_len:94 episode reward: total was 45.500000. running mean: -13.859524\n",
      "ep 2989: ep_len:59 episode reward: total was 28.000000. running mean: -13.440928\n",
      "ep 2989: ep_len:1779 episode reward: total was -113.460000. running mean: -14.441119\n",
      "ep 2989: ep_len:3639 episode reward: total was -213.820000. running mean: -16.434908\n",
      "ep 2989: ep_len:558 episode reward: total was 9.850000. running mean: -16.172059\n",
      "ep 2989: ep_len:698 episode reward: total was 48.920000. running mean: -15.521138\n",
      "ep 2989: ep_len:500 episode reward: total was 11.640000. running mean: -15.249527\n",
      "ep 2989: ep_len:173 episode reward: total was 80.500000. running mean: -14.292032\n",
      "ep 2989: ep_len:1217 episode reward: total was 11.400000. running mean: -14.035111\n",
      "ep 2989: ep_len:2833 episode reward: total was -23.290000. running mean: -14.127660\n",
      "ep 2989: ep_len:66 episode reward: total was 30.000000. running mean: -13.686384\n",
      "epsilon:0.009992 episode_count: 44983. steps_count: 48400764.000000\n",
      "ep 2990: ep_len:651 episode reward: total was -6.150000. running mean: -13.611020\n",
      "ep 2990: ep_len:758 episode reward: total was 12.600000. running mean: -13.348909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2990: ep_len:3019 episode reward: total was 12.650000. running mean: -13.088920\n",
      "ep 2990: ep_len:872 episode reward: total was 8.760000. running mean: -12.870431\n",
      "ep 2990: ep_len:664 episode reward: total was -22.860000. running mean: -12.970327\n",
      "ep 2990: ep_len:645 episode reward: total was 28.510000. running mean: -12.555524\n",
      "ep 2990: ep_len:1216 episode reward: total was 7.720000. running mean: -12.352768\n",
      "ep 2990: ep_len:787 episode reward: total was 20.050000. running mean: -12.028741\n",
      "ep 2990: ep_len:1511 episode reward: total was 2.420000. running mean: -11.884253\n",
      "ep 2990: ep_len:31 episode reward: total was 14.000000. running mean: -11.625411\n",
      "ep 2990: ep_len:49 episode reward: total was 23.000000. running mean: -11.279157\n",
      "ep 2990: ep_len:70 episode reward: total was 33.500000. running mean: -10.831365\n",
      "ep 2990: ep_len:1040 episode reward: total was 20.120000. running mean: -10.521851\n",
      "ep 2990: ep_len:2864 episode reward: total was -50.190000. running mean: -10.918533\n",
      "epsilon:0.009992 episode_count: 44997. steps_count: 48414941.000000\n",
      "ep 2991: ep_len:620 episode reward: total was 12.880000. running mean: -10.680548\n",
      "ep 2991: ep_len:869 episode reward: total was 30.270000. running mean: -10.271042\n",
      "ep 2991: ep_len:2877 episode reward: total was -19.240000. running mean: -10.360732\n",
      "ep 2991: ep_len:861 episode reward: total was 29.160000. running mean: -9.965524\n",
      "ep 2991: ep_len:55 episode reward: total was 26.000000. running mean: -9.605869\n",
      "ep 2991: ep_len:75 episode reward: total was 36.000000. running mean: -9.149810\n",
      "ep 2991: ep_len:1423 episode reward: total was -110.530000. running mean: -10.163612\n",
      "ep 2991: ep_len:659 episode reward: total was 19.070000. running mean: -9.871276\n",
      "ep 2991: ep_len:550 episode reward: total was -4.740000. running mean: -9.819963\n",
      "ep 2991: ep_len:849 episode reward: total was 46.370000. running mean: -9.258064\n",
      "ep 2991: ep_len:1058 episode reward: total was -634.950000. running mean: -15.514983\n",
      "ep 2991: ep_len:105 episode reward: total was 49.500000. running mean: -14.864833\n",
      "ep 2991: ep_len:892 episode reward: total was 21.240000. running mean: -14.503785\n",
      "ep 2991: ep_len:2843 episode reward: total was -29.220000. running mean: -14.650947\n",
      "epsilon:0.009992 episode_count: 45011. steps_count: 48428677.000000\n",
      "ep 2992: ep_len:1144 episode reward: total was -9.940000. running mean: -14.603838\n",
      "ep 2992: ep_len:704 episode reward: total was -2.300000. running mean: -14.480799\n",
      "ep 2992: ep_len:74 episode reward: total was 35.500000. running mean: -13.980991\n",
      "ep 2992: ep_len:2983 episode reward: total was 15.380000. running mean: -13.687381\n",
      "ep 2992: ep_len:1875 episode reward: total was -164.610000. running mean: -15.196608\n",
      "ep 2992: ep_len:1441 episode reward: total was 20.790000. running mean: -14.836742\n",
      "ep 2992: ep_len:3669 episode reward: total was -296.400000. running mean: -17.652374\n",
      "ep 2992: ep_len:547 episode reward: total was 2.240000. running mean: -17.453450\n",
      "ep 2992: ep_len:625 episode reward: total was -4.410000. running mean: -17.323016\n",
      "ep 2992: ep_len:571 episode reward: total was -6.580000. running mean: -17.215586\n",
      "ep 2992: ep_len:50 episode reward: total was 23.500000. running mean: -16.808430\n",
      "ep 2992: ep_len:82 episode reward: total was 39.500000. running mean: -16.245346\n",
      "ep 2992: ep_len:878 episode reward: total was 15.990000. running mean: -15.922992\n",
      "ep 2992: ep_len:2843 episode reward: total was -14.130000. running mean: -15.905062\n",
      "epsilon:0.009992 episode_count: 45025. steps_count: 48446163.000000\n",
      "ep 2993: ep_len:1107 episode reward: total was 1.810000. running mean: -15.727912\n",
      "ep 2993: ep_len:968 episode reward: total was 14.870000. running mean: -15.421932\n",
      "ep 2993: ep_len:2945 episode reward: total was -16.810000. running mean: -15.435813\n",
      "ep 2993: ep_len:1412 episode reward: total was 15.020000. running mean: -15.131255\n",
      "ep 2993: ep_len:48 episode reward: total was 21.000000. running mean: -14.769942\n",
      "ep 2993: ep_len:52 episode reward: total was 24.500000. running mean: -14.377243\n",
      "ep 2993: ep_len:23 episode reward: total was 10.000000. running mean: -14.133471\n",
      "ep 2993: ep_len:500 episode reward: total was 17.640000. running mean: -13.815736\n",
      "ep 2993: ep_len:3604 episode reward: total was -296.040000. running mean: -16.637979\n",
      "ep 2993: ep_len:659 episode reward: total was -55.190000. running mean: -17.023499\n",
      "ep 2993: ep_len:650 episode reward: total was -0.100000. running mean: -16.854264\n",
      "ep 2993: ep_len:993 episode reward: total was 3.490000. running mean: -16.650821\n",
      "ep 2993: ep_len:52 episode reward: total was 23.000000. running mean: -16.254313\n",
      "ep 2993: ep_len:739 episode reward: total was -35.200000. running mean: -16.443770\n",
      "ep 2993: ep_len:2861 episode reward: total was -33.050000. running mean: -16.609832\n",
      "ep 2993: ep_len:63 episode reward: total was 30.000000. running mean: -16.143734\n",
      "epsilon:0.009992 episode_count: 45041. steps_count: 48462839.000000\n",
      "ep 2994: ep_len:610 episode reward: total was 27.630000. running mean: -15.705996\n",
      "ep 2994: ep_len:1029 episode reward: total was 36.640000. running mean: -15.182536\n",
      "ep 2994: ep_len:51 episode reward: total was 22.500000. running mean: -14.805711\n",
      "ep 2994: ep_len:3033 episode reward: total was -23.820000. running mean: -14.895854\n",
      "ep 2994: ep_len:640 episode reward: total was -6.320000. running mean: -14.810095\n",
      "ep 2994: ep_len:159 episode reward: total was 75.000000. running mean: -13.911994\n",
      "ep 2994: ep_len:1189 episode reward: total was 20.240000. running mean: -13.570475\n",
      "ep 2994: ep_len:666 episode reward: total was 14.270000. running mean: -13.292070\n",
      "ep 2994: ep_len:785 episode reward: total was -9.870000. running mean: -13.257849\n",
      "ep 2994: ep_len:725 episode reward: total was 24.550000. running mean: -12.879771\n",
      "ep 2994: ep_len:666 episode reward: total was -37.380000. running mean: -13.124773\n",
      "ep 2994: ep_len:1141 episode reward: total was -20.070000. running mean: -13.194225\n",
      "ep 2994: ep_len:2897 episode reward: total was -61.950000. running mean: -13.681783\n",
      "epsilon:0.009992 episode_count: 45054. steps_count: 48476430.000000\n",
      "ep 2995: ep_len:656 episode reward: total was 2.350000. running mean: -13.521465\n",
      "ep 2995: ep_len:743 episode reward: total was -11.050000. running mean: -13.496750\n",
      "ep 2995: ep_len:2961 episode reward: total was -17.740000. running mean: -13.539183\n",
      "ep 2995: ep_len:502 episode reward: total was 28.350000. running mean: -13.120291\n",
      "ep 2995: ep_len:116 episode reward: total was 56.500000. running mean: -12.424088\n",
      "ep 2995: ep_len:64 episode reward: total was 30.500000. running mean: -11.994847\n",
      "ep 2995: ep_len:1087 episode reward: total was 3.630000. running mean: -11.838599\n",
      "ep 2995: ep_len:345 episode reward: total was 24.720000. running mean: -11.473013\n",
      "ep 2995: ep_len:1202 episode reward: total was 2.730000. running mean: -11.330983\n",
      "ep 2995: ep_len:902 episode reward: total was 63.520000. running mean: -10.582473\n",
      "ep 2995: ep_len:711 episode reward: total was 16.530000. running mean: -10.311348\n",
      "ep 2995: ep_len:105 episode reward: total was 51.000000. running mean: -9.698235\n",
      "ep 2995: ep_len:1151 episode reward: total was -6.840000. running mean: -9.669652\n",
      "ep 2995: ep_len:2826 episode reward: total was -25.560000. running mean: -9.828556\n",
      "ep 2995: ep_len:65 episode reward: total was 31.000000. running mean: -9.420270\n",
      "epsilon:0.009992 episode_count: 45069. steps_count: 48489866.000000\n",
      "ep 2996: ep_len:666 episode reward: total was -3.080000. running mean: -9.356868\n",
      "ep 2996: ep_len:972 episode reward: total was 17.450000. running mean: -9.088799\n",
      "ep 2996: ep_len:40 episode reward: total was 18.500000. running mean: -8.812911\n",
      "ep 2996: ep_len:2897 episode reward: total was -37.460000. running mean: -9.099382\n",
      "ep 2996: ep_len:508 episode reward: total was -53.580000. running mean: -9.544188\n",
      "ep 2996: ep_len:749 episode reward: total was -24.510000. running mean: -9.693846\n",
      "ep 2996: ep_len:3644 episode reward: total was -475.910000. running mean: -14.356008\n",
      "ep 2996: ep_len:577 episode reward: total was 1.560000. running mean: -14.196848\n",
      "ep 2996: ep_len:804 episode reward: total was 7.140000. running mean: -13.983479\n",
      "ep 2996: ep_len:768 episode reward: total was -0.960000. running mean: -13.853244\n",
      "ep 2996: ep_len:123 episode reward: total was 55.500000. running mean: -13.159712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2996: ep_len:39 episode reward: total was 16.500000. running mean: -12.863115\n",
      "ep 2996: ep_len:104 episode reward: total was 50.500000. running mean: -12.229484\n",
      "ep 2996: ep_len:682 episode reward: total was 11.820000. running mean: -11.988989\n",
      "ep 2996: ep_len:2896 episode reward: total was -62.080000. running mean: -12.489899\n",
      "epsilon:0.009992 episode_count: 45084. steps_count: 48505335.000000\n",
      "ep 2997: ep_len:1123 episode reward: total was 1.820000. running mean: -12.346800\n",
      "ep 2997: ep_len:1275 episode reward: total was -27.820000. running mean: -12.501532\n",
      "ep 2997: ep_len:103 episode reward: total was 50.000000. running mean: -11.876517\n",
      "ep 2997: ep_len:538 episode reward: total was -45.780000. running mean: -12.215551\n",
      "ep 2997: ep_len:95 episode reward: total was 43.000000. running mean: -11.663396\n",
      "ep 2997: ep_len:500 episode reward: total was 19.390000. running mean: -11.352862\n",
      "ep 2997: ep_len:3702 episode reward: total was -38.950000. running mean: -11.628833\n",
      "ep 2997: ep_len:576 episode reward: total was 6.880000. running mean: -11.443745\n",
      "ep 2997: ep_len:690 episode reward: total was 9.130000. running mean: -11.238007\n",
      "ep 2997: ep_len:725 episode reward: total was 2.030000. running mean: -11.105327\n",
      "ep 2997: ep_len:198 episode reward: total was 96.000000. running mean: -10.034274\n",
      "ep 2997: ep_len:61 episode reward: total was 29.000000. running mean: -9.643931\n",
      "ep 2997: ep_len:624 episode reward: total was -17.160000. running mean: -9.719092\n",
      "ep 2997: ep_len:2901 episode reward: total was -16.850000. running mean: -9.790401\n",
      "ep 2997: ep_len:50 episode reward: total was 22.000000. running mean: -9.472497\n",
      "epsilon:0.009992 episode_count: 45099. steps_count: 48518496.000000\n",
      "ep 2998: ep_len:968 episode reward: total was -86.090000. running mean: -10.238672\n",
      "ep 2998: ep_len:790 episode reward: total was 3.730000. running mean: -10.098985\n",
      "ep 2998: ep_len:2941 episode reward: total was -61.260000. running mean: -10.610596\n",
      "ep 2998: ep_len:618 episode reward: total was 6.470000. running mean: -10.439790\n",
      "ep 2998: ep_len:89 episode reward: total was 43.000000. running mean: -9.905392\n",
      "ep 2998: ep_len:37 episode reward: total was 17.000000. running mean: -9.636338\n",
      "ep 2998: ep_len:667 episode reward: total was -15.750000. running mean: -9.697474\n",
      "ep 2998: ep_len:694 episode reward: total was 30.010000. running mean: -9.300400\n",
      "ep 2998: ep_len:1179 episode reward: total was -26.760000. running mean: -9.474996\n",
      "ep 2998: ep_len:707 episode reward: total was 47.330000. running mean: -8.906946\n",
      "ep 2998: ep_len:690 episode reward: total was -18.440000. running mean: -9.002276\n",
      "ep 2998: ep_len:65 episode reward: total was 31.000000. running mean: -8.602254\n",
      "ep 2998: ep_len:1185 episode reward: total was -0.440000. running mean: -8.520631\n",
      "ep 2998: ep_len:2823 episode reward: total was -5.020000. running mean: -8.485625\n",
      "epsilon:0.009992 episode_count: 45113. steps_count: 48531949.000000\n",
      "ep 2999: ep_len:1102 episode reward: total was -2.280000. running mean: -8.423568\n",
      "ep 2999: ep_len:726 episode reward: total was -50.740000. running mean: -8.846733\n",
      "ep 2999: ep_len:2957 episode reward: total was -21.800000. running mean: -8.976265\n",
      "ep 2999: ep_len:605 episode reward: total was -10.280000. running mean: -8.989303\n",
      "ep 2999: ep_len:143 episode reward: total was 65.500000. running mean: -8.244410\n",
      "ep 2999: ep_len:56 episode reward: total was 26.500000. running mean: -7.896966\n",
      "ep 2999: ep_len:576 episode reward: total was -4.630000. running mean: -7.864296\n",
      "ep 2999: ep_len:3925 episode reward: total was -2.210000. running mean: -7.807753\n",
      "ep 2999: ep_len:1548 episode reward: total was -15.150000. running mean: -7.881176\n",
      "ep 2999: ep_len:700 episode reward: total was 18.110000. running mean: -7.621264\n",
      "ep 2999: ep_len:500 episode reward: total was 1.620000. running mean: -7.528851\n",
      "ep 2999: ep_len:66 episode reward: total was 30.000000. running mean: -7.153563\n",
      "ep 2999: ep_len:46 episode reward: total was 21.500000. running mean: -6.867027\n",
      "ep 2999: ep_len:795 episode reward: total was -33.000000. running mean: -7.128357\n",
      "ep 2999: ep_len:2845 episode reward: total was 0.340000. running mean: -7.053673\n",
      "ep 2999: ep_len:23 episode reward: total was 10.000000. running mean: -6.883136\n",
      "epsilon:0.009992 episode_count: 45129. steps_count: 48548562.000000\n",
      "ep 3000: ep_len:581 episode reward: total was 20.630000. running mean: -6.608005\n",
      "ep 3000: ep_len:204 episode reward: total was 5.860000. running mean: -6.483325\n",
      "ep 3000: ep_len:3070 episode reward: total was -34.980000. running mean: -6.768292\n",
      "ep 3000: ep_len:809 episode reward: total was -12.340000. running mean: -6.824009\n",
      "ep 3000: ep_len:43 episode reward: total was 20.000000. running mean: -6.555769\n",
      "ep 3000: ep_len:1467 episode reward: total was 12.110000. running mean: -6.369111\n",
      "ep 3000: ep_len:300 episode reward: total was 19.650000. running mean: -6.108920\n",
      "ep 3000: ep_len:575 episode reward: total was 3.840000. running mean: -6.009431\n",
      "ep 3000: ep_len:699 episode reward: total was 34.480000. running mean: -5.604536\n",
      "ep 3000: ep_len:1132 episode reward: total was 6.690000. running mean: -5.481591\n",
      "ep 3000: ep_len:132 episode reward: total was 61.500000. running mean: -4.811775\n",
      "ep 3000: ep_len:958 episode reward: total was -49.600000. running mean: -5.259657\n",
      "ep 3000: ep_len:2891 episode reward: total was -11.260000. running mean: -5.319661\n",
      "ep 3000: ep_len:57 episode reward: total was 25.500000. running mean: -5.011464\n",
      "epsilon:0.009992 episode_count: 45143. steps_count: 48561480.000000\n",
      "ep 3001: ep_len:978 episode reward: total was -46.530000. running mean: -5.426650\n",
      "ep 3001: ep_len:192 episode reward: total was 6.290000. running mean: -5.309483\n",
      "ep 3001: ep_len:2915 episode reward: total was -5.690000. running mean: -5.313288\n",
      "ep 3001: ep_len:1619 episode reward: total was -29.280000. running mean: -5.552955\n",
      "ep 3001: ep_len:51 episode reward: total was 24.000000. running mean: -5.257426\n",
      "ep 3001: ep_len:69 episode reward: total was 33.000000. running mean: -4.874852\n",
      "ep 3001: ep_len:500 episode reward: total was 1.170000. running mean: -4.814403\n",
      "ep 3001: ep_len:3610 episode reward: total was -40.940000. running mean: -5.175659\n",
      "ep 3001: ep_len:1241 episode reward: total was -19.070000. running mean: -5.314602\n",
      "ep 3001: ep_len:784 episode reward: total was 21.760000. running mean: -5.043856\n",
      "ep 3001: ep_len:661 episode reward: total was 0.380000. running mean: -4.989618\n",
      "ep 3001: ep_len:96 episode reward: total was 46.500000. running mean: -4.474722\n",
      "ep 3001: ep_len:75 episode reward: total was 36.000000. running mean: -4.069974\n",
      "ep 3001: ep_len:1463 episode reward: total was -35.310000. running mean: -4.382375\n",
      "ep 3001: ep_len:2767 episode reward: total was -6.590000. running mean: -4.404451\n",
      "epsilon:0.009992 episode_count: 45158. steps_count: 48578501.000000\n",
      "ep 3002: ep_len:1428 episode reward: total was 29.990000. running mean: -4.060506\n",
      "ep 3002: ep_len:705 episode reward: total was -7.700000. running mean: -4.096901\n",
      "ep 3002: ep_len:3041 episode reward: total was -10.460000. running mean: -4.160532\n",
      "ep 3002: ep_len:1396 episode reward: total was 27.160000. running mean: -3.847327\n",
      "ep 3002: ep_len:618 episode reward: total was -13.180000. running mean: -3.940654\n",
      "ep 3002: ep_len:3868 episode reward: total was -58.440000. running mean: -4.485647\n",
      "ep 3002: ep_len:939 episode reward: total was -18.050000. running mean: -4.621291\n",
      "ep 3002: ep_len:696 episode reward: total was 35.060000. running mean: -4.224478\n",
      "ep 3002: ep_len:1062 episode reward: total was 26.400000. running mean: -3.918233\n",
      "ep 3002: ep_len:82 episode reward: total was 38.000000. running mean: -3.499051\n",
      "ep 3002: ep_len:152 episode reward: total was 73.000000. running mean: -2.734060\n",
      "ep 3002: ep_len:720 episode reward: total was -35.390000. running mean: -3.060620\n",
      "ep 3002: ep_len:2856 episode reward: total was -10.840000. running mean: -3.138413\n",
      "ep 3002: ep_len:52 episode reward: total was 24.500000. running mean: -2.862029\n",
      "epsilon:0.009992 episode_count: 45172. steps_count: 48596116.000000\n",
      "ep 3003: ep_len:809 episode reward: total was -21.370000. running mean: -3.047109\n",
      "ep 3003: ep_len:938 episode reward: total was 17.140000. running mean: -2.845238\n",
      "ep 3003: ep_len:29 episode reward: total was 13.000000. running mean: -2.686786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3003: ep_len:2992 episode reward: total was -24.390000. running mean: -2.903818\n",
      "ep 3003: ep_len:1668 episode reward: total was -9.650000. running mean: -2.971280\n",
      "ep 3003: ep_len:36 episode reward: total was 15.000000. running mean: -2.791567\n",
      "ep 3003: ep_len:75 episode reward: total was 36.000000. running mean: -2.403651\n",
      "ep 3003: ep_len:78 episode reward: total was 37.500000. running mean: -2.004615\n",
      "ep 3003: ep_len:797 episode reward: total was 28.950000. running mean: -1.695068\n",
      "ep 3003: ep_len:635 episode reward: total was 14.330000. running mean: -1.534818\n",
      "ep 3003: ep_len:1586 episode reward: total was -58.910000. running mean: -2.108570\n",
      "ep 3003: ep_len:661 episode reward: total was 12.140000. running mean: -1.966084\n",
      "ep 3003: ep_len:1490 episode reward: total was 27.500000. running mean: -1.671423\n",
      "ep 3003: ep_len:69 episode reward: total was 33.000000. running mean: -1.324709\n",
      "ep 3003: ep_len:653 episode reward: total was 13.520000. running mean: -1.176262\n",
      "ep 3003: ep_len:2884 episode reward: total was -52.160000. running mean: -1.686099\n",
      "ep 3003: ep_len:60 episode reward: total was 28.500000. running mean: -1.384238\n",
      "epsilon:0.009992 episode_count: 45189. steps_count: 48611576.000000\n",
      "ep 3004: ep_len:958 episode reward: total was -22.930000. running mean: -1.599696\n",
      "ep 3004: ep_len:761 episode reward: total was -5.690000. running mean: -1.640599\n",
      "ep 3004: ep_len:2856 episode reward: total was -26.520000. running mean: -1.889393\n",
      "ep 3004: ep_len:813 episode reward: total was 44.970000. running mean: -1.420799\n",
      "ep 3004: ep_len:168 episode reward: total was 82.500000. running mean: -0.581591\n",
      "ep 3004: ep_len:46 episode reward: total was 20.000000. running mean: -0.375775\n",
      "ep 3004: ep_len:726 episode reward: total was -14.610000. running mean: -0.518117\n",
      "ep 3004: ep_len:339 episode reward: total was 20.650000. running mean: -0.306436\n",
      "ep 3004: ep_len:575 episode reward: total was 4.790000. running mean: -0.255472\n",
      "ep 3004: ep_len:661 episode reward: total was 13.060000. running mean: -0.122317\n",
      "ep 3004: ep_len:577 episode reward: total was -10.560000. running mean: -0.226694\n",
      "ep 3004: ep_len:26 episode reward: total was 11.500000. running mean: -0.109427\n",
      "ep 3004: ep_len:778 episode reward: total was -5.520000. running mean: -0.163533\n",
      "ep 3004: ep_len:2762 episode reward: total was -21.420000. running mean: -0.376097\n",
      "ep 3004: ep_len:68 episode reward: total was 32.500000. running mean: -0.047336\n",
      "epsilon:0.009992 episode_count: 45204. steps_count: 48623690.000000\n",
      "ep 3005: ep_len:621 episode reward: total was -2.720000. running mean: -0.074063\n",
      "ep 3005: ep_len:201 episode reward: total was 8.310000. running mean: 0.009778\n",
      "ep 3005: ep_len:3063 episode reward: total was -24.520000. running mean: -0.235520\n",
      "ep 3005: ep_len:895 episode reward: total was 62.980000. running mean: 0.396635\n",
      "ep 3005: ep_len:136 episode reward: total was 66.500000. running mean: 1.057669\n",
      "ep 3005: ep_len:101 episode reward: total was 47.500000. running mean: 1.522092\n",
      "ep 3005: ep_len:1538 episode reward: total was -148.300000. running mean: 0.023871\n",
      "ep 3005: ep_len:363 episode reward: total was 28.360000. running mean: 0.307232\n",
      "ep 3005: ep_len:1215 episode reward: total was -47.090000. running mean: -0.166740\n",
      "ep 3005: ep_len:7230 episode reward: total was 51.990000. running mean: 0.354828\n",
      "ep 3005: ep_len:646 episode reward: total was -13.310000. running mean: 0.218179\n",
      "ep 3005: ep_len:161 episode reward: total was 79.000000. running mean: 1.005997\n",
      "ep 3005: ep_len:591 episode reward: total was -2.440000. running mean: 0.971538\n",
      "ep 3005: ep_len:2769 episode reward: total was -22.270000. running mean: 0.739122\n",
      "ep 3005: ep_len:58 episode reward: total was 26.000000. running mean: 0.991731\n",
      "epsilon:0.009992 episode_count: 45219. steps_count: 48643278.000000\n",
      "ep 3006: ep_len:677 episode reward: total was -5.690000. running mean: 0.924914\n",
      "ep 3006: ep_len:823 episode reward: total was -0.220000. running mean: 0.913464\n",
      "ep 3006: ep_len:2885 episode reward: total was -57.140000. running mean: 0.332930\n",
      "ep 3006: ep_len:644 episode reward: total was -94.220000. running mean: -0.612599\n",
      "ep 3006: ep_len:48 episode reward: total was 22.500000. running mean: -0.381473\n",
      "ep 3006: ep_len:69 episode reward: total was 33.000000. running mean: -0.047659\n",
      "ep 3006: ep_len:661 episode reward: total was -1.640000. running mean: -0.063582\n",
      "ep 3006: ep_len:3680 episode reward: total was -348.810000. running mean: -3.551046\n",
      "ep 3006: ep_len:913 episode reward: total was -5.180000. running mean: -3.567336\n",
      "ep 3006: ep_len:829 episode reward: total was 61.130000. running mean: -2.920363\n",
      "ep 3006: ep_len:1096 episode reward: total was 30.110000. running mean: -2.590059\n",
      "ep 3006: ep_len:52 episode reward: total was 24.500000. running mean: -2.319158\n",
      "ep 3006: ep_len:207 episode reward: total was 96.000000. running mean: -1.335967\n",
      "ep 3006: ep_len:35 episode reward: total was 16.000000. running mean: -1.162607\n",
      "ep 3006: ep_len:623 episode reward: total was -18.180000. running mean: -1.332781\n",
      "ep 3006: ep_len:2843 episode reward: total was -38.830000. running mean: -1.707753\n",
      "epsilon:0.009992 episode_count: 45235. steps_count: 48659363.000000\n",
      "ep 3007: ep_len:1103 episode reward: total was -0.250000. running mean: -1.693176\n",
      "ep 3007: ep_len:723 episode reward: total was -20.410000. running mean: -1.880344\n",
      "ep 3007: ep_len:83 episode reward: total was 38.500000. running mean: -1.476540\n",
      "ep 3007: ep_len:2992 episode reward: total was -44.220000. running mean: -1.903975\n",
      "ep 3007: ep_len:1199 episode reward: total was -13.210000. running mean: -2.017035\n",
      "ep 3007: ep_len:71 episode reward: total was 32.500000. running mean: -1.671865\n",
      "ep 3007: ep_len:61 episode reward: total was 29.000000. running mean: -1.365146\n",
      "ep 3007: ep_len:887 episode reward: total was 15.130000. running mean: -1.200195\n",
      "ep 3007: ep_len:641 episode reward: total was 5.450000. running mean: -1.133693\n",
      "ep 3007: ep_len:593 episode reward: total was -67.970000. running mean: -1.802056\n",
      "ep 3007: ep_len:782 episode reward: total was 16.840000. running mean: -1.615635\n",
      "ep 3007: ep_len:1082 episode reward: total was 23.170000. running mean: -1.367779\n",
      "ep 3007: ep_len:68 episode reward: total was 32.500000. running mean: -1.029101\n",
      "ep 3007: ep_len:593 episode reward: total was -5.880000. running mean: -1.077610\n",
      "ep 3007: ep_len:2831 episode reward: total was -15.260000. running mean: -1.219434\n",
      "ep 3007: ep_len:54 episode reward: total was 25.500000. running mean: -0.952240\n",
      "epsilon:0.009992 episode_count: 45251. steps_count: 48673126.000000\n",
      "ep 3008: ep_len:857 episode reward: total was 7.910000. running mean: -0.863617\n",
      "ep 3008: ep_len:500 episode reward: total was 5.700000. running mean: -0.797981\n",
      "ep 3008: ep_len:2921 episode reward: total was -12.490000. running mean: -0.914901\n",
      "ep 3008: ep_len:588 episode reward: total was 1.100000. running mean: -0.894752\n",
      "ep 3008: ep_len:1394 episode reward: total was -143.330000. running mean: -2.319105\n",
      "ep 3008: ep_len:630 episode reward: total was 18.010000. running mean: -2.115814\n",
      "ep 3008: ep_len:1558 episode reward: total was -10.730000. running mean: -2.201956\n",
      "ep 3008: ep_len:617 episode reward: total was -1.470000. running mean: -2.194636\n",
      "ep 3008: ep_len:757 episode reward: total was -9.770000. running mean: -2.270390\n",
      "ep 3008: ep_len:179 episode reward: total was 85.000000. running mean: -1.397686\n",
      "ep 3008: ep_len:79 episode reward: total was 36.500000. running mean: -1.018709\n",
      "ep 3008: ep_len:688 episode reward: total was -71.060000. running mean: -1.719122\n",
      "ep 3008: ep_len:2862 episode reward: total was -28.140000. running mean: -1.983331\n",
      "epsilon:0.009992 episode_count: 45264. steps_count: 48686756.000000\n",
      "ep 3009: ep_len:1168 episode reward: total was -17.320000. running mean: -2.136697\n",
      "ep 3009: ep_len:731 episode reward: total was -45.240000. running mean: -2.567730\n",
      "ep 3009: ep_len:54 episode reward: total was 25.500000. running mean: -2.287053\n",
      "ep 3009: ep_len:2986 episode reward: total was 15.750000. running mean: -2.106683\n",
      "ep 3009: ep_len:500 episode reward: total was 19.420000. running mean: -1.891416\n",
      "ep 3009: ep_len:136 episode reward: total was 66.500000. running mean: -1.207502\n",
      "ep 3009: ep_len:81 episode reward: total was 36.000000. running mean: -0.835427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3009: ep_len:46 episode reward: total was 21.500000. running mean: -0.612072\n",
      "ep 3009: ep_len:500 episode reward: total was 7.660000. running mean: -0.529352\n",
      "ep 3009: ep_len:3724 episode reward: total was -1369.390000. running mean: -14.217958\n",
      "ep 3009: ep_len:953 episode reward: total was -15.890000. running mean: -14.234678\n",
      "ep 3009: ep_len:733 episode reward: total was 7.320000. running mean: -14.019132\n",
      "ep 3009: ep_len:596 episode reward: total was -15.420000. running mean: -14.033140\n",
      "ep 3009: ep_len:194 episode reward: total was 88.000000. running mean: -13.012809\n",
      "ep 3009: ep_len:44 episode reward: total was 19.000000. running mean: -12.692681\n",
      "ep 3009: ep_len:63 episode reward: total was 30.000000. running mean: -12.265754\n",
      "ep 3009: ep_len:791 episode reward: total was -0.290000. running mean: -12.145997\n",
      "ep 3009: ep_len:45 episode reward: total was 19.500000. running mean: -11.829537\n",
      "epsilon:0.009992 episode_count: 45282. steps_count: 48700101.000000\n",
      "ep 3010: ep_len:681 episode reward: total was -1.440000. running mean: -11.725641\n",
      "ep 3010: ep_len:500 episode reward: total was -3.110000. running mean: -11.639485\n",
      "ep 3010: ep_len:2881 episode reward: total was -60.720000. running mean: -12.130290\n",
      "ep 3010: ep_len:626 episode reward: total was -11.200000. running mean: -12.120987\n",
      "ep 3010: ep_len:139 episode reward: total was 68.000000. running mean: -11.319777\n",
      "ep 3010: ep_len:776 episode reward: total was -33.760000. running mean: -11.544179\n",
      "ep 3010: ep_len:3626 episode reward: total was -72.490000. running mean: -12.153638\n",
      "ep 3010: ep_len:1190 episode reward: total was 8.190000. running mean: -11.950201\n",
      "ep 3010: ep_len:924 episode reward: total was 73.020000. running mean: -11.100499\n",
      "ep 3010: ep_len:1070 episode reward: total was -2.600000. running mean: -11.015494\n",
      "ep 3010: ep_len:68 episode reward: total was 31.000000. running mean: -10.595339\n",
      "ep 3010: ep_len:38 episode reward: total was 17.500000. running mean: -10.314386\n",
      "ep 3010: ep_len:895 episode reward: total was 19.400000. running mean: -10.017242\n",
      "ep 3010: ep_len:2870 episode reward: total was -18.050000. running mean: -10.097570\n",
      "epsilon:0.009992 episode_count: 45296. steps_count: 48716385.000000\n",
      "ep 3011: ep_len:653 episode reward: total was -20.480000. running mean: -10.201394\n",
      "ep 3011: ep_len:1592 episode reward: total was -27.990000. running mean: -10.379280\n",
      "ep 3011: ep_len:64 episode reward: total was 30.500000. running mean: -9.970487\n",
      "ep 3011: ep_len:2906 episode reward: total was -52.950000. running mean: -10.400282\n",
      "ep 3011: ep_len:677 episode reward: total was -22.690000. running mean: -10.523179\n",
      "ep 3011: ep_len:37 episode reward: total was 17.000000. running mean: -10.247948\n",
      "ep 3011: ep_len:96 episode reward: total was 45.000000. running mean: -9.695468\n",
      "ep 3011: ep_len:500 episode reward: total was 9.920000. running mean: -9.499314\n",
      "ep 3011: ep_len:3637 episode reward: total was -2843.220000. running mean: -37.836520\n",
      "ep 3011: ep_len:529 episode reward: total was -27.200000. running mean: -37.730155\n",
      "ep 3011: ep_len:841 episode reward: total was 47.950000. running mean: -36.873354\n",
      "ep 3011: ep_len:701 episode reward: total was 14.660000. running mean: -36.358020\n",
      "ep 3011: ep_len:65 episode reward: total was 31.000000. running mean: -35.684440\n",
      "ep 3011: ep_len:127 episode reward: total was 59.000000. running mean: -34.737596\n",
      "ep 3011: ep_len:695 episode reward: total was 11.040000. running mean: -34.279820\n",
      "ep 3011: ep_len:47 episode reward: total was 19.000000. running mean: -33.747021\n",
      "epsilon:0.009992 episode_count: 45312. steps_count: 48729552.000000\n",
      "ep 3012: ep_len:1496 episode reward: total was 18.400000. running mean: -33.225551\n",
      "ep 3012: ep_len:964 episode reward: total was 25.640000. running mean: -32.636896\n",
      "ep 3012: ep_len:3039 episode reward: total was -22.500000. running mean: -32.535527\n",
      "ep 3012: ep_len:823 episode reward: total was 42.380000. running mean: -31.786371\n",
      "ep 3012: ep_len:163 episode reward: total was 78.500000. running mean: -30.683508\n",
      "ep 3012: ep_len:72 episode reward: total was 34.500000. running mean: -30.031673\n",
      "ep 3012: ep_len:630 episode reward: total was 35.460000. running mean: -29.376756\n",
      "ep 3012: ep_len:3834 episode reward: total was -70.630000. running mean: -29.789288\n",
      "ep 3012: ep_len:548 episode reward: total was -28.020000. running mean: -29.771595\n",
      "ep 3012: ep_len:684 episode reward: total was 6.980000. running mean: -29.404079\n",
      "ep 3012: ep_len:738 episode reward: total was 21.950000. running mean: -28.890539\n",
      "ep 3012: ep_len:57 episode reward: total was 27.000000. running mean: -28.331633\n",
      "ep 3012: ep_len:737 episode reward: total was -103.900000. running mean: -29.087317\n",
      "ep 3012: ep_len:2868 episode reward: total was 1.000000. running mean: -28.786444\n",
      "ep 3012: ep_len:65 episode reward: total was 29.500000. running mean: -28.203579\n",
      "epsilon:0.009992 episode_count: 45327. steps_count: 48746270.000000\n",
      "ep 3013: ep_len:720 episode reward: total was -26.300000. running mean: -28.184544\n",
      "ep 3013: ep_len:500 episode reward: total was 16.320000. running mean: -27.739498\n",
      "ep 3013: ep_len:72 episode reward: total was 34.500000. running mean: -27.117103\n",
      "ep 3013: ep_len:2972 episode reward: total was -24.640000. running mean: -27.092332\n",
      "ep 3013: ep_len:1169 episode reward: total was -14.620000. running mean: -26.967609\n",
      "ep 3013: ep_len:94 episode reward: total was 45.500000. running mean: -26.242933\n",
      "ep 3013: ep_len:64 episode reward: total was 30.500000. running mean: -25.675503\n",
      "ep 3013: ep_len:1461 episode reward: total was -161.300000. running mean: -27.031748\n",
      "ep 3013: ep_len:3812 episode reward: total was -73.790000. running mean: -27.499331\n",
      "ep 3013: ep_len:683 episode reward: total was -31.900000. running mean: -27.543338\n",
      "ep 3013: ep_len:667 episode reward: total was -3.260000. running mean: -27.300504\n",
      "ep 3013: ep_len:616 episode reward: total was -12.600000. running mean: -27.153499\n",
      "ep 3013: ep_len:63 episode reward: total was 30.000000. running mean: -26.581964\n",
      "ep 3013: ep_len:190 episode reward: total was 92.000000. running mean: -25.396145\n",
      "ep 3013: ep_len:90 episode reward: total was 43.500000. running mean: -24.707183\n",
      "ep 3013: ep_len:759 episode reward: total was -47.120000. running mean: -24.931311\n",
      "ep 3013: ep_len:2835 episode reward: total was -7.870000. running mean: -24.760698\n",
      "ep 3013: ep_len:46 episode reward: total was 21.500000. running mean: -24.298091\n",
      "epsilon:0.009992 episode_count: 45345. steps_count: 48763083.000000\n",
      "ep 3014: ep_len:660 episode reward: total was -2.530000. running mean: -24.080410\n",
      "ep 3014: ep_len:670 episode reward: total was -18.350000. running mean: -24.023106\n",
      "ep 3014: ep_len:59 episode reward: total was 28.000000. running mean: -23.502875\n",
      "ep 3014: ep_len:3050 episode reward: total was -53.240000. running mean: -23.800246\n",
      "ep 3014: ep_len:878 episode reward: total was 21.770000. running mean: -23.344544\n",
      "ep 3014: ep_len:114 episode reward: total was 55.500000. running mean: -22.556098\n",
      "ep 3014: ep_len:1481 episode reward: total was -140.000000. running mean: -23.730537\n",
      "ep 3014: ep_len:3714 episode reward: total was -70.720000. running mean: -24.200432\n",
      "ep 3014: ep_len:500 episode reward: total was 33.530000. running mean: -23.623128\n",
      "ep 3014: ep_len:704 episode reward: total was 11.820000. running mean: -23.268696\n",
      "ep 3014: ep_len:1016 episode reward: total was 26.280000. running mean: -22.773209\n",
      "ep 3014: ep_len:67 episode reward: total was 30.500000. running mean: -22.240477\n",
      "ep 3014: ep_len:58 episode reward: total was 27.500000. running mean: -21.743073\n",
      "ep 3014: ep_len:121 episode reward: total was 53.000000. running mean: -20.995642\n",
      "ep 3014: ep_len:798 episode reward: total was 11.440000. running mean: -20.671285\n",
      "ep 3014: ep_len:2749 episode reward: total was 4.980000. running mean: -20.414773\n",
      "epsilon:0.009992 episode_count: 45361. steps_count: 48779722.000000\n",
      "ep 3015: ep_len:500 episode reward: total was 8.050000. running mean: -20.130125\n",
      "ep 3015: ep_len:677 episode reward: total was -52.040000. running mean: -20.449224\n",
      "ep 3015: ep_len:39 episode reward: total was 18.000000. running mean: -20.064731\n",
      "ep 3015: ep_len:2964 episode reward: total was -11.530000. running mean: -19.979384\n",
      "ep 3015: ep_len:659 episode reward: total was -43.730000. running mean: -20.216890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3015: ep_len:159 episode reward: total was 76.500000. running mean: -19.249721\n",
      "ep 3015: ep_len:56 episode reward: total was 26.500000. running mean: -18.792224\n",
      "ep 3015: ep_len:500 episode reward: total was 6.370000. running mean: -18.540602\n",
      "ep 3015: ep_len:324 episode reward: total was 6.390000. running mean: -18.291296\n",
      "ep 3015: ep_len:936 episode reward: total was -27.230000. running mean: -18.380683\n",
      "ep 3015: ep_len:647 episode reward: total was 7.650000. running mean: -18.120376\n",
      "ep 3015: ep_len:679 episode reward: total was -5.500000. running mean: -17.994172\n",
      "ep 3015: ep_len:562 episode reward: total was -8.690000. running mean: -17.901131\n",
      "ep 3015: ep_len:2859 episode reward: total was -33.040000. running mean: -18.052519\n",
      "epsilon:0.009992 episode_count: 45375. steps_count: 48791283.000000\n",
      "ep 3016: ep_len:1415 episode reward: total was 15.230000. running mean: -17.719694\n",
      "ep 3016: ep_len:192 episode reward: total was 9.380000. running mean: -17.448697\n",
      "ep 3016: ep_len:74 episode reward: total was 32.500000. running mean: -16.949210\n",
      "ep 3016: ep_len:3057 episode reward: total was -6.120000. running mean: -16.840918\n",
      "ep 3016: ep_len:811 episode reward: total was -11.980000. running mean: -16.792309\n",
      "ep 3016: ep_len:112 episode reward: total was 53.000000. running mean: -16.094386\n",
      "ep 3016: ep_len:862 episode reward: total was 28.440000. running mean: -15.649042\n",
      "ep 3016: ep_len:3526 episode reward: total was -49.370000. running mean: -15.986252\n",
      "ep 3016: ep_len:889 episode reward: total was 16.980000. running mean: -15.656589\n",
      "ep 3016: ep_len:786 episode reward: total was 26.740000. running mean: -15.232623\n",
      "ep 3016: ep_len:733 episode reward: total was 28.240000. running mean: -14.797897\n",
      "ep 3016: ep_len:62 episode reward: total was 29.500000. running mean: -14.354918\n",
      "ep 3016: ep_len:36 episode reward: total was 16.500000. running mean: -14.046369\n",
      "ep 3016: ep_len:109 episode reward: total was 50.000000. running mean: -13.405905\n",
      "ep 3016: ep_len:826 episode reward: total was -13.120000. running mean: -13.403046\n",
      "ep 3016: ep_len:2829 episode reward: total was -4.650000. running mean: -13.315516\n",
      "epsilon:0.009992 episode_count: 45391. steps_count: 48807602.000000\n",
      "ep 3017: ep_len:1463 episode reward: total was 18.470000. running mean: -12.997660\n",
      "ep 3017: ep_len:820 episode reward: total was -8.270000. running mean: -12.950384\n",
      "ep 3017: ep_len:2980 episode reward: total was -30.810000. running mean: -13.128980\n",
      "ep 3017: ep_len:1232 episode reward: total was -31.950000. running mean: -13.317190\n",
      "ep 3017: ep_len:649 episode reward: total was -4.860000. running mean: -13.232618\n",
      "ep 3017: ep_len:663 episode reward: total was 26.580000. running mean: -12.834492\n",
      "ep 3017: ep_len:3901 episode reward: total was -692.400000. running mean: -19.630147\n",
      "ep 3017: ep_len:828 episode reward: total was 55.090000. running mean: -18.882946\n",
      "ep 3017: ep_len:624 episode reward: total was -86.790000. running mean: -19.562016\n",
      "ep 3017: ep_len:175 episode reward: total was 83.000000. running mean: -18.536396\n",
      "ep 3017: ep_len:108 episode reward: total was 52.500000. running mean: -17.826032\n",
      "ep 3017: ep_len:1534 episode reward: total was -6.990000. running mean: -17.717672\n",
      "ep 3017: ep_len:2872 episode reward: total was 6.670000. running mean: -17.473795\n",
      "epsilon:0.009992 episode_count: 45404. steps_count: 48825451.000000\n",
      "ep 3018: ep_len:625 episode reward: total was -12.100000. running mean: -17.420057\n",
      "ep 3018: ep_len:646 episode reward: total was 23.980000. running mean: -17.006057\n",
      "ep 3018: ep_len:2875 episode reward: total was -39.360000. running mean: -17.229596\n",
      "ep 3018: ep_len:1191 episode reward: total was -29.670000. running mean: -17.354000\n",
      "ep 3018: ep_len:56 episode reward: total was 25.000000. running mean: -16.930460\n",
      "ep 3018: ep_len:72 episode reward: total was 34.500000. running mean: -16.416155\n",
      "ep 3018: ep_len:2447 episode reward: total was -1930.100000. running mean: -35.552994\n",
      "ep 3018: ep_len:3681 episode reward: total was -8.430000. running mean: -35.281764\n",
      "ep 3018: ep_len:514 episode reward: total was 11.860000. running mean: -34.810346\n",
      "ep 3018: ep_len:696 episode reward: total was 20.480000. running mean: -34.257443\n",
      "ep 3018: ep_len:570 episode reward: total was -8.610000. running mean: -34.000968\n",
      "ep 3018: ep_len:36 episode reward: total was 16.500000. running mean: -33.495959\n",
      "ep 3018: ep_len:77 episode reward: total was 37.000000. running mean: -32.790999\n",
      "ep 3018: ep_len:1174 episode reward: total was 7.330000. running mean: -32.389789\n",
      "ep 3018: ep_len:2760 episode reward: total was -34.120000. running mean: -32.407091\n",
      "epsilon:0.009992 episode_count: 45419. steps_count: 48842871.000000\n",
      "ep 3019: ep_len:600 episode reward: total was 13.870000. running mean: -31.944320\n",
      "ep 3019: ep_len:983 episode reward: total was 9.570000. running mean: -31.529177\n",
      "ep 3019: ep_len:41 episode reward: total was 19.000000. running mean: -31.023885\n",
      "ep 3019: ep_len:2978 episode reward: total was -36.710000. running mean: -31.080747\n",
      "ep 3019: ep_len:584 episode reward: total was 9.360000. running mean: -30.676339\n",
      "ep 3019: ep_len:67 episode reward: total was 29.000000. running mean: -30.079576\n",
      "ep 3019: ep_len:106 episode reward: total was 51.500000. running mean: -29.263780\n",
      "ep 3019: ep_len:77 episode reward: total was 37.000000. running mean: -28.601142\n",
      "ep 3019: ep_len:1471 episode reward: total was 25.810000. running mean: -28.057031\n",
      "ep 3019: ep_len:4013 episode reward: total was -128.720000. running mean: -29.063660\n",
      "ep 3019: ep_len:1245 episode reward: total was -56.400000. running mean: -29.337024\n",
      "ep 3019: ep_len:845 episode reward: total was 48.960000. running mean: -28.554054\n",
      "ep 3019: ep_len:625 episode reward: total was -0.990000. running mean: -28.278413\n",
      "ep 3019: ep_len:67 episode reward: total was 32.000000. running mean: -27.675629\n",
      "ep 3019: ep_len:40 episode reward: total was 17.000000. running mean: -27.228873\n",
      "ep 3019: ep_len:84 episode reward: total was 39.000000. running mean: -26.566584\n",
      "ep 3019: ep_len:968 episode reward: total was -109.070000. running mean: -27.391618\n",
      "ep 3019: ep_len:2838 episode reward: total was -6.550000. running mean: -27.183202\n",
      "ep 3019: ep_len:44 episode reward: total was 20.500000. running mean: -26.706370\n",
      "epsilon:0.009992 episode_count: 45438. steps_count: 48860547.000000\n",
      "ep 3020: ep_len:620 episode reward: total was -20.230000. running mean: -26.641606\n",
      "ep 3020: ep_len:683 episode reward: total was -18.590000. running mean: -26.561090\n",
      "ep 3020: ep_len:57 episode reward: total was 27.000000. running mean: -26.025479\n",
      "ep 3020: ep_len:2996 episode reward: total was 4.040000. running mean: -25.724824\n",
      "ep 3020: ep_len:851 episode reward: total was 4.190000. running mean: -25.425676\n",
      "ep 3020: ep_len:58 episode reward: total was 26.000000. running mean: -24.911419\n",
      "ep 3020: ep_len:111 episode reward: total was 52.500000. running mean: -24.137305\n",
      "ep 3020: ep_len:68 episode reward: total was 32.500000. running mean: -23.570932\n",
      "ep 3020: ep_len:724 episode reward: total was -12.120000. running mean: -23.456423\n",
      "ep 3020: ep_len:3648 episode reward: total was -25.290000. running mean: -23.474759\n",
      "ep 3020: ep_len:1303 episode reward: total was -48.750000. running mean: -23.727511\n",
      "ep 3020: ep_len:811 episode reward: total was 32.090000. running mean: -23.169336\n",
      "ep 3020: ep_len:500 episode reward: total was 1.220000. running mean: -22.925443\n",
      "ep 3020: ep_len:61 episode reward: total was 26.000000. running mean: -22.436188\n",
      "ep 3020: ep_len:717 episode reward: total was 17.270000. running mean: -22.039126\n",
      "ep 3020: ep_len:2885 episode reward: total was -0.210000. running mean: -21.820835\n",
      "ep 3020: ep_len:49 episode reward: total was 23.000000. running mean: -21.372627\n",
      "epsilon:0.009992 episode_count: 45455. steps_count: 48876689.000000\n",
      "ep 3021: ep_len:1169 episode reward: total was -3.780000. running mean: -21.196700\n",
      "ep 3021: ep_len:675 episode reward: total was -17.300000. running mean: -21.157733\n",
      "ep 3021: ep_len:2937 episode reward: total was -192.850000. running mean: -22.874656\n",
      "ep 3021: ep_len:655 episode reward: total was -9.180000. running mean: -22.737709\n",
      "ep 3021: ep_len:129 episode reward: total was 60.000000. running mean: -21.910332\n",
      "ep 3021: ep_len:1428 episode reward: total was -47.550000. running mean: -22.166729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3021: ep_len:357 episode reward: total was 22.240000. running mean: -21.722662\n",
      "ep 3021: ep_len:1279 episode reward: total was -68.180000. running mean: -22.187235\n",
      "ep 3021: ep_len:808 episode reward: total was 33.800000. running mean: -21.627363\n",
      "ep 3021: ep_len:917 episode reward: total was 51.290000. running mean: -20.898189\n",
      "ep 3021: ep_len:120 episode reward: total was 58.500000. running mean: -20.104207\n",
      "ep 3021: ep_len:93 episode reward: total was 45.000000. running mean: -19.453165\n",
      "ep 3021: ep_len:1480 episode reward: total was 10.710000. running mean: -19.151534\n",
      "ep 3021: ep_len:2881 episode reward: total was 5.630000. running mean: -18.903718\n",
      "ep 3021: ep_len:31 episode reward: total was 14.000000. running mean: -18.574681\n",
      "epsilon:0.009992 episode_count: 45470. steps_count: 48891648.000000\n",
      "ep 3022: ep_len:1008 episode reward: total was -59.120000. running mean: -18.980134\n",
      "ep 3022: ep_len:1592 episode reward: total was -247.680000. running mean: -21.267133\n",
      "ep 3022: ep_len:2974 episode reward: total was -86.110000. running mean: -21.915562\n",
      "ep 3022: ep_len:509 episode reward: total was -2.060000. running mean: -21.717006\n",
      "ep 3022: ep_len:44 episode reward: total was 20.500000. running mean: -21.294836\n",
      "ep 3022: ep_len:79 episode reward: total was 38.000000. running mean: -20.701887\n",
      "ep 3022: ep_len:683 episode reward: total was -1.240000. running mean: -20.507269\n",
      "ep 3022: ep_len:3647 episode reward: total was -20.830000. running mean: -20.510496\n",
      "ep 3022: ep_len:803 episode reward: total was -44.400000. running mean: -20.749391\n",
      "ep 3022: ep_len:763 episode reward: total was 16.570000. running mean: -20.376197\n",
      "ep 3022: ep_len:587 episode reward: total was -9.450000. running mean: -20.266935\n",
      "ep 3022: ep_len:58 episode reward: total was 27.500000. running mean: -19.789266\n",
      "ep 3022: ep_len:608 episode reward: total was -3.760000. running mean: -19.628973\n",
      "ep 3022: ep_len:2826 episode reward: total was -26.320000. running mean: -19.695883\n",
      "epsilon:0.009992 episode_count: 45484. steps_count: 48907829.000000\n",
      "ep 3023: ep_len:1420 episode reward: total was 30.310000. running mean: -19.195825\n",
      "ep 3023: ep_len:749 episode reward: total was -11.570000. running mean: -19.119566\n",
      "ep 3023: ep_len:55 episode reward: total was 26.000000. running mean: -18.668371\n",
      "ep 3023: ep_len:3048 episode reward: total was 7.960000. running mean: -18.402087\n",
      "ep 3023: ep_len:1448 episode reward: total was 5.650000. running mean: -18.161566\n",
      "ep 3023: ep_len:40 episode reward: total was 18.500000. running mean: -17.794950\n",
      "ep 3023: ep_len:1438 episode reward: total was -52.480000. running mean: -18.141801\n",
      "ep 3023: ep_len:3565 episode reward: total was -34.010000. running mean: -18.300483\n",
      "ep 3023: ep_len:1477 episode reward: total was -144.950000. running mean: -19.566978\n",
      "ep 3023: ep_len:641 episode reward: total was 23.120000. running mean: -19.140108\n",
      "ep 3023: ep_len:615 episode reward: total was 8.270000. running mean: -18.866007\n",
      "ep 3023: ep_len:148 episode reward: total was 1.510000. running mean: -18.662247\n",
      "ep 3023: ep_len:39 episode reward: total was 18.000000. running mean: -18.295625\n",
      "ep 3023: ep_len:1502 episode reward: total was 6.710000. running mean: -18.045568\n",
      "ep 3023: ep_len:2820 episode reward: total was -43.800000. running mean: -18.303113\n",
      "epsilon:0.009992 episode_count: 45499. steps_count: 48926834.000000\n",
      "ep 3024: ep_len:1193 episode reward: total was 5.370000. running mean: -18.066382\n",
      "ep 3024: ep_len:690 episode reward: total was 0.310000. running mean: -17.882618\n",
      "ep 3024: ep_len:3019 episode reward: total was -43.000000. running mean: -18.133792\n",
      "ep 3024: ep_len:675 episode reward: total was 27.720000. running mean: -17.675254\n",
      "ep 3024: ep_len:52 episode reward: total was 24.500000. running mean: -17.253501\n",
      "ep 3024: ep_len:1151 episode reward: total was -13.630000. running mean: -17.217266\n",
      "ep 3024: ep_len:3722 episode reward: total was -21.190000. running mean: -17.256993\n",
      "ep 3024: ep_len:608 episode reward: total was 13.390000. running mean: -16.950524\n",
      "ep 3024: ep_len:899 episode reward: total was 53.480000. running mean: -16.246218\n",
      "ep 3024: ep_len:531 episode reward: total was 2.270000. running mean: -16.061056\n",
      "ep 3024: ep_len:117 episode reward: total was 55.500000. running mean: -15.345446\n",
      "ep 3024: ep_len:58 episode reward: total was 27.500000. running mean: -14.916991\n",
      "ep 3024: ep_len:96 episode reward: total was 45.000000. running mean: -14.317821\n",
      "ep 3024: ep_len:1033 episode reward: total was -76.490000. running mean: -14.939543\n",
      "ep 3024: ep_len:2855 episode reward: total was -32.740000. running mean: -15.117548\n",
      "ep 3024: ep_len:55 episode reward: total was 26.000000. running mean: -14.706372\n",
      "epsilon:0.009992 episode_count: 45515. steps_count: 48943588.000000\n",
      "ep 3025: ep_len:1167 episode reward: total was 18.700000. running mean: -14.372308\n",
      "ep 3025: ep_len:782 episode reward: total was -4.830000. running mean: -14.276885\n",
      "ep 3025: ep_len:2964 episode reward: total was -25.370000. running mean: -14.387816\n",
      "ep 3025: ep_len:598 episode reward: total was 0.760000. running mean: -14.236338\n",
      "ep 3025: ep_len:51 episode reward: total was 24.000000. running mean: -13.853975\n",
      "ep 3025: ep_len:122 episode reward: total was 58.000000. running mean: -13.135435\n",
      "ep 3025: ep_len:500 episode reward: total was 37.420000. running mean: -12.629881\n",
      "ep 3025: ep_len:3645 episode reward: total was -20.420000. running mean: -12.707782\n",
      "ep 3025: ep_len:512 episode reward: total was 4.340000. running mean: -12.537304\n",
      "ep 3025: ep_len:827 episode reward: total was 26.820000. running mean: -12.143731\n",
      "ep 3025: ep_len:597 episode reward: total was -0.260000. running mean: -12.024894\n",
      "ep 3025: ep_len:80 episode reward: total was 37.000000. running mean: -11.534645\n",
      "ep 3025: ep_len:119 episode reward: total was 56.500000. running mean: -10.854298\n",
      "ep 3025: ep_len:640 episode reward: total was 22.120000. running mean: -10.524555\n",
      "ep 3025: ep_len:2943 episode reward: total was -0.450000. running mean: -10.423810\n",
      "epsilon:0.009992 episode_count: 45530. steps_count: 48959135.000000\n",
      "ep 3026: ep_len:622 episode reward: total was -5.030000. running mean: -10.369872\n",
      "ep 3026: ep_len:994 episode reward: total was 20.520000. running mean: -10.060973\n",
      "ep 3026: ep_len:2849 episode reward: total was -35.680000. running mean: -10.317163\n",
      "ep 3026: ep_len:540 episode reward: total was -19.010000. running mean: -10.404092\n",
      "ep 3026: ep_len:64 episode reward: total was 29.000000. running mean: -10.010051\n",
      "ep 3026: ep_len:87 episode reward: total was 42.000000. running mean: -9.489950\n",
      "ep 3026: ep_len:63 episode reward: total was 30.000000. running mean: -9.095051\n",
      "ep 3026: ep_len:500 episode reward: total was 8.850000. running mean: -8.915600\n",
      "ep 3026: ep_len:3691 episode reward: total was -43.650000. running mean: -9.262944\n",
      "ep 3026: ep_len:538 episode reward: total was 3.680000. running mean: -9.133515\n",
      "ep 3026: ep_len:616 episode reward: total was 4.190000. running mean: -9.000280\n",
      "ep 3026: ep_len:1000 episode reward: total was -21.450000. running mean: -9.124777\n",
      "ep 3026: ep_len:46 episode reward: total was 21.500000. running mean: -8.818529\n",
      "ep 3026: ep_len:92 episode reward: total was 44.500000. running mean: -8.285344\n",
      "ep 3026: ep_len:812 episode reward: total was 18.250000. running mean: -8.019990\n",
      "ep 3026: ep_len:2873 episode reward: total was -7.400000. running mean: -8.013790\n",
      "epsilon:0.009992 episode_count: 45546. steps_count: 48974522.000000\n",
      "ep 3027: ep_len:655 episode reward: total was -8.950000. running mean: -8.023153\n",
      "ep 3027: ep_len:722 episode reward: total was -76.890000. running mean: -8.711821\n",
      "ep 3027: ep_len:2965 episode reward: total was -21.530000. running mean: -8.840003\n",
      "ep 3027: ep_len:766 episode reward: total was -27.830000. running mean: -9.029903\n",
      "ep 3027: ep_len:88 episode reward: total was 42.500000. running mean: -8.514604\n",
      "ep 3027: ep_len:1027 episode reward: total was -2.990000. running mean: -8.459358\n",
      "ep 3027: ep_len:3726 episode reward: total was 10.220000. running mean: -8.272564\n",
      "ep 3027: ep_len:815 episode reward: total was -25.910000. running mean: -8.448938\n",
      "ep 3027: ep_len:787 episode reward: total was 21.920000. running mean: -8.145249\n",
      "ep 3027: ep_len:643 episode reward: total was 1.170000. running mean: -8.052097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3027: ep_len:121 episode reward: total was 57.500000. running mean: -7.396576\n",
      "ep 3027: ep_len:721 episode reward: total was -67.700000. running mean: -7.999610\n",
      "ep 3027: ep_len:2920 episode reward: total was -9.620000. running mean: -8.015814\n",
      "epsilon:0.009992 episode_count: 45559. steps_count: 48990478.000000\n",
      "ep 3028: ep_len:1146 episode reward: total was 14.060000. running mean: -7.795056\n",
      "ep 3028: ep_len:970 episode reward: total was 31.820000. running mean: -7.398905\n",
      "ep 3028: ep_len:2913 episode reward: total was -20.770000. running mean: -7.532616\n",
      "ep 3028: ep_len:1447 episode reward: total was -56.980000. running mean: -8.027090\n",
      "ep 3028: ep_len:64 episode reward: total was 30.500000. running mean: -7.641819\n",
      "ep 3028: ep_len:102 episode reward: total was 46.500000. running mean: -7.100401\n",
      "ep 3028: ep_len:61 episode reward: total was 29.000000. running mean: -6.739397\n",
      "ep 3028: ep_len:926 episode reward: total was 54.590000. running mean: -6.126103\n",
      "ep 3028: ep_len:3941 episode reward: total was -967.190000. running mean: -15.736742\n",
      "ep 3028: ep_len:679 episode reward: total was -50.950000. running mean: -16.088874\n",
      "ep 3028: ep_len:648 episode reward: total was 19.820000. running mean: -15.729786\n",
      "ep 3028: ep_len:892 episode reward: total was 28.160000. running mean: -15.290888\n",
      "ep 3028: ep_len:80 episode reward: total was 37.000000. running mean: -14.767979\n",
      "ep 3028: ep_len:163 episode reward: total was 77.000000. running mean: -13.850299\n",
      "ep 3028: ep_len:43 episode reward: total was 18.500000. running mean: -13.526796\n",
      "ep 3028: ep_len:689 episode reward: total was 5.350000. running mean: -13.338028\n",
      "ep 3028: ep_len:2789 episode reward: total was -16.260000. running mean: -13.367248\n",
      "epsilon:0.009992 episode_count: 45576. steps_count: 49008031.000000\n",
      "ep 3029: ep_len:803 episode reward: total was -44.630000. running mean: -13.679875\n",
      "ep 3029: ep_len:697 episode reward: total was -11.090000. running mean: -13.653977\n",
      "ep 3029: ep_len:71 episode reward: total was 34.000000. running mean: -13.177437\n",
      "ep 3029: ep_len:2939 episode reward: total was -32.720000. running mean: -13.372863\n",
      "ep 3029: ep_len:827 episode reward: total was 21.100000. running mean: -13.028134\n",
      "ep 3029: ep_len:687 episode reward: total was -1.380000. running mean: -12.911653\n",
      "ep 3029: ep_len:3720 episode reward: total was -1.070000. running mean: -12.793236\n",
      "ep 3029: ep_len:847 episode reward: total was -22.830000. running mean: -12.893604\n",
      "ep 3029: ep_len:780 episode reward: total was 32.620000. running mean: -12.438468\n",
      "ep 3029: ep_len:1532 episode reward: total was -55.870000. running mean: -12.872783\n",
      "ep 3029: ep_len:47 episode reward: total was 22.000000. running mean: -12.524055\n",
      "ep 3029: ep_len:175 episode reward: total was 84.500000. running mean: -11.553815\n",
      "ep 3029: ep_len:49 episode reward: total was 23.000000. running mean: -11.208276\n",
      "ep 3029: ep_len:1539 episode reward: total was -438.870000. running mean: -15.484894\n",
      "ep 3029: ep_len:2834 episode reward: total was 1.390000. running mean: -15.316145\n",
      "ep 3029: ep_len:47 episode reward: total was 20.500000. running mean: -14.957983\n",
      "epsilon:0.009992 episode_count: 45592. steps_count: 49025625.000000\n",
      "ep 3030: ep_len:951 episode reward: total was -189.640000. running mean: -16.704803\n",
      "ep 3030: ep_len:500 episode reward: total was 6.680000. running mean: -16.470955\n",
      "ep 3030: ep_len:53 episode reward: total was 25.000000. running mean: -16.056246\n",
      "ep 3030: ep_len:2938 episode reward: total was -1.690000. running mean: -15.912583\n",
      "ep 3030: ep_len:692 episode reward: total was 5.330000. running mean: -15.700158\n",
      "ep 3030: ep_len:500 episode reward: total was -2.050000. running mean: -15.563656\n",
      "ep 3030: ep_len:678 episode reward: total was 17.330000. running mean: -15.234719\n",
      "ep 3030: ep_len:802 episode reward: total was -19.860000. running mean: -15.280972\n",
      "ep 3030: ep_len:714 episode reward: total was -1.290000. running mean: -15.141063\n",
      "ep 3030: ep_len:663 episode reward: total was -2.850000. running mean: -15.018152\n",
      "ep 3030: ep_len:42 episode reward: total was 19.500000. running mean: -14.672970\n",
      "ep 3030: ep_len:34 episode reward: total was 14.000000. running mean: -14.386241\n",
      "ep 3030: ep_len:87 episode reward: total was 42.000000. running mean: -13.822378\n",
      "ep 3030: ep_len:778 episode reward: total was -14.610000. running mean: -13.830254\n",
      "ep 3030: ep_len:2813 episode reward: total was -28.230000. running mean: -13.974252\n",
      "epsilon:0.009992 episode_count: 45607. steps_count: 49037870.000000\n",
      "ep 3031: ep_len:747 episode reward: total was -15.490000. running mean: -13.989409\n",
      "ep 3031: ep_len:683 episode reward: total was -9.700000. running mean: -13.946515\n",
      "ep 3031: ep_len:3000 episode reward: total was -17.730000. running mean: -13.984350\n",
      "ep 3031: ep_len:773 episode reward: total was -12.640000. running mean: -13.970907\n",
      "ep 3031: ep_len:41 episode reward: total was 19.000000. running mean: -13.641198\n",
      "ep 3031: ep_len:500 episode reward: total was 13.840000. running mean: -13.366386\n",
      "ep 3031: ep_len:356 episode reward: total was 22.230000. running mean: -13.010422\n",
      "ep 3031: ep_len:615 episode reward: total was 26.090000. running mean: -12.619418\n",
      "ep 3031: ep_len:7239 episode reward: total was -204.400000. running mean: -14.537223\n",
      "ep 3031: ep_len:1518 episode reward: total was -34.880000. running mean: -14.740651\n",
      "ep 3031: ep_len:65 episode reward: total was 29.500000. running mean: -14.298245\n",
      "ep 3031: ep_len:86 episode reward: total was 40.000000. running mean: -13.755262\n",
      "ep 3031: ep_len:616 episode reward: total was -10.990000. running mean: -13.727610\n",
      "ep 3031: ep_len:2835 episode reward: total was 4.860000. running mean: -13.541733\n",
      "epsilon:0.009992 episode_count: 45621. steps_count: 49056944.000000\n",
      "ep 3032: ep_len:842 episode reward: total was -32.610000. running mean: -13.732416\n",
      "ep 3032: ep_len:741 episode reward: total was -6.540000. running mean: -13.660492\n",
      "ep 3032: ep_len:76 episode reward: total was 35.000000. running mean: -13.173887\n",
      "ep 3032: ep_len:82 episode reward: total was 39.500000. running mean: -12.647148\n",
      "ep 3032: ep_len:797 episode reward: total was 16.780000. running mean: -12.352877\n",
      "ep 3032: ep_len:42 episode reward: total was 19.500000. running mean: -12.034348\n",
      "ep 3032: ep_len:48 episode reward: total was 22.500000. running mean: -11.689004\n",
      "ep 3032: ep_len:62 episode reward: total was 29.500000. running mean: -11.277114\n",
      "ep 3032: ep_len:949 episode reward: total was 6.290000. running mean: -11.101443\n",
      "ep 3032: ep_len:688 episode reward: total was 25.940000. running mean: -10.731029\n",
      "ep 3032: ep_len:597 episode reward: total was 13.520000. running mean: -10.488519\n",
      "ep 3032: ep_len:742 episode reward: total was 42.350000. running mean: -9.960133\n",
      "ep 3032: ep_len:1129 episode reward: total was -36.350000. running mean: -10.224032\n",
      "ep 3032: ep_len:93 episode reward: total was 45.000000. running mean: -9.671792\n",
      "ep 3032: ep_len:748 episode reward: total was -65.410000. running mean: -10.229174\n",
      "ep 3032: ep_len:2820 episode reward: total was -10.410000. running mean: -10.230982\n",
      "epsilon:0.009992 episode_count: 45637. steps_count: 49067400.000000\n",
      "ep 3033: ep_len:1134 episode reward: total was 1.320000. running mean: -10.115472\n",
      "ep 3033: ep_len:952 episode reward: total was 21.930000. running mean: -9.795017\n",
      "ep 3033: ep_len:3011 episode reward: total was -48.070000. running mean: -10.177767\n",
      "ep 3033: ep_len:663 episode reward: total was -2.580000. running mean: -10.101790\n",
      "ep 3033: ep_len:632 episode reward: total was 3.180000. running mean: -9.968972\n",
      "ep 3033: ep_len:635 episode reward: total was 12.310000. running mean: -9.746182\n",
      "ep 3033: ep_len:822 episode reward: total was -29.210000. running mean: -9.940820\n",
      "ep 3033: ep_len:812 episode reward: total was 36.530000. running mean: -9.476112\n",
      "ep 3033: ep_len:992 episode reward: total was -26.790000. running mean: -9.649251\n",
      "ep 3033: ep_len:47 episode reward: total was 20.500000. running mean: -9.347758\n",
      "ep 3033: ep_len:1152 episode reward: total was -9.340000. running mean: -9.347681\n",
      "ep 3033: ep_len:2842 episode reward: total was -4.620000. running mean: -9.300404\n",
      "epsilon:0.009992 episode_count: 45649. steps_count: 49081094.000000\n",
      "ep 3034: ep_len:1434 episode reward: total was 11.600000. running mean: -9.091400\n",
      "ep 3034: ep_len:647 episode reward: total was -10.410000. running mean: -9.104586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3034: ep_len:50 episode reward: total was 22.000000. running mean: -8.793540\n",
      "ep 3034: ep_len:3066 episode reward: total was -21.030000. running mean: -8.915905\n",
      "ep 3034: ep_len:832 episode reward: total was 33.190000. running mean: -8.494846\n",
      "ep 3034: ep_len:31 episode reward: total was 14.000000. running mean: -8.269897\n",
      "ep 3034: ep_len:142 episode reward: total was 66.500000. running mean: -7.522198\n",
      "ep 3034: ep_len:66 episode reward: total was 30.000000. running mean: -7.146976\n",
      "ep 3034: ep_len:1503 episode reward: total was 12.270000. running mean: -6.952806\n",
      "ep 3034: ep_len:3655 episode reward: total was -132.520000. running mean: -8.208478\n",
      "ep 3034: ep_len:1286 episode reward: total was -64.680000. running mean: -8.773194\n",
      "ep 3034: ep_len:7393 episode reward: total was 44.930000. running mean: -8.236162\n",
      "ep 3034: ep_len:1470 episode reward: total was -14.860000. running mean: -8.302400\n",
      "ep 3034: ep_len:77 episode reward: total was 37.000000. running mean: -7.849376\n",
      "ep 3034: ep_len:661 episode reward: total was 10.800000. running mean: -7.662882\n",
      "ep 3034: ep_len:2721 episode reward: total was -21.380000. running mean: -7.800053\n",
      "epsilon:0.009992 episode_count: 45665. steps_count: 49106128.000000\n",
      "ep 3035: ep_len:500 episode reward: total was 13.810000. running mean: -7.583953\n",
      "ep 3035: ep_len:755 episode reward: total was -18.680000. running mean: -7.694913\n",
      "ep 3035: ep_len:39 episode reward: total was 18.000000. running mean: -7.437964\n",
      "ep 3035: ep_len:3014 episode reward: total was -12.650000. running mean: -7.490085\n",
      "ep 3035: ep_len:780 episode reward: total was -14.590000. running mean: -7.561084\n",
      "ep 3035: ep_len:39 episode reward: total was 18.000000. running mean: -7.305473\n",
      "ep 3035: ep_len:125 episode reward: total was 59.500000. running mean: -6.637418\n",
      "ep 3035: ep_len:77 episode reward: total was 37.000000. running mean: -6.201044\n",
      "ep 3035: ep_len:1171 episode reward: total was 9.810000. running mean: -6.040934\n",
      "ep 3035: ep_len:367 episode reward: total was 13.280000. running mean: -5.847724\n",
      "ep 3035: ep_len:1618 episode reward: total was -83.600000. running mean: -6.625247\n",
      "ep 3035: ep_len:801 episode reward: total was 41.590000. running mean: -6.143095\n",
      "ep 3035: ep_len:1418 episode reward: total was -25.880000. running mean: -6.340464\n",
      "ep 3035: ep_len:57 episode reward: total was 27.000000. running mean: -6.007059\n",
      "ep 3035: ep_len:802 episode reward: total was -54.880000. running mean: -6.495788\n",
      "ep 3035: ep_len:2930 episode reward: total was -12.030000. running mean: -6.551131\n",
      "epsilon:0.009992 episode_count: 45681. steps_count: 49120621.000000\n",
      "ep 3036: ep_len:828 episode reward: total was -25.860000. running mean: -6.744219\n",
      "ep 3036: ep_len:1285 episode reward: total was -58.020000. running mean: -7.256977\n",
      "ep 3036: ep_len:2885 episode reward: total was -72.260000. running mean: -7.907007\n",
      "ep 3036: ep_len:1451 episode reward: total was 12.900000. running mean: -7.698937\n",
      "ep 3036: ep_len:99 episode reward: total was 42.000000. running mean: -7.201948\n",
      "ep 3036: ep_len:45 episode reward: total was 22.010000. running mean: -6.909828\n",
      "ep 3036: ep_len:650 episode reward: total was -12.930000. running mean: -6.970030\n",
      "ep 3036: ep_len:3960 episode reward: total was -126.800000. running mean: -8.168330\n",
      "ep 3036: ep_len:748 episode reward: total was -57.740000. running mean: -8.664046\n",
      "ep 3036: ep_len:780 episode reward: total was 59.450000. running mean: -7.982906\n",
      "ep 3036: ep_len:706 episode reward: total was 1.390000. running mean: -7.889177\n",
      "ep 3036: ep_len:50 episode reward: total was 23.500000. running mean: -7.575285\n",
      "ep 3036: ep_len:635 episode reward: total was -4.140000. running mean: -7.540932\n",
      "ep 3036: ep_len:2836 episode reward: total was -15.360000. running mean: -7.619123\n",
      "ep 3036: ep_len:41 episode reward: total was 19.000000. running mean: -7.352932\n",
      "epsilon:0.009992 episode_count: 45696. steps_count: 49137620.000000\n",
      "ep 3037: ep_len:857 episode reward: total was 17.950000. running mean: -7.099902\n",
      "ep 3037: ep_len:1184 episode reward: total was -19.640000. running mean: -7.225303\n",
      "ep 3037: ep_len:2986 episode reward: total was -31.330000. running mean: -7.466350\n",
      "ep 3037: ep_len:500 episode reward: total was 24.770000. running mean: -7.143987\n",
      "ep 3037: ep_len:46 episode reward: total was 21.500000. running mean: -6.857547\n",
      "ep 3037: ep_len:1123 episode reward: total was 10.860000. running mean: -6.680372\n",
      "ep 3037: ep_len:667 episode reward: total was 12.230000. running mean: -6.491268\n",
      "ep 3037: ep_len:717 episode reward: total was -34.330000. running mean: -6.769655\n",
      "ep 3037: ep_len:742 episode reward: total was 45.570000. running mean: -6.246259\n",
      "ep 3037: ep_len:1481 episode reward: total was 26.370000. running mean: -5.920096\n",
      "ep 3037: ep_len:89 episode reward: total was 40.000000. running mean: -5.460895\n",
      "ep 3037: ep_len:63 episode reward: total was 28.500000. running mean: -5.121286\n",
      "ep 3037: ep_len:1081 episode reward: total was 17.130000. running mean: -4.898773\n",
      "ep 3037: ep_len:2778 episode reward: total was 6.190000. running mean: -4.787885\n",
      "ep 3037: ep_len:62 episode reward: total was 28.000000. running mean: -4.460007\n",
      "epsilon:0.009992 episode_count: 45711. steps_count: 49151996.000000\n",
      "ep 3038: ep_len:629 episode reward: total was -20.060000. running mean: -4.616007\n",
      "ep 3038: ep_len:187 episode reward: total was -2.670000. running mean: -4.596546\n",
      "ep 3038: ep_len:66 episode reward: total was 31.500000. running mean: -4.235581\n",
      "ep 3038: ep_len:2904 episode reward: total was -35.860000. running mean: -4.551825\n",
      "ep 3038: ep_len:701 episode reward: total was 15.520000. running mean: -4.351107\n",
      "ep 3038: ep_len:44 episode reward: total was 20.500000. running mean: -4.102596\n",
      "ep 3038: ep_len:1460 episode reward: total was -97.680000. running mean: -5.038370\n",
      "ep 3038: ep_len:3507 episode reward: total was -101.530000. running mean: -6.003286\n",
      "ep 3038: ep_len:1593 episode reward: total was -0.610000. running mean: -5.949353\n",
      "ep 3038: ep_len:718 episode reward: total was -13.100000. running mean: -6.020860\n",
      "ep 3038: ep_len:634 episode reward: total was 5.000000. running mean: -5.910651\n",
      "ep 3038: ep_len:181 episode reward: total was 84.500000. running mean: -5.006545\n",
      "ep 3038: ep_len:82 episode reward: total was 39.500000. running mean: -4.561479\n",
      "ep 3038: ep_len:500 episode reward: total was 11.480000. running mean: -4.401064\n",
      "ep 3038: ep_len:2889 episode reward: total was -23.710000. running mean: -4.594154\n",
      "epsilon:0.009992 episode_count: 45726. steps_count: 49168091.000000\n",
      "ep 3039: ep_len:1469 episode reward: total was 12.190000. running mean: -4.426312\n",
      "ep 3039: ep_len:1641 episode reward: total was -21.160000. running mean: -4.593649\n",
      "ep 3039: ep_len:2961 episode reward: total was -21.090000. running mean: -4.758613\n",
      "ep 3039: ep_len:544 episode reward: total was -32.100000. running mean: -5.032027\n",
      "ep 3039: ep_len:671 episode reward: total was 8.260000. running mean: -4.899106\n",
      "ep 3039: ep_len:621 episode reward: total was 29.860000. running mean: -4.551515\n",
      "ep 3039: ep_len:1572 episode reward: total was -49.560000. running mean: -5.001600\n",
      "ep 3039: ep_len:820 episode reward: total was 26.020000. running mean: -4.691384\n",
      "ep 3039: ep_len:705 episode reward: total was 16.260000. running mean: -4.481870\n",
      "ep 3039: ep_len:84 episode reward: total was 39.000000. running mean: -4.047052\n",
      "ep 3039: ep_len:202 episode reward: total was 95.000000. running mean: -3.056581\n",
      "ep 3039: ep_len:902 episode reward: total was 18.000000. running mean: -2.846015\n",
      "ep 3039: ep_len:2928 episode reward: total was 4.350000. running mean: -2.774055\n",
      "epsilon:0.009992 episode_count: 45739. steps_count: 49183211.000000\n",
      "ep 3040: ep_len:642 episode reward: total was 21.960000. running mean: -2.526715\n",
      "ep 3040: ep_len:667 episode reward: total was -8.650000. running mean: -2.587947\n",
      "ep 3040: ep_len:48 episode reward: total was 22.500000. running mean: -2.337068\n",
      "ep 3040: ep_len:3041 episode reward: total was -38.030000. running mean: -2.693997\n",
      "ep 3040: ep_len:1165 episode reward: total was -28.210000. running mean: -2.949157\n",
      "ep 3040: ep_len:135 episode reward: total was 66.000000. running mean: -2.259666\n",
      "ep 3040: ep_len:707 episode reward: total was -24.230000. running mean: -2.479369\n",
      "ep 3040: ep_len:3562 episode reward: total was -105.720000. running mean: -3.511775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3040: ep_len:1561 episode reward: total was -24.070000. running mean: -3.717358\n",
      "ep 3040: ep_len:795 episode reward: total was 24.420000. running mean: -3.435984\n",
      "ep 3040: ep_len:1159 episode reward: total was -5.530000. running mean: -3.456924\n",
      "ep 3040: ep_len:55 episode reward: total was 26.000000. running mean: -3.162355\n",
      "ep 3040: ep_len:145 episode reward: total was 65.000000. running mean: -2.480731\n",
      "ep 3040: ep_len:745 episode reward: total was -21.050000. running mean: -2.666424\n",
      "ep 3040: ep_len:2843 episode reward: total was -11.040000. running mean: -2.750160\n",
      "epsilon:0.009992 episode_count: 45754. steps_count: 49200481.000000\n",
      "ep 3041: ep_len:1374 episode reward: total was 22.260000. running mean: -2.500058\n",
      "ep 3041: ep_len:670 episode reward: total was -78.160000. running mean: -3.256658\n",
      "ep 3041: ep_len:3095 episode reward: total was -0.850000. running mean: -3.232591\n",
      "ep 3041: ep_len:556 episode reward: total was -17.950000. running mean: -3.379765\n",
      "ep 3041: ep_len:110 episode reward: total was 53.500000. running mean: -2.810967\n",
      "ep 3041: ep_len:856 episode reward: total was 37.010000. running mean: -2.412758\n",
      "ep 3041: ep_len:3588 episode reward: total was -50.130000. running mean: -2.889930\n",
      "ep 3041: ep_len:3978 episode reward: total was -565.380000. running mean: -8.514831\n",
      "ep 3041: ep_len:830 episode reward: total was 58.360000. running mean: -7.846083\n",
      "ep 3041: ep_len:1089 episode reward: total was -4.340000. running mean: -7.811022\n",
      "ep 3041: ep_len:1190 episode reward: total was -7.460000. running mean: -7.807512\n",
      "ep 3041: ep_len:2889 episode reward: total was 1.450000. running mean: -7.714936\n",
      "ep 3041: ep_len:53 episode reward: total was 25.000000. running mean: -7.387787\n",
      "epsilon:0.009992 episode_count: 45767. steps_count: 49220759.000000\n",
      "ep 3042: ep_len:1132 episode reward: total was -13.640000. running mean: -7.450309\n",
      "ep 3042: ep_len:500 episode reward: total was 18.280000. running mean: -7.193006\n",
      "ep 3042: ep_len:2934 episode reward: total was -28.460000. running mean: -7.405676\n",
      "ep 3042: ep_len:664 episode reward: total was -2.650000. running mean: -7.358119\n",
      "ep 3042: ep_len:60 episode reward: total was 27.000000. running mean: -7.014538\n",
      "ep 3042: ep_len:89 episode reward: total was 40.000000. running mean: -6.544393\n",
      "ep 3042: ep_len:63 episode reward: total was 30.000000. running mean: -6.178949\n",
      "ep 3042: ep_len:1124 episode reward: total was 4.620000. running mean: -6.070959\n",
      "ep 3042: ep_len:3846 episode reward: total was -63.660000. running mean: -6.646850\n",
      "ep 3042: ep_len:716 episode reward: total was -22.860000. running mean: -6.808981\n",
      "ep 3042: ep_len:615 episode reward: total was 15.140000. running mean: -6.589491\n",
      "ep 3042: ep_len:918 episode reward: total was 41.620000. running mean: -6.107397\n",
      "ep 3042: ep_len:52 episode reward: total was 24.500000. running mean: -5.801323\n",
      "ep 3042: ep_len:160 episode reward: total was 78.500000. running mean: -4.958309\n",
      "ep 3042: ep_len:45 episode reward: total was 22.010000. running mean: -4.688626\n",
      "ep 3042: ep_len:81 episode reward: total was 36.000000. running mean: -4.281740\n",
      "ep 3042: ep_len:1140 episode reward: total was -20.080000. running mean: -4.439723\n",
      "ep 3042: ep_len:2896 episode reward: total was 1.980000. running mean: -4.375525\n",
      "epsilon:0.009992 episode_count: 45785. steps_count: 49237794.000000\n",
      "ep 3043: ep_len:1088 episode reward: total was -3.430000. running mean: -4.366070\n",
      "ep 3043: ep_len:759 episode reward: total was -14.800000. running mean: -4.470409\n",
      "ep 3043: ep_len:2916 episode reward: total was -13.090000. running mean: -4.556605\n",
      "ep 3043: ep_len:500 episode reward: total was -33.560000. running mean: -4.846639\n",
      "ep 3043: ep_len:78 episode reward: total was 37.500000. running mean: -4.423173\n",
      "ep 3043: ep_len:500 episode reward: total was -25.770000. running mean: -4.636641\n",
      "ep 3043: ep_len:3671 episode reward: total was -17.250000. running mean: -4.762775\n",
      "ep 3043: ep_len:1322 episode reward: total was -30.310000. running mean: -5.018247\n",
      "ep 3043: ep_len:752 episode reward: total was 38.470000. running mean: -4.583364\n",
      "ep 3043: ep_len:998 episode reward: total was 23.620000. running mean: -4.301331\n",
      "ep 3043: ep_len:96 episode reward: total was 46.500000. running mean: -3.793318\n",
      "ep 3043: ep_len:1017 episode reward: total was -5.850000. running mean: -3.813884\n",
      "ep 3043: ep_len:2869 episode reward: total was 8.110000. running mean: -3.694646\n",
      "ep 3043: ep_len:43 episode reward: total was 18.500000. running mean: -3.472699\n",
      "epsilon:0.009992 episode_count: 45799. steps_count: 49254403.000000\n",
      "ep 3044: ep_len:1160 episode reward: total was 2.260000. running mean: -3.415372\n",
      "ep 3044: ep_len:1293 episode reward: total was -67.030000. running mean: -4.051518\n",
      "ep 3044: ep_len:57 episode reward: total was 27.000000. running mean: -3.741003\n",
      "ep 3044: ep_len:2981 episode reward: total was 9.180000. running mean: -3.611793\n",
      "ep 3044: ep_len:2440 episode reward: total was -109.110000. running mean: -4.666775\n",
      "ep 3044: ep_len:88 episode reward: total was 42.500000. running mean: -4.195107\n",
      "ep 3044: ep_len:500 episode reward: total was 2.970000. running mean: -4.123456\n",
      "ep 3044: ep_len:314 episode reward: total was 9.200000. running mean: -3.990222\n",
      "ep 3044: ep_len:1218 episode reward: total was -35.150000. running mean: -4.301820\n",
      "ep 3044: ep_len:829 episode reward: total was 45.860000. running mean: -3.800201\n",
      "ep 3044: ep_len:544 episode reward: total was 4.260000. running mean: -3.719599\n",
      "ep 3044: ep_len:50 episode reward: total was 23.500000. running mean: -3.447403\n",
      "ep 3044: ep_len:73 episode reward: total was 35.000000. running mean: -3.062929\n",
      "ep 3044: ep_len:500 episode reward: total was 11.540000. running mean: -2.916900\n",
      "ep 3044: ep_len:46 episode reward: total was 21.500000. running mean: -2.672731\n",
      "epsilon:0.009992 episode_count: 45814. steps_count: 49266496.000000\n",
      "ep 3045: ep_len:641 episode reward: total was -15.030000. running mean: -2.796304\n",
      "ep 3045: ep_len:606 episode reward: total was 21.620000. running mean: -2.552141\n",
      "ep 3045: ep_len:2988 episode reward: total was -17.010000. running mean: -2.696719\n",
      "ep 3045: ep_len:500 episode reward: total was 26.800000. running mean: -2.401752\n",
      "ep 3045: ep_len:67 episode reward: total was 32.000000. running mean: -2.057735\n",
      "ep 3045: ep_len:82 episode reward: total was 36.500000. running mean: -1.672157\n",
      "ep 3045: ep_len:62 episode reward: total was 29.500000. running mean: -1.360436\n",
      "ep 3045: ep_len:50 episode reward: total was 22.000000. running mean: -1.126831\n",
      "ep 3045: ep_len:842 episode reward: total was 27.930000. running mean: -0.836263\n",
      "ep 3045: ep_len:3701 episode reward: total was -34.920000. running mean: -1.177100\n",
      "ep 3045: ep_len:1585 episode reward: total was -16.450000. running mean: -1.329829\n",
      "ep 3045: ep_len:785 episode reward: total was 2.110000. running mean: -1.295431\n",
      "ep 3045: ep_len:683 episode reward: total was -10.050000. running mean: -1.382977\n",
      "ep 3045: ep_len:103 episode reward: total was 48.500000. running mean: -0.884147\n",
      "ep 3045: ep_len:1068 episode reward: total was 34.140000. running mean: -0.533906\n",
      "ep 3045: ep_len:2914 episode reward: total was -0.770000. running mean: -0.536266\n",
      "ep 3045: ep_len:43 episode reward: total was 18.500000. running mean: -0.345904\n",
      "epsilon:0.009992 episode_count: 45831. steps_count: 49283216.000000\n",
      "ep 3046: ep_len:680 episode reward: total was -26.700000. running mean: -0.609445\n",
      "ep 3046: ep_len:737 episode reward: total was -9.590000. running mean: -0.699250\n",
      "ep 3046: ep_len:2931 episode reward: total was -258.680000. running mean: -3.279058\n",
      "ep 3046: ep_len:608 episode reward: total was 2.580000. running mean: -3.220467\n",
      "ep 3046: ep_len:53 episode reward: total was 25.000000. running mean: -2.938263\n",
      "ep 3046: ep_len:54 episode reward: total was 25.500000. running mean: -2.653880\n",
      "ep 3046: ep_len:1533 episode reward: total was -182.970000. running mean: -4.457041\n",
      "ep 3046: ep_len:3702 episode reward: total was 10.990000. running mean: -4.302571\n",
      "ep 3046: ep_len:2583 episode reward: total was -156.040000. running mean: -5.819945\n",
      "ep 3046: ep_len:659 episode reward: total was 14.200000. running mean: -5.619746\n",
      "ep 3046: ep_len:868 episode reward: total was 19.560000. running mean: -5.367948\n",
      "ep 3046: ep_len:205 episode reward: total was 99.500000. running mean: -4.319269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3046: ep_len:57 episode reward: total was 27.000000. running mean: -4.006076\n",
      "ep 3046: ep_len:116 episode reward: total was 52.000000. running mean: -3.446015\n",
      "ep 3046: ep_len:1123 episode reward: total was 9.870000. running mean: -3.312855\n",
      "ep 3046: ep_len:2734 episode reward: total was 7.250000. running mean: -3.207226\n",
      "epsilon:0.009992 episode_count: 45847. steps_count: 49301859.000000\n",
      "ep 3047: ep_len:3755 episode reward: total was -397.520000. running mean: -7.150354\n",
      "ep 3047: ep_len:216 episode reward: total was 14.000000. running mean: -6.938851\n",
      "ep 3047: ep_len:65 episode reward: total was 31.000000. running mean: -6.559462\n",
      "ep 3047: ep_len:2972 episode reward: total was -53.210000. running mean: -7.025968\n",
      "ep 3047: ep_len:697 episode reward: total was 7.540000. running mean: -6.880308\n",
      "ep 3047: ep_len:37 episode reward: total was 17.000000. running mean: -6.641505\n",
      "ep 3047: ep_len:56 episode reward: total was 26.500000. running mean: -6.310090\n",
      "ep 3047: ep_len:65 episode reward: total was 31.000000. running mean: -5.936989\n",
      "ep 3047: ep_len:703 episode reward: total was -52.980000. running mean: -6.407419\n",
      "ep 3047: ep_len:645 episode reward: total was 15.530000. running mean: -6.188045\n",
      "ep 3047: ep_len:1552 episode reward: total was 11.860000. running mean: -6.007564\n",
      "ep 3047: ep_len:814 episode reward: total was 13.430000. running mean: -5.813189\n",
      "ep 3047: ep_len:689 episode reward: total was -11.070000. running mean: -5.865757\n",
      "ep 3047: ep_len:39 episode reward: total was 15.000000. running mean: -5.657099\n",
      "ep 3047: ep_len:751 episode reward: total was -50.900000. running mean: -6.109528\n",
      "ep 3047: ep_len:2860 episode reward: total was 14.420000. running mean: -5.904233\n",
      "ep 3047: ep_len:52 episode reward: total was 23.000000. running mean: -5.615191\n",
      "epsilon:0.009992 episode_count: 45864. steps_count: 49317827.000000\n",
      "ep 3048: ep_len:1146 episode reward: total was 0.610000. running mean: -5.552939\n",
      "ep 3048: ep_len:970 episode reward: total was 24.070000. running mean: -5.256709\n",
      "ep 3048: ep_len:3025 episode reward: total was 25.480000. running mean: -4.949342\n",
      "ep 3048: ep_len:1495 episode reward: total was 21.270000. running mean: -4.687149\n",
      "ep 3048: ep_len:50 episode reward: total was 23.500000. running mean: -4.405277\n",
      "ep 3048: ep_len:1511 episode reward: total was 23.240000. running mean: -4.128825\n",
      "ep 3048: ep_len:4167 episode reward: total was -160.510000. running mean: -5.692636\n",
      "ep 3048: ep_len:500 episode reward: total was -24.460000. running mean: -5.880310\n",
      "ep 3048: ep_len:878 episode reward: total was 34.190000. running mean: -5.479607\n",
      "ep 3048: ep_len:1484 episode reward: total was 0.870000. running mean: -5.416111\n",
      "ep 3048: ep_len:69 episode reward: total was 33.000000. running mean: -5.031950\n",
      "ep 3048: ep_len:775 episode reward: total was -30.190000. running mean: -5.283530\n",
      "ep 3048: ep_len:2797 episode reward: total was -13.030000. running mean: -5.360995\n",
      "ep 3048: ep_len:64 episode reward: total was 30.500000. running mean: -5.002385\n",
      "epsilon:0.009992 episode_count: 45878. steps_count: 49336758.000000\n",
      "ep 3049: ep_len:1394 episode reward: total was -29.240000. running mean: -5.244761\n",
      "ep 3049: ep_len:659 episode reward: total was -10.750000. running mean: -5.299813\n",
      "ep 3049: ep_len:3119 episode reward: total was 18.000000. running mean: -5.066815\n",
      "ep 3049: ep_len:679 episode reward: total was 13.800000. running mean: -4.878147\n",
      "ep 3049: ep_len:140 episode reward: total was 67.000000. running mean: -4.159366\n",
      "ep 3049: ep_len:44 episode reward: total was 20.500000. running mean: -3.912772\n",
      "ep 3049: ep_len:1432 episode reward: total was 17.330000. running mean: -3.700344\n",
      "ep 3049: ep_len:3900 episode reward: total was -6.680000. running mean: -3.730141\n",
      "ep 3049: ep_len:576 episode reward: total was 6.910000. running mean: -3.623739\n",
      "ep 3049: ep_len:7277 episode reward: total was -15.140000. running mean: -3.738902\n",
      "ep 3049: ep_len:1063 episode reward: total was 37.890000. running mean: -3.322613\n",
      "ep 3049: ep_len:69 episode reward: total was 31.500000. running mean: -2.974387\n",
      "ep 3049: ep_len:646 episode reward: total was -3.810000. running mean: -2.982743\n",
      "ep 3049: ep_len:2912 episode reward: total was 7.100000. running mean: -2.881916\n",
      "epsilon:0.009992 episode_count: 45892. steps_count: 49360668.000000\n",
      "ep 3050: ep_len:1157 episode reward: total was 10.850000. running mean: -2.744596\n",
      "ep 3050: ep_len:174 episode reward: total was 13.760000. running mean: -2.579551\n",
      "ep 3050: ep_len:2966 episode reward: total was -88.030000. running mean: -3.434055\n",
      "ep 3050: ep_len:1477 episode reward: total was 30.670000. running mean: -3.093014\n",
      "ep 3050: ep_len:109 episode reward: total was 51.500000. running mean: -2.547084\n",
      "ep 3050: ep_len:51 episode reward: total was 24.000000. running mean: -2.281613\n",
      "ep 3050: ep_len:60 episode reward: total was 28.500000. running mean: -1.973797\n",
      "ep 3050: ep_len:1903 episode reward: total was -14.160000. running mean: -2.095659\n",
      "ep 3050: ep_len:3953 episode reward: total was -102.130000. running mean: -3.096003\n",
      "ep 3050: ep_len:775 episode reward: total was -22.520000. running mean: -3.290243\n",
      "ep 3050: ep_len:658 episode reward: total was 16.150000. running mean: -3.095840\n",
      "ep 3050: ep_len:1493 episode reward: total was 9.310000. running mean: -2.971782\n",
      "ep 3050: ep_len:55 episode reward: total was 26.000000. running mean: -2.682064\n",
      "ep 3050: ep_len:198 episode reward: total was 96.000000. running mean: -1.695243\n",
      "ep 3050: ep_len:909 episode reward: total was 27.930000. running mean: -1.398991\n",
      "ep 3050: ep_len:2919 episode reward: total was -53.200000. running mean: -1.917001\n",
      "epsilon:0.009992 episode_count: 45908. steps_count: 49379525.000000\n",
      "ep 3051: ep_len:701 episode reward: total was -8.000000. running mean: -1.977831\n",
      "ep 3051: ep_len:500 episode reward: total was 15.860000. running mean: -1.799453\n",
      "ep 3051: ep_len:2974 episode reward: total was -44.300000. running mean: -2.224458\n",
      "ep 3051: ep_len:860 episode reward: total was 69.510000. running mean: -1.507114\n",
      "ep 3051: ep_len:122 episode reward: total was 56.500000. running mean: -0.927043\n",
      "ep 3051: ep_len:97 episode reward: total was 45.500000. running mean: -0.462772\n",
      "ep 3051: ep_len:68 episode reward: total was 31.000000. running mean: -0.148144\n",
      "ep 3051: ep_len:590 episode reward: total was 26.980000. running mean: 0.123137\n",
      "ep 3051: ep_len:644 episode reward: total was 21.090000. running mean: 0.332806\n",
      "ep 3051: ep_len:1200 episode reward: total was -20.490000. running mean: 0.124578\n",
      "ep 3051: ep_len:903 episode reward: total was 57.710000. running mean: 0.700432\n",
      "ep 3051: ep_len:500 episode reward: total was 9.890000. running mean: 0.792328\n",
      "ep 3051: ep_len:2847 episode reward: total was -806.570000. running mean: -7.281296\n",
      "ep 3051: ep_len:2857 episode reward: total was -11.050000. running mean: -7.318983\n",
      "epsilon:0.009992 episode_count: 45922. steps_count: 49394388.000000\n",
      "ep 3052: ep_len:1065 episode reward: total was -1.640000. running mean: -7.262193\n",
      "ep 3052: ep_len:970 episode reward: total was -1.580000. running mean: -7.205371\n",
      "ep 3052: ep_len:2967 episode reward: total was -99.230000. running mean: -8.125617\n",
      "ep 3052: ep_len:853 episode reward: total was 4.770000. running mean: -7.996661\n",
      "ep 3052: ep_len:49 episode reward: total was 21.500000. running mean: -7.701695\n",
      "ep 3052: ep_len:63 episode reward: total was 30.000000. running mean: -7.324678\n",
      "ep 3052: ep_len:609 episode reward: total was 34.210000. running mean: -6.909331\n",
      "ep 3052: ep_len:4041 episode reward: total was -107.830000. running mean: -7.918538\n",
      "ep 3052: ep_len:532 episode reward: total was -23.130000. running mean: -8.070652\n",
      "ep 3052: ep_len:621 episode reward: total was -1.430000. running mean: -8.004246\n",
      "ep 3052: ep_len:700 episode reward: total was -12.920000. running mean: -8.053403\n",
      "ep 3052: ep_len:134 episode reward: total was 62.500000. running mean: -7.347869\n",
      "ep 3052: ep_len:87 episode reward: total was 39.000000. running mean: -6.884390\n",
      "ep 3052: ep_len:767 episode reward: total was -49.550000. running mean: -7.311047\n",
      "ep 3052: ep_len:2819 episode reward: total was 11.470000. running mean: -7.123236\n",
      "epsilon:0.009992 episode_count: 45937. steps_count: 49410665.000000\n",
      "ep 3053: ep_len:824 episode reward: total was -7.250000. running mean: -7.124504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3053: ep_len:641 episode reward: total was 20.810000. running mean: -6.845159\n",
      "ep 3053: ep_len:33 episode reward: total was 15.000000. running mean: -6.626707\n",
      "ep 3053: ep_len:3002 episode reward: total was -49.930000. running mean: -7.059740\n",
      "ep 3053: ep_len:586 episode reward: total was 5.750000. running mean: -6.931643\n",
      "ep 3053: ep_len:101 episode reward: total was 49.000000. running mean: -6.372326\n",
      "ep 3053: ep_len:500 episode reward: total was 12.590000. running mean: -6.182703\n",
      "ep 3053: ep_len:4117 episode reward: total was -124.900000. running mean: -7.369876\n",
      "ep 3053: ep_len:780 episode reward: total was -37.740000. running mean: -7.673577\n",
      "ep 3053: ep_len:750 episode reward: total was 31.740000. running mean: -7.279441\n",
      "ep 3053: ep_len:921 episode reward: total was 44.470000. running mean: -6.761947\n",
      "ep 3053: ep_len:190 episode reward: total was 90.500000. running mean: -5.789327\n",
      "ep 3053: ep_len:42 episode reward: total was 18.000000. running mean: -5.551434\n",
      "ep 3053: ep_len:737 episode reward: total was 1.140000. running mean: -5.484520\n",
      "ep 3053: ep_len:2847 episode reward: total was -49.300000. running mean: -5.922675\n",
      "epsilon:0.009992 episode_count: 45952. steps_count: 49426736.000000\n",
      "ep 3054: ep_len:2411 episode reward: total was -295.220000. running mean: -8.815648\n",
      "ep 3054: ep_len:749 episode reward: total was -20.670000. running mean: -8.934191\n",
      "ep 3054: ep_len:2868 episode reward: total was -79.460000. running mean: -9.639450\n",
      "ep 3054: ep_len:659 episode reward: total was 3.040000. running mean: -9.512655\n",
      "ep 3054: ep_len:589 episode reward: total was 3.700000. running mean: -9.380528\n",
      "ep 3054: ep_len:4104 episode reward: total was -76.820000. running mean: -10.054923\n",
      "ep 3054: ep_len:657 episode reward: total was -41.070000. running mean: -10.365074\n",
      "ep 3054: ep_len:738 episode reward: total was 57.500000. running mean: -9.686423\n",
      "ep 3054: ep_len:1146 episode reward: total was -7.900000. running mean: -9.668559\n",
      "ep 3054: ep_len:44 episode reward: total was 20.500000. running mean: -9.366873\n",
      "ep 3054: ep_len:690 episode reward: total was -52.000000. running mean: -9.793205\n",
      "ep 3054: ep_len:2868 episode reward: total was 5.590000. running mean: -9.639373\n",
      "epsilon:0.009992 episode_count: 45964. steps_count: 49444259.000000\n",
      "ep 3055: ep_len:731 episode reward: total was -20.130000. running mean: -9.744279\n",
      "ep 3055: ep_len:630 episode reward: total was 25.720000. running mean: -9.389636\n",
      "ep 3055: ep_len:44 episode reward: total was 20.500000. running mean: -9.090740\n",
      "ep 3055: ep_len:2974 episode reward: total was -41.710000. running mean: -9.416932\n",
      "ep 3055: ep_len:833 episode reward: total was 30.380000. running mean: -9.018963\n",
      "ep 3055: ep_len:90 episode reward: total was 42.000000. running mean: -8.508773\n",
      "ep 3055: ep_len:50 episode reward: total was 23.500000. running mean: -8.188686\n",
      "ep 3055: ep_len:680 episode reward: total was 17.990000. running mean: -7.926899\n",
      "ep 3055: ep_len:659 episode reward: total was 20.570000. running mean: -7.641930\n",
      "ep 3055: ep_len:844 episode reward: total was 24.800000. running mean: -7.317511\n",
      "ep 3055: ep_len:761 episode reward: total was 22.420000. running mean: -7.020135\n",
      "ep 3055: ep_len:500 episode reward: total was 20.490000. running mean: -6.745034\n",
      "ep 3055: ep_len:129 episode reward: total was 60.000000. running mean: -6.077584\n",
      "ep 3055: ep_len:62 episode reward: total was 29.500000. running mean: -5.721808\n",
      "ep 3055: ep_len:104 episode reward: total was 50.500000. running mean: -5.159590\n",
      "ep 3055: ep_len:760 episode reward: total was -9.740000. running mean: -5.205394\n",
      "ep 3055: ep_len:2842 episode reward: total was -14.290000. running mean: -5.296240\n",
      "epsilon:0.009992 episode_count: 45981. steps_count: 49456952.000000\n",
      "ep 3056: ep_len:841 episode reward: total was 22.960000. running mean: -5.013678\n",
      "ep 3056: ep_len:1190 episode reward: total was -51.900000. running mean: -5.482541\n",
      "ep 3056: ep_len:3055 episode reward: total was -38.500000. running mean: -5.812715\n",
      "ep 3056: ep_len:656 episode reward: total was 1.820000. running mean: -5.736388\n",
      "ep 3056: ep_len:879 episode reward: total was 44.990000. running mean: -5.229124\n",
      "ep 3056: ep_len:4089 episode reward: total was -62.160000. running mean: -5.798433\n",
      "ep 3056: ep_len:950 episode reward: total was -60.500000. running mean: -6.345449\n",
      "ep 3056: ep_len:663 episode reward: total was 2.730000. running mean: -6.254694\n",
      "ep 3056: ep_len:1090 episode reward: total was -8.460000. running mean: -6.276747\n",
      "ep 3056: ep_len:903 episode reward: total was -44.890000. running mean: -6.662880\n",
      "ep 3056: ep_len:2887 episode reward: total was 6.360000. running mean: -6.532651\n",
      "ep 3056: ep_len:34 episode reward: total was 15.500000. running mean: -6.312325\n",
      "epsilon:0.009992 episode_count: 45993. steps_count: 49474189.000000\n",
      "ep 3057: ep_len:659 episode reward: total was -4.450000. running mean: -6.293701\n",
      "ep 3057: ep_len:771 episode reward: total was -19.210000. running mean: -6.422864\n",
      "ep 3057: ep_len:2883 episode reward: total was -83.470000. running mean: -7.193336\n",
      "ep 3057: ep_len:887 episode reward: total was -6.880000. running mean: -7.190202\n",
      "ep 3057: ep_len:93 episode reward: total was 39.000000. running mean: -6.728300\n",
      "ep 3057: ep_len:664 episode reward: total was 5.460000. running mean: -6.606417\n",
      "ep 3057: ep_len:605 episode reward: total was 24.070000. running mean: -6.299653\n",
      "ep 3057: ep_len:604 episode reward: total was 16.870000. running mean: -6.067957\n",
      "ep 3057: ep_len:769 episode reward: total was 2.410000. running mean: -5.983177\n",
      "ep 3057: ep_len:624 episode reward: total was 2.030000. running mean: -5.903045\n",
      "ep 3057: ep_len:156 episode reward: total was 76.500000. running mean: -5.079015\n",
      "ep 3057: ep_len:36 episode reward: total was 16.500000. running mean: -4.863225\n",
      "ep 3057: ep_len:89 episode reward: total was 41.500000. running mean: -4.399592\n",
      "ep 3057: ep_len:1505 episode reward: total was 3.250000. running mean: -4.323096\n",
      "ep 3057: ep_len:2799 episode reward: total was -16.530000. running mean: -4.445165\n",
      "epsilon:0.009992 episode_count: 46008. steps_count: 49487333.000000\n",
      "ep 3058: ep_len:758 episode reward: total was -76.880000. running mean: -5.169514\n",
      "ep 3058: ep_len:216 episode reward: total was 16.020000. running mean: -4.957619\n",
      "ep 3058: ep_len:52 episode reward: total was 24.500000. running mean: -4.663043\n",
      "ep 3058: ep_len:2924 episode reward: total was -15.600000. running mean: -4.772412\n",
      "ep 3058: ep_len:536 episode reward: total was 2.160000. running mean: -4.703088\n",
      "ep 3058: ep_len:85 episode reward: total was 39.500000. running mean: -4.261057\n",
      "ep 3058: ep_len:1377 episode reward: total was -27.310000. running mean: -4.491547\n",
      "ep 3058: ep_len:366 episode reward: total was 12.750000. running mean: -4.319131\n",
      "ep 3058: ep_len:613 episode reward: total was -39.490000. running mean: -4.670840\n",
      "ep 3058: ep_len:737 episode reward: total was 6.230000. running mean: -4.561831\n",
      "ep 3058: ep_len:719 episode reward: total was -27.730000. running mean: -4.793513\n",
      "ep 3058: ep_len:60 episode reward: total was 28.500000. running mean: -4.460578\n",
      "ep 3058: ep_len:181 episode reward: total was 88.510000. running mean: -3.530872\n",
      "ep 3058: ep_len:1134 episode reward: total was 19.880000. running mean: -3.296763\n",
      "ep 3058: ep_len:2834 episode reward: total was -146.410000. running mean: -4.727896\n",
      "epsilon:0.009992 episode_count: 46023. steps_count: 49499925.000000\n",
      "ep 3059: ep_len:795 episode reward: total was -24.230000. running mean: -4.922917\n",
      "ep 3059: ep_len:1049 episode reward: total was 12.780000. running mean: -4.745888\n",
      "ep 3059: ep_len:79 episode reward: total was 36.500000. running mean: -4.333429\n",
      "ep 3059: ep_len:3102 episode reward: total was -95.180000. running mean: -5.241894\n",
      "ep 3059: ep_len:502 episode reward: total was 11.660000. running mean: -5.072876\n",
      "ep 3059: ep_len:1524 episode reward: total was 13.830000. running mean: -4.883847\n",
      "ep 3059: ep_len:3861 episode reward: total was -76.700000. running mean: -5.602008\n",
      "ep 3059: ep_len:827 episode reward: total was 5.250000. running mean: -5.493488\n",
      "ep 3059: ep_len:661 episode reward: total was 16.950000. running mean: -5.269053\n",
      "ep 3059: ep_len:623 episode reward: total was 6.470000. running mean: -5.151663\n",
      "ep 3059: ep_len:107 episode reward: total was 52.000000. running mean: -4.580146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3059: ep_len:500 episode reward: total was -0.950000. running mean: -4.543845\n",
      "ep 3059: ep_len:2814 episode reward: total was -3.730000. running mean: -4.535706\n",
      "ep 3059: ep_len:60 episode reward: total was 28.500000. running mean: -4.205349\n",
      "epsilon:0.009992 episode_count: 46037. steps_count: 49516429.000000\n",
      "ep 3060: ep_len:1488 episode reward: total was 0.630000. running mean: -4.156996\n",
      "ep 3060: ep_len:989 episode reward: total was 22.710000. running mean: -3.888326\n",
      "ep 3060: ep_len:2896 episode reward: total was -96.420000. running mean: -4.813643\n",
      "ep 3060: ep_len:1448 episode reward: total was 25.820000. running mean: -4.507306\n",
      "ep 3060: ep_len:82 episode reward: total was 36.500000. running mean: -4.097233\n",
      "ep 3060: ep_len:694 episode reward: total was 27.630000. running mean: -3.779961\n",
      "ep 3060: ep_len:4149 episode reward: total was -249.500000. running mean: -6.237161\n",
      "ep 3060: ep_len:1482 episode reward: total was -13.080000. running mean: -6.305589\n",
      "ep 3060: ep_len:828 episode reward: total was 30.870000. running mean: -5.933834\n",
      "ep 3060: ep_len:978 episode reward: total was 19.750000. running mean: -5.676995\n",
      "ep 3060: ep_len:75 episode reward: total was 33.000000. running mean: -5.290225\n",
      "ep 3060: ep_len:157 episode reward: total was 74.000000. running mean: -4.497323\n",
      "ep 3060: ep_len:2441 episode reward: total was -301.990000. running mean: -7.472250\n",
      "ep 3060: ep_len:2723 episode reward: total was -11.200000. running mean: -7.509527\n",
      "epsilon:0.009992 episode_count: 46051. steps_count: 49536859.000000\n",
      "ep 3061: ep_len:623 episode reward: total was -7.070000. running mean: -7.505132\n",
      "ep 3061: ep_len:790 episode reward: total was -19.540000. running mean: -7.625481\n",
      "ep 3061: ep_len:2960 episode reward: total was -54.880000. running mean: -8.098026\n",
      "ep 3061: ep_len:739 episode reward: total was -13.990000. running mean: -8.156946\n",
      "ep 3061: ep_len:89 episode reward: total was 43.000000. running mean: -7.645376\n",
      "ep 3061: ep_len:27 episode reward: total was 12.000000. running mean: -7.448922\n",
      "ep 3061: ep_len:500 episode reward: total was 26.610000. running mean: -7.108333\n",
      "ep 3061: ep_len:321 episode reward: total was 23.900000. running mean: -6.798250\n",
      "ep 3061: ep_len:931 episode reward: total was -5.230000. running mean: -6.782567\n",
      "ep 3061: ep_len:655 episode reward: total was -2.710000. running mean: -6.741842\n",
      "ep 3061: ep_len:1173 episode reward: total was 4.490000. running mean: -6.629523\n",
      "ep 3061: ep_len:54 episode reward: total was 24.000000. running mean: -6.323228\n",
      "ep 3061: ep_len:861 episode reward: total was 4.370000. running mean: -6.216296\n",
      "ep 3061: ep_len:2895 episode reward: total was -0.390000. running mean: -6.158033\n",
      "epsilon:0.009992 episode_count: 46065. steps_count: 49549477.000000\n",
      "ep 3062: ep_len:838 episode reward: total was 5.020000. running mean: -6.046252\n",
      "ep 3062: ep_len:806 episode reward: total was 8.980000. running mean: -5.895990\n",
      "ep 3062: ep_len:48 episode reward: total was 22.500000. running mean: -5.612030\n",
      "ep 3062: ep_len:2940 episode reward: total was -61.120000. running mean: -6.167110\n",
      "ep 3062: ep_len:573 episode reward: total was -42.400000. running mean: -6.529439\n",
      "ep 3062: ep_len:87 episode reward: total was 42.000000. running mean: -6.044144\n",
      "ep 3062: ep_len:114 episode reward: total was 52.500000. running mean: -5.458703\n",
      "ep 3062: ep_len:666 episode reward: total was -2.230000. running mean: -5.426416\n",
      "ep 3062: ep_len:663 episode reward: total was 24.220000. running mean: -5.129952\n",
      "ep 3062: ep_len:500 episode reward: total was 30.160000. running mean: -4.777052\n",
      "ep 3062: ep_len:762 episode reward: total was 35.750000. running mean: -4.371782\n",
      "ep 3062: ep_len:1002 episode reward: total was 18.920000. running mean: -4.138864\n",
      "ep 3062: ep_len:61 episode reward: total was 26.000000. running mean: -3.837475\n",
      "ep 3062: ep_len:1087 episode reward: total was -4.450000. running mean: -3.843600\n",
      "ep 3062: ep_len:2825 episode reward: total was -7.150000. running mean: -3.876664\n",
      "epsilon:0.009992 episode_count: 46080. steps_count: 49562449.000000\n",
      "ep 3063: ep_len:1452 episode reward: total was 9.390000. running mean: -3.743998\n",
      "ep 3063: ep_len:1226 episode reward: total was -30.330000. running mean: -4.009858\n",
      "ep 3063: ep_len:2914 episode reward: total was -94.610000. running mean: -4.915859\n",
      "ep 3063: ep_len:1094 episode reward: total was -5.390000. running mean: -4.920601\n",
      "ep 3063: ep_len:136 episode reward: total was 65.000000. running mean: -4.221395\n",
      "ep 3063: ep_len:59 episode reward: total was 25.000000. running mean: -3.929181\n",
      "ep 3063: ep_len:642 episode reward: total was -5.320000. running mean: -3.943089\n",
      "ep 3063: ep_len:680 episode reward: total was 16.130000. running mean: -3.742358\n",
      "ep 3063: ep_len:1582 episode reward: total was -46.550000. running mean: -4.170434\n",
      "ep 3063: ep_len:769 episode reward: total was 16.620000. running mean: -3.962530\n",
      "ep 3063: ep_len:951 episode reward: total was -3.300000. running mean: -3.955905\n",
      "ep 3063: ep_len:103 episode reward: total was 48.500000. running mean: -3.431346\n",
      "ep 3063: ep_len:62 episode reward: total was 28.000000. running mean: -3.117032\n",
      "ep 3063: ep_len:1524 episode reward: total was -11.560000. running mean: -3.201462\n",
      "ep 3063: ep_len:2836 episode reward: total was -19.550000. running mean: -3.364947\n",
      "epsilon:0.009992 episode_count: 46095. steps_count: 49578479.000000\n",
      "ep 3064: ep_len:939 episode reward: total was -87.520000. running mean: -4.206498\n",
      "ep 3064: ep_len:500 episode reward: total was 3.890000. running mean: -4.125533\n",
      "ep 3064: ep_len:62 episode reward: total was 28.000000. running mean: -3.804278\n",
      "ep 3064: ep_len:3017 episode reward: total was -53.090000. running mean: -4.297135\n",
      "ep 3064: ep_len:500 episode reward: total was 9.370000. running mean: -4.160463\n",
      "ep 3064: ep_len:43 episode reward: total was 18.500000. running mean: -3.933859\n",
      "ep 3064: ep_len:129 episode reward: total was 61.500000. running mean: -3.279520\n",
      "ep 3064: ep_len:54 episode reward: total was 24.000000. running mean: -3.006725\n",
      "ep 3064: ep_len:1406 episode reward: total was -219.420000. running mean: -5.170858\n",
      "ep 3064: ep_len:500 episode reward: total was 26.590000. running mean: -4.853249\n",
      "ep 3064: ep_len:559 episode reward: total was 1.380000. running mean: -4.790917\n",
      "ep 3064: ep_len:873 episode reward: total was 60.630000. running mean: -4.136707\n",
      "ep 3064: ep_len:693 episode reward: total was 1.300000. running mean: -4.082340\n",
      "ep 3064: ep_len:79 episode reward: total was 36.500000. running mean: -3.676517\n",
      "ep 3064: ep_len:25 episode reward: total was 9.500000. running mean: -3.544752\n",
      "ep 3064: ep_len:88 episode reward: total was 41.000000. running mean: -3.099304\n",
      "ep 3064: ep_len:500 episode reward: total was 15.370000. running mean: -2.914611\n",
      "ep 3064: ep_len:2756 episode reward: total was -16.560000. running mean: -3.051065\n",
      "ep 3064: ep_len:35 episode reward: total was 14.500000. running mean: -2.875555\n",
      "epsilon:0.009992 episode_count: 46114. steps_count: 49591237.000000\n",
      "ep 3065: ep_len:1155 episode reward: total was 0.270000. running mean: -2.844099\n",
      "ep 3065: ep_len:932 episode reward: total was 1.560000. running mean: -2.800058\n",
      "ep 3065: ep_len:3032 episode reward: total was -51.190000. running mean: -3.283957\n",
      "ep 3065: ep_len:1206 episode reward: total was 5.560000. running mean: -3.195518\n",
      "ep 3065: ep_len:1350 episode reward: total was 14.090000. running mean: -3.022663\n",
      "ep 3065: ep_len:3564 episode reward: total was -258.440000. running mean: -5.576836\n",
      "ep 3065: ep_len:1185 episode reward: total was -40.720000. running mean: -5.928268\n",
      "ep 3065: ep_len:810 episode reward: total was 36.750000. running mean: -5.501485\n",
      "ep 3065: ep_len:988 episode reward: total was 1.050000. running mean: -5.435970\n",
      "ep 3065: ep_len:202 episode reward: total was 99.500000. running mean: -4.386610\n",
      "ep 3065: ep_len:48 episode reward: total was 22.500000. running mean: -4.117744\n",
      "ep 3065: ep_len:605 episode reward: total was -21.760000. running mean: -4.294167\n",
      "ep 3065: ep_len:2811 episode reward: total was -34.280000. running mean: -4.594025\n",
      "ep 3065: ep_len:54 episode reward: total was 25.500000. running mean: -4.293085\n",
      "epsilon:0.009992 episode_count: 46128. steps_count: 49609179.000000\n",
      "ep 3066: ep_len:979 episode reward: total was -33.060000. running mean: -4.580754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3066: ep_len:1282 episode reward: total was -23.710000. running mean: -4.772047\n",
      "ep 3066: ep_len:62 episode reward: total was 29.500000. running mean: -4.429326\n",
      "ep 3066: ep_len:3006 episode reward: total was -87.080000. running mean: -5.255833\n",
      "ep 3066: ep_len:4546 episode reward: total was -1043.380000. running mean: -15.637075\n",
      "ep 3066: ep_len:47 episode reward: total was 22.000000. running mean: -15.260704\n",
      "ep 3066: ep_len:158 episode reward: total was 76.000000. running mean: -14.348097\n",
      "ep 3066: ep_len:111 episode reward: total was 54.000000. running mean: -13.664616\n",
      "ep 3066: ep_len:727 episode reward: total was 30.930000. running mean: -13.218670\n",
      "ep 3066: ep_len:319 episode reward: total was 7.930000. running mean: -13.007183\n",
      "ep 3066: ep_len:540 episode reward: total was -5.880000. running mean: -12.935911\n",
      "ep 3066: ep_len:7591 episode reward: total was -120.600000. running mean: -14.012552\n",
      "ep 3066: ep_len:737 episode reward: total was -13.000000. running mean: -14.002426\n",
      "ep 3066: ep_len:77 episode reward: total was 35.500000. running mean: -13.507402\n",
      "ep 3066: ep_len:134 episode reward: total was 62.010000. running mean: -12.752228\n",
      "ep 3066: ep_len:920 episode reward: total was -74.840000. running mean: -13.373106\n",
      "ep 3066: ep_len:45 episode reward: total was 18.000000. running mean: -13.059375\n",
      "ep 3066: ep_len:38 episode reward: total was 17.500000. running mean: -12.753781\n",
      "epsilon:0.009992 episode_count: 46146. steps_count: 49630498.000000\n",
      "ep 3067: ep_len:656 episode reward: total was -9.060000. running mean: -12.716843\n",
      "ep 3067: ep_len:922 episode reward: total was 8.220000. running mean: -12.507475\n",
      "ep 3067: ep_len:3019 episode reward: total was -40.270000. running mean: -12.785100\n",
      "ep 3067: ep_len:781 episode reward: total was -24.860000. running mean: -12.905849\n",
      "ep 3067: ep_len:136 episode reward: total was 63.500000. running mean: -12.141791\n",
      "ep 3067: ep_len:500 episode reward: total was 26.920000. running mean: -11.751173\n",
      "ep 3067: ep_len:344 episode reward: total was 4.940000. running mean: -11.584261\n",
      "ep 3067: ep_len:848 episode reward: total was 26.860000. running mean: -11.199818\n",
      "ep 3067: ep_len:821 episode reward: total was 29.240000. running mean: -10.795420\n",
      "ep 3067: ep_len:500 episode reward: total was 16.840000. running mean: -10.519066\n",
      "ep 3067: ep_len:100 episode reward: total was 48.500000. running mean: -9.928875\n",
      "ep 3067: ep_len:156 episode reward: total was 72.000000. running mean: -9.109587\n",
      "ep 3067: ep_len:62 episode reward: total was 29.500000. running mean: -8.723491\n",
      "ep 3067: ep_len:100 episode reward: total was 47.000000. running mean: -8.166256\n",
      "ep 3067: ep_len:1054 episode reward: total was -3.770000. running mean: -8.122293\n",
      "ep 3067: ep_len:2735 episode reward: total was -1234.710000. running mean: -20.388170\n",
      "ep 3067: ep_len:63 episode reward: total was 30.000000. running mean: -19.884289\n",
      "epsilon:0.009992 episode_count: 46163. steps_count: 49643295.000000\n",
      "ep 3068: ep_len:697 episode reward: total was -43.670000. running mean: -20.122146\n",
      "ep 3068: ep_len:1665 episode reward: total was -27.960000. running mean: -20.200524\n",
      "ep 3068: ep_len:3015 episode reward: total was -38.970000. running mean: -20.388219\n",
      "ep 3068: ep_len:500 episode reward: total was 15.560000. running mean: -20.028737\n",
      "ep 3068: ep_len:39 episode reward: total was 18.000000. running mean: -19.648449\n",
      "ep 3068: ep_len:95 episode reward: total was 44.500000. running mean: -19.006965\n",
      "ep 3068: ep_len:64 episode reward: total was 29.000000. running mean: -18.526895\n",
      "ep 3068: ep_len:500 episode reward: total was 44.520000. running mean: -17.896426\n",
      "ep 3068: ep_len:3869 episode reward: total was -73.680000. running mean: -18.454262\n",
      "ep 3068: ep_len:1596 episode reward: total was 11.840000. running mean: -18.151319\n",
      "ep 3068: ep_len:876 episode reward: total was 76.120000. running mean: -17.208606\n",
      "ep 3068: ep_len:643 episode reward: total was 12.470000. running mean: -16.911820\n",
      "ep 3068: ep_len:69 episode reward: total was 33.000000. running mean: -16.412702\n",
      "ep 3068: ep_len:1078 episode reward: total was -7.570000. running mean: -16.324275\n",
      "ep 3068: ep_len:2875 episode reward: total was -60.760000. running mean: -16.768632\n",
      "epsilon:0.009992 episode_count: 46178. steps_count: 49660876.000000\n",
      "ep 3069: ep_len:995 episode reward: total was -55.910000. running mean: -17.160046\n",
      "ep 3069: ep_len:964 episode reward: total was 4.760000. running mean: -16.940845\n",
      "ep 3069: ep_len:74 episode reward: total was 35.500000. running mean: -16.416437\n",
      "ep 3069: ep_len:2893 episode reward: total was -61.860000. running mean: -16.870873\n",
      "ep 3069: ep_len:1404 episode reward: total was -681.010000. running mean: -23.512264\n",
      "ep 3069: ep_len:139 episode reward: total was 66.500000. running mean: -22.612141\n",
      "ep 3069: ep_len:1009 episode reward: total was -14.070000. running mean: -22.526720\n",
      "ep 3069: ep_len:3848 episode reward: total was -80.980000. running mean: -23.111253\n",
      "ep 3069: ep_len:1272 episode reward: total was -42.350000. running mean: -23.303640\n",
      "ep 3069: ep_len:7382 episode reward: total was 44.030000. running mean: -22.630304\n",
      "ep 3069: ep_len:653 episode reward: total was 1.760000. running mean: -22.386401\n",
      "ep 3069: ep_len:85 episode reward: total was 41.000000. running mean: -21.752537\n",
      "ep 3069: ep_len:35 episode reward: total was 16.000000. running mean: -21.375011\n",
      "ep 3069: ep_len:1008 episode reward: total was -4.230000. running mean: -21.203561\n",
      "ep 3069: ep_len:2863 episode reward: total was -70.880000. running mean: -21.700326\n",
      "ep 3069: ep_len:40 episode reward: total was 17.000000. running mean: -21.313322\n",
      "epsilon:0.009992 episode_count: 46194. steps_count: 49685540.000000\n",
      "ep 3070: ep_len:1439 episode reward: total was 28.300000. running mean: -20.817189\n",
      "ep 3070: ep_len:987 episode reward: total was 38.640000. running mean: -20.222617\n",
      "ep 3070: ep_len:75 episode reward: total was 34.500000. running mean: -19.675391\n",
      "ep 3070: ep_len:3011 episode reward: total was -38.950000. running mean: -19.868137\n",
      "ep 3070: ep_len:801 episode reward: total was -11.620000. running mean: -19.785656\n",
      "ep 3070: ep_len:47 episode reward: total was 22.000000. running mean: -19.367799\n",
      "ep 3070: ep_len:644 episode reward: total was -16.420000. running mean: -19.338321\n",
      "ep 3070: ep_len:653 episode reward: total was 16.190000. running mean: -18.983038\n",
      "ep 3070: ep_len:588 episode reward: total was 15.120000. running mean: -18.642008\n",
      "ep 3070: ep_len:750 episode reward: total was 49.750000. running mean: -17.958088\n",
      "ep 3070: ep_len:500 episode reward: total was 32.680000. running mean: -17.451707\n",
      "ep 3070: ep_len:1035 episode reward: total was 12.050000. running mean: -17.156690\n",
      "ep 3070: ep_len:2860 episode reward: total was -4.710000. running mean: -17.032223\n",
      "epsilon:0.009992 episode_count: 46207. steps_count: 49698930.000000\n",
      "ep 3071: ep_len:902 episode reward: total was -22.910000. running mean: -17.091000\n",
      "ep 3071: ep_len:904 episode reward: total was -1.170000. running mean: -16.931790\n",
      "ep 3071: ep_len:2984 episode reward: total was -53.510000. running mean: -17.297573\n",
      "ep 3071: ep_len:500 episode reward: total was 2.030000. running mean: -17.104297\n",
      "ep 3071: ep_len:78 episode reward: total was 36.000000. running mean: -16.573254\n",
      "ep 3071: ep_len:1020 episode reward: total was -105.110000. running mean: -17.458621\n",
      "ep 3071: ep_len:615 episode reward: total was 27.320000. running mean: -17.010835\n",
      "ep 3071: ep_len:1125 episode reward: total was -49.800000. running mean: -17.338727\n",
      "ep 3071: ep_len:7578 episode reward: total was -128.330000. running mean: -18.448640\n",
      "ep 3071: ep_len:838 episode reward: total was 26.180000. running mean: -18.002353\n",
      "ep 3071: ep_len:60 episode reward: total was 28.500000. running mean: -17.537330\n",
      "ep 3071: ep_len:122 episode reward: total was 56.500000. running mean: -16.796956\n",
      "ep 3071: ep_len:52 episode reward: total was 23.000000. running mean: -16.398987\n",
      "ep 3071: ep_len:82 episode reward: total was 39.500000. running mean: -15.839997\n",
      "ep 3071: ep_len:1211 episode reward: total was 5.370000. running mean: -15.627897\n",
      "ep 3071: ep_len:2900 episode reward: total was 7.470000. running mean: -15.396918\n",
      "epsilon:0.009992 episode_count: 46223. steps_count: 49719901.000000\n",
      "ep 3072: ep_len:708 episode reward: total was -40.000000. running mean: -15.642949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3072: ep_len:1023 episode reward: total was 15.850000. running mean: -15.328019\n",
      "ep 3072: ep_len:74 episode reward: total was 35.500000. running mean: -14.819739\n",
      "ep 3072: ep_len:3010 episode reward: total was -29.820000. running mean: -14.969742\n",
      "ep 3072: ep_len:828 episode reward: total was 25.590000. running mean: -14.564144\n",
      "ep 3072: ep_len:41 episode reward: total was 19.000000. running mean: -14.228503\n",
      "ep 3072: ep_len:623 episode reward: total was 49.380000. running mean: -13.592418\n",
      "ep 3072: ep_len:357 episode reward: total was 9.230000. running mean: -13.364194\n",
      "ep 3072: ep_len:519 episode reward: total was -1.040000. running mean: -13.240952\n",
      "ep 3072: ep_len:848 episode reward: total was 71.190000. running mean: -12.396642\n",
      "ep 3072: ep_len:645 episode reward: total was -13.470000. running mean: -12.407376\n",
      "ep 3072: ep_len:72 episode reward: total was 34.500000. running mean: -11.938302\n",
      "ep 3072: ep_len:754 episode reward: total was -18.890000. running mean: -12.007819\n",
      "ep 3072: ep_len:2719 episode reward: total was -12.090000. running mean: -12.008641\n",
      "epsilon:0.009992 episode_count: 46237. steps_count: 49732122.000000\n",
      "ep 3073: ep_len:654 episode reward: total was -40.030000. running mean: -12.288854\n",
      "ep 3073: ep_len:635 episode reward: total was -10.860000. running mean: -12.274566\n",
      "ep 3073: ep_len:46 episode reward: total was 21.500000. running mean: -11.936820\n",
      "ep 3073: ep_len:2928 episode reward: total was -53.240000. running mean: -12.349852\n",
      "ep 3073: ep_len:734 episode reward: total was -7.980000. running mean: -12.306153\n",
      "ep 3073: ep_len:136 episode reward: total was 65.000000. running mean: -11.533092\n",
      "ep 3073: ep_len:61 episode reward: total was 27.500000. running mean: -11.142761\n",
      "ep 3073: ep_len:1423 episode reward: total was 15.070000. running mean: -10.880633\n",
      "ep 3073: ep_len:663 episode reward: total was 11.150000. running mean: -10.660327\n",
      "ep 3073: ep_len:694 episode reward: total was -87.660000. running mean: -11.430324\n",
      "ep 3073: ep_len:747 episode reward: total was 14.010000. running mean: -11.175921\n",
      "ep 3073: ep_len:616 episode reward: total was -13.200000. running mean: -11.196161\n",
      "ep 3073: ep_len:160 episode reward: total was 74.000000. running mean: -10.344200\n",
      "ep 3073: ep_len:57 episode reward: total was 25.500000. running mean: -9.985758\n",
      "ep 3073: ep_len:1001 episode reward: total was -41.670000. running mean: -10.302600\n",
      "ep 3073: ep_len:2834 episode reward: total was -720.070000. running mean: -17.400274\n",
      "epsilon:0.009992 episode_count: 46253. steps_count: 49745511.000000\n",
      "ep 3074: ep_len:1080 episode reward: total was 1.540000. running mean: -17.210871\n",
      "ep 3074: ep_len:500 episode reward: total was 15.430000. running mean: -16.884463\n",
      "ep 3074: ep_len:43 episode reward: total was 19.510000. running mean: -16.520518\n",
      "ep 3074: ep_len:2871 episode reward: total was -65.240000. running mean: -17.007713\n",
      "ep 3074: ep_len:822 episode reward: total was 21.920000. running mean: -16.618436\n",
      "ep 3074: ep_len:53 episode reward: total was 25.000000. running mean: -16.202251\n",
      "ep 3074: ep_len:144 episode reward: total was 67.500000. running mean: -15.365229\n",
      "ep 3074: ep_len:58 episode reward: total was 26.000000. running mean: -14.951577\n",
      "ep 3074: ep_len:1848 episode reward: total was -15.720000. running mean: -14.959261\n",
      "ep 3074: ep_len:342 episode reward: total was 14.010000. running mean: -14.669568\n",
      "ep 3074: ep_len:1575 episode reward: total was -127.330000. running mean: -15.796173\n",
      "ep 3074: ep_len:618 episode reward: total was 13.300000. running mean: -15.505211\n",
      "ep 3074: ep_len:867 episode reward: total was 49.340000. running mean: -14.856759\n",
      "ep 3074: ep_len:41 episode reward: total was 19.000000. running mean: -14.518191\n",
      "ep 3074: ep_len:1212 episode reward: total was 11.810000. running mean: -14.254909\n",
      "ep 3074: ep_len:2848 episode reward: total was -3.120000. running mean: -14.143560\n",
      "epsilon:0.009992 episode_count: 46269. steps_count: 49760433.000000\n",
      "ep 3075: ep_len:888 episode reward: total was 22.850000. running mean: -13.773624\n",
      "ep 3075: ep_len:676 episode reward: total was -22.180000. running mean: -13.857688\n",
      "ep 3075: ep_len:3009 episode reward: total was -52.550000. running mean: -14.244611\n",
      "ep 3075: ep_len:647 episode reward: total was 4.180000. running mean: -14.060365\n",
      "ep 3075: ep_len:95 episode reward: total was 46.000000. running mean: -13.459762\n",
      "ep 3075: ep_len:109 episode reward: total was 51.500000. running mean: -12.810164\n",
      "ep 3075: ep_len:1387 episode reward: total was -280.210000. running mean: -15.484162\n",
      "ep 3075: ep_len:641 episode reward: total was 9.400000. running mean: -15.235321\n",
      "ep 3075: ep_len:602 episode reward: total was -92.120000. running mean: -16.004168\n",
      "ep 3075: ep_len:626 episode reward: total was -0.860000. running mean: -15.852726\n",
      "ep 3075: ep_len:712 episode reward: total was -21.890000. running mean: -15.913099\n",
      "ep 3075: ep_len:72 episode reward: total was 33.000000. running mean: -15.423968\n",
      "ep 3075: ep_len:41 episode reward: total was 19.000000. running mean: -15.079728\n",
      "ep 3075: ep_len:660 episode reward: total was -3.340000. running mean: -14.962331\n",
      "ep 3075: ep_len:2763 episode reward: total was -9.050000. running mean: -14.903207\n",
      "epsilon:0.009992 episode_count: 46284. steps_count: 49773361.000000\n",
      "ep 3076: ep_len:819 episode reward: total was -39.510000. running mean: -15.149275\n",
      "ep 3076: ep_len:712 episode reward: total was -18.810000. running mean: -15.185883\n",
      "ep 3076: ep_len:3002 episode reward: total was -76.580000. running mean: -15.799824\n",
      "ep 3076: ep_len:500 episode reward: total was 26.280000. running mean: -15.379025\n",
      "ep 3076: ep_len:50 episode reward: total was 22.000000. running mean: -15.005235\n",
      "ep 3076: ep_len:100 episode reward: total was 47.000000. running mean: -14.385183\n",
      "ep 3076: ep_len:744 episode reward: total was -15.960000. running mean: -14.400931\n",
      "ep 3076: ep_len:3832 episode reward: total was -1332.540000. running mean: -27.582322\n",
      "ep 3076: ep_len:675 episode reward: total was -52.000000. running mean: -27.826498\n",
      "ep 3076: ep_len:594 episode reward: total was -16.970000. running mean: -27.717934\n",
      "ep 3076: ep_len:619 episode reward: total was -9.310000. running mean: -27.533854\n",
      "ep 3076: ep_len:212 episode reward: total was 100.000000. running mean: -26.258516\n",
      "ep 3076: ep_len:41 episode reward: total was 19.000000. running mean: -25.805930\n",
      "ep 3076: ep_len:1097 episode reward: total was -10.350000. running mean: -25.651371\n",
      "ep 3076: ep_len:2752 episode reward: total was 0.600000. running mean: -25.388857\n",
      "ep 3076: ep_len:50 episode reward: total was 22.000000. running mean: -24.914969\n",
      "epsilon:0.009992 episode_count: 46300. steps_count: 49789160.000000\n",
      "ep 3077: ep_len:671 episode reward: total was -6.590000. running mean: -24.731719\n",
      "ep 3077: ep_len:908 episode reward: total was 3.680000. running mean: -24.447602\n",
      "ep 3077: ep_len:86 episode reward: total was 40.000000. running mean: -23.803126\n",
      "ep 3077: ep_len:1182 episode reward: total was -6.890000. running mean: -23.633995\n",
      "ep 3077: ep_len:41 episode reward: total was 19.000000. running mean: -23.207655\n",
      "ep 3077: ep_len:500 episode reward: total was -7.470000. running mean: -23.050278\n",
      "ep 3077: ep_len:3605 episode reward: total was -244.490000. running mean: -25.264675\n",
      "ep 3077: ep_len:798 episode reward: total was -66.790000. running mean: -25.679929\n",
      "ep 3077: ep_len:620 episode reward: total was 17.330000. running mean: -25.249829\n",
      "ep 3077: ep_len:510 episode reward: total was -10.160000. running mean: -25.098931\n",
      "ep 3077: ep_len:679 episode reward: total was 0.120000. running mean: -24.846742\n",
      "ep 3077: ep_len:2833 episode reward: total was -15.080000. running mean: -24.749074\n",
      "epsilon:0.009992 episode_count: 46312. steps_count: 49801593.000000\n",
      "ep 3078: ep_len:1464 episode reward: total was -6.650000. running mean: -24.568084\n",
      "ep 3078: ep_len:747 episode reward: total was -67.460000. running mean: -24.997003\n",
      "ep 3078: ep_len:2945 episode reward: total was 0.560000. running mean: -24.741433\n",
      "ep 3078: ep_len:645 episode reward: total was 32.680000. running mean: -24.167218\n",
      "ep 3078: ep_len:105 episode reward: total was 51.000000. running mean: -23.415546\n",
      "ep 3078: ep_len:70 episode reward: total was 33.500000. running mean: -22.846391\n",
      "ep 3078: ep_len:635 episode reward: total was 4.130000. running mean: -22.576627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3078: ep_len:3634 episode reward: total was -76.970000. running mean: -23.120561\n",
      "ep 3078: ep_len:537 episode reward: total was -39.210000. running mean: -23.281455\n",
      "ep 3078: ep_len:583 episode reward: total was -9.860000. running mean: -23.147240\n",
      "ep 3078: ep_len:1070 episode reward: total was 44.820000. running mean: -22.467568\n",
      "ep 3078: ep_len:763 episode reward: total was -25.750000. running mean: -22.500392\n",
      "ep 3078: ep_len:2812 episode reward: total was -6.360000. running mean: -22.338988\n",
      "ep 3078: ep_len:68 episode reward: total was 31.000000. running mean: -21.805599\n",
      "epsilon:0.009992 episode_count: 46326. steps_count: 49817671.000000\n",
      "ep 3079: ep_len:787 episode reward: total was -30.680000. running mean: -21.894343\n",
      "ep 3079: ep_len:1006 episode reward: total was 33.840000. running mean: -21.336999\n",
      "ep 3079: ep_len:3064 episode reward: total was -3.390000. running mean: -21.157529\n",
      "ep 3079: ep_len:730 episode reward: total was -10.040000. running mean: -21.046354\n",
      "ep 3079: ep_len:36 episode reward: total was 15.000000. running mean: -20.685890\n",
      "ep 3079: ep_len:66 episode reward: total was 31.500000. running mean: -20.164031\n",
      "ep 3079: ep_len:781 episode reward: total was -9.530000. running mean: -20.057691\n",
      "ep 3079: ep_len:3789 episode reward: total was -370.050000. running mean: -23.557614\n",
      "ep 3079: ep_len:699 episode reward: total was -62.160000. running mean: -23.943638\n",
      "ep 3079: ep_len:836 episode reward: total was 62.520000. running mean: -23.079002\n",
      "ep 3079: ep_len:528 episode reward: total was 30.050000. running mean: -22.547712\n",
      "ep 3079: ep_len:40 episode reward: total was 18.500000. running mean: -22.137235\n",
      "ep 3079: ep_len:1103 episode reward: total was 29.870000. running mean: -21.617162\n",
      "ep 3079: ep_len:44 episode reward: total was 20.500000. running mean: -21.195991\n",
      "epsilon:0.009992 episode_count: 46340. steps_count: 49831180.000000\n",
      "ep 3080: ep_len:1028 episode reward: total was -120.180000. running mean: -22.185831\n",
      "ep 3080: ep_len:649 episode reward: total was -21.960000. running mean: -22.183572\n",
      "ep 3080: ep_len:3011 episode reward: total was -19.690000. running mean: -22.158637\n",
      "ep 3080: ep_len:1277 episode reward: total was -24.420000. running mean: -22.181250\n",
      "ep 3080: ep_len:55 episode reward: total was 24.500000. running mean: -21.714438\n",
      "ep 3080: ep_len:616 episode reward: total was 14.590000. running mean: -21.351393\n",
      "ep 3080: ep_len:336 episode reward: total was 6.880000. running mean: -21.069079\n",
      "ep 3080: ep_len:1273 episode reward: total was -22.790000. running mean: -21.086289\n",
      "ep 3080: ep_len:773 episode reward: total was 20.030000. running mean: -20.675126\n",
      "ep 3080: ep_len:1503 episode reward: total was 7.300000. running mean: -20.395375\n",
      "ep 3080: ep_len:55 episode reward: total was 23.000000. running mean: -19.961421\n",
      "ep 3080: ep_len:143 episode reward: total was 65.500000. running mean: -19.106807\n",
      "ep 3080: ep_len:1378 episode reward: total was 0.010000. running mean: -18.915639\n",
      "ep 3080: ep_len:2895 episode reward: total was -5.710000. running mean: -18.783582\n",
      "ep 3080: ep_len:58 episode reward: total was 27.500000. running mean: -18.320746\n",
      "epsilon:0.009992 episode_count: 46355. steps_count: 49846230.000000\n",
      "ep 3081: ep_len:1080 episode reward: total was 2.550000. running mean: -18.112039\n",
      "ep 3081: ep_len:732 episode reward: total was -8.000000. running mean: -18.010918\n",
      "ep 3081: ep_len:2950 episode reward: total was -11.060000. running mean: -17.941409\n",
      "ep 3081: ep_len:4841 episode reward: total was -1817.900000. running mean: -35.940995\n",
      "ep 3081: ep_len:44 episode reward: total was 20.500000. running mean: -35.376585\n",
      "ep 3081: ep_len:51 episode reward: total was 24.000000. running mean: -34.782819\n",
      "ep 3081: ep_len:1400 episode reward: total was -220.970000. running mean: -36.644691\n",
      "ep 3081: ep_len:3562 episode reward: total was -208.070000. running mean: -38.358944\n",
      "ep 3081: ep_len:1574 episode reward: total was -48.550000. running mean: -38.460855\n",
      "ep 3081: ep_len:693 episode reward: total was 35.250000. running mean: -37.723746\n",
      "ep 3081: ep_len:709 episode reward: total was -23.850000. running mean: -37.585009\n",
      "ep 3081: ep_len:41 episode reward: total was 19.000000. running mean: -37.019159\n",
      "ep 3081: ep_len:1462 episode reward: total was 11.360000. running mean: -36.535367\n",
      "ep 3081: ep_len:2731 episode reward: total was 12.060000. running mean: -36.049413\n",
      "epsilon:0.009992 episode_count: 46369. steps_count: 49868100.000000\n",
      "ep 3082: ep_len:701 episode reward: total was -44.150000. running mean: -36.130419\n",
      "ep 3082: ep_len:680 episode reward: total was -18.330000. running mean: -35.952415\n",
      "ep 3082: ep_len:67 episode reward: total was 32.000000. running mean: -35.272891\n",
      "ep 3082: ep_len:2988 episode reward: total was 5.610000. running mean: -34.864062\n",
      "ep 3082: ep_len:902 episode reward: total was 90.540000. running mean: -33.610021\n",
      "ep 3082: ep_len:37 episode reward: total was 14.000000. running mean: -33.133921\n",
      "ep 3082: ep_len:1421 episode reward: total was -155.640000. running mean: -34.358982\n",
      "ep 3082: ep_len:327 episode reward: total was 12.850000. running mean: -33.886892\n",
      "ep 3082: ep_len:678 episode reward: total was -67.270000. running mean: -34.220723\n",
      "ep 3082: ep_len:779 episode reward: total was -270.630000. running mean: -36.584816\n",
      "ep 3082: ep_len:636 episode reward: total was -7.500000. running mean: -36.293968\n",
      "ep 3082: ep_len:93 episode reward: total was 45.000000. running mean: -35.481028\n",
      "ep 3082: ep_len:990 episode reward: total was -130.360000. running mean: -36.429818\n",
      "ep 3082: ep_len:2871 episode reward: total was -20.520000. running mean: -36.270720\n",
      "epsilon:0.009992 episode_count: 46383. steps_count: 49881270.000000\n",
      "ep 3083: ep_len:1438 episode reward: total was 29.520000. running mean: -35.612813\n",
      "ep 3083: ep_len:711 episode reward: total was 5.880000. running mean: -35.197884\n",
      "ep 3083: ep_len:54 episode reward: total was 25.500000. running mean: -34.590906\n",
      "ep 3083: ep_len:2976 episode reward: total was -37.430000. running mean: -34.619297\n",
      "ep 3083: ep_len:646 episode reward: total was 6.310000. running mean: -34.210004\n",
      "ep 3083: ep_len:111 episode reward: total was 54.000000. running mean: -33.327904\n",
      "ep 3083: ep_len:51 episode reward: total was 24.000000. running mean: -32.754624\n",
      "ep 3083: ep_len:1077 episode reward: total was -53.980000. running mean: -32.966878\n",
      "ep 3083: ep_len:3920 episode reward: total was -83.310000. running mean: -33.470309\n",
      "ep 3083: ep_len:1591 episode reward: total was -21.660000. running mean: -33.352206\n",
      "ep 3083: ep_len:848 episode reward: total was 68.030000. running mean: -32.338384\n",
      "ep 3083: ep_len:831 episode reward: total was -0.280000. running mean: -32.017800\n",
      "ep 3083: ep_len:72 episode reward: total was 33.000000. running mean: -31.367622\n",
      "ep 3083: ep_len:41 episode reward: total was 19.000000. running mean: -30.863946\n",
      "ep 3083: ep_len:108 episode reward: total was 52.500000. running mean: -30.030307\n",
      "ep 3083: ep_len:812 episode reward: total was -64.450000. running mean: -30.374504\n",
      "ep 3083: ep_len:2878 episode reward: total was -25.770000. running mean: -30.328459\n",
      "epsilon:0.009992 episode_count: 46400. steps_count: 49899435.000000\n",
      "ep 3084: ep_len:1495 episode reward: total was 14.200000. running mean: -29.883174\n",
      "ep 3084: ep_len:500 episode reward: total was -72.600000. running mean: -30.310342\n",
      "ep 3084: ep_len:3099 episode reward: total was -84.790000. running mean: -30.855139\n",
      "ep 3084: ep_len:1644 episode reward: total was -53.610000. running mean: -31.082688\n",
      "ep 3084: ep_len:44 episode reward: total was 20.500000. running mean: -30.566861\n",
      "ep 3084: ep_len:97 episode reward: total was 45.500000. running mean: -29.806192\n",
      "ep 3084: ep_len:109 episode reward: total was 51.500000. running mean: -28.993130\n",
      "ep 3084: ep_len:51 episode reward: total was 24.000000. running mean: -28.463199\n",
      "ep 3084: ep_len:500 episode reward: total was 28.730000. running mean: -27.891267\n",
      "ep 3084: ep_len:641 episode reward: total was 26.540000. running mean: -27.346954\n",
      "ep 3084: ep_len:607 episode reward: total was -66.820000. running mean: -27.741685\n",
      "ep 3084: ep_len:677 episode reward: total was 31.560000. running mean: -27.148668\n",
      "ep 3084: ep_len:974 episode reward: total was 6.140000. running mean: -26.815781\n",
      "ep 3084: ep_len:59 episode reward: total was 26.500000. running mean: -26.282623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3084: ep_len:597 episode reward: total was 9.680000. running mean: -25.922997\n",
      "ep 3084: ep_len:40 episode reward: total was 18.500000. running mean: -25.478767\n",
      "ep 3084: ep_len:69 episode reward: total was 28.500000. running mean: -24.938979\n",
      "epsilon:0.009992 episode_count: 46417. steps_count: 49910638.000000\n",
      "ep 3085: ep_len:1390 episode reward: total was 8.710000. running mean: -24.602490\n",
      "ep 3085: ep_len:949 episode reward: total was 27.080000. running mean: -24.085665\n",
      "ep 3085: ep_len:72 episode reward: total was 34.500000. running mean: -23.499808\n",
      "ep 3085: ep_len:3043 episode reward: total was 3.320000. running mean: -23.231610\n",
      "ep 3085: ep_len:622 episode reward: total was -10.430000. running mean: -23.103594\n",
      "ep 3085: ep_len:43 episode reward: total was 20.000000. running mean: -22.672558\n",
      "ep 3085: ep_len:164 episode reward: total was 77.500000. running mean: -21.670832\n",
      "ep 3085: ep_len:921 episode reward: total was -10.910000. running mean: -21.563224\n",
      "ep 3085: ep_len:3661 episode reward: total was -701.490000. running mean: -28.362492\n",
      "ep 3085: ep_len:672 episode reward: total was -8.730000. running mean: -28.166167\n",
      "ep 3085: ep_len:825 episode reward: total was 40.730000. running mean: -27.477205\n",
      "ep 3085: ep_len:500 episode reward: total was 52.360000. running mean: -26.678833\n",
      "ep 3085: ep_len:180 episode reward: total was 84.000000. running mean: -25.572045\n",
      "ep 3085: ep_len:97 episode reward: total was 45.500000. running mean: -24.861324\n",
      "ep 3085: ep_len:1078 episode reward: total was 8.650000. running mean: -24.526211\n",
      "ep 3085: ep_len:2963 episode reward: total was -86.650000. running mean: -25.147449\n",
      "ep 3085: ep_len:58 episode reward: total was 27.500000. running mean: -24.620975\n",
      "epsilon:0.009992 episode_count: 46434. steps_count: 49927876.000000\n",
      "ep 3086: ep_len:1148 episode reward: total was 5.840000. running mean: -24.316365\n",
      "ep 3086: ep_len:717 episode reward: total was -19.490000. running mean: -24.268101\n",
      "ep 3086: ep_len:65 episode reward: total was 31.000000. running mean: -23.715420\n",
      "ep 3086: ep_len:2868 episode reward: total was -36.990000. running mean: -23.848166\n",
      "ep 3086: ep_len:1430 episode reward: total was 33.230000. running mean: -23.277384\n",
      "ep 3086: ep_len:118 episode reward: total was 57.500000. running mean: -22.469610\n",
      "ep 3086: ep_len:640 episode reward: total was 1.180000. running mean: -22.233114\n",
      "ep 3086: ep_len:3654 episode reward: total was -177.370000. running mean: -23.784483\n",
      "ep 3086: ep_len:519 episode reward: total was -4.070000. running mean: -23.587338\n",
      "ep 3086: ep_len:763 episode reward: total was 1.960000. running mean: -23.331865\n",
      "ep 3086: ep_len:684 episode reward: total was 2.020000. running mean: -23.078346\n",
      "ep 3086: ep_len:184 episode reward: total was 87.500000. running mean: -21.972563\n",
      "ep 3086: ep_len:86 episode reward: total was 40.000000. running mean: -21.352837\n",
      "ep 3086: ep_len:1523 episode reward: total was -8.230000. running mean: -21.221609\n",
      "ep 3086: ep_len:2825 episode reward: total was -28.880000. running mean: -21.298193\n",
      "epsilon:0.009992 episode_count: 46449. steps_count: 49945100.000000\n",
      "ep 3087: ep_len:1133 episode reward: total was 5.450000. running mean: -21.030711\n",
      "ep 3087: ep_len:1016 episode reward: total was 16.330000. running mean: -20.657104\n",
      "ep 3087: ep_len:3023 episode reward: total was 21.810000. running mean: -20.232433\n",
      "ep 3087: ep_len:561 episode reward: total was -1.630000. running mean: -20.046408\n",
      "ep 3087: ep_len:500 episode reward: total was -10.920000. running mean: -19.955144\n",
      "ep 3087: ep_len:3613 episode reward: total was -49.260000. running mean: -20.248193\n",
      "ep 3087: ep_len:752 episode reward: total was -16.530000. running mean: -20.211011\n",
      "ep 3087: ep_len:7187 episode reward: total was -146.220000. running mean: -21.471101\n",
      "ep 3087: ep_len:662 episode reward: total was -7.690000. running mean: -21.333290\n",
      "ep 3087: ep_len:60 episode reward: total was 27.000000. running mean: -20.849957\n",
      "ep 3087: ep_len:139 episode reward: total was 65.000000. running mean: -19.991457\n",
      "ep 3087: ep_len:41 episode reward: total was 19.000000. running mean: -19.601543\n",
      "ep 3087: ep_len:629 episode reward: total was -13.250000. running mean: -19.538027\n",
      "ep 3087: ep_len:2865 episode reward: total was -4.260000. running mean: -19.385247\n",
      "epsilon:0.009992 episode_count: 46463. steps_count: 49967281.000000\n",
      "ep 3088: ep_len:1541 episode reward: total was 38.020000. running mean: -18.811195\n",
      "ep 3088: ep_len:643 episode reward: total was -30.960000. running mean: -18.932683\n",
      "ep 3088: ep_len:81 episode reward: total was 37.500000. running mean: -18.368356\n",
      "ep 3088: ep_len:2923 episode reward: total was -18.930000. running mean: -18.373972\n",
      "ep 3088: ep_len:2350 episode reward: total was -220.930000. running mean: -20.399533\n",
      "ep 3088: ep_len:80 episode reward: total was 37.000000. running mean: -19.825537\n",
      "ep 3088: ep_len:753 episode reward: total was -5.770000. running mean: -19.684982\n",
      "ep 3088: ep_len:338 episode reward: total was 16.510000. running mean: -19.323032\n",
      "ep 3088: ep_len:3924 episode reward: total was -716.410000. running mean: -26.293902\n",
      "ep 3088: ep_len:749 episode reward: total was 32.380000. running mean: -25.707163\n",
      "ep 3088: ep_len:682 episode reward: total was -10.890000. running mean: -25.558991\n",
      "ep 3088: ep_len:170 episode reward: total was 82.000000. running mean: -24.483401\n",
      "ep 3088: ep_len:46 episode reward: total was 21.500000. running mean: -24.023567\n",
      "ep 3088: ep_len:72 episode reward: total was 34.500000. running mean: -23.438331\n",
      "ep 3088: ep_len:810 episode reward: total was -18.330000. running mean: -23.387248\n",
      "ep 3088: ep_len:2815 episode reward: total was -23.130000. running mean: -23.384676\n",
      "epsilon:0.009992 episode_count: 46479. steps_count: 49985258.000000\n",
      "ep 3089: ep_len:808 episode reward: total was -26.430000. running mean: -23.415129\n",
      "ep 3089: ep_len:1154 episode reward: total was -59.670000. running mean: -23.777678\n",
      "ep 3089: ep_len:2916 episode reward: total was -4.710000. running mean: -23.587001\n",
      "ep 3089: ep_len:856 episode reward: total was 72.440000. running mean: -22.626731\n",
      "ep 3089: ep_len:1464 episode reward: total was -112.370000. running mean: -23.524164\n",
      "ep 3089: ep_len:3839 episode reward: total was -53.750000. running mean: -23.826422\n",
      "ep 3089: ep_len:1232 episode reward: total was -85.820000. running mean: -24.446358\n",
      "ep 3089: ep_len:7410 episode reward: total was 47.510000. running mean: -23.726794\n",
      "ep 3089: ep_len:500 episode reward: total was 28.170000. running mean: -23.207826\n",
      "ep 3089: ep_len:85 episode reward: total was 39.500000. running mean: -22.580748\n",
      "ep 3089: ep_len:648 episode reward: total was -14.900000. running mean: -22.503940\n",
      "ep 3089: ep_len:2867 episode reward: total was -62.730000. running mean: -22.906201\n",
      "epsilon:0.009992 episode_count: 46491. steps_count: 50009037.000000\n",
      "ep 3090: ep_len:1043 episode reward: total was -94.780000. running mean: -23.624939\n",
      "ep 3090: ep_len:733 episode reward: total was -6.980000. running mean: -23.458490\n",
      "ep 3090: ep_len:2922 episode reward: total was -36.170000. running mean: -23.585605\n",
      "ep 3090: ep_len:654 episode reward: total was 8.720000. running mean: -23.262549\n",
      "ep 3090: ep_len:85 episode reward: total was 39.500000. running mean: -22.634923\n",
      "ep 3090: ep_len:71 episode reward: total was 34.000000. running mean: -22.068574\n",
      "ep 3090: ep_len:707 episode reward: total was 24.820000. running mean: -21.599688\n",
      "ep 3090: ep_len:350 episode reward: total was 13.110000. running mean: -21.252591\n",
      "ep 3090: ep_len:1261 episode reward: total was -84.520000. running mean: -21.885265\n",
      "ep 3090: ep_len:895 episode reward: total was 58.950000. running mean: -21.076913\n",
      "ep 3090: ep_len:593 episode reward: total was 15.930000. running mean: -20.706844\n",
      "ep 3090: ep_len:68 episode reward: total was 31.000000. running mean: -20.189775\n",
      "ep 3090: ep_len:52 episode reward: total was 24.500000. running mean: -19.742877\n",
      "ep 3090: ep_len:818 episode reward: total was -18.430000. running mean: -19.729749\n",
      "ep 3090: ep_len:2811 episode reward: total was -19.980000. running mean: -19.732251\n",
      "epsilon:0.009992 episode_count: 46506. steps_count: 50022100.000000\n",
      "ep 3091: ep_len:1014 episode reward: total was -77.900000. running mean: -20.313929\n",
      "ep 3091: ep_len:967 episode reward: total was 31.700000. running mean: -19.793789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3091: ep_len:3021 episode reward: total was 2.080000. running mean: -19.575051\n",
      "ep 3091: ep_len:607 episode reward: total was -77.950000. running mean: -20.158801\n",
      "ep 3091: ep_len:38 episode reward: total was 17.500000. running mean: -19.782213\n",
      "ep 3091: ep_len:1008 episode reward: total was -48.670000. running mean: -20.071091\n",
      "ep 3091: ep_len:3698 episode reward: total was -165.760000. running mean: -21.527980\n",
      "ep 3091: ep_len:600 episode reward: total was 8.100000. running mean: -21.231700\n",
      "ep 3091: ep_len:631 episode reward: total was 0.080000. running mean: -21.018583\n",
      "ep 3091: ep_len:1504 episode reward: total was -4.750000. running mean: -20.855897\n",
      "ep 3091: ep_len:58 episode reward: total was 27.500000. running mean: -20.372338\n",
      "ep 3091: ep_len:92 episode reward: total was 45.510000. running mean: -19.713515\n",
      "ep 3091: ep_len:1447 episode reward: total was 2.760000. running mean: -19.488780\n",
      "ep 3091: ep_len:2831 episode reward: total was -18.780000. running mean: -19.481692\n",
      "ep 3091: ep_len:49 episode reward: total was 23.000000. running mean: -19.056875\n",
      "epsilon:0.009992 episode_count: 46521. steps_count: 50039665.000000\n",
      "ep 3092: ep_len:1419 episode reward: total was 21.060000. running mean: -18.655706\n",
      "ep 3092: ep_len:3143 episode reward: total was -210.520000. running mean: -20.574349\n",
      "ep 3092: ep_len:3026 episode reward: total was -17.270000. running mean: -20.541306\n",
      "ep 3092: ep_len:644 episode reward: total was -5.330000. running mean: -20.389193\n",
      "ep 3092: ep_len:841 episode reward: total was 22.630000. running mean: -19.959001\n",
      "ep 3092: ep_len:3598 episode reward: total was -6.900000. running mean: -19.828411\n",
      "ep 3092: ep_len:934 episode reward: total was -33.250000. running mean: -19.962627\n",
      "ep 3092: ep_len:7355 episode reward: total was 40.080000. running mean: -19.362200\n",
      "ep 3092: ep_len:745 episode reward: total was 21.860000. running mean: -18.949978\n",
      "ep 3092: ep_len:155 episode reward: total was 18.510000. running mean: -18.575379\n",
      "ep 3092: ep_len:632 episode reward: total was 8.860000. running mean: -18.301025\n",
      "ep 3092: ep_len:2877 episode reward: total was -42.550000. running mean: -18.543515\n",
      "epsilon:0.009992 episode_count: 46533. steps_count: 50065034.000000\n",
      "ep 3093: ep_len:746 episode reward: total was -73.510000. running mean: -19.093179\n",
      "ep 3093: ep_len:500 episode reward: total was -52.540000. running mean: -19.427648\n",
      "ep 3093: ep_len:3097 episode reward: total was -24.550000. running mean: -19.478871\n",
      "ep 3093: ep_len:1194 episode reward: total was -52.870000. running mean: -19.812782\n",
      "ep 3093: ep_len:63 episode reward: total was 30.000000. running mean: -19.314655\n",
      "ep 3093: ep_len:1893 episode reward: total was -97.080000. running mean: -20.092308\n",
      "ep 3093: ep_len:4021 episode reward: total was -200.970000. running mean: -21.901085\n",
      "ep 3093: ep_len:2322 episode reward: total was -133.700000. running mean: -23.019074\n",
      "ep 3093: ep_len:753 episode reward: total was 5.810000. running mean: -22.730783\n",
      "ep 3093: ep_len:687 episode reward: total was -13.050000. running mean: -22.633976\n",
      "ep 3093: ep_len:122 episode reward: total was 59.500000. running mean: -21.812636\n",
      "ep 3093: ep_len:1041 episode reward: total was -56.180000. running mean: -22.156309\n",
      "ep 3093: ep_len:2744 episode reward: total was -0.490000. running mean: -21.939646\n",
      "ep 3093: ep_len:60 episode reward: total was 25.500000. running mean: -21.465250\n",
      "epsilon:0.009992 episode_count: 46547. steps_count: 50084277.000000\n",
      "ep 3094: ep_len:923 episode reward: total was -117.730000. running mean: -22.427897\n",
      "ep 3094: ep_len:1620 episode reward: total was -55.560000. running mean: -22.759218\n",
      "ep 3094: ep_len:2870 episode reward: total was -37.980000. running mean: -22.911426\n",
      "ep 3094: ep_len:500 episode reward: total was 7.410000. running mean: -22.608212\n",
      "ep 3094: ep_len:143 episode reward: total was 68.500000. running mean: -21.697130\n",
      "ep 3094: ep_len:1469 episode reward: total was -1.610000. running mean: -21.496259\n",
      "ep 3094: ep_len:3765 episode reward: total was -20.270000. running mean: -21.483996\n",
      "ep 3094: ep_len:666 episode reward: total was -39.970000. running mean: -21.668856\n",
      "ep 3094: ep_len:777 episode reward: total was -1.850000. running mean: -21.470667\n",
      "ep 3094: ep_len:575 episode reward: total was 0.460000. running mean: -21.251361\n",
      "ep 3094: ep_len:45 episode reward: total was 21.000000. running mean: -20.828847\n",
      "ep 3094: ep_len:4014 episode reward: total was -478.720000. running mean: -25.407759\n",
      "ep 3094: ep_len:2839 episode reward: total was -93.840000. running mean: -26.092081\n",
      "ep 3094: ep_len:65 episode reward: total was 31.000000. running mean: -25.521160\n",
      "epsilon:0.009992 episode_count: 46561. steps_count: 50104548.000000\n",
      "ep 3095: ep_len:1529 episode reward: total was -15.330000. running mean: -25.419249\n",
      "ep 3095: ep_len:500 episode reward: total was 14.610000. running mean: -25.018956\n",
      "ep 3095: ep_len:103 episode reward: total was 47.000000. running mean: -24.298767\n",
      "ep 3095: ep_len:540 episode reward: total was -66.970000. running mean: -24.725479\n",
      "ep 3095: ep_len:62 episode reward: total was 29.500000. running mean: -24.183224\n",
      "ep 3095: ep_len:116 episode reward: total was 50.500000. running mean: -23.436392\n",
      "ep 3095: ep_len:1507 episode reward: total was -74.270000. running mean: -23.944728\n",
      "ep 3095: ep_len:325 episode reward: total was 14.020000. running mean: -23.565081\n",
      "ep 3095: ep_len:719 episode reward: total was -11.790000. running mean: -23.447330\n",
      "ep 3095: ep_len:760 episode reward: total was 45.260000. running mean: -22.760257\n",
      "ep 3095: ep_len:500 episode reward: total was 2.750000. running mean: -22.505154\n",
      "ep 3095: ep_len:80 episode reward: total was 38.500000. running mean: -21.895103\n",
      "ep 3095: ep_len:53 episode reward: total was 23.500000. running mean: -21.441152\n",
      "ep 3095: ep_len:603 episode reward: total was -15.350000. running mean: -21.380240\n",
      "ep 3095: ep_len:2957 episode reward: total was -38.540000. running mean: -21.551838\n",
      "epsilon:0.009992 episode_count: 46576. steps_count: 50114902.000000\n",
      "ep 3096: ep_len:752 episode reward: total was -28.490000. running mean: -21.621219\n",
      "ep 3096: ep_len:696 episode reward: total was -27.750000. running mean: -21.682507\n",
      "ep 3096: ep_len:3045 episode reward: total was -57.270000. running mean: -22.038382\n",
      "ep 3096: ep_len:500 episode reward: total was 41.040000. running mean: -21.407598\n",
      "ep 3096: ep_len:39 episode reward: total was 18.000000. running mean: -21.013522\n",
      "ep 3096: ep_len:59 episode reward: total was 25.000000. running mean: -20.553387\n",
      "ep 3096: ep_len:500 episode reward: total was 51.200000. running mean: -19.835853\n",
      "ep 3096: ep_len:3985 episode reward: total was -79.100000. running mean: -20.428495\n",
      "ep 3096: ep_len:1203 episode reward: total was -60.860000. running mean: -20.832810\n",
      "ep 3096: ep_len:7377 episode reward: total was 17.530000. running mean: -20.449181\n",
      "ep 3096: ep_len:1023 episode reward: total was 6.910000. running mean: -20.175590\n",
      "ep 3096: ep_len:174 episode reward: total was 81.000000. running mean: -19.163834\n",
      "ep 3096: ep_len:100 episode reward: total was 44.000000. running mean: -18.532195\n",
      "ep 3096: ep_len:1148 episode reward: total was -21.530000. running mean: -18.562173\n",
      "ep 3096: ep_len:2837 episode reward: total was -8.400000. running mean: -18.460552\n",
      "epsilon:0.009992 episode_count: 46591. steps_count: 50138340.000000\n",
      "ep 3097: ep_len:1464 episode reward: total was -20.150000. running mean: -18.477446\n",
      "ep 3097: ep_len:746 episode reward: total was -13.150000. running mean: -18.424172\n",
      "ep 3097: ep_len:53 episode reward: total was 25.000000. running mean: -17.989930\n",
      "ep 3097: ep_len:102 episode reward: total was 48.000000. running mean: -17.330031\n",
      "ep 3097: ep_len:624 episode reward: total was 21.630000. running mean: -16.940430\n",
      "ep 3097: ep_len:36 episode reward: total was 15.000000. running mean: -16.621026\n",
      "ep 3097: ep_len:113 episode reward: total was 52.000000. running mean: -15.934816\n",
      "ep 3097: ep_len:56 episode reward: total was 26.500000. running mean: -15.510468\n",
      "ep 3097: ep_len:1025 episode reward: total was 14.900000. running mean: -15.206363\n",
      "ep 3097: ep_len:3919 episode reward: total was -29.250000. running mean: -15.346799\n",
      "ep 3097: ep_len:545 episode reward: total was -12.900000. running mean: -15.322331\n",
      "ep 3097: ep_len:732 episode reward: total was 38.820000. running mean: -14.780908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3097: ep_len:735 episode reward: total was -11.620000. running mean: -14.749299\n",
      "ep 3097: ep_len:178 episode reward: total was 87.500000. running mean: -13.726806\n",
      "ep 3097: ep_len:114 episode reward: total was 55.500000. running mean: -13.034538\n",
      "ep 3097: ep_len:500 episode reward: total was 35.580000. running mean: -12.548393\n",
      "ep 3097: ep_len:2870 episode reward: total was -9.820000. running mean: -12.521109\n",
      "epsilon:0.009992 episode_count: 46608. steps_count: 50152152.000000\n",
      "ep 3098: ep_len:1088 episode reward: total was -19.590000. running mean: -12.591798\n",
      "ep 3098: ep_len:1029 episode reward: total was 43.190000. running mean: -12.033980\n",
      "ep 3098: ep_len:2999 episode reward: total was -94.260000. running mean: -12.856240\n",
      "ep 3098: ep_len:658 episode reward: total was -2.680000. running mean: -12.754477\n",
      "ep 3098: ep_len:35 episode reward: total was 16.000000. running mean: -12.466933\n",
      "ep 3098: ep_len:96 episode reward: total was 46.500000. running mean: -11.877263\n",
      "ep 3098: ep_len:67 episode reward: total was 30.500000. running mean: -11.453491\n",
      "ep 3098: ep_len:48 episode reward: total was 22.500000. running mean: -11.113956\n",
      "ep 3098: ep_len:821 episode reward: total was 26.070000. running mean: -10.742116\n",
      "ep 3098: ep_len:3950 episode reward: total was -33.010000. running mean: -10.964795\n",
      "ep 3098: ep_len:796 episode reward: total was -0.720000. running mean: -10.862347\n",
      "ep 3098: ep_len:667 episode reward: total was 18.790000. running mean: -10.565824\n",
      "ep 3098: ep_len:661 episode reward: total was 5.190000. running mean: -10.408265\n",
      "ep 3098: ep_len:122 episode reward: total was 56.500000. running mean: -9.739183\n",
      "ep 3098: ep_len:71 episode reward: total was 34.000000. running mean: -9.301791\n",
      "ep 3098: ep_len:1020 episode reward: total was 7.250000. running mean: -9.136273\n",
      "ep 3098: ep_len:2887 episode reward: total was 2.200000. running mean: -9.022910\n",
      "epsilon:0.009992 episode_count: 46625. steps_count: 50169167.000000\n",
      "ep 3099: ep_len:650 episode reward: total was 18.880000. running mean: -8.743881\n",
      "ep 3099: ep_len:742 episode reward: total was -25.620000. running mean: -8.912642\n",
      "ep 3099: ep_len:62 episode reward: total was 29.500000. running mean: -8.528516\n",
      "ep 3099: ep_len:2905 episode reward: total was -176.540000. running mean: -10.208631\n",
      "ep 3099: ep_len:608 episode reward: total was 12.390000. running mean: -9.982644\n",
      "ep 3099: ep_len:55 episode reward: total was 24.500000. running mean: -9.637818\n",
      "ep 3099: ep_len:160 episode reward: total was 77.000000. running mean: -8.771440\n",
      "ep 3099: ep_len:1406 episode reward: total was -94.170000. running mean: -9.625425\n",
      "ep 3099: ep_len:361 episode reward: total was 28.860000. running mean: -9.240571\n",
      "ep 3099: ep_len:778 episode reward: total was -25.150000. running mean: -9.399665\n",
      "ep 3099: ep_len:824 episode reward: total was 39.720000. running mean: -8.908469\n",
      "ep 3099: ep_len:573 episode reward: total was -14.240000. running mean: -8.961784\n",
      "ep 3099: ep_len:70 episode reward: total was 32.000000. running mean: -8.552166\n",
      "ep 3099: ep_len:1134 episode reward: total was -38.320000. running mean: -8.849845\n",
      "ep 3099: ep_len:2904 episode reward: total was -28.820000. running mean: -9.049546\n",
      "ep 3099: ep_len:50 episode reward: total was 23.500000. running mean: -8.724051\n",
      "epsilon:0.009992 episode_count: 46641. steps_count: 50182449.000000\n",
      "ep 3100: ep_len:1462 episode reward: total was -3.490000. running mean: -8.671710\n",
      "ep 3100: ep_len:927 episode reward: total was 29.250000. running mean: -8.292493\n",
      "ep 3100: ep_len:2944 episode reward: total was -10.980000. running mean: -8.319368\n",
      "ep 3100: ep_len:1124 episode reward: total was -23.270000. running mean: -8.468875\n",
      "ep 3100: ep_len:37 episode reward: total was 15.500000. running mean: -8.229186\n",
      "ep 3100: ep_len:109 episode reward: total was 51.500000. running mean: -7.631894\n",
      "ep 3100: ep_len:869 episode reward: total was 30.410000. running mean: -7.251475\n",
      "ep 3100: ep_len:324 episode reward: total was 27.970000. running mean: -6.899260\n",
      "ep 3100: ep_len:1165 episode reward: total was -18.360000. running mean: -7.013868\n",
      "ep 3100: ep_len:789 episode reward: total was 2.280000. running mean: -6.920929\n",
      "ep 3100: ep_len:1138 episode reward: total was -25.520000. running mean: -7.106920\n",
      "ep 3100: ep_len:58 episode reward: total was 24.500000. running mean: -6.790850\n",
      "ep 3100: ep_len:696 episode reward: total was -6.340000. running mean: -6.786342\n",
      "ep 3100: ep_len:2860 episode reward: total was -107.300000. running mean: -7.791479\n",
      "ep 3100: ep_len:60 episode reward: total was 28.500000. running mean: -7.428564\n",
      "epsilon:0.009992 episode_count: 46656. steps_count: 50197011.000000\n",
      "ep 3101: ep_len:1447 episode reward: total was 14.300000. running mean: -7.211278\n",
      "ep 3101: ep_len:920 episode reward: total was 12.800000. running mean: -7.011165\n",
      "ep 3101: ep_len:84 episode reward: total was 39.000000. running mean: -6.551054\n",
      "ep 3101: ep_len:3041 episode reward: total was -38.180000. running mean: -6.867343\n",
      "ep 3101: ep_len:655 episode reward: total was 26.550000. running mean: -6.533170\n",
      "ep 3101: ep_len:954 episode reward: total was 13.910000. running mean: -6.328738\n",
      "ep 3101: ep_len:3732 episode reward: total was -155.620000. running mean: -7.821651\n",
      "ep 3101: ep_len:1263 episode reward: total was -63.260000. running mean: -8.376034\n",
      "ep 3101: ep_len:844 episode reward: total was 30.980000. running mean: -7.982474\n",
      "ep 3101: ep_len:802 episode reward: total was 14.750000. running mean: -7.755149\n",
      "ep 3101: ep_len:75 episode reward: total was 36.000000. running mean: -7.317598\n",
      "ep 3101: ep_len:797 episode reward: total was -8.360000. running mean: -7.328022\n",
      "ep 3101: ep_len:2902 episode reward: total was -10.660000. running mean: -7.361341\n",
      "epsilon:0.009992 episode_count: 46669. steps_count: 50214527.000000\n",
      "ep 3102: ep_len:1148 episode reward: total was 6.520000. running mean: -7.222528\n",
      "ep 3102: ep_len:1602 episode reward: total was -70.030000. running mean: -7.850603\n",
      "ep 3102: ep_len:2975 episode reward: total was -94.340000. running mean: -8.715497\n",
      "ep 3102: ep_len:703 episode reward: total was -9.300000. running mean: -8.721342\n",
      "ep 3102: ep_len:47 episode reward: total was 22.000000. running mean: -8.414128\n",
      "ep 3102: ep_len:144 episode reward: total was 69.000000. running mean: -7.639987\n",
      "ep 3102: ep_len:40 episode reward: total was 18.500000. running mean: -7.378587\n",
      "ep 3102: ep_len:598 episode reward: total was 45.060000. running mean: -6.854201\n",
      "ep 3102: ep_len:337 episode reward: total was 19.560000. running mean: -6.590059\n",
      "ep 3102: ep_len:1203 episode reward: total was -31.570000. running mean: -6.839859\n",
      "ep 3102: ep_len:818 episode reward: total was 38.000000. running mean: -6.391460\n",
      "ep 3102: ep_len:575 episode reward: total was 39.830000. running mean: -5.929245\n",
      "ep 3102: ep_len:59 episode reward: total was 26.500000. running mean: -5.604953\n",
      "ep 3102: ep_len:144 episode reward: total was 70.500000. running mean: -4.843903\n",
      "ep 3102: ep_len:37 episode reward: total was 15.500000. running mean: -4.640464\n",
      "ep 3102: ep_len:1421 episode reward: total was -5.640000. running mean: -4.650460\n",
      "ep 3102: ep_len:2818 episode reward: total was 0.070000. running mean: -4.603255\n",
      "epsilon:0.009992 episode_count: 46686. steps_count: 50229196.000000\n",
      "ep 3103: ep_len:744 episode reward: total was -53.330000. running mean: -5.090523\n",
      "ep 3103: ep_len:696 episode reward: total was -30.580000. running mean: -5.345417\n",
      "ep 3103: ep_len:3027 episode reward: total was -26.420000. running mean: -5.556163\n",
      "ep 3103: ep_len:1724 episode reward: total was -45.490000. running mean: -5.955502\n",
      "ep 3103: ep_len:59 episode reward: total was 28.000000. running mean: -5.615947\n",
      "ep 3103: ep_len:178 episode reward: total was 86.000000. running mean: -4.699787\n",
      "ep 3103: ep_len:1182 episode reward: total was 12.520000. running mean: -4.527589\n",
      "ep 3103: ep_len:4019 episode reward: total was -541.890000. running mean: -9.901213\n",
      "ep 3103: ep_len:1399 episode reward: total was -696.060000. running mean: -16.762801\n",
      "ep 3103: ep_len:741 episode reward: total was 46.570000. running mean: -16.129473\n",
      "ep 3103: ep_len:595 episode reward: total was 11.990000. running mean: -15.848278\n",
      "ep 3103: ep_len:66 episode reward: total was 30.000000. running mean: -15.389796\n",
      "ep 3103: ep_len:51 episode reward: total was 24.000000. running mean: -14.995898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3103: ep_len:1030 episode reward: total was -10.670000. running mean: -14.952639\n",
      "ep 3103: ep_len:2830 episode reward: total was -21.050000. running mean: -15.013612\n",
      "epsilon:0.009992 episode_count: 46701. steps_count: 50247537.000000\n",
      "ep 3104: ep_len:653 episode reward: total was -43.130000. running mean: -15.294776\n",
      "ep 3104: ep_len:637 episode reward: total was -42.960000. running mean: -15.571428\n",
      "ep 3104: ep_len:86 episode reward: total was 40.000000. running mean: -15.015714\n",
      "ep 3104: ep_len:806 episode reward: total was 50.780000. running mean: -14.357757\n",
      "ep 3104: ep_len:102 episode reward: total was 46.500000. running mean: -13.749179\n",
      "ep 3104: ep_len:41 episode reward: total was 19.000000. running mean: -13.421688\n",
      "ep 3104: ep_len:572 episode reward: total was 29.950000. running mean: -12.987971\n",
      "ep 3104: ep_len:3935 episode reward: total was -42.250000. running mean: -13.280591\n",
      "ep 3104: ep_len:723 episode reward: total was -41.620000. running mean: -13.563985\n",
      "ep 3104: ep_len:818 episode reward: total was 29.060000. running mean: -13.137745\n",
      "ep 3104: ep_len:713 episode reward: total was -16.270000. running mean: -13.169068\n",
      "ep 3104: ep_len:64 episode reward: total was 29.000000. running mean: -12.747377\n",
      "ep 3104: ep_len:58 episode reward: total was 27.500000. running mean: -12.344903\n",
      "ep 3104: ep_len:105 episode reward: total was 49.500000. running mean: -11.726454\n",
      "ep 3104: ep_len:759 episode reward: total was -92.570000. running mean: -12.534890\n",
      "ep 3104: ep_len:2831 episode reward: total was -15.750000. running mean: -12.567041\n",
      "epsilon:0.009992 episode_count: 46717. steps_count: 50260440.000000\n",
      "ep 3105: ep_len:936 episode reward: total was -106.220000. running mean: -13.503571\n",
      "ep 3105: ep_len:201 episode reward: total was 10.820000. running mean: -13.260335\n",
      "ep 3105: ep_len:2936 episode reward: total was -21.370000. running mean: -13.341431\n",
      "ep 3105: ep_len:825 episode reward: total was 32.810000. running mean: -12.879917\n",
      "ep 3105: ep_len:107 episode reward: total was 50.500000. running mean: -12.246118\n",
      "ep 3105: ep_len:31 episode reward: total was 14.000000. running mean: -11.983657\n",
      "ep 3105: ep_len:1374 episode reward: total was -45.800000. running mean: -12.321820\n",
      "ep 3105: ep_len:306 episode reward: total was 19.550000. running mean: -12.003102\n",
      "ep 3105: ep_len:2826 episode reward: total was -505.190000. running mean: -16.934971\n",
      "ep 3105: ep_len:700 episode reward: total was 41.560000. running mean: -16.350021\n",
      "ep 3105: ep_len:1538 episode reward: total was 12.250000. running mean: -16.064021\n",
      "ep 3105: ep_len:73 episode reward: total was 33.500000. running mean: -15.568381\n",
      "ep 3105: ep_len:781 episode reward: total was -16.430000. running mean: -15.576997\n",
      "ep 3105: ep_len:2844 episode reward: total was 0.240000. running mean: -15.418827\n",
      "epsilon:0.009992 episode_count: 46731. steps_count: 50275918.000000\n",
      "ep 3106: ep_len:901 episode reward: total was 15.450000. running mean: -15.110139\n",
      "ep 3106: ep_len:216 episode reward: total was 3.960000. running mean: -14.919437\n",
      "ep 3106: ep_len:3037 episode reward: total was -20.990000. running mean: -14.980143\n",
      "ep 3106: ep_len:747 episode reward: total was -15.930000. running mean: -14.989642\n",
      "ep 3106: ep_len:103 episode reward: total was 48.500000. running mean: -14.354745\n",
      "ep 3106: ep_len:500 episode reward: total was 4.200000. running mean: -14.169198\n",
      "ep 3106: ep_len:3940 episode reward: total was -38.860000. running mean: -14.416106\n",
      "ep 3106: ep_len:1296 episode reward: total was -253.700000. running mean: -16.808945\n",
      "ep 3106: ep_len:782 episode reward: total was -161.880000. running mean: -18.259655\n",
      "ep 3106: ep_len:500 episode reward: total was 4.320000. running mean: -18.033859\n",
      "ep 3106: ep_len:206 episode reward: total was 98.500000. running mean: -16.868520\n",
      "ep 3106: ep_len:53 episode reward: total was 23.500000. running mean: -16.464835\n",
      "ep 3106: ep_len:90 episode reward: total was 42.000000. running mean: -15.880187\n",
      "ep 3106: ep_len:653 episode reward: total was 21.450000. running mean: -15.506885\n",
      "ep 3106: ep_len:2903 episode reward: total was 3.830000. running mean: -15.313516\n",
      "epsilon:0.009992 episode_count: 46746. steps_count: 50291845.000000\n",
      "ep 3107: ep_len:1496 episode reward: total was 0.870000. running mean: -15.151681\n",
      "ep 3107: ep_len:752 episode reward: total was -20.900000. running mean: -15.209164\n",
      "ep 3107: ep_len:3075 episode reward: total was -36.430000. running mean: -15.421372\n",
      "ep 3107: ep_len:3071 episode reward: total was -215.760000. running mean: -17.424759\n",
      "ep 3107: ep_len:63 episode reward: total was 30.000000. running mean: -16.950511\n",
      "ep 3107: ep_len:80 episode reward: total was 37.000000. running mean: -16.411006\n",
      "ep 3107: ep_len:1465 episode reward: total was 2.510000. running mean: -16.221796\n",
      "ep 3107: ep_len:500 episode reward: total was 22.120000. running mean: -15.838378\n",
      "ep 3107: ep_len:605 episode reward: total was -4.680000. running mean: -15.726794\n",
      "ep 3107: ep_len:7275 episode reward: total was 72.740000. running mean: -14.842126\n",
      "ep 3107: ep_len:975 episode reward: total was 42.530000. running mean: -14.268405\n",
      "ep 3107: ep_len:39 episode reward: total was 16.500000. running mean: -13.960721\n",
      "ep 3107: ep_len:1584 episode reward: total was -3.520000. running mean: -13.856314\n",
      "ep 3107: ep_len:2844 episode reward: total was 7.250000. running mean: -13.645250\n",
      "epsilon:0.009992 episode_count: 46760. steps_count: 50315669.000000\n",
      "ep 3108: ep_len:593 episode reward: total was -26.100000. running mean: -13.769798\n",
      "ep 3108: ep_len:666 episode reward: total was -29.870000. running mean: -13.930800\n",
      "ep 3108: ep_len:2976 episode reward: total was -52.800000. running mean: -14.319492\n",
      "ep 3108: ep_len:879 episode reward: total was 77.390000. running mean: -13.402397\n",
      "ep 3108: ep_len:48 episode reward: total was 21.000000. running mean: -13.058373\n",
      "ep 3108: ep_len:935 episode reward: total was 65.580000. running mean: -12.271989\n",
      "ep 3108: ep_len:3984 episode reward: total was -108.420000. running mean: -13.233469\n",
      "ep 3108: ep_len:627 episode reward: total was 14.990000. running mean: -12.951235\n",
      "ep 3108: ep_len:841 episode reward: total was 50.200000. running mean: -12.319722\n",
      "ep 3108: ep_len:1546 episode reward: total was -14.280000. running mean: -12.339325\n",
      "ep 3108: ep_len:1400 episode reward: total was 10.830000. running mean: -12.107632\n",
      "ep 3108: ep_len:2810 episode reward: total was -17.180000. running mean: -12.158356\n",
      "epsilon:0.009992 episode_count: 46772. steps_count: 50332974.000000\n",
      "ep 3109: ep_len:1413 episode reward: total was 0.090000. running mean: -12.035872\n",
      "ep 3109: ep_len:754 episode reward: total was -15.860000. running mean: -12.074113\n",
      "ep 3109: ep_len:2941 episode reward: total was -21.860000. running mean: -12.171972\n",
      "ep 3109: ep_len:837 episode reward: total was 71.330000. running mean: -11.336953\n",
      "ep 3109: ep_len:35 episode reward: total was 16.000000. running mean: -11.063583\n",
      "ep 3109: ep_len:93 episode reward: total was 45.000000. running mean: -10.502947\n",
      "ep 3109: ep_len:98 episode reward: total was 46.000000. running mean: -9.937918\n",
      "ep 3109: ep_len:50 episode reward: total was 23.500000. running mean: -9.603539\n",
      "ep 3109: ep_len:762 episode reward: total was -4.670000. running mean: -9.554203\n",
      "ep 3109: ep_len:3933 episode reward: total was -85.700000. running mean: -10.315661\n",
      "ep 3109: ep_len:1193 episode reward: total was -40.790000. running mean: -10.620404\n",
      "ep 3109: ep_len:784 episode reward: total was 18.330000. running mean: -10.330900\n",
      "ep 3109: ep_len:1161 episode reward: total was -34.440000. running mean: -10.571991\n",
      "ep 3109: ep_len:94 episode reward: total was 42.500000. running mean: -10.041272\n",
      "ep 3109: ep_len:133 episode reward: total was 65.000000. running mean: -9.290859\n",
      "ep 3109: ep_len:1062 episode reward: total was 15.910000. running mean: -9.038850\n",
      "ep 3109: ep_len:2848 episode reward: total was -1.590000. running mean: -8.964362\n",
      "epsilon:0.009992 episode_count: 46789. steps_count: 50351165.000000\n",
      "ep 3110: ep_len:965 episode reward: total was -27.470000. running mean: -9.149418\n",
      "ep 3110: ep_len:757 episode reward: total was -7.750000. running mean: -9.135424\n",
      "ep 3110: ep_len:2969 episode reward: total was -20.450000. running mean: -9.248570\n",
      "ep 3110: ep_len:642 episode reward: total was 1.200000. running mean: -9.144084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3110: ep_len:38 episode reward: total was 17.500000. running mean: -8.877643\n",
      "ep 3110: ep_len:58 episode reward: total was 27.500000. running mean: -8.513867\n",
      "ep 3110: ep_len:937 episode reward: total was 68.020000. running mean: -7.748528\n",
      "ep 3110: ep_len:3909 episode reward: total was -58.610000. running mean: -8.257143\n",
      "ep 3110: ep_len:911 episode reward: total was -65.800000. running mean: -8.832571\n",
      "ep 3110: ep_len:796 episode reward: total was 16.430000. running mean: -8.579946\n",
      "ep 3110: ep_len:971 episode reward: total was 6.790000. running mean: -8.426246\n",
      "ep 3110: ep_len:172 episode reward: total was 81.500000. running mean: -7.526984\n",
      "ep 3110: ep_len:1092 episode reward: total was -15.510000. running mean: -7.606814\n",
      "ep 3110: ep_len:2959 episode reward: total was -7.360000. running mean: -7.604346\n",
      "ep 3110: ep_len:41 episode reward: total was 19.000000. running mean: -7.338302\n",
      "epsilon:0.009992 episode_count: 46804. steps_count: 50368382.000000\n",
      "ep 3111: ep_len:500 episode reward: total was 24.500000. running mean: -7.019919\n",
      "ep 3111: ep_len:643 episode reward: total was -33.990000. running mean: -7.289620\n",
      "ep 3111: ep_len:2931 episode reward: total was -67.700000. running mean: -7.893724\n",
      "ep 3111: ep_len:1502 episode reward: total was 1.630000. running mean: -7.798487\n",
      "ep 3111: ep_len:56 episode reward: total was 26.500000. running mean: -7.455502\n",
      "ep 3111: ep_len:110 episode reward: total was 52.000000. running mean: -6.860947\n",
      "ep 3111: ep_len:1540 episode reward: total was -36.260000. running mean: -7.154937\n",
      "ep 3111: ep_len:3823 episode reward: total was -124.600000. running mean: -8.329388\n",
      "ep 3111: ep_len:704 episode reward: total was -62.540000. running mean: -8.871494\n",
      "ep 3111: ep_len:720 episode reward: total was 36.710000. running mean: -8.415679\n",
      "ep 3111: ep_len:683 episode reward: total was -14.500000. running mean: -8.476522\n",
      "ep 3111: ep_len:201 episode reward: total was 96.000000. running mean: -7.431757\n",
      "ep 3111: ep_len:44 episode reward: total was 19.000000. running mean: -7.167439\n",
      "ep 3111: ep_len:85 episode reward: total was 38.000000. running mean: -6.715765\n",
      "ep 3111: ep_len:602 episode reward: total was 15.240000. running mean: -6.496207\n",
      "ep 3111: ep_len:34 episode reward: total was 14.000000. running mean: -6.291245\n",
      "epsilon:0.009992 episode_count: 46820. steps_count: 50382560.000000\n",
      "ep 3112: ep_len:1043 episode reward: total was -21.050000. running mean: -6.438833\n",
      "ep 3112: ep_len:194 episode reward: total was 8.210000. running mean: -6.292345\n",
      "ep 3112: ep_len:3001 episode reward: total was -25.540000. running mean: -6.484821\n",
      "ep 3112: ep_len:791 episode reward: total was 29.900000. running mean: -6.120973\n",
      "ep 3112: ep_len:1008 episode reward: total was -37.560000. running mean: -6.435363\n",
      "ep 3112: ep_len:3901 episode reward: total was -48.860000. running mean: -6.859610\n",
      "ep 3112: ep_len:892 episode reward: total was -24.640000. running mean: -7.037413\n",
      "ep 3112: ep_len:834 episode reward: total was 54.570000. running mean: -6.421339\n",
      "ep 3112: ep_len:557 episode reward: total was -0.150000. running mean: -6.358626\n",
      "ep 3112: ep_len:101 episode reward: total was 46.000000. running mean: -5.835040\n",
      "ep 3112: ep_len:643 episode reward: total was -0.940000. running mean: -5.786089\n",
      "ep 3112: ep_len:2852 episode reward: total was -23.280000. running mean: -5.961028\n",
      "ep 3112: ep_len:46 episode reward: total was 21.500000. running mean: -5.686418\n",
      "epsilon:0.009992 episode_count: 46833. steps_count: 50398423.000000\n",
      "ep 3113: ep_len:1112 episode reward: total was -20.330000. running mean: -5.832854\n",
      "ep 3113: ep_len:198 episode reward: total was 8.800000. running mean: -5.686525\n",
      "ep 3113: ep_len:3067 episode reward: total was -71.010000. running mean: -6.339760\n",
      "ep 3113: ep_len:732 episode reward: total was -8.000000. running mean: -6.356363\n",
      "ep 3113: ep_len:88 episode reward: total was 42.500000. running mean: -5.867799\n",
      "ep 3113: ep_len:988 episode reward: total was -0.510000. running mean: -5.814221\n",
      "ep 3113: ep_len:3908 episode reward: total was -114.780000. running mean: -6.903879\n",
      "ep 3113: ep_len:674 episode reward: total was 3.950000. running mean: -6.795340\n",
      "ep 3113: ep_len:664 episode reward: total was -15.690000. running mean: -6.884287\n",
      "ep 3113: ep_len:1122 episode reward: total was -18.360000. running mean: -6.999044\n",
      "ep 3113: ep_len:49 episode reward: total was 23.000000. running mean: -6.699053\n",
      "ep 3113: ep_len:1532 episode reward: total was 10.380000. running mean: -6.528263\n",
      "ep 3113: ep_len:45 episode reward: total was 21.000000. running mean: -6.252980\n",
      "epsilon:0.009992 episode_count: 46846. steps_count: 50412602.000000\n",
      "ep 3114: ep_len:1380 episode reward: total was 0.710000. running mean: -6.183350\n",
      "ep 3114: ep_len:500 episode reward: total was -4.650000. running mean: -6.168017\n",
      "ep 3114: ep_len:54 episode reward: total was -33.490000. running mean: -6.441237\n",
      "ep 3114: ep_len:2985 episode reward: total was -23.630000. running mean: -6.613124\n",
      "ep 3114: ep_len:515 episode reward: total was -59.140000. running mean: -7.138393\n",
      "ep 3114: ep_len:93 episode reward: total was 43.500000. running mean: -6.632009\n",
      "ep 3114: ep_len:68 episode reward: total was 32.500000. running mean: -6.240689\n",
      "ep 3114: ep_len:500 episode reward: total was 30.100000. running mean: -5.877282\n",
      "ep 3114: ep_len:652 episode reward: total was 32.250000. running mean: -5.496009\n",
      "ep 3114: ep_len:616 episode reward: total was -16.230000. running mean: -5.603349\n",
      "ep 3114: ep_len:7331 episode reward: total was -34.040000. running mean: -5.887716\n",
      "ep 3114: ep_len:900 episode reward: total was 21.170000. running mean: -5.617139\n",
      "ep 3114: ep_len:60 episode reward: total was 28.500000. running mean: -5.275967\n",
      "ep 3114: ep_len:38 episode reward: total was 17.500000. running mean: -5.048207\n",
      "ep 3114: ep_len:108 episode reward: total was 48.000000. running mean: -4.517725\n",
      "ep 3114: ep_len:781 episode reward: total was -19.720000. running mean: -4.669748\n",
      "ep 3114: ep_len:2796 episode reward: total was -17.290000. running mean: -4.795951\n",
      "ep 3114: ep_len:43 episode reward: total was 18.500000. running mean: -4.562991\n",
      "epsilon:0.009992 episode_count: 46864. steps_count: 50432022.000000\n",
      "ep 3115: ep_len:1141 episode reward: total was -5.930000. running mean: -4.576661\n",
      "ep 3115: ep_len:622 episode reward: total was -19.200000. running mean: -4.722895\n",
      "ep 3115: ep_len:2989 episode reward: total was 5.680000. running mean: -4.618866\n",
      "ep 3115: ep_len:899 episode reward: total was 18.030000. running mean: -4.392377\n",
      "ep 3115: ep_len:63 episode reward: total was 30.000000. running mean: -4.048453\n",
      "ep 3115: ep_len:104 episode reward: total was 50.500000. running mean: -3.502969\n",
      "ep 3115: ep_len:688 episode reward: total was 29.220000. running mean: -3.175739\n",
      "ep 3115: ep_len:3677 episode reward: total was -275.110000. running mean: -5.895082\n",
      "ep 3115: ep_len:1278 episode reward: total was -59.100000. running mean: -6.427131\n",
      "ep 3115: ep_len:764 episode reward: total was 16.480000. running mean: -6.198060\n",
      "ep 3115: ep_len:1491 episode reward: total was 5.370000. running mean: -6.082379\n",
      "ep 3115: ep_len:623 episode reward: total was -5.050000. running mean: -6.072055\n",
      "ep 3115: ep_len:2918 episode reward: total was 7.440000. running mean: -5.936935\n",
      "ep 3115: ep_len:69 episode reward: total was 30.000000. running mean: -5.577565\n",
      "epsilon:0.009992 episode_count: 46878. steps_count: 50449348.000000\n",
      "ep 3116: ep_len:1137 episode reward: total was -1.900000. running mean: -5.540790\n",
      "ep 3116: ep_len:189 episode reward: total was 3.230000. running mean: -5.453082\n",
      "ep 3116: ep_len:3011 episode reward: total was 2.540000. running mean: -5.373151\n",
      "ep 3116: ep_len:1177 episode reward: total was -96.470000. running mean: -6.284119\n",
      "ep 3116: ep_len:54 episode reward: total was 25.500000. running mean: -5.966278\n",
      "ep 3116: ep_len:41 episode reward: total was 19.000000. running mean: -5.716615\n",
      "ep 3116: ep_len:43 episode reward: total was 20.000000. running mean: -5.459449\n",
      "ep 3116: ep_len:878 episode reward: total was 21.130000. running mean: -5.193555\n",
      "ep 3116: ep_len:4079 episode reward: total was -24.200000. running mean: -5.383619\n",
      "ep 3116: ep_len:1581 episode reward: total was -53.880000. running mean: -5.868583\n",
      "ep 3116: ep_len:824 episode reward: total was 65.710000. running mean: -5.152797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3116: ep_len:663 episode reward: total was -11.330000. running mean: -5.214569\n",
      "ep 3116: ep_len:68 episode reward: total was 32.500000. running mean: -4.837423\n",
      "ep 3116: ep_len:40 episode reward: total was 17.000000. running mean: -4.619049\n",
      "ep 3116: ep_len:75 episode reward: total was 34.500000. running mean: -4.227859\n",
      "ep 3116: ep_len:1103 episode reward: total was -11.790000. running mean: -4.303480\n",
      "ep 3116: ep_len:2826 episode reward: total was -37.010000. running mean: -4.630545\n",
      "epsilon:0.009992 episode_count: 46895. steps_count: 50467137.000000\n",
      "ep 3117: ep_len:1468 episode reward: total was 7.260000. running mean: -4.511640\n",
      "ep 3117: ep_len:752 episode reward: total was -32.020000. running mean: -4.786724\n",
      "ep 3117: ep_len:2986 episode reward: total was -16.620000. running mean: -4.905056\n",
      "ep 3117: ep_len:636 episode reward: total was 5.890000. running mean: -4.797106\n",
      "ep 3117: ep_len:68 episode reward: total was 32.500000. running mean: -4.424135\n",
      "ep 3117: ep_len:62 episode reward: total was 29.500000. running mean: -4.084893\n",
      "ep 3117: ep_len:1152 episode reward: total was -4.810000. running mean: -4.092144\n",
      "ep 3117: ep_len:3938 episode reward: total was -39.130000. running mean: -4.442523\n",
      "ep 3117: ep_len:1558 episode reward: total was -17.310000. running mean: -4.571198\n",
      "ep 3117: ep_len:701 episode reward: total was 8.010000. running mean: -4.445386\n",
      "ep 3117: ep_len:560 episode reward: total was -1.670000. running mean: -4.417632\n",
      "ep 3117: ep_len:1135 episode reward: total was 22.110000. running mean: -4.152356\n",
      "ep 3117: ep_len:2868 episode reward: total was 1.490000. running mean: -4.095932\n",
      "ep 3117: ep_len:55 episode reward: total was 23.000000. running mean: -3.824973\n",
      "epsilon:0.009992 episode_count: 46909. steps_count: 50485076.000000\n",
      "ep 3118: ep_len:917 episode reward: total was -337.460000. running mean: -7.161323\n",
      "ep 3118: ep_len:695 episode reward: total was -45.420000. running mean: -7.543910\n",
      "ep 3118: ep_len:3048 episode reward: total was -15.530000. running mean: -7.623771\n",
      "ep 3118: ep_len:693 episode reward: total was 12.730000. running mean: -7.420233\n",
      "ep 3118: ep_len:1450 episode reward: total was -14.030000. running mean: -7.486331\n",
      "ep 3118: ep_len:566 episode reward: total was 31.270000. running mean: -7.098767\n",
      "ep 3118: ep_len:1189 episode reward: total was 13.570000. running mean: -6.892080\n",
      "ep 3118: ep_len:759 episode reward: total was 33.790000. running mean: -6.485259\n",
      "ep 3118: ep_len:594 episode reward: total was 17.120000. running mean: -6.249206\n",
      "ep 3118: ep_len:206 episode reward: total was 100.000000. running mean: -5.186714\n",
      "ep 3118: ep_len:500 episode reward: total was 36.560000. running mean: -4.769247\n",
      "ep 3118: ep_len:2903 episode reward: total was 4.900000. running mean: -4.672555\n",
      "epsilon:0.009992 episode_count: 46921. steps_count: 50498596.000000\n",
      "ep 3119: ep_len:965 episode reward: total was -51.120000. running mean: -5.137029\n",
      "ep 3119: ep_len:500 episode reward: total was -2.810000. running mean: -5.113759\n",
      "ep 3119: ep_len:56 episode reward: total was 26.500000. running mean: -4.797621\n",
      "ep 3119: ep_len:2998 episode reward: total was -4.760000. running mean: -4.797245\n",
      "ep 3119: ep_len:769 episode reward: total was -25.960000. running mean: -5.008872\n",
      "ep 3119: ep_len:1093 episode reward: total was -24.590000. running mean: -5.204684\n",
      "ep 3119: ep_len:3916 episode reward: total was -187.360000. running mean: -7.026237\n",
      "ep 3119: ep_len:1221 episode reward: total was -77.850000. running mean: -7.734475\n",
      "ep 3119: ep_len:711 episode reward: total was 43.750000. running mean: -7.219630\n",
      "ep 3119: ep_len:932 episode reward: total was 51.440000. running mean: -6.633034\n",
      "ep 3119: ep_len:171 episode reward: total was 84.000000. running mean: -5.726703\n",
      "ep 3119: ep_len:36 episode reward: total was 15.000000. running mean: -5.519436\n",
      "ep 3119: ep_len:67 episode reward: total was 32.000000. running mean: -5.144242\n",
      "ep 3119: ep_len:640 episode reward: total was 4.950000. running mean: -5.043299\n",
      "ep 3119: ep_len:2837 episode reward: total was -18.470000. running mean: -5.177566\n",
      "epsilon:0.009992 episode_count: 46936. steps_count: 50515508.000000\n",
      "ep 3120: ep_len:1409 episode reward: total was -448.420000. running mean: -9.609991\n",
      "ep 3120: ep_len:643 episode reward: total was 27.380000. running mean: -9.240091\n",
      "ep 3120: ep_len:3012 episode reward: total was -23.690000. running mean: -9.384590\n",
      "ep 3120: ep_len:516 episode reward: total was -16.710000. running mean: -9.457844\n",
      "ep 3120: ep_len:59 episode reward: total was 28.000000. running mean: -9.083266\n",
      "ep 3120: ep_len:76 episode reward: total was 36.500000. running mean: -8.627433\n",
      "ep 3120: ep_len:940 episode reward: total was 81.460000. running mean: -7.726559\n",
      "ep 3120: ep_len:4192 episode reward: total was -2992.030000. running mean: -37.569593\n",
      "ep 3120: ep_len:573 episode reward: total was -24.740000. running mean: -37.441297\n",
      "ep 3120: ep_len:667 episode reward: total was 2.040000. running mean: -37.046484\n",
      "ep 3120: ep_len:1195 episode reward: total was -4.380000. running mean: -36.719819\n",
      "ep 3120: ep_len:62 episode reward: total was 29.500000. running mean: -36.057621\n",
      "ep 3120: ep_len:124 episode reward: total was 60.500000. running mean: -35.092045\n",
      "ep 3120: ep_len:41 episode reward: total was 17.500000. running mean: -34.566124\n",
      "ep 3120: ep_len:73 episode reward: total was 35.000000. running mean: -33.870463\n",
      "ep 3120: ep_len:500 episode reward: total was 2.790000. running mean: -33.503859\n",
      "ep 3120: ep_len:2844 episode reward: total was -41.370000. running mean: -33.582520\n",
      "epsilon:0.009992 episode_count: 46953. steps_count: 50532434.000000\n",
      "ep 3121: ep_len:1518 episode reward: total was 5.650000. running mean: -33.190195\n",
      "ep 3121: ep_len:682 episode reward: total was -15.980000. running mean: -33.018093\n",
      "ep 3121: ep_len:55 episode reward: total was 24.500000. running mean: -32.442912\n",
      "ep 3121: ep_len:2989 episode reward: total was -55.740000. running mean: -32.675883\n",
      "ep 3121: ep_len:671 episode reward: total was 1.370000. running mean: -32.335424\n",
      "ep 3121: ep_len:88 episode reward: total was 42.500000. running mean: -31.587070\n",
      "ep 3121: ep_len:52 episode reward: total was 24.500000. running mean: -31.026199\n",
      "ep 3121: ep_len:1460 episode reward: total was -9.030000. running mean: -30.806237\n",
      "ep 3121: ep_len:659 episode reward: total was 23.600000. running mean: -30.262175\n",
      "ep 3121: ep_len:749 episode reward: total was -49.890000. running mean: -30.458453\n",
      "ep 3121: ep_len:870 episode reward: total was 44.190000. running mean: -29.711968\n",
      "ep 3121: ep_len:1087 episode reward: total was -25.660000. running mean: -29.671449\n",
      "ep 3121: ep_len:130 episode reward: total was 62.000000. running mean: -28.754734\n",
      "ep 3121: ep_len:602 episode reward: total was -10.310000. running mean: -28.570287\n",
      "ep 3121: ep_len:2942 episode reward: total was -19.010000. running mean: -28.474684\n",
      "epsilon:0.009992 episode_count: 46968. steps_count: 50546988.000000\n",
      "ep 3122: ep_len:1435 episode reward: total was 8.180000. running mean: -28.108137\n",
      "ep 3122: ep_len:500 episode reward: total was 4.840000. running mean: -27.778656\n",
      "ep 3122: ep_len:2904 episode reward: total was -59.950000. running mean: -28.100369\n",
      "ep 3122: ep_len:625 episode reward: total was 5.850000. running mean: -27.760865\n",
      "ep 3122: ep_len:131 episode reward: total was 62.500000. running mean: -26.858257\n",
      "ep 3122: ep_len:54 episode reward: total was 25.500000. running mean: -26.334674\n",
      "ep 3122: ep_len:686 episode reward: total was -7.450000. running mean: -26.145828\n",
      "ep 3122: ep_len:3811 episode reward: total was -421.690000. running mean: -30.101269\n",
      "ep 3122: ep_len:627 episode reward: total was 6.100000. running mean: -29.739257\n",
      "ep 3122: ep_len:690 episode reward: total was 16.050000. running mean: -29.281364\n",
      "ep 3122: ep_len:1096 episode reward: total was -0.500000. running mean: -28.993550\n",
      "ep 3122: ep_len:83 episode reward: total was 40.000000. running mean: -28.303615\n",
      "ep 3122: ep_len:40 episode reward: total was 18.500000. running mean: -27.835579\n",
      "ep 3122: ep_len:750 episode reward: total was -103.770000. running mean: -28.594923\n",
      "ep 3122: ep_len:2841 episode reward: total was -14.150000. running mean: -28.450474\n",
      "epsilon:0.009992 episode_count: 46983. steps_count: 50563261.000000\n",
      "ep 3123: ep_len:938 episode reward: total was -110.540000. running mean: -29.271369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3123: ep_len:500 episode reward: total was 5.490000. running mean: -28.923755\n",
      "ep 3123: ep_len:3050 episode reward: total was -22.120000. running mean: -28.855718\n",
      "ep 3123: ep_len:568 episode reward: total was -39.420000. running mean: -28.961361\n",
      "ep 3123: ep_len:37 episode reward: total was 17.000000. running mean: -28.501747\n",
      "ep 3123: ep_len:1066 episode reward: total was -2.640000. running mean: -28.243129\n",
      "ep 3123: ep_len:4374 episode reward: total was -712.300000. running mean: -35.083698\n",
      "ep 3123: ep_len:909 episode reward: total was -19.360000. running mean: -34.926461\n",
      "ep 3123: ep_len:713 episode reward: total was 35.050000. running mean: -34.226697\n",
      "ep 3123: ep_len:500 episode reward: total was 30.560000. running mean: -33.578830\n",
      "ep 3123: ep_len:77 episode reward: total was 35.500000. running mean: -32.888041\n",
      "ep 3123: ep_len:1192 episode reward: total was -16.990000. running mean: -32.729061\n",
      "ep 3123: ep_len:2926 episode reward: total was -48.280000. running mean: -32.884570\n",
      "epsilon:0.009992 episode_count: 46996. steps_count: 50580111.000000\n",
      "ep 3124: ep_len:768 episode reward: total was -18.750000. running mean: -32.743225\n",
      "ep 3124: ep_len:946 episode reward: total was 18.260000. running mean: -32.233192\n",
      "ep 3124: ep_len:2984 episode reward: total was -13.080000. running mean: -32.041660\n",
      "ep 3124: ep_len:665 episode reward: total was 0.690000. running mean: -31.714344\n",
      "ep 3124: ep_len:152 episode reward: total was 73.000000. running mean: -30.667200\n",
      "ep 3124: ep_len:61 episode reward: total was 27.500000. running mean: -30.085528\n",
      "ep 3124: ep_len:595 episode reward: total was 33.030000. running mean: -29.454373\n",
      "ep 3124: ep_len:640 episode reward: total was 4.120000. running mean: -29.118629\n",
      "ep 3124: ep_len:1290 episode reward: total was -52.330000. running mean: -29.350743\n",
      "ep 3124: ep_len:768 episode reward: total was 1.120000. running mean: -29.046036\n",
      "ep 3124: ep_len:500 episode reward: total was 15.130000. running mean: -28.604275\n",
      "ep 3124: ep_len:60 episode reward: total was 28.500000. running mean: -28.033233\n",
      "ep 3124: ep_len:766 episode reward: total was -83.410000. running mean: -28.587000\n",
      "ep 3124: ep_len:2916 episode reward: total was -43.270000. running mean: -28.733830\n",
      "epsilon:0.009992 episode_count: 47010. steps_count: 50593222.000000\n",
      "ep 3125: ep_len:646 episode reward: total was -14.890000. running mean: -28.595392\n",
      "ep 3125: ep_len:815 episode reward: total was -12.090000. running mean: -28.430338\n",
      "ep 3125: ep_len:2960 episode reward: total was -57.760000. running mean: -28.723635\n",
      "ep 3125: ep_len:701 episode reward: total was 25.700000. running mean: -28.179398\n",
      "ep 3125: ep_len:1474 episode reward: total was -44.300000. running mean: -28.340604\n",
      "ep 3125: ep_len:4037 episode reward: total was -355.930000. running mean: -31.616498\n",
      "ep 3125: ep_len:883 episode reward: total was 9.610000. running mean: -31.204233\n",
      "ep 3125: ep_len:7397 episode reward: total was -714.610000. running mean: -38.038291\n",
      "ep 3125: ep_len:531 episode reward: total was 36.260000. running mean: -37.295308\n",
      "ep 3125: ep_len:41 episode reward: total was 17.500000. running mean: -36.747355\n",
      "ep 3125: ep_len:652 episode reward: total was -7.380000. running mean: -36.453681\n",
      "ep 3125: ep_len:2819 episode reward: total was -1.640000. running mean: -36.105545\n",
      "ep 3125: ep_len:41 episode reward: total was 19.000000. running mean: -35.554489\n",
      "epsilon:0.009992 episode_count: 47023. steps_count: 50616219.000000\n",
      "ep 3126: ep_len:1440 episode reward: total was 1.890000. running mean: -35.180044\n",
      "ep 3126: ep_len:500 episode reward: total was 13.540000. running mean: -34.692844\n",
      "ep 3126: ep_len:3015 episode reward: total was -48.570000. running mean: -34.831615\n",
      "ep 3126: ep_len:782 episode reward: total was 29.380000. running mean: -34.189499\n",
      "ep 3126: ep_len:58 episode reward: total was 27.500000. running mean: -33.572604\n",
      "ep 3126: ep_len:675 episode reward: total was -2.510000. running mean: -33.261978\n",
      "ep 3126: ep_len:338 episode reward: total was 23.090000. running mean: -32.698458\n",
      "ep 3126: ep_len:737 episode reward: total was -6.360000. running mean: -32.435074\n",
      "ep 3126: ep_len:715 episode reward: total was 37.300000. running mean: -31.737723\n",
      "ep 3126: ep_len:609 episode reward: total was -4.180000. running mean: -31.462146\n",
      "ep 3126: ep_len:663 episode reward: total was 2.860000. running mean: -31.118924\n",
      "ep 3126: ep_len:2871 episode reward: total was -21.410000. running mean: -31.021835\n",
      "epsilon:0.009992 episode_count: 47035. steps_count: 50628622.000000\n",
      "ep 3127: ep_len:936 episode reward: total was -48.590000. running mean: -31.197517\n",
      "ep 3127: ep_len:893 episode reward: total was 16.380000. running mean: -30.721742\n",
      "ep 3127: ep_len:2918 episode reward: total was -4.870000. running mean: -30.463224\n",
      "ep 3127: ep_len:681 episode reward: total was 0.450000. running mean: -30.154092\n",
      "ep 3127: ep_len:145 episode reward: total was 71.000000. running mean: -29.142551\n",
      "ep 3127: ep_len:77 episode reward: total was 37.000000. running mean: -28.481126\n",
      "ep 3127: ep_len:58 episode reward: total was 27.500000. running mean: -27.921314\n",
      "ep 3127: ep_len:834 episode reward: total was 38.110000. running mean: -27.261001\n",
      "ep 3127: ep_len:3933 episode reward: total was -230.590000. running mean: -29.294291\n",
      "ep 3127: ep_len:808 episode reward: total was -117.430000. running mean: -30.175648\n",
      "ep 3127: ep_len:855 episode reward: total was 45.450000. running mean: -29.419392\n",
      "ep 3127: ep_len:919 episode reward: total was -26.210000. running mean: -29.387298\n",
      "ep 3127: ep_len:62 episode reward: total was 29.500000. running mean: -28.798425\n",
      "ep 3127: ep_len:157 episode reward: total was 77.000000. running mean: -27.740441\n",
      "ep 3127: ep_len:107 episode reward: total was 50.500000. running mean: -26.958036\n",
      "ep 3127: ep_len:1001 episode reward: total was 7.460000. running mean: -26.613856\n",
      "ep 3127: ep_len:2950 episode reward: total was -0.750000. running mean: -26.355217\n",
      "ep 3127: ep_len:47 episode reward: total was 22.000000. running mean: -25.871665\n",
      "epsilon:0.009992 episode_count: 47053. steps_count: 50646003.000000\n",
      "ep 3128: ep_len:853 episode reward: total was 6.460000. running mean: -25.548348\n",
      "ep 3128: ep_len:955 episode reward: total was 16.790000. running mean: -25.124965\n",
      "ep 3128: ep_len:43 episode reward: total was 20.000000. running mean: -24.673715\n",
      "ep 3128: ep_len:3033 episode reward: total was -25.880000. running mean: -24.685778\n",
      "ep 3128: ep_len:500 episode reward: total was -15.440000. running mean: -24.593320\n",
      "ep 3128: ep_len:59 episode reward: total was 28.000000. running mean: -24.067387\n",
      "ep 3128: ep_len:500 episode reward: total was -2.870000. running mean: -23.855413\n",
      "ep 3128: ep_len:4052 episode reward: total was -144.190000. running mean: -25.058759\n",
      "ep 3128: ep_len:3946 episode reward: total was -514.190000. running mean: -29.950072\n",
      "ep 3128: ep_len:783 episode reward: total was 8.560000. running mean: -29.564971\n",
      "ep 3128: ep_len:742 episode reward: total was 3.210000. running mean: -29.237221\n",
      "ep 3128: ep_len:57 episode reward: total was 27.000000. running mean: -28.674849\n",
      "ep 3128: ep_len:187 episode reward: total was 91.510000. running mean: -27.473000\n",
      "ep 3128: ep_len:40 episode reward: total was 18.500000. running mean: -27.013270\n",
      "ep 3128: ep_len:3448 episode reward: total was -232.580000. running mean: -29.068938\n",
      "ep 3128: ep_len:2839 episode reward: total was -2.230000. running mean: -28.800548\n",
      "ep 3128: ep_len:44 episode reward: total was 17.500000. running mean: -28.337543\n",
      "epsilon:0.009992 episode_count: 47070. steps_count: 50668084.000000\n",
      "ep 3129: ep_len:688 episode reward: total was 1.660000. running mean: -28.037567\n",
      "ep 3129: ep_len:500 episode reward: total was 16.660000. running mean: -27.590592\n",
      "ep 3129: ep_len:3034 episode reward: total was -27.380000. running mean: -27.588486\n",
      "ep 3129: ep_len:619 episode reward: total was 0.970000. running mean: -27.302901\n",
      "ep 3129: ep_len:51 episode reward: total was 22.500000. running mean: -26.804872\n",
      "ep 3129: ep_len:1050 episode reward: total was -36.110000. running mean: -26.897923\n",
      "ep 3129: ep_len:3907 episode reward: total was -90.000000. running mean: -27.528944\n",
      "ep 3129: ep_len:4403 episode reward: total was -608.950000. running mean: -33.343155\n",
      "ep 3129: ep_len:7308 episode reward: total was -20.340000. running mean: -33.213123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3129: ep_len:591 episode reward: total was 35.210000. running mean: -32.528892\n",
      "ep 3129: ep_len:63 episode reward: total was 30.000000. running mean: -31.903603\n",
      "ep 3129: ep_len:40 episode reward: total was 18.500000. running mean: -31.399567\n",
      "ep 3129: ep_len:125 episode reward: total was 59.500000. running mean: -30.490571\n",
      "ep 3129: ep_len:640 episode reward: total was -16.260000. running mean: -30.348265\n",
      "ep 3129: ep_len:2942 episode reward: total was -33.920000. running mean: -30.383983\n",
      "epsilon:0.009992 episode_count: 47085. steps_count: 50694045.000000\n",
      "ep 3130: ep_len:900 episode reward: total was -62.240000. running mean: -30.702543\n",
      "ep 3130: ep_len:953 episode reward: total was 17.440000. running mean: -30.221118\n",
      "ep 3130: ep_len:2948 episode reward: total was -16.530000. running mean: -30.084206\n",
      "ep 3130: ep_len:714 episode reward: total was -22.810000. running mean: -30.011464\n",
      "ep 3130: ep_len:109 episode reward: total was 53.000000. running mean: -29.181350\n",
      "ep 3130: ep_len:80 episode reward: total was 37.000000. running mean: -28.519536\n",
      "ep 3130: ep_len:1413 episode reward: total was -2.070000. running mean: -28.255041\n",
      "ep 3130: ep_len:334 episode reward: total was 5.880000. running mean: -27.913690\n",
      "ep 3130: ep_len:1544 episode reward: total was -18.550000. running mean: -27.820054\n",
      "ep 3130: ep_len:849 episode reward: total was -25.630000. running mean: -27.798153\n",
      "ep 3130: ep_len:610 episode reward: total was -0.480000. running mean: -27.524971\n",
      "ep 3130: ep_len:67 episode reward: total was 30.500000. running mean: -26.944722\n",
      "ep 3130: ep_len:167 episode reward: total was 79.000000. running mean: -25.885275\n",
      "ep 3130: ep_len:1176 episode reward: total was -37.900000. running mean: -26.005422\n",
      "ep 3130: ep_len:2790 episode reward: total was -54.970000. running mean: -26.295068\n",
      "ep 3130: ep_len:44 episode reward: total was 19.000000. running mean: -25.842117\n",
      "epsilon:0.009992 episode_count: 47101. steps_count: 50708743.000000\n",
      "ep 3131: ep_len:1105 episode reward: total was -22.940000. running mean: -25.813096\n",
      "ep 3131: ep_len:500 episode reward: total was 8.420000. running mean: -25.470765\n",
      "ep 3131: ep_len:2874 episode reward: total was -46.470000. running mean: -25.680757\n",
      "ep 3131: ep_len:1152 episode reward: total was -65.410000. running mean: -26.078050\n",
      "ep 3131: ep_len:1388 episode reward: total was -46.890000. running mean: -26.286169\n",
      "ep 3131: ep_len:322 episode reward: total was 4.260000. running mean: -25.980707\n",
      "ep 3131: ep_len:931 episode reward: total was -7.350000. running mean: -25.794400\n",
      "ep 3131: ep_len:680 episode reward: total was 24.460000. running mean: -25.291856\n",
      "ep 3131: ep_len:1504 episode reward: total was -42.340000. running mean: -25.462338\n",
      "ep 3131: ep_len:101 episode reward: total was 46.000000. running mean: -24.747714\n",
      "ep 3131: ep_len:910 episode reward: total was -5.390000. running mean: -24.554137\n",
      "ep 3131: ep_len:2901 episode reward: total was 0.870000. running mean: -24.299896\n",
      "ep 3131: ep_len:59 episode reward: total was 28.000000. running mean: -23.776897\n",
      "epsilon:0.009992 episode_count: 47114. steps_count: 50723170.000000\n",
      "ep 3132: ep_len:1102 episode reward: total was -3.600000. running mean: -23.575128\n",
      "ep 3132: ep_len:818 episode reward: total was 2.050000. running mean: -23.318877\n",
      "ep 3132: ep_len:55 episode reward: total was 26.000000. running mean: -22.825688\n",
      "ep 3132: ep_len:2914 episode reward: total was -12.400000. running mean: -22.721431\n",
      "ep 3132: ep_len:500 episode reward: total was -11.450000. running mean: -22.608717\n",
      "ep 3132: ep_len:156 episode reward: total was 75.000000. running mean: -21.632629\n",
      "ep 3132: ep_len:500 episode reward: total was 18.960000. running mean: -21.226703\n",
      "ep 3132: ep_len:3999 episode reward: total was -855.670000. running mean: -29.571136\n",
      "ep 3132: ep_len:802 episode reward: total was 24.440000. running mean: -29.031025\n",
      "ep 3132: ep_len:7311 episode reward: total was -48.440000. running mean: -29.225115\n",
      "ep 3132: ep_len:743 episode reward: total was -15.970000. running mean: -29.092563\n",
      "ep 3132: ep_len:47 episode reward: total was 20.500000. running mean: -28.596638\n",
      "ep 3132: ep_len:733 episode reward: total was -184.740000. running mean: -30.158071\n",
      "ep 3132: ep_len:2791 episode reward: total was -24.900000. running mean: -30.105491\n",
      "ep 3132: ep_len:39 episode reward: total was 16.500000. running mean: -29.639436\n",
      "epsilon:0.009992 episode_count: 47129. steps_count: 50745680.000000\n",
      "ep 3133: ep_len:648 episode reward: total was -2.780000. running mean: -29.370841\n",
      "ep 3133: ep_len:656 episode reward: total was -8.060000. running mean: -29.157733\n",
      "ep 3133: ep_len:51 episode reward: total was 23.510000. running mean: -28.631056\n",
      "ep 3133: ep_len:90 episode reward: total was 43.500000. running mean: -27.909745\n",
      "ep 3133: ep_len:832 episode reward: total was 39.590000. running mean: -27.234748\n",
      "ep 3133: ep_len:79 episode reward: total was 38.000000. running mean: -26.582400\n",
      "ep 3133: ep_len:1437 episode reward: total was 4.340000. running mean: -26.273176\n",
      "ep 3133: ep_len:3862 episode reward: total was -2139.600000. running mean: -47.406444\n",
      "ep 3133: ep_len:583 episode reward: total was -62.010000. running mean: -47.552480\n",
      "ep 3133: ep_len:856 episode reward: total was 47.390000. running mean: -46.603055\n",
      "ep 3133: ep_len:548 episode reward: total was 27.000000. running mean: -45.867025\n",
      "ep 3133: ep_len:103 episode reward: total was 47.000000. running mean: -44.938354\n",
      "ep 3133: ep_len:63 episode reward: total was 30.000000. running mean: -44.188971\n",
      "ep 3133: ep_len:63 episode reward: total was 27.000000. running mean: -43.477081\n",
      "ep 3133: ep_len:793 episode reward: total was -52.840000. running mean: -43.570710\n",
      "ep 3133: ep_len:2800 episode reward: total was -16.060000. running mean: -43.295603\n",
      "epsilon:0.009992 episode_count: 47145. steps_count: 50759144.000000\n",
      "ep 3134: ep_len:860 episode reward: total was 9.840000. running mean: -42.764247\n",
      "ep 3134: ep_len:1622 episode reward: total was -106.810000. running mean: -43.404705\n",
      "ep 3134: ep_len:3026 episode reward: total was -170.060000. running mean: -44.671258\n",
      "ep 3134: ep_len:509 episode reward: total was 23.430000. running mean: -43.990245\n",
      "ep 3134: ep_len:53 episode reward: total was 25.000000. running mean: -43.300343\n",
      "ep 3134: ep_len:102 episode reward: total was 49.500000. running mean: -42.372339\n",
      "ep 3134: ep_len:87 episode reward: total was 42.000000. running mean: -41.528616\n",
      "ep 3134: ep_len:45 episode reward: total was 21.000000. running mean: -40.903330\n",
      "ep 3134: ep_len:501 episode reward: total was -13.890000. running mean: -40.633196\n",
      "ep 3134: ep_len:651 episode reward: total was 16.020000. running mean: -40.066664\n",
      "ep 3134: ep_len:833 episode reward: total was 5.990000. running mean: -39.606098\n",
      "ep 3134: ep_len:870 episode reward: total was 26.590000. running mean: -38.944137\n",
      "ep 3134: ep_len:500 episode reward: total was 42.900000. running mean: -38.125695\n",
      "ep 3134: ep_len:49 episode reward: total was 23.000000. running mean: -37.514438\n",
      "ep 3134: ep_len:65 episode reward: total was 31.000000. running mean: -36.829294\n",
      "ep 3134: ep_len:1537 episode reward: total was -0.890000. running mean: -36.469901\n",
      "ep 3134: ep_len:47 episode reward: total was 19.000000. running mean: -35.915202\n",
      "ep 3134: ep_len:34 episode reward: total was 15.500000. running mean: -35.401050\n",
      "epsilon:0.009992 episode_count: 47163. steps_count: 50770535.000000\n",
      "ep 3135: ep_len:500 episode reward: total was 13.410000. running mean: -34.912940\n",
      "ep 3135: ep_len:661 episode reward: total was 3.560000. running mean: -34.528210\n",
      "ep 3135: ep_len:2988 episode reward: total was -53.070000. running mean: -34.713628\n",
      "ep 3135: ep_len:711 episode reward: total was 7.110000. running mean: -34.295392\n",
      "ep 3135: ep_len:1409 episode reward: total was -181.190000. running mean: -35.764338\n",
      "ep 3135: ep_len:360 episode reward: total was 12.200000. running mean: -35.284695\n",
      "ep 3135: ep_len:541 episode reward: total was 13.730000. running mean: -34.794548\n",
      "ep 3135: ep_len:792 episode reward: total was 15.420000. running mean: -34.292402\n",
      "ep 3135: ep_len:1141 episode reward: total was -13.000000. running mean: -34.079478\n",
      "ep 3135: ep_len:145 episode reward: total was 68.000000. running mean: -33.058683\n",
      "ep 3135: ep_len:66 episode reward: total was 31.500000. running mean: -32.413096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3135: ep_len:597 episode reward: total was -10.910000. running mean: -32.198065\n",
      "ep 3135: ep_len:2966 episode reward: total was 5.380000. running mean: -31.822285\n",
      "epsilon:0.009992 episode_count: 47176. steps_count: 50783412.000000\n",
      "ep 3136: ep_len:672 episode reward: total was 5.860000. running mean: -31.445462\n",
      "ep 3136: ep_len:1151 episode reward: total was -20.310000. running mean: -31.334107\n",
      "ep 3136: ep_len:68 episode reward: total was 31.000000. running mean: -30.710766\n",
      "ep 3136: ep_len:3015 episode reward: total was -69.760000. running mean: -31.101259\n",
      "ep 3136: ep_len:1233 episode reward: total was -27.010000. running mean: -31.060346\n",
      "ep 3136: ep_len:46 episode reward: total was 21.500000. running mean: -30.534743\n",
      "ep 3136: ep_len:1036 episode reward: total was -25.160000. running mean: -30.480995\n",
      "ep 3136: ep_len:3942 episode reward: total was -388.090000. running mean: -34.057085\n",
      "ep 3136: ep_len:788 episode reward: total was -20.730000. running mean: -33.923814\n",
      "ep 3136: ep_len:709 episode reward: total was 26.540000. running mean: -33.319176\n",
      "ep 3136: ep_len:680 episode reward: total was -31.090000. running mean: -33.296884\n",
      "ep 3136: ep_len:58 episode reward: total was 27.500000. running mean: -32.688916\n",
      "ep 3136: ep_len:1034 episode reward: total was -3.810000. running mean: -32.400126\n",
      "ep 3136: ep_len:2823 episode reward: total was 5.660000. running mean: -32.019525\n",
      "epsilon:0.009992 episode_count: 47190. steps_count: 50800667.000000\n",
      "ep 3137: ep_len:873 episode reward: total was -4.030000. running mean: -31.739630\n",
      "ep 3137: ep_len:599 episode reward: total was -40.640000. running mean: -31.828634\n",
      "ep 3137: ep_len:2889 episode reward: total was -103.640000. running mean: -32.546747\n",
      "ep 3137: ep_len:500 episode reward: total was 18.220000. running mean: -32.039080\n",
      "ep 3137: ep_len:148 episode reward: total was 71.000000. running mean: -31.008689\n",
      "ep 3137: ep_len:72 episode reward: total was 34.500000. running mean: -30.353602\n",
      "ep 3137: ep_len:798 episode reward: total was -40.610000. running mean: -30.456166\n",
      "ep 3137: ep_len:351 episode reward: total was 18.140000. running mean: -29.970204\n",
      "ep 3137: ep_len:897 episode reward: total was -30.040000. running mean: -29.970902\n",
      "ep 3137: ep_len:7216 episode reward: total was -1.410000. running mean: -29.685293\n",
      "ep 3137: ep_len:712 episode reward: total was -8.570000. running mean: -29.474140\n",
      "ep 3137: ep_len:528 episode reward: total was 8.060000. running mean: -29.098799\n",
      "ep 3137: ep_len:2818 episode reward: total was -24.290000. running mean: -29.050711\n",
      "ep 3137: ep_len:50 episode reward: total was 23.500000. running mean: -28.525204\n",
      "epsilon:0.009992 episode_count: 47204. steps_count: 50819118.000000\n",
      "ep 3138: ep_len:716 episode reward: total was -29.210000. running mean: -28.532052\n",
      "ep 3138: ep_len:1207 episode reward: total was -44.140000. running mean: -28.688131\n",
      "ep 3138: ep_len:2878 episode reward: total was -80.380000. running mean: -29.205050\n",
      "ep 3138: ep_len:549 episode reward: total was -56.290000. running mean: -29.475900\n",
      "ep 3138: ep_len:500 episode reward: total was 14.550000. running mean: -29.035641\n",
      "ep 3138: ep_len:4122 episode reward: total was -264.240000. running mean: -31.387684\n",
      "ep 3138: ep_len:549 episode reward: total was -36.090000. running mean: -31.434707\n",
      "ep 3138: ep_len:751 episode reward: total was -2.210000. running mean: -31.142460\n",
      "ep 3138: ep_len:507 episode reward: total was -0.270000. running mean: -30.833736\n",
      "ep 3138: ep_len:135 episode reward: total was 66.000000. running mean: -29.865398\n",
      "ep 3138: ep_len:942 episode reward: total was -57.410000. running mean: -30.140844\n",
      "ep 3138: ep_len:2840 episode reward: total was 6.140000. running mean: -29.778036\n",
      "epsilon:0.009992 episode_count: 47216. steps_count: 50834814.000000\n",
      "ep 3139: ep_len:1031 episode reward: total was -110.050000. running mean: -30.580756\n",
      "ep 3139: ep_len:658 episode reward: total was 13.390000. running mean: -30.141048\n",
      "ep 3139: ep_len:36 episode reward: total was 16.500000. running mean: -29.674637\n",
      "ep 3139: ep_len:2905 episode reward: total was -55.900000. running mean: -29.936891\n",
      "ep 3139: ep_len:694 episode reward: total was 13.010000. running mean: -29.507422\n",
      "ep 3139: ep_len:60 episode reward: total was 28.500000. running mean: -28.927348\n",
      "ep 3139: ep_len:500 episode reward: total was 17.700000. running mean: -28.461074\n",
      "ep 3139: ep_len:3983 episode reward: total was -189.200000. running mean: -30.068464\n",
      "ep 3139: ep_len:817 episode reward: total was -62.810000. running mean: -30.395879\n",
      "ep 3139: ep_len:884 episode reward: total was 31.200000. running mean: -29.779920\n",
      "ep 3139: ep_len:1087 episode reward: total was -4.450000. running mean: -29.526621\n",
      "ep 3139: ep_len:99 episode reward: total was 48.000000. running mean: -28.751355\n",
      "ep 3139: ep_len:123 episode reward: total was 60.000000. running mean: -27.863841\n",
      "ep 3139: ep_len:1069 episode reward: total was -5.580000. running mean: -27.641003\n",
      "ep 3139: ep_len:2880 episode reward: total was -5.710000. running mean: -27.421693\n",
      "ep 3139: ep_len:51 episode reward: total was 23.510000. running mean: -26.912376\n",
      "epsilon:0.009992 episode_count: 47232. steps_count: 50851691.000000\n",
      "ep 3140: ep_len:841 episode reward: total was 28.840000. running mean: -26.354852\n",
      "ep 3140: ep_len:1227 episode reward: total was -50.520000. running mean: -26.596504\n",
      "ep 3140: ep_len:2933 episode reward: total was -98.490000. running mean: -27.315439\n",
      "ep 3140: ep_len:1091 episode reward: total was -11.480000. running mean: -27.157084\n",
      "ep 3140: ep_len:884 episode reward: total was 43.050000. running mean: -26.455013\n",
      "ep 3140: ep_len:634 episode reward: total was 25.000000. running mean: -25.940463\n",
      "ep 3140: ep_len:881 episode reward: total was -23.740000. running mean: -25.918459\n",
      "ep 3140: ep_len:634 episode reward: total was 21.770000. running mean: -25.441574\n",
      "ep 3140: ep_len:709 episode reward: total was 15.930000. running mean: -25.027858\n",
      "ep 3140: ep_len:56 episode reward: total was 26.500000. running mean: -24.512580\n",
      "ep 3140: ep_len:184 episode reward: total was 89.000000. running mean: -23.377454\n",
      "ep 3140: ep_len:86 episode reward: total was 40.000000. running mean: -22.743679\n",
      "ep 3140: ep_len:841 episode reward: total was 10.630000. running mean: -22.409943\n",
      "ep 3140: ep_len:2912 episode reward: total was 10.840000. running mean: -22.077443\n",
      "epsilon:0.009992 episode_count: 47246. steps_count: 50865604.000000\n",
      "ep 3141: ep_len:871 episode reward: total was -10.510000. running mean: -21.961769\n",
      "ep 3141: ep_len:708 episode reward: total was -15.310000. running mean: -21.895251\n",
      "ep 3141: ep_len:66 episode reward: total was 30.000000. running mean: -21.376299\n",
      "ep 3141: ep_len:2971 episode reward: total was -63.780000. running mean: -21.800336\n",
      "ep 3141: ep_len:769 episode reward: total was -23.790000. running mean: -21.820232\n",
      "ep 3141: ep_len:86 episode reward: total was 41.500000. running mean: -21.187030\n",
      "ep 3141: ep_len:500 episode reward: total was 40.730000. running mean: -20.567860\n",
      "ep 3141: ep_len:3931 episode reward: total was -201.350000. running mean: -22.375681\n",
      "ep 3141: ep_len:1008 episode reward: total was -18.480000. running mean: -22.336724\n",
      "ep 3141: ep_len:7364 episode reward: total was 27.620000. running mean: -21.837157\n",
      "ep 3141: ep_len:560 episode reward: total was -9.260000. running mean: -21.711385\n",
      "ep 3141: ep_len:52 episode reward: total was 23.000000. running mean: -21.264272\n",
      "ep 3141: ep_len:1191 episode reward: total was 19.530000. running mean: -20.856329\n",
      "ep 3141: ep_len:2867 episode reward: total was -3.200000. running mean: -20.679766\n",
      "epsilon:0.009992 episode_count: 47260. steps_count: 50888548.000000\n",
      "ep 3142: ep_len:909 episode reward: total was -48.130000. running mean: -20.954268\n",
      "ep 3142: ep_len:1166 episode reward: total was -6.180000. running mean: -20.806525\n",
      "ep 3142: ep_len:49 episode reward: total was 21.500000. running mean: -20.383460\n",
      "ep 3142: ep_len:2877 episode reward: total was -51.420000. running mean: -20.693825\n",
      "ep 3142: ep_len:500 episode reward: total was -6.430000. running mean: -20.551187\n",
      "ep 3142: ep_len:155 episode reward: total was 74.500000. running mean: -19.600675\n",
      "ep 3142: ep_len:623 episode reward: total was 43.350000. running mean: -18.971168\n",
      "ep 3142: ep_len:3982 episode reward: total was -1526.470000. running mean: -34.046157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3142: ep_len:800 episode reward: total was -35.520000. running mean: -34.060895\n",
      "ep 3142: ep_len:821 episode reward: total was 38.980000. running mean: -33.330486\n",
      "ep 3142: ep_len:651 episode reward: total was 8.570000. running mean: -32.911481\n",
      "ep 3142: ep_len:210 episode reward: total was 102.000000. running mean: -31.562367\n",
      "ep 3142: ep_len:53 episode reward: total was 25.000000. running mean: -30.996743\n",
      "ep 3142: ep_len:1486 episode reward: total was -0.760000. running mean: -30.694375\n",
      "ep 3142: ep_len:2917 episode reward: total was -48.320000. running mean: -30.870632\n",
      "ep 3142: ep_len:49 episode reward: total was 23.000000. running mean: -30.331925\n",
      "epsilon:0.009992 episode_count: 47276. steps_count: 50905796.000000\n",
      "ep 3143: ep_len:657 episode reward: total was -12.110000. running mean: -30.149706\n",
      "ep 3143: ep_len:984 episode reward: total was 12.860000. running mean: -29.719609\n",
      "ep 3143: ep_len:2923 episode reward: total was -40.280000. running mean: -29.825213\n",
      "ep 3143: ep_len:846 episode reward: total was -5.460000. running mean: -29.581561\n",
      "ep 3143: ep_len:38 episode reward: total was 17.500000. running mean: -29.110745\n",
      "ep 3143: ep_len:500 episode reward: total was 21.570000. running mean: -28.603938\n",
      "ep 3143: ep_len:3871 episode reward: total was -8.760000. running mean: -28.405498\n",
      "ep 3143: ep_len:796 episode reward: total was -47.420000. running mean: -28.595643\n",
      "ep 3143: ep_len:661 episode reward: total was 10.360000. running mean: -28.206087\n",
      "ep 3143: ep_len:1029 episode reward: total was 27.110000. running mean: -27.652926\n",
      "ep 3143: ep_len:87 episode reward: total was 42.000000. running mean: -26.956397\n",
      "ep 3143: ep_len:119 episode reward: total was 58.000000. running mean: -26.106833\n",
      "ep 3143: ep_len:1505 episode reward: total was 11.610000. running mean: -25.729665\n",
      "ep 3143: ep_len:2856 episode reward: total was 10.860000. running mean: -25.363768\n",
      "epsilon:0.009992 episode_count: 47290. steps_count: 50922668.000000\n",
      "ep 3144: ep_len:1082 episode reward: total was -3.730000. running mean: -25.147430\n",
      "ep 3144: ep_len:1019 episode reward: total was 41.560000. running mean: -24.480356\n",
      "ep 3144: ep_len:57 episode reward: total was 27.000000. running mean: -23.965552\n",
      "ep 3144: ep_len:2942 episode reward: total was -46.710000. running mean: -24.192997\n",
      "ep 3144: ep_len:815 episode reward: total was 30.690000. running mean: -23.644167\n",
      "ep 3144: ep_len:64 episode reward: total was 29.000000. running mean: -23.117725\n",
      "ep 3144: ep_len:121 episode reward: total was 57.500000. running mean: -22.311548\n",
      "ep 3144: ep_len:50 episode reward: total was 22.000000. running mean: -21.868433\n",
      "ep 3144: ep_len:1408 episode reward: total was -140.620000. running mean: -23.055948\n",
      "ep 3144: ep_len:351 episode reward: total was 9.600000. running mean: -22.729389\n",
      "ep 3144: ep_len:1149 episode reward: total was -32.390000. running mean: -22.825995\n",
      "ep 3144: ep_len:765 episode reward: total was -0.290000. running mean: -22.600635\n",
      "ep 3144: ep_len:995 episode reward: total was 2.350000. running mean: -22.351129\n",
      "ep 3144: ep_len:500 episode reward: total was 8.910000. running mean: -22.038517\n",
      "ep 3144: ep_len:2887 episode reward: total was 6.730000. running mean: -21.750832\n",
      "ep 3144: ep_len:56 episode reward: total was 25.000000. running mean: -21.283324\n",
      "epsilon:0.009992 episode_count: 47306. steps_count: 50936929.000000\n",
      "ep 3145: ep_len:1441 episode reward: total was 19.780000. running mean: -20.872691\n",
      "ep 3145: ep_len:3079 episode reward: total was -210.620000. running mean: -22.770164\n",
      "ep 3145: ep_len:3155 episode reward: total was -13.900000. running mean: -22.681462\n",
      "ep 3145: ep_len:500 episode reward: total was 18.100000. running mean: -22.273647\n",
      "ep 3145: ep_len:59 episode reward: total was 28.000000. running mean: -21.770911\n",
      "ep 3145: ep_len:500 episode reward: total was 15.620000. running mean: -21.397002\n",
      "ep 3145: ep_len:4229 episode reward: total was -623.770000. running mean: -27.420732\n",
      "ep 3145: ep_len:727 episode reward: total was -13.660000. running mean: -27.283124\n",
      "ep 3145: ep_len:677 episode reward: total was 29.390000. running mean: -26.716393\n",
      "ep 3145: ep_len:1517 episode reward: total was 1.840000. running mean: -26.430829\n",
      "ep 3145: ep_len:85 episode reward: total was 39.500000. running mean: -25.771521\n",
      "ep 3145: ep_len:69 episode reward: total was 33.000000. running mean: -25.183806\n",
      "ep 3145: ep_len:500 episode reward: total was -0.640000. running mean: -24.938368\n",
      "ep 3145: ep_len:2870 episode reward: total was -15.140000. running mean: -24.840384\n",
      "epsilon:0.009992 episode_count: 47320. steps_count: 50956337.000000\n",
      "ep 3146: ep_len:1378 episode reward: total was 21.630000. running mean: -24.375680\n",
      "ep 3146: ep_len:731 episode reward: total was -27.830000. running mean: -24.410223\n",
      "ep 3146: ep_len:54 episode reward: total was 24.000000. running mean: -23.926121\n",
      "ep 3146: ep_len:2901 episode reward: total was -73.570000. running mean: -24.422560\n",
      "ep 3146: ep_len:1655 episode reward: total was -27.020000. running mean: -24.448534\n",
      "ep 3146: ep_len:623 episode reward: total was -15.500000. running mean: -24.359049\n",
      "ep 3146: ep_len:4074 episode reward: total was -176.070000. running mean: -25.876158\n",
      "ep 3146: ep_len:725 episode reward: total was -18.820000. running mean: -25.805597\n",
      "ep 3146: ep_len:7286 episode reward: total was -5.510000. running mean: -25.602641\n",
      "ep 3146: ep_len:1107 episode reward: total was -5.230000. running mean: -25.398915\n",
      "ep 3146: ep_len:914 episode reward: total was 17.390000. running mean: -24.971025\n",
      "ep 3146: ep_len:2920 episode reward: total was -20.490000. running mean: -24.926215\n",
      "epsilon:0.009992 episode_count: 47332. steps_count: 50980705.000000\n",
      "ep 3147: ep_len:1111 episode reward: total was 4.700000. running mean: -24.629953\n",
      "ep 3147: ep_len:972 episode reward: total was 21.340000. running mean: -24.170253\n",
      "ep 3147: ep_len:83 episode reward: total was 37.000000. running mean: -23.558551\n",
      "ep 3147: ep_len:2989 episode reward: total was -29.650000. running mean: -23.619465\n",
      "ep 3147: ep_len:589 episode reward: total was -6.400000. running mean: -23.447271\n",
      "ep 3147: ep_len:121 episode reward: total was 56.000000. running mean: -22.652798\n",
      "ep 3147: ep_len:64 episode reward: total was 30.500000. running mean: -22.121270\n",
      "ep 3147: ep_len:74 episode reward: total was 35.500000. running mean: -21.545057\n",
      "ep 3147: ep_len:725 episode reward: total was -9.080000. running mean: -21.420407\n",
      "ep 3147: ep_len:647 episode reward: total was -13.160000. running mean: -21.337803\n",
      "ep 3147: ep_len:746 episode reward: total was -36.550000. running mean: -21.489925\n",
      "ep 3147: ep_len:867 episode reward: total was 63.540000. running mean: -20.639625\n",
      "ep 3147: ep_len:643 episode reward: total was -1.210000. running mean: -20.445329\n",
      "ep 3147: ep_len:1146 episode reward: total was -10.930000. running mean: -20.350176\n",
      "ep 3147: ep_len:2867 episode reward: total was -7.120000. running mean: -20.217874\n",
      "epsilon:0.009992 episode_count: 47347. steps_count: 50994349.000000\n",
      "ep 3148: ep_len:825 episode reward: total was -16.160000. running mean: -20.177295\n",
      "ep 3148: ep_len:711 episode reward: total was -2.150000. running mean: -19.997022\n",
      "ep 3148: ep_len:67 episode reward: total was 32.000000. running mean: -19.477052\n",
      "ep 3148: ep_len:2915 episode reward: total was -46.800000. running mean: -19.750282\n",
      "ep 3148: ep_len:4747 episode reward: total was -1789.090000. running mean: -37.443679\n",
      "ep 3148: ep_len:40 episode reward: total was 18.500000. running mean: -36.884242\n",
      "ep 3148: ep_len:632 episode reward: total was -11.180000. running mean: -36.627200\n",
      "ep 3148: ep_len:4106 episode reward: total was -414.520000. running mean: -40.406128\n",
      "ep 3148: ep_len:866 episode reward: total was -64.660000. running mean: -40.648666\n",
      "ep 3148: ep_len:7359 episode reward: total was 51.080000. running mean: -39.731380\n",
      "ep 3148: ep_len:710 episode reward: total was 34.590000. running mean: -38.988166\n",
      "ep 3148: ep_len:171 episode reward: total was 82.010000. running mean: -37.778184\n",
      "ep 3148: ep_len:735 episode reward: total was -11.000000. running mean: -37.510402\n",
      "ep 3148: ep_len:2888 episode reward: total was -74.000000. running mean: -37.875298\n",
      "ep 3148: ep_len:66 episode reward: total was 30.000000. running mean: -37.196545\n",
      "epsilon:0.009992 episode_count: 47362. steps_count: 51021187.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3149: ep_len:1405 episode reward: total was 10.720000. running mean: -36.717380\n",
      "ep 3149: ep_len:662 episode reward: total was -56.170000. running mean: -36.911906\n",
      "ep 3149: ep_len:3032 episode reward: total was -42.340000. running mean: -36.966187\n",
      "ep 3149: ep_len:688 episode reward: total was 23.750000. running mean: -36.359025\n",
      "ep 3149: ep_len:69 episode reward: total was 33.000000. running mean: -35.665435\n",
      "ep 3149: ep_len:55 episode reward: total was 26.000000. running mean: -35.048781\n",
      "ep 3149: ep_len:500 episode reward: total was 1.500000. running mean: -34.683293\n",
      "ep 3149: ep_len:630 episode reward: total was 26.920000. running mean: -34.067260\n",
      "ep 3149: ep_len:2860 episode reward: total was -784.330000. running mean: -41.569887\n",
      "ep 3149: ep_len:850 episode reward: total was 59.630000. running mean: -40.557888\n",
      "ep 3149: ep_len:738 episode reward: total was 5.380000. running mean: -40.098510\n",
      "ep 3149: ep_len:115 episode reward: total was 53.000000. running mean: -39.167524\n",
      "ep 3149: ep_len:1117 episode reward: total was 29.580000. running mean: -38.480049\n",
      "ep 3149: ep_len:2841 episode reward: total was -24.640000. running mean: -38.341649\n",
      "ep 3149: ep_len:59 episode reward: total was 26.500000. running mean: -37.693232\n",
      "epsilon:0.009992 episode_count: 47377. steps_count: 51036808.000000\n",
      "ep 3150: ep_len:1148 episode reward: total was -6.870000. running mean: -37.385000\n",
      "ep 3150: ep_len:1692 episode reward: total was -55.480000. running mean: -37.565950\n",
      "ep 3150: ep_len:83 episode reward: total was 40.000000. running mean: -36.790290\n",
      "ep 3150: ep_len:3006 episode reward: total was 2.210000. running mean: -36.400288\n",
      "ep 3150: ep_len:801 episode reward: total was 31.680000. running mean: -35.719485\n",
      "ep 3150: ep_len:73 episode reward: total was 35.000000. running mean: -35.012290\n",
      "ep 3150: ep_len:825 episode reward: total was 31.280000. running mean: -34.349367\n",
      "ep 3150: ep_len:3788 episode reward: total was -268.610000. running mean: -36.691973\n",
      "ep 3150: ep_len:1149 episode reward: total was -41.630000. running mean: -36.741353\n",
      "ep 3150: ep_len:682 episode reward: total was 28.890000. running mean: -36.085040\n",
      "ep 3150: ep_len:666 episode reward: total was -37.530000. running mean: -36.099490\n",
      "ep 3150: ep_len:1170 episode reward: total was 2.260000. running mean: -35.715895\n",
      "ep 3150: ep_len:2870 episode reward: total was 9.040000. running mean: -35.268336\n",
      "epsilon:0.009992 episode_count: 47390. steps_count: 51054761.000000\n",
      "ep 3151: ep_len:994 episode reward: total was -1.080000. running mean: -34.926452\n",
      "ep 3151: ep_len:1646 episode reward: total was -26.620000. running mean: -34.843388\n",
      "ep 3151: ep_len:43 episode reward: total was 20.000000. running mean: -34.294954\n",
      "ep 3151: ep_len:2949 episode reward: total was -17.840000. running mean: -34.130404\n",
      "ep 3151: ep_len:915 episode reward: total was 74.320000. running mean: -33.045900\n",
      "ep 3151: ep_len:55 episode reward: total was 26.000000. running mean: -32.455441\n",
      "ep 3151: ep_len:161 episode reward: total was 77.500000. running mean: -31.355887\n",
      "ep 3151: ep_len:828 episode reward: total was 54.620000. running mean: -30.496128\n",
      "ep 3151: ep_len:323 episode reward: total was 23.920000. running mean: -29.951967\n",
      "ep 3151: ep_len:1219 episode reward: total was -0.520000. running mean: -29.657647\n",
      "ep 3151: ep_len:729 episode reward: total was -6.740000. running mean: -29.428471\n",
      "ep 3151: ep_len:1030 episode reward: total was 18.460000. running mean: -28.949586\n",
      "ep 3151: ep_len:41 episode reward: total was 19.000000. running mean: -28.470090\n",
      "ep 3151: ep_len:773 episode reward: total was -0.520000. running mean: -28.190589\n",
      "ep 3151: ep_len:2747 episode reward: total was -79.110000. running mean: -28.699783\n",
      "epsilon:0.009992 episode_count: 47405. steps_count: 51069214.000000\n",
      "ep 3152: ep_len:1427 episode reward: total was 15.380000. running mean: -28.258985\n",
      "ep 3152: ep_len:1252 episode reward: total was -41.180000. running mean: -28.388196\n",
      "ep 3152: ep_len:47 episode reward: total was 22.000000. running mean: -27.884314\n",
      "ep 3152: ep_len:3029 episode reward: total was -32.610000. running mean: -27.931571\n",
      "ep 3152: ep_len:587 episode reward: total was 9.960000. running mean: -27.552655\n",
      "ep 3152: ep_len:23 episode reward: total was 8.500000. running mean: -27.192128\n",
      "ep 3152: ep_len:107 episode reward: total was 44.500000. running mean: -26.475207\n",
      "ep 3152: ep_len:36 episode reward: total was 13.500000. running mean: -26.075455\n",
      "ep 3152: ep_len:500 episode reward: total was 36.840000. running mean: -25.446300\n",
      "ep 3152: ep_len:354 episode reward: total was 22.730000. running mean: -24.964537\n",
      "ep 3152: ep_len:1564 episode reward: total was -19.420000. running mean: -24.909092\n",
      "ep 3152: ep_len:697 episode reward: total was 35.020000. running mean: -24.309801\n",
      "ep 3152: ep_len:500 episode reward: total was 18.770000. running mean: -23.879003\n",
      "ep 3152: ep_len:106 episode reward: total was -4.490000. running mean: -23.685113\n",
      "ep 3152: ep_len:1438 episode reward: total was 18.400000. running mean: -23.264262\n",
      "ep 3152: ep_len:2852 episode reward: total was -25.820000. running mean: -23.289819\n",
      "ep 3152: ep_len:58 episode reward: total was 27.500000. running mean: -22.781921\n",
      "epsilon:0.009992 episode_count: 47422. steps_count: 51083791.000000\n",
      "ep 3153: ep_len:641 episode reward: total was 0.190000. running mean: -22.552202\n",
      "ep 3153: ep_len:681 episode reward: total was -24.440000. running mean: -22.571080\n",
      "ep 3153: ep_len:2962 episode reward: total was 14.780000. running mean: -22.197569\n",
      "ep 3153: ep_len:820 episode reward: total was 31.440000. running mean: -21.661193\n",
      "ep 3153: ep_len:64 episode reward: total was 30.500000. running mean: -21.139581\n",
      "ep 3153: ep_len:47 episode reward: total was 22.000000. running mean: -20.708186\n",
      "ep 3153: ep_len:500 episode reward: total was 12.370000. running mean: -20.377404\n",
      "ep 3153: ep_len:659 episode reward: total was 17.780000. running mean: -19.995830\n",
      "ep 3153: ep_len:540 episode reward: total was -53.530000. running mean: -20.331171\n",
      "ep 3153: ep_len:714 episode reward: total was 6.860000. running mean: -20.059260\n",
      "ep 3153: ep_len:652 episode reward: total was -15.870000. running mean: -20.017367\n",
      "ep 3153: ep_len:87 episode reward: total was 42.000000. running mean: -19.397193\n",
      "ep 3153: ep_len:94 episode reward: total was 44.000000. running mean: -18.763222\n",
      "ep 3153: ep_len:830 episode reward: total was 32.150000. running mean: -18.254089\n",
      "ep 3153: ep_len:2800 episode reward: total was -32.700000. running mean: -18.398548\n",
      "epsilon:0.009992 episode_count: 47437. steps_count: 51095882.000000\n",
      "ep 3154: ep_len:500 episode reward: total was 16.990000. running mean: -18.044663\n",
      "ep 3154: ep_len:740 episode reward: total was 11.160000. running mean: -17.752616\n",
      "ep 3154: ep_len:43 episode reward: total was 19.510000. running mean: -17.379990\n",
      "ep 3154: ep_len:3051 episode reward: total was -21.980000. running mean: -17.425990\n",
      "ep 3154: ep_len:662 episode reward: total was -9.710000. running mean: -17.348830\n",
      "ep 3154: ep_len:70 episode reward: total was 32.000000. running mean: -16.855342\n",
      "ep 3154: ep_len:745 episode reward: total was -2.820000. running mean: -16.714989\n",
      "ep 3154: ep_len:4180 episode reward: total was -487.980000. running mean: -21.427639\n",
      "ep 3154: ep_len:547 episode reward: total was 19.940000. running mean: -21.013962\n",
      "ep 3154: ep_len:823 episode reward: total was 37.980000. running mean: -20.424023\n",
      "ep 3154: ep_len:630 episode reward: total was 3.620000. running mean: -20.183582\n",
      "ep 3154: ep_len:88 episode reward: total was 42.500000. running mean: -19.556747\n",
      "ep 3154: ep_len:1172 episode reward: total was -15.720000. running mean: -19.518379\n",
      "ep 3154: ep_len:28 episode reward: total was 12.500000. running mean: -19.198195\n",
      "epsilon:0.009992 episode_count: 47451. steps_count: 51109161.000000\n",
      "ep 3155: ep_len:640 episode reward: total was -9.130000. running mean: -19.097513\n",
      "ep 3155: ep_len:192 episode reward: total was 17.800000. running mean: -18.728538\n",
      "ep 3155: ep_len:53 episode reward: total was 25.000000. running mean: -18.291253\n",
      "ep 3155: ep_len:2965 episode reward: total was 8.990000. running mean: -18.018440\n",
      "ep 3155: ep_len:780 episode reward: total was -17.620000. running mean: -18.014456\n",
      "ep 3155: ep_len:167 episode reward: total was 82.000000. running mean: -17.014311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3155: ep_len:61 episode reward: total was 29.000000. running mean: -16.554168\n",
      "ep 3155: ep_len:74 episode reward: total was 34.000000. running mean: -16.048627\n",
      "ep 3155: ep_len:640 episode reward: total was 5.220000. running mean: -15.835940\n",
      "ep 3155: ep_len:665 episode reward: total was 14.870000. running mean: -15.528881\n",
      "ep 3155: ep_len:955 episode reward: total was -11.830000. running mean: -15.491892\n",
      "ep 3155: ep_len:790 episode reward: total was 7.800000. running mean: -15.258973\n",
      "ep 3155: ep_len:578 episode reward: total was -26.130000. running mean: -15.367683\n",
      "ep 3155: ep_len:205 episode reward: total was 100.510000. running mean: -14.208907\n",
      "ep 3155: ep_len:584 episode reward: total was -34.730000. running mean: -14.414118\n",
      "ep 3155: ep_len:2866 episode reward: total was -16.900000. running mean: -14.438976\n",
      "epsilon:0.009992 episode_count: 47467. steps_count: 51121376.000000\n",
      "ep 3156: ep_len:913 episode reward: total was 2.790000. running mean: -14.266687\n",
      "ep 3156: ep_len:976 episode reward: total was 13.940000. running mean: -13.984620\n",
      "ep 3156: ep_len:47 episode reward: total was 20.500000. running mean: -13.639774\n",
      "ep 3156: ep_len:97 episode reward: total was 45.500000. running mean: -13.048376\n",
      "ep 3156: ep_len:644 episode reward: total was 4.490000. running mean: -12.872992\n",
      "ep 3156: ep_len:52 episode reward: total was 23.000000. running mean: -12.514262\n",
      "ep 3156: ep_len:83 episode reward: total was 35.500000. running mean: -12.034120\n",
      "ep 3156: ep_len:1441 episode reward: total was -231.730000. running mean: -14.231078\n",
      "ep 3156: ep_len:336 episode reward: total was 9.660000. running mean: -13.992168\n",
      "ep 3156: ep_len:1178 episode reward: total was -16.670000. running mean: -14.018946\n",
      "ep 3156: ep_len:724 episode reward: total was 32.030000. running mean: -13.558456\n",
      "ep 3156: ep_len:1428 episode reward: total was 1.310000. running mean: -13.409772\n",
      "ep 3156: ep_len:91 episode reward: total was 41.000000. running mean: -12.865674\n",
      "ep 3156: ep_len:108 episode reward: total was 48.000000. running mean: -12.257017\n",
      "ep 3156: ep_len:631 episode reward: total was 2.370000. running mean: -12.110747\n",
      "ep 3156: ep_len:2937 episode reward: total was 0.070000. running mean: -11.988940\n",
      "epsilon:0.009992 episode_count: 47483. steps_count: 51133062.000000\n",
      "ep 3157: ep_len:1114 episode reward: total was 21.020000. running mean: -11.658850\n",
      "ep 3157: ep_len:198 episode reward: total was 10.300000. running mean: -11.439262\n",
      "ep 3157: ep_len:69 episode reward: total was 28.500000. running mean: -11.039869\n",
      "ep 3157: ep_len:2980 episode reward: total was -19.490000. running mean: -11.124371\n",
      "ep 3157: ep_len:635 episode reward: total was -2.460000. running mean: -11.037727\n",
      "ep 3157: ep_len:32 episode reward: total was 14.500000. running mean: -10.782350\n",
      "ep 3157: ep_len:93 episode reward: total was 45.000000. running mean: -10.224526\n",
      "ep 3157: ep_len:1438 episode reward: total was -141.330000. running mean: -11.535581\n",
      "ep 3157: ep_len:4076 episode reward: total was -74.580000. running mean: -12.166025\n",
      "ep 3157: ep_len:739 episode reward: total was -38.950000. running mean: -12.433865\n",
      "ep 3157: ep_len:882 episode reward: total was 53.590000. running mean: -11.773626\n",
      "ep 3157: ep_len:737 episode reward: total was -20.570000. running mean: -11.861590\n",
      "ep 3157: ep_len:62 episode reward: total was 29.500000. running mean: -11.447974\n",
      "ep 3157: ep_len:89 episode reward: total was 43.000000. running mean: -10.903494\n",
      "ep 3157: ep_len:991 episode reward: total was 23.730000. running mean: -10.557159\n",
      "ep 3157: ep_len:2846 episode reward: total was -44.120000. running mean: -10.892788\n",
      "ep 3157: ep_len:61 episode reward: total was 29.000000. running mean: -10.493860\n",
      "epsilon:0.009992 episode_count: 47500. steps_count: 51150104.000000\n",
      "ep 3158: ep_len:1070 episode reward: total was -5.630000. running mean: -10.445221\n",
      "ep 3158: ep_len:627 episode reward: total was -12.080000. running mean: -10.461569\n",
      "ep 3158: ep_len:2890 episode reward: total was 12.700000. running mean: -10.229953\n",
      "ep 3158: ep_len:794 episode reward: total was -8.390000. running mean: -10.211554\n",
      "ep 3158: ep_len:42 episode reward: total was 19.500000. running mean: -9.914438\n",
      "ep 3158: ep_len:671 episode reward: total was -30.260000. running mean: -10.117894\n",
      "ep 3158: ep_len:588 episode reward: total was 30.020000. running mean: -9.716515\n",
      "ep 3158: ep_len:812 episode reward: total was -81.150000. running mean: -10.430850\n",
      "ep 3158: ep_len:658 episode reward: total was 15.170000. running mean: -10.174841\n",
      "ep 3158: ep_len:663 episode reward: total was -9.700000. running mean: -10.170093\n",
      "ep 3158: ep_len:92 episode reward: total was 43.000000. running mean: -9.638392\n",
      "ep 3158: ep_len:500 episode reward: total was -3.670000. running mean: -9.578708\n",
      "ep 3158: ep_len:2851 episode reward: total was 1.500000. running mean: -9.467921\n",
      "ep 3158: ep_len:29 episode reward: total was 13.000000. running mean: -9.243242\n",
      "epsilon:0.009992 episode_count: 47514. steps_count: 51162391.000000\n",
      "ep 3159: ep_len:1078 episode reward: total was -10.020000. running mean: -9.251009\n",
      "ep 3159: ep_len:729 episode reward: total was -13.080000. running mean: -9.289299\n",
      "ep 3159: ep_len:72 episode reward: total was 34.500000. running mean: -8.851406\n",
      "ep 3159: ep_len:3006 episode reward: total was -1.400000. running mean: -8.776892\n",
      "ep 3159: ep_len:775 episode reward: total was 36.750000. running mean: -8.321623\n",
      "ep 3159: ep_len:99 episode reward: total was 46.500000. running mean: -7.773407\n",
      "ep 3159: ep_len:861 episode reward: total was 38.930000. running mean: -7.306373\n",
      "ep 3159: ep_len:3964 episode reward: total was -19.160000. running mean: -7.424909\n",
      "ep 3159: ep_len:589 episode reward: total was -36.610000. running mean: -7.716760\n",
      "ep 3159: ep_len:810 episode reward: total was 23.870000. running mean: -7.400893\n",
      "ep 3159: ep_len:1005 episode reward: total was 31.280000. running mean: -7.014084\n",
      "ep 3159: ep_len:62 episode reward: total was 28.000000. running mean: -6.663943\n",
      "ep 3159: ep_len:47 episode reward: total was 22.000000. running mean: -6.377303\n",
      "ep 3159: ep_len:905 episode reward: total was 9.740000. running mean: -6.216130\n",
      "ep 3159: ep_len:2795 episode reward: total was -15.100000. running mean: -6.304969\n",
      "epsilon:0.009992 episode_count: 47529. steps_count: 51179188.000000\n",
      "ep 3160: ep_len:1449 episode reward: total was 0.970000. running mean: -6.232219\n",
      "ep 3160: ep_len:675 episode reward: total was -21.700000. running mean: -6.386897\n",
      "ep 3160: ep_len:2999 episode reward: total was -26.350000. running mean: -6.586528\n",
      "ep 3160: ep_len:804 episode reward: total was -9.360000. running mean: -6.614263\n",
      "ep 3160: ep_len:135 episode reward: total was 64.500000. running mean: -5.903120\n",
      "ep 3160: ep_len:67 episode reward: total was 29.000000. running mean: -5.554089\n",
      "ep 3160: ep_len:878 episode reward: total was 37.810000. running mean: -5.120448\n",
      "ep 3160: ep_len:3892 episode reward: total was -166.020000. running mean: -6.729444\n",
      "ep 3160: ep_len:1176 episode reward: total was -19.900000. running mean: -6.861149\n",
      "ep 3160: ep_len:653 episode reward: total was -10.470000. running mean: -6.897238\n",
      "ep 3160: ep_len:620 episode reward: total was -2.150000. running mean: -6.849765\n",
      "ep 3160: ep_len:152 episode reward: total was 71.010000. running mean: -6.071168\n",
      "ep 3160: ep_len:48 episode reward: total was 22.500000. running mean: -5.785456\n",
      "ep 3160: ep_len:799 episode reward: total was 4.460000. running mean: -5.683001\n",
      "ep 3160: ep_len:2857 episode reward: total was -722.430000. running mean: -12.850471\n",
      "epsilon:0.009992 episode_count: 47544. steps_count: 51196392.000000\n",
      "ep 3161: ep_len:500 episode reward: total was 7.990000. running mean: -12.642067\n",
      "ep 3161: ep_len:1680 episode reward: total was -61.050000. running mean: -13.126146\n",
      "ep 3161: ep_len:80 episode reward: total was 34.000000. running mean: -12.654885\n",
      "ep 3161: ep_len:2941 episode reward: total was -36.520000. running mean: -12.893536\n",
      "ep 3161: ep_len:648 episode reward: total was -27.830000. running mean: -13.042900\n",
      "ep 3161: ep_len:84 episode reward: total was 37.500000. running mean: -12.537471\n",
      "ep 3161: ep_len:775 episode reward: total was -4.540000. running mean: -12.457497\n",
      "ep 3161: ep_len:344 episode reward: total was 17.090000. running mean: -12.162022\n",
      "ep 3161: ep_len:843 episode reward: total was 5.660000. running mean: -11.983801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3161: ep_len:871 episode reward: total was 44.690000. running mean: -11.417063\n",
      "ep 3161: ep_len:612 episode reward: total was -5.200000. running mean: -11.354893\n",
      "ep 3161: ep_len:63 episode reward: total was 27.000000. running mean: -10.971344\n",
      "ep 3161: ep_len:632 episode reward: total was -3.680000. running mean: -10.898430\n",
      "ep 3161: ep_len:2763 episode reward: total was -31.490000. running mean: -11.104346\n",
      "ep 3161: ep_len:66 episode reward: total was 31.500000. running mean: -10.678303\n",
      "epsilon:0.009992 episode_count: 47559. steps_count: 51209294.000000\n",
      "ep 3162: ep_len:1002 episode reward: total was -73.220000. running mean: -11.303720\n",
      "ep 3162: ep_len:936 episode reward: total was 9.650000. running mean: -11.094182\n",
      "ep 3162: ep_len:55 episode reward: total was 21.500000. running mean: -10.768241\n",
      "ep 3162: ep_len:2994 episode reward: total was -18.680000. running mean: -10.847358\n",
      "ep 3162: ep_len:1647 episode reward: total was -42.010000. running mean: -11.158985\n",
      "ep 3162: ep_len:52 episode reward: total was 23.000000. running mean: -10.817395\n",
      "ep 3162: ep_len:90 episode reward: total was 42.000000. running mean: -10.289221\n",
      "ep 3162: ep_len:735 episode reward: total was 34.440000. running mean: -9.841929\n",
      "ep 3162: ep_len:3647 episode reward: total was -2896.590000. running mean: -38.709409\n",
      "ep 3162: ep_len:896 episode reward: total was -36.600000. running mean: -38.688315\n",
      "ep 3162: ep_len:800 episode reward: total was 16.230000. running mean: -38.139132\n",
      "ep 3162: ep_len:1104 episode reward: total was -15.390000. running mean: -37.911641\n",
      "ep 3162: ep_len:85 episode reward: total was 39.500000. running mean: -37.137524\n",
      "ep 3162: ep_len:162 episode reward: total was 78.000000. running mean: -35.986149\n",
      "ep 3162: ep_len:51 episode reward: total was 22.500000. running mean: -35.401288\n",
      "ep 3162: ep_len:111 episode reward: total was 51.000000. running mean: -34.537275\n",
      "ep 3162: ep_len:1030 episode reward: total was 2.180000. running mean: -34.170102\n",
      "ep 3162: ep_len:2908 episode reward: total was -46.870000. running mean: -34.297101\n",
      "ep 3162: ep_len:54 episode reward: total was 25.500000. running mean: -33.699130\n",
      "epsilon:0.009992 episode_count: 47578. steps_count: 51227653.000000\n",
      "ep 3163: ep_len:1133 episode reward: total was -17.120000. running mean: -33.533339\n",
      "ep 3163: ep_len:1015 episode reward: total was 40.690000. running mean: -32.791105\n",
      "ep 3163: ep_len:65 episode reward: total was 28.000000. running mean: -32.183194\n",
      "ep 3163: ep_len:3015 episode reward: total was -15.480000. running mean: -32.016162\n",
      "ep 3163: ep_len:779 episode reward: total was 11.470000. running mean: -31.581301\n",
      "ep 3163: ep_len:61 episode reward: total was 27.500000. running mean: -30.990488\n",
      "ep 3163: ep_len:157 episode reward: total was 75.500000. running mean: -29.925583\n",
      "ep 3163: ep_len:77 episode reward: total was 35.500000. running mean: -29.271327\n",
      "ep 3163: ep_len:1159 episode reward: total was -4.370000. running mean: -29.022314\n",
      "ep 3163: ep_len:350 episode reward: total was 25.200000. running mean: -28.480091\n",
      "ep 3163: ep_len:1251 episode reward: total was -74.520000. running mean: -28.940490\n",
      "ep 3163: ep_len:907 episode reward: total was 55.280000. running mean: -28.098285\n",
      "ep 3163: ep_len:1084 episode reward: total was 12.790000. running mean: -27.689402\n",
      "ep 3163: ep_len:51 episode reward: total was 24.000000. running mean: -27.172508\n",
      "ep 3163: ep_len:35 episode reward: total was 16.000000. running mean: -26.740783\n",
      "ep 3163: ep_len:1151 episode reward: total was -7.850000. running mean: -26.551875\n",
      "ep 3163: ep_len:2781 episode reward: total was -31.400000. running mean: -26.600356\n",
      "epsilon:0.009992 episode_count: 47595. steps_count: 51242724.000000\n",
      "ep 3164: ep_len:1170 episode reward: total was 1.430000. running mean: -26.320053\n",
      "ep 3164: ep_len:500 episode reward: total was 23.150000. running mean: -25.825352\n",
      "ep 3164: ep_len:3009 episode reward: total was -32.690000. running mean: -25.893999\n",
      "ep 3164: ep_len:1647 episode reward: total was -12.530000. running mean: -25.760359\n",
      "ep 3164: ep_len:63 episode reward: total was 28.500000. running mean: -25.217755\n",
      "ep 3164: ep_len:63 episode reward: total was 30.000000. running mean: -24.665578\n",
      "ep 3164: ep_len:500 episode reward: total was 42.900000. running mean: -23.989922\n",
      "ep 3164: ep_len:333 episode reward: total was 3.600000. running mean: -23.714023\n",
      "ep 3164: ep_len:754 episode reward: total was 21.660000. running mean: -23.260282\n",
      "ep 3164: ep_len:718 episode reward: total was 40.680000. running mean: -22.620879\n",
      "ep 3164: ep_len:1517 episode reward: total was 17.760000. running mean: -22.217071\n",
      "ep 3164: ep_len:649 episode reward: total was -4.790000. running mean: -22.042800\n",
      "ep 3164: ep_len:2909 episode reward: total was -66.450000. running mean: -22.486872\n",
      "ep 3164: ep_len:55 episode reward: total was 26.000000. running mean: -22.002003\n",
      "epsilon:0.009992 episode_count: 47609. steps_count: 51256611.000000\n",
      "ep 3165: ep_len:997 episode reward: total was -42.140000. running mean: -22.203383\n",
      "ep 3165: ep_len:1169 episode reward: total was -115.150000. running mean: -23.132849\n",
      "ep 3165: ep_len:102 episode reward: total was 49.500000. running mean: -22.406521\n",
      "ep 3165: ep_len:1228 episode reward: total was -50.510000. running mean: -22.687556\n",
      "ep 3165: ep_len:46 episode reward: total was 20.000000. running mean: -22.260680\n",
      "ep 3165: ep_len:98 episode reward: total was 46.000000. running mean: -21.578073\n",
      "ep 3165: ep_len:85 episode reward: total was 38.000000. running mean: -20.982293\n",
      "ep 3165: ep_len:736 episode reward: total was -43.830000. running mean: -21.210770\n",
      "ep 3165: ep_len:3859 episode reward: total was -214.710000. running mean: -23.145762\n",
      "ep 3165: ep_len:578 episode reward: total was 7.330000. running mean: -22.841004\n",
      "ep 3165: ep_len:886 episode reward: total was 57.270000. running mean: -22.039894\n",
      "ep 3165: ep_len:500 episode reward: total was 13.350000. running mean: -21.685995\n",
      "ep 3165: ep_len:100 episode reward: total was 47.000000. running mean: -20.999135\n",
      "ep 3165: ep_len:38 episode reward: total was 17.500000. running mean: -20.614144\n",
      "ep 3165: ep_len:118 episode reward: total was 56.000000. running mean: -19.848003\n",
      "ep 3165: ep_len:817 episode reward: total was 32.080000. running mean: -19.328723\n",
      "ep 3165: ep_len:47 episode reward: total was 22.000000. running mean: -18.915435\n",
      "epsilon:0.009992 episode_count: 47626. steps_count: 51268015.000000\n",
      "ep 3166: ep_len:958 episode reward: total was -71.920000. running mean: -19.445481\n",
      "ep 3166: ep_len:500 episode reward: total was 5.540000. running mean: -19.195626\n",
      "ep 3166: ep_len:75 episode reward: total was 36.000000. running mean: -18.643670\n",
      "ep 3166: ep_len:2949 episode reward: total was -15.850000. running mean: -18.615733\n",
      "ep 3166: ep_len:1575 episode reward: total was -31.650000. running mean: -18.746076\n",
      "ep 3166: ep_len:56 episode reward: total was 26.500000. running mean: -18.293615\n",
      "ep 3166: ep_len:128 episode reward: total was 61.000000. running mean: -17.500679\n",
      "ep 3166: ep_len:104 episode reward: total was 49.000000. running mean: -16.835672\n",
      "ep 3166: ep_len:43 episode reward: total was 20.000000. running mean: -16.467315\n",
      "ep 3166: ep_len:500 episode reward: total was -25.060000. running mean: -16.553242\n",
      "ep 3166: ep_len:4366 episode reward: total was -1500.970000. running mean: -31.397410\n",
      "ep 3166: ep_len:1291 episode reward: total was -31.510000. running mean: -31.398536\n",
      "ep 3166: ep_len:678 episode reward: total was 27.810000. running mean: -30.806450\n",
      "ep 3166: ep_len:1455 episode reward: total was 4.210000. running mean: -30.456286\n",
      "ep 3166: ep_len:861 episode reward: total was -31.230000. running mean: -30.464023\n",
      "ep 3166: ep_len:2864 episode reward: total was -37.670000. running mean: -30.536083\n",
      "ep 3166: ep_len:43 episode reward: total was 20.000000. running mean: -30.030722\n",
      "epsilon:0.009992 episode_count: 47643. steps_count: 51286461.000000\n",
      "ep 3167: ep_len:830 episode reward: total was -16.950000. running mean: -29.899915\n",
      "ep 3167: ep_len:3858 episode reward: total was -387.270000. running mean: -33.473616\n",
      "ep 3167: ep_len:59 episode reward: total was 28.000000. running mean: -32.858880\n",
      "ep 3167: ep_len:3062 episode reward: total was 0.990000. running mean: -32.520391\n",
      "ep 3167: ep_len:1446 episode reward: total was -4.540000. running mean: -32.240587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3167: ep_len:53 episode reward: total was 25.000000. running mean: -31.668181\n",
      "ep 3167: ep_len:92 episode reward: total was 44.500000. running mean: -30.906499\n",
      "ep 3167: ep_len:62 episode reward: total was 29.500000. running mean: -30.302434\n",
      "ep 3167: ep_len:500 episode reward: total was 37.910000. running mean: -29.620310\n",
      "ep 3167: ep_len:3618 episode reward: total was -635.490000. running mean: -35.679007\n",
      "ep 3167: ep_len:659 episode reward: total was -4.690000. running mean: -35.369117\n",
      "ep 3167: ep_len:737 episode reward: total was -1.060000. running mean: -35.026025\n",
      "ep 3167: ep_len:500 episode reward: total was 7.260000. running mean: -34.603165\n",
      "ep 3167: ep_len:113 episode reward: total was 55.000000. running mean: -33.707134\n",
      "ep 3167: ep_len:616 episode reward: total was -25.840000. running mean: -33.628462\n",
      "ep 3167: ep_len:2757 episode reward: total was -23.160000. running mean: -33.523778\n",
      "ep 3167: ep_len:66 episode reward: total was 30.000000. running mean: -32.888540\n",
      "epsilon:0.009992 episode_count: 47660. steps_count: 51305489.000000\n",
      "ep 3168: ep_len:1068 episode reward: total was 2.430000. running mean: -32.535354\n",
      "ep 3168: ep_len:1590 episode reward: total was -48.200000. running mean: -32.692001\n",
      "ep 3168: ep_len:3086 episode reward: total was -48.620000. running mean: -32.851281\n",
      "ep 3168: ep_len:603 episode reward: total was -8.550000. running mean: -32.608268\n",
      "ep 3168: ep_len:142 episode reward: total was 69.500000. running mean: -31.587185\n",
      "ep 3168: ep_len:729 episode reward: total was -10.050000. running mean: -31.371814\n",
      "ep 3168: ep_len:500 episode reward: total was 34.730000. running mean: -30.710795\n",
      "ep 3168: ep_len:938 episode reward: total was -8.680000. running mean: -30.490487\n",
      "ep 3168: ep_len:861 episode reward: total was 40.790000. running mean: -29.777683\n",
      "ep 3168: ep_len:1130 episode reward: total was 20.020000. running mean: -29.279706\n",
      "ep 3168: ep_len:175 episode reward: total was 81.500000. running mean: -28.171909\n",
      "ep 3168: ep_len:43 episode reward: total was 18.500000. running mean: -27.705190\n",
      "ep 3168: ep_len:81 episode reward: total was 36.000000. running mean: -27.068138\n",
      "ep 3168: ep_len:742 episode reward: total was -59.410000. running mean: -27.391556\n",
      "ep 3168: ep_len:2801 episode reward: total was -36.190000. running mean: -27.479541\n",
      "ep 3168: ep_len:40 episode reward: total was 18.500000. running mean: -27.019745\n",
      "epsilon:0.009992 episode_count: 47676. steps_count: 51320018.000000\n",
      "ep 3169: ep_len:1023 episode reward: total was -88.920000. running mean: -27.638748\n",
      "ep 3169: ep_len:3796 episode reward: total was -633.200000. running mean: -33.694360\n",
      "ep 3169: ep_len:2899 episode reward: total was -54.520000. running mean: -33.902617\n",
      "ep 3169: ep_len:1190 episode reward: total was -22.640000. running mean: -33.789991\n",
      "ep 3169: ep_len:1509 episode reward: total was 48.630000. running mean: -32.965791\n",
      "ep 3169: ep_len:3924 episode reward: total was -135.710000. running mean: -33.993233\n",
      "ep 3169: ep_len:1319 episode reward: total was -38.490000. running mean: -34.038201\n",
      "ep 3169: ep_len:771 episode reward: total was 24.540000. running mean: -33.452418\n",
      "ep 3169: ep_len:500 episode reward: total was 16.660000. running mean: -32.951294\n",
      "ep 3169: ep_len:211 episode reward: total was 99.500000. running mean: -31.626781\n",
      "ep 3169: ep_len:27 episode reward: total was 12.000000. running mean: -31.190514\n",
      "ep 3169: ep_len:762 episode reward: total was -22.650000. running mean: -31.105108\n",
      "ep 3169: ep_len:2786 episode reward: total was -19.410000. running mean: -30.988157\n",
      "ep 3169: ep_len:47 episode reward: total was 22.000000. running mean: -30.458276\n",
      "epsilon:0.009992 episode_count: 47690. steps_count: 51340782.000000\n",
      "ep 3170: ep_len:1343 episode reward: total was 11.140000. running mean: -30.042293\n",
      "ep 3170: ep_len:759 episode reward: total was -25.910000. running mean: -30.000970\n",
      "ep 3170: ep_len:48 episode reward: total was 22.500000. running mean: -29.475960\n",
      "ep 3170: ep_len:3000 episode reward: total was -16.160000. running mean: -29.342801\n",
      "ep 3170: ep_len:526 episode reward: total was -4.490000. running mean: -29.094273\n",
      "ep 3170: ep_len:648 episode reward: total was -7.220000. running mean: -28.875530\n",
      "ep 3170: ep_len:657 episode reward: total was -21.350000. running mean: -28.800275\n",
      "ep 3170: ep_len:783 episode reward: total was -14.230000. running mean: -28.654572\n",
      "ep 3170: ep_len:811 episode reward: total was 37.770000. running mean: -27.990326\n",
      "ep 3170: ep_len:1433 episode reward: total was -880.140000. running mean: -36.511823\n",
      "ep 3170: ep_len:87 episode reward: total was 40.500000. running mean: -35.741705\n",
      "ep 3170: ep_len:67 episode reward: total was 32.000000. running mean: -35.064288\n",
      "ep 3170: ep_len:1530 episode reward: total was 7.020000. running mean: -34.643445\n",
      "ep 3170: ep_len:2885 episode reward: total was -32.440000. running mean: -34.621410\n",
      "ep 3170: ep_len:72 episode reward: total was 33.000000. running mean: -33.945196\n",
      "epsilon:0.009992 episode_count: 47705. steps_count: 51355431.000000\n",
      "ep 3171: ep_len:1452 episode reward: total was 27.360000. running mean: -33.332144\n",
      "ep 3171: ep_len:837 episode reward: total was 4.480000. running mean: -32.954023\n",
      "ep 3171: ep_len:3014 episode reward: total was 3.020000. running mean: -32.594283\n",
      "ep 3171: ep_len:1369 episode reward: total was -148.940000. running mean: -33.757740\n",
      "ep 3171: ep_len:82 episode reward: total was 36.500000. running mean: -33.055162\n",
      "ep 3171: ep_len:873 episode reward: total was 13.640000. running mean: -32.588211\n",
      "ep 3171: ep_len:3845 episode reward: total was -114.340000. running mean: -33.405729\n",
      "ep 3171: ep_len:534 episode reward: total was 12.370000. running mean: -32.947971\n",
      "ep 3171: ep_len:899 episode reward: total was 65.450000. running mean: -31.963992\n",
      "ep 3171: ep_len:666 episode reward: total was -1.070000. running mean: -31.655052\n",
      "ep 3171: ep_len:120 episode reward: total was 58.500000. running mean: -30.753501\n",
      "ep 3171: ep_len:631 episode reward: total was 1.880000. running mean: -30.427166\n",
      "ep 3171: ep_len:2876 episode reward: total was 4.910000. running mean: -30.073795\n",
      "epsilon:0.009992 episode_count: 47718. steps_count: 51372629.000000\n",
      "ep 3172: ep_len:1043 episode reward: total was -2.870000. running mean: -29.801757\n",
      "ep 3172: ep_len:846 episode reward: total was 18.870000. running mean: -29.315039\n",
      "ep 3172: ep_len:2930 episode reward: total was -27.480000. running mean: -29.296689\n",
      "ep 3172: ep_len:759 episode reward: total was 22.140000. running mean: -28.782322\n",
      "ep 3172: ep_len:46 episode reward: total was 21.500000. running mean: -28.279499\n",
      "ep 3172: ep_len:858 episode reward: total was 32.870000. running mean: -27.668004\n",
      "ep 3172: ep_len:3925 episode reward: total was -36.870000. running mean: -27.760024\n",
      "ep 3172: ep_len:660 episode reward: total was -25.370000. running mean: -27.736123\n",
      "ep 3172: ep_len:806 episode reward: total was 23.820000. running mean: -27.220562\n",
      "ep 3172: ep_len:605 episode reward: total was 46.010000. running mean: -26.488256\n",
      "ep 3172: ep_len:66 episode reward: total was 31.500000. running mean: -25.908374\n",
      "ep 3172: ep_len:26 episode reward: total was 10.000000. running mean: -25.549290\n",
      "ep 3172: ep_len:627 episode reward: total was -30.100000. running mean: -25.594797\n",
      "ep 3172: ep_len:2869 episode reward: total was -32.970000. running mean: -25.668549\n",
      "epsilon:0.009992 episode_count: 47732. steps_count: 51388695.000000\n",
      "ep 3173: ep_len:1434 episode reward: total was 25.770000. running mean: -25.154164\n",
      "ep 3173: ep_len:1281 episode reward: total was -65.130000. running mean: -25.553922\n",
      "ep 3173: ep_len:2917 episode reward: total was -14.700000. running mean: -25.445383\n",
      "ep 3173: ep_len:818 episode reward: total was -9.160000. running mean: -25.282529\n",
      "ep 3173: ep_len:116 episode reward: total was 55.000000. running mean: -24.479704\n",
      "ep 3173: ep_len:77 episode reward: total was 37.000000. running mean: -23.864907\n",
      "ep 3173: ep_len:624 episode reward: total was -3.940000. running mean: -23.665658\n",
      "ep 3173: ep_len:4007 episode reward: total was -53.560000. running mean: -23.964601\n",
      "ep 3173: ep_len:1257 episode reward: total was -65.370000. running mean: -24.378655\n",
      "ep 3173: ep_len:718 episode reward: total was 41.130000. running mean: -23.723569\n",
      "ep 3173: ep_len:745 episode reward: total was -0.800000. running mean: -23.494333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3173: ep_len:136 episode reward: total was 65.000000. running mean: -22.609390\n",
      "ep 3173: ep_len:1482 episode reward: total was -12.840000. running mean: -22.511696\n",
      "ep 3173: ep_len:41 episode reward: total was 16.000000. running mean: -22.126579\n",
      "ep 3173: ep_len:45 episode reward: total was 19.500000. running mean: -21.710313\n",
      "epsilon:0.009992 episode_count: 47747. steps_count: 51404393.000000\n",
      "ep 3174: ep_len:1463 episode reward: total was 6.530000. running mean: -21.427910\n",
      "ep 3174: ep_len:1288 episode reward: total was -43.850000. running mean: -21.652131\n",
      "ep 3174: ep_len:66 episode reward: total was 31.500000. running mean: -21.120609\n",
      "ep 3174: ep_len:2981 episode reward: total was -6.920000. running mean: -20.978603\n",
      "ep 3174: ep_len:518 episode reward: total was -16.200000. running mean: -20.930817\n",
      "ep 3174: ep_len:165 episode reward: total was 81.000000. running mean: -19.911509\n",
      "ep 3174: ep_len:913 episode reward: total was 69.460000. running mean: -19.017794\n",
      "ep 3174: ep_len:646 episode reward: total was 27.020000. running mean: -18.557416\n",
      "ep 3174: ep_len:1544 episode reward: total was -6.650000. running mean: -18.438342\n",
      "ep 3174: ep_len:842 episode reward: total was 50.250000. running mean: -17.751458\n",
      "ep 3174: ep_len:1088 episode reward: total was 46.960000. running mean: -17.104344\n",
      "ep 3174: ep_len:42 episode reward: total was 19.500000. running mean: -16.738300\n",
      "ep 3174: ep_len:614 episode reward: total was -17.260000. running mean: -16.743517\n",
      "ep 3174: ep_len:2748 episode reward: total was -4.000000. running mean: -16.616082\n",
      "ep 3174: ep_len:41 episode reward: total was 17.500000. running mean: -16.274921\n",
      "epsilon:0.009992 episode_count: 47762. steps_count: 51419352.000000\n",
      "ep 3175: ep_len:723 episode reward: total was -7.830000. running mean: -16.190472\n",
      "ep 3175: ep_len:500 episode reward: total was 21.280000. running mean: -15.815768\n",
      "ep 3175: ep_len:82 episode reward: total was 39.500000. running mean: -15.262610\n",
      "ep 3175: ep_len:500 episode reward: total was 12.380000. running mean: -14.986184\n",
      "ep 3175: ep_len:65 episode reward: total was 31.000000. running mean: -14.526322\n",
      "ep 3175: ep_len:1508 episode reward: total was -183.050000. running mean: -16.211559\n",
      "ep 3175: ep_len:500 episode reward: total was 15.600000. running mean: -15.893443\n",
      "ep 3175: ep_len:3971 episode reward: total was -1173.350000. running mean: -27.468009\n",
      "ep 3175: ep_len:675 episode reward: total was 4.930000. running mean: -27.144029\n",
      "ep 3175: ep_len:709 episode reward: total was -20.450000. running mean: -27.077088\n",
      "ep 3175: ep_len:110 episode reward: total was 49.000000. running mean: -26.316317\n",
      "ep 3175: ep_len:657 episode reward: total was -22.890000. running mean: -26.282054\n",
      "ep 3175: ep_len:2841 episode reward: total was -31.780000. running mean: -26.337034\n",
      "epsilon:0.009992 episode_count: 47775. steps_count: 51432193.000000\n",
      "ep 3176: ep_len:1161 episode reward: total was -0.650000. running mean: -26.080163\n",
      "ep 3176: ep_len:721 episode reward: total was -24.270000. running mean: -26.062062\n",
      "ep 3176: ep_len:51 episode reward: total was 21.000000. running mean: -25.591441\n",
      "ep 3176: ep_len:3002 episode reward: total was -201.560000. running mean: -27.351127\n",
      "ep 3176: ep_len:682 episode reward: total was 9.430000. running mean: -26.983315\n",
      "ep 3176: ep_len:47 episode reward: total was 20.500000. running mean: -26.508482\n",
      "ep 3176: ep_len:666 episode reward: total was -2.600000. running mean: -26.269397\n",
      "ep 3176: ep_len:4126 episode reward: total was -191.970000. running mean: -27.926403\n",
      "ep 3176: ep_len:710 episode reward: total was -13.690000. running mean: -27.784039\n",
      "ep 3176: ep_len:767 episode reward: total was 7.020000. running mean: -27.435999\n",
      "ep 3176: ep_len:726 episode reward: total was -8.060000. running mean: -27.242239\n",
      "ep 3176: ep_len:77 episode reward: total was 35.500000. running mean: -26.614817\n",
      "ep 3176: ep_len:144 episode reward: total was 69.000000. running mean: -25.658669\n",
      "ep 3176: ep_len:91 episode reward: total was 44.000000. running mean: -24.962082\n",
      "ep 3176: ep_len:891 episode reward: total was 12.140000. running mean: -24.591061\n",
      "ep 3176: ep_len:45 episode reward: total was 19.500000. running mean: -24.150150\n",
      "ep 3176: ep_len:36 episode reward: total was 16.500000. running mean: -23.743649\n",
      "epsilon:0.009992 episode_count: 47792. steps_count: 51446136.000000\n",
      "ep 3177: ep_len:1146 episode reward: total was -16.010000. running mean: -23.666312\n",
      "ep 3177: ep_len:500 episode reward: total was 5.660000. running mean: -23.373049\n",
      "ep 3177: ep_len:2967 episode reward: total was -104.820000. running mean: -24.187519\n",
      "ep 3177: ep_len:500 episode reward: total was 21.000000. running mean: -23.735644\n",
      "ep 3177: ep_len:1078 episode reward: total was -16.660000. running mean: -23.664887\n",
      "ep 3177: ep_len:3931 episode reward: total was -86.210000. running mean: -24.290338\n",
      "ep 3177: ep_len:1167 episode reward: total was -30.250000. running mean: -24.349935\n",
      "ep 3177: ep_len:782 episode reward: total was 57.910000. running mean: -23.527336\n",
      "ep 3177: ep_len:584 episode reward: total was 17.360000. running mean: -23.118462\n",
      "ep 3177: ep_len:190 episode reward: total was 90.500000. running mean: -21.982278\n",
      "ep 3177: ep_len:1188 episode reward: total was 7.250000. running mean: -21.689955\n",
      "ep 3177: ep_len:2870 episode reward: total was -24.870000. running mean: -21.721755\n",
      "epsilon:0.009992 episode_count: 47804. steps_count: 51463039.000000\n",
      "ep 3178: ep_len:1098 episode reward: total was -3.540000. running mean: -21.539938\n",
      "ep 3178: ep_len:698 episode reward: total was -39.060000. running mean: -21.715138\n",
      "ep 3178: ep_len:3073 episode reward: total was -28.280000. running mean: -21.780787\n",
      "ep 3178: ep_len:500 episode reward: total was 35.490000. running mean: -21.208079\n",
      "ep 3178: ep_len:177 episode reward: total was 87.000000. running mean: -20.125998\n",
      "ep 3178: ep_len:48 episode reward: total was 22.500000. running mean: -19.699738\n",
      "ep 3178: ep_len:1392 episode reward: total was -154.830000. running mean: -21.051041\n",
      "ep 3178: ep_len:660 episode reward: total was 22.600000. running mean: -20.614531\n",
      "ep 3178: ep_len:1496 episode reward: total was -638.240000. running mean: -26.790785\n",
      "ep 3178: ep_len:723 episode reward: total was 40.260000. running mean: -26.120277\n",
      "ep 3178: ep_len:888 episode reward: total was 14.380000. running mean: -25.715275\n",
      "ep 3178: ep_len:79 episode reward: total was 38.000000. running mean: -25.078122\n",
      "ep 3178: ep_len:52 episode reward: total was 24.500000. running mean: -24.582341\n",
      "ep 3178: ep_len:599 episode reward: total was -21.450000. running mean: -24.551017\n",
      "ep 3178: ep_len:2857 episode reward: total was -16.830000. running mean: -24.473807\n",
      "epsilon:0.009992 episode_count: 47819. steps_count: 51477379.000000\n",
      "ep 3179: ep_len:875 episode reward: total was 23.180000. running mean: -23.997269\n",
      "ep 3179: ep_len:891 episode reward: total was 13.510000. running mean: -23.622196\n",
      "ep 3179: ep_len:55 episode reward: total was 26.000000. running mean: -23.125974\n",
      "ep 3179: ep_len:3084 episode reward: total was -86.780000. running mean: -23.762515\n",
      "ep 3179: ep_len:1078 episode reward: total was 12.140000. running mean: -23.403489\n",
      "ep 3179: ep_len:1424 episode reward: total was -256.900000. running mean: -25.738455\n",
      "ep 3179: ep_len:3872 episode reward: total was -152.970000. running mean: -27.010770\n",
      "ep 3179: ep_len:500 episode reward: total was 10.070000. running mean: -26.639962\n",
      "ep 3179: ep_len:7284 episode reward: total was -1188.580000. running mean: -38.259363\n",
      "ep 3179: ep_len:904 episode reward: total was 9.240000. running mean: -37.784369\n",
      "ep 3179: ep_len:858 episode reward: total was 9.820000. running mean: -37.308325\n",
      "ep 3179: ep_len:2849 episode reward: total was 2.770000. running mean: -36.907542\n",
      "epsilon:0.009992 episode_count: 47831. steps_count: 51501053.000000\n",
      "ep 3180: ep_len:668 episode reward: total was 17.580000. running mean: -36.362667\n",
      "ep 3180: ep_len:1650 episode reward: total was -36.160000. running mean: -36.360640\n",
      "ep 3180: ep_len:3034 episode reward: total was -70.950000. running mean: -36.706534\n",
      "ep 3180: ep_len:1435 episode reward: total was 12.830000. running mean: -36.211168\n",
      "ep 3180: ep_len:128 episode reward: total was 61.000000. running mean: -35.239057\n",
      "ep 3180: ep_len:107 episode reward: total was 52.000000. running mean: -34.366666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3180: ep_len:653 episode reward: total was 12.230000. running mean: -33.900699\n",
      "ep 3180: ep_len:358 episode reward: total was 12.670000. running mean: -33.434992\n",
      "ep 3180: ep_len:1145 episode reward: total was -19.720000. running mean: -33.297842\n",
      "ep 3180: ep_len:672 episode reward: total was 9.160000. running mean: -32.873264\n",
      "ep 3180: ep_len:703 episode reward: total was 23.030000. running mean: -32.314231\n",
      "ep 3180: ep_len:51 episode reward: total was 24.000000. running mean: -31.751089\n",
      "ep 3180: ep_len:46 episode reward: total was 21.500000. running mean: -31.218578\n",
      "ep 3180: ep_len:1001 episode reward: total was -2.000000. running mean: -30.926392\n",
      "ep 3180: ep_len:2845 episode reward: total was -27.610000. running mean: -30.893228\n",
      "ep 3180: ep_len:49 episode reward: total was 23.000000. running mean: -30.354296\n",
      "epsilon:0.009992 episode_count: 47847. steps_count: 51515598.000000\n",
      "ep 3181: ep_len:677 episode reward: total was 21.050000. running mean: -29.840253\n",
      "ep 3181: ep_len:731 episode reward: total was -62.960000. running mean: -30.171451\n",
      "ep 3181: ep_len:61 episode reward: total was 29.000000. running mean: -29.579736\n",
      "ep 3181: ep_len:2978 episode reward: total was -41.170000. running mean: -29.695639\n",
      "ep 3181: ep_len:824 episode reward: total was 23.100000. running mean: -29.167682\n",
      "ep 3181: ep_len:49 episode reward: total was 23.000000. running mean: -28.646006\n",
      "ep 3181: ep_len:659 episode reward: total was 0.290000. running mean: -28.356646\n",
      "ep 3181: ep_len:665 episode reward: total was 7.900000. running mean: -27.994079\n",
      "ep 3181: ep_len:1582 episode reward: total was -78.050000. running mean: -28.494638\n",
      "ep 3181: ep_len:735 episode reward: total was 27.240000. running mean: -27.937292\n",
      "ep 3181: ep_len:976 episode reward: total was 7.110000. running mean: -27.586819\n",
      "ep 3181: ep_len:207 episode reward: total was 99.000000. running mean: -26.320951\n",
      "ep 3181: ep_len:1118 episode reward: total was -11.210000. running mean: -26.169841\n",
      "ep 3181: ep_len:2909 episode reward: total was 0.120000. running mean: -25.906943\n",
      "epsilon:0.009992 episode_count: 47861. steps_count: 51529769.000000\n",
      "ep 3182: ep_len:1444 episode reward: total was 21.920000. running mean: -25.428673\n",
      "ep 3182: ep_len:791 episode reward: total was -47.700000. running mean: -25.651387\n",
      "ep 3182: ep_len:2978 episode reward: total was -79.730000. running mean: -26.192173\n",
      "ep 3182: ep_len:552 episode reward: total was -0.710000. running mean: -25.937351\n",
      "ep 3182: ep_len:92 episode reward: total was 44.500000. running mean: -25.232978\n",
      "ep 3182: ep_len:65 episode reward: total was 31.000000. running mean: -24.670648\n",
      "ep 3182: ep_len:778 episode reward: total was 29.710000. running mean: -24.126841\n",
      "ep 3182: ep_len:660 episode reward: total was 26.700000. running mean: -23.618573\n",
      "ep 3182: ep_len:1578 episode reward: total was -72.460000. running mean: -24.106987\n",
      "ep 3182: ep_len:654 episode reward: total was -1.090000. running mean: -23.876817\n",
      "ep 3182: ep_len:1123 episode reward: total was -0.410000. running mean: -23.642149\n",
      "ep 3182: ep_len:1181 episode reward: total was 19.610000. running mean: -23.209628\n",
      "ep 3182: ep_len:2809 episode reward: total was -32.340000. running mean: -23.300931\n",
      "epsilon:0.009992 episode_count: 47874. steps_count: 51544474.000000\n",
      "ep 3183: ep_len:776 episode reward: total was -80.280000. running mean: -23.870722\n",
      "ep 3183: ep_len:500 episode reward: total was 16.530000. running mean: -23.466715\n",
      "ep 3183: ep_len:78 episode reward: total was 34.500000. running mean: -22.887048\n",
      "ep 3183: ep_len:2980 episode reward: total was -24.470000. running mean: -22.902877\n",
      "ep 3183: ep_len:1082 episode reward: total was -59.580000. running mean: -23.269648\n",
      "ep 3183: ep_len:79 episode reward: total was 38.000000. running mean: -22.656952\n",
      "ep 3183: ep_len:49 episode reward: total was 23.000000. running mean: -22.200382\n",
      "ep 3183: ep_len:741 episode reward: total was -12.960000. running mean: -22.107979\n",
      "ep 3183: ep_len:631 episode reward: total was 23.440000. running mean: -21.652499\n",
      "ep 3183: ep_len:596 episode reward: total was -42.690000. running mean: -21.862874\n",
      "ep 3183: ep_len:760 episode reward: total was 5.110000. running mean: -21.593145\n",
      "ep 3183: ep_len:611 episode reward: total was -3.330000. running mean: -21.410514\n",
      "ep 3183: ep_len:91 episode reward: total was 44.000000. running mean: -20.756409\n",
      "ep 3183: ep_len:1135 episode reward: total was -26.190000. running mean: -20.810744\n",
      "ep 3183: ep_len:2833 episode reward: total was -35.900000. running mean: -20.961637\n",
      "ep 3183: ep_len:38 episode reward: total was 17.500000. running mean: -20.577021\n",
      "epsilon:0.009992 episode_count: 47890. steps_count: 51557454.000000\n",
      "ep 3184: ep_len:651 episode reward: total was -1.220000. running mean: -20.383450\n",
      "ep 3184: ep_len:683 episode reward: total was -32.730000. running mean: -20.506916\n",
      "ep 3184: ep_len:49 episode reward: total was 23.000000. running mean: -20.071847\n",
      "ep 3184: ep_len:2891 episode reward: total was -70.190000. running mean: -20.573028\n",
      "ep 3184: ep_len:1716 episode reward: total was -56.590000. running mean: -20.933198\n",
      "ep 3184: ep_len:60 episode reward: total was 27.000000. running mean: -20.453866\n",
      "ep 3184: ep_len:81 episode reward: total was 37.500000. running mean: -19.874327\n",
      "ep 3184: ep_len:66 episode reward: total was 31.500000. running mean: -19.360584\n",
      "ep 3184: ep_len:79 episode reward: total was 38.000000. running mean: -18.786978\n",
      "ep 3184: ep_len:1110 episode reward: total was 5.430000. running mean: -18.544808\n",
      "ep 3184: ep_len:331 episode reward: total was 12.400000. running mean: -18.235360\n",
      "ep 3184: ep_len:673 episode reward: total was -22.700000. running mean: -18.280007\n",
      "ep 3184: ep_len:861 episode reward: total was 47.400000. running mean: -17.623207\n",
      "ep 3184: ep_len:500 episode reward: total was 12.710000. running mean: -17.319875\n",
      "ep 3184: ep_len:621 episode reward: total was -10.120000. running mean: -17.247876\n",
      "ep 3184: ep_len:46 episode reward: total was 21.500000. running mean: -16.860397\n",
      "epsilon:0.009992 episode_count: 47906. steps_count: 51567872.000000\n",
      "ep 3185: ep_len:1488 episode reward: total was 26.000000. running mean: -16.431793\n",
      "ep 3185: ep_len:697 episode reward: total was -20.370000. running mean: -16.471175\n",
      "ep 3185: ep_len:3096 episode reward: total was 2.130000. running mean: -16.285163\n",
      "ep 3185: ep_len:822 episode reward: total was 38.410000. running mean: -15.738212\n",
      "ep 3185: ep_len:177 episode reward: total was 87.000000. running mean: -14.710830\n",
      "ep 3185: ep_len:63 episode reward: total was 28.500000. running mean: -14.278721\n",
      "ep 3185: ep_len:1110 episode reward: total was -14.320000. running mean: -14.279134\n",
      "ep 3185: ep_len:3967 episode reward: total was -598.300000. running mean: -20.119343\n",
      "ep 3185: ep_len:1215 episode reward: total was -47.610000. running mean: -20.394249\n",
      "ep 3185: ep_len:650 episode reward: total was -2.300000. running mean: -20.213307\n",
      "ep 3185: ep_len:596 episode reward: total was -8.840000. running mean: -20.099574\n",
      "ep 3185: ep_len:55 episode reward: total was 26.000000. running mean: -19.638578\n",
      "ep 3185: ep_len:94 episode reward: total was 45.500000. running mean: -18.987192\n",
      "ep 3185: ep_len:635 episode reward: total was -16.040000. running mean: -18.957720\n",
      "ep 3185: ep_len:2788 episode reward: total was -25.940000. running mean: -19.027543\n",
      "ep 3185: ep_len:53 episode reward: total was 23.500000. running mean: -18.602268\n",
      "epsilon:0.009992 episode_count: 47922. steps_count: 51585378.000000\n",
      "ep 3186: ep_len:1101 episode reward: total was -4.310000. running mean: -18.459345\n",
      "ep 3186: ep_len:694 episode reward: total was -18.290000. running mean: -18.457652\n",
      "ep 3186: ep_len:2892 episode reward: total was -11.010000. running mean: -18.383175\n",
      "ep 3186: ep_len:547 episode reward: total was -26.010000. running mean: -18.459443\n",
      "ep 3186: ep_len:53 episode reward: total was 25.000000. running mean: -18.024849\n",
      "ep 3186: ep_len:101 episode reward: total was 49.000000. running mean: -17.354600\n",
      "ep 3186: ep_len:1036 episode reward: total was -59.410000. running mean: -17.775154\n",
      "ep 3186: ep_len:327 episode reward: total was 12.480000. running mean: -17.472603\n",
      "ep 3186: ep_len:532 episode reward: total was -0.910000. running mean: -17.306977\n",
      "ep 3186: ep_len:754 episode reward: total was 12.310000. running mean: -17.010807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3186: ep_len:878 episode reward: total was 22.170000. running mean: -16.618999\n",
      "ep 3186: ep_len:55 episode reward: total was 26.000000. running mean: -16.192809\n",
      "ep 3186: ep_len:47 episode reward: total was 22.000000. running mean: -15.810881\n",
      "ep 3186: ep_len:1151 episode reward: total was -0.800000. running mean: -15.660772\n",
      "ep 3186: ep_len:33 episode reward: total was -15.000000. running mean: -15.654164\n",
      "epsilon:0.009992 episode_count: 47937. steps_count: 51595579.000000\n",
      "ep 3187: ep_len:1405 episode reward: total was 11.370000. running mean: -15.383923\n",
      "ep 3187: ep_len:1631 episode reward: total was -37.380000. running mean: -15.603884\n",
      "ep 3187: ep_len:68 episode reward: total was 31.000000. running mean: -15.137845\n",
      "ep 3187: ep_len:3062 episode reward: total was -48.630000. running mean: -15.472766\n",
      "ep 3187: ep_len:693 episode reward: total was 1.880000. running mean: -15.299239\n",
      "ep 3187: ep_len:99 episode reward: total was 43.500000. running mean: -14.711246\n",
      "ep 3187: ep_len:47 episode reward: total was 22.000000. running mean: -14.344134\n",
      "ep 3187: ep_len:500 episode reward: total was 58.210000. running mean: -13.618592\n",
      "ep 3187: ep_len:309 episode reward: total was 4.800000. running mean: -13.434407\n",
      "ep 3187: ep_len:1288 episode reward: total was -40.750000. running mean: -13.707562\n",
      "ep 3187: ep_len:664 episode reward: total was 30.360000. running mean: -13.266887\n",
      "ep 3187: ep_len:686 episode reward: total was -30.290000. running mean: -13.437118\n",
      "ep 3187: ep_len:53 episode reward: total was 23.500000. running mean: -13.067747\n",
      "ep 3187: ep_len:36 episode reward: total was 13.500000. running mean: -12.802069\n",
      "ep 3187: ep_len:107 episode reward: total was 52.000000. running mean: -12.154049\n",
      "ep 3187: ep_len:1185 episode reward: total was -15.400000. running mean: -12.186508\n",
      "ep 3187: ep_len:45 episode reward: total was 21.000000. running mean: -11.854643\n",
      "epsilon:0.009992 episode_count: 47954. steps_count: 51607457.000000\n",
      "ep 3188: ep_len:641 episode reward: total was -20.020000. running mean: -11.936297\n",
      "ep 3188: ep_len:696 episode reward: total was -18.460000. running mean: -12.001534\n",
      "ep 3188: ep_len:88 episode reward: total was 41.000000. running mean: -11.471518\n",
      "ep 3188: ep_len:809 episode reward: total was -16.320000. running mean: -11.520003\n",
      "ep 3188: ep_len:58 episode reward: total was 27.500000. running mean: -11.129803\n",
      "ep 3188: ep_len:932 episode reward: total was 66.160000. running mean: -10.356905\n",
      "ep 3188: ep_len:3926 episode reward: total was -146.370000. running mean: -11.717036\n",
      "ep 3188: ep_len:1199 episode reward: total was -33.510000. running mean: -11.934966\n",
      "ep 3188: ep_len:820 episode reward: total was 35.200000. running mean: -11.463616\n",
      "ep 3188: ep_len:671 episode reward: total was -18.660000. running mean: -11.535580\n",
      "ep 3188: ep_len:87 episode reward: total was 42.000000. running mean: -11.000224\n",
      "ep 3188: ep_len:1223 episode reward: total was -15.960000. running mean: -11.049822\n",
      "ep 3188: ep_len:2838 episode reward: total was -45.150000. running mean: -11.390824\n",
      "ep 3188: ep_len:59 episode reward: total was 28.000000. running mean: -10.996915\n",
      "epsilon:0.009992 episode_count: 47968. steps_count: 51621504.000000\n",
      "ep 3189: ep_len:1073 episode reward: total was -7.620000. running mean: -10.963146\n",
      "ep 3189: ep_len:653 episode reward: total was -10.290000. running mean: -10.956415\n",
      "ep 3189: ep_len:58 episode reward: total was 26.000000. running mean: -10.586851\n",
      "ep 3189: ep_len:2989 episode reward: total was -48.470000. running mean: -10.965682\n",
      "ep 3189: ep_len:822 episode reward: total was -9.660000. running mean: -10.952625\n",
      "ep 3189: ep_len:60 episode reward: total was 27.000000. running mean: -10.573099\n",
      "ep 3189: ep_len:500 episode reward: total was 15.770000. running mean: -10.309668\n",
      "ep 3189: ep_len:315 episode reward: total was 20.990000. running mean: -9.996671\n",
      "ep 3189: ep_len:689 episode reward: total was -6.560000. running mean: -9.962305\n",
      "ep 3189: ep_len:857 episode reward: total was 57.160000. running mean: -9.291082\n",
      "ep 3189: ep_len:660 episode reward: total was -28.470000. running mean: -9.482871\n",
      "ep 3189: ep_len:1081 episode reward: total was 9.050000. running mean: -9.297542\n",
      "ep 3189: ep_len:2855 episode reward: total was -4.920000. running mean: -9.253767\n",
      "ep 3189: ep_len:67 episode reward: total was 30.500000. running mean: -8.856229\n",
      "epsilon:0.009992 episode_count: 47982. steps_count: 51634183.000000\n",
      "ep 3190: ep_len:764 episode reward: total was -115.750000. running mean: -9.925167\n",
      "ep 3190: ep_len:1157 episode reward: total was 3.230000. running mean: -9.793615\n",
      "ep 3190: ep_len:56 episode reward: total was 25.000000. running mean: -9.445679\n",
      "ep 3190: ep_len:3091 episode reward: total was -43.950000. running mean: -9.790722\n",
      "ep 3190: ep_len:753 episode reward: total was -38.490000. running mean: -10.077715\n",
      "ep 3190: ep_len:41 episode reward: total was 17.500000. running mean: -9.801938\n",
      "ep 3190: ep_len:77 episode reward: total was 37.000000. running mean: -9.333918\n",
      "ep 3190: ep_len:747 episode reward: total was -20.630000. running mean: -9.446879\n",
      "ep 3190: ep_len:3998 episode reward: total was -55.760000. running mean: -9.910010\n",
      "ep 3190: ep_len:500 episode reward: total was -23.020000. running mean: -10.041110\n",
      "ep 3190: ep_len:7435 episode reward: total was 23.410000. running mean: -9.706599\n",
      "ep 3190: ep_len:864 episode reward: total was -1.170000. running mean: -9.621233\n",
      "ep 3190: ep_len:76 episode reward: total was 36.500000. running mean: -9.160021\n",
      "ep 3190: ep_len:170 episode reward: total was 82.000000. running mean: -8.248421\n",
      "ep 3190: ep_len:76 episode reward: total was 35.000000. running mean: -7.815936\n",
      "ep 3190: ep_len:1501 episode reward: total was -4.840000. running mean: -7.786177\n",
      "ep 3190: ep_len:2866 episode reward: total was -32.690000. running mean: -8.035215\n",
      "ep 3190: ep_len:70 episode reward: total was 32.000000. running mean: -7.634863\n",
      "epsilon:0.009992 episode_count: 48000. steps_count: 51658425.000000\n",
      "ep 3191: ep_len:839 episode reward: total was 0.210000. running mean: -7.556414\n",
      "ep 3191: ep_len:500 episode reward: total was 5.210000. running mean: -7.428750\n",
      "ep 3191: ep_len:66 episode reward: total was 31.500000. running mean: -7.039463\n",
      "ep 3191: ep_len:2991 episode reward: total was -46.070000. running mean: -7.429768\n",
      "ep 3191: ep_len:668 episode reward: total was 26.730000. running mean: -7.088171\n",
      "ep 3191: ep_len:54 episode reward: total was 25.500000. running mean: -6.762289\n",
      "ep 3191: ep_len:86 episode reward: total was 41.500000. running mean: -6.279666\n",
      "ep 3191: ep_len:1456 episode reward: total was -249.730000. running mean: -8.714169\n",
      "ep 3191: ep_len:3902 episode reward: total was -71.850000. running mean: -9.345528\n",
      "ep 3191: ep_len:867 episode reward: total was 16.980000. running mean: -9.082272\n",
      "ep 3191: ep_len:884 episode reward: total was 54.430000. running mean: -8.447150\n",
      "ep 3191: ep_len:1471 episode reward: total was -27.490000. running mean: -8.637578\n",
      "ep 3191: ep_len:65 episode reward: total was 29.500000. running mean: -8.256202\n",
      "ep 3191: ep_len:44 episode reward: total was 20.500000. running mean: -7.968640\n",
      "ep 3191: ep_len:1101 episode reward: total was 39.990000. running mean: -7.489054\n",
      "ep 3191: ep_len:2768 episode reward: total was -16.620000. running mean: -7.580363\n",
      "epsilon:0.009992 episode_count: 48016. steps_count: 51676187.000000\n",
      "ep 3192: ep_len:657 episode reward: total was -37.920000. running mean: -7.883760\n",
      "ep 3192: ep_len:982 episode reward: total was 11.310000. running mean: -7.691822\n",
      "ep 3192: ep_len:65 episode reward: total was 29.500000. running mean: -7.319904\n",
      "ep 3192: ep_len:2865 episode reward: total was -48.030000. running mean: -7.727005\n",
      "ep 3192: ep_len:500 episode reward: total was 23.670000. running mean: -7.413035\n",
      "ep 3192: ep_len:53 episode reward: total was 25.000000. running mean: -7.088904\n",
      "ep 3192: ep_len:60 episode reward: total was 27.000000. running mean: -6.748015\n",
      "ep 3192: ep_len:590 episode reward: total was -3.420000. running mean: -6.714735\n",
      "ep 3192: ep_len:344 episode reward: total was 23.150000. running mean: -6.416088\n",
      "ep 3192: ep_len:541 episode reward: total was -7.830000. running mean: -6.430227\n",
      "ep 3192: ep_len:758 episode reward: total was 36.850000. running mean: -5.997425\n",
      "ep 3192: ep_len:1056 episode reward: total was -0.690000. running mean: -5.944351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3192: ep_len:63 episode reward: total was 30.000000. running mean: -5.584907\n",
      "ep 3192: ep_len:62 episode reward: total was 25.000000. running mean: -5.279058\n",
      "ep 3192: ep_len:794 episode reward: total was -81.720000. running mean: -6.043467\n",
      "ep 3192: ep_len:2857 episode reward: total was -1.830000. running mean: -6.001333\n",
      "epsilon:0.009992 episode_count: 48032. steps_count: 51688434.000000\n",
      "ep 3193: ep_len:2050 episode reward: total was -1036.620000. running mean: -16.307519\n",
      "ep 3193: ep_len:197 episode reward: total was 12.860000. running mean: -16.015844\n",
      "ep 3193: ep_len:2881 episode reward: total was -38.910000. running mean: -16.244786\n",
      "ep 3193: ep_len:560 episode reward: total was 3.780000. running mean: -16.044538\n",
      "ep 3193: ep_len:73 episode reward: total was 35.000000. running mean: -15.534092\n",
      "ep 3193: ep_len:500 episode reward: total was -12.760000. running mean: -15.506352\n",
      "ep 3193: ep_len:3986 episode reward: total was -43.760000. running mean: -15.788888\n",
      "ep 3193: ep_len:619 episode reward: total was -33.370000. running mean: -15.964699\n",
      "ep 3193: ep_len:7392 episode reward: total was 33.720000. running mean: -15.467852\n",
      "ep 3193: ep_len:500 episode reward: total was 0.550000. running mean: -15.307674\n",
      "ep 3193: ep_len:87 episode reward: total was 40.500000. running mean: -14.749597\n",
      "ep 3193: ep_len:914 episode reward: total was 15.890000. running mean: -14.443201\n",
      "ep 3193: ep_len:2851 episode reward: total was 1.290000. running mean: -14.285869\n",
      "epsilon:0.009992 episode_count: 48045. steps_count: 51711044.000000\n",
      "ep 3194: ep_len:1061 episode reward: total was -0.700000. running mean: -14.150010\n",
      "ep 3194: ep_len:711 episode reward: total was -44.770000. running mean: -14.456210\n",
      "ep 3194: ep_len:2882 episode reward: total was -50.580000. running mean: -14.817448\n",
      "ep 3194: ep_len:664 episode reward: total was -3.270000. running mean: -14.701974\n",
      "ep 3194: ep_len:79 episode reward: total was 38.000000. running mean: -14.174954\n",
      "ep 3194: ep_len:500 episode reward: total was 10.530000. running mean: -13.927904\n",
      "ep 3194: ep_len:3958 episode reward: total was -73.330000. running mean: -14.521925\n",
      "ep 3194: ep_len:868 episode reward: total was -66.990000. running mean: -15.046606\n",
      "ep 3194: ep_len:672 episode reward: total was 17.730000. running mean: -14.718840\n",
      "ep 3194: ep_len:1006 episode reward: total was 17.240000. running mean: -14.399252\n",
      "ep 3194: ep_len:90 episode reward: total was 43.500000. running mean: -13.820259\n",
      "ep 3194: ep_len:54 episode reward: total was 24.000000. running mean: -13.442056\n",
      "ep 3194: ep_len:715 episode reward: total was -33.420000. running mean: -13.641836\n",
      "ep 3194: ep_len:2755 episode reward: total was -31.590000. running mean: -13.821318\n",
      "ep 3194: ep_len:71 episode reward: total was 34.000000. running mean: -13.343104\n",
      "epsilon:0.009992 episode_count: 48060. steps_count: 51727130.000000\n",
      "ep 3195: ep_len:734 episode reward: total was 6.360000. running mean: -13.146073\n",
      "ep 3195: ep_len:500 episode reward: total was -0.670000. running mean: -13.021313\n",
      "ep 3195: ep_len:3035 episode reward: total was -26.420000. running mean: -13.155299\n",
      "ep 3195: ep_len:806 episode reward: total was 27.350000. running mean: -12.750246\n",
      "ep 3195: ep_len:116 episode reward: total was 53.500000. running mean: -12.087744\n",
      "ep 3195: ep_len:972 episode reward: total was -18.700000. running mean: -12.153867\n",
      "ep 3195: ep_len:646 episode reward: total was 17.070000. running mean: -11.861628\n",
      "ep 3195: ep_len:946 episode reward: total was -25.590000. running mean: -11.998912\n",
      "ep 3195: ep_len:683 episode reward: total was 6.420000. running mean: -11.814722\n",
      "ep 3195: ep_len:936 episode reward: total was 31.230000. running mean: -11.384275\n",
      "ep 3195: ep_len:218 episode reward: total was 106.000000. running mean: -10.210432\n",
      "ep 3195: ep_len:1164 episode reward: total was -32.970000. running mean: -10.438028\n",
      "ep 3195: ep_len:2775 episode reward: total was -10.520000. running mean: -10.438848\n",
      "ep 3195: ep_len:66 episode reward: total was 28.500000. running mean: -10.049459\n",
      "epsilon:0.009992 episode_count: 48074. steps_count: 51740727.000000\n",
      "ep 3196: ep_len:827 episode reward: total was -12.790000. running mean: -10.076865\n",
      "ep 3196: ep_len:681 episode reward: total was -12.920000. running mean: -10.105296\n",
      "ep 3196: ep_len:2919 episode reward: total was 20.350000. running mean: -9.800743\n",
      "ep 3196: ep_len:1431 episode reward: total was -6.340000. running mean: -9.766136\n",
      "ep 3196: ep_len:110 episode reward: total was 53.500000. running mean: -9.133474\n",
      "ep 3196: ep_len:79 episode reward: total was 38.000000. running mean: -8.662140\n",
      "ep 3196: ep_len:1036 episode reward: total was 10.230000. running mean: -8.473218\n",
      "ep 3196: ep_len:500 episode reward: total was -36.060000. running mean: -8.749086\n",
      "ep 3196: ep_len:749 episode reward: total was -19.620000. running mean: -8.857795\n",
      "ep 3196: ep_len:799 episode reward: total was 31.400000. running mean: -8.455217\n",
      "ep 3196: ep_len:645 episode reward: total was 11.660000. running mean: -8.254065\n",
      "ep 3196: ep_len:52 episode reward: total was 21.500000. running mean: -7.956524\n",
      "ep 3196: ep_len:37 episode reward: total was 17.000000. running mean: -7.706959\n",
      "ep 3196: ep_len:121 episode reward: total was 54.500000. running mean: -7.084890\n",
      "ep 3196: ep_len:1419 episode reward: total was -1.500000. running mean: -7.029041\n",
      "ep 3196: ep_len:2929 episode reward: total was -2.980000. running mean: -6.988550\n",
      "epsilon:0.009992 episode_count: 48090. steps_count: 51755061.000000\n",
      "ep 3197: ep_len:719 episode reward: total was -13.970000. running mean: -7.058365\n",
      "ep 3197: ep_len:724 episode reward: total was -35.670000. running mean: -7.344481\n",
      "ep 3197: ep_len:3007 episode reward: total was -23.890000. running mean: -7.509936\n",
      "ep 3197: ep_len:537 episode reward: total was -116.400000. running mean: -8.598837\n",
      "ep 3197: ep_len:99 episode reward: total was 46.500000. running mean: -8.047849\n",
      "ep 3197: ep_len:43 episode reward: total was 18.500000. running mean: -7.782370\n",
      "ep 3197: ep_len:709 episode reward: total was -10.250000. running mean: -7.807046\n",
      "ep 3197: ep_len:4100 episode reward: total was -118.530000. running mean: -8.914276\n",
      "ep 3197: ep_len:844 episode reward: total was 30.950000. running mean: -8.515633\n",
      "ep 3197: ep_len:730 episode reward: total was 7.440000. running mean: -8.356077\n",
      "ep 3197: ep_len:985 episode reward: total was 11.610000. running mean: -8.156416\n",
      "ep 3197: ep_len:59 episode reward: total was 28.000000. running mean: -7.794852\n",
      "ep 3197: ep_len:113 episode reward: total was 53.500000. running mean: -7.181903\n",
      "ep 3197: ep_len:791 episode reward: total was -15.300000. running mean: -7.263084\n",
      "ep 3197: ep_len:2762 episode reward: total was -5.080000. running mean: -7.241254\n",
      "epsilon:0.009992 episode_count: 48105. steps_count: 51771283.000000\n",
      "ep 3198: ep_len:2832 episode reward: total was -1451.700000. running mean: -21.685841\n",
      "ep 3198: ep_len:688 episode reward: total was -31.670000. running mean: -21.785683\n",
      "ep 3198: ep_len:2940 episode reward: total was -7.030000. running mean: -21.638126\n",
      "ep 3198: ep_len:1482 episode reward: total was 9.540000. running mean: -21.326345\n",
      "ep 3198: ep_len:57 episode reward: total was 27.000000. running mean: -20.843081\n",
      "ep 3198: ep_len:50 episode reward: total was 22.000000. running mean: -20.414650\n",
      "ep 3198: ep_len:587 episode reward: total was 48.100000. running mean: -19.729504\n",
      "ep 3198: ep_len:3765 episode reward: total was -1546.050000. running mean: -34.992709\n",
      "ep 3198: ep_len:633 episode reward: total was -30.200000. running mean: -34.944782\n",
      "ep 3198: ep_len:7402 episode reward: total was 44.870000. running mean: -34.146634\n",
      "ep 3198: ep_len:576 episode reward: total was -5.580000. running mean: -33.860967\n",
      "ep 3198: ep_len:637 episode reward: total was -10.970000. running mean: -33.632058\n",
      "ep 3198: ep_len:2875 episode reward: total was 0.730000. running mean: -33.288437\n",
      "ep 3198: ep_len:52 episode reward: total was 20.000000. running mean: -32.755553\n",
      "epsilon:0.009992 episode_count: 48119. steps_count: 51795859.000000\n",
      "ep 3199: ep_len:1525 episode reward: total was -14.360000. running mean: -32.571597\n",
      "ep 3199: ep_len:1681 episode reward: total was -67.160000. running mean: -32.917481\n",
      "ep 3199: ep_len:70 episode reward: total was 31.510000. running mean: -32.273207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3199: ep_len:2932 episode reward: total was 9.240000. running mean: -31.858074\n",
      "ep 3199: ep_len:608 episode reward: total was 0.860000. running mean: -31.530894\n",
      "ep 3199: ep_len:179 episode reward: total was 86.500000. running mean: -30.350585\n",
      "ep 3199: ep_len:653 episode reward: total was 18.720000. running mean: -29.859879\n",
      "ep 3199: ep_len:500 episode reward: total was 15.140000. running mean: -29.409880\n",
      "ep 3199: ep_len:537 episode reward: total was 6.980000. running mean: -29.045981\n",
      "ep 3199: ep_len:722 episode reward: total was -1.460000. running mean: -28.770122\n",
      "ep 3199: ep_len:682 episode reward: total was -33.330000. running mean: -28.815720\n",
      "ep 3199: ep_len:116 episode reward: total was 55.000000. running mean: -27.977563\n",
      "ep 3199: ep_len:39 episode reward: total was 18.000000. running mean: -27.517787\n",
      "ep 3199: ep_len:500 episode reward: total was -2.260000. running mean: -27.265210\n",
      "ep 3199: ep_len:2780 episode reward: total was -29.450000. running mean: -27.287058\n",
      "epsilon:0.009992 episode_count: 48134. steps_count: 51809383.000000\n",
      "ep 3200: ep_len:1035 episode reward: total was -8.610000. running mean: -27.100287\n",
      "ep 3200: ep_len:500 episode reward: total was 10.900000. running mean: -26.720284\n",
      "ep 3200: ep_len:3061 episode reward: total was 20.570000. running mean: -26.247381\n",
      "ep 3200: ep_len:652 episode reward: total was -7.790000. running mean: -26.062807\n",
      "ep 3200: ep_len:148 episode reward: total was 69.500000. running mean: -25.107179\n",
      "ep 3200: ep_len:1473 episode reward: total was -186.330000. running mean: -26.719408\n",
      "ep 3200: ep_len:643 episode reward: total was 23.960000. running mean: -26.212613\n",
      "ep 3200: ep_len:1240 episode reward: total was -82.680000. running mean: -26.777287\n",
      "ep 3200: ep_len:7223 episode reward: total was 52.690000. running mean: -25.982614\n",
      "ep 3200: ep_len:670 episode reward: total was -1.550000. running mean: -25.738288\n",
      "ep 3200: ep_len:608 episode reward: total was -13.770000. running mean: -25.618605\n",
      "ep 3200: ep_len:2825 episode reward: total was -10.730000. running mean: -25.469719\n",
      "epsilon:0.009992 episode_count: 48146. steps_count: 51829461.000000\n",
      "ep 3201: ep_len:976 episode reward: total was -43.990000. running mean: -25.654922\n",
      "ep 3201: ep_len:1208 episode reward: total was -71.920000. running mean: -26.117573\n",
      "ep 3201: ep_len:3077 episode reward: total was -2.770000. running mean: -25.884097\n",
      "ep 3201: ep_len:1666 episode reward: total was -60.540000. running mean: -26.230656\n",
      "ep 3201: ep_len:60 episode reward: total was 27.000000. running mean: -25.698350\n",
      "ep 3201: ep_len:90 episode reward: total was 43.500000. running mean: -25.006366\n",
      "ep 3201: ep_len:52 episode reward: total was 23.000000. running mean: -24.526303\n",
      "ep 3201: ep_len:685 episode reward: total was -5.440000. running mean: -24.335440\n",
      "ep 3201: ep_len:320 episode reward: total was 25.420000. running mean: -23.837885\n",
      "ep 3201: ep_len:1235 episode reward: total was -43.370000. running mean: -24.033206\n",
      "ep 3201: ep_len:876 episode reward: total was 50.740000. running mean: -23.285474\n",
      "ep 3201: ep_len:682 episode reward: total was -24.760000. running mean: -23.300219\n",
      "ep 3201: ep_len:67 episode reward: total was 30.500000. running mean: -22.762217\n",
      "ep 3201: ep_len:1048 episode reward: total was 21.890000. running mean: -22.315695\n",
      "ep 3201: ep_len:2881 episode reward: total was -5.210000. running mean: -22.144638\n",
      "ep 3201: ep_len:41 episode reward: total was 17.500000. running mean: -21.748192\n",
      "epsilon:0.009992 episode_count: 48162. steps_count: 51844425.000000\n",
      "ep 3202: ep_len:1082 episode reward: total was 8.380000. running mean: -21.446910\n",
      "ep 3202: ep_len:1567 episode reward: total was -38.430000. running mean: -21.616741\n",
      "ep 3202: ep_len:2997 episode reward: total was 27.920000. running mean: -21.121373\n",
      "ep 3202: ep_len:657 episode reward: total was 9.370000. running mean: -20.816460\n",
      "ep 3202: ep_len:139 episode reward: total was 65.000000. running mean: -19.958295\n",
      "ep 3202: ep_len:109 episode reward: total was 53.000000. running mean: -19.228712\n",
      "ep 3202: ep_len:500 episode reward: total was 1.810000. running mean: -19.018325\n",
      "ep 3202: ep_len:633 episode reward: total was 28.910000. running mean: -18.539042\n",
      "ep 3202: ep_len:1122 episode reward: total was -30.820000. running mean: -18.661851\n",
      "ep 3202: ep_len:705 episode reward: total was 27.310000. running mean: -18.202133\n",
      "ep 3202: ep_len:500 episode reward: total was 58.360000. running mean: -17.436511\n",
      "ep 3202: ep_len:83 episode reward: total was 38.500000. running mean: -16.877146\n",
      "ep 3202: ep_len:97 episode reward: total was 47.000000. running mean: -16.238375\n",
      "ep 3202: ep_len:29 episode reward: total was 11.500000. running mean: -15.960991\n",
      "ep 3202: ep_len:952 episode reward: total was -38.370000. running mean: -16.185081\n",
      "ep 3202: ep_len:2903 episode reward: total was -19.310000. running mean: -16.216330\n",
      "epsilon:0.009992 episode_count: 48178. steps_count: 51858500.000000\n",
      "ep 3203: ep_len:2268 episode reward: total was -211.960000. running mean: -18.173767\n",
      "ep 3203: ep_len:771 episode reward: total was 10.010000. running mean: -17.891929\n",
      "ep 3203: ep_len:30 episode reward: total was 13.500000. running mean: -17.578010\n",
      "ep 3203: ep_len:2964 episode reward: total was -15.470000. running mean: -17.556930\n",
      "ep 3203: ep_len:1668 episode reward: total was -60.260000. running mean: -17.983961\n",
      "ep 3203: ep_len:50 episode reward: total was 23.500000. running mean: -17.569121\n",
      "ep 3203: ep_len:500 episode reward: total was 52.580000. running mean: -16.867630\n",
      "ep 3203: ep_len:674 episode reward: total was 27.360000. running mean: -16.425354\n",
      "ep 3203: ep_len:4318 episode reward: total was -912.160000. running mean: -25.382700\n",
      "ep 3203: ep_len:717 episode reward: total was 36.620000. running mean: -24.762673\n",
      "ep 3203: ep_len:542 episode reward: total was -4.970000. running mean: -24.564746\n",
      "ep 3203: ep_len:151 episode reward: total was 71.000000. running mean: -23.609099\n",
      "ep 3203: ep_len:45 episode reward: total was 21.000000. running mean: -23.163008\n",
      "ep 3203: ep_len:634 episode reward: total was 27.290000. running mean: -22.658478\n",
      "ep 3203: ep_len:2898 episode reward: total was -12.780000. running mean: -22.559693\n",
      "ep 3203: ep_len:31 episode reward: total was 14.000000. running mean: -22.194096\n",
      "epsilon:0.009992 episode_count: 48194. steps_count: 51876761.000000\n",
      "ep 3204: ep_len:675 episode reward: total was 0.440000. running mean: -21.967755\n",
      "ep 3204: ep_len:815 episode reward: total was -0.420000. running mean: -21.752278\n",
      "ep 3204: ep_len:51 episode reward: total was 24.000000. running mean: -21.294755\n",
      "ep 3204: ep_len:2882 episode reward: total was 0.570000. running mean: -21.076107\n",
      "ep 3204: ep_len:850 episode reward: total was 54.070000. running mean: -20.324646\n",
      "ep 3204: ep_len:43 episode reward: total was 18.500000. running mean: -19.936400\n",
      "ep 3204: ep_len:680 episode reward: total was -4.000000. running mean: -19.777036\n",
      "ep 3204: ep_len:318 episode reward: total was 11.380000. running mean: -19.465465\n",
      "ep 3204: ep_len:978 episode reward: total was -20.990000. running mean: -19.480711\n",
      "ep 3204: ep_len:655 episode reward: total was 8.830000. running mean: -19.197604\n",
      "ep 3204: ep_len:686 episode reward: total was 12.840000. running mean: -18.877228\n",
      "ep 3204: ep_len:784 episode reward: total was -7.480000. running mean: -18.763255\n",
      "ep 3204: ep_len:2685 episode reward: total was -30.340000. running mean: -18.879023\n",
      "epsilon:0.009992 episode_count: 48207. steps_count: 51888863.000000\n",
      "ep 3205: ep_len:915 episode reward: total was 26.060000. running mean: -18.429633\n",
      "ep 3205: ep_len:500 episode reward: total was 11.510000. running mean: -18.130236\n",
      "ep 3205: ep_len:82 episode reward: total was 39.500000. running mean: -17.553934\n",
      "ep 3205: ep_len:2942 episode reward: total was -22.460000. running mean: -17.602994\n",
      "ep 3205: ep_len:864 episode reward: total was 25.400000. running mean: -17.172965\n",
      "ep 3205: ep_len:101 episode reward: total was 47.500000. running mean: -16.526235\n",
      "ep 3205: ep_len:847 episode reward: total was 29.850000. running mean: -16.062473\n",
      "ep 3205: ep_len:4004 episode reward: total was -95.440000. running mean: -16.856248\n",
      "ep 3205: ep_len:622 episode reward: total was -38.910000. running mean: -17.076785\n",
      "ep 3205: ep_len:632 episode reward: total was -10.470000. running mean: -17.010717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3205: ep_len:918 episode reward: total was 25.660000. running mean: -16.584010\n",
      "ep 3205: ep_len:850 episode reward: total was 9.890000. running mean: -16.319270\n",
      "ep 3205: ep_len:2803 episode reward: total was -2.010000. running mean: -16.176178\n",
      "ep 3205: ep_len:48 episode reward: total was 21.000000. running mean: -15.804416\n",
      "epsilon:0.009992 episode_count: 48221. steps_count: 51904991.000000\n",
      "ep 3206: ep_len:2490 episode reward: total was -933.760000. running mean: -24.983972\n",
      "ep 3206: ep_len:731 episode reward: total was -24.400000. running mean: -24.978132\n",
      "ep 3206: ep_len:61 episode reward: total was 26.000000. running mean: -24.468351\n",
      "ep 3206: ep_len:3013 episode reward: total was -0.760000. running mean: -24.231267\n",
      "ep 3206: ep_len:778 episode reward: total was -7.540000. running mean: -24.064354\n",
      "ep 3206: ep_len:104 episode reward: total was 49.000000. running mean: -23.333711\n",
      "ep 3206: ep_len:87 episode reward: total was 40.500000. running mean: -22.695374\n",
      "ep 3206: ep_len:72 episode reward: total was 33.000000. running mean: -22.138420\n",
      "ep 3206: ep_len:998 episode reward: total was 9.820000. running mean: -21.818836\n",
      "ep 3206: ep_len:4089 episode reward: total was -123.510000. running mean: -22.835747\n",
      "ep 3206: ep_len:566 episode reward: total was -3.600000. running mean: -22.643390\n",
      "ep 3206: ep_len:622 episode reward: total was 12.490000. running mean: -22.292056\n",
      "ep 3206: ep_len:622 episode reward: total was -11.610000. running mean: -22.185235\n",
      "ep 3206: ep_len:82 episode reward: total was 39.500000. running mean: -21.568383\n",
      "ep 3206: ep_len:47 episode reward: total was 22.000000. running mean: -21.132699\n",
      "ep 3206: ep_len:74 episode reward: total was 35.500000. running mean: -20.566372\n",
      "ep 3206: ep_len:1041 episode reward: total was 25.240000. running mean: -20.108309\n",
      "ep 3206: ep_len:2862 episode reward: total was -16.020000. running mean: -20.067426\n",
      "ep 3206: ep_len:58 episode reward: total was 27.500000. running mean: -19.591751\n",
      "epsilon:0.009992 episode_count: 48240. steps_count: 51923388.000000\n",
      "ep 3207: ep_len:500 episode reward: total was -2.840000. running mean: -19.424234\n",
      "ep 3207: ep_len:500 episode reward: total was -3.880000. running mean: -19.268791\n",
      "ep 3207: ep_len:3001 episode reward: total was -23.270000. running mean: -19.308803\n",
      "ep 3207: ep_len:802 episode reward: total was 16.540000. running mean: -18.950315\n",
      "ep 3207: ep_len:89 episode reward: total was 43.000000. running mean: -18.330812\n",
      "ep 3207: ep_len:500 episode reward: total was 35.860000. running mean: -17.788904\n",
      "ep 3207: ep_len:355 episode reward: total was 16.340000. running mean: -17.447615\n",
      "ep 3207: ep_len:549 episode reward: total was 14.570000. running mean: -17.127439\n",
      "ep 3207: ep_len:723 episode reward: total was 1.950000. running mean: -16.936665\n",
      "ep 3207: ep_len:618 episode reward: total was -7.120000. running mean: -16.838498\n",
      "ep 3207: ep_len:43 episode reward: total was 20.000000. running mean: -16.470113\n",
      "ep 3207: ep_len:124 episode reward: total was 60.500000. running mean: -15.700412\n",
      "ep 3207: ep_len:2400 episode reward: total was -269.070000. running mean: -18.234108\n",
      "ep 3207: ep_len:2880 episode reward: total was -15.900000. running mean: -18.210767\n",
      "ep 3207: ep_len:27 episode reward: total was 12.000000. running mean: -17.908659\n",
      "epsilon:0.009992 episode_count: 48255. steps_count: 51936499.000000\n",
      "ep 3208: ep_len:810 episode reward: total was -20.350000. running mean: -17.933072\n",
      "ep 3208: ep_len:682 episode reward: total was -22.000000. running mean: -17.973742\n",
      "ep 3208: ep_len:2999 episode reward: total was -48.190000. running mean: -18.275904\n",
      "ep 3208: ep_len:766 episode reward: total was -23.820000. running mean: -18.331345\n",
      "ep 3208: ep_len:100 episode reward: total was 47.000000. running mean: -17.678032\n",
      "ep 3208: ep_len:58 episode reward: total was 27.500000. running mean: -17.226251\n",
      "ep 3208: ep_len:1150 episode reward: total was -22.210000. running mean: -17.276089\n",
      "ep 3208: ep_len:4033 episode reward: total was -89.900000. running mean: -18.002328\n",
      "ep 3208: ep_len:598 episode reward: total was 16.930000. running mean: -17.653005\n",
      "ep 3208: ep_len:659 episode reward: total was -142.910000. running mean: -18.905575\n",
      "ep 3208: ep_len:641 episode reward: total was 27.160000. running mean: -18.444919\n",
      "ep 3208: ep_len:87 episode reward: total was 40.500000. running mean: -17.855470\n",
      "ep 3208: ep_len:648 episode reward: total was 11.270000. running mean: -17.564215\n",
      "ep 3208: ep_len:2861 episode reward: total was 12.930000. running mean: -17.259273\n",
      "ep 3208: ep_len:41 episode reward: total was 19.000000. running mean: -16.896680\n",
      "epsilon:0.009992 episode_count: 48270. steps_count: 51952632.000000\n",
      "ep 3209: ep_len:1110 episode reward: total was -24.940000. running mean: -16.977113\n",
      "ep 3209: ep_len:670 episode reward: total was -16.730000. running mean: -16.974642\n",
      "ep 3209: ep_len:2935 episode reward: total was -72.800000. running mean: -17.532896\n",
      "ep 3209: ep_len:1050 episode reward: total was -539.620000. running mean: -22.753767\n",
      "ep 3209: ep_len:51 episode reward: total was 24.000000. running mean: -22.286229\n",
      "ep 3209: ep_len:1092 episode reward: total was -58.940000. running mean: -22.652767\n",
      "ep 3209: ep_len:4144 episode reward: total was -74.200000. running mean: -23.168239\n",
      "ep 3209: ep_len:1615 episode reward: total was -122.310000. running mean: -24.159657\n",
      "ep 3209: ep_len:871 episode reward: total was 48.420000. running mean: -23.433860\n",
      "ep 3209: ep_len:500 episode reward: total was 39.500000. running mean: -22.804522\n",
      "ep 3209: ep_len:214 episode reward: total was 98.000000. running mean: -21.596476\n",
      "ep 3209: ep_len:657 episode reward: total was -4.060000. running mean: -21.421112\n",
      "ep 3209: ep_len:2865 episode reward: total was -7.880000. running mean: -21.285701\n",
      "epsilon:0.009992 episode_count: 48283. steps_count: 51970406.000000\n",
      "ep 3210: ep_len:1013 episode reward: total was -75.890000. running mean: -21.831744\n",
      "ep 3210: ep_len:500 episode reward: total was -5.960000. running mean: -21.673026\n",
      "ep 3210: ep_len:2904 episode reward: total was -95.970000. running mean: -22.415996\n",
      "ep 3210: ep_len:500 episode reward: total was -1.000000. running mean: -22.201836\n",
      "ep 3210: ep_len:42 episode reward: total was 18.000000. running mean: -21.799818\n",
      "ep 3210: ep_len:87 episode reward: total was 42.000000. running mean: -21.161819\n",
      "ep 3210: ep_len:61 episode reward: total was 29.000000. running mean: -20.660201\n",
      "ep 3210: ep_len:1057 episode reward: total was -53.720000. running mean: -20.990799\n",
      "ep 3210: ep_len:4103 episode reward: total was -147.760000. running mean: -22.258491\n",
      "ep 3210: ep_len:536 episode reward: total was 3.080000. running mean: -22.005106\n",
      "ep 3210: ep_len:671 episode reward: total was 28.720000. running mean: -21.497855\n",
      "ep 3210: ep_len:593 episode reward: total was -11.900000. running mean: -21.401877\n",
      "ep 3210: ep_len:61 episode reward: total was 29.000000. running mean: -20.897858\n",
      "ep 3210: ep_len:203 episode reward: total was 98.500000. running mean: -19.703879\n",
      "ep 3210: ep_len:97 episode reward: total was 45.500000. running mean: -19.051841\n",
      "ep 3210: ep_len:1135 episode reward: total was -34.270000. running mean: -19.204022\n",
      "ep 3210: ep_len:2946 episode reward: total was 8.700000. running mean: -18.924982\n",
      "ep 3210: ep_len:48 episode reward: total was 21.000000. running mean: -18.525732\n",
      "epsilon:0.009992 episode_count: 48301. steps_count: 51986963.000000\n",
      "ep 3211: ep_len:1117 episode reward: total was -99.580000. running mean: -19.336275\n",
      "ep 3211: ep_len:754 episode reward: total was 2.850000. running mean: -19.114412\n",
      "ep 3211: ep_len:3074 episode reward: total was -12.110000. running mean: -19.044368\n",
      "ep 3211: ep_len:4365 episode reward: total was -802.810000. running mean: -26.882024\n",
      "ep 3211: ep_len:34 episode reward: total was 14.000000. running mean: -26.473204\n",
      "ep 3211: ep_len:653 episode reward: total was 4.340000. running mean: -26.165072\n",
      "ep 3211: ep_len:670 episode reward: total was 24.140000. running mean: -25.662021\n",
      "ep 3211: ep_len:1604 episode reward: total was -67.970000. running mean: -26.085101\n",
      "ep 3211: ep_len:770 episode reward: total was 25.820000. running mean: -25.566050\n",
      "ep 3211: ep_len:570 episode reward: total was 0.040000. running mean: -25.309989\n",
      "ep 3211: ep_len:35 episode reward: total was 16.000000. running mean: -24.896890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3211: ep_len:1175 episode reward: total was -48.010000. running mean: -25.128021\n",
      "ep 3211: ep_len:2912 episode reward: total was 1.780000. running mean: -24.858940\n",
      "ep 3211: ep_len:38 episode reward: total was 17.500000. running mean: -24.435351\n",
      "epsilon:0.009992 episode_count: 48315. steps_count: 52004734.000000\n",
      "ep 3212: ep_len:1071 episode reward: total was -22.790000. running mean: -24.418898\n",
      "ep 3212: ep_len:1632 episode reward: total was -112.150000. running mean: -25.296209\n",
      "ep 3212: ep_len:84 episode reward: total was 40.500000. running mean: -24.638247\n",
      "ep 3212: ep_len:3163 episode reward: total was -30.800000. running mean: -24.699864\n",
      "ep 3212: ep_len:1406 episode reward: total was -8.000000. running mean: -24.532865\n",
      "ep 3212: ep_len:61 episode reward: total was 29.000000. running mean: -23.997537\n",
      "ep 3212: ep_len:968 episode reward: total was -4.630000. running mean: -23.803861\n",
      "ep 3212: ep_len:3886 episode reward: total was -94.740000. running mean: -24.513223\n",
      "ep 3212: ep_len:769 episode reward: total was -60.990000. running mean: -24.877991\n",
      "ep 3212: ep_len:748 episode reward: total was 2.020000. running mean: -24.609011\n",
      "ep 3212: ep_len:500 episode reward: total was 50.370000. running mean: -23.859221\n",
      "ep 3212: ep_len:93 episode reward: total was 42.000000. running mean: -23.200628\n",
      "ep 3212: ep_len:129 episode reward: total was 63.000000. running mean: -22.338622\n",
      "ep 3212: ep_len:80 episode reward: total was 37.000000. running mean: -21.745236\n",
      "ep 3212: ep_len:770 episode reward: total was -61.640000. running mean: -22.144183\n",
      "ep 3212: ep_len:2952 episode reward: total was -6.980000. running mean: -21.992542\n",
      "ep 3212: ep_len:67 episode reward: total was 32.000000. running mean: -21.452616\n",
      "epsilon:0.009992 episode_count: 48332. steps_count: 52023113.000000\n",
      "ep 3213: ep_len:1143 episode reward: total was -5.910000. running mean: -21.297190\n",
      "ep 3213: ep_len:200 episode reward: total was 6.430000. running mean: -21.019918\n",
      "ep 3213: ep_len:3003 episode reward: total was 0.130000. running mean: -20.808419\n",
      "ep 3213: ep_len:1249 episode reward: total was -59.390000. running mean: -21.194235\n",
      "ep 3213: ep_len:46 episode reward: total was 20.000000. running mean: -20.782292\n",
      "ep 3213: ep_len:500 episode reward: total was -1.560000. running mean: -20.590070\n",
      "ep 3213: ep_len:357 episode reward: total was 14.800000. running mean: -20.236169\n",
      "ep 3213: ep_len:1570 episode reward: total was -98.050000. running mean: -21.014307\n",
      "ep 3213: ep_len:645 episode reward: total was 29.830000. running mean: -20.505864\n",
      "ep 3213: ep_len:1506 episode reward: total was -21.010000. running mean: -20.510905\n",
      "ep 3213: ep_len:164 episode reward: total was 79.000000. running mean: -19.515796\n",
      "ep 3213: ep_len:41 episode reward: total was 19.000000. running mean: -19.130638\n",
      "ep 3213: ep_len:109 episode reward: total was 53.000000. running mean: -18.409332\n",
      "ep 3213: ep_len:1443 episode reward: total was 20.260000. running mean: -18.022639\n",
      "ep 3213: ep_len:46 episode reward: total was 21.500000. running mean: -17.627412\n",
      "epsilon:0.009992 episode_count: 48347. steps_count: 52035135.000000\n",
      "ep 3214: ep_len:713 episode reward: total was -67.780000. running mean: -18.128938\n",
      "ep 3214: ep_len:689 episode reward: total was -59.940000. running mean: -18.547049\n",
      "ep 3214: ep_len:2982 episode reward: total was -3.540000. running mean: -18.396978\n",
      "ep 3214: ep_len:1138 episode reward: total was -13.030000. running mean: -18.343309\n",
      "ep 3214: ep_len:95 episode reward: total was 41.500000. running mean: -17.744875\n",
      "ep 3214: ep_len:71 episode reward: total was 34.000000. running mean: -17.227427\n",
      "ep 3214: ep_len:659 episode reward: total was 3.390000. running mean: -17.021252\n",
      "ep 3214: ep_len:322 episode reward: total was 0.890000. running mean: -16.842140\n",
      "ep 3214: ep_len:1304 episode reward: total was -99.240000. running mean: -17.666118\n",
      "ep 3214: ep_len:803 episode reward: total was 5.420000. running mean: -17.435257\n",
      "ep 3214: ep_len:707 episode reward: total was 21.360000. running mean: -17.047305\n",
      "ep 3214: ep_len:73 episode reward: total was 33.500000. running mean: -16.541832\n",
      "ep 3214: ep_len:101 episode reward: total was 49.000000. running mean: -15.886413\n",
      "ep 3214: ep_len:1003 episode reward: total was -94.940000. running mean: -16.676949\n",
      "ep 3214: ep_len:2835 episode reward: total was -13.470000. running mean: -16.644880\n",
      "epsilon:0.009992 episode_count: 48362. steps_count: 52048630.000000\n",
      "ep 3215: ep_len:645 episode reward: total was -18.940000. running mean: -16.667831\n",
      "ep 3215: ep_len:717 episode reward: total was -27.510000. running mean: -16.776253\n",
      "ep 3215: ep_len:3008 episode reward: total was -36.730000. running mean: -16.975790\n",
      "ep 3215: ep_len:662 episode reward: total was -5.150000. running mean: -16.857532\n",
      "ep 3215: ep_len:54 episode reward: total was 25.500000. running mean: -16.433957\n",
      "ep 3215: ep_len:574 episode reward: total was 24.580000. running mean: -16.023817\n",
      "ep 3215: ep_len:655 episode reward: total was 37.580000. running mean: -15.487779\n",
      "ep 3215: ep_len:1234 episode reward: total was 1.770000. running mean: -15.315201\n",
      "ep 3215: ep_len:7374 episode reward: total was 49.550000. running mean: -14.666549\n",
      "ep 3215: ep_len:726 episode reward: total was -36.930000. running mean: -14.889184\n",
      "ep 3215: ep_len:97 episode reward: total was 44.000000. running mean: -14.300292\n",
      "ep 3215: ep_len:614 episode reward: total was 10.600000. running mean: -14.051289\n",
      "ep 3215: ep_len:2809 episode reward: total was -8.410000. running mean: -13.994876\n",
      "ep 3215: ep_len:64 episode reward: total was 30.500000. running mean: -13.549927\n",
      "epsilon:0.009992 episode_count: 48376. steps_count: 52067863.000000\n",
      "ep 3216: ep_len:644 episode reward: total was -18.370000. running mean: -13.598128\n",
      "ep 3216: ep_len:980 episode reward: total was -39.950000. running mean: -13.861647\n",
      "ep 3216: ep_len:2950 episode reward: total was -52.530000. running mean: -14.248330\n",
      "ep 3216: ep_len:658 episode reward: total was 22.330000. running mean: -13.882547\n",
      "ep 3216: ep_len:1840 episode reward: total was -260.410000. running mean: -16.347822\n",
      "ep 3216: ep_len:354 episode reward: total was 9.110000. running mean: -16.093243\n",
      "ep 3216: ep_len:914 episode reward: total was -31.430000. running mean: -16.246611\n",
      "ep 3216: ep_len:7523 episode reward: total was 9.970000. running mean: -15.984445\n",
      "ep 3216: ep_len:500 episode reward: total was -8.320000. running mean: -15.907800\n",
      "ep 3216: ep_len:52 episode reward: total was 23.000000. running mean: -15.518722\n",
      "ep 3216: ep_len:676 episode reward: total was -19.290000. running mean: -15.556435\n",
      "ep 3216: ep_len:2890 episode reward: total was -14.880000. running mean: -15.549671\n",
      "epsilon:0.009992 episode_count: 48388. steps_count: 52087844.000000\n",
      "ep 3217: ep_len:1446 episode reward: total was 0.090000. running mean: -15.393274\n",
      "ep 3217: ep_len:182 episode reward: total was 18.190000. running mean: -15.057441\n",
      "ep 3217: ep_len:43 episode reward: total was 20.000000. running mean: -14.706867\n",
      "ep 3217: ep_len:2972 episode reward: total was -18.250000. running mean: -14.742298\n",
      "ep 3217: ep_len:1306 episode reward: total was -559.290000. running mean: -20.187775\n",
      "ep 3217: ep_len:66 episode reward: total was 28.500000. running mean: -19.700898\n",
      "ep 3217: ep_len:103 episode reward: total was 48.500000. running mean: -19.018889\n",
      "ep 3217: ep_len:47 episode reward: total was 22.000000. running mean: -18.608700\n",
      "ep 3217: ep_len:500 episode reward: total was 33.870000. running mean: -18.083913\n",
      "ep 3217: ep_len:320 episode reward: total was 8.770000. running mean: -17.815374\n",
      "ep 3217: ep_len:913 episode reward: total was -46.590000. running mean: -18.103120\n",
      "ep 3217: ep_len:7167 episode reward: total was 30.180000. running mean: -17.620289\n",
      "ep 3217: ep_len:579 episode reward: total was -28.140000. running mean: -17.725486\n",
      "ep 3217: ep_len:87 episode reward: total was 42.000000. running mean: -17.128231\n",
      "ep 3217: ep_len:165 episode reward: total was 81.000000. running mean: -16.146949\n",
      "ep 3217: ep_len:639 episode reward: total was 2.130000. running mean: -15.964179\n",
      "ep 3217: ep_len:2901 episode reward: total was 8.310000. running mean: -15.721437\n",
      "epsilon:0.009992 episode_count: 48405. steps_count: 52107280.000000\n",
      "ep 3218: ep_len:1134 episode reward: total was 23.030000. running mean: -15.333923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3218: ep_len:735 episode reward: total was -32.440000. running mean: -15.504984\n",
      "ep 3218: ep_len:3017 episode reward: total was -33.870000. running mean: -15.688634\n",
      "ep 3218: ep_len:854 episode reward: total was 32.770000. running mean: -15.204048\n",
      "ep 3218: ep_len:33 episode reward: total was 15.000000. running mean: -14.902007\n",
      "ep 3218: ep_len:63 episode reward: total was 28.500000. running mean: -14.467987\n",
      "ep 3218: ep_len:1389 episode reward: total was 6.160000. running mean: -14.261707\n",
      "ep 3218: ep_len:4175 episode reward: total was -266.310000. running mean: -16.782190\n",
      "ep 3218: ep_len:770 episode reward: total was -31.010000. running mean: -16.924468\n",
      "ep 3218: ep_len:666 episode reward: total was 25.880000. running mean: -16.496423\n",
      "ep 3218: ep_len:500 episode reward: total was -12.370000. running mean: -16.455159\n",
      "ep 3218: ep_len:91 episode reward: total was 44.000000. running mean: -15.850608\n",
      "ep 3218: ep_len:745 episode reward: total was -66.880000. running mean: -16.360902\n",
      "ep 3218: ep_len:2914 episode reward: total was -0.590000. running mean: -16.203193\n",
      "epsilon:0.009992 episode_count: 48419. steps_count: 52124366.000000\n",
      "ep 3219: ep_len:1139 episode reward: total was 15.270000. running mean: -15.888461\n",
      "ep 3219: ep_len:955 episode reward: total was 6.390000. running mean: -15.665676\n",
      "ep 3219: ep_len:3060 episode reward: total was -58.120000. running mean: -16.090219\n",
      "ep 3219: ep_len:1247 episode reward: total was -20.130000. running mean: -16.130617\n",
      "ep 3219: ep_len:99 episode reward: total was 48.000000. running mean: -15.489311\n",
      "ep 3219: ep_len:1435 episode reward: total was -127.150000. running mean: -16.605918\n",
      "ep 3219: ep_len:3928 episode reward: total was -131.110000. running mean: -17.750959\n",
      "ep 3219: ep_len:1547 episode reward: total was -27.580000. running mean: -17.849249\n",
      "ep 3219: ep_len:7237 episode reward: total was 36.690000. running mean: -17.303857\n",
      "ep 3219: ep_len:951 episode reward: total was 2.580000. running mean: -17.105018\n",
      "ep 3219: ep_len:1027 episode reward: total was -115.630000. running mean: -18.090268\n",
      "ep 3219: ep_len:2811 episode reward: total was -1.870000. running mean: -17.928065\n",
      "ep 3219: ep_len:69 episode reward: total was 33.000000. running mean: -17.418784\n",
      "epsilon:0.009992 episode_count: 48432. steps_count: 52149871.000000\n",
      "ep 3220: ep_len:822 episode reward: total was -21.240000. running mean: -17.456997\n",
      "ep 3220: ep_len:695 episode reward: total was -18.900000. running mean: -17.471427\n",
      "ep 3220: ep_len:101 episode reward: total was 49.000000. running mean: -16.806712\n",
      "ep 3220: ep_len:659 episode reward: total was -28.930000. running mean: -16.927945\n",
      "ep 3220: ep_len:993 episode reward: total was -11.450000. running mean: -16.873166\n",
      "ep 3220: ep_len:4175 episode reward: total was -61.250000. running mean: -17.316934\n",
      "ep 3220: ep_len:1571 episode reward: total was 5.720000. running mean: -17.086565\n",
      "ep 3220: ep_len:779 episode reward: total was 24.650000. running mean: -16.669199\n",
      "ep 3220: ep_len:1527 episode reward: total was 0.100000. running mean: -16.501507\n",
      "ep 3220: ep_len:51 episode reward: total was 24.000000. running mean: -16.096492\n",
      "ep 3220: ep_len:106 episode reward: total was 50.000000. running mean: -15.435527\n",
      "ep 3220: ep_len:1065 episode reward: total was 16.000000. running mean: -15.121172\n",
      "ep 3220: ep_len:2816 episode reward: total was -7.850000. running mean: -15.048460\n",
      "epsilon:0.009992 episode_count: 48445. steps_count: 52165231.000000\n",
      "ep 3221: ep_len:500 episode reward: total was 13.260000. running mean: -14.765376\n",
      "ep 3221: ep_len:691 episode reward: total was -16.610000. running mean: -14.783822\n",
      "ep 3221: ep_len:56 episode reward: total was 26.500000. running mean: -14.370984\n",
      "ep 3221: ep_len:3061 episode reward: total was 4.050000. running mean: -14.186774\n",
      "ep 3221: ep_len:629 episode reward: total was 12.290000. running mean: -13.922006\n",
      "ep 3221: ep_len:500 episode reward: total was 12.250000. running mean: -13.660286\n",
      "ep 3221: ep_len:3932 episode reward: total was -90.760000. running mean: -14.431283\n",
      "ep 3221: ep_len:1262 episode reward: total was -35.020000. running mean: -14.637170\n",
      "ep 3221: ep_len:670 episode reward: total was 28.860000. running mean: -14.202199\n",
      "ep 3221: ep_len:631 episode reward: total was -18.000000. running mean: -14.240177\n",
      "ep 3221: ep_len:1201 episode reward: total was 11.790000. running mean: -13.979875\n",
      "ep 3221: ep_len:2830 episode reward: total was -5.260000. running mean: -13.892676\n",
      "epsilon:0.009992 episode_count: 48457. steps_count: 52181194.000000\n",
      "ep 3222: ep_len:691 episode reward: total was 20.410000. running mean: -13.549649\n",
      "ep 3222: ep_len:500 episode reward: total was 8.790000. running mean: -13.326253\n",
      "ep 3222: ep_len:3006 episode reward: total was -40.270000. running mean: -13.595690\n",
      "ep 3222: ep_len:505 episode reward: total was -0.040000. running mean: -13.460133\n",
      "ep 3222: ep_len:767 episode reward: total was -45.020000. running mean: -13.775732\n",
      "ep 3222: ep_len:653 episode reward: total was 23.050000. running mean: -13.407475\n",
      "ep 3222: ep_len:554 episode reward: total was -4.730000. running mean: -13.320700\n",
      "ep 3222: ep_len:699 episode reward: total was 23.790000. running mean: -12.949593\n",
      "ep 3222: ep_len:1012 episode reward: total was 47.360000. running mean: -12.346497\n",
      "ep 3222: ep_len:77 episode reward: total was 37.000000. running mean: -11.853032\n",
      "ep 3222: ep_len:98 episode reward: total was 46.000000. running mean: -11.274502\n",
      "ep 3222: ep_len:659 episode reward: total was -6.190000. running mean: -11.223657\n",
      "ep 3222: ep_len:2944 episode reward: total was -41.210000. running mean: -11.523520\n",
      "epsilon:0.009992 episode_count: 48470. steps_count: 52193359.000000\n",
      "ep 3223: ep_len:651 episode reward: total was -11.420000. running mean: -11.522485\n",
      "ep 3223: ep_len:609 episode reward: total was -40.540000. running mean: -11.812660\n",
      "ep 3223: ep_len:3040 episode reward: total was -107.430000. running mean: -12.768834\n",
      "ep 3223: ep_len:710 episode reward: total was -18.320000. running mean: -12.824345\n",
      "ep 3223: ep_len:66 episode reward: total was 31.500000. running mean: -12.381102\n",
      "ep 3223: ep_len:804 episode reward: total was 26.510000. running mean: -11.992191\n",
      "ep 3223: ep_len:4083 episode reward: total was -2450.220000. running mean: -36.374469\n",
      "ep 3223: ep_len:629 episode reward: total was -4.990000. running mean: -36.060624\n",
      "ep 3223: ep_len:799 episode reward: total was 19.040000. running mean: -35.509618\n",
      "ep 3223: ep_len:884 episode reward: total was 24.100000. running mean: -34.913522\n",
      "ep 3223: ep_len:35 episode reward: total was 16.000000. running mean: -34.404386\n",
      "ep 3223: ep_len:500 episode reward: total was -3.000000. running mean: -34.090343\n",
      "ep 3223: ep_len:2923 episode reward: total was 8.990000. running mean: -33.659539\n",
      "epsilon:0.009992 episode_count: 48483. steps_count: 52209092.000000\n",
      "ep 3224: ep_len:1145 episode reward: total was -56.880000. running mean: -33.891744\n",
      "ep 3224: ep_len:739 episode reward: total was -17.510000. running mean: -33.727926\n",
      "ep 3224: ep_len:34 episode reward: total was 11.000000. running mean: -33.280647\n",
      "ep 3224: ep_len:3003 episode reward: total was -30.730000. running mean: -33.255141\n",
      "ep 3224: ep_len:692 episode reward: total was 16.170000. running mean: -32.760889\n",
      "ep 3224: ep_len:95 episode reward: total was 43.000000. running mean: -32.003280\n",
      "ep 3224: ep_len:67 episode reward: total was 30.500000. running mean: -31.378248\n",
      "ep 3224: ep_len:1469 episode reward: total was 13.450000. running mean: -30.929965\n",
      "ep 3224: ep_len:365 episode reward: total was 10.200000. running mean: -30.518665\n",
      "ep 3224: ep_len:779 episode reward: total was -59.260000. running mean: -30.806079\n",
      "ep 3224: ep_len:649 episode reward: total was -3.690000. running mean: -30.534918\n",
      "ep 3224: ep_len:881 episode reward: total was 33.070000. running mean: -29.898869\n",
      "ep 3224: ep_len:42 episode reward: total was 19.500000. running mean: -29.404880\n",
      "ep 3224: ep_len:1486 episode reward: total was 18.800000. running mean: -28.922831\n",
      "ep 3224: ep_len:2874 episode reward: total was -29.030000. running mean: -28.923903\n",
      "epsilon:0.009992 episode_count: 48498. steps_count: 52223412.000000\n",
      "ep 3225: ep_len:1122 episode reward: total was -4.650000. running mean: -28.681164\n",
      "ep 3225: ep_len:500 episode reward: total was -1.470000. running mean: -28.409052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3225: ep_len:3069 episode reward: total was -44.870000. running mean: -28.573662\n",
      "ep 3225: ep_len:606 episode reward: total was -11.280000. running mean: -28.400725\n",
      "ep 3225: ep_len:87 episode reward: total was 42.000000. running mean: -27.696718\n",
      "ep 3225: ep_len:500 episode reward: total was -1.280000. running mean: -27.432551\n",
      "ep 3225: ep_len:651 episode reward: total was 25.600000. running mean: -26.902225\n",
      "ep 3225: ep_len:1553 episode reward: total was -7.450000. running mean: -26.707703\n",
      "ep 3225: ep_len:723 episode reward: total was 7.070000. running mean: -26.369926\n",
      "ep 3225: ep_len:676 episode reward: total was -1.410000. running mean: -26.120327\n",
      "ep 3225: ep_len:58 episode reward: total was 27.500000. running mean: -25.584123\n",
      "ep 3225: ep_len:123 episode reward: total was 55.500000. running mean: -24.773282\n",
      "ep 3225: ep_len:1028 episode reward: total was -46.730000. running mean: -24.992849\n",
      "ep 3225: ep_len:2840 episode reward: total was -897.280000. running mean: -33.715721\n",
      "ep 3225: ep_len:62 episode reward: total was 28.000000. running mean: -33.098564\n",
      "epsilon:0.009992 episode_count: 48513. steps_count: 52237010.000000\n",
      "ep 3226: ep_len:953 episode reward: total was -72.700000. running mean: -33.494578\n",
      "ep 3226: ep_len:729 episode reward: total was -16.070000. running mean: -33.320332\n",
      "ep 3226: ep_len:3065 episode reward: total was -74.050000. running mean: -33.727629\n",
      "ep 3226: ep_len:819 episode reward: total was 16.310000. running mean: -33.227253\n",
      "ep 3226: ep_len:138 episode reward: total was 64.500000. running mean: -32.249980\n",
      "ep 3226: ep_len:1022 episode reward: total was 6.230000. running mean: -31.865180\n",
      "ep 3226: ep_len:323 episode reward: total was -0.570000. running mean: -31.552229\n",
      "ep 3226: ep_len:584 episode reward: total was -9.480000. running mean: -31.331506\n",
      "ep 3226: ep_len:666 episode reward: total was 38.620000. running mean: -30.631991\n",
      "ep 3226: ep_len:677 episode reward: total was 8.890000. running mean: -30.236771\n",
      "ep 3226: ep_len:679 episode reward: total was -15.290000. running mean: -30.087304\n",
      "ep 3226: ep_len:2859 episode reward: total was -85.460000. running mean: -30.641031\n",
      "epsilon:0.009992 episode_count: 48525. steps_count: 52249524.000000\n",
      "ep 3227: ep_len:1496 episode reward: total was 6.280000. running mean: -30.271820\n",
      "ep 3227: ep_len:650 episode reward: total was -15.890000. running mean: -30.128002\n",
      "ep 3227: ep_len:2921 episode reward: total was -20.480000. running mean: -30.031522\n",
      "ep 3227: ep_len:2163 episode reward: total was -1337.140000. running mean: -43.102607\n",
      "ep 3227: ep_len:110 episode reward: total was 53.500000. running mean: -42.136581\n",
      "ep 3227: ep_len:69 episode reward: total was 33.000000. running mean: -41.385215\n",
      "ep 3227: ep_len:1487 episode reward: total was 10.810000. running mean: -40.863263\n",
      "ep 3227: ep_len:367 episode reward: total was 18.300000. running mean: -40.271630\n",
      "ep 3227: ep_len:1460 episode reward: total was -21.510000. running mean: -40.084014\n",
      "ep 3227: ep_len:840 episode reward: total was 46.370000. running mean: -39.219474\n",
      "ep 3227: ep_len:1565 episode reward: total was 30.580000. running mean: -38.521479\n",
      "ep 3227: ep_len:198 episode reward: total was 91.500000. running mean: -37.221264\n",
      "ep 3227: ep_len:119 episode reward: total was 58.000000. running mean: -36.269052\n",
      "ep 3227: ep_len:1095 episode reward: total was -26.590000. running mean: -36.172261\n",
      "ep 3227: ep_len:2827 episode reward: total was -104.730000. running mean: -36.857838\n",
      "ep 3227: ep_len:48 episode reward: total was 19.500000. running mean: -36.294260\n",
      "epsilon:0.009992 episode_count: 48541. steps_count: 52266939.000000\n",
      "ep 3228: ep_len:628 episode reward: total was -15.560000. running mean: -36.086917\n",
      "ep 3228: ep_len:948 episode reward: total was -4.310000. running mean: -35.769148\n",
      "ep 3228: ep_len:101 episode reward: total was 49.000000. running mean: -34.921457\n",
      "ep 3228: ep_len:692 episode reward: total was 1.600000. running mean: -34.556242\n",
      "ep 3228: ep_len:500 episode reward: total was 15.500000. running mean: -34.055680\n",
      "ep 3228: ep_len:4243 episode reward: total was -1389.030000. running mean: -47.605423\n",
      "ep 3228: ep_len:573 episode reward: total was -85.830000. running mean: -47.987669\n",
      "ep 3228: ep_len:7282 episode reward: total was -69.270000. running mean: -48.200492\n",
      "ep 3228: ep_len:1179 episode reward: total was -4.020000. running mean: -47.758687\n",
      "ep 3228: ep_len:85 episode reward: total was 39.500000. running mean: -46.886100\n",
      "ep 3228: ep_len:124 episode reward: total was 59.000000. running mean: -45.827239\n",
      "ep 3228: ep_len:90 episode reward: total was 43.500000. running mean: -44.933967\n",
      "ep 3228: ep_len:1094 episode reward: total was -42.550000. running mean: -44.910127\n",
      "ep 3228: ep_len:2906 episode reward: total was -49.120000. running mean: -44.952226\n",
      "ep 3228: ep_len:55 episode reward: total was 26.000000. running mean: -44.242704\n",
      "epsilon:0.009992 episode_count: 48556. steps_count: 52287439.000000\n",
      "ep 3229: ep_len:594 episode reward: total was -11.400000. running mean: -43.914277\n",
      "ep 3229: ep_len:860 episode reward: total was 1.650000. running mean: -43.458634\n",
      "ep 3229: ep_len:2977 episode reward: total was -16.330000. running mean: -43.187348\n",
      "ep 3229: ep_len:1473 episode reward: total was -4.170000. running mean: -42.797174\n",
      "ep 3229: ep_len:47 episode reward: total was 22.000000. running mean: -42.149202\n",
      "ep 3229: ep_len:105 episode reward: total was 49.500000. running mean: -41.232710\n",
      "ep 3229: ep_len:59 episode reward: total was 28.000000. running mean: -40.540383\n",
      "ep 3229: ep_len:880 episode reward: total was 27.150000. running mean: -39.863479\n",
      "ep 3229: ep_len:3947 episode reward: total was -131.370000. running mean: -40.778545\n",
      "ep 3229: ep_len:796 episode reward: total was -33.230000. running mean: -40.703059\n",
      "ep 3229: ep_len:792 episode reward: total was -2.010000. running mean: -40.316129\n",
      "ep 3229: ep_len:554 episode reward: total was -11.680000. running mean: -40.029767\n",
      "ep 3229: ep_len:89 episode reward: total was 40.000000. running mean: -39.229470\n",
      "ep 3229: ep_len:196 episode reward: total was 89.000000. running mean: -37.947175\n",
      "ep 3229: ep_len:86 episode reward: total was 41.500000. running mean: -37.152703\n",
      "ep 3229: ep_len:1436 episode reward: total was 12.720000. running mean: -36.653976\n",
      "ep 3229: ep_len:2834 episode reward: total was 2.980000. running mean: -36.257636\n",
      "epsilon:0.009992 episode_count: 48573. steps_count: 52305164.000000\n",
      "ep 3230: ep_len:963 episode reward: total was -96.370000. running mean: -36.858760\n",
      "ep 3230: ep_len:1235 episode reward: total was -52.370000. running mean: -37.013872\n",
      "ep 3230: ep_len:3015 episode reward: total was -58.920000. running mean: -37.232934\n",
      "ep 3230: ep_len:634 episode reward: total was -6.840000. running mean: -36.929004\n",
      "ep 3230: ep_len:45 episode reward: total was 21.000000. running mean: -36.349714\n",
      "ep 3230: ep_len:924 episode reward: total was 80.230000. running mean: -35.183917\n",
      "ep 3230: ep_len:4041 episode reward: total was -357.040000. running mean: -38.402478\n",
      "ep 3230: ep_len:1578 episode reward: total was -28.870000. running mean: -38.307153\n",
      "ep 3230: ep_len:920 episode reward: total was 78.950000. running mean: -37.134582\n",
      "ep 3230: ep_len:652 episode reward: total was 12.670000. running mean: -36.636536\n",
      "ep 3230: ep_len:37 episode reward: total was 17.000000. running mean: -36.100170\n",
      "ep 3230: ep_len:860 episode reward: total was 0.780000. running mean: -35.731369\n",
      "ep 3230: ep_len:2828 episode reward: total was -4.760000. running mean: -35.421655\n",
      "ep 3230: ep_len:27 episode reward: total was 12.000000. running mean: -34.947439\n",
      "epsilon:0.009992 episode_count: 48587. steps_count: 52322923.000000\n",
      "ep 3231: ep_len:1013 episode reward: total was -119.320000. running mean: -35.791164\n",
      "ep 3231: ep_len:715 episode reward: total was -31.400000. running mean: -35.747252\n",
      "ep 3231: ep_len:2913 episode reward: total was -25.950000. running mean: -35.649280\n",
      "ep 3231: ep_len:500 episode reward: total was 13.500000. running mean: -35.157787\n",
      "ep 3231: ep_len:988 episode reward: total was 2.370000. running mean: -34.782509\n",
      "ep 3231: ep_len:4075 episode reward: total was -218.380000. running mean: -36.618484\n",
      "ep 3231: ep_len:1168 episode reward: total was -27.760000. running mean: -36.529899\n",
      "ep 3231: ep_len:679 episode reward: total was 2.530000. running mean: -36.139300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3231: ep_len:613 episode reward: total was 12.470000. running mean: -35.653207\n",
      "ep 3231: ep_len:155 episode reward: total was 76.000000. running mean: -34.536675\n",
      "ep 3231: ep_len:702 episode reward: total was -23.450000. running mean: -34.425809\n",
      "ep 3231: ep_len:2817 episode reward: total was -82.450000. running mean: -34.906050\n",
      "ep 3231: ep_len:45 episode reward: total was 21.000000. running mean: -34.346990\n",
      "epsilon:0.009992 episode_count: 48600. steps_count: 52339306.000000\n",
      "ep 3232: ep_len:891 episode reward: total was -54.030000. running mean: -34.543820\n",
      "ep 3232: ep_len:813 episode reward: total was -15.690000. running mean: -34.355282\n",
      "ep 3232: ep_len:3041 episode reward: total was 7.460000. running mean: -33.937129\n",
      "ep 3232: ep_len:851 episode reward: total was 65.990000. running mean: -32.937858\n",
      "ep 3232: ep_len:81 episode reward: total was 39.000000. running mean: -32.218479\n",
      "ep 3232: ep_len:725 episode reward: total was -15.140000. running mean: -32.047694\n",
      "ep 3232: ep_len:3944 episode reward: total was -30.040000. running mean: -32.027617\n",
      "ep 3232: ep_len:950 episode reward: total was -3.460000. running mean: -31.741941\n",
      "ep 3232: ep_len:773 episode reward: total was 9.280000. running mean: -31.331722\n",
      "ep 3232: ep_len:1505 episode reward: total was -5.170000. running mean: -31.070105\n",
      "ep 3232: ep_len:168 episode reward: total was 76.500000. running mean: -29.994404\n",
      "ep 3232: ep_len:1214 episode reward: total was -7.610000. running mean: -29.770560\n",
      "ep 3232: ep_len:46 episode reward: total was 21.010000. running mean: -29.262754\n",
      "epsilon:0.009992 episode_count: 48613. steps_count: 52354308.000000\n",
      "ep 3233: ep_len:636 episode reward: total was -17.040000. running mean: -29.140526\n",
      "ep 3233: ep_len:710 episode reward: total was -29.570000. running mean: -29.144821\n",
      "ep 3233: ep_len:44 episode reward: total was 19.000000. running mean: -28.663373\n",
      "ep 3233: ep_len:2984 episode reward: total was -7.510000. running mean: -28.451839\n",
      "ep 3233: ep_len:656 episode reward: total was 17.120000. running mean: -27.996121\n",
      "ep 3233: ep_len:52 episode reward: total was 23.000000. running mean: -27.486160\n",
      "ep 3233: ep_len:2859 episode reward: total was -378.960000. running mean: -31.000898\n",
      "ep 3233: ep_len:660 episode reward: total was 17.580000. running mean: -30.515089\n",
      "ep 3233: ep_len:565 episode reward: total was -5.630000. running mean: -30.266238\n",
      "ep 3233: ep_len:7365 episode reward: total was 44.010000. running mean: -29.523476\n",
      "ep 3233: ep_len:1448 episode reward: total was -19.000000. running mean: -29.418241\n",
      "ep 3233: ep_len:62 episode reward: total was 29.500000. running mean: -28.829059\n",
      "ep 3233: ep_len:184 episode reward: total was 87.500000. running mean: -27.665768\n",
      "ep 3233: ep_len:54 episode reward: total was 25.500000. running mean: -27.134110\n",
      "ep 3233: ep_len:1222 episode reward: total was -8.570000. running mean: -26.948469\n",
      "ep 3233: ep_len:2841 episode reward: total was -11.060000. running mean: -26.789585\n",
      "epsilon:0.009992 episode_count: 48629. steps_count: 52376650.000000\n",
      "ep 3234: ep_len:944 episode reward: total was -9.940000. running mean: -26.621089\n",
      "ep 3234: ep_len:931 episode reward: total was 20.220000. running mean: -26.152678\n",
      "ep 3234: ep_len:60 episode reward: total was 24.000000. running mean: -25.651151\n",
      "ep 3234: ep_len:3008 episode reward: total was -11.240000. running mean: -25.507039\n",
      "ep 3234: ep_len:659 episode reward: total was -9.200000. running mean: -25.343969\n",
      "ep 3234: ep_len:52 episode reward: total was 24.500000. running mean: -24.845529\n",
      "ep 3234: ep_len:145 episode reward: total was 71.000000. running mean: -23.887074\n",
      "ep 3234: ep_len:50 episode reward: total was 23.500000. running mean: -23.413203\n",
      "ep 3234: ep_len:33 episode reward: total was 15.000000. running mean: -23.029071\n",
      "ep 3234: ep_len:998 episode reward: total was -44.730000. running mean: -23.246081\n",
      "ep 3234: ep_len:4455 episode reward: total was -344.710000. running mean: -26.460720\n",
      "ep 3234: ep_len:589 episode reward: total was 9.770000. running mean: -26.098413\n",
      "ep 3234: ep_len:777 episode reward: total was 16.210000. running mean: -25.675328\n",
      "ep 3234: ep_len:741 episode reward: total was -9.500000. running mean: -25.513575\n",
      "ep 3234: ep_len:53 episode reward: total was 23.500000. running mean: -25.023439\n",
      "ep 3234: ep_len:704 episode reward: total was 3.360000. running mean: -24.739605\n",
      "ep 3234: ep_len:2752 episode reward: total was 0.970000. running mean: -24.482509\n",
      "epsilon:0.009992 episode_count: 48646. steps_count: 52393601.000000\n",
      "ep 3235: ep_len:1201 episode reward: total was 12.830000. running mean: -24.109384\n",
      "ep 3235: ep_len:721 episode reward: total was -3.230000. running mean: -23.900590\n",
      "ep 3235: ep_len:73 episode reward: total was 35.000000. running mean: -23.311584\n",
      "ep 3235: ep_len:3012 episode reward: total was 26.700000. running mean: -22.811468\n",
      "ep 3235: ep_len:573 episode reward: total was -27.770000. running mean: -22.861054\n",
      "ep 3235: ep_len:80 episode reward: total was 37.000000. running mean: -22.262443\n",
      "ep 3235: ep_len:1405 episode reward: total was 10.570000. running mean: -21.934119\n",
      "ep 3235: ep_len:323 episode reward: total was 24.500000. running mean: -21.469778\n",
      "ep 3235: ep_len:665 episode reward: total was 3.450000. running mean: -21.220580\n",
      "ep 3235: ep_len:813 episode reward: total was 39.750000. running mean: -20.610874\n",
      "ep 3235: ep_len:1499 episode reward: total was 3.340000. running mean: -20.371365\n",
      "ep 3235: ep_len:62 episode reward: total was 26.500000. running mean: -19.902652\n",
      "ep 3235: ep_len:91 episode reward: total was 44.000000. running mean: -19.263625\n",
      "ep 3235: ep_len:606 episode reward: total was -3.380000. running mean: -19.104789\n",
      "ep 3235: ep_len:2905 episode reward: total was 6.450000. running mean: -18.849241\n",
      "epsilon:0.009992 episode_count: 48661. steps_count: 52407630.000000\n",
      "ep 3236: ep_len:1086 episode reward: total was -1.270000. running mean: -18.673448\n",
      "ep 3236: ep_len:694 episode reward: total was -24.540000. running mean: -18.732114\n",
      "ep 3236: ep_len:77 episode reward: total was 37.000000. running mean: -18.174793\n",
      "ep 3236: ep_len:3055 episode reward: total was 26.910000. running mean: -17.723945\n",
      "ep 3236: ep_len:500 episode reward: total was 12.100000. running mean: -17.425705\n",
      "ep 3236: ep_len:51 episode reward: total was 24.000000. running mean: -17.011448\n",
      "ep 3236: ep_len:136 episode reward: total was 65.000000. running mean: -16.191334\n",
      "ep 3236: ep_len:769 episode reward: total was 19.760000. running mean: -15.831821\n",
      "ep 3236: ep_len:3939 episode reward: total was -215.410000. running mean: -17.827602\n",
      "ep 3236: ep_len:767 episode reward: total was -48.310000. running mean: -18.132426\n",
      "ep 3236: ep_len:879 episode reward: total was 71.530000. running mean: -17.235802\n",
      "ep 3236: ep_len:1482 episode reward: total was 12.420000. running mean: -16.939244\n",
      "ep 3236: ep_len:77 episode reward: total was 35.500000. running mean: -16.414852\n",
      "ep 3236: ep_len:636 episode reward: total was -17.070000. running mean: -16.421403\n",
      "ep 3236: ep_len:2772 episode reward: total was -15.480000. running mean: -16.411989\n",
      "epsilon:0.009992 episode_count: 48676. steps_count: 52424550.000000\n",
      "ep 3237: ep_len:684 episode reward: total was -12.520000. running mean: -16.373069\n",
      "ep 3237: ep_len:1607 episode reward: total was -75.500000. running mean: -16.964339\n",
      "ep 3237: ep_len:2983 episode reward: total was -30.250000. running mean: -17.097195\n",
      "ep 3237: ep_len:1095 episode reward: total was -6.390000. running mean: -16.990123\n",
      "ep 3237: ep_len:29 episode reward: total was 13.000000. running mean: -16.690222\n",
      "ep 3237: ep_len:863 episode reward: total was 28.940000. running mean: -16.233920\n",
      "ep 3237: ep_len:352 episode reward: total was 7.590000. running mean: -15.995681\n",
      "ep 3237: ep_len:3899 episode reward: total was -1128.740000. running mean: -27.123124\n",
      "ep 3237: ep_len:919 episode reward: total was 73.000000. running mean: -26.121892\n",
      "ep 3237: ep_len:632 episode reward: total was 8.960000. running mean: -25.771074\n",
      "ep 3237: ep_len:639 episode reward: total was 7.150000. running mean: -25.441863\n",
      "ep 3237: ep_len:2868 episode reward: total was -12.900000. running mean: -25.316444\n",
      "epsilon:0.009992 episode_count: 48688. steps_count: 52441120.000000\n",
      "ep 3238: ep_len:1017 episode reward: total was -15.250000. running mean: -25.215780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3238: ep_len:772 episode reward: total was -18.710000. running mean: -25.150722\n",
      "ep 3238: ep_len:77 episode reward: total was 34.000000. running mean: -24.559215\n",
      "ep 3238: ep_len:101 episode reward: total was 47.500000. running mean: -23.838623\n",
      "ep 3238: ep_len:621 episode reward: total was -8.100000. running mean: -23.681236\n",
      "ep 3238: ep_len:49 episode reward: total was 21.500000. running mean: -23.229424\n",
      "ep 3238: ep_len:1122 episode reward: total was -6.550000. running mean: -23.062630\n",
      "ep 3238: ep_len:4000 episode reward: total was -153.620000. running mean: -24.368203\n",
      "ep 3238: ep_len:686 episode reward: total was -44.230000. running mean: -24.566821\n",
      "ep 3238: ep_len:734 episode reward: total was 39.580000. running mean: -23.925353\n",
      "ep 3238: ep_len:1119 episode reward: total was -6.150000. running mean: -23.747600\n",
      "ep 3238: ep_len:62 episode reward: total was 26.500000. running mean: -23.245124\n",
      "ep 3238: ep_len:127 episode reward: total was 60.500000. running mean: -22.407672\n",
      "ep 3238: ep_len:44 episode reward: total was 19.000000. running mean: -21.993596\n",
      "ep 3238: ep_len:1096 episode reward: total was 7.120000. running mean: -21.702460\n",
      "ep 3238: ep_len:2886 episode reward: total was -30.900000. running mean: -21.794435\n",
      "epsilon:0.009992 episode_count: 48704. steps_count: 52455633.000000\n",
      "ep 3239: ep_len:774 episode reward: total was -70.110000. running mean: -22.277591\n",
      "ep 3239: ep_len:500 episode reward: total was 11.180000. running mean: -21.943015\n",
      "ep 3239: ep_len:2957 episode reward: total was -18.860000. running mean: -21.912185\n",
      "ep 3239: ep_len:543 episode reward: total was -0.800000. running mean: -21.701063\n",
      "ep 3239: ep_len:40 episode reward: total was 18.500000. running mean: -21.299052\n",
      "ep 3239: ep_len:53 episode reward: total was 25.000000. running mean: -20.836062\n",
      "ep 3239: ep_len:689 episode reward: total was -15.290000. running mean: -20.780601\n",
      "ep 3239: ep_len:343 episode reward: total was 23.750000. running mean: -20.335295\n",
      "ep 3239: ep_len:920 episode reward: total was -45.480000. running mean: -20.586742\n",
      "ep 3239: ep_len:7434 episode reward: total was -402.480000. running mean: -24.405675\n",
      "ep 3239: ep_len:681 episode reward: total was -5.090000. running mean: -24.212518\n",
      "ep 3239: ep_len:97 episode reward: total was 45.500000. running mean: -23.515393\n",
      "ep 3239: ep_len:73 episode reward: total was 32.000000. running mean: -22.960239\n",
      "ep 3239: ep_len:1155 episode reward: total was -8.820000. running mean: -22.818837\n",
      "ep 3239: ep_len:2879 episode reward: total was -51.080000. running mean: -23.101448\n",
      "epsilon:0.009992 episode_count: 48719. steps_count: 52474771.000000\n",
      "ep 3240: ep_len:1101 episode reward: total was 1.750000. running mean: -22.852934\n",
      "ep 3240: ep_len:955 episode reward: total was 3.780000. running mean: -22.586604\n",
      "ep 3240: ep_len:61 episode reward: total was 29.000000. running mean: -22.070738\n",
      "ep 3240: ep_len:3062 episode reward: total was -14.220000. running mean: -21.992231\n",
      "ep 3240: ep_len:564 episode reward: total was -10.410000. running mean: -21.876409\n",
      "ep 3240: ep_len:41 episode reward: total was 19.000000. running mean: -21.467645\n",
      "ep 3240: ep_len:98 episode reward: total was 44.500000. running mean: -20.807968\n",
      "ep 3240: ep_len:500 episode reward: total was 15.160000. running mean: -20.448288\n",
      "ep 3240: ep_len:352 episode reward: total was 17.750000. running mean: -20.066305\n",
      "ep 3240: ep_len:1196 episode reward: total was -0.720000. running mean: -19.872842\n",
      "ep 3240: ep_len:593 episode reward: total was -17.380000. running mean: -19.847914\n",
      "ep 3240: ep_len:669 episode reward: total was 9.310000. running mean: -19.556335\n",
      "ep 3240: ep_len:89 episode reward: total was 43.000000. running mean: -18.930772\n",
      "ep 3240: ep_len:46 episode reward: total was 21.500000. running mean: -18.526464\n",
      "ep 3240: ep_len:123 episode reward: total was 58.500000. running mean: -17.756199\n",
      "ep 3240: ep_len:634 episode reward: total was 0.610000. running mean: -17.572537\n",
      "ep 3240: ep_len:2834 episode reward: total was -24.960000. running mean: -17.646412\n",
      "epsilon:0.009992 episode_count: 48736. steps_count: 52487689.000000\n",
      "ep 3241: ep_len:1055 episode reward: total was 10.600000. running mean: -17.363948\n",
      "ep 3241: ep_len:975 episode reward: total was 0.960000. running mean: -17.180708\n",
      "ep 3241: ep_len:3039 episode reward: total was -19.680000. running mean: -17.205701\n",
      "ep 3241: ep_len:538 episode reward: total was -101.850000. running mean: -18.052144\n",
      "ep 3241: ep_len:131 episode reward: total was 59.500000. running mean: -17.276623\n",
      "ep 3241: ep_len:45 episode reward: total was 21.000000. running mean: -16.893856\n",
      "ep 3241: ep_len:1479 episode reward: total was -148.920000. running mean: -18.214118\n",
      "ep 3241: ep_len:653 episode reward: total was 12.670000. running mean: -17.905277\n",
      "ep 3241: ep_len:1287 episode reward: total was -116.610000. running mean: -18.892324\n",
      "ep 3241: ep_len:755 episode reward: total was 20.860000. running mean: -18.494801\n",
      "ep 3241: ep_len:589 episode reward: total was 13.670000. running mean: -18.173153\n",
      "ep 3241: ep_len:142 episode reward: total was 63.500000. running mean: -17.356421\n",
      "ep 3241: ep_len:792 episode reward: total was -0.870000. running mean: -17.191557\n",
      "ep 3241: ep_len:2869 episode reward: total was -15.360000. running mean: -17.173241\n",
      "epsilon:0.009992 episode_count: 48750. steps_count: 52502038.000000\n",
      "ep 3242: ep_len:1082 episode reward: total was -30.270000. running mean: -17.304209\n",
      "ep 3242: ep_len:1605 episode reward: total was -61.740000. running mean: -17.748567\n",
      "ep 3242: ep_len:70 episode reward: total was 32.000000. running mean: -17.251081\n",
      "ep 3242: ep_len:3003 episode reward: total was 13.320000. running mean: -16.945370\n",
      "ep 3242: ep_len:680 episode reward: total was 9.250000. running mean: -16.683417\n",
      "ep 3242: ep_len:52 episode reward: total was 23.000000. running mean: -16.286583\n",
      "ep 3242: ep_len:160 episode reward: total was 77.000000. running mean: -15.353717\n",
      "ep 3242: ep_len:50 episode reward: total was 22.000000. running mean: -14.980180\n",
      "ep 3242: ep_len:1096 episode reward: total was -10.360000. running mean: -14.933978\n",
      "ep 3242: ep_len:662 episode reward: total was 27.670000. running mean: -14.507938\n",
      "ep 3242: ep_len:656 episode reward: total was -85.520000. running mean: -15.218059\n",
      "ep 3242: ep_len:683 episode reward: total was 24.790000. running mean: -14.817978\n",
      "ep 3242: ep_len:654 episode reward: total was 10.060000. running mean: -14.569198\n",
      "ep 3242: ep_len:65 episode reward: total was 29.500000. running mean: -14.128506\n",
      "ep 3242: ep_len:1154 episode reward: total was -4.790000. running mean: -14.035121\n",
      "ep 3242: ep_len:2875 episode reward: total was -17.330000. running mean: -14.068070\n",
      "epsilon:0.009992 episode_count: 48766. steps_count: 52516585.000000\n",
      "ep 3243: ep_len:3725 episode reward: total was -702.240000. running mean: -20.949789\n",
      "ep 3243: ep_len:679 episode reward: total was -17.620000. running mean: -20.916491\n",
      "ep 3243: ep_len:2944 episode reward: total was 3.850000. running mean: -20.668826\n",
      "ep 3243: ep_len:511 episode reward: total was 19.930000. running mean: -20.262838\n",
      "ep 3243: ep_len:106 episode reward: total was 47.000000. running mean: -19.590210\n",
      "ep 3243: ep_len:500 episode reward: total was 6.830000. running mean: -19.326008\n",
      "ep 3243: ep_len:4049 episode reward: total was -90.610000. running mean: -20.038848\n",
      "ep 3243: ep_len:564 episode reward: total was -48.060000. running mean: -20.319059\n",
      "ep 3243: ep_len:674 episode reward: total was 14.260000. running mean: -19.973269\n",
      "ep 3243: ep_len:1001 episode reward: total was 19.550000. running mean: -19.578036\n",
      "ep 3243: ep_len:77 episode reward: total was 37.000000. running mean: -19.012256\n",
      "ep 3243: ep_len:197 episode reward: total was 94.000000. running mean: -17.882133\n",
      "ep 3243: ep_len:1080 episode reward: total was -15.630000. running mean: -17.859612\n",
      "ep 3243: ep_len:2768 episode reward: total was 9.120000. running mean: -17.589816\n",
      "ep 3243: ep_len:61 episode reward: total was 27.500000. running mean: -17.138917\n",
      "epsilon:0.009992 episode_count: 48781. steps_count: 52535521.000000\n",
      "ep 3244: ep_len:3661 episode reward: total was -628.860000. running mean: -23.256128\n",
      "ep 3244: ep_len:213 episode reward: total was 10.970000. running mean: -22.913867\n",
      "ep 3244: ep_len:99 episode reward: total was 45.000000. running mean: -22.234728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3244: ep_len:901 episode reward: total was 66.800000. running mean: -21.344381\n",
      "ep 3244: ep_len:40 episode reward: total was 18.500000. running mean: -20.945937\n",
      "ep 3244: ep_len:100 episode reward: total was 47.000000. running mean: -20.266478\n",
      "ep 3244: ep_len:55 episode reward: total was 20.000000. running mean: -19.863813\n",
      "ep 3244: ep_len:2787 episode reward: total was -258.130000. running mean: -22.246475\n",
      "ep 3244: ep_len:3850 episode reward: total was -60.760000. running mean: -22.631610\n",
      "ep 3244: ep_len:1552 episode reward: total was -18.470000. running mean: -22.589994\n",
      "ep 3244: ep_len:767 episode reward: total was 25.020000. running mean: -22.113894\n",
      "ep 3244: ep_len:1019 episode reward: total was 23.190000. running mean: -21.660855\n",
      "ep 3244: ep_len:85 episode reward: total was 39.500000. running mean: -21.049247\n",
      "ep 3244: ep_len:151 episode reward: total was 75.010000. running mean: -20.088654\n",
      "ep 3244: ep_len:57 episode reward: total was 25.500000. running mean: -19.632768\n",
      "ep 3244: ep_len:103 episode reward: total was 47.000000. running mean: -18.966440\n",
      "ep 3244: ep_len:987 episode reward: total was -45.850000. running mean: -19.235276\n",
      "ep 3244: ep_len:2846 episode reward: total was -12.600000. running mean: -19.168923\n",
      "ep 3244: ep_len:42 episode reward: total was 19.500000. running mean: -18.782234\n",
      "epsilon:0.009992 episode_count: 48800. steps_count: 52554836.000000\n",
      "ep 3245: ep_len:1085 episode reward: total was 0.580000. running mean: -18.588611\n",
      "ep 3245: ep_len:500 episode reward: total was 18.800000. running mean: -18.214725\n",
      "ep 3245: ep_len:70 episode reward: total was 33.500000. running mean: -17.697578\n",
      "ep 3245: ep_len:2781 episode reward: total was -54.630000. running mean: -18.066902\n",
      "ep 3245: ep_len:842 episode reward: total was 38.000000. running mean: -17.506233\n",
      "ep 3245: ep_len:43 episode reward: total was 20.000000. running mean: -17.131171\n",
      "ep 3245: ep_len:500 episode reward: total was 28.690000. running mean: -16.672959\n",
      "ep 3245: ep_len:4228 episode reward: total was -38.410000. running mean: -16.890329\n",
      "ep 3245: ep_len:1260 episode reward: total was -36.050000. running mean: -17.081926\n",
      "ep 3245: ep_len:673 episode reward: total was 27.540000. running mean: -16.635707\n",
      "ep 3245: ep_len:624 episode reward: total was 14.360000. running mean: -16.325750\n",
      "ep 3245: ep_len:60 episode reward: total was 28.500000. running mean: -15.877492\n",
      "ep 3245: ep_len:84 episode reward: total was 40.500000. running mean: -15.313717\n",
      "ep 3245: ep_len:674 episode reward: total was -0.170000. running mean: -15.162280\n",
      "ep 3245: ep_len:2750 episode reward: total was -22.190000. running mean: -15.232557\n",
      "epsilon:0.009992 episode_count: 48815. steps_count: 52571010.000000\n",
      "ep 3246: ep_len:703 episode reward: total was -51.720000. running mean: -15.597432\n",
      "ep 3246: ep_len:725 episode reward: total was 7.740000. running mean: -15.364057\n",
      "ep 3246: ep_len:2997 episode reward: total was -66.210000. running mean: -15.872517\n",
      "ep 3246: ep_len:682 episode reward: total was 24.880000. running mean: -15.464992\n",
      "ep 3246: ep_len:1390 episode reward: total was -128.240000. running mean: -16.592742\n",
      "ep 3246: ep_len:625 episode reward: total was 21.820000. running mean: -16.208614\n",
      "ep 3246: ep_len:827 episode reward: total was 13.180000. running mean: -15.914728\n",
      "ep 3246: ep_len:861 episode reward: total was 62.620000. running mean: -15.129381\n",
      "ep 3246: ep_len:1072 episode reward: total was -36.360000. running mean: -15.341687\n",
      "ep 3246: ep_len:682 episode reward: total was -4.460000. running mean: -15.232870\n",
      "ep 3246: ep_len:2885 episode reward: total was 1.630000. running mean: -15.064242\n",
      "ep 3246: ep_len:69 episode reward: total was 31.500000. running mean: -14.598599\n",
      "epsilon:0.009992 episode_count: 48827. steps_count: 52584528.000000\n",
      "ep 3247: ep_len:601 episode reward: total was 7.760000. running mean: -14.375013\n",
      "ep 3247: ep_len:500 episode reward: total was 22.200000. running mean: -14.009263\n",
      "ep 3247: ep_len:3043 episode reward: total was -92.160000. running mean: -14.790770\n",
      "ep 3247: ep_len:693 episode reward: total was 11.400000. running mean: -14.528863\n",
      "ep 3247: ep_len:57 episode reward: total was 27.000000. running mean: -14.113574\n",
      "ep 3247: ep_len:138 episode reward: total was 66.000000. running mean: -13.312438\n",
      "ep 3247: ep_len:1128 episode reward: total was -0.000000. running mean: -13.179314\n",
      "ep 3247: ep_len:343 episode reward: total was 10.770000. running mean: -12.939821\n",
      "ep 3247: ep_len:591 episode reward: total was -12.440000. running mean: -12.934823\n",
      "ep 3247: ep_len:725 episode reward: total was 35.510000. running mean: -12.450374\n",
      "ep 3247: ep_len:1506 episode reward: total was 2.890000. running mean: -12.296971\n",
      "ep 3247: ep_len:78 episode reward: total was 36.000000. running mean: -11.814001\n",
      "ep 3247: ep_len:610 episode reward: total was -18.800000. running mean: -11.883861\n",
      "ep 3247: ep_len:2858 episode reward: total was 12.290000. running mean: -11.642122\n",
      "ep 3247: ep_len:53 episode reward: total was 23.500000. running mean: -11.290701\n",
      "epsilon:0.009992 episode_count: 48842. steps_count: 52597452.000000\n",
      "ep 3248: ep_len:737 episode reward: total was -62.490000. running mean: -11.802694\n",
      "ep 3248: ep_len:752 episode reward: total was -5.760000. running mean: -11.742267\n",
      "ep 3248: ep_len:45 episode reward: total was 18.000000. running mean: -11.444844\n",
      "ep 3248: ep_len:3031 episode reward: total was -49.450000. running mean: -11.824896\n",
      "ep 3248: ep_len:862 episode reward: total was 43.720000. running mean: -11.269447\n",
      "ep 3248: ep_len:53 episode reward: total was 25.000000. running mean: -10.906753\n",
      "ep 3248: ep_len:886 episode reward: total was 54.770000. running mean: -10.249985\n",
      "ep 3248: ep_len:646 episode reward: total was 11.590000. running mean: -10.031585\n",
      "ep 3248: ep_len:1257 episode reward: total was -38.100000. running mean: -10.312269\n",
      "ep 3248: ep_len:771 episode reward: total was -1.060000. running mean: -10.219747\n",
      "ep 3248: ep_len:1480 episode reward: total was 15.490000. running mean: -9.962649\n",
      "ep 3248: ep_len:59 episode reward: total was 28.000000. running mean: -9.583023\n",
      "ep 3248: ep_len:103 episode reward: total was 48.500000. running mean: -9.002192\n",
      "ep 3248: ep_len:500 episode reward: total was -1.830000. running mean: -8.930471\n",
      "ep 3248: ep_len:2827 episode reward: total was 2.970000. running mean: -8.811466\n",
      "epsilon:0.009992 episode_count: 48857. steps_count: 52611461.000000\n",
      "ep 3249: ep_len:500 episode reward: total was 8.510000. running mean: -8.638251\n",
      "ep 3249: ep_len:1256 episode reward: total was -57.300000. running mean: -9.124869\n",
      "ep 3249: ep_len:2934 episode reward: total was -61.360000. running mean: -9.647220\n",
      "ep 3249: ep_len:1497 episode reward: total was 15.720000. running mean: -9.393548\n",
      "ep 3249: ep_len:50 episode reward: total was 20.500000. running mean: -9.094612\n",
      "ep 3249: ep_len:737 episode reward: total was -113.510000. running mean: -10.138766\n",
      "ep 3249: ep_len:334 episode reward: total was 22.530000. running mean: -9.812079\n",
      "ep 3249: ep_len:1141 episode reward: total was -21.630000. running mean: -9.930258\n",
      "ep 3249: ep_len:7439 episode reward: total was 45.300000. running mean: -9.377955\n",
      "ep 3249: ep_len:1168 episode reward: total was -8.690000. running mean: -9.371076\n",
      "ep 3249: ep_len:175 episode reward: total was 86.000000. running mean: -8.417365\n",
      "ep 3249: ep_len:44 episode reward: total was 20.500000. running mean: -8.128191\n",
      "ep 3249: ep_len:617 episode reward: total was 14.010000. running mean: -7.906809\n",
      "ep 3249: ep_len:2720 episode reward: total was 10.840000. running mean: -7.719341\n",
      "ep 3249: ep_len:61 episode reward: total was 29.000000. running mean: -7.352148\n",
      "epsilon:0.009992 episode_count: 48872. steps_count: 52632134.000000\n",
      "ep 3250: ep_len:1185 episode reward: total was -17.970000. running mean: -7.458326\n",
      "ep 3250: ep_len:1211 episode reward: total was -33.510000. running mean: -7.718843\n",
      "ep 3250: ep_len:2928 episode reward: total was -65.920000. running mean: -8.300855\n",
      "ep 3250: ep_len:715 episode reward: total was 6.570000. running mean: -8.152146\n",
      "ep 3250: ep_len:101 episode reward: total was 47.500000. running mean: -7.595625\n",
      "ep 3250: ep_len:73 episode reward: total was 33.500000. running mean: -7.184668\n",
      "ep 3250: ep_len:500 episode reward: total was -9.000000. running mean: -7.202822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3250: ep_len:4091 episode reward: total was -76.600000. running mean: -7.896793\n",
      "ep 3250: ep_len:566 episode reward: total was -6.510000. running mean: -7.882926\n",
      "ep 3250: ep_len:712 episode reward: total was 4.810000. running mean: -7.755996\n",
      "ep 3250: ep_len:916 episode reward: total was 20.600000. running mean: -7.472436\n",
      "ep 3250: ep_len:164 episode reward: total was 77.500000. running mean: -6.622712\n",
      "ep 3250: ep_len:93 episode reward: total was 45.000000. running mean: -6.106485\n",
      "ep 3250: ep_len:617 episode reward: total was -9.150000. running mean: -6.136920\n",
      "ep 3250: ep_len:2900 episode reward: total was -33.820000. running mean: -6.413751\n",
      "ep 3250: ep_len:58 episode reward: total was 23.000000. running mean: -6.119613\n",
      "epsilon:0.009992 episode_count: 48888. steps_count: 52648964.000000\n",
      "ep 3251: ep_len:831 episode reward: total was -25.190000. running mean: -6.310317\n",
      "ep 3251: ep_len:712 episode reward: total was -36.340000. running mean: -6.610614\n",
      "ep 3251: ep_len:3062 episode reward: total was -55.570000. running mean: -7.100208\n",
      "ep 3251: ep_len:635 episode reward: total was 15.220000. running mean: -6.877006\n",
      "ep 3251: ep_len:68 episode reward: total was 32.500000. running mean: -6.483236\n",
      "ep 3251: ep_len:70 episode reward: total was 32.000000. running mean: -6.098403\n",
      "ep 3251: ep_len:500 episode reward: total was 26.950000. running mean: -5.767919\n",
      "ep 3251: ep_len:659 episode reward: total was 25.650000. running mean: -5.453740\n",
      "ep 3251: ep_len:2522 episode reward: total was -221.710000. running mean: -7.616303\n",
      "ep 3251: ep_len:760 episode reward: total was 21.520000. running mean: -7.324940\n",
      "ep 3251: ep_len:1111 episode reward: total was -2.130000. running mean: -7.272990\n",
      "ep 3251: ep_len:128 episode reward: total was 62.010000. running mean: -6.580160\n",
      "ep 3251: ep_len:97 episode reward: total was 47.000000. running mean: -6.044359\n",
      "ep 3251: ep_len:565 episode reward: total was -0.760000. running mean: -5.991515\n",
      "ep 3251: ep_len:2899 episode reward: total was -16.540000. running mean: -6.097000\n",
      "ep 3251: ep_len:40 episode reward: total was 18.500000. running mean: -5.851030\n",
      "epsilon:0.009992 episode_count: 48904. steps_count: 52663623.000000\n",
      "ep 3252: ep_len:630 episode reward: total was 7.530000. running mean: -5.717220\n",
      "ep 3252: ep_len:711 episode reward: total was -38.710000. running mean: -6.047148\n",
      "ep 3252: ep_len:3070 episode reward: total was -91.020000. running mean: -6.896876\n",
      "ep 3252: ep_len:783 episode reward: total was 12.950000. running mean: -6.698407\n",
      "ep 3252: ep_len:50 episode reward: total was 23.500000. running mean: -6.396423\n",
      "ep 3252: ep_len:1820 episode reward: total was -132.890000. running mean: -7.661359\n",
      "ep 3252: ep_len:671 episode reward: total was 16.160000. running mean: -7.423145\n",
      "ep 3252: ep_len:723 episode reward: total was -54.080000. running mean: -7.889714\n",
      "ep 3252: ep_len:717 episode reward: total was 28.320000. running mean: -7.527617\n",
      "ep 3252: ep_len:1437 episode reward: total was -3.370000. running mean: -7.486041\n",
      "ep 3252: ep_len:1176 episode reward: total was -14.670000. running mean: -7.557880\n",
      "ep 3252: ep_len:2777 episode reward: total was -33.820000. running mean: -7.820501\n",
      "ep 3252: ep_len:45 episode reward: total was 19.500000. running mean: -7.547296\n",
      "epsilon:0.009992 episode_count: 48917. steps_count: 52678233.000000\n",
      "ep 3253: ep_len:641 episode reward: total was 16.610000. running mean: -7.305723\n",
      "ep 3253: ep_len:1264 episode reward: total was -51.160000. running mean: -7.744266\n",
      "ep 3253: ep_len:2919 episode reward: total was -85.170000. running mean: -8.518524\n",
      "ep 3253: ep_len:780 episode reward: total was -65.060000. running mean: -9.083938\n",
      "ep 3253: ep_len:759 episode reward: total was -12.780000. running mean: -9.120899\n",
      "ep 3253: ep_len:674 episode reward: total was 27.820000. running mean: -8.751490\n",
      "ep 3253: ep_len:869 episode reward: total was -29.860000. running mean: -8.962575\n",
      "ep 3253: ep_len:7143 episode reward: total was 40.530000. running mean: -8.467649\n",
      "ep 3253: ep_len:1164 episode reward: total was 1.370000. running mean: -8.369273\n",
      "ep 3253: ep_len:51 episode reward: total was 24.000000. running mean: -8.045580\n",
      "ep 3253: ep_len:1179 episode reward: total was 2.530000. running mean: -7.939824\n",
      "ep 3253: ep_len:2793 episode reward: total was -9.420000. running mean: -7.954626\n",
      "epsilon:0.009992 episode_count: 48929. steps_count: 52698469.000000\n",
      "ep 3254: ep_len:642 episode reward: total was 21.550000. running mean: -7.659580\n",
      "ep 3254: ep_len:1003 episode reward: total was -5.370000. running mean: -7.636684\n",
      "ep 3254: ep_len:3029 episode reward: total was -69.360000. running mean: -8.253917\n",
      "ep 3254: ep_len:661 episode reward: total was 15.340000. running mean: -8.017978\n",
      "ep 3254: ep_len:41 episode reward: total was 17.500000. running mean: -7.762798\n",
      "ep 3254: ep_len:131 episode reward: total was 62.500000. running mean: -7.060170\n",
      "ep 3254: ep_len:88 episode reward: total was 39.500000. running mean: -6.594569\n",
      "ep 3254: ep_len:500 episode reward: total was 47.690000. running mean: -6.051723\n",
      "ep 3254: ep_len:4010 episode reward: total was -121.850000. running mean: -7.209706\n",
      "ep 3254: ep_len:642 episode reward: total was -11.930000. running mean: -7.256909\n",
      "ep 3254: ep_len:649 episode reward: total was 8.190000. running mean: -7.102439\n",
      "ep 3254: ep_len:725 episode reward: total was -20.290000. running mean: -7.234315\n",
      "ep 3254: ep_len:78 episode reward: total was 37.500000. running mean: -6.786972\n",
      "ep 3254: ep_len:88 episode reward: total was 39.500000. running mean: -6.324102\n",
      "ep 3254: ep_len:1485 episode reward: total was 17.650000. running mean: -6.084361\n",
      "ep 3254: ep_len:2811 episode reward: total was 5.080000. running mean: -5.972718\n",
      "ep 3254: ep_len:58 episode reward: total was 26.000000. running mean: -5.652990\n",
      "epsilon:0.009992 episode_count: 48946. steps_count: 52715110.000000\n",
      "ep 3255: ep_len:939 episode reward: total was -98.900000. running mean: -6.585460\n",
      "ep 3255: ep_len:705 episode reward: total was -39.660000. running mean: -6.916206\n",
      "ep 3255: ep_len:79 episode reward: total was 38.000000. running mean: -6.467044\n",
      "ep 3255: ep_len:3013 episode reward: total was -1591.950000. running mean: -22.321873\n",
      "ep 3255: ep_len:528 episode reward: total was 0.060000. running mean: -22.098055\n",
      "ep 3255: ep_len:38 episode reward: total was 16.000000. running mean: -21.717074\n",
      "ep 3255: ep_len:36 episode reward: total was 16.500000. running mean: -21.334903\n",
      "ep 3255: ep_len:500 episode reward: total was 21.500000. running mean: -20.906554\n",
      "ep 3255: ep_len:631 episode reward: total was 17.380000. running mean: -20.523689\n",
      "ep 3255: ep_len:627 episode reward: total was -5.010000. running mean: -20.368552\n",
      "ep 3255: ep_len:710 episode reward: total was 27.970000. running mean: -19.885166\n",
      "ep 3255: ep_len:702 episode reward: total was 15.400000. running mean: -19.532315\n",
      "ep 3255: ep_len:1455 episode reward: total was 11.870000. running mean: -19.218292\n",
      "ep 3255: ep_len:2834 episode reward: total was 4.700000. running mean: -18.979109\n",
      "epsilon:0.009992 episode_count: 48960. steps_count: 52727907.000000\n",
      "ep 3256: ep_len:641 episode reward: total was -5.940000. running mean: -18.848718\n",
      "ep 3256: ep_len:637 episode reward: total was -15.010000. running mean: -18.810330\n",
      "ep 3256: ep_len:75 episode reward: total was 36.000000. running mean: -18.262227\n",
      "ep 3256: ep_len:529 episode reward: total was -12.050000. running mean: -18.200105\n",
      "ep 3256: ep_len:92 episode reward: total was 43.000000. running mean: -17.588104\n",
      "ep 3256: ep_len:61 episode reward: total was 29.000000. running mean: -17.122223\n",
      "ep 3256: ep_len:631 episode reward: total was -3.960000. running mean: -16.990601\n",
      "ep 3256: ep_len:645 episode reward: total was 16.690000. running mean: -16.653794\n",
      "ep 3256: ep_len:945 episode reward: total was -46.260000. running mean: -16.949857\n",
      "ep 3256: ep_len:639 episode reward: total was 3.100000. running mean: -16.749358\n",
      "ep 3256: ep_len:1533 episode reward: total was 1.090000. running mean: -16.570964\n",
      "ep 3256: ep_len:52 episode reward: total was 24.500000. running mean: -16.160255\n",
      "ep 3256: ep_len:105 episode reward: total was 49.500000. running mean: -15.503652\n",
      "ep 3256: ep_len:844 episode reward: total was -23.540000. running mean: -15.584016\n",
      "ep 3256: ep_len:2898 episode reward: total was -2.590000. running mean: -15.454076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3256: ep_len:60 episode reward: total was 28.500000. running mean: -15.014535\n",
      "epsilon:0.009992 episode_count: 48976. steps_count: 52738294.000000\n",
      "ep 3257: ep_len:1055 episode reward: total was -12.730000. running mean: -14.991689\n",
      "ep 3257: ep_len:1272 episode reward: total was -45.020000. running mean: -15.291973\n",
      "ep 3257: ep_len:2859 episode reward: total was -242.740000. running mean: -17.566453\n",
      "ep 3257: ep_len:791 episode reward: total was -3.390000. running mean: -17.424688\n",
      "ep 3257: ep_len:128 episode reward: total was 59.500000. running mean: -16.655441\n",
      "ep 3257: ep_len:75 episode reward: total was 36.000000. running mean: -16.128887\n",
      "ep 3257: ep_len:500 episode reward: total was 23.890000. running mean: -15.728698\n",
      "ep 3257: ep_len:4236 episode reward: total was -971.200000. running mean: -25.283411\n",
      "ep 3257: ep_len:1261 episode reward: total was -45.130000. running mean: -25.481877\n",
      "ep 3257: ep_len:748 episode reward: total was 19.910000. running mean: -25.027958\n",
      "ep 3257: ep_len:500 episode reward: total was -1.130000. running mean: -24.788979\n",
      "ep 3257: ep_len:54 episode reward: total was 24.000000. running mean: -24.301089\n",
      "ep 3257: ep_len:180 episode reward: total was 85.500000. running mean: -23.203078\n",
      "ep 3257: ep_len:30 episode reward: total was 13.500000. running mean: -22.836047\n",
      "ep 3257: ep_len:646 episode reward: total was 20.400000. running mean: -22.403687\n",
      "ep 3257: ep_len:2857 episode reward: total was -2.630000. running mean: -22.205950\n",
      "ep 3257: ep_len:44 episode reward: total was 20.500000. running mean: -21.778890\n",
      "epsilon:0.009992 episode_count: 48993. steps_count: 52755530.000000\n",
      "ep 3258: ep_len:770 episode reward: total was -79.330000. running mean: -22.354401\n",
      "ep 3258: ep_len:952 episode reward: total was 15.390000. running mean: -21.976957\n",
      "ep 3258: ep_len:3077 episode reward: total was -190.340000. running mean: -23.660588\n",
      "ep 3258: ep_len:795 episode reward: total was 15.830000. running mean: -23.265682\n",
      "ep 3258: ep_len:1013 episode reward: total was -94.070000. running mean: -23.973725\n",
      "ep 3258: ep_len:3651 episode reward: total was -22.260000. running mean: -23.956588\n",
      "ep 3258: ep_len:4198 episode reward: total was -622.460000. running mean: -29.941622\n",
      "ep 3258: ep_len:741 episode reward: total was 36.010000. running mean: -29.282106\n",
      "ep 3258: ep_len:754 episode reward: total was -24.340000. running mean: -29.232685\n",
      "ep 3258: ep_len:81 episode reward: total was 37.500000. running mean: -28.565358\n",
      "ep 3258: ep_len:136 episode reward: total was 66.500000. running mean: -27.614704\n",
      "ep 3258: ep_len:51 episode reward: total was 24.000000. running mean: -27.098557\n",
      "ep 3258: ep_len:95 episode reward: total was 46.000000. running mean: -26.367572\n",
      "ep 3258: ep_len:737 episode reward: total was -39.260000. running mean: -26.496496\n",
      "ep 3258: ep_len:2895 episode reward: total was -0.410000. running mean: -26.235631\n",
      "ep 3258: ep_len:45 episode reward: total was 21.000000. running mean: -25.763275\n",
      "epsilon:0.009992 episode_count: 49009. steps_count: 52775521.000000\n",
      "ep 3259: ep_len:595 episode reward: total was -13.810000. running mean: -25.643742\n",
      "ep 3259: ep_len:198 episode reward: total was -2.830000. running mean: -25.415605\n",
      "ep 3259: ep_len:3085 episode reward: total was -185.040000. running mean: -27.011849\n",
      "ep 3259: ep_len:1277 episode reward: total was -19.620000. running mean: -26.937930\n",
      "ep 3259: ep_len:165 episode reward: total was 78.000000. running mean: -25.888551\n",
      "ep 3259: ep_len:500 episode reward: total was 24.990000. running mean: -25.379765\n",
      "ep 3259: ep_len:4197 episode reward: total was -60.300000. running mean: -25.728968\n",
      "ep 3259: ep_len:645 episode reward: total was -17.220000. running mean: -25.643878\n",
      "ep 3259: ep_len:7264 episode reward: total was -32.850000. running mean: -25.715939\n",
      "ep 3259: ep_len:1418 episode reward: total was -13.050000. running mean: -25.589280\n",
      "ep 3259: ep_len:56 episode reward: total was 26.500000. running mean: -25.068387\n",
      "ep 3259: ep_len:67 episode reward: total was 32.000000. running mean: -24.497703\n",
      "ep 3259: ep_len:1019 episode reward: total was -6.160000. running mean: -24.314326\n",
      "ep 3259: ep_len:2848 episode reward: total was -3.610000. running mean: -24.107283\n",
      "epsilon:0.009992 episode_count: 49023. steps_count: 52798855.000000\n",
      "ep 3260: ep_len:1429 episode reward: total was 5.020000. running mean: -23.816010\n",
      "ep 3260: ep_len:1235 episode reward: total was -43.860000. running mean: -24.016450\n",
      "ep 3260: ep_len:3019 episode reward: total was -74.360000. running mean: -24.519885\n",
      "ep 3260: ep_len:527 episode reward: total was -57.520000. running mean: -24.849887\n",
      "ep 3260: ep_len:124 episode reward: total was 57.500000. running mean: -24.026388\n",
      "ep 3260: ep_len:74 episode reward: total was 34.000000. running mean: -23.446124\n",
      "ep 3260: ep_len:1033 episode reward: total was -3.570000. running mean: -23.247363\n",
      "ep 3260: ep_len:3591 episode reward: total was 1.170000. running mean: -23.003189\n",
      "ep 3260: ep_len:845 episode reward: total was 12.690000. running mean: -22.646257\n",
      "ep 3260: ep_len:802 episode reward: total was 42.460000. running mean: -21.995194\n",
      "ep 3260: ep_len:971 episode reward: total was 26.900000. running mean: -21.506242\n",
      "ep 3260: ep_len:128 episode reward: total was 62.500000. running mean: -20.666180\n",
      "ep 3260: ep_len:1445 episode reward: total was 14.220000. running mean: -20.317318\n",
      "ep 3260: ep_len:2848 episode reward: total was -19.130000. running mean: -20.305445\n",
      "epsilon:0.009992 episode_count: 49037. steps_count: 52816926.000000\n",
      "ep 3261: ep_len:1091 episode reward: total was -25.870000. running mean: -20.361091\n",
      "ep 3261: ep_len:973 episode reward: total was 7.820000. running mean: -20.079280\n",
      "ep 3261: ep_len:2877 episode reward: total was -111.880000. running mean: -20.997287\n",
      "ep 3261: ep_len:731 episode reward: total was -21.140000. running mean: -20.998714\n",
      "ep 3261: ep_len:169 episode reward: total was 81.500000. running mean: -19.973727\n",
      "ep 3261: ep_len:1910 episode reward: total was -135.230000. running mean: -21.126290\n",
      "ep 3261: ep_len:351 episode reward: total was 20.160000. running mean: -20.713427\n",
      "ep 3261: ep_len:848 episode reward: total was -22.610000. running mean: -20.732392\n",
      "ep 3261: ep_len:794 episode reward: total was 62.410000. running mean: -19.900969\n",
      "ep 3261: ep_len:501 episode reward: total was 7.910000. running mean: -19.622859\n",
      "ep 3261: ep_len:199 episode reward: total was 96.500000. running mean: -18.461630\n",
      "ep 3261: ep_len:48 episode reward: total was 22.500000. running mean: -18.052014\n",
      "ep 3261: ep_len:84 episode reward: total was 40.500000. running mean: -17.466494\n",
      "ep 3261: ep_len:1060 episode reward: total was 13.310000. running mean: -17.158729\n",
      "ep 3261: ep_len:2900 episode reward: total was -34.730000. running mean: -17.334442\n",
      "epsilon:0.009992 episode_count: 49052. steps_count: 52831462.000000\n",
      "ep 3262: ep_len:628 episode reward: total was -11.700000. running mean: -17.278097\n",
      "ep 3262: ep_len:658 episode reward: total was -10.760000. running mean: -17.212916\n",
      "ep 3262: ep_len:2946 episode reward: total was -57.710000. running mean: -17.617887\n",
      "ep 3262: ep_len:1232 episode reward: total was -26.630000. running mean: -17.708008\n",
      "ep 3262: ep_len:69 episode reward: total was 33.000000. running mean: -17.200928\n",
      "ep 3262: ep_len:886 episode reward: total was 28.620000. running mean: -16.742719\n",
      "ep 3262: ep_len:634 episode reward: total was 15.450000. running mean: -16.420792\n",
      "ep 3262: ep_len:554 episode reward: total was -3.200000. running mean: -16.288584\n",
      "ep 3262: ep_len:898 episode reward: total was 40.590000. running mean: -15.719798\n",
      "ep 3262: ep_len:500 episode reward: total was 9.580000. running mean: -15.466800\n",
      "ep 3262: ep_len:191 episode reward: total was 92.500000. running mean: -14.387132\n",
      "ep 3262: ep_len:30 episode reward: total was 13.500000. running mean: -14.108261\n",
      "ep 3262: ep_len:1130 episode reward: total was 20.900000. running mean: -13.758178\n",
      "ep 3262: ep_len:2775 episode reward: total was -2.900000. running mean: -13.649596\n",
      "epsilon:0.009992 episode_count: 49066. steps_count: 52844593.000000\n",
      "ep 3263: ep_len:655 episode reward: total was 0.320000. running mean: -13.509900\n",
      "ep 3263: ep_len:827 episode reward: total was 0.400000. running mean: -13.370801\n",
      "ep 3263: ep_len:59 episode reward: total was 28.000000. running mean: -12.957093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3263: ep_len:2940 episode reward: total was -57.520000. running mean: -13.402722\n",
      "ep 3263: ep_len:675 episode reward: total was 28.130000. running mean: -12.987395\n",
      "ep 3263: ep_len:78 episode reward: total was 37.500000. running mean: -12.482521\n",
      "ep 3263: ep_len:724 episode reward: total was -9.090000. running mean: -12.448596\n",
      "ep 3263: ep_len:652 episode reward: total was 15.110000. running mean: -12.173010\n",
      "ep 3263: ep_len:696 episode reward: total was -24.940000. running mean: -12.300680\n",
      "ep 3263: ep_len:816 episode reward: total was 20.810000. running mean: -11.969573\n",
      "ep 3263: ep_len:941 episode reward: total was 8.170000. running mean: -11.768177\n",
      "ep 3263: ep_len:91 episode reward: total was 41.000000. running mean: -11.240496\n",
      "ep 3263: ep_len:595 episode reward: total was 0.230000. running mean: -11.125791\n",
      "ep 3263: ep_len:2726 episode reward: total was 11.090000. running mean: -10.903633\n",
      "epsilon:0.009992 episode_count: 49080. steps_count: 52857068.000000\n",
      "ep 3264: ep_len:1446 episode reward: total was 10.920000. running mean: -10.685396\n",
      "ep 3264: ep_len:500 episode reward: total was 33.160000. running mean: -10.246942\n",
      "ep 3264: ep_len:3095 episode reward: total was -61.020000. running mean: -10.754673\n",
      "ep 3264: ep_len:567 episode reward: total was 5.450000. running mean: -10.592626\n",
      "ep 3264: ep_len:51 episode reward: total was 22.500000. running mean: -10.261700\n",
      "ep 3264: ep_len:111 episode reward: total was 54.000000. running mean: -9.619083\n",
      "ep 3264: ep_len:64 episode reward: total was 29.000000. running mean: -9.232892\n",
      "ep 3264: ep_len:500 episode reward: total was 24.160000. running mean: -8.898963\n",
      "ep 3264: ep_len:3827 episode reward: total was -231.260000. running mean: -11.122574\n",
      "ep 3264: ep_len:696 episode reward: total was -18.380000. running mean: -11.195148\n",
      "ep 3264: ep_len:680 episode reward: total was 8.110000. running mean: -11.002096\n",
      "ep 3264: ep_len:940 episode reward: total was 29.840000. running mean: -10.593675\n",
      "ep 3264: ep_len:146 episode reward: total was 71.500000. running mean: -9.772739\n",
      "ep 3264: ep_len:80 episode reward: total was 37.000000. running mean: -9.305011\n",
      "ep 3264: ep_len:671 episode reward: total was 12.400000. running mean: -9.087961\n",
      "ep 3264: ep_len:2793 episode reward: total was -12.210000. running mean: -9.119182\n",
      "ep 3264: ep_len:55 episode reward: total was 24.500000. running mean: -8.782990\n",
      "epsilon:0.009992 episode_count: 49097. steps_count: 52873290.000000\n",
      "ep 3265: ep_len:1444 episode reward: total was 4.620000. running mean: -8.648960\n",
      "ep 3265: ep_len:500 episode reward: total was 5.210000. running mean: -8.510370\n",
      "ep 3265: ep_len:42 episode reward: total was 19.500000. running mean: -8.230267\n",
      "ep 3265: ep_len:102 episode reward: total was 49.500000. running mean: -7.652964\n",
      "ep 3265: ep_len:683 episode reward: total was -6.080000. running mean: -7.637234\n",
      "ep 3265: ep_len:40 episode reward: total was 18.500000. running mean: -7.375862\n",
      "ep 3265: ep_len:500 episode reward: total was 14.820000. running mean: -7.153903\n",
      "ep 3265: ep_len:4044 episode reward: total was -2813.820000. running mean: -35.220564\n",
      "ep 3265: ep_len:517 episode reward: total was -6.510000. running mean: -34.933459\n",
      "ep 3265: ep_len:7518 episode reward: total was -23.670000. running mean: -34.820824\n",
      "ep 3265: ep_len:662 episode reward: total was -3.340000. running mean: -34.506016\n",
      "ep 3265: ep_len:92 episode reward: total was 44.500000. running mean: -33.715956\n",
      "ep 3265: ep_len:164 episode reward: total was 80.500000. running mean: -32.573796\n",
      "ep 3265: ep_len:868 episode reward: total was 17.570000. running mean: -32.072358\n",
      "ep 3265: ep_len:2825 episode reward: total was 4.490000. running mean: -31.706735\n",
      "epsilon:0.009992 episode_count: 49112. steps_count: 52893291.000000\n",
      "ep 3266: ep_len:1141 episode reward: total was -2.900000. running mean: -31.418667\n",
      "ep 3266: ep_len:500 episode reward: total was 29.790000. running mean: -30.806581\n",
      "ep 3266: ep_len:53 episode reward: total was 25.000000. running mean: -30.248515\n",
      "ep 3266: ep_len:3033 episode reward: total was -111.190000. running mean: -31.057930\n",
      "ep 3266: ep_len:752 episode reward: total was -29.530000. running mean: -31.042650\n",
      "ep 3266: ep_len:58 episode reward: total was 26.000000. running mean: -30.472224\n",
      "ep 3266: ep_len:123 episode reward: total was 60.000000. running mean: -29.567502\n",
      "ep 3266: ep_len:98 episode reward: total was 46.000000. running mean: -28.811827\n",
      "ep 3266: ep_len:1083 episode reward: total was -10.550000. running mean: -28.629208\n",
      "ep 3266: ep_len:4215 episode reward: total was -97.350000. running mean: -29.316416\n",
      "ep 3266: ep_len:1239 episode reward: total was -20.980000. running mean: -29.233052\n",
      "ep 3266: ep_len:673 episode reward: total was 17.190000. running mean: -28.768821\n",
      "ep 3266: ep_len:895 episode reward: total was 26.630000. running mean: -28.214833\n",
      "ep 3266: ep_len:52 episode reward: total was 24.500000. running mean: -27.687685\n",
      "ep 3266: ep_len:76 episode reward: total was 33.500000. running mean: -27.075808\n",
      "ep 3266: ep_len:938 episode reward: total was -57.550000. running mean: -27.380550\n",
      "ep 3266: ep_len:2811 episode reward: total was -23.170000. running mean: -27.338444\n",
      "epsilon:0.009992 episode_count: 49129. steps_count: 52911031.000000\n",
      "ep 3267: ep_len:668 episode reward: total was -21.740000. running mean: -27.282460\n",
      "ep 3267: ep_len:676 episode reward: total was -13.580000. running mean: -27.145435\n",
      "ep 3267: ep_len:2942 episode reward: total was -39.350000. running mean: -27.267481\n",
      "ep 3267: ep_len:676 episode reward: total was 19.150000. running mean: -26.803306\n",
      "ep 3267: ep_len:48 episode reward: total was 22.500000. running mean: -26.310273\n",
      "ep 3267: ep_len:989 episode reward: total was -20.610000. running mean: -26.253270\n",
      "ep 3267: ep_len:3857 episode reward: total was -9.650000. running mean: -26.087238\n",
      "ep 3267: ep_len:1575 episode reward: total was -73.560000. running mean: -26.561965\n",
      "ep 3267: ep_len:686 episode reward: total was 29.880000. running mean: -25.997546\n",
      "ep 3267: ep_len:517 episode reward: total was 22.100000. running mean: -25.516570\n",
      "ep 3267: ep_len:137 episode reward: total was 65.500000. running mean: -24.606405\n",
      "ep 3267: ep_len:57 episode reward: total was 27.000000. running mean: -24.090341\n",
      "ep 3267: ep_len:1137 episode reward: total was -22.130000. running mean: -24.070737\n",
      "ep 3267: ep_len:2800 episode reward: total was -3.140000. running mean: -23.861430\n",
      "ep 3267: ep_len:46 episode reward: total was 21.500000. running mean: -23.407815\n",
      "epsilon:0.009992 episode_count: 49144. steps_count: 52927842.000000\n",
      "ep 3268: ep_len:699 episode reward: total was -36.150000. running mean: -23.535237\n",
      "ep 3268: ep_len:500 episode reward: total was 22.380000. running mean: -23.076085\n",
      "ep 3268: ep_len:2994 episode reward: total was -83.560000. running mean: -23.680924\n",
      "ep 3268: ep_len:500 episode reward: total was 20.240000. running mean: -23.241715\n",
      "ep 3268: ep_len:54 episode reward: total was 24.000000. running mean: -22.769298\n",
      "ep 3268: ep_len:137 episode reward: total was 67.000000. running mean: -21.871605\n",
      "ep 3268: ep_len:823 episode reward: total was 10.560000. running mean: -21.547289\n",
      "ep 3268: ep_len:311 episode reward: total was 22.790000. running mean: -21.103916\n",
      "ep 3268: ep_len:802 episode reward: total was -25.770000. running mean: -21.150577\n",
      "ep 3268: ep_len:674 episode reward: total was 17.570000. running mean: -20.763371\n",
      "ep 3268: ep_len:975 episode reward: total was -0.400000. running mean: -20.559737\n",
      "ep 3268: ep_len:500 episode reward: total was 39.870000. running mean: -19.955440\n",
      "ep 3268: ep_len:2793 episode reward: total was 10.780000. running mean: -19.648085\n",
      "ep 3268: ep_len:37 episode reward: total was 15.500000. running mean: -19.296605\n",
      "epsilon:0.009992 episode_count: 49158. steps_count: 52939641.000000\n",
      "ep 3269: ep_len:1418 episode reward: total was 3.970000. running mean: -19.063938\n",
      "ep 3269: ep_len:3150 episode reward: total was -494.880000. running mean: -23.822099\n",
      "ep 3269: ep_len:2954 episode reward: total was -40.830000. running mean: -23.992178\n",
      "ep 3269: ep_len:868 episode reward: total was 68.000000. running mean: -23.072256\n",
      "ep 3269: ep_len:55 episode reward: total was 26.000000. running mean: -22.581534\n",
      "ep 3269: ep_len:97 episode reward: total was 44.000000. running mean: -21.915718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3269: ep_len:67 episode reward: total was 32.000000. running mean: -21.376561\n",
      "ep 3269: ep_len:69 episode reward: total was 33.000000. running mean: -20.832796\n",
      "ep 3269: ep_len:715 episode reward: total was 28.450000. running mean: -20.339968\n",
      "ep 3269: ep_len:3925 episode reward: total was -38.890000. running mean: -20.525468\n",
      "ep 3269: ep_len:788 episode reward: total was 6.260000. running mean: -20.257613\n",
      "ep 3269: ep_len:670 episode reward: total was 15.600000. running mean: -19.899037\n",
      "ep 3269: ep_len:500 episode reward: total was 32.830000. running mean: -19.371747\n",
      "ep 3269: ep_len:68 episode reward: total was 32.500000. running mean: -18.853029\n",
      "ep 3269: ep_len:78 episode reward: total was 37.500000. running mean: -18.289499\n",
      "ep 3269: ep_len:642 episode reward: total was 4.340000. running mean: -18.063204\n",
      "ep 3269: ep_len:2867 episode reward: total was 10.080000. running mean: -17.781772\n",
      "ep 3269: ep_len:71 episode reward: total was 34.000000. running mean: -17.263954\n",
      "epsilon:0.009992 episode_count: 49176. steps_count: 52958643.000000\n",
      "ep 3270: ep_len:633 episode reward: total was -18.080000. running mean: -17.272115\n",
      "ep 3270: ep_len:1137 episode reward: total was 14.140000. running mean: -16.957994\n",
      "ep 3270: ep_len:70 episode reward: total was 33.500000. running mean: -16.453414\n",
      "ep 3270: ep_len:3030 episode reward: total was -99.060000. running mean: -17.279480\n",
      "ep 3270: ep_len:693 episode reward: total was 10.390000. running mean: -17.002785\n",
      "ep 3270: ep_len:52 episode reward: total was 23.000000. running mean: -16.602757\n",
      "ep 3270: ep_len:74 episode reward: total was 34.000000. running mean: -16.096729\n",
      "ep 3270: ep_len:539 episode reward: total was 15.460000. running mean: -15.781162\n",
      "ep 3270: ep_len:4156 episode reward: total was -109.380000. running mean: -16.717150\n",
      "ep 3270: ep_len:614 episode reward: total was 23.080000. running mean: -16.319179\n",
      "ep 3270: ep_len:787 episode reward: total was 48.680000. running mean: -15.669187\n",
      "ep 3270: ep_len:669 episode reward: total was -4.470000. running mean: -15.557195\n",
      "ep 3270: ep_len:134 episode reward: total was 65.500000. running mean: -14.746623\n",
      "ep 3270: ep_len:51 episode reward: total was 22.500000. running mean: -14.374157\n",
      "ep 3270: ep_len:549 episode reward: total was -4.780000. running mean: -14.278215\n",
      "ep 3270: ep_len:44 episode reward: total was 20.500000. running mean: -13.930433\n",
      "epsilon:0.009992 episode_count: 49192. steps_count: 52971875.000000\n",
      "ep 3271: ep_len:681 episode reward: total was -42.850000. running mean: -14.219629\n",
      "ep 3271: ep_len:989 episode reward: total was 35.870000. running mean: -13.718733\n",
      "ep 3271: ep_len:3020 episode reward: total was -22.050000. running mean: -13.802045\n",
      "ep 3271: ep_len:674 episode reward: total was 1.330000. running mean: -13.650725\n",
      "ep 3271: ep_len:500 episode reward: total was 22.300000. running mean: -13.291218\n",
      "ep 3271: ep_len:4022 episode reward: total was -40.370000. running mean: -13.562006\n",
      "ep 3271: ep_len:1222 episode reward: total was -20.270000. running mean: -13.629085\n",
      "ep 3271: ep_len:690 episode reward: total was 30.930000. running mean: -13.183495\n",
      "ep 3271: ep_len:634 episode reward: total was -4.610000. running mean: -13.097760\n",
      "ep 3271: ep_len:46 episode reward: total was 21.500000. running mean: -12.751782\n",
      "ep 3271: ep_len:650 episode reward: total was -8.570000. running mean: -12.709964\n",
      "ep 3271: ep_len:2787 episode reward: total was -8.170000. running mean: -12.664565\n",
      "epsilon:0.009992 episode_count: 49204. steps_count: 52987790.000000\n",
      "ep 3272: ep_len:1443 episode reward: total was 12.240000. running mean: -12.415519\n",
      "ep 3272: ep_len:3092 episode reward: total was -211.650000. running mean: -14.407864\n",
      "ep 3272: ep_len:83 episode reward: total was 40.000000. running mean: -13.863785\n",
      "ep 3272: ep_len:2980 episode reward: total was -24.740000. running mean: -13.972547\n",
      "ep 3272: ep_len:676 episode reward: total was -14.620000. running mean: -13.979022\n",
      "ep 3272: ep_len:1017 episode reward: total was 6.210000. running mean: -13.777132\n",
      "ep 3272: ep_len:3945 episode reward: total was -39.930000. running mean: -14.038660\n",
      "ep 3272: ep_len:1548 episode reward: total was -51.200000. running mean: -14.410274\n",
      "ep 3272: ep_len:722 episode reward: total was 29.840000. running mean: -13.967771\n",
      "ep 3272: ep_len:636 episode reward: total was -5.120000. running mean: -13.879293\n",
      "ep 3272: ep_len:1019 episode reward: total was 7.120000. running mean: -13.669300\n",
      "ep 3272: ep_len:2813 episode reward: total was -9.380000. running mean: -13.626407\n",
      "epsilon:0.009992 episode_count: 49216. steps_count: 53007764.000000\n",
      "ep 3273: ep_len:967 episode reward: total was -49.760000. running mean: -13.987743\n",
      "ep 3273: ep_len:500 episode reward: total was 23.090000. running mean: -13.616966\n",
      "ep 3273: ep_len:3049 episode reward: total was 6.560000. running mean: -13.415196\n",
      "ep 3273: ep_len:840 episode reward: total was 27.060000. running mean: -13.010444\n",
      "ep 3273: ep_len:500 episode reward: total was 14.550000. running mean: -12.734840\n",
      "ep 3273: ep_len:3783 episode reward: total was 12.110000. running mean: -12.486391\n",
      "ep 3273: ep_len:3050 episode reward: total was -1023.100000. running mean: -22.592527\n",
      "ep 3273: ep_len:651 episode reward: total was 5.920000. running mean: -22.307402\n",
      "ep 3273: ep_len:636 episode reward: total was 14.040000. running mean: -21.943928\n",
      "ep 3273: ep_len:133 episode reward: total was 62.000000. running mean: -21.104489\n",
      "ep 3273: ep_len:90 episode reward: total was 42.000000. running mean: -20.473444\n",
      "ep 3273: ep_len:500 episode reward: total was 26.880000. running mean: -19.999910\n",
      "ep 3273: ep_len:2862 episode reward: total was -28.320000. running mean: -20.083110\n",
      "epsilon:0.009992 episode_count: 49229. steps_count: 53025325.000000\n",
      "ep 3274: ep_len:1471 episode reward: total was 10.440000. running mean: -19.777879\n",
      "ep 3274: ep_len:1896 episode reward: total was -140.260000. running mean: -20.982701\n",
      "ep 3274: ep_len:2973 episode reward: total was -41.380000. running mean: -21.186674\n",
      "ep 3274: ep_len:832 episode reward: total was 0.880000. running mean: -20.966007\n",
      "ep 3274: ep_len:37 episode reward: total was 17.000000. running mean: -20.586347\n",
      "ep 3274: ep_len:99 episode reward: total was 46.500000. running mean: -19.915483\n",
      "ep 3274: ep_len:990 episode reward: total was -5.630000. running mean: -19.772628\n",
      "ep 3274: ep_len:361 episode reward: total was 16.220000. running mean: -19.412702\n",
      "ep 3274: ep_len:1594 episode reward: total was -48.720000. running mean: -19.705775\n",
      "ep 3274: ep_len:711 episode reward: total was 31.080000. running mean: -19.197917\n",
      "ep 3274: ep_len:501 episode reward: total was 4.970000. running mean: -18.956238\n",
      "ep 3274: ep_len:85 episode reward: total was 39.500000. running mean: -18.371676\n",
      "ep 3274: ep_len:48 episode reward: total was 22.500000. running mean: -17.962959\n",
      "ep 3274: ep_len:863 episode reward: total was -16.000000. running mean: -17.943329\n",
      "ep 3274: ep_len:2895 episode reward: total was -0.140000. running mean: -17.765296\n",
      "epsilon:0.009992 episode_count: 49244. steps_count: 53040681.000000\n",
      "ep 3275: ep_len:599 episode reward: total was -21.420000. running mean: -17.801843\n",
      "ep 3275: ep_len:1213 episode reward: total was -9.740000. running mean: -17.721225\n",
      "ep 3275: ep_len:53 episode reward: total was 25.000000. running mean: -17.294013\n",
      "ep 3275: ep_len:2986 episode reward: total was -10.000000. running mean: -17.221072\n",
      "ep 3275: ep_len:560 episode reward: total was -5.790000. running mean: -17.106762\n",
      "ep 3275: ep_len:63 episode reward: total was 28.500000. running mean: -16.650694\n",
      "ep 3275: ep_len:108 episode reward: total was 51.000000. running mean: -15.974187\n",
      "ep 3275: ep_len:100 episode reward: total was 48.500000. running mean: -15.329445\n",
      "ep 3275: ep_len:1058 episode reward: total was -15.760000. running mean: -15.333751\n",
      "ep 3275: ep_len:3964 episode reward: total was -169.560000. running mean: -16.876013\n",
      "ep 3275: ep_len:736 episode reward: total was -58.170000. running mean: -17.288953\n",
      "ep 3275: ep_len:609 episode reward: total was 19.610000. running mean: -16.919964\n",
      "ep 3275: ep_len:1421 episode reward: total was -12.010000. running mean: -16.870864\n",
      "ep 3275: ep_len:126 episode reward: total was 60.000000. running mean: -16.102155\n",
      "ep 3275: ep_len:1177 episode reward: total was -17.690000. running mean: -16.118034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3275: ep_len:2864 episode reward: total was -2.100000. running mean: -15.977853\n",
      "epsilon:0.009992 episode_count: 49260. steps_count: 53058318.000000\n",
      "ep 3276: ep_len:674 episode reward: total was 4.590000. running mean: -15.772175\n",
      "ep 3276: ep_len:711 episode reward: total was -27.800000. running mean: -15.892453\n",
      "ep 3276: ep_len:2946 episode reward: total was -50.210000. running mean: -16.235629\n",
      "ep 3276: ep_len:632 episode reward: total was -9.000000. running mean: -16.163272\n",
      "ep 3276: ep_len:56 episode reward: total was 25.000000. running mean: -15.751640\n",
      "ep 3276: ep_len:115 episode reward: total was 56.000000. running mean: -15.034123\n",
      "ep 3276: ep_len:67 episode reward: total was 32.000000. running mean: -14.563782\n",
      "ep 3276: ep_len:666 episode reward: total was 4.470000. running mean: -14.373444\n",
      "ep 3276: ep_len:338 episode reward: total was 16.110000. running mean: -14.068610\n",
      "ep 3276: ep_len:561 episode reward: total was -4.110000. running mean: -13.969024\n",
      "ep 3276: ep_len:723 episode reward: total was 9.850000. running mean: -13.730833\n",
      "ep 3276: ep_len:661 episode reward: total was 2.240000. running mean: -13.571125\n",
      "ep 3276: ep_len:73 episode reward: total was 35.000000. running mean: -13.085414\n",
      "ep 3276: ep_len:103 episode reward: total was 50.000000. running mean: -12.454560\n",
      "ep 3276: ep_len:28 episode reward: total was 12.500000. running mean: -12.205014\n",
      "ep 3276: ep_len:631 episode reward: total was -13.050000. running mean: -12.213464\n",
      "ep 3276: ep_len:2886 episode reward: total was 11.620000. running mean: -11.975129\n",
      "epsilon:0.009992 episode_count: 49277. steps_count: 53070189.000000\n",
      "ep 3277: ep_len:1055 episode reward: total was -1.740000. running mean: -11.872778\n",
      "ep 3277: ep_len:739 episode reward: total was -17.420000. running mean: -11.928250\n",
      "ep 3277: ep_len:69 episode reward: total was 33.000000. running mean: -11.478968\n",
      "ep 3277: ep_len:3063 episode reward: total was 7.980000. running mean: -11.284378\n",
      "ep 3277: ep_len:1680 episode reward: total was -55.970000. running mean: -11.731234\n",
      "ep 3277: ep_len:106 episode reward: total was 51.500000. running mean: -11.098922\n",
      "ep 3277: ep_len:55 episode reward: total was 24.500000. running mean: -10.742933\n",
      "ep 3277: ep_len:1496 episode reward: total was -23.760000. running mean: -10.873103\n",
      "ep 3277: ep_len:326 episode reward: total was 12.840000. running mean: -10.635972\n",
      "ep 3277: ep_len:1629 episode reward: total was -429.970000. running mean: -14.829313\n",
      "ep 3277: ep_len:852 episode reward: total was 23.800000. running mean: -14.443019\n",
      "ep 3277: ep_len:967 episode reward: total was 24.380000. running mean: -14.054789\n",
      "ep 3277: ep_len:111 episode reward: total was 51.000000. running mean: -13.404241\n",
      "ep 3277: ep_len:956 episode reward: total was -70.890000. running mean: -13.979099\n",
      "ep 3277: ep_len:2846 episode reward: total was -83.330000. running mean: -14.672608\n",
      "epsilon:0.009992 episode_count: 49292. steps_count: 53086139.000000\n",
      "ep 3278: ep_len:1125 episode reward: total was -7.890000. running mean: -14.604782\n",
      "ep 3278: ep_len:697 episode reward: total was -13.640000. running mean: -14.595134\n",
      "ep 3278: ep_len:71 episode reward: total was 32.500000. running mean: -14.124183\n",
      "ep 3278: ep_len:2998 episode reward: total was -22.120000. running mean: -14.204141\n",
      "ep 3278: ep_len:500 episode reward: total was 28.270000. running mean: -13.779400\n",
      "ep 3278: ep_len:59 episode reward: total was 28.000000. running mean: -13.361606\n",
      "ep 3278: ep_len:110 episode reward: total was 50.500000. running mean: -12.722989\n",
      "ep 3278: ep_len:996 episode reward: total was -4.350000. running mean: -12.639260\n",
      "ep 3278: ep_len:587 episode reward: total was 11.310000. running mean: -12.399767\n",
      "ep 3278: ep_len:633 episode reward: total was -54.410000. running mean: -12.819869\n",
      "ep 3278: ep_len:646 episode reward: total was -3.170000. running mean: -12.723371\n",
      "ep 3278: ep_len:1473 episode reward: total was -11.580000. running mean: -12.711937\n",
      "ep 3278: ep_len:110 episode reward: total was 53.500000. running mean: -12.049818\n",
      "ep 3278: ep_len:567 episode reward: total was -12.680000. running mean: -12.056119\n",
      "ep 3278: ep_len:2857 episode reward: total was 9.640000. running mean: -11.839158\n",
      "epsilon:0.009992 episode_count: 49307. steps_count: 53099568.000000\n",
      "ep 3279: ep_len:1134 episode reward: total was -13.070000. running mean: -11.851467\n",
      "ep 3279: ep_len:740 episode reward: total was -21.940000. running mean: -11.952352\n",
      "ep 3279: ep_len:3012 episode reward: total was 9.800000. running mean: -11.734828\n",
      "ep 3279: ep_len:589 episode reward: total was 4.740000. running mean: -11.570080\n",
      "ep 3279: ep_len:83 episode reward: total was 38.500000. running mean: -11.069379\n",
      "ep 3279: ep_len:500 episode reward: total was 24.470000. running mean: -10.713986\n",
      "ep 3279: ep_len:3558 episode reward: total was -3.600000. running mean: -10.642846\n",
      "ep 3279: ep_len:772 episode reward: total was -43.820000. running mean: -10.974617\n",
      "ep 3279: ep_len:640 episode reward: total was -12.350000. running mean: -10.988371\n",
      "ep 3279: ep_len:1105 episode reward: total was -14.370000. running mean: -11.022187\n",
      "ep 3279: ep_len:50 episode reward: total was 23.500000. running mean: -10.676965\n",
      "ep 3279: ep_len:1479 episode reward: total was 15.570000. running mean: -10.414496\n",
      "ep 3279: ep_len:31 episode reward: total was 12.500000. running mean: -10.185351\n",
      "epsilon:0.009992 episode_count: 49320. steps_count: 53113261.000000\n",
      "ep 3280: ep_len:3749 episode reward: total was -1018.270000. running mean: -20.266197\n",
      "ep 3280: ep_len:628 episode reward: total was -17.210000. running mean: -20.235635\n",
      "ep 3280: ep_len:56 episode reward: total was 26.500000. running mean: -19.768279\n",
      "ep 3280: ep_len:2888 episode reward: total was -14.320000. running mean: -19.713796\n",
      "ep 3280: ep_len:1068 episode reward: total was -12.720000. running mean: -19.643858\n",
      "ep 3280: ep_len:42 episode reward: total was 19.500000. running mean: -19.252420\n",
      "ep 3280: ep_len:1390 episode reward: total was 19.760000. running mean: -18.862295\n",
      "ep 3280: ep_len:3876 episode reward: total was -17.940000. running mean: -18.853073\n",
      "ep 3280: ep_len:2546 episode reward: total was -172.730000. running mean: -20.391842\n",
      "ep 3280: ep_len:809 episode reward: total was 21.440000. running mean: -19.973523\n",
      "ep 3280: ep_len:527 episode reward: total was 17.180000. running mean: -19.601988\n",
      "ep 3280: ep_len:171 episode reward: total was 82.500000. running mean: -18.580968\n",
      "ep 3280: ep_len:1493 episode reward: total was -2.750000. running mean: -18.422659\n",
      "ep 3280: ep_len:2846 episode reward: total was 8.740000. running mean: -18.151032\n",
      "epsilon:0.009992 episode_count: 49334. steps_count: 53135350.000000\n",
      "ep 3281: ep_len:668 episode reward: total was 3.480000. running mean: -17.934722\n",
      "ep 3281: ep_len:198 episode reward: total was 10.820000. running mean: -17.647174\n",
      "ep 3281: ep_len:3104 episode reward: total was -45.850000. running mean: -17.929203\n",
      "ep 3281: ep_len:673 episode reward: total was 9.040000. running mean: -17.659511\n",
      "ep 3281: ep_len:1939 episode reward: total was -33.420000. running mean: -17.817116\n",
      "ep 3281: ep_len:3851 episode reward: total was 13.310000. running mean: -17.505844\n",
      "ep 3281: ep_len:675 episode reward: total was -44.930000. running mean: -17.780086\n",
      "ep 3281: ep_len:835 episode reward: total was 55.560000. running mean: -17.046685\n",
      "ep 3281: ep_len:618 episode reward: total was 0.830000. running mean: -16.867918\n",
      "ep 3281: ep_len:150 episode reward: total was 72.000000. running mean: -15.979239\n",
      "ep 3281: ep_len:30 episode reward: total was 13.500000. running mean: -15.684447\n",
      "ep 3281: ep_len:625 episode reward: total was 4.740000. running mean: -15.480202\n",
      "ep 3281: ep_len:2871 episode reward: total was -21.480000. running mean: -15.540200\n",
      "epsilon:0.009992 episode_count: 49347. steps_count: 53151587.000000\n",
      "ep 3282: ep_len:701 episode reward: total was -0.830000. running mean: -15.393098\n",
      "ep 3282: ep_len:934 episode reward: total was 10.640000. running mean: -15.132767\n",
      "ep 3282: ep_len:75 episode reward: total was 36.000000. running mean: -14.621440\n",
      "ep 3282: ep_len:2941 episode reward: total was -32.060000. running mean: -14.795825\n",
      "ep 3282: ep_len:553 episode reward: total was 3.880000. running mean: -14.609067\n",
      "ep 3282: ep_len:48 episode reward: total was 21.000000. running mean: -14.252976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3282: ep_len:871 episode reward: total was 30.300000. running mean: -13.807446\n",
      "ep 3282: ep_len:4074 episode reward: total was -31.140000. running mean: -13.980772\n",
      "ep 3282: ep_len:804 episode reward: total was -56.140000. running mean: -14.402364\n",
      "ep 3282: ep_len:805 episode reward: total was 35.630000. running mean: -13.902041\n",
      "ep 3282: ep_len:3638 episode reward: total was -615.060000. running mean: -19.913620\n",
      "ep 3282: ep_len:88 episode reward: total was 42.500000. running mean: -19.289484\n",
      "ep 3282: ep_len:176 episode reward: total was 83.500000. running mean: -18.261589\n",
      "ep 3282: ep_len:68 episode reward: total was 29.500000. running mean: -17.783973\n",
      "ep 3282: ep_len:772 episode reward: total was -17.080000. running mean: -17.776934\n",
      "ep 3282: ep_len:2813 episode reward: total was -34.000000. running mean: -17.939164\n",
      "epsilon:0.009992 episode_count: 49363. steps_count: 53170948.000000\n",
      "ep 3283: ep_len:988 episode reward: total was -81.290000. running mean: -18.572673\n",
      "ep 3283: ep_len:1286 episode reward: total was -51.430000. running mean: -18.901246\n",
      "ep 3283: ep_len:70 episode reward: total was 32.000000. running mean: -18.392233\n",
      "ep 3283: ep_len:2958 episode reward: total was -31.370000. running mean: -18.522011\n",
      "ep 3283: ep_len:1162 episode reward: total was -44.100000. running mean: -18.777791\n",
      "ep 3283: ep_len:47 episode reward: total was 22.000000. running mean: -18.370013\n",
      "ep 3283: ep_len:1354 episode reward: total was -173.450000. running mean: -19.920813\n",
      "ep 3283: ep_len:4017 episode reward: total was -38.490000. running mean: -20.106505\n",
      "ep 3283: ep_len:549 episode reward: total was -12.280000. running mean: -20.028240\n",
      "ep 3283: ep_len:7208 episode reward: total was -131.610000. running mean: -21.144057\n",
      "ep 3283: ep_len:584 episode reward: total was 1.440000. running mean: -20.918217\n",
      "ep 3283: ep_len:65 episode reward: total was 31.000000. running mean: -20.399035\n",
      "ep 3283: ep_len:601 episode reward: total was -12.230000. running mean: -20.317344\n",
      "ep 3283: ep_len:2824 episode reward: total was -1.650000. running mean: -20.130671\n",
      "ep 3283: ep_len:51 episode reward: total was 24.000000. running mean: -19.689364\n",
      "epsilon:0.009992 episode_count: 49378. steps_count: 53194712.000000\n",
      "ep 3284: ep_len:1126 episode reward: total was -5.740000. running mean: -19.549870\n",
      "ep 3284: ep_len:1659 episode reward: total was -60.740000. running mean: -19.961772\n",
      "ep 3284: ep_len:69 episode reward: total was 33.000000. running mean: -19.432154\n",
      "ep 3284: ep_len:86 episode reward: total was 40.000000. running mean: -18.837833\n",
      "ep 3284: ep_len:500 episode reward: total was 35.740000. running mean: -18.292054\n",
      "ep 3284: ep_len:45 episode reward: total was 21.000000. running mean: -17.899134\n",
      "ep 3284: ep_len:122 episode reward: total was 59.500000. running mean: -17.125142\n",
      "ep 3284: ep_len:558 episode reward: total was 45.790000. running mean: -16.495991\n",
      "ep 3284: ep_len:3863 episode reward: total was -47.010000. running mean: -16.801131\n",
      "ep 3284: ep_len:547 episode reward: total was -3.300000. running mean: -16.666120\n",
      "ep 3284: ep_len:770 episode reward: total was 13.780000. running mean: -16.361658\n",
      "ep 3284: ep_len:720 episode reward: total was -11.150000. running mean: -16.309542\n",
      "ep 3284: ep_len:70 episode reward: total was 33.500000. running mean: -15.811446\n",
      "ep 3284: ep_len:185 episode reward: total was 89.500000. running mean: -14.758332\n",
      "ep 3284: ep_len:44 episode reward: total was 20.500000. running mean: -14.405749\n",
      "ep 3284: ep_len:114 episode reward: total was 52.500000. running mean: -13.736691\n",
      "ep 3284: ep_len:1018 episode reward: total was 23.570000. running mean: -13.363624\n",
      "ep 3284: ep_len:2835 episode reward: total was 4.620000. running mean: -13.183788\n",
      "ep 3284: ep_len:74 episode reward: total was 35.500000. running mean: -12.696950\n",
      "epsilon:0.009992 episode_count: 49397. steps_count: 53209117.000000\n",
      "ep 3285: ep_len:726 episode reward: total was -57.550000. running mean: -13.145481\n",
      "ep 3285: ep_len:1588 episode reward: total was -68.150000. running mean: -13.695526\n",
      "ep 3285: ep_len:85 episode reward: total was 41.000000. running mean: -13.148571\n",
      "ep 3285: ep_len:835 episode reward: total was -7.290000. running mean: -13.089985\n",
      "ep 3285: ep_len:110 episode reward: total was 53.500000. running mean: -12.424085\n",
      "ep 3285: ep_len:79 episode reward: total was 36.500000. running mean: -11.934844\n",
      "ep 3285: ep_len:1134 episode reward: total was 2.670000. running mean: -11.788796\n",
      "ep 3285: ep_len:3841 episode reward: total was 7.390000. running mean: -11.597008\n",
      "ep 3285: ep_len:539 episode reward: total was -3.380000. running mean: -11.514838\n",
      "ep 3285: ep_len:721 episode reward: total was 33.200000. running mean: -11.067689\n",
      "ep 3285: ep_len:608 episode reward: total was -12.830000. running mean: -11.085312\n",
      "ep 3285: ep_len:500 episode reward: total was -13.010000. running mean: -11.104559\n",
      "ep 3285: ep_len:2710 episode reward: total was -10.810000. running mean: -11.101614\n",
      "ep 3285: ep_len:60 episode reward: total was 28.500000. running mean: -10.705598\n",
      "epsilon:0.009992 episode_count: 49411. steps_count: 53222653.000000\n",
      "ep 3286: ep_len:836 episode reward: total was -40.290000. running mean: -11.001442\n",
      "ep 3286: ep_len:730 episode reward: total was -13.520000. running mean: -11.026627\n",
      "ep 3286: ep_len:2959 episode reward: total was -79.520000. running mean: -11.711561\n",
      "ep 3286: ep_len:761 episode reward: total was -18.820000. running mean: -11.782645\n",
      "ep 3286: ep_len:46 episode reward: total was 21.500000. running mean: -11.449819\n",
      "ep 3286: ep_len:92 episode reward: total was 43.000000. running mean: -10.905321\n",
      "ep 3286: ep_len:24 episode reward: total was 10.500000. running mean: -10.691267\n",
      "ep 3286: ep_len:4112 episode reward: total was -2187.370000. running mean: -32.458055\n",
      "ep 3286: ep_len:349 episode reward: total was 17.110000. running mean: -31.962374\n",
      "ep 3286: ep_len:670 episode reward: total was -64.900000. running mean: -32.291750\n",
      "ep 3286: ep_len:678 episode reward: total was 19.200000. running mean: -31.776833\n",
      "ep 3286: ep_len:952 episode reward: total was 10.090000. running mean: -31.358165\n",
      "ep 3286: ep_len:125 episode reward: total was 61.000000. running mean: -30.434583\n",
      "ep 3286: ep_len:101 episode reward: total was 49.000000. running mean: -29.640237\n",
      "ep 3286: ep_len:1503 episode reward: total was 23.400000. running mean: -29.109835\n",
      "ep 3286: ep_len:2849 episode reward: total was -51.770000. running mean: -29.336436\n",
      "ep 3286: ep_len:74 episode reward: total was 35.500000. running mean: -28.688072\n",
      "epsilon:0.009992 episode_count: 49428. steps_count: 53239514.000000\n",
      "ep 3287: ep_len:824 episode reward: total was -31.320000. running mean: -28.714391\n",
      "ep 3287: ep_len:963 episode reward: total was 20.850000. running mean: -28.218747\n",
      "ep 3287: ep_len:54 episode reward: total was 24.000000. running mean: -27.696560\n",
      "ep 3287: ep_len:2934 episode reward: total was -55.210000. running mean: -27.971694\n",
      "ep 3287: ep_len:3811 episode reward: total was -405.860000. running mean: -31.750577\n",
      "ep 3287: ep_len:1469 episode reward: total was 15.410000. running mean: -31.278972\n",
      "ep 3287: ep_len:3792 episode reward: total was 10.090000. running mean: -30.865282\n",
      "ep 3287: ep_len:1266 episode reward: total was -73.360000. running mean: -31.290229\n",
      "ep 3287: ep_len:715 episode reward: total was 34.300000. running mean: -30.634327\n",
      "ep 3287: ep_len:946 episode reward: total was 73.380000. running mean: -29.594184\n",
      "ep 3287: ep_len:114 episode reward: total was 55.500000. running mean: -28.743242\n",
      "ep 3287: ep_len:500 episode reward: total was 12.130000. running mean: -28.334509\n",
      "ep 3287: ep_len:2851 episode reward: total was -6.730000. running mean: -28.118464\n",
      "ep 3287: ep_len:54 episode reward: total was 24.000000. running mean: -27.597280\n",
      "epsilon:0.009992 episode_count: 49442. steps_count: 53259807.000000\n",
      "ep 3288: ep_len:1167 episode reward: total was 5.570000. running mean: -27.265607\n",
      "ep 3288: ep_len:711 episode reward: total was -18.310000. running mean: -27.176051\n",
      "ep 3288: ep_len:63 episode reward: total was 28.500000. running mean: -26.619290\n",
      "ep 3288: ep_len:2932 episode reward: total was -40.940000. running mean: -26.762497\n",
      "ep 3288: ep_len:535 episode reward: total was -43.300000. running mean: -26.927872\n",
      "ep 3288: ep_len:45 episode reward: total was 21.000000. running mean: -26.448594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3288: ep_len:53 episode reward: total was 23.500000. running mean: -25.949108\n",
      "ep 3288: ep_len:500 episode reward: total was 34.850000. running mean: -25.341117\n",
      "ep 3288: ep_len:662 episode reward: total was 18.730000. running mean: -24.900405\n",
      "ep 3288: ep_len:610 episode reward: total was 22.320000. running mean: -24.428201\n",
      "ep 3288: ep_len:7552 episode reward: total was -136.500000. running mean: -25.548919\n",
      "ep 3288: ep_len:500 episode reward: total was 15.620000. running mean: -25.137230\n",
      "ep 3288: ep_len:59 episode reward: total was 28.000000. running mean: -24.605858\n",
      "ep 3288: ep_len:1117 episode reward: total was 18.380000. running mean: -24.175999\n",
      "ep 3288: ep_len:2821 episode reward: total was -40.610000. running mean: -24.340339\n",
      "epsilon:0.009992 episode_count: 49457. steps_count: 53279134.000000\n",
      "ep 3289: ep_len:1140 episode reward: total was -1.900000. running mean: -24.115936\n",
      "ep 3289: ep_len:762 episode reward: total was -12.420000. running mean: -23.998977\n",
      "ep 3289: ep_len:46 episode reward: total was 21.500000. running mean: -23.543987\n",
      "ep 3289: ep_len:2979 episode reward: total was -82.370000. running mean: -24.132247\n",
      "ep 3289: ep_len:500 episode reward: total was 3.920000. running mean: -23.851724\n",
      "ep 3289: ep_len:126 episode reward: total was 55.500000. running mean: -23.058207\n",
      "ep 3289: ep_len:671 episode reward: total was -11.030000. running mean: -22.937925\n",
      "ep 3289: ep_len:630 episode reward: total was 9.070000. running mean: -22.617846\n",
      "ep 3289: ep_len:675 episode reward: total was -65.710000. running mean: -23.048767\n",
      "ep 3289: ep_len:639 episode reward: total was 3.250000. running mean: -22.785780\n",
      "ep 3289: ep_len:928 episode reward: total was 75.310000. running mean: -21.804822\n",
      "ep 3289: ep_len:95 episode reward: total was 44.500000. running mean: -21.141774\n",
      "ep 3289: ep_len:45 episode reward: total was 21.000000. running mean: -20.720356\n",
      "ep 3289: ep_len:77 episode reward: total was 37.000000. running mean: -20.143152\n",
      "ep 3289: ep_len:500 episode reward: total was 53.950000. running mean: -19.402221\n",
      "ep 3289: ep_len:2785 episode reward: total was -48.770000. running mean: -19.695899\n",
      "epsilon:0.009992 episode_count: 49473. steps_count: 53291732.000000\n",
      "ep 3290: ep_len:500 episode reward: total was 5.540000. running mean: -19.443540\n",
      "ep 3290: ep_len:760 episode reward: total was -22.550000. running mean: -19.474604\n",
      "ep 3290: ep_len:80 episode reward: total was 37.000000. running mean: -18.909858\n",
      "ep 3290: ep_len:3097 episode reward: total was -38.350000. running mean: -19.104260\n",
      "ep 3290: ep_len:581 episode reward: total was -4.460000. running mean: -18.957817\n",
      "ep 3290: ep_len:67 episode reward: total was 32.000000. running mean: -18.448239\n",
      "ep 3290: ep_len:614 episode reward: total was -8.050000. running mean: -18.344257\n",
      "ep 3290: ep_len:3577 episode reward: total was -138.410000. running mean: -19.544914\n",
      "ep 3290: ep_len:2060 episode reward: total was -352.140000. running mean: -22.870865\n",
      "ep 3290: ep_len:642 episode reward: total was 10.450000. running mean: -22.537656\n",
      "ep 3290: ep_len:691 episode reward: total was 8.380000. running mean: -22.228480\n",
      "ep 3290: ep_len:209 episode reward: total was 98.500000. running mean: -21.021195\n",
      "ep 3290: ep_len:65 episode reward: total was 31.000000. running mean: -20.500983\n",
      "ep 3290: ep_len:87 episode reward: total was 42.000000. running mean: -19.875973\n",
      "ep 3290: ep_len:742 episode reward: total was -43.740000. running mean: -20.114613\n",
      "ep 3290: ep_len:2915 episode reward: total was -10.160000. running mean: -20.015067\n",
      "epsilon:0.009992 episode_count: 49489. steps_count: 53308419.000000\n",
      "ep 3291: ep_len:1417 episode reward: total was 16.660000. running mean: -19.648317\n",
      "ep 3291: ep_len:500 episode reward: total was 22.840000. running mean: -19.223433\n",
      "ep 3291: ep_len:2880 episode reward: total was -95.020000. running mean: -19.981399\n",
      "ep 3291: ep_len:801 episode reward: total was -40.120000. running mean: -20.182785\n",
      "ep 3291: ep_len:147 episode reward: total was 72.000000. running mean: -19.260957\n",
      "ep 3291: ep_len:1488 episode reward: total was 4.380000. running mean: -19.024548\n",
      "ep 3291: ep_len:666 episode reward: total was 29.330000. running mean: -18.541002\n",
      "ep 3291: ep_len:1574 episode reward: total was -89.880000. running mean: -19.254392\n",
      "ep 3291: ep_len:757 episode reward: total was 50.280000. running mean: -18.559048\n",
      "ep 3291: ep_len:692 episode reward: total was 16.290000. running mean: -18.210558\n",
      "ep 3291: ep_len:188 episode reward: total was 91.000000. running mean: -17.118452\n",
      "ep 3291: ep_len:54 episode reward: total was 25.500000. running mean: -16.692268\n",
      "ep 3291: ep_len:74 episode reward: total was 35.500000. running mean: -16.170345\n",
      "ep 3291: ep_len:1078 episode reward: total was 10.240000. running mean: -15.906241\n",
      "ep 3291: ep_len:2779 episode reward: total was -32.860000. running mean: -16.075779\n",
      "ep 3291: ep_len:43 episode reward: total was 20.000000. running mean: -15.715021\n",
      "epsilon:0.009992 episode_count: 49505. steps_count: 53323557.000000\n",
      "ep 3292: ep_len:1033 episode reward: total was -64.580000. running mean: -16.203671\n",
      "ep 3292: ep_len:646 episode reward: total was -70.380000. running mean: -16.745434\n",
      "ep 3292: ep_len:2968 episode reward: total was -102.080000. running mean: -17.598780\n",
      "ep 3292: ep_len:500 episode reward: total was 9.860000. running mean: -17.324192\n",
      "ep 3292: ep_len:1820 episode reward: total was 22.930000. running mean: -16.921650\n",
      "ep 3292: ep_len:661 episode reward: total was 22.940000. running mean: -16.523034\n",
      "ep 3292: ep_len:817 episode reward: total was 4.720000. running mean: -16.310603\n",
      "ep 3292: ep_len:735 episode reward: total was 35.770000. running mean: -15.789797\n",
      "ep 3292: ep_len:1502 episode reward: total was -22.490000. running mean: -15.856799\n",
      "ep 3292: ep_len:47 episode reward: total was 20.500000. running mean: -15.493231\n",
      "ep 3292: ep_len:83 episode reward: total was 40.000000. running mean: -14.938299\n",
      "ep 3292: ep_len:500 episode reward: total was 21.680000. running mean: -14.572116\n",
      "ep 3292: ep_len:2856 episode reward: total was 5.740000. running mean: -14.368995\n",
      "epsilon:0.009992 episode_count: 49518. steps_count: 53337725.000000\n",
      "ep 3293: ep_len:742 episode reward: total was -15.400000. running mean: -14.379305\n",
      "ep 3293: ep_len:718 episode reward: total was -26.320000. running mean: -14.498712\n",
      "ep 3293: ep_len:52 episode reward: total was 24.500000. running mean: -14.108725\n",
      "ep 3293: ep_len:2890 episode reward: total was -73.390000. running mean: -14.701538\n",
      "ep 3293: ep_len:546 episode reward: total was -35.600000. running mean: -14.910522\n",
      "ep 3293: ep_len:143 episode reward: total was 68.500000. running mean: -14.076417\n",
      "ep 3293: ep_len:82 episode reward: total was 36.500000. running mean: -13.570653\n",
      "ep 3293: ep_len:69 episode reward: total was 33.000000. running mean: -13.104946\n",
      "ep 3293: ep_len:1058 episode reward: total was -9.180000. running mean: -13.065697\n",
      "ep 3293: ep_len:333 episode reward: total was 8.190000. running mean: -12.853140\n",
      "ep 3293: ep_len:531 episode reward: total was -34.250000. running mean: -13.067108\n",
      "ep 3293: ep_len:803 episode reward: total was 21.310000. running mean: -12.723337\n",
      "ep 3293: ep_len:500 episode reward: total was 14.570000. running mean: -12.450404\n",
      "ep 3293: ep_len:89 episode reward: total was 41.500000. running mean: -11.910900\n",
      "ep 3293: ep_len:168 episode reward: total was 81.000000. running mean: -10.981791\n",
      "ep 3293: ep_len:2457 episode reward: total was -534.130000. running mean: -16.213273\n",
      "ep 3293: ep_len:2882 episode reward: total was -38.360000. running mean: -16.434740\n",
      "ep 3293: ep_len:66 episode reward: total was 31.500000. running mean: -15.955393\n",
      "epsilon:0.009992 episode_count: 49536. steps_count: 53351854.000000\n",
      "ep 3294: ep_len:985 episode reward: total was -145.780000. running mean: -17.253639\n",
      "ep 3294: ep_len:214 episode reward: total was 1.370000. running mean: -17.067403\n",
      "ep 3294: ep_len:3043 episode reward: total was -28.390000. running mean: -17.180629\n",
      "ep 3294: ep_len:652 episode reward: total was 4.140000. running mean: -16.967422\n",
      "ep 3294: ep_len:50 episode reward: total was 23.500000. running mean: -16.562748\n",
      "ep 3294: ep_len:500 episode reward: total was 46.940000. running mean: -15.927721\n",
      "ep 3294: ep_len:3638 episode reward: total was -237.210000. running mean: -18.140543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3294: ep_len:603 episode reward: total was 24.470000. running mean: -17.714438\n",
      "ep 3294: ep_len:732 episode reward: total was 2.990000. running mean: -17.507394\n",
      "ep 3294: ep_len:586 episode reward: total was -7.440000. running mean: -17.406720\n",
      "ep 3294: ep_len:195 episode reward: total was 91.500000. running mean: -16.317652\n",
      "ep 3294: ep_len:89 episode reward: total was 43.000000. running mean: -15.724476\n",
      "ep 3294: ep_len:840 episode reward: total was -8.810000. running mean: -15.655331\n",
      "ep 3294: ep_len:46 episode reward: total was 21.500000. running mean: -15.283778\n",
      "epsilon:0.009992 episode_count: 49550. steps_count: 53364027.000000\n",
      "ep 3295: ep_len:670 episode reward: total was 19.370000. running mean: -14.937240\n",
      "ep 3295: ep_len:738 episode reward: total was -24.100000. running mean: -15.028868\n",
      "ep 3295: ep_len:2993 episode reward: total was -82.950000. running mean: -15.708079\n",
      "ep 3295: ep_len:500 episode reward: total was 16.630000. running mean: -15.384698\n",
      "ep 3295: ep_len:82 episode reward: total was 36.500000. running mean: -14.865851\n",
      "ep 3295: ep_len:82 episode reward: total was 39.500000. running mean: -14.322193\n",
      "ep 3295: ep_len:665 episode reward: total was 6.270000. running mean: -14.116271\n",
      "ep 3295: ep_len:329 episode reward: total was 20.950000. running mean: -13.765608\n",
      "ep 3295: ep_len:635 episode reward: total was 18.390000. running mean: -13.444052\n",
      "ep 3295: ep_len:642 episode reward: total was 21.440000. running mean: -13.095211\n",
      "ep 3295: ep_len:592 episode reward: total was -21.400000. running mean: -13.178259\n",
      "ep 3295: ep_len:37 episode reward: total was 15.500000. running mean: -12.891477\n",
      "ep 3295: ep_len:87 episode reward: total was 40.500000. running mean: -12.357562\n",
      "ep 3295: ep_len:689 episode reward: total was 20.040000. running mean: -12.033586\n",
      "ep 3295: ep_len:36 episode reward: total was 15.000000. running mean: -11.763251\n",
      "epsilon:0.009992 episode_count: 49565. steps_count: 53372804.000000\n",
      "ep 3296: ep_len:971 episode reward: total was -60.150000. running mean: -12.247118\n",
      "ep 3296: ep_len:500 episode reward: total was -2.900000. running mean: -12.153647\n",
      "ep 3296: ep_len:61 episode reward: total was 29.000000. running mean: -11.742110\n",
      "ep 3296: ep_len:2995 episode reward: total was -84.760000. running mean: -12.472289\n",
      "ep 3296: ep_len:693 episode reward: total was 11.400000. running mean: -12.233566\n",
      "ep 3296: ep_len:61 episode reward: total was 24.500000. running mean: -11.866231\n",
      "ep 3296: ep_len:1512 episode reward: total was 23.980000. running mean: -11.507768\n",
      "ep 3296: ep_len:635 episode reward: total was 21.490000. running mean: -11.177791\n",
      "ep 3296: ep_len:924 episode reward: total was -55.540000. running mean: -11.621413\n",
      "ep 3296: ep_len:858 episode reward: total was 44.710000. running mean: -11.058099\n",
      "ep 3296: ep_len:1099 episode reward: total was -31.570000. running mean: -11.263218\n",
      "ep 3296: ep_len:634 episode reward: total was -16.050000. running mean: -11.311086\n",
      "ep 3296: ep_len:2845 episode reward: total was -21.210000. running mean: -11.410075\n",
      "epsilon:0.009992 episode_count: 49578. steps_count: 53386592.000000\n",
      "ep 3297: ep_len:1178 episode reward: total was 16.210000. running mean: -11.133874\n",
      "ep 3297: ep_len:643 episode reward: total was -11.920000. running mean: -11.141735\n",
      "ep 3297: ep_len:3040 episode reward: total was -14.320000. running mean: -11.173518\n",
      "ep 3297: ep_len:790 episode reward: total was -14.490000. running mean: -11.206683\n",
      "ep 3297: ep_len:948 episode reward: total was -26.040000. running mean: -11.355016\n",
      "ep 3297: ep_len:622 episode reward: total was 22.250000. running mean: -11.018966\n",
      "ep 3297: ep_len:577 episode reward: total was 13.960000. running mean: -10.769176\n",
      "ep 3297: ep_len:751 episode reward: total was 25.140000. running mean: -10.410084\n",
      "ep 3297: ep_len:1064 episode reward: total was 42.400000. running mean: -9.881983\n",
      "ep 3297: ep_len:93 episode reward: total was 42.000000. running mean: -9.363164\n",
      "ep 3297: ep_len:125 episode reward: total was 53.500000. running mean: -8.734532\n",
      "ep 3297: ep_len:38 episode reward: total was 16.000000. running mean: -8.487187\n",
      "ep 3297: ep_len:83 episode reward: total was 38.500000. running mean: -8.017315\n",
      "ep 3297: ep_len:511 episode reward: total was 10.920000. running mean: -7.827942\n",
      "ep 3297: ep_len:2820 episode reward: total was -28.370000. running mean: -8.033362\n",
      "epsilon:0.009992 episode_count: 49593. steps_count: 53399875.000000\n",
      "ep 3298: ep_len:863 episode reward: total was -9.800000. running mean: -8.051029\n",
      "ep 3298: ep_len:1617 episode reward: total was -14.330000. running mean: -8.113818\n",
      "ep 3298: ep_len:2921 episode reward: total was -22.660000. running mean: -8.259280\n",
      "ep 3298: ep_len:805 episode reward: total was 15.040000. running mean: -8.026287\n",
      "ep 3298: ep_len:92 episode reward: total was 44.500000. running mean: -7.501024\n",
      "ep 3298: ep_len:58 episode reward: total was 26.000000. running mean: -7.166014\n",
      "ep 3298: ep_len:500 episode reward: total was 9.800000. running mean: -6.996354\n",
      "ep 3298: ep_len:3859 episode reward: total was -153.020000. running mean: -8.456590\n",
      "ep 3298: ep_len:1181 episode reward: total was -60.560000. running mean: -8.977625\n",
      "ep 3298: ep_len:7341 episode reward: total was -97.540000. running mean: -9.863248\n",
      "ep 3298: ep_len:1103 episode reward: total was 10.410000. running mean: -9.660516\n",
      "ep 3298: ep_len:58 episode reward: total was 26.000000. running mean: -9.303911\n",
      "ep 3298: ep_len:38 episode reward: total was 17.500000. running mean: -9.035872\n",
      "ep 3298: ep_len:1130 episode reward: total was -4.020000. running mean: -8.985713\n",
      "ep 3298: ep_len:2906 episode reward: total was -12.460000. running mean: -9.020456\n",
      "epsilon:0.009992 episode_count: 49608. steps_count: 53424347.000000\n",
      "ep 3299: ep_len:1092 episode reward: total was -1.960000. running mean: -8.949851\n",
      "ep 3299: ep_len:687 episode reward: total was 14.270000. running mean: -8.717653\n",
      "ep 3299: ep_len:2931 episode reward: total was -82.930000. running mean: -9.459776\n",
      "ep 3299: ep_len:513 episode reward: total was 15.170000. running mean: -9.213478\n",
      "ep 3299: ep_len:65 episode reward: total was 29.500000. running mean: -8.826344\n",
      "ep 3299: ep_len:903 episode reward: total was 65.350000. running mean: -8.084580\n",
      "ep 3299: ep_len:3632 episode reward: total was -859.740000. running mean: -16.601134\n",
      "ep 3299: ep_len:639 episode reward: total was -68.950000. running mean: -17.124623\n",
      "ep 3299: ep_len:7113 episode reward: total was -24.200000. running mean: -17.195377\n",
      "ep 3299: ep_len:579 episode reward: total was 51.870000. running mean: -16.504723\n",
      "ep 3299: ep_len:113 episode reward: total was 52.000000. running mean: -15.819676\n",
      "ep 3299: ep_len:52 episode reward: total was 23.000000. running mean: -15.431479\n",
      "ep 3299: ep_len:115 episode reward: total was 54.500000. running mean: -14.732164\n",
      "ep 3299: ep_len:1133 episode reward: total was -11.910000. running mean: -14.703943\n",
      "ep 3299: ep_len:47 episode reward: total was 22.000000. running mean: -14.336903\n",
      "epsilon:0.009992 episode_count: 49623. steps_count: 53443961.000000\n",
      "ep 3300: ep_len:608 episode reward: total was -6.210000. running mean: -14.255634\n",
      "ep 3300: ep_len:685 episode reward: total was -10.490000. running mean: -14.217978\n",
      "ep 3300: ep_len:2990 episode reward: total was -47.790000. running mean: -14.553698\n",
      "ep 3300: ep_len:558 episode reward: total was 15.280000. running mean: -14.255361\n",
      "ep 3300: ep_len:500 episode reward: total was 42.260000. running mean: -13.690207\n",
      "ep 3300: ep_len:3612 episode reward: total was -127.960000. running mean: -14.832905\n",
      "ep 3300: ep_len:802 episode reward: total was -19.000000. running mean: -14.874576\n",
      "ep 3300: ep_len:818 episode reward: total was 55.760000. running mean: -14.168231\n",
      "ep 3300: ep_len:1114 episode reward: total was -17.310000. running mean: -14.199648\n",
      "ep 3300: ep_len:66 episode reward: total was 31.500000. running mean: -13.742652\n",
      "ep 3300: ep_len:722 episode reward: total was -56.580000. running mean: -14.171025\n",
      "ep 3300: ep_len:2850 episode reward: total was -17.980000. running mean: -14.209115\n",
      "epsilon:0.009992 episode_count: 49635. steps_count: 53459286.000000\n",
      "ep 3301: ep_len:642 episode reward: total was 5.480000. running mean: -14.012224\n",
      "ep 3301: ep_len:918 episode reward: total was 9.960000. running mean: -13.772502\n",
      "ep 3301: ep_len:23 episode reward: total was 10.000000. running mean: -13.534777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3301: ep_len:2940 episode reward: total was -45.990000. running mean: -13.859329\n",
      "ep 3301: ep_len:859 episode reward: total was 42.650000. running mean: -13.294236\n",
      "ep 3301: ep_len:636 episode reward: total was -24.540000. running mean: -13.406693\n",
      "ep 3301: ep_len:327 episode reward: total was 6.820000. running mean: -13.204426\n",
      "ep 3301: ep_len:1270 episode reward: total was -70.780000. running mean: -13.780182\n",
      "ep 3301: ep_len:672 episode reward: total was 8.330000. running mean: -13.559080\n",
      "ep 3301: ep_len:592 episode reward: total was 16.520000. running mean: -13.258289\n",
      "ep 3301: ep_len:59 episode reward: total was 28.000000. running mean: -12.845706\n",
      "ep 3301: ep_len:123 episode reward: total was 60.000000. running mean: -12.117249\n",
      "ep 3301: ep_len:644 episode reward: total was 1.870000. running mean: -11.977377\n",
      "ep 3301: ep_len:2810 episode reward: total was -46.900000. running mean: -12.326603\n",
      "epsilon:0.009992 episode_count: 49649. steps_count: 53471801.000000\n",
      "ep 3302: ep_len:591 episode reward: total was 4.970000. running mean: -12.153637\n",
      "ep 3302: ep_len:684 episode reward: total was -28.680000. running mean: -12.318901\n",
      "ep 3302: ep_len:2920 episode reward: total was -38.700000. running mean: -12.582712\n",
      "ep 3302: ep_len:541 episode reward: total was -15.970000. running mean: -12.616585\n",
      "ep 3302: ep_len:142 episode reward: total was 66.500000. running mean: -11.825419\n",
      "ep 3302: ep_len:500 episode reward: total was 52.390000. running mean: -11.183265\n",
      "ep 3302: ep_len:338 episode reward: total was 14.490000. running mean: -10.926532\n",
      "ep 3302: ep_len:572 episode reward: total was 5.650000. running mean: -10.760767\n",
      "ep 3302: ep_len:777 episode reward: total was 32.480000. running mean: -10.328359\n",
      "ep 3302: ep_len:920 episode reward: total was 35.980000. running mean: -9.865275\n",
      "ep 3302: ep_len:1429 episode reward: total was 7.750000. running mean: -9.689123\n",
      "ep 3302: ep_len:2830 episode reward: total was -20.780000. running mean: -9.800031\n",
      "ep 3302: ep_len:52 episode reward: total was 23.000000. running mean: -9.472031\n",
      "epsilon:0.009992 episode_count: 49662. steps_count: 53484097.000000\n",
      "ep 3303: ep_len:1518 episode reward: total was -923.630000. running mean: -18.613611\n",
      "ep 3303: ep_len:500 episode reward: total was 19.600000. running mean: -18.231475\n",
      "ep 3303: ep_len:53 episode reward: total was 25.000000. running mean: -17.799160\n",
      "ep 3303: ep_len:2950 episode reward: total was -46.790000. running mean: -18.089068\n",
      "ep 3303: ep_len:776 episode reward: total was 28.860000. running mean: -17.619578\n",
      "ep 3303: ep_len:44 episode reward: total was 19.000000. running mean: -17.253382\n",
      "ep 3303: ep_len:54 episode reward: total was 25.500000. running mean: -16.825848\n",
      "ep 3303: ep_len:672 episode reward: total was -106.880000. running mean: -17.726390\n",
      "ep 3303: ep_len:3707 episode reward: total was -383.750000. running mean: -21.386626\n",
      "ep 3303: ep_len:602 episode reward: total was -53.130000. running mean: -21.704059\n",
      "ep 3303: ep_len:825 episode reward: total was 27.530000. running mean: -21.211719\n",
      "ep 3303: ep_len:1442 episode reward: total was 2.160000. running mean: -20.978002\n",
      "ep 3303: ep_len:82 episode reward: total was 39.500000. running mean: -20.373222\n",
      "ep 3303: ep_len:129 episode reward: total was 63.000000. running mean: -19.539489\n",
      "ep 3303: ep_len:58 episode reward: total was 27.500000. running mean: -19.069094\n",
      "ep 3303: ep_len:664 episode reward: total was 16.970000. running mean: -18.708704\n",
      "ep 3303: ep_len:2926 episode reward: total was -492.970000. running mean: -23.451317\n",
      "ep 3303: ep_len:70 episode reward: total was 30.500000. running mean: -22.911803\n",
      "epsilon:0.009992 episode_count: 49680. steps_count: 53501169.000000\n",
      "ep 3304: ep_len:1045 episode reward: total was -34.870000. running mean: -23.031385\n",
      "ep 3304: ep_len:824 episode reward: total was -9.100000. running mean: -22.892071\n",
      "ep 3304: ep_len:2981 episode reward: total was -7.560000. running mean: -22.738751\n",
      "ep 3304: ep_len:530 episode reward: total was -29.210000. running mean: -22.803463\n",
      "ep 3304: ep_len:172 episode reward: total was 80.000000. running mean: -21.775429\n",
      "ep 3304: ep_len:58 episode reward: total was 18.010000. running mean: -21.377574\n",
      "ep 3304: ep_len:962 episode reward: total was -37.250000. running mean: -21.536299\n",
      "ep 3304: ep_len:652 episode reward: total was 16.150000. running mean: -21.159436\n",
      "ep 3304: ep_len:3899 episode reward: total was -467.680000. running mean: -25.624641\n",
      "ep 3304: ep_len:875 episode reward: total was 47.210000. running mean: -24.896295\n",
      "ep 3304: ep_len:668 episode reward: total was -15.620000. running mean: -24.803532\n",
      "ep 3304: ep_len:89 episode reward: total was 41.500000. running mean: -24.140497\n",
      "ep 3304: ep_len:120 episode reward: total was 56.510000. running mean: -23.333992\n",
      "ep 3304: ep_len:749 episode reward: total was -71.460000. running mean: -23.815252\n",
      "ep 3304: ep_len:2924 episode reward: total was -45.190000. running mean: -24.028999\n",
      "ep 3304: ep_len:72 episode reward: total was 34.500000. running mean: -23.443709\n",
      "epsilon:0.009992 episode_count: 49696. steps_count: 53517789.000000\n",
      "ep 3305: ep_len:1511 episode reward: total was -9.850000. running mean: -23.307772\n",
      "ep 3305: ep_len:500 episode reward: total was 14.730000. running mean: -22.927394\n",
      "ep 3305: ep_len:3088 episode reward: total was -42.240000. running mean: -23.120520\n",
      "ep 3305: ep_len:646 episode reward: total was 2.770000. running mean: -22.861615\n",
      "ep 3305: ep_len:51 episode reward: total was 24.000000. running mean: -22.392999\n",
      "ep 3305: ep_len:1031 episode reward: total was -7.030000. running mean: -22.239369\n",
      "ep 3305: ep_len:3785 episode reward: total was -142.250000. running mean: -23.439475\n",
      "ep 3305: ep_len:1183 episode reward: total was -49.280000. running mean: -23.697881\n",
      "ep 3305: ep_len:643 episode reward: total was -3.900000. running mean: -23.499902\n",
      "ep 3305: ep_len:584 episode reward: total was 2.860000. running mean: -23.236303\n",
      "ep 3305: ep_len:58 episode reward: total was 27.500000. running mean: -22.728940\n",
      "ep 3305: ep_len:511 episode reward: total was 4.860000. running mean: -22.453050\n",
      "ep 3305: ep_len:2832 episode reward: total was -17.210000. running mean: -22.400620\n",
      "epsilon:0.009992 episode_count: 49709. steps_count: 53534212.000000\n",
      "ep 3306: ep_len:1088 episode reward: total was -6.950000. running mean: -22.246114\n",
      "ep 3306: ep_len:978 episode reward: total was 23.670000. running mean: -21.786953\n",
      "ep 3306: ep_len:50 episode reward: total was 23.500000. running mean: -21.334083\n",
      "ep 3306: ep_len:3044 episode reward: total was -17.850000. running mean: -21.299242\n",
      "ep 3306: ep_len:649 episode reward: total was -19.550000. running mean: -21.281750\n",
      "ep 3306: ep_len:127 episode reward: total was 60.500000. running mean: -20.463932\n",
      "ep 3306: ep_len:40 episode reward: total was 18.500000. running mean: -20.074293\n",
      "ep 3306: ep_len:500 episode reward: total was 47.400000. running mean: -19.399550\n",
      "ep 3306: ep_len:3729 episode reward: total was 1.070000. running mean: -19.194854\n",
      "ep 3306: ep_len:504 episode reward: total was -9.390000. running mean: -19.096806\n",
      "ep 3306: ep_len:835 episode reward: total was 46.560000. running mean: -18.440238\n",
      "ep 3306: ep_len:1209 episode reward: total was 8.230000. running mean: -18.173536\n",
      "ep 3306: ep_len:1184 episode reward: total was -6.380000. running mean: -18.055600\n",
      "ep 3306: ep_len:46 episode reward: total was 20.000000. running mean: -17.675044\n",
      "epsilon:0.009992 episode_count: 49723. steps_count: 53548195.000000\n",
      "ep 3307: ep_len:988 episode reward: total was -54.170000. running mean: -18.039994\n",
      "ep 3307: ep_len:1547 episode reward: total was -47.840000. running mean: -18.337994\n",
      "ep 3307: ep_len:40 episode reward: total was 18.500000. running mean: -17.969614\n",
      "ep 3307: ep_len:3055 episode reward: total was -20.070000. running mean: -17.990618\n",
      "ep 3307: ep_len:1677 episode reward: total was -36.260000. running mean: -18.173312\n",
      "ep 3307: ep_len:61 episode reward: total was 29.000000. running mean: -17.701578\n",
      "ep 3307: ep_len:150 episode reward: total was 73.500000. running mean: -16.789563\n",
      "ep 3307: ep_len:59 episode reward: total was 28.000000. running mean: -16.341667\n",
      "ep 3307: ep_len:1373 episode reward: total was -95.520000. running mean: -17.133450\n",
      "ep 3307: ep_len:3606 episode reward: total was -67.760000. running mean: -17.639716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3307: ep_len:639 episode reward: total was -50.340000. running mean: -17.966719\n",
      "ep 3307: ep_len:613 episode reward: total was 45.160000. running mean: -17.335451\n",
      "ep 3307: ep_len:627 episode reward: total was 1.530000. running mean: -17.146797\n",
      "ep 3307: ep_len:55 episode reward: total was 21.500000. running mean: -16.760329\n",
      "ep 3307: ep_len:79 episode reward: total was 33.500000. running mean: -16.257726\n",
      "ep 3307: ep_len:1040 episode reward: total was 12.290000. running mean: -15.972248\n",
      "ep 3307: ep_len:2848 episode reward: total was -7.680000. running mean: -15.889326\n",
      "epsilon:0.009992 episode_count: 49740. steps_count: 53566652.000000\n",
      "ep 3308: ep_len:750 episode reward: total was -61.350000. running mean: -16.343933\n",
      "ep 3308: ep_len:664 episode reward: total was -39.900000. running mean: -16.579493\n",
      "ep 3308: ep_len:2999 episode reward: total was -12.920000. running mean: -16.542898\n",
      "ep 3308: ep_len:663 episode reward: total was 28.760000. running mean: -16.089869\n",
      "ep 3308: ep_len:37 episode reward: total was 17.000000. running mean: -15.758971\n",
      "ep 3308: ep_len:83 episode reward: total was 40.000000. running mean: -15.201381\n",
      "ep 3308: ep_len:92 episode reward: total was 44.500000. running mean: -14.604367\n",
      "ep 3308: ep_len:500 episode reward: total was 39.470000. running mean: -14.063624\n",
      "ep 3308: ep_len:3543 episode reward: total was -1239.880000. running mean: -26.321787\n",
      "ep 3308: ep_len:1335 episode reward: total was -49.440000. running mean: -26.552969\n",
      "ep 3308: ep_len:792 episode reward: total was 19.150000. running mean: -26.095940\n",
      "ep 3308: ep_len:1139 episode reward: total was 6.170000. running mean: -25.773280\n",
      "ep 3308: ep_len:52 episode reward: total was 24.500000. running mean: -25.270548\n",
      "ep 3308: ep_len:66 episode reward: total was 28.500000. running mean: -24.732842\n",
      "ep 3308: ep_len:1467 episode reward: total was 3.600000. running mean: -24.449514\n",
      "ep 3308: ep_len:2818 episode reward: total was -20.650000. running mean: -24.411519\n",
      "ep 3308: ep_len:50 episode reward: total was 22.000000. running mean: -23.947403\n",
      "epsilon:0.009992 episode_count: 49757. steps_count: 53583702.000000\n",
      "ep 3309: ep_len:884 episode reward: total was -170.130000. running mean: -25.409229\n",
      "ep 3309: ep_len:208 episode reward: total was 12.910000. running mean: -25.026037\n",
      "ep 3309: ep_len:3073 episode reward: total was -32.570000. running mean: -25.101477\n",
      "ep 3309: ep_len:580 episode reward: total was -6.930000. running mean: -24.919762\n",
      "ep 3309: ep_len:103 episode reward: total was 50.000000. running mean: -24.170564\n",
      "ep 3309: ep_len:500 episode reward: total was 9.370000. running mean: -23.835159\n",
      "ep 3309: ep_len:3620 episode reward: total was -52.870000. running mean: -24.125507\n",
      "ep 3309: ep_len:603 episode reward: total was 33.900000. running mean: -23.545252\n",
      "ep 3309: ep_len:844 episode reward: total was 54.030000. running mean: -22.769499\n",
      "ep 3309: ep_len:1045 episode reward: total was 37.680000. running mean: -22.165004\n",
      "ep 3309: ep_len:78 episode reward: total was 37.500000. running mean: -21.568354\n",
      "ep 3309: ep_len:160 episode reward: total was 77.000000. running mean: -20.582671\n",
      "ep 3309: ep_len:673 episode reward: total was 19.340000. running mean: -20.183444\n",
      "ep 3309: ep_len:2779 episode reward: total was -4.640000. running mean: -20.028010\n",
      "ep 3309: ep_len:71 episode reward: total was 31.000000. running mean: -19.517730\n",
      "epsilon:0.009992 episode_count: 49772. steps_count: 53598923.000000\n",
      "ep 3310: ep_len:1460 episode reward: total was 46.080000. running mean: -18.861752\n",
      "ep 3310: ep_len:734 episode reward: total was -151.250000. running mean: -20.185635\n",
      "ep 3310: ep_len:3056 episode reward: total was -58.610000. running mean: -20.569878\n",
      "ep 3310: ep_len:1670 episode reward: total was -128.820000. running mean: -21.652380\n",
      "ep 3310: ep_len:31 episode reward: total was 12.500000. running mean: -21.310856\n",
      "ep 3310: ep_len:1418 episode reward: total was -236.470000. running mean: -23.462447\n",
      "ep 3310: ep_len:608 episode reward: total was 32.090000. running mean: -22.906923\n",
      "ep 3310: ep_len:832 episode reward: total was 12.680000. running mean: -22.551054\n",
      "ep 3310: ep_len:824 episode reward: total was 27.950000. running mean: -22.046043\n",
      "ep 3310: ep_len:935 episode reward: total was 44.520000. running mean: -21.380383\n",
      "ep 3310: ep_len:46 episode reward: total was 21.500000. running mean: -20.951579\n",
      "ep 3310: ep_len:95 episode reward: total was 46.000000. running mean: -20.282063\n",
      "ep 3310: ep_len:1480 episode reward: total was 18.670000. running mean: -19.892542\n",
      "ep 3310: ep_len:2824 episode reward: total was -8.440000. running mean: -19.778017\n",
      "epsilon:0.009992 episode_count: 49786. steps_count: 53614936.000000\n",
      "ep 3311: ep_len:662 episode reward: total was -10.720000. running mean: -19.687437\n",
      "ep 3311: ep_len:946 episode reward: total was 6.660000. running mean: -19.423962\n",
      "ep 3311: ep_len:39 episode reward: total was 18.000000. running mean: -19.049723\n",
      "ep 3311: ep_len:3015 episode reward: total was -20.540000. running mean: -19.064626\n",
      "ep 3311: ep_len:853 episode reward: total was -9.120000. running mean: -18.965179\n",
      "ep 3311: ep_len:59 episode reward: total was 28.000000. running mean: -18.495528\n",
      "ep 3311: ep_len:107 episode reward: total was 50.500000. running mean: -17.805572\n",
      "ep 3311: ep_len:835 episode reward: total was 46.870000. running mean: -17.158817\n",
      "ep 3311: ep_len:3760 episode reward: total was -35.010000. running mean: -17.337328\n",
      "ep 3311: ep_len:4208 episode reward: total was -675.740000. running mean: -23.921355\n",
      "ep 3311: ep_len:615 episode reward: total was -0.480000. running mean: -23.686942\n",
      "ep 3311: ep_len:797 episode reward: total was 7.790000. running mean: -23.372172\n",
      "ep 3311: ep_len:83 episode reward: total was 38.500000. running mean: -22.753450\n",
      "ep 3311: ep_len:34 episode reward: total was 15.500000. running mean: -22.370916\n",
      "ep 3311: ep_len:652 episode reward: total was 5.730000. running mean: -22.089907\n",
      "ep 3311: ep_len:2901 episode reward: total was -209.900000. running mean: -23.968008\n",
      "epsilon:0.009992 episode_count: 49802. steps_count: 53634502.000000\n",
      "ep 3312: ep_len:708 episode reward: total was -19.350000. running mean: -23.921828\n",
      "ep 3312: ep_len:500 episode reward: total was 29.730000. running mean: -23.385309\n",
      "ep 3312: ep_len:38 episode reward: total was 16.000000. running mean: -22.991456\n",
      "ep 3312: ep_len:3006 episode reward: total was -29.060000. running mean: -23.052142\n",
      "ep 3312: ep_len:783 episode reward: total was -17.590000. running mean: -22.997520\n",
      "ep 3312: ep_len:46 episode reward: total was 21.500000. running mean: -22.552545\n",
      "ep 3312: ep_len:104 episode reward: total was 46.000000. running mean: -21.867020\n",
      "ep 3312: ep_len:68 episode reward: total was 32.500000. running mean: -21.323349\n",
      "ep 3312: ep_len:501 episode reward: total was 20.680000. running mean: -20.903316\n",
      "ep 3312: ep_len:4109 episode reward: total was -935.580000. running mean: -30.050083\n",
      "ep 3312: ep_len:842 episode reward: total was 29.340000. running mean: -29.456182\n",
      "ep 3312: ep_len:763 episode reward: total was 0.210000. running mean: -29.159520\n",
      "ep 3312: ep_len:1501 episode reward: total was 21.940000. running mean: -28.648525\n",
      "ep 3312: ep_len:38 episode reward: total was 17.500000. running mean: -28.187040\n",
      "ep 3312: ep_len:500 episode reward: total was -8.510000. running mean: -27.990269\n",
      "ep 3312: ep_len:2880 episode reward: total was -23.370000. running mean: -27.944067\n",
      "ep 3312: ep_len:67 episode reward: total was 32.000000. running mean: -27.344626\n",
      "epsilon:0.009992 episode_count: 49819. steps_count: 53650956.000000\n",
      "ep 3313: ep_len:655 episode reward: total was -25.330000. running mean: -27.324480\n",
      "ep 3313: ep_len:708 episode reward: total was 6.220000. running mean: -26.989035\n",
      "ep 3313: ep_len:53 episode reward: total was 23.500000. running mean: -26.484144\n",
      "ep 3313: ep_len:2966 episode reward: total was -17.490000. running mean: -26.394203\n",
      "ep 3313: ep_len:644 episode reward: total was -20.090000. running mean: -26.331161\n",
      "ep 3313: ep_len:53 episode reward: total was 25.000000. running mean: -25.817849\n",
      "ep 3313: ep_len:52 episode reward: total was 24.500000. running mean: -25.314671\n",
      "ep 3313: ep_len:677 episode reward: total was -47.170000. running mean: -25.533224\n",
      "ep 3313: ep_len:3690 episode reward: total was -29.980000. running mean: -25.577692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3313: ep_len:841 episode reward: total was 12.160000. running mean: -25.200315\n",
      "ep 3313: ep_len:807 episode reward: total was 20.800000. running mean: -24.740312\n",
      "ep 3313: ep_len:631 episode reward: total was -8.250000. running mean: -24.575409\n",
      "ep 3313: ep_len:50 episode reward: total was 22.000000. running mean: -24.109655\n",
      "ep 3313: ep_len:93 episode reward: total was 45.000000. running mean: -23.418558\n",
      "ep 3313: ep_len:645 episode reward: total was -24.140000. running mean: -23.425773\n",
      "ep 3313: ep_len:2828 episode reward: total was 2.440000. running mean: -23.167115\n",
      "epsilon:0.009992 episode_count: 49835. steps_count: 53666349.000000\n",
      "ep 3314: ep_len:1495 episode reward: total was 1.310000. running mean: -22.922344\n",
      "ep 3314: ep_len:500 episode reward: total was 19.230000. running mean: -22.500820\n",
      "ep 3314: ep_len:3022 episode reward: total was -35.040000. running mean: -22.626212\n",
      "ep 3314: ep_len:864 episode reward: total was 12.600000. running mean: -22.273950\n",
      "ep 3314: ep_len:42 episode reward: total was 19.500000. running mean: -21.856210\n",
      "ep 3314: ep_len:122 episode reward: total was 59.500000. running mean: -21.042648\n",
      "ep 3314: ep_len:615 episode reward: total was 44.710000. running mean: -20.385122\n",
      "ep 3314: ep_len:3692 episode reward: total was -10.340000. running mean: -20.284671\n",
      "ep 3314: ep_len:515 episode reward: total was -50.050000. running mean: -20.582324\n",
      "ep 3314: ep_len:788 episode reward: total was 18.960000. running mean: -20.186901\n",
      "ep 3314: ep_len:986 episode reward: total was 19.700000. running mean: -19.788032\n",
      "ep 3314: ep_len:180 episode reward: total was 85.500000. running mean: -18.735151\n",
      "ep 3314: ep_len:40 episode reward: total was 18.500000. running mean: -18.362800\n",
      "ep 3314: ep_len:1437 episode reward: total was 26.260000. running mean: -17.916572\n",
      "ep 3314: ep_len:2838 episode reward: total was -1.720000. running mean: -17.754606\n",
      "epsilon:0.009992 episode_count: 49850. steps_count: 53683485.000000\n",
      "ep 3315: ep_len:1453 episode reward: total was 4.040000. running mean: -17.536660\n",
      "ep 3315: ep_len:1009 episode reward: total was 11.830000. running mean: -17.242993\n",
      "ep 3315: ep_len:64 episode reward: total was 30.500000. running mean: -16.765564\n",
      "ep 3315: ep_len:2966 episode reward: total was -19.070000. running mean: -16.788608\n",
      "ep 3315: ep_len:512 episode reward: total was -21.310000. running mean: -16.833822\n",
      "ep 3315: ep_len:100 episode reward: total was 48.500000. running mean: -16.180484\n",
      "ep 3315: ep_len:43 episode reward: total was 20.000000. running mean: -15.818679\n",
      "ep 3315: ep_len:1412 episode reward: total was 26.290000. running mean: -15.397592\n",
      "ep 3315: ep_len:3864 episode reward: total was -32.170000. running mean: -15.565316\n",
      "ep 3315: ep_len:1241 episode reward: total was -55.430000. running mean: -15.963963\n",
      "ep 3315: ep_len:660 episode reward: total was -7.700000. running mean: -15.881323\n",
      "ep 3315: ep_len:710 episode reward: total was 28.890000. running mean: -15.433610\n",
      "ep 3315: ep_len:38 episode reward: total was 17.500000. running mean: -15.104274\n",
      "ep 3315: ep_len:151 episode reward: total was 74.000000. running mean: -14.213231\n",
      "ep 3315: ep_len:1083 episode reward: total was 19.570000. running mean: -13.875399\n",
      "ep 3315: ep_len:2848 episode reward: total was -25.370000. running mean: -13.990345\n",
      "epsilon:0.009992 episode_count: 49866. steps_count: 53701639.000000\n",
      "ep 3316: ep_len:678 episode reward: total was 2.030000. running mean: -13.830141\n",
      "ep 3316: ep_len:716 episode reward: total was -3.610000. running mean: -13.727940\n",
      "ep 3316: ep_len:47 episode reward: total was 22.000000. running mean: -13.370661\n",
      "ep 3316: ep_len:2932 episode reward: total was 19.500000. running mean: -13.041954\n",
      "ep 3316: ep_len:669 episode reward: total was -8.580000. running mean: -12.997334\n",
      "ep 3316: ep_len:103 episode reward: total was 48.500000. running mean: -12.382361\n",
      "ep 3316: ep_len:500 episode reward: total was 69.450000. running mean: -11.564038\n",
      "ep 3316: ep_len:3906 episode reward: total was -55.670000. running mean: -12.005097\n",
      "ep 3316: ep_len:598 episode reward: total was 11.910000. running mean: -11.765946\n",
      "ep 3316: ep_len:899 episode reward: total was 59.080000. running mean: -11.057487\n",
      "ep 3316: ep_len:571 episode reward: total was -24.760000. running mean: -11.194512\n",
      "ep 3316: ep_len:41 episode reward: total was 19.000000. running mean: -10.892567\n",
      "ep 3316: ep_len:1556 episode reward: total was 27.910000. running mean: -10.504541\n",
      "ep 3316: ep_len:2800 episode reward: total was -3.480000. running mean: -10.434296\n",
      "epsilon:0.009992 episode_count: 49880. steps_count: 53717655.000000\n",
      "ep 3317: ep_len:928 episode reward: total was -67.540000. running mean: -11.005353\n",
      "ep 3317: ep_len:678 episode reward: total was -13.160000. running mean: -11.026899\n",
      "ep 3317: ep_len:69 episode reward: total was 33.000000. running mean: -10.586630\n",
      "ep 3317: ep_len:2993 episode reward: total was -35.950000. running mean: -10.840264\n",
      "ep 3317: ep_len:787 episode reward: total was -22.140000. running mean: -10.953261\n",
      "ep 3317: ep_len:500 episode reward: total was 24.250000. running mean: -10.601229\n",
      "ep 3317: ep_len:660 episode reward: total was 20.330000. running mean: -10.291916\n",
      "ep 3317: ep_len:1220 episode reward: total was -33.320000. running mean: -10.522197\n",
      "ep 3317: ep_len:852 episode reward: total was 52.700000. running mean: -9.889975\n",
      "ep 3317: ep_len:862 episode reward: total was 14.510000. running mean: -9.645975\n",
      "ep 3317: ep_len:77 episode reward: total was 35.500000. running mean: -9.194516\n",
      "ep 3317: ep_len:61 episode reward: total was 29.000000. running mean: -8.812571\n",
      "ep 3317: ep_len:1246 episode reward: total was 13.990000. running mean: -8.584545\n",
      "ep 3317: ep_len:2866 episode reward: total was -2.940000. running mean: -8.528099\n",
      "ep 3317: ep_len:55 episode reward: total was 26.000000. running mean: -8.182818\n",
      "epsilon:0.009992 episode_count: 49895. steps_count: 53731509.000000\n",
      "ep 3318: ep_len:846 episode reward: total was -67.440000. running mean: -8.775390\n",
      "ep 3318: ep_len:1297 episode reward: total was -29.530000. running mean: -8.982936\n",
      "ep 3318: ep_len:46 episode reward: total was 21.500000. running mean: -8.678107\n",
      "ep 3318: ep_len:89 episode reward: total was 40.000000. running mean: -8.191326\n",
      "ep 3318: ep_len:631 episode reward: total was -8.650000. running mean: -8.195913\n",
      "ep 3318: ep_len:41 episode reward: total was 16.000000. running mean: -7.953953\n",
      "ep 3318: ep_len:157 episode reward: total was 71.000000. running mean: -7.164414\n",
      "ep 3318: ep_len:1109 episode reward: total was 23.700000. running mean: -6.855770\n",
      "ep 3318: ep_len:288 episode reward: total was -146.700000. running mean: -8.254212\n",
      "ep 3318: ep_len:683 episode reward: total was -53.940000. running mean: -8.711070\n",
      "ep 3318: ep_len:7314 episode reward: total was 42.240000. running mean: -8.201559\n",
      "ep 3318: ep_len:997 episode reward: total was -12.720000. running mean: -8.246744\n",
      "ep 3318: ep_len:91 episode reward: total was 41.000000. running mean: -7.754276\n",
      "ep 3318: ep_len:97 episode reward: total was 47.000000. running mean: -7.206733\n",
      "ep 3318: ep_len:1428 episode reward: total was 24.150000. running mean: -6.893166\n",
      "ep 3318: ep_len:2843 episode reward: total was 0.260000. running mean: -6.821635\n",
      "ep 3318: ep_len:54 episode reward: total was 22.500000. running mean: -6.528418\n",
      "epsilon:0.009992 episode_count: 49912. steps_count: 53749520.000000\n",
      "ep 3319: ep_len:1470 episode reward: total was 31.640000. running mean: -6.146734\n",
      "ep 3319: ep_len:741 episode reward: total was -12.960000. running mean: -6.214867\n",
      "ep 3319: ep_len:2888 episode reward: total was -49.780000. running mean: -6.650518\n",
      "ep 3319: ep_len:1630 episode reward: total was -52.370000. running mean: -7.107713\n",
      "ep 3319: ep_len:65 episode reward: total was 31.000000. running mean: -6.726636\n",
      "ep 3319: ep_len:658 episode reward: total was -0.200000. running mean: -6.661369\n",
      "ep 3319: ep_len:687 episode reward: total was 35.970000. running mean: -6.235056\n",
      "ep 3319: ep_len:523 episode reward: total was -17.560000. running mean: -6.348305\n",
      "ep 3319: ep_len:893 episode reward: total was 48.520000. running mean: -5.799622\n",
      "ep 3319: ep_len:675 episode reward: total was -7.750000. running mean: -5.819126\n",
      "ep 3319: ep_len:97 episode reward: total was 45.500000. running mean: -5.305935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3319: ep_len:1504 episode reward: total was 20.870000. running mean: -5.044175\n",
      "ep 3319: ep_len:2839 episode reward: total was -20.470000. running mean: -5.198433\n",
      "epsilon:0.009992 episode_count: 49925. steps_count: 53764190.000000\n",
      "ep 3320: ep_len:614 episode reward: total was -13.220000. running mean: -5.278649\n",
      "ep 3320: ep_len:823 episode reward: total was 9.580000. running mean: -5.130063\n",
      "ep 3320: ep_len:3003 episode reward: total was -12.450000. running mean: -5.203262\n",
      "ep 3320: ep_len:777 episode reward: total was -23.190000. running mean: -5.383129\n",
      "ep 3320: ep_len:48 episode reward: total was 21.000000. running mean: -5.119298\n",
      "ep 3320: ep_len:80 episode reward: total was 38.500000. running mean: -4.683105\n",
      "ep 3320: ep_len:500 episode reward: total was 31.050000. running mean: -4.325774\n",
      "ep 3320: ep_len:643 episode reward: total was 30.510000. running mean: -3.977416\n",
      "ep 3320: ep_len:852 episode reward: total was 20.900000. running mean: -3.728642\n",
      "ep 3320: ep_len:7276 episode reward: total was 32.310000. running mean: -3.368256\n",
      "ep 3320: ep_len:956 episode reward: total was -0.920000. running mean: -3.343773\n",
      "ep 3320: ep_len:211 episode reward: total was 104.000000. running mean: -2.270335\n",
      "ep 3320: ep_len:71 episode reward: total was 34.000000. running mean: -1.907632\n",
      "ep 3320: ep_len:1188 episode reward: total was 0.850000. running mean: -1.880056\n",
      "ep 3320: ep_len:2899 episode reward: total was -21.830000. running mean: -2.079555\n",
      "ep 3320: ep_len:67 episode reward: total was 30.500000. running mean: -1.753760\n",
      "epsilon:0.009992 episode_count: 49941. steps_count: 53784198.000000\n",
      "ep 3321: ep_len:686 episode reward: total was -47.820000. running mean: -2.214422\n",
      "ep 3321: ep_len:704 episode reward: total was -11.020000. running mean: -2.302478\n",
      "ep 3321: ep_len:3024 episode reward: total was -26.470000. running mean: -2.544153\n",
      "ep 3321: ep_len:1203 episode reward: total was -15.590000. running mean: -2.674612\n",
      "ep 3321: ep_len:59 episode reward: total was 28.000000. running mean: -2.367865\n",
      "ep 3321: ep_len:157 episode reward: total was 72.500000. running mean: -1.619187\n",
      "ep 3321: ep_len:73 episode reward: total was 35.000000. running mean: -1.252995\n",
      "ep 3321: ep_len:500 episode reward: total was 29.950000. running mean: -0.940965\n",
      "ep 3321: ep_len:3543 episode reward: total was -6.780000. running mean: -0.999355\n",
      "ep 3321: ep_len:835 episode reward: total was 28.260000. running mean: -0.706762\n",
      "ep 3321: ep_len:688 episode reward: total was 30.880000. running mean: -0.390894\n",
      "ep 3321: ep_len:1546 episode reward: total was 11.720000. running mean: -0.269785\n",
      "ep 3321: ep_len:87 episode reward: total was 42.000000. running mean: 0.152913\n",
      "ep 3321: ep_len:152 episode reward: total was 73.000000. running mean: 0.881384\n",
      "ep 3321: ep_len:65 episode reward: total was 31.000000. running mean: 1.182570\n",
      "ep 3321: ep_len:75 episode reward: total was 34.500000. running mean: 1.515744\n",
      "ep 3321: ep_len:625 episode reward: total was 7.360000. running mean: 1.574187\n",
      "ep 3321: ep_len:2837 episode reward: total was -21.160000. running mean: 1.346845\n",
      "epsilon:0.009992 episode_count: 49959. steps_count: 53801057.000000\n",
      "ep 3322: ep_len:642 episode reward: total was 25.200000. running mean: 1.585376\n",
      "ep 3322: ep_len:827 episode reward: total was 15.580000. running mean: 1.725323\n",
      "ep 3322: ep_len:74 episode reward: total was 35.500000. running mean: 2.063069\n",
      "ep 3322: ep_len:3057 episode reward: total was -38.900000. running mean: 1.653439\n",
      "ep 3322: ep_len:872 episode reward: total was -8.020000. running mean: 1.556704\n",
      "ep 3322: ep_len:92 episode reward: total was 41.500000. running mean: 1.956137\n",
      "ep 3322: ep_len:50 episode reward: total was 23.500000. running mean: 2.171576\n",
      "ep 3322: ep_len:938 episode reward: total was 73.480000. running mean: 2.884660\n",
      "ep 3322: ep_len:655 episode reward: total was 24.350000. running mean: 3.099313\n",
      "ep 3322: ep_len:3794 episode reward: total was -464.230000. running mean: -1.573980\n",
      "ep 3322: ep_len:635 episode reward: total was -1.870000. running mean: -1.576940\n",
      "ep 3322: ep_len:568 episode reward: total was -8.630000. running mean: -1.647471\n",
      "ep 3322: ep_len:165 episode reward: total was 78.000000. running mean: -0.850996\n",
      "ep 3322: ep_len:909 episode reward: total was -41.580000. running mean: -1.258286\n",
      "ep 3322: ep_len:2797 episode reward: total was -20.130000. running mean: -1.447003\n",
      "epsilon:0.009992 episode_count: 49974. steps_count: 53817132.000000\n",
      "ep 3323: ep_len:1162 episode reward: total was 8.430000. running mean: -1.348233\n",
      "ep 3323: ep_len:867 episode reward: total was 4.940000. running mean: -1.285351\n",
      "ep 3323: ep_len:2985 episode reward: total was -42.430000. running mean: -1.696797\n",
      "ep 3323: ep_len:708 episode reward: total was -13.290000. running mean: -1.812729\n",
      "ep 3323: ep_len:102 episode reward: total was 49.500000. running mean: -1.299602\n",
      "ep 3323: ep_len:78 episode reward: total was 31.500000. running mean: -0.971606\n",
      "ep 3323: ep_len:1439 episode reward: total was -181.320000. running mean: -2.775090\n",
      "ep 3323: ep_len:3617 episode reward: total was 12.140000. running mean: -2.625939\n",
      "ep 3323: ep_len:1560 episode reward: total was -65.840000. running mean: -3.258079\n",
      "ep 3323: ep_len:611 episode reward: total was -3.390000. running mean: -3.259399\n",
      "ep 3323: ep_len:563 episode reward: total was -0.080000. running mean: -3.227605\n",
      "ep 3323: ep_len:80 episode reward: total was 37.000000. running mean: -2.825329\n",
      "ep 3323: ep_len:138 episode reward: total was 64.010000. running mean: -2.156975\n",
      "ep 3323: ep_len:1133 episode reward: total was 13.310000. running mean: -2.002306\n",
      "ep 3323: ep_len:2908 episode reward: total was -9.350000. running mean: -2.075783\n",
      "ep 3323: ep_len:53 episode reward: total was 25.000000. running mean: -1.805025\n",
      "epsilon:0.009992 episode_count: 49990. steps_count: 53835136.000000\n",
      "ep 3324: ep_len:640 episode reward: total was -24.040000. running mean: -2.027374\n",
      "ep 3324: ep_len:500 episode reward: total was 15.160000. running mean: -1.855501\n",
      "ep 3324: ep_len:2948 episode reward: total was -28.970000. running mean: -2.126646\n",
      "ep 3324: ep_len:1686 episode reward: total was -28.490000. running mean: -2.390279\n",
      "ep 3324: ep_len:48 episode reward: total was 22.500000. running mean: -2.141376\n",
      "ep 3324: ep_len:50 episode reward: total was 22.000000. running mean: -1.899963\n",
      "ep 3324: ep_len:602 episode reward: total was -5.510000. running mean: -1.936063\n",
      "ep 3324: ep_len:677 episode reward: total was 28.830000. running mean: -1.628402\n",
      "ep 3324: ep_len:1121 episode reward: total was -12.250000. running mean: -1.734618\n",
      "ep 3324: ep_len:874 episode reward: total was 56.200000. running mean: -1.155272\n",
      "ep 3324: ep_len:653 episode reward: total was -9.760000. running mean: -1.241320\n",
      "ep 3324: ep_len:215 episode reward: total was 100.000000. running mean: -0.228906\n",
      "ep 3324: ep_len:41 episode reward: total was 17.500000. running mean: -0.051617\n",
      "ep 3324: ep_len:1126 episode reward: total was -6.080000. running mean: -0.111901\n",
      "ep 3324: ep_len:2772 episode reward: total was -20.990000. running mean: -0.320682\n",
      "ep 3324: ep_len:75 episode reward: total was 34.500000. running mean: 0.027525\n",
      "epsilon:0.009992 episode_count: 50006. steps_count: 53849164.000000\n",
      "ep 3325: ep_len:1122 episode reward: total was 9.800000. running mean: 0.125249\n",
      "ep 3325: ep_len:975 episode reward: total was 8.970000. running mean: 0.213697\n",
      "ep 3325: ep_len:44 episode reward: total was 19.000000. running mean: 0.401560\n",
      "ep 3325: ep_len:2980 episode reward: total was -35.420000. running mean: 0.043344\n",
      "ep 3325: ep_len:1143 episode reward: total was -13.990000. running mean: -0.096989\n",
      "ep 3325: ep_len:47 episode reward: total was 22.000000. running mean: 0.123981\n",
      "ep 3325: ep_len:128 episode reward: total was 61.000000. running mean: 0.732741\n",
      "ep 3325: ep_len:75 episode reward: total was 34.500000. running mean: 1.070414\n",
      "ep 3325: ep_len:1030 episode reward: total was -35.900000. running mean: 0.700710\n",
      "ep 3325: ep_len:3492 episode reward: total was -11.790000. running mean: 0.575802\n",
      "ep 3325: ep_len:604 episode reward: total was -33.920000. running mean: 0.230844\n",
      "ep 3325: ep_len:720 episode reward: total was -12.560000. running mean: 0.102936\n",
      "ep 3325: ep_len:683 episode reward: total was -10.510000. running mean: -0.003193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3325: ep_len:77 episode reward: total was 37.000000. running mean: 0.366839\n",
      "ep 3325: ep_len:857 episode reward: total was -27.970000. running mean: 0.083470\n",
      "ep 3325: ep_len:2762 episode reward: total was -15.550000. running mean: -0.072865\n",
      "ep 3325: ep_len:51 episode reward: total was 24.000000. running mean: 0.167864\n",
      "epsilon:0.009992 episode_count: 50023. steps_count: 53865954.000000\n",
      "ep 3326: ep_len:642 episode reward: total was -94.110000. running mean: -0.774915\n",
      "ep 3326: ep_len:1565 episode reward: total was -20.210000. running mean: -0.969265\n",
      "ep 3326: ep_len:66 episode reward: total was 31.500000. running mean: -0.644573\n",
      "ep 3326: ep_len:3013 episode reward: total was -11.900000. running mean: -0.757127\n",
      "ep 3326: ep_len:911 episode reward: total was 70.700000. running mean: -0.042556\n",
      "ep 3326: ep_len:102 episode reward: total was 48.000000. running mean: 0.437870\n",
      "ep 3326: ep_len:739 episode reward: total was -9.950000. running mean: 0.333991\n",
      "ep 3326: ep_len:668 episode reward: total was 23.380000. running mean: 0.564451\n",
      "ep 3326: ep_len:865 episode reward: total was 20.480000. running mean: 0.763607\n",
      "ep 3326: ep_len:690 episode reward: total was 47.770000. running mean: 1.233671\n",
      "ep 3326: ep_len:500 episode reward: total was 36.010000. running mean: 1.581434\n",
      "ep 3326: ep_len:81 episode reward: total was 36.000000. running mean: 1.925620\n",
      "ep 3326: ep_len:1149 episode reward: total was -7.870000. running mean: 1.827663\n",
      "ep 3326: ep_len:2729 episode reward: total was -26.380000. running mean: 1.545587\n",
      "epsilon:0.009992 episode_count: 50037. steps_count: 53879674.000000\n",
      "ep 3327: ep_len:1106 episode reward: total was 1.620000. running mean: 1.546331\n",
      "ep 3327: ep_len:1634 episode reward: total was -19.110000. running mean: 1.339768\n",
      "ep 3327: ep_len:3025 episode reward: total was -42.410000. running mean: 0.902270\n",
      "ep 3327: ep_len:624 episode reward: total was -2.010000. running mean: 0.873147\n",
      "ep 3327: ep_len:47 episode reward: total was 19.000000. running mean: 1.054416\n",
      "ep 3327: ep_len:104 episode reward: total was 47.500000. running mean: 1.518872\n",
      "ep 3327: ep_len:1459 episode reward: total was -69.410000. running mean: 0.809583\n",
      "ep 3327: ep_len:687 episode reward: total was 25.410000. running mean: 1.055587\n",
      "ep 3327: ep_len:500 episode reward: total was 27.400000. running mean: 1.319031\n",
      "ep 3327: ep_len:750 episode reward: total was 12.910000. running mean: 1.434941\n",
      "ep 3327: ep_len:650 episode reward: total was 18.460000. running mean: 1.605191\n",
      "ep 3327: ep_len:58 episode reward: total was 27.500000. running mean: 1.864140\n",
      "ep 3327: ep_len:189 episode reward: total was 93.000000. running mean: 2.775498\n",
      "ep 3327: ep_len:777 episode reward: total was -34.300000. running mean: 2.404743\n",
      "ep 3327: ep_len:2845 episode reward: total was -15.150000. running mean: 2.229196\n",
      "ep 3327: ep_len:43 episode reward: total was 20.000000. running mean: 2.406904\n",
      "epsilon:0.009992 episode_count: 50053. steps_count: 53894172.000000\n",
      "ep 3328: ep_len:957 episode reward: total was -64.780000. running mean: 1.735035\n",
      "ep 3328: ep_len:500 episode reward: total was 22.320000. running mean: 1.940884\n",
      "ep 3328: ep_len:50 episode reward: total was 22.000000. running mean: 2.141476\n",
      "ep 3328: ep_len:2879 episode reward: total was -30.830000. running mean: 1.811761\n",
      "ep 3328: ep_len:633 episode reward: total was -38.770000. running mean: 1.405943\n",
      "ep 3328: ep_len:49 episode reward: total was 23.000000. running mean: 1.621884\n",
      "ep 3328: ep_len:111 episode reward: total was 54.000000. running mean: 2.145665\n",
      "ep 3328: ep_len:41 episode reward: total was 19.000000. running mean: 2.314208\n",
      "ep 3328: ep_len:500 episode reward: total was 43.080000. running mean: 2.721866\n",
      "ep 3328: ep_len:3694 episode reward: total was -22.930000. running mean: 2.465347\n",
      "ep 3328: ep_len:2243 episode reward: total was -154.980000. running mean: 0.890894\n",
      "ep 3328: ep_len:899 episode reward: total was 69.860000. running mean: 1.580585\n",
      "ep 3328: ep_len:1149 episode reward: total was -28.070000. running mean: 1.284079\n",
      "ep 3328: ep_len:34 episode reward: total was 15.500000. running mean: 1.426238\n",
      "ep 3328: ep_len:90 episode reward: total was 42.000000. running mean: 1.831976\n",
      "ep 3328: ep_len:1468 episode reward: total was 7.530000. running mean: 1.888956\n",
      "ep 3328: ep_len:2850 episode reward: total was 1.220000. running mean: 1.882267\n",
      "epsilon:0.009992 episode_count: 50070. steps_count: 53912319.000000\n",
      "ep 3329: ep_len:1178 episode reward: total was 10.310000. running mean: 1.966544\n",
      "ep 3329: ep_len:773 episode reward: total was 20.310000. running mean: 2.149979\n",
      "ep 3329: ep_len:68 episode reward: total was 32.500000. running mean: 2.453479\n",
      "ep 3329: ep_len:2985 episode reward: total was -65.750000. running mean: 1.771444\n",
      "ep 3329: ep_len:526 episode reward: total was -14.100000. running mean: 1.612730\n",
      "ep 3329: ep_len:58 episode reward: total was 27.500000. running mean: 1.871602\n",
      "ep 3329: ep_len:1023 episode reward: total was -17.420000. running mean: 1.678686\n",
      "ep 3329: ep_len:3653 episode reward: total was -28.880000. running mean: 1.373099\n",
      "ep 3329: ep_len:2146 episode reward: total was -152.430000. running mean: -0.164932\n",
      "ep 3329: ep_len:629 episode reward: total was -7.410000. running mean: -0.237382\n",
      "ep 3329: ep_len:617 episode reward: total was -6.590000. running mean: -0.300908\n",
      "ep 3329: ep_len:155 episode reward: total was 76.000000. running mean: 0.462101\n",
      "ep 3329: ep_len:49 episode reward: total was 23.000000. running mean: 0.687480\n",
      "ep 3329: ep_len:922 episode reward: total was -0.530000. running mean: 0.675305\n",
      "ep 3329: ep_len:2862 episode reward: total was -2.370000. running mean: 0.644852\n",
      "epsilon:0.009992 episode_count: 50085. steps_count: 53929963.000000\n",
      "ep 3330: ep_len:1106 episode reward: total was -14.850000. running mean: 0.489903\n",
      "ep 3330: ep_len:731 episode reward: total was -20.130000. running mean: 0.283704\n",
      "ep 3330: ep_len:71 episode reward: total was 34.000000. running mean: 0.620867\n",
      "ep 3330: ep_len:90 episode reward: total was 42.000000. running mean: 1.034659\n",
      "ep 3330: ep_len:500 episode reward: total was 21.400000. running mean: 1.238312\n",
      "ep 3330: ep_len:95 episode reward: total was 43.000000. running mean: 1.655929\n",
      "ep 3330: ep_len:1003 episode reward: total was -23.470000. running mean: 1.404670\n",
      "ep 3330: ep_len:655 episode reward: total was 17.530000. running mean: 1.565923\n",
      "ep 3330: ep_len:2126 episode reward: total was -153.240000. running mean: 0.017864\n",
      "ep 3330: ep_len:873 episode reward: total was 64.120000. running mean: 0.658885\n",
      "ep 3330: ep_len:500 episode reward: total was 15.430000. running mean: 0.806596\n",
      "ep 3330: ep_len:171 episode reward: total was 84.000000. running mean: 1.638530\n",
      "ep 3330: ep_len:36 episode reward: total was 16.500000. running mean: 1.787145\n",
      "ep 3330: ep_len:1481 episode reward: total was -13.800000. running mean: 1.631273\n",
      "ep 3330: ep_len:2907 episode reward: total was -11.530000. running mean: 1.499661\n",
      "epsilon:0.009992 episode_count: 50100. steps_count: 53942308.000000\n",
      "ep 3331: ep_len:1456 episode reward: total was 7.780000. running mean: 1.562464\n",
      "ep 3331: ep_len:1686 episode reward: total was -42.410000. running mean: 1.122739\n",
      "ep 3331: ep_len:30 episode reward: total was 13.500000. running mean: 1.246512\n",
      "ep 3331: ep_len:3003 episode reward: total was -16.120000. running mean: 1.072847\n",
      "ep 3331: ep_len:702 episode reward: total was 9.500000. running mean: 1.157118\n",
      "ep 3331: ep_len:39 episode reward: total was 18.000000. running mean: 1.325547\n",
      "ep 3331: ep_len:1409 episode reward: total was -6.190000. running mean: 1.250392\n",
      "ep 3331: ep_len:644 episode reward: total was 12.370000. running mean: 1.361588\n",
      "ep 3331: ep_len:1285 episode reward: total was -75.980000. running mean: 0.588172\n",
      "ep 3331: ep_len:7253 episode reward: total was 64.770000. running mean: 1.229990\n",
      "ep 3331: ep_len:1136 episode reward: total was -25.660000. running mean: 0.961090\n",
      "ep 3331: ep_len:180 episode reward: total was 85.500000. running mean: 1.806479\n",
      "ep 3331: ep_len:1145 episode reward: total was -90.580000. running mean: 0.882615\n",
      "ep 3331: ep_len:2719 episode reward: total was -10.440000. running mean: 0.769389\n",
      "ep 3331: ep_len:54 episode reward: total was 25.500000. running mean: 1.016695\n",
      "epsilon:0.009992 episode_count: 50115. steps_count: 53965049.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3332: ep_len:621 episode reward: total was 8.300000. running mean: 1.089528\n",
      "ep 3332: ep_len:607 episode reward: total was -41.370000. running mean: 0.664932\n",
      "ep 3332: ep_len:2985 episode reward: total was -60.760000. running mean: 0.050683\n",
      "ep 3332: ep_len:533 episode reward: total was -2.020000. running mean: 0.029976\n",
      "ep 3332: ep_len:56 episode reward: total was 26.500000. running mean: 0.294677\n",
      "ep 3332: ep_len:41 episode reward: total was 19.000000. running mean: 0.481730\n",
      "ep 3332: ep_len:1026 episode reward: total was 9.760000. running mean: 0.574512\n",
      "ep 3332: ep_len:334 episode reward: total was 7.380000. running mean: 0.642567\n",
      "ep 3332: ep_len:1274 episode reward: total was -47.450000. running mean: 0.161642\n",
      "ep 3332: ep_len:768 episode reward: total was 16.400000. running mean: 0.324025\n",
      "ep 3332: ep_len:624 episode reward: total was -12.110000. running mean: 0.199685\n",
      "ep 3332: ep_len:80 episode reward: total was 38.500000. running mean: 0.582688\n",
      "ep 3332: ep_len:155 episode reward: total was 74.500000. running mean: 1.321861\n",
      "ep 3332: ep_len:626 episode reward: total was -41.230000. running mean: 0.896343\n",
      "ep 3332: ep_len:2870 episode reward: total was -45.530000. running mean: 0.432079\n",
      "epsilon:0.009992 episode_count: 50130. steps_count: 53977649.000000\n",
      "ep 3333: ep_len:661 episode reward: total was 1.390000. running mean: 0.441658\n",
      "ep 3333: ep_len:500 episode reward: total was 20.240000. running mean: 0.639642\n",
      "ep 3333: ep_len:2932 episode reward: total was -37.020000. running mean: 0.263045\n",
      "ep 3333: ep_len:722 episode reward: total was 3.430000. running mean: 0.294715\n",
      "ep 3333: ep_len:1098 episode reward: total was -5.350000. running mean: 0.238268\n",
      "ep 3333: ep_len:3897 episode reward: total was -69.990000. running mean: -0.464015\n",
      "ep 3333: ep_len:558 episode reward: total was -10.290000. running mean: -0.562275\n",
      "ep 3333: ep_len:828 episode reward: total was 13.030000. running mean: -0.426352\n",
      "ep 3333: ep_len:696 episode reward: total was 7.040000. running mean: -0.351688\n",
      "ep 3333: ep_len:45 episode reward: total was 21.000000. running mean: -0.138172\n",
      "ep 3333: ep_len:58 episode reward: total was 27.500000. running mean: 0.138210\n",
      "ep 3333: ep_len:71 episode reward: total was 32.500000. running mean: 0.461828\n",
      "ep 3333: ep_len:1087 episode reward: total was 23.190000. running mean: 0.689110\n",
      "ep 3333: ep_len:2874 episode reward: total was 8.220000. running mean: 0.764419\n",
      "epsilon:0.009992 episode_count: 50144. steps_count: 53993676.000000\n",
      "ep 3334: ep_len:623 episode reward: total was -4.040000. running mean: 0.716374\n",
      "ep 3334: ep_len:704 episode reward: total was -392.940000. running mean: -3.220189\n",
      "ep 3334: ep_len:2984 episode reward: total was -114.180000. running mean: -4.329787\n",
      "ep 3334: ep_len:652 episode reward: total was 46.750000. running mean: -3.818990\n",
      "ep 3334: ep_len:63 episode reward: total was 30.000000. running mean: -3.480800\n",
      "ep 3334: ep_len:2947 episode reward: total was -1193.070000. running mean: -15.376692\n",
      "ep 3334: ep_len:4032 episode reward: total was -137.700000. running mean: -16.599925\n",
      "ep 3334: ep_len:577 episode reward: total was 12.310000. running mean: -16.310825\n",
      "ep 3334: ep_len:7203 episode reward: total was 58.360000. running mean: -15.564117\n",
      "ep 3334: ep_len:1500 episode reward: total was -37.490000. running mean: -15.783376\n",
      "ep 3334: ep_len:64 episode reward: total was 28.510000. running mean: -15.340442\n",
      "ep 3334: ep_len:162 episode reward: total was 79.010000. running mean: -14.396938\n",
      "ep 3334: ep_len:61 episode reward: total was 29.000000. running mean: -13.962968\n",
      "ep 3334: ep_len:661 episode reward: total was 2.790000. running mean: -13.795439\n",
      "ep 3334: ep_len:37 episode reward: total was 17.000000. running mean: -13.487484\n",
      "ep 3334: ep_len:67 episode reward: total was 30.500000. running mean: -13.047610\n",
      "epsilon:0.009992 episode_count: 50160. steps_count: 54016013.000000\n",
      "ep 3335: ep_len:1100 episode reward: total was -25.530000. running mean: -13.172433\n",
      "ep 3335: ep_len:205 episode reward: total was 1.800000. running mean: -13.022709\n",
      "ep 3335: ep_len:83 episode reward: total was 40.000000. running mean: -12.492482\n",
      "ep 3335: ep_len:3023 episode reward: total was -75.490000. running mean: -13.122457\n",
      "ep 3335: ep_len:692 episode reward: total was -4.770000. running mean: -13.038933\n",
      "ep 3335: ep_len:83 episode reward: total was 37.000000. running mean: -12.538543\n",
      "ep 3335: ep_len:2821 episode reward: total was -1285.990000. running mean: -25.273058\n",
      "ep 3335: ep_len:3852 episode reward: total was -60.250000. running mean: -25.622827\n",
      "ep 3335: ep_len:609 episode reward: total was -48.620000. running mean: -25.852799\n",
      "ep 3335: ep_len:628 episode reward: total was -1.510000. running mean: -25.609371\n",
      "ep 3335: ep_len:500 episode reward: total was 8.480000. running mean: -25.268477\n",
      "ep 3335: ep_len:205 episode reward: total was 99.500000. running mean: -24.020793\n",
      "ep 3335: ep_len:40 episode reward: total was 18.500000. running mean: -23.595585\n",
      "ep 3335: ep_len:108 episode reward: total was 51.000000. running mean: -22.849629\n",
      "ep 3335: ep_len:651 episode reward: total was 1.880000. running mean: -22.602332\n",
      "ep 3335: ep_len:2932 episode reward: total was -242.450000. running mean: -24.800809\n",
      "ep 3335: ep_len:37 episode reward: total was 17.000000. running mean: -24.382801\n",
      "epsilon:0.009992 episode_count: 50177. steps_count: 54033582.000000\n",
      "ep 3336: ep_len:1115 episode reward: total was -26.670000. running mean: -24.405673\n",
      "ep 3336: ep_len:711 episode reward: total was -4.890000. running mean: -24.210516\n",
      "ep 3336: ep_len:3058 episode reward: total was -35.710000. running mean: -24.325511\n",
      "ep 3336: ep_len:915 episode reward: total was 75.580000. running mean: -23.326456\n",
      "ep 3336: ep_len:500 episode reward: total was 47.340000. running mean: -22.619791\n",
      "ep 3336: ep_len:3918 episode reward: total was -37.370000. running mean: -22.767294\n",
      "ep 3336: ep_len:1070 episode reward: total was -32.360000. running mean: -22.863221\n",
      "ep 3336: ep_len:829 episode reward: total was 60.220000. running mean: -22.032388\n",
      "ep 3336: ep_len:610 episode reward: total was -14.270000. running mean: -21.954765\n",
      "ep 3336: ep_len:99 episode reward: total was 48.000000. running mean: -21.255217\n",
      "ep 3336: ep_len:752 episode reward: total was -73.190000. running mean: -21.774565\n",
      "ep 3336: ep_len:2912 episode reward: total was 8.300000. running mean: -21.473819\n",
      "epsilon:0.009992 episode_count: 50189. steps_count: 54050071.000000\n",
      "ep 3337: ep_len:1405 episode reward: total was 5.340000. running mean: -21.205681\n",
      "ep 3337: ep_len:500 episode reward: total was -8.510000. running mean: -21.078724\n",
      "ep 3337: ep_len:76 episode reward: total was 35.000000. running mean: -20.517937\n",
      "ep 3337: ep_len:2998 episode reward: total was -33.130000. running mean: -20.644057\n",
      "ep 3337: ep_len:608 episode reward: total was -3.620000. running mean: -20.473817\n",
      "ep 3337: ep_len:49 episode reward: total was 23.000000. running mean: -20.039079\n",
      "ep 3337: ep_len:1166 episode reward: total was 5.320000. running mean: -19.785488\n",
      "ep 3337: ep_len:3726 episode reward: total was -15.050000. running mean: -19.738133\n",
      "ep 3337: ep_len:927 episode reward: total was -38.370000. running mean: -19.924452\n",
      "ep 3337: ep_len:780 episode reward: total was 17.250000. running mean: -19.552707\n",
      "ep 3337: ep_len:610 episode reward: total was -6.190000. running mean: -19.419080\n",
      "ep 3337: ep_len:105 episode reward: total was 45.000000. running mean: -18.774889\n",
      "ep 3337: ep_len:1506 episode reward: total was -14.280000. running mean: -18.729940\n",
      "ep 3337: ep_len:2809 episode reward: total was -1.980000. running mean: -18.562441\n",
      "ep 3337: ep_len:34 episode reward: total was 15.500000. running mean: -18.221817\n",
      "epsilon:0.009992 episode_count: 50204. steps_count: 54067370.000000\n",
      "ep 3338: ep_len:1001 episode reward: total was -40.300000. running mean: -18.442598\n",
      "ep 3338: ep_len:500 episode reward: total was 28.020000. running mean: -17.977972\n",
      "ep 3338: ep_len:2975 episode reward: total was -11.810000. running mean: -17.916293\n",
      "ep 3338: ep_len:516 episode reward: total was -23.230000. running mean: -17.969430\n",
      "ep 3338: ep_len:80 episode reward: total was 37.000000. running mean: -17.419736\n",
      "ep 3338: ep_len:694 episode reward: total was 23.740000. running mean: -17.008138\n",
      "ep 3338: ep_len:3688 episode reward: total was -0.280000. running mean: -16.840857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3338: ep_len:538 episode reward: total was -34.670000. running mean: -17.019148\n",
      "ep 3338: ep_len:797 episode reward: total was 22.260000. running mean: -16.626357\n",
      "ep 3338: ep_len:479 episode reward: total was 27.750000. running mean: -16.182593\n",
      "ep 3338: ep_len:120 episode reward: total was 57.000000. running mean: -15.450767\n",
      "ep 3338: ep_len:100 episode reward: total was 47.000000. running mean: -14.826260\n",
      "ep 3338: ep_len:500 episode reward: total was 40.540000. running mean: -14.272597\n",
      "ep 3338: ep_len:2845 episode reward: total was -5.970000. running mean: -14.189571\n",
      "epsilon:0.009992 episode_count: 50218. steps_count: 54082203.000000\n",
      "ep 3339: ep_len:681 episode reward: total was -35.780000. running mean: -14.405475\n",
      "ep 3339: ep_len:975 episode reward: total was 22.690000. running mean: -14.034521\n",
      "ep 3339: ep_len:53 episode reward: total was 23.500000. running mean: -13.659175\n",
      "ep 3339: ep_len:2982 episode reward: total was -11.680000. running mean: -13.639384\n",
      "ep 3339: ep_len:779 episode reward: total was 26.990000. running mean: -13.233090\n",
      "ep 3339: ep_len:44 episode reward: total was 20.500000. running mean: -12.895759\n",
      "ep 3339: ep_len:107 episode reward: total was 52.000000. running mean: -12.246801\n",
      "ep 3339: ep_len:91 episode reward: total was 44.000000. running mean: -11.684333\n",
      "ep 3339: ep_len:500 episode reward: total was 32.980000. running mean: -11.237690\n",
      "ep 3339: ep_len:3720 episode reward: total was -8.820000. running mean: -11.213513\n",
      "ep 3339: ep_len:792 episode reward: total was -17.760000. running mean: -11.278978\n",
      "ep 3339: ep_len:817 episode reward: total was 27.730000. running mean: -10.888888\n",
      "ep 3339: ep_len:900 episode reward: total was 37.130000. running mean: -10.408699\n",
      "ep 3339: ep_len:771 episode reward: total was -22.410000. running mean: -10.528712\n",
      "ep 3339: ep_len:2796 episode reward: total was 5.510000. running mean: -10.368325\n",
      "ep 3339: ep_len:73 episode reward: total was 35.000000. running mean: -9.914642\n",
      "epsilon:0.009992 episode_count: 50234. steps_count: 54098284.000000\n",
      "ep 3340: ep_len:1066 episode reward: total was -9.310000. running mean: -9.908595\n",
      "ep 3340: ep_len:1265 episode reward: total was -67.220000. running mean: -10.481709\n",
      "ep 3340: ep_len:2995 episode reward: total was -22.080000. running mean: -10.597692\n",
      "ep 3340: ep_len:501 episode reward: total was -17.380000. running mean: -10.665515\n",
      "ep 3340: ep_len:29 episode reward: total was 13.000000. running mean: -10.428860\n",
      "ep 3340: ep_len:122 episode reward: total was 58.000000. running mean: -9.744572\n",
      "ep 3340: ep_len:50 episode reward: total was 23.500000. running mean: -9.412126\n",
      "ep 3340: ep_len:661 episode reward: total was 14.220000. running mean: -9.175805\n",
      "ep 3340: ep_len:500 episode reward: total was 13.520000. running mean: -8.948847\n",
      "ep 3340: ep_len:530 episode reward: total was -13.050000. running mean: -8.989858\n",
      "ep 3340: ep_len:7247 episode reward: total was 54.700000. running mean: -8.352960\n",
      "ep 3340: ep_len:500 episode reward: total was -5.850000. running mean: -8.327930\n",
      "ep 3340: ep_len:164 episode reward: total was 76.000000. running mean: -7.484651\n",
      "ep 3340: ep_len:85 episode reward: total was 39.500000. running mean: -7.014804\n",
      "ep 3340: ep_len:669 episode reward: total was -20.130000. running mean: -7.145956\n",
      "ep 3340: ep_len:2794 episode reward: total was -6.690000. running mean: -7.141397\n",
      "epsilon:0.009992 episode_count: 50250. steps_count: 54117462.000000\n",
      "ep 3341: ep_len:1138 episode reward: total was 3.230000. running mean: -7.037683\n",
      "ep 3341: ep_len:809 episode reward: total was 0.500000. running mean: -6.962306\n",
      "ep 3341: ep_len:3001 episode reward: total was -20.180000. running mean: -7.094483\n",
      "ep 3341: ep_len:756 episode reward: total was 11.030000. running mean: -6.913238\n",
      "ep 3341: ep_len:709 episode reward: total was 22.410000. running mean: -6.620006\n",
      "ep 3341: ep_len:3737 episode reward: total was -71.420000. running mean: -7.268006\n",
      "ep 3341: ep_len:584 episode reward: total was -25.060000. running mean: -7.445925\n",
      "ep 3341: ep_len:695 episode reward: total was 34.280000. running mean: -7.028666\n",
      "ep 3341: ep_len:540 episode reward: total was 11.580000. running mean: -6.842580\n",
      "ep 3341: ep_len:149 episode reward: total was 73.000000. running mean: -6.044154\n",
      "ep 3341: ep_len:60 episode reward: total was 25.500000. running mean: -5.728712\n",
      "ep 3341: ep_len:500 episode reward: total was 15.250000. running mean: -5.518925\n",
      "ep 3341: ep_len:2805 episode reward: total was 0.060000. running mean: -5.463136\n",
      "epsilon:0.009992 episode_count: 50263. steps_count: 54132945.000000\n",
      "ep 3342: ep_len:1555 episode reward: total was 13.580000. running mean: -5.272704\n",
      "ep 3342: ep_len:898 episode reward: total was 2.410000. running mean: -5.195877\n",
      "ep 3342: ep_len:92 episode reward: total was 43.000000. running mean: -4.713919\n",
      "ep 3342: ep_len:618 episode reward: total was -5.100000. running mean: -4.717779\n",
      "ep 3342: ep_len:48 episode reward: total was 21.000000. running mean: -4.460602\n",
      "ep 3342: ep_len:1026 episode reward: total was -65.660000. running mean: -5.072596\n",
      "ep 3342: ep_len:4043 episode reward: total was -71.470000. running mean: -5.736570\n",
      "ep 3342: ep_len:559 episode reward: total was 3.650000. running mean: -5.642704\n",
      "ep 3342: ep_len:867 episode reward: total was 34.240000. running mean: -5.243877\n",
      "ep 3342: ep_len:500 episode reward: total was -9.090000. running mean: -5.282338\n",
      "ep 3342: ep_len:86 episode reward: total was 41.500000. running mean: -4.814515\n",
      "ep 3342: ep_len:81 episode reward: total was 37.500000. running mean: -4.391370\n",
      "ep 3342: ep_len:1464 episode reward: total was -7.750000. running mean: -4.424956\n",
      "ep 3342: ep_len:2815 episode reward: total was -45.290000. running mean: -4.833606\n",
      "ep 3342: ep_len:65 episode reward: total was 29.500000. running mean: -4.490270\n",
      "epsilon:0.009992 episode_count: 50278. steps_count: 54147662.000000\n",
      "ep 3343: ep_len:1459 episode reward: total was 11.880000. running mean: -4.326568\n",
      "ep 3343: ep_len:500 episode reward: total was 16.040000. running mean: -4.122902\n",
      "ep 3343: ep_len:3103 episode reward: total was -93.170000. running mean: -5.013373\n",
      "ep 3343: ep_len:791 episode reward: total was 22.950000. running mean: -4.733739\n",
      "ep 3343: ep_len:109 episode reward: total was 51.500000. running mean: -4.171402\n",
      "ep 3343: ep_len:890 episode reward: total was 18.530000. running mean: -3.944388\n",
      "ep 3343: ep_len:3903 episode reward: total was -677.600000. running mean: -10.680944\n",
      "ep 3343: ep_len:800 episode reward: total was -13.910000. running mean: -10.713234\n",
      "ep 3343: ep_len:614 episode reward: total was 7.320000. running mean: -10.532902\n",
      "ep 3343: ep_len:1500 episode reward: total was 0.100000. running mean: -10.426573\n",
      "ep 3343: ep_len:62 episode reward: total was 28.000000. running mean: -10.042307\n",
      "ep 3343: ep_len:108 episode reward: total was 52.500000. running mean: -9.416884\n",
      "ep 3343: ep_len:500 episode reward: total was 1.040000. running mean: -9.312315\n",
      "ep 3343: ep_len:2797 episode reward: total was -18.080000. running mean: -9.399992\n",
      "epsilon:0.009992 episode_count: 50292. steps_count: 54164798.000000\n",
      "ep 3344: ep_len:656 episode reward: total was -11.690000. running mean: -9.422892\n",
      "ep 3344: ep_len:500 episode reward: total was 8.420000. running mean: -9.244463\n",
      "ep 3344: ep_len:94 episode reward: total was 42.500000. running mean: -8.727019\n",
      "ep 3344: ep_len:674 episode reward: total was 4.060000. running mean: -8.599149\n",
      "ep 3344: ep_len:142 episode reward: total was 69.500000. running mean: -7.818157\n",
      "ep 3344: ep_len:64 episode reward: total was 30.500000. running mean: -7.434976\n",
      "ep 3344: ep_len:1006 episode reward: total was -19.370000. running mean: -7.554326\n",
      "ep 3344: ep_len:3431 episode reward: total was -31.970000. running mean: -7.798483\n",
      "ep 3344: ep_len:942 episode reward: total was -50.240000. running mean: -8.222898\n",
      "ep 3344: ep_len:7431 episode reward: total was -138.460000. running mean: -9.525269\n",
      "ep 3344: ep_len:732 episode reward: total was 22.100000. running mean: -9.209016\n",
      "ep 3344: ep_len:70 episode reward: total was 32.000000. running mean: -8.796926\n",
      "ep 3344: ep_len:762 episode reward: total was -57.190000. running mean: -9.280857\n",
      "ep 3344: ep_len:2850 episode reward: total was 1.680000. running mean: -9.171248\n",
      "epsilon:0.009992 episode_count: 50306. steps_count: 54184152.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3345: ep_len:3395 episode reward: total was -577.270000. running mean: -14.852236\n",
      "ep 3345: ep_len:727 episode reward: total was 3.190000. running mean: -14.671813\n",
      "ep 3345: ep_len:2948 episode reward: total was -72.840000. running mean: -15.253495\n",
      "ep 3345: ep_len:632 episode reward: total was -10.400000. running mean: -15.204960\n",
      "ep 3345: ep_len:81 episode reward: total was -52.990000. running mean: -15.582811\n",
      "ep 3345: ep_len:65 episode reward: total was 29.500000. running mean: -15.131982\n",
      "ep 3345: ep_len:897 episode reward: total was 57.080000. running mean: -14.409863\n",
      "ep 3345: ep_len:308 episode reward: total was 18.470000. running mean: -14.081064\n",
      "ep 3345: ep_len:632 episode reward: total was -62.680000. running mean: -14.567053\n",
      "ep 3345: ep_len:636 episode reward: total was 3.680000. running mean: -14.384583\n",
      "ep 3345: ep_len:1180 episode reward: total was -23.720000. running mean: -14.477937\n",
      "ep 3345: ep_len:111 episode reward: total was 52.500000. running mean: -13.808158\n",
      "ep 3345: ep_len:639 episode reward: total was 0.160000. running mean: -13.668476\n",
      "ep 3345: ep_len:2829 episode reward: total was -47.500000. running mean: -14.006791\n",
      "epsilon:0.009992 episode_count: 50320. steps_count: 54199232.000000\n",
      "ep 3346: ep_len:702 episode reward: total was -6.280000. running mean: -13.929523\n",
      "ep 3346: ep_len:209 episode reward: total was -8.750000. running mean: -13.877728\n",
      "ep 3346: ep_len:2950 episode reward: total was -31.440000. running mean: -14.053351\n",
      "ep 3346: ep_len:803 episode reward: total was 6.650000. running mean: -13.846317\n",
      "ep 3346: ep_len:40 episode reward: total was 17.000000. running mean: -13.537854\n",
      "ep 3346: ep_len:922 episode reward: total was -2.050000. running mean: -13.422976\n",
      "ep 3346: ep_len:3934 episode reward: total was -24.080000. running mean: -13.529546\n",
      "ep 3346: ep_len:549 episode reward: total was 1.280000. running mean: -13.381450\n",
      "ep 3346: ep_len:7389 episode reward: total was 52.570000. running mean: -12.721936\n",
      "ep 3346: ep_len:659 episode reward: total was 10.550000. running mean: -12.489217\n",
      "ep 3346: ep_len:61 episode reward: total was 26.000000. running mean: -12.104324\n",
      "ep 3346: ep_len:102 episode reward: total was 49.500000. running mean: -11.488281\n",
      "ep 3346: ep_len:777 episode reward: total was -6.090000. running mean: -11.434298\n",
      "ep 3346: ep_len:2973 episode reward: total was -41.200000. running mean: -11.731955\n",
      "epsilon:0.009992 episode_count: 50334. steps_count: 54221302.000000\n",
      "ep 3347: ep_len:1085 episode reward: total was 8.790000. running mean: -11.526736\n",
      "ep 3347: ep_len:500 episode reward: total was 23.270000. running mean: -11.178768\n",
      "ep 3347: ep_len:56 episode reward: total was 26.500000. running mean: -10.801981\n",
      "ep 3347: ep_len:2917 episode reward: total was -166.850000. running mean: -12.362461\n",
      "ep 3347: ep_len:614 episode reward: total was 28.700000. running mean: -11.951836\n",
      "ep 3347: ep_len:44 episode reward: total was 19.000000. running mean: -11.642318\n",
      "ep 3347: ep_len:2860 episode reward: total was -295.230000. running mean: -14.478195\n",
      "ep 3347: ep_len:4149 episode reward: total was -30.850000. running mean: -14.641913\n",
      "ep 3347: ep_len:4303 episode reward: total was -444.050000. running mean: -18.935994\n",
      "ep 3347: ep_len:728 episode reward: total was -12.020000. running mean: -18.866834\n",
      "ep 3347: ep_len:1115 episode reward: total was -19.320000. running mean: -18.871365\n",
      "ep 3347: ep_len:137 episode reward: total was 65.010000. running mean: -18.032552\n",
      "ep 3347: ep_len:1416 episode reward: total was 12.430000. running mean: -17.727926\n",
      "ep 3347: ep_len:2841 episode reward: total was 15.940000. running mean: -17.391247\n",
      "ep 3347: ep_len:65 episode reward: total was 29.500000. running mean: -16.922335\n",
      "epsilon:0.009992 episode_count: 50349. steps_count: 54244132.000000\n",
      "ep 3348: ep_len:1039 episode reward: total was 6.270000. running mean: -16.690411\n",
      "ep 3348: ep_len:1271 episode reward: total was -43.010000. running mean: -16.953607\n",
      "ep 3348: ep_len:3132 episode reward: total was -114.800000. running mean: -17.932071\n",
      "ep 3348: ep_len:1486 episode reward: total was -11.880000. running mean: -17.871550\n",
      "ep 3348: ep_len:37 episode reward: total was 15.500000. running mean: -17.537835\n",
      "ep 3348: ep_len:150 episode reward: total was 73.500000. running mean: -16.627456\n",
      "ep 3348: ep_len:78 episode reward: total was 36.000000. running mean: -16.101182\n",
      "ep 3348: ep_len:1130 episode reward: total was -11.090000. running mean: -16.051070\n",
      "ep 3348: ep_len:3872 episode reward: total was -0.140000. running mean: -15.891959\n",
      "ep 3348: ep_len:3162 episode reward: total was -204.920000. running mean: -17.782240\n",
      "ep 3348: ep_len:733 episode reward: total was -0.980000. running mean: -17.614217\n",
      "ep 3348: ep_len:652 episode reward: total was 28.280000. running mean: -17.155275\n",
      "ep 3348: ep_len:94 episode reward: total was 44.000000. running mean: -16.543722\n",
      "ep 3348: ep_len:100 episode reward: total was 48.500000. running mean: -15.893285\n",
      "ep 3348: ep_len:687 episode reward: total was 17.430000. running mean: -15.560052\n",
      "ep 3348: ep_len:2870 episode reward: total was -59.250000. running mean: -15.996952\n",
      "epsilon:0.009992 episode_count: 50365. steps_count: 54264625.000000\n",
      "ep 3349: ep_len:721 episode reward: total was -6.480000. running mean: -15.901782\n",
      "ep 3349: ep_len:1643 episode reward: total was -41.100000. running mean: -16.153765\n",
      "ep 3349: ep_len:3005 episode reward: total was 0.320000. running mean: -15.989027\n",
      "ep 3349: ep_len:678 episode reward: total was 8.350000. running mean: -15.745637\n",
      "ep 3349: ep_len:92 episode reward: total was 44.500000. running mean: -15.143180\n",
      "ep 3349: ep_len:58 episode reward: total was 26.000000. running mean: -14.731748\n",
      "ep 3349: ep_len:996 episode reward: total was 12.430000. running mean: -14.460131\n",
      "ep 3349: ep_len:3941 episode reward: total was -134.210000. running mean: -15.657630\n",
      "ep 3349: ep_len:567 episode reward: total was -11.670000. running mean: -15.617753\n",
      "ep 3349: ep_len:652 episode reward: total was -2.580000. running mean: -15.487376\n",
      "ep 3349: ep_len:743 episode reward: total was -20.010000. running mean: -15.532602\n",
      "ep 3349: ep_len:130 episode reward: total was 62.000000. running mean: -14.757276\n",
      "ep 3349: ep_len:65 episode reward: total was 29.500000. running mean: -14.314703\n",
      "ep 3349: ep_len:500 episode reward: total was 40.790000. running mean: -13.763656\n",
      "ep 3349: ep_len:2823 episode reward: total was 6.460000. running mean: -13.561420\n",
      "epsilon:0.009992 episode_count: 50380. steps_count: 54281239.000000\n",
      "ep 3350: ep_len:972 episode reward: total was -52.220000. running mean: -13.948005\n",
      "ep 3350: ep_len:734 episode reward: total was -12.050000. running mean: -13.929025\n",
      "ep 3350: ep_len:2929 episode reward: total was -13.880000. running mean: -13.928535\n",
      "ep 3350: ep_len:821 episode reward: total was 9.200000. running mean: -13.697250\n",
      "ep 3350: ep_len:53 episode reward: total was 22.000000. running mean: -13.340277\n",
      "ep 3350: ep_len:667 episode reward: total was 31.890000. running mean: -12.887975\n",
      "ep 3350: ep_len:500 episode reward: total was -7.420000. running mean: -12.833295\n",
      "ep 3350: ep_len:789 episode reward: total was -1.870000. running mean: -12.723662\n",
      "ep 3350: ep_len:683 episode reward: total was 29.820000. running mean: -12.298225\n",
      "ep 3350: ep_len:1109 episode reward: total was 6.610000. running mean: -12.109143\n",
      "ep 3350: ep_len:64 episode reward: total was 29.000000. running mean: -11.698052\n",
      "ep 3350: ep_len:732 episode reward: total was -17.380000. running mean: -11.754871\n",
      "ep 3350: ep_len:2824 episode reward: total was 3.040000. running mean: -11.606922\n",
      "epsilon:0.009992 episode_count: 50393. steps_count: 54294116.000000\n",
      "ep 3351: ep_len:1092 episode reward: total was 1.300000. running mean: -11.477853\n",
      "ep 3351: ep_len:1244 episode reward: total was -48.330000. running mean: -11.846375\n",
      "ep 3351: ep_len:59 episode reward: total was 28.000000. running mean: -11.447911\n",
      "ep 3351: ep_len:3008 episode reward: total was -5.820000. running mean: -11.391632\n",
      "ep 3351: ep_len:1667 episode reward: total was -37.430000. running mean: -11.652015\n",
      "ep 3351: ep_len:41 episode reward: total was 19.000000. running mean: -11.345495\n",
      "ep 3351: ep_len:500 episode reward: total was 12.400000. running mean: -11.108040\n",
      "ep 3351: ep_len:347 episode reward: total was 25.410000. running mean: -10.742860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3351: ep_len:751 episode reward: total was -9.960000. running mean: -10.735031\n",
      "ep 3351: ep_len:696 episode reward: total was 29.000000. running mean: -10.337681\n",
      "ep 3351: ep_len:1464 episode reward: total was -187.310000. running mean: -12.107404\n",
      "ep 3351: ep_len:195 episode reward: total was 91.500000. running mean: -11.071330\n",
      "ep 3351: ep_len:1182 episode reward: total was -15.950000. running mean: -11.120117\n",
      "ep 3351: ep_len:2862 episode reward: total was 0.810000. running mean: -11.000816\n",
      "ep 3351: ep_len:52 episode reward: total was 24.500000. running mean: -10.645808\n",
      "epsilon:0.009992 episode_count: 50408. steps_count: 54309276.000000\n",
      "ep 3352: ep_len:1117 episode reward: total was -12.230000. running mean: -10.661649\n",
      "ep 3352: ep_len:1009 episode reward: total was -32.560000. running mean: -10.880633\n",
      "ep 3352: ep_len:3007 episode reward: total was -66.020000. running mean: -11.432027\n",
      "ep 3352: ep_len:1222 episode reward: total was -17.350000. running mean: -11.491206\n",
      "ep 3352: ep_len:153 episode reward: total was 73.500000. running mean: -10.641294\n",
      "ep 3352: ep_len:76 episode reward: total was 35.000000. running mean: -10.184881\n",
      "ep 3352: ep_len:72 episode reward: total was 30.000000. running mean: -9.783033\n",
      "ep 3352: ep_len:591 episode reward: total was 8.790000. running mean: -9.597302\n",
      "ep 3352: ep_len:3509 episode reward: total was -409.470000. running mean: -13.596029\n",
      "ep 3352: ep_len:703 episode reward: total was -45.100000. running mean: -13.911069\n",
      "ep 3352: ep_len:788 episode reward: total was 32.920000. running mean: -13.442758\n",
      "ep 3352: ep_len:500 episode reward: total was 12.130000. running mean: -13.187031\n",
      "ep 3352: ep_len:63 episode reward: total was 30.000000. running mean: -12.755160\n",
      "ep 3352: ep_len:36 episode reward: total was 16.500000. running mean: -12.462609\n",
      "ep 3352: ep_len:617 episode reward: total was -32.070000. running mean: -12.658683\n",
      "ep 3352: ep_len:2870 episode reward: total was -7.860000. running mean: -12.610696\n",
      "ep 3352: ep_len:57 episode reward: total was 24.000000. running mean: -12.244589\n",
      "epsilon:0.009992 episode_count: 50425. steps_count: 54325666.000000\n",
      "ep 3353: ep_len:730 episode reward: total was -50.830000. running mean: -12.630443\n",
      "ep 3353: ep_len:650 episode reward: total was -42.550000. running mean: -12.929639\n",
      "ep 3353: ep_len:46 episode reward: total was 21.500000. running mean: -12.585342\n",
      "ep 3353: ep_len:2989 episode reward: total was -55.700000. running mean: -13.016489\n",
      "ep 3353: ep_len:1687 episode reward: total was -24.620000. running mean: -13.132524\n",
      "ep 3353: ep_len:121 episode reward: total was 56.000000. running mean: -12.441199\n",
      "ep 3353: ep_len:102 episode reward: total was 49.500000. running mean: -11.821787\n",
      "ep 3353: ep_len:63 episode reward: total was 30.000000. running mean: -11.403569\n",
      "ep 3353: ep_len:500 episode reward: total was 8.330000. running mean: -11.206233\n",
      "ep 3353: ep_len:3815 episode reward: total was -86.160000. running mean: -11.955771\n",
      "ep 3353: ep_len:1194 episode reward: total was -68.360000. running mean: -12.519813\n",
      "ep 3353: ep_len:748 episode reward: total was -5.240000. running mean: -12.447015\n",
      "ep 3353: ep_len:1501 episode reward: total was -2.270000. running mean: -12.345245\n",
      "ep 3353: ep_len:110 episode reward: total was 52.000000. running mean: -11.701792\n",
      "ep 3353: ep_len:50 episode reward: total was 23.500000. running mean: -11.349774\n",
      "ep 3353: ep_len:98 episode reward: total was 47.500000. running mean: -10.761277\n",
      "ep 3353: ep_len:620 episode reward: total was -28.340000. running mean: -10.937064\n",
      "ep 3353: ep_len:2852 episode reward: total was 6.720000. running mean: -10.760493\n",
      "epsilon:0.009992 episode_count: 50443. steps_count: 54343542.000000\n",
      "ep 3354: ep_len:691 episode reward: total was -0.100000. running mean: -10.653888\n",
      "ep 3354: ep_len:775 episode reward: total was 0.150000. running mean: -10.545849\n",
      "ep 3354: ep_len:83 episode reward: total was 40.000000. running mean: -10.040391\n",
      "ep 3354: ep_len:99 episode reward: total was 48.000000. running mean: -9.459987\n",
      "ep 3354: ep_len:787 episode reward: total was -5.430000. running mean: -9.419687\n",
      "ep 3354: ep_len:157 episode reward: total was 75.500000. running mean: -8.570490\n",
      "ep 3354: ep_len:101 episode reward: total was 49.000000. running mean: -7.994785\n",
      "ep 3354: ep_len:1124 episode reward: total was 12.180000. running mean: -7.793037\n",
      "ep 3354: ep_len:3572 episode reward: total was -43.860000. running mean: -8.153707\n",
      "ep 3354: ep_len:538 episode reward: total was -6.910000. running mean: -8.141270\n",
      "ep 3354: ep_len:726 episode reward: total was 39.280000. running mean: -7.667057\n",
      "ep 3354: ep_len:981 episode reward: total was 52.940000. running mean: -7.060987\n",
      "ep 3354: ep_len:59 episode reward: total was 29.010000. running mean: -6.700277\n",
      "ep 3354: ep_len:696 episode reward: total was -2.300000. running mean: -6.656274\n",
      "ep 3354: ep_len:41 episode reward: total was 17.500000. running mean: -6.414711\n",
      "epsilon:0.009992 episode_count: 50458. steps_count: 54353972.000000\n",
      "ep 3355: ep_len:1002 episode reward: total was -19.050000. running mean: -6.541064\n",
      "ep 3355: ep_len:1607 episode reward: total was -23.490000. running mean: -6.710554\n",
      "ep 3355: ep_len:2956 episode reward: total was -1.640000. running mean: -6.659848\n",
      "ep 3355: ep_len:643 episode reward: total was -7.950000. running mean: -6.672750\n",
      "ep 3355: ep_len:62 episode reward: total was 28.000000. running mean: -6.326022\n",
      "ep 3355: ep_len:67 episode reward: total was 30.500000. running mean: -5.957762\n",
      "ep 3355: ep_len:500 episode reward: total was 18.520000. running mean: -5.712984\n",
      "ep 3355: ep_len:3678 episode reward: total was -7.940000. running mean: -5.735254\n",
      "ep 3355: ep_len:657 episode reward: total was -0.670000. running mean: -5.684602\n",
      "ep 3355: ep_len:7292 episode reward: total was -16.860000. running mean: -5.796356\n",
      "ep 3355: ep_len:1080 episode reward: total was 5.580000. running mean: -5.682592\n",
      "ep 3355: ep_len:672 episode reward: total was 8.220000. running mean: -5.543566\n",
      "ep 3355: ep_len:2866 episode reward: total was -24.790000. running mean: -5.736031\n",
      "epsilon:0.009992 episode_count: 50471. steps_count: 54377054.000000\n",
      "ep 3356: ep_len:3754 episode reward: total was -554.640000. running mean: -11.225070\n",
      "ep 3356: ep_len:500 episode reward: total was 5.600000. running mean: -11.056820\n",
      "ep 3356: ep_len:3003 episode reward: total was -14.380000. running mean: -11.090052\n",
      "ep 3356: ep_len:529 episode reward: total was -17.100000. running mean: -11.150151\n",
      "ep 3356: ep_len:116 episode reward: total was 53.500000. running mean: -10.503649\n",
      "ep 3356: ep_len:92 episode reward: total was 44.500000. running mean: -9.953613\n",
      "ep 3356: ep_len:993 episode reward: total was -1.350000. running mean: -9.867577\n",
      "ep 3356: ep_len:4132 episode reward: total was -60.130000. running mean: -10.370201\n",
      "ep 3356: ep_len:553 episode reward: total was -44.010000. running mean: -10.706599\n",
      "ep 3356: ep_len:692 episode reward: total was 15.670000. running mean: -10.442833\n",
      "ep 3356: ep_len:1110 episode reward: total was 1.840000. running mean: -10.320005\n",
      "ep 3356: ep_len:51 episode reward: total was 24.000000. running mean: -9.976805\n",
      "ep 3356: ep_len:36 episode reward: total was 16.500000. running mean: -9.712037\n",
      "ep 3356: ep_len:957 episode reward: total was -28.220000. running mean: -9.897116\n",
      "ep 3356: ep_len:2852 episode reward: total was -32.060000. running mean: -10.118745\n",
      "epsilon:0.009992 episode_count: 50486. steps_count: 54396424.000000\n",
      "ep 3357: ep_len:1475 episode reward: total was 23.640000. running mean: -9.781158\n",
      "ep 3357: ep_len:958 episode reward: total was 18.050000. running mean: -9.502846\n",
      "ep 3357: ep_len:3046 episode reward: total was -78.420000. running mean: -10.192018\n",
      "ep 3357: ep_len:512 episode reward: total was -22.220000. running mean: -10.312297\n",
      "ep 3357: ep_len:46 episode reward: total was 21.500000. running mean: -9.994175\n",
      "ep 3357: ep_len:95 episode reward: total was 46.000000. running mean: -9.434233\n",
      "ep 3357: ep_len:39 episode reward: total was 16.500000. running mean: -9.174890\n",
      "ep 3357: ep_len:778 episode reward: total was -47.450000. running mean: -9.557642\n",
      "ep 3357: ep_len:651 episode reward: total was 10.480000. running mean: -9.357265\n",
      "ep 3357: ep_len:611 episode reward: total was 12.200000. running mean: -9.141692\n",
      "ep 3357: ep_len:623 episode reward: total was 11.970000. running mean: -8.930576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3357: ep_len:975 episode reward: total was 18.440000. running mean: -8.656870\n",
      "ep 3357: ep_len:132 episode reward: total was 61.500000. running mean: -7.955301\n",
      "ep 3357: ep_len:42 episode reward: total was 19.500000. running mean: -7.680748\n",
      "ep 3357: ep_len:1084 episode reward: total was -12.950000. running mean: -7.733441\n",
      "ep 3357: ep_len:45 episode reward: total was 19.500000. running mean: -7.461106\n",
      "ep 3357: ep_len:55 episode reward: total was 26.000000. running mean: -7.126495\n",
      "epsilon:0.009992 episode_count: 50503. steps_count: 54407591.000000\n",
      "ep 3358: ep_len:1136 episode reward: total was -5.980000. running mean: -7.115030\n",
      "ep 3358: ep_len:817 episode reward: total was 6.120000. running mean: -6.982680\n",
      "ep 3358: ep_len:59 episode reward: total was 25.000000. running mean: -6.662853\n",
      "ep 3358: ep_len:2996 episode reward: total was -47.700000. running mean: -7.073225\n",
      "ep 3358: ep_len:500 episode reward: total was 28.510000. running mean: -6.717392\n",
      "ep 3358: ep_len:63 episode reward: total was 28.500000. running mean: -6.365218\n",
      "ep 3358: ep_len:133 episode reward: total was 62.000000. running mean: -5.681566\n",
      "ep 3358: ep_len:56 episode reward: total was 26.500000. running mean: -5.359751\n",
      "ep 3358: ep_len:607 episode reward: total was 54.700000. running mean: -4.759153\n",
      "ep 3358: ep_len:3612 episode reward: total was -212.130000. running mean: -6.832861\n",
      "ep 3358: ep_len:1179 episode reward: total was -58.320000. running mean: -7.347733\n",
      "ep 3358: ep_len:716 episode reward: total was 29.630000. running mean: -6.977956\n",
      "ep 3358: ep_len:589 episode reward: total was 9.030000. running mean: -6.817876\n",
      "ep 3358: ep_len:66 episode reward: total was 31.500000. running mean: -6.434697\n",
      "ep 3358: ep_len:152 episode reward: total was 72.510000. running mean: -5.645250\n",
      "ep 3358: ep_len:1163 episode reward: total was -0.660000. running mean: -5.595398\n",
      "ep 3358: ep_len:2858 episode reward: total was -1.610000. running mean: -5.555544\n",
      "ep 3358: ep_len:68 episode reward: total was 33.510000. running mean: -5.164888\n",
      "epsilon:0.009992 episode_count: 50521. steps_count: 54424361.000000\n",
      "ep 3359: ep_len:714 episode reward: total was -7.170000. running mean: -5.184939\n",
      "ep 3359: ep_len:797 episode reward: total was 5.550000. running mean: -5.077590\n",
      "ep 3359: ep_len:2919 episode reward: total was -79.020000. running mean: -5.817014\n",
      "ep 3359: ep_len:548 episode reward: total was -24.990000. running mean: -6.008744\n",
      "ep 3359: ep_len:41 episode reward: total was 19.000000. running mean: -5.758657\n",
      "ep 3359: ep_len:172 episode reward: total was 83.000000. running mean: -4.871070\n",
      "ep 3359: ep_len:76 episode reward: total was 35.000000. running mean: -4.472359\n",
      "ep 3359: ep_len:859 episode reward: total was 25.530000. running mean: -4.172336\n",
      "ep 3359: ep_len:3941 episode reward: total was -58.930000. running mean: -4.719912\n",
      "ep 3359: ep_len:816 episode reward: total was -3.770000. running mean: -4.710413\n",
      "ep 3359: ep_len:656 episode reward: total was -8.970000. running mean: -4.753009\n",
      "ep 3359: ep_len:886 episode reward: total was 18.000000. running mean: -4.525479\n",
      "ep 3359: ep_len:137 episode reward: total was 67.000000. running mean: -3.810224\n",
      "ep 3359: ep_len:676 episode reward: total was 24.530000. running mean: -3.526822\n",
      "ep 3359: ep_len:2850 episode reward: total was 20.380000. running mean: -3.287754\n",
      "epsilon:0.009992 episode_count: 50536. steps_count: 54440449.000000\n",
      "ep 3360: ep_len:649 episode reward: total was -39.620000. running mean: -3.651076\n",
      "ep 3360: ep_len:970 episode reward: total was 27.170000. running mean: -3.342865\n",
      "ep 3360: ep_len:3022 episode reward: total was -34.240000. running mean: -3.651837\n",
      "ep 3360: ep_len:634 episode reward: total was 9.040000. running mean: -3.524918\n",
      "ep 3360: ep_len:41 episode reward: total was 17.500000. running mean: -3.314669\n",
      "ep 3360: ep_len:62 episode reward: total was 28.000000. running mean: -3.001523\n",
      "ep 3360: ep_len:712 episode reward: total was 17.000000. running mean: -2.801507\n",
      "ep 3360: ep_len:3689 episode reward: total was 2.610000. running mean: -2.747392\n",
      "ep 3360: ep_len:1293 episode reward: total was -37.000000. running mean: -3.089918\n",
      "ep 3360: ep_len:7378 episode reward: total was 11.300000. running mean: -2.946019\n",
      "ep 3360: ep_len:500 episode reward: total was 3.770000. running mean: -2.878859\n",
      "ep 3360: ep_len:92 episode reward: total was 44.500000. running mean: -2.405070\n",
      "ep 3360: ep_len:111 episode reward: total was 54.000000. running mean: -1.841020\n",
      "ep 3360: ep_len:53 episode reward: total was 25.000000. running mean: -1.572609\n",
      "ep 3360: ep_len:586 episode reward: total was -10.960000. running mean: -1.666483\n",
      "ep 3360: ep_len:2831 episode reward: total was -5.220000. running mean: -1.702019\n",
      "epsilon:0.009992 episode_count: 50552. steps_count: 54463072.000000\n",
      "ep 3361: ep_len:644 episode reward: total was -26.320000. running mean: -1.948198\n",
      "ep 3361: ep_len:725 episode reward: total was -15.500000. running mean: -2.083716\n",
      "ep 3361: ep_len:59 episode reward: total was 28.000000. running mean: -1.782879\n",
      "ep 3361: ep_len:2932 episode reward: total was -50.030000. running mean: -2.265350\n",
      "ep 3361: ep_len:500 episode reward: total was 2.300000. running mean: -2.219697\n",
      "ep 3361: ep_len:49 episode reward: total was 20.000000. running mean: -1.997500\n",
      "ep 3361: ep_len:105 episode reward: total was 51.000000. running mean: -1.467525\n",
      "ep 3361: ep_len:500 episode reward: total was 2.710000. running mean: -1.425750\n",
      "ep 3361: ep_len:3614 episode reward: total was -0.620000. running mean: -1.417692\n",
      "ep 3361: ep_len:901 episode reward: total was 11.290000. running mean: -1.290615\n",
      "ep 3361: ep_len:7242 episode reward: total was 64.080000. running mean: -0.636909\n",
      "ep 3361: ep_len:1075 episode reward: total was -5.580000. running mean: -0.686340\n",
      "ep 3361: ep_len:75 episode reward: total was 36.000000. running mean: -0.319477\n",
      "ep 3361: ep_len:1145 episode reward: total was -13.970000. running mean: -0.455982\n",
      "ep 3361: ep_len:2862 episode reward: total was 9.910000. running mean: -0.352322\n",
      "epsilon:0.009992 episode_count: 50567. steps_count: 54485500.000000\n",
      "ep 3362: ep_len:753 episode reward: total was -21.930000. running mean: -0.568099\n",
      "ep 3362: ep_len:695 episode reward: total was -29.580000. running mean: -0.858218\n",
      "ep 3362: ep_len:2986 episode reward: total was -31.240000. running mean: -1.162036\n",
      "ep 3362: ep_len:659 episode reward: total was -4.690000. running mean: -1.197315\n",
      "ep 3362: ep_len:19 episode reward: total was 8.000000. running mean: -1.105342\n",
      "ep 3362: ep_len:79 episode reward: total was 38.000000. running mean: -0.714289\n",
      "ep 3362: ep_len:667 episode reward: total was 11.480000. running mean: -0.592346\n",
      "ep 3362: ep_len:3788 episode reward: total was 3.860000. running mean: -0.547822\n",
      "ep 3362: ep_len:1568 episode reward: total was 20.290000. running mean: -0.339444\n",
      "ep 3362: ep_len:618 episode reward: total was 2.520000. running mean: -0.310850\n",
      "ep 3362: ep_len:665 episode reward: total was -16.750000. running mean: -0.475241\n",
      "ep 3362: ep_len:74 episode reward: total was 35.500000. running mean: -0.115489\n",
      "ep 3362: ep_len:752 episode reward: total was -24.450000. running mean: -0.358834\n",
      "ep 3362: ep_len:2777 episode reward: total was -31.350000. running mean: -0.668746\n",
      "epsilon:0.009992 episode_count: 50581. steps_count: 54501600.000000\n",
      "ep 3363: ep_len:1094 episode reward: total was -8.180000. running mean: -0.743858\n",
      "ep 3363: ep_len:176 episode reward: total was -12.240000. running mean: -0.858820\n",
      "ep 3363: ep_len:66 episode reward: total was 31.500000. running mean: -0.535231\n",
      "ep 3363: ep_len:3030 episode reward: total was -51.480000. running mean: -1.044679\n",
      "ep 3363: ep_len:695 episode reward: total was 14.340000. running mean: -0.890832\n",
      "ep 3363: ep_len:47 episode reward: total was 22.000000. running mean: -0.661924\n",
      "ep 3363: ep_len:82 episode reward: total was 39.500000. running mean: -0.260305\n",
      "ep 3363: ep_len:63 episode reward: total was 28.500000. running mean: 0.027298\n",
      "ep 3363: ep_len:732 episode reward: total was 21.640000. running mean: 0.243425\n",
      "ep 3363: ep_len:3755 episode reward: total was -8.100000. running mean: 0.159991\n",
      "ep 3363: ep_len:595 episode reward: total was 11.930000. running mean: 0.277691\n",
      "ep 3363: ep_len:773 episode reward: total was 0.160000. running mean: 0.276514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3363: ep_len:1076 episode reward: total was -52.100000. running mean: -0.247251\n",
      "ep 3363: ep_len:793 episode reward: total was -39.710000. running mean: -0.641878\n",
      "ep 3363: ep_len:2829 episode reward: total was -24.920000. running mean: -0.884660\n",
      "epsilon:0.009992 episode_count: 50596. steps_count: 54517406.000000\n",
      "ep 3364: ep_len:2654 episode reward: total was -276.530000. running mean: -3.641113\n",
      "ep 3364: ep_len:728 episode reward: total was -7.030000. running mean: -3.675002\n",
      "ep 3364: ep_len:2984 episode reward: total was -9.030000. running mean: -3.728552\n",
      "ep 3364: ep_len:604 episode reward: total was -11.300000. running mean: -3.804266\n",
      "ep 3364: ep_len:58 episode reward: total was 26.000000. running mean: -3.506224\n",
      "ep 3364: ep_len:1480 episode reward: total was 25.710000. running mean: -3.214061\n",
      "ep 3364: ep_len:3831 episode reward: total was -91.510000. running mean: -4.097021\n",
      "ep 3364: ep_len:773 episode reward: total was -18.340000. running mean: -4.239451\n",
      "ep 3364: ep_len:846 episode reward: total was 39.510000. running mean: -3.801956\n",
      "ep 3364: ep_len:1505 episode reward: total was 14.170000. running mean: -3.622237\n",
      "ep 3364: ep_len:54 episode reward: total was 25.500000. running mean: -3.331014\n",
      "ep 3364: ep_len:125 episode reward: total was 59.500000. running mean: -2.702704\n",
      "ep 3364: ep_len:107 episode reward: total was 52.000000. running mean: -2.155677\n",
      "ep 3364: ep_len:578 episode reward: total was -7.520000. running mean: -2.209320\n",
      "ep 3364: ep_len:2825 episode reward: total was -11.280000. running mean: -2.300027\n",
      "epsilon:0.009992 episode_count: 50611. steps_count: 54536558.000000\n",
      "ep 3365: ep_len:1076 episode reward: total was -3.550000. running mean: -2.312527\n",
      "ep 3365: ep_len:751 episode reward: total was 5.800000. running mean: -2.231401\n",
      "ep 3365: ep_len:62 episode reward: total was 29.500000. running mean: -1.914087\n",
      "ep 3365: ep_len:2985 episode reward: total was -50.560000. running mean: -2.400547\n",
      "ep 3365: ep_len:513 episode reward: total was -0.170000. running mean: -2.378241\n",
      "ep 3365: ep_len:500 episode reward: total was -19.690000. running mean: -2.551359\n",
      "ep 3365: ep_len:3589 episode reward: total was -294.170000. running mean: -5.467545\n",
      "ep 3365: ep_len:1175 episode reward: total was -47.310000. running mean: -5.885970\n",
      "ep 3365: ep_len:675 episode reward: total was 19.260000. running mean: -5.634510\n",
      "ep 3365: ep_len:1508 episode reward: total was 24.120000. running mean: -5.336965\n",
      "ep 3365: ep_len:30 episode reward: total was 13.500000. running mean: -5.148595\n",
      "ep 3365: ep_len:1185 episode reward: total was -5.180000. running mean: -5.148909\n",
      "ep 3365: ep_len:2837 episode reward: total was -46.930000. running mean: -5.566720\n",
      "epsilon:0.009992 episode_count: 50624. steps_count: 54553444.000000\n",
      "ep 3366: ep_len:1423 episode reward: total was 24.250000. running mean: -5.268553\n",
      "ep 3366: ep_len:917 episode reward: total was 19.530000. running mean: -5.020567\n",
      "ep 3366: ep_len:2967 episode reward: total was -21.010000. running mean: -5.180462\n",
      "ep 3366: ep_len:500 episode reward: total was 17.270000. running mean: -4.955957\n",
      "ep 3366: ep_len:69 episode reward: total was 31.500000. running mean: -4.591398\n",
      "ep 3366: ep_len:704 episode reward: total was 34.160000. running mean: -4.203884\n",
      "ep 3366: ep_len:308 episode reward: total was 19.420000. running mean: -3.967645\n",
      "ep 3366: ep_len:4525 episode reward: total was -1191.880000. running mean: -15.846768\n",
      "ep 3366: ep_len:741 episode reward: total was 14.330000. running mean: -15.545001\n",
      "ep 3366: ep_len:841 episode reward: total was 35.820000. running mean: -15.031351\n",
      "ep 3366: ep_len:812 episode reward: total was -7.010000. running mean: -14.951137\n",
      "ep 3366: ep_len:2892 episode reward: total was 11.220000. running mean: -14.689426\n",
      "ep 3366: ep_len:62 episode reward: total was 29.500000. running mean: -14.247531\n",
      "epsilon:0.009992 episode_count: 50637. steps_count: 54570205.000000\n",
      "ep 3367: ep_len:1160 episode reward: total was 11.350000. running mean: -13.991556\n",
      "ep 3367: ep_len:649 episode reward: total was 3.900000. running mean: -13.812641\n",
      "ep 3367: ep_len:2835 episode reward: total was -65.110000. running mean: -14.325614\n",
      "ep 3367: ep_len:526 episode reward: total was -19.150000. running mean: -14.373858\n",
      "ep 3367: ep_len:47 episode reward: total was 22.000000. running mean: -14.010119\n",
      "ep 3367: ep_len:90 episode reward: total was 42.000000. running mean: -13.450018\n",
      "ep 3367: ep_len:1442 episode reward: total was 30.870000. running mean: -13.006818\n",
      "ep 3367: ep_len:333 episode reward: total was 20.010000. running mean: -12.676650\n",
      "ep 3367: ep_len:1549 episode reward: total was 1.550000. running mean: -12.534383\n",
      "ep 3367: ep_len:692 episode reward: total was 14.600000. running mean: -12.263040\n",
      "ep 3367: ep_len:708 episode reward: total was -10.880000. running mean: -12.249209\n",
      "ep 3367: ep_len:54 episode reward: total was 24.000000. running mean: -11.886717\n",
      "ep 3367: ep_len:616 episode reward: total was -11.180000. running mean: -11.879650\n",
      "ep 3367: ep_len:2826 episode reward: total was 2.780000. running mean: -11.733053\n",
      "epsilon:0.009992 episode_count: 50651. steps_count: 54583732.000000\n",
      "ep 3368: ep_len:1107 episode reward: total was 2.820000. running mean: -11.587523\n",
      "ep 3368: ep_len:792 episode reward: total was -2.310000. running mean: -11.494748\n",
      "ep 3368: ep_len:3024 episode reward: total was -73.100000. running mean: -12.110800\n",
      "ep 3368: ep_len:1233 episode reward: total was -4.480000. running mean: -12.034492\n",
      "ep 3368: ep_len:115 episode reward: total was 56.000000. running mean: -11.354147\n",
      "ep 3368: ep_len:73 episode reward: total was 32.000000. running mean: -10.920606\n",
      "ep 3368: ep_len:500 episode reward: total was 54.870000. running mean: -10.262700\n",
      "ep 3368: ep_len:3651 episode reward: total was -164.270000. running mean: -11.802773\n",
      "ep 3368: ep_len:562 episode reward: total was -3.120000. running mean: -11.715945\n",
      "ep 3368: ep_len:652 episode reward: total was 10.100000. running mean: -11.497786\n",
      "ep 3368: ep_len:547 episode reward: total was 35.290000. running mean: -11.029908\n",
      "ep 3368: ep_len:74 episode reward: total was 32.500000. running mean: -10.594609\n",
      "ep 3368: ep_len:714 episode reward: total was 3.940000. running mean: -10.449263\n",
      "ep 3368: ep_len:2868 episode reward: total was -16.690000. running mean: -10.511670\n",
      "ep 3368: ep_len:41 episode reward: total was 19.000000. running mean: -10.216553\n",
      "epsilon:0.009992 episode_count: 50666. steps_count: 54599685.000000\n",
      "ep 3369: ep_len:3629 episode reward: total was -504.230000. running mean: -15.156688\n",
      "ep 3369: ep_len:781 episode reward: total was -21.070000. running mean: -15.215821\n",
      "ep 3369: ep_len:2941 episode reward: total was -33.900000. running mean: -15.402663\n",
      "ep 3369: ep_len:554 episode reward: total was -14.830000. running mean: -15.396936\n",
      "ep 3369: ep_len:50 episode reward: total was 23.500000. running mean: -15.007967\n",
      "ep 3369: ep_len:174 episode reward: total was 84.000000. running mean: -14.017887\n",
      "ep 3369: ep_len:89 episode reward: total was 43.000000. running mean: -13.447708\n",
      "ep 3369: ep_len:500 episode reward: total was 30.930000. running mean: -13.003931\n",
      "ep 3369: ep_len:609 episode reward: total was 21.960000. running mean: -12.654292\n",
      "ep 3369: ep_len:544 episode reward: total was -2.010000. running mean: -12.547849\n",
      "ep 3369: ep_len:776 episode reward: total was 29.000000. running mean: -12.132370\n",
      "ep 3369: ep_len:724 episode reward: total was -32.320000. running mean: -12.334247\n",
      "ep 3369: ep_len:63 episode reward: total was 28.500000. running mean: -11.925904\n",
      "ep 3369: ep_len:17 episode reward: total was 7.000000. running mean: -11.736645\n",
      "ep 3369: ep_len:75 episode reward: total was 34.500000. running mean: -11.274279\n",
      "ep 3369: ep_len:1075 episode reward: total was 25.520000. running mean: -10.906336\n",
      "ep 3369: ep_len:2858 episode reward: total was -6.360000. running mean: -10.860872\n",
      "epsilon:0.009992 episode_count: 50683. steps_count: 54615144.000000\n",
      "ep 3370: ep_len:1130 episode reward: total was 5.070000. running mean: -10.701564\n",
      "ep 3370: ep_len:653 episode reward: total was -38.480000. running mean: -10.979348\n",
      "ep 3370: ep_len:72 episode reward: total was 33.000000. running mean: -10.539555\n",
      "ep 3370: ep_len:3064 episode reward: total was -91.210000. running mean: -11.346259\n",
      "ep 3370: ep_len:895 episode reward: total was 61.810000. running mean: -10.614696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3370: ep_len:150 episode reward: total was 69.000000. running mean: -9.818550\n",
      "ep 3370: ep_len:871 episode reward: total was 34.310000. running mean: -9.377264\n",
      "ep 3370: ep_len:3631 episode reward: total was -164.470000. running mean: -10.928191\n",
      "ep 3370: ep_len:830 episode reward: total was -38.360000. running mean: -11.202509\n",
      "ep 3370: ep_len:678 episode reward: total was 27.130000. running mean: -10.819184\n",
      "ep 3370: ep_len:500 episode reward: total was 40.450000. running mean: -10.306493\n",
      "ep 3370: ep_len:69 episode reward: total was 33.000000. running mean: -9.873428\n",
      "ep 3370: ep_len:789 episode reward: total was -4.400000. running mean: -9.818693\n",
      "ep 3370: ep_len:2877 episode reward: total was -42.710000. running mean: -10.147606\n",
      "epsilon:0.009992 episode_count: 50697. steps_count: 54631353.000000\n",
      "ep 3371: ep_len:990 episode reward: total was -58.380000. running mean: -10.629930\n",
      "ep 3371: ep_len:500 episode reward: total was 5.330000. running mean: -10.470331\n",
      "ep 3371: ep_len:82 episode reward: total was 39.500000. running mean: -9.970628\n",
      "ep 3371: ep_len:678 episode reward: total was 3.690000. running mean: -9.834021\n",
      "ep 3371: ep_len:108 episode reward: total was 52.500000. running mean: -9.210681\n",
      "ep 3371: ep_len:66 episode reward: total was 28.500000. running mean: -8.833574\n",
      "ep 3371: ep_len:871 episode reward: total was 31.650000. running mean: -8.428739\n",
      "ep 3371: ep_len:335 episode reward: total was 26.150000. running mean: -8.082951\n",
      "ep 3371: ep_len:633 episode reward: total was -38.830000. running mean: -8.390422\n",
      "ep 3371: ep_len:636 episode reward: total was -0.910000. running mean: -8.315618\n",
      "ep 3371: ep_len:1448 episode reward: total was 16.630000. running mean: -8.066161\n",
      "ep 3371: ep_len:60 episode reward: total was 28.500000. running mean: -7.700500\n",
      "ep 3371: ep_len:120 episode reward: total was 58.500000. running mean: -7.038495\n",
      "ep 3371: ep_len:58 episode reward: total was 27.500000. running mean: -6.693110\n",
      "ep 3371: ep_len:1083 episode reward: total was -0.080000. running mean: -6.626979\n",
      "ep 3371: ep_len:2858 episode reward: total was -20.280000. running mean: -6.763509\n",
      "epsilon:0.009992 episode_count: 50713. steps_count: 54641879.000000\n",
      "ep 3372: ep_len:1102 episode reward: total was -0.260000. running mean: -6.698474\n",
      "ep 3372: ep_len:208 episode reward: total was 2.960000. running mean: -6.601889\n",
      "ep 3372: ep_len:64 episode reward: total was 30.500000. running mean: -6.230870\n",
      "ep 3372: ep_len:3060 episode reward: total was -71.020000. running mean: -6.878762\n",
      "ep 3372: ep_len:1430 episode reward: total was 22.760000. running mean: -6.582374\n",
      "ep 3372: ep_len:112 episode reward: total was 53.000000. running mean: -5.986550\n",
      "ep 3372: ep_len:105 episode reward: total was 49.500000. running mean: -5.431685\n",
      "ep 3372: ep_len:1643 episode reward: total was -303.910000. running mean: -8.416468\n",
      "ep 3372: ep_len:3621 episode reward: total was -49.860000. running mean: -8.830903\n",
      "ep 3372: ep_len:695 episode reward: total was -36.770000. running mean: -9.110294\n",
      "ep 3372: ep_len:724 episode reward: total was 39.660000. running mean: -8.622591\n",
      "ep 3372: ep_len:513 episode reward: total was 5.890000. running mean: -8.477465\n",
      "ep 3372: ep_len:42 episode reward: total was 18.000000. running mean: -8.212691\n",
      "ep 3372: ep_len:59 episode reward: total was 28.000000. running mean: -7.850564\n",
      "ep 3372: ep_len:779 episode reward: total was -56.010000. running mean: -8.332158\n",
      "ep 3372: ep_len:2801 episode reward: total was -14.760000. running mean: -8.396436\n",
      "ep 3372: ep_len:32 episode reward: total was 13.000000. running mean: -8.182472\n",
      "epsilon:0.009992 episode_count: 50730. steps_count: 54658869.000000\n",
      "ep 3373: ep_len:719 episode reward: total was -73.710000. running mean: -8.837747\n",
      "ep 3373: ep_len:662 episode reward: total was -67.280000. running mean: -9.422170\n",
      "ep 3373: ep_len:83 episode reward: total was 38.500000. running mean: -8.942948\n",
      "ep 3373: ep_len:3005 episode reward: total was -97.890000. running mean: -9.832419\n",
      "ep 3373: ep_len:552 episode reward: total was -9.800000. running mean: -9.832095\n",
      "ep 3373: ep_len:31 episode reward: total was 14.000000. running mean: -9.593774\n",
      "ep 3373: ep_len:86 episode reward: total was 41.500000. running mean: -9.082836\n",
      "ep 3373: ep_len:75 episode reward: total was 36.000000. running mean: -8.632008\n",
      "ep 3373: ep_len:944 episode reward: total was 53.940000. running mean: -8.006287\n",
      "ep 3373: ep_len:312 episode reward: total was 14.840000. running mean: -7.777825\n",
      "ep 3373: ep_len:803 episode reward: total was -30.780000. running mean: -8.007846\n",
      "ep 3373: ep_len:746 episode reward: total was -18.200000. running mean: -8.109768\n",
      "ep 3373: ep_len:746 episode reward: total was 2.160000. running mean: -8.007070\n",
      "ep 3373: ep_len:61 episode reward: total was 27.500000. running mean: -7.651999\n",
      "ep 3373: ep_len:38 episode reward: total was 16.000000. running mean: -7.415479\n",
      "ep 3373: ep_len:713 episode reward: total was -71.820000. running mean: -8.059525\n",
      "ep 3373: ep_len:2808 episode reward: total was -17.870000. running mean: -8.157629\n",
      "ep 3373: ep_len:54 episode reward: total was 25.500000. running mean: -7.821053\n",
      "epsilon:0.009992 episode_count: 50748. steps_count: 54671307.000000\n",
      "ep 3374: ep_len:884 episode reward: total was 17.180000. running mean: -7.571043\n",
      "ep 3374: ep_len:201 episode reward: total was 10.850000. running mean: -7.386832\n",
      "ep 3374: ep_len:55 episode reward: total was 26.000000. running mean: -7.052964\n",
      "ep 3374: ep_len:2985 episode reward: total was -80.330000. running mean: -7.785734\n",
      "ep 3374: ep_len:500 episode reward: total was 25.540000. running mean: -7.452477\n",
      "ep 3374: ep_len:51 episode reward: total was 24.000000. running mean: -7.137952\n",
      "ep 3374: ep_len:500 episode reward: total was 25.510000. running mean: -6.811473\n",
      "ep 3374: ep_len:3715 episode reward: total was -16.670000. running mean: -6.910058\n",
      "ep 3374: ep_len:676 episode reward: total was -32.800000. running mean: -7.168957\n",
      "ep 3374: ep_len:757 episode reward: total was 27.800000. running mean: -6.819268\n",
      "ep 3374: ep_len:500 episode reward: total was 23.270000. running mean: -6.518375\n",
      "ep 3374: ep_len:76 episode reward: total was 36.500000. running mean: -6.088191\n",
      "ep 3374: ep_len:701 episode reward: total was -64.870000. running mean: -6.676009\n",
      "ep 3374: ep_len:2953 episode reward: total was 11.740000. running mean: -6.491849\n",
      "ep 3374: ep_len:51 episode reward: total was 21.000000. running mean: -6.216931\n",
      "epsilon:0.009992 episode_count: 50763. steps_count: 54685912.000000\n",
      "ep 3375: ep_len:808 episode reward: total was -16.320000. running mean: -6.317961\n",
      "ep 3375: ep_len:3845 episode reward: total was -513.160000. running mean: -11.386382\n",
      "ep 3375: ep_len:55 episode reward: total was 26.000000. running mean: -11.012518\n",
      "ep 3375: ep_len:2923 episode reward: total was -89.510000. running mean: -11.797493\n",
      "ep 3375: ep_len:663 episode reward: total was -5.790000. running mean: -11.737418\n",
      "ep 3375: ep_len:57 episode reward: total was 27.000000. running mean: -11.350044\n",
      "ep 3375: ep_len:46 episode reward: total was 21.500000. running mean: -11.021543\n",
      "ep 3375: ep_len:1467 episode reward: total was 17.470000. running mean: -10.736628\n",
      "ep 3375: ep_len:3658 episode reward: total was -3.300000. running mean: -10.662262\n",
      "ep 3375: ep_len:1282 episode reward: total was -40.140000. running mean: -10.957039\n",
      "ep 3375: ep_len:812 episode reward: total was 47.980000. running mean: -10.367669\n",
      "ep 3375: ep_len:946 episode reward: total was -2.370000. running mean: -10.287692\n",
      "ep 3375: ep_len:77 episode reward: total was 37.000000. running mean: -9.814815\n",
      "ep 3375: ep_len:164 episode reward: total was 79.000000. running mean: -8.926667\n",
      "ep 3375: ep_len:654 episode reward: total was -22.800000. running mean: -9.065400\n",
      "ep 3375: ep_len:2808 episode reward: total was -12.460000. running mean: -9.099346\n",
      "ep 3375: ep_len:45 episode reward: total was 19.500000. running mean: -8.813353\n",
      "epsilon:0.009992 episode_count: 50780. steps_count: 54706222.000000\n",
      "ep 3376: ep_len:1101 episode reward: total was 1.750000. running mean: -8.707719\n",
      "ep 3376: ep_len:199 episode reward: total was 10.280000. running mean: -8.517842\n",
      "ep 3376: ep_len:82 episode reward: total was 39.500000. running mean: -8.037664\n",
      "ep 3376: ep_len:2989 episode reward: total was -162.580000. running mean: -9.583087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3376: ep_len:730 episode reward: total was -12.060000. running mean: -9.607856\n",
      "ep 3376: ep_len:95 episode reward: total was 44.500000. running mean: -9.066778\n",
      "ep 3376: ep_len:78 episode reward: total was 37.500000. running mean: -8.601110\n",
      "ep 3376: ep_len:580 episode reward: total was 41.020000. running mean: -8.104899\n",
      "ep 3376: ep_len:660 episode reward: total was 20.820000. running mean: -7.815650\n",
      "ep 3376: ep_len:788 episode reward: total was -19.820000. running mean: -7.935693\n",
      "ep 3376: ep_len:755 episode reward: total was 10.950000. running mean: -7.746836\n",
      "ep 3376: ep_len:594 episode reward: total was -18.930000. running mean: -7.858668\n",
      "ep 3376: ep_len:117 episode reward: total was 55.500000. running mean: -7.225081\n",
      "ep 3376: ep_len:1204 episode reward: total was -58.680000. running mean: -7.739630\n",
      "ep 3376: ep_len:2891 episode reward: total was -126.630000. running mean: -8.928534\n",
      "ep 3376: ep_len:51 episode reward: total was 24.000000. running mean: -8.599249\n",
      "epsilon:0.009992 episode_count: 50796. steps_count: 54719136.000000\n",
      "ep 3377: ep_len:829 episode reward: total was -28.240000. running mean: -8.795656\n",
      "ep 3377: ep_len:1670 episode reward: total was -18.330000. running mean: -8.891000\n",
      "ep 3377: ep_len:52 episode reward: total was 24.500000. running mean: -8.557090\n",
      "ep 3377: ep_len:3114 episode reward: total was -88.870000. running mean: -9.360219\n",
      "ep 3377: ep_len:584 episode reward: total was -3.100000. running mean: -9.297617\n",
      "ep 3377: ep_len:100 episode reward: total was 48.500000. running mean: -8.719640\n",
      "ep 3377: ep_len:82 episode reward: total was 39.500000. running mean: -8.237444\n",
      "ep 3377: ep_len:749 episode reward: total was -9.850000. running mean: -8.253570\n",
      "ep 3377: ep_len:3629 episode reward: total was -172.420000. running mean: -9.895234\n",
      "ep 3377: ep_len:533 episode reward: total was -22.110000. running mean: -10.017382\n",
      "ep 3377: ep_len:748 episode reward: total was 57.570000. running mean: -9.341508\n",
      "ep 3377: ep_len:613 episode reward: total was 12.260000. running mean: -9.125493\n",
      "ep 3377: ep_len:100 episode reward: total was 48.500000. running mean: -8.549238\n",
      "ep 3377: ep_len:41 episode reward: total was 19.000000. running mean: -8.273745\n",
      "ep 3377: ep_len:631 episode reward: total was -0.410000. running mean: -8.195108\n",
      "ep 3377: ep_len:39 episode reward: total was 18.000000. running mean: -7.933157\n",
      "epsilon:0.009992 episode_count: 50812. steps_count: 54732650.000000\n",
      "ep 3378: ep_len:1196 episode reward: total was -42.810000. running mean: -8.281925\n",
      "ep 3378: ep_len:1690 episode reward: total was -35.300000. running mean: -8.552106\n",
      "ep 3378: ep_len:2917 episode reward: total was -83.130000. running mean: -9.297885\n",
      "ep 3378: ep_len:741 episode reward: total was -33.650000. running mean: -9.541406\n",
      "ep 3378: ep_len:139 episode reward: total was 65.000000. running mean: -8.795992\n",
      "ep 3378: ep_len:561 episode reward: total was 42.850000. running mean: -8.279532\n",
      "ep 3378: ep_len:3576 episode reward: total was -36.750000. running mean: -8.564237\n",
      "ep 3378: ep_len:1269 episode reward: total was -62.220000. running mean: -9.100794\n",
      "ep 3378: ep_len:669 episode reward: total was 8.300000. running mean: -8.926786\n",
      "ep 3378: ep_len:684 episode reward: total was 10.130000. running mean: -8.736219\n",
      "ep 3378: ep_len:58 episode reward: total was 26.000000. running mean: -8.388856\n",
      "ep 3378: ep_len:96 episode reward: total was 43.500000. running mean: -7.869968\n",
      "ep 3378: ep_len:564 episode reward: total was -6.650000. running mean: -7.857768\n",
      "ep 3378: ep_len:2825 episode reward: total was 10.550000. running mean: -7.673691\n",
      "ep 3378: ep_len:66 episode reward: total was 31.500000. running mean: -7.281954\n",
      "epsilon:0.009992 episode_count: 50827. steps_count: 54749701.000000\n",
      "ep 3379: ep_len:1426 episode reward: total was 32.310000. running mean: -6.886034\n",
      "ep 3379: ep_len:208 episode reward: total was 6.880000. running mean: -6.748374\n",
      "ep 3379: ep_len:100 episode reward: total was 45.500000. running mean: -6.225890\n",
      "ep 3379: ep_len:1641 episode reward: total was -33.680000. running mean: -6.500431\n",
      "ep 3379: ep_len:155 episode reward: total was 73.000000. running mean: -5.705427\n",
      "ep 3379: ep_len:77 episode reward: total was 37.000000. running mean: -5.278373\n",
      "ep 3379: ep_len:944 episode reward: total was 66.990000. running mean: -4.555689\n",
      "ep 3379: ep_len:616 episode reward: total was 9.420000. running mean: -4.415932\n",
      "ep 3379: ep_len:500 episode reward: total was -24.580000. running mean: -4.617573\n",
      "ep 3379: ep_len:780 episode reward: total was 14.430000. running mean: -4.427097\n",
      "ep 3379: ep_len:884 episode reward: total was 31.570000. running mean: -4.067126\n",
      "ep 3379: ep_len:217 episode reward: total was 105.500000. running mean: -2.971455\n",
      "ep 3379: ep_len:93 episode reward: total was 45.000000. running mean: -2.491740\n",
      "ep 3379: ep_len:719 episode reward: total was 0.960000. running mean: -2.457223\n",
      "ep 3379: ep_len:2908 episode reward: total was -3.740000. running mean: -2.470050\n",
      "epsilon:0.009992 episode_count: 50842. steps_count: 54760969.000000\n",
      "ep 3380: ep_len:631 episode reward: total was 31.610000. running mean: -2.129250\n",
      "ep 3380: ep_len:740 episode reward: total was -49.010000. running mean: -2.598057\n",
      "ep 3380: ep_len:2933 episode reward: total was -63.620000. running mean: -3.208277\n",
      "ep 3380: ep_len:669 episode reward: total was 10.750000. running mean: -3.068694\n",
      "ep 3380: ep_len:50 episode reward: total was 23.500000. running mean: -2.803007\n",
      "ep 3380: ep_len:104 episode reward: total was 50.500000. running mean: -2.269977\n",
      "ep 3380: ep_len:1068 episode reward: total was -4.640000. running mean: -2.293677\n",
      "ep 3380: ep_len:3720 episode reward: total was -23.190000. running mean: -2.502641\n",
      "ep 3380: ep_len:599 episode reward: total was -41.650000. running mean: -2.894114\n",
      "ep 3380: ep_len:816 episode reward: total was 21.510000. running mean: -2.650073\n",
      "ep 3380: ep_len:614 episode reward: total was 12.820000. running mean: -2.495372\n",
      "ep 3380: ep_len:1405 episode reward: total was 17.980000. running mean: -2.290619\n",
      "ep 3380: ep_len:30 episode reward: total was 13.500000. running mean: -2.132712\n",
      "ep 3380: ep_len:60 episode reward: total was 27.000000. running mean: -1.841385\n",
      "epsilon:0.009992 episode_count: 50856. steps_count: 54774408.000000\n",
      "ep 3381: ep_len:500 episode reward: total was 15.430000. running mean: -1.668671\n",
      "ep 3381: ep_len:973 episode reward: total was -17.070000. running mean: -1.822685\n",
      "ep 3381: ep_len:3034 episode reward: total was -150.920000. running mean: -3.313658\n",
      "ep 3381: ep_len:535 episode reward: total was -30.660000. running mean: -3.587121\n",
      "ep 3381: ep_len:82 episode reward: total was 38.000000. running mean: -3.171250\n",
      "ep 3381: ep_len:680 episode reward: total was -1.880000. running mean: -3.158338\n",
      "ep 3381: ep_len:367 episode reward: total was 25.370000. running mean: -2.873054\n",
      "ep 3381: ep_len:1220 episode reward: total was -56.030000. running mean: -3.404624\n",
      "ep 3381: ep_len:742 episode reward: total was 4.560000. running mean: -3.324977\n",
      "ep 3381: ep_len:624 episode reward: total was 3.040000. running mean: -3.261328\n",
      "ep 3381: ep_len:28 episode reward: total was 12.500000. running mean: -3.103714\n",
      "ep 3381: ep_len:143 episode reward: total was 68.500000. running mean: -2.387677\n",
      "ep 3381: ep_len:111 episode reward: total was 52.500000. running mean: -1.838800\n",
      "ep 3381: ep_len:1078 episode reward: total was -13.990000. running mean: -1.960312\n",
      "ep 3381: ep_len:2885 episode reward: total was -13.100000. running mean: -2.071709\n",
      "ep 3381: ep_len:64 episode reward: total was 26.000000. running mean: -1.790992\n",
      "epsilon:0.009992 episode_count: 50872. steps_count: 54787474.000000\n",
      "ep 3382: ep_len:579 episode reward: total was -0.440000. running mean: -1.777482\n",
      "ep 3382: ep_len:814 episode reward: total was -23.340000. running mean: -1.993107\n",
      "ep 3382: ep_len:50 episode reward: total was 23.500000. running mean: -1.738176\n",
      "ep 3382: ep_len:3013 episode reward: total was -69.580000. running mean: -2.416595\n",
      "ep 3382: ep_len:652 episode reward: total was 2.260000. running mean: -2.369829\n",
      "ep 3382: ep_len:500 episode reward: total was 20.670000. running mean: -2.139430\n",
      "ep 3382: ep_len:668 episode reward: total was 22.400000. running mean: -1.894036\n",
      "ep 3382: ep_len:525 episode reward: total was -9.060000. running mean: -1.965696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3382: ep_len:767 episode reward: total was -12.120000. running mean: -2.067239\n",
      "ep 3382: ep_len:653 episode reward: total was -18.200000. running mean: -2.228566\n",
      "ep 3382: ep_len:1139 episode reward: total was -14.030000. running mean: -2.346581\n",
      "ep 3382: ep_len:46 episode reward: total was 21.500000. running mean: -2.108115\n",
      "ep 3382: ep_len:52 episode reward: total was 24.500000. running mean: -1.842034\n",
      "epsilon:0.009992 episode_count: 50885. steps_count: 54796932.000000\n",
      "ep 3383: ep_len:926 episode reward: total was -30.460000. running mean: -2.128213\n",
      "ep 3383: ep_len:944 episode reward: total was 11.720000. running mean: -1.989731\n",
      "ep 3383: ep_len:53 episode reward: total was 25.000000. running mean: -1.719834\n",
      "ep 3383: ep_len:2953 episode reward: total was -79.010000. running mean: -2.492736\n",
      "ep 3383: ep_len:825 episode reward: total was -4.170000. running mean: -2.509508\n",
      "ep 3383: ep_len:59 episode reward: total was 28.000000. running mean: -2.204413\n",
      "ep 3383: ep_len:500 episode reward: total was 24.130000. running mean: -1.941069\n",
      "ep 3383: ep_len:3745 episode reward: total was -13.800000. running mean: -2.059658\n",
      "ep 3383: ep_len:599 episode reward: total was -41.650000. running mean: -2.455562\n",
      "ep 3383: ep_len:7282 episode reward: total was -191.930000. running mean: -4.350306\n",
      "ep 3383: ep_len:521 episode reward: total was 9.890000. running mean: -4.207903\n",
      "ep 3383: ep_len:58 episode reward: total was 27.500000. running mean: -3.890824\n",
      "ep 3383: ep_len:133 episode reward: total was 65.000000. running mean: -3.201916\n",
      "ep 3383: ep_len:77 episode reward: total was 37.000000. running mean: -2.799897\n",
      "ep 3383: ep_len:659 episode reward: total was -1.660000. running mean: -2.788498\n",
      "ep 3383: ep_len:2834 episode reward: total was -21.230000. running mean: -2.972913\n",
      "epsilon:0.009992 episode_count: 50901. steps_count: 54819100.000000\n",
      "ep 3384: ep_len:846 episode reward: total was 19.400000. running mean: -2.749184\n",
      "ep 3384: ep_len:1190 episode reward: total was -35.740000. running mean: -3.079092\n",
      "ep 3384: ep_len:103 episode reward: total was 48.500000. running mean: -2.563301\n",
      "ep 3384: ep_len:500 episode reward: total was 4.070000. running mean: -2.496968\n",
      "ep 3384: ep_len:64 episode reward: total was 29.000000. running mean: -2.181998\n",
      "ep 3384: ep_len:150 episode reward: total was 69.000000. running mean: -1.470178\n",
      "ep 3384: ep_len:1439 episode reward: total was -191.080000. running mean: -3.366276\n",
      "ep 3384: ep_len:3923 episode reward: total was -96.820000. running mean: -4.300814\n",
      "ep 3384: ep_len:1208 episode reward: total was -46.670000. running mean: -4.724505\n",
      "ep 3384: ep_len:865 episode reward: total was 55.500000. running mean: -4.122260\n",
      "ep 3384: ep_len:575 episode reward: total was 25.160000. running mean: -3.829438\n",
      "ep 3384: ep_len:73 episode reward: total was 33.500000. running mean: -3.456143\n",
      "ep 3384: ep_len:113 episode reward: total was 55.000000. running mean: -2.871582\n",
      "ep 3384: ep_len:782 episode reward: total was -30.640000. running mean: -3.149266\n",
      "ep 3384: ep_len:2874 episode reward: total was 1.270000. running mean: -3.105074\n",
      "epsilon:0.009992 episode_count: 50916. steps_count: 54833805.000000\n",
      "ep 3385: ep_len:1471 episode reward: total was -1.070000. running mean: -3.084723\n",
      "ep 3385: ep_len:502 episode reward: total was -3.840000. running mean: -3.092276\n",
      "ep 3385: ep_len:55 episode reward: total was 24.500000. running mean: -2.816353\n",
      "ep 3385: ep_len:3028 episode reward: total was -163.970000. running mean: -4.427889\n",
      "ep 3385: ep_len:804 episode reward: total was 2.540000. running mean: -4.358210\n",
      "ep 3385: ep_len:107 episode reward: total was 49.000000. running mean: -3.824628\n",
      "ep 3385: ep_len:97 episode reward: total was 45.500000. running mean: -3.331382\n",
      "ep 3385: ep_len:782 episode reward: total was -14.080000. running mean: -3.438868\n",
      "ep 3385: ep_len:3983 episode reward: total was -44.460000. running mean: -3.849080\n",
      "ep 3385: ep_len:1288 episode reward: total was -35.770000. running mean: -4.168289\n",
      "ep 3385: ep_len:909 episode reward: total was 82.820000. running mean: -3.298406\n",
      "ep 3385: ep_len:1461 episode reward: total was 10.060000. running mean: -3.164822\n",
      "ep 3385: ep_len:54 episode reward: total was 25.500000. running mean: -2.878174\n",
      "ep 3385: ep_len:115 episode reward: total was 54.500000. running mean: -2.304392\n",
      "ep 3385: ep_len:3572 episode reward: total was -286.310000. running mean: -5.144448\n",
      "ep 3385: ep_len:2797 episode reward: total was -16.610000. running mean: -5.259103\n",
      "epsilon:0.009992 episode_count: 50932. steps_count: 54854830.000000\n",
      "ep 3386: ep_len:680 episode reward: total was -3.870000. running mean: -5.245212\n",
      "ep 3386: ep_len:641 episode reward: total was -24.060000. running mean: -5.433360\n",
      "ep 3386: ep_len:65 episode reward: total was 29.500000. running mean: -5.084027\n",
      "ep 3386: ep_len:2857 episode reward: total was -204.540000. running mean: -7.078586\n",
      "ep 3386: ep_len:862 episode reward: total was 58.230000. running mean: -6.425501\n",
      "ep 3386: ep_len:100 episode reward: total was 48.500000. running mean: -5.876246\n",
      "ep 3386: ep_len:79 episode reward: total was 38.000000. running mean: -5.437483\n",
      "ep 3386: ep_len:948 episode reward: total was 77.100000. running mean: -4.612108\n",
      "ep 3386: ep_len:348 episode reward: total was 20.250000. running mean: -4.363487\n",
      "ep 3386: ep_len:1271 episode reward: total was -39.980000. running mean: -4.719652\n",
      "ep 3386: ep_len:819 episode reward: total was 56.020000. running mean: -4.112256\n",
      "ep 3386: ep_len:719 episode reward: total was -13.180000. running mean: -4.202933\n",
      "ep 3386: ep_len:85 episode reward: total was 39.500000. running mean: -3.765904\n",
      "ep 3386: ep_len:98 episode reward: total was 47.500000. running mean: -3.253245\n",
      "ep 3386: ep_len:3512 episode reward: total was -214.310000. running mean: -5.363812\n",
      "ep 3386: ep_len:2861 episode reward: total was 3.720000. running mean: -5.272974\n",
      "ep 3386: ep_len:41 episode reward: total was 19.000000. running mean: -5.030245\n",
      "epsilon:0.009992 episode_count: 50949. steps_count: 54870816.000000\n",
      "ep 3387: ep_len:924 episode reward: total was -67.080000. running mean: -5.650742\n",
      "ep 3387: ep_len:1639 episode reward: total was -23.570000. running mean: -5.829935\n",
      "ep 3387: ep_len:3003 episode reward: total was -129.340000. running mean: -7.065035\n",
      "ep 3387: ep_len:722 episode reward: total was -24.720000. running mean: -7.241585\n",
      "ep 3387: ep_len:46 episode reward: total was 20.000000. running mean: -6.969169\n",
      "ep 3387: ep_len:58 episode reward: total was 24.500000. running mean: -6.654477\n",
      "ep 3387: ep_len:1484 episode reward: total was -238.840000. running mean: -8.976333\n",
      "ep 3387: ep_len:643 episode reward: total was 11.410000. running mean: -8.772469\n",
      "ep 3387: ep_len:1204 episode reward: total was -99.140000. running mean: -9.676145\n",
      "ep 3387: ep_len:828 episode reward: total was 36.510000. running mean: -9.214283\n",
      "ep 3387: ep_len:690 episode reward: total was -8.420000. running mean: -9.206340\n",
      "ep 3387: ep_len:81 episode reward: total was 39.000000. running mean: -8.724277\n",
      "ep 3387: ep_len:33 episode reward: total was 15.000000. running mean: -8.487034\n",
      "ep 3387: ep_len:656 episode reward: total was 8.060000. running mean: -8.321564\n",
      "ep 3387: ep_len:2832 episode reward: total was -17.110000. running mean: -8.409448\n",
      "epsilon:0.009992 episode_count: 50964. steps_count: 54885659.000000\n",
      "ep 3388: ep_len:1166 episode reward: total was -7.450000. running mean: -8.399854\n",
      "ep 3388: ep_len:992 episode reward: total was 2.560000. running mean: -8.290255\n",
      "ep 3388: ep_len:60 episode reward: total was 28.500000. running mean: -7.922353\n",
      "ep 3388: ep_len:2939 episode reward: total was -96.420000. running mean: -8.807329\n",
      "ep 3388: ep_len:500 episode reward: total was 4.810000. running mean: -8.671156\n",
      "ep 3388: ep_len:49 episode reward: total was 21.500000. running mean: -8.369444\n",
      "ep 3388: ep_len:98 episode reward: total was 44.500000. running mean: -7.840750\n",
      "ep 3388: ep_len:36 episode reward: total was 16.500000. running mean: -7.597342\n",
      "ep 3388: ep_len:1034 episode reward: total was -48.900000. running mean: -8.010369\n",
      "ep 3388: ep_len:665 episode reward: total was 28.710000. running mean: -7.643165\n",
      "ep 3388: ep_len:588 episode reward: total was -16.720000. running mean: -7.733934\n",
      "ep 3388: ep_len:7269 episode reward: total was -15.840000. running mean: -7.814994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3388: ep_len:721 episode reward: total was -12.250000. running mean: -7.859344\n",
      "ep 3388: ep_len:67 episode reward: total was 30.500000. running mean: -7.475751\n",
      "ep 3388: ep_len:132 episode reward: total was 63.000000. running mean: -6.770993\n",
      "ep 3388: ep_len:619 episode reward: total was 9.320000. running mean: -6.610083\n",
      "ep 3388: ep_len:2727 episode reward: total was -10.270000. running mean: -6.646683\n",
      "ep 3388: ep_len:45 episode reward: total was 21.000000. running mean: -6.370216\n",
      "epsilon:0.009992 episode_count: 50982. steps_count: 54905366.000000\n",
      "ep 3389: ep_len:889 episode reward: total was 23.840000. running mean: -6.068114\n",
      "ep 3389: ep_len:984 episode reward: total was 9.120000. running mean: -5.916232\n",
      "ep 3389: ep_len:88 episode reward: total was 41.000000. running mean: -5.447070\n",
      "ep 3389: ep_len:1230 episode reward: total was -19.900000. running mean: -5.591599\n",
      "ep 3389: ep_len:63 episode reward: total was 28.500000. running mean: -5.250683\n",
      "ep 3389: ep_len:85 episode reward: total was 41.000000. running mean: -4.788177\n",
      "ep 3389: ep_len:621 episode reward: total was 21.750000. running mean: -4.522795\n",
      "ep 3389: ep_len:342 episode reward: total was -4.820000. running mean: -4.525767\n",
      "ep 3389: ep_len:641 episode reward: total was -29.110000. running mean: -4.771609\n",
      "ep 3389: ep_len:904 episode reward: total was 70.740000. running mean: -4.016493\n",
      "ep 3389: ep_len:872 episode reward: total was 31.480000. running mean: -3.661528\n",
      "ep 3389: ep_len:97 episode reward: total was 45.500000. running mean: -3.169913\n",
      "ep 3389: ep_len:501 episode reward: total was 24.380000. running mean: -2.894414\n",
      "ep 3389: ep_len:2776 episode reward: total was -302.530000. running mean: -5.890770\n",
      "ep 3389: ep_len:39 episode reward: total was 16.500000. running mean: -5.666862\n",
      "epsilon:0.009992 episode_count: 50997. steps_count: 54915498.000000\n",
      "ep 3390: ep_len:775 episode reward: total was -24.740000. running mean: -5.857593\n",
      "ep 3390: ep_len:693 episode reward: total was -24.700000. running mean: -6.046017\n",
      "ep 3390: ep_len:2999 episode reward: total was -120.850000. running mean: -7.194057\n",
      "ep 3390: ep_len:684 episode reward: total was 8.560000. running mean: -7.036517\n",
      "ep 3390: ep_len:76 episode reward: total was 36.500000. running mean: -6.601151\n",
      "ep 3390: ep_len:1007 episode reward: total was -32.520000. running mean: -6.860340\n",
      "ep 3390: ep_len:643 episode reward: total was 15.970000. running mean: -6.632037\n",
      "ep 3390: ep_len:1576 episode reward: total was -28.300000. running mean: -6.848716\n",
      "ep 3390: ep_len:694 episode reward: total was 17.130000. running mean: -6.608929\n",
      "ep 3390: ep_len:1045 episode reward: total was -14.970000. running mean: -6.692540\n",
      "ep 3390: ep_len:92 episode reward: total was 43.000000. running mean: -6.195614\n",
      "ep 3390: ep_len:601 episode reward: total was -15.340000. running mean: -6.287058\n",
      "ep 3390: ep_len:2827 episode reward: total was 5.820000. running mean: -6.165988\n",
      "epsilon:0.009992 episode_count: 51010. steps_count: 54929210.000000\n",
      "ep 3391: ep_len:990 episode reward: total was -85.640000. running mean: -6.960728\n",
      "ep 3391: ep_len:500 episode reward: total was 11.880000. running mean: -6.772320\n",
      "ep 3391: ep_len:103 episode reward: total was 50.000000. running mean: -6.204597\n",
      "ep 3391: ep_len:1203 episode reward: total was -18.520000. running mean: -6.327751\n",
      "ep 3391: ep_len:48 episode reward: total was 22.500000. running mean: -6.039474\n",
      "ep 3391: ep_len:1001 episode reward: total was -12.100000. running mean: -6.100079\n",
      "ep 3391: ep_len:311 episode reward: total was 29.860000. running mean: -5.740478\n",
      "ep 3391: ep_len:3068 episode reward: total was -1965.250000. running mean: -25.335573\n",
      "ep 3391: ep_len:640 episode reward: total was 24.300000. running mean: -24.839218\n",
      "ep 3391: ep_len:502 episode reward: total was -6.260000. running mean: -24.653426\n",
      "ep 3391: ep_len:48 episode reward: total was 22.500000. running mean: -24.181891\n",
      "ep 3391: ep_len:123 episode reward: total was 54.000000. running mean: -23.400072\n",
      "ep 3391: ep_len:934 episode reward: total was 24.110000. running mean: -22.924972\n",
      "ep 3391: ep_len:2821 episode reward: total was 3.100000. running mean: -22.664722\n",
      "epsilon:0.009992 episode_count: 51024. steps_count: 54941502.000000\n",
      "ep 3392: ep_len:706 episode reward: total was -42.600000. running mean: -22.864075\n",
      "ep 3392: ep_len:724 episode reward: total was -7.000000. running mean: -22.705434\n",
      "ep 3392: ep_len:2949 episode reward: total was -113.700000. running mean: -23.615380\n",
      "ep 3392: ep_len:500 episode reward: total was 18.490000. running mean: -23.194326\n",
      "ep 3392: ep_len:51 episode reward: total was 24.000000. running mean: -22.722383\n",
      "ep 3392: ep_len:105 episode reward: total was 49.500000. running mean: -22.000159\n",
      "ep 3392: ep_len:742 episode reward: total was -23.940000. running mean: -22.019557\n",
      "ep 3392: ep_len:659 episode reward: total was 17.750000. running mean: -21.621862\n",
      "ep 3392: ep_len:725 episode reward: total was -29.260000. running mean: -21.698243\n",
      "ep 3392: ep_len:648 episode reward: total was -4.460000. running mean: -21.525861\n",
      "ep 3392: ep_len:500 episode reward: total was 3.550000. running mean: -21.275102\n",
      "ep 3392: ep_len:83 episode reward: total was 40.000000. running mean: -20.662351\n",
      "ep 3392: ep_len:177 episode reward: total was 84.000000. running mean: -19.615727\n",
      "ep 3392: ep_len:500 episode reward: total was 23.550000. running mean: -19.184070\n",
      "ep 3392: ep_len:2863 episode reward: total was -4.290000. running mean: -19.035129\n",
      "epsilon:0.009992 episode_count: 51039. steps_count: 54953434.000000\n",
      "ep 3393: ep_len:650 episode reward: total was 2.660000. running mean: -18.818178\n",
      "ep 3393: ep_len:706 episode reward: total was 4.120000. running mean: -18.588796\n",
      "ep 3393: ep_len:3021 episode reward: total was -54.200000. running mean: -18.944908\n",
      "ep 3393: ep_len:530 episode reward: total was -19.110000. running mean: -18.946559\n",
      "ep 3393: ep_len:774 episode reward: total was -16.120000. running mean: -18.918294\n",
      "ep 3393: ep_len:662 episode reward: total was 26.320000. running mean: -18.465911\n",
      "ep 3393: ep_len:1551 episode reward: total was -46.430000. running mean: -18.745552\n",
      "ep 3393: ep_len:759 episode reward: total was 39.670000. running mean: -18.161396\n",
      "ep 3393: ep_len:1466 episode reward: total was 10.570000. running mean: -17.874082\n",
      "ep 3393: ep_len:58 episode reward: total was 26.000000. running mean: -17.435341\n",
      "ep 3393: ep_len:1025 episode reward: total was -4.150000. running mean: -17.302488\n",
      "ep 3393: ep_len:2863 episode reward: total was -2.700000. running mean: -17.156463\n",
      "epsilon:0.009992 episode_count: 51051. steps_count: 54967499.000000\n",
      "ep 3394: ep_len:890 episode reward: total was 25.380000. running mean: -16.731098\n",
      "ep 3394: ep_len:648 episode reward: total was -17.930000. running mean: -16.743087\n",
      "ep 3394: ep_len:2869 episode reward: total was -75.800000. running mean: -17.333657\n",
      "ep 3394: ep_len:548 episode reward: total was -4.660000. running mean: -17.206920\n",
      "ep 3394: ep_len:40 episode reward: total was 17.000000. running mean: -16.864851\n",
      "ep 3394: ep_len:656 episode reward: total was 5.310000. running mean: -16.643102\n",
      "ep 3394: ep_len:3933 episode reward: total was -853.300000. running mean: -25.009671\n",
      "ep 3394: ep_len:1591 episode reward: total was -60.500000. running mean: -25.364575\n",
      "ep 3394: ep_len:657 episode reward: total was 9.560000. running mean: -25.015329\n",
      "ep 3394: ep_len:631 episode reward: total was 13.530000. running mean: -24.629876\n",
      "ep 3394: ep_len:210 episode reward: total was 100.500000. running mean: -23.378577\n",
      "ep 3394: ep_len:645 episode reward: total was -3.820000. running mean: -23.182991\n",
      "ep 3394: ep_len:2900 episode reward: total was -33.210000. running mean: -23.283261\n",
      "ep 3394: ep_len:56 episode reward: total was 26.500000. running mean: -22.785428\n",
      "epsilon:0.009992 episode_count: 51065. steps_count: 54983773.000000\n",
      "ep 3395: ep_len:1525 episode reward: total was 31.180000. running mean: -22.245774\n",
      "ep 3395: ep_len:504 episode reward: total was -67.900000. running mean: -22.702316\n",
      "ep 3395: ep_len:2906 episode reward: total was -821.090000. running mean: -30.686193\n",
      "ep 3395: ep_len:536 episode reward: total was 7.190000. running mean: -30.307431\n",
      "ep 3395: ep_len:37 episode reward: total was 17.000000. running mean: -29.834357\n",
      "ep 3395: ep_len:79 episode reward: total was 36.500000. running mean: -29.171013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3395: ep_len:43 episode reward: total was 20.000000. running mean: -28.679303\n",
      "ep 3395: ep_len:881 episode reward: total was 49.210000. running mean: -27.900410\n",
      "ep 3395: ep_len:344 episode reward: total was 1.110000. running mean: -27.610306\n",
      "ep 3395: ep_len:508 episode reward: total was 3.900000. running mean: -27.295203\n",
      "ep 3395: ep_len:686 episode reward: total was 10.280000. running mean: -26.919451\n",
      "ep 3395: ep_len:1134 episode reward: total was -10.040000. running mean: -26.750657\n",
      "ep 3395: ep_len:1074 episode reward: total was 28.600000. running mean: -26.197150\n",
      "ep 3395: ep_len:2860 episode reward: total was -28.770000. running mean: -26.222879\n",
      "epsilon:0.009992 episode_count: 51079. steps_count: 54996890.000000\n",
      "ep 3396: ep_len:624 episode reward: total was -14.010000. running mean: -26.100750\n",
      "ep 3396: ep_len:726 episode reward: total was -62.220000. running mean: -26.461942\n",
      "ep 3396: ep_len:47 episode reward: total was 22.000000. running mean: -25.977323\n",
      "ep 3396: ep_len:3066 episode reward: total was -61.370000. running mean: -26.331250\n",
      "ep 3396: ep_len:590 episode reward: total was -29.620000. running mean: -26.364137\n",
      "ep 3396: ep_len:41 episode reward: total was 17.500000. running mean: -25.925496\n",
      "ep 3396: ep_len:105 episode reward: total was 48.000000. running mean: -25.186241\n",
      "ep 3396: ep_len:1810 episode reward: total was -83.400000. running mean: -25.768378\n",
      "ep 3396: ep_len:331 episode reward: total was 8.450000. running mean: -25.426195\n",
      "ep 3396: ep_len:879 episode reward: total was -94.610000. running mean: -26.118033\n",
      "ep 3396: ep_len:759 episode reward: total was -5.150000. running mean: -25.908352\n",
      "ep 3396: ep_len:500 episode reward: total was 20.920000. running mean: -25.440069\n",
      "ep 3396: ep_len:79 episode reward: total was 38.000000. running mean: -24.805668\n",
      "ep 3396: ep_len:1417 episode reward: total was -0.790000. running mean: -24.565511\n",
      "ep 3396: ep_len:38 episode reward: total was 17.500000. running mean: -24.144856\n",
      "ep 3396: ep_len:69 episode reward: total was 28.500000. running mean: -23.618408\n",
      "epsilon:0.009992 episode_count: 51095. steps_count: 55007971.000000\n",
      "ep 3397: ep_len:931 episode reward: total was -30.360000. running mean: -23.685824\n",
      "ep 3397: ep_len:933 episode reward: total was 6.680000. running mean: -23.382165\n",
      "ep 3397: ep_len:2999 episode reward: total was -14.790000. running mean: -23.296244\n",
      "ep 3397: ep_len:565 episode reward: total was -12.110000. running mean: -23.184381\n",
      "ep 3397: ep_len:118 episode reward: total was 57.500000. running mean: -22.377538\n",
      "ep 3397: ep_len:692 episode reward: total was -50.820000. running mean: -22.661962\n",
      "ep 3397: ep_len:348 episode reward: total was 14.070000. running mean: -22.294643\n",
      "ep 3397: ep_len:637 episode reward: total was -20.060000. running mean: -22.272296\n",
      "ep 3397: ep_len:7276 episode reward: total was 28.800000. running mean: -21.761573\n",
      "ep 3397: ep_len:1507 episode reward: total was 3.050000. running mean: -21.513457\n",
      "ep 3397: ep_len:47 episode reward: total was 20.500000. running mean: -21.093323\n",
      "ep 3397: ep_len:1071 episode reward: total was 33.040000. running mean: -20.551990\n",
      "ep 3397: ep_len:2867 episode reward: total was 7.510000. running mean: -20.271370\n",
      "ep 3397: ep_len:55 episode reward: total was 23.000000. running mean: -19.838656\n",
      "epsilon:0.009992 episode_count: 51109. steps_count: 55028017.000000\n",
      "ep 3398: ep_len:763 episode reward: total was -5.690000. running mean: -19.697169\n",
      "ep 3398: ep_len:617 episode reward: total was -8.140000. running mean: -19.581598\n",
      "ep 3398: ep_len:2983 episode reward: total was -18.240000. running mean: -19.568182\n",
      "ep 3398: ep_len:501 episode reward: total was -0.470000. running mean: -19.377200\n",
      "ep 3398: ep_len:70 episode reward: total was 32.000000. running mean: -18.863428\n",
      "ep 3398: ep_len:889 episode reward: total was 46.470000. running mean: -18.210094\n",
      "ep 3398: ep_len:647 episode reward: total was 11.420000. running mean: -17.913793\n",
      "ep 3398: ep_len:1544 episode reward: total was -38.260000. running mean: -18.117255\n",
      "ep 3398: ep_len:592 episode reward: total was 9.520000. running mean: -17.840882\n",
      "ep 3398: ep_len:618 episode reward: total was -0.050000. running mean: -17.662973\n",
      "ep 3398: ep_len:169 episode reward: total was 81.500000. running mean: -16.671344\n",
      "ep 3398: ep_len:36 episode reward: total was 16.500000. running mean: -16.339630\n",
      "ep 3398: ep_len:68 episode reward: total was 32.500000. running mean: -15.851234\n",
      "ep 3398: ep_len:665 episode reward: total was 0.420000. running mean: -15.688522\n",
      "ep 3398: ep_len:2724 episode reward: total was -28.330000. running mean: -15.814936\n",
      "ep 3398: ep_len:56 episode reward: total was 25.000000. running mean: -15.406787\n",
      "epsilon:0.009992 episode_count: 51125. steps_count: 55040959.000000\n",
      "ep 3399: ep_len:1509 episode reward: total was 7.150000. running mean: -15.181219\n",
      "ep 3399: ep_len:1156 episode reward: total was -75.810000. running mean: -15.787507\n",
      "ep 3399: ep_len:61 episode reward: total was 29.000000. running mean: -15.339632\n",
      "ep 3399: ep_len:2928 episode reward: total was -90.120000. running mean: -16.087436\n",
      "ep 3399: ep_len:643 episode reward: total was -15.960000. running mean: -16.086161\n",
      "ep 3399: ep_len:500 episode reward: total was 10.620000. running mean: -15.819100\n",
      "ep 3399: ep_len:3940 episode reward: total was -198.750000. running mean: -17.648409\n",
      "ep 3399: ep_len:1157 episode reward: total was -15.470000. running mean: -17.626625\n",
      "ep 3399: ep_len:752 episode reward: total was 18.470000. running mean: -17.265658\n",
      "ep 3399: ep_len:1023 episode reward: total was 0.360000. running mean: -17.089402\n",
      "ep 3399: ep_len:70 episode reward: total was 33.500000. running mean: -16.583508\n",
      "ep 3399: ep_len:64 episode reward: total was 27.500000. running mean: -16.142673\n",
      "ep 3399: ep_len:1491 episode reward: total was 2.930000. running mean: -15.951946\n",
      "ep 3399: ep_len:2876 episode reward: total was -13.240000. running mean: -15.924826\n",
      "epsilon:0.009992 episode_count: 51139. steps_count: 55059129.000000\n",
      "ep 3400: ep_len:635 episode reward: total was 21.570000. running mean: -15.549878\n",
      "ep 3400: ep_len:1269 episode reward: total was -49.090000. running mean: -15.885279\n",
      "ep 3400: ep_len:70 episode reward: total was 33.500000. running mean: -15.391427\n",
      "ep 3400: ep_len:3023 episode reward: total was -54.060000. running mean: -15.778112\n",
      "ep 3400: ep_len:645 episode reward: total was 9.580000. running mean: -15.524531\n",
      "ep 3400: ep_len:157 episode reward: total was 77.000000. running mean: -14.599286\n",
      "ep 3400: ep_len:1293 episode reward: total was 1.400000. running mean: -14.439293\n",
      "ep 3400: ep_len:598 episode reward: total was -6.280000. running mean: -14.357700\n",
      "ep 3400: ep_len:1537 episode reward: total was -123.150000. running mean: -15.445623\n",
      "ep 3400: ep_len:753 episode reward: total was 33.550000. running mean: -14.955667\n",
      "ep 3400: ep_len:612 episode reward: total was 10.320000. running mean: -14.702910\n",
      "ep 3400: ep_len:62 episode reward: total was 29.500000. running mean: -14.260881\n",
      "ep 3400: ep_len:777 episode reward: total was -79.260000. running mean: -14.910872\n",
      "ep 3400: ep_len:2813 episode reward: total was -9.680000. running mean: -14.858564\n",
      "epsilon:0.009992 episode_count: 51153. steps_count: 55073373.000000\n",
      "ep 3401: ep_len:1136 episode reward: total was 6.670000. running mean: -14.643278\n",
      "ep 3401: ep_len:1186 episode reward: total was -73.640000. running mean: -15.233245\n",
      "ep 3401: ep_len:2974 episode reward: total was -46.700000. running mean: -15.547913\n",
      "ep 3401: ep_len:1421 episode reward: total was 12.970000. running mean: -15.262734\n",
      "ep 3401: ep_len:1359 episode reward: total was -125.960000. running mean: -16.369706\n",
      "ep 3401: ep_len:669 episode reward: total was 18.220000. running mean: -16.023809\n",
      "ep 3401: ep_len:768 episode reward: total was -51.720000. running mean: -16.380771\n",
      "ep 3401: ep_len:690 episode reward: total was 22.140000. running mean: -15.995563\n",
      "ep 3401: ep_len:1031 episode reward: total was 32.990000. running mean: -15.505708\n",
      "ep 3401: ep_len:148 episode reward: total was 72.500000. running mean: -14.625651\n",
      "ep 3401: ep_len:85 episode reward: total was 36.500000. running mean: -14.114394\n",
      "ep 3401: ep_len:1139 episode reward: total was -11.000000. running mean: -14.083250\n",
      "ep 3401: ep_len:2837 episode reward: total was -10.300000. running mean: -14.045418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3401: ep_len:25 episode reward: total was 11.000000. running mean: -13.794964\n",
      "epsilon:0.009992 episode_count: 51167. steps_count: 55088841.000000\n",
      "ep 3402: ep_len:608 episode reward: total was 6.880000. running mean: -13.588214\n",
      "ep 3402: ep_len:755 episode reward: total was -17.440000. running mean: -13.626732\n",
      "ep 3402: ep_len:3055 episode reward: total was -45.540000. running mean: -13.945864\n",
      "ep 3402: ep_len:1244 episode reward: total was -49.280000. running mean: -14.299206\n",
      "ep 3402: ep_len:36 episode reward: total was 15.000000. running mean: -14.006214\n",
      "ep 3402: ep_len:57 episode reward: total was 27.000000. running mean: -13.596152\n",
      "ep 3402: ep_len:1091 episode reward: total was 2.020000. running mean: -13.439990\n",
      "ep 3402: ep_len:295 episode reward: total was 23.150000. running mean: -13.074090\n",
      "ep 3402: ep_len:1234 episode reward: total was -49.440000. running mean: -13.437749\n",
      "ep 3402: ep_len:817 episode reward: total was 47.420000. running mean: -12.829172\n",
      "ep 3402: ep_len:916 episode reward: total was 47.150000. running mean: -12.229380\n",
      "ep 3402: ep_len:83 episode reward: total was 38.500000. running mean: -11.722086\n",
      "ep 3402: ep_len:663 episode reward: total was 11.810000. running mean: -11.486765\n",
      "ep 3402: ep_len:2725 episode reward: total was -23.540000. running mean: -11.607298\n",
      "epsilon:0.009992 episode_count: 51181. steps_count: 55102420.000000\n",
      "ep 3403: ep_len:1126 episode reward: total was -9.170000. running mean: -11.582925\n",
      "ep 3403: ep_len:1593 episode reward: total was -66.050000. running mean: -12.127596\n",
      "ep 3403: ep_len:2884 episode reward: total was -75.880000. running mean: -12.765120\n",
      "ep 3403: ep_len:500 episode reward: total was 17.730000. running mean: -12.460168\n",
      "ep 3403: ep_len:62 episode reward: total was 28.000000. running mean: -12.055567\n",
      "ep 3403: ep_len:161 episode reward: total was 76.000000. running mean: -11.175011\n",
      "ep 3403: ep_len:78 episode reward: total was 37.500000. running mean: -10.688261\n",
      "ep 3403: ep_len:1846 episode reward: total was -101.130000. running mean: -11.592678\n",
      "ep 3403: ep_len:3617 episode reward: total was -271.670000. running mean: -14.193452\n",
      "ep 3403: ep_len:1124 episode reward: total was -27.980000. running mean: -14.331317\n",
      "ep 3403: ep_len:757 episode reward: total was -0.130000. running mean: -14.189304\n",
      "ep 3403: ep_len:500 episode reward: total was -3.360000. running mean: -14.081011\n",
      "ep 3403: ep_len:159 episode reward: total was 78.000000. running mean: -13.160201\n",
      "ep 3403: ep_len:60 episode reward: total was 28.500000. running mean: -12.743599\n",
      "ep 3403: ep_len:64 episode reward: total was 30.500000. running mean: -12.311163\n",
      "ep 3403: ep_len:665 episode reward: total was 2.440000. running mean: -12.163651\n",
      "ep 3403: ep_len:2832 episode reward: total was 7.160000. running mean: -11.970415\n",
      "epsilon:0.009992 episode_count: 51198. steps_count: 55120448.000000\n",
      "ep 3404: ep_len:931 episode reward: total was -57.840000. running mean: -12.429110\n",
      "ep 3404: ep_len:679 episode reward: total was -56.000000. running mean: -12.864819\n",
      "ep 3404: ep_len:2917 episode reward: total was -15.580000. running mean: -12.891971\n",
      "ep 3404: ep_len:778 episode reward: total was -21.340000. running mean: -12.976451\n",
      "ep 3404: ep_len:1054 episode reward: total was -36.020000. running mean: -13.206887\n",
      "ep 3404: ep_len:335 episode reward: total was 10.910000. running mean: -12.965718\n",
      "ep 3404: ep_len:1273 episode reward: total was -40.410000. running mean: -13.240161\n",
      "ep 3404: ep_len:825 episode reward: total was 29.770000. running mean: -12.810059\n",
      "ep 3404: ep_len:934 episode reward: total was 55.070000. running mean: -12.131259\n",
      "ep 3404: ep_len:161 episode reward: total was 77.500000. running mean: -11.234946\n",
      "ep 3404: ep_len:663 episode reward: total was -27.880000. running mean: -11.401397\n",
      "ep 3404: ep_len:2825 episode reward: total was 4.120000. running mean: -11.246183\n",
      "ep 3404: ep_len:69 episode reward: total was 33.000000. running mean: -10.803721\n",
      "epsilon:0.009992 episode_count: 51211. steps_count: 55133892.000000\n",
      "ep 3405: ep_len:1144 episode reward: total was -8.930000. running mean: -10.784984\n",
      "ep 3405: ep_len:797 episode reward: total was -32.600000. running mean: -11.003134\n",
      "ep 3405: ep_len:49 episode reward: total was 21.500000. running mean: -10.678102\n",
      "ep 3405: ep_len:2918 episode reward: total was -49.600000. running mean: -11.067321\n",
      "ep 3405: ep_len:529 episode reward: total was -8.130000. running mean: -11.037948\n",
      "ep 3405: ep_len:48 episode reward: total was 21.000000. running mean: -10.717569\n",
      "ep 3405: ep_len:855 episode reward: total was 34.490000. running mean: -10.265493\n",
      "ep 3405: ep_len:3900 episode reward: total was -129.460000. running mean: -11.457438\n",
      "ep 3405: ep_len:618 episode reward: total was -5.320000. running mean: -11.396064\n",
      "ep 3405: ep_len:612 episode reward: total was 22.280000. running mean: -11.059303\n",
      "ep 3405: ep_len:593 episode reward: total was -10.400000. running mean: -11.052710\n",
      "ep 3405: ep_len:43 episode reward: total was 18.500000. running mean: -10.757183\n",
      "ep 3405: ep_len:104 episode reward: total was 50.500000. running mean: -10.144611\n",
      "ep 3405: ep_len:1515 episode reward: total was 15.080000. running mean: -9.892365\n",
      "ep 3405: ep_len:2833 episode reward: total was 12.340000. running mean: -9.670041\n",
      "ep 3405: ep_len:50 episode reward: total was 22.000000. running mean: -9.353341\n",
      "epsilon:0.009992 episode_count: 51227. steps_count: 55150500.000000\n",
      "ep 3406: ep_len:637 episode reward: total was -20.030000. running mean: -9.460108\n",
      "ep 3406: ep_len:991 episode reward: total was 11.060000. running mean: -9.254906\n",
      "ep 3406: ep_len:75 episode reward: total was 34.500000. running mean: -8.817357\n",
      "ep 3406: ep_len:2906 episode reward: total was -26.840000. running mean: -8.997584\n",
      "ep 3406: ep_len:820 episode reward: total was -1.880000. running mean: -8.926408\n",
      "ep 3406: ep_len:55 episode reward: total was 26.000000. running mean: -8.577144\n",
      "ep 3406: ep_len:1445 episode reward: total was -77.110000. running mean: -9.262472\n",
      "ep 3406: ep_len:606 episode reward: total was 15.690000. running mean: -9.012948\n",
      "ep 3406: ep_len:580 episode reward: total was 9.710000. running mean: -8.825718\n",
      "ep 3406: ep_len:813 episode reward: total was 23.340000. running mean: -8.504061\n",
      "ep 3406: ep_len:694 episode reward: total was 18.840000. running mean: -8.230620\n",
      "ep 3406: ep_len:155 episode reward: total was 76.000000. running mean: -7.388314\n",
      "ep 3406: ep_len:65 episode reward: total was 31.000000. running mean: -7.004431\n",
      "ep 3406: ep_len:99 episode reward: total was 46.500000. running mean: -6.469387\n",
      "ep 3406: ep_len:602 episode reward: total was 2.600000. running mean: -6.378693\n",
      "ep 3406: ep_len:2825 episode reward: total was 10.180000. running mean: -6.213106\n",
      "ep 3406: ep_len:62 episode reward: total was 28.000000. running mean: -5.870975\n",
      "epsilon:0.009992 episode_count: 51244. steps_count: 55163930.000000\n",
      "ep 3407: ep_len:607 episode reward: total was 13.610000. running mean: -5.676165\n",
      "ep 3407: ep_len:673 episode reward: total was -15.660000. running mean: -5.776004\n",
      "ep 3407: ep_len:43 episode reward: total was 18.500000. running mean: -5.533244\n",
      "ep 3407: ep_len:2946 episode reward: total was -104.030000. running mean: -6.518211\n",
      "ep 3407: ep_len:683 episode reward: total was -5.350000. running mean: -6.506529\n",
      "ep 3407: ep_len:1326 episode reward: total was -8.120000. running mean: -6.522664\n",
      "ep 3407: ep_len:634 episode reward: total was 22.340000. running mean: -6.234037\n",
      "ep 3407: ep_len:1019 episode reward: total was -37.620000. running mean: -6.547897\n",
      "ep 3407: ep_len:778 episode reward: total was 22.680000. running mean: -6.255618\n",
      "ep 3407: ep_len:572 episode reward: total was 47.050000. running mean: -5.722562\n",
      "ep 3407: ep_len:88 episode reward: total was 42.500000. running mean: -5.240336\n",
      "ep 3407: ep_len:1437 episode reward: total was -1.170000. running mean: -5.199633\n",
      "ep 3407: ep_len:2788 episode reward: total was 2.770000. running mean: -5.119936\n",
      "ep 3407: ep_len:52 episode reward: total was 24.500000. running mean: -4.823737\n",
      "epsilon:0.009992 episode_count: 51258. steps_count: 55177576.000000\n",
      "ep 3408: ep_len:2467 episode reward: total was -544.620000. running mean: -10.221699\n",
      "ep 3408: ep_len:785 episode reward: total was -6.460000. running mean: -10.184082\n",
      "ep 3408: ep_len:44 episode reward: total was 20.500000. running mean: -9.877242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3408: ep_len:87 episode reward: total was 40.500000. running mean: -9.373469\n",
      "ep 3408: ep_len:594 episode reward: total was 8.080000. running mean: -9.198935\n",
      "ep 3408: ep_len:93 episode reward: total was 42.000000. running mean: -8.686945\n",
      "ep 3408: ep_len:1421 episode reward: total was 17.040000. running mean: -8.429676\n",
      "ep 3408: ep_len:4250 episode reward: total was -262.900000. running mean: -10.974379\n",
      "ep 3408: ep_len:707 episode reward: total was -20.420000. running mean: -11.068835\n",
      "ep 3408: ep_len:727 episode reward: total was 35.890000. running mean: -10.599247\n",
      "ep 3408: ep_len:1496 episode reward: total was 16.470000. running mean: -10.328554\n",
      "ep 3408: ep_len:65 episode reward: total was 28.000000. running mean: -9.945269\n",
      "ep 3408: ep_len:635 episode reward: total was 1.130000. running mean: -9.834516\n",
      "ep 3408: ep_len:2849 episode reward: total was -2.770000. running mean: -9.763871\n",
      "ep 3408: ep_len:58 episode reward: total was 27.500000. running mean: -9.391232\n",
      "epsilon:0.009992 episode_count: 51273. steps_count: 55193854.000000\n",
      "ep 3409: ep_len:1457 episode reward: total was 6.960000. running mean: -9.227720\n",
      "ep 3409: ep_len:185 episode reward: total was 12.680000. running mean: -9.008643\n",
      "ep 3409: ep_len:2851 episode reward: total was -55.400000. running mean: -9.472556\n",
      "ep 3409: ep_len:538 episode reward: total was -28.120000. running mean: -9.659031\n",
      "ep 3409: ep_len:68 episode reward: total was 32.500000. running mean: -9.237440\n",
      "ep 3409: ep_len:1349 episode reward: total was -19.950000. running mean: -9.344566\n",
      "ep 3409: ep_len:4204 episode reward: total was -625.520000. running mean: -15.506320\n",
      "ep 3409: ep_len:733 episode reward: total was -12.070000. running mean: -15.471957\n",
      "ep 3409: ep_len:631 episode reward: total was -2.980000. running mean: -15.347038\n",
      "ep 3409: ep_len:585 episode reward: total was -20.470000. running mean: -15.398267\n",
      "ep 3409: ep_len:611 episode reward: total was 15.550000. running mean: -15.088785\n",
      "ep 3409: ep_len:2933 episode reward: total was -10.620000. running mean: -15.044097\n",
      "epsilon:0.009992 episode_count: 51285. steps_count: 55209999.000000\n",
      "ep 3410: ep_len:668 episode reward: total was 2.470000. running mean: -14.868956\n",
      "ep 3410: ep_len:697 episode reward: total was 17.830000. running mean: -14.541966\n",
      "ep 3410: ep_len:3064 episode reward: total was -4.310000. running mean: -14.439647\n",
      "ep 3410: ep_len:664 episode reward: total was 9.360000. running mean: -14.201650\n",
      "ep 3410: ep_len:170 episode reward: total was 82.000000. running mean: -13.239634\n",
      "ep 3410: ep_len:500 episode reward: total was -7.100000. running mean: -13.178237\n",
      "ep 3410: ep_len:4140 episode reward: total was -89.480000. running mean: -13.941255\n",
      "ep 3410: ep_len:615 episode reward: total was 0.930000. running mean: -13.792542\n",
      "ep 3410: ep_len:872 episode reward: total was 76.170000. running mean: -12.892917\n",
      "ep 3410: ep_len:500 episode reward: total was 12.130000. running mean: -12.642688\n",
      "ep 3410: ep_len:63 episode reward: total was 30.000000. running mean: -12.216261\n",
      "ep 3410: ep_len:615 episode reward: total was 16.510000. running mean: -11.928998\n",
      "ep 3410: ep_len:2924 episode reward: total was -4.410000. running mean: -11.853808\n",
      "epsilon:0.009992 episode_count: 51298. steps_count: 55225491.000000\n",
      "ep 3411: ep_len:1427 episode reward: total was 7.120000. running mean: -11.664070\n",
      "ep 3411: ep_len:960 episode reward: total was 17.270000. running mean: -11.374729\n",
      "ep 3411: ep_len:2941 episode reward: total was -23.890000. running mean: -11.499882\n",
      "ep 3411: ep_len:500 episode reward: total was 8.630000. running mean: -11.298583\n",
      "ep 3411: ep_len:1424 episode reward: total was -12.960000. running mean: -11.315198\n",
      "ep 3411: ep_len:349 episode reward: total was 12.060000. running mean: -11.081446\n",
      "ep 3411: ep_len:621 episode reward: total was 14.070000. running mean: -10.829931\n",
      "ep 3411: ep_len:677 episode reward: total was 13.310000. running mean: -10.588532\n",
      "ep 3411: ep_len:658 episode reward: total was 2.370000. running mean: -10.458946\n",
      "ep 3411: ep_len:68 episode reward: total was 32.500000. running mean: -10.029357\n",
      "ep 3411: ep_len:58 episode reward: total was 26.000000. running mean: -9.669063\n",
      "ep 3411: ep_len:122 episode reward: total was 56.500000. running mean: -9.007373\n",
      "ep 3411: ep_len:628 episode reward: total was 13.600000. running mean: -8.781299\n",
      "ep 3411: ep_len:2772 episode reward: total was -10.680000. running mean: -8.800286\n",
      "epsilon:0.009992 episode_count: 51312. steps_count: 55238696.000000\n",
      "ep 3412: ep_len:920 episode reward: total was -88.090000. running mean: -9.593183\n",
      "ep 3412: ep_len:680 episode reward: total was 15.550000. running mean: -9.341751\n",
      "ep 3412: ep_len:56 episode reward: total was 25.000000. running mean: -8.998334\n",
      "ep 3412: ep_len:2950 episode reward: total was -12.250000. running mean: -9.030851\n",
      "ep 3412: ep_len:1481 episode reward: total was -3.630000. running mean: -8.976842\n",
      "ep 3412: ep_len:65 episode reward: total was 31.000000. running mean: -8.577074\n",
      "ep 3412: ep_len:78 episode reward: total was 37.500000. running mean: -8.116303\n",
      "ep 3412: ep_len:43 episode reward: total was 20.000000. running mean: -7.835140\n",
      "ep 3412: ep_len:667 episode reward: total was -5.050000. running mean: -7.807288\n",
      "ep 3412: ep_len:652 episode reward: total was 21.170000. running mean: -7.517516\n",
      "ep 3412: ep_len:866 episode reward: total was -44.030000. running mean: -7.882640\n",
      "ep 3412: ep_len:771 episode reward: total was 12.140000. running mean: -7.682414\n",
      "ep 3412: ep_len:1132 episode reward: total was -16.060000. running mean: -7.766190\n",
      "ep 3412: ep_len:160 episode reward: total was 77.000000. running mean: -6.918528\n",
      "ep 3412: ep_len:600 episode reward: total was -17.400000. running mean: -7.023343\n",
      "ep 3412: ep_len:2841 episode reward: total was -1.450000. running mean: -6.967609\n",
      "epsilon:0.009992 episode_count: 51328. steps_count: 55252658.000000\n",
      "ep 3413: ep_len:1126 episode reward: total was 6.570000. running mean: -6.832233\n",
      "ep 3413: ep_len:737 episode reward: total was -16.650000. running mean: -6.930411\n",
      "ep 3413: ep_len:56 episode reward: total was 26.500000. running mean: -6.596107\n",
      "ep 3413: ep_len:2870 episode reward: total was -30.540000. running mean: -6.835546\n",
      "ep 3413: ep_len:705 episode reward: total was -12.930000. running mean: -6.896490\n",
      "ep 3413: ep_len:49 episode reward: total was 21.500000. running mean: -6.612525\n",
      "ep 3413: ep_len:599 episode reward: total was 5.820000. running mean: -6.488200\n",
      "ep 3413: ep_len:338 episode reward: total was 15.990000. running mean: -6.263418\n",
      "ep 3413: ep_len:1223 episode reward: total was -73.790000. running mean: -6.938684\n",
      "ep 3413: ep_len:841 episode reward: total was -243.670000. running mean: -9.305997\n",
      "ep 3413: ep_len:961 episode reward: total was 18.570000. running mean: -9.027237\n",
      "ep 3413: ep_len:67 episode reward: total was 32.000000. running mean: -8.616965\n",
      "ep 3413: ep_len:92 episode reward: total was 43.000000. running mean: -8.100795\n",
      "ep 3413: ep_len:1485 episode reward: total was 12.440000. running mean: -7.895387\n",
      "ep 3413: ep_len:41 episode reward: total was 17.500000. running mean: -7.641433\n",
      "epsilon:0.009992 episode_count: 51343. steps_count: 55263848.000000\n",
      "ep 3414: ep_len:782 episode reward: total was -109.510000. running mean: -8.660119\n",
      "ep 3414: ep_len:1604 episode reward: total was -58.750000. running mean: -9.161018\n",
      "ep 3414: ep_len:3069 episode reward: total was -29.680000. running mean: -9.366208\n",
      "ep 3414: ep_len:500 episode reward: total was 16.780000. running mean: -9.104745\n",
      "ep 3414: ep_len:154 episode reward: total was 75.500000. running mean: -8.258698\n",
      "ep 3414: ep_len:83 episode reward: total was 40.000000. running mean: -7.776111\n",
      "ep 3414: ep_len:673 episode reward: total was 18.290000. running mean: -7.515450\n",
      "ep 3414: ep_len:357 episode reward: total was 25.790000. running mean: -7.182395\n",
      "ep 3414: ep_len:842 episode reward: total was -40.240000. running mean: -7.512971\n",
      "ep 3414: ep_len:785 episode reward: total was 8.190000. running mean: -7.355942\n",
      "ep 3414: ep_len:932 episode reward: total was 53.490000. running mean: -6.747482\n",
      "ep 3414: ep_len:118 episode reward: total was 54.500000. running mean: -6.135007\n",
      "ep 3414: ep_len:621 episode reward: total was -15.540000. running mean: -6.229057\n",
      "ep 3414: ep_len:2908 episode reward: total was -21.030000. running mean: -6.377067\n",
      "epsilon:0.009992 episode_count: 51357. steps_count: 55277276.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3415: ep_len:788 episode reward: total was -17.690000. running mean: -6.490196\n",
      "ep 3415: ep_len:740 episode reward: total was -2.130000. running mean: -6.446594\n",
      "ep 3415: ep_len:3040 episode reward: total was -2.560000. running mean: -6.407728\n",
      "ep 3415: ep_len:537 episode reward: total was -18.030000. running mean: -6.523951\n",
      "ep 3415: ep_len:47 episode reward: total was 22.000000. running mean: -6.238711\n",
      "ep 3415: ep_len:888 episode reward: total was -17.550000. running mean: -6.351824\n",
      "ep 3415: ep_len:342 episode reward: total was 19.180000. running mean: -6.096506\n",
      "ep 3415: ep_len:1161 episode reward: total was -47.110000. running mean: -6.506641\n",
      "ep 3415: ep_len:893 episode reward: total was 59.270000. running mean: -5.848875\n",
      "ep 3415: ep_len:605 episode reward: total was -16.340000. running mean: -5.953786\n",
      "ep 3415: ep_len:62 episode reward: total was 28.000000. running mean: -5.614248\n",
      "ep 3415: ep_len:538 episode reward: total was -15.050000. running mean: -5.708606\n",
      "ep 3415: ep_len:2904 episode reward: total was -4.180000. running mean: -5.693320\n",
      "ep 3415: ep_len:54 episode reward: total was 25.500000. running mean: -5.381386\n",
      "epsilon:0.009992 episode_count: 51371. steps_count: 55289875.000000\n",
      "ep 3416: ep_len:1409 episode reward: total was 7.060000. running mean: -5.256972\n",
      "ep 3416: ep_len:642 episode reward: total was -51.200000. running mean: -5.716403\n",
      "ep 3416: ep_len:64 episode reward: total was 30.500000. running mean: -5.354239\n",
      "ep 3416: ep_len:2955 episode reward: total was -35.500000. running mean: -5.655696\n",
      "ep 3416: ep_len:681 episode reward: total was -12.960000. running mean: -5.728739\n",
      "ep 3416: ep_len:55 episode reward: total was 26.000000. running mean: -5.411452\n",
      "ep 3416: ep_len:106 episode reward: total was 50.000000. running mean: -4.857337\n",
      "ep 3416: ep_len:67 episode reward: total was 32.000000. running mean: -4.488764\n",
      "ep 3416: ep_len:58 episode reward: total was 27.500000. running mean: -4.168876\n",
      "ep 3416: ep_len:500 episode reward: total was 39.380000. running mean: -3.733388\n",
      "ep 3416: ep_len:627 episode reward: total was 17.000000. running mean: -3.526054\n",
      "ep 3416: ep_len:1539 episode reward: total was -90.780000. running mean: -4.398593\n",
      "ep 3416: ep_len:850 episode reward: total was 61.040000. running mean: -3.744207\n",
      "ep 3416: ep_len:1511 episode reward: total was -6.450000. running mean: -3.771265\n",
      "ep 3416: ep_len:77 episode reward: total was 35.500000. running mean: -3.378553\n",
      "ep 3416: ep_len:1166 episode reward: total was -27.900000. running mean: -3.623767\n",
      "ep 3416: ep_len:2804 episode reward: total was -45.860000. running mean: -4.046129\n",
      "epsilon:0.009992 episode_count: 51388. steps_count: 55304986.000000\n",
      "ep 3417: ep_len:1485 episode reward: total was 13.730000. running mean: -3.868368\n",
      "ep 3417: ep_len:719 episode reward: total was -2.890000. running mean: -3.858584\n",
      "ep 3417: ep_len:3002 episode reward: total was -0.250000. running mean: -3.822499\n",
      "ep 3417: ep_len:4246 episode reward: total was -1304.680000. running mean: -16.831074\n",
      "ep 3417: ep_len:47 episode reward: total was 22.000000. running mean: -16.442763\n",
      "ep 3417: ep_len:99 episode reward: total was 48.000000. running mean: -15.798335\n",
      "ep 3417: ep_len:675 episode reward: total was -20.700000. running mean: -15.847352\n",
      "ep 3417: ep_len:3684 episode reward: total was -205.590000. running mean: -17.744778\n",
      "ep 3417: ep_len:500 episode reward: total was 25.630000. running mean: -17.311031\n",
      "ep 3417: ep_len:7338 episode reward: total was -656.930000. running mean: -23.707220\n",
      "ep 3417: ep_len:500 episode reward: total was -6.880000. running mean: -23.538948\n",
      "ep 3417: ep_len:1121 episode reward: total was -8.610000. running mean: -23.389659\n",
      "ep 3417: ep_len:2867 episode reward: total was -31.970000. running mean: -23.475462\n",
      "epsilon:0.009992 episode_count: 51401. steps_count: 55331269.000000\n",
      "ep 3418: ep_len:1505 episode reward: total was 18.710000. running mean: -23.053607\n",
      "ep 3418: ep_len:500 episode reward: total was 12.210000. running mean: -22.700971\n",
      "ep 3418: ep_len:56 episode reward: total was 25.000000. running mean: -22.223962\n",
      "ep 3418: ep_len:3118 episode reward: total was 10.280000. running mean: -21.898922\n",
      "ep 3418: ep_len:500 episode reward: total was 18.920000. running mean: -21.490733\n",
      "ep 3418: ep_len:53 episode reward: total was 22.000000. running mean: -21.055825\n",
      "ep 3418: ep_len:57 episode reward: total was 25.500000. running mean: -20.590267\n",
      "ep 3418: ep_len:676 episode reward: total was -1.490000. running mean: -20.399264\n",
      "ep 3418: ep_len:3581 episode reward: total was -94.270000. running mean: -21.137972\n",
      "ep 3418: ep_len:1141 episode reward: total was -34.060000. running mean: -21.267192\n",
      "ep 3418: ep_len:781 episode reward: total was 1.460000. running mean: -21.039920\n",
      "ep 3418: ep_len:1149 episode reward: total was -14.940000. running mean: -20.978921\n",
      "ep 3418: ep_len:45 episode reward: total was 21.000000. running mean: -20.559132\n",
      "ep 3418: ep_len:1188 episode reward: total was 5.230000. running mean: -20.301240\n",
      "ep 3418: ep_len:2794 episode reward: total was -29.650000. running mean: -20.394728\n",
      "epsilon:0.009992 episode_count: 51416. steps_count: 55348413.000000\n",
      "ep 3419: ep_len:1064 episode reward: total was -9.730000. running mean: -20.288081\n",
      "ep 3419: ep_len:731 episode reward: total was -26.420000. running mean: -20.349400\n",
      "ep 3419: ep_len:2982 episode reward: total was 3.350000. running mean: -20.112406\n",
      "ep 3419: ep_len:1609 episode reward: total was -116.360000. running mean: -21.074882\n",
      "ep 3419: ep_len:94 episode reward: total was 42.500000. running mean: -20.439133\n",
      "ep 3419: ep_len:1402 episode reward: total was -4.790000. running mean: -20.282642\n",
      "ep 3419: ep_len:3397 episode reward: total was -152.500000. running mean: -21.604815\n",
      "ep 3419: ep_len:540 episode reward: total was -26.970000. running mean: -21.658467\n",
      "ep 3419: ep_len:703 episode reward: total was 24.260000. running mean: -21.199283\n",
      "ep 3419: ep_len:663 episode reward: total was 7.470000. running mean: -20.912590\n",
      "ep 3419: ep_len:992 episode reward: total was 5.200000. running mean: -20.651464\n",
      "ep 3419: ep_len:2838 episode reward: total was -26.210000. running mean: -20.707049\n",
      "epsilon:0.009992 episode_count: 51428. steps_count: 55365428.000000\n",
      "ep 3420: ep_len:622 episode reward: total was 3.070000. running mean: -20.469279\n",
      "ep 3420: ep_len:731 episode reward: total was -25.900000. running mean: -20.523586\n",
      "ep 3420: ep_len:76 episode reward: total was 36.500000. running mean: -19.953350\n",
      "ep 3420: ep_len:2877 episode reward: total was -47.240000. running mean: -20.226217\n",
      "ep 3420: ep_len:655 episode reward: total was -0.360000. running mean: -20.027554\n",
      "ep 3420: ep_len:41 episode reward: total was 16.000000. running mean: -19.667279\n",
      "ep 3420: ep_len:81 episode reward: total was 39.000000. running mean: -19.080606\n",
      "ep 3420: ep_len:63 episode reward: total was 28.500000. running mean: -18.604800\n",
      "ep 3420: ep_len:1399 episode reward: total was -146.770000. running mean: -19.886452\n",
      "ep 3420: ep_len:609 episode reward: total was 6.500000. running mean: -19.622587\n",
      "ep 3420: ep_len:1564 episode reward: total was -43.020000. running mean: -19.856562\n",
      "ep 3420: ep_len:7261 episode reward: total was 59.800000. running mean: -19.059996\n",
      "ep 3420: ep_len:1087 episode reward: total was 32.470000. running mean: -18.544696\n",
      "ep 3420: ep_len:171 episode reward: total was 84.000000. running mean: -17.519249\n",
      "ep 3420: ep_len:42 episode reward: total was 19.500000. running mean: -17.149057\n",
      "ep 3420: ep_len:723 episode reward: total was -62.630000. running mean: -17.603866\n",
      "ep 3420: ep_len:2810 episode reward: total was -23.970000. running mean: -17.667527\n",
      "epsilon:0.009992 episode_count: 51445. steps_count: 55386240.000000\n",
      "ep 3421: ep_len:1008 episode reward: total was -81.240000. running mean: -18.303252\n",
      "ep 3421: ep_len:660 episode reward: total was -18.820000. running mean: -18.308420\n",
      "ep 3421: ep_len:39 episode reward: total was 16.500000. running mean: -17.960335\n",
      "ep 3421: ep_len:3071 episode reward: total was -51.630000. running mean: -18.297032\n",
      "ep 3421: ep_len:661 episode reward: total was 6.240000. running mean: -18.051662\n",
      "ep 3421: ep_len:52 episode reward: total was 24.500000. running mean: -17.626145\n",
      "ep 3421: ep_len:500 episode reward: total was -0.920000. running mean: -17.459084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3421: ep_len:318 episode reward: total was 30.050000. running mean: -16.983993\n",
      "ep 3421: ep_len:1554 episode reward: total was -148.780000. running mean: -18.301953\n",
      "ep 3421: ep_len:660 episode reward: total was 13.200000. running mean: -17.986933\n",
      "ep 3421: ep_len:659 episode reward: total was -28.910000. running mean: -18.096164\n",
      "ep 3421: ep_len:173 episode reward: total was 82.000000. running mean: -17.095202\n",
      "ep 3421: ep_len:39 episode reward: total was 16.500000. running mean: -16.759250\n",
      "ep 3421: ep_len:1156 episode reward: total was 6.600000. running mean: -16.525658\n",
      "ep 3421: ep_len:2831 episode reward: total was -85.840000. running mean: -17.218801\n",
      "epsilon:0.009992 episode_count: 51460. steps_count: 55399621.000000\n",
      "ep 3422: ep_len:941 episode reward: total was -33.230000. running mean: -17.378913\n",
      "ep 3422: ep_len:500 episode reward: total was 11.940000. running mean: -17.085724\n",
      "ep 3422: ep_len:58 episode reward: total was 27.500000. running mean: -16.639867\n",
      "ep 3422: ep_len:2984 episode reward: total was -12.220000. running mean: -16.595668\n",
      "ep 3422: ep_len:541 episode reward: total was -3.850000. running mean: -16.468211\n",
      "ep 3422: ep_len:108 episode reward: total was 51.000000. running mean: -15.793529\n",
      "ep 3422: ep_len:58 episode reward: total was 27.500000. running mean: -15.360594\n",
      "ep 3422: ep_len:644 episode reward: total was -2.710000. running mean: -15.234088\n",
      "ep 3422: ep_len:3603 episode reward: total was -16.280000. running mean: -15.244547\n",
      "ep 3422: ep_len:874 episode reward: total was -61.630000. running mean: -15.708402\n",
      "ep 3422: ep_len:732 episode reward: total was 50.760000. running mean: -15.043718\n",
      "ep 3422: ep_len:500 episode reward: total was 4.020000. running mean: -14.853081\n",
      "ep 3422: ep_len:81 episode reward: total was 37.500000. running mean: -14.329550\n",
      "ep 3422: ep_len:102 episode reward: total was 49.500000. running mean: -13.691254\n",
      "ep 3422: ep_len:38 episode reward: total was 16.000000. running mean: -13.394342\n",
      "ep 3422: ep_len:500 episode reward: total was 8.300000. running mean: -13.177398\n",
      "ep 3422: ep_len:2927 episode reward: total was -35.290000. running mean: -13.398524\n",
      "epsilon:0.009992 episode_count: 51477. steps_count: 55414812.000000\n",
      "ep 3423: ep_len:625 episode reward: total was -22.170000. running mean: -13.486239\n",
      "ep 3423: ep_len:734 episode reward: total was -6.890000. running mean: -13.420277\n",
      "ep 3423: ep_len:2989 episode reward: total was -0.080000. running mean: -13.286874\n",
      "ep 3423: ep_len:1261 episode reward: total was -24.530000. running mean: -13.399305\n",
      "ep 3423: ep_len:1023 episode reward: total was -65.670000. running mean: -13.922012\n",
      "ep 3423: ep_len:3567 episode reward: total was -14.310000. running mean: -13.925892\n",
      "ep 3423: ep_len:1265 episode reward: total was -57.700000. running mean: -14.363633\n",
      "ep 3423: ep_len:629 episode reward: total was -9.120000. running mean: -14.311197\n",
      "ep 3423: ep_len:1087 episode reward: total was -2.370000. running mean: -14.191785\n",
      "ep 3423: ep_len:1549 episode reward: total was 36.540000. running mean: -13.684467\n",
      "ep 3423: ep_len:45 episode reward: total was 21.000000. running mean: -13.337622\n",
      "epsilon:0.009992 episode_count: 51488. steps_count: 55429586.000000\n",
      "ep 3424: ep_len:982 episode reward: total was 14.780000. running mean: -13.056446\n",
      "ep 3424: ep_len:656 episode reward: total was -30.430000. running mean: -13.230182\n",
      "ep 3424: ep_len:2964 episode reward: total was -99.440000. running mean: -14.092280\n",
      "ep 3424: ep_len:1224 episode reward: total was -37.630000. running mean: -14.327657\n",
      "ep 3424: ep_len:664 episode reward: total was -32.380000. running mean: -14.508180\n",
      "ep 3424: ep_len:643 episode reward: total was 21.660000. running mean: -14.146499\n",
      "ep 3424: ep_len:1553 episode reward: total was -492.600000. running mean: -18.931034\n",
      "ep 3424: ep_len:7372 episode reward: total was 83.350000. running mean: -17.908223\n",
      "ep 3424: ep_len:500 episode reward: total was 8.570000. running mean: -17.643441\n",
      "ep 3424: ep_len:81 episode reward: total was 37.500000. running mean: -17.092007\n",
      "ep 3424: ep_len:1496 episode reward: total was 2.180000. running mean: -16.899287\n",
      "ep 3424: ep_len:2764 episode reward: total was -13.910000. running mean: -16.869394\n",
      "ep 3424: ep_len:39 episode reward: total was 18.000000. running mean: -16.520700\n",
      "epsilon:0.009992 episode_count: 51501. steps_count: 55450524.000000\n",
      "ep 3425: ep_len:684 episode reward: total was -43.830000. running mean: -16.793793\n",
      "ep 3425: ep_len:1576 episode reward: total was -35.830000. running mean: -16.984155\n",
      "ep 3425: ep_len:59 episode reward: total was 26.500000. running mean: -16.549313\n",
      "ep 3425: ep_len:3027 episode reward: total was -20.600000. running mean: -16.589820\n",
      "ep 3425: ep_len:640 episode reward: total was 6.820000. running mean: -16.355722\n",
      "ep 3425: ep_len:49 episode reward: total was 23.000000. running mean: -15.962165\n",
      "ep 3425: ep_len:156 episode reward: total was 76.500000. running mean: -15.037543\n",
      "ep 3425: ep_len:87 episode reward: total was 42.000000. running mean: -14.467168\n",
      "ep 3425: ep_len:1017 episode reward: total was -5.820000. running mean: -14.380696\n",
      "ep 3425: ep_len:3594 episode reward: total was -108.280000. running mean: -15.319689\n",
      "ep 3425: ep_len:2325 episode reward: total was -206.170000. running mean: -17.228192\n",
      "ep 3425: ep_len:795 episode reward: total was -76.860000. running mean: -17.824510\n",
      "ep 3425: ep_len:1555 episode reward: total was 31.430000. running mean: -17.331965\n",
      "ep 3425: ep_len:115 episode reward: total was 54.500000. running mean: -16.613645\n",
      "ep 3425: ep_len:48 episode reward: total was 22.500000. running mean: -16.222509\n",
      "ep 3425: ep_len:649 episode reward: total was -6.810000. running mean: -16.128384\n",
      "ep 3425: ep_len:2869 episode reward: total was -54.270000. running mean: -16.509800\n",
      "epsilon:0.009992 episode_count: 51518. steps_count: 55469769.000000\n",
      "ep 3426: ep_len:1457 episode reward: total was -5.190000. running mean: -16.396602\n",
      "ep 3426: ep_len:1631 episode reward: total was -39.400000. running mean: -16.626636\n",
      "ep 3426: ep_len:71 episode reward: total was 32.500000. running mean: -16.135370\n",
      "ep 3426: ep_len:3004 episode reward: total was -51.690000. running mean: -16.490916\n",
      "ep 3426: ep_len:535 episode reward: total was -6.940000. running mean: -16.395407\n",
      "ep 3426: ep_len:59 episode reward: total was 26.500000. running mean: -15.966453\n",
      "ep 3426: ep_len:51 episode reward: total was 22.500000. running mean: -15.581788\n",
      "ep 3426: ep_len:1059 episode reward: total was -42.100000. running mean: -15.846970\n",
      "ep 3426: ep_len:327 episode reward: total was 19.580000. running mean: -15.492701\n",
      "ep 3426: ep_len:669 episode reward: total was -20.590000. running mean: -15.543674\n",
      "ep 3426: ep_len:754 episode reward: total was 24.920000. running mean: -15.139037\n",
      "ep 3426: ep_len:500 episode reward: total was 24.960000. running mean: -14.738047\n",
      "ep 3426: ep_len:117 episode reward: total was 55.500000. running mean: -14.035666\n",
      "ep 3426: ep_len:756 episode reward: total was -13.760000. running mean: -14.032909\n",
      "ep 3426: ep_len:2863 episode reward: total was -27.550000. running mean: -14.168080\n",
      "ep 3426: ep_len:43 episode reward: total was 20.000000. running mean: -13.826400\n",
      "epsilon:0.009992 episode_count: 51534. steps_count: 55483665.000000\n",
      "ep 3427: ep_len:500 episode reward: total was 6.980000. running mean: -13.618336\n",
      "ep 3427: ep_len:689 episode reward: total was -21.120000. running mean: -13.693352\n",
      "ep 3427: ep_len:2996 episode reward: total was 6.660000. running mean: -13.489819\n",
      "ep 3427: ep_len:649 episode reward: total was -0.180000. running mean: -13.356720\n",
      "ep 3427: ep_len:52 episode reward: total was 24.500000. running mean: -12.978153\n",
      "ep 3427: ep_len:136 episode reward: total was 63.500000. running mean: -12.213372\n",
      "ep 3427: ep_len:84 episode reward: total was 40.500000. running mean: -11.686238\n",
      "ep 3427: ep_len:1176 episode reward: total was -63.230000. running mean: -12.201676\n",
      "ep 3427: ep_len:3702 episode reward: total was 1.270000. running mean: -12.066959\n",
      "ep 3427: ep_len:1529 episode reward: total was -126.870000. running mean: -13.214989\n",
      "ep 3427: ep_len:796 episode reward: total was 46.290000. running mean: -12.619939\n",
      "ep 3427: ep_len:1500 episode reward: total was 12.130000. running mean: -12.372440\n",
      "ep 3427: ep_len:91 episode reward: total was 42.500000. running mean: -11.823716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3427: ep_len:43 episode reward: total was 18.500000. running mean: -11.520478\n",
      "ep 3427: ep_len:84 episode reward: total was 40.500000. running mean: -11.000274\n",
      "ep 3427: ep_len:647 episode reward: total was -7.840000. running mean: -10.968671\n",
      "ep 3427: ep_len:2884 episode reward: total was -28.010000. running mean: -11.139084\n",
      "epsilon:0.009992 episode_count: 51551. steps_count: 55501223.000000\n",
      "ep 3428: ep_len:639 episode reward: total was 5.140000. running mean: -10.976293\n",
      "ep 3428: ep_len:500 episode reward: total was 11.510000. running mean: -10.751430\n",
      "ep 3428: ep_len:3044 episode reward: total was 2.990000. running mean: -10.614016\n",
      "ep 3428: ep_len:533 episode reward: total was 5.160000. running mean: -10.456276\n",
      "ep 3428: ep_len:123 episode reward: total was 58.500000. running mean: -9.766713\n",
      "ep 3428: ep_len:63 episode reward: total was 27.000000. running mean: -9.399046\n",
      "ep 3428: ep_len:4388 episode reward: total was -1048.100000. running mean: -19.786056\n",
      "ep 3428: ep_len:3804 episode reward: total was 14.250000. running mean: -19.445695\n",
      "ep 3428: ep_len:837 episode reward: total was -66.570000. running mean: -19.916938\n",
      "ep 3428: ep_len:7354 episode reward: total was -18.720000. running mean: -19.904969\n",
      "ep 3428: ep_len:594 episode reward: total was -26.900000. running mean: -19.974919\n",
      "ep 3428: ep_len:162 episode reward: total was 78.000000. running mean: -18.995170\n",
      "ep 3428: ep_len:723 episode reward: total was -21.590000. running mean: -19.021118\n",
      "ep 3428: ep_len:2857 episode reward: total was -37.520000. running mean: -19.206107\n",
      "epsilon:0.009992 episode_count: 51565. steps_count: 55526844.000000\n",
      "ep 3429: ep_len:1467 episode reward: total was -7.880000. running mean: -19.092846\n",
      "ep 3429: ep_len:3882 episode reward: total was -1605.460000. running mean: -34.956517\n",
      "ep 3429: ep_len:68 episode reward: total was 32.500000. running mean: -34.281952\n",
      "ep 3429: ep_len:2999 episode reward: total was -80.600000. running mean: -34.745133\n",
      "ep 3429: ep_len:1428 episode reward: total was 26.720000. running mean: -34.130481\n",
      "ep 3429: ep_len:41 episode reward: total was 19.000000. running mean: -33.599177\n",
      "ep 3429: ep_len:110 episode reward: total was 49.000000. running mean: -32.773185\n",
      "ep 3429: ep_len:43 episode reward: total was 18.500000. running mean: -32.260453\n",
      "ep 3429: ep_len:1141 episode reward: total was 0.900000. running mean: -31.928848\n",
      "ep 3429: ep_len:317 episode reward: total was 12.500000. running mean: -31.484560\n",
      "ep 3429: ep_len:617 episode reward: total was -4.040000. running mean: -31.210114\n",
      "ep 3429: ep_len:775 episode reward: total was -10.540000. running mean: -31.003413\n",
      "ep 3429: ep_len:983 episode reward: total was 62.550000. running mean: -30.067879\n",
      "ep 3429: ep_len:85 episode reward: total was 39.500000. running mean: -29.372200\n",
      "ep 3429: ep_len:37 episode reward: total was 17.000000. running mean: -28.908478\n",
      "ep 3429: ep_len:86 episode reward: total was 41.500000. running mean: -28.204394\n",
      "ep 3429: ep_len:824 episode reward: total was -1.040000. running mean: -27.932750\n",
      "ep 3429: ep_len:2926 episode reward: total was -1028.760000. running mean: -37.941022\n",
      "ep 3429: ep_len:42 episode reward: total was 19.500000. running mean: -37.366612\n",
      "epsilon:0.009992 episode_count: 51584. steps_count: 55544715.000000\n",
      "ep 3430: ep_len:804 episode reward: total was -111.310000. running mean: -38.106046\n",
      "ep 3430: ep_len:751 episode reward: total was -47.200000. running mean: -38.196985\n",
      "ep 3430: ep_len:43 episode reward: total was 18.500000. running mean: -37.630015\n",
      "ep 3430: ep_len:2915 episode reward: total was -36.410000. running mean: -37.617815\n",
      "ep 3430: ep_len:536 episode reward: total was -81.670000. running mean: -38.058337\n",
      "ep 3430: ep_len:50 episode reward: total was 23.500000. running mean: -37.442754\n",
      "ep 3430: ep_len:1059 episode reward: total was -22.590000. running mean: -37.294226\n",
      "ep 3430: ep_len:651 episode reward: total was 24.590000. running mean: -36.675384\n",
      "ep 3430: ep_len:1277 episode reward: total was -48.920000. running mean: -36.797830\n",
      "ep 3430: ep_len:720 episode reward: total was 30.040000. running mean: -36.129452\n",
      "ep 3430: ep_len:582 episode reward: total was -6.470000. running mean: -35.832857\n",
      "ep 3430: ep_len:194 episode reward: total was 10.500000. running mean: -35.369529\n",
      "ep 3430: ep_len:79 episode reward: total was 38.000000. running mean: -34.635833\n",
      "ep 3430: ep_len:1140 episode reward: total was -2.020000. running mean: -34.309675\n",
      "ep 3430: ep_len:2939 episode reward: total was -46.280000. running mean: -34.429378\n",
      "epsilon:0.009992 episode_count: 51599. steps_count: 55558455.000000\n",
      "ep 3431: ep_len:977 episode reward: total was -78.270000. running mean: -34.867785\n",
      "ep 3431: ep_len:935 episode reward: total was -19.560000. running mean: -34.714707\n",
      "ep 3431: ep_len:48 episode reward: total was 21.000000. running mean: -34.157560\n",
      "ep 3431: ep_len:2873 episode reward: total was -53.280000. running mean: -34.348784\n",
      "ep 3431: ep_len:743 episode reward: total was -39.200000. running mean: -34.397296\n",
      "ep 3431: ep_len:103 episode reward: total was 48.500000. running mean: -33.568323\n",
      "ep 3431: ep_len:1497 episode reward: total was 29.010000. running mean: -32.942540\n",
      "ep 3431: ep_len:3533 episode reward: total was -204.070000. running mean: -34.653815\n",
      "ep 3431: ep_len:568 episode reward: total was -21.760000. running mean: -34.524876\n",
      "ep 3431: ep_len:7414 episode reward: total was 37.520000. running mean: -33.804428\n",
      "ep 3431: ep_len:752 episode reward: total was 10.180000. running mean: -33.364583\n",
      "ep 3431: ep_len:194 episode reward: total was 95.500000. running mean: -32.075938\n",
      "ep 3431: ep_len:800 episode reward: total was -28.980000. running mean: -32.044978\n",
      "ep 3431: ep_len:2830 episode reward: total was -40.300000. running mean: -32.127528\n",
      "epsilon:0.009992 episode_count: 51613. steps_count: 55581722.000000\n",
      "ep 3432: ep_len:946 episode reward: total was -43.290000. running mean: -32.239153\n",
      "ep 3432: ep_len:1615 episode reward: total was -27.730000. running mean: -32.194062\n",
      "ep 3432: ep_len:3000 episode reward: total was -106.510000. running mean: -32.937221\n",
      "ep 3432: ep_len:794 episode reward: total was -33.650000. running mean: -32.944349\n",
      "ep 3432: ep_len:82 episode reward: total was 38.000000. running mean: -32.234905\n",
      "ep 3432: ep_len:46 episode reward: total was 21.500000. running mean: -31.697556\n",
      "ep 3432: ep_len:500 episode reward: total was 30.440000. running mean: -31.076181\n",
      "ep 3432: ep_len:319 episode reward: total was 11.300000. running mean: -30.652419\n",
      "ep 3432: ep_len:560 episode reward: total was -7.180000. running mean: -30.417695\n",
      "ep 3432: ep_len:640 episode reward: total was 11.260000. running mean: -30.000918\n",
      "ep 3432: ep_len:678 episode reward: total was -19.720000. running mean: -29.898109\n",
      "ep 3432: ep_len:65 episode reward: total was 29.500000. running mean: -29.304127\n",
      "ep 3432: ep_len:1423 episode reward: total was -13.250000. running mean: -29.143586\n",
      "ep 3432: ep_len:2862 episode reward: total was -46.560000. running mean: -29.317750\n",
      "epsilon:0.009992 episode_count: 51627. steps_count: 55595252.000000\n",
      "ep 3433: ep_len:696 episode reward: total was -36.120000. running mean: -29.385773\n",
      "ep 3433: ep_len:1472 episode reward: total was -284.410000. running mean: -31.936015\n",
      "ep 3433: ep_len:80 episode reward: total was 38.500000. running mean: -31.231655\n",
      "ep 3433: ep_len:99 episode reward: total was 48.000000. running mean: -30.439338\n",
      "ep 3433: ep_len:532 episode reward: total was -71.610000. running mean: -30.851045\n",
      "ep 3433: ep_len:733 episode reward: total was -7.990000. running mean: -30.622435\n",
      "ep 3433: ep_len:671 episode reward: total was 25.770000. running mean: -30.058510\n",
      "ep 3433: ep_len:1281 episode reward: total was -57.050000. running mean: -30.328425\n",
      "ep 3433: ep_len:791 episode reward: total was 38.000000. running mean: -29.645141\n",
      "ep 3433: ep_len:500 episode reward: total was 34.600000. running mean: -29.002689\n",
      "ep 3433: ep_len:176 episode reward: total was 83.500000. running mean: -27.877663\n",
      "ep 3433: ep_len:60 episode reward: total was 27.000000. running mean: -27.328886\n",
      "ep 3433: ep_len:833 episode reward: total was 15.890000. running mean: -26.896697\n",
      "ep 3433: ep_len:2793 episode reward: total was -32.590000. running mean: -26.953630\n",
      "epsilon:0.009992 episode_count: 51641. steps_count: 55605969.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3434: ep_len:1406 episode reward: total was 6.330000. running mean: -26.620794\n",
      "ep 3434: ep_len:210 episode reward: total was -6.200000. running mean: -26.416586\n",
      "ep 3434: ep_len:3019 episode reward: total was -86.980000. running mean: -27.022220\n",
      "ep 3434: ep_len:692 episode reward: total was 3.020000. running mean: -26.721798\n",
      "ep 3434: ep_len:97 episode reward: total was 47.000000. running mean: -25.984580\n",
      "ep 3434: ep_len:85 episode reward: total was 39.500000. running mean: -25.329734\n",
      "ep 3434: ep_len:50 episode reward: total was 23.500000. running mean: -24.841437\n",
      "ep 3434: ep_len:1012 episode reward: total was 2.300000. running mean: -24.570022\n",
      "ep 3434: ep_len:3672 episode reward: total was -142.850000. running mean: -25.752822\n",
      "ep 3434: ep_len:927 episode reward: total was -101.020000. running mean: -26.505494\n",
      "ep 3434: ep_len:798 episode reward: total was 41.930000. running mean: -25.821139\n",
      "ep 3434: ep_len:1504 episode reward: total was -13.370000. running mean: -25.696628\n",
      "ep 3434: ep_len:157 episode reward: total was 74.000000. running mean: -24.699661\n",
      "ep 3434: ep_len:50 episode reward: total was 20.500000. running mean: -24.247665\n",
      "ep 3434: ep_len:1240 episode reward: total was -5.780000. running mean: -24.062988\n",
      "ep 3434: ep_len:2862 episode reward: total was -79.100000. running mean: -24.613358\n",
      "epsilon:0.009992 episode_count: 51657. steps_count: 55623750.000000\n",
      "ep 3435: ep_len:701 episode reward: total was -41.610000. running mean: -24.783325\n",
      "ep 3435: ep_len:719 episode reward: total was 12.610000. running mean: -24.409391\n",
      "ep 3435: ep_len:70 episode reward: total was 33.500000. running mean: -23.830297\n",
      "ep 3435: ep_len:2986 episode reward: total was -57.530000. running mean: -24.167294\n",
      "ep 3435: ep_len:500 episode reward: total was 35.530000. running mean: -23.570322\n",
      "ep 3435: ep_len:85 episode reward: total was 41.000000. running mean: -22.924618\n",
      "ep 3435: ep_len:73 episode reward: total was 35.000000. running mean: -22.345372\n",
      "ep 3435: ep_len:992 episode reward: total was 9.940000. running mean: -22.022518\n",
      "ep 3435: ep_len:3676 episode reward: total was -50.900000. running mean: -22.311293\n",
      "ep 3435: ep_len:1295 episode reward: total was -57.920000. running mean: -22.667380\n",
      "ep 3435: ep_len:803 episode reward: total was 22.930000. running mean: -22.211406\n",
      "ep 3435: ep_len:1470 episode reward: total was -11.210000. running mean: -22.101392\n",
      "ep 3435: ep_len:62 episode reward: total was 28.000000. running mean: -21.600378\n",
      "ep 3435: ep_len:174 episode reward: total was 82.500000. running mean: -20.559375\n",
      "ep 3435: ep_len:591 episode reward: total was -10.420000. running mean: -20.457981\n",
      "ep 3435: ep_len:2845 episode reward: total was -0.670000. running mean: -20.260101\n",
      "epsilon:0.009992 episode_count: 51673. steps_count: 55640792.000000\n",
      "ep 3436: ep_len:1044 episode reward: total was -116.990000. running mean: -21.227400\n",
      "ep 3436: ep_len:772 episode reward: total was -14.670000. running mean: -21.161826\n",
      "ep 3436: ep_len:2957 episode reward: total was -22.260000. running mean: -21.172808\n",
      "ep 3436: ep_len:1248 episode reward: total was -19.360000. running mean: -21.154680\n",
      "ep 3436: ep_len:112 episode reward: total was 54.500000. running mean: -20.398133\n",
      "ep 3436: ep_len:592 episode reward: total was 40.710000. running mean: -19.787052\n",
      "ep 3436: ep_len:4048 episode reward: total was -71.550000. running mean: -20.304681\n",
      "ep 3436: ep_len:778 episode reward: total was -21.350000. running mean: -20.315134\n",
      "ep 3436: ep_len:7384 episode reward: total was -12.000000. running mean: -20.231983\n",
      "ep 3436: ep_len:1050 episode reward: total was 22.640000. running mean: -19.803263\n",
      "ep 3436: ep_len:114 episode reward: total was 55.500000. running mean: -19.050231\n",
      "ep 3436: ep_len:1097 episode reward: total was -1.320000. running mean: -18.872928\n",
      "ep 3436: ep_len:2762 episode reward: total was -20.020000. running mean: -18.884399\n",
      "epsilon:0.009992 episode_count: 51686. steps_count: 55664750.000000\n",
      "ep 3437: ep_len:1151 episode reward: total was -0.780000. running mean: -18.703355\n",
      "ep 3437: ep_len:1647 episode reward: total was -35.610000. running mean: -18.872421\n",
      "ep 3437: ep_len:2891 episode reward: total was -43.980000. running mean: -19.123497\n",
      "ep 3437: ep_len:808 episode reward: total was -52.050000. running mean: -19.452762\n",
      "ep 3437: ep_len:39 episode reward: total was 16.500000. running mean: -19.093235\n",
      "ep 3437: ep_len:129 episode reward: total was 58.500000. running mean: -18.317302\n",
      "ep 3437: ep_len:977 episode reward: total was -19.630000. running mean: -18.330429\n",
      "ep 3437: ep_len:3757 episode reward: total was -1286.080000. running mean: -31.007925\n",
      "ep 3437: ep_len:676 episode reward: total was -0.480000. running mean: -30.702646\n",
      "ep 3437: ep_len:7335 episode reward: total was 58.030000. running mean: -29.815319\n",
      "ep 3437: ep_len:1069 episode reward: total was 13.120000. running mean: -29.385966\n",
      "ep 3437: ep_len:53 episode reward: total was 25.000000. running mean: -28.842106\n",
      "ep 3437: ep_len:1101 episode reward: total was 9.740000. running mean: -28.456285\n",
      "ep 3437: ep_len:2834 episode reward: total was -51.990000. running mean: -28.691622\n",
      "epsilon:0.009992 episode_count: 51700. steps_count: 55689217.000000\n",
      "ep 3438: ep_len:1507 episode reward: total was 2.350000. running mean: -28.381206\n",
      "ep 3438: ep_len:741 episode reward: total was -17.000000. running mean: -28.267394\n",
      "ep 3438: ep_len:3050 episode reward: total was -21.130000. running mean: -28.196020\n",
      "ep 3438: ep_len:830 episode reward: total was 39.900000. running mean: -27.515060\n",
      "ep 3438: ep_len:973 episode reward: total was -2.620000. running mean: -27.266109\n",
      "ep 3438: ep_len:654 episode reward: total was 18.560000. running mean: -26.807848\n",
      "ep 3438: ep_len:655 episode reward: total was -0.280000. running mean: -26.542570\n",
      "ep 3438: ep_len:7443 episode reward: total was -94.560000. running mean: -27.222744\n",
      "ep 3438: ep_len:706 episode reward: total was -25.500000. running mean: -27.205517\n",
      "ep 3438: ep_len:142 episode reward: total was 69.500000. running mean: -26.238462\n",
      "ep 3438: ep_len:47 episode reward: total was 22.000000. running mean: -25.756077\n",
      "ep 3438: ep_len:69 episode reward: total was 33.000000. running mean: -25.168516\n",
      "ep 3438: ep_len:715 episode reward: total was -24.240000. running mean: -25.159231\n",
      "ep 3438: ep_len:2758 episode reward: total was -102.570000. running mean: -25.933339\n",
      "epsilon:0.009992 episode_count: 51714. steps_count: 55709507.000000\n",
      "ep 3439: ep_len:1408 episode reward: total was -2.530000. running mean: -25.699305\n",
      "ep 3439: ep_len:185 episode reward: total was 12.680000. running mean: -25.315512\n",
      "ep 3439: ep_len:41 episode reward: total was 17.500000. running mean: -24.887357\n",
      "ep 3439: ep_len:3043 episode reward: total was -44.930000. running mean: -25.087784\n",
      "ep 3439: ep_len:1087 episode reward: total was -12.930000. running mean: -24.966206\n",
      "ep 3439: ep_len:124 episode reward: total was 60.500000. running mean: -24.111544\n",
      "ep 3439: ep_len:500 episode reward: total was 5.910000. running mean: -23.811328\n",
      "ep 3439: ep_len:4149 episode reward: total was -461.280000. running mean: -28.186015\n",
      "ep 3439: ep_len:745 episode reward: total was -19.660000. running mean: -28.100755\n",
      "ep 3439: ep_len:835 episode reward: total was 54.550000. running mean: -27.274247\n",
      "ep 3439: ep_len:500 episode reward: total was 9.920000. running mean: -26.902305\n",
      "ep 3439: ep_len:61 episode reward: total was 29.000000. running mean: -26.343282\n",
      "ep 3439: ep_len:109 episode reward: total was 50.000000. running mean: -25.579849\n",
      "ep 3439: ep_len:3424 episode reward: total was -261.870000. running mean: -27.942750\n",
      "ep 3439: ep_len:2818 episode reward: total was -4.950000. running mean: -27.712823\n",
      "ep 3439: ep_len:62 episode reward: total was 26.500000. running mean: -27.170695\n",
      "epsilon:0.009992 episode_count: 51730. steps_count: 55728598.000000\n",
      "ep 3440: ep_len:824 episode reward: total was -41.420000. running mean: -27.313188\n",
      "ep 3440: ep_len:775 episode reward: total was -22.950000. running mean: -27.269556\n",
      "ep 3440: ep_len:51 episode reward: total was 24.000000. running mean: -26.756860\n",
      "ep 3440: ep_len:2964 episode reward: total was -3.790000. running mean: -26.527192\n",
      "ep 3440: ep_len:1213 episode reward: total was -19.530000. running mean: -26.457220\n",
      "ep 3440: ep_len:38 episode reward: total was 16.000000. running mean: -26.032648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3440: ep_len:125 episode reward: total was 61.000000. running mean: -25.162321\n",
      "ep 3440: ep_len:1144 episode reward: total was -9.940000. running mean: -25.010098\n",
      "ep 3440: ep_len:3649 episode reward: total was -9.210000. running mean: -24.852097\n",
      "ep 3440: ep_len:551 episode reward: total was 2.590000. running mean: -24.577676\n",
      "ep 3440: ep_len:844 episode reward: total was 54.670000. running mean: -23.785199\n",
      "ep 3440: ep_len:500 episode reward: total was -52.950000. running mean: -24.076847\n",
      "ep 3440: ep_len:58 episode reward: total was 27.500000. running mean: -23.561079\n",
      "ep 3440: ep_len:162 episode reward: total was 79.500000. running mean: -22.530468\n",
      "ep 3440: ep_len:54 episode reward: total was 25.500000. running mean: -22.050163\n",
      "ep 3440: ep_len:1511 episode reward: total was -195.710000. running mean: -23.786762\n",
      "ep 3440: ep_len:2832 episode reward: total was -20.360000. running mean: -23.752494\n",
      "ep 3440: ep_len:64 episode reward: total was 29.000000. running mean: -23.224969\n",
      "epsilon:0.009992 episode_count: 51748. steps_count: 55745957.000000\n",
      "ep 3441: ep_len:1489 episode reward: total was 7.400000. running mean: -22.918719\n",
      "ep 3441: ep_len:500 episode reward: total was -0.420000. running mean: -22.693732\n",
      "ep 3441: ep_len:2996 episode reward: total was -31.220000. running mean: -22.778995\n",
      "ep 3441: ep_len:1152 episode reward: total was -12.370000. running mean: -22.674905\n",
      "ep 3441: ep_len:39 episode reward: total was 18.000000. running mean: -22.268156\n",
      "ep 3441: ep_len:157 episode reward: total was 72.500000. running mean: -21.320474\n",
      "ep 3441: ep_len:74 episode reward: total was 35.500000. running mean: -20.752270\n",
      "ep 3441: ep_len:1482 episode reward: total was 11.070000. running mean: -20.434047\n",
      "ep 3441: ep_len:3912 episode reward: total was -67.730000. running mean: -20.907006\n",
      "ep 3441: ep_len:855 episode reward: total was 27.050000. running mean: -20.427436\n",
      "ep 3441: ep_len:664 episode reward: total was 39.240000. running mean: -19.830762\n",
      "ep 3441: ep_len:541 episode reward: total was 27.140000. running mean: -19.361054\n",
      "ep 3441: ep_len:102 episode reward: total was 46.500000. running mean: -18.702444\n",
      "ep 3441: ep_len:623 episode reward: total was -10.720000. running mean: -18.622619\n",
      "ep 3441: ep_len:2827 episode reward: total was 6.500000. running mean: -18.371393\n",
      "ep 3441: ep_len:65 episode reward: total was 31.000000. running mean: -17.877679\n",
      "epsilon:0.009992 episode_count: 51764. steps_count: 55763435.000000\n",
      "ep 3442: ep_len:796 episode reward: total was -9.280000. running mean: -17.791702\n",
      "ep 3442: ep_len:500 episode reward: total was 23.880000. running mean: -17.374985\n",
      "ep 3442: ep_len:2964 episode reward: total was -54.730000. running mean: -17.748536\n",
      "ep 3442: ep_len:687 episode reward: total was -24.650000. running mean: -17.817550\n",
      "ep 3442: ep_len:58 episode reward: total was 27.500000. running mean: -17.364375\n",
      "ep 3442: ep_len:623 episode reward: total was -0.120000. running mean: -17.191931\n",
      "ep 3442: ep_len:3623 episode reward: total was -45.860000. running mean: -17.478612\n",
      "ep 3442: ep_len:796 episode reward: total was -41.630000. running mean: -17.720126\n",
      "ep 3442: ep_len:7335 episode reward: total was -115.110000. running mean: -18.694024\n",
      "ep 3442: ep_len:500 episode reward: total was -2.390000. running mean: -18.530984\n",
      "ep 3442: ep_len:50 episode reward: total was 23.500000. running mean: -18.110674\n",
      "ep 3442: ep_len:1401 episode reward: total was -3.600000. running mean: -17.965567\n",
      "ep 3442: ep_len:2901 episode reward: total was -19.550000. running mean: -17.981412\n",
      "epsilon:0.009992 episode_count: 51777. steps_count: 55785669.000000\n",
      "ep 3443: ep_len:750 episode reward: total was -32.290000. running mean: -18.124498\n",
      "ep 3443: ep_len:743 episode reward: total was 2.030000. running mean: -17.922953\n",
      "ep 3443: ep_len:2908 episode reward: total was -43.640000. running mean: -18.180123\n",
      "ep 3443: ep_len:808 episode reward: total was 34.140000. running mean: -17.656922\n",
      "ep 3443: ep_len:119 episode reward: total was 55.000000. running mean: -16.930353\n",
      "ep 3443: ep_len:1394 episode reward: total was 14.440000. running mean: -16.616649\n",
      "ep 3443: ep_len:334 episode reward: total was 25.040000. running mean: -16.200083\n",
      "ep 3443: ep_len:811 episode reward: total was -43.570000. running mean: -16.473782\n",
      "ep 3443: ep_len:7239 episode reward: total was -39.020000. running mean: -16.699244\n",
      "ep 3443: ep_len:578 episode reward: total was -11.560000. running mean: -16.647852\n",
      "ep 3443: ep_len:81 episode reward: total was 36.000000. running mean: -16.121373\n",
      "ep 3443: ep_len:1127 episode reward: total was -0.010000. running mean: -15.960259\n",
      "ep 3443: ep_len:2721 episode reward: total was -3.190000. running mean: -15.832557\n",
      "ep 3443: ep_len:41 episode reward: total was 19.000000. running mean: -15.484231\n",
      "epsilon:0.009992 episode_count: 51791. steps_count: 55805323.000000\n",
      "ep 3444: ep_len:566 episode reward: total was -8.140000. running mean: -15.410789\n",
      "ep 3444: ep_len:769 episode reward: total was -8.640000. running mean: -15.343081\n",
      "ep 3444: ep_len:2981 episode reward: total was -18.910000. running mean: -15.378750\n",
      "ep 3444: ep_len:501 episode reward: total was -9.300000. running mean: -15.317963\n",
      "ep 3444: ep_len:73 episode reward: total was 35.000000. running mean: -14.814783\n",
      "ep 3444: ep_len:105 episode reward: total was 51.000000. running mean: -14.156635\n",
      "ep 3444: ep_len:641 episode reward: total was -6.100000. running mean: -14.076069\n",
      "ep 3444: ep_len:642 episode reward: total was 27.220000. running mean: -13.663108\n",
      "ep 3444: ep_len:1232 episode reward: total was -30.470000. running mean: -13.831177\n",
      "ep 3444: ep_len:634 episode reward: total was -12.010000. running mean: -13.812965\n",
      "ep 3444: ep_len:1084 episode reward: total was -14.580000. running mean: -13.820636\n",
      "ep 3444: ep_len:88 episode reward: total was 41.000000. running mean: -13.272429\n",
      "ep 3444: ep_len:786 episode reward: total was -33.070000. running mean: -13.470405\n",
      "ep 3444: ep_len:2978 episode reward: total was -34.030000. running mean: -13.676001\n",
      "ep 3444: ep_len:39 episode reward: total was 18.000000. running mean: -13.359241\n",
      "epsilon:0.009992 episode_count: 51806. steps_count: 55818442.000000\n",
      "ep 3445: ep_len:817 episode reward: total was -19.910000. running mean: -13.424749\n",
      "ep 3445: ep_len:833 episode reward: total was 23.730000. running mean: -13.053201\n",
      "ep 3445: ep_len:2961 episode reward: total was -65.020000. running mean: -13.572869\n",
      "ep 3445: ep_len:865 episode reward: total was 14.130000. running mean: -13.295840\n",
      "ep 3445: ep_len:64 episode reward: total was 30.500000. running mean: -12.857882\n",
      "ep 3445: ep_len:126 episode reward: total was 55.500000. running mean: -12.174303\n",
      "ep 3445: ep_len:77 episode reward: total was 37.000000. running mean: -11.682560\n",
      "ep 3445: ep_len:1447 episode reward: total was 12.250000. running mean: -11.443235\n",
      "ep 3445: ep_len:3687 episode reward: total was -31.600000. running mean: -11.644802\n",
      "ep 3445: ep_len:923 episode reward: total was -43.460000. running mean: -11.962954\n",
      "ep 3445: ep_len:850 episode reward: total was 54.310000. running mean: -11.300225\n",
      "ep 3445: ep_len:657 episode reward: total was -26.110000. running mean: -11.448322\n",
      "ep 3445: ep_len:60 episode reward: total was 27.000000. running mean: -11.063839\n",
      "ep 3445: ep_len:64 episode reward: total was 30.500000. running mean: -10.648201\n",
      "ep 3445: ep_len:1175 episode reward: total was -0.560000. running mean: -10.547319\n",
      "ep 3445: ep_len:2756 episode reward: total was -1.900000. running mean: -10.460846\n",
      "epsilon:0.009992 episode_count: 51822. steps_count: 55835804.000000\n",
      "ep 3446: ep_len:647 episode reward: total was -78.120000. running mean: -11.137437\n",
      "ep 3446: ep_len:3894 episode reward: total was -630.470000. running mean: -17.330763\n",
      "ep 3446: ep_len:2939 episode reward: total was 27.160000. running mean: -16.885855\n",
      "ep 3446: ep_len:699 episode reward: total was 16.930000. running mean: -16.547697\n",
      "ep 3446: ep_len:35 episode reward: total was 16.000000. running mean: -16.222220\n",
      "ep 3446: ep_len:71 episode reward: total was 32.500000. running mean: -15.734997\n",
      "ep 3446: ep_len:1419 episode reward: total was -174.510000. running mean: -17.322747\n",
      "ep 3446: ep_len:333 episode reward: total was 23.100000. running mean: -16.918520\n",
      "ep 3446: ep_len:532 episode reward: total was -1.920000. running mean: -16.768535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3446: ep_len:752 episode reward: total was 32.080000. running mean: -16.280049\n",
      "ep 3446: ep_len:687 episode reward: total was 15.110000. running mean: -15.966149\n",
      "ep 3446: ep_len:57 episode reward: total was 27.000000. running mean: -15.536487\n",
      "ep 3446: ep_len:183 episode reward: total was 88.500000. running mean: -14.496123\n",
      "ep 3446: ep_len:695 episode reward: total was 7.160000. running mean: -14.279561\n",
      "ep 3446: ep_len:2779 episode reward: total was -9.590000. running mean: -14.232666\n",
      "ep 3446: ep_len:62 episode reward: total was 29.500000. running mean: -13.795339\n",
      "epsilon:0.009992 episode_count: 51838. steps_count: 55851588.000000\n",
      "ep 3447: ep_len:1112 episode reward: total was 5.900000. running mean: -13.598386\n",
      "ep 3447: ep_len:753 episode reward: total was -11.830000. running mean: -13.580702\n",
      "ep 3447: ep_len:3004 episode reward: total was -27.690000. running mean: -13.721795\n",
      "ep 3447: ep_len:500 episode reward: total was -8.310000. running mean: -13.667677\n",
      "ep 3447: ep_len:43 episode reward: total was 18.500000. running mean: -13.346000\n",
      "ep 3447: ep_len:910 episode reward: total was 54.770000. running mean: -12.664840\n",
      "ep 3447: ep_len:3883 episode reward: total was 6.100000. running mean: -12.477192\n",
      "ep 3447: ep_len:1244 episode reward: total was -41.380000. running mean: -12.766220\n",
      "ep 3447: ep_len:671 episode reward: total was 29.970000. running mean: -12.338858\n",
      "ep 3447: ep_len:577 episode reward: total was 39.910000. running mean: -11.816369\n",
      "ep 3447: ep_len:181 episode reward: total was 86.000000. running mean: -10.838205\n",
      "ep 3447: ep_len:806 episode reward: total was -7.520000. running mean: -10.805023\n",
      "ep 3447: ep_len:2847 episode reward: total was -20.970000. running mean: -10.906673\n",
      "epsilon:0.009992 episode_count: 51851. steps_count: 55868119.000000\n",
      "ep 3448: ep_len:692 episode reward: total was -52.840000. running mean: -11.326006\n",
      "ep 3448: ep_len:650 episode reward: total was -37.100000. running mean: -11.583746\n",
      "ep 3448: ep_len:2923 episode reward: total was -60.520000. running mean: -12.073109\n",
      "ep 3448: ep_len:805 episode reward: total was 6.370000. running mean: -11.888678\n",
      "ep 3448: ep_len:833 episode reward: total was 23.740000. running mean: -11.532391\n",
      "ep 3448: ep_len:3734 episode reward: total was -39.310000. running mean: -11.810167\n",
      "ep 3448: ep_len:1646 episode reward: total was -129.200000. running mean: -12.984065\n",
      "ep 3448: ep_len:790 episode reward: total was 40.470000. running mean: -12.449525\n",
      "ep 3448: ep_len:995 episode reward: total was 3.140000. running mean: -12.293629\n",
      "ep 3448: ep_len:35 episode reward: total was 16.000000. running mean: -12.010693\n",
      "ep 3448: ep_len:133 episode reward: total was 63.500000. running mean: -11.255586\n",
      "ep 3448: ep_len:739 episode reward: total was -18.430000. running mean: -11.327330\n",
      "ep 3448: ep_len:2822 episode reward: total was -29.820000. running mean: -11.512257\n",
      "epsilon:0.009992 episode_count: 51864. steps_count: 55884916.000000\n",
      "ep 3449: ep_len:1068 episode reward: total was -14.530000. running mean: -11.542434\n",
      "ep 3449: ep_len:992 episode reward: total was 5.290000. running mean: -11.374110\n",
      "ep 3449: ep_len:3052 episode reward: total was -21.200000. running mean: -11.472369\n",
      "ep 3449: ep_len:1732 episode reward: total was -49.970000. running mean: -11.857345\n",
      "ep 3449: ep_len:123 episode reward: total was 58.500000. running mean: -11.153772\n",
      "ep 3449: ep_len:649 episode reward: total was 2.280000. running mean: -11.019434\n",
      "ep 3449: ep_len:3985 episode reward: total was -3327.510000. running mean: -44.184340\n",
      "ep 3449: ep_len:965 episode reward: total was -23.850000. running mean: -43.980996\n",
      "ep 3449: ep_len:742 episode reward: total was 40.790000. running mean: -43.133286\n",
      "ep 3449: ep_len:500 episode reward: total was 27.410000. running mean: -42.427854\n",
      "ep 3449: ep_len:121 episode reward: total was 59.000000. running mean: -41.413575\n",
      "ep 3449: ep_len:105 episode reward: total was 51.000000. running mean: -40.489439\n",
      "ep 3449: ep_len:1096 episode reward: total was 0.170000. running mean: -40.082845\n",
      "ep 3449: ep_len:2813 episode reward: total was 9.630000. running mean: -39.585716\n",
      "ep 3449: ep_len:40 episode reward: total was 18.500000. running mean: -39.004859\n",
      "epsilon:0.009992 episode_count: 51879. steps_count: 55902899.000000\n",
      "ep 3450: ep_len:1428 episode reward: total was 6.610000. running mean: -38.548711\n",
      "ep 3450: ep_len:1578 episode reward: total was -18.180000. running mean: -38.345024\n",
      "ep 3450: ep_len:2955 episode reward: total was -44.930000. running mean: -38.410873\n",
      "ep 3450: ep_len:653 episode reward: total was 9.760000. running mean: -37.929165\n",
      "ep 3450: ep_len:99 episode reward: total was 48.000000. running mean: -37.069873\n",
      "ep 3450: ep_len:68 episode reward: total was 32.500000. running mean: -36.374174\n",
      "ep 3450: ep_len:1363 episode reward: total was 32.870000. running mean: -35.681732\n",
      "ep 3450: ep_len:327 episode reward: total was 26.500000. running mean: -35.059915\n",
      "ep 3450: ep_len:882 episode reward: total was -25.690000. running mean: -34.966216\n",
      "ep 3450: ep_len:905 episode reward: total was 71.670000. running mean: -33.899854\n",
      "ep 3450: ep_len:1467 episode reward: total was -8.920000. running mean: -33.650055\n",
      "ep 3450: ep_len:180 episode reward: total was 85.500000. running mean: -32.458555\n",
      "ep 3450: ep_len:1125 episode reward: total was -31.340000. running mean: -32.447369\n",
      "ep 3450: ep_len:2779 episode reward: total was -2.040000. running mean: -32.143296\n",
      "ep 3450: ep_len:75 episode reward: total was 33.000000. running mean: -31.491863\n",
      "epsilon:0.009992 episode_count: 51894. steps_count: 55918783.000000\n",
      "ep 3451: ep_len:1116 episode reward: total was -9.210000. running mean: -31.269044\n",
      "ep 3451: ep_len:500 episode reward: total was -29.200000. running mean: -31.248353\n",
      "ep 3451: ep_len:3035 episode reward: total was -44.630000. running mean: -31.382170\n",
      "ep 3451: ep_len:500 episode reward: total was -57.050000. running mean: -31.638848\n",
      "ep 3451: ep_len:49 episode reward: total was 23.000000. running mean: -31.092460\n",
      "ep 3451: ep_len:500 episode reward: total was 4.880000. running mean: -30.732735\n",
      "ep 3451: ep_len:3788 episode reward: total was -5.780000. running mean: -30.483208\n",
      "ep 3451: ep_len:582 episode reward: total was 3.240000. running mean: -30.145976\n",
      "ep 3451: ep_len:783 episode reward: total was 7.570000. running mean: -29.768816\n",
      "ep 3451: ep_len:724 episode reward: total was 3.310000. running mean: -29.438028\n",
      "ep 3451: ep_len:71 episode reward: total was 32.500000. running mean: -28.818648\n",
      "ep 3451: ep_len:1464 episode reward: total was -2.760000. running mean: -28.558061\n",
      "ep 3451: ep_len:45 episode reward: total was 21.000000. running mean: -28.062480\n",
      "epsilon:0.009992 episode_count: 51907. steps_count: 55931940.000000\n",
      "ep 3452: ep_len:1116 episode reward: total was -108.040000. running mean: -28.862256\n",
      "ep 3452: ep_len:738 episode reward: total was -0.870000. running mean: -28.582333\n",
      "ep 3452: ep_len:2864 episode reward: total was -19.120000. running mean: -28.487710\n",
      "ep 3452: ep_len:785 episode reward: total was 18.330000. running mean: -28.019533\n",
      "ep 3452: ep_len:74 episode reward: total was 35.500000. running mean: -27.384337\n",
      "ep 3452: ep_len:1430 episode reward: total was -193.470000. running mean: -29.045194\n",
      "ep 3452: ep_len:334 episode reward: total was 12.150000. running mean: -28.633242\n",
      "ep 3452: ep_len:872 episode reward: total was 24.560000. running mean: -28.101310\n",
      "ep 3452: ep_len:675 episode reward: total was 39.350000. running mean: -27.426797\n",
      "ep 3452: ep_len:679 episode reward: total was 28.120000. running mean: -26.871329\n",
      "ep 3452: ep_len:63 episode reward: total was 28.500000. running mean: -26.317615\n",
      "ep 3452: ep_len:138 episode reward: total was 66.000000. running mean: -25.394439\n",
      "ep 3452: ep_len:54 episode reward: total was 25.500000. running mean: -24.885495\n",
      "ep 3452: ep_len:808 episode reward: total was 2.510000. running mean: -24.611540\n",
      "ep 3452: ep_len:2883 episode reward: total was -29.030000. running mean: -24.655724\n",
      "ep 3452: ep_len:56 episode reward: total was 26.500000. running mean: -24.144167\n",
      "epsilon:0.009992 episode_count: 51923. steps_count: 55945509.000000\n",
      "ep 3453: ep_len:1142 episode reward: total was -16.110000. running mean: -24.063825\n",
      "ep 3453: ep_len:765 episode reward: total was -0.320000. running mean: -23.826387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3453: ep_len:2909 episode reward: total was -66.910000. running mean: -24.257223\n",
      "ep 3453: ep_len:584 episode reward: total was -17.500000. running mean: -24.189651\n",
      "ep 3453: ep_len:115 episode reward: total was 56.000000. running mean: -23.387755\n",
      "ep 3453: ep_len:108 episode reward: total was 49.500000. running mean: -22.658877\n",
      "ep 3453: ep_len:1001 episode reward: total was 2.810000. running mean: -22.404188\n",
      "ep 3453: ep_len:341 episode reward: total was 15.340000. running mean: -22.026746\n",
      "ep 3453: ep_len:1620 episode reward: total was -97.100000. running mean: -22.777479\n",
      "ep 3453: ep_len:823 episode reward: total was 46.500000. running mean: -22.084704\n",
      "ep 3453: ep_len:1372 episode reward: total was -130.140000. running mean: -23.165257\n",
      "ep 3453: ep_len:46 episode reward: total was 21.500000. running mean: -22.718605\n",
      "ep 3453: ep_len:1479 episode reward: total was 0.660000. running mean: -22.484818\n",
      "ep 3453: ep_len:2810 episode reward: total was 0.910000. running mean: -22.250870\n",
      "ep 3453: ep_len:36 episode reward: total was 15.000000. running mean: -21.878362\n",
      "epsilon:0.009992 episode_count: 51938. steps_count: 55960660.000000\n",
      "ep 3454: ep_len:1158 episode reward: total was 1.600000. running mean: -21.643578\n",
      "ep 3454: ep_len:1507 episode reward: total was -29.570000. running mean: -21.722842\n",
      "ep 3454: ep_len:46 episode reward: total was 21.500000. running mean: -21.290614\n",
      "ep 3454: ep_len:3025 episode reward: total was -20.970000. running mean: -21.287408\n",
      "ep 3454: ep_len:500 episode reward: total was 0.460000. running mean: -21.069934\n",
      "ep 3454: ep_len:64 episode reward: total was 27.500000. running mean: -20.584234\n",
      "ep 3454: ep_len:120 episode reward: total was 57.000000. running mean: -19.808392\n",
      "ep 3454: ep_len:45 episode reward: total was 19.500000. running mean: -19.415308\n",
      "ep 3454: ep_len:865 episode reward: total was 26.110000. running mean: -18.960055\n",
      "ep 3454: ep_len:325 episode reward: total was 9.460000. running mean: -18.675854\n",
      "ep 3454: ep_len:576 episode reward: total was -53.880000. running mean: -19.027896\n",
      "ep 3454: ep_len:641 episode reward: total was -17.510000. running mean: -19.012717\n",
      "ep 3454: ep_len:500 episode reward: total was 3.500000. running mean: -18.787590\n",
      "ep 3454: ep_len:958 episode reward: total was -82.260000. running mean: -19.422314\n",
      "ep 3454: ep_len:2807 episode reward: total was -204.920000. running mean: -21.277291\n",
      "epsilon:0.009992 episode_count: 51953. steps_count: 55973797.000000\n",
      "ep 3455: ep_len:1095 episode reward: total was 2.700000. running mean: -21.037518\n",
      "ep 3455: ep_len:964 episode reward: total was -0.570000. running mean: -20.832843\n",
      "ep 3455: ep_len:2946 episode reward: total was -118.140000. running mean: -21.805914\n",
      "ep 3455: ep_len:637 episode reward: total was -1.140000. running mean: -21.599255\n",
      "ep 3455: ep_len:74 episode reward: total was 34.000000. running mean: -21.043262\n",
      "ep 3455: ep_len:500 episode reward: total was 16.050000. running mean: -20.672330\n",
      "ep 3455: ep_len:3885 episode reward: total was -102.320000. running mean: -21.488807\n",
      "ep 3455: ep_len:1240 episode reward: total was -70.070000. running mean: -21.974618\n",
      "ep 3455: ep_len:877 episode reward: total was 51.830000. running mean: -21.236572\n",
      "ep 3455: ep_len:500 episode reward: total was 9.370000. running mean: -20.930507\n",
      "ep 3455: ep_len:80 episode reward: total was 38.500000. running mean: -20.336201\n",
      "ep 3455: ep_len:57 episode reward: total was 27.000000. running mean: -19.862839\n",
      "ep 3455: ep_len:768 episode reward: total was -9.660000. running mean: -19.760811\n",
      "ep 3455: ep_len:2802 episode reward: total was 19.410000. running mean: -19.369103\n",
      "epsilon:0.009992 episode_count: 51967. steps_count: 55990222.000000\n",
      "ep 3456: ep_len:1394 episode reward: total was 0.730000. running mean: -19.168112\n",
      "ep 3456: ep_len:664 episode reward: total was -36.960000. running mean: -19.346031\n",
      "ep 3456: ep_len:3037 episode reward: total was -20.930000. running mean: -19.361870\n",
      "ep 3456: ep_len:1191 episode reward: total was -37.750000. running mean: -19.545752\n",
      "ep 3456: ep_len:105 episode reward: total was 49.500000. running mean: -18.855294\n",
      "ep 3456: ep_len:592 episode reward: total was 6.720000. running mean: -18.599541\n",
      "ep 3456: ep_len:627 episode reward: total was 4.690000. running mean: -18.366646\n",
      "ep 3456: ep_len:1624 episode reward: total was -108.020000. running mean: -19.263179\n",
      "ep 3456: ep_len:781 episode reward: total was 32.390000. running mean: -18.746648\n",
      "ep 3456: ep_len:500 episode reward: total was 4.880000. running mean: -18.510381\n",
      "ep 3456: ep_len:87 episode reward: total was 40.500000. running mean: -17.920277\n",
      "ep 3456: ep_len:150 episode reward: total was 72.000000. running mean: -17.021075\n",
      "ep 3456: ep_len:35 episode reward: total was 16.000000. running mean: -16.690864\n",
      "ep 3456: ep_len:104 episode reward: total was 47.500000. running mean: -16.048955\n",
      "ep 3456: ep_len:500 episode reward: total was 30.560000. running mean: -15.582866\n",
      "ep 3456: ep_len:2899 episode reward: total was 14.870000. running mean: -15.278337\n",
      "ep 3456: ep_len:42 episode reward: total was 19.500000. running mean: -14.930554\n",
      "epsilon:0.009992 episode_count: 51984. steps_count: 56004554.000000\n",
      "ep 3457: ep_len:1439 episode reward: total was 20.060000. running mean: -14.580648\n",
      "ep 3457: ep_len:1631 episode reward: total was -42.190000. running mean: -14.856742\n",
      "ep 3457: ep_len:3005 episode reward: total was -6.520000. running mean: -14.773374\n",
      "ep 3457: ep_len:1246 episode reward: total was -18.830000. running mean: -14.813940\n",
      "ep 3457: ep_len:51 episode reward: total was 24.000000. running mean: -14.425801\n",
      "ep 3457: ep_len:146 episode reward: total was 71.500000. running mean: -13.566543\n",
      "ep 3457: ep_len:86 episode reward: total was 40.000000. running mean: -13.030878\n",
      "ep 3457: ep_len:50 episode reward: total was 23.500000. running mean: -12.665569\n",
      "ep 3457: ep_len:1421 episode reward: total was 7.760000. running mean: -12.461313\n",
      "ep 3457: ep_len:4032 episode reward: total was -245.300000. running mean: -14.789700\n",
      "ep 3457: ep_len:525 episode reward: total was 5.080000. running mean: -14.591003\n",
      "ep 3457: ep_len:713 episode reward: total was 50.570000. running mean: -13.939393\n",
      "ep 3457: ep_len:639 episode reward: total was 11.360000. running mean: -13.686399\n",
      "ep 3457: ep_len:64 episode reward: total was 30.500000. running mean: -13.244535\n",
      "ep 3457: ep_len:78 episode reward: total was 36.000000. running mean: -12.752090\n",
      "ep 3457: ep_len:1130 episode reward: total was 9.700000. running mean: -12.527569\n",
      "ep 3457: ep_len:2854 episode reward: total was -1.930000. running mean: -12.421593\n",
      "epsilon:0.009992 episode_count: 52001. steps_count: 56023664.000000\n",
      "ep 3458: ep_len:840 episode reward: total was 16.190000. running mean: -12.135477\n",
      "ep 3458: ep_len:727 episode reward: total was -28.450000. running mean: -12.298622\n",
      "ep 3458: ep_len:2951 episode reward: total was -35.020000. running mean: -12.525836\n",
      "ep 3458: ep_len:841 episode reward: total was 23.910000. running mean: -12.161478\n",
      "ep 3458: ep_len:50 episode reward: total was 23.500000. running mean: -11.804863\n",
      "ep 3458: ep_len:43 episode reward: total was 20.000000. running mean: -11.486814\n",
      "ep 3458: ep_len:1144 episode reward: total was 6.040000. running mean: -11.311546\n",
      "ep 3458: ep_len:4094 episode reward: total was -94.440000. running mean: -12.142831\n",
      "ep 3458: ep_len:528 episode reward: total was -12.060000. running mean: -12.142003\n",
      "ep 3458: ep_len:790 episode reward: total was -87.060000. running mean: -12.891182\n",
      "ep 3458: ep_len:562 episode reward: total was -14.750000. running mean: -12.909771\n",
      "ep 3458: ep_len:94 episode reward: total was 45.500000. running mean: -12.325673\n",
      "ep 3458: ep_len:174 episode reward: total was 85.010000. running mean: -11.352316\n",
      "ep 3458: ep_len:120 episode reward: total was 57.000000. running mean: -10.668793\n",
      "ep 3458: ep_len:604 episode reward: total was -16.810000. running mean: -10.730205\n",
      "ep 3458: ep_len:2825 episode reward: total was -27.440000. running mean: -10.897303\n",
      "ep 3458: ep_len:48 episode reward: total was 21.000000. running mean: -10.578330\n",
      "epsilon:0.009992 episode_count: 52018. steps_count: 56040099.000000\n",
      "ep 3459: ep_len:970 episode reward: total was -77.550000. running mean: -11.248047\n",
      "ep 3459: ep_len:758 episode reward: total was -45.800000. running mean: -11.593566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3459: ep_len:2977 episode reward: total was -47.620000. running mean: -11.953831\n",
      "ep 3459: ep_len:748 episode reward: total was -22.590000. running mean: -12.060192\n",
      "ep 3459: ep_len:83 episode reward: total was 38.500000. running mean: -11.554590\n",
      "ep 3459: ep_len:857 episode reward: total was 33.290000. running mean: -11.106144\n",
      "ep 3459: ep_len:3938 episode reward: total was -46.260000. running mean: -11.457683\n",
      "ep 3459: ep_len:705 episode reward: total was 4.170000. running mean: -11.301406\n",
      "ep 3459: ep_len:866 episode reward: total was 57.280000. running mean: -10.615592\n",
      "ep 3459: ep_len:748 episode reward: total was -1.620000. running mean: -10.525636\n",
      "ep 3459: ep_len:1041 episode reward: total was 27.880000. running mean: -10.141580\n",
      "ep 3459: ep_len:2816 episode reward: total was -3.350000. running mean: -10.073664\n",
      "epsilon:0.009992 episode_count: 52030. steps_count: 56056606.000000\n",
      "ep 3460: ep_len:1131 episode reward: total was 26.640000. running mean: -9.706527\n",
      "ep 3460: ep_len:188 episode reward: total was 9.800000. running mean: -9.511462\n",
      "ep 3460: ep_len:73 episode reward: total was 32.000000. running mean: -9.096348\n",
      "ep 3460: ep_len:2953 episode reward: total was -61.940000. running mean: -9.624784\n",
      "ep 3460: ep_len:696 episode reward: total was -11.390000. running mean: -9.642436\n",
      "ep 3460: ep_len:54 episode reward: total was 25.500000. running mean: -9.291012\n",
      "ep 3460: ep_len:66 episode reward: total was 31.500000. running mean: -8.883102\n",
      "ep 3460: ep_len:56 episode reward: total was 26.500000. running mean: -8.529271\n",
      "ep 3460: ep_len:1634 episode reward: total was -400.930000. running mean: -12.453278\n",
      "ep 3460: ep_len:636 episode reward: total was 28.790000. running mean: -12.040845\n",
      "ep 3460: ep_len:917 episode reward: total was -39.480000. running mean: -12.315237\n",
      "ep 3460: ep_len:820 episode reward: total was 6.390000. running mean: -12.128184\n",
      "ep 3460: ep_len:1445 episode reward: total was 24.350000. running mean: -11.763403\n",
      "ep 3460: ep_len:809 episode reward: total was 10.190000. running mean: -11.543869\n",
      "ep 3460: ep_len:2796 episode reward: total was -7.650000. running mean: -11.504930\n",
      "epsilon:0.009992 episode_count: 52045. steps_count: 56070880.000000\n",
      "ep 3461: ep_len:1132 episode reward: total was 11.430000. running mean: -11.275581\n",
      "ep 3461: ep_len:630 episode reward: total was -148.250000. running mean: -12.645325\n",
      "ep 3461: ep_len:71 episode reward: total was 31.000000. running mean: -12.208872\n",
      "ep 3461: ep_len:2956 episode reward: total was -36.960000. running mean: -12.456383\n",
      "ep 3461: ep_len:884 episode reward: total was 13.840000. running mean: -12.193419\n",
      "ep 3461: ep_len:861 episode reward: total was 15.780000. running mean: -11.913685\n",
      "ep 3461: ep_len:355 episode reward: total was 25.250000. running mean: -11.542048\n",
      "ep 3461: ep_len:1610 episode reward: total was -70.600000. running mean: -12.132627\n",
      "ep 3461: ep_len:806 episode reward: total was 29.180000. running mean: -11.719501\n",
      "ep 3461: ep_len:899 episode reward: total was 49.480000. running mean: -11.107506\n",
      "ep 3461: ep_len:49 episode reward: total was 21.500000. running mean: -10.781431\n",
      "ep 3461: ep_len:119 episode reward: total was 58.000000. running mean: -10.093617\n",
      "ep 3461: ep_len:1540 episode reward: total was 15.630000. running mean: -9.836381\n",
      "ep 3461: ep_len:2880 episode reward: total was 4.950000. running mean: -9.688517\n",
      "epsilon:0.009992 episode_count: 52059. steps_count: 56085672.000000\n",
      "ep 3462: ep_len:622 episode reward: total was 12.080000. running mean: -9.470832\n",
      "ep 3462: ep_len:753 episode reward: total was -34.160000. running mean: -9.717723\n",
      "ep 3462: ep_len:78 episode reward: total was 36.000000. running mean: -9.260546\n",
      "ep 3462: ep_len:2825 episode reward: total was -34.420000. running mean: -9.512141\n",
      "ep 3462: ep_len:784 episode reward: total was 36.870000. running mean: -9.048319\n",
      "ep 3462: ep_len:140 episode reward: total was 67.000000. running mean: -8.287836\n",
      "ep 3462: ep_len:83 episode reward: total was 40.000000. running mean: -7.804958\n",
      "ep 3462: ep_len:847 episode reward: total was 27.430000. running mean: -7.452608\n",
      "ep 3462: ep_len:636 episode reward: total was 12.440000. running mean: -7.253682\n",
      "ep 3462: ep_len:1533 episode reward: total was 1.020000. running mean: -7.170945\n",
      "ep 3462: ep_len:776 episode reward: total was 29.860000. running mean: -6.800636\n",
      "ep 3462: ep_len:886 episode reward: total was 21.430000. running mean: -6.518329\n",
      "ep 3462: ep_len:35 episode reward: total was 16.000000. running mean: -6.293146\n",
      "ep 3462: ep_len:2969 episode reward: total was -341.000000. running mean: -9.640215\n",
      "ep 3462: ep_len:47 episode reward: total was 22.000000. running mean: -9.323812\n",
      "epsilon:0.009992 episode_count: 52074. steps_count: 56098686.000000\n",
      "ep 3463: ep_len:939 episode reward: total was -116.600000. running mean: -10.396574\n",
      "ep 3463: ep_len:500 episode reward: total was 4.870000. running mean: -10.243909\n",
      "ep 3463: ep_len:2943 episode reward: total was -15.110000. running mean: -10.292570\n",
      "ep 3463: ep_len:630 episode reward: total was 1.330000. running mean: -10.176344\n",
      "ep 3463: ep_len:130 episode reward: total was 63.500000. running mean: -9.439580\n",
      "ep 3463: ep_len:52 episode reward: total was 24.500000. running mean: -9.100185\n",
      "ep 3463: ep_len:500 episode reward: total was 22.010000. running mean: -8.789083\n",
      "ep 3463: ep_len:678 episode reward: total was 22.780000. running mean: -8.473392\n",
      "ep 3463: ep_len:561 episode reward: total was -6.680000. running mean: -8.455458\n",
      "ep 3463: ep_len:722 episode reward: total was 58.870000. running mean: -7.782203\n",
      "ep 3463: ep_len:1074 episode reward: total was -5.590000. running mean: -7.760281\n",
      "ep 3463: ep_len:69 episode reward: total was 31.500000. running mean: -7.367679\n",
      "ep 3463: ep_len:933 episode reward: total was 17.920000. running mean: -7.114802\n",
      "ep 3463: ep_len:2803 episode reward: total was 7.730000. running mean: -6.966354\n",
      "epsilon:0.009992 episode_count: 52088. steps_count: 56111220.000000\n",
      "ep 3464: ep_len:1102 episode reward: total was -12.380000. running mean: -7.020490\n",
      "ep 3464: ep_len:688 episode reward: total was -34.150000. running mean: -7.291785\n",
      "ep 3464: ep_len:2859 episode reward: total was -32.670000. running mean: -7.545567\n",
      "ep 3464: ep_len:1637 episode reward: total was -33.260000. running mean: -7.802712\n",
      "ep 3464: ep_len:180 episode reward: total was 82.500000. running mean: -6.899685\n",
      "ep 3464: ep_len:87 episode reward: total was 40.500000. running mean: -6.425688\n",
      "ep 3464: ep_len:674 episode reward: total was -4.510000. running mean: -6.406531\n",
      "ep 3464: ep_len:359 episode reward: total was 20.240000. running mean: -6.140066\n",
      "ep 3464: ep_len:602 episode reward: total was -40.820000. running mean: -6.486865\n",
      "ep 3464: ep_len:747 episode reward: total was 4.580000. running mean: -6.376196\n",
      "ep 3464: ep_len:650 episode reward: total was 15.630000. running mean: -6.156134\n",
      "ep 3464: ep_len:128 episode reward: total was 58.000000. running mean: -5.514573\n",
      "ep 3464: ep_len:1146 episode reward: total was 0.180000. running mean: -5.457627\n",
      "ep 3464: ep_len:2877 episode reward: total was 9.540000. running mean: -5.307651\n",
      "epsilon:0.009992 episode_count: 52102. steps_count: 56124956.000000\n",
      "ep 3465: ep_len:715 episode reward: total was -17.670000. running mean: -5.431275\n",
      "ep 3465: ep_len:196 episode reward: total was 1.430000. running mean: -5.362662\n",
      "ep 3465: ep_len:2968 episode reward: total was 3.510000. running mean: -5.273935\n",
      "ep 3465: ep_len:644 episode reward: total was 8.200000. running mean: -5.139196\n",
      "ep 3465: ep_len:128 episode reward: total was 61.000000. running mean: -4.477804\n",
      "ep 3465: ep_len:72 episode reward: total was 33.000000. running mean: -4.103026\n",
      "ep 3465: ep_len:649 episode reward: total was -2.840000. running mean: -4.090396\n",
      "ep 3465: ep_len:330 episode reward: total was 25.060000. running mean: -3.798892\n",
      "ep 3465: ep_len:671 episode reward: total was -3.590000. running mean: -3.796803\n",
      "ep 3465: ep_len:7249 episode reward: total was -11.510000. running mean: -3.873935\n",
      "ep 3465: ep_len:526 episode reward: total was 38.720000. running mean: -3.447995\n",
      "ep 3465: ep_len:124 episode reward: total was 61.510000. running mean: -2.798415\n",
      "ep 3465: ep_len:1081 episode reward: total was 3.880000. running mean: -2.731631\n",
      "ep 3465: ep_len:2852 episode reward: total was 2.370000. running mean: -2.680615\n",
      "epsilon:0.009992 episode_count: 52116. steps_count: 56143161.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3466: ep_len:812 episode reward: total was -41.540000. running mean: -3.069209\n",
      "ep 3466: ep_len:1655 episode reward: total was -15.970000. running mean: -3.198217\n",
      "ep 3466: ep_len:2840 episode reward: total was -15.750000. running mean: -3.323734\n",
      "ep 3466: ep_len:602 episode reward: total was -8.000000. running mean: -3.370497\n",
      "ep 3466: ep_len:111 episode reward: total was 54.000000. running mean: -2.796792\n",
      "ep 3466: ep_len:55 episode reward: total was 26.000000. running mean: -2.508824\n",
      "ep 3466: ep_len:1460 episode reward: total was 28.660000. running mean: -2.197136\n",
      "ep 3466: ep_len:647 episode reward: total was 32.170000. running mean: -1.853465\n",
      "ep 3466: ep_len:626 episode reward: total was 7.100000. running mean: -1.763930\n",
      "ep 3466: ep_len:663 episode reward: total was -6.050000. running mean: -1.806791\n",
      "ep 3466: ep_len:1023 episode reward: total was -10.260000. running mean: -1.891323\n",
      "ep 3466: ep_len:158 episode reward: total was 75.510000. running mean: -1.117310\n",
      "ep 3466: ep_len:33 episode reward: total was 15.000000. running mean: -0.956136\n",
      "ep 3466: ep_len:77 episode reward: total was 35.500000. running mean: -0.591575\n",
      "ep 3466: ep_len:622 episode reward: total was 6.320000. running mean: -0.522459\n",
      "ep 3466: ep_len:2917 episode reward: total was -0.690000. running mean: -0.524135\n",
      "ep 3466: ep_len:29 episode reward: total was 11.500000. running mean: -0.403893\n",
      "epsilon:0.009992 episode_count: 52133. steps_count: 56157491.000000\n",
      "ep 3467: ep_len:819 episode reward: total was -32.160000. running mean: -0.721454\n",
      "ep 3467: ep_len:693 episode reward: total was -30.610000. running mean: -1.020340\n",
      "ep 3467: ep_len:3027 episode reward: total was -45.580000. running mean: -1.465937\n",
      "ep 3467: ep_len:625 episode reward: total was 0.020000. running mean: -1.451077\n",
      "ep 3467: ep_len:91 episode reward: total was 41.000000. running mean: -1.026566\n",
      "ep 3467: ep_len:62 episode reward: total was 29.500000. running mean: -0.721301\n",
      "ep 3467: ep_len:885 episode reward: total was 39.810000. running mean: -0.315988\n",
      "ep 3467: ep_len:4012 episode reward: total was -1128.720000. running mean: -11.600028\n",
      "ep 3467: ep_len:635 episode reward: total was -4.990000. running mean: -11.533928\n",
      "ep 3467: ep_len:894 episode reward: total was 64.760000. running mean: -10.770988\n",
      "ep 3467: ep_len:618 episode reward: total was -16.210000. running mean: -10.825378\n",
      "ep 3467: ep_len:129 episode reward: total was 61.500000. running mean: -10.102125\n",
      "ep 3467: ep_len:33 episode reward: total was 15.000000. running mean: -9.851103\n",
      "ep 3467: ep_len:86 episode reward: total was 40.000000. running mean: -9.352592\n",
      "ep 3467: ep_len:884 episode reward: total was 13.570000. running mean: -9.123366\n",
      "ep 3467: ep_len:2770 episode reward: total was -0.440000. running mean: -9.036533\n",
      "ep 3467: ep_len:57 episode reward: total was 27.000000. running mean: -8.676167\n",
      "epsilon:0.009992 episode_count: 52150. steps_count: 56173811.000000\n",
      "ep 3468: ep_len:588 episode reward: total was -39.680000. running mean: -8.986206\n",
      "ep 3468: ep_len:500 episode reward: total was 13.260000. running mean: -8.763744\n",
      "ep 3468: ep_len:3005 episode reward: total was 24.330000. running mean: -8.432806\n",
      "ep 3468: ep_len:4772 episode reward: total was -1757.930000. running mean: -25.927778\n",
      "ep 3468: ep_len:500 episode reward: total was 23.740000. running mean: -25.431100\n",
      "ep 3468: ep_len:662 episode reward: total was 26.720000. running mean: -24.909589\n",
      "ep 3468: ep_len:652 episode reward: total was -23.670000. running mean: -24.897194\n",
      "ep 3468: ep_len:637 episode reward: total was 19.830000. running mean: -24.449922\n",
      "ep 3468: ep_len:813 episode reward: total was 9.050000. running mean: -24.114922\n",
      "ep 3468: ep_len:758 episode reward: total was -6.730000. running mean: -23.941073\n",
      "ep 3468: ep_len:2761 episode reward: total was -1.510000. running mean: -23.716762\n",
      "epsilon:0.009992 episode_count: 52161. steps_count: 56189459.000000\n",
      "ep 3469: ep_len:929 episode reward: total was -153.490000. running mean: -25.014495\n",
      "ep 3469: ep_len:1606 episode reward: total was -23.170000. running mean: -24.996050\n",
      "ep 3469: ep_len:99 episode reward: total was 46.500000. running mean: -24.281089\n",
      "ep 3469: ep_len:512 episode reward: total was -45.550000. running mean: -24.493778\n",
      "ep 3469: ep_len:55 episode reward: total was 23.000000. running mean: -24.018841\n",
      "ep 3469: ep_len:48 episode reward: total was 21.000000. running mean: -23.568652\n",
      "ep 3469: ep_len:500 episode reward: total was -16.790000. running mean: -23.500866\n",
      "ep 3469: ep_len:3804 episode reward: total was -182.680000. running mean: -25.092657\n",
      "ep 3469: ep_len:991 episode reward: total was -46.820000. running mean: -25.309931\n",
      "ep 3469: ep_len:645 episode reward: total was 5.180000. running mean: -25.005031\n",
      "ep 3469: ep_len:877 episode reward: total was 16.410000. running mean: -24.590881\n",
      "ep 3469: ep_len:1047 episode reward: total was -3.630000. running mean: -24.381272\n",
      "ep 3469: ep_len:2813 episode reward: total was 10.550000. running mean: -24.031959\n",
      "ep 3469: ep_len:54 episode reward: total was 25.500000. running mean: -23.536640\n",
      "epsilon:0.009992 episode_count: 52175. steps_count: 56203439.000000\n",
      "ep 3470: ep_len:953 episode reward: total was -17.650000. running mean: -23.477773\n",
      "ep 3470: ep_len:667 episode reward: total was -18.750000. running mean: -23.430496\n",
      "ep 3470: ep_len:68 episode reward: total was 31.000000. running mean: -22.886191\n",
      "ep 3470: ep_len:3023 episode reward: total was -18.550000. running mean: -22.842829\n",
      "ep 3470: ep_len:614 episode reward: total was 7.360000. running mean: -22.540800\n",
      "ep 3470: ep_len:115 episode reward: total was 56.000000. running mean: -21.755392\n",
      "ep 3470: ep_len:84 episode reward: total was 39.000000. running mean: -21.147839\n",
      "ep 3470: ep_len:1072 episode reward: total was -231.310000. running mean: -23.249460\n",
      "ep 3470: ep_len:4302 episode reward: total was -181.670000. running mean: -24.833666\n",
      "ep 3470: ep_len:1182 episode reward: total was -40.070000. running mean: -24.986029\n",
      "ep 3470: ep_len:744 episode reward: total was -0.780000. running mean: -24.743969\n",
      "ep 3470: ep_len:1139 episode reward: total was -5.920000. running mean: -24.555729\n",
      "ep 3470: ep_len:123 episode reward: total was 55.500000. running mean: -23.755172\n",
      "ep 3470: ep_len:613 episode reward: total was -17.180000. running mean: -23.689420\n",
      "ep 3470: ep_len:2882 episode reward: total was -20.320000. running mean: -23.655726\n",
      "epsilon:0.009992 episode_count: 52190. steps_count: 56221020.000000\n",
      "ep 3471: ep_len:3747 episode reward: total was -1065.360000. running mean: -34.072768\n",
      "ep 3471: ep_len:1719 episode reward: total was -8.250000. running mean: -33.814541\n",
      "ep 3471: ep_len:54 episode reward: total was 25.500000. running mean: -33.221395\n",
      "ep 3471: ep_len:2888 episode reward: total was 2.830000. running mean: -32.860881\n",
      "ep 3471: ep_len:1180 episode reward: total was -29.140000. running mean: -32.823673\n",
      "ep 3471: ep_len:93 episode reward: total was 45.000000. running mean: -32.045436\n",
      "ep 3471: ep_len:43 episode reward: total was 20.000000. running mean: -31.524982\n",
      "ep 3471: ep_len:1454 episode reward: total was 23.550000. running mean: -30.974232\n",
      "ep 3471: ep_len:3775 episode reward: total was -2454.550000. running mean: -55.209989\n",
      "ep 3471: ep_len:640 episode reward: total was -16.020000. running mean: -54.818090\n",
      "ep 3471: ep_len:780 episode reward: total was 33.140000. running mean: -53.938509\n",
      "ep 3471: ep_len:500 episode reward: total was -3.250000. running mean: -53.431624\n",
      "ep 3471: ep_len:665 episode reward: total was 9.720000. running mean: -52.800107\n",
      "ep 3471: ep_len:2815 episode reward: total was 10.420000. running mean: -52.167906\n",
      "epsilon:0.009992 episode_count: 52204. steps_count: 56241373.000000\n",
      "ep 3472: ep_len:1401 episode reward: total was 16.010000. running mean: -51.486127\n",
      "ep 3472: ep_len:923 episode reward: total was 25.080000. running mean: -50.720466\n",
      "ep 3472: ep_len:2749 episode reward: total was -27.130000. running mean: -50.484561\n",
      "ep 3472: ep_len:500 episode reward: total was 9.380000. running mean: -49.885916\n",
      "ep 3472: ep_len:48 episode reward: total was 22.500000. running mean: -49.162056\n",
      "ep 3472: ep_len:500 episode reward: total was -9.920000. running mean: -48.769636\n",
      "ep 3472: ep_len:3990 episode reward: total was -61.250000. running mean: -48.894440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3472: ep_len:595 episode reward: total was -31.040000. running mean: -48.715895\n",
      "ep 3472: ep_len:707 episode reward: total was 39.030000. running mean: -47.838436\n",
      "ep 3472: ep_len:500 episode reward: total was 41.490000. running mean: -46.945152\n",
      "ep 3472: ep_len:72 episode reward: total was 34.500000. running mean: -46.130700\n",
      "ep 3472: ep_len:830 episode reward: total was -17.330000. running mean: -45.842693\n",
      "ep 3472: ep_len:2907 episode reward: total was 1.940000. running mean: -45.364866\n",
      "epsilon:0.009992 episode_count: 52217. steps_count: 56257095.000000\n",
      "ep 3473: ep_len:1130 episode reward: total was -2.000000. running mean: -44.931218\n",
      "ep 3473: ep_len:913 episode reward: total was 17.290000. running mean: -44.309006\n",
      "ep 3473: ep_len:47 episode reward: total was 20.500000. running mean: -43.660915\n",
      "ep 3473: ep_len:3043 episode reward: total was -47.890000. running mean: -43.703206\n",
      "ep 3473: ep_len:620 episode reward: total was -19.290000. running mean: -43.459074\n",
      "ep 3473: ep_len:88 episode reward: total was 42.500000. running mean: -42.599484\n",
      "ep 3473: ep_len:1403 episode reward: total was 17.930000. running mean: -41.994189\n",
      "ep 3473: ep_len:670 episode reward: total was 31.300000. running mean: -41.261247\n",
      "ep 3473: ep_len:532 episode reward: total was -38.280000. running mean: -41.231434\n",
      "ep 3473: ep_len:659 episode reward: total was 0.030000. running mean: -40.818820\n",
      "ep 3473: ep_len:926 episode reward: total was 17.600000. running mean: -40.234632\n",
      "ep 3473: ep_len:80 episode reward: total was 35.500000. running mean: -39.477285\n",
      "ep 3473: ep_len:60 episode reward: total was 27.000000. running mean: -38.812513\n",
      "ep 3473: ep_len:132 episode reward: total was 64.500000. running mean: -37.779387\n",
      "ep 3473: ep_len:898 episode reward: total was -71.260000. running mean: -38.114194\n",
      "ep 3473: ep_len:2811 episode reward: total was 6.300000. running mean: -37.670052\n",
      "ep 3473: ep_len:57 episode reward: total was 27.000000. running mean: -37.023351\n",
      "epsilon:0.009992 episode_count: 52234. steps_count: 56271164.000000\n",
      "ep 3474: ep_len:1452 episode reward: total was 28.950000. running mean: -36.363618\n",
      "ep 3474: ep_len:771 episode reward: total was -15.340000. running mean: -36.153381\n",
      "ep 3474: ep_len:70 episode reward: total was 32.000000. running mean: -35.471848\n",
      "ep 3474: ep_len:2875 episode reward: total was -76.030000. running mean: -35.877429\n",
      "ep 3474: ep_len:667 episode reward: total was 13.640000. running mean: -35.382255\n",
      "ep 3474: ep_len:61 episode reward: total was 29.000000. running mean: -34.738432\n",
      "ep 3474: ep_len:77 episode reward: total was 37.000000. running mean: -34.021048\n",
      "ep 3474: ep_len:1078 episode reward: total was -156.990000. running mean: -35.250738\n",
      "ep 3474: ep_len:3905 episode reward: total was -343.740000. running mean: -38.335630\n",
      "ep 3474: ep_len:1540 episode reward: total was -0.690000. running mean: -37.959174\n",
      "ep 3474: ep_len:775 episode reward: total was 15.360000. running mean: -37.425982\n",
      "ep 3474: ep_len:586 episode reward: total was 43.640000. running mean: -36.615322\n",
      "ep 3474: ep_len:105 episode reward: total was 51.000000. running mean: -35.739169\n",
      "ep 3474: ep_len:1198 episode reward: total was 2.270000. running mean: -35.359077\n",
      "ep 3474: ep_len:2819 episode reward: total was -4.850000. running mean: -35.053987\n",
      "epsilon:0.009992 episode_count: 52249. steps_count: 56289143.000000\n",
      "ep 3475: ep_len:1497 episode reward: total was 21.440000. running mean: -34.489047\n",
      "ep 3475: ep_len:639 episode reward: total was -24.080000. running mean: -34.384956\n",
      "ep 3475: ep_len:69 episode reward: total was 33.000000. running mean: -33.711107\n",
      "ep 3475: ep_len:2908 episode reward: total was -17.840000. running mean: -33.552396\n",
      "ep 3475: ep_len:610 episode reward: total was -3.880000. running mean: -33.255672\n",
      "ep 3475: ep_len:62 episode reward: total was 26.500000. running mean: -32.658115\n",
      "ep 3475: ep_len:1156 episode reward: total was 12.470000. running mean: -32.206834\n",
      "ep 3475: ep_len:326 episode reward: total was 24.960000. running mean: -31.635165\n",
      "ep 3475: ep_len:879 episode reward: total was -13.750000. running mean: -31.456314\n",
      "ep 3475: ep_len:828 episode reward: total was 43.330000. running mean: -30.708451\n",
      "ep 3475: ep_len:662 episode reward: total was 16.740000. running mean: -30.233966\n",
      "ep 3475: ep_len:41 episode reward: total was 19.000000. running mean: -29.741627\n",
      "ep 3475: ep_len:592 episode reward: total was 0.180000. running mean: -29.442410\n",
      "ep 3475: ep_len:2781 episode reward: total was -1.100000. running mean: -29.158986\n",
      "ep 3475: ep_len:68 episode reward: total was 31.000000. running mean: -28.557396\n",
      "epsilon:0.009992 episode_count: 52264. steps_count: 56302261.000000\n",
      "ep 3476: ep_len:1467 episode reward: total was 32.740000. running mean: -27.944422\n",
      "ep 3476: ep_len:963 episode reward: total was 19.410000. running mean: -27.470878\n",
      "ep 3476: ep_len:2888 episode reward: total was -46.270000. running mean: -27.658869\n",
      "ep 3476: ep_len:2786 episode reward: total was -1213.080000. running mean: -39.513081\n",
      "ep 3476: ep_len:168 episode reward: total was 82.500000. running mean: -38.292950\n",
      "ep 3476: ep_len:609 episode reward: total was 0.870000. running mean: -37.901320\n",
      "ep 3476: ep_len:638 episode reward: total was 20.570000. running mean: -37.316607\n",
      "ep 3476: ep_len:612 episode reward: total was 20.410000. running mean: -36.739341\n",
      "ep 3476: ep_len:7323 episode reward: total was -497.900000. running mean: -41.350948\n",
      "ep 3476: ep_len:718 episode reward: total was 14.700000. running mean: -40.790438\n",
      "ep 3476: ep_len:85 episode reward: total was 41.000000. running mean: -39.972534\n",
      "ep 3476: ep_len:600 episode reward: total was -3.260000. running mean: -39.605408\n",
      "ep 3476: ep_len:2868 episode reward: total was -4.970000. running mean: -39.259054\n",
      "epsilon:0.009992 episode_count: 52277. steps_count: 56323986.000000\n",
      "ep 3477: ep_len:1424 episode reward: total was 31.300000. running mean: -38.553464\n",
      "ep 3477: ep_len:500 episode reward: total was 4.040000. running mean: -38.127529\n",
      "ep 3477: ep_len:2990 episode reward: total was -58.230000. running mean: -38.328554\n",
      "ep 3477: ep_len:550 episode reward: total was -63.350000. running mean: -38.578768\n",
      "ep 3477: ep_len:1083 episode reward: total was -7.980000. running mean: -38.272781\n",
      "ep 3477: ep_len:625 episode reward: total was 24.540000. running mean: -37.644653\n",
      "ep 3477: ep_len:928 episode reward: total was -31.290000. running mean: -37.581106\n",
      "ep 3477: ep_len:863 episode reward: total was 38.270000. running mean: -36.822595\n",
      "ep 3477: ep_len:500 episode reward: total was 11.350000. running mean: -36.340869\n",
      "ep 3477: ep_len:86 episode reward: total was 40.000000. running mean: -35.577461\n",
      "ep 3477: ep_len:45 episode reward: total was 21.000000. running mean: -35.011686\n",
      "ep 3477: ep_len:653 episode reward: total was 13.700000. running mean: -34.524569\n",
      "ep 3477: ep_len:43 episode reward: total was 20.000000. running mean: -33.979323\n",
      "epsilon:0.009992 episode_count: 52290. steps_count: 56334276.000000\n",
      "ep 3478: ep_len:616 episode reward: total was -6.130000. running mean: -33.700830\n",
      "ep 3478: ep_len:1250 episode reward: total was -22.470000. running mean: -33.588522\n",
      "ep 3478: ep_len:41 episode reward: total was 17.500000. running mean: -33.077637\n",
      "ep 3478: ep_len:2967 episode reward: total was -40.290000. running mean: -33.149760\n",
      "ep 3478: ep_len:620 episode reward: total was -34.260000. running mean: -33.160863\n",
      "ep 3478: ep_len:37 episode reward: total was 17.000000. running mean: -32.659254\n",
      "ep 3478: ep_len:156 episode reward: total was 75.000000. running mean: -31.582662\n",
      "ep 3478: ep_len:63 episode reward: total was 28.500000. running mean: -30.981835\n",
      "ep 3478: ep_len:500 episode reward: total was 52.880000. running mean: -30.143217\n",
      "ep 3478: ep_len:344 episode reward: total was 23.640000. running mean: -29.605384\n",
      "ep 3478: ep_len:562 episode reward: total was -0.610000. running mean: -29.315431\n",
      "ep 3478: ep_len:761 episode reward: total was 37.250000. running mean: -28.649776\n",
      "ep 3478: ep_len:529 episode reward: total was 38.720000. running mean: -27.976079\n",
      "ep 3478: ep_len:82 episode reward: total was 38.000000. running mean: -27.316318\n",
      "ep 3478: ep_len:748 episode reward: total was -81.050000. running mean: -27.853655\n",
      "ep 3478: ep_len:46 episode reward: total was 21.500000. running mean: -27.360118\n",
      "epsilon:0.009992 episode_count: 52306. steps_count: 56343598.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3479: ep_len:1429 episode reward: total was 23.850000. running mean: -26.848017\n",
      "ep 3479: ep_len:980 episode reward: total was 14.960000. running mean: -26.429937\n",
      "ep 3479: ep_len:52 episode reward: total was 23.000000. running mean: -25.935637\n",
      "ep 3479: ep_len:3069 episode reward: total was -2901.240000. running mean: -54.688681\n",
      "ep 3479: ep_len:665 episode reward: total was 15.150000. running mean: -53.990294\n",
      "ep 3479: ep_len:53 episode reward: total was 22.000000. running mean: -53.230391\n",
      "ep 3479: ep_len:129 episode reward: total was 63.000000. running mean: -52.068087\n",
      "ep 3479: ep_len:93 episode reward: total was 45.000000. running mean: -51.097406\n",
      "ep 3479: ep_len:500 episode reward: total was 0.340000. running mean: -50.583032\n",
      "ep 3479: ep_len:337 episode reward: total was 17.020000. running mean: -49.907002\n",
      "ep 3479: ep_len:941 episode reward: total was -62.470000. running mean: -50.032632\n",
      "ep 3479: ep_len:809 episode reward: total was 9.460000. running mean: -49.437706\n",
      "ep 3479: ep_len:500 episode reward: total was 2.880000. running mean: -48.914529\n",
      "ep 3479: ep_len:122 episode reward: total was 58.000000. running mean: -47.845383\n",
      "ep 3479: ep_len:854 episode reward: total was 25.130000. running mean: -47.115629\n",
      "ep 3479: ep_len:2888 episode reward: total was -68.520000. running mean: -47.329673\n",
      "epsilon:0.009992 episode_count: 52322. steps_count: 56357019.000000\n",
      "ep 3480: ep_len:1064 episode reward: total was -39.850000. running mean: -47.254876\n",
      "ep 3480: ep_len:1000 episode reward: total was 31.940000. running mean: -46.462928\n",
      "ep 3480: ep_len:2971 episode reward: total was -31.020000. running mean: -46.308498\n",
      "ep 3480: ep_len:1176 episode reward: total was -31.320000. running mean: -46.158613\n",
      "ep 3480: ep_len:1686 episode reward: total was -459.620000. running mean: -50.293227\n",
      "ep 3480: ep_len:3798 episode reward: total was -237.480000. running mean: -52.165095\n",
      "ep 3480: ep_len:519 episode reward: total was 3.000000. running mean: -51.613444\n",
      "ep 3480: ep_len:742 episode reward: total was 39.050000. running mean: -50.706810\n",
      "ep 3480: ep_len:500 episode reward: total was 3.560000. running mean: -50.164142\n",
      "ep 3480: ep_len:1204 episode reward: total was 14.270000. running mean: -49.519800\n",
      "ep 3480: ep_len:2822 episode reward: total was -37.990000. running mean: -49.404502\n",
      "ep 3480: ep_len:52 episode reward: total was 23.000000. running mean: -48.680457\n",
      "epsilon:0.009992 episode_count: 52334. steps_count: 56374553.000000\n",
      "ep 3481: ep_len:1495 episode reward: total was 28.670000. running mean: -47.906953\n",
      "ep 3481: ep_len:1652 episode reward: total was -10.920000. running mean: -47.537083\n",
      "ep 3481: ep_len:66 episode reward: total was 31.500000. running mean: -46.746712\n",
      "ep 3481: ep_len:2978 episode reward: total was -71.540000. running mean: -46.994645\n",
      "ep 3481: ep_len:680 episode reward: total was -1.850000. running mean: -46.543199\n",
      "ep 3481: ep_len:41 episode reward: total was 17.500000. running mean: -45.902767\n",
      "ep 3481: ep_len:72 episode reward: total was 34.500000. running mean: -45.098739\n",
      "ep 3481: ep_len:68 episode reward: total was 32.500000. running mean: -44.322752\n",
      "ep 3481: ep_len:1404 episode reward: total was 17.940000. running mean: -43.700124\n",
      "ep 3481: ep_len:662 episode reward: total was 20.290000. running mean: -43.060223\n",
      "ep 3481: ep_len:987 episode reward: total was -7.490000. running mean: -42.704521\n",
      "ep 3481: ep_len:7391 episode reward: total was 19.480000. running mean: -42.082675\n",
      "ep 3481: ep_len:660 episode reward: total was -13.770000. running mean: -41.799549\n",
      "ep 3481: ep_len:81 episode reward: total was 39.000000. running mean: -40.991553\n",
      "ep 3481: ep_len:199 episode reward: total was 93.500000. running mean: -39.646638\n",
      "ep 3481: ep_len:1120 episode reward: total was -7.150000. running mean: -39.321671\n",
      "ep 3481: ep_len:2913 episode reward: total was -18.540000. running mean: -39.113855\n",
      "epsilon:0.009992 episode_count: 52351. steps_count: 56397022.000000\n",
      "ep 3482: ep_len:659 episode reward: total was -4.040000. running mean: -38.763116\n",
      "ep 3482: ep_len:500 episode reward: total was 0.250000. running mean: -38.372985\n",
      "ep 3482: ep_len:55 episode reward: total was 26.000000. running mean: -37.729255\n",
      "ep 3482: ep_len:3030 episode reward: total was -74.710000. running mean: -38.099062\n",
      "ep 3482: ep_len:638 episode reward: total was -0.350000. running mean: -37.721572\n",
      "ep 3482: ep_len:48 episode reward: total was 21.000000. running mean: -37.134356\n",
      "ep 3482: ep_len:93 episode reward: total was 45.000000. running mean: -36.313013\n",
      "ep 3482: ep_len:44 episode reward: total was 20.010000. running mean: -35.749782\n",
      "ep 3482: ep_len:1385 episode reward: total was 18.240000. running mean: -35.209885\n",
      "ep 3482: ep_len:3995 episode reward: total was -66.900000. running mean: -35.526786\n",
      "ep 3482: ep_len:1609 episode reward: total was -88.110000. running mean: -36.052618\n",
      "ep 3482: ep_len:844 episode reward: total was 17.300000. running mean: -35.519092\n",
      "ep 3482: ep_len:776 episode reward: total was -11.600000. running mean: -35.279901\n",
      "ep 3482: ep_len:89 episode reward: total was 43.000000. running mean: -34.497102\n",
      "ep 3482: ep_len:116 episode reward: total was 56.500000. running mean: -33.587131\n",
      "ep 3482: ep_len:1515 episode reward: total was 10.420000. running mean: -33.147059\n",
      "ep 3482: ep_len:2722 episode reward: total was -6.700000. running mean: -32.882589\n",
      "ep 3482: ep_len:53 episode reward: total was 25.000000. running mean: -32.303763\n",
      "epsilon:0.009992 episode_count: 52369. steps_count: 56415193.000000\n",
      "ep 3483: ep_len:1127 episode reward: total was 15.790000. running mean: -31.822825\n",
      "ep 3483: ep_len:174 episode reward: total was -12.350000. running mean: -31.628097\n",
      "ep 3483: ep_len:3120 episode reward: total was -84.130000. running mean: -32.153116\n",
      "ep 3483: ep_len:1164 episode reward: total was -9.680000. running mean: -31.928385\n",
      "ep 3483: ep_len:131 episode reward: total was 59.500000. running mean: -31.014101\n",
      "ep 3483: ep_len:1073 episode reward: total was -92.460000. running mean: -31.628560\n",
      "ep 3483: ep_len:678 episode reward: total was 19.260000. running mean: -31.119674\n",
      "ep 3483: ep_len:1271 episode reward: total was -27.860000. running mean: -31.087078\n",
      "ep 3483: ep_len:838 episode reward: total was -3.280000. running mean: -30.809007\n",
      "ep 3483: ep_len:787 episode reward: total was 16.100000. running mean: -30.339917\n",
      "ep 3483: ep_len:58 episode reward: total was 27.500000. running mean: -29.761518\n",
      "ep 3483: ep_len:194 episode reward: total was 92.500000. running mean: -28.538903\n",
      "ep 3483: ep_len:690 episode reward: total was 5.770000. running mean: -28.195814\n",
      "ep 3483: ep_len:46 episode reward: total was 20.000000. running mean: -27.713855\n",
      "epsilon:0.009992 episode_count: 52383. steps_count: 56426544.000000\n",
      "ep 3484: ep_len:1125 episode reward: total was 18.100000. running mean: -27.255717\n",
      "ep 3484: ep_len:701 episode reward: total was -28.070000. running mean: -27.263860\n",
      "ep 3484: ep_len:62 episode reward: total was 28.000000. running mean: -26.711221\n",
      "ep 3484: ep_len:2858 episode reward: total was -65.300000. running mean: -27.097109\n",
      "ep 3484: ep_len:500 episode reward: total was 8.210000. running mean: -26.744038\n",
      "ep 3484: ep_len:40 episode reward: total was 18.500000. running mean: -26.291597\n",
      "ep 3484: ep_len:97 episode reward: total was 47.000000. running mean: -25.558681\n",
      "ep 3484: ep_len:32 episode reward: total was 14.500000. running mean: -25.158095\n",
      "ep 3484: ep_len:623 episode reward: total was 39.890000. running mean: -24.507614\n",
      "ep 3484: ep_len:652 episode reward: total was 20.560000. running mean: -24.056938\n",
      "ep 3484: ep_len:527 episode reward: total was -19.140000. running mean: -24.007768\n",
      "ep 3484: ep_len:737 episode reward: total was -11.310000. running mean: -23.880790\n",
      "ep 3484: ep_len:618 episode reward: total was -10.150000. running mean: -23.743483\n",
      "ep 3484: ep_len:147 episode reward: total was 72.000000. running mean: -22.786048\n",
      "ep 3484: ep_len:46 episode reward: total was 21.500000. running mean: -22.343187\n",
      "ep 3484: ep_len:590 episode reward: total was -5.130000. running mean: -22.171055\n",
      "ep 3484: ep_len:2874 episode reward: total was -17.820000. running mean: -22.127545\n",
      "epsilon:0.009992 episode_count: 52400. steps_count: 56438773.000000\n",
      "ep 3485: ep_len:863 episode reward: total was -14.380000. running mean: -22.050069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3485: ep_len:786 episode reward: total was -16.790000. running mean: -21.997469\n",
      "ep 3485: ep_len:2981 episode reward: total was -1343.760000. running mean: -35.215094\n",
      "ep 3485: ep_len:1195 episode reward: total was -21.180000. running mean: -35.074743\n",
      "ep 3485: ep_len:56 episode reward: total was 26.500000. running mean: -34.458996\n",
      "ep 3485: ep_len:711 episode reward: total was -17.250000. running mean: -34.286906\n",
      "ep 3485: ep_len:347 episode reward: total was 18.620000. running mean: -33.757837\n",
      "ep 3485: ep_len:573 episode reward: total was 4.130000. running mean: -33.378958\n",
      "ep 3485: ep_len:835 episode reward: total was 14.570000. running mean: -32.899469\n",
      "ep 3485: ep_len:973 episode reward: total was 55.890000. running mean: -32.011574\n",
      "ep 3485: ep_len:1156 episode reward: total was 8.250000. running mean: -31.608958\n",
      "ep 3485: ep_len:2917 episode reward: total was 9.450000. running mean: -31.198369\n",
      "epsilon:0.009992 episode_count: 52412. steps_count: 56452166.000000\n",
      "ep 3486: ep_len:836 episode reward: total was -12.400000. running mean: -31.010385\n",
      "ep 3486: ep_len:818 episode reward: total was 17.120000. running mean: -30.529081\n",
      "ep 3486: ep_len:82 episode reward: total was 38.000000. running mean: -29.843790\n",
      "ep 3486: ep_len:2957 episode reward: total was -47.600000. running mean: -30.021352\n",
      "ep 3486: ep_len:877 episode reward: total was 18.760000. running mean: -29.533539\n",
      "ep 3486: ep_len:88 episode reward: total was 42.500000. running mean: -28.813203\n",
      "ep 3486: ep_len:44 episode reward: total was 19.000000. running mean: -28.335071\n",
      "ep 3486: ep_len:1407 episode reward: total was -191.010000. running mean: -29.961821\n",
      "ep 3486: ep_len:649 episode reward: total was -56.330000. running mean: -30.225503\n",
      "ep 3486: ep_len:757 episode reward: total was -12.260000. running mean: -30.045848\n",
      "ep 3486: ep_len:709 episode reward: total was 18.480000. running mean: -29.560589\n",
      "ep 3486: ep_len:910 episode reward: total was 23.290000. running mean: -29.032083\n",
      "ep 3486: ep_len:620 episode reward: total was -9.120000. running mean: -28.832962\n",
      "ep 3486: ep_len:2877 episode reward: total was -18.810000. running mean: -28.732733\n",
      "epsilon:0.009992 episode_count: 52426. steps_count: 56465797.000000\n",
      "ep 3487: ep_len:703 episode reward: total was -27.480000. running mean: -28.720205\n",
      "ep 3487: ep_len:937 episode reward: total was 14.170000. running mean: -28.291303\n",
      "ep 3487: ep_len:72 episode reward: total was 34.500000. running mean: -27.663390\n",
      "ep 3487: ep_len:2930 episode reward: total was -114.140000. running mean: -28.528156\n",
      "ep 3487: ep_len:1402 episode reward: total was 26.460000. running mean: -27.978275\n",
      "ep 3487: ep_len:643 episode reward: total was 3.230000. running mean: -27.666192\n",
      "ep 3487: ep_len:500 episode reward: total was 25.640000. running mean: -27.133130\n",
      "ep 3487: ep_len:1574 episode reward: total was -61.870000. running mean: -27.480499\n",
      "ep 3487: ep_len:649 episode reward: total was 0.720000. running mean: -27.198494\n",
      "ep 3487: ep_len:935 episode reward: total was 2.360000. running mean: -26.902909\n",
      "ep 3487: ep_len:48 episode reward: total was 22.500000. running mean: -26.408880\n",
      "ep 3487: ep_len:1056 episode reward: total was 3.790000. running mean: -26.106891\n",
      "ep 3487: ep_len:2766 episode reward: total was -0.390000. running mean: -25.849722\n",
      "epsilon:0.009992 episode_count: 52439. steps_count: 56480012.000000\n",
      "ep 3488: ep_len:614 episode reward: total was -5.200000. running mean: -25.643225\n",
      "ep 3488: ep_len:195 episode reward: total was 3.810000. running mean: -25.348693\n",
      "ep 3488: ep_len:43 episode reward: total was 17.000000. running mean: -24.925206\n",
      "ep 3488: ep_len:3000 episode reward: total was -92.530000. running mean: -25.601254\n",
      "ep 3488: ep_len:3810 episode reward: total was -861.870000. running mean: -33.963941\n",
      "ep 3488: ep_len:69 episode reward: total was 33.000000. running mean: -33.294302\n",
      "ep 3488: ep_len:98 episode reward: total was 47.500000. running mean: -32.486359\n",
      "ep 3488: ep_len:656 episode reward: total was 2.350000. running mean: -32.137995\n",
      "ep 3488: ep_len:3898 episode reward: total was -322.850000. running mean: -35.045115\n",
      "ep 3488: ep_len:1562 episode reward: total was -64.660000. running mean: -35.341264\n",
      "ep 3488: ep_len:677 episode reward: total was 26.580000. running mean: -34.722051\n",
      "ep 3488: ep_len:634 episode reward: total was -227.570000. running mean: -36.650531\n",
      "ep 3488: ep_len:159 episode reward: total was 76.500000. running mean: -35.519026\n",
      "ep 3488: ep_len:931 episode reward: total was -7.720000. running mean: -35.241035\n",
      "ep 3488: ep_len:2835 episode reward: total was -574.620000. running mean: -40.634825\n",
      "ep 3488: ep_len:42 episode reward: total was 18.000000. running mean: -40.048477\n",
      "epsilon:0.009992 episode_count: 52455. steps_count: 56499235.000000\n",
      "ep 3489: ep_len:1083 episode reward: total was 1.110000. running mean: -39.636892\n",
      "ep 3489: ep_len:743 episode reward: total was -3.350000. running mean: -39.274023\n",
      "ep 3489: ep_len:39 episode reward: total was 18.000000. running mean: -38.701283\n",
      "ep 3489: ep_len:3053 episode reward: total was -17.920000. running mean: -38.493470\n",
      "ep 3489: ep_len:691 episode reward: total was -13.950000. running mean: -38.248035\n",
      "ep 3489: ep_len:49 episode reward: total was 23.000000. running mean: -37.635555\n",
      "ep 3489: ep_len:96 episode reward: total was 46.500000. running mean: -36.794199\n",
      "ep 3489: ep_len:87 episode reward: total was 42.000000. running mean: -36.006257\n",
      "ep 3489: ep_len:63 episode reward: total was 30.000000. running mean: -35.346195\n",
      "ep 3489: ep_len:719 episode reward: total was -9.140000. running mean: -35.084133\n",
      "ep 3489: ep_len:3963 episode reward: total was -62.240000. running mean: -35.355691\n",
      "ep 3489: ep_len:1159 episode reward: total was -20.960000. running mean: -35.211735\n",
      "ep 3489: ep_len:615 episode reward: total was -9.050000. running mean: -34.950117\n",
      "ep 3489: ep_len:560 episode reward: total was -6.690000. running mean: -34.667516\n",
      "ep 3489: ep_len:57 episode reward: total was 27.000000. running mean: -34.050841\n",
      "ep 3489: ep_len:680 episode reward: total was 26.930000. running mean: -33.441032\n",
      "ep 3489: ep_len:2871 episode reward: total was -76.330000. running mean: -33.869922\n",
      "epsilon:0.009992 episode_count: 52472. steps_count: 56515763.000000\n",
      "ep 3490: ep_len:688 episode reward: total was -93.730000. running mean: -34.468523\n",
      "ep 3490: ep_len:627 episode reward: total was -6.410000. running mean: -34.187938\n",
      "ep 3490: ep_len:2999 episode reward: total was -52.170000. running mean: -34.367758\n",
      "ep 3490: ep_len:599 episode reward: total was -5.730000. running mean: -34.081381\n",
      "ep 3490: ep_len:35 episode reward: total was 16.000000. running mean: -33.580567\n",
      "ep 3490: ep_len:1409 episode reward: total was -63.850000. running mean: -33.883261\n",
      "ep 3490: ep_len:631 episode reward: total was 12.880000. running mean: -33.415629\n",
      "ep 3490: ep_len:535 episode reward: total was -0.940000. running mean: -33.090872\n",
      "ep 3490: ep_len:645 episode reward: total was 20.800000. running mean: -32.551964\n",
      "ep 3490: ep_len:500 episode reward: total was 29.430000. running mean: -31.932144\n",
      "ep 3490: ep_len:1524 episode reward: total was 20.640000. running mean: -31.406423\n",
      "ep 3490: ep_len:2793 episode reward: total was -81.170000. running mean: -31.904058\n",
      "ep 3490: ep_len:63 episode reward: total was 30.000000. running mean: -31.285018\n",
      "epsilon:0.009992 episode_count: 52485. steps_count: 56528811.000000\n",
      "ep 3491: ep_len:675 episode reward: total was -49.980000. running mean: -31.471968\n",
      "ep 3491: ep_len:949 episode reward: total was 35.040000. running mean: -30.806848\n",
      "ep 3491: ep_len:2923 episode reward: total was -25.510000. running mean: -30.753879\n",
      "ep 3491: ep_len:500 episode reward: total was -0.080000. running mean: -30.447141\n",
      "ep 3491: ep_len:55 episode reward: total was 26.000000. running mean: -29.882669\n",
      "ep 3491: ep_len:142 episode reward: total was 68.000000. running mean: -28.903843\n",
      "ep 3491: ep_len:60 episode reward: total was 28.500000. running mean: -28.329804\n",
      "ep 3491: ep_len:1023 episode reward: total was -47.510000. running mean: -28.521606\n",
      "ep 3491: ep_len:325 episode reward: total was 4.620000. running mean: -28.190190\n",
      "ep 3491: ep_len:792 episode reward: total was -48.090000. running mean: -28.389188\n",
      "ep 3491: ep_len:683 episode reward: total was 12.270000. running mean: -27.982596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3491: ep_len:500 episode reward: total was 11.330000. running mean: -27.589470\n",
      "ep 3491: ep_len:656 episode reward: total was -0.310000. running mean: -27.316676\n",
      "ep 3491: ep_len:2906 episode reward: total was -11.810000. running mean: -27.161609\n",
      "ep 3491: ep_len:69 episode reward: total was 33.000000. running mean: -26.559993\n",
      "epsilon:0.009992 episode_count: 52500. steps_count: 56541069.000000\n",
      "ep 3492: ep_len:622 episode reward: total was 18.590000. running mean: -26.108493\n",
      "ep 3492: ep_len:190 episode reward: total was 14.260000. running mean: -25.704808\n",
      "ep 3492: ep_len:43 episode reward: total was 17.000000. running mean: -25.277760\n",
      "ep 3492: ep_len:2966 episode reward: total was -9.270000. running mean: -25.117682\n",
      "ep 3492: ep_len:616 episode reward: total was 24.950000. running mean: -24.617005\n",
      "ep 3492: ep_len:62 episode reward: total was 29.500000. running mean: -24.075835\n",
      "ep 3492: ep_len:71 episode reward: total was 31.000000. running mean: -23.525077\n",
      "ep 3492: ep_len:500 episode reward: total was 1.070000. running mean: -23.279126\n",
      "ep 3492: ep_len:649 episode reward: total was 19.670000. running mean: -22.849635\n",
      "ep 3492: ep_len:529 episode reward: total was -24.170000. running mean: -22.862839\n",
      "ep 3492: ep_len:789 episode reward: total was 13.880000. running mean: -22.495410\n",
      "ep 3492: ep_len:991 episode reward: total was 18.680000. running mean: -22.083656\n",
      "ep 3492: ep_len:162 episode reward: total was 79.500000. running mean: -21.067820\n",
      "ep 3492: ep_len:775 episode reward: total was -5.600000. running mean: -20.913141\n",
      "ep 3492: ep_len:2877 episode reward: total was -6.690000. running mean: -20.770910\n",
      "epsilon:0.009992 episode_count: 52515. steps_count: 56552911.000000\n",
      "ep 3493: ep_len:869 episode reward: total was 35.210000. running mean: -20.211101\n",
      "ep 3493: ep_len:1622 episode reward: total was -85.230000. running mean: -20.861290\n",
      "ep 3493: ep_len:56 episode reward: total was 25.000000. running mean: -20.402677\n",
      "ep 3493: ep_len:3043 episode reward: total was 17.060000. running mean: -20.028050\n",
      "ep 3493: ep_len:1261 episode reward: total was -38.490000. running mean: -20.212670\n",
      "ep 3493: ep_len:40 episode reward: total was 18.500000. running mean: -19.825543\n",
      "ep 3493: ep_len:58 episode reward: total was 26.000000. running mean: -19.367288\n",
      "ep 3493: ep_len:1443 episode reward: total was -154.280000. running mean: -20.716415\n",
      "ep 3493: ep_len:678 episode reward: total was 20.390000. running mean: -20.305351\n",
      "ep 3493: ep_len:580 episode reward: total was -58.000000. running mean: -20.682297\n",
      "ep 3493: ep_len:797 episode reward: total was 27.900000. running mean: -20.196474\n",
      "ep 3493: ep_len:926 episode reward: total was 46.880000. running mean: -19.525709\n",
      "ep 3493: ep_len:73 episode reward: total was 35.000000. running mean: -18.980452\n",
      "ep 3493: ep_len:717 episode reward: total was -656.050000. running mean: -25.351148\n",
      "ep 3493: ep_len:2865 episode reward: total was -15.500000. running mean: -25.252636\n",
      "ep 3493: ep_len:51 episode reward: total was 24.000000. running mean: -24.760110\n",
      "epsilon:0.009992 episode_count: 52531. steps_count: 56567990.000000\n",
      "ep 3494: ep_len:1534 episode reward: total was 24.360000. running mean: -24.268909\n",
      "ep 3494: ep_len:729 episode reward: total was -12.070000. running mean: -24.146920\n",
      "ep 3494: ep_len:2974 episode reward: total was -39.290000. running mean: -24.298350\n",
      "ep 3494: ep_len:702 episode reward: total was 19.750000. running mean: -23.857867\n",
      "ep 3494: ep_len:96 episode reward: total was 46.500000. running mean: -23.154288\n",
      "ep 3494: ep_len:70 episode reward: total was 32.000000. running mean: -22.602745\n",
      "ep 3494: ep_len:1460 episode reward: total was -53.250000. running mean: -22.909218\n",
      "ep 3494: ep_len:636 episode reward: total was 25.630000. running mean: -22.423826\n",
      "ep 3494: ep_len:2002 episode reward: total was -87.240000. running mean: -23.071988\n",
      "ep 3494: ep_len:902 episode reward: total was 59.970000. running mean: -22.241568\n",
      "ep 3494: ep_len:535 episode reward: total was 35.080000. running mean: -21.668352\n",
      "ep 3494: ep_len:1488 episode reward: total was 10.330000. running mean: -21.348368\n",
      "ep 3494: ep_len:2734 episode reward: total was -21.190000. running mean: -21.346785\n",
      "epsilon:0.009992 episode_count: 52544. steps_count: 56583852.000000\n",
      "ep 3495: ep_len:991 episode reward: total was -39.750000. running mean: -21.530817\n",
      "ep 3495: ep_len:500 episode reward: total was 16.840000. running mean: -21.147109\n",
      "ep 3495: ep_len:3012 episode reward: total was -1.900000. running mean: -20.954638\n",
      "ep 3495: ep_len:500 episode reward: total was 20.080000. running mean: -20.544291\n",
      "ep 3495: ep_len:151 episode reward: total was 72.500000. running mean: -19.613848\n",
      "ep 3495: ep_len:74 episode reward: total was 34.000000. running mean: -19.077710\n",
      "ep 3495: ep_len:1439 episode reward: total was 14.680000. running mean: -18.740133\n",
      "ep 3495: ep_len:633 episode reward: total was 20.860000. running mean: -18.344131\n",
      "ep 3495: ep_len:1536 episode reward: total was -76.730000. running mean: -18.927990\n",
      "ep 3495: ep_len:7394 episode reward: total was -640.600000. running mean: -25.144710\n",
      "ep 3495: ep_len:1131 episode reward: total was 2.940000. running mean: -24.863863\n",
      "ep 3495: ep_len:24 episode reward: total was 10.500000. running mean: -24.510224\n",
      "ep 3495: ep_len:76 episode reward: total was 35.000000. running mean: -23.915122\n",
      "ep 3495: ep_len:608 episode reward: total was 1.870000. running mean: -23.657271\n",
      "ep 3495: ep_len:2844 episode reward: total was -17.550000. running mean: -23.596198\n",
      "epsilon:0.009992 episode_count: 52559. steps_count: 56604765.000000\n",
      "ep 3496: ep_len:1450 episode reward: total was 20.540000. running mean: -23.154836\n",
      "ep 3496: ep_len:500 episode reward: total was 10.140000. running mean: -22.821888\n",
      "ep 3496: ep_len:3026 episode reward: total was -28.180000. running mean: -22.875469\n",
      "ep 3496: ep_len:766 episode reward: total was 17.410000. running mean: -22.472614\n",
      "ep 3496: ep_len:146 episode reward: total was 68.500000. running mean: -21.562888\n",
      "ep 3496: ep_len:861 episode reward: total was 33.600000. running mean: -21.011259\n",
      "ep 3496: ep_len:500 episode reward: total was 21.540000. running mean: -20.585747\n",
      "ep 3496: ep_len:1250 episode reward: total was -39.670000. running mean: -20.776589\n",
      "ep 3496: ep_len:681 episode reward: total was 23.760000. running mean: -20.331223\n",
      "ep 3496: ep_len:567 episode reward: total was 48.960000. running mean: -19.638311\n",
      "ep 3496: ep_len:737 episode reward: total was -106.930000. running mean: -20.511228\n",
      "ep 3496: ep_len:2778 episode reward: total was 0.960000. running mean: -20.296516\n",
      "epsilon:0.009992 episode_count: 52571. steps_count: 56618027.000000\n",
      "ep 3497: ep_len:1052 episode reward: total was -3.270000. running mean: -20.126251\n",
      "ep 3497: ep_len:974 episode reward: total was 16.090000. running mean: -19.764088\n",
      "ep 3497: ep_len:2914 episode reward: total was 9.360000. running mean: -19.472847\n",
      "ep 3497: ep_len:680 episode reward: total was 0.160000. running mean: -19.276519\n",
      "ep 3497: ep_len:41 episode reward: total was 17.500000. running mean: -18.908754\n",
      "ep 3497: ep_len:907 episode reward: total was 65.880000. running mean: -18.060866\n",
      "ep 3497: ep_len:3986 episode reward: total was -532.490000. running mean: -23.205157\n",
      "ep 3497: ep_len:1155 episode reward: total was -7.160000. running mean: -23.044706\n",
      "ep 3497: ep_len:804 episode reward: total was 48.970000. running mean: -22.324559\n",
      "ep 3497: ep_len:596 episode reward: total was 1.750000. running mean: -22.083813\n",
      "ep 3497: ep_len:79 episode reward: total was 38.000000. running mean: -21.482975\n",
      "ep 3497: ep_len:42 episode reward: total was 19.500000. running mean: -21.073145\n",
      "ep 3497: ep_len:986 episode reward: total was 20.800000. running mean: -20.654414\n",
      "ep 3497: ep_len:2823 episode reward: total was -6.090000. running mean: -20.508770\n",
      "ep 3497: ep_len:75 episode reward: total was 34.500000. running mean: -19.958682\n",
      "epsilon:0.009992 episode_count: 52586. steps_count: 56635141.000000\n",
      "ep 3498: ep_len:898 episode reward: total was -40.360000. running mean: -20.162695\n",
      "ep 3498: ep_len:784 episode reward: total was -6.470000. running mean: -20.025768\n",
      "ep 3498: ep_len:44 episode reward: total was 20.500000. running mean: -19.620511\n",
      "ep 3498: ep_len:2924 episode reward: total was -42.090000. running mean: -19.845205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3498: ep_len:554 episode reward: total was -33.010000. running mean: -19.976853\n",
      "ep 3498: ep_len:63 episode reward: total was 30.000000. running mean: -19.477085\n",
      "ep 3498: ep_len:99 episode reward: total was 46.500000. running mean: -18.817314\n",
      "ep 3498: ep_len:912 episode reward: total was 63.450000. running mean: -17.994641\n",
      "ep 3498: ep_len:684 episode reward: total was 21.890000. running mean: -17.595794\n",
      "ep 3498: ep_len:1585 episode reward: total was -17.100000. running mean: -17.590837\n",
      "ep 3498: ep_len:663 episode reward: total was 15.930000. running mean: -17.255628\n",
      "ep 3498: ep_len:991 episode reward: total was 23.210000. running mean: -16.850972\n",
      "ep 3498: ep_len:89 episode reward: total was 43.000000. running mean: -16.252462\n",
      "ep 3498: ep_len:60 episode reward: total was 27.000000. running mean: -15.819938\n",
      "ep 3498: ep_len:586 episode reward: total was 0.490000. running mean: -15.656838\n",
      "ep 3498: ep_len:2881 episode reward: total was 10.250000. running mean: -15.397770\n",
      "epsilon:0.009992 episode_count: 52602. steps_count: 56648958.000000\n",
      "ep 3499: ep_len:1535 episode reward: total was -520.330000. running mean: -20.447092\n",
      "ep 3499: ep_len:500 episode reward: total was 26.820000. running mean: -19.974421\n",
      "ep 3499: ep_len:3037 episode reward: total was -14.470000. running mean: -19.919377\n",
      "ep 3499: ep_len:630 episode reward: total was 9.270000. running mean: -19.627483\n",
      "ep 3499: ep_len:42 episode reward: total was 18.000000. running mean: -19.251208\n",
      "ep 3499: ep_len:78 episode reward: total was 33.000000. running mean: -18.728696\n",
      "ep 3499: ep_len:500 episode reward: total was 37.970000. running mean: -18.161709\n",
      "ep 3499: ep_len:322 episode reward: total was 17.020000. running mean: -17.809892\n",
      "ep 3499: ep_len:874 episode reward: total was -24.030000. running mean: -17.872093\n",
      "ep 3499: ep_len:7387 episode reward: total was 24.090000. running mean: -17.452472\n",
      "ep 3499: ep_len:1462 episode reward: total was 1.560000. running mean: -17.262348\n",
      "ep 3499: ep_len:44 episode reward: total was 19.000000. running mean: -16.899724\n",
      "ep 3499: ep_len:785 episode reward: total was -10.800000. running mean: -16.838727\n",
      "ep 3499: ep_len:3032 episode reward: total was -22.010000. running mean: -16.890440\n",
      "ep 3499: ep_len:45 episode reward: total was 19.500000. running mean: -16.526535\n",
      "epsilon:0.009992 episode_count: 52617. steps_count: 56669231.000000\n",
      "ep 3500: ep_len:1115 episode reward: total was -0.550000. running mean: -16.366770\n",
      "ep 3500: ep_len:781 episode reward: total was 4.070000. running mean: -16.162402\n",
      "ep 3500: ep_len:2994 episode reward: total was 7.570000. running mean: -15.925078\n",
      "ep 3500: ep_len:521 episode reward: total was -24.250000. running mean: -16.008327\n",
      "ep 3500: ep_len:69 episode reward: total was 31.500000. running mean: -15.533244\n",
      "ep 3500: ep_len:618 episode reward: total was -15.240000. running mean: -15.530312\n",
      "ep 3500: ep_len:3952 episode reward: total was -261.650000. running mean: -17.991509\n",
      "ep 3500: ep_len:745 episode reward: total was -44.550000. running mean: -18.257093\n",
      "ep 3500: ep_len:742 episode reward: total was 23.640000. running mean: -17.838123\n",
      "ep 3500: ep_len:500 episode reward: total was 10.590000. running mean: -17.553841\n",
      "ep 3500: ep_len:1092 episode reward: total was 21.220000. running mean: -17.166103\n",
      "ep 3500: ep_len:2800 episode reward: total was -11.500000. running mean: -17.109442\n",
      "epsilon:0.009992 episode_count: 52629. steps_count: 56685160.000000\n",
      "ep 3501: ep_len:672 episode reward: total was -19.160000. running mean: -17.129947\n",
      "ep 3501: ep_len:1254 episode reward: total was -40.150000. running mean: -17.360148\n",
      "ep 3501: ep_len:84 episode reward: total was 39.000000. running mean: -16.796546\n",
      "ep 3501: ep_len:1259 episode reward: total was 1.050000. running mean: -16.618081\n",
      "ep 3501: ep_len:56 episode reward: total was 26.500000. running mean: -16.186900\n",
      "ep 3501: ep_len:102 episode reward: total was 46.500000. running mean: -15.560031\n",
      "ep 3501: ep_len:43 episode reward: total was 18.500000. running mean: -15.219431\n",
      "ep 3501: ep_len:1041 episode reward: total was -15.010000. running mean: -15.217337\n",
      "ep 3501: ep_len:345 episode reward: total was 19.090000. running mean: -14.874263\n",
      "ep 3501: ep_len:664 episode reward: total was -8.880000. running mean: -14.814321\n",
      "ep 3501: ep_len:7224 episode reward: total was 39.110000. running mean: -14.275077\n",
      "ep 3501: ep_len:588 episode reward: total was 36.680000. running mean: -13.765527\n",
      "ep 3501: ep_len:55 episode reward: total was 26.000000. running mean: -13.367871\n",
      "ep 3501: ep_len:67 episode reward: total was 32.000000. running mean: -12.914193\n",
      "ep 3501: ep_len:601 episode reward: total was -0.700000. running mean: -12.792051\n",
      "ep 3501: ep_len:2897 episode reward: total was -24.020000. running mean: -12.904330\n",
      "ep 3501: ep_len:41 episode reward: total was 19.000000. running mean: -12.585287\n",
      "epsilon:0.009992 episode_count: 52646. steps_count: 56702153.000000\n",
      "ep 3502: ep_len:1128 episode reward: total was 6.650000. running mean: -12.392934\n",
      "ep 3502: ep_len:845 episode reward: total was -0.460000. running mean: -12.273605\n",
      "ep 3502: ep_len:35 episode reward: total was 16.000000. running mean: -11.990869\n",
      "ep 3502: ep_len:3001 episode reward: total was -10.630000. running mean: -11.977260\n",
      "ep 3502: ep_len:776 episode reward: total was 13.860000. running mean: -11.718887\n",
      "ep 3502: ep_len:58 episode reward: total was 27.500000. running mean: -11.326698\n",
      "ep 3502: ep_len:67 episode reward: total was 32.000000. running mean: -10.893431\n",
      "ep 3502: ep_len:689 episode reward: total was -2.990000. running mean: -10.814397\n",
      "ep 3502: ep_len:341 episode reward: total was 16.540000. running mean: -10.540853\n",
      "ep 3502: ep_len:664 episode reward: total was -67.170000. running mean: -11.107145\n",
      "ep 3502: ep_len:799 episode reward: total was 50.700000. running mean: -10.489073\n",
      "ep 3502: ep_len:901 episode reward: total was 33.390000. running mean: -10.050283\n",
      "ep 3502: ep_len:94 episode reward: total was 45.500000. running mean: -9.494780\n",
      "ep 3502: ep_len:173 episode reward: total was 82.000000. running mean: -8.579832\n",
      "ep 3502: ep_len:99 episode reward: total was 46.500000. running mean: -8.029034\n",
      "ep 3502: ep_len:1072 episode reward: total was 23.540000. running mean: -7.713343\n",
      "ep 3502: ep_len:2893 episode reward: total was 2.320000. running mean: -7.613010\n",
      "epsilon:0.009992 episode_count: 52663. steps_count: 56715788.000000\n",
      "ep 3503: ep_len:1379 episode reward: total was 5.080000. running mean: -7.486080\n",
      "ep 3503: ep_len:724 episode reward: total was -14.370000. running mean: -7.554919\n",
      "ep 3503: ep_len:60 episode reward: total was 28.500000. running mean: -7.194370\n",
      "ep 3503: ep_len:2950 episode reward: total was -85.690000. running mean: -7.979326\n",
      "ep 3503: ep_len:611 episode reward: total was 8.240000. running mean: -7.817133\n",
      "ep 3503: ep_len:155 episode reward: total was 71.500000. running mean: -7.023961\n",
      "ep 3503: ep_len:106 episode reward: total was 50.000000. running mean: -6.453722\n",
      "ep 3503: ep_len:500 episode reward: total was 18.410000. running mean: -6.205085\n",
      "ep 3503: ep_len:639 episode reward: total was 2.090000. running mean: -6.122134\n",
      "ep 3503: ep_len:632 episode reward: total was -40.310000. running mean: -6.464012\n",
      "ep 3503: ep_len:849 episode reward: total was 32.430000. running mean: -6.075072\n",
      "ep 3503: ep_len:1145 episode reward: total was -21.040000. running mean: -6.224722\n",
      "ep 3503: ep_len:70 episode reward: total was 33.500000. running mean: -5.827474\n",
      "ep 3503: ep_len:169 episode reward: total was 81.500000. running mean: -4.954200\n",
      "ep 3503: ep_len:812 episode reward: total was 17.610000. running mean: -4.728558\n",
      "ep 3503: ep_len:2819 episode reward: total was -27.890000. running mean: -4.960172\n",
      "epsilon:0.009992 episode_count: 52679. steps_count: 56729408.000000\n",
      "ep 3504: ep_len:633 episode reward: total was -0.910000. running mean: -4.919670\n",
      "ep 3504: ep_len:964 episode reward: total was 52.640000. running mean: -4.344074\n",
      "ep 3504: ep_len:2963 episode reward: total was -36.000000. running mean: -4.660633\n",
      "ep 3504: ep_len:723 episode reward: total was 14.550000. running mean: -4.468527\n",
      "ep 3504: ep_len:140 episode reward: total was 67.000000. running mean: -3.753841\n",
      "ep 3504: ep_len:69 episode reward: total was 33.000000. running mean: -3.386303\n",
      "ep 3504: ep_len:650 episode reward: total was -4.420000. running mean: -3.396640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3504: ep_len:3933 episode reward: total was -3658.300000. running mean: -39.945673\n",
      "ep 3504: ep_len:2142 episode reward: total was -287.690000. running mean: -42.423117\n",
      "ep 3504: ep_len:7343 episode reward: total was -79.520000. running mean: -42.794086\n",
      "ep 3504: ep_len:674 episode reward: total was 6.580000. running mean: -42.300345\n",
      "ep 3504: ep_len:64 episode reward: total was 30.500000. running mean: -41.572341\n",
      "ep 3504: ep_len:98 episode reward: total was 44.500000. running mean: -40.711618\n",
      "ep 3504: ep_len:624 episode reward: total was -58.480000. running mean: -40.889302\n",
      "ep 3504: ep_len:2909 episode reward: total was 3.400000. running mean: -40.446409\n",
      "ep 3504: ep_len:42 episode reward: total was 19.500000. running mean: -39.846945\n",
      "epsilon:0.009992 episode_count: 52695. steps_count: 56753379.000000\n",
      "ep 3505: ep_len:1443 episode reward: total was 8.840000. running mean: -39.360075\n",
      "ep 3505: ep_len:726 episode reward: total was -22.430000. running mean: -39.190774\n",
      "ep 3505: ep_len:82 episode reward: total was 39.500000. running mean: -38.403867\n",
      "ep 3505: ep_len:2963 episode reward: total was -20.980000. running mean: -38.229628\n",
      "ep 3505: ep_len:516 episode reward: total was -16.650000. running mean: -38.013832\n",
      "ep 3505: ep_len:69 episode reward: total was 33.000000. running mean: -37.303693\n",
      "ep 3505: ep_len:83 episode reward: total was 40.000000. running mean: -36.530656\n",
      "ep 3505: ep_len:500 episode reward: total was 2.390000. running mean: -36.141450\n",
      "ep 3505: ep_len:312 episode reward: total was 11.930000. running mean: -35.660735\n",
      "ep 3505: ep_len:1284 episode reward: total was -69.630000. running mean: -36.000428\n",
      "ep 3505: ep_len:762 episode reward: total was -0.080000. running mean: -35.641224\n",
      "ep 3505: ep_len:627 episode reward: total was -0.970000. running mean: -35.294511\n",
      "ep 3505: ep_len:74 episode reward: total was 32.500000. running mean: -34.616566\n",
      "ep 3505: ep_len:119 episode reward: total was 58.000000. running mean: -33.690401\n",
      "ep 3505: ep_len:1081 episode reward: total was 33.720000. running mean: -33.016297\n",
      "ep 3505: ep_len:35 episode reward: total was 16.000000. running mean: -32.526134\n",
      "ep 3505: ep_len:49 episode reward: total was 21.500000. running mean: -31.985872\n",
      "epsilon:0.009992 episode_count: 52712. steps_count: 56764104.000000\n",
      "ep 3506: ep_len:576 episode reward: total was -4.980000. running mean: -31.715814\n",
      "ep 3506: ep_len:201 episode reward: total was 17.890000. running mean: -31.219756\n",
      "ep 3506: ep_len:45 episode reward: total was 21.000000. running mean: -30.697558\n",
      "ep 3506: ep_len:2990 episode reward: total was -20.930000. running mean: -30.599882\n",
      "ep 3506: ep_len:852 episode reward: total was 28.770000. running mean: -30.006184\n",
      "ep 3506: ep_len:152 episode reward: total was 73.000000. running mean: -28.976122\n",
      "ep 3506: ep_len:66 episode reward: total was 31.500000. running mean: -28.371361\n",
      "ep 3506: ep_len:68 episode reward: total was 32.500000. running mean: -27.762647\n",
      "ep 3506: ep_len:585 episode reward: total was 44.380000. running mean: -27.041220\n",
      "ep 3506: ep_len:310 episode reward: total was 6.770000. running mean: -26.703108\n",
      "ep 3506: ep_len:540 episode reward: total was -28.100000. running mean: -26.717077\n",
      "ep 3506: ep_len:794 episode reward: total was 45.960000. running mean: -25.990306\n",
      "ep 3506: ep_len:1001 episode reward: total was 10.610000. running mean: -25.624303\n",
      "ep 3506: ep_len:132 episode reward: total was 63.000000. running mean: -24.738060\n",
      "ep 3506: ep_len:58 episode reward: total was 24.500000. running mean: -24.245680\n",
      "ep 3506: ep_len:100 episode reward: total was 48.500000. running mean: -23.518223\n",
      "ep 3506: ep_len:868 episode reward: total was 25.640000. running mean: -23.026641\n",
      "ep 3506: ep_len:2840 episode reward: total was -5.010000. running mean: -22.846474\n",
      "ep 3506: ep_len:40 episode reward: total was 18.500000. running mean: -22.433010\n",
      "epsilon:0.009992 episode_count: 52731. steps_count: 56776322.000000\n",
      "ep 3507: ep_len:1413 episode reward: total was 22.010000. running mean: -21.988579\n",
      "ep 3507: ep_len:973 episode reward: total was 36.320000. running mean: -21.405494\n",
      "ep 3507: ep_len:67 episode reward: total was 30.500000. running mean: -20.886439\n",
      "ep 3507: ep_len:2913 episode reward: total was -23.100000. running mean: -20.908574\n",
      "ep 3507: ep_len:1679 episode reward: total was -28.190000. running mean: -20.981389\n",
      "ep 3507: ep_len:36 episode reward: total was 15.000000. running mean: -20.621575\n",
      "ep 3507: ep_len:80 episode reward: total was 38.500000. running mean: -20.030359\n",
      "ep 3507: ep_len:59 episode reward: total was 26.500000. running mean: -19.565055\n",
      "ep 3507: ep_len:500 episode reward: total was 17.940000. running mean: -19.190005\n",
      "ep 3507: ep_len:3881 episode reward: total was -848.250000. running mean: -27.480605\n",
      "ep 3507: ep_len:1525 episode reward: total was -19.020000. running mean: -27.395999\n",
      "ep 3507: ep_len:790 episode reward: total was 12.520000. running mean: -26.996839\n",
      "ep 3507: ep_len:559 episode reward: total was 31.000000. running mean: -26.416870\n",
      "ep 3507: ep_len:27 episode reward: total was 12.000000. running mean: -26.032702\n",
      "ep 3507: ep_len:752 episode reward: total was -24.900000. running mean: -26.021375\n",
      "ep 3507: ep_len:2837 episode reward: total was -0.410000. running mean: -25.765261\n",
      "epsilon:0.009992 episode_count: 52747. steps_count: 56794413.000000\n",
      "ep 3508: ep_len:613 episode reward: total was -1.360000. running mean: -25.521208\n",
      "ep 3508: ep_len:1602 episode reward: total was -16.470000. running mean: -25.430696\n",
      "ep 3508: ep_len:2920 episode reward: total was -24.590000. running mean: -25.422289\n",
      "ep 3508: ep_len:671 episode reward: total was 8.010000. running mean: -25.087966\n",
      "ep 3508: ep_len:47 episode reward: total was 22.000000. running mean: -24.617087\n",
      "ep 3508: ep_len:149 episode reward: total was 70.000000. running mean: -23.670916\n",
      "ep 3508: ep_len:75 episode reward: total was 36.000000. running mean: -23.074207\n",
      "ep 3508: ep_len:45 episode reward: total was 21.000000. running mean: -22.633465\n",
      "ep 3508: ep_len:1414 episode reward: total was -31.970000. running mean: -22.726830\n",
      "ep 3508: ep_len:658 episode reward: total was 30.230000. running mean: -22.197262\n",
      "ep 3508: ep_len:2139 episode reward: total was -491.860000. running mean: -26.893889\n",
      "ep 3508: ep_len:889 episode reward: total was 52.980000. running mean: -26.095150\n",
      "ep 3508: ep_len:1181 episode reward: total was -13.610000. running mean: -25.970299\n",
      "ep 3508: ep_len:1208 episode reward: total was -315.730000. running mean: -28.867896\n",
      "ep 3508: ep_len:2852 episode reward: total was -31.670000. running mean: -28.895917\n",
      "epsilon:0.009992 episode_count: 52762. steps_count: 56810876.000000\n",
      "ep 3509: ep_len:1057 episode reward: total was -13.420000. running mean: -28.741157\n",
      "ep 3509: ep_len:929 episode reward: total was 6.340000. running mean: -28.390346\n",
      "ep 3509: ep_len:3047 episode reward: total was -8.650000. running mean: -28.192942\n",
      "ep 3509: ep_len:500 episode reward: total was 8.650000. running mean: -27.824513\n",
      "ep 3509: ep_len:822 episode reward: total was 36.700000. running mean: -27.179268\n",
      "ep 3509: ep_len:311 episode reward: total was -122.130000. running mean: -28.128775\n",
      "ep 3509: ep_len:570 episode reward: total was -1.540000. running mean: -27.862887\n",
      "ep 3509: ep_len:7353 episode reward: total was -87.870000. running mean: -28.462959\n",
      "ep 3509: ep_len:1487 episode reward: total was 8.420000. running mean: -28.094129\n",
      "ep 3509: ep_len:134 episode reward: total was 65.500000. running mean: -27.158188\n",
      "ep 3509: ep_len:29 episode reward: total was 13.000000. running mean: -26.756606\n",
      "ep 3509: ep_len:114 episode reward: total was 54.000000. running mean: -25.949040\n",
      "ep 3509: ep_len:1157 episode reward: total was -14.860000. running mean: -25.838149\n",
      "ep 3509: ep_len:2814 episode reward: total was -3.830000. running mean: -25.618068\n",
      "ep 3509: ep_len:36 episode reward: total was 16.500000. running mean: -25.196887\n",
      "epsilon:0.009992 episode_count: 52777. steps_count: 56831236.000000\n",
      "ep 3510: ep_len:3530 episode reward: total was -919.890000. running mean: -34.143818\n",
      "ep 3510: ep_len:621 episode reward: total was 12.560000. running mean: -33.676780\n",
      "ep 3510: ep_len:3050 episode reward: total was 9.200000. running mean: -33.248012\n",
      "ep 3510: ep_len:3443 episode reward: total was -338.580000. running mean: -36.301332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3510: ep_len:22 episode reward: total was 9.500000. running mean: -35.843319\n",
      "ep 3510: ep_len:82 episode reward: total was 39.500000. running mean: -35.089886\n",
      "ep 3510: ep_len:80 episode reward: total was 38.500000. running mean: -34.353987\n",
      "ep 3510: ep_len:500 episode reward: total was 10.500000. running mean: -33.905447\n",
      "ep 3510: ep_len:3628 episode reward: total was -35.220000. running mean: -33.918593\n",
      "ep 3510: ep_len:614 episode reward: total was -25.000000. running mean: -33.829407\n",
      "ep 3510: ep_len:722 episode reward: total was 15.390000. running mean: -33.337213\n",
      "ep 3510: ep_len:1098 episode reward: total was -75.050000. running mean: -33.754340\n",
      "ep 3510: ep_len:57 episode reward: total was 27.000000. running mean: -33.146797\n",
      "ep 3510: ep_len:116 episode reward: total was 56.500000. running mean: -32.250329\n",
      "ep 3510: ep_len:750 episode reward: total was -32.730000. running mean: -32.255126\n",
      "ep 3510: ep_len:2924 episode reward: total was -11.450000. running mean: -32.047074\n",
      "ep 3510: ep_len:44 episode reward: total was 20.500000. running mean: -31.521604\n",
      "epsilon:0.009992 episode_count: 52794. steps_count: 56852517.000000\n",
      "ep 3511: ep_len:1031 episode reward: total was -72.680000. running mean: -31.933188\n",
      "ep 3511: ep_len:1338 episode reward: total was -23.150000. running mean: -31.845356\n",
      "ep 3511: ep_len:37 episode reward: total was 17.000000. running mean: -31.356902\n",
      "ep 3511: ep_len:2972 episode reward: total was -26.980000. running mean: -31.313133\n",
      "ep 3511: ep_len:529 episode reward: total was -0.380000. running mean: -31.003802\n",
      "ep 3511: ep_len:115 episode reward: total was 53.000000. running mean: -30.163764\n",
      "ep 3511: ep_len:65 episode reward: total was 29.500000. running mean: -29.567126\n",
      "ep 3511: ep_len:1441 episode reward: total was -110.230000. running mean: -30.373755\n",
      "ep 3511: ep_len:3949 episode reward: total was -122.910000. running mean: -31.299117\n",
      "ep 3511: ep_len:1198 episode reward: total was -24.240000. running mean: -31.228526\n",
      "ep 3511: ep_len:787 episode reward: total was 3.360000. running mean: -30.882641\n",
      "ep 3511: ep_len:1077 episode reward: total was -1.730000. running mean: -30.591115\n",
      "ep 3511: ep_len:189 episode reward: total was 93.000000. running mean: -29.355203\n",
      "ep 3511: ep_len:45 episode reward: total was 19.500000. running mean: -28.866651\n",
      "ep 3511: ep_len:1132 episode reward: total was -23.190000. running mean: -28.809885\n",
      "ep 3511: ep_len:2788 episode reward: total was -36.010000. running mean: -28.881886\n",
      "ep 3511: ep_len:39 episode reward: total was 18.000000. running mean: -28.413067\n",
      "epsilon:0.009992 episode_count: 52811. steps_count: 56871249.000000\n",
      "ep 3512: ep_len:709 episode reward: total was -91.050000. running mean: -29.039437\n",
      "ep 3512: ep_len:1643 episode reward: total was -17.800000. running mean: -28.927042\n",
      "ep 3512: ep_len:55 episode reward: total was 26.000000. running mean: -28.377772\n",
      "ep 3512: ep_len:2999 episode reward: total was -40.620000. running mean: -28.500194\n",
      "ep 3512: ep_len:1635 episode reward: total was -21.530000. running mean: -28.430492\n",
      "ep 3512: ep_len:644 episode reward: total was -40.750000. running mean: -28.553687\n",
      "ep 3512: ep_len:579 episode reward: total was -40.100000. running mean: -28.669150\n",
      "ep 3512: ep_len:2557 episode reward: total was -194.640000. running mean: -30.328859\n",
      "ep 3512: ep_len:855 episode reward: total was 64.430000. running mean: -29.381270\n",
      "ep 3512: ep_len:980 episode reward: total was 20.530000. running mean: -28.882157\n",
      "ep 3512: ep_len:89 episode reward: total was 41.500000. running mean: -28.178336\n",
      "ep 3512: ep_len:53 episode reward: total was 25.000000. running mean: -27.646553\n",
      "ep 3512: ep_len:1124 episode reward: total was -17.210000. running mean: -27.542187\n",
      "ep 3512: ep_len:46 episode reward: total was 21.500000. running mean: -27.051765\n",
      "epsilon:0.009992 episode_count: 52825. steps_count: 56885217.000000\n",
      "ep 3513: ep_len:683 episode reward: total was -5.460000. running mean: -26.835848\n",
      "ep 3513: ep_len:1623 episode reward: total was -12.250000. running mean: -26.689989\n",
      "ep 3513: ep_len:2940 episode reward: total was -38.220000. running mean: -26.805289\n",
      "ep 3513: ep_len:794 episode reward: total was -29.600000. running mean: -26.833236\n",
      "ep 3513: ep_len:1404 episode reward: total was -164.680000. running mean: -28.211704\n",
      "ep 3513: ep_len:3842 episode reward: total was -42.080000. running mean: -28.350387\n",
      "ep 3513: ep_len:558 episode reward: total was 3.730000. running mean: -28.029583\n",
      "ep 3513: ep_len:666 episode reward: total was 6.770000. running mean: -27.681587\n",
      "ep 3513: ep_len:619 episode reward: total was 28.820000. running mean: -27.116571\n",
      "ep 3513: ep_len:72 episode reward: total was 34.500000. running mean: -26.500406\n",
      "ep 3513: ep_len:1527 episode reward: total was 12.710000. running mean: -26.108302\n",
      "ep 3513: ep_len:2730 episode reward: total was -0.110000. running mean: -25.848318\n",
      "ep 3513: ep_len:40 episode reward: total was 18.500000. running mean: -25.404835\n",
      "epsilon:0.009992 episode_count: 52838. steps_count: 56902715.000000\n",
      "ep 3514: ep_len:1029 episode reward: total was -97.950000. running mean: -26.130287\n",
      "ep 3514: ep_len:763 episode reward: total was 16.470000. running mean: -25.704284\n",
      "ep 3514: ep_len:48 episode reward: total was 22.500000. running mean: -25.222241\n",
      "ep 3514: ep_len:3002 episode reward: total was -9.620000. running mean: -25.066219\n",
      "ep 3514: ep_len:822 episode reward: total was 12.210000. running mean: -24.693457\n",
      "ep 3514: ep_len:59 episode reward: total was 26.500000. running mean: -24.181522\n",
      "ep 3514: ep_len:56 episode reward: total was 25.000000. running mean: -23.689707\n",
      "ep 3514: ep_len:1073 episode reward: total was -9.640000. running mean: -23.549210\n",
      "ep 3514: ep_len:639 episode reward: total was 34.910000. running mean: -22.964618\n",
      "ep 3514: ep_len:1584 episode reward: total was -42.890000. running mean: -23.163872\n",
      "ep 3514: ep_len:7278 episode reward: total was 25.970000. running mean: -22.672533\n",
      "ep 3514: ep_len:614 episode reward: total was -11.180000. running mean: -22.557607\n",
      "ep 3514: ep_len:65 episode reward: total was 29.500000. running mean: -22.037031\n",
      "ep 3514: ep_len:1114 episode reward: total was 10.460000. running mean: -21.712061\n",
      "ep 3514: ep_len:2846 episode reward: total was -26.770000. running mean: -21.762640\n",
      "epsilon:0.009992 episode_count: 52853. steps_count: 56923707.000000\n",
      "ep 3515: ep_len:773 episode reward: total was -87.870000. running mean: -22.423714\n",
      "ep 3515: ep_len:622 episode reward: total was 13.580000. running mean: -22.063677\n",
      "ep 3515: ep_len:2874 episode reward: total was -50.150000. running mean: -22.344540\n",
      "ep 3515: ep_len:1482 episode reward: total was 34.240000. running mean: -21.778695\n",
      "ep 3515: ep_len:82 episode reward: total was 36.500000. running mean: -21.195908\n",
      "ep 3515: ep_len:72 episode reward: total was 34.500000. running mean: -20.638949\n",
      "ep 3515: ep_len:769 episode reward: total was -20.520000. running mean: -20.637759\n",
      "ep 3515: ep_len:4028 episode reward: total was -58.130000. running mean: -21.012682\n",
      "ep 3515: ep_len:948 episode reward: total was -40.180000. running mean: -21.204355\n",
      "ep 3515: ep_len:652 episode reward: total was -15.840000. running mean: -21.150711\n",
      "ep 3515: ep_len:1107 episode reward: total was 7.380000. running mean: -20.865404\n",
      "ep 3515: ep_len:49 episode reward: total was 23.000000. running mean: -20.426750\n",
      "ep 3515: ep_len:652 episode reward: total was -2.740000. running mean: -20.249883\n",
      "ep 3515: ep_len:2810 episode reward: total was -4.390000. running mean: -20.091284\n",
      "epsilon:0.009992 episode_count: 52867. steps_count: 56940627.000000\n",
      "ep 3516: ep_len:660 episode reward: total was 9.200000. running mean: -19.798371\n",
      "ep 3516: ep_len:755 episode reward: total was -7.770000. running mean: -19.678087\n",
      "ep 3516: ep_len:79 episode reward: total was 35.000000. running mean: -19.131306\n",
      "ep 3516: ep_len:2965 episode reward: total was -41.520000. running mean: -19.355193\n",
      "ep 3516: ep_len:717 episode reward: total was 26.510000. running mean: -18.896541\n",
      "ep 3516: ep_len:67 episode reward: total was 32.000000. running mean: -18.387576\n",
      "ep 3516: ep_len:954 episode reward: total was -13.520000. running mean: -18.338900\n",
      "ep 3516: ep_len:3957 episode reward: total was -80.390000. running mean: -18.959411\n",
      "ep 3516: ep_len:1239 episode reward: total was -41.680000. running mean: -19.186617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3516: ep_len:846 episode reward: total was 21.630000. running mean: -18.778451\n",
      "ep 3516: ep_len:631 episode reward: total was -38.370000. running mean: -18.974366\n",
      "ep 3516: ep_len:57 episode reward: total was 25.500000. running mean: -18.529623\n",
      "ep 3516: ep_len:108 episode reward: total was 51.000000. running mean: -17.834327\n",
      "ep 3516: ep_len:1109 episode reward: total was 0.420000. running mean: -17.651783\n",
      "ep 3516: ep_len:2830 episode reward: total was -38.380000. running mean: -17.859065\n",
      "epsilon:0.009992 episode_count: 52882. steps_count: 56957601.000000\n",
      "ep 3517: ep_len:759 episode reward: total was -80.450000. running mean: -18.484975\n",
      "ep 3517: ep_len:660 episode reward: total was -38.440000. running mean: -18.684525\n",
      "ep 3517: ep_len:3028 episode reward: total was -130.510000. running mean: -19.802780\n",
      "ep 3517: ep_len:613 episode reward: total was -6.770000. running mean: -19.672452\n",
      "ep 3517: ep_len:66 episode reward: total was 25.500000. running mean: -19.220727\n",
      "ep 3517: ep_len:60 episode reward: total was 27.000000. running mean: -18.758520\n",
      "ep 3517: ep_len:979 episode reward: total was -3.510000. running mean: -18.606035\n",
      "ep 3517: ep_len:3980 episode reward: total was -70.060000. running mean: -19.120575\n",
      "ep 3517: ep_len:675 episode reward: total was -27.760000. running mean: -19.206969\n",
      "ep 3517: ep_len:815 episode reward: total was 58.450000. running mean: -18.430399\n",
      "ep 3517: ep_len:746 episode reward: total was -10.890000. running mean: -18.354995\n",
      "ep 3517: ep_len:39 episode reward: total was 18.000000. running mean: -17.991445\n",
      "ep 3517: ep_len:625 episode reward: total was -20.180000. running mean: -18.013331\n",
      "ep 3517: ep_len:2882 episode reward: total was -9.670000. running mean: -17.929897\n",
      "ep 3517: ep_len:44 episode reward: total was 20.500000. running mean: -17.545599\n",
      "epsilon:0.009992 episode_count: 52897. steps_count: 56973572.000000\n",
      "ep 3518: ep_len:1095 episode reward: total was -4.370000. running mean: -17.413843\n",
      "ep 3518: ep_len:1100 episode reward: total was 9.270000. running mean: -17.147004\n",
      "ep 3518: ep_len:80 episode reward: total was 38.500000. running mean: -16.590534\n",
      "ep 3518: ep_len:2957 episode reward: total was -39.680000. running mean: -16.821429\n",
      "ep 3518: ep_len:1135 episode reward: total was -11.040000. running mean: -16.763614\n",
      "ep 3518: ep_len:1846 episode reward: total was -59.330000. running mean: -17.189278\n",
      "ep 3518: ep_len:638 episode reward: total was -22.400000. running mean: -17.241386\n",
      "ep 3518: ep_len:563 episode reward: total was -7.670000. running mean: -17.145672\n",
      "ep 3518: ep_len:826 episode reward: total was 32.810000. running mean: -16.646115\n",
      "ep 3518: ep_len:1038 episode reward: total was 29.590000. running mean: -16.183754\n",
      "ep 3518: ep_len:649 episode reward: total was 9.290000. running mean: -15.929016\n",
      "ep 3518: ep_len:2910 episode reward: total was -2.780000. running mean: -15.797526\n",
      "epsilon:0.009992 episode_count: 52909. steps_count: 56988409.000000\n",
      "ep 3519: ep_len:500 episode reward: total was 5.080000. running mean: -15.588751\n",
      "ep 3519: ep_len:636 episode reward: total was 12.220000. running mean: -15.310663\n",
      "ep 3519: ep_len:49 episode reward: total was 23.000000. running mean: -14.927557\n",
      "ep 3519: ep_len:2919 episode reward: total was -48.200000. running mean: -15.260281\n",
      "ep 3519: ep_len:680 episode reward: total was -10.220000. running mean: -15.209878\n",
      "ep 3519: ep_len:630 episode reward: total was 36.050000. running mean: -14.697280\n",
      "ep 3519: ep_len:3895 episode reward: total was -2534.550000. running mean: -39.895807\n",
      "ep 3519: ep_len:529 episode reward: total was -11.410000. running mean: -39.610949\n",
      "ep 3519: ep_len:676 episode reward: total was 28.560000. running mean: -38.929239\n",
      "ep 3519: ep_len:790 episode reward: total was 21.860000. running mean: -38.321347\n",
      "ep 3519: ep_len:101 episode reward: total was 49.000000. running mean: -37.448133\n",
      "ep 3519: ep_len:500 episode reward: total was 36.970000. running mean: -36.703952\n",
      "ep 3519: ep_len:2957 episode reward: total was -8.730000. running mean: -36.424212\n",
      "ep 3519: ep_len:42 episode reward: total was 16.500000. running mean: -35.894970\n",
      "epsilon:0.009992 episode_count: 52923. steps_count: 57003313.000000\n",
      "ep 3520: ep_len:1093 episode reward: total was 19.400000. running mean: -35.342021\n",
      "ep 3520: ep_len:1714 episode reward: total was -27.500000. running mean: -35.263600\n",
      "ep 3520: ep_len:69 episode reward: total was 33.000000. running mean: -34.580964\n",
      "ep 3520: ep_len:2930 episode reward: total was -16.770000. running mean: -34.402855\n",
      "ep 3520: ep_len:738 episode reward: total was -9.960000. running mean: -34.158426\n",
      "ep 3520: ep_len:500 episode reward: total was 20.700000. running mean: -33.609842\n",
      "ep 3520: ep_len:669 episode reward: total was 16.140000. running mean: -33.112344\n",
      "ep 3520: ep_len:784 episode reward: total was -18.750000. running mean: -32.968720\n",
      "ep 3520: ep_len:841 episode reward: total was 33.610000. running mean: -32.302933\n",
      "ep 3520: ep_len:762 episode reward: total was -25.880000. running mean: -32.238704\n",
      "ep 3520: ep_len:131 episode reward: total was 62.500000. running mean: -31.291317\n",
      "ep 3520: ep_len:633 episode reward: total was 5.450000. running mean: -30.923903\n",
      "ep 3520: ep_len:2790 episode reward: total was -26.870000. running mean: -30.883364\n",
      "epsilon:0.009992 episode_count: 52936. steps_count: 57016967.000000\n",
      "ep 3521: ep_len:1447 episode reward: total was 4.140000. running mean: -30.533131\n",
      "ep 3521: ep_len:1608 episode reward: total was -40.160000. running mean: -30.629399\n",
      "ep 3521: ep_len:2957 episode reward: total was -2.970000. running mean: -30.352805\n",
      "ep 3521: ep_len:781 episode reward: total was -21.400000. running mean: -30.263277\n",
      "ep 3521: ep_len:105 episode reward: total was 49.500000. running mean: -29.465645\n",
      "ep 3521: ep_len:50 episode reward: total was 22.000000. running mean: -28.950988\n",
      "ep 3521: ep_len:592 episode reward: total was 33.700000. running mean: -28.324478\n",
      "ep 3521: ep_len:664 episode reward: total was 6.110000. running mean: -27.980133\n",
      "ep 3521: ep_len:943 episode reward: total was -18.040000. running mean: -27.880732\n",
      "ep 3521: ep_len:7495 episode reward: total was 30.070000. running mean: -27.301225\n",
      "ep 3521: ep_len:3531 episode reward: total was -673.880000. running mean: -33.767013\n",
      "ep 3521: ep_len:199 episode reward: total was 94.510000. running mean: -32.484242\n",
      "ep 3521: ep_len:1171 episode reward: total was 11.000000. running mean: -32.049400\n",
      "ep 3521: ep_len:2815 episode reward: total was -13.980000. running mean: -31.868706\n",
      "epsilon:0.009992 episode_count: 52950. steps_count: 57041325.000000\n",
      "ep 3522: ep_len:714 episode reward: total was -60.700000. running mean: -32.157019\n",
      "ep 3522: ep_len:713 episode reward: total was -38.200000. running mean: -32.217449\n",
      "ep 3522: ep_len:52 episode reward: total was 23.000000. running mean: -31.665274\n",
      "ep 3522: ep_len:2951 episode reward: total was -40.960000. running mean: -31.758222\n",
      "ep 3522: ep_len:767 episode reward: total was -13.740000. running mean: -31.578039\n",
      "ep 3522: ep_len:99 episode reward: total was 48.000000. running mean: -30.782259\n",
      "ep 3522: ep_len:61 episode reward: total was 29.000000. running mean: -30.184436\n",
      "ep 3522: ep_len:595 episode reward: total was 42.640000. running mean: -29.456192\n",
      "ep 3522: ep_len:3626 episode reward: total was -101.900000. running mean: -30.180630\n",
      "ep 3522: ep_len:957 episode reward: total was -11.250000. running mean: -29.991324\n",
      "ep 3522: ep_len:868 episode reward: total was 41.140000. running mean: -29.280011\n",
      "ep 3522: ep_len:737 episode reward: total was -40.270000. running mean: -29.389910\n",
      "ep 3522: ep_len:64 episode reward: total was 29.000000. running mean: -28.806011\n",
      "ep 3522: ep_len:763 episode reward: total was -28.900000. running mean: -28.806951\n",
      "ep 3522: ep_len:2819 episode reward: total was -9.010000. running mean: -28.608982\n",
      "epsilon:0.009992 episode_count: 52965. steps_count: 57057111.000000\n",
      "ep 3523: ep_len:639 episode reward: total was 0.160000. running mean: -28.321292\n",
      "ep 3523: ep_len:753 episode reward: total was -11.310000. running mean: -28.151179\n",
      "ep 3523: ep_len:2891 episode reward: total was -50.310000. running mean: -28.372767\n",
      "ep 3523: ep_len:666 episode reward: total was -1.070000. running mean: -28.099739\n",
      "ep 3523: ep_len:110 episode reward: total was 52.000000. running mean: -27.298742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3523: ep_len:110 episode reward: total was 53.500000. running mean: -26.490755\n",
      "ep 3523: ep_len:43 episode reward: total was 18.500000. running mean: -26.040847\n",
      "ep 3523: ep_len:696 episode reward: total was 23.850000. running mean: -25.541939\n",
      "ep 3523: ep_len:3714 episode reward: total was -53.830000. running mean: -25.824819\n",
      "ep 3523: ep_len:856 episode reward: total was 21.920000. running mean: -25.347371\n",
      "ep 3523: ep_len:622 episode reward: total was 10.400000. running mean: -24.989897\n",
      "ep 3523: ep_len:967 episode reward: total was 11.430000. running mean: -24.625698\n",
      "ep 3523: ep_len:74 episode reward: total was 34.000000. running mean: -24.039441\n",
      "ep 3523: ep_len:1442 episode reward: total was 5.620000. running mean: -23.742847\n",
      "ep 3523: ep_len:2821 episode reward: total was -44.280000. running mean: -23.948219\n",
      "epsilon:0.009992 episode_count: 52980. steps_count: 57073515.000000\n",
      "ep 3524: ep_len:1099 episode reward: total was -5.920000. running mean: -23.767936\n",
      "ep 3524: ep_len:751 episode reward: total was -15.890000. running mean: -23.689157\n",
      "ep 3524: ep_len:2908 episode reward: total was -17.980000. running mean: -23.632065\n",
      "ep 3524: ep_len:1088 episode reward: total was -9.950000. running mean: -23.495245\n",
      "ep 3524: ep_len:61 episode reward: total was 29.000000. running mean: -22.970292\n",
      "ep 3524: ep_len:693 episode reward: total was -7.080000. running mean: -22.811389\n",
      "ep 3524: ep_len:3830 episode reward: total was -191.320000. running mean: -24.496475\n",
      "ep 3524: ep_len:600 episode reward: total was -20.430000. running mean: -24.455811\n",
      "ep 3524: ep_len:7272 episode reward: total was 6.780000. running mean: -24.143453\n",
      "ep 3524: ep_len:1012 episode reward: total was 5.270000. running mean: -23.849318\n",
      "ep 3524: ep_len:34 episode reward: total was 15.500000. running mean: -23.455825\n",
      "ep 3524: ep_len:42 episode reward: total was 19.500000. running mean: -23.026267\n",
      "ep 3524: ep_len:61 episode reward: total was 27.500000. running mean: -22.521004\n",
      "ep 3524: ep_len:1157 episode reward: total was -1.730000. running mean: -22.313094\n",
      "ep 3524: ep_len:44 episode reward: total was 19.000000. running mean: -21.899963\n",
      "ep 3524: ep_len:48 episode reward: total was 22.500000. running mean: -21.455963\n",
      "epsilon:0.009992 episode_count: 52996. steps_count: 57094215.000000\n",
      "ep 3525: ep_len:885 episode reward: total was 7.910000. running mean: -21.162304\n",
      "ep 3525: ep_len:500 episode reward: total was 32.120000. running mean: -20.629481\n",
      "ep 3525: ep_len:45 episode reward: total was 21.000000. running mean: -20.213186\n",
      "ep 3525: ep_len:3048 episode reward: total was -36.770000. running mean: -20.378754\n",
      "ep 3525: ep_len:742 episode reward: total was -11.940000. running mean: -20.294367\n",
      "ep 3525: ep_len:60 episode reward: total was 28.500000. running mean: -19.806423\n",
      "ep 3525: ep_len:500 episode reward: total was 35.580000. running mean: -19.252559\n",
      "ep 3525: ep_len:3625 episode reward: total was -11.990000. running mean: -19.179933\n",
      "ep 3525: ep_len:898 episode reward: total was -22.500000. running mean: -19.213134\n",
      "ep 3525: ep_len:875 episode reward: total was 40.200000. running mean: -18.619002\n",
      "ep 3525: ep_len:1160 episode reward: total was -4.210000. running mean: -18.474912\n",
      "ep 3525: ep_len:53 episode reward: total was 23.500000. running mean: -18.055163\n",
      "ep 3525: ep_len:104 episode reward: total was 50.500000. running mean: -17.369612\n",
      "ep 3525: ep_len:816 episode reward: total was 10.950000. running mean: -17.086415\n",
      "ep 3525: ep_len:2826 episode reward: total was -65.100000. running mean: -17.566551\n",
      "epsilon:0.009992 episode_count: 53011. steps_count: 57110352.000000\n",
      "ep 3526: ep_len:666 episode reward: total was -85.200000. running mean: -18.242886\n",
      "ep 3526: ep_len:1106 episode reward: total was -63.730000. running mean: -18.697757\n",
      "ep 3526: ep_len:2917 episode reward: total was -67.250000. running mean: -19.183279\n",
      "ep 3526: ep_len:500 episode reward: total was -20.600000. running mean: -19.197447\n",
      "ep 3526: ep_len:49 episode reward: total was 21.500000. running mean: -18.790472\n",
      "ep 3526: ep_len:46 episode reward: total was 20.000000. running mean: -18.402567\n",
      "ep 3526: ep_len:650 episode reward: total was -1.870000. running mean: -18.237242\n",
      "ep 3526: ep_len:304 episode reward: total was 4.350000. running mean: -18.011369\n",
      "ep 3526: ep_len:555 episode reward: total was -1.690000. running mean: -17.848156\n",
      "ep 3526: ep_len:750 episode reward: total was -17.400000. running mean: -17.843674\n",
      "ep 3526: ep_len:659 episode reward: total was 0.880000. running mean: -17.656437\n",
      "ep 3526: ep_len:77 episode reward: total was 37.000000. running mean: -17.109873\n",
      "ep 3526: ep_len:123 episode reward: total was 55.500000. running mean: -16.383774\n",
      "ep 3526: ep_len:500 episode reward: total was 0.620000. running mean: -16.213736\n",
      "ep 3526: ep_len:46 episode reward: total was 21.500000. running mean: -15.836599\n",
      "ep 3526: ep_len:35 episode reward: total was 16.000000. running mean: -15.518233\n",
      "epsilon:0.009992 episode_count: 53027. steps_count: 57119335.000000\n",
      "ep 3527: ep_len:672 episode reward: total was -36.390000. running mean: -15.726951\n",
      "ep 3527: ep_len:738 episode reward: total was -10.970000. running mean: -15.679381\n",
      "ep 3527: ep_len:46 episode reward: total was 21.500000. running mean: -15.307587\n",
      "ep 3527: ep_len:2875 episode reward: total was -37.440000. running mean: -15.528912\n",
      "ep 3527: ep_len:1250 episode reward: total was -34.500000. running mean: -15.718622\n",
      "ep 3527: ep_len:73 episode reward: total was 35.000000. running mean: -15.211436\n",
      "ep 3527: ep_len:500 episode reward: total was 22.660000. running mean: -14.832722\n",
      "ep 3527: ep_len:636 episode reward: total was 17.730000. running mean: -14.507095\n",
      "ep 3527: ep_len:583 episode reward: total was -3.430000. running mean: -14.396324\n",
      "ep 3527: ep_len:7377 episode reward: total was 41.860000. running mean: -13.833760\n",
      "ep 3527: ep_len:1036 episode reward: total was -41.170000. running mean: -14.107123\n",
      "ep 3527: ep_len:68 episode reward: total was 32.500000. running mean: -13.641052\n",
      "ep 3527: ep_len:785 episode reward: total was -5.380000. running mean: -13.558441\n",
      "ep 3527: ep_len:2804 episode reward: total was -41.480000. running mean: -13.837657\n",
      "ep 3527: ep_len:52 episode reward: total was 23.000000. running mean: -13.469280\n",
      "epsilon:0.009992 episode_count: 53042. steps_count: 57138830.000000\n",
      "ep 3528: ep_len:695 episode reward: total was -46.840000. running mean: -13.802987\n",
      "ep 3528: ep_len:1171 episode reward: total was -12.640000. running mean: -13.791357\n",
      "ep 3528: ep_len:2909 episode reward: total was -29.050000. running mean: -13.943944\n",
      "ep 3528: ep_len:843 episode reward: total was 10.790000. running mean: -13.696604\n",
      "ep 3528: ep_len:101 episode reward: total was 46.000000. running mean: -13.099638\n",
      "ep 3528: ep_len:64 episode reward: total was 29.000000. running mean: -12.678642\n",
      "ep 3528: ep_len:66 episode reward: total was 30.000000. running mean: -12.251856\n",
      "ep 3528: ep_len:1389 episode reward: total was -252.490000. running mean: -14.654237\n",
      "ep 3528: ep_len:3708 episode reward: total was -40.480000. running mean: -14.912495\n",
      "ep 3528: ep_len:1250 episode reward: total was -65.900000. running mean: -15.422370\n",
      "ep 3528: ep_len:7253 episode reward: total was 27.160000. running mean: -14.996546\n",
      "ep 3528: ep_len:629 episode reward: total was -5.230000. running mean: -14.898881\n",
      "ep 3528: ep_len:81 episode reward: total was 39.000000. running mean: -14.359892\n",
      "ep 3528: ep_len:103 episode reward: total was 48.500000. running mean: -13.731293\n",
      "ep 3528: ep_len:27 episode reward: total was 12.000000. running mean: -13.473980\n",
      "ep 3528: ep_len:646 episode reward: total was -15.140000. running mean: -13.490640\n",
      "ep 3528: ep_len:2813 episode reward: total was -1.750000. running mean: -13.373234\n",
      "epsilon:0.009992 episode_count: 53059. steps_count: 57162578.000000\n",
      "ep 3529: ep_len:1117 episode reward: total was 3.930000. running mean: -13.200201\n",
      "ep 3529: ep_len:1590 episode reward: total was -18.580000. running mean: -13.253999\n",
      "ep 3529: ep_len:2963 episode reward: total was -10.340000. running mean: -13.224859\n",
      "ep 3529: ep_len:770 episode reward: total was -15.700000. running mean: -13.249611\n",
      "ep 3529: ep_len:1106 episode reward: total was -6.280000. running mean: -13.179915\n",
      "ep 3529: ep_len:665 episode reward: total was 2.050000. running mean: -13.027616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3529: ep_len:562 episode reward: total was 0.710000. running mean: -12.890239\n",
      "ep 3529: ep_len:639 episode reward: total was 9.930000. running mean: -12.662037\n",
      "ep 3529: ep_len:1084 episode reward: total was -0.930000. running mean: -12.544717\n",
      "ep 3529: ep_len:55 episode reward: total was 26.000000. running mean: -12.159269\n",
      "ep 3529: ep_len:105 episode reward: total was 51.000000. running mean: -11.527677\n",
      "ep 3529: ep_len:622 episode reward: total was -4.060000. running mean: -11.453000\n",
      "ep 3529: ep_len:2778 episode reward: total was -28.860000. running mean: -11.627070\n",
      "ep 3529: ep_len:45 episode reward: total was 21.000000. running mean: -11.300799\n",
      "epsilon:0.009992 episode_count: 53073. steps_count: 57176679.000000\n",
      "ep 3530: ep_len:959 episode reward: total was -66.810000. running mean: -11.855891\n",
      "ep 3530: ep_len:787 episode reward: total was -96.260000. running mean: -12.699932\n",
      "ep 3530: ep_len:57 episode reward: total was 27.000000. running mean: -12.302933\n",
      "ep 3530: ep_len:3004 episode reward: total was -35.860000. running mean: -12.538504\n",
      "ep 3530: ep_len:822 episode reward: total was 39.520000. running mean: -12.017919\n",
      "ep 3530: ep_len:59 episode reward: total was 28.000000. running mean: -11.617740\n",
      "ep 3530: ep_len:88 episode reward: total was 42.500000. running mean: -11.076562\n",
      "ep 3530: ep_len:613 episode reward: total was 41.200000. running mean: -10.553796\n",
      "ep 3530: ep_len:3867 episode reward: total was -400.850000. running mean: -14.456759\n",
      "ep 3530: ep_len:1244 episode reward: total was -27.120000. running mean: -14.583391\n",
      "ep 3530: ep_len:818 episode reward: total was 41.920000. running mean: -14.018357\n",
      "ep 3530: ep_len:718 episode reward: total was 27.530000. running mean: -13.602873\n",
      "ep 3530: ep_len:41 episode reward: total was 19.000000. running mean: -13.276845\n",
      "ep 3530: ep_len:91 episode reward: total was 44.000000. running mean: -12.704076\n",
      "ep 3530: ep_len:682 episode reward: total was -8.120000. running mean: -12.658236\n",
      "ep 3530: ep_len:2869 episode reward: total was -0.400000. running mean: -12.535653\n",
      "ep 3530: ep_len:65 episode reward: total was 31.000000. running mean: -12.100297\n",
      "epsilon:0.009992 episode_count: 53090. steps_count: 57193463.000000\n",
      "ep 3531: ep_len:655 episode reward: total was -18.280000. running mean: -12.162094\n",
      "ep 3531: ep_len:970 episode reward: total was -13.920000. running mean: -12.179673\n",
      "ep 3531: ep_len:46 episode reward: total was 20.000000. running mean: -11.857876\n",
      "ep 3531: ep_len:2877 episode reward: total was -19.810000. running mean: -11.937397\n",
      "ep 3531: ep_len:2865 episode reward: total was -339.480000. running mean: -15.212823\n",
      "ep 3531: ep_len:118 episode reward: total was 56.000000. running mean: -14.500695\n",
      "ep 3531: ep_len:70 episode reward: total was 32.000000. running mean: -14.035688\n",
      "ep 3531: ep_len:1013 episode reward: total was -55.690000. running mean: -14.452231\n",
      "ep 3531: ep_len:3932 episode reward: total was -116.010000. running mean: -15.467809\n",
      "ep 3531: ep_len:1595 episode reward: total was -1.390000. running mean: -15.327031\n",
      "ep 3531: ep_len:792 episode reward: total was 33.390000. running mean: -14.839860\n",
      "ep 3531: ep_len:639 episode reward: total was 11.140000. running mean: -14.580062\n",
      "ep 3531: ep_len:134 episode reward: total was 64.000000. running mean: -13.794261\n",
      "ep 3531: ep_len:117 episode reward: total was 57.000000. running mean: -13.086319\n",
      "ep 3531: ep_len:606 episode reward: total was -9.930000. running mean: -13.054755\n",
      "ep 3531: ep_len:2898 episode reward: total was -53.860000. running mean: -13.462808\n",
      "epsilon:0.009992 episode_count: 53106. steps_count: 57212790.000000\n",
      "ep 3532: ep_len:1135 episode reward: total was -1.950000. running mean: -13.347680\n",
      "ep 3532: ep_len:647 episode reward: total was -35.790000. running mean: -13.572103\n",
      "ep 3532: ep_len:2975 episode reward: total was -62.230000. running mean: -14.058682\n",
      "ep 3532: ep_len:783 episode reward: total was -29.710000. running mean: -14.215195\n",
      "ep 3532: ep_len:1544 episode reward: total was -7.280000. running mean: -14.145843\n",
      "ep 3532: ep_len:330 episode reward: total was 7.950000. running mean: -13.924885\n",
      "ep 3532: ep_len:500 episode reward: total was 25.480000. running mean: -13.530836\n",
      "ep 3532: ep_len:665 episode reward: total was 4.000000. running mean: -13.355528\n",
      "ep 3532: ep_len:1472 episode reward: total was 19.020000. running mean: -13.031772\n",
      "ep 3532: ep_len:81 episode reward: total was 39.000000. running mean: -12.511455\n",
      "ep 3532: ep_len:1110 episode reward: total was 25.760000. running mean: -12.128740\n",
      "ep 3532: ep_len:2740 episode reward: total was -82.890000. running mean: -12.836353\n",
      "epsilon:0.009992 episode_count: 53118. steps_count: 57226772.000000\n",
      "ep 3533: ep_len:1160 episode reward: total was 29.380000. running mean: -12.414189\n",
      "ep 3533: ep_len:684 episode reward: total was -8.020000. running mean: -12.370247\n",
      "ep 3533: ep_len:67 episode reward: total was 29.000000. running mean: -11.956545\n",
      "ep 3533: ep_len:2925 episode reward: total was -0.300000. running mean: -11.839979\n",
      "ep 3533: ep_len:595 episode reward: total was -36.640000. running mean: -12.087980\n",
      "ep 3533: ep_len:55 episode reward: total was 24.500000. running mean: -11.722100\n",
      "ep 3533: ep_len:106 episode reward: total was 51.500000. running mean: -11.089879\n",
      "ep 3533: ep_len:42 episode reward: total was 19.500000. running mean: -10.783980\n",
      "ep 3533: ep_len:804 episode reward: total was -32.990000. running mean: -11.006040\n",
      "ep 3533: ep_len:3800 episode reward: total was 3.370000. running mean: -10.862280\n",
      "ep 3533: ep_len:958 episode reward: total was -35.030000. running mean: -11.103957\n",
      "ep 3533: ep_len:7496 episode reward: total was -178.820000. running mean: -12.781117\n",
      "ep 3533: ep_len:1512 episode reward: total was 20.770000. running mean: -12.445606\n",
      "ep 3533: ep_len:73 episode reward: total was 35.000000. running mean: -11.971150\n",
      "ep 3533: ep_len:716 episode reward: total was -23.310000. running mean: -12.084539\n",
      "ep 3533: ep_len:2830 episode reward: total was -20.160000. running mean: -12.165293\n",
      "epsilon:0.009992 episode_count: 53134. steps_count: 57250595.000000\n",
      "ep 3534: ep_len:1065 episode reward: total was 12.010000. running mean: -11.923540\n",
      "ep 3534: ep_len:995 episode reward: total was 14.560000. running mean: -11.658705\n",
      "ep 3534: ep_len:2927 episode reward: total was -26.900000. running mean: -11.811118\n",
      "ep 3534: ep_len:502 episode reward: total was -117.850000. running mean: -12.871507\n",
      "ep 3534: ep_len:131 episode reward: total was 61.000000. running mean: -12.132792\n",
      "ep 3534: ep_len:65 episode reward: total was 29.500000. running mean: -11.716464\n",
      "ep 3534: ep_len:1087 episode reward: total was -6.470000. running mean: -11.663999\n",
      "ep 3534: ep_len:3947 episode reward: total was -31.340000. running mean: -11.860759\n",
      "ep 3534: ep_len:1572 episode reward: total was -9.090000. running mean: -11.833051\n",
      "ep 3534: ep_len:7404 episode reward: total was 11.830000. running mean: -11.596421\n",
      "ep 3534: ep_len:933 episode reward: total was 20.310000. running mean: -11.277357\n",
      "ep 3534: ep_len:143 episode reward: total was 70.000000. running mean: -10.464583\n",
      "ep 3534: ep_len:39 episode reward: total was 18.000000. running mean: -10.179937\n",
      "ep 3534: ep_len:586 episode reward: total was 9.260000. running mean: -9.985538\n",
      "ep 3534: ep_len:2903 episode reward: total was -6.000000. running mean: -9.945683\n",
      "ep 3534: ep_len:71 episode reward: total was 34.000000. running mean: -9.506226\n",
      "epsilon:0.009992 episode_count: 53150. steps_count: 57274965.000000\n",
      "ep 3535: ep_len:697 episode reward: total was -0.270000. running mean: -9.413864\n",
      "ep 3535: ep_len:1243 episode reward: total was -40.260000. running mean: -9.722325\n",
      "ep 3535: ep_len:57 episode reward: total was 24.000000. running mean: -9.385102\n",
      "ep 3535: ep_len:2989 episode reward: total was -49.700000. running mean: -9.788251\n",
      "ep 3535: ep_len:1206 episode reward: total was -23.120000. running mean: -9.921568\n",
      "ep 3535: ep_len:36 episode reward: total was 15.000000. running mean: -9.672352\n",
      "ep 3535: ep_len:1432 episode reward: total was -133.680000. running mean: -10.912429\n",
      "ep 3535: ep_len:3535 episode reward: total was -59.470000. running mean: -11.398005\n",
      "ep 3535: ep_len:1263 episode reward: total was -29.960000. running mean: -11.583625\n",
      "ep 3535: ep_len:770 episode reward: total was 3.860000. running mean: -11.429188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3535: ep_len:1128 episode reward: total was -7.070000. running mean: -11.385596\n",
      "ep 3535: ep_len:85 episode reward: total was 38.000000. running mean: -10.891740\n",
      "ep 3535: ep_len:212 episode reward: total was 100.000000. running mean: -9.782823\n",
      "ep 3535: ep_len:765 episode reward: total was -21.620000. running mean: -9.901195\n",
      "ep 3535: ep_len:2893 episode reward: total was -38.780000. running mean: -10.189983\n",
      "ep 3535: ep_len:69 episode reward: total was 31.500000. running mean: -9.773083\n",
      "epsilon:0.009992 episode_count: 53166. steps_count: 57293345.000000\n",
      "ep 3536: ep_len:1431 episode reward: total was 28.070000. running mean: -9.394652\n",
      "ep 3536: ep_len:500 episode reward: total was 15.490000. running mean: -9.145806\n",
      "ep 3536: ep_len:2928 episode reward: total was -657.410000. running mean: -15.628448\n",
      "ep 3536: ep_len:874 episode reward: total was 15.640000. running mean: -15.315763\n",
      "ep 3536: ep_len:761 episode reward: total was -21.850000. running mean: -15.381106\n",
      "ep 3536: ep_len:321 episode reward: total was 18.480000. running mean: -15.042494\n",
      "ep 3536: ep_len:1001 episode reward: total was -44.420000. running mean: -15.336270\n",
      "ep 3536: ep_len:827 episode reward: total was 41.950000. running mean: -14.763407\n",
      "ep 3536: ep_len:658 episode reward: total was 12.710000. running mean: -14.488673\n",
      "ep 3536: ep_len:141 episode reward: total was 63.000000. running mean: -13.713786\n",
      "ep 3536: ep_len:61 episode reward: total was 27.500000. running mean: -13.301648\n",
      "ep 3536: ep_len:500 episode reward: total was 45.440000. running mean: -12.714232\n",
      "ep 3536: ep_len:2838 episode reward: total was -23.330000. running mean: -12.820389\n",
      "epsilon:0.009992 episode_count: 53179. steps_count: 57306186.000000\n",
      "ep 3537: ep_len:1156 episode reward: total was -25.920000. running mean: -12.951386\n",
      "ep 3537: ep_len:709 episode reward: total was 8.280000. running mean: -12.739072\n",
      "ep 3537: ep_len:3002 episode reward: total was -54.980000. running mean: -13.161481\n",
      "ep 3537: ep_len:1068 episode reward: total was 5.460000. running mean: -12.975266\n",
      "ep 3537: ep_len:1880 episode reward: total was 17.530000. running mean: -12.670213\n",
      "ep 3537: ep_len:3538 episode reward: total was -10.440000. running mean: -12.647911\n",
      "ep 3537: ep_len:908 episode reward: total was 15.370000. running mean: -12.367732\n",
      "ep 3537: ep_len:838 episode reward: total was 54.250000. running mean: -11.701555\n",
      "ep 3537: ep_len:661 episode reward: total was 31.430000. running mean: -11.270239\n",
      "ep 3537: ep_len:199 episode reward: total was 98.000000. running mean: -10.177537\n",
      "ep 3537: ep_len:89 episode reward: total was 41.500000. running mean: -9.660762\n",
      "ep 3537: ep_len:894 episode reward: total was 21.740000. running mean: -9.346754\n",
      "ep 3537: ep_len:2816 episode reward: total was -13.480000. running mean: -9.388086\n",
      "epsilon:0.009992 episode_count: 53192. steps_count: 57323944.000000\n",
      "ep 3538: ep_len:967 episode reward: total was -14.440000. running mean: -9.438606\n",
      "ep 3538: ep_len:645 episode reward: total was 23.050000. running mean: -9.113720\n",
      "ep 3538: ep_len:2891 episode reward: total was -46.000000. running mean: -9.482582\n",
      "ep 3538: ep_len:565 episode reward: total was 2.160000. running mean: -9.366156\n",
      "ep 3538: ep_len:160 episode reward: total was 78.500000. running mean: -8.487495\n",
      "ep 3538: ep_len:95 episode reward: total was 46.000000. running mean: -7.942620\n",
      "ep 3538: ep_len:869 episode reward: total was 32.490000. running mean: -7.538294\n",
      "ep 3538: ep_len:3701 episode reward: total was 4.960000. running mean: -7.413311\n",
      "ep 3538: ep_len:677 episode reward: total was -29.760000. running mean: -7.636778\n",
      "ep 3538: ep_len:780 episode reward: total was 31.190000. running mean: -7.248510\n",
      "ep 3538: ep_len:948 episode reward: total was 4.540000. running mean: -7.130625\n",
      "ep 3538: ep_len:74 episode reward: total was 35.500000. running mean: -6.704319\n",
      "ep 3538: ep_len:3563 episode reward: total was -615.260000. running mean: -12.789875\n",
      "ep 3538: ep_len:2777 episode reward: total was -0.130000. running mean: -12.663277\n",
      "epsilon:0.009992 episode_count: 53206. steps_count: 57342656.000000\n",
      "ep 3539: ep_len:1204 episode reward: total was 8.760000. running mean: -12.449044\n",
      "ep 3539: ep_len:672 episode reward: total was -27.180000. running mean: -12.596353\n",
      "ep 3539: ep_len:3042 episode reward: total was -22.650000. running mean: -12.696890\n",
      "ep 3539: ep_len:751 episode reward: total was -15.770000. running mean: -12.727621\n",
      "ep 3539: ep_len:74 episode reward: total was 34.000000. running mean: -12.260345\n",
      "ep 3539: ep_len:50 episode reward: total was 23.500000. running mean: -11.902741\n",
      "ep 3539: ep_len:500 episode reward: total was 8.820000. running mean: -11.695514\n",
      "ep 3539: ep_len:638 episode reward: total was 36.370000. running mean: -11.214859\n",
      "ep 3539: ep_len:1166 episode reward: total was -39.010000. running mean: -11.492810\n",
      "ep 3539: ep_len:7270 episode reward: total was 58.300000. running mean: -10.794882\n",
      "ep 3539: ep_len:663 episode reward: total was -10.200000. running mean: -10.788933\n",
      "ep 3539: ep_len:56 episode reward: total was 26.500000. running mean: -10.416044\n",
      "ep 3539: ep_len:215 episode reward: total was 104.500000. running mean: -9.266884\n",
      "ep 3539: ep_len:571 episode reward: total was -7.070000. running mean: -9.244915\n",
      "ep 3539: ep_len:2848 episode reward: total was -1.680000. running mean: -9.169266\n",
      "epsilon:0.009992 episode_count: 53221. steps_count: 57362376.000000\n",
      "ep 3540: ep_len:900 episode reward: total was -43.600000. running mean: -9.513573\n",
      "ep 3540: ep_len:759 episode reward: total was -8.800000. running mean: -9.506437\n",
      "ep 3540: ep_len:62 episode reward: total was 29.500000. running mean: -9.116373\n",
      "ep 3540: ep_len:2965 episode reward: total was -17.210000. running mean: -9.197309\n",
      "ep 3540: ep_len:1234 episode reward: total was -23.180000. running mean: -9.337136\n",
      "ep 3540: ep_len:57 episode reward: total was 27.000000. running mean: -8.973765\n",
      "ep 3540: ep_len:158 episode reward: total was 77.500000. running mean: -8.109027\n",
      "ep 3540: ep_len:89 episode reward: total was 43.000000. running mean: -7.597937\n",
      "ep 3540: ep_len:1423 episode reward: total was 4.050000. running mean: -7.481457\n",
      "ep 3540: ep_len:659 episode reward: total was 21.120000. running mean: -7.195443\n",
      "ep 3540: ep_len:783 episode reward: total was 18.770000. running mean: -6.935788\n",
      "ep 3540: ep_len:731 episode reward: total was 34.030000. running mean: -6.526130\n",
      "ep 3540: ep_len:975 episode reward: total was 14.360000. running mean: -6.317269\n",
      "ep 3540: ep_len:76 episode reward: total was 36.500000. running mean: -5.889096\n",
      "ep 3540: ep_len:155 episode reward: total was 71.500000. running mean: -5.115205\n",
      "ep 3540: ep_len:617 episode reward: total was -18.670000. running mean: -5.250753\n",
      "ep 3540: ep_len:2779 episode reward: total was -2.520000. running mean: -5.223446\n",
      "epsilon:0.009992 episode_count: 53238. steps_count: 57376798.000000\n",
      "ep 3541: ep_len:1442 episode reward: total was 1.180000. running mean: -5.159411\n",
      "ep 3541: ep_len:1172 episode reward: total was -105.780000. running mean: -6.165617\n",
      "ep 3541: ep_len:2885 episode reward: total was -34.610000. running mean: -6.450061\n",
      "ep 3541: ep_len:1135 episode reward: total was -15.080000. running mean: -6.536361\n",
      "ep 3541: ep_len:49 episode reward: total was 23.000000. running mean: -6.240997\n",
      "ep 3541: ep_len:1108 episode reward: total was 10.920000. running mean: -6.069387\n",
      "ep 3541: ep_len:341 episode reward: total was 13.660000. running mean: -5.872093\n",
      "ep 3541: ep_len:769 episode reward: total was -34.550000. running mean: -6.158872\n",
      "ep 3541: ep_len:886 episode reward: total was 67.010000. running mean: -5.427183\n",
      "ep 3541: ep_len:960 episode reward: total was 1.750000. running mean: -5.355412\n",
      "ep 3541: ep_len:51 episode reward: total was 24.000000. running mean: -5.061857\n",
      "ep 3541: ep_len:201 episode reward: total was 97.500000. running mean: -4.036239\n",
      "ep 3541: ep_len:68 episode reward: total was 32.500000. running mean: -3.670877\n",
      "ep 3541: ep_len:987 episode reward: total was 21.490000. running mean: -3.419268\n",
      "ep 3541: ep_len:2737 episode reward: total was -517.650000. running mean: -8.561575\n",
      "epsilon:0.009992 episode_count: 53253. steps_count: 57391589.000000\n",
      "ep 3542: ep_len:1422 episode reward: total was 26.170000. running mean: -8.214259\n",
      "ep 3542: ep_len:1384 episode reward: total was -147.410000. running mean: -9.606217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3542: ep_len:2986 episode reward: total was -54.510000. running mean: -10.055255\n",
      "ep 3542: ep_len:1579 episode reward: total was -51.470000. running mean: -10.469402\n",
      "ep 3542: ep_len:163 episode reward: total was 75.500000. running mean: -9.609708\n",
      "ep 3542: ep_len:1883 episode reward: total was -21.980000. running mean: -9.733411\n",
      "ep 3542: ep_len:640 episode reward: total was 7.180000. running mean: -9.564277\n",
      "ep 3542: ep_len:623 episode reward: total was -101.860000. running mean: -10.487234\n",
      "ep 3542: ep_len:696 episode reward: total was 24.460000. running mean: -10.137762\n",
      "ep 3542: ep_len:938 episode reward: total was 19.250000. running mean: -9.843884\n",
      "ep 3542: ep_len:67 episode reward: total was 30.500000. running mean: -9.440445\n",
      "ep 3542: ep_len:220 episode reward: total was 107.000000. running mean: -8.276041\n",
      "ep 3542: ep_len:77 episode reward: total was 37.000000. running mean: -7.823280\n",
      "ep 3542: ep_len:797 episode reward: total was -85.980000. running mean: -8.604848\n",
      "ep 3542: ep_len:2902 episode reward: total was -44.630000. running mean: -8.965099\n",
      "ep 3542: ep_len:66 episode reward: total was 31.500000. running mean: -8.560448\n",
      "epsilon:0.009992 episode_count: 53269. steps_count: 57408032.000000\n",
      "ep 3543: ep_len:1070 episode reward: total was -3.610000. running mean: -8.510944\n",
      "ep 3543: ep_len:215 episode reward: total was 14.510000. running mean: -8.280734\n",
      "ep 3543: ep_len:2923 episode reward: total was -40.470000. running mean: -8.602627\n",
      "ep 3543: ep_len:696 episode reward: total was 4.360000. running mean: -8.473001\n",
      "ep 3543: ep_len:76 episode reward: total was 36.500000. running mean: -8.023271\n",
      "ep 3543: ep_len:75 episode reward: total was 36.000000. running mean: -7.583038\n",
      "ep 3543: ep_len:725 episode reward: total was -7.060000. running mean: -7.577807\n",
      "ep 3543: ep_len:656 episode reward: total was 30.180000. running mean: -7.200229\n",
      "ep 3543: ep_len:670 episode reward: total was -4.580000. running mean: -7.174027\n",
      "ep 3543: ep_len:744 episode reward: total was 18.600000. running mean: -6.916287\n",
      "ep 3543: ep_len:1168 episode reward: total was -11.720000. running mean: -6.964324\n",
      "ep 3543: ep_len:86 episode reward: total was 41.500000. running mean: -6.479681\n",
      "ep 3543: ep_len:139 episode reward: total was 65.000000. running mean: -5.764884\n",
      "ep 3543: ep_len:59 episode reward: total was 26.500000. running mean: -5.442235\n",
      "ep 3543: ep_len:82 episode reward: total was 38.000000. running mean: -5.007813\n",
      "ep 3543: ep_len:676 episode reward: total was -0.420000. running mean: -4.961935\n",
      "ep 3543: ep_len:2876 episode reward: total was -590.920000. running mean: -10.821515\n",
      "epsilon:0.009992 episode_count: 53286. steps_count: 57420968.000000\n",
      "ep 3544: ep_len:957 episode reward: total was -47.200000. running mean: -11.185300\n",
      "ep 3544: ep_len:1120 episode reward: total was -0.330000. running mean: -11.076747\n",
      "ep 3544: ep_len:45 episode reward: total was 21.000000. running mean: -10.755980\n",
      "ep 3544: ep_len:3062 episode reward: total was -22.570000. running mean: -10.874120\n",
      "ep 3544: ep_len:1684 episode reward: total was -1.960000. running mean: -10.784979\n",
      "ep 3544: ep_len:39 episode reward: total was 18.000000. running mean: -10.497129\n",
      "ep 3544: ep_len:113 episode reward: total was 52.000000. running mean: -9.872158\n",
      "ep 3544: ep_len:1136 episode reward: total was -9.010000. running mean: -9.863536\n",
      "ep 3544: ep_len:3626 episode reward: total was -302.340000. running mean: -12.788301\n",
      "ep 3544: ep_len:640 episode reward: total was -1.240000. running mean: -12.672818\n",
      "ep 3544: ep_len:7076 episode reward: total was -823.880000. running mean: -20.784889\n",
      "ep 3544: ep_len:1065 episode reward: total was 24.260000. running mean: -20.334441\n",
      "ep 3544: ep_len:92 episode reward: total was 43.000000. running mean: -19.701096\n",
      "ep 3544: ep_len:102 episode reward: total was 46.500000. running mean: -19.039085\n",
      "ep 3544: ep_len:628 episode reward: total was -0.070000. running mean: -18.849394\n",
      "ep 3544: ep_len:2827 episode reward: total was -4.340000. running mean: -18.704300\n",
      "epsilon:0.009992 episode_count: 53302. steps_count: 57445180.000000\n",
      "ep 3545: ep_len:732 episode reward: total was -31.230000. running mean: -18.829557\n",
      "ep 3545: ep_len:500 episode reward: total was 18.950000. running mean: -18.451762\n",
      "ep 3545: ep_len:2981 episode reward: total was -16.220000. running mean: -18.429444\n",
      "ep 3545: ep_len:502 episode reward: total was 24.060000. running mean: -18.004550\n",
      "ep 3545: ep_len:40 episode reward: total was 18.500000. running mean: -17.639504\n",
      "ep 3545: ep_len:134 episode reward: total was 62.500000. running mean: -16.838109\n",
      "ep 3545: ep_len:67 episode reward: total was 30.500000. running mean: -16.364728\n",
      "ep 3545: ep_len:686 episode reward: total was -2.280000. running mean: -16.223881\n",
      "ep 3545: ep_len:3801 episode reward: total was -50.310000. running mean: -16.564742\n",
      "ep 3545: ep_len:1254 episode reward: total was -24.940000. running mean: -16.648495\n",
      "ep 3545: ep_len:655 episode reward: total was 20.470000. running mean: -16.277310\n",
      "ep 3545: ep_len:591 episode reward: total was -3.350000. running mean: -16.148037\n",
      "ep 3545: ep_len:113 episode reward: total was 55.000000. running mean: -15.436556\n",
      "ep 3545: ep_len:1054 episode reward: total was 16.960000. running mean: -15.112591\n",
      "ep 3545: ep_len:2729 episode reward: total was -38.590000. running mean: -15.347365\n",
      "epsilon:0.009992 episode_count: 53317. steps_count: 57461019.000000\n",
      "ep 3546: ep_len:638 episode reward: total was -24.090000. running mean: -15.434791\n",
      "ep 3546: ep_len:1129 episode reward: total was 10.690000. running mean: -15.173543\n",
      "ep 3546: ep_len:78 episode reward: total was 37.500000. running mean: -14.646808\n",
      "ep 3546: ep_len:3047 episode reward: total was -8.730000. running mean: -14.587640\n",
      "ep 3546: ep_len:588 episode reward: total was 8.740000. running mean: -14.354363\n",
      "ep 3546: ep_len:129 episode reward: total was 57.000000. running mean: -13.640820\n",
      "ep 3546: ep_len:98 episode reward: total was 46.000000. running mean: -13.044411\n",
      "ep 3546: ep_len:61 episode reward: total was 29.000000. running mean: -12.623967\n",
      "ep 3546: ep_len:1474 episode reward: total was -194.500000. running mean: -14.442728\n",
      "ep 3546: ep_len:3577 episode reward: total was -79.620000. running mean: -15.094500\n",
      "ep 3546: ep_len:615 episode reward: total was -194.860000. running mean: -16.892155\n",
      "ep 3546: ep_len:809 episode reward: total was 39.930000. running mean: -16.323934\n",
      "ep 3546: ep_len:700 episode reward: total was 30.270000. running mean: -15.857994\n",
      "ep 3546: ep_len:81 episode reward: total was 37.500000. running mean: -15.324415\n",
      "ep 3546: ep_len:1185 episode reward: total was 25.070000. running mean: -14.920470\n",
      "ep 3546: ep_len:2779 episode reward: total was -10.120000. running mean: -14.872466\n",
      "ep 3546: ep_len:35 episode reward: total was 16.000000. running mean: -14.563741\n",
      "epsilon:0.009992 episode_count: 53334. steps_count: 57478042.000000\n",
      "ep 3547: ep_len:1159 episode reward: total was 16.110000. running mean: -14.257004\n",
      "ep 3547: ep_len:756 episode reward: total was -30.520000. running mean: -14.419634\n",
      "ep 3547: ep_len:3016 episode reward: total was -8.840000. running mean: -14.363837\n",
      "ep 3547: ep_len:683 episode reward: total was 15.740000. running mean: -14.062799\n",
      "ep 3547: ep_len:49 episode reward: total was 21.500000. running mean: -13.707171\n",
      "ep 3547: ep_len:828 episode reward: total was 34.280000. running mean: -13.227299\n",
      "ep 3547: ep_len:311 episode reward: total was 28.880000. running mean: -12.806226\n",
      "ep 3547: ep_len:1177 episode reward: total was 4.140000. running mean: -12.636764\n",
      "ep 3547: ep_len:759 episode reward: total was 35.150000. running mean: -12.158896\n",
      "ep 3547: ep_len:546 episode reward: total was 7.750000. running mean: -11.959807\n",
      "ep 3547: ep_len:36 episode reward: total was 16.500000. running mean: -11.675209\n",
      "ep 3547: ep_len:637 episode reward: total was -7.200000. running mean: -11.630457\n",
      "ep 3547: ep_len:2839 episode reward: total was -31.890000. running mean: -11.833053\n",
      "ep 3547: ep_len:43 episode reward: total was 17.000000. running mean: -11.544722\n",
      "epsilon:0.009992 episode_count: 53348. steps_count: 57490881.000000\n",
      "ep 3548: ep_len:975 episode reward: total was -3.560000. running mean: -11.464875\n",
      "ep 3548: ep_len:1299 episode reward: total was -26.570000. running mean: -11.615926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3548: ep_len:67 episode reward: total was 32.000000. running mean: -11.179767\n",
      "ep 3548: ep_len:2889 episode reward: total was -22.970000. running mean: -11.297669\n",
      "ep 3548: ep_len:1097 episode reward: total was -7.380000. running mean: -11.258492\n",
      "ep 3548: ep_len:676 episode reward: total was 18.170000. running mean: -10.964208\n",
      "ep 3548: ep_len:354 episode reward: total was 9.600000. running mean: -10.758565\n",
      "ep 3548: ep_len:1573 episode reward: total was -30.880000. running mean: -10.959780\n",
      "ep 3548: ep_len:698 episode reward: total was 12.090000. running mean: -10.729282\n",
      "ep 3548: ep_len:1074 episode reward: total was 34.970000. running mean: -10.272289\n",
      "ep 3548: ep_len:97 episode reward: total was 44.000000. running mean: -9.729566\n",
      "ep 3548: ep_len:134 episode reward: total was 65.500000. running mean: -8.977271\n",
      "ep 3548: ep_len:45 episode reward: total was 19.500000. running mean: -8.692498\n",
      "ep 3548: ep_len:647 episode reward: total was 21.730000. running mean: -8.388273\n",
      "ep 3548: ep_len:2683 episode reward: total was -19.340000. running mean: -8.497790\n",
      "epsilon:0.009992 episode_count: 53363. steps_count: 57505189.000000\n",
      "ep 3549: ep_len:1115 episode reward: total was 21.730000. running mean: -8.195512\n",
      "ep 3549: ep_len:745 episode reward: total was -34.270000. running mean: -8.456257\n",
      "ep 3549: ep_len:2979 episode reward: total was -37.040000. running mean: -8.742095\n",
      "ep 3549: ep_len:1075 episode reward: total was 1.490000. running mean: -8.639774\n",
      "ep 3549: ep_len:1418 episode reward: total was 6.560000. running mean: -8.487776\n",
      "ep 3549: ep_len:346 episode reward: total was 17.080000. running mean: -8.232098\n",
      "ep 3549: ep_len:1608 episode reward: total was -1.720000. running mean: -8.166977\n",
      "ep 3549: ep_len:921 episode reward: total was -359.980000. running mean: -11.685107\n",
      "ep 3549: ep_len:1487 episode reward: total was 17.300000. running mean: -11.395256\n",
      "ep 3549: ep_len:68 episode reward: total was 31.000000. running mean: -10.971304\n",
      "ep 3549: ep_len:50 episode reward: total was 23.500000. running mean: -10.626591\n",
      "ep 3549: ep_len:85 episode reward: total was 41.000000. running mean: -10.110325\n",
      "ep 3549: ep_len:992 episode reward: total was -36.710000. running mean: -10.376322\n",
      "ep 3549: ep_len:2872 episode reward: total was -22.870000. running mean: -10.501258\n",
      "ep 3549: ep_len:70 episode reward: total was 33.500000. running mean: -10.061246\n",
      "epsilon:0.009992 episode_count: 53378. steps_count: 57521020.000000\n",
      "ep 3550: ep_len:1071 episode reward: total was 7.540000. running mean: -9.885233\n",
      "ep 3550: ep_len:1675 episode reward: total was 9.640000. running mean: -9.689981\n",
      "ep 3550: ep_len:2979 episode reward: total was -31.180000. running mean: -9.904881\n",
      "ep 3550: ep_len:1107 episode reward: total was -193.980000. running mean: -11.745632\n",
      "ep 3550: ep_len:68 episode reward: total was 31.000000. running mean: -11.318176\n",
      "ep 3550: ep_len:741 episode reward: total was -24.590000. running mean: -11.450894\n",
      "ep 3550: ep_len:304 episode reward: total was 2.820000. running mean: -11.308185\n",
      "ep 3550: ep_len:582 episode reward: total was 25.220000. running mean: -10.942904\n",
      "ep 3550: ep_len:741 episode reward: total was 41.760000. running mean: -10.415874\n",
      "ep 3550: ep_len:1100 episode reward: total was 15.950000. running mean: -10.152216\n",
      "ep 3550: ep_len:55 episode reward: total was 26.000000. running mean: -9.790694\n",
      "ep 3550: ep_len:70 episode reward: total was 32.000000. running mean: -9.372787\n",
      "ep 3550: ep_len:671 episode reward: total was -2.550000. running mean: -9.304559\n",
      "ep 3550: ep_len:2827 episode reward: total was -0.270000. running mean: -9.214213\n",
      "epsilon:0.009992 episode_count: 53392. steps_count: 57535011.000000\n",
      "ep 3551: ep_len:1154 episode reward: total was 17.400000. running mean: -8.948071\n",
      "ep 3551: ep_len:729 episode reward: total was -15.240000. running mean: -9.010990\n",
      "ep 3551: ep_len:71 episode reward: total was 32.500000. running mean: -8.595880\n",
      "ep 3551: ep_len:82 episode reward: total was 39.500000. running mean: -8.114922\n",
      "ep 3551: ep_len:643 episode reward: total was -10.550000. running mean: -8.139272\n",
      "ep 3551: ep_len:66 episode reward: total was 31.500000. running mean: -7.742880\n",
      "ep 3551: ep_len:65 episode reward: total was 29.500000. running mean: -7.370451\n",
      "ep 3551: ep_len:1539 episode reward: total was 11.690000. running mean: -7.179846\n",
      "ep 3551: ep_len:3691 episode reward: total was -74.410000. running mean: -7.852148\n",
      "ep 3551: ep_len:1559 episode reward: total was -321.370000. running mean: -10.987326\n",
      "ep 3551: ep_len:604 episode reward: total was -121.790000. running mean: -12.095353\n",
      "ep 3551: ep_len:684 episode reward: total was -18.680000. running mean: -12.161200\n",
      "ep 3551: ep_len:172 episode reward: total was 83.000000. running mean: -11.209588\n",
      "ep 3551: ep_len:500 episode reward: total was 15.770000. running mean: -10.939792\n",
      "ep 3551: ep_len:2830 episode reward: total was 5.210000. running mean: -10.778294\n",
      "epsilon:0.009992 episode_count: 53407. steps_count: 57549400.000000\n",
      "ep 3552: ep_len:1104 episode reward: total was 6.560000. running mean: -10.604911\n",
      "ep 3552: ep_len:1251 episode reward: total was -26.040000. running mean: -10.759262\n",
      "ep 3552: ep_len:60 episode reward: total was 27.000000. running mean: -10.381669\n",
      "ep 3552: ep_len:2913 episode reward: total was -19.320000. running mean: -10.471053\n",
      "ep 3552: ep_len:608 episode reward: total was -14.120000. running mean: -10.507542\n",
      "ep 3552: ep_len:85 episode reward: total was 39.500000. running mean: -10.007467\n",
      "ep 3552: ep_len:679 episode reward: total was 17.890000. running mean: -9.728492\n",
      "ep 3552: ep_len:631 episode reward: total was 14.750000. running mean: -9.483707\n",
      "ep 3552: ep_len:500 episode reward: total was -1.710000. running mean: -9.405970\n",
      "ep 3552: ep_len:698 episode reward: total was 24.420000. running mean: -9.067710\n",
      "ep 3552: ep_len:1484 episode reward: total was 8.300000. running mean: -8.894033\n",
      "ep 3552: ep_len:43 episode reward: total was 20.000000. running mean: -8.605093\n",
      "ep 3552: ep_len:1144 episode reward: total was -0.260000. running mean: -8.521642\n",
      "ep 3552: ep_len:2839 episode reward: total was 3.220000. running mean: -8.404225\n",
      "epsilon:0.009992 episode_count: 53421. steps_count: 57563439.000000\n",
      "ep 3553: ep_len:1109 episode reward: total was -8.820000. running mean: -8.408383\n",
      "ep 3553: ep_len:623 episode reward: total was 10.800000. running mean: -8.216299\n",
      "ep 3553: ep_len:57 episode reward: total was 27.000000. running mean: -7.864136\n",
      "ep 3553: ep_len:102 episode reward: total was -2.500000. running mean: -7.810495\n",
      "ep 3553: ep_len:853 episode reward: total was 44.670000. running mean: -7.285690\n",
      "ep 3553: ep_len:35 episode reward: total was 16.000000. running mean: -7.052833\n",
      "ep 3553: ep_len:1057 episode reward: total was -1.770000. running mean: -7.000005\n",
      "ep 3553: ep_len:3586 episode reward: total was -79.070000. running mean: -7.720705\n",
      "ep 3553: ep_len:2049 episode reward: total was -267.530000. running mean: -10.318798\n",
      "ep 3553: ep_len:771 episode reward: total was 14.800000. running mean: -10.067610\n",
      "ep 3553: ep_len:1477 episode reward: total was 11.850000. running mean: -9.848434\n",
      "ep 3553: ep_len:73 episode reward: total was 33.500000. running mean: -9.414949\n",
      "ep 3553: ep_len:56 episode reward: total was 26.500000. running mean: -9.055800\n",
      "ep 3553: ep_len:1147 episode reward: total was 4.980000. running mean: -8.915442\n",
      "ep 3553: ep_len:2852 episode reward: total was -81.120000. running mean: -9.637487\n",
      "epsilon:0.009992 episode_count: 53436. steps_count: 57579286.000000\n",
      "ep 3554: ep_len:1151 episode reward: total was -12.900000. running mean: -9.670113\n",
      "ep 3554: ep_len:1683 episode reward: total was -10.910000. running mean: -9.682511\n",
      "ep 3554: ep_len:3062 episode reward: total was 23.250000. running mean: -9.353186\n",
      "ep 3554: ep_len:500 episode reward: total was 6.680000. running mean: -9.192854\n",
      "ep 3554: ep_len:48 episode reward: total was 22.500000. running mean: -8.875926\n",
      "ep 3554: ep_len:80 episode reward: total was 38.500000. running mean: -8.402167\n",
      "ep 3554: ep_len:1094 episode reward: total was -7.410000. running mean: -8.392245\n",
      "ep 3554: ep_len:3616 episode reward: total was -26.740000. running mean: -8.575722\n",
      "ep 3554: ep_len:500 episode reward: total was 24.190000. running mean: -8.248065\n",
      "ep 3554: ep_len:618 episode reward: total was -6.810000. running mean: -8.233685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3554: ep_len:935 episode reward: total was 73.880000. running mean: -7.412548\n",
      "ep 3554: ep_len:205 episode reward: total was 99.500000. running mean: -6.343422\n",
      "ep 3554: ep_len:1460 episode reward: total was 18.530000. running mean: -6.094688\n",
      "ep 3554: ep_len:2774 episode reward: total was -36.550000. running mean: -6.399241\n",
      "ep 3554: ep_len:44 episode reward: total was 20.500000. running mean: -6.130249\n",
      "epsilon:0.009992 episode_count: 53451. steps_count: 57597056.000000\n",
      "ep 3555: ep_len:879 episode reward: total was -9.640000. running mean: -6.165346\n",
      "ep 3555: ep_len:1559 episode reward: total was -18.040000. running mean: -6.284093\n",
      "ep 3555: ep_len:3002 episode reward: total was -57.640000. running mean: -6.797652\n",
      "ep 3555: ep_len:838 episode reward: total was 37.320000. running mean: -6.356475\n",
      "ep 3555: ep_len:58 episode reward: total was 27.500000. running mean: -6.017911\n",
      "ep 3555: ep_len:1352 episode reward: total was -163.340000. running mean: -7.591132\n",
      "ep 3555: ep_len:360 episode reward: total was 22.390000. running mean: -7.291320\n",
      "ep 3555: ep_len:1587 episode reward: total was -35.300000. running mean: -7.571407\n",
      "ep 3555: ep_len:7365 episode reward: total was -144.350000. running mean: -8.939193\n",
      "ep 3555: ep_len:723 episode reward: total was 11.110000. running mean: -8.738701\n",
      "ep 3555: ep_len:50 episode reward: total was 20.500000. running mean: -8.446314\n",
      "ep 3555: ep_len:1109 episode reward: total was -11.300000. running mean: -8.474851\n",
      "ep 3555: ep_len:2791 episode reward: total was -42.590000. running mean: -8.816002\n",
      "epsilon:0.009992 episode_count: 53464. steps_count: 57618729.000000\n",
      "ep 3556: ep_len:814 episode reward: total was -29.400000. running mean: -9.021842\n",
      "ep 3556: ep_len:500 episode reward: total was 4.720000. running mean: -8.884424\n",
      "ep 3556: ep_len:3038 episode reward: total was -24.130000. running mean: -9.036880\n",
      "ep 3556: ep_len:875 episode reward: total was 59.310000. running mean: -8.353411\n",
      "ep 3556: ep_len:46 episode reward: total was 21.500000. running mean: -8.054877\n",
      "ep 3556: ep_len:151 episode reward: total was 72.500000. running mean: -7.249328\n",
      "ep 3556: ep_len:50 episode reward: total was 23.500000. running mean: -6.941835\n",
      "ep 3556: ep_len:1457 episode reward: total was 14.120000. running mean: -6.731216\n",
      "ep 3556: ep_len:3560 episode reward: total was -125.760000. running mean: -7.921504\n",
      "ep 3556: ep_len:574 episode reward: total was -25.620000. running mean: -8.098489\n",
      "ep 3556: ep_len:744 episode reward: total was -2.680000. running mean: -8.044304\n",
      "ep 3556: ep_len:1482 episode reward: total was 11.190000. running mean: -7.851961\n",
      "ep 3556: ep_len:93 episode reward: total was 42.000000. running mean: -7.353442\n",
      "ep 3556: ep_len:1049 episode reward: total was -28.990000. running mean: -7.569807\n",
      "ep 3556: ep_len:2819 episode reward: total was -18.710000. running mean: -7.681209\n",
      "ep 3556: ep_len:59 episode reward: total was 26.500000. running mean: -7.339397\n",
      "epsilon:0.009992 episode_count: 53480. steps_count: 57636040.000000\n",
      "ep 3557: ep_len:627 episode reward: total was 14.540000. running mean: -7.120603\n",
      "ep 3557: ep_len:737 episode reward: total was -50.080000. running mean: -7.550197\n",
      "ep 3557: ep_len:49 episode reward: total was 23.000000. running mean: -7.244695\n",
      "ep 3557: ep_len:3118 episode reward: total was -37.110000. running mean: -7.543348\n",
      "ep 3557: ep_len:622 episode reward: total was -3.890000. running mean: -7.506815\n",
      "ep 3557: ep_len:169 episode reward: total was 78.500000. running mean: -6.646746\n",
      "ep 3557: ep_len:66 episode reward: total was 31.500000. running mean: -6.265279\n",
      "ep 3557: ep_len:48 episode reward: total was 21.000000. running mean: -5.992626\n",
      "ep 3557: ep_len:650 episode reward: total was 0.260000. running mean: -5.930100\n",
      "ep 3557: ep_len:323 episode reward: total was 15.990000. running mean: -5.710899\n",
      "ep 3557: ep_len:808 episode reward: total was 9.440000. running mean: -5.559390\n",
      "ep 3557: ep_len:7230 episode reward: total was -175.160000. running mean: -7.255396\n",
      "ep 3557: ep_len:708 episode reward: total was 18.920000. running mean: -6.993642\n",
      "ep 3557: ep_len:74 episode reward: total was 35.500000. running mean: -6.568706\n",
      "ep 3557: ep_len:78 episode reward: total was 36.000000. running mean: -6.143019\n",
      "ep 3557: ep_len:1046 episode reward: total was 34.020000. running mean: -5.741388\n",
      "ep 3557: ep_len:2824 episode reward: total was -73.880000. running mean: -6.422775\n",
      "ep 3557: ep_len:42 episode reward: total was 16.500000. running mean: -6.193547\n",
      "epsilon:0.009992 episode_count: 53498. steps_count: 57655259.000000\n",
      "ep 3558: ep_len:1399 episode reward: total was 13.820000. running mean: -5.993411\n",
      "ep 3558: ep_len:608 episode reward: total was -1.160000. running mean: -5.945077\n",
      "ep 3558: ep_len:2949 episode reward: total was -11.810000. running mean: -6.003726\n",
      "ep 3558: ep_len:1389 episode reward: total was 13.260000. running mean: -5.811089\n",
      "ep 3558: ep_len:112 episode reward: total was 54.500000. running mean: -5.207978\n",
      "ep 3558: ep_len:96 episode reward: total was 46.500000. running mean: -4.690899\n",
      "ep 3558: ep_len:1514 episode reward: total was -123.840000. running mean: -5.882390\n",
      "ep 3558: ep_len:333 episode reward: total was 20.500000. running mean: -5.618566\n",
      "ep 3558: ep_len:938 episode reward: total was -32.630000. running mean: -5.888680\n",
      "ep 3558: ep_len:791 episode reward: total was 21.070000. running mean: -5.619093\n",
      "ep 3558: ep_len:500 episode reward: total was 14.670000. running mean: -5.416202\n",
      "ep 3558: ep_len:135 episode reward: total was 64.500000. running mean: -4.717040\n",
      "ep 3558: ep_len:89 episode reward: total was 43.000000. running mean: -4.239870\n",
      "ep 3558: ep_len:706 episode reward: total was -6.240000. running mean: -4.259871\n",
      "ep 3558: ep_len:2867 episode reward: total was -2.530000. running mean: -4.242572\n",
      "epsilon:0.009992 episode_count: 53513. steps_count: 57669685.000000\n",
      "ep 3559: ep_len:2395 episode reward: total was -497.200000. running mean: -9.172147\n",
      "ep 3559: ep_len:1154 episode reward: total was -33.470000. running mean: -9.415125\n",
      "ep 3559: ep_len:3031 episode reward: total was -1.810000. running mean: -9.339074\n",
      "ep 3559: ep_len:545 episode reward: total was -11.360000. running mean: -9.359283\n",
      "ep 3559: ep_len:38 episode reward: total was 14.500000. running mean: -9.120690\n",
      "ep 3559: ep_len:70 episode reward: total was 33.500000. running mean: -8.694483\n",
      "ep 3559: ep_len:46 episode reward: total was 21.500000. running mean: -8.392539\n",
      "ep 3559: ep_len:1479 episode reward: total was 4.330000. running mean: -8.265313\n",
      "ep 3559: ep_len:3607 episode reward: total was -57.650000. running mean: -8.759160\n",
      "ep 3559: ep_len:3737 episode reward: total was -1074.660000. running mean: -19.418169\n",
      "ep 3559: ep_len:735 episode reward: total was 31.080000. running mean: -18.913187\n",
      "ep 3559: ep_len:1460 episode reward: total was 2.430000. running mean: -18.699755\n",
      "ep 3559: ep_len:98 episode reward: total was 46.000000. running mean: -18.052757\n",
      "ep 3559: ep_len:103 episode reward: total was 50.000000. running mean: -17.372230\n",
      "ep 3559: ep_len:656 episode reward: total was -26.940000. running mean: -17.467908\n",
      "ep 3559: ep_len:2839 episode reward: total was -22.030000. running mean: -17.513528\n",
      "epsilon:0.009992 episode_count: 53529. steps_count: 57691678.000000\n",
      "ep 3560: ep_len:1565 episode reward: total was 9.790000. running mean: -17.240493\n",
      "ep 3560: ep_len:1004 episode reward: total was 23.040000. running mean: -16.837688\n",
      "ep 3560: ep_len:72 episode reward: total was 31.010000. running mean: -16.359211\n",
      "ep 3560: ep_len:3036 episode reward: total was 21.060000. running mean: -15.985019\n",
      "ep 3560: ep_len:773 episode reward: total was 22.310000. running mean: -15.602069\n",
      "ep 3560: ep_len:91 episode reward: total was 44.000000. running mean: -15.006048\n",
      "ep 3560: ep_len:500 episode reward: total was 13.420000. running mean: -14.721788\n",
      "ep 3560: ep_len:3782 episode reward: total was -1631.440000. running mean: -30.888970\n",
      "ep 3560: ep_len:672 episode reward: total was 2.510000. running mean: -30.554980\n",
      "ep 3560: ep_len:615 episode reward: total was -0.410000. running mean: -30.253531\n",
      "ep 3560: ep_len:1061 episode reward: total was 42.190000. running mean: -29.529095\n",
      "ep 3560: ep_len:83 episode reward: total was 38.500000. running mean: -28.848804\n",
      "ep 3560: ep_len:980 episode reward: total was -39.860000. running mean: -28.958916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3560: ep_len:2883 episode reward: total was -0.600000. running mean: -28.675327\n",
      "epsilon:0.009992 episode_count: 53543. steps_count: 57708795.000000\n",
      "ep 3561: ep_len:982 episode reward: total was -79.450000. running mean: -29.183074\n",
      "ep 3561: ep_len:684 episode reward: total was 25.660000. running mean: -28.634643\n",
      "ep 3561: ep_len:2899 episode reward: total was -19.200000. running mean: -28.540297\n",
      "ep 3561: ep_len:634 episode reward: total was 23.950000. running mean: -28.015394\n",
      "ep 3561: ep_len:178 episode reward: total was 83.000000. running mean: -26.905240\n",
      "ep 3561: ep_len:500 episode reward: total was 2.120000. running mean: -26.614987\n",
      "ep 3561: ep_len:3856 episode reward: total was -1880.090000. running mean: -45.149737\n",
      "ep 3561: ep_len:516 episode reward: total was 6.000000. running mean: -44.638240\n",
      "ep 3561: ep_len:7225 episode reward: total was 36.790000. running mean: -43.823958\n",
      "ep 3561: ep_len:591 episode reward: total was -81.810000. running mean: -44.203818\n",
      "ep 3561: ep_len:174 episode reward: total was 62.000000. running mean: -43.141780\n",
      "ep 3561: ep_len:39 episode reward: total was 16.500000. running mean: -42.545362\n",
      "ep 3561: ep_len:1117 episode reward: total was -19.300000. running mean: -42.312908\n",
      "ep 3561: ep_len:2773 episode reward: total was -6.750000. running mean: -41.957279\n",
      "ep 3561: ep_len:32 episode reward: total was 14.500000. running mean: -41.392707\n",
      "epsilon:0.009992 episode_count: 53558. steps_count: 57730995.000000\n",
      "ep 3562: ep_len:1412 episode reward: total was -0.470000. running mean: -40.983480\n",
      "ep 3562: ep_len:1000 episode reward: total was 8.670000. running mean: -40.486945\n",
      "ep 3562: ep_len:2988 episode reward: total was -6.390000. running mean: -40.145975\n",
      "ep 3562: ep_len:1162 episode reward: total was -14.810000. running mean: -39.892616\n",
      "ep 3562: ep_len:113 episode reward: total was 55.000000. running mean: -38.943689\n",
      "ep 3562: ep_len:500 episode reward: total was -14.350000. running mean: -38.697753\n",
      "ep 3562: ep_len:3681 episode reward: total was -37.730000. running mean: -38.688075\n",
      "ep 3562: ep_len:1552 episode reward: total was -137.050000. running mean: -39.671694\n",
      "ep 3562: ep_len:7414 episode reward: total was -141.370000. running mean: -40.688677\n",
      "ep 3562: ep_len:966 episode reward: total was 12.190000. running mean: -40.159891\n",
      "ep 3562: ep_len:53 episode reward: total was 23.500000. running mean: -39.523292\n",
      "ep 3562: ep_len:73 episode reward: total was 33.500000. running mean: -38.793059\n",
      "ep 3562: ep_len:745 episode reward: total was -28.070000. running mean: -38.685828\n",
      "ep 3562: ep_len:2835 episode reward: total was -2.330000. running mean: -38.322270\n",
      "epsilon:0.009992 episode_count: 53572. steps_count: 57755489.000000\n",
      "ep 3563: ep_len:750 episode reward: total was -72.460000. running mean: -38.663647\n",
      "ep 3563: ep_len:1652 episode reward: total was -63.930000. running mean: -38.916311\n",
      "ep 3563: ep_len:70 episode reward: total was 33.500000. running mean: -38.192148\n",
      "ep 3563: ep_len:2932 episode reward: total was -391.520000. running mean: -41.725426\n",
      "ep 3563: ep_len:608 episode reward: total was -19.340000. running mean: -41.501572\n",
      "ep 3563: ep_len:626 episode reward: total was 1.040000. running mean: -41.076156\n",
      "ep 3563: ep_len:302 episode reward: total was 18.660000. running mean: -40.478795\n",
      "ep 3563: ep_len:1165 episode reward: total was -19.370000. running mean: -40.267707\n",
      "ep 3563: ep_len:680 episode reward: total was -16.620000. running mean: -40.031230\n",
      "ep 3563: ep_len:1518 episode reward: total was -5.010000. running mean: -39.681017\n",
      "ep 3563: ep_len:61 episode reward: total was 29.000000. running mean: -38.994207\n",
      "ep 3563: ep_len:142 episode reward: total was 68.000000. running mean: -37.924265\n",
      "ep 3563: ep_len:1148 episode reward: total was 19.760000. running mean: -37.347422\n",
      "ep 3563: ep_len:40 episode reward: total was 18.500000. running mean: -36.788948\n",
      "epsilon:0.009992 episode_count: 53586. steps_count: 57767183.000000\n",
      "ep 3564: ep_len:692 episode reward: total was -8.830000. running mean: -36.509359\n",
      "ep 3564: ep_len:500 episode reward: total was 12.590000. running mean: -36.018365\n",
      "ep 3564: ep_len:2990 episode reward: total was 3.370000. running mean: -35.624481\n",
      "ep 3564: ep_len:1418 episode reward: total was 25.550000. running mean: -35.012737\n",
      "ep 3564: ep_len:47 episode reward: total was 22.000000. running mean: -34.442609\n",
      "ep 3564: ep_len:134 episode reward: total was 65.500000. running mean: -33.443183\n",
      "ep 3564: ep_len:39 episode reward: total was 16.500000. running mean: -32.943751\n",
      "ep 3564: ep_len:1422 episode reward: total was -77.270000. running mean: -33.387014\n",
      "ep 3564: ep_len:3656 episode reward: total was -14.980000. running mean: -33.202944\n",
      "ep 3564: ep_len:834 episode reward: total was -33.090000. running mean: -33.201814\n",
      "ep 3564: ep_len:668 episode reward: total was 12.490000. running mean: -32.744896\n",
      "ep 3564: ep_len:1667 episode reward: total was -204.670000. running mean: -34.464147\n",
      "ep 3564: ep_len:131 episode reward: total was 61.000000. running mean: -33.509506\n",
      "ep 3564: ep_len:54 episode reward: total was 22.500000. running mean: -32.949411\n",
      "ep 3564: ep_len:105 episode reward: total was 51.000000. running mean: -32.109916\n",
      "ep 3564: ep_len:673 episode reward: total was 28.910000. running mean: -31.499717\n",
      "ep 3564: ep_len:2924 episode reward: total was -5.600000. running mean: -31.240720\n",
      "epsilon:0.009992 episode_count: 53603. steps_count: 57785137.000000\n",
      "ep 3565: ep_len:630 episode reward: total was 11.450000. running mean: -30.813813\n",
      "ep 3565: ep_len:925 episode reward: total was -122.400000. running mean: -31.729675\n",
      "ep 3565: ep_len:3126 episode reward: total was -33.280000. running mean: -31.745178\n",
      "ep 3565: ep_len:1197 episode reward: total was -56.880000. running mean: -31.996526\n",
      "ep 3565: ep_len:46 episode reward: total was 21.500000. running mean: -31.461561\n",
      "ep 3565: ep_len:859 episode reward: total was 55.660000. running mean: -30.590345\n",
      "ep 3565: ep_len:3919 episode reward: total was -26.740000. running mean: -30.551842\n",
      "ep 3565: ep_len:690 episode reward: total was -4.680000. running mean: -30.293124\n",
      "ep 3565: ep_len:727 episode reward: total was 27.780000. running mean: -29.712392\n",
      "ep 3565: ep_len:572 episode reward: total was 33.310000. running mean: -29.082168\n",
      "ep 3565: ep_len:80 episode reward: total was 37.000000. running mean: -28.421347\n",
      "ep 3565: ep_len:126 episode reward: total was 60.000000. running mean: -27.537133\n",
      "ep 3565: ep_len:1389 episode reward: total was 9.160000. running mean: -27.170162\n",
      "ep 3565: ep_len:2820 episode reward: total was 4.960000. running mean: -26.848860\n",
      "epsilon:0.009992 episode_count: 53617. steps_count: 57802243.000000\n",
      "ep 3566: ep_len:619 episode reward: total was 10.300000. running mean: -26.477372\n",
      "ep 3566: ep_len:515 episode reward: total was -61.300000. running mean: -26.825598\n",
      "ep 3566: ep_len:2923 episode reward: total was -34.660000. running mean: -26.903942\n",
      "ep 3566: ep_len:519 episode reward: total was -75.260000. running mean: -27.387503\n",
      "ep 3566: ep_len:48 episode reward: total was 22.500000. running mean: -26.888628\n",
      "ep 3566: ep_len:101 episode reward: total was 44.500000. running mean: -26.174741\n",
      "ep 3566: ep_len:642 episode reward: total was 2.210000. running mean: -25.890894\n",
      "ep 3566: ep_len:691 episode reward: total was 17.890000. running mean: -25.453085\n",
      "ep 3566: ep_len:1304 episode reward: total was -53.790000. running mean: -25.736454\n",
      "ep 3566: ep_len:693 episode reward: total was 10.290000. running mean: -25.376190\n",
      "ep 3566: ep_len:603 episode reward: total was 54.500000. running mean: -24.577428\n",
      "ep 3566: ep_len:37 episode reward: total was 17.000000. running mean: -24.161653\n",
      "ep 3566: ep_len:637 episode reward: total was -4.490000. running mean: -23.964937\n",
      "ep 3566: ep_len:2850 episode reward: total was 2.590000. running mean: -23.699387\n",
      "ep 3566: ep_len:60 episode reward: total was 28.500000. running mean: -23.177394\n",
      "epsilon:0.009992 episode_count: 53632. steps_count: 57814485.000000\n",
      "ep 3567: ep_len:1401 episode reward: total was 0.530000. running mean: -22.940320\n",
      "ep 3567: ep_len:1644 episode reward: total was -89.290000. running mean: -23.603816\n",
      "ep 3567: ep_len:3009 episode reward: total was -32.260000. running mean: -23.690378\n",
      "ep 3567: ep_len:500 episode reward: total was 1.640000. running mean: -23.437074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3567: ep_len:91 episode reward: total was 44.000000. running mean: -22.762704\n",
      "ep 3567: ep_len:654 episode reward: total was 2.330000. running mean: -22.511777\n",
      "ep 3567: ep_len:640 episode reward: total was 16.150000. running mean: -22.125159\n",
      "ep 3567: ep_len:502 episode reward: total was -30.500000. running mean: -22.208907\n",
      "ep 3567: ep_len:612 episode reward: total was -1.820000. running mean: -22.005018\n",
      "ep 3567: ep_len:726 episode reward: total was -2.050000. running mean: -21.805468\n",
      "ep 3567: ep_len:108 episode reward: total was 49.500000. running mean: -21.092413\n",
      "ep 3567: ep_len:1514 episode reward: total was 17.730000. running mean: -20.704189\n",
      "ep 3567: ep_len:2755 episode reward: total was -60.340000. running mean: -21.100547\n",
      "epsilon:0.009992 episode_count: 53645. steps_count: 57828641.000000\n",
      "ep 3568: ep_len:1420 episode reward: total was 33.160000. running mean: -20.557942\n",
      "ep 3568: ep_len:704 episode reward: total was -15.000000. running mean: -20.502362\n",
      "ep 3568: ep_len:72 episode reward: total was 34.500000. running mean: -19.952339\n",
      "ep 3568: ep_len:90 episode reward: total was 43.500000. running mean: -19.317815\n",
      "ep 3568: ep_len:618 episode reward: total was -21.460000. running mean: -19.339237\n",
      "ep 3568: ep_len:696 episode reward: total was 18.340000. running mean: -18.962445\n",
      "ep 3568: ep_len:610 episode reward: total was 30.180000. running mean: -18.471020\n",
      "ep 3568: ep_len:914 episode reward: total was -15.050000. running mean: -18.436810\n",
      "ep 3568: ep_len:774 episode reward: total was -88.710000. running mean: -19.139542\n",
      "ep 3568: ep_len:972 episode reward: total was -11.660000. running mean: -19.064747\n",
      "ep 3568: ep_len:1499 episode reward: total was -17.690000. running mean: -19.050999\n",
      "ep 3568: ep_len:2887 episode reward: total was -11.700000. running mean: -18.977489\n",
      "ep 3568: ep_len:54 episode reward: total was 24.000000. running mean: -18.547714\n",
      "epsilon:0.009992 episode_count: 53658. steps_count: 57839951.000000\n",
      "ep 3569: ep_len:1384 episode reward: total was 12.170000. running mean: -18.240537\n",
      "ep 3569: ep_len:1203 episode reward: total was -68.940000. running mean: -18.747532\n",
      "ep 3569: ep_len:2915 episode reward: total was -41.010000. running mean: -18.970157\n",
      "ep 3569: ep_len:805 episode reward: total was -10.300000. running mean: -18.883455\n",
      "ep 3569: ep_len:43 episode reward: total was 20.000000. running mean: -18.494620\n",
      "ep 3569: ep_len:68 episode reward: total was 31.000000. running mean: -17.999674\n",
      "ep 3569: ep_len:71 episode reward: total was 34.000000. running mean: -17.479678\n",
      "ep 3569: ep_len:1426 episode reward: total was -140.830000. running mean: -18.713181\n",
      "ep 3569: ep_len:3920 episode reward: total was -28.750000. running mean: -18.813549\n",
      "ep 3569: ep_len:529 episode reward: total was -40.330000. running mean: -19.028713\n",
      "ep 3569: ep_len:724 episode reward: total was 24.790000. running mean: -18.590526\n",
      "ep 3569: ep_len:939 episode reward: total was 28.240000. running mean: -18.122221\n",
      "ep 3569: ep_len:79 episode reward: total was 38.000000. running mean: -17.560999\n",
      "ep 3569: ep_len:121 episode reward: total was 59.000000. running mean: -16.795389\n",
      "ep 3569: ep_len:505 episode reward: total was 4.830000. running mean: -16.579135\n",
      "ep 3569: ep_len:2906 episode reward: total was -37.300000. running mean: -16.786344\n",
      "epsilon:0.009992 episode_count: 53674. steps_count: 57857589.000000\n",
      "ep 3570: ep_len:1401 episode reward: total was -0.700000. running mean: -16.625480\n",
      "ep 3570: ep_len:932 episode reward: total was 9.360000. running mean: -16.365625\n",
      "ep 3570: ep_len:2953 episode reward: total was -24.890000. running mean: -16.450869\n",
      "ep 3570: ep_len:733 episode reward: total was -17.080000. running mean: -16.457160\n",
      "ep 3570: ep_len:1442 episode reward: total was -94.360000. running mean: -17.236189\n",
      "ep 3570: ep_len:333 episode reward: total was 30.660000. running mean: -16.757227\n",
      "ep 3570: ep_len:590 episode reward: total was -11.190000. running mean: -16.701555\n",
      "ep 3570: ep_len:7293 episode reward: total was -102.730000. running mean: -17.561839\n",
      "ep 3570: ep_len:1116 episode reward: total was -25.370000. running mean: -17.639921\n",
      "ep 3570: ep_len:38 episode reward: total was 16.000000. running mean: -17.303522\n",
      "ep 3570: ep_len:1146 episode reward: total was -44.260000. running mean: -17.573086\n",
      "ep 3570: ep_len:2787 episode reward: total was 14.180000. running mean: -17.255555\n",
      "epsilon:0.009992 episode_count: 53686. steps_count: 57878353.000000\n",
      "ep 3571: ep_len:813 episode reward: total was -13.250000. running mean: -17.215500\n",
      "ep 3571: ep_len:822 episode reward: total was 10.690000. running mean: -16.936445\n",
      "ep 3571: ep_len:2983 episode reward: total was 22.030000. running mean: -16.546780\n",
      "ep 3571: ep_len:508 episode reward: total was -66.800000. running mean: -17.049313\n",
      "ep 3571: ep_len:51 episode reward: total was 24.000000. running mean: -16.638820\n",
      "ep 3571: ep_len:615 episode reward: total was 1.010000. running mean: -16.462331\n",
      "ep 3571: ep_len:340 episode reward: total was 0.550000. running mean: -16.292208\n",
      "ep 3571: ep_len:547 episode reward: total was 4.420000. running mean: -16.085086\n",
      "ep 3571: ep_len:804 episode reward: total was -13.170000. running mean: -16.055935\n",
      "ep 3571: ep_len:870 episode reward: total was 23.010000. running mean: -15.665276\n",
      "ep 3571: ep_len:995 episode reward: total was -44.030000. running mean: -15.948923\n",
      "ep 3571: ep_len:2846 episode reward: total was -4.950000. running mean: -15.838934\n",
      "epsilon:0.009992 episode_count: 53698. steps_count: 57890547.000000\n",
      "ep 3572: ep_len:641 episode reward: total was -0.830000. running mean: -15.688844\n",
      "ep 3572: ep_len:1615 episode reward: total was -35.410000. running mean: -15.886056\n",
      "ep 3572: ep_len:2974 episode reward: total was -42.840000. running mean: -16.155595\n",
      "ep 3572: ep_len:1202 episode reward: total was -34.970000. running mean: -16.343739\n",
      "ep 3572: ep_len:149 episode reward: total was 70.000000. running mean: -15.480302\n",
      "ep 3572: ep_len:1487 episode reward: total was -32.840000. running mean: -15.653899\n",
      "ep 3572: ep_len:332 episode reward: total was 24.010000. running mean: -15.257260\n",
      "ep 3572: ep_len:954 episode reward: total was -43.510000. running mean: -15.539787\n",
      "ep 3572: ep_len:792 episode reward: total was -15.530000. running mean: -15.539690\n",
      "ep 3572: ep_len:722 episode reward: total was -9.730000. running mean: -15.481593\n",
      "ep 3572: ep_len:1120 episode reward: total was -34.140000. running mean: -15.668177\n",
      "ep 3572: ep_len:2783 episode reward: total was -16.470000. running mean: -15.676195\n",
      "ep 3572: ep_len:41 episode reward: total was 19.000000. running mean: -15.329433\n",
      "epsilon:0.009992 episode_count: 53711. steps_count: 57905359.000000\n",
      "ep 3573: ep_len:1451 episode reward: total was 26.920000. running mean: -14.906939\n",
      "ep 3573: ep_len:500 episode reward: total was 30.680000. running mean: -14.451069\n",
      "ep 3573: ep_len:2902 episode reward: total was -38.540000. running mean: -14.691959\n",
      "ep 3573: ep_len:688 episode reward: total was 4.070000. running mean: -14.504339\n",
      "ep 3573: ep_len:45 episode reward: total was 21.000000. running mean: -14.149296\n",
      "ep 3573: ep_len:149 episode reward: total was 71.500000. running mean: -13.292803\n",
      "ep 3573: ep_len:761 episode reward: total was -11.780000. running mean: -13.277675\n",
      "ep 3573: ep_len:4103 episode reward: total was -229.420000. running mean: -15.439098\n",
      "ep 3573: ep_len:1580 episode reward: total was -42.440000. running mean: -15.709107\n",
      "ep 3573: ep_len:628 episode reward: total was -3.870000. running mean: -15.590716\n",
      "ep 3573: ep_len:655 episode reward: total was 1.330000. running mean: -15.421509\n",
      "ep 3573: ep_len:52 episode reward: total was 23.000000. running mean: -15.037294\n",
      "ep 3573: ep_len:628 episode reward: total was -13.080000. running mean: -15.017721\n",
      "ep 3573: ep_len:2801 episode reward: total was -1.390000. running mean: -14.881443\n",
      "epsilon:0.009992 episode_count: 53725. steps_count: 57922302.000000\n",
      "ep 3574: ep_len:1489 episode reward: total was 22.830000. running mean: -14.504329\n",
      "ep 3574: ep_len:1665 episode reward: total was -27.990000. running mean: -14.639186\n",
      "ep 3574: ep_len:3034 episode reward: total was -0.080000. running mean: -14.493594\n",
      "ep 3574: ep_len:500 episode reward: total was -7.040000. running mean: -14.419058\n",
      "ep 3574: ep_len:1086 episode reward: total was -2.350000. running mean: -14.298367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3574: ep_len:3890 episode reward: total was -53.410000. running mean: -14.689484\n",
      "ep 3574: ep_len:617 episode reward: total was 14.000000. running mean: -14.402589\n",
      "ep 3574: ep_len:776 episode reward: total was 16.180000. running mean: -14.096763\n",
      "ep 3574: ep_len:537 episode reward: total was 31.420000. running mean: -13.641595\n",
      "ep 3574: ep_len:51 episode reward: total was 24.000000. running mean: -13.265179\n",
      "ep 3574: ep_len:88 episode reward: total was 41.000000. running mean: -12.722528\n",
      "ep 3574: ep_len:772 episode reward: total was -30.830000. running mean: -12.903602\n",
      "ep 3574: ep_len:2841 episode reward: total was -7.570000. running mean: -12.850266\n",
      "ep 3574: ep_len:44 episode reward: total was 20.500000. running mean: -12.516764\n",
      "epsilon:0.009992 episode_count: 53739. steps_count: 57939692.000000\n",
      "ep 3575: ep_len:500 episode reward: total was 14.300000. running mean: -12.248596\n",
      "ep 3575: ep_len:675 episode reward: total was -26.750000. running mean: -12.393610\n",
      "ep 3575: ep_len:3030 episode reward: total was -35.030000. running mean: -12.619974\n",
      "ep 3575: ep_len:1471 episode reward: total was 22.680000. running mean: -12.266974\n",
      "ep 3575: ep_len:675 episode reward: total was 25.020000. running mean: -11.894104\n",
      "ep 3575: ep_len:337 episode reward: total was 11.200000. running mean: -11.663163\n",
      "ep 3575: ep_len:1541 episode reward: total was -25.840000. running mean: -11.804932\n",
      "ep 3575: ep_len:685 episode reward: total was 14.220000. running mean: -11.544682\n",
      "ep 3575: ep_len:1491 episode reward: total was 10.240000. running mean: -11.326836\n",
      "ep 3575: ep_len:111 episode reward: total was 52.500000. running mean: -10.688567\n",
      "ep 3575: ep_len:848 episode reward: total was 14.790000. running mean: -10.433782\n",
      "ep 3575: ep_len:2898 episode reward: total was -18.720000. running mean: -10.516644\n",
      "epsilon:0.009992 episode_count: 53751. steps_count: 57953954.000000\n",
      "ep 3576: ep_len:1004 episode reward: total was -38.800000. running mean: -10.799477\n",
      "ep 3576: ep_len:657 episode reward: total was -4.300000. running mean: -10.734483\n",
      "ep 3576: ep_len:41 episode reward: total was 19.000000. running mean: -10.437138\n",
      "ep 3576: ep_len:3025 episode reward: total was -21.820000. running mean: -10.550966\n",
      "ep 3576: ep_len:621 episode reward: total was -0.370000. running mean: -10.449157\n",
      "ep 3576: ep_len:114 episode reward: total was 55.500000. running mean: -9.789665\n",
      "ep 3576: ep_len:500 episode reward: total was -42.840000. running mean: -10.120168\n",
      "ep 3576: ep_len:3967 episode reward: total was -74.650000. running mean: -10.765467\n",
      "ep 3576: ep_len:576 episode reward: total was -11.580000. running mean: -10.773612\n",
      "ep 3576: ep_len:7362 episode reward: total was 36.930000. running mean: -10.296576\n",
      "ep 3576: ep_len:1007 episode reward: total was 8.650000. running mean: -10.107110\n",
      "ep 3576: ep_len:67 episode reward: total was 32.000000. running mean: -9.686039\n",
      "ep 3576: ep_len:42 episode reward: total was 18.000000. running mean: -9.409179\n",
      "ep 3576: ep_len:1064 episode reward: total was -14.570000. running mean: -9.460787\n",
      "ep 3576: ep_len:2809 episode reward: total was -8.010000. running mean: -9.446279\n",
      "epsilon:0.009992 episode_count: 53766. steps_count: 57976810.000000\n",
      "ep 3577: ep_len:741 episode reward: total was -6.640000. running mean: -9.418216\n",
      "ep 3577: ep_len:673 episode reward: total was -20.710000. running mean: -9.531134\n",
      "ep 3577: ep_len:72 episode reward: total was 34.500000. running mean: -9.090823\n",
      "ep 3577: ep_len:3035 episode reward: total was -15.500000. running mean: -9.154915\n",
      "ep 3577: ep_len:670 episode reward: total was 15.750000. running mean: -8.905865\n",
      "ep 3577: ep_len:79 episode reward: total was 36.500000. running mean: -8.451807\n",
      "ep 3577: ep_len:903 episode reward: total was 28.390000. running mean: -8.083389\n",
      "ep 3577: ep_len:334 episode reward: total was 7.410000. running mean: -7.928455\n",
      "ep 3577: ep_len:1256 episode reward: total was -48.210000. running mean: -8.331270\n",
      "ep 3577: ep_len:782 episode reward: total was 19.140000. running mean: -8.056558\n",
      "ep 3577: ep_len:1549 episode reward: total was 5.840000. running mean: -7.917592\n",
      "ep 3577: ep_len:113 episode reward: total was 52.000000. running mean: -7.318416\n",
      "ep 3577: ep_len:2375 episode reward: total was -310.770000. running mean: -10.352932\n",
      "ep 3577: ep_len:2861 episode reward: total was -5.560000. running mean: -10.305003\n",
      "ep 3577: ep_len:19 episode reward: total was 8.000000. running mean: -10.121953\n",
      "epsilon:0.009992 episode_count: 53781. steps_count: 57992272.000000\n",
      "ep 3578: ep_len:1202 episode reward: total was 13.760000. running mean: -9.883133\n",
      "ep 3578: ep_len:500 episode reward: total was 17.980000. running mean: -9.604502\n",
      "ep 3578: ep_len:2889 episode reward: total was -91.410000. running mean: -10.422557\n",
      "ep 3578: ep_len:769 episode reward: total was -32.880000. running mean: -10.647131\n",
      "ep 3578: ep_len:137 episode reward: total was 67.000000. running mean: -9.870660\n",
      "ep 3578: ep_len:59 episode reward: total was 26.500000. running mean: -9.506953\n",
      "ep 3578: ep_len:47 episode reward: total was 22.000000. running mean: -9.191884\n",
      "ep 3578: ep_len:689 episode reward: total was 31.300000. running mean: -8.786965\n",
      "ep 3578: ep_len:3779 episode reward: total was -79.650000. running mean: -9.495595\n",
      "ep 3578: ep_len:560 episode reward: total was 2.800000. running mean: -9.372639\n",
      "ep 3578: ep_len:718 episode reward: total was -3.600000. running mean: -9.314913\n",
      "ep 3578: ep_len:733 episode reward: total was -6.980000. running mean: -9.291564\n",
      "ep 3578: ep_len:84 episode reward: total was 40.500000. running mean: -8.793648\n",
      "ep 3578: ep_len:500 episode reward: total was 6.310000. running mean: -8.642612\n",
      "ep 3578: ep_len:2860 episode reward: total was -7.070000. running mean: -8.626885\n",
      "epsilon:0.009992 episode_count: 53796. steps_count: 58007798.000000\n",
      "ep 3579: ep_len:729 episode reward: total was -54.490000. running mean: -9.085517\n",
      "ep 3579: ep_len:1628 episode reward: total was -51.070000. running mean: -9.505361\n",
      "ep 3579: ep_len:47 episode reward: total was 22.000000. running mean: -9.190308\n",
      "ep 3579: ep_len:2972 episode reward: total was 5.080000. running mean: -9.047605\n",
      "ep 3579: ep_len:579 episode reward: total was -8.520000. running mean: -9.042329\n",
      "ep 3579: ep_len:63 episode reward: total was 27.000000. running mean: -8.681905\n",
      "ep 3579: ep_len:1145 episode reward: total was 6.480000. running mean: -8.530286\n",
      "ep 3579: ep_len:4140 episode reward: total was -36.320000. running mean: -8.808184\n",
      "ep 3579: ep_len:536 episode reward: total was -15.010000. running mean: -8.870202\n",
      "ep 3579: ep_len:851 episode reward: total was 41.400000. running mean: -8.367500\n",
      "ep 3579: ep_len:4526 episode reward: total was -1966.640000. running mean: -27.950225\n",
      "ep 3579: ep_len:69 episode reward: total was 33.000000. running mean: -27.340722\n",
      "ep 3579: ep_len:46 episode reward: total was 20.000000. running mean: -26.867315\n",
      "ep 3579: ep_len:636 episode reward: total was 6.140000. running mean: -26.537242\n",
      "ep 3579: ep_len:2888 episode reward: total was -25.690000. running mean: -26.528770\n",
      "epsilon:0.009992 episode_count: 53811. steps_count: 58028653.000000\n",
      "ep 3580: ep_len:788 episode reward: total was -25.100000. running mean: -26.514482\n",
      "ep 3580: ep_len:923 episode reward: total was 5.110000. running mean: -26.198237\n",
      "ep 3580: ep_len:38 episode reward: total was 17.500000. running mean: -25.761255\n",
      "ep 3580: ep_len:2885 episode reward: total was 4.000000. running mean: -25.463642\n",
      "ep 3580: ep_len:653 episode reward: total was -16.870000. running mean: -25.377706\n",
      "ep 3580: ep_len:737 episode reward: total was -5.410000. running mean: -25.178029\n",
      "ep 3580: ep_len:631 episode reward: total was 16.890000. running mean: -24.757348\n",
      "ep 3580: ep_len:591 episode reward: total was -9.130000. running mean: -24.601075\n",
      "ep 3580: ep_len:758 episode reward: total was 24.780000. running mean: -24.107264\n",
      "ep 3580: ep_len:646 episode reward: total was 7.780000. running mean: -23.788392\n",
      "ep 3580: ep_len:1408 episode reward: total was 11.920000. running mean: -23.431308\n",
      "ep 3580: ep_len:2854 episode reward: total was -2.390000. running mean: -23.220895\n",
      "ep 3580: ep_len:72 episode reward: total was 34.500000. running mean: -22.643686\n",
      "epsilon:0.009992 episode_count: 53824. steps_count: 58041637.000000\n",
      "ep 3581: ep_len:670 episode reward: total was -20.490000. running mean: -22.622149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3581: ep_len:1127 episode reward: total was 2.340000. running mean: -22.372527\n",
      "ep 3581: ep_len:2973 episode reward: total was -35.990000. running mean: -22.508702\n",
      "ep 3581: ep_len:678 episode reward: total was 6.830000. running mean: -22.215315\n",
      "ep 3581: ep_len:72 episode reward: total was 33.000000. running mean: -21.663162\n",
      "ep 3581: ep_len:500 episode reward: total was 32.610000. running mean: -21.120430\n",
      "ep 3581: ep_len:330 episode reward: total was 20.470000. running mean: -20.704526\n",
      "ep 3581: ep_len:673 episode reward: total was -9.830000. running mean: -20.595781\n",
      "ep 3581: ep_len:693 episode reward: total was 36.040000. running mean: -20.029423\n",
      "ep 3581: ep_len:1187 episode reward: total was -25.670000. running mean: -20.085829\n",
      "ep 3581: ep_len:121 episode reward: total was 59.000000. running mean: -19.294970\n",
      "ep 3581: ep_len:753 episode reward: total was -52.230000. running mean: -19.624321\n",
      "ep 3581: ep_len:2825 episode reward: total was -18.220000. running mean: -19.610277\n",
      "epsilon:0.009992 episode_count: 53837. steps_count: 58054239.000000\n",
      "ep 3582: ep_len:1152 episode reward: total was 0.270000. running mean: -19.411475\n",
      "ep 3582: ep_len:500 episode reward: total was 35.610000. running mean: -18.861260\n",
      "ep 3582: ep_len:43 episode reward: total was 18.500000. running mean: -18.487647\n",
      "ep 3582: ep_len:3054 episode reward: total was -17.390000. running mean: -18.476671\n",
      "ep 3582: ep_len:500 episode reward: total was -2.300000. running mean: -18.314904\n",
      "ep 3582: ep_len:112 episode reward: total was 53.000000. running mean: -17.601755\n",
      "ep 3582: ep_len:48 episode reward: total was 22.500000. running mean: -17.200738\n",
      "ep 3582: ep_len:1382 episode reward: total was 12.180000. running mean: -16.906930\n",
      "ep 3582: ep_len:4156 episode reward: total was -84.760000. running mean: -17.585461\n",
      "ep 3582: ep_len:1239 episode reward: total was -50.400000. running mean: -17.913606\n",
      "ep 3582: ep_len:727 episode reward: total was 23.650000. running mean: -17.497970\n",
      "ep 3582: ep_len:1451 episode reward: total was 5.520000. running mean: -17.267790\n",
      "ep 3582: ep_len:54 episode reward: total was 22.500000. running mean: -16.870113\n",
      "ep 3582: ep_len:99 episode reward: total was 46.500000. running mean: -16.236411\n",
      "ep 3582: ep_len:606 episode reward: total was 13.290000. running mean: -15.941147\n",
      "ep 3582: ep_len:2941 episode reward: total was -29.740000. running mean: -16.079136\n",
      "epsilon:0.009992 episode_count: 53853. steps_count: 58072303.000000\n",
      "ep 3583: ep_len:1005 episode reward: total was -182.610000. running mean: -17.744445\n",
      "ep 3583: ep_len:719 episode reward: total was -10.780000. running mean: -17.674800\n",
      "ep 3583: ep_len:63 episode reward: total was 30.000000. running mean: -17.198052\n",
      "ep 3583: ep_len:2830 episode reward: total was -61.120000. running mean: -17.637272\n",
      "ep 3583: ep_len:517 episode reward: total was 22.130000. running mean: -17.239599\n",
      "ep 3583: ep_len:33 episode reward: total was 15.000000. running mean: -16.917203\n",
      "ep 3583: ep_len:170 episode reward: total was 80.500000. running mean: -15.943031\n",
      "ep 3583: ep_len:77 episode reward: total was 35.500000. running mean: -15.428600\n",
      "ep 3583: ep_len:1163 episode reward: total was 12.540000. running mean: -15.148914\n",
      "ep 3583: ep_len:4013 episode reward: total was -46.520000. running mean: -15.462625\n",
      "ep 3583: ep_len:682 episode reward: total was -76.170000. running mean: -16.069699\n",
      "ep 3583: ep_len:7342 episode reward: total was 10.270000. running mean: -15.806302\n",
      "ep 3583: ep_len:918 episode reward: total was 20.060000. running mean: -15.447639\n",
      "ep 3583: ep_len:45 episode reward: total was 21.000000. running mean: -15.083163\n",
      "ep 3583: ep_len:943 episode reward: total was -23.250000. running mean: -15.164831\n",
      "ep 3583: ep_len:2887 episode reward: total was 6.450000. running mean: -14.948683\n",
      "ep 3583: ep_len:58 episode reward: total was 27.500000. running mean: -14.524196\n",
      "epsilon:0.009992 episode_count: 53870. steps_count: 58095768.000000\n",
      "ep 3584: ep_len:1002 episode reward: total was -61.860000. running mean: -14.997554\n",
      "ep 3584: ep_len:500 episode reward: total was 15.100000. running mean: -14.696578\n",
      "ep 3584: ep_len:80 episode reward: total was 38.500000. running mean: -14.164613\n",
      "ep 3584: ep_len:3056 episode reward: total was 3.750000. running mean: -13.985467\n",
      "ep 3584: ep_len:500 episode reward: total was 5.020000. running mean: -13.795412\n",
      "ep 3584: ep_len:98 episode reward: total was 47.500000. running mean: -13.182458\n",
      "ep 3584: ep_len:62 episode reward: total was 29.500000. running mean: -12.755633\n",
      "ep 3584: ep_len:1396 episode reward: total was -29.840000. running mean: -12.926477\n",
      "ep 3584: ep_len:360 episode reward: total was 13.700000. running mean: -12.660212\n",
      "ep 3584: ep_len:1538 episode reward: total was -38.820000. running mean: -12.921810\n",
      "ep 3584: ep_len:795 episode reward: total was 45.300000. running mean: -12.339592\n",
      "ep 3584: ep_len:690 episode reward: total was -1.350000. running mean: -12.229696\n",
      "ep 3584: ep_len:101 episode reward: total was 49.000000. running mean: -11.617399\n",
      "ep 3584: ep_len:717 episode reward: total was -131.310000. running mean: -12.814325\n",
      "ep 3584: ep_len:2851 episode reward: total was -4.370000. running mean: -12.729882\n",
      "epsilon:0.009992 episode_count: 53885. steps_count: 58109514.000000\n",
      "ep 3585: ep_len:712 episode reward: total was -31.000000. running mean: -12.912583\n",
      "ep 3585: ep_len:207 episode reward: total was 12.040000. running mean: -12.663057\n",
      "ep 3585: ep_len:2916 episode reward: total was -52.850000. running mean: -13.064927\n",
      "ep 3585: ep_len:529 episode reward: total was -8.500000. running mean: -13.019277\n",
      "ep 3585: ep_len:1074 episode reward: total was -54.050000. running mean: -13.429584\n",
      "ep 3585: ep_len:3953 episode reward: total was -116.840000. running mean: -14.463689\n",
      "ep 3585: ep_len:904 episode reward: total was -51.270000. running mean: -14.831752\n",
      "ep 3585: ep_len:763 episode reward: total was 3.210000. running mean: -14.651334\n",
      "ep 3585: ep_len:500 episode reward: total was 35.280000. running mean: -14.152021\n",
      "ep 3585: ep_len:112 episode reward: total was 53.000000. running mean: -13.480501\n",
      "ep 3585: ep_len:55 episode reward: total was 26.000000. running mean: -13.085696\n",
      "ep 3585: ep_len:640 episode reward: total was 14.270000. running mean: -12.812139\n",
      "ep 3585: ep_len:2789 episode reward: total was -20.880000. running mean: -12.892817\n",
      "ep 3585: ep_len:56 episode reward: total was 25.000000. running mean: -12.513889\n",
      "epsilon:0.009992 episode_count: 53899. steps_count: 58124724.000000\n",
      "ep 3586: ep_len:1088 episode reward: total was -26.970000. running mean: -12.658450\n",
      "ep 3586: ep_len:753 episode reward: total was -49.200000. running mean: -13.023866\n",
      "ep 3586: ep_len:2951 episode reward: total was -4.370000. running mean: -12.937327\n",
      "ep 3586: ep_len:672 episode reward: total was -26.780000. running mean: -13.075754\n",
      "ep 3586: ep_len:157 episode reward: total was 75.500000. running mean: -12.189996\n",
      "ep 3586: ep_len:75 episode reward: total was 36.000000. running mean: -11.708096\n",
      "ep 3586: ep_len:670 episode reward: total was 8.270000. running mean: -11.508315\n",
      "ep 3586: ep_len:3823 episode reward: total was -79.700000. running mean: -12.190232\n",
      "ep 3586: ep_len:1539 episode reward: total was 0.620000. running mean: -12.062130\n",
      "ep 3586: ep_len:731 episode reward: total was 56.630000. running mean: -11.375209\n",
      "ep 3586: ep_len:632 episode reward: total was 4.630000. running mean: -11.215156\n",
      "ep 3586: ep_len:120 episode reward: total was 55.500000. running mean: -10.548005\n",
      "ep 3586: ep_len:60 episode reward: total was 28.500000. running mean: -10.157525\n",
      "ep 3586: ep_len:87 episode reward: total was 40.500000. running mean: -9.650950\n",
      "ep 3586: ep_len:1070 episode reward: total was -6.640000. running mean: -9.620840\n",
      "ep 3586: ep_len:2852 episode reward: total was -41.000000. running mean: -9.934632\n",
      "epsilon:0.009992 episode_count: 53915. steps_count: 58142004.000000\n",
      "ep 3587: ep_len:1464 episode reward: total was 12.630000. running mean: -9.708985\n",
      "ep 3587: ep_len:936 episode reward: total was -56.710000. running mean: -10.178996\n",
      "ep 3587: ep_len:3009 episode reward: total was -42.480000. running mean: -10.502006\n",
      "ep 3587: ep_len:819 episode reward: total was -26.270000. running mean: -10.659686\n",
      "ep 3587: ep_len:88 episode reward: total was 42.500000. running mean: -10.128089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3587: ep_len:55 episode reward: total was 26.000000. running mean: -9.766808\n",
      "ep 3587: ep_len:995 episode reward: total was 0.020000. running mean: -9.668940\n",
      "ep 3587: ep_len:636 episode reward: total was 14.430000. running mean: -9.427950\n",
      "ep 3587: ep_len:503 episode reward: total was 3.850000. running mean: -9.295171\n",
      "ep 3587: ep_len:736 episode reward: total was 48.140000. running mean: -8.720819\n",
      "ep 3587: ep_len:767 episode reward: total was -0.580000. running mean: -8.639411\n",
      "ep 3587: ep_len:59 episode reward: total was 28.000000. running mean: -8.273017\n",
      "ep 3587: ep_len:83 episode reward: total was 38.500000. running mean: -7.805287\n",
      "ep 3587: ep_len:1502 episode reward: total was 9.370000. running mean: -7.633534\n",
      "ep 3587: ep_len:2897 episode reward: total was -13.580000. running mean: -7.692998\n",
      "ep 3587: ep_len:68 episode reward: total was 32.500000. running mean: -7.291068\n",
      "epsilon:0.009992 episode_count: 53931. steps_count: 58156621.000000\n",
      "ep 3588: ep_len:758 episode reward: total was -43.090000. running mean: -7.649058\n",
      "ep 3588: ep_len:666 episode reward: total was -6.260000. running mean: -7.635167\n",
      "ep 3588: ep_len:2870 episode reward: total was -15.230000. running mean: -7.711116\n",
      "ep 3588: ep_len:717 episode reward: total was -31.260000. running mean: -7.946604\n",
      "ep 3588: ep_len:75 episode reward: total was 36.000000. running mean: -7.507138\n",
      "ep 3588: ep_len:40 episode reward: total was 18.500000. running mean: -7.247067\n",
      "ep 3588: ep_len:500 episode reward: total was 35.280000. running mean: -6.821796\n",
      "ep 3588: ep_len:3892 episode reward: total was -30.940000. running mean: -7.062978\n",
      "ep 3588: ep_len:653 episode reward: total was -13.800000. running mean: -7.130349\n",
      "ep 3588: ep_len:778 episode reward: total was 8.720000. running mean: -6.971845\n",
      "ep 3588: ep_len:895 episode reward: total was 27.880000. running mean: -6.623327\n",
      "ep 3588: ep_len:77 episode reward: total was 37.000000. running mean: -6.187093\n",
      "ep 3588: ep_len:31 episode reward: total was 14.000000. running mean: -5.985222\n",
      "ep 3588: ep_len:912 episode reward: total was 32.980000. running mean: -5.595570\n",
      "ep 3588: ep_len:2788 episode reward: total was -47.030000. running mean: -6.009914\n",
      "epsilon:0.009992 episode_count: 53946. steps_count: 58172273.000000\n",
      "ep 3589: ep_len:1402 episode reward: total was 38.060000. running mean: -5.569215\n",
      "ep 3589: ep_len:204 episode reward: total was 12.900000. running mean: -5.384523\n",
      "ep 3589: ep_len:3076 episode reward: total was -73.550000. running mean: -6.066178\n",
      "ep 3589: ep_len:500 episode reward: total was -12.430000. running mean: -6.129816\n",
      "ep 3589: ep_len:145 episode reward: total was 69.500000. running mean: -5.373518\n",
      "ep 3589: ep_len:1456 episode reward: total was -5.480000. running mean: -5.374583\n",
      "ep 3589: ep_len:3867 episode reward: total was 16.280000. running mean: -5.158037\n",
      "ep 3589: ep_len:544 episode reward: total was 1.230000. running mean: -5.094157\n",
      "ep 3589: ep_len:738 episode reward: total was 9.970000. running mean: -4.943515\n",
      "ep 3589: ep_len:553 episode reward: total was -7.770000. running mean: -4.971780\n",
      "ep 3589: ep_len:68 episode reward: total was 26.500000. running mean: -4.657062\n",
      "ep 3589: ep_len:42 episode reward: total was 19.500000. running mean: -4.415491\n",
      "ep 3589: ep_len:715 episode reward: total was -120.280000. running mean: -5.574137\n",
      "ep 3589: ep_len:2859 episode reward: total was -14.030000. running mean: -5.658695\n",
      "epsilon:0.009992 episode_count: 53960. steps_count: 58188442.000000\n",
      "ep 3590: ep_len:629 episode reward: total was 1.070000. running mean: -5.591408\n",
      "ep 3590: ep_len:657 episode reward: total was -44.720000. running mean: -5.982694\n",
      "ep 3590: ep_len:60 episode reward: total was 24.000000. running mean: -5.682867\n",
      "ep 3590: ep_len:2977 episode reward: total was -93.590000. running mean: -6.561939\n",
      "ep 3590: ep_len:500 episode reward: total was 11.240000. running mean: -6.383919\n",
      "ep 3590: ep_len:35 episode reward: total was 16.000000. running mean: -6.160080\n",
      "ep 3590: ep_len:59 episode reward: total was 28.000000. running mean: -5.818479\n",
      "ep 3590: ep_len:1378 episode reward: total was -76.280000. running mean: -6.523094\n",
      "ep 3590: ep_len:3905 episode reward: total was -109.150000. running mean: -7.549363\n",
      "ep 3590: ep_len:640 episode reward: total was 4.210000. running mean: -7.431770\n",
      "ep 3590: ep_len:717 episode reward: total was 38.850000. running mean: -6.968952\n",
      "ep 3590: ep_len:669 episode reward: total was -30.850000. running mean: -7.207763\n",
      "ep 3590: ep_len:107 episode reward: total was 52.000000. running mean: -6.615685\n",
      "ep 3590: ep_len:598 episode reward: total was -8.330000. running mean: -6.632828\n",
      "ep 3590: ep_len:2808 episode reward: total was -28.590000. running mean: -6.852400\n",
      "epsilon:0.009992 episode_count: 53975. steps_count: 58204181.000000\n",
      "ep 3591: ep_len:693 episode reward: total was -18.490000. running mean: -6.968776\n",
      "ep 3591: ep_len:928 episode reward: total was 30.260000. running mean: -6.596488\n",
      "ep 3591: ep_len:79 episode reward: total was 35.000000. running mean: -6.180523\n",
      "ep 3591: ep_len:3010 episode reward: total was -102.940000. running mean: -7.148118\n",
      "ep 3591: ep_len:567 episode reward: total was -5.610000. running mean: -7.132737\n",
      "ep 3591: ep_len:57 episode reward: total was 27.000000. running mean: -6.791409\n",
      "ep 3591: ep_len:632 episode reward: total was -3.950000. running mean: -6.762995\n",
      "ep 3591: ep_len:3949 episode reward: total was -36.510000. running mean: -7.060465\n",
      "ep 3591: ep_len:1533 episode reward: total was -17.100000. running mean: -7.160861\n",
      "ep 3591: ep_len:742 episode reward: total was -14.850000. running mean: -7.237752\n",
      "ep 3591: ep_len:546 episode reward: total was 3.340000. running mean: -7.131975\n",
      "ep 3591: ep_len:164 episode reward: total was -162.990000. running mean: -8.690555\n",
      "ep 3591: ep_len:30 episode reward: total was 13.500000. running mean: -8.468649\n",
      "ep 3591: ep_len:89 episode reward: total was 40.000000. running mean: -7.983963\n",
      "ep 3591: ep_len:771 episode reward: total was -13.670000. running mean: -8.040823\n",
      "ep 3591: ep_len:2815 episode reward: total was 1.910000. running mean: -7.941315\n",
      "epsilon:0.009992 episode_count: 53991. steps_count: 58220786.000000\n",
      "ep 3592: ep_len:721 episode reward: total was -19.220000. running mean: -8.054102\n",
      "ep 3592: ep_len:981 episode reward: total was 31.960000. running mean: -7.653961\n",
      "ep 3592: ep_len:69 episode reward: total was 31.500000. running mean: -7.262421\n",
      "ep 3592: ep_len:3027 episode reward: total was -41.700000. running mean: -7.606797\n",
      "ep 3592: ep_len:1475 episode reward: total was -0.940000. running mean: -7.540129\n",
      "ep 3592: ep_len:67 episode reward: total was 30.500000. running mean: -7.159728\n",
      "ep 3592: ep_len:1076 episode reward: total was -1.530000. running mean: -7.103430\n",
      "ep 3592: ep_len:662 episode reward: total was 21.270000. running mean: -6.819696\n",
      "ep 3592: ep_len:3639 episode reward: total was -280.200000. running mean: -9.553499\n",
      "ep 3592: ep_len:851 episode reward: total was 45.130000. running mean: -9.006664\n",
      "ep 3592: ep_len:663 episode reward: total was 0.820000. running mean: -8.908398\n",
      "ep 3592: ep_len:81 episode reward: total was 36.000000. running mean: -8.459314\n",
      "ep 3592: ep_len:631 episode reward: total was 18.100000. running mean: -8.193720\n",
      "ep 3592: ep_len:2870 episode reward: total was -14.160000. running mean: -8.253383\n",
      "epsilon:0.009992 episode_count: 54005. steps_count: 58237599.000000\n",
      "ep 3593: ep_len:623 episode reward: total was -21.210000. running mean: -8.382949\n",
      "ep 3593: ep_len:940 episode reward: total was -3.160000. running mean: -8.330720\n",
      "ep 3593: ep_len:51 episode reward: total was 24.000000. running mean: -8.007413\n",
      "ep 3593: ep_len:3060 episode reward: total was -36.590000. running mean: -8.293239\n",
      "ep 3593: ep_len:1232 episode reward: total was -1.090000. running mean: -8.221206\n",
      "ep 3593: ep_len:113 episode reward: total was 55.000000. running mean: -7.588994\n",
      "ep 3593: ep_len:43 episode reward: total was 20.000000. running mean: -7.313104\n",
      "ep 3593: ep_len:1631 episode reward: total was -321.470000. running mean: -10.454673\n",
      "ep 3593: ep_len:3669 episode reward: total was -48.800000. running mean: -10.838126\n",
      "ep 3593: ep_len:555 episode reward: total was 13.220000. running mean: -10.597545\n",
      "ep 3593: ep_len:661 episode reward: total was 10.850000. running mean: -10.383070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3593: ep_len:528 episode reward: total was -13.500000. running mean: -10.414239\n",
      "ep 3593: ep_len:38 episode reward: total was 17.500000. running mean: -10.135097\n",
      "ep 3593: ep_len:1110 episode reward: total was -4.890000. running mean: -10.082646\n",
      "ep 3593: ep_len:2842 episode reward: total was -64.090000. running mean: -10.622719\n",
      "ep 3593: ep_len:52 episode reward: total was 24.500000. running mean: -10.271492\n",
      "epsilon:0.009992 episode_count: 54021. steps_count: 58254747.000000\n",
      "ep 3594: ep_len:1367 episode reward: total was 22.220000. running mean: -9.946577\n",
      "ep 3594: ep_len:682 episode reward: total was -32.590000. running mean: -10.173011\n",
      "ep 3594: ep_len:2973 episode reward: total was -87.020000. running mean: -10.941481\n",
      "ep 3594: ep_len:514 episode reward: total was -38.460000. running mean: -11.216666\n",
      "ep 3594: ep_len:112 episode reward: total was 51.500000. running mean: -10.589500\n",
      "ep 3594: ep_len:50 episode reward: total was 22.000000. running mean: -10.263605\n",
      "ep 3594: ep_len:2403 episode reward: total was -1102.780000. running mean: -21.188769\n",
      "ep 3594: ep_len:4029 episode reward: total was -48.380000. running mean: -21.460681\n",
      "ep 3594: ep_len:1237 episode reward: total was -28.200000. running mean: -21.528074\n",
      "ep 3594: ep_len:664 episode reward: total was 41.660000. running mean: -20.896193\n",
      "ep 3594: ep_len:610 episode reward: total was 16.880000. running mean: -20.518431\n",
      "ep 3594: ep_len:106 episode reward: total was 51.500000. running mean: -19.798247\n",
      "ep 3594: ep_len:772 episode reward: total was -45.980000. running mean: -20.060065\n",
      "ep 3594: ep_len:2914 episode reward: total was -9.900000. running mean: -19.958464\n",
      "epsilon:0.009992 episode_count: 54035. steps_count: 58273180.000000\n",
      "ep 3595: ep_len:567 episode reward: total was -25.810000. running mean: -20.016979\n",
      "ep 3595: ep_len:715 episode reward: total was -19.180000. running mean: -20.008610\n",
      "ep 3595: ep_len:2990 episode reward: total was -8.650000. running mean: -19.895024\n",
      "ep 3595: ep_len:1258 episode reward: total was -27.990000. running mean: -19.975973\n",
      "ep 3595: ep_len:39 episode reward: total was 16.500000. running mean: -19.611214\n",
      "ep 3595: ep_len:98 episode reward: total was 46.000000. running mean: -18.955101\n",
      "ep 3595: ep_len:61 episode reward: total was 29.000000. running mean: -18.475550\n",
      "ep 3595: ep_len:52 episode reward: total was 24.500000. running mean: -18.045795\n",
      "ep 3595: ep_len:903 episode reward: total was 44.800000. running mean: -17.417337\n",
      "ep 3595: ep_len:3923 episode reward: total was -82.490000. running mean: -18.068064\n",
      "ep 3595: ep_len:921 episode reward: total was -36.410000. running mean: -18.251483\n",
      "ep 3595: ep_len:822 episode reward: total was 13.300000. running mean: -17.935968\n",
      "ep 3595: ep_len:627 episode reward: total was -14.100000. running mean: -17.897608\n",
      "ep 3595: ep_len:100 episode reward: total was 48.500000. running mean: -17.233632\n",
      "ep 3595: ep_len:40 episode reward: total was 18.500000. running mean: -16.876296\n",
      "ep 3595: ep_len:695 episode reward: total was -7.360000. running mean: -16.781133\n",
      "ep 3595: ep_len:2944 episode reward: total was -0.960000. running mean: -16.622922\n",
      "ep 3595: ep_len:58 episode reward: total was 23.000000. running mean: -16.226693\n",
      "epsilon:0.009992 episode_count: 54053. steps_count: 58289993.000000\n",
      "ep 3596: ep_len:1418 episode reward: total was -0.440000. running mean: -16.068826\n",
      "ep 3596: ep_len:1136 episode reward: total was 8.770000. running mean: -15.820437\n",
      "ep 3596: ep_len:3033 episode reward: total was 1.960000. running mean: -15.642633\n",
      "ep 3596: ep_len:500 episode reward: total was 5.230000. running mean: -15.433907\n",
      "ep 3596: ep_len:42 episode reward: total was 19.500000. running mean: -15.084568\n",
      "ep 3596: ep_len:101 episode reward: total was 49.000000. running mean: -14.443722\n",
      "ep 3596: ep_len:55 episode reward: total was 23.000000. running mean: -14.069285\n",
      "ep 3596: ep_len:1670 episode reward: total was -295.430000. running mean: -16.882892\n",
      "ep 3596: ep_len:363 episode reward: total was 21.320000. running mean: -16.500863\n",
      "ep 3596: ep_len:1236 episode reward: total was -34.270000. running mean: -16.678554\n",
      "ep 3596: ep_len:7298 episode reward: total was -2078.040000. running mean: -37.292169\n",
      "ep 3596: ep_len:1054 episode reward: total was -22.040000. running mean: -37.139647\n",
      "ep 3596: ep_len:36 episode reward: total was 16.500000. running mean: -36.603251\n",
      "ep 3596: ep_len:1073 episode reward: total was 13.220000. running mean: -36.105018\n",
      "ep 3596: ep_len:2840 episode reward: total was 4.850000. running mean: -35.695468\n",
      "epsilon:0.009992 episode_count: 54068. steps_count: 58311848.000000\n",
      "ep 3597: ep_len:668 episode reward: total was -28.840000. running mean: -35.626913\n",
      "ep 3597: ep_len:1644 episode reward: total was -42.640000. running mean: -35.697044\n",
      "ep 3597: ep_len:63 episode reward: total was 30.000000. running mean: -35.040074\n",
      "ep 3597: ep_len:3027 episode reward: total was -10.260000. running mean: -34.792273\n",
      "ep 3597: ep_len:1413 episode reward: total was 6.340000. running mean: -34.380950\n",
      "ep 3597: ep_len:124 episode reward: total was 60.500000. running mean: -33.432141\n",
      "ep 3597: ep_len:615 episode reward: total was 11.380000. running mean: -32.984019\n",
      "ep 3597: ep_len:3605 episode reward: total was -23.210000. running mean: -32.886279\n",
      "ep 3597: ep_len:581 episode reward: total was 4.540000. running mean: -32.512016\n",
      "ep 3597: ep_len:802 episode reward: total was 12.450000. running mean: -32.062396\n",
      "ep 3597: ep_len:638 episode reward: total was 7.400000. running mean: -31.667772\n",
      "ep 3597: ep_len:206 episode reward: total was 100.000000. running mean: -30.351094\n",
      "ep 3597: ep_len:1174 episode reward: total was -8.630000. running mean: -30.133883\n",
      "ep 3597: ep_len:2905 episode reward: total was -0.930000. running mean: -29.841845\n",
      "epsilon:0.009992 episode_count: 54082. steps_count: 58329313.000000\n",
      "ep 3598: ep_len:642 episode reward: total was -9.910000. running mean: -29.642526\n",
      "ep 3598: ep_len:1589 episode reward: total was -52.390000. running mean: -29.870001\n",
      "ep 3598: ep_len:3029 episode reward: total was -45.400000. running mean: -30.025301\n",
      "ep 3598: ep_len:500 episode reward: total was 15.740000. running mean: -29.567648\n",
      "ep 3598: ep_len:53 episode reward: total was 25.000000. running mean: -29.021971\n",
      "ep 3598: ep_len:1084 episode reward: total was 15.330000. running mean: -28.578452\n",
      "ep 3598: ep_len:3803 episode reward: total was -49.660000. running mean: -28.789267\n",
      "ep 3598: ep_len:583 episode reward: total was -44.840000. running mean: -28.949775\n",
      "ep 3598: ep_len:863 episode reward: total was 46.230000. running mean: -28.197977\n",
      "ep 3598: ep_len:669 episode reward: total was 12.640000. running mean: -27.789597\n",
      "ep 3598: ep_len:126 episode reward: total was 60.000000. running mean: -26.911701\n",
      "ep 3598: ep_len:1143 episode reward: total was -7.410000. running mean: -26.716684\n",
      "ep 3598: ep_len:2782 episode reward: total was -229.750000. running mean: -28.747017\n",
      "epsilon:0.009992 episode_count: 54095. steps_count: 58346179.000000\n",
      "ep 3599: ep_len:934 episode reward: total was -38.460000. running mean: -28.844147\n",
      "ep 3599: ep_len:913 episode reward: total was 0.970000. running mean: -28.546006\n",
      "ep 3599: ep_len:48 episode reward: total was 21.000000. running mean: -28.050546\n",
      "ep 3599: ep_len:3157 episode reward: total was -102.730000. running mean: -28.797340\n",
      "ep 3599: ep_len:800 episode reward: total was 19.520000. running mean: -28.314167\n",
      "ep 3599: ep_len:65 episode reward: total was 31.000000. running mean: -27.721025\n",
      "ep 3599: ep_len:44 episode reward: total was 19.000000. running mean: -27.253815\n",
      "ep 3599: ep_len:500 episode reward: total was 1.510000. running mean: -26.966177\n",
      "ep 3599: ep_len:3681 episode reward: total was -9.070000. running mean: -26.787215\n",
      "ep 3599: ep_len:527 episode reward: total was -18.130000. running mean: -26.700643\n",
      "ep 3599: ep_len:643 episode reward: total was 1.250000. running mean: -26.421136\n",
      "ep 3599: ep_len:500 episode reward: total was 8.020000. running mean: -26.076725\n",
      "ep 3599: ep_len:160 episode reward: total was 77.000000. running mean: -25.045958\n",
      "ep 3599: ep_len:36 episode reward: total was 16.500000. running mean: -24.630498\n",
      "ep 3599: ep_len:500 episode reward: total was 26.730000. running mean: -24.116893\n",
      "ep 3599: ep_len:2819 episode reward: total was -2.860000. running mean: -23.904324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3599: ep_len:39 episode reward: total was 18.000000. running mean: -23.485281\n",
      "epsilon:0.009992 episode_count: 54112. steps_count: 58361545.000000\n",
      "ep 3600: ep_len:1438 episode reward: total was 8.390000. running mean: -23.166528\n",
      "ep 3600: ep_len:199 episode reward: total was 11.290000. running mean: -22.821963\n",
      "ep 3600: ep_len:54 episode reward: total was 25.500000. running mean: -22.338743\n",
      "ep 3600: ep_len:2987 episode reward: total was 15.920000. running mean: -21.956156\n",
      "ep 3600: ep_len:500 episode reward: total was -1.590000. running mean: -21.752494\n",
      "ep 3600: ep_len:500 episode reward: total was 30.530000. running mean: -21.229669\n",
      "ep 3600: ep_len:646 episode reward: total was 9.450000. running mean: -20.922873\n",
      "ep 3600: ep_len:3888 episode reward: total was -1214.120000. running mean: -32.854844\n",
      "ep 3600: ep_len:7421 episode reward: total was -3.670000. running mean: -32.562995\n",
      "ep 3600: ep_len:1082 episode reward: total was 22.560000. running mean: -32.011765\n",
      "ep 3600: ep_len:97 episode reward: total was 45.500000. running mean: -31.236648\n",
      "ep 3600: ep_len:79 episode reward: total was 36.500000. running mean: -30.559281\n",
      "ep 3600: ep_len:624 episode reward: total was 15.830000. running mean: -30.095389\n",
      "ep 3600: ep_len:2806 episode reward: total was -5.990000. running mean: -29.854335\n",
      "epsilon:0.009992 episode_count: 54126. steps_count: 58383866.000000\n",
      "ep 3601: ep_len:946 episode reward: total was -36.840000. running mean: -29.924191\n",
      "ep 3601: ep_len:782 episode reward: total was -42.910000. running mean: -30.054049\n",
      "ep 3601: ep_len:3024 episode reward: total was -37.700000. running mean: -30.130509\n",
      "ep 3601: ep_len:660 episode reward: total was 5.350000. running mean: -29.775704\n",
      "ep 3601: ep_len:66 episode reward: total was 30.000000. running mean: -29.177947\n",
      "ep 3601: ep_len:47 episode reward: total was 22.000000. running mean: -28.666167\n",
      "ep 3601: ep_len:1336 episode reward: total was -27.670000. running mean: -28.656206\n",
      "ep 3601: ep_len:3876 episode reward: total was -28.000000. running mean: -28.649644\n",
      "ep 3601: ep_len:785 episode reward: total was -20.730000. running mean: -28.570447\n",
      "ep 3601: ep_len:781 episode reward: total was 7.740000. running mean: -28.207343\n",
      "ep 3601: ep_len:661 episode reward: total was -4.150000. running mean: -27.966769\n",
      "ep 3601: ep_len:123 episode reward: total was 60.000000. running mean: -27.087102\n",
      "ep 3601: ep_len:838 episode reward: total was 9.270000. running mean: -26.723531\n",
      "ep 3601: ep_len:47 episode reward: total was 20.500000. running mean: -26.251295\n",
      "epsilon:0.009992 episode_count: 54140. steps_count: 58397838.000000\n",
      "ep 3602: ep_len:622 episode reward: total was -8.180000. running mean: -26.070582\n",
      "ep 3602: ep_len:179 episode reward: total was 2.760000. running mean: -25.782276\n",
      "ep 3602: ep_len:2962 episode reward: total was -22.050000. running mean: -25.744954\n",
      "ep 3602: ep_len:1247 episode reward: total was -47.620000. running mean: -25.963704\n",
      "ep 3602: ep_len:135 episode reward: total was 63.000000. running mean: -25.074067\n",
      "ep 3602: ep_len:50 episode reward: total was 22.000000. running mean: -24.603326\n",
      "ep 3602: ep_len:44 episode reward: total was 20.500000. running mean: -24.152293\n",
      "ep 3602: ep_len:500 episode reward: total was 11.510000. running mean: -23.795670\n",
      "ep 3602: ep_len:3683 episode reward: total was -42.660000. running mean: -23.984314\n",
      "ep 3602: ep_len:1289 episode reward: total was -92.320000. running mean: -24.667670\n",
      "ep 3602: ep_len:7240 episode reward: total was -35.480000. running mean: -24.775794\n",
      "ep 3602: ep_len:710 episode reward: total was 21.420000. running mean: -24.313836\n",
      "ep 3602: ep_len:95 episode reward: total was 43.000000. running mean: -23.640697\n",
      "ep 3602: ep_len:186 episode reward: total was 91.500000. running mean: -22.489290\n",
      "ep 3602: ep_len:82 episode reward: total was 39.500000. running mean: -21.869398\n",
      "ep 3602: ep_len:752 episode reward: total was -55.070000. running mean: -22.201404\n",
      "ep 3602: ep_len:2835 episode reward: total was -23.050000. running mean: -22.209889\n",
      "ep 3602: ep_len:49 episode reward: total was 21.500000. running mean: -21.772791\n",
      "epsilon:0.009992 episode_count: 54158. steps_count: 58420498.000000\n",
      "ep 3603: ep_len:1126 episode reward: total was -19.410000. running mean: -21.749163\n",
      "ep 3603: ep_len:707 episode reward: total was -17.300000. running mean: -21.704671\n",
      "ep 3603: ep_len:2963 episode reward: total was 10.260000. running mean: -21.385024\n",
      "ep 3603: ep_len:842 episode reward: total was 7.530000. running mean: -21.095874\n",
      "ep 3603: ep_len:607 episode reward: total was 6.910000. running mean: -20.815815\n",
      "ep 3603: ep_len:3628 episode reward: total was -67.540000. running mean: -21.283057\n",
      "ep 3603: ep_len:658 episode reward: total was -57.220000. running mean: -21.642427\n",
      "ep 3603: ep_len:618 episode reward: total was -24.230000. running mean: -21.668302\n",
      "ep 3603: ep_len:579 episode reward: total was 51.230000. running mean: -20.939319\n",
      "ep 3603: ep_len:125 episode reward: total was 59.500000. running mean: -20.134926\n",
      "ep 3603: ep_len:1438 episode reward: total was -5.990000. running mean: -19.993477\n",
      "ep 3603: ep_len:2748 episode reward: total was -38.740000. running mean: -20.180942\n",
      "epsilon:0.009992 episode_count: 54170. steps_count: 58436537.000000\n",
      "ep 3604: ep_len:1131 episode reward: total was 1.590000. running mean: -19.963233\n",
      "ep 3604: ep_len:695 episode reward: total was -20.490000. running mean: -19.968500\n",
      "ep 3604: ep_len:2973 episode reward: total was -5.780000. running mean: -19.826615\n",
      "ep 3604: ep_len:673 episode reward: total was -15.060000. running mean: -19.778949\n",
      "ep 3604: ep_len:82 episode reward: total was 39.500000. running mean: -19.186160\n",
      "ep 3604: ep_len:611 episode reward: total was 4.930000. running mean: -18.944998\n",
      "ep 3604: ep_len:500 episode reward: total was 33.200000. running mean: -18.423548\n",
      "ep 3604: ep_len:543 episode reward: total was 12.980000. running mean: -18.109513\n",
      "ep 3604: ep_len:739 episode reward: total was 1.190000. running mean: -17.916518\n",
      "ep 3604: ep_len:1131 episode reward: total was -7.040000. running mean: -17.807752\n",
      "ep 3604: ep_len:51 episode reward: total was 22.500000. running mean: -17.404675\n",
      "ep 3604: ep_len:113 episode reward: total was 53.010000. running mean: -16.700528\n",
      "ep 3604: ep_len:664 episode reward: total was -6.560000. running mean: -16.599123\n",
      "ep 3604: ep_len:2778 episode reward: total was -7.250000. running mean: -16.505632\n",
      "epsilon:0.009992 episode_count: 54184. steps_count: 58449221.000000\n",
      "ep 3605: ep_len:1093 episode reward: total was -5.090000. running mean: -16.391475\n",
      "ep 3605: ep_len:791 episode reward: total was -21.550000. running mean: -16.443061\n",
      "ep 3605: ep_len:64 episode reward: total was 30.500000. running mean: -15.973630\n",
      "ep 3605: ep_len:2927 episode reward: total was -43.220000. running mean: -16.246094\n",
      "ep 3605: ep_len:781 episode reward: total was -20.610000. running mean: -16.289733\n",
      "ep 3605: ep_len:56 episode reward: total was 26.500000. running mean: -15.861835\n",
      "ep 3605: ep_len:78 episode reward: total was 37.500000. running mean: -15.328217\n",
      "ep 3605: ep_len:70 episode reward: total was 33.500000. running mean: -14.839935\n",
      "ep 3605: ep_len:871 episode reward: total was 22.040000. running mean: -14.471135\n",
      "ep 3605: ep_len:3565 episode reward: total was -37.350000. running mean: -14.699924\n",
      "ep 3605: ep_len:653 episode reward: total was -14.850000. running mean: -14.701425\n",
      "ep 3605: ep_len:715 episode reward: total was 6.620000. running mean: -14.488211\n",
      "ep 3605: ep_len:943 episode reward: total was 38.190000. running mean: -13.961429\n",
      "ep 3605: ep_len:75 episode reward: total was 36.000000. running mean: -13.461814\n",
      "ep 3605: ep_len:83 episode reward: total was 38.500000. running mean: -12.942196\n",
      "ep 3605: ep_len:1487 episode reward: total was 2.060000. running mean: -12.792174\n",
      "ep 3605: ep_len:2892 episode reward: total was 7.240000. running mean: -12.591852\n",
      "ep 3605: ep_len:53 episode reward: total was 25.000000. running mean: -12.215934\n",
      "epsilon:0.009992 episode_count: 54202. steps_count: 58466418.000000\n",
      "ep 3606: ep_len:975 episode reward: total was -25.500000. running mean: -12.348775\n",
      "ep 3606: ep_len:968 episode reward: total was 26.470000. running mean: -11.960587\n",
      "ep 3606: ep_len:2947 episode reward: total was -7.950000. running mean: -11.920481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3606: ep_len:578 episode reward: total was -10.550000. running mean: -11.906776\n",
      "ep 3606: ep_len:67 episode reward: total was 32.000000. running mean: -11.467708\n",
      "ep 3606: ep_len:68 episode reward: total was 32.500000. running mean: -11.028031\n",
      "ep 3606: ep_len:500 episode reward: total was 39.930000. running mean: -10.518451\n",
      "ep 3606: ep_len:3868 episode reward: total was -47.550000. running mean: -10.888766\n",
      "ep 3606: ep_len:1531 episode reward: total was -27.740000. running mean: -11.057279\n",
      "ep 3606: ep_len:732 episode reward: total was 55.020000. running mean: -10.396506\n",
      "ep 3606: ep_len:686 episode reward: total was 17.290000. running mean: -10.119641\n",
      "ep 3606: ep_len:108 episode reward: total was 52.500000. running mean: -9.493445\n",
      "ep 3606: ep_len:1427 episode reward: total was 2.650000. running mean: -9.372010\n",
      "ep 3606: ep_len:2793 episode reward: total was -18.090000. running mean: -9.459190\n",
      "ep 3606: ep_len:65 episode reward: total was 29.500000. running mean: -9.069598\n",
      "epsilon:0.009992 episode_count: 54217. steps_count: 58483731.000000\n",
      "ep 3607: ep_len:615 episode reward: total was -2.740000. running mean: -9.006302\n",
      "ep 3607: ep_len:1267 episode reward: total was -56.180000. running mean: -9.478039\n",
      "ep 3607: ep_len:53 episode reward: total was 25.000000. running mean: -9.133259\n",
      "ep 3607: ep_len:3003 episode reward: total was -167.930000. running mean: -10.721226\n",
      "ep 3607: ep_len:755 episode reward: total was -21.300000. running mean: -10.827014\n",
      "ep 3607: ep_len:111 episode reward: total was 54.000000. running mean: -10.178744\n",
      "ep 3607: ep_len:89 episode reward: total was 41.500000. running mean: -9.661956\n",
      "ep 3607: ep_len:72 episode reward: total was 31.500000. running mean: -9.250337\n",
      "ep 3607: ep_len:1421 episode reward: total was -82.400000. running mean: -9.981833\n",
      "ep 3607: ep_len:4386 episode reward: total was -1378.020000. running mean: -23.662215\n",
      "ep 3607: ep_len:742 episode reward: total was -37.940000. running mean: -23.804993\n",
      "ep 3607: ep_len:720 episode reward: total was -2.240000. running mean: -23.589343\n",
      "ep 3607: ep_len:500 episode reward: total was 7.930000. running mean: -23.274149\n",
      "ep 3607: ep_len:175 episode reward: total was 83.000000. running mean: -22.211408\n",
      "ep 3607: ep_len:48 episode reward: total was 22.500000. running mean: -21.764294\n",
      "ep 3607: ep_len:1179 episode reward: total was -8.580000. running mean: -21.632451\n",
      "ep 3607: ep_len:2914 episode reward: total was -2.030000. running mean: -21.436426\n",
      "ep 3607: ep_len:67 episode reward: total was 32.000000. running mean: -20.902062\n",
      "epsilon:0.009992 episode_count: 54235. steps_count: 58501848.000000\n",
      "ep 3608: ep_len:985 episode reward: total was -46.030000. running mean: -21.153342\n",
      "ep 3608: ep_len:739 episode reward: total was -44.710000. running mean: -21.388908\n",
      "ep 3608: ep_len:3051 episode reward: total was -253.460000. running mean: -23.709619\n",
      "ep 3608: ep_len:654 episode reward: total was 29.660000. running mean: -23.175923\n",
      "ep 3608: ep_len:604 episode reward: total was 30.330000. running mean: -22.640864\n",
      "ep 3608: ep_len:3656 episode reward: total was -459.630000. running mean: -27.010755\n",
      "ep 3608: ep_len:1204 episode reward: total was -43.560000. running mean: -27.176247\n",
      "ep 3608: ep_len:772 episode reward: total was -1.780000. running mean: -26.922285\n",
      "ep 3608: ep_len:1036 episode reward: total was 25.930000. running mean: -26.393762\n",
      "ep 3608: ep_len:123 episode reward: total was 57.000000. running mean: -25.559825\n",
      "ep 3608: ep_len:24 episode reward: total was 9.000000. running mean: -25.214226\n",
      "ep 3608: ep_len:116 episode reward: total was 56.500000. running mean: -24.397084\n",
      "ep 3608: ep_len:1188 episode reward: total was -20.120000. running mean: -24.354313\n",
      "ep 3608: ep_len:2873 episode reward: total was -5.220000. running mean: -24.162970\n",
      "epsilon:0.009992 episode_count: 54249. steps_count: 58518873.000000\n",
      "ep 3609: ep_len:1130 episode reward: total was -10.080000. running mean: -24.022140\n",
      "ep 3609: ep_len:595 episode reward: total was 13.890000. running mean: -23.643019\n",
      "ep 3609: ep_len:3000 episode reward: total was -51.560000. running mean: -23.922189\n",
      "ep 3609: ep_len:500 episode reward: total was -114.640000. running mean: -24.829367\n",
      "ep 3609: ep_len:500 episode reward: total was 40.420000. running mean: -24.176873\n",
      "ep 3609: ep_len:3618 episode reward: total was -49.460000. running mean: -24.429704\n",
      "ep 3609: ep_len:541 episode reward: total was -4.830000. running mean: -24.233707\n",
      "ep 3609: ep_len:802 episode reward: total was 12.430000. running mean: -23.867070\n",
      "ep 3609: ep_len:1080 episode reward: total was 3.780000. running mean: -23.590600\n",
      "ep 3609: ep_len:35 episode reward: total was 16.000000. running mean: -23.194694\n",
      "ep 3609: ep_len:707 episode reward: total was -9.260000. running mean: -23.055347\n",
      "ep 3609: ep_len:2860 episode reward: total was -42.000000. running mean: -23.244793\n",
      "ep 3609: ep_len:67 episode reward: total was 32.000000. running mean: -22.692345\n",
      "epsilon:0.009992 episode_count: 54262. steps_count: 58534308.000000\n",
      "ep 3610: ep_len:1138 episode reward: total was -0.910000. running mean: -22.474522\n",
      "ep 3610: ep_len:1860 episode reward: total was -141.630000. running mean: -23.666077\n",
      "ep 3610: ep_len:45 episode reward: total was 19.500000. running mean: -23.234416\n",
      "ep 3610: ep_len:2992 episode reward: total was -71.620000. running mean: -23.718272\n",
      "ep 3610: ep_len:767 episode reward: total was -663.510000. running mean: -30.116189\n",
      "ep 3610: ep_len:43 episode reward: total was 20.000000. running mean: -29.615027\n",
      "ep 3610: ep_len:1099 episode reward: total was -7.360000. running mean: -29.392477\n",
      "ep 3610: ep_len:330 episode reward: total was 21.570000. running mean: -28.882852\n",
      "ep 3610: ep_len:1225 episode reward: total was -14.670000. running mean: -28.740724\n",
      "ep 3610: ep_len:911 episode reward: total was -275.850000. running mean: -31.211816\n",
      "ep 3610: ep_len:500 episode reward: total was 12.380000. running mean: -30.775898\n",
      "ep 3610: ep_len:160 episode reward: total was 78.500000. running mean: -29.683139\n",
      "ep 3610: ep_len:1508 episode reward: total was -5.840000. running mean: -29.444708\n",
      "ep 3610: ep_len:2878 episode reward: total was -5.090000. running mean: -29.201161\n",
      "ep 3610: ep_len:57 episode reward: total was 27.000000. running mean: -28.639149\n",
      "epsilon:0.009992 episode_count: 54277. steps_count: 58549821.000000\n",
      "ep 3611: ep_len:1429 episode reward: total was 25.570000. running mean: -28.097058\n",
      "ep 3611: ep_len:1558 episode reward: total was -94.870000. running mean: -28.764787\n",
      "ep 3611: ep_len:2987 episode reward: total was 14.560000. running mean: -28.331539\n",
      "ep 3611: ep_len:796 episode reward: total was 6.680000. running mean: -27.981424\n",
      "ep 3611: ep_len:51 episode reward: total was 24.000000. running mean: -27.461610\n",
      "ep 3611: ep_len:630 episode reward: total was 32.490000. running mean: -26.862093\n",
      "ep 3611: ep_len:3877 episode reward: total was -76.660000. running mean: -27.360072\n",
      "ep 3611: ep_len:536 episode reward: total was 2.160000. running mean: -27.064872\n",
      "ep 3611: ep_len:708 episode reward: total was 35.180000. running mean: -26.442423\n",
      "ep 3611: ep_len:674 episode reward: total was 0.910000. running mean: -26.168899\n",
      "ep 3611: ep_len:735 episode reward: total was -59.910000. running mean: -26.506310\n",
      "ep 3611: ep_len:2874 episode reward: total was 5.650000. running mean: -26.184747\n",
      "ep 3611: ep_len:52 episode reward: total was 24.500000. running mean: -25.677899\n",
      "epsilon:0.009992 episode_count: 54290. steps_count: 58566728.000000\n",
      "ep 3612: ep_len:965 episode reward: total was -29.900000. running mean: -25.720120\n",
      "ep 3612: ep_len:1155 episode reward: total was 24.940000. running mean: -25.213519\n",
      "ep 3612: ep_len:3013 episode reward: total was -15.790000. running mean: -25.119284\n",
      "ep 3612: ep_len:838 episode reward: total was -17.230000. running mean: -25.040391\n",
      "ep 3612: ep_len:144 episode reward: total was 67.500000. running mean: -24.114987\n",
      "ep 3612: ep_len:42 episode reward: total was 19.500000. running mean: -23.678837\n",
      "ep 3612: ep_len:1466 episode reward: total was -20.650000. running mean: -23.648549\n",
      "ep 3612: ep_len:292 episode reward: total was 20.580000. running mean: -23.206263\n",
      "ep 3612: ep_len:540 episode reward: total was -10.320000. running mean: -23.077401\n",
      "ep 3612: ep_len:903 episode reward: total was 57.690000. running mean: -22.269727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3612: ep_len:798 episode reward: total was 20.530000. running mean: -21.841729\n",
      "ep 3612: ep_len:87 episode reward: total was 42.000000. running mean: -21.203312\n",
      "ep 3612: ep_len:40 episode reward: total was 18.500000. running mean: -20.806279\n",
      "ep 3612: ep_len:72 episode reward: total was 33.000000. running mean: -20.268216\n",
      "ep 3612: ep_len:1144 episode reward: total was -6.900000. running mean: -20.134534\n",
      "ep 3612: ep_len:2765 episode reward: total was 0.060000. running mean: -19.932589\n",
      "epsilon:0.009992 episode_count: 54306. steps_count: 58580992.000000\n",
      "ep 3613: ep_len:967 episode reward: total was -112.710000. running mean: -20.860363\n",
      "ep 3613: ep_len:660 episode reward: total was 4.900000. running mean: -20.602759\n",
      "ep 3613: ep_len:49 episode reward: total was 23.000000. running mean: -20.166732\n",
      "ep 3613: ep_len:3004 episode reward: total was -30.850000. running mean: -20.273564\n",
      "ep 3613: ep_len:605 episode reward: total was -67.260000. running mean: -20.743429\n",
      "ep 3613: ep_len:598 episode reward: total was -2.180000. running mean: -20.557794\n",
      "ep 3613: ep_len:343 episode reward: total was 21.090000. running mean: -20.141316\n",
      "ep 3613: ep_len:3850 episode reward: total was -1914.000000. running mean: -39.079903\n",
      "ep 3613: ep_len:677 episode reward: total was 7.110000. running mean: -38.618004\n",
      "ep 3613: ep_len:653 episode reward: total was 5.310000. running mean: -38.178724\n",
      "ep 3613: ep_len:214 episode reward: total was 104.000000. running mean: -36.756937\n",
      "ep 3613: ep_len:69 episode reward: total was 31.500000. running mean: -36.074368\n",
      "ep 3613: ep_len:815 episode reward: total was -5.560000. running mean: -35.769224\n",
      "ep 3613: ep_len:2842 episode reward: total was -210.510000. running mean: -37.516632\n",
      "ep 3613: ep_len:58 episode reward: total was 26.000000. running mean: -36.881465\n",
      "epsilon:0.009992 episode_count: 54321. steps_count: 58596396.000000\n",
      "ep 3614: ep_len:788 episode reward: total was -60.970000. running mean: -37.122351\n",
      "ep 3614: ep_len:783 episode reward: total was -15.570000. running mean: -36.906827\n",
      "ep 3614: ep_len:46 episode reward: total was 21.500000. running mean: -36.322759\n",
      "ep 3614: ep_len:2934 episode reward: total was 0.900000. running mean: -35.950531\n",
      "ep 3614: ep_len:598 episode reward: total was -19.440000. running mean: -35.785426\n",
      "ep 3614: ep_len:94 episode reward: total was 44.000000. running mean: -34.987572\n",
      "ep 3614: ep_len:74 episode reward: total was 34.000000. running mean: -34.297696\n",
      "ep 3614: ep_len:500 episode reward: total was -3.520000. running mean: -33.989919\n",
      "ep 3614: ep_len:347 episode reward: total was 8.670000. running mean: -33.563320\n",
      "ep 3614: ep_len:703 episode reward: total was -15.750000. running mean: -33.385187\n",
      "ep 3614: ep_len:7305 episode reward: total was 40.130000. running mean: -32.650035\n",
      "ep 3614: ep_len:501 episode reward: total was 27.140000. running mean: -32.052135\n",
      "ep 3614: ep_len:60 episode reward: total was 28.500000. running mean: -31.446613\n",
      "ep 3614: ep_len:611 episode reward: total was 4.930000. running mean: -31.082847\n",
      "ep 3614: ep_len:2812 episode reward: total was -70.940000. running mean: -31.481419\n",
      "epsilon:0.009992 episode_count: 54336. steps_count: 58614552.000000\n",
      "ep 3615: ep_len:1152 episode reward: total was -66.420000. running mean: -31.830804\n",
      "ep 3615: ep_len:1702 episode reward: total was -179.120000. running mean: -33.303696\n",
      "ep 3615: ep_len:3011 episode reward: total was -3.010000. running mean: -33.000759\n",
      "ep 3615: ep_len:500 episode reward: total was 6.660000. running mean: -32.604152\n",
      "ep 3615: ep_len:44 episode reward: total was 20.500000. running mean: -32.073110\n",
      "ep 3615: ep_len:1451 episode reward: total was -2.130000. running mean: -31.773679\n",
      "ep 3615: ep_len:3720 episode reward: total was -137.100000. running mean: -32.826942\n",
      "ep 3615: ep_len:1595 episode reward: total was -50.330000. running mean: -33.001973\n",
      "ep 3615: ep_len:644 episode reward: total was -8.290000. running mean: -32.754853\n",
      "ep 3615: ep_len:944 episode reward: total was 35.160000. running mean: -32.075705\n",
      "ep 3615: ep_len:99 episode reward: total was 46.500000. running mean: -31.289948\n",
      "ep 3615: ep_len:92 episode reward: total was 43.000000. running mean: -30.547048\n",
      "ep 3615: ep_len:639 episode reward: total was 11.540000. running mean: -30.126178\n",
      "ep 3615: ep_len:2832 episode reward: total was -33.330000. running mean: -30.158216\n",
      "epsilon:0.009992 episode_count: 54350. steps_count: 58632977.000000\n",
      "ep 3616: ep_len:1101 episode reward: total was -9.360000. running mean: -29.950234\n",
      "ep 3616: ep_len:690 episode reward: total was -43.360000. running mean: -30.084331\n",
      "ep 3616: ep_len:2963 episode reward: total was 6.860000. running mean: -29.714888\n",
      "ep 3616: ep_len:622 episode reward: total was 9.930000. running mean: -29.318439\n",
      "ep 3616: ep_len:1477 episode reward: total was -21.030000. running mean: -29.235555\n",
      "ep 3616: ep_len:3919 episode reward: total was -34.890000. running mean: -29.292099\n",
      "ep 3616: ep_len:1209 episode reward: total was -41.580000. running mean: -29.414978\n",
      "ep 3616: ep_len:744 episode reward: total was 41.060000. running mean: -28.710228\n",
      "ep 3616: ep_len:1431 episode reward: total was -20.300000. running mean: -28.626126\n",
      "ep 3616: ep_len:670 episode reward: total was -4.410000. running mean: -28.383965\n",
      "ep 3616: ep_len:2825 episode reward: total was -21.780000. running mean: -28.317925\n",
      "ep 3616: ep_len:68 episode reward: total was 31.000000. running mean: -27.724746\n",
      "epsilon:0.009992 episode_count: 54362. steps_count: 58650696.000000\n",
      "ep 3617: ep_len:1083 episode reward: total was -7.520000. running mean: -27.522699\n",
      "ep 3617: ep_len:663 episode reward: total was -51.110000. running mean: -27.758572\n",
      "ep 3617: ep_len:3077 episode reward: total was -13.350000. running mean: -27.614486\n",
      "ep 3617: ep_len:1224 episode reward: total was -49.600000. running mean: -27.834341\n",
      "ep 3617: ep_len:674 episode reward: total was -3.960000. running mean: -27.595598\n",
      "ep 3617: ep_len:3969 episode reward: total was -608.900000. running mean: -33.408642\n",
      "ep 3617: ep_len:1518 episode reward: total was -47.610000. running mean: -33.550655\n",
      "ep 3617: ep_len:708 episode reward: total was 25.510000. running mean: -32.960049\n",
      "ep 3617: ep_len:1127 episode reward: total was -23.240000. running mean: -32.862848\n",
      "ep 3617: ep_len:100 episode reward: total was 48.500000. running mean: -32.049220\n",
      "ep 3617: ep_len:33 episode reward: total was 15.000000. running mean: -31.578728\n",
      "ep 3617: ep_len:113 episode reward: total was 55.000000. running mean: -30.712940\n",
      "ep 3617: ep_len:1037 episode reward: total was 4.480000. running mean: -30.361011\n",
      "ep 3617: ep_len:2866 episode reward: total was 3.520000. running mean: -30.022201\n",
      "epsilon:0.009992 episode_count: 54376. steps_count: 58668888.000000\n",
      "ep 3618: ep_len:1018 episode reward: total was -70.980000. running mean: -30.431779\n",
      "ep 3618: ep_len:1117 episode reward: total was -93.660000. running mean: -31.064061\n",
      "ep 3618: ep_len:2964 episode reward: total was -0.090000. running mean: -30.754320\n",
      "ep 3618: ep_len:500 episode reward: total was -4.800000. running mean: -30.494777\n",
      "ep 3618: ep_len:120 episode reward: total was 57.000000. running mean: -29.619829\n",
      "ep 3618: ep_len:66 episode reward: total was 27.000000. running mean: -29.053631\n",
      "ep 3618: ep_len:613 episode reward: total was 18.760000. running mean: -28.575495\n",
      "ep 3618: ep_len:3533 episode reward: total was -48.290000. running mean: -28.772640\n",
      "ep 3618: ep_len:585 episode reward: total was -21.590000. running mean: -28.700813\n",
      "ep 3618: ep_len:786 episode reward: total was 28.060000. running mean: -28.133205\n",
      "ep 3618: ep_len:1005 episode reward: total was 25.800000. running mean: -27.593873\n",
      "ep 3618: ep_len:42 episode reward: total was 18.000000. running mean: -27.137934\n",
      "ep 3618: ep_len:699 episode reward: total was -10.350000. running mean: -26.970055\n",
      "ep 3618: ep_len:2881 episode reward: total was -37.680000. running mean: -27.077155\n",
      "epsilon:0.009992 episode_count: 54390. steps_count: 58684817.000000\n",
      "ep 3619: ep_len:840 episode reward: total was -23.080000. running mean: -27.037183\n",
      "ep 3619: ep_len:208 episode reward: total was 8.470000. running mean: -26.682111\n",
      "ep 3619: ep_len:69 episode reward: total was 33.000000. running mean: -26.085290\n",
      "ep 3619: ep_len:3059 episode reward: total was 8.400000. running mean: -25.740437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3619: ep_len:4476 episode reward: total was -1842.170000. running mean: -43.904733\n",
      "ep 3619: ep_len:63 episode reward: total was 28.500000. running mean: -43.180685\n",
      "ep 3619: ep_len:136 episode reward: total was 66.500000. running mean: -42.083879\n",
      "ep 3619: ep_len:105 episode reward: total was 51.000000. running mean: -41.153040\n",
      "ep 3619: ep_len:600 episode reward: total was 41.710000. running mean: -40.324409\n",
      "ep 3619: ep_len:3664 episode reward: total was -775.570000. running mean: -47.676865\n",
      "ep 3619: ep_len:844 episode reward: total was -112.020000. running mean: -48.320297\n",
      "ep 3619: ep_len:788 episode reward: total was 12.650000. running mean: -47.710594\n",
      "ep 3619: ep_len:643 episode reward: total was -8.290000. running mean: -47.316388\n",
      "ep 3619: ep_len:91 episode reward: total was 42.500000. running mean: -46.418224\n",
      "ep 3619: ep_len:58 episode reward: total was 24.500000. running mean: -45.709042\n",
      "ep 3619: ep_len:127 episode reward: total was 59.000000. running mean: -44.661951\n",
      "ep 3619: ep_len:1171 episode reward: total was -18.240000. running mean: -44.397732\n",
      "ep 3619: ep_len:2823 episode reward: total was -28.710000. running mean: -44.240854\n",
      "epsilon:0.009992 episode_count: 54408. steps_count: 58704582.000000\n",
      "ep 3620: ep_len:880 episode reward: total was -134.270000. running mean: -45.141146\n",
      "ep 3620: ep_len:792 episode reward: total was 5.370000. running mean: -44.636034\n",
      "ep 3620: ep_len:3064 episode reward: total was 19.220000. running mean: -43.997474\n",
      "ep 3620: ep_len:652 episode reward: total was 4.620000. running mean: -43.511299\n",
      "ep 3620: ep_len:107 episode reward: total was 50.500000. running mean: -42.571186\n",
      "ep 3620: ep_len:84 episode reward: total was 40.500000. running mean: -41.740474\n",
      "ep 3620: ep_len:1002 episode reward: total was -53.780000. running mean: -41.860870\n",
      "ep 3620: ep_len:3538 episode reward: total was -26.020000. running mean: -41.702461\n",
      "ep 3620: ep_len:948 episode reward: total was -23.950000. running mean: -41.524936\n",
      "ep 3620: ep_len:7410 episode reward: total was 41.890000. running mean: -40.690787\n",
      "ep 3620: ep_len:872 episode reward: total was -2.310000. running mean: -40.306979\n",
      "ep 3620: ep_len:873 episode reward: total was -3.740000. running mean: -39.941309\n",
      "ep 3620: ep_len:2881 episode reward: total was -25.990000. running mean: -39.801796\n",
      "ep 3620: ep_len:72 episode reward: total was 33.000000. running mean: -39.073778\n",
      "epsilon:0.009992 episode_count: 54422. steps_count: 58727757.000000\n",
      "ep 3621: ep_len:956 episode reward: total was -18.130000. running mean: -38.864341\n",
      "ep 3621: ep_len:1148 episode reward: total was -3.290000. running mean: -38.508597\n",
      "ep 3621: ep_len:3071 episode reward: total was 18.810000. running mean: -37.935411\n",
      "ep 3621: ep_len:624 episode reward: total was -11.370000. running mean: -37.669757\n",
      "ep 3621: ep_len:42 episode reward: total was 19.500000. running mean: -37.098060\n",
      "ep 3621: ep_len:54 episode reward: total was 21.000000. running mean: -36.517079\n",
      "ep 3621: ep_len:742 episode reward: total was -7.870000. running mean: -36.230608\n",
      "ep 3621: ep_len:3704 episode reward: total was -61.440000. running mean: -36.482702\n",
      "ep 3621: ep_len:1263 episode reward: total was -69.410000. running mean: -36.811975\n",
      "ep 3621: ep_len:7302 episode reward: total was -39.450000. running mean: -36.838355\n",
      "ep 3621: ep_len:680 episode reward: total was 9.190000. running mean: -36.378072\n",
      "ep 3621: ep_len:54 episode reward: total was 25.500000. running mean: -35.759291\n",
      "ep 3621: ep_len:131 episode reward: total was 64.000000. running mean: -34.761698\n",
      "ep 3621: ep_len:51 episode reward: total was 22.500000. running mean: -34.189081\n",
      "ep 3621: ep_len:1426 episode reward: total was 13.600000. running mean: -33.711190\n",
      "ep 3621: ep_len:2872 episode reward: total was 3.060000. running mean: -33.343478\n",
      "epsilon:0.009992 episode_count: 54438. steps_count: 58751877.000000\n",
      "ep 3622: ep_len:726 episode reward: total was -8.960000. running mean: -33.099644\n",
      "ep 3622: ep_len:181 episode reward: total was -8.450000. running mean: -32.853147\n",
      "ep 3622: ep_len:2899 episode reward: total was -21.160000. running mean: -32.736216\n",
      "ep 3622: ep_len:1353 episode reward: total was 14.340000. running mean: -32.265454\n",
      "ep 3622: ep_len:46 episode reward: total was 21.500000. running mean: -31.727799\n",
      "ep 3622: ep_len:1034 episode reward: total was -6.540000. running mean: -31.475921\n",
      "ep 3622: ep_len:3730 episode reward: total was -95.200000. running mean: -32.113162\n",
      "ep 3622: ep_len:851 episode reward: total was 2.770000. running mean: -31.764330\n",
      "ep 3622: ep_len:611 episode reward: total was -7.280000. running mean: -31.519487\n",
      "ep 3622: ep_len:704 episode reward: total was -14.340000. running mean: -31.347692\n",
      "ep 3622: ep_len:107 episode reward: total was 50.500000. running mean: -30.529215\n",
      "ep 3622: ep_len:53 episode reward: total was 25.000000. running mean: -29.973923\n",
      "ep 3622: ep_len:1011 episode reward: total was -19.000000. running mean: -29.864184\n",
      "ep 3622: ep_len:2805 episode reward: total was -26.020000. running mean: -29.825742\n",
      "epsilon:0.009992 episode_count: 54452. steps_count: 58767988.000000\n",
      "ep 3623: ep_len:1420 episode reward: total was -13.100000. running mean: -29.658484\n",
      "ep 3623: ep_len:940 episode reward: total was 25.520000. running mean: -29.106700\n",
      "ep 3623: ep_len:77 episode reward: total was 35.500000. running mean: -28.460633\n",
      "ep 3623: ep_len:96 episode reward: total was 45.000000. running mean: -27.726026\n",
      "ep 3623: ep_len:1427 episode reward: total was 13.000000. running mean: -27.318766\n",
      "ep 3623: ep_len:1073 episode reward: total was -4.960000. running mean: -27.095178\n",
      "ep 3623: ep_len:4035 episode reward: total was -578.240000. running mean: -32.606627\n",
      "ep 3623: ep_len:3775 episode reward: total was -1119.880000. running mean: -43.479360\n",
      "ep 3623: ep_len:800 episode reward: total was 9.430000. running mean: -42.950267\n",
      "ep 3623: ep_len:500 episode reward: total was 8.890000. running mean: -42.431864\n",
      "ep 3623: ep_len:94 episode reward: total was 45.500000. running mean: -41.552545\n",
      "ep 3623: ep_len:3992 episode reward: total was -815.460000. running mean: -49.291620\n",
      "ep 3623: ep_len:2841 episode reward: total was -14.610000. running mean: -48.944804\n",
      "epsilon:0.009992 episode_count: 54465. steps_count: 58789058.000000\n",
      "ep 3624: ep_len:500 episode reward: total was -6.970000. running mean: -48.525056\n",
      "ep 3624: ep_len:500 episode reward: total was -4.400000. running mean: -48.083805\n",
      "ep 3624: ep_len:3076 episode reward: total was -2.140000. running mean: -47.624367\n",
      "ep 3624: ep_len:807 episode reward: total was -20.810000. running mean: -47.356223\n",
      "ep 3624: ep_len:53 episode reward: total was 23.500000. running mean: -46.647661\n",
      "ep 3624: ep_len:150 episode reward: total was 73.500000. running mean: -45.446185\n",
      "ep 3624: ep_len:65 episode reward: total was 28.000000. running mean: -44.711723\n",
      "ep 3624: ep_len:36 episode reward: total was 16.500000. running mean: -44.099606\n",
      "ep 3624: ep_len:500 episode reward: total was 24.440000. running mean: -43.414209\n",
      "ep 3624: ep_len:647 episode reward: total was 17.630000. running mean: -42.803767\n",
      "ep 3624: ep_len:852 episode reward: total was 9.240000. running mean: -42.283330\n",
      "ep 3624: ep_len:642 episode reward: total was 4.660000. running mean: -41.813896\n",
      "ep 3624: ep_len:901 episode reward: total was 7.710000. running mean: -41.318657\n",
      "ep 3624: ep_len:178 episode reward: total was -176.990000. running mean: -42.675371\n",
      "ep 3624: ep_len:994 episode reward: total was 2.650000. running mean: -42.222117\n",
      "ep 3624: ep_len:2859 episode reward: total was -16.840000. running mean: -41.968296\n",
      "epsilon:0.009992 episode_count: 54481. steps_count: 58801818.000000\n",
      "ep 3625: ep_len:672 episode reward: total was -99.530000. running mean: -42.543913\n",
      "ep 3625: ep_len:737 episode reward: total was -44.310000. running mean: -42.561574\n",
      "ep 3625: ep_len:2707 episode reward: total was -128.490000. running mean: -43.420858\n",
      "ep 3625: ep_len:853 episode reward: total was -11.910000. running mean: -43.105750\n",
      "ep 3625: ep_len:42 episode reward: total was 19.500000. running mean: -42.479692\n",
      "ep 3625: ep_len:102 episode reward: total was 48.000000. running mean: -41.574895\n",
      "ep 3625: ep_len:1346 episode reward: total was -82.660000. running mean: -41.985746\n",
      "ep 3625: ep_len:3712 episode reward: total was -3.630000. running mean: -41.602189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3625: ep_len:836 episode reward: total was -3.080000. running mean: -41.216967\n",
      "ep 3625: ep_len:717 episode reward: total was 47.610000. running mean: -40.328697\n",
      "ep 3625: ep_len:1038 episode reward: total was -32.210000. running mean: -40.247510\n",
      "ep 3625: ep_len:80 episode reward: total was 38.500000. running mean: -39.460035\n",
      "ep 3625: ep_len:192 episode reward: total was 93.000000. running mean: -38.135435\n",
      "ep 3625: ep_len:1176 episode reward: total was -6.270000. running mean: -37.816780\n",
      "ep 3625: ep_len:2900 episode reward: total was -7.280000. running mean: -37.511413\n",
      "ep 3625: ep_len:45 episode reward: total was 21.000000. running mean: -36.926298\n",
      "epsilon:0.009992 episode_count: 54497. steps_count: 58818973.000000\n",
      "ep 3626: ep_len:665 episode reward: total was 12.020000. running mean: -36.436835\n",
      "ep 3626: ep_len:1258 episode reward: total was -111.820000. running mean: -37.190667\n",
      "ep 3626: ep_len:2928 episode reward: total was -97.550000. running mean: -37.794260\n",
      "ep 3626: ep_len:630 episode reward: total was 6.520000. running mean: -37.351118\n",
      "ep 3626: ep_len:36 episode reward: total was 16.500000. running mean: -36.812607\n",
      "ep 3626: ep_len:89 episode reward: total was 40.000000. running mean: -36.044481\n",
      "ep 3626: ep_len:79 episode reward: total was 38.000000. running mean: -35.304036\n",
      "ep 3626: ep_len:1909 episode reward: total was -81.560000. running mean: -35.766595\n",
      "ep 3626: ep_len:3851 episode reward: total was -31.380000. running mean: -35.722730\n",
      "ep 3626: ep_len:608 episode reward: total was 10.210000. running mean: -35.263402\n",
      "ep 3626: ep_len:7358 episode reward: total was -52.710000. running mean: -35.437868\n",
      "ep 3626: ep_len:692 episode reward: total was -1.300000. running mean: -35.096490\n",
      "ep 3626: ep_len:91 episode reward: total was 41.000000. running mean: -34.335525\n",
      "ep 3626: ep_len:500 episode reward: total was 36.130000. running mean: -33.630869\n",
      "ep 3626: ep_len:2842 episode reward: total was -5.720000. running mean: -33.351761\n",
      "epsilon:0.009992 episode_count: 54512. steps_count: 58842509.000000\n",
      "ep 3627: ep_len:796 episode reward: total was -97.620000. running mean: -33.994443\n",
      "ep 3627: ep_len:1658 episode reward: total was -52.670000. running mean: -34.181199\n",
      "ep 3627: ep_len:2928 episode reward: total was -49.290000. running mean: -34.332287\n",
      "ep 3627: ep_len:536 episode reward: total was -64.500000. running mean: -34.633964\n",
      "ep 3627: ep_len:108 episode reward: total was 51.000000. running mean: -33.777624\n",
      "ep 3627: ep_len:81 episode reward: total was 37.500000. running mean: -33.064848\n",
      "ep 3627: ep_len:668 episode reward: total was 26.440000. running mean: -32.469799\n",
      "ep 3627: ep_len:651 episode reward: total was 15.130000. running mean: -31.993801\n",
      "ep 3627: ep_len:612 episode reward: total was 18.620000. running mean: -31.487663\n",
      "ep 3627: ep_len:825 episode reward: total was 38.370000. running mean: -30.789087\n",
      "ep 3627: ep_len:619 episode reward: total was 34.300000. running mean: -30.138196\n",
      "ep 3627: ep_len:72 episode reward: total was 33.000000. running mean: -29.506814\n",
      "ep 3627: ep_len:1454 episode reward: total was 11.280000. running mean: -29.098946\n",
      "ep 3627: ep_len:2839 episode reward: total was -15.450000. running mean: -28.962456\n",
      "ep 3627: ep_len:67 episode reward: total was 32.000000. running mean: -28.352832\n",
      "epsilon:0.009992 episode_count: 54527. steps_count: 58856423.000000\n",
      "ep 3628: ep_len:774 episode reward: total was -14.650000. running mean: -28.215803\n",
      "ep 3628: ep_len:782 episode reward: total was -14.260000. running mean: -28.076245\n",
      "ep 3628: ep_len:54 episode reward: total was 22.500000. running mean: -27.570483\n",
      "ep 3628: ep_len:3076 episode reward: total was -1.010000. running mean: -27.304878\n",
      "ep 3628: ep_len:860 episode reward: total was 37.640000. running mean: -26.655429\n",
      "ep 3628: ep_len:51 episode reward: total was 22.500000. running mean: -26.163875\n",
      "ep 3628: ep_len:93 episode reward: total was 43.500000. running mean: -25.467236\n",
      "ep 3628: ep_len:1162 episode reward: total was 9.500000. running mean: -25.117564\n",
      "ep 3628: ep_len:3597 episode reward: total was -26.440000. running mean: -25.130788\n",
      "ep 3628: ep_len:644 episode reward: total was -35.990000. running mean: -25.239380\n",
      "ep 3628: ep_len:800 episode reward: total was -11.540000. running mean: -25.102387\n",
      "ep 3628: ep_len:758 episode reward: total was -15.820000. running mean: -25.009563\n",
      "ep 3628: ep_len:55 episode reward: total was 24.500000. running mean: -24.514467\n",
      "ep 3628: ep_len:961 episode reward: total was -32.590000. running mean: -24.595222\n",
      "ep 3628: ep_len:2856 episode reward: total was -8.980000. running mean: -24.439070\n",
      "ep 3628: ep_len:49 episode reward: total was 23.000000. running mean: -23.964680\n",
      "epsilon:0.009992 episode_count: 54543. steps_count: 58872995.000000\n",
      "ep 3629: ep_len:676 episode reward: total was -51.990000. running mean: -24.244933\n",
      "ep 3629: ep_len:757 episode reward: total was -10.760000. running mean: -24.110083\n",
      "ep 3629: ep_len:2984 episode reward: total was -47.200000. running mean: -24.340983\n",
      "ep 3629: ep_len:1139 episode reward: total was -13.020000. running mean: -24.227773\n",
      "ep 3629: ep_len:46 episode reward: total was 21.500000. running mean: -23.770495\n",
      "ep 3629: ep_len:121 episode reward: total was 59.000000. running mean: -22.942790\n",
      "ep 3629: ep_len:80 episode reward: total was 38.500000. running mean: -22.328362\n",
      "ep 3629: ep_len:1089 episode reward: total was -23.830000. running mean: -22.343379\n",
      "ep 3629: ep_len:630 episode reward: total was 26.370000. running mean: -21.856245\n",
      "ep 3629: ep_len:1264 episode reward: total was -143.590000. running mean: -23.073582\n",
      "ep 3629: ep_len:668 episode reward: total was 15.550000. running mean: -22.687347\n",
      "ep 3629: ep_len:602 episode reward: total was -15.350000. running mean: -22.613973\n",
      "ep 3629: ep_len:24 episode reward: total was 10.500000. running mean: -22.282833\n",
      "ep 3629: ep_len:729 episode reward: total was -86.810000. running mean: -22.928105\n",
      "ep 3629: ep_len:2753 episode reward: total was -21.490000. running mean: -22.913724\n",
      "ep 3629: ep_len:53 episode reward: total was 25.000000. running mean: -22.434587\n",
      "epsilon:0.009992 episode_count: 54559. steps_count: 58886610.000000\n",
      "ep 3630: ep_len:973 episode reward: total was -15.600000. running mean: -22.366241\n",
      "ep 3630: ep_len:743 episode reward: total was -33.590000. running mean: -22.478478\n",
      "ep 3630: ep_len:73 episode reward: total was 35.000000. running mean: -21.903694\n",
      "ep 3630: ep_len:3013 episode reward: total was 0.100000. running mean: -21.683657\n",
      "ep 3630: ep_len:681 episode reward: total was 35.170000. running mean: -21.115120\n",
      "ep 3630: ep_len:48 episode reward: total was 19.500000. running mean: -20.708969\n",
      "ep 3630: ep_len:54 episode reward: total was 24.000000. running mean: -20.261879\n",
      "ep 3630: ep_len:718 episode reward: total was -16.220000. running mean: -20.221460\n",
      "ep 3630: ep_len:3711 episode reward: total was -1244.830000. running mean: -32.467546\n",
      "ep 3630: ep_len:1539 episode reward: total was -74.220000. running mean: -32.885070\n",
      "ep 3630: ep_len:734 episode reward: total was 5.430000. running mean: -32.501920\n",
      "ep 3630: ep_len:500 episode reward: total was 18.130000. running mean: -31.995600\n",
      "ep 3630: ep_len:90 episode reward: total was 42.000000. running mean: -31.255644\n",
      "ep 3630: ep_len:141 episode reward: total was 69.000000. running mean: -30.253088\n",
      "ep 3630: ep_len:603 episode reward: total was -31.510000. running mean: -30.265657\n",
      "ep 3630: ep_len:2886 episode reward: total was -1.940000. running mean: -29.982401\n",
      "epsilon:0.009992 episode_count: 54575. steps_count: 58903117.000000\n",
      "ep 3631: ep_len:1058 episode reward: total was -5.750000. running mean: -29.740077\n",
      "ep 3631: ep_len:1010 episode reward: total was 22.300000. running mean: -29.219676\n",
      "ep 3631: ep_len:2964 episode reward: total was -23.070000. running mean: -29.158179\n",
      "ep 3631: ep_len:500 episode reward: total was -47.690000. running mean: -29.343497\n",
      "ep 3631: ep_len:96 episode reward: total was 45.000000. running mean: -28.600062\n",
      "ep 3631: ep_len:66 episode reward: total was 30.000000. running mean: -28.014062\n",
      "ep 3631: ep_len:857 episode reward: total was 35.490000. running mean: -27.379021\n",
      "ep 3631: ep_len:663 episode reward: total was 29.790000. running mean: -26.807331\n",
      "ep 3631: ep_len:762 episode reward: total was -0.430000. running mean: -26.543558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3631: ep_len:7137 episode reward: total was -21.880000. running mean: -26.496922\n",
      "ep 3631: ep_len:637 episode reward: total was 7.210000. running mean: -26.159853\n",
      "ep 3631: ep_len:500 episode reward: total was 30.990000. running mean: -25.588354\n",
      "ep 3631: ep_len:2804 episode reward: total was -29.730000. running mean: -25.629771\n",
      "ep 3631: ep_len:40 episode reward: total was 18.500000. running mean: -25.188473\n",
      "epsilon:0.009992 episode_count: 54589. steps_count: 58922211.000000\n",
      "ep 3632: ep_len:606 episode reward: total was 4.380000. running mean: -24.892788\n",
      "ep 3632: ep_len:797 episode reward: total was -21.490000. running mean: -24.858760\n",
      "ep 3632: ep_len:2967 episode reward: total was -33.600000. running mean: -24.946173\n",
      "ep 3632: ep_len:561 episode reward: total was -21.710000. running mean: -24.913811\n",
      "ep 3632: ep_len:45 episode reward: total was 21.000000. running mean: -24.454673\n",
      "ep 3632: ep_len:1003 episode reward: total was 2.310000. running mean: -24.187026\n",
      "ep 3632: ep_len:3660 episode reward: total was -7.080000. running mean: -24.015956\n",
      "ep 3632: ep_len:1266 episode reward: total was -78.900000. running mean: -24.564796\n",
      "ep 3632: ep_len:662 episode reward: total was 16.250000. running mean: -24.156648\n",
      "ep 3632: ep_len:740 episode reward: total was -24.080000. running mean: -24.155882\n",
      "ep 3632: ep_len:44 episode reward: total was 19.000000. running mean: -23.724323\n",
      "ep 3632: ep_len:101 episode reward: total was 49.000000. running mean: -22.997080\n",
      "ep 3632: ep_len:798 episode reward: total was -52.790000. running mean: -23.295009\n",
      "ep 3632: ep_len:2888 episode reward: total was -65.030000. running mean: -23.712359\n",
      "epsilon:0.009992 episode_count: 54603. steps_count: 58938349.000000\n",
      "ep 3633: ep_len:1096 episode reward: total was -20.520000. running mean: -23.680435\n",
      "ep 3633: ep_len:1712 episode reward: total was -77.010000. running mean: -24.213731\n",
      "ep 3633: ep_len:66 episode reward: total was 30.000000. running mean: -23.671594\n",
      "ep 3633: ep_len:2967 episode reward: total was -4.800000. running mean: -23.482878\n",
      "ep 3633: ep_len:678 episode reward: total was -22.260000. running mean: -23.470649\n",
      "ep 3633: ep_len:1421 episode reward: total was -168.770000. running mean: -24.923643\n",
      "ep 3633: ep_len:3822 episode reward: total was 7.110000. running mean: -24.603306\n",
      "ep 3633: ep_len:700 episode reward: total was -55.790000. running mean: -24.915173\n",
      "ep 3633: ep_len:662 episode reward: total was -4.630000. running mean: -24.712321\n",
      "ep 3633: ep_len:1473 episode reward: total was -28.350000. running mean: -24.748698\n",
      "ep 3633: ep_len:130 episode reward: total was -77.990000. running mean: -25.281111\n",
      "ep 3633: ep_len:43 episode reward: total was 20.000000. running mean: -24.828300\n",
      "ep 3633: ep_len:92 episode reward: total was 41.500000. running mean: -24.165017\n",
      "ep 3633: ep_len:825 episode reward: total was -53.980000. running mean: -24.463167\n",
      "ep 3633: ep_len:2871 episode reward: total was -34.540000. running mean: -24.563935\n",
      "ep 3633: ep_len:61 episode reward: total was 26.000000. running mean: -24.058296\n",
      "epsilon:0.009992 episode_count: 54619. steps_count: 58956968.000000\n",
      "ep 3634: ep_len:768 episode reward: total was -13.700000. running mean: -23.954713\n",
      "ep 3634: ep_len:710 episode reward: total was -46.600000. running mean: -24.181166\n",
      "ep 3634: ep_len:3009 episode reward: total was 25.720000. running mean: -23.682154\n",
      "ep 3634: ep_len:761 episode reward: total was -9.700000. running mean: -23.542333\n",
      "ep 3634: ep_len:90 episode reward: total was 43.500000. running mean: -22.871909\n",
      "ep 3634: ep_len:987 episode reward: total was -10.500000. running mean: -22.748190\n",
      "ep 3634: ep_len:3961 episode reward: total was -141.930000. running mean: -23.940008\n",
      "ep 3634: ep_len:552 episode reward: total was -4.750000. running mean: -23.748108\n",
      "ep 3634: ep_len:728 episode reward: total was 44.170000. running mean: -23.068927\n",
      "ep 3634: ep_len:670 episode reward: total was -5.590000. running mean: -22.894138\n",
      "ep 3634: ep_len:57 episode reward: total was 27.000000. running mean: -22.395196\n",
      "ep 3634: ep_len:216 episode reward: total was 100.500000. running mean: -21.166244\n",
      "ep 3634: ep_len:50 episode reward: total was 23.500000. running mean: -20.719582\n",
      "ep 3634: ep_len:590 episode reward: total was -9.420000. running mean: -20.606586\n",
      "ep 3634: ep_len:2735 episode reward: total was -21.730000. running mean: -20.617820\n",
      "epsilon:0.009992 episode_count: 54634. steps_count: 58972852.000000\n",
      "ep 3635: ep_len:1471 episode reward: total was 1.500000. running mean: -20.396642\n",
      "ep 3635: ep_len:764 episode reward: total was -0.760000. running mean: -20.200276\n",
      "ep 3635: ep_len:2936 episode reward: total was -8.680000. running mean: -20.085073\n",
      "ep 3635: ep_len:532 episode reward: total was -5.960000. running mean: -19.943822\n",
      "ep 3635: ep_len:77 episode reward: total was 37.000000. running mean: -19.374384\n",
      "ep 3635: ep_len:868 episode reward: total was 21.090000. running mean: -18.969740\n",
      "ep 3635: ep_len:321 episode reward: total was 15.820000. running mean: -18.621843\n",
      "ep 3635: ep_len:768 episode reward: total was -19.620000. running mean: -18.631824\n",
      "ep 3635: ep_len:730 episode reward: total was 48.050000. running mean: -17.965006\n",
      "ep 3635: ep_len:1072 episode reward: total was 33.940000. running mean: -17.445956\n",
      "ep 3635: ep_len:203 episode reward: total was 100.000000. running mean: -16.271496\n",
      "ep 3635: ep_len:47 episode reward: total was 22.000000. running mean: -15.888781\n",
      "ep 3635: ep_len:1435 episode reward: total was 2.520000. running mean: -15.704694\n",
      "ep 3635: ep_len:2836 episode reward: total was -17.660000. running mean: -15.724247\n",
      "epsilon:0.009992 episode_count: 54648. steps_count: 58986912.000000\n",
      "ep 3636: ep_len:3771 episode reward: total was -413.830000. running mean: -19.705304\n",
      "ep 3636: ep_len:631 episode reward: total was 9.660000. running mean: -19.411651\n",
      "ep 3636: ep_len:65 episode reward: total was 28.000000. running mean: -18.937535\n",
      "ep 3636: ep_len:3041 episode reward: total was -60.440000. running mean: -19.352559\n",
      "ep 3636: ep_len:602 episode reward: total was -0.210000. running mean: -19.161134\n",
      "ep 3636: ep_len:843 episode reward: total was 71.270000. running mean: -18.256822\n",
      "ep 3636: ep_len:663 episode reward: total was 20.270000. running mean: -17.871554\n",
      "ep 3636: ep_len:522 episode reward: total was -44.440000. running mean: -18.137239\n",
      "ep 3636: ep_len:663 episode reward: total was -7.100000. running mean: -18.026866\n",
      "ep 3636: ep_len:686 episode reward: total was 2.240000. running mean: -17.824198\n",
      "ep 3636: ep_len:53 episode reward: total was 25.000000. running mean: -17.395956\n",
      "ep 3636: ep_len:181 episode reward: total was 87.500000. running mean: -16.346996\n",
      "ep 3636: ep_len:81 episode reward: total was 39.000000. running mean: -15.793526\n",
      "ep 3636: ep_len:790 episode reward: total was -3.310000. running mean: -15.668691\n",
      "ep 3636: ep_len:2797 episode reward: total was -18.200000. running mean: -15.694004\n",
      "epsilon:0.009992 episode_count: 54663. steps_count: 59002301.000000\n",
      "ep 3637: ep_len:1070 episode reward: total was -17.750000. running mean: -15.714564\n",
      "ep 3637: ep_len:500 episode reward: total was 17.510000. running mean: -15.382318\n",
      "ep 3637: ep_len:63 episode reward: total was 30.000000. running mean: -14.928495\n",
      "ep 3637: ep_len:3014 episode reward: total was -18.050000. running mean: -14.959710\n",
      "ep 3637: ep_len:501 episode reward: total was 26.840000. running mean: -14.541713\n",
      "ep 3637: ep_len:500 episode reward: total was 21.920000. running mean: -14.177096\n",
      "ep 3637: ep_len:662 episode reward: total was 23.140000. running mean: -13.803925\n",
      "ep 3637: ep_len:591 episode reward: total was 9.540000. running mean: -13.570486\n",
      "ep 3637: ep_len:808 episode reward: total was 20.960000. running mean: -13.225181\n",
      "ep 3637: ep_len:1478 episode reward: total was -6.820000. running mean: -13.161129\n",
      "ep 3637: ep_len:48 episode reward: total was 21.000000. running mean: -12.819518\n",
      "ep 3637: ep_len:55 episode reward: total was 26.000000. running mean: -12.431323\n",
      "ep 3637: ep_len:116 episode reward: total was 56.500000. running mean: -11.742009\n",
      "ep 3637: ep_len:1247 episode reward: total was -52.820000. running mean: -12.152789\n",
      "ep 3637: ep_len:2875 episode reward: total was -1.380000. running mean: -12.045061\n",
      "epsilon:0.009992 episode_count: 54678. steps_count: 59015829.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3638: ep_len:1111 episode reward: total was -12.290000. running mean: -12.047511\n",
      "ep 3638: ep_len:969 episode reward: total was 10.410000. running mean: -11.822936\n",
      "ep 3638: ep_len:47 episode reward: total was 20.500000. running mean: -11.499706\n",
      "ep 3638: ep_len:3023 episode reward: total was -16.260000. running mean: -11.547309\n",
      "ep 3638: ep_len:591 episode reward: total was 16.260000. running mean: -11.269236\n",
      "ep 3638: ep_len:100 episode reward: total was 48.500000. running mean: -10.671544\n",
      "ep 3638: ep_len:95 episode reward: total was 46.000000. running mean: -10.104828\n",
      "ep 3638: ep_len:625 episode reward: total was 26.260000. running mean: -9.741180\n",
      "ep 3638: ep_len:4114 episode reward: total was -215.170000. running mean: -11.795468\n",
      "ep 3638: ep_len:587 episode reward: total was -1.520000. running mean: -11.692714\n",
      "ep 3638: ep_len:743 episode reward: total was 40.620000. running mean: -11.169586\n",
      "ep 3638: ep_len:1114 episode reward: total was 5.010000. running mean: -11.007791\n",
      "ep 3638: ep_len:58 episode reward: total was 26.000000. running mean: -10.637713\n",
      "ep 3638: ep_len:773 episode reward: total was -59.950000. running mean: -11.130836\n",
      "ep 3638: ep_len:2891 episode reward: total was -2.960000. running mean: -11.049127\n",
      "ep 3638: ep_len:40 episode reward: total was 18.500000. running mean: -10.753636\n",
      "epsilon:0.009992 episode_count: 54694. steps_count: 59032710.000000\n",
      "ep 3639: ep_len:1120 episode reward: total was 2.640000. running mean: -10.619700\n",
      "ep 3639: ep_len:776 episode reward: total was -8.570000. running mean: -10.599203\n",
      "ep 3639: ep_len:3034 episode reward: total was 3.280000. running mean: -10.460411\n",
      "ep 3639: ep_len:827 episode reward: total was 10.730000. running mean: -10.248506\n",
      "ep 3639: ep_len:57 episode reward: total was 27.000000. running mean: -9.876021\n",
      "ep 3639: ep_len:67 episode reward: total was 29.000000. running mean: -9.487261\n",
      "ep 3639: ep_len:500 episode reward: total was 35.490000. running mean: -9.037489\n",
      "ep 3639: ep_len:325 episode reward: total was 16.440000. running mean: -8.782714\n",
      "ep 3639: ep_len:677 episode reward: total was -2.490000. running mean: -8.719786\n",
      "ep 3639: ep_len:664 episode reward: total was 15.320000. running mean: -8.479389\n",
      "ep 3639: ep_len:1441 episode reward: total was -12.850000. running mean: -8.523095\n",
      "ep 3639: ep_len:70 episode reward: total was 33.500000. running mean: -8.102864\n",
      "ep 3639: ep_len:156 episode reward: total was 75.000000. running mean: -7.271835\n",
      "ep 3639: ep_len:761 episode reward: total was -28.920000. running mean: -7.488317\n",
      "ep 3639: ep_len:2708 episode reward: total was -39.870000. running mean: -7.812134\n",
      "ep 3639: ep_len:49 episode reward: total was 23.000000. running mean: -7.504012\n",
      "epsilon:0.009992 episode_count: 54710. steps_count: 59045942.000000\n",
      "ep 3640: ep_len:3682 episode reward: total was -738.510000. running mean: -14.814072\n",
      "ep 3640: ep_len:1221 episode reward: total was -89.970000. running mean: -15.565631\n",
      "ep 3640: ep_len:3037 episode reward: total was -44.630000. running mean: -15.856275\n",
      "ep 3640: ep_len:500 episode reward: total was 33.440000. running mean: -15.363312\n",
      "ep 3640: ep_len:1475 episode reward: total was 1.020000. running mean: -15.199479\n",
      "ep 3640: ep_len:652 episode reward: total was 22.240000. running mean: -14.825084\n",
      "ep 3640: ep_len:1591 episode reward: total was -55.450000. running mean: -15.231334\n",
      "ep 3640: ep_len:7215 episode reward: total was -3.710000. running mean: -15.116120\n",
      "ep 3640: ep_len:659 episode reward: total was -12.750000. running mean: -15.092459\n",
      "ep 3640: ep_len:79 episode reward: total was 38.000000. running mean: -14.561534\n",
      "ep 3640: ep_len:62 episode reward: total was 29.500000. running mean: -14.120919\n",
      "ep 3640: ep_len:121 episode reward: total was 57.500000. running mean: -13.404710\n",
      "ep 3640: ep_len:699 episode reward: total was -12.860000. running mean: -13.399263\n",
      "ep 3640: ep_len:2779 episode reward: total was -69.980000. running mean: -13.965070\n",
      "ep 3640: ep_len:39 episode reward: total was 18.000000. running mean: -13.645420\n",
      "epsilon:0.009992 episode_count: 54725. steps_count: 59069753.000000\n",
      "ep 3641: ep_len:662 episode reward: total was -34.100000. running mean: -13.849965\n",
      "ep 3641: ep_len:1631 episode reward: total was -40.720000. running mean: -14.118666\n",
      "ep 3641: ep_len:3022 episode reward: total was -50.870000. running mean: -14.486179\n",
      "ep 3641: ep_len:1594 episode reward: total was -90.100000. running mean: -15.242317\n",
      "ep 3641: ep_len:31 episode reward: total was 14.000000. running mean: -14.949894\n",
      "ep 3641: ep_len:106 episode reward: total was 50.000000. running mean: -14.300395\n",
      "ep 3641: ep_len:860 episode reward: total was 34.570000. running mean: -13.811691\n",
      "ep 3641: ep_len:357 episode reward: total was 16.330000. running mean: -13.510274\n",
      "ep 3641: ep_len:1215 episode reward: total was -58.410000. running mean: -13.959272\n",
      "ep 3641: ep_len:684 episode reward: total was 5.490000. running mean: -13.764779\n",
      "ep 3641: ep_len:965 episode reward: total was 27.210000. running mean: -13.355031\n",
      "ep 3641: ep_len:54 episode reward: total was 25.500000. running mean: -12.966481\n",
      "ep 3641: ep_len:181 episode reward: total was 87.500000. running mean: -11.961816\n",
      "ep 3641: ep_len:1439 episode reward: total was 10.480000. running mean: -11.737398\n",
      "ep 3641: ep_len:2908 episode reward: total was -18.010000. running mean: -11.800124\n",
      "epsilon:0.009992 episode_count: 54740. steps_count: 59085462.000000\n",
      "ep 3642: ep_len:1454 episode reward: total was -3.300000. running mean: -11.715123\n",
      "ep 3642: ep_len:940 episode reward: total was 14.680000. running mean: -11.451171\n",
      "ep 3642: ep_len:2959 episode reward: total was -27.650000. running mean: -11.613160\n",
      "ep 3642: ep_len:650 episode reward: total was -5.150000. running mean: -11.548528\n",
      "ep 3642: ep_len:93 episode reward: total was 45.000000. running mean: -10.983043\n",
      "ep 3642: ep_len:67 episode reward: total was 29.000000. running mean: -10.583212\n",
      "ep 3642: ep_len:1409 episode reward: total was -88.090000. running mean: -11.358280\n",
      "ep 3642: ep_len:4009 episode reward: total was -218.850000. running mean: -13.433197\n",
      "ep 3642: ep_len:1189 episode reward: total was -89.220000. running mean: -14.191065\n",
      "ep 3642: ep_len:7213 episode reward: total was 10.100000. running mean: -13.948155\n",
      "ep 3642: ep_len:1147 episode reward: total was -44.250000. running mean: -14.251173\n",
      "ep 3642: ep_len:35 episode reward: total was 14.500000. running mean: -13.963661\n",
      "ep 3642: ep_len:791 episode reward: total was -75.590000. running mean: -14.579925\n",
      "ep 3642: ep_len:2832 episode reward: total was -8.670000. running mean: -14.520826\n",
      "ep 3642: ep_len:55 episode reward: total was 24.500000. running mean: -14.130617\n",
      "epsilon:0.009992 episode_count: 54755. steps_count: 59110305.000000\n",
      "ep 3643: ep_len:1475 episode reward: total was 27.070000. running mean: -13.718611\n",
      "ep 3643: ep_len:203 episode reward: total was 7.380000. running mean: -13.507625\n",
      "ep 3643: ep_len:70 episode reward: total was 33.500000. running mean: -13.037549\n",
      "ep 3643: ep_len:3015 episode reward: total was -32.530000. running mean: -13.232473\n",
      "ep 3643: ep_len:1701 episode reward: total was -59.160000. running mean: -13.691749\n",
      "ep 3643: ep_len:98 episode reward: total was 46.000000. running mean: -13.094831\n",
      "ep 3643: ep_len:68 episode reward: total was 32.500000. running mean: -12.638883\n",
      "ep 3643: ep_len:500 episode reward: total was 1.620000. running mean: -12.496294\n",
      "ep 3643: ep_len:590 episode reward: total was 16.630000. running mean: -12.205031\n",
      "ep 3643: ep_len:762 episode reward: total was -51.080000. running mean: -12.593781\n",
      "ep 3643: ep_len:782 episode reward: total was 40.760000. running mean: -12.060243\n",
      "ep 3643: ep_len:913 episode reward: total was 41.110000. running mean: -11.528540\n",
      "ep 3643: ep_len:57 episode reward: total was 25.500000. running mean: -11.158255\n",
      "ep 3643: ep_len:164 episode reward: total was 80.500000. running mean: -10.241673\n",
      "ep 3643: ep_len:102 episode reward: total was 49.500000. running mean: -9.644256\n",
      "ep 3643: ep_len:500 episode reward: total was 23.730000. running mean: -9.310513\n",
      "ep 3643: ep_len:43 episode reward: total was 20.000000. running mean: -9.017408\n",
      "ep 3643: ep_len:71 episode reward: total was 32.500000. running mean: -8.602234\n",
      "epsilon:0.009992 episode_count: 54773. steps_count: 59121419.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3644: ep_len:720 episode reward: total was -31.350000. running mean: -8.829712\n",
      "ep 3644: ep_len:758 episode reward: total was -38.820000. running mean: -9.129615\n",
      "ep 3644: ep_len:63 episode reward: total was 30.000000. running mean: -8.738318\n",
      "ep 3644: ep_len:3050 episode reward: total was -41.490000. running mean: -9.065835\n",
      "ep 3644: ep_len:1659 episode reward: total was -27.470000. running mean: -9.249877\n",
      "ep 3644: ep_len:49 episode reward: total was 23.000000. running mean: -8.927378\n",
      "ep 3644: ep_len:105 episode reward: total was 49.500000. running mean: -8.343104\n",
      "ep 3644: ep_len:51 episode reward: total was 24.000000. running mean: -8.019673\n",
      "ep 3644: ep_len:1055 episode reward: total was -53.250000. running mean: -8.471977\n",
      "ep 3644: ep_len:3641 episode reward: total was -3.960000. running mean: -8.426857\n",
      "ep 3644: ep_len:1221 episode reward: total was -59.670000. running mean: -8.939288\n",
      "ep 3644: ep_len:700 episode reward: total was -23.990000. running mean: -9.089795\n",
      "ep 3644: ep_len:517 episode reward: total was 12.480000. running mean: -8.874097\n",
      "ep 3644: ep_len:120 episode reward: total was 58.500000. running mean: -8.200356\n",
      "ep 3644: ep_len:1230 episode reward: total was -27.260000. running mean: -8.390953\n",
      "ep 3644: ep_len:47 episode reward: total was 22.000000. running mean: -8.087043\n",
      "epsilon:0.009992 episode_count: 54789. steps_count: 59136405.000000\n",
      "ep 3645: ep_len:1413 episode reward: total was -3.920000. running mean: -8.045373\n",
      "ep 3645: ep_len:1613 episode reward: total was -35.580000. running mean: -8.320719\n",
      "ep 3645: ep_len:3015 episode reward: total was -32.480000. running mean: -8.562312\n",
      "ep 3645: ep_len:649 episode reward: total was -0.750000. running mean: -8.484189\n",
      "ep 3645: ep_len:71 episode reward: total was 31.000000. running mean: -8.089347\n",
      "ep 3645: ep_len:951 episode reward: total was 78.080000. running mean: -7.227653\n",
      "ep 3645: ep_len:649 episode reward: total was 23.590000. running mean: -6.919477\n",
      "ep 3645: ep_len:1213 episode reward: total was -29.910000. running mean: -7.149382\n",
      "ep 3645: ep_len:7286 episode reward: total was 51.760000. running mean: -6.560288\n",
      "ep 3645: ep_len:1160 episode reward: total was -1.330000. running mean: -6.507985\n",
      "ep 3645: ep_len:191 episode reward: total was 92.500000. running mean: -5.517906\n",
      "ep 3645: ep_len:76 episode reward: total was 35.000000. running mean: -5.112727\n",
      "ep 3645: ep_len:1054 episode reward: total was 8.470000. running mean: -4.976899\n",
      "ep 3645: ep_len:2872 episode reward: total was -35.440000. running mean: -5.281530\n",
      "ep 3645: ep_len:58 episode reward: total was 26.000000. running mean: -4.968715\n",
      "epsilon:0.009992 episode_count: 54804. steps_count: 59158676.000000\n",
      "ep 3646: ep_len:1461 episode reward: total was 22.000000. running mean: -4.699028\n",
      "ep 3646: ep_len:667 episode reward: total was -7.640000. running mean: -4.728438\n",
      "ep 3646: ep_len:3089 episode reward: total was -106.960000. running mean: -5.750753\n",
      "ep 3646: ep_len:745 episode reward: total was -7.160000. running mean: -5.764846\n",
      "ep 3646: ep_len:92 episode reward: total was 43.000000. running mean: -5.277197\n",
      "ep 3646: ep_len:1862 episode reward: total was -138.840000. running mean: -6.612825\n",
      "ep 3646: ep_len:3601 episode reward: total was -379.270000. running mean: -10.339397\n",
      "ep 3646: ep_len:599 episode reward: total was 20.270000. running mean: -10.033303\n",
      "ep 3646: ep_len:661 episode reward: total was 10.560000. running mean: -9.827370\n",
      "ep 3646: ep_len:882 episode reward: total was 24.940000. running mean: -9.479696\n",
      "ep 3646: ep_len:66 episode reward: total was 31.500000. running mean: -9.069899\n",
      "ep 3646: ep_len:157 episode reward: total was 77.000000. running mean: -8.209200\n",
      "ep 3646: ep_len:46 episode reward: total was 20.000000. running mean: -7.927108\n",
      "ep 3646: ep_len:93 episode reward: total was 43.500000. running mean: -7.412837\n",
      "ep 3646: ep_len:956 episode reward: total was -47.180000. running mean: -7.810509\n",
      "ep 3646: ep_len:2839 episode reward: total was 5.120000. running mean: -7.681204\n",
      "epsilon:0.009992 episode_count: 54820. steps_count: 59176492.000000\n",
      "ep 3647: ep_len:996 episode reward: total was -74.040000. running mean: -8.344792\n",
      "ep 3647: ep_len:1696 episode reward: total was -16.810000. running mean: -8.429444\n",
      "ep 3647: ep_len:51 episode reward: total was 24.000000. running mean: -8.105149\n",
      "ep 3647: ep_len:2895 episode reward: total was -53.550000. running mean: -8.559598\n",
      "ep 3647: ep_len:505 episode reward: total was -8.250000. running mean: -8.556502\n",
      "ep 3647: ep_len:64 episode reward: total was 30.500000. running mean: -8.165937\n",
      "ep 3647: ep_len:500 episode reward: total was 10.200000. running mean: -7.982278\n",
      "ep 3647: ep_len:3718 episode reward: total was -25.770000. running mean: -8.160155\n",
      "ep 3647: ep_len:611 episode reward: total was -16.430000. running mean: -8.242853\n",
      "ep 3647: ep_len:810 episode reward: total was 50.440000. running mean: -7.656025\n",
      "ep 3647: ep_len:603 episode reward: total was 13.110000. running mean: -7.448364\n",
      "ep 3647: ep_len:138 episode reward: total was 67.500000. running mean: -6.698881\n",
      "ep 3647: ep_len:595 episode reward: total was -319.290000. running mean: -9.824792\n",
      "ep 3647: ep_len:2898 episode reward: total was 6.840000. running mean: -9.658144\n",
      "epsilon:0.009992 episode_count: 54834. steps_count: 59192572.000000\n",
      "ep 3648: ep_len:730 episode reward: total was -16.280000. running mean: -9.724363\n",
      "ep 3648: ep_len:176 episode reward: total was 7.260000. running mean: -9.554519\n",
      "ep 3648: ep_len:3063 episode reward: total was -36.820000. running mean: -9.827174\n",
      "ep 3648: ep_len:1163 episode reward: total was -27.930000. running mean: -10.008202\n",
      "ep 3648: ep_len:50 episode reward: total was 23.500000. running mean: -9.673120\n",
      "ep 3648: ep_len:500 episode reward: total was 1.250000. running mean: -9.563889\n",
      "ep 3648: ep_len:322 episode reward: total was 28.990000. running mean: -9.178350\n",
      "ep 3648: ep_len:500 episode reward: total was 41.030000. running mean: -8.676266\n",
      "ep 3648: ep_len:789 episode reward: total was 14.190000. running mean: -8.447604\n",
      "ep 3648: ep_len:609 episode reward: total was -1.280000. running mean: -8.375928\n",
      "ep 3648: ep_len:69 episode reward: total was 31.500000. running mean: -7.977168\n",
      "ep 3648: ep_len:150 episode reward: total was 72.000000. running mean: -7.177397\n",
      "ep 3648: ep_len:895 episode reward: total was 7.190000. running mean: -7.033723\n",
      "ep 3648: ep_len:2836 episode reward: total was -20.930000. running mean: -7.172686\n",
      "epsilon:0.009992 episode_count: 54848. steps_count: 59204424.000000\n",
      "ep 3649: ep_len:1141 episode reward: total was -3.390000. running mean: -7.134859\n",
      "ep 3649: ep_len:755 episode reward: total was 14.800000. running mean: -6.915510\n",
      "ep 3649: ep_len:2967 episode reward: total was -10.330000. running mean: -6.949655\n",
      "ep 3649: ep_len:684 episode reward: total was 6.840000. running mean: -6.811758\n",
      "ep 3649: ep_len:1017 episode reward: total was -20.540000. running mean: -6.949041\n",
      "ep 3649: ep_len:3785 episode reward: total was 26.600000. running mean: -6.613550\n",
      "ep 3649: ep_len:1224 episode reward: total was -19.560000. running mean: -6.743015\n",
      "ep 3649: ep_len:753 episode reward: total was 40.840000. running mean: -6.267185\n",
      "ep 3649: ep_len:1514 episode reward: total was 24.560000. running mean: -5.958913\n",
      "ep 3649: ep_len:56 episode reward: total was 26.500000. running mean: -5.634324\n",
      "ep 3649: ep_len:58 episode reward: total was 27.500000. running mean: -5.302981\n",
      "ep 3649: ep_len:678 episode reward: total was 4.110000. running mean: -5.208851\n",
      "ep 3649: ep_len:2857 episode reward: total was -10.830000. running mean: -5.265062\n",
      "epsilon:0.009992 episode_count: 54861. steps_count: 59221913.000000\n",
      "ep 3650: ep_len:1404 episode reward: total was 20.480000. running mean: -5.007612\n",
      "ep 3650: ep_len:665 episode reward: total was -3.680000. running mean: -4.994336\n",
      "ep 3650: ep_len:3045 episode reward: total was -44.610000. running mean: -5.390492\n",
      "ep 3650: ep_len:1070 episode reward: total was -9.670000. running mean: -5.433287\n",
      "ep 3650: ep_len:32 episode reward: total was 14.500000. running mean: -5.233954\n",
      "ep 3650: ep_len:132 episode reward: total was 63.000000. running mean: -4.551615\n",
      "ep 3650: ep_len:50 episode reward: total was 23.500000. running mean: -4.271099\n",
      "ep 3650: ep_len:46 episode reward: total was 20.000000. running mean: -4.028388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3650: ep_len:894 episode reward: total was 56.350000. running mean: -3.424604\n",
      "ep 3650: ep_len:3602 episode reward: total was -30.430000. running mean: -3.694658\n",
      "ep 3650: ep_len:789 episode reward: total was -30.060000. running mean: -3.958311\n",
      "ep 3650: ep_len:749 episode reward: total was 24.500000. running mean: -3.673728\n",
      "ep 3650: ep_len:500 episode reward: total was -4.440000. running mean: -3.681391\n",
      "ep 3650: ep_len:71 episode reward: total was 34.000000. running mean: -3.304577\n",
      "ep 3650: ep_len:1375 episode reward: total was 1.000000. running mean: -3.261531\n",
      "ep 3650: ep_len:2862 episode reward: total was -5.580000. running mean: -3.284716\n",
      "ep 3650: ep_len:56 episode reward: total was 25.000000. running mean: -3.001869\n",
      "epsilon:0.009992 episode_count: 54878. steps_count: 59239255.000000\n",
      "ep 3651: ep_len:1447 episode reward: total was 7.810000. running mean: -2.893750\n",
      "ep 3651: ep_len:3172 episode reward: total was -310.100000. running mean: -5.965813\n",
      "ep 3651: ep_len:67 episode reward: total was 32.000000. running mean: -5.586154\n",
      "ep 3651: ep_len:3069 episode reward: total was -31.660000. running mean: -5.846893\n",
      "ep 3651: ep_len:500 episode reward: total was 13.840000. running mean: -5.650024\n",
      "ep 3651: ep_len:69 episode reward: total was 33.000000. running mean: -5.263524\n",
      "ep 3651: ep_len:40 episode reward: total was 17.000000. running mean: -5.040888\n",
      "ep 3651: ep_len:500 episode reward: total was 7.010000. running mean: -4.920380\n",
      "ep 3651: ep_len:4242 episode reward: total was -81.180000. running mean: -5.682976\n",
      "ep 3651: ep_len:561 episode reward: total was -16.170000. running mean: -5.787846\n",
      "ep 3651: ep_len:745 episode reward: total was 47.250000. running mean: -5.257468\n",
      "ep 3651: ep_len:504 episode reward: total was 0.990000. running mean: -5.194993\n",
      "ep 3651: ep_len:50 episode reward: total was 20.500000. running mean: -4.938043\n",
      "ep 3651: ep_len:500 episode reward: total was -3.950000. running mean: -4.928163\n",
      "ep 3651: ep_len:2832 episode reward: total was -9.130000. running mean: -4.970181\n",
      "epsilon:0.009992 episode_count: 54893. steps_count: 59257553.000000\n",
      "ep 3652: ep_len:667 episode reward: total was 1.680000. running mean: -4.903679\n",
      "ep 3652: ep_len:500 episode reward: total was 10.100000. running mean: -4.753642\n",
      "ep 3652: ep_len:53 episode reward: total was 23.500000. running mean: -4.471106\n",
      "ep 3652: ep_len:3083 episode reward: total was -22.360000. running mean: -4.649995\n",
      "ep 3652: ep_len:828 episode reward: total was 21.790000. running mean: -4.385595\n",
      "ep 3652: ep_len:47 episode reward: total was 22.000000. running mean: -4.121739\n",
      "ep 3652: ep_len:1081 episode reward: total was -4.510000. running mean: -4.125622\n",
      "ep 3652: ep_len:3957 episode reward: total was -44.050000. running mean: -4.524865\n",
      "ep 3652: ep_len:776 episode reward: total was -0.900000. running mean: -4.488617\n",
      "ep 3652: ep_len:622 episode reward: total was -5.430000. running mean: -4.498030\n",
      "ep 3652: ep_len:1002 episode reward: total was 30.640000. running mean: -4.146650\n",
      "ep 3652: ep_len:38 episode reward: total was 16.000000. running mean: -3.945184\n",
      "ep 3652: ep_len:1079 episode reward: total was -62.940000. running mean: -4.535132\n",
      "ep 3652: ep_len:2955 episode reward: total was 2.910000. running mean: -4.460681\n",
      "epsilon:0.009992 episode_count: 54907. steps_count: 59274241.000000\n",
      "ep 3653: ep_len:1057 episode reward: total was -6.770000. running mean: -4.483774\n",
      "ep 3653: ep_len:760 episode reward: total was -10.750000. running mean: -4.546436\n",
      "ep 3653: ep_len:2976 episode reward: total was -39.900000. running mean: -4.899972\n",
      "ep 3653: ep_len:547 episode reward: total was -42.170000. running mean: -5.272672\n",
      "ep 3653: ep_len:106 episode reward: total was 51.500000. running mean: -4.704945\n",
      "ep 3653: ep_len:770 episode reward: total was -38.380000. running mean: -5.041696\n",
      "ep 3653: ep_len:652 episode reward: total was 34.580000. running mean: -4.645479\n",
      "ep 3653: ep_len:1227 episode reward: total was -40.420000. running mean: -5.003224\n",
      "ep 3653: ep_len:757 episode reward: total was 1.470000. running mean: -4.938492\n",
      "ep 3653: ep_len:500 episode reward: total was 32.770000. running mean: -4.561407\n",
      "ep 3653: ep_len:61 episode reward: total was 27.500000. running mean: -4.240793\n",
      "ep 3653: ep_len:190 episode reward: total was 87.500000. running mean: -3.323385\n",
      "ep 3653: ep_len:120 episode reward: total was 58.500000. running mean: -2.705151\n",
      "ep 3653: ep_len:845 episode reward: total was 16.900000. running mean: -2.509099\n",
      "ep 3653: ep_len:33 episode reward: total was 15.000000. running mean: -2.334008\n",
      "ep 3653: ep_len:41 episode reward: total was 19.000000. running mean: -2.120668\n",
      "epsilon:0.009992 episode_count: 54923. steps_count: 59284883.000000\n",
      "ep 3654: ep_len:933 episode reward: total was -96.370000. running mean: -3.063162\n",
      "ep 3654: ep_len:765 episode reward: total was -29.280000. running mean: -3.325330\n",
      "ep 3654: ep_len:64 episode reward: total was 29.000000. running mean: -3.002077\n",
      "ep 3654: ep_len:3031 episode reward: total was -36.690000. running mean: -3.338956\n",
      "ep 3654: ep_len:676 episode reward: total was 15.860000. running mean: -3.146966\n",
      "ep 3654: ep_len:44 episode reward: total was 20.500000. running mean: -2.910497\n",
      "ep 3654: ep_len:133 episode reward: total was 65.000000. running mean: -2.231392\n",
      "ep 3654: ep_len:754 episode reward: total was -14.180000. running mean: -2.350878\n",
      "ep 3654: ep_len:351 episode reward: total was 9.080000. running mean: -2.236569\n",
      "ep 3654: ep_len:790 episode reward: total was -25.520000. running mean: -2.469403\n",
      "ep 3654: ep_len:775 episode reward: total was 4.080000. running mean: -2.403909\n",
      "ep 3654: ep_len:596 episode reward: total was -14.410000. running mean: -2.523970\n",
      "ep 3654: ep_len:116 episode reward: total was 55.000000. running mean: -1.948731\n",
      "ep 3654: ep_len:51 episode reward: total was 22.500000. running mean: -1.704243\n",
      "ep 3654: ep_len:770 episode reward: total was -24.790000. running mean: -1.935101\n",
      "ep 3654: ep_len:2789 episode reward: total was 3.570000. running mean: -1.880050\n",
      "ep 3654: ep_len:61 episode reward: total was 29.000000. running mean: -1.571249\n",
      "epsilon:0.009992 episode_count: 54940. steps_count: 59297582.000000\n",
      "ep 3655: ep_len:938 episode reward: total was -46.340000. running mean: -2.018937\n",
      "ep 3655: ep_len:691 episode reward: total was -23.640000. running mean: -2.235148\n",
      "ep 3655: ep_len:56 episode reward: total was 26.500000. running mean: -1.947796\n",
      "ep 3655: ep_len:2992 episode reward: total was 1.120000. running mean: -1.917118\n",
      "ep 3655: ep_len:500 episode reward: total was -75.680000. running mean: -2.654747\n",
      "ep 3655: ep_len:57 episode reward: total was 27.000000. running mean: -2.358199\n",
      "ep 3655: ep_len:64 episode reward: total was 29.000000. running mean: -2.044617\n",
      "ep 3655: ep_len:646 episode reward: total was -1.740000. running mean: -2.041571\n",
      "ep 3655: ep_len:3781 episode reward: total was -23.450000. running mean: -2.255656\n",
      "ep 3655: ep_len:1279 episode reward: total was -84.890000. running mean: -3.081999\n",
      "ep 3655: ep_len:743 episode reward: total was -17.560000. running mean: -3.226779\n",
      "ep 3655: ep_len:500 episode reward: total was 16.920000. running mean: -3.025311\n",
      "ep 3655: ep_len:57 episode reward: total was 25.500000. running mean: -2.740058\n",
      "ep 3655: ep_len:102 episode reward: total was 46.500000. running mean: -2.247658\n",
      "ep 3655: ep_len:765 episode reward: total was -15.660000. running mean: -2.381781\n",
      "ep 3655: ep_len:2916 episode reward: total was 5.730000. running mean: -2.300663\n",
      "ep 3655: ep_len:40 episode reward: total was 18.500000. running mean: -2.092656\n",
      "epsilon:0.009992 episode_count: 54957. steps_count: 59313709.000000\n",
      "ep 3656: ep_len:1136 episode reward: total was -5.490000. running mean: -2.126630\n",
      "ep 3656: ep_len:631 episode reward: total was -11.340000. running mean: -2.218764\n",
      "ep 3656: ep_len:88 episode reward: total was 39.500000. running mean: -1.801576\n",
      "ep 3656: ep_len:885 episode reward: total was 8.770000. running mean: -1.695860\n",
      "ep 3656: ep_len:57 episode reward: total was 27.000000. running mean: -1.408902\n",
      "ep 3656: ep_len:104 episode reward: total was 47.500000. running mean: -0.919813\n",
      "ep 3656: ep_len:975 episode reward: total was -17.750000. running mean: -1.088114\n",
      "ep 3656: ep_len:3669 episode reward: total was -1.480000. running mean: -1.092033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3656: ep_len:2798 episode reward: total was -871.090000. running mean: -9.792013\n",
      "ep 3656: ep_len:823 episode reward: total was 59.240000. running mean: -9.101693\n",
      "ep 3656: ep_len:500 episode reward: total was -4.720000. running mean: -9.057876\n",
      "ep 3656: ep_len:73 episode reward: total was 33.500000. running mean: -8.632297\n",
      "ep 3656: ep_len:629 episode reward: total was 16.540000. running mean: -8.380574\n",
      "ep 3656: ep_len:2833 episode reward: total was 4.200000. running mean: -8.254768\n",
      "ep 3656: ep_len:44 episode reward: total was 20.500000. running mean: -7.967221\n",
      "epsilon:0.009992 episode_count: 54972. steps_count: 59328954.000000\n",
      "ep 3657: ep_len:693 episode reward: total was -5.360000. running mean: -7.941149\n",
      "ep 3657: ep_len:698 episode reward: total was -19.930000. running mean: -8.061037\n",
      "ep 3657: ep_len:2985 episode reward: total was -27.520000. running mean: -8.255627\n",
      "ep 3657: ep_len:1441 episode reward: total was 25.810000. running mean: -7.914970\n",
      "ep 3657: ep_len:135 episode reward: total was 63.000000. running mean: -7.205821\n",
      "ep 3657: ep_len:63 episode reward: total was 30.000000. running mean: -6.833763\n",
      "ep 3657: ep_len:500 episode reward: total was 26.180000. running mean: -6.503625\n",
      "ep 3657: ep_len:3635 episode reward: total was 0.200000. running mean: -6.436589\n",
      "ep 3657: ep_len:556 episode reward: total was 9.310000. running mean: -6.279123\n",
      "ep 3657: ep_len:806 episode reward: total was -4.590000. running mean: -6.262232\n",
      "ep 3657: ep_len:664 episode reward: total was 17.650000. running mean: -6.023109\n",
      "ep 3657: ep_len:85 episode reward: total was 41.000000. running mean: -5.552878\n",
      "ep 3657: ep_len:96 episode reward: total was 42.000000. running mean: -5.077349\n",
      "ep 3657: ep_len:75 episode reward: total was 34.500000. running mean: -4.681576\n",
      "ep 3657: ep_len:1521 episode reward: total was -324.920000. running mean: -7.883960\n",
      "ep 3657: ep_len:2879 episode reward: total was -9.270000. running mean: -7.897821\n",
      "epsilon:0.009992 episode_count: 54988. steps_count: 59345786.000000\n",
      "ep 3658: ep_len:903 episode reward: total was -80.450000. running mean: -8.623342\n",
      "ep 3658: ep_len:500 episode reward: total was 9.980000. running mean: -8.437309\n",
      "ep 3658: ep_len:2904 episode reward: total was -45.250000. running mean: -8.805436\n",
      "ep 3658: ep_len:682 episode reward: total was 0.980000. running mean: -8.707581\n",
      "ep 3658: ep_len:53 episode reward: total was 23.500000. running mean: -8.385506\n",
      "ep 3658: ep_len:620 episode reward: total was -3.010000. running mean: -8.331751\n",
      "ep 3658: ep_len:4050 episode reward: total was -62.870000. running mean: -8.877133\n",
      "ep 3658: ep_len:898 episode reward: total was -31.590000. running mean: -9.104262\n",
      "ep 3658: ep_len:731 episode reward: total was -10.600000. running mean: -9.119219\n",
      "ep 3658: ep_len:945 episode reward: total was 24.990000. running mean: -8.778127\n",
      "ep 3658: ep_len:92 episode reward: total was 43.000000. running mean: -8.260346\n",
      "ep 3658: ep_len:131 episode reward: total was 63.510000. running mean: -7.542642\n",
      "ep 3658: ep_len:619 episode reward: total was 3.840000. running mean: -7.428816\n",
      "ep 3658: ep_len:2774 episode reward: total was 9.180000. running mean: -7.262728\n",
      "epsilon:0.009992 episode_count: 55002. steps_count: 59361688.000000\n",
      "ep 3659: ep_len:1476 episode reward: total was 14.680000. running mean: -7.043300\n",
      "ep 3659: ep_len:668 episode reward: total was 17.170000. running mean: -6.801167\n",
      "ep 3659: ep_len:61 episode reward: total was 27.500000. running mean: -6.458156\n",
      "ep 3659: ep_len:2880 episode reward: total was -43.020000. running mean: -6.823774\n",
      "ep 3659: ep_len:633 episode reward: total was 12.720000. running mean: -6.628336\n",
      "ep 3659: ep_len:52 episode reward: total was 24.500000. running mean: -6.317053\n",
      "ep 3659: ep_len:66 episode reward: total was 31.500000. running mean: -5.938882\n",
      "ep 3659: ep_len:863 episode reward: total was 22.510000. running mean: -5.654394\n",
      "ep 3659: ep_len:3627 episode reward: total was -6.310000. running mean: -5.660950\n",
      "ep 3659: ep_len:542 episode reward: total was -0.810000. running mean: -5.612440\n",
      "ep 3659: ep_len:677 episode reward: total was 25.140000. running mean: -5.304916\n",
      "ep 3659: ep_len:645 episode reward: total was 0.220000. running mean: -5.249667\n",
      "ep 3659: ep_len:41 episode reward: total was 19.000000. running mean: -5.007170\n",
      "ep 3659: ep_len:95 episode reward: total was 46.000000. running mean: -4.497098\n",
      "ep 3659: ep_len:1183 episode reward: total was -4.580000. running mean: -4.497927\n",
      "ep 3659: ep_len:2826 episode reward: total was -21.460000. running mean: -4.667548\n",
      "epsilon:0.009992 episode_count: 55018. steps_count: 59378023.000000\n",
      "ep 3660: ep_len:956 episode reward: total was -35.020000. running mean: -4.971073\n",
      "ep 3660: ep_len:723 episode reward: total was -6.270000. running mean: -4.984062\n",
      "ep 3660: ep_len:53 episode reward: total was 25.000000. running mean: -4.684221\n",
      "ep 3660: ep_len:3047 episode reward: total was -6.470000. running mean: -4.702079\n",
      "ep 3660: ep_len:571 episode reward: total was -3.980000. running mean: -4.694858\n",
      "ep 3660: ep_len:62 episode reward: total was 29.500000. running mean: -4.352910\n",
      "ep 3660: ep_len:50 episode reward: total was 23.500000. running mean: -4.074381\n",
      "ep 3660: ep_len:890 episode reward: total was 50.000000. running mean: -3.533637\n",
      "ep 3660: ep_len:3948 episode reward: total was -73.610000. running mean: -4.234400\n",
      "ep 3660: ep_len:1162 episode reward: total was -67.670000. running mean: -4.868756\n",
      "ep 3660: ep_len:855 episode reward: total was 38.720000. running mean: -4.432869\n",
      "ep 3660: ep_len:1446 episode reward: total was 4.400000. running mean: -4.344540\n",
      "ep 3660: ep_len:82 episode reward: total was 39.500000. running mean: -3.906095\n",
      "ep 3660: ep_len:612 episode reward: total was 5.490000. running mean: -3.812134\n",
      "ep 3660: ep_len:2809 episode reward: total was -17.920000. running mean: -3.953212\n",
      "ep 3660: ep_len:25 episode reward: total was 11.000000. running mean: -3.803680\n",
      "epsilon:0.009992 episode_count: 55034. steps_count: 59395314.000000\n",
      "ep 3661: ep_len:578 episode reward: total was 2.450000. running mean: -3.741143\n",
      "ep 3661: ep_len:781 episode reward: total was -7.050000. running mean: -3.774232\n",
      "ep 3661: ep_len:73 episode reward: total was 35.000000. running mean: -3.386490\n",
      "ep 3661: ep_len:2858 episode reward: total was -21.480000. running mean: -3.567425\n",
      "ep 3661: ep_len:718 episode reward: total was -10.020000. running mean: -3.631951\n",
      "ep 3661: ep_len:155 episode reward: total was 73.000000. running mean: -2.865631\n",
      "ep 3661: ep_len:500 episode reward: total was 11.590000. running mean: -2.721075\n",
      "ep 3661: ep_len:3890 episode reward: total was -2.060000. running mean: -2.714464\n",
      "ep 3661: ep_len:789 episode reward: total was -21.700000. running mean: -2.904319\n",
      "ep 3661: ep_len:623 episode reward: total was 6.610000. running mean: -2.809176\n",
      "ep 3661: ep_len:879 episode reward: total was 22.950000. running mean: -2.551584\n",
      "ep 3661: ep_len:959 episode reward: total was -76.430000. running mean: -3.290369\n",
      "ep 3661: ep_len:2869 episode reward: total was -8.480000. running mean: -3.342265\n",
      "ep 3661: ep_len:74 episode reward: total was 34.000000. running mean: -2.968842\n",
      "epsilon:0.009992 episode_count: 55048. steps_count: 59411060.000000\n",
      "ep 3662: ep_len:865 episode reward: total was 19.560000. running mean: -2.743554\n",
      "ep 3662: ep_len:199 episode reward: total was 9.820000. running mean: -2.617918\n",
      "ep 3662: ep_len:70 episode reward: total was 33.500000. running mean: -2.256739\n",
      "ep 3662: ep_len:2990 episode reward: total was -1.750000. running mean: -2.251672\n",
      "ep 3662: ep_len:1153 episode reward: total was -6.790000. running mean: -2.297055\n",
      "ep 3662: ep_len:66 episode reward: total was 31.500000. running mean: -1.959084\n",
      "ep 3662: ep_len:1506 episode reward: total was -3.600000. running mean: -1.975494\n",
      "ep 3662: ep_len:3647 episode reward: total was -34.050000. running mean: -2.296239\n",
      "ep 3662: ep_len:820 episode reward: total was -48.780000. running mean: -2.761076\n",
      "ep 3662: ep_len:719 episode reward: total was 49.160000. running mean: -2.241866\n",
      "ep 3662: ep_len:682 episode reward: total was 26.720000. running mean: -1.952247\n",
      "ep 3662: ep_len:23 episode reward: total was 10.000000. running mean: -1.832724\n",
      "ep 3662: ep_len:1460 episode reward: total was 13.390000. running mean: -1.680497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3662: ep_len:2892 episode reward: total was 1.880000. running mean: -1.644892\n",
      "epsilon:0.009992 episode_count: 55062. steps_count: 59428152.000000\n",
      "ep 3663: ep_len:1128 episode reward: total was -9.090000. running mean: -1.719343\n",
      "ep 3663: ep_len:690 episode reward: total was -19.530000. running mean: -1.897450\n",
      "ep 3663: ep_len:2949 episode reward: total was -5.560000. running mean: -1.934075\n",
      "ep 3663: ep_len:538 episode reward: total was 12.200000. running mean: -1.792735\n",
      "ep 3663: ep_len:51 episode reward: total was 24.000000. running mean: -1.534807\n",
      "ep 3663: ep_len:114 episode reward: total was 54.000000. running mean: -0.979459\n",
      "ep 3663: ep_len:500 episode reward: total was 15.830000. running mean: -0.811365\n",
      "ep 3663: ep_len:3622 episode reward: total was -45.380000. running mean: -1.257051\n",
      "ep 3663: ep_len:1267 episode reward: total was -49.140000. running mean: -1.735880\n",
      "ep 3663: ep_len:746 episode reward: total was 21.110000. running mean: -1.507422\n",
      "ep 3663: ep_len:847 episode reward: total was 11.790000. running mean: -1.374447\n",
      "ep 3663: ep_len:107 episode reward: total was 49.000000. running mean: -0.870703\n",
      "ep 3663: ep_len:500 episode reward: total was 15.800000. running mean: -0.703996\n",
      "ep 3663: ep_len:2880 episode reward: total was -438.390000. running mean: -5.080856\n",
      "epsilon:0.009992 episode_count: 55076. steps_count: 59444091.000000\n",
      "ep 3664: ep_len:1108 episode reward: total was 0.810000. running mean: -5.021947\n",
      "ep 3664: ep_len:752 episode reward: total was -1.350000. running mean: -4.985228\n",
      "ep 3664: ep_len:68 episode reward: total was 31.000000. running mean: -4.625376\n",
      "ep 3664: ep_len:3076 episode reward: total was -9.010000. running mean: -4.669222\n",
      "ep 3664: ep_len:697 episode reward: total was -5.730000. running mean: -4.679830\n",
      "ep 3664: ep_len:500 episode reward: total was 21.720000. running mean: -4.415831\n",
      "ep 3664: ep_len:3618 episode reward: total was -7.040000. running mean: -4.442073\n",
      "ep 3664: ep_len:500 episode reward: total was -45.390000. running mean: -4.851552\n",
      "ep 3664: ep_len:818 episode reward: total was 19.510000. running mean: -4.607937\n",
      "ep 3664: ep_len:614 episode reward: total was -1.100000. running mean: -4.572857\n",
      "ep 3664: ep_len:143 episode reward: total was 68.500000. running mean: -3.842129\n",
      "ep 3664: ep_len:116 episode reward: total was 55.000000. running mean: -3.253708\n",
      "ep 3664: ep_len:1505 episode reward: total was 5.240000. running mean: -3.168770\n",
      "ep 3664: ep_len:2850 episode reward: total was 5.320000. running mean: -3.083883\n",
      "ep 3664: ep_len:61 episode reward: total was 29.000000. running mean: -2.763044\n",
      "epsilon:0.009992 episode_count: 55091. steps_count: 59460517.000000\n",
      "ep 3665: ep_len:639 episode reward: total was -1.120000. running mean: -2.746613\n",
      "ep 3665: ep_len:1144 episode reward: total was -0.760000. running mean: -2.726747\n",
      "ep 3665: ep_len:61 episode reward: total was 29.000000. running mean: -2.409480\n",
      "ep 3665: ep_len:3025 episode reward: total was -21.750000. running mean: -2.602885\n",
      "ep 3665: ep_len:803 episode reward: total was 21.690000. running mean: -2.359956\n",
      "ep 3665: ep_len:46 episode reward: total was 20.000000. running mean: -2.136357\n",
      "ep 3665: ep_len:3461 episode reward: total was -524.530000. running mean: -7.360293\n",
      "ep 3665: ep_len:340 episode reward: total was 13.070000. running mean: -7.155990\n",
      "ep 3665: ep_len:851 episode reward: total was 18.320000. running mean: -6.901230\n",
      "ep 3665: ep_len:745 episode reward: total was -0.860000. running mean: -6.840818\n",
      "ep 3665: ep_len:1165 episode reward: total was -9.730000. running mean: -6.869710\n",
      "ep 3665: ep_len:207 episode reward: total was 100.500000. running mean: -5.796013\n",
      "ep 3665: ep_len:61 episode reward: total was 26.000000. running mean: -5.478053\n",
      "ep 3665: ep_len:94 episode reward: total was 45.500000. running mean: -4.968272\n",
      "ep 3665: ep_len:1455 episode reward: total was 22.550000. running mean: -4.693089\n",
      "ep 3665: ep_len:2750 episode reward: total was -4.680000. running mean: -4.692958\n",
      "epsilon:0.009992 episode_count: 55107. steps_count: 59477364.000000\n",
      "ep 3666: ep_len:920 episode reward: total was -43.950000. running mean: -5.085529\n",
      "ep 3666: ep_len:786 episode reward: total was -15.320000. running mean: -5.187874\n",
      "ep 3666: ep_len:52 episode reward: total was 21.500000. running mean: -4.920995\n",
      "ep 3666: ep_len:3009 episode reward: total was 19.080000. running mean: -4.680985\n",
      "ep 3666: ep_len:566 episode reward: total was -10.060000. running mean: -4.734775\n",
      "ep 3666: ep_len:37 episode reward: total was 17.000000. running mean: -4.517427\n",
      "ep 3666: ep_len:100 episode reward: total was 48.500000. running mean: -3.987253\n",
      "ep 3666: ep_len:1361 episode reward: total was -41.100000. running mean: -4.358380\n",
      "ep 3666: ep_len:3768 episode reward: total was -14.390000. running mean: -4.458697\n",
      "ep 3666: ep_len:620 episode reward: total was -31.710000. running mean: -4.731210\n",
      "ep 3666: ep_len:701 episode reward: total was 8.130000. running mean: -4.602598\n",
      "ep 3666: ep_len:987 episode reward: total was 18.000000. running mean: -4.376572\n",
      "ep 3666: ep_len:69 episode reward: total was 33.000000. running mean: -4.002806\n",
      "ep 3666: ep_len:58 episode reward: total was 27.500000. running mean: -3.687778\n",
      "ep 3666: ep_len:1144 episode reward: total was -10.210000. running mean: -3.753000\n",
      "ep 3666: ep_len:2875 episode reward: total was 10.740000. running mean: -3.608070\n",
      "epsilon:0.009992 episode_count: 55123. steps_count: 59494417.000000\n",
      "ep 3667: ep_len:1407 episode reward: total was 27.000000. running mean: -3.301989\n",
      "ep 3667: ep_len:949 episode reward: total was -0.080000. running mean: -3.269769\n",
      "ep 3667: ep_len:60 episode reward: total was 28.500000. running mean: -2.952072\n",
      "ep 3667: ep_len:2935 episode reward: total was -66.890000. running mean: -3.591451\n",
      "ep 3667: ep_len:1183 episode reward: total was -13.090000. running mean: -3.686437\n",
      "ep 3667: ep_len:158 episode reward: total was 73.000000. running mean: -2.919572\n",
      "ep 3667: ep_len:33 episode reward: total was 15.000000. running mean: -2.740376\n",
      "ep 3667: ep_len:1672 episode reward: total was -428.850000. running mean: -7.001473\n",
      "ep 3667: ep_len:349 episode reward: total was 24.180000. running mean: -6.689658\n",
      "ep 3667: ep_len:1285 episode reward: total was -48.610000. running mean: -7.108861\n",
      "ep 3667: ep_len:728 episode reward: total was 11.740000. running mean: -6.920373\n",
      "ep 3667: ep_len:926 episode reward: total was -13.160000. running mean: -6.982769\n",
      "ep 3667: ep_len:65 episode reward: total was 29.500000. running mean: -6.617941\n",
      "ep 3667: ep_len:204 episode reward: total was 99.000000. running mean: -5.561762\n",
      "ep 3667: ep_len:94 episode reward: total was 45.500000. running mean: -5.051144\n",
      "ep 3667: ep_len:641 episode reward: total was -15.400000. running mean: -5.154633\n",
      "ep 3667: ep_len:2809 episode reward: total was -18.540000. running mean: -5.288487\n",
      "epsilon:0.009992 episode_count: 55140. steps_count: 59509915.000000\n",
      "ep 3668: ep_len:651 episode reward: total was -5.690000. running mean: -5.292502\n",
      "ep 3668: ep_len:653 episode reward: total was -9.800000. running mean: -5.337577\n",
      "ep 3668: ep_len:2991 episode reward: total was 20.030000. running mean: -5.083901\n",
      "ep 3668: ep_len:700 episode reward: total was -20.050000. running mean: -5.233562\n",
      "ep 3668: ep_len:100 episode reward: total was 48.500000. running mean: -4.696226\n",
      "ep 3668: ep_len:70 episode reward: total was 33.500000. running mean: -4.314264\n",
      "ep 3668: ep_len:617 episode reward: total was 1.960000. running mean: -4.251521\n",
      "ep 3668: ep_len:3632 episode reward: total was 6.070000. running mean: -4.148306\n",
      "ep 3668: ep_len:1288 episode reward: total was -38.800000. running mean: -4.494823\n",
      "ep 3668: ep_len:746 episode reward: total was -7.430000. running mean: -4.524175\n",
      "ep 3668: ep_len:580 episode reward: total was -2.450000. running mean: -4.503433\n",
      "ep 3668: ep_len:21 episode reward: total was 9.000000. running mean: -4.368399\n",
      "ep 3668: ep_len:64 episode reward: total was 30.500000. running mean: -4.019715\n",
      "ep 3668: ep_len:700 episode reward: total was -7.310000. running mean: -4.052618\n",
      "ep 3668: ep_len:2863 episode reward: total was -0.000000. running mean: -4.012091\n",
      "epsilon:0.009992 episode_count: 55155. steps_count: 59525591.000000\n",
      "ep 3669: ep_len:500 episode reward: total was 16.440000. running mean: -3.807571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3669: ep_len:732 episode reward: total was -0.150000. running mean: -3.770995\n",
      "ep 3669: ep_len:54 episode reward: total was 25.500000. running mean: -3.478285\n",
      "ep 3669: ep_len:2911 episode reward: total was -11.390000. running mean: -3.557402\n",
      "ep 3669: ep_len:728 episode reward: total was -9.050000. running mean: -3.612328\n",
      "ep 3669: ep_len:59 episode reward: total was 28.000000. running mean: -3.296205\n",
      "ep 3669: ep_len:74 episode reward: total was 35.500000. running mean: -2.908243\n",
      "ep 3669: ep_len:1387 episode reward: total was -26.110000. running mean: -3.140260\n",
      "ep 3669: ep_len:3725 episode reward: total was -2.830000. running mean: -3.137158\n",
      "ep 3669: ep_len:739 episode reward: total was -25.690000. running mean: -3.362686\n",
      "ep 3669: ep_len:603 episode reward: total was 2.370000. running mean: -3.305359\n",
      "ep 3669: ep_len:500 episode reward: total was 12.310000. running mean: -3.149206\n",
      "ep 3669: ep_len:173 episode reward: total was 80.500000. running mean: -2.312714\n",
      "ep 3669: ep_len:500 episode reward: total was 32.030000. running mean: -1.969286\n",
      "ep 3669: ep_len:2762 episode reward: total was -1.680000. running mean: -1.966394\n",
      "ep 3669: ep_len:42 episode reward: total was 19.500000. running mean: -1.751730\n",
      "epsilon:0.009992 episode_count: 55171. steps_count: 59541080.000000\n",
      "ep 3670: ep_len:937 episode reward: total was -86.140000. running mean: -2.595612\n",
      "ep 3670: ep_len:500 episode reward: total was -0.300000. running mean: -2.572656\n",
      "ep 3670: ep_len:3011 episode reward: total was -2.550000. running mean: -2.572430\n",
      "ep 3670: ep_len:1179 episode reward: total was -16.660000. running mean: -2.713305\n",
      "ep 3670: ep_len:50 episode reward: total was 16.000000. running mean: -2.526172\n",
      "ep 3670: ep_len:1059 episode reward: total was -53.210000. running mean: -3.033011\n",
      "ep 3670: ep_len:646 episode reward: total was 23.070000. running mean: -2.771980\n",
      "ep 3670: ep_len:914 episode reward: total was -23.620000. running mean: -2.980461\n",
      "ep 3670: ep_len:725 episode reward: total was 27.000000. running mean: -2.680656\n",
      "ep 3670: ep_len:604 episode reward: total was 19.210000. running mean: -2.461750\n",
      "ep 3670: ep_len:43 episode reward: total was 20.000000. running mean: -2.237132\n",
      "ep 3670: ep_len:603 episode reward: total was 1.350000. running mean: -2.201261\n",
      "ep 3670: ep_len:2807 episode reward: total was -53.140000. running mean: -2.710648\n",
      "ep 3670: ep_len:52 episode reward: total was 23.000000. running mean: -2.453542\n",
      "epsilon:0.009992 episode_count: 55185. steps_count: 59554210.000000\n",
      "ep 3671: ep_len:1147 episode reward: total was 10.930000. running mean: -2.319706\n",
      "ep 3671: ep_len:1200 episode reward: total was -21.500000. running mean: -2.511509\n",
      "ep 3671: ep_len:2857 episode reward: total was -20.750000. running mean: -2.693894\n",
      "ep 3671: ep_len:793 episode reward: total was -16.480000. running mean: -2.831755\n",
      "ep 3671: ep_len:40 episode reward: total was 18.500000. running mean: -2.618438\n",
      "ep 3671: ep_len:149 episode reward: total was 70.000000. running mean: -1.892253\n",
      "ep 3671: ep_len:1408 episode reward: total was -35.300000. running mean: -2.226331\n",
      "ep 3671: ep_len:3530 episode reward: total was -195.260000. running mean: -4.156667\n",
      "ep 3671: ep_len:895 episode reward: total was -29.600000. running mean: -4.411101\n",
      "ep 3671: ep_len:691 episode reward: total was 17.310000. running mean: -4.193890\n",
      "ep 3671: ep_len:1497 episode reward: total was 1.820000. running mean: -4.133751\n",
      "ep 3671: ep_len:87 episode reward: total was 42.000000. running mean: -3.672413\n",
      "ep 3671: ep_len:1209 episode reward: total was 20.720000. running mean: -3.428489\n",
      "ep 3671: ep_len:2909 episode reward: total was 15.610000. running mean: -3.238104\n",
      "epsilon:0.009992 episode_count: 55199. steps_count: 59572622.000000\n",
      "ep 3672: ep_len:1121 episode reward: total was 13.400000. running mean: -3.071723\n",
      "ep 3672: ep_len:690 episode reward: total was -15.490000. running mean: -3.195906\n",
      "ep 3672: ep_len:2960 episode reward: total was 6.360000. running mean: -3.100347\n",
      "ep 3672: ep_len:500 episode reward: total was 8.330000. running mean: -2.986043\n",
      "ep 3672: ep_len:111 episode reward: total was 54.000000. running mean: -2.416183\n",
      "ep 3672: ep_len:66 episode reward: total was 31.500000. running mean: -2.077021\n",
      "ep 3672: ep_len:957 episode reward: total was -43.880000. running mean: -2.495051\n",
      "ep 3672: ep_len:658 episode reward: total was 29.370000. running mean: -2.176400\n",
      "ep 3672: ep_len:1187 episode reward: total was -60.070000. running mean: -2.755336\n",
      "ep 3672: ep_len:833 episode reward: total was 64.910000. running mean: -2.078683\n",
      "ep 3672: ep_len:658 episode reward: total was 13.930000. running mean: -1.918596\n",
      "ep 3672: ep_len:69 episode reward: total was 31.500000. running mean: -1.584410\n",
      "ep 3672: ep_len:646 episode reward: total was 16.760000. running mean: -1.400966\n",
      "ep 3672: ep_len:2892 episode reward: total was -174.640000. running mean: -3.133357\n",
      "ep 3672: ep_len:69 episode reward: total was 31.500000. running mean: -2.787023\n",
      "epsilon:0.009992 episode_count: 55214. steps_count: 59586039.000000\n",
      "ep 3673: ep_len:1465 episode reward: total was 27.520000. running mean: -2.483953\n",
      "ep 3673: ep_len:816 episode reward: total was 12.290000. running mean: -2.336213\n",
      "ep 3673: ep_len:38 episode reward: total was 16.000000. running mean: -2.152851\n",
      "ep 3673: ep_len:2959 episode reward: total was -9.970000. running mean: -2.231023\n",
      "ep 3673: ep_len:743 episode reward: total was -6.880000. running mean: -2.277512\n",
      "ep 3673: ep_len:1431 episode reward: total was -13.130000. running mean: -2.386037\n",
      "ep 3673: ep_len:3678 episode reward: total was 3.660000. running mean: -2.325577\n",
      "ep 3673: ep_len:547 episode reward: total was -28.030000. running mean: -2.582621\n",
      "ep 3673: ep_len:7402 episode reward: total was -139.970000. running mean: -3.956495\n",
      "ep 3673: ep_len:688 episode reward: total was -8.930000. running mean: -4.006230\n",
      "ep 3673: ep_len:67 episode reward: total was 32.000000. running mean: -3.646168\n",
      "ep 3673: ep_len:50 episode reward: total was 23.500000. running mean: -3.374706\n",
      "ep 3673: ep_len:104 episode reward: total was 46.000000. running mean: -2.880959\n",
      "ep 3673: ep_len:901 episode reward: total was 0.060000. running mean: -2.851549\n",
      "ep 3673: ep_len:2856 episode reward: total was -47.940000. running mean: -3.302434\n",
      "epsilon:0.009992 episode_count: 55229. steps_count: 59609784.000000\n",
      "ep 3674: ep_len:1485 episode reward: total was 18.260000. running mean: -3.086809\n",
      "ep 3674: ep_len:998 episode reward: total was 18.750000. running mean: -2.868441\n",
      "ep 3674: ep_len:46 episode reward: total was 21.500000. running mean: -2.624757\n",
      "ep 3674: ep_len:2957 episode reward: total was -14.060000. running mean: -2.739109\n",
      "ep 3674: ep_len:583 episode reward: total was -4.870000. running mean: -2.760418\n",
      "ep 3674: ep_len:60 episode reward: total was 27.000000. running mean: -2.462814\n",
      "ep 3674: ep_len:500 episode reward: total was 11.370000. running mean: -2.324486\n",
      "ep 3674: ep_len:3584 episode reward: total was -60.910000. running mean: -2.910341\n",
      "ep 3674: ep_len:823 episode reward: total was 21.740000. running mean: -2.663838\n",
      "ep 3674: ep_len:783 episode reward: total was 0.620000. running mean: -2.630999\n",
      "ep 3674: ep_len:619 episode reward: total was 11.870000. running mean: -2.485989\n",
      "ep 3674: ep_len:64 episode reward: total was 30.500000. running mean: -2.156129\n",
      "ep 3674: ep_len:116 episode reward: total was 56.500000. running mean: -1.569568\n",
      "ep 3674: ep_len:1419 episode reward: total was 30.150000. running mean: -1.252372\n",
      "ep 3674: ep_len:2903 episode reward: total was -204.310000. running mean: -3.282949\n",
      "epsilon:0.009992 episode_count: 55244. steps_count: 59626724.000000\n",
      "ep 3675: ep_len:669 episode reward: total was -286.430000. running mean: -6.114419\n",
      "ep 3675: ep_len:752 episode reward: total was 13.240000. running mean: -5.920875\n",
      "ep 3675: ep_len:3010 episode reward: total was 9.500000. running mean: -5.766666\n",
      "ep 3675: ep_len:1484 episode reward: total was 27.740000. running mean: -5.431600\n",
      "ep 3675: ep_len:500 episode reward: total was 10.970000. running mean: -5.267584\n",
      "ep 3675: ep_len:625 episode reward: total was 23.410000. running mean: -4.980808\n",
      "ep 3675: ep_len:942 episode reward: total was -20.880000. running mean: -5.139800\n",
      "ep 3675: ep_len:723 episode reward: total was -6.530000. running mean: -5.153702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3675: ep_len:968 episode reward: total was 16.830000. running mean: -4.933865\n",
      "ep 3675: ep_len:60 episode reward: total was 28.500000. running mean: -4.599526\n",
      "ep 3675: ep_len:205 episode reward: total was 98.000000. running mean: -3.573531\n",
      "ep 3675: ep_len:61 episode reward: total was 27.500000. running mean: -3.262795\n",
      "ep 3675: ep_len:634 episode reward: total was 12.820000. running mean: -3.101968\n",
      "ep 3675: ep_len:2798 episode reward: total was 16.950000. running mean: -2.901448\n",
      "epsilon:0.009992 episode_count: 55258. steps_count: 59640155.000000\n",
      "ep 3676: ep_len:611 episode reward: total was 0.890000. running mean: -2.863533\n",
      "ep 3676: ep_len:1525 episode reward: total was -38.480000. running mean: -3.219698\n",
      "ep 3676: ep_len:3063 episode reward: total was -16.730000. running mean: -3.354801\n",
      "ep 3676: ep_len:896 episode reward: total was 26.450000. running mean: -3.056753\n",
      "ep 3676: ep_len:38 episode reward: total was 17.500000. running mean: -2.851186\n",
      "ep 3676: ep_len:87 episode reward: total was 40.500000. running mean: -2.417674\n",
      "ep 3676: ep_len:712 episode reward: total was 3.920000. running mean: -2.354297\n",
      "ep 3676: ep_len:303 episode reward: total was 16.680000. running mean: -2.163954\n",
      "ep 3676: ep_len:624 episode reward: total was -23.190000. running mean: -2.374214\n",
      "ep 3676: ep_len:619 episode reward: total was -8.150000. running mean: -2.431972\n",
      "ep 3676: ep_len:906 episode reward: total was 15.630000. running mean: -2.251353\n",
      "ep 3676: ep_len:792 episode reward: total was 3.010000. running mean: -2.198739\n",
      "ep 3676: ep_len:2785 episode reward: total was -11.590000. running mean: -2.292652\n",
      "epsilon:0.009992 episode_count: 55271. steps_count: 59653116.000000\n",
      "ep 3677: ep_len:3679 episode reward: total was -613.050000. running mean: -8.400225\n",
      "ep 3677: ep_len:1617 episode reward: total was -23.640000. running mean: -8.552623\n",
      "ep 3677: ep_len:2884 episode reward: total was -89.380000. running mean: -9.360897\n",
      "ep 3677: ep_len:713 episode reward: total was -128.350000. running mean: -10.550788\n",
      "ep 3677: ep_len:1371 episode reward: total was -844.930000. running mean: -18.894580\n",
      "ep 3677: ep_len:3851 episode reward: total was -81.660000. running mean: -19.522234\n",
      "ep 3677: ep_len:1141 episode reward: total was -1.700000. running mean: -19.344012\n",
      "ep 3677: ep_len:871 episode reward: total was 55.800000. running mean: -18.592572\n",
      "ep 3677: ep_len:1501 episode reward: total was -23.270000. running mean: -18.639346\n",
      "ep 3677: ep_len:100 episode reward: total was 48.500000. running mean: -17.967952\n",
      "ep 3677: ep_len:1053 episode reward: total was 17.440000. running mean: -17.613873\n",
      "ep 3677: ep_len:2822 episode reward: total was -3.990000. running mean: -17.477634\n",
      "epsilon:0.009992 episode_count: 55283. steps_count: 59674719.000000\n",
      "ep 3678: ep_len:914 episode reward: total was -93.040000. running mean: -18.233258\n",
      "ep 3678: ep_len:949 episode reward: total was -92.170000. running mean: -18.972625\n",
      "ep 3678: ep_len:55 episode reward: total was 24.500000. running mean: -18.537899\n",
      "ep 3678: ep_len:2970 episode reward: total was 8.770000. running mean: -18.264820\n",
      "ep 3678: ep_len:1713 episode reward: total was -29.350000. running mean: -18.375672\n",
      "ep 3678: ep_len:142 episode reward: total was 66.500000. running mean: -17.526915\n",
      "ep 3678: ep_len:79 episode reward: total was 35.000000. running mean: -17.001646\n",
      "ep 3678: ep_len:837 episode reward: total was 57.400000. running mean: -16.257629\n",
      "ep 3678: ep_len:348 episode reward: total was 12.570000. running mean: -15.969353\n",
      "ep 3678: ep_len:585 episode reward: total was 22.130000. running mean: -15.588360\n",
      "ep 3678: ep_len:859 episode reward: total was 64.190000. running mean: -14.790576\n",
      "ep 3678: ep_len:728 episode reward: total was 27.880000. running mean: -14.363870\n",
      "ep 3678: ep_len:88 episode reward: total was 38.000000. running mean: -13.840232\n",
      "ep 3678: ep_len:195 episode reward: total was 94.500000. running mean: -12.756829\n",
      "ep 3678: ep_len:53 episode reward: total was 25.000000. running mean: -12.379261\n",
      "ep 3678: ep_len:107 episode reward: total was 50.500000. running mean: -11.750468\n",
      "ep 3678: ep_len:1464 episode reward: total was 18.660000. running mean: -11.446364\n",
      "ep 3678: ep_len:2885 episode reward: total was -36.940000. running mean: -11.701300\n",
      "epsilon:0.009992 episode_count: 55301. steps_count: 59689690.000000\n",
      "ep 3679: ep_len:875 episode reward: total was -69.560000. running mean: -12.279887\n",
      "ep 3679: ep_len:969 episode reward: total was 42.220000. running mean: -11.734888\n",
      "ep 3679: ep_len:63 episode reward: total was 30.000000. running mean: -11.317539\n",
      "ep 3679: ep_len:2965 episode reward: total was -56.550000. running mean: -11.769864\n",
      "ep 3679: ep_len:847 episode reward: total was -24.090000. running mean: -11.893065\n",
      "ep 3679: ep_len:38 episode reward: total was 17.500000. running mean: -11.599135\n",
      "ep 3679: ep_len:171 episode reward: total was 82.500000. running mean: -10.658143\n",
      "ep 3679: ep_len:1350 episode reward: total was -170.490000. running mean: -12.256462\n",
      "ep 3679: ep_len:336 episode reward: total was 17.990000. running mean: -11.953997\n",
      "ep 3679: ep_len:770 episode reward: total was -31.510000. running mean: -12.149557\n",
      "ep 3679: ep_len:736 episode reward: total was 39.080000. running mean: -11.637262\n",
      "ep 3679: ep_len:1181 episode reward: total was -2.990000. running mean: -11.550789\n",
      "ep 3679: ep_len:68 episode reward: total was 31.000000. running mean: -11.125281\n",
      "ep 3679: ep_len:500 episode reward: total was 32.210000. running mean: -10.691928\n",
      "ep 3679: ep_len:2915 episode reward: total was -14.750000. running mean: -10.732509\n",
      "epsilon:0.009992 episode_count: 55316. steps_count: 59703474.000000\n",
      "ep 3680: ep_len:744 episode reward: total was -64.930000. running mean: -11.274484\n",
      "ep 3680: ep_len:692 episode reward: total was -20.790000. running mean: -11.369639\n",
      "ep 3680: ep_len:2837 episode reward: total was -24.350000. running mean: -11.499443\n",
      "ep 3680: ep_len:500 episode reward: total was 12.290000. running mean: -11.261548\n",
      "ep 3680: ep_len:63 episode reward: total was 30.000000. running mean: -10.848933\n",
      "ep 3680: ep_len:1148 episode reward: total was 16.190000. running mean: -10.578543\n",
      "ep 3680: ep_len:3692 episode reward: total was -46.940000. running mean: -10.942158\n",
      "ep 3680: ep_len:870 episode reward: total was -16.200000. running mean: -10.994736\n",
      "ep 3680: ep_len:780 episode reward: total was 9.010000. running mean: -10.794689\n",
      "ep 3680: ep_len:1017 episode reward: total was 0.090000. running mean: -10.685842\n",
      "ep 3680: ep_len:217 episode reward: total was 107.000000. running mean: -9.508984\n",
      "ep 3680: ep_len:52 episode reward: total was 24.500000. running mean: -9.168894\n",
      "ep 3680: ep_len:1084 episode reward: total was -42.950000. running mean: -9.506705\n",
      "ep 3680: ep_len:2862 episode reward: total was 6.450000. running mean: -9.347138\n",
      "epsilon:0.009992 episode_count: 55330. steps_count: 59720032.000000\n",
      "ep 3681: ep_len:1380 episode reward: total was 35.850000. running mean: -8.895167\n",
      "ep 3681: ep_len:500 episode reward: total was 11.180000. running mean: -8.694415\n",
      "ep 3681: ep_len:39 episode reward: total was 16.500000. running mean: -8.442471\n",
      "ep 3681: ep_len:2829 episode reward: total was -5.040000. running mean: -8.408446\n",
      "ep 3681: ep_len:526 episode reward: total was -18.140000. running mean: -8.505762\n",
      "ep 3681: ep_len:52 episode reward: total was 24.500000. running mean: -8.175704\n",
      "ep 3681: ep_len:984 episode reward: total was -15.240000. running mean: -8.246347\n",
      "ep 3681: ep_len:635 episode reward: total was 14.390000. running mean: -8.019983\n",
      "ep 3681: ep_len:616 episode reward: total was 26.420000. running mean: -7.675584\n",
      "ep 3681: ep_len:714 episode reward: total was 12.730000. running mean: -7.471528\n",
      "ep 3681: ep_len:599 episode reward: total was -18.420000. running mean: -7.581013\n",
      "ep 3681: ep_len:982 episode reward: total was -29.740000. running mean: -7.802602\n",
      "ep 3681: ep_len:2930 episode reward: total was -22.990000. running mean: -7.954476\n",
      "epsilon:0.009992 episode_count: 55343. steps_count: 59732818.000000\n",
      "ep 3682: ep_len:637 episode reward: total was -4.880000. running mean: -7.923732\n",
      "ep 3682: ep_len:200 episode reward: total was 0.770000. running mean: -7.836794\n",
      "ep 3682: ep_len:62 episode reward: total was 29.500000. running mean: -7.463426\n",
      "ep 3682: ep_len:3071 episode reward: total was -12.380000. running mean: -7.512592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3682: ep_len:819 episode reward: total was -3.250000. running mean: -7.469966\n",
      "ep 3682: ep_len:100 episode reward: total was 45.500000. running mean: -6.940267\n",
      "ep 3682: ep_len:859 episode reward: total was 10.290000. running mean: -6.767964\n",
      "ep 3682: ep_len:3649 episode reward: total was -62.280000. running mean: -7.323084\n",
      "ep 3682: ep_len:1218 episode reward: total was -28.390000. running mean: -7.533753\n",
      "ep 3682: ep_len:826 episode reward: total was 39.390000. running mean: -7.064516\n",
      "ep 3682: ep_len:595 episode reward: total was -6.340000. running mean: -7.057271\n",
      "ep 3682: ep_len:25 episode reward: total was 11.000000. running mean: -6.876698\n",
      "ep 3682: ep_len:1117 episode reward: total was -1.120000. running mean: -6.819131\n",
      "ep 3682: ep_len:2820 episode reward: total was -21.460000. running mean: -6.965540\n",
      "epsilon:0.009992 episode_count: 55357. steps_count: 59748816.000000\n",
      "ep 3683: ep_len:601 episode reward: total was -5.920000. running mean: -6.955084\n",
      "ep 3683: ep_len:670 episode reward: total was -8.020000. running mean: -6.965733\n",
      "ep 3683: ep_len:50 episode reward: total was 23.500000. running mean: -6.661076\n",
      "ep 3683: ep_len:3071 episode reward: total was -10.510000. running mean: -6.699565\n",
      "ep 3683: ep_len:556 episode reward: total was 16.090000. running mean: -6.471670\n",
      "ep 3683: ep_len:139 episode reward: total was 66.500000. running mean: -5.741953\n",
      "ep 3683: ep_len:49 episode reward: total was 23.000000. running mean: -5.454533\n",
      "ep 3683: ep_len:822 episode reward: total was 26.780000. running mean: -5.132188\n",
      "ep 3683: ep_len:630 episode reward: total was 30.470000. running mean: -4.776166\n",
      "ep 3683: ep_len:832 episode reward: total was -19.040000. running mean: -4.918805\n",
      "ep 3683: ep_len:620 episode reward: total was -2.940000. running mean: -4.899017\n",
      "ep 3683: ep_len:521 episode reward: total was 20.880000. running mean: -4.641226\n",
      "ep 3683: ep_len:88 episode reward: total was 42.500000. running mean: -4.169814\n",
      "ep 3683: ep_len:195 episode reward: total was 91.500000. running mean: -3.213116\n",
      "ep 3683: ep_len:51 episode reward: total was 24.000000. running mean: -2.940985\n",
      "ep 3683: ep_len:92 episode reward: total was 43.000000. running mean: -2.481575\n",
      "ep 3683: ep_len:1156 episode reward: total was -8.810000. running mean: -2.544859\n",
      "ep 3683: ep_len:2784 episode reward: total was 4.780000. running mean: -2.471611\n",
      "epsilon:0.009992 episode_count: 55375. steps_count: 59761743.000000\n",
      "ep 3684: ep_len:684 episode reward: total was -27.670000. running mean: -2.723595\n",
      "ep 3684: ep_len:1675 episode reward: total was -14.240000. running mean: -2.838759\n",
      "ep 3684: ep_len:75 episode reward: total was 36.000000. running mean: -2.450371\n",
      "ep 3684: ep_len:2989 episode reward: total was -36.280000. running mean: -2.788667\n",
      "ep 3684: ep_len:636 episode reward: total was 0.980000. running mean: -2.750981\n",
      "ep 3684: ep_len:84 episode reward: total was 40.500000. running mean: -2.318471\n",
      "ep 3684: ep_len:74 episode reward: total was 35.500000. running mean: -1.940286\n",
      "ep 3684: ep_len:1454 episode reward: total was -130.110000. running mean: -3.221983\n",
      "ep 3684: ep_len:500 episode reward: total was 21.230000. running mean: -2.977463\n",
      "ep 3684: ep_len:1102 episode reward: total was -20.460000. running mean: -3.152289\n",
      "ep 3684: ep_len:810 episode reward: total was 16.210000. running mean: -2.958666\n",
      "ep 3684: ep_len:1030 episode reward: total was 14.270000. running mean: -2.786379\n",
      "ep 3684: ep_len:83 episode reward: total was 40.000000. running mean: -2.358515\n",
      "ep 3684: ep_len:153 episode reward: total was 72.000000. running mean: -1.614930\n",
      "ep 3684: ep_len:1398 episode reward: total was 20.510000. running mean: -1.393681\n",
      "ep 3684: ep_len:31 episode reward: total was 14.000000. running mean: -1.239744\n",
      "ep 3684: ep_len:39 episode reward: total was 18.000000. running mean: -1.047347\n",
      "epsilon:0.009992 episode_count: 55392. steps_count: 59774560.000000\n",
      "ep 3685: ep_len:1161 episode reward: total was 13.320000. running mean: -0.903673\n",
      "ep 3685: ep_len:500 episode reward: total was 34.170000. running mean: -0.552937\n",
      "ep 3685: ep_len:2996 episode reward: total was -77.480000. running mean: -1.322207\n",
      "ep 3685: ep_len:783 episode reward: total was -33.690000. running mean: -1.645885\n",
      "ep 3685: ep_len:95 episode reward: total was 44.500000. running mean: -1.184426\n",
      "ep 3685: ep_len:57 episode reward: total was 24.000000. running mean: -0.932582\n",
      "ep 3685: ep_len:500 episode reward: total was 17.910000. running mean: -0.744156\n",
      "ep 3685: ep_len:3840 episode reward: total was -29.990000. running mean: -1.036615\n",
      "ep 3685: ep_len:601 episode reward: total was -36.640000. running mean: -1.392648\n",
      "ep 3685: ep_len:755 episode reward: total was 4.170000. running mean: -1.337022\n",
      "ep 3685: ep_len:500 episode reward: total was 38.520000. running mean: -0.938452\n",
      "ep 3685: ep_len:1055 episode reward: total was -37.870000. running mean: -1.307767\n",
      "ep 3685: ep_len:2804 episode reward: total was -6.250000. running mean: -1.357190\n",
      "ep 3685: ep_len:51 episode reward: total was 24.000000. running mean: -1.103618\n",
      "epsilon:0.009992 episode_count: 55406. steps_count: 59790258.000000\n",
      "ep 3686: ep_len:1431 episode reward: total was 29.260000. running mean: -0.799981\n",
      "ep 3686: ep_len:500 episode reward: total was 12.580000. running mean: -0.666182\n",
      "ep 3686: ep_len:2926 episode reward: total was -62.450000. running mean: -1.284020\n",
      "ep 3686: ep_len:643 episode reward: total was 2.220000. running mean: -1.248980\n",
      "ep 3686: ep_len:81 episode reward: total was 37.500000. running mean: -0.861490\n",
      "ep 3686: ep_len:109 episode reward: total was 50.000000. running mean: -0.352875\n",
      "ep 3686: ep_len:984 episode reward: total was -22.590000. running mean: -0.575246\n",
      "ep 3686: ep_len:324 episode reward: total was 16.550000. running mean: -0.403994\n",
      "ep 3686: ep_len:1249 episode reward: total was -52.320000. running mean: -0.923154\n",
      "ep 3686: ep_len:717 episode reward: total was 27.800000. running mean: -0.635922\n",
      "ep 3686: ep_len:633 episode reward: total was 2.120000. running mean: -0.608363\n",
      "ep 3686: ep_len:62 episode reward: total was 29.500000. running mean: -0.307279\n",
      "ep 3686: ep_len:163 episode reward: total was 78.500000. running mean: 0.480793\n",
      "ep 3686: ep_len:119 episode reward: total was 58.000000. running mean: 1.055985\n",
      "ep 3686: ep_len:601 episode reward: total was 19.210000. running mean: 1.237526\n",
      "ep 3686: ep_len:2850 episode reward: total was -5.550000. running mean: 1.169650\n",
      "epsilon:0.009992 episode_count: 55422. steps_count: 59803650.000000\n",
      "ep 3687: ep_len:820 episode reward: total was -24.740000. running mean: 0.910554\n",
      "ep 3687: ep_len:749 episode reward: total was 6.060000. running mean: 0.962048\n",
      "ep 3687: ep_len:2936 episode reward: total was -28.100000. running mean: 0.671428\n",
      "ep 3687: ep_len:4991 episode reward: total was -1831.990000. running mean: -17.655186\n",
      "ep 3687: ep_len:51 episode reward: total was 24.000000. running mean: -17.238635\n",
      "ep 3687: ep_len:689 episode reward: total was -3.380000. running mean: -17.100048\n",
      "ep 3687: ep_len:3809 episode reward: total was -3.150000. running mean: -16.960548\n",
      "ep 3687: ep_len:635 episode reward: total was 3.150000. running mean: -16.759442\n",
      "ep 3687: ep_len:707 episode reward: total was 25.770000. running mean: -16.334148\n",
      "ep 3687: ep_len:625 episode reward: total was -37.350000. running mean: -16.544306\n",
      "ep 3687: ep_len:187 episode reward: total was 89.000000. running mean: -15.488863\n",
      "ep 3687: ep_len:1515 episode reward: total was 6.450000. running mean: -15.269475\n",
      "ep 3687: ep_len:2833 episode reward: total was -0.550000. running mean: -15.122280\n",
      "epsilon:0.009992 episode_count: 55435. steps_count: 59824197.000000\n",
      "ep 3688: ep_len:1112 episode reward: total was -5.210000. running mean: -15.023157\n",
      "ep 3688: ep_len:1634 episode reward: total was -26.950000. running mean: -15.142426\n",
      "ep 3688: ep_len:3013 episode reward: total was -31.180000. running mean: -15.302801\n",
      "ep 3688: ep_len:1157 episode reward: total was -25.750000. running mean: -15.407273\n",
      "ep 3688: ep_len:39 episode reward: total was 16.500000. running mean: -15.088201\n",
      "ep 3688: ep_len:995 episode reward: total was -47.300000. running mean: -15.410319\n",
      "ep 3688: ep_len:500 episode reward: total was 30.690000. running mean: -14.949315\n",
      "ep 3688: ep_len:1189 episode reward: total was -42.820000. running mean: -15.228022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3688: ep_len:724 episode reward: total was 42.480000. running mean: -14.650942\n",
      "ep 3688: ep_len:575 episode reward: total was 1.540000. running mean: -14.489033\n",
      "ep 3688: ep_len:62 episode reward: total was 29.500000. running mean: -14.049142\n",
      "ep 3688: ep_len:621 episode reward: total was -9.110000. running mean: -13.999751\n",
      "ep 3688: ep_len:2848 episode reward: total was -2.350000. running mean: -13.883253\n",
      "ep 3688: ep_len:63 episode reward: total was 28.500000. running mean: -13.459421\n",
      "epsilon:0.009992 episode_count: 55449. steps_count: 59838729.000000\n",
      "ep 3689: ep_len:703 episode reward: total was -40.550000. running mean: -13.730327\n",
      "ep 3689: ep_len:645 episode reward: total was -31.460000. running mean: -13.907623\n",
      "ep 3689: ep_len:3056 episode reward: total was -34.820000. running mean: -14.116747\n",
      "ep 3689: ep_len:698 episode reward: total was 21.450000. running mean: -13.761080\n",
      "ep 3689: ep_len:65 episode reward: total was 31.000000. running mean: -13.313469\n",
      "ep 3689: ep_len:87 episode reward: total was 40.500000. running mean: -12.775334\n",
      "ep 3689: ep_len:1054 episode reward: total was -1.930000. running mean: -12.666881\n",
      "ep 3689: ep_len:632 episode reward: total was 4.740000. running mean: -12.492812\n",
      "ep 3689: ep_len:804 episode reward: total was -22.380000. running mean: -12.591684\n",
      "ep 3689: ep_len:762 episode reward: total was 20.990000. running mean: -12.255867\n",
      "ep 3689: ep_len:1505 episode reward: total was -3.180000. running mean: -12.165108\n",
      "ep 3689: ep_len:75 episode reward: total was 36.000000. running mean: -11.683457\n",
      "ep 3689: ep_len:52 episode reward: total was 24.500000. running mean: -11.321623\n",
      "ep 3689: ep_len:690 episode reward: total was -8.420000. running mean: -11.292606\n",
      "ep 3689: ep_len:2764 episode reward: total was -49.140000. running mean: -11.671080\n",
      "epsilon:0.009992 episode_count: 55464. steps_count: 59852321.000000\n",
      "ep 3690: ep_len:860 episode reward: total was 29.520000. running mean: -11.259170\n",
      "ep 3690: ep_len:767 episode reward: total was -42.800000. running mean: -11.574578\n",
      "ep 3690: ep_len:45 episode reward: total was 21.000000. running mean: -11.248832\n",
      "ep 3690: ep_len:2943 episode reward: total was -54.200000. running mean: -11.678344\n",
      "ep 3690: ep_len:1221 episode reward: total was -31.080000. running mean: -11.872360\n",
      "ep 3690: ep_len:627 episode reward: total was -1.980000. running mean: -11.773437\n",
      "ep 3690: ep_len:309 episode reward: total was 21.270000. running mean: -11.443002\n",
      "ep 3690: ep_len:584 episode reward: total was 5.950000. running mean: -11.269072\n",
      "ep 3690: ep_len:7313 episode reward: total was -1359.370000. running mean: -24.750082\n",
      "ep 3690: ep_len:566 episode reward: total was -10.670000. running mean: -24.609281\n",
      "ep 3690: ep_len:128 episode reward: total was 59.500000. running mean: -23.768188\n",
      "ep 3690: ep_len:979 episode reward: total was -68.120000. running mean: -24.211706\n",
      "ep 3690: ep_len:2815 episode reward: total was -4.790000. running mean: -24.017489\n",
      "epsilon:0.009992 episode_count: 55477. steps_count: 59871478.000000\n",
      "ep 3691: ep_len:1137 episode reward: total was -9.000000. running mean: -23.867314\n",
      "ep 3691: ep_len:500 episode reward: total was 7.500000. running mean: -23.553641\n",
      "ep 3691: ep_len:64 episode reward: total was 29.000000. running mean: -23.028105\n",
      "ep 3691: ep_len:2895 episode reward: total was -38.400000. running mean: -23.181824\n",
      "ep 3691: ep_len:500 episode reward: total was 5.000000. running mean: -22.900005\n",
      "ep 3691: ep_len:57 episode reward: total was 27.000000. running mean: -22.401005\n",
      "ep 3691: ep_len:1081 episode reward: total was -12.500000. running mean: -22.301995\n",
      "ep 3691: ep_len:315 episode reward: total was 17.780000. running mean: -21.901175\n",
      "ep 3691: ep_len:1187 episode reward: total was -69.500000. running mean: -22.377164\n",
      "ep 3691: ep_len:702 episode reward: total was 17.540000. running mean: -21.977992\n",
      "ep 3691: ep_len:640 episode reward: total was -2.850000. running mean: -21.786712\n",
      "ep 3691: ep_len:112 episode reward: total was 53.000000. running mean: -21.038845\n",
      "ep 3691: ep_len:652 episode reward: total was -5.770000. running mean: -20.886156\n",
      "ep 3691: ep_len:2754 episode reward: total was -14.160000. running mean: -20.818895\n",
      "epsilon:0.009992 episode_count: 55491. steps_count: 59884074.000000\n",
      "ep 3692: ep_len:1146 episode reward: total was 3.520000. running mean: -20.575506\n",
      "ep 3692: ep_len:187 episode reward: total was 13.770000. running mean: -20.232051\n",
      "ep 3692: ep_len:99 episode reward: total was 46.500000. running mean: -19.564730\n",
      "ep 3692: ep_len:834 episode reward: total was 26.510000. running mean: -19.103983\n",
      "ep 3692: ep_len:678 episode reward: total was 23.090000. running mean: -18.682043\n",
      "ep 3692: ep_len:637 episode reward: total was 22.370000. running mean: -18.271523\n",
      "ep 3692: ep_len:651 episode reward: total was -15.720000. running mean: -18.246008\n",
      "ep 3692: ep_len:782 episode reward: total was 32.610000. running mean: -17.737447\n",
      "ep 3692: ep_len:601 episode reward: total was -10.150000. running mean: -17.661573\n",
      "ep 3692: ep_len:68 episode reward: total was 31.000000. running mean: -17.174957\n",
      "ep 3692: ep_len:39 episode reward: total was 18.000000. running mean: -16.823208\n",
      "ep 3692: ep_len:81 episode reward: total was 37.010000. running mean: -16.284876\n",
      "ep 3692: ep_len:1110 episode reward: total was -22.430000. running mean: -16.346327\n",
      "ep 3692: ep_len:2898 episode reward: total was -4.450000. running mean: -16.227364\n",
      "epsilon:0.009992 episode_count: 55505. steps_count: 59893885.000000\n",
      "ep 3693: ep_len:842 episode reward: total was -46.230000. running mean: -16.527390\n",
      "ep 3693: ep_len:806 episode reward: total was -10.770000. running mean: -16.469816\n",
      "ep 3693: ep_len:3046 episode reward: total was -24.110000. running mean: -16.546218\n",
      "ep 3693: ep_len:849 episode reward: total was 34.800000. running mean: -16.032756\n",
      "ep 3693: ep_len:34 episode reward: total was 15.500000. running mean: -15.717428\n",
      "ep 3693: ep_len:111 episode reward: total was 51.000000. running mean: -15.050254\n",
      "ep 3693: ep_len:48 episode reward: total was 22.500000. running mean: -14.674751\n",
      "ep 3693: ep_len:1164 episode reward: total was -3.950000. running mean: -14.567504\n",
      "ep 3693: ep_len:3880 episode reward: total was -1719.630000. running mean: -31.618129\n",
      "ep 3693: ep_len:704 episode reward: total was -11.540000. running mean: -31.417347\n",
      "ep 3693: ep_len:788 episode reward: total was 7.990000. running mean: -31.023274\n",
      "ep 3693: ep_len:500 episode reward: total was 26.920000. running mean: -30.443841\n",
      "ep 3693: ep_len:67 episode reward: total was 30.500000. running mean: -29.834403\n",
      "ep 3693: ep_len:969 episode reward: total was -8.440000. running mean: -29.620459\n",
      "ep 3693: ep_len:2823 episode reward: total was -8.170000. running mean: -29.405954\n",
      "epsilon:0.009992 episode_count: 55520. steps_count: 59910516.000000\n",
      "ep 3694: ep_len:783 episode reward: total was -12.480000. running mean: -29.236695\n",
      "ep 3694: ep_len:785 episode reward: total was 13.120000. running mean: -28.813128\n",
      "ep 3694: ep_len:2898 episode reward: total was -46.260000. running mean: -28.987596\n",
      "ep 3694: ep_len:500 episode reward: total was -6.920000. running mean: -28.766921\n",
      "ep 3694: ep_len:115 episode reward: total was 54.500000. running mean: -27.934251\n",
      "ep 3694: ep_len:75 episode reward: total was 34.500000. running mean: -27.309909\n",
      "ep 3694: ep_len:770 episode reward: total was -33.390000. running mean: -27.370710\n",
      "ep 3694: ep_len:3705 episode reward: total was -40.910000. running mean: -27.506103\n",
      "ep 3694: ep_len:651 episode reward: total was 4.840000. running mean: -27.182642\n",
      "ep 3694: ep_len:816 episode reward: total was 21.590000. running mean: -26.694915\n",
      "ep 3694: ep_len:500 episode reward: total was -3.940000. running mean: -26.467366\n",
      "ep 3694: ep_len:79 episode reward: total was 36.500000. running mean: -25.837692\n",
      "ep 3694: ep_len:98 episode reward: total was 46.000000. running mean: -25.119315\n",
      "ep 3694: ep_len:1105 episode reward: total was -20.430000. running mean: -25.072422\n",
      "ep 3694: ep_len:2921 episode reward: total was -13.810000. running mean: -24.959798\n",
      "epsilon:0.009992 episode_count: 55535. steps_count: 59926317.000000\n",
      "ep 3695: ep_len:1136 episode reward: total was -6.620000. running mean: -24.776400\n",
      "ep 3695: ep_len:1004 episode reward: total was 15.050000. running mean: -24.378136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3695: ep_len:45 episode reward: total was 19.500000. running mean: -23.939355\n",
      "ep 3695: ep_len:2958 episode reward: total was -20.060000. running mean: -23.900561\n",
      "ep 3695: ep_len:828 episode reward: total was 42.460000. running mean: -23.236956\n",
      "ep 3695: ep_len:98 episode reward: total was 47.500000. running mean: -22.529586\n",
      "ep 3695: ep_len:56 episode reward: total was 26.500000. running mean: -22.039290\n",
      "ep 3695: ep_len:1435 episode reward: total was 9.190000. running mean: -21.726997\n",
      "ep 3695: ep_len:294 episode reward: total was 13.680000. running mean: -21.372927\n",
      "ep 3695: ep_len:2905 episode reward: total was -713.320000. running mean: -28.292398\n",
      "ep 3695: ep_len:7355 episode reward: total was -57.280000. running mean: -28.582274\n",
      "ep 3695: ep_len:677 episode reward: total was -4.510000. running mean: -28.341551\n",
      "ep 3695: ep_len:148 episode reward: total was 69.500000. running mean: -27.363136\n",
      "ep 3695: ep_len:45 episode reward: total was 19.500000. running mean: -26.894504\n",
      "ep 3695: ep_len:1012 episode reward: total was -6.350000. running mean: -26.689059\n",
      "ep 3695: ep_len:2858 episode reward: total was -9.170000. running mean: -26.513869\n",
      "epsilon:0.009992 episode_count: 55551. steps_count: 59949171.000000\n",
      "ep 3696: ep_len:1090 episode reward: total was -5.180000. running mean: -26.300530\n",
      "ep 3696: ep_len:975 episode reward: total was 8.850000. running mean: -25.949025\n",
      "ep 3696: ep_len:61 episode reward: total was 29.000000. running mean: -25.399535\n",
      "ep 3696: ep_len:3030 episode reward: total was -80.480000. running mean: -25.950339\n",
      "ep 3696: ep_len:1533 episode reward: total was -2.500000. running mean: -25.715836\n",
      "ep 3696: ep_len:63 episode reward: total was 30.000000. running mean: -25.158677\n",
      "ep 3696: ep_len:958 episode reward: total was 15.720000. running mean: -24.749891\n",
      "ep 3696: ep_len:3937 episode reward: total was -79.080000. running mean: -25.293192\n",
      "ep 3696: ep_len:1564 episode reward: total was -14.740000. running mean: -25.187660\n",
      "ep 3696: ep_len:7419 episode reward: total was -147.580000. running mean: -26.411583\n",
      "ep 3696: ep_len:500 episode reward: total was 9.890000. running mean: -26.048567\n",
      "ep 3696: ep_len:80 episode reward: total was 37.000000. running mean: -25.418082\n",
      "ep 3696: ep_len:118 episode reward: total was 54.500000. running mean: -24.618901\n",
      "ep 3696: ep_len:974 episode reward: total was -98.260000. running mean: -25.355312\n",
      "ep 3696: ep_len:2790 episode reward: total was -26.160000. running mean: -25.363359\n",
      "epsilon:0.009992 episode_count: 55566. steps_count: 59974263.000000\n",
      "ep 3697: ep_len:664 episode reward: total was 13.230000. running mean: -24.977425\n",
      "ep 3697: ep_len:770 episode reward: total was -47.430000. running mean: -25.201951\n",
      "ep 3697: ep_len:59 episode reward: total was 26.500000. running mean: -24.684931\n",
      "ep 3697: ep_len:2998 episode reward: total was -66.510000. running mean: -25.103182\n",
      "ep 3697: ep_len:658 episode reward: total was -16.400000. running mean: -25.016150\n",
      "ep 3697: ep_len:64 episode reward: total was 30.500000. running mean: -24.460989\n",
      "ep 3697: ep_len:792 episode reward: total was -37.700000. running mean: -24.593379\n",
      "ep 3697: ep_len:3705 episode reward: total was -35.520000. running mean: -24.702645\n",
      "ep 3697: ep_len:1250 episode reward: total was -49.770000. running mean: -24.953319\n",
      "ep 3697: ep_len:861 episode reward: total was 45.110000. running mean: -24.252685\n",
      "ep 3697: ep_len:500 episode reward: total was 10.540000. running mean: -23.904759\n",
      "ep 3697: ep_len:60 episode reward: total was 27.000000. running mean: -23.395711\n",
      "ep 3697: ep_len:49 episode reward: total was 21.500000. running mean: -22.946754\n",
      "ep 3697: ep_len:1136 episode reward: total was -26.180000. running mean: -22.979086\n",
      "ep 3697: ep_len:2801 episode reward: total was -24.560000. running mean: -22.994896\n",
      "epsilon:0.009992 episode_count: 55581. steps_count: 59990630.000000\n",
      "ep 3698: ep_len:1113 episode reward: total was -10.400000. running mean: -22.868947\n",
      "ep 3698: ep_len:754 episode reward: total was -17.940000. running mean: -22.819657\n",
      "ep 3698: ep_len:2960 episode reward: total was -720.850000. running mean: -29.799961\n",
      "ep 3698: ep_len:576 episode reward: total was 14.450000. running mean: -29.357461\n",
      "ep 3698: ep_len:140 episode reward: total was 67.000000. running mean: -28.393886\n",
      "ep 3698: ep_len:100 episode reward: total was 47.000000. running mean: -27.639947\n",
      "ep 3698: ep_len:47 episode reward: total was 22.000000. running mean: -27.143548\n",
      "ep 3698: ep_len:890 episode reward: total was 28.750000. running mean: -26.584613\n",
      "ep 3698: ep_len:3678 episode reward: total was 2.650000. running mean: -26.292266\n",
      "ep 3698: ep_len:766 episode reward: total was -11.930000. running mean: -26.148644\n",
      "ep 3698: ep_len:757 episode reward: total was 2.570000. running mean: -25.861457\n",
      "ep 3698: ep_len:1475 episode reward: total was -1.890000. running mean: -25.621743\n",
      "ep 3698: ep_len:90 episode reward: total was 42.000000. running mean: -24.945525\n",
      "ep 3698: ep_len:69 episode reward: total was 31.500000. running mean: -24.381070\n",
      "ep 3698: ep_len:1062 episode reward: total was 19.640000. running mean: -23.940859\n",
      "ep 3698: ep_len:2867 episode reward: total was 1.510000. running mean: -23.686351\n",
      "ep 3698: ep_len:61 episode reward: total was 29.000000. running mean: -23.159487\n",
      "epsilon:0.009992 episode_count: 55598. steps_count: 60008035.000000\n",
      "ep 3699: ep_len:952 episode reward: total was -110.570000. running mean: -24.033592\n",
      "ep 3699: ep_len:715 episode reward: total was -40.630000. running mean: -24.199556\n",
      "ep 3699: ep_len:43 episode reward: total was 20.000000. running mean: -23.757561\n",
      "ep 3699: ep_len:3068 episode reward: total was -107.250000. running mean: -24.592485\n",
      "ep 3699: ep_len:629 episode reward: total was -1.190000. running mean: -24.358460\n",
      "ep 3699: ep_len:59 episode reward: total was 26.500000. running mean: -23.849876\n",
      "ep 3699: ep_len:998 episode reward: total was -44.480000. running mean: -24.056177\n",
      "ep 3699: ep_len:628 episode reward: total was 20.130000. running mean: -23.614315\n",
      "ep 3699: ep_len:886 episode reward: total was 9.240000. running mean: -23.285772\n",
      "ep 3699: ep_len:830 episode reward: total was 30.220000. running mean: -22.750714\n",
      "ep 3699: ep_len:895 episode reward: total was 21.770000. running mean: -22.305507\n",
      "ep 3699: ep_len:51 episode reward: total was 24.000000. running mean: -21.842452\n",
      "ep 3699: ep_len:1102 episode reward: total was -23.490000. running mean: -21.858928\n",
      "ep 3699: ep_len:2827 episode reward: total was -28.000000. running mean: -21.920338\n",
      "ep 3699: ep_len:56 episode reward: total was 26.500000. running mean: -21.436135\n",
      "epsilon:0.009992 episode_count: 55613. steps_count: 60021774.000000\n",
      "ep 3700: ep_len:1445 episode reward: total was 2.190000. running mean: -21.199874\n",
      "ep 3700: ep_len:715 episode reward: total was -35.410000. running mean: -21.341975\n",
      "ep 3700: ep_len:2905 episode reward: total was -104.840000. running mean: -22.176955\n",
      "ep 3700: ep_len:500 episode reward: total was 13.480000. running mean: -21.820386\n",
      "ep 3700: ep_len:163 episode reward: total was 78.500000. running mean: -20.817182\n",
      "ep 3700: ep_len:79 episode reward: total was 36.500000. running mean: -20.244010\n",
      "ep 3700: ep_len:54 episode reward: total was 25.500000. running mean: -19.786570\n",
      "ep 3700: ep_len:1834 episode reward: total was -160.440000. running mean: -21.193104\n",
      "ep 3700: ep_len:3611 episode reward: total was -76.800000. running mean: -21.749173\n",
      "ep 3700: ep_len:1239 episode reward: total was -66.560000. running mean: -22.197281\n",
      "ep 3700: ep_len:689 episode reward: total was 0.850000. running mean: -21.966809\n",
      "ep 3700: ep_len:979 episode reward: total was 81.700000. running mean: -20.930140\n",
      "ep 3700: ep_len:38 episode reward: total was 17.500000. running mean: -20.545839\n",
      "ep 3700: ep_len:66 episode reward: total was 31.500000. running mean: -20.025381\n",
      "ep 3700: ep_len:674 episode reward: total was -22.720000. running mean: -20.052327\n",
      "ep 3700: ep_len:2839 episode reward: total was 3.490000. running mean: -19.816904\n",
      "ep 3700: ep_len:56 episode reward: total was 23.500000. running mean: -19.383735\n",
      "epsilon:0.009992 episode_count: 55630. steps_count: 60039660.000000\n",
      "ep 3701: ep_len:663 episode reward: total was -11.040000. running mean: -19.300297\n",
      "ep 3701: ep_len:653 episode reward: total was 6.630000. running mean: -19.040994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3701: ep_len:3024 episode reward: total was -29.810000. running mean: -19.148684\n",
      "ep 3701: ep_len:660 episode reward: total was 7.250000. running mean: -18.884697\n",
      "ep 3701: ep_len:117 episode reward: total was 57.000000. running mean: -18.125850\n",
      "ep 3701: ep_len:1015 episode reward: total was -87.470000. running mean: -18.819292\n",
      "ep 3701: ep_len:661 episode reward: total was 7.150000. running mean: -18.559599\n",
      "ep 3701: ep_len:625 episode reward: total was -51.490000. running mean: -18.888903\n",
      "ep 3701: ep_len:633 episode reward: total was 8.000000. running mean: -18.620014\n",
      "ep 3701: ep_len:1477 episode reward: total was -13.130000. running mean: -18.565114\n",
      "ep 3701: ep_len:162 episode reward: total was 76.500000. running mean: -17.614463\n",
      "ep 3701: ep_len:985 episode reward: total was -9.510000. running mean: -17.533418\n",
      "ep 3701: ep_len:2896 episode reward: total was 2.600000. running mean: -17.332084\n",
      "ep 3701: ep_len:46 episode reward: total was 21.500000. running mean: -16.943763\n",
      "epsilon:0.009992 episode_count: 55644. steps_count: 60053277.000000\n",
      "ep 3702: ep_len:1159 episode reward: total was 8.490000. running mean: -16.689425\n",
      "ep 3702: ep_len:1249 episode reward: total was -39.190000. running mean: -16.914431\n",
      "ep 3702: ep_len:3065 episode reward: total was -25.180000. running mean: -16.997087\n",
      "ep 3702: ep_len:569 episode reward: total was 0.470000. running mean: -16.822416\n",
      "ep 3702: ep_len:44 episode reward: total was 20.500000. running mean: -16.449192\n",
      "ep 3702: ep_len:100 episode reward: total was 47.000000. running mean: -15.814700\n",
      "ep 3702: ep_len:943 episode reward: total was -6.900000. running mean: -15.725553\n",
      "ep 3702: ep_len:3664 episode reward: total was -29.170000. running mean: -15.859997\n",
      "ep 3702: ep_len:768 episode reward: total was -31.360000. running mean: -16.014997\n",
      "ep 3702: ep_len:670 episode reward: total was 23.680000. running mean: -15.618047\n",
      "ep 3702: ep_len:983 episode reward: total was 10.950000. running mean: -15.352367\n",
      "ep 3702: ep_len:58 episode reward: total was 26.000000. running mean: -14.938843\n",
      "ep 3702: ep_len:58 episode reward: total was 27.500000. running mean: -14.514455\n",
      "ep 3702: ep_len:686 episode reward: total was -53.880000. running mean: -14.908110\n",
      "ep 3702: ep_len:2841 episode reward: total was 1.160000. running mean: -14.747429\n",
      "epsilon:0.009992 episode_count: 55659. steps_count: 60070134.000000\n",
      "ep 3703: ep_len:1152 episode reward: total was -22.900000. running mean: -14.828955\n",
      "ep 3703: ep_len:874 episode reward: total was 18.610000. running mean: -14.494565\n",
      "ep 3703: ep_len:2970 episode reward: total was -40.760000. running mean: -14.757220\n",
      "ep 3703: ep_len:500 episode reward: total was 16.990000. running mean: -14.439748\n",
      "ep 3703: ep_len:784 episode reward: total was 17.370000. running mean: -14.121650\n",
      "ep 3703: ep_len:316 episode reward: total was 2.940000. running mean: -13.951034\n",
      "ep 3703: ep_len:1538 episode reward: total was -68.140000. running mean: -14.492923\n",
      "ep 3703: ep_len:670 episode reward: total was 13.820000. running mean: -14.209794\n",
      "ep 3703: ep_len:1491 episode reward: total was 1.950000. running mean: -14.048196\n",
      "ep 3703: ep_len:123 episode reward: total was 58.500000. running mean: -13.322714\n",
      "ep 3703: ep_len:132 episode reward: total was 64.500000. running mean: -12.544487\n",
      "ep 3703: ep_len:1128 episode reward: total was -5.740000. running mean: -12.476442\n",
      "ep 3703: ep_len:2756 episode reward: total was -21.730000. running mean: -12.568978\n",
      "epsilon:0.009992 episode_count: 55672. steps_count: 60084568.000000\n",
      "ep 3704: ep_len:1431 episode reward: total was 8.540000. running mean: -12.357888\n",
      "ep 3704: ep_len:196 episode reward: total was 0.970000. running mean: -12.224609\n",
      "ep 3704: ep_len:74 episode reward: total was 34.000000. running mean: -11.762363\n",
      "ep 3704: ep_len:2898 episode reward: total was -28.970000. running mean: -11.934439\n",
      "ep 3704: ep_len:1281 episode reward: total was -41.560000. running mean: -12.230695\n",
      "ep 3704: ep_len:39 episode reward: total was 18.000000. running mean: -11.928388\n",
      "ep 3704: ep_len:159 episode reward: total was 76.500000. running mean: -11.044104\n",
      "ep 3704: ep_len:51 episode reward: total was 22.500000. running mean: -10.708663\n",
      "ep 3704: ep_len:1004 episode reward: total was 5.830000. running mean: -10.543276\n",
      "ep 3704: ep_len:681 episode reward: total was 18.950000. running mean: -10.248344\n",
      "ep 3704: ep_len:500 episode reward: total was 4.770000. running mean: -10.098160\n",
      "ep 3704: ep_len:740 episode reward: total was 31.770000. running mean: -9.679479\n",
      "ep 3704: ep_len:755 episode reward: total was -3.850000. running mean: -9.621184\n",
      "ep 3704: ep_len:82 episode reward: total was 39.500000. running mean: -9.129972\n",
      "ep 3704: ep_len:143 episode reward: total was 67.000000. running mean: -8.368672\n",
      "ep 3704: ep_len:66 episode reward: total was 30.000000. running mean: -7.984986\n",
      "ep 3704: ep_len:500 episode reward: total was 22.200000. running mean: -7.683136\n",
      "ep 3704: ep_len:2782 episode reward: total was 9.630000. running mean: -7.510004\n",
      "epsilon:0.009992 episode_count: 55690. steps_count: 60097950.000000\n",
      "ep 3705: ep_len:961 episode reward: total was -63.500000. running mean: -8.069904\n",
      "ep 3705: ep_len:662 episode reward: total was -44.050000. running mean: -8.429705\n",
      "ep 3705: ep_len:3022 episode reward: total was -11.770000. running mean: -8.463108\n",
      "ep 3705: ep_len:1136 episode reward: total was -24.160000. running mean: -8.620077\n",
      "ep 3705: ep_len:991 episode reward: total was -16.240000. running mean: -8.696276\n",
      "ep 3705: ep_len:3795 episode reward: total was 5.920000. running mean: -8.550114\n",
      "ep 3705: ep_len:1279 episode reward: total was -37.260000. running mean: -8.837212\n",
      "ep 3705: ep_len:719 episode reward: total was 25.710000. running mean: -8.491740\n",
      "ep 3705: ep_len:593 episode reward: total was 66.950000. running mean: -7.737323\n",
      "ep 3705: ep_len:52 episode reward: total was 20.000000. running mean: -7.459950\n",
      "ep 3705: ep_len:625 episode reward: total was 4.420000. running mean: -7.341150\n",
      "ep 3705: ep_len:2750 episode reward: total was -46.340000. running mean: -7.731139\n",
      "epsilon:0.009992 episode_count: 55702. steps_count: 60114535.000000\n",
      "ep 3706: ep_len:1416 episode reward: total was -6.000000. running mean: -7.713827\n",
      "ep 3706: ep_len:1269 episode reward: total was -41.010000. running mean: -8.046789\n",
      "ep 3706: ep_len:78 episode reward: total was 30.000000. running mean: -7.666321\n",
      "ep 3706: ep_len:2978 episode reward: total was 2.330000. running mean: -7.566358\n",
      "ep 3706: ep_len:500 episode reward: total was 11.820000. running mean: -7.372494\n",
      "ep 3706: ep_len:63 episode reward: total was 27.000000. running mean: -7.028769\n",
      "ep 3706: ep_len:1847 episode reward: total was -130.310000. running mean: -8.261582\n",
      "ep 3706: ep_len:651 episode reward: total was 23.030000. running mean: -7.948666\n",
      "ep 3706: ep_len:524 episode reward: total was -36.340000. running mean: -8.232579\n",
      "ep 3706: ep_len:781 episode reward: total was 34.960000. running mean: -7.800653\n",
      "ep 3706: ep_len:777 episode reward: total was -41.800000. running mean: -8.140647\n",
      "ep 3706: ep_len:624 episode reward: total was -1.710000. running mean: -8.076340\n",
      "ep 3706: ep_len:2731 episode reward: total was -6.430000. running mean: -8.059877\n",
      "epsilon:0.009992 episode_count: 55715. steps_count: 60128774.000000\n",
      "ep 3707: ep_len:830 episode reward: total was -31.260000. running mean: -8.291878\n",
      "ep 3707: ep_len:500 episode reward: total was 13.290000. running mean: -8.076059\n",
      "ep 3707: ep_len:101 episode reward: total was 47.500000. running mean: -7.520299\n",
      "ep 3707: ep_len:1724 episode reward: total was -44.360000. running mean: -7.888696\n",
      "ep 3707: ep_len:41 episode reward: total was 19.000000. running mean: -7.619809\n",
      "ep 3707: ep_len:721 episode reward: total was -13.160000. running mean: -7.675211\n",
      "ep 3707: ep_len:3655 episode reward: total was -8.690000. running mean: -7.685359\n",
      "ep 3707: ep_len:648 episode reward: total was -26.650000. running mean: -7.875005\n",
      "ep 3707: ep_len:860 episode reward: total was 48.100000. running mean: -7.315255\n",
      "ep 3707: ep_len:595 episode reward: total was 11.040000. running mean: -7.131703\n",
      "ep 3707: ep_len:49 episode reward: total was 23.000000. running mean: -6.830386\n",
      "ep 3707: ep_len:26 episode reward: total was 11.500000. running mean: -6.647082\n",
      "ep 3707: ep_len:667 episode reward: total was 6.840000. running mean: -6.512211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3707: ep_len:2828 episode reward: total was -44.150000. running mean: -6.888589\n",
      "epsilon:0.009992 episode_count: 55729. steps_count: 60142019.000000\n",
      "ep 3708: ep_len:1416 episode reward: total was 19.470000. running mean: -6.625003\n",
      "ep 3708: ep_len:924 episode reward: total was 10.270000. running mean: -6.456053\n",
      "ep 3708: ep_len:3043 episode reward: total was -67.090000. running mean: -7.062392\n",
      "ep 3708: ep_len:749 episode reward: total was -9.850000. running mean: -7.090268\n",
      "ep 3708: ep_len:50 episode reward: total was 22.000000. running mean: -6.799366\n",
      "ep 3708: ep_len:75 episode reward: total was 34.500000. running mean: -6.386372\n",
      "ep 3708: ep_len:1202 episode reward: total was 2.990000. running mean: -6.292608\n",
      "ep 3708: ep_len:3713 episode reward: total was 2.290000. running mean: -6.206782\n",
      "ep 3708: ep_len:953 episode reward: total was -24.980000. running mean: -6.394514\n",
      "ep 3708: ep_len:691 episode reward: total was -1.130000. running mean: -6.341869\n",
      "ep 3708: ep_len:989 episode reward: total was 59.880000. running mean: -5.679651\n",
      "ep 3708: ep_len:1067 episode reward: total was -5.900000. running mean: -5.681854\n",
      "ep 3708: ep_len:2885 episode reward: total was -33.320000. running mean: -5.958236\n",
      "ep 3708: ep_len:57 episode reward: total was 27.000000. running mean: -5.628653\n",
      "epsilon:0.009992 episode_count: 55743. steps_count: 60159833.000000\n",
      "ep 3709: ep_len:984 episode reward: total was -66.330000. running mean: -6.235667\n",
      "ep 3709: ep_len:970 episode reward: total was 1.300000. running mean: -6.160310\n",
      "ep 3709: ep_len:2951 episode reward: total was -30.880000. running mean: -6.407507\n",
      "ep 3709: ep_len:626 episode reward: total was -1.530000. running mean: -6.358732\n",
      "ep 3709: ep_len:85 episode reward: total was 38.000000. running mean: -5.915144\n",
      "ep 3709: ep_len:43 episode reward: total was 20.000000. running mean: -5.655993\n",
      "ep 3709: ep_len:609 episode reward: total was -3.090000. running mean: -5.630333\n",
      "ep 3709: ep_len:3562 episode reward: total was -10.600000. running mean: -5.680030\n",
      "ep 3709: ep_len:820 episode reward: total was -24.270000. running mean: -5.865929\n",
      "ep 3709: ep_len:694 episode reward: total was 25.620000. running mean: -5.551070\n",
      "ep 3709: ep_len:500 episode reward: total was 32.240000. running mean: -5.173159\n",
      "ep 3709: ep_len:94 episode reward: total was 45.500000. running mean: -4.666428\n",
      "ep 3709: ep_len:1126 episode reward: total was -16.180000. running mean: -4.781564\n",
      "ep 3709: ep_len:2902 episode reward: total was 8.840000. running mean: -4.645348\n",
      "ep 3709: ep_len:73 episode reward: total was 35.000000. running mean: -4.248894\n",
      "epsilon:0.009992 episode_count: 55758. steps_count: 60175872.000000\n",
      "ep 3710: ep_len:950 episode reward: total was -24.570000. running mean: -4.452106\n",
      "ep 3710: ep_len:727 episode reward: total was -9.780000. running mean: -4.505384\n",
      "ep 3710: ep_len:43 episode reward: total was 20.000000. running mean: -4.260331\n",
      "ep 3710: ep_len:2889 episode reward: total was -20.310000. running mean: -4.420827\n",
      "ep 3710: ep_len:647 episode reward: total was 6.830000. running mean: -4.308319\n",
      "ep 3710: ep_len:21 episode reward: total was 7.500000. running mean: -4.190236\n",
      "ep 3710: ep_len:107 episode reward: total was 50.500000. running mean: -3.643334\n",
      "ep 3710: ep_len:903 episode reward: total was 46.580000. running mean: -3.141100\n",
      "ep 3710: ep_len:3549 episode reward: total was -42.710000. running mean: -3.536789\n",
      "ep 3710: ep_len:707 episode reward: total was -14.150000. running mean: -3.642921\n",
      "ep 3710: ep_len:742 episode reward: total was 25.020000. running mean: -3.356292\n",
      "ep 3710: ep_len:926 episode reward: total was -4.410000. running mean: -3.366829\n",
      "ep 3710: ep_len:78 episode reward: total was 37.500000. running mean: -2.958161\n",
      "ep 3710: ep_len:645 episode reward: total was -9.820000. running mean: -3.026779\n",
      "ep 3710: ep_len:2817 episode reward: total was -1.260000. running mean: -3.009111\n",
      "epsilon:0.009992 episode_count: 55773. steps_count: 60191623.000000\n",
      "ep 3711: ep_len:756 episode reward: total was -93.610000. running mean: -3.915120\n",
      "ep 3711: ep_len:659 episode reward: total was -11.910000. running mean: -3.995069\n",
      "ep 3711: ep_len:63 episode reward: total was 30.000000. running mean: -3.655118\n",
      "ep 3711: ep_len:2942 episode reward: total was -29.230000. running mean: -3.910867\n",
      "ep 3711: ep_len:500 episode reward: total was 31.230000. running mean: -3.559459\n",
      "ep 3711: ep_len:2055 episode reward: total was -1300.700000. running mean: -16.530864\n",
      "ep 3711: ep_len:3573 episode reward: total was -159.850000. running mean: -17.964055\n",
      "ep 3711: ep_len:1208 episode reward: total was -31.060000. running mean: -18.095015\n",
      "ep 3711: ep_len:722 episode reward: total was -21.230000. running mean: -18.126365\n",
      "ep 3711: ep_len:672 episode reward: total was -11.240000. running mean: -18.057501\n",
      "ep 3711: ep_len:98 episode reward: total was 47.500000. running mean: -17.401926\n",
      "ep 3711: ep_len:79 episode reward: total was 36.500000. running mean: -16.862907\n",
      "ep 3711: ep_len:736 episode reward: total was -54.420000. running mean: -17.238478\n",
      "ep 3711: ep_len:2843 episode reward: total was -5.400000. running mean: -17.120093\n",
      "ep 3711: ep_len:38 episode reward: total was 17.500000. running mean: -16.773892\n",
      "epsilon:0.009992 episode_count: 55788. steps_count: 60208567.000000\n",
      "ep 3712: ep_len:913 episode reward: total was -40.660000. running mean: -17.012753\n",
      "ep 3712: ep_len:706 episode reward: total was 11.090000. running mean: -16.731726\n",
      "ep 3712: ep_len:3003 episode reward: total was -71.780000. running mean: -17.282208\n",
      "ep 3712: ep_len:1716 episode reward: total was -102.620000. running mean: -18.135586\n",
      "ep 3712: ep_len:500 episode reward: total was 18.650000. running mean: -17.767730\n",
      "ep 3712: ep_len:602 episode reward: total was 23.180000. running mean: -17.358253\n",
      "ep 3712: ep_len:1334 episode reward: total was -102.980000. running mean: -18.214470\n",
      "ep 3712: ep_len:704 episode reward: total was 39.580000. running mean: -17.636526\n",
      "ep 3712: ep_len:628 episode reward: total was 1.670000. running mean: -17.443461\n",
      "ep 3712: ep_len:1210 episode reward: total was -7.770000. running mean: -17.346726\n",
      "ep 3712: ep_len:2886 episode reward: total was -8.490000. running mean: -17.258159\n",
      "epsilon:0.009992 episode_count: 55799. steps_count: 60222769.000000\n",
      "ep 3713: ep_len:1106 episode reward: total was -12.250000. running mean: -17.208077\n",
      "ep 3713: ep_len:696 episode reward: total was -9.600000. running mean: -17.131996\n",
      "ep 3713: ep_len:30 episode reward: total was 13.500000. running mean: -16.825676\n",
      "ep 3713: ep_len:2965 episode reward: total was -77.910000. running mean: -17.436520\n",
      "ep 3713: ep_len:813 episode reward: total was 16.680000. running mean: -17.095354\n",
      "ep 3713: ep_len:1984 episode reward: total was -721.790000. running mean: -24.142301\n",
      "ep 3713: ep_len:319 episode reward: total was 15.060000. running mean: -23.750278\n",
      "ep 3713: ep_len:645 episode reward: total was 2.240000. running mean: -23.490375\n",
      "ep 3713: ep_len:898 episode reward: total was 58.250000. running mean: -22.672971\n",
      "ep 3713: ep_len:770 episode reward: total was -8.630000. running mean: -22.532542\n",
      "ep 3713: ep_len:140 episode reward: total was 67.000000. running mean: -21.637216\n",
      "ep 3713: ep_len:50 episode reward: total was 22.000000. running mean: -21.200844\n",
      "ep 3713: ep_len:1421 episode reward: total was -0.040000. running mean: -20.989236\n",
      "ep 3713: ep_len:2825 episode reward: total was -20.950000. running mean: -20.988843\n",
      "epsilon:0.009992 episode_count: 55813. steps_count: 60237431.000000\n",
      "ep 3714: ep_len:1000 episode reward: total was -142.680000. running mean: -22.205755\n",
      "ep 3714: ep_len:949 episode reward: total was 4.300000. running mean: -21.940697\n",
      "ep 3714: ep_len:49 episode reward: total was 21.500000. running mean: -21.506290\n",
      "ep 3714: ep_len:3060 episode reward: total was 22.190000. running mean: -21.069327\n",
      "ep 3714: ep_len:569 episode reward: total was 8.320000. running mean: -20.775434\n",
      "ep 3714: ep_len:41 episode reward: total was 19.000000. running mean: -20.377680\n",
      "ep 3714: ep_len:123 episode reward: total was 55.500000. running mean: -19.618903\n",
      "ep 3714: ep_len:43 episode reward: total was 20.000000. running mean: -19.222714\n",
      "ep 3714: ep_len:620 episode reward: total was 20.330000. running mean: -18.827187\n",
      "ep 3714: ep_len:628 episode reward: total was 17.380000. running mean: -18.465115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3714: ep_len:717 episode reward: total was -7.650000. running mean: -18.356964\n",
      "ep 3714: ep_len:7267 episode reward: total was -186.510000. running mean: -20.038494\n",
      "ep 3714: ep_len:1036 episode reward: total was 31.320000. running mean: -19.524909\n",
      "ep 3714: ep_len:73 episode reward: total was 35.000000. running mean: -18.979660\n",
      "ep 3714: ep_len:1174 episode reward: total was -11.660000. running mean: -18.906463\n",
      "ep 3714: ep_len:2850 episode reward: total was -5.030000. running mean: -18.767699\n",
      "ep 3714: ep_len:47 episode reward: total was 20.500000. running mean: -18.375022\n",
      "epsilon:0.009992 episode_count: 55830. steps_count: 60257677.000000\n",
      "ep 3715: ep_len:866 episode reward: total was 19.050000. running mean: -18.000772\n",
      "ep 3715: ep_len:190 episode reward: total was 10.770000. running mean: -17.713064\n",
      "ep 3715: ep_len:2952 episode reward: total was -42.710000. running mean: -17.963033\n",
      "ep 3715: ep_len:521 episode reward: total was 1.000000. running mean: -17.773403\n",
      "ep 3715: ep_len:126 episode reward: total was 61.500000. running mean: -16.980669\n",
      "ep 3715: ep_len:68 episode reward: total was 32.500000. running mean: -16.485862\n",
      "ep 3715: ep_len:872 episode reward: total was 35.860000. running mean: -15.962404\n",
      "ep 3715: ep_len:360 episode reward: total was 13.240000. running mean: -15.670380\n",
      "ep 3715: ep_len:2526 episode reward: total was -156.220000. running mean: -17.075876\n",
      "ep 3715: ep_len:795 episode reward: total was -564.010000. running mean: -22.545217\n",
      "ep 3715: ep_len:1442 episode reward: total was -17.920000. running mean: -22.498965\n",
      "ep 3715: ep_len:61 episode reward: total was 29.000000. running mean: -21.983975\n",
      "ep 3715: ep_len:124 episode reward: total was 59.000000. running mean: -21.174135\n",
      "ep 3715: ep_len:37 episode reward: total was 15.500000. running mean: -20.807394\n",
      "ep 3715: ep_len:606 episode reward: total was 21.500000. running mean: -20.384320\n",
      "ep 3715: ep_len:2832 episode reward: total was 14.600000. running mean: -20.034477\n",
      "epsilon:0.009992 episode_count: 55846. steps_count: 60272055.000000\n",
      "ep 3716: ep_len:965 episode reward: total was -17.250000. running mean: -20.006632\n",
      "ep 3716: ep_len:740 episode reward: total was -18.020000. running mean: -19.986766\n",
      "ep 3716: ep_len:2978 episode reward: total was -31.500000. running mean: -20.101898\n",
      "ep 3716: ep_len:857 episode reward: total was 3.370000. running mean: -19.867179\n",
      "ep 3716: ep_len:45 episode reward: total was 21.000000. running mean: -19.458507\n",
      "ep 3716: ep_len:74 episode reward: total was 29.500000. running mean: -18.968922\n",
      "ep 3716: ep_len:1373 episode reward: total was -368.070000. running mean: -22.459933\n",
      "ep 3716: ep_len:314 episode reward: total was 21.990000. running mean: -22.015434\n",
      "ep 3716: ep_len:1257 episode reward: total was -78.500000. running mean: -22.580279\n",
      "ep 3716: ep_len:763 episode reward: total was -10.780000. running mean: -22.462277\n",
      "ep 3716: ep_len:624 episode reward: total was 10.560000. running mean: -22.132054\n",
      "ep 3716: ep_len:931 episode reward: total was 5.500000. running mean: -21.855733\n",
      "ep 3716: ep_len:2854 episode reward: total was -14.570000. running mean: -21.782876\n",
      "epsilon:0.009992 episode_count: 55859. steps_count: 60285830.000000\n",
      "ep 3717: ep_len:966 episode reward: total was -65.440000. running mean: -22.219447\n",
      "ep 3717: ep_len:1202 episode reward: total was -146.340000. running mean: -23.460653\n",
      "ep 3717: ep_len:2998 episode reward: total was 2.980000. running mean: -23.196246\n",
      "ep 3717: ep_len:756 episode reward: total was -8.770000. running mean: -23.051984\n",
      "ep 3717: ep_len:77 episode reward: total was 37.000000. running mean: -22.451464\n",
      "ep 3717: ep_len:665 episode reward: total was 10.590000. running mean: -22.121049\n",
      "ep 3717: ep_len:360 episode reward: total was 18.750000. running mean: -21.712339\n",
      "ep 3717: ep_len:819 episode reward: total was -59.650000. running mean: -22.091715\n",
      "ep 3717: ep_len:735 episode reward: total was 0.020000. running mean: -21.870598\n",
      "ep 3717: ep_len:923 episode reward: total was 53.240000. running mean: -21.119492\n",
      "ep 3717: ep_len:63 episode reward: total was 27.000000. running mean: -20.638297\n",
      "ep 3717: ep_len:67 episode reward: total was 32.000000. running mean: -20.111914\n",
      "ep 3717: ep_len:636 episode reward: total was -1.100000. running mean: -19.921795\n",
      "ep 3717: ep_len:2814 episode reward: total was -71.900000. running mean: -20.441577\n",
      "ep 3717: ep_len:55 episode reward: total was 26.000000. running mean: -19.977162\n",
      "epsilon:0.009992 episode_count: 55874. steps_count: 60298966.000000\n",
      "ep 3718: ep_len:859 episode reward: total was -5.860000. running mean: -19.835990\n",
      "ep 3718: ep_len:1646 episode reward: total was -99.280000. running mean: -20.630430\n",
      "ep 3718: ep_len:3078 episode reward: total was -436.850000. running mean: -24.792626\n",
      "ep 3718: ep_len:675 episode reward: total was 16.270000. running mean: -24.381999\n",
      "ep 3718: ep_len:42 episode reward: total was 19.500000. running mean: -23.943179\n",
      "ep 3718: ep_len:106 episode reward: total was 48.500000. running mean: -23.218748\n",
      "ep 3718: ep_len:68 episode reward: total was 31.000000. running mean: -22.676560\n",
      "ep 3718: ep_len:45 episode reward: total was 19.500000. running mean: -22.254795\n",
      "ep 3718: ep_len:668 episode reward: total was 14.570000. running mean: -21.886547\n",
      "ep 3718: ep_len:660 episode reward: total was 17.420000. running mean: -21.493481\n",
      "ep 3718: ep_len:4198 episode reward: total was -856.380000. running mean: -29.842346\n",
      "ep 3718: ep_len:7229 episode reward: total was -28.880000. running mean: -29.832723\n",
      "ep 3718: ep_len:975 episode reward: total was 23.450000. running mean: -29.299896\n",
      "ep 3718: ep_len:73 episode reward: total was 35.000000. running mean: -28.656897\n",
      "ep 3718: ep_len:876 episode reward: total was 4.550000. running mean: -28.324828\n",
      "ep 3718: ep_len:2769 episode reward: total was -44.040000. running mean: -28.481979\n",
      "epsilon:0.009992 episode_count: 55890. steps_count: 60322933.000000\n",
      "ep 3719: ep_len:1169 episode reward: total was -33.840000. running mean: -28.535560\n",
      "ep 3719: ep_len:708 episode reward: total was -2.200000. running mean: -28.272204\n",
      "ep 3719: ep_len:53 episode reward: total was 23.500000. running mean: -27.754482\n",
      "ep 3719: ep_len:3017 episode reward: total was -2.360000. running mean: -27.500537\n",
      "ep 3719: ep_len:634 episode reward: total was 10.540000. running mean: -27.120132\n",
      "ep 3719: ep_len:112 episode reward: total was 53.000000. running mean: -26.318931\n",
      "ep 3719: ep_len:897 episode reward: total was 75.210000. running mean: -25.303641\n",
      "ep 3719: ep_len:605 episode reward: total was 27.620000. running mean: -24.774405\n",
      "ep 3719: ep_len:957 episode reward: total was -10.080000. running mean: -24.627461\n",
      "ep 3719: ep_len:685 episode reward: total was 15.660000. running mean: -24.224586\n",
      "ep 3719: ep_len:789 episode reward: total was 22.430000. running mean: -23.758040\n",
      "ep 3719: ep_len:47 episode reward: total was 20.500000. running mean: -23.315460\n",
      "ep 3719: ep_len:114 episode reward: total was 54.000000. running mean: -22.542305\n",
      "ep 3719: ep_len:1015 episode reward: total was -0.270000. running mean: -22.319582\n",
      "ep 3719: ep_len:2861 episode reward: total was -2.130000. running mean: -22.117686\n",
      "ep 3719: ep_len:38 episode reward: total was 17.500000. running mean: -21.721510\n",
      "epsilon:0.009992 episode_count: 55906. steps_count: 60336634.000000\n",
      "ep 3720: ep_len:500 episode reward: total was 9.310000. running mean: -21.411194\n",
      "ep 3720: ep_len:1584 episode reward: total was -93.380000. running mean: -22.130883\n",
      "ep 3720: ep_len:2920 episode reward: total was -35.700000. running mean: -22.266574\n",
      "ep 3720: ep_len:625 episode reward: total was 3.440000. running mean: -22.009508\n",
      "ep 3720: ep_len:61 episode reward: total was 29.000000. running mean: -21.499413\n",
      "ep 3720: ep_len:968 episode reward: total was 1.950000. running mean: -21.264919\n",
      "ep 3720: ep_len:346 episode reward: total was 20.630000. running mean: -20.845970\n",
      "ep 3720: ep_len:579 episode reward: total was -46.900000. running mean: -21.106510\n",
      "ep 3720: ep_len:870 episode reward: total was 61.790000. running mean: -20.277545\n",
      "ep 3720: ep_len:1127 episode reward: total was -36.860000. running mean: -20.443369\n",
      "ep 3720: ep_len:73 episode reward: total was 35.000000. running mean: -19.888936\n",
      "ep 3720: ep_len:657 episode reward: total was -7.750000. running mean: -19.767546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3720: ep_len:2765 episode reward: total was -7.440000. running mean: -19.644271\n",
      "epsilon:0.009992 episode_count: 55919. steps_count: 60349709.000000\n",
      "ep 3721: ep_len:1009 episode reward: total was -45.580000. running mean: -19.903628\n",
      "ep 3721: ep_len:727 episode reward: total was -8.220000. running mean: -19.786792\n",
      "ep 3721: ep_len:59 episode reward: total was 26.500000. running mean: -19.323924\n",
      "ep 3721: ep_len:2979 episode reward: total was -15.380000. running mean: -19.284485\n",
      "ep 3721: ep_len:500 episode reward: total was 9.580000. running mean: -18.995840\n",
      "ep 3721: ep_len:38 episode reward: total was 17.500000. running mean: -18.630881\n",
      "ep 3721: ep_len:91 episode reward: total was 41.000000. running mean: -18.034573\n",
      "ep 3721: ep_len:44 episode reward: total was 20.500000. running mean: -17.649227\n",
      "ep 3721: ep_len:500 episode reward: total was 19.560000. running mean: -17.277135\n",
      "ep 3721: ep_len:3664 episode reward: total was -132.310000. running mean: -18.427463\n",
      "ep 3721: ep_len:788 episode reward: total was -4.300000. running mean: -18.286189\n",
      "ep 3721: ep_len:690 episode reward: total was 46.910000. running mean: -17.634227\n",
      "ep 3721: ep_len:649 episode reward: total was 0.260000. running mean: -17.455284\n",
      "ep 3721: ep_len:139 episode reward: total was 68.000000. running mean: -16.600732\n",
      "ep 3721: ep_len:36 episode reward: total was 16.500000. running mean: -16.269724\n",
      "ep 3721: ep_len:58 episode reward: total was 27.500000. running mean: -15.832027\n",
      "ep 3721: ep_len:497 episode reward: total was 48.380000. running mean: -15.189907\n",
      "ep 3721: ep_len:2810 episode reward: total was -10.300000. running mean: -15.141008\n",
      "ep 3721: ep_len:63 episode reward: total was 28.500000. running mean: -14.704598\n",
      "epsilon:0.009992 episode_count: 55938. steps_count: 60365050.000000\n",
      "ep 3722: ep_len:4967 episode reward: total was -962.430000. running mean: -24.181852\n",
      "ep 3722: ep_len:943 episode reward: total was -23.120000. running mean: -24.171233\n",
      "ep 3722: ep_len:100 episode reward: total was 48.500000. running mean: -23.444521\n",
      "ep 3722: ep_len:515 episode reward: total was -10.170000. running mean: -23.311776\n",
      "ep 3722: ep_len:66 episode reward: total was 31.500000. running mean: -22.763658\n",
      "ep 3722: ep_len:162 episode reward: total was 76.500000. running mean: -21.771021\n",
      "ep 3722: ep_len:75 episode reward: total was 36.000000. running mean: -21.193311\n",
      "ep 3722: ep_len:984 episode reward: total was 0.580000. running mean: -20.975578\n",
      "ep 3722: ep_len:616 episode reward: total was 27.970000. running mean: -20.486122\n",
      "ep 3722: ep_len:1183 episode reward: total was -17.590000. running mean: -20.457161\n",
      "ep 3722: ep_len:620 episode reward: total was -7.680000. running mean: -20.329389\n",
      "ep 3722: ep_len:609 episode reward: total was -7.180000. running mean: -20.197895\n",
      "ep 3722: ep_len:125 episode reward: total was 59.500000. running mean: -19.400917\n",
      "ep 3722: ep_len:39 episode reward: total was 18.000000. running mean: -19.026907\n",
      "ep 3722: ep_len:71 episode reward: total was 34.000000. running mean: -18.496638\n",
      "ep 3722: ep_len:1451 episode reward: total was -3.510000. running mean: -18.346772\n",
      "ep 3722: ep_len:2919 episode reward: total was 9.930000. running mean: -18.064004\n",
      "epsilon:0.009992 episode_count: 55955. steps_count: 60380495.000000\n",
      "ep 3723: ep_len:753 episode reward: total was -16.880000. running mean: -18.052164\n",
      "ep 3723: ep_len:500 episode reward: total was -327.290000. running mean: -21.144542\n",
      "ep 3723: ep_len:2920 episode reward: total was -13.040000. running mean: -21.063497\n",
      "ep 3723: ep_len:789 episode reward: total was -21.050000. running mean: -21.063362\n",
      "ep 3723: ep_len:89 episode reward: total was 43.000000. running mean: -20.422728\n",
      "ep 3723: ep_len:60 episode reward: total was 24.000000. running mean: -19.978501\n",
      "ep 3723: ep_len:658 episode reward: total was 2.370000. running mean: -19.755016\n",
      "ep 3723: ep_len:355 episode reward: total was 12.150000. running mean: -19.435966\n",
      "ep 3723: ep_len:1132 episode reward: total was -10.370000. running mean: -19.345306\n",
      "ep 3723: ep_len:638 episode reward: total was 20.730000. running mean: -18.944553\n",
      "ep 3723: ep_len:656 episode reward: total was -14.530000. running mean: -18.900408\n",
      "ep 3723: ep_len:94 episode reward: total was 44.000000. running mean: -18.271404\n",
      "ep 3723: ep_len:129 episode reward: total was 61.500000. running mean: -17.473690\n",
      "ep 3723: ep_len:648 episode reward: total was -28.000000. running mean: -17.578953\n",
      "ep 3723: ep_len:2783 episode reward: total was -0.560000. running mean: -17.408763\n",
      "ep 3723: ep_len:55 episode reward: total was 24.500000. running mean: -16.989676\n",
      "epsilon:0.009992 episode_count: 55971. steps_count: 60392754.000000\n",
      "ep 3724: ep_len:1069 episode reward: total was -13.720000. running mean: -16.956979\n",
      "ep 3724: ep_len:728 episode reward: total was 5.840000. running mean: -16.729009\n",
      "ep 3724: ep_len:2953 episode reward: total was -38.520000. running mean: -16.946919\n",
      "ep 3724: ep_len:1221 episode reward: total was -57.650000. running mean: -17.353950\n",
      "ep 3724: ep_len:101 episode reward: total was 49.000000. running mean: -16.690410\n",
      "ep 3724: ep_len:1946 episode reward: total was -173.090000. running mean: -18.254406\n",
      "ep 3724: ep_len:3774 episode reward: total was -20.090000. running mean: -18.272762\n",
      "ep 3724: ep_len:964 episode reward: total was -25.170000. running mean: -18.341734\n",
      "ep 3724: ep_len:871 episode reward: total was 46.460000. running mean: -17.693717\n",
      "ep 3724: ep_len:945 episode reward: total was 27.710000. running mean: -17.239680\n",
      "ep 3724: ep_len:86 episode reward: total was 37.000000. running mean: -16.697283\n",
      "ep 3724: ep_len:609 episode reward: total was -16.300000. running mean: -16.693310\n",
      "ep 3724: ep_len:2836 episode reward: total was -2.290000. running mean: -16.549277\n",
      "epsilon:0.009992 episode_count: 55984. steps_count: 60410857.000000\n",
      "ep 3725: ep_len:1452 episode reward: total was 6.910000. running mean: -16.314684\n",
      "ep 3725: ep_len:939 episode reward: total was -4.000000. running mean: -16.191538\n",
      "ep 3725: ep_len:62 episode reward: total was 26.500000. running mean: -15.764622\n",
      "ep 3725: ep_len:3015 episode reward: total was -14.570000. running mean: -15.752676\n",
      "ep 3725: ep_len:641 episode reward: total was -12.310000. running mean: -15.718249\n",
      "ep 3725: ep_len:143 episode reward: total was 67.000000. running mean: -14.891067\n",
      "ep 3725: ep_len:594 episode reward: total was 3.750000. running mean: -14.704656\n",
      "ep 3725: ep_len:3702 episode reward: total was -3.170000. running mean: -14.589310\n",
      "ep 3725: ep_len:518 episode reward: total was -23.270000. running mean: -14.676116\n",
      "ep 3725: ep_len:791 episode reward: total was 37.300000. running mean: -14.156355\n",
      "ep 3725: ep_len:500 episode reward: total was 22.570000. running mean: -13.789092\n",
      "ep 3725: ep_len:72 episode reward: total was 34.500000. running mean: -13.306201\n",
      "ep 3725: ep_len:643 episode reward: total was 7.840000. running mean: -13.094739\n",
      "ep 3725: ep_len:2858 episode reward: total was -19.910000. running mean: -13.162891\n",
      "epsilon:0.009992 episode_count: 55998. steps_count: 60426787.000000\n",
      "ep 3726: ep_len:1423 episode reward: total was 10.080000. running mean: -12.930462\n",
      "ep 3726: ep_len:719 episode reward: total was -14.820000. running mean: -12.949358\n",
      "ep 3726: ep_len:51 episode reward: total was 22.500000. running mean: -12.594864\n",
      "ep 3726: ep_len:3037 episode reward: total was -52.240000. running mean: -12.991316\n",
      "ep 3726: ep_len:660 episode reward: total was -1.620000. running mean: -12.877602\n",
      "ep 3726: ep_len:944 episode reward: total was -18.150000. running mean: -12.930326\n",
      "ep 3726: ep_len:3757 episode reward: total was 13.960000. running mean: -12.661423\n",
      "ep 3726: ep_len:1584 episode reward: total was -113.350000. running mean: -13.668309\n",
      "ep 3726: ep_len:710 episode reward: total was 7.840000. running mean: -13.453226\n",
      "ep 3726: ep_len:500 episode reward: total was 2.830000. running mean: -13.290394\n",
      "ep 3726: ep_len:197 episode reward: total was 95.010000. running mean: -12.207390\n",
      "ep 3726: ep_len:665 episode reward: total was -6.650000. running mean: -12.151816\n",
      "ep 3726: ep_len:2887 episode reward: total was 9.540000. running mean: -11.934898\n",
      "ep 3726: ep_len:55 episode reward: total was 26.000000. running mean: -11.555549\n",
      "epsilon:0.009992 episode_count: 56012. steps_count: 60443976.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3727: ep_len:861 episode reward: total was -30.950000. running mean: -11.749493\n",
      "ep 3727: ep_len:216 episode reward: total was 7.420000. running mean: -11.557798\n",
      "ep 3727: ep_len:55 episode reward: total was 26.000000. running mean: -11.182220\n",
      "ep 3727: ep_len:3084 episode reward: total was 3.780000. running mean: -11.032598\n",
      "ep 3727: ep_len:551 episode reward: total was -27.990000. running mean: -11.202172\n",
      "ep 3727: ep_len:55 episode reward: total was 24.500000. running mean: -10.845150\n",
      "ep 3727: ep_len:132 episode reward: total was 64.500000. running mean: -10.091699\n",
      "ep 3727: ep_len:73 episode reward: total was 33.500000. running mean: -9.655782\n",
      "ep 3727: ep_len:500 episode reward: total was 28.910000. running mean: -9.270124\n",
      "ep 3727: ep_len:641 episode reward: total was 27.460000. running mean: -8.902823\n",
      "ep 3727: ep_len:897 episode reward: total was 16.730000. running mean: -8.646495\n",
      "ep 3727: ep_len:665 episode reward: total was 8.870000. running mean: -8.471330\n",
      "ep 3727: ep_len:760 episode reward: total was 17.820000. running mean: -8.208416\n",
      "ep 3727: ep_len:34 episode reward: total was 15.500000. running mean: -7.971332\n",
      "ep 3727: ep_len:759 episode reward: total was -25.910000. running mean: -8.150719\n",
      "ep 3727: ep_len:2865 episode reward: total was -6.310000. running mean: -8.132312\n",
      "epsilon:0.009992 episode_count: 56028. steps_count: 60456124.000000\n",
      "ep 3728: ep_len:500 episode reward: total was 14.390000. running mean: -7.907089\n",
      "ep 3728: ep_len:766 episode reward: total was -8.380000. running mean: -7.911818\n",
      "ep 3728: ep_len:3003 episode reward: total was -69.510000. running mean: -8.527799\n",
      "ep 3728: ep_len:500 episode reward: total was 15.740000. running mean: -8.285121\n",
      "ep 3728: ep_len:106 episode reward: total was 50.000000. running mean: -7.702270\n",
      "ep 3728: ep_len:46 episode reward: total was 21.500000. running mean: -7.410248\n",
      "ep 3728: ep_len:711 episode reward: total was 28.990000. running mean: -7.046245\n",
      "ep 3728: ep_len:344 episode reward: total was 25.140000. running mean: -6.724383\n",
      "ep 3728: ep_len:604 episode reward: total was -31.500000. running mean: -6.972139\n",
      "ep 3728: ep_len:675 episode reward: total was 27.070000. running mean: -6.631717\n",
      "ep 3728: ep_len:507 episode reward: total was -2.170000. running mean: -6.587100\n",
      "ep 3728: ep_len:65 episode reward: total was 29.500000. running mean: -6.226229\n",
      "ep 3728: ep_len:166 episode reward: total was 81.500000. running mean: -5.348967\n",
      "ep 3728: ep_len:34 episode reward: total was 14.000000. running mean: -5.155477\n",
      "ep 3728: ep_len:1092 episode reward: total was -32.100000. running mean: -5.424923\n",
      "ep 3728: ep_len:2916 episode reward: total was 16.140000. running mean: -5.209273\n",
      "epsilon:0.009992 episode_count: 56044. steps_count: 60468159.000000\n",
      "ep 3729: ep_len:1126 episode reward: total was -2.500000. running mean: -5.182181\n",
      "ep 3729: ep_len:1266 episode reward: total was -53.160000. running mean: -5.661959\n",
      "ep 3729: ep_len:2965 episode reward: total was -1628.790000. running mean: -21.893239\n",
      "ep 3729: ep_len:799 episode reward: total was -22.830000. running mean: -21.902607\n",
      "ep 3729: ep_len:54 episode reward: total was 24.000000. running mean: -21.443581\n",
      "ep 3729: ep_len:70 episode reward: total was 33.500000. running mean: -20.894145\n",
      "ep 3729: ep_len:52 episode reward: total was 24.500000. running mean: -20.440203\n",
      "ep 3729: ep_len:500 episode reward: total was 3.980000. running mean: -20.196001\n",
      "ep 3729: ep_len:663 episode reward: total was 11.970000. running mean: -19.874341\n",
      "ep 3729: ep_len:939 episode reward: total was -20.490000. running mean: -19.880498\n",
      "ep 3729: ep_len:861 episode reward: total was 59.620000. running mean: -19.085493\n",
      "ep 3729: ep_len:605 episode reward: total was 54.610000. running mean: -18.348538\n",
      "ep 3729: ep_len:65 episode reward: total was 29.500000. running mean: -17.870053\n",
      "ep 3729: ep_len:1501 episode reward: total was -2.360000. running mean: -17.714952\n",
      "ep 3729: ep_len:2792 episode reward: total was -14.700000. running mean: -17.684803\n",
      "epsilon:0.009992 episode_count: 56059. steps_count: 60482417.000000\n",
      "ep 3730: ep_len:872 episode reward: total was -0.710000. running mean: -17.515055\n",
      "ep 3730: ep_len:945 episode reward: total was 14.670000. running mean: -17.193204\n",
      "ep 3730: ep_len:2915 episode reward: total was -24.270000. running mean: -17.263972\n",
      "ep 3730: ep_len:810 episode reward: total was 3.120000. running mean: -17.060132\n",
      "ep 3730: ep_len:40 episode reward: total was 18.500000. running mean: -16.704531\n",
      "ep 3730: ep_len:156 episode reward: total was 75.000000. running mean: -15.787486\n",
      "ep 3730: ep_len:105 episode reward: total was 46.500000. running mean: -15.164611\n",
      "ep 3730: ep_len:742 episode reward: total was -19.290000. running mean: -15.205865\n",
      "ep 3730: ep_len:642 episode reward: total was 35.000000. running mean: -14.703806\n",
      "ep 3730: ep_len:768 episode reward: total was -10.280000. running mean: -14.659568\n",
      "ep 3730: ep_len:705 episode reward: total was 42.990000. running mean: -14.083072\n",
      "ep 3730: ep_len:1062 episode reward: total was -60.720000. running mean: -14.549442\n",
      "ep 3730: ep_len:100 episode reward: total was 48.500000. running mean: -13.918947\n",
      "ep 3730: ep_len:94 episode reward: total was 44.000000. running mean: -13.339758\n",
      "ep 3730: ep_len:1098 episode reward: total was 2.680000. running mean: -13.179560\n",
      "ep 3730: ep_len:2883 episode reward: total was -27.530000. running mean: -13.323065\n",
      "epsilon:0.009992 episode_count: 56075. steps_count: 60496354.000000\n",
      "ep 3731: ep_len:622 episode reward: total was -45.610000. running mean: -13.645934\n",
      "ep 3731: ep_len:698 episode reward: total was -11.080000. running mean: -13.620275\n",
      "ep 3731: ep_len:2920 episode reward: total was -29.510000. running mean: -13.779172\n",
      "ep 3731: ep_len:842 episode reward: total was 13.600000. running mean: -13.505380\n",
      "ep 3731: ep_len:156 episode reward: total was 72.000000. running mean: -12.650326\n",
      "ep 3731: ep_len:788 episode reward: total was -19.070000. running mean: -12.714523\n",
      "ep 3731: ep_len:3605 episode reward: total was -104.130000. running mean: -13.628678\n",
      "ep 3731: ep_len:579 episode reward: total was -11.240000. running mean: -13.604791\n",
      "ep 3731: ep_len:794 episode reward: total was 50.860000. running mean: -12.960143\n",
      "ep 3731: ep_len:652 episode reward: total was -13.740000. running mean: -12.967942\n",
      "ep 3731: ep_len:81 episode reward: total was 36.000000. running mean: -12.478262\n",
      "ep 3731: ep_len:169 episode reward: total was 83.000000. running mean: -11.523480\n",
      "ep 3731: ep_len:47 episode reward: total was 22.000000. running mean: -11.188245\n",
      "ep 3731: ep_len:1471 episode reward: total was 33.150000. running mean: -10.744862\n",
      "ep 3731: ep_len:2773 episode reward: total was -1.940000. running mean: -10.656814\n",
      "ep 3731: ep_len:52 episode reward: total was 23.000000. running mean: -10.320246\n",
      "epsilon:0.009992 episode_count: 56091. steps_count: 60512603.000000\n",
      "ep 3732: ep_len:1121 episode reward: total was -13.140000. running mean: -10.348443\n",
      "ep 3732: ep_len:741 episode reward: total was -12.960000. running mean: -10.374559\n",
      "ep 3732: ep_len:2993 episode reward: total was -41.950000. running mean: -10.690313\n",
      "ep 3732: ep_len:651 episode reward: total was 14.920000. running mean: -10.434210\n",
      "ep 3732: ep_len:52 episode reward: total was 24.500000. running mean: -10.084868\n",
      "ep 3732: ep_len:138 episode reward: total was 66.000000. running mean: -9.324019\n",
      "ep 3732: ep_len:500 episode reward: total was 16.500000. running mean: -9.065779\n",
      "ep 3732: ep_len:3912 episode reward: total was -52.090000. running mean: -9.496021\n",
      "ep 3732: ep_len:1180 episode reward: total was -54.570000. running mean: -9.946761\n",
      "ep 3732: ep_len:681 episode reward: total was 14.490000. running mean: -9.702393\n",
      "ep 3732: ep_len:663 episode reward: total was -18.870000. running mean: -9.794070\n",
      "ep 3732: ep_len:145 episode reward: total was 69.500000. running mean: -9.001129\n",
      "ep 3732: ep_len:78 episode reward: total was 37.500000. running mean: -8.536118\n",
      "ep 3732: ep_len:879 episode reward: total was 19.520000. running mean: -8.255556\n",
      "ep 3732: ep_len:2774 episode reward: total was -12.550000. running mean: -8.298501\n",
      "epsilon:0.009992 episode_count: 56106. steps_count: 60529111.000000\n",
      "ep 3733: ep_len:1380 episode reward: total was 13.260000. running mean: -8.082916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3733: ep_len:762 episode reward: total was -32.310000. running mean: -8.325187\n",
      "ep 3733: ep_len:2984 episode reward: total was -33.650000. running mean: -8.578435\n",
      "ep 3733: ep_len:500 episode reward: total was 12.960000. running mean: -8.363050\n",
      "ep 3733: ep_len:48 episode reward: total was 21.000000. running mean: -8.069420\n",
      "ep 3733: ep_len:863 episode reward: total was 23.550000. running mean: -7.753226\n",
      "ep 3733: ep_len:3626 episode reward: total was -31.140000. running mean: -7.987093\n",
      "ep 3733: ep_len:615 episode reward: total was -27.350000. running mean: -8.180723\n",
      "ep 3733: ep_len:884 episode reward: total was 50.600000. running mean: -7.592915\n",
      "ep 3733: ep_len:500 episode reward: total was 44.340000. running mean: -7.073586\n",
      "ep 3733: ep_len:136 episode reward: total was 63.500000. running mean: -6.367850\n",
      "ep 3733: ep_len:33 episode reward: total was 15.000000. running mean: -6.154172\n",
      "ep 3733: ep_len:70 episode reward: total was 33.500000. running mean: -5.757630\n",
      "ep 3733: ep_len:500 episode reward: total was 15.680000. running mean: -5.543254\n",
      "ep 3733: ep_len:2963 episode reward: total was -14.090000. running mean: -5.628721\n",
      "epsilon:0.009992 episode_count: 56121. steps_count: 60544975.000000\n",
      "ep 3734: ep_len:1111 episode reward: total was -0.170000. running mean: -5.574134\n",
      "ep 3734: ep_len:626 episode reward: total was 13.310000. running mean: -5.385293\n",
      "ep 3734: ep_len:53 episode reward: total was 25.000000. running mean: -5.081440\n",
      "ep 3734: ep_len:2985 episode reward: total was -41.650000. running mean: -5.447125\n",
      "ep 3734: ep_len:500 episode reward: total was 9.800000. running mean: -5.294654\n",
      "ep 3734: ep_len:47 episode reward: total was 19.000000. running mean: -5.051708\n",
      "ep 3734: ep_len:89 episode reward: total was 41.500000. running mean: -4.586190\n",
      "ep 3734: ep_len:500 episode reward: total was 26.550000. running mean: -4.274829\n",
      "ep 3734: ep_len:350 episode reward: total was 10.750000. running mean: -4.124580\n",
      "ep 3734: ep_len:576 episode reward: total was -32.150000. running mean: -4.404834\n",
      "ep 3734: ep_len:815 episode reward: total was 32.670000. running mean: -4.034086\n",
      "ep 3734: ep_len:607 episode reward: total was -22.380000. running mean: -4.217545\n",
      "ep 3734: ep_len:92 episode reward: total was 44.500000. running mean: -3.730370\n",
      "ep 3734: ep_len:1187 episode reward: total was -31.730000. running mean: -4.010366\n",
      "ep 3734: ep_len:2825 episode reward: total was -13.570000. running mean: -4.105962\n",
      "ep 3734: ep_len:56 episode reward: total was 25.000000. running mean: -3.814903\n",
      "epsilon:0.009992 episode_count: 56137. steps_count: 60557394.000000\n",
      "ep 3735: ep_len:1452 episode reward: total was 21.480000. running mean: -3.561954\n",
      "ep 3735: ep_len:500 episode reward: total was 23.270000. running mean: -3.293634\n",
      "ep 3735: ep_len:63 episode reward: total was 30.000000. running mean: -2.960698\n",
      "ep 3735: ep_len:2982 episode reward: total was -36.360000. running mean: -3.294691\n",
      "ep 3735: ep_len:838 episode reward: total was 7.800000. running mean: -3.183744\n",
      "ep 3735: ep_len:39 episode reward: total was 18.000000. running mean: -2.971907\n",
      "ep 3735: ep_len:53 episode reward: total was 23.500000. running mean: -2.707188\n",
      "ep 3735: ep_len:850 episode reward: total was 33.980000. running mean: -2.340316\n",
      "ep 3735: ep_len:656 episode reward: total was 35.690000. running mean: -1.960012\n",
      "ep 3735: ep_len:569 episode reward: total was 3.690000. running mean: -1.903512\n",
      "ep 3735: ep_len:811 episode reward: total was 27.910000. running mean: -1.605377\n",
      "ep 3735: ep_len:968 episode reward: total was 29.960000. running mean: -1.289723\n",
      "ep 3735: ep_len:179 episode reward: total was 86.500000. running mean: -0.411826\n",
      "ep 3735: ep_len:92 episode reward: total was 44.500000. running mean: 0.037292\n",
      "ep 3735: ep_len:756 episode reward: total was -12.380000. running mean: -0.086881\n",
      "ep 3735: ep_len:2890 episode reward: total was -26.030000. running mean: -0.346312\n",
      "ep 3735: ep_len:53 episode reward: total was 25.000000. running mean: -0.092849\n",
      "epsilon:0.009992 episode_count: 56154. steps_count: 60571145.000000\n",
      "ep 3736: ep_len:964 episode reward: total was -11.470000. running mean: -0.206620\n",
      "ep 3736: ep_len:936 episode reward: total was 18.560000. running mean: -0.018954\n",
      "ep 3736: ep_len:2983 episode reward: total was -23.370000. running mean: -0.252465\n",
      "ep 3736: ep_len:882 episode reward: total was 65.750000. running mean: 0.407560\n",
      "ep 3736: ep_len:109 episode reward: total was 53.000000. running mean: 0.933484\n",
      "ep 3736: ep_len:49 episode reward: total was 20.000000. running mean: 1.124149\n",
      "ep 3736: ep_len:653 episode reward: total was 5.710000. running mean: 1.170008\n",
      "ep 3736: ep_len:3521 episode reward: total was -106.470000. running mean: 0.093608\n",
      "ep 3736: ep_len:1572 episode reward: total was -13.190000. running mean: -0.039228\n",
      "ep 3736: ep_len:827 episode reward: total was 27.100000. running mean: 0.232164\n",
      "ep 3736: ep_len:918 episode reward: total was 61.710000. running mean: 0.846942\n",
      "ep 3736: ep_len:212 episode reward: total was 100.000000. running mean: 1.838473\n",
      "ep 3736: ep_len:500 episode reward: total was 43.940000. running mean: 2.259488\n",
      "ep 3736: ep_len:2831 episode reward: total was -4.910000. running mean: 2.187793\n",
      "epsilon:0.009992 episode_count: 56168. steps_count: 60588102.000000\n",
      "ep 3737: ep_len:1480 episode reward: total was 21.300000. running mean: 2.378915\n",
      "ep 3737: ep_len:744 episode reward: total was -26.580000. running mean: 2.089326\n",
      "ep 3737: ep_len:2983 episode reward: total was -7.540000. running mean: 1.993033\n",
      "ep 3737: ep_len:696 episode reward: total was 12.720000. running mean: 2.100303\n",
      "ep 3737: ep_len:759 episode reward: total was -11.590000. running mean: 1.963400\n",
      "ep 3737: ep_len:330 episode reward: total was 17.440000. running mean: 2.118166\n",
      "ep 3737: ep_len:597 episode reward: total was -19.940000. running mean: 1.897584\n",
      "ep 3737: ep_len:676 episode reward: total was -6.100000. running mean: 1.817608\n",
      "ep 3737: ep_len:603 episode reward: total was -21.900000. running mean: 1.580432\n",
      "ep 3737: ep_len:68 episode reward: total was 32.500000. running mean: 1.889628\n",
      "ep 3737: ep_len:61 episode reward: total was 29.000000. running mean: 2.160732\n",
      "ep 3737: ep_len:103 episode reward: total was 47.000000. running mean: 2.609124\n",
      "ep 3737: ep_len:1186 episode reward: total was 7.290000. running mean: 2.655933\n",
      "ep 3737: ep_len:2889 episode reward: total was -31.850000. running mean: 2.310874\n",
      "epsilon:0.009992 episode_count: 56182. steps_count: 60601277.000000\n",
      "ep 3738: ep_len:640 episode reward: total was -16.660000. running mean: 2.121165\n",
      "ep 3738: ep_len:500 episode reward: total was -3.460000. running mean: 2.065353\n",
      "ep 3738: ep_len:82 episode reward: total was 38.000000. running mean: 2.424700\n",
      "ep 3738: ep_len:3049 episode reward: total was -57.840000. running mean: 1.822053\n",
      "ep 3738: ep_len:1134 episode reward: total was -14.080000. running mean: 1.663032\n",
      "ep 3738: ep_len:906 episode reward: total was 60.230000. running mean: 2.248702\n",
      "ep 3738: ep_len:304 episode reward: total was -5.140000. running mean: 2.174815\n",
      "ep 3738: ep_len:611 episode reward: total was -18.300000. running mean: 1.970067\n",
      "ep 3738: ep_len:866 episode reward: total was 53.180000. running mean: 2.482166\n",
      "ep 3738: ep_len:1034 episode reward: total was 24.960000. running mean: 2.706944\n",
      "ep 3738: ep_len:184 episode reward: total was 84.500000. running mean: 3.524875\n",
      "ep 3738: ep_len:626 episode reward: total was 12.510000. running mean: 3.614726\n",
      "ep 3738: ep_len:2901 episode reward: total was -8.340000. running mean: 3.495179\n",
      "epsilon:0.009992 episode_count: 56195. steps_count: 60614114.000000\n",
      "ep 3739: ep_len:919 episode reward: total was -32.310000. running mean: 3.137127\n",
      "ep 3739: ep_len:1031 episode reward: total was 35.530000. running mean: 3.461056\n",
      "ep 3739: ep_len:72 episode reward: total was 33.000000. running mean: 3.756445\n",
      "ep 3739: ep_len:3011 episode reward: total was -24.660000. running mean: 3.472281\n",
      "ep 3739: ep_len:502 episode reward: total was -2.130000. running mean: 3.416258\n",
      "ep 3739: ep_len:42 episode reward: total was 19.500000. running mean: 3.577095\n",
      "ep 3739: ep_len:892 episode reward: total was 65.240000. running mean: 4.193725\n",
      "ep 3739: ep_len:3692 episode reward: total was -31.550000. running mean: 3.836287\n",
      "ep 3739: ep_len:1216 episode reward: total was -14.270000. running mean: 3.655224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3739: ep_len:780 episode reward: total was 26.750000. running mean: 3.886172\n",
      "ep 3739: ep_len:999 episode reward: total was 35.290000. running mean: 4.200210\n",
      "ep 3739: ep_len:75 episode reward: total was 36.000000. running mean: 4.518208\n",
      "ep 3739: ep_len:729 episode reward: total was 5.700000. running mean: 4.530026\n",
      "ep 3739: ep_len:2825 episode reward: total was -14.340000. running mean: 4.341326\n",
      "epsilon:0.009992 episode_count: 56209. steps_count: 60630899.000000\n",
      "ep 3740: ep_len:625 episode reward: total was 11.580000. running mean: 4.413713\n",
      "ep 3740: ep_len:968 episode reward: total was -0.740000. running mean: 4.362176\n",
      "ep 3740: ep_len:68 episode reward: total was 32.500000. running mean: 4.643554\n",
      "ep 3740: ep_len:2991 episode reward: total was -7.550000. running mean: 4.521618\n",
      "ep 3740: ep_len:612 episode reward: total was -5.160000. running mean: 4.424802\n",
      "ep 3740: ep_len:40 episode reward: total was 18.500000. running mean: 4.565554\n",
      "ep 3740: ep_len:97 episode reward: total was 45.500000. running mean: 4.974899\n",
      "ep 3740: ep_len:64 episode reward: total was 30.500000. running mean: 5.230150\n",
      "ep 3740: ep_len:987 episode reward: total was -82.210000. running mean: 4.355748\n",
      "ep 3740: ep_len:347 episode reward: total was 13.780000. running mean: 4.449991\n",
      "ep 3740: ep_len:1107 episode reward: total was -81.930000. running mean: 3.586191\n",
      "ep 3740: ep_len:710 episode reward: total was -9.600000. running mean: 3.454329\n",
      "ep 3740: ep_len:627 episode reward: total was 14.140000. running mean: 3.561185\n",
      "ep 3740: ep_len:847 episode reward: total was 12.610000. running mean: 3.651674\n",
      "ep 3740: ep_len:2771 episode reward: total was -0.150000. running mean: 3.613657\n",
      "epsilon:0.009992 episode_count: 56224. steps_count: 60643760.000000\n",
      "ep 3741: ep_len:1110 episode reward: total was 2.850000. running mean: 3.606020\n",
      "ep 3741: ep_len:991 episode reward: total was 28.150000. running mean: 3.851460\n",
      "ep 3741: ep_len:2924 episode reward: total was -22.460000. running mean: 3.588346\n",
      "ep 3741: ep_len:657 episode reward: total was 12.030000. running mean: 3.672762\n",
      "ep 3741: ep_len:36 episode reward: total was 16.500000. running mean: 3.801034\n",
      "ep 3741: ep_len:695 episode reward: total was -14.470000. running mean: 3.618324\n",
      "ep 3741: ep_len:335 episode reward: total was 17.490000. running mean: 3.757041\n",
      "ep 3741: ep_len:837 episode reward: total was -11.240000. running mean: 3.607070\n",
      "ep 3741: ep_len:842 episode reward: total was 47.830000. running mean: 4.049300\n",
      "ep 3741: ep_len:575 episode reward: total was 43.930000. running mean: 4.448107\n",
      "ep 3741: ep_len:78 episode reward: total was 37.500000. running mean: 4.778626\n",
      "ep 3741: ep_len:53 episode reward: total was 23.500000. running mean: 4.965839\n",
      "ep 3741: ep_len:689 episode reward: total was -73.440000. running mean: 4.181781\n",
      "ep 3741: ep_len:2868 episode reward: total was -12.070000. running mean: 4.019263\n",
      "epsilon:0.009992 episode_count: 56238. steps_count: 60656450.000000\n",
      "ep 3742: ep_len:2122 episode reward: total was -1113.350000. running mean: -7.154429\n",
      "ep 3742: ep_len:998 episode reward: total was 56.530000. running mean: -6.517585\n",
      "ep 3742: ep_len:3030 episode reward: total was -75.280000. running mean: -7.205209\n",
      "ep 3742: ep_len:1212 episode reward: total was 9.970000. running mean: -7.033457\n",
      "ep 3742: ep_len:92 episode reward: total was 44.500000. running mean: -6.518123\n",
      "ep 3742: ep_len:63 episode reward: total was 30.000000. running mean: -6.152941\n",
      "ep 3742: ep_len:678 episode reward: total was 18.650000. running mean: -5.904912\n",
      "ep 3742: ep_len:3580 episode reward: total was -95.140000. running mean: -6.797263\n",
      "ep 3742: ep_len:1203 episode reward: total was -3.780000. running mean: -6.767090\n",
      "ep 3742: ep_len:660 episode reward: total was 11.400000. running mean: -6.585419\n",
      "ep 3742: ep_len:764 episode reward: total was -8.690000. running mean: -6.606465\n",
      "ep 3742: ep_len:120 episode reward: total was 57.000000. running mean: -5.970400\n",
      "ep 3742: ep_len:784 episode reward: total was 17.510000. running mean: -5.735596\n",
      "ep 3742: ep_len:2759 episode reward: total was -3.000000. running mean: -5.708240\n",
      "ep 3742: ep_len:45 episode reward: total was 19.500000. running mean: -5.456158\n",
      "epsilon:0.009992 episode_count: 56253. steps_count: 60674560.000000\n",
      "ep 3743: ep_len:639 episode reward: total was -17.010000. running mean: -5.571697\n",
      "ep 3743: ep_len:1254 episode reward: total was -29.040000. running mean: -5.806380\n",
      "ep 3743: ep_len:3021 episode reward: total was 6.060000. running mean: -5.687716\n",
      "ep 3743: ep_len:714 episode reward: total was -11.210000. running mean: -5.742939\n",
      "ep 3743: ep_len:38 episode reward: total was 16.000000. running mean: -5.525509\n",
      "ep 3743: ep_len:125 episode reward: total was 56.500000. running mean: -4.905254\n",
      "ep 3743: ep_len:1418 episode reward: total was -198.790000. running mean: -6.844102\n",
      "ep 3743: ep_len:620 episode reward: total was 14.020000. running mean: -6.635461\n",
      "ep 3743: ep_len:547 episode reward: total was -0.270000. running mean: -6.571806\n",
      "ep 3743: ep_len:770 episode reward: total was -19.830000. running mean: -6.704388\n",
      "ep 3743: ep_len:767 episode reward: total was -0.580000. running mean: -6.643144\n",
      "ep 3743: ep_len:154 episode reward: total was 75.500000. running mean: -5.821713\n",
      "ep 3743: ep_len:39 episode reward: total was 18.000000. running mean: -5.583495\n",
      "ep 3743: ep_len:1485 episode reward: total was 20.650000. running mean: -5.321160\n",
      "ep 3743: ep_len:35 episode reward: total was 16.000000. running mean: -5.107949\n",
      "epsilon:0.009992 episode_count: 56268. steps_count: 60686186.000000\n",
      "ep 3744: ep_len:500 episode reward: total was 9.430000. running mean: -4.962569\n",
      "ep 3744: ep_len:1006 episode reward: total was 30.930000. running mean: -4.603644\n",
      "ep 3744: ep_len:51 episode reward: total was 24.000000. running mean: -4.317607\n",
      "ep 3744: ep_len:2887 episode reward: total was -39.460000. running mean: -4.669031\n",
      "ep 3744: ep_len:1467 episode reward: total was -0.230000. running mean: -4.624641\n",
      "ep 3744: ep_len:50 episode reward: total was 23.500000. running mean: -4.343394\n",
      "ep 3744: ep_len:56 episode reward: total was 25.000000. running mean: -4.049961\n",
      "ep 3744: ep_len:1034 episode reward: total was -59.520000. running mean: -4.604661\n",
      "ep 3744: ep_len:3789 episode reward: total was -10.500000. running mean: -4.663614\n",
      "ep 3744: ep_len:629 episode reward: total was -15.030000. running mean: -4.767278\n",
      "ep 3744: ep_len:635 episode reward: total was 6.710000. running mean: -4.652505\n",
      "ep 3744: ep_len:1136 episode reward: total was -9.990000. running mean: -4.705880\n",
      "ep 3744: ep_len:141 episode reward: total was 67.500000. running mean: -3.983822\n",
      "ep 3744: ep_len:1073 episode reward: total was 18.950000. running mean: -3.754483\n",
      "ep 3744: ep_len:2859 episode reward: total was -12.800000. running mean: -3.844938\n",
      "ep 3744: ep_len:33 episode reward: total was 15.000000. running mean: -3.656489\n",
      "epsilon:0.009992 episode_count: 56284. steps_count: 60703532.000000\n",
      "ep 3745: ep_len:632 episode reward: total was 13.890000. running mean: -3.481024\n",
      "ep 3745: ep_len:1604 episode reward: total was -29.090000. running mean: -3.737114\n",
      "ep 3745: ep_len:3079 episode reward: total was -10.130000. running mean: -3.801043\n",
      "ep 3745: ep_len:681 episode reward: total was -3.460000. running mean: -3.797632\n",
      "ep 3745: ep_len:159 episode reward: total was 78.000000. running mean: -2.979656\n",
      "ep 3745: ep_len:1515 episode reward: total was -452.190000. running mean: -7.471760\n",
      "ep 3745: ep_len:346 episode reward: total was 16.590000. running mean: -7.231142\n",
      "ep 3745: ep_len:500 episode reward: total was 35.180000. running mean: -6.807030\n",
      "ep 3745: ep_len:859 episode reward: total was 10.310000. running mean: -6.635860\n",
      "ep 3745: ep_len:1116 episode reward: total was -8.200000. running mean: -6.651502\n",
      "ep 3745: ep_len:94 episode reward: total was 45.500000. running mean: -6.129987\n",
      "ep 3745: ep_len:1010 episode reward: total was 8.340000. running mean: -5.985287\n",
      "ep 3745: ep_len:2925 episode reward: total was -3.600000. running mean: -5.961434\n",
      "epsilon:0.009992 episode_count: 56297. steps_count: 60718052.000000\n",
      "ep 3746: ep_len:1192 episode reward: total was 22.320000. running mean: -5.678620\n",
      "ep 3746: ep_len:1588 episode reward: total was -18.940000. running mean: -5.811233\n",
      "ep 3746: ep_len:3029 episode reward: total was -5.610000. running mean: -5.809221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3746: ep_len:500 episode reward: total was -7.880000. running mean: -5.829929\n",
      "ep 3746: ep_len:1403 episode reward: total was -74.490000. running mean: -6.516529\n",
      "ep 3746: ep_len:3595 episode reward: total was -1.210000. running mean: -6.463464\n",
      "ep 3746: ep_len:675 episode reward: total was -6.550000. running mean: -6.464330\n",
      "ep 3746: ep_len:7437 episode reward: total was 31.600000. running mean: -6.083686\n",
      "ep 3746: ep_len:592 episode reward: total was -2.330000. running mean: -6.046149\n",
      "ep 3746: ep_len:200 episode reward: total was 95.500000. running mean: -5.030688\n",
      "ep 3746: ep_len:24 episode reward: total was 10.500000. running mean: -4.875381\n",
      "ep 3746: ep_len:95 episode reward: total was 44.500000. running mean: -4.381627\n",
      "ep 3746: ep_len:1062 episode reward: total was -86.760000. running mean: -5.205411\n",
      "ep 3746: ep_len:2842 episode reward: total was -3.360000. running mean: -5.186957\n",
      "epsilon:0.009992 episode_count: 56311. steps_count: 60742286.000000\n",
      "ep 3747: ep_len:791 episode reward: total was -63.820000. running mean: -5.773287\n",
      "ep 3747: ep_len:500 episode reward: total was 15.620000. running mean: -5.559354\n",
      "ep 3747: ep_len:72 episode reward: total was 34.500000. running mean: -5.158761\n",
      "ep 3747: ep_len:2915 episode reward: total was -42.160000. running mean: -5.528773\n",
      "ep 3747: ep_len:655 episode reward: total was 21.420000. running mean: -5.259285\n",
      "ep 3747: ep_len:65 episode reward: total was 28.000000. running mean: -4.926693\n",
      "ep 3747: ep_len:1351 episode reward: total was -188.660000. running mean: -6.764026\n",
      "ep 3747: ep_len:3587 episode reward: total was 0.730000. running mean: -6.689085\n",
      "ep 3747: ep_len:600 episode reward: total was 7.280000. running mean: -6.549395\n",
      "ep 3747: ep_len:691 episode reward: total was 16.300000. running mean: -6.320901\n",
      "ep 3747: ep_len:854 episode reward: total was -5.650000. running mean: -6.314192\n",
      "ep 3747: ep_len:57 episode reward: total was 27.000000. running mean: -5.981050\n",
      "ep 3747: ep_len:1494 episode reward: total was 14.710000. running mean: -5.774139\n",
      "ep 3747: ep_len:2834 episode reward: total was -5.830000. running mean: -5.774698\n",
      "epsilon:0.009992 episode_count: 56325. steps_count: 60758752.000000\n",
      "ep 3748: ep_len:869 episode reward: total was 12.650000. running mean: -5.590451\n",
      "ep 3748: ep_len:972 episode reward: total was 30.710000. running mean: -5.227446\n",
      "ep 3748: ep_len:3035 episode reward: total was -24.880000. running mean: -5.423972\n",
      "ep 3748: ep_len:500 episode reward: total was 4.530000. running mean: -5.324432\n",
      "ep 3748: ep_len:65 episode reward: total was 31.000000. running mean: -4.961188\n",
      "ep 3748: ep_len:101 episode reward: total was 46.000000. running mean: -4.451576\n",
      "ep 3748: ep_len:946 episode reward: total was 5.710000. running mean: -4.349960\n",
      "ep 3748: ep_len:3808 episode reward: total was -64.410000. running mean: -4.950561\n",
      "ep 3748: ep_len:583 episode reward: total was -5.450000. running mean: -4.955555\n",
      "ep 3748: ep_len:7320 episode reward: total was -92.610000. running mean: -5.832099\n",
      "ep 3748: ep_len:686 episode reward: total was 16.080000. running mean: -5.612978\n",
      "ep 3748: ep_len:68 episode reward: total was 32.500000. running mean: -5.231849\n",
      "ep 3748: ep_len:121 episode reward: total was 59.000000. running mean: -4.589530\n",
      "ep 3748: ep_len:553 episode reward: total was 2.230000. running mean: -4.521335\n",
      "ep 3748: ep_len:2729 episode reward: total was -22.830000. running mean: -4.704422\n",
      "ep 3748: ep_len:42 episode reward: total was 19.500000. running mean: -4.462377\n",
      "epsilon:0.009992 episode_count: 56341. steps_count: 60781150.000000\n",
      "ep 3749: ep_len:930 episode reward: total was -32.390000. running mean: -4.741654\n",
      "ep 3749: ep_len:965 episode reward: total was 23.930000. running mean: -4.454937\n",
      "ep 3749: ep_len:95 episode reward: total was 44.500000. running mean: -3.965388\n",
      "ep 3749: ep_len:552 episode reward: total was -39.090000. running mean: -4.316634\n",
      "ep 3749: ep_len:28 episode reward: total was 11.000000. running mean: -4.163467\n",
      "ep 3749: ep_len:1169 episode reward: total was 19.400000. running mean: -3.927833\n",
      "ep 3749: ep_len:643 episode reward: total was 21.630000. running mean: -3.672254\n",
      "ep 3749: ep_len:500 episode reward: total was 18.710000. running mean: -3.448432\n",
      "ep 3749: ep_len:850 episode reward: total was 52.860000. running mean: -2.885348\n",
      "ep 3749: ep_len:1513 episode reward: total was -12.920000. running mean: -2.985694\n",
      "ep 3749: ep_len:40 episode reward: total was 18.500000. running mean: -2.770837\n",
      "ep 3749: ep_len:989 episode reward: total was 23.160000. running mean: -2.511529\n",
      "ep 3749: ep_len:2867 episode reward: total was 20.640000. running mean: -2.280013\n",
      "ep 3749: ep_len:57 episode reward: total was 25.500000. running mean: -2.002213\n",
      "epsilon:0.009992 episode_count: 56355. steps_count: 60792348.000000\n",
      "ep 3750: ep_len:650 episode reward: total was -15.890000. running mean: -2.141091\n",
      "ep 3750: ep_len:500 episode reward: total was -6.830000. running mean: -2.187980\n",
      "ep 3750: ep_len:2920 episode reward: total was -11.120000. running mean: -2.277301\n",
      "ep 3750: ep_len:500 episode reward: total was 1.480000. running mean: -2.239727\n",
      "ep 3750: ep_len:69 episode reward: total was 33.000000. running mean: -1.887330\n",
      "ep 3750: ep_len:613 episode reward: total was 53.780000. running mean: -1.330657\n",
      "ep 3750: ep_len:681 episode reward: total was 2.880000. running mean: -1.288550\n",
      "ep 3750: ep_len:870 episode reward: total was 11.190000. running mean: -1.163765\n",
      "ep 3750: ep_len:879 episode reward: total was 74.680000. running mean: -0.405327\n",
      "ep 3750: ep_len:700 episode reward: total was 24.630000. running mean: -0.154974\n",
      "ep 3750: ep_len:69 episode reward: total was 33.000000. running mean: 0.176576\n",
      "ep 3750: ep_len:68 episode reward: total was 32.500000. running mean: 0.499810\n",
      "ep 3750: ep_len:500 episode reward: total was 17.820000. running mean: 0.673012\n",
      "ep 3750: ep_len:2796 episode reward: total was -29.630000. running mean: 0.369982\n",
      "epsilon:0.009992 episode_count: 56369. steps_count: 60804163.000000\n",
      "ep 3751: ep_len:876 episode reward: total was 17.710000. running mean: 0.543382\n",
      "ep 3751: ep_len:787 episode reward: total was -27.650000. running mean: 0.261448\n",
      "ep 3751: ep_len:54 episode reward: total was 22.500000. running mean: 0.483834\n",
      "ep 3751: ep_len:2915 episode reward: total was -68.990000. running mean: -0.210905\n",
      "ep 3751: ep_len:800 episode reward: total was -21.950000. running mean: -0.428296\n",
      "ep 3751: ep_len:50 episode reward: total was 23.500000. running mean: -0.189013\n",
      "ep 3751: ep_len:96 episode reward: total was 45.000000. running mean: 0.262878\n",
      "ep 3751: ep_len:52 episode reward: total was 23.000000. running mean: 0.490249\n",
      "ep 3751: ep_len:1463 episode reward: total was -11.210000. running mean: 0.373246\n",
      "ep 3751: ep_len:3685 episode reward: total was -229.670000. running mean: -1.927186\n",
      "ep 3751: ep_len:850 episode reward: total was 9.460000. running mean: -1.813314\n",
      "ep 3751: ep_len:663 episode reward: total was 31.670000. running mean: -1.478481\n",
      "ep 3751: ep_len:1479 episode reward: total was -56.670000. running mean: -2.030396\n",
      "ep 3751: ep_len:139 episode reward: total was 68.000000. running mean: -1.330092\n",
      "ep 3751: ep_len:19 episode reward: total was 8.000000. running mean: -1.236792\n",
      "ep 3751: ep_len:115 episode reward: total was 56.000000. running mean: -0.664424\n",
      "ep 3751: ep_len:653 episode reward: total was 1.310000. running mean: -0.644679\n",
      "ep 3751: ep_len:2871 episode reward: total was -14.950000. running mean: -0.787733\n",
      "epsilon:0.009992 episode_count: 56387. steps_count: 60821730.000000\n",
      "ep 3752: ep_len:1075 episode reward: total was -34.870000. running mean: -1.128555\n",
      "ep 3752: ep_len:929 episode reward: total was 10.410000. running mean: -1.013170\n",
      "ep 3752: ep_len:61 episode reward: total was 29.000000. running mean: -0.713038\n",
      "ep 3752: ep_len:2902 episode reward: total was -54.120000. running mean: -1.247108\n",
      "ep 3752: ep_len:874 episode reward: total was -3.000000. running mean: -1.264637\n",
      "ep 3752: ep_len:113 episode reward: total was 52.000000. running mean: -0.731990\n",
      "ep 3752: ep_len:78 episode reward: total was 36.000000. running mean: -0.364670\n",
      "ep 3752: ep_len:71 episode reward: total was 32.500000. running mean: -0.036024\n",
      "ep 3752: ep_len:643 episode reward: total was -2.830000. running mean: -0.063963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3752: ep_len:643 episode reward: total was 27.050000. running mean: 0.207176\n",
      "ep 3752: ep_len:638 episode reward: total was -25.100000. running mean: -0.045895\n",
      "ep 3752: ep_len:7374 episode reward: total was -71.290000. running mean: -0.758337\n",
      "ep 3752: ep_len:691 episode reward: total was 9.100000. running mean: -0.659753\n",
      "ep 3752: ep_len:66 episode reward: total was 28.500000. running mean: -0.368156\n",
      "ep 3752: ep_len:32 episode reward: total was 14.500000. running mean: -0.219474\n",
      "ep 3752: ep_len:1544 episode reward: total was -2.020000. running mean: -0.237479\n",
      "ep 3752: ep_len:2821 episode reward: total was 1.290000. running mean: -0.222205\n",
      "epsilon:0.009992 episode_count: 56404. steps_count: 60842285.000000\n",
      "ep 3753: ep_len:784 episode reward: total was -95.350000. running mean: -1.173482\n",
      "ep 3753: ep_len:663 episode reward: total was -5.440000. running mean: -1.216148\n",
      "ep 3753: ep_len:3003 episode reward: total was 14.820000. running mean: -1.055786\n",
      "ep 3753: ep_len:1196 episode reward: total was -27.020000. running mean: -1.315428\n",
      "ep 3753: ep_len:111 episode reward: total was 54.000000. running mean: -0.762274\n",
      "ep 3753: ep_len:74 episode reward: total was 35.500000. running mean: -0.399651\n",
      "ep 3753: ep_len:3351 episode reward: total was -2121.480000. running mean: -21.610455\n",
      "ep 3753: ep_len:3937 episode reward: total was -52.510000. running mean: -21.919450\n",
      "ep 3753: ep_len:654 episode reward: total was 1.320000. running mean: -21.687056\n",
      "ep 3753: ep_len:7582 episode reward: total was -61.130000. running mean: -22.081485\n",
      "ep 3753: ep_len:592 episode reward: total was -0.100000. running mean: -21.861670\n",
      "ep 3753: ep_len:61 episode reward: total was 27.500000. running mean: -21.368054\n",
      "ep 3753: ep_len:150 episode reward: total was 73.010000. running mean: -20.424273\n",
      "ep 3753: ep_len:1471 episode reward: total was 20.080000. running mean: -20.019230\n",
      "ep 3753: ep_len:2775 episode reward: total was -11.710000. running mean: -19.936138\n",
      "ep 3753: ep_len:64 episode reward: total was 30.500000. running mean: -19.431777\n",
      "epsilon:0.009992 episode_count: 56420. steps_count: 60868753.000000\n",
      "ep 3754: ep_len:717 episode reward: total was -6.760000. running mean: -19.305059\n",
      "ep 3754: ep_len:675 episode reward: total was -26.140000. running mean: -19.373408\n",
      "ep 3754: ep_len:50 episode reward: total was 23.500000. running mean: -18.944674\n",
      "ep 3754: ep_len:3086 episode reward: total was -91.200000. running mean: -19.667227\n",
      "ep 3754: ep_len:1416 episode reward: total was -1.770000. running mean: -19.488255\n",
      "ep 3754: ep_len:105 episode reward: total was 51.000000. running mean: -18.783373\n",
      "ep 3754: ep_len:61 episode reward: total was 27.500000. running mean: -18.320539\n",
      "ep 3754: ep_len:931 episode reward: total was 63.210000. running mean: -17.505234\n",
      "ep 3754: ep_len:355 episode reward: total was 21.210000. running mean: -17.118081\n",
      "ep 3754: ep_len:531 episode reward: total was 6.150000. running mean: -16.885400\n",
      "ep 3754: ep_len:760 episode reward: total was 9.790000. running mean: -16.618646\n",
      "ep 3754: ep_len:1469 episode reward: total was -4.000000. running mean: -16.492460\n",
      "ep 3754: ep_len:120 episode reward: total was 57.000000. running mean: -15.757535\n",
      "ep 3754: ep_len:1138 episode reward: total was -11.500000. running mean: -15.714960\n",
      "ep 3754: ep_len:2835 episode reward: total was -10.080000. running mean: -15.658610\n",
      "epsilon:0.009992 episode_count: 56435. steps_count: 60883002.000000\n",
      "ep 3755: ep_len:656 episode reward: total was 12.260000. running mean: -15.379424\n",
      "ep 3755: ep_len:948 episode reward: total was 21.740000. running mean: -15.008230\n",
      "ep 3755: ep_len:2989 episode reward: total was 16.270000. running mean: -14.695448\n",
      "ep 3755: ep_len:500 episode reward: total was 12.740000. running mean: -14.421093\n",
      "ep 3755: ep_len:47 episode reward: total was 22.000000. running mean: -14.056882\n",
      "ep 3755: ep_len:1026 episode reward: total was -15.160000. running mean: -14.067913\n",
      "ep 3755: ep_len:3590 episode reward: total was -7.320000. running mean: -14.000434\n",
      "ep 3755: ep_len:937 episode reward: total was -21.860000. running mean: -14.079030\n",
      "ep 3755: ep_len:7315 episode reward: total was 16.240000. running mean: -13.775840\n",
      "ep 3755: ep_len:646 episode reward: total was -22.400000. running mean: -13.862081\n",
      "ep 3755: ep_len:43 episode reward: total was 20.000000. running mean: -13.523461\n",
      "ep 3755: ep_len:610 episode reward: total was -16.750000. running mean: -13.555726\n",
      "ep 3755: ep_len:2834 episode reward: total was -1.480000. running mean: -13.434969\n",
      "ep 3755: ep_len:46 episode reward: total was 20.000000. running mean: -13.100619\n",
      "epsilon:0.009992 episode_count: 56449. steps_count: 60905189.000000\n",
      "ep 3756: ep_len:649 episode reward: total was -1.420000. running mean: -12.983813\n",
      "ep 3756: ep_len:729 episode reward: total was -10.770000. running mean: -12.961675\n",
      "ep 3756: ep_len:3003 episode reward: total was -27.300000. running mean: -13.105058\n",
      "ep 3756: ep_len:822 episode reward: total was 22.220000. running mean: -12.751807\n",
      "ep 3756: ep_len:58 episode reward: total was 26.000000. running mean: -12.364289\n",
      "ep 3756: ep_len:1332 episode reward: total was -10.080000. running mean: -12.341446\n",
      "ep 3756: ep_len:3883 episode reward: total was -53.880000. running mean: -12.756832\n",
      "ep 3756: ep_len:688 episode reward: total was -10.460000. running mean: -12.733864\n",
      "ep 3756: ep_len:753 episode reward: total was 9.750000. running mean: -12.509025\n",
      "ep 3756: ep_len:588 episode reward: total was 38.610000. running mean: -11.997835\n",
      "ep 3756: ep_len:52 episode reward: total was 24.500000. running mean: -11.632856\n",
      "ep 3756: ep_len:107 episode reward: total was 49.000000. running mean: -11.026528\n",
      "ep 3756: ep_len:656 episode reward: total was 12.690000. running mean: -10.789362\n",
      "ep 3756: ep_len:2877 episode reward: total was -24.410000. running mean: -10.925569\n",
      "epsilon:0.009992 episode_count: 56463. steps_count: 60921386.000000\n",
      "ep 3757: ep_len:1082 episode reward: total was -9.090000. running mean: -10.907213\n",
      "ep 3757: ep_len:758 episode reward: total was 1.570000. running mean: -10.782441\n",
      "ep 3757: ep_len:79 episode reward: total was 36.500000. running mean: -10.309617\n",
      "ep 3757: ep_len:2950 episode reward: total was -19.390000. running mean: -10.400420\n",
      "ep 3757: ep_len:1217 episode reward: total was -59.520000. running mean: -10.891616\n",
      "ep 3757: ep_len:57 episode reward: total was 27.000000. running mean: -10.512700\n",
      "ep 3757: ep_len:58 episode reward: total was 26.000000. running mean: -10.147573\n",
      "ep 3757: ep_len:1399 episode reward: total was -26.580000. running mean: -10.311897\n",
      "ep 3757: ep_len:3664 episode reward: total was 8.830000. running mean: -10.120478\n",
      "ep 3757: ep_len:573 episode reward: total was 0.510000. running mean: -10.014174\n",
      "ep 3757: ep_len:828 episode reward: total was 41.650000. running mean: -9.497532\n",
      "ep 3757: ep_len:635 episode reward: total was -1.470000. running mean: -9.417257\n",
      "ep 3757: ep_len:47 episode reward: total was 22.000000. running mean: -9.103084\n",
      "ep 3757: ep_len:1093 episode reward: total was 21.810000. running mean: -8.793953\n",
      "ep 3757: ep_len:2942 episode reward: total was -31.750000. running mean: -9.023514\n",
      "ep 3757: ep_len:52 episode reward: total was 25.510000. running mean: -8.678178\n",
      "epsilon:0.009992 episode_count: 56479. steps_count: 60938820.000000\n",
      "ep 3758: ep_len:641 episode reward: total was -8.170000. running mean: -8.673097\n",
      "ep 3758: ep_len:784 episode reward: total was -29.930000. running mean: -8.885666\n",
      "ep 3758: ep_len:2995 episode reward: total was -60.770000. running mean: -9.404509\n",
      "ep 3758: ep_len:502 episode reward: total was 6.750000. running mean: -9.242964\n",
      "ep 3758: ep_len:34 episode reward: total was 14.000000. running mean: -9.010534\n",
      "ep 3758: ep_len:79 episode reward: total was 38.000000. running mean: -8.540429\n",
      "ep 3758: ep_len:614 episode reward: total was 54.280000. running mean: -7.912225\n",
      "ep 3758: ep_len:4083 episode reward: total was -17.760000. running mean: -8.010702\n",
      "ep 3758: ep_len:671 episode reward: total was -23.270000. running mean: -8.163295\n",
      "ep 3758: ep_len:880 episode reward: total was 71.510000. running mean: -7.366562\n",
      "ep 3758: ep_len:917 episode reward: total was 19.100000. running mean: -7.101897\n",
      "ep 3758: ep_len:88 episode reward: total was 41.000000. running mean: -6.620878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3758: ep_len:1111 episode reward: total was 35.130000. running mean: -6.203369\n",
      "ep 3758: ep_len:2792 episode reward: total was -42.460000. running mean: -6.565935\n",
      "ep 3758: ep_len:54 episode reward: total was 21.000000. running mean: -6.290276\n",
      "epsilon:0.009992 episode_count: 56494. steps_count: 60955065.000000\n",
      "ep 3759: ep_len:1121 episode reward: total was -21.400000. running mean: -6.441373\n",
      "ep 3759: ep_len:961 episode reward: total was 14.980000. running mean: -6.227160\n",
      "ep 3759: ep_len:50 episode reward: total was 20.500000. running mean: -5.959888\n",
      "ep 3759: ep_len:3063 episode reward: total was -73.680000. running mean: -6.637089\n",
      "ep 3759: ep_len:531 episode reward: total was -16.070000. running mean: -6.731418\n",
      "ep 3759: ep_len:25 episode reward: total was 9.500000. running mean: -6.569104\n",
      "ep 3759: ep_len:115 episode reward: total was 51.500000. running mean: -5.988413\n",
      "ep 3759: ep_len:672 episode reward: total was 8.500000. running mean: -5.843529\n",
      "ep 3759: ep_len:500 episode reward: total was 18.140000. running mean: -5.603694\n",
      "ep 3759: ep_len:1113 episode reward: total was -0.300000. running mean: -5.550657\n",
      "ep 3759: ep_len:778 episode reward: total was 7.310000. running mean: -5.422050\n",
      "ep 3759: ep_len:613 episode reward: total was 45.170000. running mean: -4.916130\n",
      "ep 3759: ep_len:60 episode reward: total was 28.500000. running mean: -4.581968\n",
      "ep 3759: ep_len:626 episode reward: total was -10.590000. running mean: -4.642049\n",
      "ep 3759: ep_len:2710 episode reward: total was -14.420000. running mean: -4.739828\n",
      "epsilon:0.009992 episode_count: 56509. steps_count: 60968003.000000\n",
      "ep 3760: ep_len:1441 episode reward: total was -1.980000. running mean: -4.712230\n",
      "ep 3760: ep_len:778 episode reward: total was -1.660000. running mean: -4.681708\n",
      "ep 3760: ep_len:45 episode reward: total was 21.000000. running mean: -4.424890\n",
      "ep 3760: ep_len:3009 episode reward: total was -54.780000. running mean: -4.928442\n",
      "ep 3760: ep_len:654 episode reward: total was 20.920000. running mean: -4.669957\n",
      "ep 3760: ep_len:98 episode reward: total was 46.000000. running mean: -4.163258\n",
      "ep 3760: ep_len:70 episode reward: total was 32.000000. running mean: -3.801625\n",
      "ep 3760: ep_len:815 episode reward: total was 37.610000. running mean: -3.387509\n",
      "ep 3760: ep_len:3644 episode reward: total was 9.260000. running mean: -3.261034\n",
      "ep 3760: ep_len:583 episode reward: total was -14.600000. running mean: -3.374423\n",
      "ep 3760: ep_len:901 episode reward: total was 72.390000. running mean: -2.616779\n",
      "ep 3760: ep_len:654 episode reward: total was -6.620000. running mean: -2.656811\n",
      "ep 3760: ep_len:98 episode reward: total was 46.000000. running mean: -2.170243\n",
      "ep 3760: ep_len:679 episode reward: total was -2.470000. running mean: -2.173241\n",
      "ep 3760: ep_len:2913 episode reward: total was -8.380000. running mean: -2.235308\n",
      "epsilon:0.009992 episode_count: 56524. steps_count: 60984385.000000\n",
      "ep 3761: ep_len:1140 episode reward: total was -19.070000. running mean: -2.403655\n",
      "ep 3761: ep_len:676 episode reward: total was -30.350000. running mean: -2.683119\n",
      "ep 3761: ep_len:3073 episode reward: total was 34.160000. running mean: -2.314688\n",
      "ep 3761: ep_len:515 episode reward: total was 18.070000. running mean: -2.110841\n",
      "ep 3761: ep_len:49 episode reward: total was 23.000000. running mean: -1.859732\n",
      "ep 3761: ep_len:1023 episode reward: total was 10.060000. running mean: -1.740535\n",
      "ep 3761: ep_len:321 episode reward: total was 16.550000. running mean: -1.557630\n",
      "ep 3761: ep_len:570 episode reward: total was 13.650000. running mean: -1.405553\n",
      "ep 3761: ep_len:858 episode reward: total was 61.120000. running mean: -0.780298\n",
      "ep 3761: ep_len:544 episode reward: total was 35.870000. running mean: -0.413795\n",
      "ep 3761: ep_len:58 episode reward: total was 27.500000. running mean: -0.134657\n",
      "ep 3761: ep_len:1184 episode reward: total was -43.880000. running mean: -0.572110\n",
      "ep 3761: ep_len:2849 episode reward: total was -31.770000. running mean: -0.884089\n",
      "epsilon:0.009992 episode_count: 56537. steps_count: 60997245.000000\n",
      "ep 3762: ep_len:770 episode reward: total was -46.610000. running mean: -1.341348\n",
      "ep 3762: ep_len:500 episode reward: total was 14.520000. running mean: -1.182735\n",
      "ep 3762: ep_len:2986 episode reward: total was -693.550000. running mean: -8.106407\n",
      "ep 3762: ep_len:701 episode reward: total was 25.940000. running mean: -7.765943\n",
      "ep 3762: ep_len:54 episode reward: total was 25.500000. running mean: -7.433284\n",
      "ep 3762: ep_len:1431 episode reward: total was -8.110000. running mean: -7.440051\n",
      "ep 3762: ep_len:620 episode reward: total was 23.300000. running mean: -7.132651\n",
      "ep 3762: ep_len:671 episode reward: total was 0.480000. running mean: -7.056524\n",
      "ep 3762: ep_len:753 episode reward: total was 4.670000. running mean: -6.939259\n",
      "ep 3762: ep_len:692 episode reward: total was -15.570000. running mean: -7.025566\n",
      "ep 3762: ep_len:89 episode reward: total was 43.000000. running mean: -6.525311\n",
      "ep 3762: ep_len:1105 episode reward: total was -18.410000. running mean: -6.644157\n",
      "ep 3762: ep_len:2804 episode reward: total was -23.080000. running mean: -6.808516\n",
      "ep 3762: ep_len:44 episode reward: total was 19.000000. running mean: -6.550431\n",
      "epsilon:0.009992 episode_count: 56551. steps_count: 61010465.000000\n",
      "ep 3763: ep_len:2089 episode reward: total was -634.580000. running mean: -12.830726\n",
      "ep 3763: ep_len:966 episode reward: total was 33.460000. running mean: -12.367819\n",
      "ep 3763: ep_len:3130 episode reward: total was -21.960000. running mean: -12.463741\n",
      "ep 3763: ep_len:569 episode reward: total was 6.530000. running mean: -12.273804\n",
      "ep 3763: ep_len:40 episode reward: total was 17.000000. running mean: -11.981066\n",
      "ep 3763: ep_len:1495 episode reward: total was 16.990000. running mean: -11.691355\n",
      "ep 3763: ep_len:353 episode reward: total was 23.210000. running mean: -11.342341\n",
      "ep 3763: ep_len:651 episode reward: total was -32.040000. running mean: -11.549318\n",
      "ep 3763: ep_len:914 episode reward: total was 45.370000. running mean: -10.980125\n",
      "ep 3763: ep_len:500 episode reward: total was -14.350000. running mean: -11.013823\n",
      "ep 3763: ep_len:74 episode reward: total was 35.500000. running mean: -10.548685\n",
      "ep 3763: ep_len:54 episode reward: total was 25.500000. running mean: -10.188198\n",
      "ep 3763: ep_len:1144 episode reward: total was -17.010000. running mean: -10.256416\n",
      "ep 3763: ep_len:2850 episode reward: total was -25.750000. running mean: -10.411352\n",
      "epsilon:0.009992 episode_count: 56565. steps_count: 61025294.000000\n",
      "ep 3764: ep_len:1040 episode reward: total was -15.020000. running mean: -10.457439\n",
      "ep 3764: ep_len:1584 episode reward: total was -76.520000. running mean: -11.118064\n",
      "ep 3764: ep_len:41 episode reward: total was 19.000000. running mean: -10.816884\n",
      "ep 3764: ep_len:2966 episode reward: total was -10.130000. running mean: -10.810015\n",
      "ep 3764: ep_len:845 episode reward: total was 39.180000. running mean: -10.310115\n",
      "ep 3764: ep_len:99 episode reward: total was 43.500000. running mean: -9.772014\n",
      "ep 3764: ep_len:51 episode reward: total was 22.500000. running mean: -9.449293\n",
      "ep 3764: ep_len:755 episode reward: total was 9.920000. running mean: -9.255600\n",
      "ep 3764: ep_len:4001 episode reward: total was -1905.620000. running mean: -28.219244\n",
      "ep 3764: ep_len:677 episode reward: total was -25.600000. running mean: -28.193052\n",
      "ep 3764: ep_len:781 episode reward: total was 26.450000. running mean: -27.646622\n",
      "ep 3764: ep_len:500 episode reward: total was 42.410000. running mean: -26.946055\n",
      "ep 3764: ep_len:56 episode reward: total was 26.500000. running mean: -26.411595\n",
      "ep 3764: ep_len:885 episode reward: total was -31.720000. running mean: -26.464679\n",
      "ep 3764: ep_len:2815 episode reward: total was -9.660000. running mean: -26.296632\n",
      "ep 3764: ep_len:59 episode reward: total was 28.000000. running mean: -25.753666\n",
      "epsilon:0.009992 episode_count: 56581. steps_count: 61042449.000000\n",
      "ep 3765: ep_len:666 episode reward: total was -19.000000. running mean: -25.686129\n",
      "ep 3765: ep_len:947 episode reward: total was 15.640000. running mean: -25.272868\n",
      "ep 3765: ep_len:3031 episode reward: total was -30.550000. running mean: -25.325639\n",
      "ep 3765: ep_len:639 episode reward: total was 40.430000. running mean: -24.668083\n",
      "ep 3765: ep_len:106 episode reward: total was 50.000000. running mean: -23.921402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3765: ep_len:1133 episode reward: total was 11.020000. running mean: -23.571988\n",
      "ep 3765: ep_len:357 episode reward: total was 11.160000. running mean: -23.224668\n",
      "ep 3765: ep_len:1294 episode reward: total was -65.000000. running mean: -23.642421\n",
      "ep 3765: ep_len:663 episode reward: total was 13.260000. running mean: -23.273397\n",
      "ep 3765: ep_len:663 episode reward: total was 3.430000. running mean: -23.006363\n",
      "ep 3765: ep_len:58 episode reward: total was 27.500000. running mean: -22.501299\n",
      "ep 3765: ep_len:92 episode reward: total was 44.500000. running mean: -21.831286\n",
      "ep 3765: ep_len:1041 episode reward: total was 12.750000. running mean: -21.485474\n",
      "ep 3765: ep_len:2834 episode reward: total was 6.690000. running mean: -21.203719\n",
      "ep 3765: ep_len:64 episode reward: total was 30.500000. running mean: -20.686682\n",
      "epsilon:0.009992 episode_count: 56596. steps_count: 61056037.000000\n",
      "ep 3766: ep_len:1040 episode reward: total was -4.860000. running mean: -20.528415\n",
      "ep 3766: ep_len:708 episode reward: total was -8.380000. running mean: -20.406931\n",
      "ep 3766: ep_len:2912 episode reward: total was -9.730000. running mean: -20.300161\n",
      "ep 3766: ep_len:672 episode reward: total was 12.540000. running mean: -19.971760\n",
      "ep 3766: ep_len:47 episode reward: total was 20.500000. running mean: -19.567042\n",
      "ep 3766: ep_len:143 episode reward: total was 68.500000. running mean: -18.686372\n",
      "ep 3766: ep_len:100 episode reward: total was 47.000000. running mean: -18.029508\n",
      "ep 3766: ep_len:64 episode reward: total was 29.000000. running mean: -17.559213\n",
      "ep 3766: ep_len:642 episode reward: total was 4.740000. running mean: -17.336221\n",
      "ep 3766: ep_len:367 episode reward: total was 21.850000. running mean: -16.944359\n",
      "ep 3766: ep_len:763 episode reward: total was -26.890000. running mean: -17.043815\n",
      "ep 3766: ep_len:771 episode reward: total was 20.010000. running mean: -16.673277\n",
      "ep 3766: ep_len:1476 episode reward: total was -13.450000. running mean: -16.641044\n",
      "ep 3766: ep_len:1027 episode reward: total was -42.420000. running mean: -16.898834\n",
      "ep 3766: ep_len:46 episode reward: total was 21.500000. running mean: -16.514845\n",
      "ep 3766: ep_len:58 episode reward: total was 27.500000. running mean: -16.074697\n",
      "epsilon:0.009992 episode_count: 56612. steps_count: 61066873.000000\n",
      "ep 3767: ep_len:761 episode reward: total was -69.320000. running mean: -16.607150\n",
      "ep 3767: ep_len:796 episode reward: total was -6.500000. running mean: -16.506078\n",
      "ep 3767: ep_len:2982 episode reward: total was -3.610000. running mean: -16.377118\n",
      "ep 3767: ep_len:1155 episode reward: total was -28.010000. running mean: -16.493446\n",
      "ep 3767: ep_len:770 episode reward: total was -2.570000. running mean: -16.354212\n",
      "ep 3767: ep_len:646 episode reward: total was 8.010000. running mean: -16.110570\n",
      "ep 3767: ep_len:1494 episode reward: total was -59.890000. running mean: -16.548364\n",
      "ep 3767: ep_len:751 episode reward: total was 27.830000. running mean: -16.104581\n",
      "ep 3767: ep_len:644 episode reward: total was 4.250000. running mean: -15.901035\n",
      "ep 3767: ep_len:52 episode reward: total was 24.500000. running mean: -15.497024\n",
      "ep 3767: ep_len:115 episode reward: total was 56.000000. running mean: -14.782054\n",
      "ep 3767: ep_len:535 episode reward: total was 11.040000. running mean: -14.523834\n",
      "ep 3767: ep_len:2949 episode reward: total was -1.100000. running mean: -14.389595\n",
      "epsilon:0.009992 episode_count: 56625. steps_count: 61080523.000000\n",
      "ep 3768: ep_len:838 episode reward: total was 26.820000. running mean: -13.977499\n",
      "ep 3768: ep_len:820 episode reward: total was 14.870000. running mean: -13.689024\n",
      "ep 3768: ep_len:3066 episode reward: total was -8.280000. running mean: -13.634934\n",
      "ep 3768: ep_len:500 episode reward: total was -0.640000. running mean: -13.504985\n",
      "ep 3768: ep_len:166 episode reward: total was 78.500000. running mean: -12.584935\n",
      "ep 3768: ep_len:1071 episode reward: total was -1.580000. running mean: -12.474886\n",
      "ep 3768: ep_len:3869 episode reward: total was -32.790000. running mean: -12.678037\n",
      "ep 3768: ep_len:553 episode reward: total was -41.100000. running mean: -12.962256\n",
      "ep 3768: ep_len:7210 episode reward: total was 50.780000. running mean: -12.324834\n",
      "ep 3768: ep_len:1475 episode reward: total was 9.560000. running mean: -12.105985\n",
      "ep 3768: ep_len:101 episode reward: total was 46.000000. running mean: -11.524926\n",
      "ep 3768: ep_len:30 episode reward: total was 13.500000. running mean: -11.274676\n",
      "ep 3768: ep_len:104 episode reward: total was 49.000000. running mean: -10.671930\n",
      "ep 3768: ep_len:613 episode reward: total was -15.250000. running mean: -10.717710\n",
      "ep 3768: ep_len:2847 episode reward: total was -1.320000. running mean: -10.623733\n",
      "epsilon:0.009992 episode_count: 56640. steps_count: 61103786.000000\n",
      "ep 3769: ep_len:840 episode reward: total was -51.590000. running mean: -11.033396\n",
      "ep 3769: ep_len:718 episode reward: total was -6.010000. running mean: -10.983162\n",
      "ep 3769: ep_len:70 episode reward: total was 33.500000. running mean: -10.538330\n",
      "ep 3769: ep_len:2999 episode reward: total was -11.650000. running mean: -10.549447\n",
      "ep 3769: ep_len:4534 episode reward: total was -922.210000. running mean: -19.666052\n",
      "ep 3769: ep_len:46 episode reward: total was 20.000000. running mean: -19.269392\n",
      "ep 3769: ep_len:833 episode reward: total was 50.140000. running mean: -18.575298\n",
      "ep 3769: ep_len:3977 episode reward: total was -21.570000. running mean: -18.605245\n",
      "ep 3769: ep_len:601 episode reward: total was 16.250000. running mean: -18.256693\n",
      "ep 3769: ep_len:659 episode reward: total was 23.390000. running mean: -17.840226\n",
      "ep 3769: ep_len:1041 episode reward: total was 10.090000. running mean: -17.560923\n",
      "ep 3769: ep_len:65 episode reward: total was 26.500000. running mean: -17.120314\n",
      "ep 3769: ep_len:79 episode reward: total was 38.000000. running mean: -16.569111\n",
      "ep 3769: ep_len:63 episode reward: total was 30.000000. running mean: -16.103420\n",
      "ep 3769: ep_len:658 episode reward: total was -7.730000. running mean: -16.019686\n",
      "ep 3769: ep_len:2792 episode reward: total was 0.150000. running mean: -15.857989\n",
      "ep 3769: ep_len:60 episode reward: total was 28.500000. running mean: -15.414409\n",
      "epsilon:0.009992 episode_count: 56657. steps_count: 61123821.000000\n",
      "ep 3770: ep_len:1104 episode reward: total was -10.430000. running mean: -15.364565\n",
      "ep 3770: ep_len:995 episode reward: total was 13.160000. running mean: -15.079319\n",
      "ep 3770: ep_len:3003 episode reward: total was -11.300000. running mean: -15.041526\n",
      "ep 3770: ep_len:500 episode reward: total was -20.800000. running mean: -15.099111\n",
      "ep 3770: ep_len:99 episode reward: total was 48.000000. running mean: -14.468120\n",
      "ep 3770: ep_len:1079 episode reward: total was -10.590000. running mean: -14.429338\n",
      "ep 3770: ep_len:3361 episode reward: total was -22.570000. running mean: -14.510745\n",
      "ep 3770: ep_len:583 episode reward: total was 12.370000. running mean: -14.241938\n",
      "ep 3770: ep_len:731 episode reward: total was 32.070000. running mean: -13.778818\n",
      "ep 3770: ep_len:500 episode reward: total was 15.190000. running mean: -13.489130\n",
      "ep 3770: ep_len:57 episode reward: total was 27.000000. running mean: -13.084239\n",
      "ep 3770: ep_len:500 episode reward: total was 17.640000. running mean: -12.776996\n",
      "ep 3770: ep_len:2854 episode reward: total was 13.380000. running mean: -12.515426\n",
      "ep 3770: ep_len:40 episode reward: total was 17.000000. running mean: -12.220272\n",
      "epsilon:0.009992 episode_count: 56671. steps_count: 61139227.000000\n",
      "ep 3771: ep_len:976 episode reward: total was -39.140000. running mean: -12.489469\n",
      "ep 3771: ep_len:695 episode reward: total was -79.150000. running mean: -13.156075\n",
      "ep 3771: ep_len:2991 episode reward: total was -27.710000. running mean: -13.301614\n",
      "ep 3771: ep_len:529 episode reward: total was -2.960000. running mean: -13.198198\n",
      "ep 3771: ep_len:126 episode reward: total was 60.000000. running mean: -12.466216\n",
      "ep 3771: ep_len:847 episode reward: total was 23.510000. running mean: -12.106454\n",
      "ep 3771: ep_len:500 episode reward: total was 27.690000. running mean: -11.708489\n",
      "ep 3771: ep_len:530 episode reward: total was -31.200000. running mean: -11.903404\n",
      "ep 3771: ep_len:790 episode reward: total was 41.940000. running mean: -11.364970\n",
      "ep 3771: ep_len:853 episode reward: total was 18.960000. running mean: -11.061721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3771: ep_len:87 episode reward: total was 40.500000. running mean: -10.546103\n",
      "ep 3771: ep_len:114 episode reward: total was 52.500000. running mean: -9.915642\n",
      "ep 3771: ep_len:632 episode reward: total was 3.540000. running mean: -9.781086\n",
      "ep 3771: ep_len:2864 episode reward: total was -20.930000. running mean: -9.892575\n",
      "epsilon:0.009992 episode_count: 56685. steps_count: 61151761.000000\n",
      "ep 3772: ep_len:1158 episode reward: total was -8.970000. running mean: -9.883349\n",
      "ep 3772: ep_len:187 episode reward: total was -10.230000. running mean: -9.886816\n",
      "ep 3772: ep_len:3067 episode reward: total was 9.210000. running mean: -9.695848\n",
      "ep 3772: ep_len:1659 episode reward: total was -30.930000. running mean: -9.908189\n",
      "ep 3772: ep_len:742 episode reward: total was -3.770000. running mean: -9.846807\n",
      "ep 3772: ep_len:359 episode reward: total was 2.700000. running mean: -9.721339\n",
      "ep 3772: ep_len:577 episode reward: total was 1.350000. running mean: -9.610626\n",
      "ep 3772: ep_len:743 episode reward: total was -118.840000. running mean: -10.702920\n",
      "ep 3772: ep_len:1498 episode reward: total was -8.330000. running mean: -10.679190\n",
      "ep 3772: ep_len:54 episode reward: total was 25.500000. running mean: -10.317398\n",
      "ep 3772: ep_len:646 episode reward: total was 4.210000. running mean: -10.172124\n",
      "ep 3772: ep_len:2779 episode reward: total was -0.260000. running mean: -10.073003\n",
      "epsilon:0.009992 episode_count: 56697. steps_count: 61165230.000000\n",
      "ep 3773: ep_len:742 episode reward: total was -31.580000. running mean: -10.288073\n",
      "ep 3773: ep_len:810 episode reward: total was 18.230000. running mean: -10.002892\n",
      "ep 3773: ep_len:75 episode reward: total was 36.000000. running mean: -9.542864\n",
      "ep 3773: ep_len:3002 episode reward: total was 11.380000. running mean: -9.333635\n",
      "ep 3773: ep_len:524 episode reward: total was -35.330000. running mean: -9.593599\n",
      "ep 3773: ep_len:96 episode reward: total was 43.500000. running mean: -9.062663\n",
      "ep 3773: ep_len:1060 episode reward: total was -20.910000. running mean: -9.181136\n",
      "ep 3773: ep_len:614 episode reward: total was 22.720000. running mean: -8.862125\n",
      "ep 3773: ep_len:543 episode reward: total was -20.480000. running mean: -8.978303\n",
      "ep 3773: ep_len:697 episode reward: total was -4.150000. running mean: -8.930020\n",
      "ep 3773: ep_len:1149 episode reward: total was -13.740000. running mean: -8.978120\n",
      "ep 3773: ep_len:97 episode reward: total was 47.000000. running mean: -8.418339\n",
      "ep 3773: ep_len:1078 episode reward: total was -30.800000. running mean: -8.642155\n",
      "ep 3773: ep_len:2756 episode reward: total was -8.390000. running mean: -8.639634\n",
      "epsilon:0.009992 episode_count: 56711. steps_count: 61178473.000000\n",
      "ep 3774: ep_len:1087 episode reward: total was -8.490000. running mean: -8.638138\n",
      "ep 3774: ep_len:835 episode reward: total was 13.370000. running mean: -8.418056\n",
      "ep 3774: ep_len:63 episode reward: total was 30.000000. running mean: -8.033876\n",
      "ep 3774: ep_len:3000 episode reward: total was 13.870000. running mean: -7.814837\n",
      "ep 3774: ep_len:1239 episode reward: total was -39.290000. running mean: -8.129589\n",
      "ep 3774: ep_len:818 episode reward: total was 25.270000. running mean: -7.795593\n",
      "ep 3774: ep_len:655 episode reward: total was 34.670000. running mean: -7.370937\n",
      "ep 3774: ep_len:605 episode reward: total was 19.710000. running mean: -7.100127\n",
      "ep 3774: ep_len:7334 episode reward: total was 10.610000. running mean: -6.923026\n",
      "ep 3774: ep_len:603 episode reward: total was -5.340000. running mean: -6.907196\n",
      "ep 3774: ep_len:60 episode reward: total was 28.500000. running mean: -6.553124\n",
      "ep 3774: ep_len:1118 episode reward: total was -1.110000. running mean: -6.498693\n",
      "ep 3774: ep_len:2880 episode reward: total was -1.240000. running mean: -6.446106\n",
      "epsilon:0.009992 episode_count: 56724. steps_count: 61198770.000000\n",
      "ep 3775: ep_len:500 episode reward: total was 8.850000. running mean: -6.293145\n",
      "ep 3775: ep_len:883 episode reward: total was -3.870000. running mean: -6.268913\n",
      "ep 3775: ep_len:64 episode reward: total was 29.000000. running mean: -5.916224\n",
      "ep 3775: ep_len:100 episode reward: total was 47.000000. running mean: -5.387062\n",
      "ep 3775: ep_len:832 episode reward: total was -66.320000. running mean: -5.996391\n",
      "ep 3775: ep_len:103 episode reward: total was 50.000000. running mean: -5.436427\n",
      "ep 3775: ep_len:83 episode reward: total was 38.500000. running mean: -4.997063\n",
      "ep 3775: ep_len:627 episode reward: total was -15.190000. running mean: -5.098992\n",
      "ep 3775: ep_len:3726 episode reward: total was -130.190000. running mean: -6.349902\n",
      "ep 3775: ep_len:4022 episode reward: total was -910.360000. running mean: -15.390003\n",
      "ep 3775: ep_len:781 episode reward: total was 16.740000. running mean: -15.068703\n",
      "ep 3775: ep_len:1039 episode reward: total was 9.520000. running mean: -14.822816\n",
      "ep 3775: ep_len:158 episode reward: total was 76.000000. running mean: -13.914588\n",
      "ep 3775: ep_len:38 episode reward: total was 16.000000. running mean: -13.615442\n",
      "ep 3775: ep_len:1124 episode reward: total was 20.140000. running mean: -13.277888\n",
      "ep 3775: ep_len:2789 episode reward: total was 1.280000. running mean: -13.132309\n",
      "epsilon:0.009992 episode_count: 56740. steps_count: 61215639.000000\n",
      "ep 3776: ep_len:672 episode reward: total was 1.500000. running mean: -12.985986\n",
      "ep 3776: ep_len:500 episode reward: total was 3.890000. running mean: -12.817226\n",
      "ep 3776: ep_len:65 episode reward: total was 29.500000. running mean: -12.394054\n",
      "ep 3776: ep_len:2912 episode reward: total was -25.650000. running mean: -12.526613\n",
      "ep 3776: ep_len:1658 episode reward: total was -36.820000. running mean: -12.769547\n",
      "ep 3776: ep_len:917 episode reward: total was 64.300000. running mean: -11.998852\n",
      "ep 3776: ep_len:3889 episode reward: total was -394.520000. running mean: -15.824063\n",
      "ep 3776: ep_len:570 episode reward: total was 0.270000. running mean: -15.663123\n",
      "ep 3776: ep_len:686 episode reward: total was 21.210000. running mean: -15.294391\n",
      "ep 3776: ep_len:1107 episode reward: total was -4.250000. running mean: -15.183947\n",
      "ep 3776: ep_len:45 episode reward: total was 21.000000. running mean: -14.822108\n",
      "ep 3776: ep_len:71 episode reward: total was 34.000000. running mean: -14.333887\n",
      "ep 3776: ep_len:1397 episode reward: total was 24.940000. running mean: -13.941148\n",
      "ep 3776: ep_len:2867 episode reward: total was 4.170000. running mean: -13.760036\n",
      "epsilon:0.009992 episode_count: 56754. steps_count: 61232995.000000\n",
      "ep 3777: ep_len:1129 episode reward: total was -0.940000. running mean: -13.631836\n",
      "ep 3777: ep_len:1677 episode reward: total was -26.620000. running mean: -13.761718\n",
      "ep 3777: ep_len:3023 episode reward: total was 5.010000. running mean: -13.574001\n",
      "ep 3777: ep_len:602 episode reward: total was -10.280000. running mean: -13.541061\n",
      "ep 3777: ep_len:40 episode reward: total was 18.500000. running mean: -13.220650\n",
      "ep 3777: ep_len:601 episode reward: total was 6.240000. running mean: -13.026043\n",
      "ep 3777: ep_len:666 episode reward: total was 26.730000. running mean: -12.628483\n",
      "ep 3777: ep_len:551 episode reward: total was -0.990000. running mean: -12.512098\n",
      "ep 3777: ep_len:771 episode reward: total was 27.330000. running mean: -12.113677\n",
      "ep 3777: ep_len:721 episode reward: total was -1.110000. running mean: -12.003640\n",
      "ep 3777: ep_len:76 episode reward: total was 35.000000. running mean: -11.533604\n",
      "ep 3777: ep_len:51 episode reward: total was 24.000000. running mean: -11.178268\n",
      "ep 3777: ep_len:500 episode reward: total was 4.410000. running mean: -11.022385\n",
      "ep 3777: ep_len:2858 episode reward: total was -16.460000. running mean: -11.076761\n",
      "ep 3777: ep_len:51 episode reward: total was 24.000000. running mean: -10.725994\n",
      "epsilon:0.009992 episode_count: 56769. steps_count: 61246312.000000\n",
      "ep 3778: ep_len:500 episode reward: total was 4.810000. running mean: -10.570634\n",
      "ep 3778: ep_len:973 episode reward: total was 28.940000. running mean: -10.175528\n",
      "ep 3778: ep_len:65 episode reward: total was 29.500000. running mean: -9.778772\n",
      "ep 3778: ep_len:2877 episode reward: total was -3.530000. running mean: -9.716285\n",
      "ep 3778: ep_len:500 episode reward: total was 17.570000. running mean: -9.443422\n",
      "ep 3778: ep_len:136 episode reward: total was 65.000000. running mean: -8.698988\n",
      "ep 3778: ep_len:1464 episode reward: total was -148.620000. running mean: -10.098198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3778: ep_len:3631 episode reward: total was -367.480000. running mean: -13.672016\n",
      "ep 3778: ep_len:1544 episode reward: total was -49.350000. running mean: -14.028796\n",
      "ep 3778: ep_len:7398 episode reward: total was 54.990000. running mean: -13.338608\n",
      "ep 3778: ep_len:1136 episode reward: total was -9.960000. running mean: -13.304821\n",
      "ep 3778: ep_len:48 episode reward: total was 21.000000. running mean: -12.961773\n",
      "ep 3778: ep_len:500 episode reward: total was 29.330000. running mean: -12.538856\n",
      "ep 3778: ep_len:2792 episode reward: total was -2.420000. running mean: -12.437667\n",
      "epsilon:0.009992 episode_count: 56783. steps_count: 61269876.000000\n",
      "ep 3779: ep_len:1368 episode reward: total was 18.160000. running mean: -12.131690\n",
      "ep 3779: ep_len:678 episode reward: total was -33.790000. running mean: -12.348273\n",
      "ep 3779: ep_len:55 episode reward: total was 24.500000. running mean: -11.979791\n",
      "ep 3779: ep_len:3030 episode reward: total was -152.540000. running mean: -13.385393\n",
      "ep 3779: ep_len:667 episode reward: total was 0.150000. running mean: -13.250039\n",
      "ep 3779: ep_len:500 episode reward: total was 19.660000. running mean: -12.920938\n",
      "ep 3779: ep_len:329 episode reward: total was 7.600000. running mean: -12.715729\n",
      "ep 3779: ep_len:1552 episode reward: total was -72.590000. running mean: -13.314472\n",
      "ep 3779: ep_len:716 episode reward: total was 47.080000. running mean: -12.710527\n",
      "ep 3779: ep_len:704 episode reward: total was -2.430000. running mean: -12.607722\n",
      "ep 3779: ep_len:679 episode reward: total was 30.740000. running mean: -12.174245\n",
      "ep 3779: ep_len:2855 episode reward: total was -11.040000. running mean: -12.162902\n",
      "epsilon:0.009992 episode_count: 56795. steps_count: 61283009.000000\n",
      "ep 3780: ep_len:718 episode reward: total was -71.770000. running mean: -12.758973\n",
      "ep 3780: ep_len:705 episode reward: total was -9.670000. running mean: -12.728083\n",
      "ep 3780: ep_len:48 episode reward: total was 19.500000. running mean: -12.405803\n",
      "ep 3780: ep_len:2951 episode reward: total was -26.210000. running mean: -12.543845\n",
      "ep 3780: ep_len:716 episode reward: total was -19.270000. running mean: -12.611106\n",
      "ep 3780: ep_len:53 episode reward: total was 23.500000. running mean: -12.249995\n",
      "ep 3780: ep_len:148 episode reward: total was 72.500000. running mean: -11.402495\n",
      "ep 3780: ep_len:52 episode reward: total was 23.000000. running mean: -11.058470\n",
      "ep 3780: ep_len:782 episode reward: total was -2.970000. running mean: -10.977585\n",
      "ep 3780: ep_len:3741 episode reward: total was -36.180000. running mean: -11.229610\n",
      "ep 3780: ep_len:614 episode reward: total was -41.900000. running mean: -11.536313\n",
      "ep 3780: ep_len:7350 episode reward: total was 21.970000. running mean: -11.201250\n",
      "ep 3780: ep_len:857 episode reward: total was 30.140000. running mean: -10.787838\n",
      "ep 3780: ep_len:33 episode reward: total was 15.000000. running mean: -10.529959\n",
      "ep 3780: ep_len:46 episode reward: total was 20.000000. running mean: -10.224660\n",
      "ep 3780: ep_len:1492 episode reward: total was -0.830000. running mean: -10.130713\n",
      "ep 3780: ep_len:34 episode reward: total was 14.000000. running mean: -9.889406\n",
      "epsilon:0.009992 episode_count: 56812. steps_count: 61303349.000000\n",
      "ep 3781: ep_len:656 episode reward: total was -12.280000. running mean: -9.913312\n",
      "ep 3781: ep_len:726 episode reward: total was 1.010000. running mean: -9.804079\n",
      "ep 3781: ep_len:2909 episode reward: total was 18.440000. running mean: -9.521638\n",
      "ep 3781: ep_len:563 episode reward: total was -5.650000. running mean: -9.482922\n",
      "ep 3781: ep_len:55 episode reward: total was 24.500000. running mean: -9.143093\n",
      "ep 3781: ep_len:69 episode reward: total was 27.000000. running mean: -8.781662\n",
      "ep 3781: ep_len:769 episode reward: total was 2.470000. running mean: -8.669145\n",
      "ep 3781: ep_len:3621 episode reward: total was -15.090000. running mean: -8.733354\n",
      "ep 3781: ep_len:581 episode reward: total was -5.470000. running mean: -8.700720\n",
      "ep 3781: ep_len:685 episode reward: total was 50.570000. running mean: -8.108013\n",
      "ep 3781: ep_len:500 episode reward: total was 34.850000. running mean: -7.678433\n",
      "ep 3781: ep_len:60 episode reward: total was 27.000000. running mean: -7.331648\n",
      "ep 3781: ep_len:601 episode reward: total was 5.840000. running mean: -7.199932\n",
      "ep 3781: ep_len:2756 episode reward: total was -2.230000. running mean: -7.150233\n",
      "epsilon:0.009992 episode_count: 56826. steps_count: 61317900.000000\n",
      "ep 3782: ep_len:1028 episode reward: total was -95.740000. running mean: -8.036130\n",
      "ep 3782: ep_len:721 episode reward: total was -35.390000. running mean: -8.309669\n",
      "ep 3782: ep_len:51 episode reward: total was 24.000000. running mean: -7.986572\n",
      "ep 3782: ep_len:2970 episode reward: total was 12.300000. running mean: -7.783707\n",
      "ep 3782: ep_len:500 episode reward: total was 19.360000. running mean: -7.512269\n",
      "ep 3782: ep_len:93 episode reward: total was 43.500000. running mean: -7.002147\n",
      "ep 3782: ep_len:1389 episode reward: total was -185.250000. running mean: -8.784625\n",
      "ep 3782: ep_len:4083 episode reward: total was -137.020000. running mean: -10.066979\n",
      "ep 3782: ep_len:530 episode reward: total was -17.090000. running mean: -10.137209\n",
      "ep 3782: ep_len:756 episode reward: total was 15.940000. running mean: -9.876437\n",
      "ep 3782: ep_len:500 episode reward: total was -34.580000. running mean: -10.123473\n",
      "ep 3782: ep_len:104 episode reward: total was 49.000000. running mean: -9.532238\n",
      "ep 3782: ep_len:1477 episode reward: total was 12.820000. running mean: -9.308716\n",
      "ep 3782: ep_len:2809 episode reward: total was -12.880000. running mean: -9.344429\n",
      "epsilon:0.009992 episode_count: 56840. steps_count: 61334911.000000\n",
      "ep 3783: ep_len:652 episode reward: total was -38.980000. running mean: -9.640784\n",
      "ep 3783: ep_len:1594 episode reward: total was -23.070000. running mean: -9.775076\n",
      "ep 3783: ep_len:3118 episode reward: total was -15.650000. running mean: -9.833826\n",
      "ep 3783: ep_len:1697 episode reward: total was -39.210000. running mean: -10.127587\n",
      "ep 3783: ep_len:29 episode reward: total was 13.000000. running mean: -9.896311\n",
      "ep 3783: ep_len:60 episode reward: total was 28.500000. running mean: -9.512348\n",
      "ep 3783: ep_len:997 episode reward: total was -25.550000. running mean: -9.672725\n",
      "ep 3783: ep_len:3680 episode reward: total was -31.670000. running mean: -9.892698\n",
      "ep 3783: ep_len:578 episode reward: total was 18.200000. running mean: -9.611771\n",
      "ep 3783: ep_len:802 episode reward: total was 43.630000. running mean: -9.079353\n",
      "ep 3783: ep_len:683 episode reward: total was 10.390000. running mean: -8.884659\n",
      "ep 3783: ep_len:215 episode reward: total was 106.000000. running mean: -7.735813\n",
      "ep 3783: ep_len:76 episode reward: total was 36.500000. running mean: -7.293455\n",
      "ep 3783: ep_len:968 episode reward: total was -50.080000. running mean: -7.721320\n",
      "ep 3783: ep_len:2747 episode reward: total was -7.560000. running mean: -7.719707\n",
      "ep 3783: ep_len:56 episode reward: total was 22.000000. running mean: -7.422510\n",
      "epsilon:0.009992 episode_count: 56856. steps_count: 61352863.000000\n",
      "ep 3784: ep_len:1407 episode reward: total was -11.170000. running mean: -7.459985\n",
      "ep 3784: ep_len:782 episode reward: total was -7.370000. running mean: -7.459085\n",
      "ep 3784: ep_len:48 episode reward: total was 22.500000. running mean: -7.159494\n",
      "ep 3784: ep_len:3011 episode reward: total was -14.890000. running mean: -7.236799\n",
      "ep 3784: ep_len:1159 episode reward: total was -16.950000. running mean: -7.333931\n",
      "ep 3784: ep_len:67 episode reward: total was 30.500000. running mean: -6.955592\n",
      "ep 3784: ep_len:130 episode reward: total was 59.000000. running mean: -6.296036\n",
      "ep 3784: ep_len:70 episode reward: total was 33.500000. running mean: -5.898076\n",
      "ep 3784: ep_len:45 episode reward: total was 18.000000. running mean: -5.659095\n",
      "ep 3784: ep_len:656 episode reward: total was 6.010000. running mean: -5.542404\n",
      "ep 3784: ep_len:3822 episode reward: total was -37.120000. running mean: -5.858180\n",
      "ep 3784: ep_len:645 episode reward: total was -17.960000. running mean: -5.979198\n",
      "ep 3784: ep_len:674 episode reward: total was -44.050000. running mean: -6.359906\n",
      "ep 3784: ep_len:1052 episode reward: total was 32.640000. running mean: -5.969907\n",
      "ep 3784: ep_len:184 episode reward: total was 84.500000. running mean: -5.065208\n",
      "ep 3784: ep_len:56 episode reward: total was 26.500000. running mean: -4.749556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3784: ep_len:1151 episode reward: total was -6.870000. running mean: -4.770760\n",
      "ep 3784: ep_len:2793 episode reward: total was -4.950000. running mean: -4.772553\n",
      "ep 3784: ep_len:68 episode reward: total was 32.500000. running mean: -4.399827\n",
      "epsilon:0.009992 episode_count: 56875. steps_count: 61370683.000000\n",
      "ep 3785: ep_len:1127 episode reward: total was 8.070000. running mean: -4.275129\n",
      "ep 3785: ep_len:1600 episode reward: total was -31.640000. running mean: -4.548778\n",
      "ep 3785: ep_len:2918 episode reward: total was -5.320000. running mean: -4.556490\n",
      "ep 3785: ep_len:579 episode reward: total was -57.000000. running mean: -5.080925\n",
      "ep 3785: ep_len:52 episode reward: total was 24.500000. running mean: -4.785116\n",
      "ep 3785: ep_len:145 episode reward: total was 66.500000. running mean: -4.072265\n",
      "ep 3785: ep_len:1127 episode reward: total was -4.050000. running mean: -4.072042\n",
      "ep 3785: ep_len:3526 episode reward: total was -0.090000. running mean: -4.032221\n",
      "ep 3785: ep_len:518 episode reward: total was -24.280000. running mean: -4.234699\n",
      "ep 3785: ep_len:663 episode reward: total was 23.000000. running mean: -3.962352\n",
      "ep 3785: ep_len:949 episode reward: total was 12.170000. running mean: -3.801029\n",
      "ep 3785: ep_len:53 episode reward: total was 23.500000. running mean: -3.528018\n",
      "ep 3785: ep_len:181 episode reward: total was 87.500000. running mean: -2.617738\n",
      "ep 3785: ep_len:1159 episode reward: total was -29.380000. running mean: -2.885361\n",
      "ep 3785: ep_len:2916 episode reward: total was -8.440000. running mean: -2.940907\n",
      "ep 3785: ep_len:56 episode reward: total was 26.500000. running mean: -2.646498\n",
      "epsilon:0.009992 episode_count: 56891. steps_count: 61388252.000000\n",
      "ep 3786: ep_len:500 episode reward: total was 1.780000. running mean: -2.602233\n",
      "ep 3786: ep_len:713 episode reward: total was -11.330000. running mean: -2.689511\n",
      "ep 3786: ep_len:43 episode reward: total was 20.000000. running mean: -2.462616\n",
      "ep 3786: ep_len:2988 episode reward: total was -5.970000. running mean: -2.497690\n",
      "ep 3786: ep_len:689 episode reward: total was 24.800000. running mean: -2.224713\n",
      "ep 3786: ep_len:126 episode reward: total was 58.500000. running mean: -1.617466\n",
      "ep 3786: ep_len:74 episode reward: total was 34.000000. running mean: -1.261291\n",
      "ep 3786: ep_len:755 episode reward: total was 4.320000. running mean: -1.205478\n",
      "ep 3786: ep_len:636 episode reward: total was 31.660000. running mean: -0.876823\n",
      "ep 3786: ep_len:521 episode reward: total was -6.070000. running mean: -0.928755\n",
      "ep 3786: ep_len:711 episode reward: total was 34.080000. running mean: -0.578667\n",
      "ep 3786: ep_len:1546 episode reward: total was 27.350000. running mean: -0.299381\n",
      "ep 3786: ep_len:51 episode reward: total was 24.000000. running mean: -0.056387\n",
      "ep 3786: ep_len:170 episode reward: total was 77.500000. running mean: 0.719177\n",
      "ep 3786: ep_len:79 episode reward: total was 38.000000. running mean: 1.091985\n",
      "ep 3786: ep_len:1134 episode reward: total was 1.070000. running mean: 1.091765\n",
      "ep 3786: ep_len:2817 episode reward: total was -12.310000. running mean: 0.957748\n",
      "ep 3786: ep_len:36 episode reward: total was 15.000000. running mean: 1.098170\n",
      "epsilon:0.009992 episode_count: 56909. steps_count: 61401841.000000\n",
      "ep 3787: ep_len:1010 episode reward: total was -68.850000. running mean: 0.398688\n",
      "ep 3787: ep_len:733 episode reward: total was 11.300000. running mean: 0.507702\n",
      "ep 3787: ep_len:2998 episode reward: total was -0.650000. running mean: 0.496125\n",
      "ep 3787: ep_len:634 episode reward: total was 13.130000. running mean: 0.622463\n",
      "ep 3787: ep_len:170 episode reward: total was 79.000000. running mean: 1.406239\n",
      "ep 3787: ep_len:82 episode reward: total was 36.500000. running mean: 1.757176\n",
      "ep 3787: ep_len:38 episode reward: total was 17.500000. running mean: 1.914605\n",
      "ep 3787: ep_len:500 episode reward: total was 40.360000. running mean: 2.299058\n",
      "ep 3787: ep_len:354 episode reward: total was 17.680000. running mean: 2.452868\n",
      "ep 3787: ep_len:610 episode reward: total was 16.550000. running mean: 2.593839\n",
      "ep 3787: ep_len:748 episode reward: total was 42.080000. running mean: 2.988701\n",
      "ep 3787: ep_len:3740 episode reward: total was -586.210000. running mean: -2.903286\n",
      "ep 3787: ep_len:93 episode reward: total was 45.000000. running mean: -2.424253\n",
      "ep 3787: ep_len:36 episode reward: total was 16.500000. running mean: -2.235011\n",
      "ep 3787: ep_len:827 episode reward: total was 26.300000. running mean: -1.949661\n",
      "ep 3787: ep_len:2848 episode reward: total was 10.840000. running mean: -1.821764\n",
      "epsilon:0.009992 episode_count: 56925. steps_count: 61417262.000000\n",
      "ep 3788: ep_len:658 episode reward: total was -7.730000. running mean: -1.880846\n",
      "ep 3788: ep_len:1205 episode reward: total was -122.040000. running mean: -3.082438\n",
      "ep 3788: ep_len:2877 episode reward: total was -48.040000. running mean: -3.532014\n",
      "ep 3788: ep_len:1245 episode reward: total was -34.180000. running mean: -3.838493\n",
      "ep 3788: ep_len:100 episode reward: total was 48.500000. running mean: -3.315109\n",
      "ep 3788: ep_len:71 episode reward: total was 34.000000. running mean: -2.941957\n",
      "ep 3788: ep_len:78 episode reward: total was 37.500000. running mean: -2.537538\n",
      "ep 3788: ep_len:610 episode reward: total was 47.780000. running mean: -2.034362\n",
      "ep 3788: ep_len:4175 episode reward: total was -247.430000. running mean: -4.488319\n",
      "ep 3788: ep_len:528 episode reward: total was -15.090000. running mean: -4.594336\n",
      "ep 3788: ep_len:6974 episode reward: total was -543.620000. running mean: -9.984592\n",
      "ep 3788: ep_len:500 episode reward: total was 42.290000. running mean: -9.461846\n",
      "ep 3788: ep_len:148 episode reward: total was 71.000000. running mean: -8.657228\n",
      "ep 3788: ep_len:43 episode reward: total was 20.000000. running mean: -8.370656\n",
      "ep 3788: ep_len:1531 episode reward: total was 20.230000. running mean: -8.084649\n",
      "ep 3788: ep_len:2858 episode reward: total was -12.080000. running mean: -8.124603\n",
      "epsilon:0.009992 episode_count: 56941. steps_count: 61440863.000000\n",
      "ep 3789: ep_len:715 episode reward: total was 11.060000. running mean: -7.932757\n",
      "ep 3789: ep_len:1193 episode reward: total was -50.860000. running mean: -8.362029\n",
      "ep 3789: ep_len:3011 episode reward: total was -16.690000. running mean: -8.445309\n",
      "ep 3789: ep_len:600 episode reward: total was -8.310000. running mean: -8.443956\n",
      "ep 3789: ep_len:54 episode reward: total was 22.500000. running mean: -8.134516\n",
      "ep 3789: ep_len:90 episode reward: total was 43.500000. running mean: -7.618171\n",
      "ep 3789: ep_len:46 episode reward: total was 20.000000. running mean: -7.341989\n",
      "ep 3789: ep_len:500 episode reward: total was 8.240000. running mean: -7.186169\n",
      "ep 3789: ep_len:322 episode reward: total was 15.340000. running mean: -6.960908\n",
      "ep 3789: ep_len:525 episode reward: total was -45.420000. running mean: -7.345499\n",
      "ep 3789: ep_len:668 episode reward: total was 6.950000. running mean: -7.202544\n",
      "ep 3789: ep_len:723 episode reward: total was -16.170000. running mean: -7.292218\n",
      "ep 3789: ep_len:188 episode reward: total was 92.500000. running mean: -6.294296\n",
      "ep 3789: ep_len:52 episode reward: total was 24.500000. running mean: -5.986353\n",
      "ep 3789: ep_len:122 episode reward: total was 59.500000. running mean: -5.331489\n",
      "ep 3789: ep_len:500 episode reward: total was 10.570000. running mean: -5.172475\n",
      "ep 3789: ep_len:2814 episode reward: total was -19.190000. running mean: -5.312650\n",
      "epsilon:0.009992 episode_count: 56958. steps_count: 61452986.000000\n",
      "ep 3790: ep_len:638 episode reward: total was 0.150000. running mean: -5.258023\n",
      "ep 3790: ep_len:500 episode reward: total was 17.790000. running mean: -5.027543\n",
      "ep 3790: ep_len:77 episode reward: total was 37.000000. running mean: -4.607268\n",
      "ep 3790: ep_len:2978 episode reward: total was 20.050000. running mean: -4.360695\n",
      "ep 3790: ep_len:500 episode reward: total was -63.050000. running mean: -4.947588\n",
      "ep 3790: ep_len:47 episode reward: total was 20.500000. running mean: -4.693112\n",
      "ep 3790: ep_len:156 episode reward: total was 76.500000. running mean: -3.881181\n",
      "ep 3790: ep_len:88 episode reward: total was 42.500000. running mean: -3.417369\n",
      "ep 3790: ep_len:896 episode reward: total was 24.340000. running mean: -3.139796\n",
      "ep 3790: ep_len:313 episode reward: total was 21.920000. running mean: -2.889198\n",
      "ep 3790: ep_len:562 episode reward: total was -45.050000. running mean: -3.310806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3790: ep_len:7550 episode reward: total was -151.130000. running mean: -4.788998\n",
      "ep 3790: ep_len:500 episode reward: total was 9.500000. running mean: -4.646108\n",
      "ep 3790: ep_len:140 episode reward: total was 67.000000. running mean: -3.929646\n",
      "ep 3790: ep_len:779 episode reward: total was -82.330000. running mean: -4.713650\n",
      "ep 3790: ep_len:2801 episode reward: total was 8.010000. running mean: -4.586414\n",
      "epsilon:0.009992 episode_count: 56974. steps_count: 61471511.000000\n",
      "ep 3791: ep_len:983 episode reward: total was -69.190000. running mean: -5.232449\n",
      "ep 3791: ep_len:786 episode reward: total was -12.540000. running mean: -5.305525\n",
      "ep 3791: ep_len:2980 episode reward: total was -11.460000. running mean: -5.367070\n",
      "ep 3791: ep_len:669 episode reward: total was -9.500000. running mean: -5.408399\n",
      "ep 3791: ep_len:41 episode reward: total was 19.000000. running mean: -5.164315\n",
      "ep 3791: ep_len:1448 episode reward: total was 1.940000. running mean: -5.093272\n",
      "ep 3791: ep_len:3743 episode reward: total was -113.860000. running mean: -6.180939\n",
      "ep 3791: ep_len:1546 episode reward: total was -68.890000. running mean: -6.808030\n",
      "ep 3791: ep_len:717 episode reward: total was -8.510000. running mean: -6.825049\n",
      "ep 3791: ep_len:1102 episode reward: total was 3.780000. running mean: -6.718999\n",
      "ep 3791: ep_len:39 episode reward: total was 18.000000. running mean: -6.471809\n",
      "ep 3791: ep_len:104 episode reward: total was 49.000000. running mean: -5.917091\n",
      "ep 3791: ep_len:696 episode reward: total was -65.930000. running mean: -6.517220\n",
      "ep 3791: ep_len:2826 episode reward: total was -16.410000. running mean: -6.616148\n",
      "ep 3791: ep_len:41 episode reward: total was 19.000000. running mean: -6.359986\n",
      "epsilon:0.009992 episode_count: 56989. steps_count: 61489232.000000\n",
      "ep 3792: ep_len:632 episode reward: total was 14.560000. running mean: -6.150786\n",
      "ep 3792: ep_len:1011 episode reward: total was 26.330000. running mean: -5.825979\n",
      "ep 3792: ep_len:48 episode reward: total was 22.500000. running mean: -5.542719\n",
      "ep 3792: ep_len:2973 episode reward: total was -30.210000. running mean: -5.789392\n",
      "ep 3792: ep_len:500 episode reward: total was 24.410000. running mean: -5.487398\n",
      "ep 3792: ep_len:40 episode reward: total was 17.000000. running mean: -5.262524\n",
      "ep 3792: ep_len:112 episode reward: total was 50.000000. running mean: -4.709898\n",
      "ep 3792: ep_len:720 episode reward: total was 9.630000. running mean: -4.566499\n",
      "ep 3792: ep_len:659 episode reward: total was 25.160000. running mean: -4.269234\n",
      "ep 3792: ep_len:1523 episode reward: total was -37.290000. running mean: -4.599442\n",
      "ep 3792: ep_len:718 episode reward: total was 34.370000. running mean: -4.209748\n",
      "ep 3792: ep_len:500 episode reward: total was 1.190000. running mean: -4.155750\n",
      "ep 3792: ep_len:82 episode reward: total was 38.000000. running mean: -3.734193\n",
      "ep 3792: ep_len:35 episode reward: total was 16.000000. running mean: -3.536851\n",
      "ep 3792: ep_len:741 episode reward: total was -41.240000. running mean: -3.913882\n",
      "ep 3792: ep_len:2800 episode reward: total was -11.500000. running mean: -3.989743\n",
      "epsilon:0.009992 episode_count: 57005. steps_count: 61502326.000000\n",
      "ep 3793: ep_len:1469 episode reward: total was 23.400000. running mean: -3.715846\n",
      "ep 3793: ep_len:715 episode reward: total was 7.330000. running mean: -3.605388\n",
      "ep 3793: ep_len:2956 episode reward: total was -47.740000. running mean: -4.046734\n",
      "ep 3793: ep_len:1206 episode reward: total was -12.430000. running mean: -4.130566\n",
      "ep 3793: ep_len:56 episode reward: total was 25.000000. running mean: -3.839261\n",
      "ep 3793: ep_len:1010 episode reward: total was -54.460000. running mean: -4.345468\n",
      "ep 3793: ep_len:318 episode reward: total was 12.270000. running mean: -4.179313\n",
      "ep 3793: ep_len:821 episode reward: total was -38.370000. running mean: -4.521220\n",
      "ep 3793: ep_len:570 episode reward: total was -10.540000. running mean: -4.581408\n",
      "ep 3793: ep_len:509 episode reward: total was 11.570000. running mean: -4.419894\n",
      "ep 3793: ep_len:42 episode reward: total was 14.510000. running mean: -4.230595\n",
      "ep 3793: ep_len:966 episode reward: total was -43.310000. running mean: -4.621389\n",
      "ep 3793: ep_len:2781 episode reward: total was -3.700000. running mean: -4.612175\n",
      "ep 3793: ep_len:46 episode reward: total was 20.000000. running mean: -4.366053\n",
      "epsilon:0.009992 episode_count: 57019. steps_count: 61515791.000000\n",
      "ep 3794: ep_len:1174 episode reward: total was -20.660000. running mean: -4.528993\n",
      "ep 3794: ep_len:958 episode reward: total was 13.330000. running mean: -4.350403\n",
      "ep 3794: ep_len:37 episode reward: total was 17.000000. running mean: -4.136899\n",
      "ep 3794: ep_len:101 episode reward: total was 49.000000. running mean: -3.605530\n",
      "ep 3794: ep_len:1150 episode reward: total was -15.420000. running mean: -3.723675\n",
      "ep 3794: ep_len:50 episode reward: total was 23.500000. running mean: -3.451438\n",
      "ep 3794: ep_len:47 episode reward: total was 22.000000. running mean: -3.196924\n",
      "ep 3794: ep_len:44 episode reward: total was 20.500000. running mean: -2.959954\n",
      "ep 3794: ep_len:1444 episode reward: total was 12.340000. running mean: -2.806955\n",
      "ep 3794: ep_len:3745 episode reward: total was -189.360000. running mean: -4.672485\n",
      "ep 3794: ep_len:686 episode reward: total was -56.780000. running mean: -5.193560\n",
      "ep 3794: ep_len:793 episode reward: total was 11.100000. running mean: -5.030625\n",
      "ep 3794: ep_len:604 episode reward: total was 12.960000. running mean: -4.850718\n",
      "ep 3794: ep_len:144 episode reward: total was 69.000000. running mean: -4.112211\n",
      "ep 3794: ep_len:106 episode reward: total was 50.000000. running mean: -3.571089\n",
      "ep 3794: ep_len:726 episode reward: total was 3.800000. running mean: -3.497378\n",
      "ep 3794: ep_len:2881 episode reward: total was -3.580000. running mean: -3.498205\n",
      "epsilon:0.009992 episode_count: 57036. steps_count: 61530481.000000\n",
      "ep 3795: ep_len:646 episode reward: total was 11.540000. running mean: -3.347822\n",
      "ep 3795: ep_len:741 episode reward: total was -15.180000. running mean: -3.466144\n",
      "ep 3795: ep_len:80 episode reward: total was 38.500000. running mean: -3.046483\n",
      "ep 3795: ep_len:3005 episode reward: total was 27.140000. running mean: -2.744618\n",
      "ep 3795: ep_len:500 episode reward: total was -13.620000. running mean: -2.853372\n",
      "ep 3795: ep_len:50 episode reward: total was 22.000000. running mean: -2.604838\n",
      "ep 3795: ep_len:122 episode reward: total was 58.000000. running mean: -1.998790\n",
      "ep 3795: ep_len:1374 episode reward: total was -211.660000. running mean: -4.095402\n",
      "ep 3795: ep_len:658 episode reward: total was 15.660000. running mean: -3.897848\n",
      "ep 3795: ep_len:876 episode reward: total was -56.810000. running mean: -4.426969\n",
      "ep 3795: ep_len:669 episode reward: total was 2.700000. running mean: -4.355700\n",
      "ep 3795: ep_len:1513 episode reward: total was 12.630000. running mean: -4.185843\n",
      "ep 3795: ep_len:101 episode reward: total was 47.500000. running mean: -3.668984\n",
      "ep 3795: ep_len:141 episode reward: total was 67.500000. running mean: -2.957294\n",
      "ep 3795: ep_len:796 episode reward: total was -32.760000. running mean: -3.255321\n",
      "ep 3795: ep_len:2814 episode reward: total was -2.330000. running mean: -3.246068\n",
      "epsilon:0.009992 episode_count: 57052. steps_count: 61544567.000000\n",
      "ep 3796: ep_len:611 episode reward: total was 13.980000. running mean: -3.073808\n",
      "ep 3796: ep_len:501 episode reward: total was -2.430000. running mean: -3.067369\n",
      "ep 3796: ep_len:2918 episode reward: total was 10.930000. running mean: -2.927396\n",
      "ep 3796: ep_len:1181 episode reward: total was -33.810000. running mean: -3.236222\n",
      "ep 3796: ep_len:151 episode reward: total was 71.000000. running mean: -2.493860\n",
      "ep 3796: ep_len:68 episode reward: total was 31.000000. running mean: -2.158921\n",
      "ep 3796: ep_len:4871 episode reward: total was -3162.300000. running mean: -33.760332\n",
      "ep 3796: ep_len:691 episode reward: total was 18.960000. running mean: -33.233128\n",
      "ep 3796: ep_len:771 episode reward: total was -11.260000. running mean: -33.013397\n",
      "ep 3796: ep_len:711 episode reward: total was 46.020000. running mean: -32.223063\n",
      "ep 3796: ep_len:710 episode reward: total was -4.180000. running mean: -31.942633\n",
      "ep 3796: ep_len:74 episode reward: total was 35.500000. running mean: -31.268206\n",
      "ep 3796: ep_len:55 episode reward: total was 24.500000. running mean: -30.710524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3796: ep_len:68 episode reward: total was 29.500000. running mean: -30.108419\n",
      "ep 3796: ep_len:1042 episode reward: total was 1.740000. running mean: -29.789935\n",
      "ep 3796: ep_len:2837 episode reward: total was -9.930000. running mean: -29.591335\n",
      "ep 3796: ep_len:49 episode reward: total was 23.000000. running mean: -29.065422\n",
      "epsilon:0.009992 episode_count: 57069. steps_count: 61561876.000000\n",
      "ep 3797: ep_len:659 episode reward: total was -7.720000. running mean: -28.851968\n",
      "ep 3797: ep_len:184 episode reward: total was -6.830000. running mean: -28.631748\n",
      "ep 3797: ep_len:55 episode reward: total was 26.000000. running mean: -28.085431\n",
      "ep 3797: ep_len:2995 episode reward: total was -26.070000. running mean: -28.065276\n",
      "ep 3797: ep_len:550 episode reward: total was -36.080000. running mean: -28.145424\n",
      "ep 3797: ep_len:905 episode reward: total was 77.650000. running mean: -27.087469\n",
      "ep 3797: ep_len:3840 episode reward: total was -77.920000. running mean: -27.595795\n",
      "ep 3797: ep_len:534 episode reward: total was -30.180000. running mean: -27.621637\n",
      "ep 3797: ep_len:781 episode reward: total was -1.040000. running mean: -27.355820\n",
      "ep 3797: ep_len:1083 episode reward: total was 19.730000. running mean: -26.884962\n",
      "ep 3797: ep_len:40 episode reward: total was 18.500000. running mean: -26.431113\n",
      "ep 3797: ep_len:50 episode reward: total was 22.000000. running mean: -25.946801\n",
      "ep 3797: ep_len:716 episode reward: total was -74.820000. running mean: -26.435533\n",
      "ep 3797: ep_len:2806 episode reward: total was -404.240000. running mean: -30.213578\n",
      "ep 3797: ep_len:51 episode reward: total was 24.000000. running mean: -29.671442\n",
      "epsilon:0.009992 episode_count: 57084. steps_count: 61577125.000000\n",
      "ep 3798: ep_len:1175 episode reward: total was 24.500000. running mean: -29.129728\n",
      "ep 3798: ep_len:673 episode reward: total was -3.540000. running mean: -28.873831\n",
      "ep 3798: ep_len:3001 episode reward: total was -90.440000. running mean: -29.489492\n",
      "ep 3798: ep_len:1079 episode reward: total was -5.540000. running mean: -29.249997\n",
      "ep 3798: ep_len:52 episode reward: total was 24.500000. running mean: -28.712497\n",
      "ep 3798: ep_len:42 episode reward: total was 19.500000. running mean: -28.230372\n",
      "ep 3798: ep_len:500 episode reward: total was 43.700000. running mean: -27.511069\n",
      "ep 3798: ep_len:3691 episode reward: total was -76.000000. running mean: -27.995958\n",
      "ep 3798: ep_len:761 episode reward: total was -51.400000. running mean: -28.229998\n",
      "ep 3798: ep_len:884 episode reward: total was 62.050000. running mean: -27.327198\n",
      "ep 3798: ep_len:630 episode reward: total was 1.110000. running mean: -27.042826\n",
      "ep 3798: ep_len:70 episode reward: total was 30.500000. running mean: -26.467398\n",
      "ep 3798: ep_len:90 episode reward: total was 42.000000. running mean: -25.782724\n",
      "ep 3798: ep_len:655 episode reward: total was -1.700000. running mean: -25.541897\n",
      "ep 3798: ep_len:46 episode reward: total was 21.500000. running mean: -25.071478\n",
      "ep 3798: ep_len:39 episode reward: total was 18.000000. running mean: -24.640763\n",
      "epsilon:0.009992 episode_count: 57100. steps_count: 61590513.000000\n",
      "ep 3799: ep_len:1449 episode reward: total was -9.860000. running mean: -24.492956\n",
      "ep 3799: ep_len:964 episode reward: total was 24.410000. running mean: -24.003926\n",
      "ep 3799: ep_len:3014 episode reward: total was -60.800000. running mean: -24.371887\n",
      "ep 3799: ep_len:543 episode reward: total was -15.950000. running mean: -24.287668\n",
      "ep 3799: ep_len:174 episode reward: total was 84.000000. running mean: -23.204791\n",
      "ep 3799: ep_len:45 episode reward: total was 19.500000. running mean: -22.777743\n",
      "ep 3799: ep_len:1373 episode reward: total was -189.450000. running mean: -24.444466\n",
      "ep 3799: ep_len:3646 episode reward: total was -37.670000. running mean: -24.576721\n",
      "ep 3799: ep_len:556 episode reward: total was 1.350000. running mean: -24.317454\n",
      "ep 3799: ep_len:845 episode reward: total was 23.420000. running mean: -23.840079\n",
      "ep 3799: ep_len:721 episode reward: total was -11.140000. running mean: -23.713079\n",
      "ep 3799: ep_len:58 episode reward: total was 27.500000. running mean: -23.200948\n",
      "ep 3799: ep_len:161 episode reward: total was 73.000000. running mean: -22.238938\n",
      "ep 3799: ep_len:1454 episode reward: total was 2.710000. running mean: -21.989449\n",
      "ep 3799: ep_len:2870 episode reward: total was -22.880000. running mean: -21.998355\n",
      "epsilon:0.009992 episode_count: 57115. steps_count: 61608386.000000\n",
      "ep 3800: ep_len:675 episode reward: total was 25.740000. running mean: -21.520971\n",
      "ep 3800: ep_len:500 episode reward: total was 14.090000. running mean: -21.164861\n",
      "ep 3800: ep_len:3010 episode reward: total was 5.220000. running mean: -20.901013\n",
      "ep 3800: ep_len:1191 episode reward: total was -36.740000. running mean: -21.059403\n",
      "ep 3800: ep_len:42 episode reward: total was 20.510000. running mean: -20.643708\n",
      "ep 3800: ep_len:136 episode reward: total was 65.000000. running mean: -19.787271\n",
      "ep 3800: ep_len:56 episode reward: total was 26.500000. running mean: -19.324399\n",
      "ep 3800: ep_len:74 episode reward: total was 34.000000. running mean: -18.791155\n",
      "ep 3800: ep_len:859 episode reward: total was 32.210000. running mean: -18.281143\n",
      "ep 3800: ep_len:4133 episode reward: total was -782.750000. running mean: -25.925832\n",
      "ep 3800: ep_len:557 episode reward: total was -58.080000. running mean: -26.247373\n",
      "ep 3800: ep_len:809 episode reward: total was 17.730000. running mean: -25.807600\n",
      "ep 3800: ep_len:1509 episode reward: total was 1.600000. running mean: -25.533524\n",
      "ep 3800: ep_len:132 episode reward: total was 63.000000. running mean: -24.648188\n",
      "ep 3800: ep_len:42 episode reward: total was 18.000000. running mean: -24.221707\n",
      "ep 3800: ep_len:63 episode reward: total was 30.000000. running mean: -23.679489\n",
      "ep 3800: ep_len:1197 episode reward: total was -7.390000. running mean: -23.516595\n",
      "ep 3800: ep_len:2773 episode reward: total was -13.820000. running mean: -23.419629\n",
      "ep 3800: ep_len:63 episode reward: total was 28.500000. running mean: -22.900432\n",
      "epsilon:0.009992 episode_count: 57134. steps_count: 61626207.000000\n",
      "ep 3801: ep_len:1439 episode reward: total was 30.900000. running mean: -22.362428\n",
      "ep 3801: ep_len:662 episode reward: total was -11.180000. running mean: -22.250604\n",
      "ep 3801: ep_len:39 episode reward: total was 16.500000. running mean: -21.863098\n",
      "ep 3801: ep_len:2983 episode reward: total was -12.270000. running mean: -21.767167\n",
      "ep 3801: ep_len:500 episode reward: total was 6.920000. running mean: -21.480295\n",
      "ep 3801: ep_len:120 episode reward: total was 57.000000. running mean: -20.695492\n",
      "ep 3801: ep_len:1073 episode reward: total was 0.550000. running mean: -20.483037\n",
      "ep 3801: ep_len:3535 episode reward: total was -28.070000. running mean: -20.558907\n",
      "ep 3801: ep_len:1281 episode reward: total was -28.770000. running mean: -20.641018\n",
      "ep 3801: ep_len:738 episode reward: total was 36.650000. running mean: -20.068108\n",
      "ep 3801: ep_len:708 episode reward: total was -12.280000. running mean: -19.990227\n",
      "ep 3801: ep_len:135 episode reward: total was 64.500000. running mean: -19.145324\n",
      "ep 3801: ep_len:993 episode reward: total was 9.370000. running mean: -18.860171\n",
      "ep 3801: ep_len:2922 episode reward: total was -15.940000. running mean: -18.830969\n",
      "epsilon:0.009992 episode_count: 57148. steps_count: 61643335.000000\n",
      "ep 3802: ep_len:598 episode reward: total was -33.490000. running mean: -18.977560\n",
      "ep 3802: ep_len:723 episode reward: total was -12.130000. running mean: -18.909084\n",
      "ep 3802: ep_len:35 episode reward: total was 16.000000. running mean: -18.559993\n",
      "ep 3802: ep_len:3101 episode reward: total was -5.750000. running mean: -18.431893\n",
      "ep 3802: ep_len:500 episode reward: total was 16.910000. running mean: -18.078474\n",
      "ep 3802: ep_len:52 episode reward: total was 24.500000. running mean: -17.652690\n",
      "ep 3802: ep_len:834 episode reward: total was 28.370000. running mean: -17.192463\n",
      "ep 3802: ep_len:343 episode reward: total was 23.110000. running mean: -16.789438\n",
      "ep 3802: ep_len:578 episode reward: total was -40.850000. running mean: -17.030044\n",
      "ep 3802: ep_len:678 episode reward: total was 3.830000. running mean: -16.821443\n",
      "ep 3802: ep_len:500 episode reward: total was -11.480000. running mean: -16.768029\n",
      "ep 3802: ep_len:55 episode reward: total was 24.500000. running mean: -16.355348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3802: ep_len:111 episode reward: total was 54.000000. running mean: -15.651795\n",
      "ep 3802: ep_len:672 episode reward: total was 33.980000. running mean: -15.155477\n",
      "ep 3802: ep_len:2819 episode reward: total was -3.200000. running mean: -15.035922\n",
      "epsilon:0.009992 episode_count: 57163. steps_count: 61654934.000000\n",
      "ep 3803: ep_len:663 episode reward: total was -50.100000. running mean: -15.386563\n",
      "ep 3803: ep_len:1625 episode reward: total was -19.020000. running mean: -15.422897\n",
      "ep 3803: ep_len:3066 episode reward: total was -0.490000. running mean: -15.273568\n",
      "ep 3803: ep_len:500 episode reward: total was 0.690000. running mean: -15.113933\n",
      "ep 3803: ep_len:874 episode reward: total was 43.350000. running mean: -14.529293\n",
      "ep 3803: ep_len:339 episode reward: total was -27.280000. running mean: -14.656801\n",
      "ep 3803: ep_len:901 episode reward: total was 5.660000. running mean: -14.453633\n",
      "ep 3803: ep_len:843 episode reward: total was 42.160000. running mean: -13.887496\n",
      "ep 3803: ep_len:962 episode reward: total was 10.680000. running mean: -13.641821\n",
      "ep 3803: ep_len:67 episode reward: total was 32.000000. running mean: -13.185403\n",
      "ep 3803: ep_len:728 episode reward: total was 5.890000. running mean: -12.994649\n",
      "ep 3803: ep_len:2856 episode reward: total was -12.710000. running mean: -12.991802\n",
      "ep 3803: ep_len:59 episode reward: total was 26.500000. running mean: -12.596884\n",
      "epsilon:0.009992 episode_count: 57176. steps_count: 61668417.000000\n",
      "ep 3804: ep_len:715 episode reward: total was -35.440000. running mean: -12.825316\n",
      "ep 3804: ep_len:500 episode reward: total was -0.180000. running mean: -12.698862\n",
      "ep 3804: ep_len:53 episode reward: total was 23.500000. running mean: -12.336874\n",
      "ep 3804: ep_len:2972 episode reward: total was -6.210000. running mean: -12.275605\n",
      "ep 3804: ep_len:684 episode reward: total was -5.390000. running mean: -12.206749\n",
      "ep 3804: ep_len:1053 episode reward: total was -8.430000. running mean: -12.168982\n",
      "ep 3804: ep_len:3553 episode reward: total was -132.500000. running mean: -13.372292\n",
      "ep 3804: ep_len:504 episode reward: total was -26.440000. running mean: -13.502969\n",
      "ep 3804: ep_len:741 episode reward: total was 45.340000. running mean: -12.914539\n",
      "ep 3804: ep_len:974 episode reward: total was 8.540000. running mean: -12.699994\n",
      "ep 3804: ep_len:207 episode reward: total was 99.000000. running mean: -11.582994\n",
      "ep 3804: ep_len:42 episode reward: total was 19.500000. running mean: -11.272164\n",
      "ep 3804: ep_len:744 episode reward: total was -45.250000. running mean: -11.611942\n",
      "ep 3804: ep_len:2823 episode reward: total was -15.460000. running mean: -11.650423\n",
      "epsilon:0.009992 episode_count: 57190. steps_count: 61683982.000000\n",
      "ep 3805: ep_len:1435 episode reward: total was 0.800000. running mean: -11.525919\n",
      "ep 3805: ep_len:500 episode reward: total was -1.400000. running mean: -11.424659\n",
      "ep 3805: ep_len:3107 episode reward: total was -44.590000. running mean: -11.756313\n",
      "ep 3805: ep_len:1137 episode reward: total was -5.970000. running mean: -11.698450\n",
      "ep 3805: ep_len:28 episode reward: total was 12.500000. running mean: -11.456465\n",
      "ep 3805: ep_len:120 episode reward: total was 57.000000. running mean: -10.771901\n",
      "ep 3805: ep_len:53 episode reward: total was 25.000000. running mean: -10.414182\n",
      "ep 3805: ep_len:500 episode reward: total was 54.600000. running mean: -9.764040\n",
      "ep 3805: ep_len:643 episode reward: total was 2.900000. running mean: -9.637399\n",
      "ep 3805: ep_len:1589 episode reward: total was -23.150000. running mean: -9.772525\n",
      "ep 3805: ep_len:719 episode reward: total was 52.960000. running mean: -9.145200\n",
      "ep 3805: ep_len:869 episode reward: total was 20.550000. running mean: -8.848248\n",
      "ep 3805: ep_len:87 episode reward: total was 39.000000. running mean: -8.369766\n",
      "ep 3805: ep_len:84 episode reward: total was 40.500000. running mean: -7.881068\n",
      "ep 3805: ep_len:1099 episode reward: total was -13.420000. running mean: -7.936457\n",
      "ep 3805: ep_len:2852 episode reward: total was 3.200000. running mean: -7.825093\n",
      "epsilon:0.009992 episode_count: 57206. steps_count: 61698804.000000\n",
      "ep 3806: ep_len:3731 episode reward: total was -502.650000. running mean: -12.773342\n",
      "ep 3806: ep_len:727 episode reward: total was -24.210000. running mean: -12.887708\n",
      "ep 3806: ep_len:2910 episode reward: total was -23.590000. running mean: -12.994731\n",
      "ep 3806: ep_len:631 episode reward: total was 4.120000. running mean: -12.823584\n",
      "ep 3806: ep_len:65 episode reward: total was 31.000000. running mean: -12.385348\n",
      "ep 3806: ep_len:93 episode reward: total was 43.500000. running mean: -11.826495\n",
      "ep 3806: ep_len:111 episode reward: total was 51.000000. running mean: -11.198230\n",
      "ep 3806: ep_len:968 episode reward: total was 5.230000. running mean: -11.033947\n",
      "ep 3806: ep_len:3637 episode reward: total was -53.310000. running mean: -11.456708\n",
      "ep 3806: ep_len:550 episode reward: total was -14.230000. running mean: -11.484441\n",
      "ep 3806: ep_len:704 episode reward: total was 24.490000. running mean: -11.124696\n",
      "ep 3806: ep_len:891 episode reward: total was 14.960000. running mean: -10.863849\n",
      "ep 3806: ep_len:76 episode reward: total was 36.500000. running mean: -10.390211\n",
      "ep 3806: ep_len:143 episode reward: total was 70.000000. running mean: -9.586309\n",
      "ep 3806: ep_len:1210 episode reward: total was -7.680000. running mean: -9.567246\n",
      "ep 3806: ep_len:2878 episode reward: total was -16.260000. running mean: -9.634173\n",
      "epsilon:0.009992 episode_count: 57222. steps_count: 61718129.000000\n",
      "ep 3807: ep_len:550 episode reward: total was -27.450000. running mean: -9.812332\n",
      "ep 3807: ep_len:785 episode reward: total was -6.180000. running mean: -9.776008\n",
      "ep 3807: ep_len:2982 episode reward: total was -22.190000. running mean: -9.900148\n",
      "ep 3807: ep_len:663 episode reward: total was 1.000000. running mean: -9.791147\n",
      "ep 3807: ep_len:75 episode reward: total was 36.000000. running mean: -9.333235\n",
      "ep 3807: ep_len:1923 episode reward: total was -85.700000. running mean: -10.096903\n",
      "ep 3807: ep_len:342 episode reward: total was 16.700000. running mean: -9.828934\n",
      "ep 3807: ep_len:2365 episode reward: total was -177.840000. running mean: -11.509044\n",
      "ep 3807: ep_len:750 episode reward: total was 42.190000. running mean: -10.972054\n",
      "ep 3807: ep_len:957 episode reward: total was 14.730000. running mean: -10.715034\n",
      "ep 3807: ep_len:151 episode reward: total was 69.500000. running mean: -9.912883\n",
      "ep 3807: ep_len:648 episode reward: total was 11.140000. running mean: -9.702354\n",
      "ep 3807: ep_len:2821 episode reward: total was -35.520000. running mean: -9.960531\n",
      "epsilon:0.009992 episode_count: 57235. steps_count: 61733141.000000\n",
      "ep 3808: ep_len:1018 episode reward: total was -109.630000. running mean: -10.957225\n",
      "ep 3808: ep_len:772 episode reward: total was -29.830000. running mean: -11.145953\n",
      "ep 3808: ep_len:3058 episode reward: total was -108.800000. running mean: -12.122494\n",
      "ep 3808: ep_len:557 episode reward: total was 4.390000. running mean: -11.957369\n",
      "ep 3808: ep_len:107 episode reward: total was 50.500000. running mean: -11.332795\n",
      "ep 3808: ep_len:1401 episode reward: total was -67.970000. running mean: -11.899167\n",
      "ep 3808: ep_len:4167 episode reward: total was -53.930000. running mean: -12.319475\n",
      "ep 3808: ep_len:608 episode reward: total was -0.060000. running mean: -12.196881\n",
      "ep 3808: ep_len:719 episode reward: total was -18.080000. running mean: -12.255712\n",
      "ep 3808: ep_len:1145 episode reward: total was -8.920000. running mean: -12.222355\n",
      "ep 3808: ep_len:81 episode reward: total was 37.500000. running mean: -11.725131\n",
      "ep 3808: ep_len:143 episode reward: total was 68.500000. running mean: -10.922880\n",
      "ep 3808: ep_len:39 episode reward: total was 18.000000. running mean: -10.633651\n",
      "ep 3808: ep_len:94 episode reward: total was 44.000000. running mean: -10.087315\n",
      "ep 3808: ep_len:780 episode reward: total was -13.580000. running mean: -10.122241\n",
      "ep 3808: ep_len:2822 episode reward: total was -44.730000. running mean: -10.468319\n",
      "ep 3808: ep_len:42 episode reward: total was 16.500000. running mean: -10.198636\n",
      "epsilon:0.009992 episode_count: 57252. steps_count: 61750694.000000\n",
      "ep 3809: ep_len:1482 episode reward: total was -204.520000. running mean: -12.141850\n",
      "ep 3809: ep_len:1282 episode reward: total was -54.010000. running mean: -12.560531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3809: ep_len:2965 episode reward: total was -28.630000. running mean: -12.721226\n",
      "ep 3809: ep_len:650 episode reward: total was 2.840000. running mean: -12.565613\n",
      "ep 3809: ep_len:656 episode reward: total was -8.670000. running mean: -12.526657\n",
      "ep 3809: ep_len:604 episode reward: total was 18.820000. running mean: -12.213191\n",
      "ep 3809: ep_len:1607 episode reward: total was -83.370000. running mean: -12.924759\n",
      "ep 3809: ep_len:7266 episode reward: total was -86.970000. running mean: -13.665211\n",
      "ep 3809: ep_len:673 episode reward: total was -2.710000. running mean: -13.555659\n",
      "ep 3809: ep_len:128 episode reward: total was 62.500000. running mean: -12.795103\n",
      "ep 3809: ep_len:96 episode reward: total was 46.500000. running mean: -12.202152\n",
      "ep 3809: ep_len:1109 episode reward: total was -12.800000. running mean: -12.208130\n",
      "ep 3809: ep_len:2791 episode reward: total was 5.160000. running mean: -12.034449\n",
      "epsilon:0.009992 episode_count: 57265. steps_count: 61772003.000000\n",
      "ep 3810: ep_len:1151 episode reward: total was -14.730000. running mean: -12.061404\n",
      "ep 3810: ep_len:811 episode reward: total was -12.310000. running mean: -12.063890\n",
      "ep 3810: ep_len:3058 episode reward: total was -2.100000. running mean: -11.964251\n",
      "ep 3810: ep_len:516 episode reward: total was 16.080000. running mean: -11.683809\n",
      "ep 3810: ep_len:50 episode reward: total was 23.500000. running mean: -11.331971\n",
      "ep 3810: ep_len:724 episode reward: total was -23.630000. running mean: -11.454951\n",
      "ep 3810: ep_len:3652 episode reward: total was -150.400000. running mean: -12.844401\n",
      "ep 3810: ep_len:2197 episode reward: total was -819.000000. running mean: -20.905957\n",
      "ep 3810: ep_len:835 episode reward: total was 39.700000. running mean: -20.299898\n",
      "ep 3810: ep_len:702 episode reward: total was -9.900000. running mean: -20.195899\n",
      "ep 3810: ep_len:142 episode reward: total was 66.500000. running mean: -19.328940\n",
      "ep 3810: ep_len:646 episode reward: total was -9.290000. running mean: -19.228550\n",
      "ep 3810: ep_len:2793 episode reward: total was -23.600000. running mean: -19.272265\n",
      "epsilon:0.009992 episode_count: 57278. steps_count: 61789280.000000\n",
      "ep 3811: ep_len:673 episode reward: total was -6.050000. running mean: -19.140042\n",
      "ep 3811: ep_len:753 episode reward: total was -12.760000. running mean: -19.076242\n",
      "ep 3811: ep_len:2982 episode reward: total was -35.830000. running mean: -19.243779\n",
      "ep 3811: ep_len:560 episode reward: total was -56.870000. running mean: -19.620042\n",
      "ep 3811: ep_len:747 episode reward: total was -12.900000. running mean: -19.552841\n",
      "ep 3811: ep_len:3704 episode reward: total was -17.290000. running mean: -19.530213\n",
      "ep 3811: ep_len:859 episode reward: total was 12.490000. running mean: -19.210011\n",
      "ep 3811: ep_len:7425 episode reward: total was 16.180000. running mean: -18.856111\n",
      "ep 3811: ep_len:500 episode reward: total was -1.990000. running mean: -18.687450\n",
      "ep 3811: ep_len:73 episode reward: total was 33.500000. running mean: -18.165575\n",
      "ep 3811: ep_len:847 episode reward: total was 21.970000. running mean: -17.764219\n",
      "ep 3811: ep_len:2784 episode reward: total was -0.760000. running mean: -17.594177\n",
      "epsilon:0.009992 episode_count: 57290. steps_count: 61811187.000000\n",
      "ep 3812: ep_len:1116 episode reward: total was -2.140000. running mean: -17.439635\n",
      "ep 3812: ep_len:216 episode reward: total was 2.920000. running mean: -17.236039\n",
      "ep 3812: ep_len:93 episode reward: total was 43.500000. running mean: -16.628679\n",
      "ep 3812: ep_len:832 episode reward: total was 13.320000. running mean: -16.329192\n",
      "ep 3812: ep_len:61 episode reward: total was 29.000000. running mean: -15.875900\n",
      "ep 3812: ep_len:50 episode reward: total was 22.000000. running mean: -15.497141\n",
      "ep 3812: ep_len:624 episode reward: total was 2.030000. running mean: -15.321869\n",
      "ep 3812: ep_len:4003 episode reward: total was -99.140000. running mean: -16.160051\n",
      "ep 3812: ep_len:579 episode reward: total was 9.360000. running mean: -15.904850\n",
      "ep 3812: ep_len:732 episode reward: total was 50.060000. running mean: -15.245202\n",
      "ep 3812: ep_len:635 episode reward: total was -13.280000. running mean: -15.225550\n",
      "ep 3812: ep_len:57 episode reward: total was 25.500000. running mean: -14.818294\n",
      "ep 3812: ep_len:815 episode reward: total was -17.380000. running mean: -14.843911\n",
      "ep 3812: ep_len:2799 episode reward: total was 8.790000. running mean: -14.607572\n",
      "ep 3812: ep_len:51 episode reward: total was 24.000000. running mean: -14.221496\n",
      "epsilon:0.009992 episode_count: 57305. steps_count: 61823850.000000\n",
      "ep 3813: ep_len:1464 episode reward: total was 28.160000. running mean: -13.797682\n",
      "ep 3813: ep_len:500 episode reward: total was 31.290000. running mean: -13.346805\n",
      "ep 3813: ep_len:3070 episode reward: total was -83.570000. running mean: -14.049037\n",
      "ep 3813: ep_len:616 episode reward: total was 15.930000. running mean: -13.749246\n",
      "ep 3813: ep_len:52 episode reward: total was 24.500000. running mean: -13.366754\n",
      "ep 3813: ep_len:1096 episode reward: total was -23.550000. running mean: -13.468586\n",
      "ep 3813: ep_len:3620 episode reward: total was -14.490000. running mean: -13.478800\n",
      "ep 3813: ep_len:819 episode reward: total was -19.900000. running mean: -13.543012\n",
      "ep 3813: ep_len:7278 episode reward: total was 32.710000. running mean: -13.080482\n",
      "ep 3813: ep_len:1152 episode reward: total was -28.990000. running mean: -13.239577\n",
      "ep 3813: ep_len:175 episode reward: total was 78.500000. running mean: -12.322182\n",
      "ep 3813: ep_len:1111 episode reward: total was -13.210000. running mean: -12.331060\n",
      "ep 3813: ep_len:2755 episode reward: total was 1.640000. running mean: -12.191349\n",
      "epsilon:0.009992 episode_count: 57318. steps_count: 61847558.000000\n",
      "ep 3814: ep_len:1080 episode reward: total was -14.620000. running mean: -12.215636\n",
      "ep 3814: ep_len:712 episode reward: total was -22.570000. running mean: -12.319179\n",
      "ep 3814: ep_len:2923 episode reward: total was -41.580000. running mean: -12.611788\n",
      "ep 3814: ep_len:1693 episode reward: total was -100.370000. running mean: -13.489370\n",
      "ep 3814: ep_len:1431 episode reward: total was -45.450000. running mean: -13.808976\n",
      "ep 3814: ep_len:3631 episode reward: total was -19.030000. running mean: -13.861186\n",
      "ep 3814: ep_len:916 episode reward: total was -37.430000. running mean: -14.096874\n",
      "ep 3814: ep_len:831 episode reward: total was 37.300000. running mean: -13.582906\n",
      "ep 3814: ep_len:631 episode reward: total was 4.240000. running mean: -13.404677\n",
      "ep 3814: ep_len:107 episode reward: total was 52.000000. running mean: -12.750630\n",
      "ep 3814: ep_len:653 episode reward: total was -5.760000. running mean: -12.680724\n",
      "ep 3814: ep_len:2893 episode reward: total was 12.700000. running mean: -12.426916\n",
      "ep 3814: ep_len:59 episode reward: total was 26.500000. running mean: -12.037647\n",
      "epsilon:0.009992 episode_count: 57331. steps_count: 61865118.000000\n",
      "ep 3815: ep_len:1421 episode reward: total was 3.880000. running mean: -11.878471\n",
      "ep 3815: ep_len:697 episode reward: total was -15.420000. running mean: -11.913886\n",
      "ep 3815: ep_len:2965 episode reward: total was -85.070000. running mean: -12.645447\n",
      "ep 3815: ep_len:584 episode reward: total was 1.630000. running mean: -12.502693\n",
      "ep 3815: ep_len:79 episode reward: total was 38.000000. running mean: -11.997666\n",
      "ep 3815: ep_len:1409 episode reward: total was -1.550000. running mean: -11.893189\n",
      "ep 3815: ep_len:3647 episode reward: total was -12.810000. running mean: -11.902357\n",
      "ep 3815: ep_len:743 episode reward: total was -14.570000. running mean: -11.929034\n",
      "ep 3815: ep_len:859 episode reward: total was 64.160000. running mean: -11.168143\n",
      "ep 3815: ep_len:658 episode reward: total was -8.740000. running mean: -11.143862\n",
      "ep 3815: ep_len:80 episode reward: total was 38.500000. running mean: -10.647423\n",
      "ep 3815: ep_len:32 episode reward: total was 14.500000. running mean: -10.395949\n",
      "ep 3815: ep_len:500 episode reward: total was 23.030000. running mean: -10.061689\n",
      "ep 3815: ep_len:2908 episode reward: total was -30.370000. running mean: -10.264773\n",
      "epsilon:0.009992 episode_count: 57345. steps_count: 61881700.000000\n",
      "ep 3816: ep_len:944 episode reward: total was -81.210000. running mean: -10.974225\n",
      "ep 3816: ep_len:721 episode reward: total was -25.280000. running mean: -11.117283\n",
      "ep 3816: ep_len:2950 episode reward: total was -67.470000. running mean: -11.680810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3816: ep_len:840 episode reward: total was -11.220000. running mean: -11.676202\n",
      "ep 3816: ep_len:47 episode reward: total was 20.500000. running mean: -11.354440\n",
      "ep 3816: ep_len:752 episode reward: total was -17.900000. running mean: -11.419895\n",
      "ep 3816: ep_len:3926 episode reward: total was -63.550000. running mean: -11.941196\n",
      "ep 3816: ep_len:1134 episode reward: total was -115.880000. running mean: -12.980584\n",
      "ep 3816: ep_len:773 episode reward: total was 13.260000. running mean: -12.718179\n",
      "ep 3816: ep_len:658 episode reward: total was -4.460000. running mean: -12.635597\n",
      "ep 3816: ep_len:71 episode reward: total was 29.500000. running mean: -12.214241\n",
      "ep 3816: ep_len:83 episode reward: total was 40.000000. running mean: -11.692098\n",
      "ep 3816: ep_len:1000 episode reward: total was -2.010000. running mean: -11.595277\n",
      "ep 3816: ep_len:2836 episode reward: total was -7.620000. running mean: -11.555525\n",
      "ep 3816: ep_len:71 episode reward: total was 32.500000. running mean: -11.114969\n",
      "epsilon:0.009992 episode_count: 57360. steps_count: 61898506.000000\n",
      "ep 3817: ep_len:623 episode reward: total was -7.560000. running mean: -11.079420\n",
      "ep 3817: ep_len:971 episode reward: total was 11.810000. running mean: -10.850525\n",
      "ep 3817: ep_len:2878 episode reward: total was -48.470000. running mean: -11.226720\n",
      "ep 3817: ep_len:501 episode reward: total was 1.880000. running mean: -11.095653\n",
      "ep 3817: ep_len:46 episode reward: total was 21.500000. running mean: -10.769696\n",
      "ep 3817: ep_len:129 episode reward: total was 63.000000. running mean: -10.032000\n",
      "ep 3817: ep_len:56 episode reward: total was 25.000000. running mean: -9.681680\n",
      "ep 3817: ep_len:856 episode reward: total was 20.020000. running mean: -9.384663\n",
      "ep 3817: ep_len:316 episode reward: total was -1.010000. running mean: -9.300916\n",
      "ep 3817: ep_len:594 episode reward: total was -27.040000. running mean: -9.478307\n",
      "ep 3817: ep_len:7150 episode reward: total was -11.800000. running mean: -9.501524\n",
      "ep 3817: ep_len:594 episode reward: total was -4.330000. running mean: -9.449809\n",
      "ep 3817: ep_len:133 episode reward: total was 63.500000. running mean: -8.720311\n",
      "ep 3817: ep_len:85 episode reward: total was 39.500000. running mean: -8.238107\n",
      "ep 3817: ep_len:1203 episode reward: total was -26.030000. running mean: -8.416026\n",
      "ep 3817: ep_len:2857 episode reward: total was -0.210000. running mean: -8.333966\n",
      "ep 3817: ep_len:54 episode reward: total was 25.500000. running mean: -7.995626\n",
      "epsilon:0.009992 episode_count: 57377. steps_count: 61917552.000000\n",
      "ep 3818: ep_len:671 episode reward: total was -17.700000. running mean: -8.092670\n",
      "ep 3818: ep_len:500 episode reward: total was 25.380000. running mean: -7.757943\n",
      "ep 3818: ep_len:2942 episode reward: total was -105.560000. running mean: -8.735964\n",
      "ep 3818: ep_len:653 episode reward: total was -0.730000. running mean: -8.655904\n",
      "ep 3818: ep_len:139 episode reward: total was 63.500000. running mean: -7.934345\n",
      "ep 3818: ep_len:73 episode reward: total was 35.000000. running mean: -7.505002\n",
      "ep 3818: ep_len:41 episode reward: total was 19.000000. running mean: -7.239952\n",
      "ep 3818: ep_len:878 episode reward: total was 36.740000. running mean: -6.800152\n",
      "ep 3818: ep_len:3554 episode reward: total was -12.670000. running mean: -6.858851\n",
      "ep 3818: ep_len:548 episode reward: total was -41.150000. running mean: -7.201762\n",
      "ep 3818: ep_len:7308 episode reward: total was -2.010000. running mean: -7.149845\n",
      "ep 3818: ep_len:741 episode reward: total was -23.060000. running mean: -7.308946\n",
      "ep 3818: ep_len:62 episode reward: total was 29.500000. running mean: -6.940857\n",
      "ep 3818: ep_len:56 episode reward: total was 25.000000. running mean: -6.621448\n",
      "ep 3818: ep_len:500 episode reward: total was 19.080000. running mean: -6.364434\n",
      "ep 3818: ep_len:2834 episode reward: total was -27.780000. running mean: -6.578589\n",
      "epsilon:0.009992 episode_count: 57393. steps_count: 61939052.000000\n",
      "ep 3819: ep_len:1385 episode reward: total was -42.490000. running mean: -6.937704\n",
      "ep 3819: ep_len:629 episode reward: total was 4.680000. running mean: -6.821526\n",
      "ep 3819: ep_len:52 episode reward: total was 24.500000. running mean: -6.508311\n",
      "ep 3819: ep_len:3003 episode reward: total was -91.920000. running mean: -7.362428\n",
      "ep 3819: ep_len:650 episode reward: total was -163.280000. running mean: -8.921604\n",
      "ep 3819: ep_len:44 episode reward: total was 20.500000. running mean: -8.627388\n",
      "ep 3819: ep_len:42 episode reward: total was 19.500000. running mean: -8.346114\n",
      "ep 3819: ep_len:1089 episode reward: total was -36.230000. running mean: -8.624953\n",
      "ep 3819: ep_len:657 episode reward: total was 11.460000. running mean: -8.424103\n",
      "ep 3819: ep_len:521 episode reward: total was 2.010000. running mean: -8.319762\n",
      "ep 3819: ep_len:677 episode reward: total was 13.190000. running mean: -8.104665\n",
      "ep 3819: ep_len:1148 episode reward: total was -21.250000. running mean: -8.236118\n",
      "ep 3819: ep_len:91 episode reward: total was 41.000000. running mean: -7.743757\n",
      "ep 3819: ep_len:1164 episode reward: total was -19.310000. running mean: -7.859419\n",
      "ep 3819: ep_len:2787 episode reward: total was -8.810000. running mean: -7.868925\n",
      "ep 3819: ep_len:67 episode reward: total was 30.500000. running mean: -7.485236\n",
      "epsilon:0.009992 episode_count: 57409. steps_count: 61953058.000000\n",
      "ep 3820: ep_len:652 episode reward: total was -3.990000. running mean: -7.450283\n",
      "ep 3820: ep_len:713 episode reward: total was -12.230000. running mean: -7.498081\n",
      "ep 3820: ep_len:3094 episode reward: total was -64.900000. running mean: -8.072100\n",
      "ep 3820: ep_len:1438 episode reward: total was -9.540000. running mean: -8.086779\n",
      "ep 3820: ep_len:30 episode reward: total was 13.500000. running mean: -7.870911\n",
      "ep 3820: ep_len:106 episode reward: total was 51.500000. running mean: -7.277202\n",
      "ep 3820: ep_len:62 episode reward: total was 29.500000. running mean: -6.909430\n",
      "ep 3820: ep_len:500 episode reward: total was 49.450000. running mean: -6.345836\n",
      "ep 3820: ep_len:668 episode reward: total was 32.660000. running mean: -5.955777\n",
      "ep 3820: ep_len:636 episode reward: total was -2.350000. running mean: -5.919719\n",
      "ep 3820: ep_len:724 episode reward: total was -57.630000. running mean: -6.436822\n",
      "ep 3820: ep_len:673 episode reward: total was 27.850000. running mean: -6.093954\n",
      "ep 3820: ep_len:57 episode reward: total was 25.500000. running mean: -5.778014\n",
      "ep 3820: ep_len:121 episode reward: total was 57.500000. running mean: -5.145234\n",
      "ep 3820: ep_len:1499 episode reward: total was 7.790000. running mean: -5.015882\n",
      "ep 3820: ep_len:2848 episode reward: total was 8.390000. running mean: -4.881823\n",
      "ep 3820: ep_len:61 episode reward: total was 29.000000. running mean: -4.543005\n",
      "epsilon:0.009992 episode_count: 57426. steps_count: 61966940.000000\n",
      "ep 3821: ep_len:1089 episode reward: total was -18.570000. running mean: -4.683275\n",
      "ep 3821: ep_len:634 episode reward: total was -7.970000. running mean: -4.716142\n",
      "ep 3821: ep_len:70 episode reward: total was 29.000000. running mean: -4.378981\n",
      "ep 3821: ep_len:2985 episode reward: total was -86.250000. running mean: -5.197691\n",
      "ep 3821: ep_len:599 episode reward: total was -8.320000. running mean: -5.228914\n",
      "ep 3821: ep_len:58 episode reward: total was 23.000000. running mean: -4.946625\n",
      "ep 3821: ep_len:55 episode reward: total was 24.500000. running mean: -4.652159\n",
      "ep 3821: ep_len:635 episode reward: total was 1.130000. running mean: -4.594337\n",
      "ep 3821: ep_len:3857 episode reward: total was -19.530000. running mean: -4.743694\n",
      "ep 3821: ep_len:669 episode reward: total was -42.160000. running mean: -5.117857\n",
      "ep 3821: ep_len:691 episode reward: total was 1.570000. running mean: -5.050978\n",
      "ep 3821: ep_len:500 episode reward: total was 23.310000. running mean: -4.767368\n",
      "ep 3821: ep_len:43 episode reward: total was 20.000000. running mean: -4.519695\n",
      "ep 3821: ep_len:92 episode reward: total was 44.500000. running mean: -4.029498\n",
      "ep 3821: ep_len:958 episode reward: total was -57.870000. running mean: -4.567903\n",
      "ep 3821: ep_len:2853 episode reward: total was -86.690000. running mean: -5.389124\n",
      "epsilon:0.009992 episode_count: 57442. steps_count: 61982728.000000\n",
      "ep 3822: ep_len:721 episode reward: total was -21.730000. running mean: -5.552532\n",
      "ep 3822: ep_len:1674 episode reward: total was -50.120000. running mean: -5.998207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3822: ep_len:2984 episode reward: total was -35.530000. running mean: -6.293525\n",
      "ep 3822: ep_len:548 episode reward: total was -6.810000. running mean: -6.298690\n",
      "ep 3822: ep_len:154 episode reward: total was 74.000000. running mean: -5.495703\n",
      "ep 3822: ep_len:87 episode reward: total was 40.500000. running mean: -5.035746\n",
      "ep 3822: ep_len:47 episode reward: total was 20.500000. running mean: -4.780388\n",
      "ep 3822: ep_len:848 episode reward: total was 26.550000. running mean: -4.467085\n",
      "ep 3822: ep_len:314 episode reward: total was 18.780000. running mean: -4.234614\n",
      "ep 3822: ep_len:942 episode reward: total was -34.180000. running mean: -4.534068\n",
      "ep 3822: ep_len:855 episode reward: total was 62.250000. running mean: -3.866227\n",
      "ep 3822: ep_len:1500 episode reward: total was 16.390000. running mean: -3.663665\n",
      "ep 3822: ep_len:66 episode reward: total was 30.000000. running mean: -3.327028\n",
      "ep 3822: ep_len:172 episode reward: total was 80.000000. running mean: -2.493758\n",
      "ep 3822: ep_len:42 episode reward: total was 19.500000. running mean: -2.273820\n",
      "ep 3822: ep_len:84 episode reward: total was 40.500000. running mean: -1.846082\n",
      "ep 3822: ep_len:719 episode reward: total was -56.610000. running mean: -2.393721\n",
      "ep 3822: ep_len:2879 episode reward: total was -4.590000. running mean: -2.415684\n",
      "epsilon:0.009992 episode_count: 57460. steps_count: 61997364.000000\n",
      "ep 3823: ep_len:908 episode reward: total was -177.350000. running mean: -4.165027\n",
      "ep 3823: ep_len:669 episode reward: total was -16.710000. running mean: -4.290477\n",
      "ep 3823: ep_len:67 episode reward: total was 30.500000. running mean: -3.942572\n",
      "ep 3823: ep_len:2924 episode reward: total was -65.470000. running mean: -4.557846\n",
      "ep 3823: ep_len:649 episode reward: total was 15.090000. running mean: -4.361368\n",
      "ep 3823: ep_len:79 episode reward: total was 36.500000. running mean: -3.952754\n",
      "ep 3823: ep_len:1104 episode reward: total was -1.370000. running mean: -3.926927\n",
      "ep 3823: ep_len:500 episode reward: total was 25.180000. running mean: -3.635857\n",
      "ep 3823: ep_len:780 episode reward: total was -12.980000. running mean: -3.729299\n",
      "ep 3823: ep_len:842 episode reward: total was 36.370000. running mean: -3.328306\n",
      "ep 3823: ep_len:559 episode reward: total was 31.790000. running mean: -2.977123\n",
      "ep 3823: ep_len:82 episode reward: total was 36.500000. running mean: -2.582352\n",
      "ep 3823: ep_len:121 episode reward: total was 56.000000. running mean: -1.996528\n",
      "ep 3823: ep_len:52 episode reward: total was 24.500000. running mean: -1.731563\n",
      "ep 3823: ep_len:790 episode reward: total was -26.610000. running mean: -1.980347\n",
      "ep 3823: ep_len:2853 episode reward: total was -20.970000. running mean: -2.170244\n",
      "epsilon:0.009992 episode_count: 57476. steps_count: 62010343.000000\n",
      "ep 3824: ep_len:608 episode reward: total was -9.240000. running mean: -2.240941\n",
      "ep 3824: ep_len:683 episode reward: total was -13.020000. running mean: -2.348732\n",
      "ep 3824: ep_len:82 episode reward: total was 39.500000. running mean: -1.930244\n",
      "ep 3824: ep_len:1117 episode reward: total was -7.670000. running mean: -1.987642\n",
      "ep 3824: ep_len:131 episode reward: total was 61.000000. running mean: -1.357766\n",
      "ep 3824: ep_len:83 episode reward: total was 40.000000. running mean: -0.944188\n",
      "ep 3824: ep_len:1019 episode reward: total was -25.330000. running mean: -1.188046\n",
      "ep 3824: ep_len:3746 episode reward: total was -29.680000. running mean: -1.472966\n",
      "ep 3824: ep_len:531 episode reward: total was -29.200000. running mean: -1.750236\n",
      "ep 3824: ep_len:747 episode reward: total was -5.370000. running mean: -1.786434\n",
      "ep 3824: ep_len:500 episode reward: total was 29.210000. running mean: -1.476469\n",
      "ep 3824: ep_len:63 episode reward: total was 28.500000. running mean: -1.176705\n",
      "ep 3824: ep_len:963 episode reward: total was -27.150000. running mean: -1.436438\n",
      "ep 3824: ep_len:2841 episode reward: total was -2.910000. running mean: -1.451173\n",
      "ep 3824: ep_len:64 episode reward: total was 30.500000. running mean: -1.131661\n",
      "epsilon:0.009992 episode_count: 57491. steps_count: 62023521.000000\n",
      "ep 3825: ep_len:730 episode reward: total was -39.330000. running mean: -1.513645\n",
      "ep 3825: ep_len:3544 episode reward: total was -604.060000. running mean: -7.539108\n",
      "ep 3825: ep_len:2935 episode reward: total was -83.620000. running mean: -8.299917\n",
      "ep 3825: ep_len:840 episode reward: total was 8.900000. running mean: -8.127918\n",
      "ep 3825: ep_len:65 episode reward: total was 31.000000. running mean: -7.736639\n",
      "ep 3825: ep_len:117 episode reward: total was 57.000000. running mean: -7.089273\n",
      "ep 3825: ep_len:68 episode reward: total was 32.500000. running mean: -6.693380\n",
      "ep 3825: ep_len:1079 episode reward: total was -7.560000. running mean: -6.702046\n",
      "ep 3825: ep_len:624 episode reward: total was 12.010000. running mean: -6.514926\n",
      "ep 3825: ep_len:744 episode reward: total was 9.220000. running mean: -6.357576\n",
      "ep 3825: ep_len:7306 episode reward: total was 56.310000. running mean: -5.730901\n",
      "ep 3825: ep_len:619 episode reward: total was 7.790000. running mean: -5.595692\n",
      "ep 3825: ep_len:131 episode reward: total was 63.510000. running mean: -4.904635\n",
      "ep 3825: ep_len:42 episode reward: total was 19.500000. running mean: -4.660588\n",
      "ep 3825: ep_len:69 episode reward: total was 30.000000. running mean: -4.313982\n",
      "ep 3825: ep_len:1440 episode reward: total was 2.690000. running mean: -4.243943\n",
      "ep 3825: ep_len:2793 episode reward: total was -30.820000. running mean: -4.509703\n",
      "epsilon:0.009992 episode_count: 57508. steps_count: 62046667.000000\n",
      "ep 3826: ep_len:977 episode reward: total was -33.570000. running mean: -4.800306\n",
      "ep 3826: ep_len:781 episode reward: total was 3.160000. running mean: -4.720703\n",
      "ep 3826: ep_len:3008 episode reward: total was -105.800000. running mean: -5.731496\n",
      "ep 3826: ep_len:500 episode reward: total was -46.920000. running mean: -6.143381\n",
      "ep 3826: ep_len:48 episode reward: total was 21.000000. running mean: -5.871947\n",
      "ep 3826: ep_len:68 episode reward: total was 32.500000. running mean: -5.488228\n",
      "ep 3826: ep_len:57 episode reward: total was 25.500000. running mean: -5.178345\n",
      "ep 3826: ep_len:1119 episode reward: total was -15.730000. running mean: -5.283862\n",
      "ep 3826: ep_len:353 episode reward: total was 20.700000. running mean: -5.024023\n",
      "ep 3826: ep_len:570 episode reward: total was 6.270000. running mean: -4.911083\n",
      "ep 3826: ep_len:7391 episode reward: total was 65.240000. running mean: -4.209572\n",
      "ep 3826: ep_len:1471 episode reward: total was -18.520000. running mean: -4.352677\n",
      "ep 3826: ep_len:188 episode reward: total was 91.000000. running mean: -3.399150\n",
      "ep 3826: ep_len:67 episode reward: total was 29.000000. running mean: -3.075158\n",
      "ep 3826: ep_len:77 episode reward: total was 37.000000. running mean: -2.674407\n",
      "ep 3826: ep_len:1034 episode reward: total was -20.800000. running mean: -2.855663\n",
      "ep 3826: ep_len:2862 episode reward: total was -0.810000. running mean: -2.835206\n",
      "epsilon:0.009992 episode_count: 57525. steps_count: 62067238.000000\n",
      "ep 3827: ep_len:1442 episode reward: total was 0.750000. running mean: -2.799354\n",
      "ep 3827: ep_len:766 episode reward: total was -22.810000. running mean: -2.999460\n",
      "ep 3827: ep_len:3074 episode reward: total was -85.100000. running mean: -3.820466\n",
      "ep 3827: ep_len:712 episode reward: total was 7.890000. running mean: -3.703361\n",
      "ep 3827: ep_len:41 episode reward: total was 19.000000. running mean: -3.476328\n",
      "ep 3827: ep_len:648 episode reward: total was -0.760000. running mean: -3.449164\n",
      "ep 3827: ep_len:618 episode reward: total was 20.220000. running mean: -3.212473\n",
      "ep 3827: ep_len:914 episode reward: total was -36.480000. running mean: -3.545148\n",
      "ep 3827: ep_len:741 episode reward: total was 47.850000. running mean: -3.031196\n",
      "ep 3827: ep_len:1482 episode reward: total was 4.640000. running mean: -2.954484\n",
      "ep 3827: ep_len:52 episode reward: total was 23.000000. running mean: -2.694940\n",
      "ep 3827: ep_len:1051 episode reward: total was 26.470000. running mean: -2.403290\n",
      "ep 3827: ep_len:2922 episode reward: total was 9.620000. running mean: -2.283057\n",
      "ep 3827: ep_len:65 episode reward: total was 30.510000. running mean: -1.955127\n",
      "epsilon:0.009992 episode_count: 57539. steps_count: 62081766.000000\n",
      "ep 3828: ep_len:1489 episode reward: total was -7.530000. running mean: -2.010876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3828: ep_len:632 episode reward: total was -11.970000. running mean: -2.110467\n",
      "ep 3828: ep_len:3039 episode reward: total was -99.980000. running mean: -3.089162\n",
      "ep 3828: ep_len:841 episode reward: total was -128.060000. running mean: -4.338870\n",
      "ep 3828: ep_len:76 episode reward: total was 33.500000. running mean: -3.960482\n",
      "ep 3828: ep_len:1161 episode reward: total was -6.000000. running mean: -3.980877\n",
      "ep 3828: ep_len:633 episode reward: total was 15.010000. running mean: -3.790968\n",
      "ep 3828: ep_len:1612 episode reward: total was 5.640000. running mean: -3.696658\n",
      "ep 3828: ep_len:886 episode reward: total was 73.040000. running mean: -2.929292\n",
      "ep 3828: ep_len:537 episode reward: total was 25.110000. running mean: -2.648899\n",
      "ep 3828: ep_len:88 episode reward: total was 41.000000. running mean: -2.212410\n",
      "ep 3828: ep_len:95 episode reward: total was 46.000000. running mean: -1.730286\n",
      "ep 3828: ep_len:1122 episode reward: total was 17.660000. running mean: -1.536383\n",
      "ep 3828: ep_len:2865 episode reward: total was 0.290000. running mean: -1.518119\n",
      "epsilon:0.009992 episode_count: 57553. steps_count: 62096842.000000\n",
      "ep 3829: ep_len:1468 episode reward: total was 1.960000. running mean: -1.483338\n",
      "ep 3829: ep_len:685 episode reward: total was -27.080000. running mean: -1.739305\n",
      "ep 3829: ep_len:67 episode reward: total was 32.000000. running mean: -1.401912\n",
      "ep 3829: ep_len:3078 episode reward: total was -54.150000. running mean: -1.929392\n",
      "ep 3829: ep_len:693 episode reward: total was 32.350000. running mean: -1.586599\n",
      "ep 3829: ep_len:92 episode reward: total was 44.500000. running mean: -1.125733\n",
      "ep 3829: ep_len:1027 episode reward: total was -54.540000. running mean: -1.659875\n",
      "ep 3829: ep_len:668 episode reward: total was 27.270000. running mean: -1.370576\n",
      "ep 3829: ep_len:1147 episode reward: total was -43.080000. running mean: -1.787671\n",
      "ep 3829: ep_len:802 episode reward: total was 3.360000. running mean: -1.736194\n",
      "ep 3829: ep_len:595 episode reward: total was -5.330000. running mean: -1.772132\n",
      "ep 3829: ep_len:93 episode reward: total was 45.000000. running mean: -1.304411\n",
      "ep 3829: ep_len:87 episode reward: total was 40.500000. running mean: -0.886367\n",
      "ep 3829: ep_len:589 episode reward: total was -13.930000. running mean: -1.016803\n",
      "ep 3829: ep_len:2855 episode reward: total was -1.180000. running mean: -1.018435\n",
      "epsilon:0.009992 episode_count: 57568. steps_count: 62110788.000000\n",
      "ep 3830: ep_len:647 episode reward: total was 24.480000. running mean: -0.763451\n",
      "ep 3830: ep_len:735 episode reward: total was -49.550000. running mean: -1.251316\n",
      "ep 3830: ep_len:69 episode reward: total was 33.000000. running mean: -0.908803\n",
      "ep 3830: ep_len:2895 episode reward: total was -110.200000. running mean: -2.001715\n",
      "ep 3830: ep_len:500 episode reward: total was 0.610000. running mean: -1.975598\n",
      "ep 3830: ep_len:43 episode reward: total was 20.000000. running mean: -1.755842\n",
      "ep 3830: ep_len:121 episode reward: total was 54.500000. running mean: -1.193283\n",
      "ep 3830: ep_len:32 episode reward: total was 14.500000. running mean: -1.036351\n",
      "ep 3830: ep_len:1458 episode reward: total was 21.970000. running mean: -0.806287\n",
      "ep 3830: ep_len:338 episode reward: total was 12.990000. running mean: -0.668324\n",
      "ep 3830: ep_len:1585 episode reward: total was -47.040000. running mean: -1.132041\n",
      "ep 3830: ep_len:729 episode reward: total was 48.530000. running mean: -0.635421\n",
      "ep 3830: ep_len:1118 episode reward: total was -10.200000. running mean: -0.731066\n",
      "ep 3830: ep_len:73 episode reward: total was 35.000000. running mean: -0.373756\n",
      "ep 3830: ep_len:178 episode reward: total was 87.500000. running mean: 0.504982\n",
      "ep 3830: ep_len:61 episode reward: total was 27.500000. running mean: 0.774932\n",
      "ep 3830: ep_len:1153 episode reward: total was -20.960000. running mean: 0.557583\n",
      "ep 3830: ep_len:2855 episode reward: total was -57.340000. running mean: -0.021393\n",
      "epsilon:0.009992 episode_count: 57586. steps_count: 62125378.000000\n",
      "ep 3831: ep_len:958 episode reward: total was -229.310000. running mean: -2.314279\n",
      "ep 3831: ep_len:957 episode reward: total was 2.360000. running mean: -2.267536\n",
      "ep 3831: ep_len:65 episode reward: total was 29.500000. running mean: -1.949861\n",
      "ep 3831: ep_len:2904 episode reward: total was -126.670000. running mean: -3.197062\n",
      "ep 3831: ep_len:1228 episode reward: total was -1.250000. running mean: -3.177592\n",
      "ep 3831: ep_len:49 episode reward: total was 22.510000. running mean: -2.920716\n",
      "ep 3831: ep_len:500 episode reward: total was 37.150000. running mean: -2.520009\n",
      "ep 3831: ep_len:3710 episode reward: total was -321.730000. running mean: -5.712109\n",
      "ep 3831: ep_len:1190 episode reward: total was 4.360000. running mean: -5.611387\n",
      "ep 3831: ep_len:7214 episode reward: total was -32.800000. running mean: -5.883274\n",
      "ep 3831: ep_len:551 episode reward: total was 43.350000. running mean: -5.390941\n",
      "ep 3831: ep_len:66 episode reward: total was 30.000000. running mean: -5.037031\n",
      "ep 3831: ep_len:720 episode reward: total was -24.200000. running mean: -5.228661\n",
      "ep 3831: ep_len:2824 episode reward: total was 7.720000. running mean: -5.099175\n",
      "ep 3831: ep_len:58 episode reward: total was 24.500000. running mean: -4.803183\n",
      "epsilon:0.009992 episode_count: 57601. steps_count: 62148372.000000\n",
      "ep 3832: ep_len:772 episode reward: total was -28.810000. running mean: -5.043251\n",
      "ep 3832: ep_len:3595 episode reward: total was -1466.030000. running mean: -19.653118\n",
      "ep 3832: ep_len:50 episode reward: total was 23.500000. running mean: -19.221587\n",
      "ep 3832: ep_len:2986 episode reward: total was -62.790000. running mean: -19.657271\n",
      "ep 3832: ep_len:845 episode reward: total was 53.620000. running mean: -18.924499\n",
      "ep 3832: ep_len:79 episode reward: total was 36.500000. running mean: -18.370254\n",
      "ep 3832: ep_len:79 episode reward: total was 38.000000. running mean: -17.806551\n",
      "ep 3832: ep_len:1036 episode reward: total was -37.280000. running mean: -18.001286\n",
      "ep 3832: ep_len:3738 episode reward: total was -63.760000. running mean: -18.458873\n",
      "ep 3832: ep_len:1168 episode reward: total was -19.710000. running mean: -18.471384\n",
      "ep 3832: ep_len:880 episode reward: total was 45.240000. running mean: -17.834270\n",
      "ep 3832: ep_len:1029 episode reward: total was 30.020000. running mean: -17.355728\n",
      "ep 3832: ep_len:107 episode reward: total was 52.000000. running mean: -16.662170\n",
      "ep 3832: ep_len:1092 episode reward: total was 26.300000. running mean: -16.232549\n",
      "ep 3832: ep_len:2877 episode reward: total was -11.860000. running mean: -16.188823\n",
      "ep 3832: ep_len:56 episode reward: total was 26.500000. running mean: -15.761935\n",
      "epsilon:0.009992 episode_count: 57617. steps_count: 62168761.000000\n",
      "ep 3833: ep_len:1420 episode reward: total was -20.500000. running mean: -15.809315\n",
      "ep 3833: ep_len:680 episode reward: total was 3.600000. running mean: -15.615222\n",
      "ep 3833: ep_len:65 episode reward: total was 31.000000. running mean: -15.149070\n",
      "ep 3833: ep_len:2904 episode reward: total was -41.330000. running mean: -15.410879\n",
      "ep 3833: ep_len:501 episode reward: total was -3.620000. running mean: -15.292971\n",
      "ep 3833: ep_len:38 episode reward: total was 17.500000. running mean: -14.965041\n",
      "ep 3833: ep_len:746 episode reward: total was -27.020000. running mean: -15.085590\n",
      "ep 3833: ep_len:325 episode reward: total was 15.370000. running mean: -14.781035\n",
      "ep 3833: ep_len:891 episode reward: total was -41.760000. running mean: -15.050824\n",
      "ep 3833: ep_len:780 episode reward: total was 8.860000. running mean: -14.811716\n",
      "ep 3833: ep_len:1119 episode reward: total was -10.190000. running mean: -14.765499\n",
      "ep 3833: ep_len:96 episode reward: total was 45.000000. running mean: -14.167844\n",
      "ep 3833: ep_len:133 episode reward: total was 62.000000. running mean: -13.406165\n",
      "ep 3833: ep_len:876 episode reward: total was 17.990000. running mean: -13.092204\n",
      "ep 3833: ep_len:2838 episode reward: total was 8.660000. running mean: -12.874682\n",
      "epsilon:0.009992 episode_count: 57632. steps_count: 62182173.000000\n",
      "ep 3834: ep_len:500 episode reward: total was 30.010000. running mean: -12.445835\n",
      "ep 3834: ep_len:1216 episode reward: total was -44.020000. running mean: -12.761577\n",
      "ep 3834: ep_len:46 episode reward: total was 21.500000. running mean: -12.418961\n",
      "ep 3834: ep_len:2992 episode reward: total was -30.130000. running mean: -12.596071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3834: ep_len:773 episode reward: total was 25.950000. running mean: -12.210610\n",
      "ep 3834: ep_len:139 episode reward: total was 68.000000. running mean: -11.408504\n",
      "ep 3834: ep_len:77 episode reward: total was 37.000000. running mean: -10.924419\n",
      "ep 3834: ep_len:40 episode reward: total was 18.500000. running mean: -10.630175\n",
      "ep 3834: ep_len:802 episode reward: total was 15.470000. running mean: -10.369173\n",
      "ep 3834: ep_len:339 episode reward: total was 24.630000. running mean: -10.019182\n",
      "ep 3834: ep_len:1576 episode reward: total was -55.520000. running mean: -10.474190\n",
      "ep 3834: ep_len:729 episode reward: total was -5.520000. running mean: -10.424648\n",
      "ep 3834: ep_len:643 episode reward: total was -11.840000. running mean: -10.438801\n",
      "ep 3834: ep_len:52 episode reward: total was 24.500000. running mean: -10.089413\n",
      "ep 3834: ep_len:58 episode reward: total was 27.500000. running mean: -9.713519\n",
      "ep 3834: ep_len:78 episode reward: total was 37.500000. running mean: -9.241384\n",
      "ep 3834: ep_len:1154 episode reward: total was 2.170000. running mean: -9.127270\n",
      "ep 3834: ep_len:2820 episode reward: total was -8.970000. running mean: -9.125698\n",
      "ep 3834: ep_len:58 episode reward: total was 26.000000. running mean: -8.774441\n",
      "epsilon:0.009992 episode_count: 57651. steps_count: 62196265.000000\n",
      "ep 3835: ep_len:1212 episode reward: total was 5.440000. running mean: -8.632296\n",
      "ep 3835: ep_len:704 episode reward: total was 6.910000. running mean: -8.476873\n",
      "ep 3835: ep_len:3109 episode reward: total was -15.860000. running mean: -8.550704\n",
      "ep 3835: ep_len:661 episode reward: total was 25.620000. running mean: -8.208997\n",
      "ep 3835: ep_len:45 episode reward: total was 21.000000. running mean: -7.916907\n",
      "ep 3835: ep_len:63 episode reward: total was 30.000000. running mean: -7.537738\n",
      "ep 3835: ep_len:670 episode reward: total was -11.650000. running mean: -7.578861\n",
      "ep 3835: ep_len:3575 episode reward: total was -66.510000. running mean: -8.168172\n",
      "ep 3835: ep_len:813 episode reward: total was 9.090000. running mean: -7.995591\n",
      "ep 3835: ep_len:740 episode reward: total was 56.140000. running mean: -7.354235\n",
      "ep 3835: ep_len:1051 episode reward: total was 54.150000. running mean: -6.739192\n",
      "ep 3835: ep_len:76 episode reward: total was 35.000000. running mean: -6.321800\n",
      "ep 3835: ep_len:1090 episode reward: total was 11.260000. running mean: -6.145982\n",
      "ep 3835: ep_len:2843 episode reward: total was -1.670000. running mean: -6.101223\n",
      "ep 3835: ep_len:58 episode reward: total was 27.500000. running mean: -5.765210\n",
      "epsilon:0.009992 episode_count: 57666. steps_count: 62212975.000000\n",
      "ep 3836: ep_len:1049 episode reward: total was 14.820000. running mean: -5.559358\n",
      "ep 3836: ep_len:692 episode reward: total was -15.470000. running mean: -5.658465\n",
      "ep 3836: ep_len:2966 episode reward: total was -73.100000. running mean: -6.332880\n",
      "ep 3836: ep_len:800 episode reward: total was -260.630000. running mean: -8.875851\n",
      "ep 3836: ep_len:53 episode reward: total was 25.000000. running mean: -8.537093\n",
      "ep 3836: ep_len:98 episode reward: total was 44.500000. running mean: -8.006722\n",
      "ep 3836: ep_len:81 episode reward: total was 37.500000. running mean: -7.551655\n",
      "ep 3836: ep_len:1104 episode reward: total was -53.750000. running mean: -8.013638\n",
      "ep 3836: ep_len:3580 episode reward: total was -11.950000. running mean: -8.053002\n",
      "ep 3836: ep_len:1244 episode reward: total was -38.720000. running mean: -8.359672\n",
      "ep 3836: ep_len:654 episode reward: total was 2.790000. running mean: -8.248175\n",
      "ep 3836: ep_len:628 episode reward: total was 9.410000. running mean: -8.071593\n",
      "ep 3836: ep_len:1343 episode reward: total was 6.950000. running mean: -7.921377\n",
      "ep 3836: ep_len:2823 episode reward: total was -4.070000. running mean: -7.882864\n",
      "epsilon:0.009992 episode_count: 57680. steps_count: 62230090.000000\n",
      "ep 3837: ep_len:1072 episode reward: total was 3.480000. running mean: -7.769235\n",
      "ep 3837: ep_len:732 episode reward: total was 11.050000. running mean: -7.581043\n",
      "ep 3837: ep_len:3006 episode reward: total was -5.500000. running mean: -7.560232\n",
      "ep 3837: ep_len:689 episode reward: total was -2.380000. running mean: -7.508430\n",
      "ep 3837: ep_len:96 episode reward: total was 45.000000. running mean: -6.983345\n",
      "ep 3837: ep_len:707 episode reward: total was 26.380000. running mean: -6.649712\n",
      "ep 3837: ep_len:354 episode reward: total was 7.580000. running mean: -6.507415\n",
      "ep 3837: ep_len:2117 episode reward: total was -368.790000. running mean: -10.130241\n",
      "ep 3837: ep_len:782 episode reward: total was 46.490000. running mean: -9.564038\n",
      "ep 3837: ep_len:693 episode reward: total was -9.010000. running mean: -9.558498\n",
      "ep 3837: ep_len:92 episode reward: total was 41.500000. running mean: -9.047913\n",
      "ep 3837: ep_len:144 episode reward: total was 69.000000. running mean: -8.267434\n",
      "ep 3837: ep_len:616 episode reward: total was -69.610000. running mean: -8.880860\n",
      "ep 3837: ep_len:2865 episode reward: total was -40.500000. running mean: -9.197051\n",
      "epsilon:0.009992 episode_count: 57694. steps_count: 62244055.000000\n",
      "ep 3838: ep_len:686 episode reward: total was -3.410000. running mean: -9.139180\n",
      "ep 3838: ep_len:1303 episode reward: total was -37.640000. running mean: -9.424189\n",
      "ep 3838: ep_len:2996 episode reward: total was -45.320000. running mean: -9.783147\n",
      "ep 3838: ep_len:562 episode reward: total was -22.210000. running mean: -9.907415\n",
      "ep 3838: ep_len:28 episode reward: total was 12.500000. running mean: -9.683341\n",
      "ep 3838: ep_len:149 episode reward: total was 68.500000. running mean: -8.901508\n",
      "ep 3838: ep_len:1387 episode reward: total was -46.070000. running mean: -9.273193\n",
      "ep 3838: ep_len:339 episode reward: total was 5.440000. running mean: -9.126061\n",
      "ep 3838: ep_len:1589 episode reward: total was 1.800000. running mean: -9.016800\n",
      "ep 3838: ep_len:715 episode reward: total was 31.180000. running mean: -8.614832\n",
      "ep 3838: ep_len:600 episode reward: total was 49.480000. running mean: -8.033884\n",
      "ep 3838: ep_len:56 episode reward: total was 25.000000. running mean: -7.703545\n",
      "ep 3838: ep_len:51 episode reward: total was 24.000000. running mean: -7.386509\n",
      "ep 3838: ep_len:75 episode reward: total was 36.000000. running mean: -6.952644\n",
      "ep 3838: ep_len:1056 episode reward: total was 15.710000. running mean: -6.726018\n",
      "ep 3838: ep_len:2812 episode reward: total was -25.390000. running mean: -6.912658\n",
      "epsilon:0.009992 episode_count: 57710. steps_count: 62258459.000000\n",
      "ep 3839: ep_len:658 episode reward: total was 7.970000. running mean: -6.763831\n",
      "ep 3839: ep_len:624 episode reward: total was -10.180000. running mean: -6.797993\n",
      "ep 3839: ep_len:58 episode reward: total was 27.500000. running mean: -6.455013\n",
      "ep 3839: ep_len:2949 episode reward: total was -34.340000. running mean: -6.733863\n",
      "ep 3839: ep_len:725 episode reward: total was -7.060000. running mean: -6.737124\n",
      "ep 3839: ep_len:1385 episode reward: total was 16.160000. running mean: -6.508153\n",
      "ep 3839: ep_len:4455 episode reward: total was -3472.350000. running mean: -41.166571\n",
      "ep 3839: ep_len:1600 episode reward: total was -49.830000. running mean: -41.253206\n",
      "ep 3839: ep_len:869 episode reward: total was 42.470000. running mean: -40.415974\n",
      "ep 3839: ep_len:1045 episode reward: total was -3.860000. running mean: -40.050414\n",
      "ep 3839: ep_len:55 episode reward: total was 24.500000. running mean: -39.404910\n",
      "ep 3839: ep_len:68 episode reward: total was 31.000000. running mean: -38.700861\n",
      "ep 3839: ep_len:2282 episode reward: total was -1623.670000. running mean: -54.550552\n",
      "ep 3839: ep_len:2705 episode reward: total was -36.840000. running mean: -54.373447\n",
      "epsilon:0.009992 episode_count: 57724. steps_count: 62277937.000000\n",
      "ep 3840: ep_len:1417 episode reward: total was -10.490000. running mean: -53.934612\n",
      "ep 3840: ep_len:1489 episode reward: total was -171.820000. running mean: -55.113466\n",
      "ep 3840: ep_len:2954 episode reward: total was -0.670000. running mean: -54.569031\n",
      "ep 3840: ep_len:808 episode reward: total was 14.150000. running mean: -53.881841\n",
      "ep 3840: ep_len:138 episode reward: total was 64.500000. running mean: -52.698023\n",
      "ep 3840: ep_len:50 episode reward: total was 22.000000. running mean: -51.951042\n",
      "ep 3840: ep_len:622 episode reward: total was -18.360000. running mean: -51.615132\n",
      "ep 3840: ep_len:3684 episode reward: total was -31.780000. running mean: -51.416781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3840: ep_len:1555 episode reward: total was -26.830000. running mean: -51.170913\n",
      "ep 3840: ep_len:706 episode reward: total was 31.210000. running mean: -50.347104\n",
      "ep 3840: ep_len:1006 episode reward: total was 15.160000. running mean: -49.692033\n",
      "ep 3840: ep_len:110 episode reward: total was 53.500000. running mean: -48.660112\n",
      "ep 3840: ep_len:804 episode reward: total was 6.820000. running mean: -48.105311\n",
      "ep 3840: ep_len:2828 episode reward: total was -41.410000. running mean: -48.038358\n",
      "ep 3840: ep_len:58 episode reward: total was 27.500000. running mean: -47.282974\n",
      "epsilon:0.009992 episode_count: 57739. steps_count: 62296166.000000\n",
      "ep 3841: ep_len:1166 episode reward: total was -5.220000. running mean: -46.862345\n",
      "ep 3841: ep_len:726 episode reward: total was 1.100000. running mean: -46.382721\n",
      "ep 3841: ep_len:2920 episode reward: total was -34.130000. running mean: -46.260194\n",
      "ep 3841: ep_len:885 episode reward: total was 7.270000. running mean: -45.724892\n",
      "ep 3841: ep_len:48 episode reward: total was 22.500000. running mean: -45.042643\n",
      "ep 3841: ep_len:1036 episode reward: total was 12.550000. running mean: -44.466717\n",
      "ep 3841: ep_len:336 episode reward: total was 8.530000. running mean: -43.936750\n",
      "ep 3841: ep_len:794 episode reward: total was -70.190000. running mean: -44.199282\n",
      "ep 3841: ep_len:755 episode reward: total was 20.160000. running mean: -43.555689\n",
      "ep 3841: ep_len:1071 episode reward: total was -9.250000. running mean: -43.212632\n",
      "ep 3841: ep_len:62 episode reward: total was 29.500000. running mean: -42.485506\n",
      "ep 3841: ep_len:94 episode reward: total was 42.500000. running mean: -41.635651\n",
      "ep 3841: ep_len:893 episode reward: total was -74.060000. running mean: -41.959895\n",
      "ep 3841: ep_len:2839 episode reward: total was -19.250000. running mean: -41.732796\n",
      "ep 3841: ep_len:44 episode reward: total was 20.500000. running mean: -41.110468\n",
      "epsilon:0.009992 episode_count: 57754. steps_count: 62309835.000000\n",
      "ep 3842: ep_len:1105 episode reward: total was -13.050000. running mean: -40.829863\n",
      "ep 3842: ep_len:961 episode reward: total was 0.260000. running mean: -40.418964\n",
      "ep 3842: ep_len:42 episode reward: total was 19.500000. running mean: -39.819775\n",
      "ep 3842: ep_len:2992 episode reward: total was -38.610000. running mean: -39.807677\n",
      "ep 3842: ep_len:1483 episode reward: total was 7.130000. running mean: -39.338300\n",
      "ep 3842: ep_len:4446 episode reward: total was -823.850000. running mean: -47.183417\n",
      "ep 3842: ep_len:500 episode reward: total was 24.200000. running mean: -46.469583\n",
      "ep 3842: ep_len:546 episode reward: total was -30.060000. running mean: -46.305487\n",
      "ep 3842: ep_len:7435 episode reward: total was -146.640000. running mean: -47.308832\n",
      "ep 3842: ep_len:598 episode reward: total was -8.330000. running mean: -46.919044\n",
      "ep 3842: ep_len:56 episode reward: total was 26.500000. running mean: -46.184854\n",
      "ep 3842: ep_len:55 episode reward: total was 26.000000. running mean: -45.463005\n",
      "ep 3842: ep_len:71 episode reward: total was 32.500000. running mean: -44.683375\n",
      "ep 3842: ep_len:572 episode reward: total was -2.530000. running mean: -44.261841\n",
      "ep 3842: ep_len:2780 episode reward: total was -26.880000. running mean: -44.088023\n",
      "epsilon:0.009992 episode_count: 57769. steps_count: 62333477.000000\n",
      "ep 3843: ep_len:825 episode reward: total was -26.750000. running mean: -43.914643\n",
      "ep 3843: ep_len:1142 episode reward: total was -6.500000. running mean: -43.540496\n",
      "ep 3843: ep_len:3028 episode reward: total was -67.920000. running mean: -43.784291\n",
      "ep 3843: ep_len:500 episode reward: total was 15.890000. running mean: -43.187548\n",
      "ep 3843: ep_len:135 episode reward: total was 64.500000. running mean: -42.110673\n",
      "ep 3843: ep_len:82 episode reward: total was 38.000000. running mean: -41.309566\n",
      "ep 3843: ep_len:1038 episode reward: total was -9.990000. running mean: -40.996370\n",
      "ep 3843: ep_len:3857 episode reward: total was -736.650000. running mean: -47.952907\n",
      "ep 3843: ep_len:525 episode reward: total was -15.120000. running mean: -47.624578\n",
      "ep 3843: ep_len:621 episode reward: total was -4.820000. running mean: -47.196532\n",
      "ep 3843: ep_len:724 episode reward: total was -13.620000. running mean: -46.860767\n",
      "ep 3843: ep_len:150 episode reward: total was 69.000000. running mean: -45.702159\n",
      "ep 3843: ep_len:664 episode reward: total was -10.370000. running mean: -45.348837\n",
      "ep 3843: ep_len:2774 episode reward: total was 14.050000. running mean: -44.754849\n",
      "epsilon:0.009992 episode_count: 57783. steps_count: 62349542.000000\n",
      "ep 3844: ep_len:701 episode reward: total was -38.580000. running mean: -44.693100\n",
      "ep 3844: ep_len:205 episode reward: total was -34.540000. running mean: -44.591569\n",
      "ep 3844: ep_len:3196 episode reward: total was -54.570000. running mean: -44.691354\n",
      "ep 3844: ep_len:768 episode reward: total was -17.740000. running mean: -44.421840\n",
      "ep 3844: ep_len:37 episode reward: total was 15.500000. running mean: -43.822622\n",
      "ep 3844: ep_len:42 episode reward: total was 19.500000. running mean: -43.189396\n",
      "ep 3844: ep_len:1823 episode reward: total was -15.970000. running mean: -42.917202\n",
      "ep 3844: ep_len:3631 episode reward: total was -6.910000. running mean: -42.557130\n",
      "ep 3844: ep_len:678 episode reward: total was -0.460000. running mean: -42.136158\n",
      "ep 3844: ep_len:622 episode reward: total was -5.460000. running mean: -41.769397\n",
      "ep 3844: ep_len:959 episode reward: total was 49.070000. running mean: -40.861003\n",
      "ep 3844: ep_len:95 episode reward: total was 44.500000. running mean: -40.007393\n",
      "ep 3844: ep_len:40 episode reward: total was 18.500000. running mean: -39.422319\n",
      "ep 3844: ep_len:72 episode reward: total was 31.500000. running mean: -38.713096\n",
      "ep 3844: ep_len:1136 episode reward: total was -13.940000. running mean: -38.465365\n",
      "ep 3844: ep_len:2813 episode reward: total was -29.820000. running mean: -38.378911\n",
      "epsilon:0.009992 episode_count: 57799. steps_count: 62366360.000000\n",
      "ep 3845: ep_len:1114 episode reward: total was -13.270000. running mean: -38.127822\n",
      "ep 3845: ep_len:212 episode reward: total was -27.460000. running mean: -38.021144\n",
      "ep 3845: ep_len:2984 episode reward: total was -16.590000. running mean: -37.806832\n",
      "ep 3845: ep_len:596 episode reward: total was 0.150000. running mean: -37.427264\n",
      "ep 3845: ep_len:55 episode reward: total was 26.000000. running mean: -36.792991\n",
      "ep 3845: ep_len:1436 episode reward: total was 7.150000. running mean: -36.353561\n",
      "ep 3845: ep_len:3647 episode reward: total was -19.360000. running mean: -36.183626\n",
      "ep 3845: ep_len:648 episode reward: total was -103.260000. running mean: -36.854389\n",
      "ep 3845: ep_len:892 episode reward: total was 59.010000. running mean: -35.895746\n",
      "ep 3845: ep_len:597 episode reward: total was -12.380000. running mean: -35.660588\n",
      "ep 3845: ep_len:133 episode reward: total was 63.500000. running mean: -34.668982\n",
      "ep 3845: ep_len:796 episode reward: total was -31.140000. running mean: -34.633692\n",
      "ep 3845: ep_len:2831 episode reward: total was 1.520000. running mean: -34.272155\n",
      "ep 3845: ep_len:26 episode reward: total was 10.000000. running mean: -33.829434\n",
      "epsilon:0.009992 episode_count: 57813. steps_count: 62382327.000000\n",
      "ep 3846: ep_len:733 episode reward: total was -67.580000. running mean: -34.166940\n",
      "ep 3846: ep_len:3865 episode reward: total was -252.240000. running mean: -36.347670\n",
      "ep 3846: ep_len:3041 episode reward: total was -55.720000. running mean: -36.541394\n",
      "ep 3846: ep_len:1237 episode reward: total was -5.080000. running mean: -36.226780\n",
      "ep 3846: ep_len:846 episode reward: total was 32.870000. running mean: -35.535812\n",
      "ep 3846: ep_len:337 episode reward: total was 18.030000. running mean: -35.000154\n",
      "ep 3846: ep_len:974 episode reward: total was -25.780000. running mean: -34.907952\n",
      "ep 3846: ep_len:833 episode reward: total was 41.420000. running mean: -34.144673\n",
      "ep 3846: ep_len:1185 episode reward: total was -1.450000. running mean: -33.817726\n",
      "ep 3846: ep_len:70 episode reward: total was 32.000000. running mean: -33.159549\n",
      "ep 3846: ep_len:47 episode reward: total was 20.500000. running mean: -32.622953\n",
      "ep 3846: ep_len:645 episode reward: total was 12.670000. running mean: -32.170024\n",
      "ep 3846: ep_len:2863 episode reward: total was -26.260000. running mean: -32.110923\n",
      "epsilon:0.009992 episode_count: 57826. steps_count: 62399003.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3847: ep_len:621 episode reward: total was 3.400000. running mean: -31.755814\n",
      "ep 3847: ep_len:500 episode reward: total was 18.590000. running mean: -31.252356\n",
      "ep 3847: ep_len:42 episode reward: total was 19.500000. running mean: -30.744832\n",
      "ep 3847: ep_len:2983 episode reward: total was -32.120000. running mean: -30.758584\n",
      "ep 3847: ep_len:1524 episode reward: total was 1.850000. running mean: -30.432498\n",
      "ep 3847: ep_len:35 episode reward: total was 14.500000. running mean: -29.983173\n",
      "ep 3847: ep_len:58 episode reward: total was 26.000000. running mean: -29.423342\n",
      "ep 3847: ep_len:1382 episode reward: total was -127.810000. running mean: -30.407208\n",
      "ep 3847: ep_len:584 episode reward: total was -6.720000. running mean: -30.170336\n",
      "ep 3847: ep_len:859 episode reward: total was -64.970000. running mean: -30.518333\n",
      "ep 3847: ep_len:792 episode reward: total was 30.790000. running mean: -29.905249\n",
      "ep 3847: ep_len:962 episode reward: total was 47.390000. running mean: -29.132297\n",
      "ep 3847: ep_len:26 episode reward: total was 10.000000. running mean: -28.740974\n",
      "ep 3847: ep_len:778 episode reward: total was -0.150000. running mean: -28.455064\n",
      "ep 3847: ep_len:2900 episode reward: total was 17.390000. running mean: -27.996614\n",
      "ep 3847: ep_len:41 episode reward: total was 17.500000. running mean: -27.541647\n",
      "epsilon:0.009992 episode_count: 57842. steps_count: 62413090.000000\n",
      "ep 3848: ep_len:656 episode reward: total was -11.170000. running mean: -27.377931\n",
      "ep 3848: ep_len:204 episode reward: total was 8.830000. running mean: -27.015852\n",
      "ep 3848: ep_len:2934 episode reward: total was -27.870000. running mean: -27.024393\n",
      "ep 3848: ep_len:633 episode reward: total was -4.400000. running mean: -26.798149\n",
      "ep 3848: ep_len:56 episode reward: total was 26.500000. running mean: -26.265168\n",
      "ep 3848: ep_len:112 episode reward: total was 53.000000. running mean: -25.472516\n",
      "ep 3848: ep_len:79 episode reward: total was 36.500000. running mean: -24.852791\n",
      "ep 3848: ep_len:500 episode reward: total was -9.300000. running mean: -24.697263\n",
      "ep 3848: ep_len:647 episode reward: total was 8.910000. running mean: -24.361190\n",
      "ep 3848: ep_len:648 episode reward: total was -33.170000. running mean: -24.449278\n",
      "ep 3848: ep_len:864 episode reward: total was 46.480000. running mean: -23.739986\n",
      "ep 3848: ep_len:540 episode reward: total was 26.610000. running mean: -23.236486\n",
      "ep 3848: ep_len:211 episode reward: total was 103.510000. running mean: -21.969021\n",
      "ep 3848: ep_len:39 episode reward: total was 18.000000. running mean: -21.569331\n",
      "ep 3848: ep_len:116 episode reward: total was 52.000000. running mean: -20.833637\n",
      "ep 3848: ep_len:718 episode reward: total was -70.760000. running mean: -21.332901\n",
      "ep 3848: ep_len:2745 episode reward: total was -2.710000. running mean: -21.146672\n",
      "epsilon:0.009992 episode_count: 57859. steps_count: 62424792.000000\n",
      "ep 3849: ep_len:838 episode reward: total was 22.350000. running mean: -20.711705\n",
      "ep 3849: ep_len:1591 episode reward: total was -44.680000. running mean: -20.951388\n",
      "ep 3849: ep_len:44 episode reward: total was 20.500000. running mean: -20.536874\n",
      "ep 3849: ep_len:2966 episode reward: total was -51.610000. running mean: -20.847606\n",
      "ep 3849: ep_len:500 episode reward: total was 18.430000. running mean: -20.454830\n",
      "ep 3849: ep_len:100 episode reward: total was 47.000000. running mean: -19.780281\n",
      "ep 3849: ep_len:1437 episode reward: total was -164.750000. running mean: -21.229978\n",
      "ep 3849: ep_len:297 episode reward: total was -35.630000. running mean: -21.373979\n",
      "ep 3849: ep_len:580 episode reward: total was 20.790000. running mean: -20.952339\n",
      "ep 3849: ep_len:675 episode reward: total was 19.660000. running mean: -20.546215\n",
      "ep 3849: ep_len:517 episode reward: total was -0.050000. running mean: -20.341253\n",
      "ep 3849: ep_len:68 episode reward: total was 32.500000. running mean: -19.812841\n",
      "ep 3849: ep_len:111 episode reward: total was 54.000000. running mean: -19.074712\n",
      "ep 3849: ep_len:43 episode reward: total was 20.000000. running mean: -18.683965\n",
      "ep 3849: ep_len:104 episode reward: total was 50.500000. running mean: -17.992126\n",
      "ep 3849: ep_len:500 episode reward: total was 38.700000. running mean: -17.425204\n",
      "ep 3849: ep_len:2812 episode reward: total was -8.260000. running mean: -17.333552\n",
      "epsilon:0.009992 episode_count: 57876. steps_count: 62437975.000000\n",
      "ep 3850: ep_len:1123 episode reward: total was -6.110000. running mean: -17.221317\n",
      "ep 3850: ep_len:630 episode reward: total was -30.630000. running mean: -17.355404\n",
      "ep 3850: ep_len:2955 episode reward: total was -32.310000. running mean: -17.504950\n",
      "ep 3850: ep_len:1724 episode reward: total was -128.220000. running mean: -18.612100\n",
      "ep 3850: ep_len:665 episode reward: total was 1.430000. running mean: -18.411679\n",
      "ep 3850: ep_len:349 episode reward: total was 6.150000. running mean: -18.166062\n",
      "ep 3850: ep_len:774 episode reward: total was -24.520000. running mean: -18.229602\n",
      "ep 3850: ep_len:686 episode reward: total was 11.810000. running mean: -17.929206\n",
      "ep 3850: ep_len:629 episode reward: total was 2.530000. running mean: -17.724614\n",
      "ep 3850: ep_len:658 episode reward: total was -7.720000. running mean: -17.624567\n",
      "ep 3850: ep_len:2878 episode reward: total was -25.770000. running mean: -17.706022\n",
      "ep 3850: ep_len:49 episode reward: total was 23.000000. running mean: -17.298962\n",
      "epsilon:0.009992 episode_count: 57888. steps_count: 62451095.000000\n",
      "ep 3851: ep_len:740 episode reward: total was -51.320000. running mean: -17.639172\n",
      "ep 3851: ep_len:703 episode reward: total was -49.120000. running mean: -17.953980\n",
      "ep 3851: ep_len:3065 episode reward: total was -3.720000. running mean: -17.811640\n",
      "ep 3851: ep_len:1673 episode reward: total was -116.210000. running mean: -18.795624\n",
      "ep 3851: ep_len:924 episode reward: total was 72.760000. running mean: -17.880068\n",
      "ep 3851: ep_len:618 episode reward: total was 5.620000. running mean: -17.645067\n",
      "ep 3851: ep_len:1250 episode reward: total was -31.100000. running mean: -17.779616\n",
      "ep 3851: ep_len:753 episode reward: total was 16.490000. running mean: -17.436920\n",
      "ep 3851: ep_len:1085 episode reward: total was -4.870000. running mean: -17.311251\n",
      "ep 3851: ep_len:68 episode reward: total was 32.500000. running mean: -16.813139\n",
      "ep 3851: ep_len:958 episode reward: total was -24.660000. running mean: -16.891607\n",
      "ep 3851: ep_len:2960 episode reward: total was 3.790000. running mean: -16.684791\n",
      "ep 3851: ep_len:45 episode reward: total was 21.000000. running mean: -16.307943\n",
      "epsilon:0.009992 episode_count: 57901. steps_count: 62465937.000000\n",
      "ep 3852: ep_len:1428 episode reward: total was 0.950000. running mean: -16.135364\n",
      "ep 3852: ep_len:783 episode reward: total was -22.640000. running mean: -16.200410\n",
      "ep 3852: ep_len:2965 episode reward: total was -33.190000. running mean: -16.370306\n",
      "ep 3852: ep_len:675 episode reward: total was -1.540000. running mean: -16.222003\n",
      "ep 3852: ep_len:92 episode reward: total was 44.500000. running mean: -15.614783\n",
      "ep 3852: ep_len:54 episode reward: total was 24.000000. running mean: -15.218635\n",
      "ep 3852: ep_len:1061 episode reward: total was -34.420000. running mean: -15.410649\n",
      "ep 3852: ep_len:331 episode reward: total was 13.900000. running mean: -15.117542\n",
      "ep 3852: ep_len:1189 episode reward: total was -19.650000. running mean: -15.162867\n",
      "ep 3852: ep_len:800 episode reward: total was 23.060000. running mean: -14.780638\n",
      "ep 3852: ep_len:1113 episode reward: total was -11.260000. running mean: -14.745432\n",
      "ep 3852: ep_len:134 episode reward: total was 65.500000. running mean: -13.942977\n",
      "ep 3852: ep_len:65 episode reward: total was 29.500000. running mean: -13.508548\n",
      "ep 3852: ep_len:501 episode reward: total was 15.140000. running mean: -13.222062\n",
      "ep 3852: ep_len:2882 episode reward: total was 13.230000. running mean: -12.957542\n",
      "ep 3852: ep_len:46 episode reward: total was 21.500000. running mean: -12.612966\n",
      "epsilon:0.009992 episode_count: 57917. steps_count: 62480056.000000\n",
      "ep 3853: ep_len:932 episode reward: total was -310.380000. running mean: -15.590637\n",
      "ep 3853: ep_len:760 episode reward: total was -14.630000. running mean: -15.581030\n",
      "ep 3853: ep_len:2997 episode reward: total was -21.120000. running mean: -15.636420\n",
      "ep 3853: ep_len:728 episode reward: total was -15.600000. running mean: -15.636056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3853: ep_len:110 episode reward: total was 52.000000. running mean: -14.959695\n",
      "ep 3853: ep_len:500 episode reward: total was 27.710000. running mean: -14.532998\n",
      "ep 3853: ep_len:662 episode reward: total was 14.750000. running mean: -14.240168\n",
      "ep 3853: ep_len:500 episode reward: total was 23.670000. running mean: -13.861066\n",
      "ep 3853: ep_len:808 episode reward: total was 22.680000. running mean: -13.495656\n",
      "ep 3853: ep_len:1155 episode reward: total was -16.380000. running mean: -13.524499\n",
      "ep 3853: ep_len:67 episode reward: total was 30.500000. running mean: -13.084254\n",
      "ep 3853: ep_len:40 episode reward: total was 18.500000. running mean: -12.768412\n",
      "ep 3853: ep_len:72 episode reward: total was 34.500000. running mean: -12.295728\n",
      "ep 3853: ep_len:1123 episode reward: total was -7.110000. running mean: -12.243870\n",
      "ep 3853: ep_len:2865 episode reward: total was 2.340000. running mean: -12.098032\n",
      "ep 3853: ep_len:71 episode reward: total was 34.000000. running mean: -11.637051\n",
      "epsilon:0.009992 episode_count: 57933. steps_count: 62493446.000000\n",
      "ep 3854: ep_len:1152 episode reward: total was -1.080000. running mean: -11.531481\n",
      "ep 3854: ep_len:1685 episode reward: total was -30.690000. running mean: -11.723066\n",
      "ep 3854: ep_len:2992 episode reward: total was -16.670000. running mean: -11.772535\n",
      "ep 3854: ep_len:699 episode reward: total was 19.370000. running mean: -11.461110\n",
      "ep 3854: ep_len:99 episode reward: total was 48.000000. running mean: -10.866499\n",
      "ep 3854: ep_len:75 episode reward: total was 33.000000. running mean: -10.427834\n",
      "ep 3854: ep_len:1917 episode reward: total was -64.400000. running mean: -10.967556\n",
      "ep 3854: ep_len:3657 episode reward: total was -73.140000. running mean: -11.589280\n",
      "ep 3854: ep_len:776 episode reward: total was -0.290000. running mean: -11.476287\n",
      "ep 3854: ep_len:7408 episode reward: total was -16.960000. running mean: -11.531124\n",
      "ep 3854: ep_len:994 episode reward: total was 38.750000. running mean: -11.028313\n",
      "ep 3854: ep_len:201 episode reward: total was 97.500000. running mean: -9.943030\n",
      "ep 3854: ep_len:59 episode reward: total was 28.000000. running mean: -9.563600\n",
      "ep 3854: ep_len:1040 episode reward: total was 15.720000. running mean: -9.310764\n",
      "ep 3854: ep_len:2829 episode reward: total was -16.810000. running mean: -9.385756\n",
      "epsilon:0.009992 episode_count: 57948. steps_count: 62519029.000000\n",
      "ep 3855: ep_len:1165 episode reward: total was -3.670000. running mean: -9.328598\n",
      "ep 3855: ep_len:712 episode reward: total was -9.810000. running mean: -9.333412\n",
      "ep 3855: ep_len:2960 episode reward: total was -37.830000. running mean: -9.618378\n",
      "ep 3855: ep_len:823 episode reward: total was 26.730000. running mean: -9.254895\n",
      "ep 3855: ep_len:35 episode reward: total was 16.000000. running mean: -9.002346\n",
      "ep 3855: ep_len:131 episode reward: total was 61.000000. running mean: -8.302322\n",
      "ep 3855: ep_len:64 episode reward: total was 29.000000. running mean: -7.929299\n",
      "ep 3855: ep_len:1321 episode reward: total was -84.410000. running mean: -8.694106\n",
      "ep 3855: ep_len:3627 episode reward: total was -46.980000. running mean: -9.076965\n",
      "ep 3855: ep_len:1207 episode reward: total was -41.510000. running mean: -9.401295\n",
      "ep 3855: ep_len:661 episode reward: total was 33.610000. running mean: -8.971182\n",
      "ep 3855: ep_len:888 episode reward: total was 35.620000. running mean: -8.525270\n",
      "ep 3855: ep_len:87 episode reward: total was 42.000000. running mean: -8.020018\n",
      "ep 3855: ep_len:42 episode reward: total was 19.500000. running mean: -7.744818\n",
      "ep 3855: ep_len:85 episode reward: total was 39.500000. running mean: -7.272369\n",
      "ep 3855: ep_len:1100 episode reward: total was 34.220000. running mean: -6.857446\n",
      "ep 3855: ep_len:2762 episode reward: total was -0.980000. running mean: -6.798671\n",
      "ep 3855: ep_len:57 episode reward: total was 27.000000. running mean: -6.460685\n",
      "epsilon:0.009992 episode_count: 57966. steps_count: 62536756.000000\n",
      "ep 3856: ep_len:1152 episode reward: total was 0.270000. running mean: -6.393378\n",
      "ep 3856: ep_len:1246 episode reward: total was -44.270000. running mean: -6.772144\n",
      "ep 3856: ep_len:2851 episode reward: total was -117.650000. running mean: -7.880922\n",
      "ep 3856: ep_len:1119 episode reward: total was -21.760000. running mean: -8.019713\n",
      "ep 3856: ep_len:65 episode reward: total was 29.500000. running mean: -7.644516\n",
      "ep 3856: ep_len:73 episode reward: total was 33.500000. running mean: -7.233071\n",
      "ep 3856: ep_len:980 episode reward: total was -13.600000. running mean: -7.296740\n",
      "ep 3856: ep_len:336 episode reward: total was 21.540000. running mean: -7.008373\n",
      "ep 3856: ep_len:1236 episode reward: total was -74.150000. running mean: -7.679789\n",
      "ep 3856: ep_len:835 episode reward: total was 31.770000. running mean: -7.285291\n",
      "ep 3856: ep_len:608 episode reward: total was 10.960000. running mean: -7.102838\n",
      "ep 3856: ep_len:65 episode reward: total was 31.000000. running mean: -6.721810\n",
      "ep 3856: ep_len:197 episode reward: total was 95.500000. running mean: -5.699592\n",
      "ep 3856: ep_len:39 episode reward: total was 18.000000. running mean: -5.462596\n",
      "ep 3856: ep_len:72 episode reward: total was 33.000000. running mean: -5.077970\n",
      "ep 3856: ep_len:1154 episode reward: total was -5.520000. running mean: -5.082390\n",
      "ep 3856: ep_len:2861 episode reward: total was -0.230000. running mean: -5.033866\n",
      "ep 3856: ep_len:53 episode reward: total was 23.500000. running mean: -4.748528\n",
      "epsilon:0.009992 episode_count: 57984. steps_count: 62551698.000000\n",
      "ep 3857: ep_len:1006 episode reward: total was -63.940000. running mean: -5.340442\n",
      "ep 3857: ep_len:735 episode reward: total was 15.120000. running mean: -5.135838\n",
      "ep 3857: ep_len:76 episode reward: total was 36.500000. running mean: -4.719480\n",
      "ep 3857: ep_len:3003 episode reward: total was -58.250000. running mean: -5.254785\n",
      "ep 3857: ep_len:777 episode reward: total was 15.340000. running mean: -5.048837\n",
      "ep 3857: ep_len:644 episode reward: total was -27.230000. running mean: -5.270649\n",
      "ep 3857: ep_len:342 episode reward: total was 11.620000. running mean: -5.101742\n",
      "ep 3857: ep_len:1299 episode reward: total was -110.400000. running mean: -6.154725\n",
      "ep 3857: ep_len:7276 episode reward: total was 70.690000. running mean: -5.386277\n",
      "ep 3857: ep_len:677 episode reward: total was -25.330000. running mean: -5.585715\n",
      "ep 3857: ep_len:201 episode reward: total was 97.500000. running mean: -4.554858\n",
      "ep 3857: ep_len:861 episode reward: total was 24.830000. running mean: -4.261009\n",
      "ep 3857: ep_len:2858 episode reward: total was -23.960000. running mean: -4.457999\n",
      "epsilon:0.009992 episode_count: 57997. steps_count: 62571453.000000\n",
      "ep 3858: ep_len:1440 episode reward: total was 12.480000. running mean: -4.288619\n",
      "ep 3858: ep_len:1618 episode reward: total was -19.210000. running mean: -4.437833\n",
      "ep 3858: ep_len:79 episode reward: total was 36.500000. running mean: -4.028454\n",
      "ep 3858: ep_len:3059 episode reward: total was -2.010000. running mean: -4.008270\n",
      "ep 3858: ep_len:655 episode reward: total was 1.330000. running mean: -3.954887\n",
      "ep 3858: ep_len:1526 episode reward: total was 27.860000. running mean: -3.636738\n",
      "ep 3858: ep_len:3790 episode reward: total was -34.240000. running mean: -3.942771\n",
      "ep 3858: ep_len:817 episode reward: total was 9.410000. running mean: -3.809243\n",
      "ep 3858: ep_len:713 episode reward: total was 45.550000. running mean: -3.315651\n",
      "ep 3858: ep_len:603 episode reward: total was 37.970000. running mean: -2.902794\n",
      "ep 3858: ep_len:72 episode reward: total was 34.500000. running mean: -2.528766\n",
      "ep 3858: ep_len:91 episode reward: total was 44.000000. running mean: -2.063479\n",
      "ep 3858: ep_len:53 episode reward: total was 23.500000. running mean: -1.807844\n",
      "ep 3858: ep_len:573 episode reward: total was -24.740000. running mean: -2.037165\n",
      "ep 3858: ep_len:2801 episode reward: total was -13.970000. running mean: -2.156494\n",
      "ep 3858: ep_len:47 episode reward: total was 20.500000. running mean: -1.929929\n",
      "epsilon:0.009992 episode_count: 58013. steps_count: 62589390.000000\n",
      "ep 3859: ep_len:1116 episode reward: total was -4.160000. running mean: -1.952230\n",
      "ep 3859: ep_len:829 episode reward: total was 12.390000. running mean: -1.808807\n",
      "ep 3859: ep_len:2993 episode reward: total was -23.630000. running mean: -2.027019\n",
      "ep 3859: ep_len:500 episode reward: total was 6.470000. running mean: -1.942049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3859: ep_len:544 episode reward: total was 23.470000. running mean: -1.687928\n",
      "ep 3859: ep_len:3698 episode reward: total was -1.190000. running mean: -1.682949\n",
      "ep 3859: ep_len:4391 episode reward: total was -1251.520000. running mean: -14.181320\n",
      "ep 3859: ep_len:844 episode reward: total was 47.290000. running mean: -13.566606\n",
      "ep 3859: ep_len:760 episode reward: total was 21.800000. running mean: -13.212940\n",
      "ep 3859: ep_len:77 episode reward: total was 35.500000. running mean: -12.725811\n",
      "ep 3859: ep_len:790 episode reward: total was -16.110000. running mean: -12.759653\n",
      "ep 3859: ep_len:22 episode reward: total was 9.500000. running mean: -12.537056\n",
      "ep 3859: ep_len:41 episode reward: total was 16.000000. running mean: -12.251686\n",
      "epsilon:0.009992 episode_count: 58026. steps_count: 62605995.000000\n",
      "ep 3860: ep_len:832 episode reward: total was -11.860000. running mean: -12.247769\n",
      "ep 3860: ep_len:681 episode reward: total was -37.970000. running mean: -12.504991\n",
      "ep 3860: ep_len:2950 episode reward: total was -46.660000. running mean: -12.846541\n",
      "ep 3860: ep_len:1647 episode reward: total was -122.900000. running mean: -13.947076\n",
      "ep 3860: ep_len:27 episode reward: total was 12.000000. running mean: -13.687605\n",
      "ep 3860: ep_len:1845 episode reward: total was -24.900000. running mean: -13.799729\n",
      "ep 3860: ep_len:343 episode reward: total was 20.080000. running mean: -13.460932\n",
      "ep 3860: ep_len:1276 episode reward: total was -61.050000. running mean: -13.936823\n",
      "ep 3860: ep_len:767 episode reward: total was 45.940000. running mean: -13.338054\n",
      "ep 3860: ep_len:725 episode reward: total was 7.820000. running mean: -13.126474\n",
      "ep 3860: ep_len:184 episode reward: total was 89.000000. running mean: -12.105209\n",
      "ep 3860: ep_len:63 episode reward: total was 30.000000. running mean: -11.684157\n",
      "ep 3860: ep_len:898 episode reward: total was 11.530000. running mean: -11.452015\n",
      "ep 3860: ep_len:2910 episode reward: total was 5.490000. running mean: -11.282595\n",
      "ep 3860: ep_len:55 episode reward: total was 26.000000. running mean: -10.909769\n",
      "epsilon:0.009992 episode_count: 58041. steps_count: 62621198.000000\n",
      "ep 3861: ep_len:1399 episode reward: total was 9.350000. running mean: -10.707172\n",
      "ep 3861: ep_len:663 episode reward: total was -61.210000. running mean: -11.212200\n",
      "ep 3861: ep_len:57 episode reward: total was 27.000000. running mean: -10.830078\n",
      "ep 3861: ep_len:92 episode reward: total was 44.500000. running mean: -10.276777\n",
      "ep 3861: ep_len:513 episode reward: total was 28.000000. running mean: -9.894009\n",
      "ep 3861: ep_len:47 episode reward: total was 22.000000. running mean: -9.575069\n",
      "ep 3861: ep_len:103 episode reward: total was 50.000000. running mean: -8.979319\n",
      "ep 3861: ep_len:78 episode reward: total was 36.000000. running mean: -8.529525\n",
      "ep 3861: ep_len:1388 episode reward: total was 8.670000. running mean: -8.357530\n",
      "ep 3861: ep_len:3970 episode reward: total was -105.510000. running mean: -9.329055\n",
      "ep 3861: ep_len:1422 episode reward: total was -30.380000. running mean: -9.539564\n",
      "ep 3861: ep_len:697 episode reward: total was 14.310000. running mean: -9.301069\n",
      "ep 3861: ep_len:500 episode reward: total was 28.080000. running mean: -8.927258\n",
      "ep 3861: ep_len:4055 episode reward: total was -379.240000. running mean: -12.630385\n",
      "ep 3861: ep_len:2881 episode reward: total was -10.410000. running mean: -12.608181\n",
      "epsilon:0.009992 episode_count: 58056. steps_count: 62639063.000000\n",
      "ep 3862: ep_len:1097 episode reward: total was -2.790000. running mean: -12.510000\n",
      "ep 3862: ep_len:736 episode reward: total was -12.000000. running mean: -12.504900\n",
      "ep 3862: ep_len:70 episode reward: total was 33.500000. running mean: -12.044851\n",
      "ep 3862: ep_len:3010 episode reward: total was -35.790000. running mean: -12.282302\n",
      "ep 3862: ep_len:1694 episode reward: total was -110.340000. running mean: -13.262879\n",
      "ep 3862: ep_len:36 episode reward: total was 16.500000. running mean: -12.965250\n",
      "ep 3862: ep_len:105 episode reward: total was 51.000000. running mean: -12.325598\n",
      "ep 3862: ep_len:735 episode reward: total was -12.260000. running mean: -12.324942\n",
      "ep 3862: ep_len:659 episode reward: total was 29.170000. running mean: -11.909992\n",
      "ep 3862: ep_len:1234 episode reward: total was -48.430000. running mean: -12.275193\n",
      "ep 3862: ep_len:710 episode reward: total was 6.080000. running mean: -12.091641\n",
      "ep 3862: ep_len:848 episode reward: total was 23.710000. running mean: -11.733624\n",
      "ep 3862: ep_len:35 episode reward: total was 16.000000. running mean: -11.456288\n",
      "ep 3862: ep_len:121 episode reward: total was 57.500000. running mean: -10.766725\n",
      "ep 3862: ep_len:1113 episode reward: total was -18.480000. running mean: -10.843858\n",
      "ep 3862: ep_len:45 episode reward: total was 21.000000. running mean: -10.525419\n",
      "epsilon:0.009992 episode_count: 58072. steps_count: 62651311.000000\n",
      "ep 3863: ep_len:952 episode reward: total was -142.570000. running mean: -11.845865\n",
      "ep 3863: ep_len:978 episode reward: total was 30.770000. running mean: -11.419706\n",
      "ep 3863: ep_len:50 episode reward: total was 22.000000. running mean: -11.085509\n",
      "ep 3863: ep_len:2937 episode reward: total was -7.270000. running mean: -11.047354\n",
      "ep 3863: ep_len:636 episode reward: total was -4.920000. running mean: -10.986081\n",
      "ep 3863: ep_len:56 episode reward: total was 26.500000. running mean: -10.611220\n",
      "ep 3863: ep_len:156 episode reward: total was 76.500000. running mean: -9.740108\n",
      "ep 3863: ep_len:59 episode reward: total was 28.000000. running mean: -9.362707\n",
      "ep 3863: ep_len:65 episode reward: total was 31.000000. running mean: -8.959080\n",
      "ep 3863: ep_len:1479 episode reward: total was 7.270000. running mean: -8.796789\n",
      "ep 3863: ep_len:3541 episode reward: total was 1.280000. running mean: -8.696021\n",
      "ep 3863: ep_len:861 episode reward: total was -63.270000. running mean: -9.241761\n",
      "ep 3863: ep_len:703 episode reward: total was 17.860000. running mean: -8.970743\n",
      "ep 3863: ep_len:740 episode reward: total was -4.890000. running mean: -8.929936\n",
      "ep 3863: ep_len:163 episode reward: total was 78.500000. running mean: -8.055636\n",
      "ep 3863: ep_len:56 episode reward: total was 26.500000. running mean: -7.710080\n",
      "ep 3863: ep_len:766 episode reward: total was -47.050000. running mean: -8.103479\n",
      "ep 3863: ep_len:2939 episode reward: total was -10.630000. running mean: -8.128744\n",
      "epsilon:0.009992 episode_count: 58090. steps_count: 62668448.000000\n",
      "ep 3864: ep_len:610 episode reward: total was 17.800000. running mean: -7.869457\n",
      "ep 3864: ep_len:1638 episode reward: total was -27.250000. running mean: -8.063262\n",
      "ep 3864: ep_len:3031 episode reward: total was -37.220000. running mean: -8.354830\n",
      "ep 3864: ep_len:674 episode reward: total was 12.790000. running mean: -8.143381\n",
      "ep 3864: ep_len:65 episode reward: total was 29.500000. running mean: -7.766948\n",
      "ep 3864: ep_len:101 episode reward: total was 47.500000. running mean: -7.214278\n",
      "ep 3864: ep_len:61 episode reward: total was 29.000000. running mean: -6.852135\n",
      "ep 3864: ep_len:663 episode reward: total was -1.170000. running mean: -6.795314\n",
      "ep 3864: ep_len:343 episode reward: total was 19.160000. running mean: -6.535761\n",
      "ep 3864: ep_len:582 episode reward: total was 10.650000. running mean: -6.363903\n",
      "ep 3864: ep_len:671 episode reward: total was 26.600000. running mean: -6.034264\n",
      "ep 3864: ep_len:500 episode reward: total was -8.230000. running mean: -6.056222\n",
      "ep 3864: ep_len:116 episode reward: total was 56.500000. running mean: -5.430659\n",
      "ep 3864: ep_len:66 episode reward: total was 28.500000. running mean: -5.091353\n",
      "ep 3864: ep_len:1096 episode reward: total was -9.320000. running mean: -5.133639\n",
      "ep 3864: ep_len:2848 episode reward: total was -34.220000. running mean: -5.424503\n",
      "epsilon:0.009992 episode_count: 58106. steps_count: 62681513.000000\n",
      "ep 3865: ep_len:636 episode reward: total was -10.980000. running mean: -5.480058\n",
      "ep 3865: ep_len:750 episode reward: total was -15.000000. running mean: -5.575257\n",
      "ep 3865: ep_len:2941 episode reward: total was -172.610000. running mean: -7.245605\n",
      "ep 3865: ep_len:659 episode reward: total was 29.360000. running mean: -6.879549\n",
      "ep 3865: ep_len:98 episode reward: total was 43.000000. running mean: -6.380753\n",
      "ep 3865: ep_len:57 episode reward: total was 27.000000. running mean: -6.046946\n",
      "ep 3865: ep_len:992 episode reward: total was -10.450000. running mean: -6.090976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3865: ep_len:348 episode reward: total was 20.710000. running mean: -5.822966\n",
      "ep 3865: ep_len:500 episode reward: total was 30.220000. running mean: -5.462537\n",
      "ep 3865: ep_len:839 episode reward: total was 56.370000. running mean: -4.844211\n",
      "ep 3865: ep_len:500 episode reward: total was 19.320000. running mean: -4.602569\n",
      "ep 3865: ep_len:1482 episode reward: total was -961.550000. running mean: -14.172044\n",
      "ep 3865: ep_len:31 episode reward: total was 14.000000. running mean: -13.890323\n",
      "epsilon:0.009992 episode_count: 58119. steps_count: 62691346.000000\n",
      "ep 3866: ep_len:906 episode reward: total was 25.970000. running mean: -13.491720\n",
      "ep 3866: ep_len:743 episode reward: total was -26.360000. running mean: -13.620403\n",
      "ep 3866: ep_len:3003 episode reward: total was -1.640000. running mean: -13.500599\n",
      "ep 3866: ep_len:1158 episode reward: total was -11.820000. running mean: -13.483793\n",
      "ep 3866: ep_len:62 episode reward: total was 29.500000. running mean: -13.053955\n",
      "ep 3866: ep_len:47 episode reward: total was 20.500000. running mean: -12.718415\n",
      "ep 3866: ep_len:601 episode reward: total was -1.230000. running mean: -12.603531\n",
      "ep 3866: ep_len:330 episode reward: total was 20.990000. running mean: -12.267596\n",
      "ep 3866: ep_len:665 episode reward: total was -243.010000. running mean: -14.575020\n",
      "ep 3866: ep_len:827 episode reward: total was 47.000000. running mean: -13.959270\n",
      "ep 3866: ep_len:671 episode reward: total was 7.690000. running mean: -13.742777\n",
      "ep 3866: ep_len:49 episode reward: total was 23.000000. running mean: -13.375349\n",
      "ep 3866: ep_len:515 episode reward: total was -61.700000. running mean: -13.858596\n",
      "ep 3866: ep_len:2731 episode reward: total was 9.420000. running mean: -13.625810\n",
      "epsilon:0.009992 episode_count: 58133. steps_count: 62703654.000000\n",
      "ep 3867: ep_len:701 episode reward: total was -35.580000. running mean: -13.845352\n",
      "ep 3867: ep_len:717 episode reward: total was -184.900000. running mean: -15.555898\n",
      "ep 3867: ep_len:2988 episode reward: total was -75.330000. running mean: -16.153639\n",
      "ep 3867: ep_len:522 episode reward: total was -47.470000. running mean: -16.466803\n",
      "ep 3867: ep_len:51 episode reward: total was 22.500000. running mean: -16.077135\n",
      "ep 3867: ep_len:914 episode reward: total was 51.470000. running mean: -15.401663\n",
      "ep 3867: ep_len:619 episode reward: total was 33.330000. running mean: -14.914347\n",
      "ep 3867: ep_len:813 episode reward: total was -16.770000. running mean: -14.932903\n",
      "ep 3867: ep_len:710 episode reward: total was 22.740000. running mean: -14.556174\n",
      "ep 3867: ep_len:500 episode reward: total was 17.880000. running mean: -14.231812\n",
      "ep 3867: ep_len:90 episode reward: total was 42.000000. running mean: -13.669494\n",
      "ep 3867: ep_len:1495 episode reward: total was -14.480000. running mean: -13.677599\n",
      "ep 3867: ep_len:2831 episode reward: total was -10.850000. running mean: -13.649323\n",
      "epsilon:0.009992 episode_count: 58146. steps_count: 62716605.000000\n",
      "ep 3868: ep_len:967 episode reward: total was -102.940000. running mean: -14.542230\n",
      "ep 3868: ep_len:1259 episode reward: total was -60.300000. running mean: -14.999808\n",
      "ep 3868: ep_len:44 episode reward: total was 20.500000. running mean: -14.644810\n",
      "ep 3868: ep_len:2867 episode reward: total was -27.630000. running mean: -14.774662\n",
      "ep 3868: ep_len:585 episode reward: total was -12.390000. running mean: -14.750815\n",
      "ep 3868: ep_len:50 episode reward: total was 22.000000. running mean: -14.383307\n",
      "ep 3868: ep_len:75 episode reward: total was 36.000000. running mean: -13.879474\n",
      "ep 3868: ep_len:671 episode reward: total was -2.030000. running mean: -13.760979\n",
      "ep 3868: ep_len:3949 episode reward: total was -519.810000. running mean: -18.821469\n",
      "ep 3868: ep_len:746 episode reward: total was -4.930000. running mean: -18.682555\n",
      "ep 3868: ep_len:739 episode reward: total was -0.950000. running mean: -18.505229\n",
      "ep 3868: ep_len:698 episode reward: total was -8.340000. running mean: -18.403577\n",
      "ep 3868: ep_len:137 episode reward: total was 62.500000. running mean: -17.594541\n",
      "ep 3868: ep_len:59 episode reward: total was 28.000000. running mean: -17.138596\n",
      "ep 3868: ep_len:1005 episode reward: total was -14.580000. running mean: -17.113010\n",
      "ep 3868: ep_len:2885 episode reward: total was -32.350000. running mean: -17.265380\n",
      "ep 3868: ep_len:45 episode reward: total was 21.000000. running mean: -16.882726\n",
      "epsilon:0.009992 episode_count: 58163. steps_count: 62733386.000000\n",
      "ep 3869: ep_len:638 episode reward: total was 11.010000. running mean: -16.603798\n",
      "ep 3869: ep_len:1019 episode reward: total was 24.910000. running mean: -16.188660\n",
      "ep 3869: ep_len:66 episode reward: total was 31.500000. running mean: -15.711774\n",
      "ep 3869: ep_len:3147 episode reward: total was 8.400000. running mean: -15.470656\n",
      "ep 3869: ep_len:876 episode reward: total was 21.720000. running mean: -15.098750\n",
      "ep 3869: ep_len:62 episode reward: total was 28.000000. running mean: -14.667762\n",
      "ep 3869: ep_len:60 episode reward: total was 27.000000. running mean: -14.251084\n",
      "ep 3869: ep_len:1425 episode reward: total was 6.000000. running mean: -14.048574\n",
      "ep 3869: ep_len:575 episode reward: total was -6.570000. running mean: -13.973788\n",
      "ep 3869: ep_len:811 episode reward: total was -54.820000. running mean: -14.382250\n",
      "ep 3869: ep_len:808 episode reward: total was 14.510000. running mean: -14.093327\n",
      "ep 3869: ep_len:662 episode reward: total was 0.350000. running mean: -13.948894\n",
      "ep 3869: ep_len:1227 episode reward: total was 9.940000. running mean: -13.710005\n",
      "ep 3869: ep_len:2994 episode reward: total was 3.090000. running mean: -13.542005\n",
      "ep 3869: ep_len:66 episode reward: total was 30.000000. running mean: -13.106585\n",
      "epsilon:0.009992 episode_count: 58178. steps_count: 62747822.000000\n",
      "ep 3870: ep_len:969 episode reward: total was -21.350000. running mean: -13.189019\n",
      "ep 3870: ep_len:182 episode reward: total was 3.250000. running mean: -13.024629\n",
      "ep 3870: ep_len:2955 episode reward: total was 5.710000. running mean: -12.837283\n",
      "ep 3870: ep_len:3545 episode reward: total was -293.980000. running mean: -15.648710\n",
      "ep 3870: ep_len:791 episode reward: total was 20.040000. running mean: -15.291823\n",
      "ep 3870: ep_len:334 episode reward: total was 32.110000. running mean: -14.817805\n",
      "ep 3870: ep_len:796 episode reward: total was -19.480000. running mean: -14.864427\n",
      "ep 3870: ep_len:645 episode reward: total was 3.710000. running mean: -14.678682\n",
      "ep 3870: ep_len:1501 episode reward: total was -11.940000. running mean: -14.651296\n",
      "ep 3870: ep_len:173 episode reward: total was 83.500000. running mean: -13.669783\n",
      "ep 3870: ep_len:41 episode reward: total was 19.000000. running mean: -13.343085\n",
      "ep 3870: ep_len:952 episode reward: total was -10.330000. running mean: -13.312954\n",
      "ep 3870: ep_len:2863 episode reward: total was 14.110000. running mean: -13.038724\n",
      "epsilon:0.009992 episode_count: 58191. steps_count: 62763569.000000\n",
      "ep 3871: ep_len:980 episode reward: total was -26.560000. running mean: -13.173937\n",
      "ep 3871: ep_len:719 episode reward: total was -20.940000. running mean: -13.251598\n",
      "ep 3871: ep_len:63 episode reward: total was 30.000000. running mean: -12.819082\n",
      "ep 3871: ep_len:2916 episode reward: total was -25.090000. running mean: -12.941791\n",
      "ep 3871: ep_len:701 episode reward: total was 14.280000. running mean: -12.669573\n",
      "ep 3871: ep_len:162 episode reward: total was 78.000000. running mean: -11.762877\n",
      "ep 3871: ep_len:87 episode reward: total was 40.500000. running mean: -11.240249\n",
      "ep 3871: ep_len:71 episode reward: total was 32.500000. running mean: -10.802846\n",
      "ep 3871: ep_len:1467 episode reward: total was 16.490000. running mean: -10.529918\n",
      "ep 3871: ep_len:616 episode reward: total was 27.730000. running mean: -10.147318\n",
      "ep 3871: ep_len:2143 episode reward: total was -571.610000. running mean: -15.761945\n",
      "ep 3871: ep_len:776 episode reward: total was 18.250000. running mean: -15.421826\n",
      "ep 3871: ep_len:653 episode reward: total was 21.980000. running mean: -15.047808\n",
      "ep 3871: ep_len:64 episode reward: total was 29.000000. running mean: -14.607329\n",
      "ep 3871: ep_len:772 episode reward: total was -61.130000. running mean: -15.072556\n",
      "ep 3871: ep_len:2808 episode reward: total was -2.480000. running mean: -14.946631\n",
      "ep 3871: ep_len:49 episode reward: total was 23.000000. running mean: -14.567164\n",
      "epsilon:0.009992 episode_count: 58208. steps_count: 62778616.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3872: ep_len:993 episode reward: total was -69.020000. running mean: -15.111693\n",
      "ep 3872: ep_len:1213 episode reward: total was -61.770000. running mean: -15.578276\n",
      "ep 3872: ep_len:73 episode reward: total was 32.000000. running mean: -15.102493\n",
      "ep 3872: ep_len:2974 episode reward: total was -9.380000. running mean: -15.045268\n",
      "ep 3872: ep_len:1623 episode reward: total was -76.620000. running mean: -15.661015\n",
      "ep 3872: ep_len:38 episode reward: total was 17.500000. running mean: -15.329405\n",
      "ep 3872: ep_len:137 episode reward: total was 65.500000. running mean: -14.521111\n",
      "ep 3872: ep_len:84 episode reward: total was 40.500000. running mean: -13.970900\n",
      "ep 3872: ep_len:500 episode reward: total was 0.990000. running mean: -13.821291\n",
      "ep 3872: ep_len:3840 episode reward: total was -385.990000. running mean: -17.542978\n",
      "ep 3872: ep_len:909 episode reward: total was -77.780000. running mean: -18.145348\n",
      "ep 3872: ep_len:864 episode reward: total was 68.680000. running mean: -17.277095\n",
      "ep 3872: ep_len:713 episode reward: total was -11.220000. running mean: -17.216524\n",
      "ep 3872: ep_len:773 episode reward: total was -34.860000. running mean: -17.392959\n",
      "ep 3872: ep_len:2838 episode reward: total was 9.730000. running mean: -17.121729\n",
      "epsilon:0.009992 episode_count: 58223. steps_count: 62796188.000000\n",
      "ep 3873: ep_len:593 episode reward: total was -15.240000. running mean: -17.102912\n",
      "ep 3873: ep_len:183 episode reward: total was 11.340000. running mean: -16.818483\n",
      "ep 3873: ep_len:3053 episode reward: total was 3.170000. running mean: -16.618598\n",
      "ep 3873: ep_len:573 episode reward: total was -8.580000. running mean: -16.538212\n",
      "ep 3873: ep_len:58 episode reward: total was 27.500000. running mean: -16.097830\n",
      "ep 3873: ep_len:119 episode reward: total was 58.000000. running mean: -15.356851\n",
      "ep 3873: ep_len:64 episode reward: total was 30.500000. running mean: -14.898283\n",
      "ep 3873: ep_len:625 episode reward: total was 37.860000. running mean: -14.370700\n",
      "ep 3873: ep_len:631 episode reward: total was 28.520000. running mean: -13.941793\n",
      "ep 3873: ep_len:811 episode reward: total was 20.950000. running mean: -13.592875\n",
      "ep 3873: ep_len:722 episode reward: total was 34.410000. running mean: -13.112846\n",
      "ep 3873: ep_len:1402 episode reward: total was 7.480000. running mean: -12.906918\n",
      "ep 3873: ep_len:83 episode reward: total was 37.000000. running mean: -12.407849\n",
      "ep 3873: ep_len:115 episode reward: total was 56.000000. running mean: -11.723770\n",
      "ep 3873: ep_len:55 episode reward: total was 26.000000. running mean: -11.346533\n",
      "ep 3873: ep_len:1440 episode reward: total was -8.260000. running mean: -11.315667\n",
      "ep 3873: ep_len:2824 episode reward: total was -6.270000. running mean: -11.265211\n",
      "ep 3873: ep_len:54 episode reward: total was 25.500000. running mean: -10.897558\n",
      "epsilon:0.009992 episode_count: 58241. steps_count: 62809593.000000\n",
      "ep 3874: ep_len:946 episode reward: total was 0.140000. running mean: -10.787183\n",
      "ep 3874: ep_len:1007 episode reward: total was 30.360000. running mean: -10.375711\n",
      "ep 3874: ep_len:2865 episode reward: total was -24.470000. running mean: -10.516654\n",
      "ep 3874: ep_len:717 episode reward: total was 22.290000. running mean: -10.188587\n",
      "ep 3874: ep_len:54 episode reward: total was 25.500000. running mean: -9.831702\n",
      "ep 3874: ep_len:65 episode reward: total was 31.000000. running mean: -9.423385\n",
      "ep 3874: ep_len:80 episode reward: total was 35.500000. running mean: -8.974151\n",
      "ep 3874: ep_len:1401 episode reward: total was 10.010000. running mean: -8.784309\n",
      "ep 3874: ep_len:672 episode reward: total was 27.340000. running mean: -8.423066\n",
      "ep 3874: ep_len:598 episode reward: total was 11.230000. running mean: -8.226535\n",
      "ep 3874: ep_len:821 episode reward: total was 72.570000. running mean: -7.418570\n",
      "ep 3874: ep_len:697 episode reward: total was -31.100000. running mean: -7.655384\n",
      "ep 3874: ep_len:50 episode reward: total was 20.500000. running mean: -7.373831\n",
      "ep 3874: ep_len:1061 episode reward: total was -13.800000. running mean: -7.438092\n",
      "ep 3874: ep_len:2879 episode reward: total was 8.060000. running mean: -7.283111\n",
      "epsilon:0.009992 episode_count: 58256. steps_count: 62823506.000000\n",
      "ep 3875: ep_len:999 episode reward: total was -101.280000. running mean: -8.223080\n",
      "ep 3875: ep_len:768 episode reward: total was -5.620000. running mean: -8.197049\n",
      "ep 3875: ep_len:3123 episode reward: total was -16.550000. running mean: -8.280579\n",
      "ep 3875: ep_len:2632 episode reward: total was -175.140000. running mean: -9.949173\n",
      "ep 3875: ep_len:44 episode reward: total was 20.500000. running mean: -9.644681\n",
      "ep 3875: ep_len:118 episode reward: total was 56.000000. running mean: -8.988235\n",
      "ep 3875: ep_len:62 episode reward: total was 26.500000. running mean: -8.633352\n",
      "ep 3875: ep_len:699 episode reward: total was -5.300000. running mean: -8.600019\n",
      "ep 3875: ep_len:337 episode reward: total was 23.570000. running mean: -8.278319\n",
      "ep 3875: ep_len:1595 episode reward: total was -71.120000. running mean: -8.906735\n",
      "ep 3875: ep_len:672 episode reward: total was 33.500000. running mean: -8.482668\n",
      "ep 3875: ep_len:692 episode reward: total was -37.540000. running mean: -8.773241\n",
      "ep 3875: ep_len:51 episode reward: total was 24.000000. running mean: -8.445509\n",
      "ep 3875: ep_len:117 episode reward: total was 54.000000. running mean: -7.821054\n",
      "ep 3875: ep_len:965 episode reward: total was -80.200000. running mean: -8.544843\n",
      "ep 3875: ep_len:2820 episode reward: total was -10.960000. running mean: -8.568995\n",
      "ep 3875: ep_len:39 episode reward: total was 18.000000. running mean: -8.303305\n",
      "epsilon:0.009992 episode_count: 58273. steps_count: 62839239.000000\n",
      "ep 3876: ep_len:1190 episode reward: total was 11.770000. running mean: -8.102572\n",
      "ep 3876: ep_len:694 episode reward: total was 14.680000. running mean: -7.874746\n",
      "ep 3876: ep_len:44 episode reward: total was 20.500000. running mean: -7.590999\n",
      "ep 3876: ep_len:2948 episode reward: total was -26.450000. running mean: -7.779589\n",
      "ep 3876: ep_len:661 episode reward: total was -2.650000. running mean: -7.728293\n",
      "ep 3876: ep_len:52 episode reward: total was 24.500000. running mean: -7.406010\n",
      "ep 3876: ep_len:74 episode reward: total was 32.500000. running mean: -7.006950\n",
      "ep 3876: ep_len:64 episode reward: total was 30.500000. running mean: -6.631880\n",
      "ep 3876: ep_len:816 episode reward: total was -410.440000. running mean: -10.669961\n",
      "ep 3876: ep_len:362 episode reward: total was 17.300000. running mean: -10.390262\n",
      "ep 3876: ep_len:622 episode reward: total was 7.060000. running mean: -10.215759\n",
      "ep 3876: ep_len:666 episode reward: total was 5.820000. running mean: -10.055402\n",
      "ep 3876: ep_len:1511 episode reward: total was -0.640000. running mean: -9.961248\n",
      "ep 3876: ep_len:52 episode reward: total was 24.500000. running mean: -9.616635\n",
      "ep 3876: ep_len:181 episode reward: total was 87.500000. running mean: -8.645469\n",
      "ep 3876: ep_len:51 episode reward: total was 24.000000. running mean: -8.319014\n",
      "ep 3876: ep_len:1140 episode reward: total was -4.930000. running mean: -8.285124\n",
      "ep 3876: ep_len:2794 episode reward: total was -15.510000. running mean: -8.357373\n",
      "ep 3876: ep_len:64 episode reward: total was 29.000000. running mean: -7.983799\n",
      "epsilon:0.009992 episode_count: 58292. steps_count: 62853225.000000\n",
      "ep 3877: ep_len:980 episode reward: total was -24.670000. running mean: -8.150661\n",
      "ep 3877: ep_len:796 episode reward: total was -4.720000. running mean: -8.116354\n",
      "ep 3877: ep_len:2990 episode reward: total was -6.490000. running mean: -8.100091\n",
      "ep 3877: ep_len:746 episode reward: total was -34.120000. running mean: -8.360290\n",
      "ep 3877: ep_len:703 episode reward: total was 2.850000. running mean: -8.248187\n",
      "ep 3877: ep_len:323 episode reward: total was 26.460000. running mean: -7.901105\n",
      "ep 3877: ep_len:748 episode reward: total was -32.430000. running mean: -8.146394\n",
      "ep 3877: ep_len:7312 episode reward: total was -974.760000. running mean: -17.812530\n",
      "ep 3877: ep_len:649 episode reward: total was 19.920000. running mean: -17.435205\n",
      "ep 3877: ep_len:39 episode reward: total was 16.500000. running mean: -17.095853\n",
      "ep 3877: ep_len:944 episode reward: total was -40.220000. running mean: -17.327094\n",
      "ep 3877: ep_len:2864 episode reward: total was 3.260000. running mean: -17.121223\n",
      "epsilon:0.009992 episode_count: 58304. steps_count: 62872319.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3878: ep_len:988 episode reward: total was -102.400000. running mean: -17.974011\n",
      "ep 3878: ep_len:760 episode reward: total was -24.040000. running mean: -18.034671\n",
      "ep 3878: ep_len:69 episode reward: total was 33.000000. running mean: -17.524324\n",
      "ep 3878: ep_len:2968 episode reward: total was -25.610000. running mean: -17.605181\n",
      "ep 3878: ep_len:501 episode reward: total was 21.260000. running mean: -17.216529\n",
      "ep 3878: ep_len:58 episode reward: total was 27.500000. running mean: -16.769364\n",
      "ep 3878: ep_len:114 episode reward: total was 54.000000. running mean: -16.061670\n",
      "ep 3878: ep_len:640 episode reward: total was 0.250000. running mean: -15.898554\n",
      "ep 3878: ep_len:3662 episode reward: total was -110.630000. running mean: -16.845868\n",
      "ep 3878: ep_len:1528 episode reward: total was -19.230000. running mean: -16.869709\n",
      "ep 3878: ep_len:779 episode reward: total was 22.140000. running mean: -16.479612\n",
      "ep 3878: ep_len:500 episode reward: total was 2.430000. running mean: -16.290516\n",
      "ep 3878: ep_len:113 episode reward: total was 53.010000. running mean: -15.597511\n",
      "ep 3878: ep_len:840 episode reward: total was -17.700000. running mean: -15.618536\n",
      "ep 3878: ep_len:2801 episode reward: total was -4.290000. running mean: -15.505251\n",
      "ep 3878: ep_len:32 episode reward: total was 14.500000. running mean: -15.205198\n",
      "epsilon:0.009992 episode_count: 58320. steps_count: 62888672.000000\n",
      "ep 3879: ep_len:804 episode reward: total was -3.640000. running mean: -15.089546\n",
      "ep 3879: ep_len:866 episode reward: total was 8.910000. running mean: -14.849551\n",
      "ep 3879: ep_len:68 episode reward: total was 31.000000. running mean: -14.391055\n",
      "ep 3879: ep_len:101 episode reward: total was 47.500000. running mean: -13.772145\n",
      "ep 3879: ep_len:524 episode reward: total was -5.790000. running mean: -13.692323\n",
      "ep 3879: ep_len:36 episode reward: total was 16.500000. running mean: -13.390400\n",
      "ep 3879: ep_len:500 episode reward: total was 9.280000. running mean: -13.163696\n",
      "ep 3879: ep_len:4169 episode reward: total was -149.750000. running mean: -14.529559\n",
      "ep 3879: ep_len:909 episode reward: total was -26.000000. running mean: -14.644263\n",
      "ep 3879: ep_len:739 episode reward: total was -15.820000. running mean: -14.656021\n",
      "ep 3879: ep_len:954 episode reward: total was 32.010000. running mean: -14.189360\n",
      "ep 3879: ep_len:69 episode reward: total was 33.000000. running mean: -13.717467\n",
      "ep 3879: ep_len:108 episode reward: total was 49.500000. running mean: -13.085292\n",
      "ep 3879: ep_len:1526 episode reward: total was 34.440000. running mean: -12.610039\n",
      "ep 3879: ep_len:2872 episode reward: total was -29.170000. running mean: -12.775639\n",
      "ep 3879: ep_len:59 episode reward: total was 25.000000. running mean: -12.397882\n",
      "epsilon:0.009992 episode_count: 58336. steps_count: 62902976.000000\n",
      "ep 3880: ep_len:799 episode reward: total was -57.830000. running mean: -12.852204\n",
      "ep 3880: ep_len:721 episode reward: total was -30.210000. running mean: -13.025782\n",
      "ep 3880: ep_len:71 episode reward: total was 34.000000. running mean: -12.555524\n",
      "ep 3880: ep_len:3050 episode reward: total was -235.170000. running mean: -14.781669\n",
      "ep 3880: ep_len:1223 episode reward: total was -12.850000. running mean: -14.762352\n",
      "ep 3880: ep_len:152 episode reward: total was 74.500000. running mean: -13.869728\n",
      "ep 3880: ep_len:78 episode reward: total was 37.500000. running mean: -13.356031\n",
      "ep 3880: ep_len:692 episode reward: total was 13.160000. running mean: -13.090871\n",
      "ep 3880: ep_len:641 episode reward: total was 16.900000. running mean: -12.790962\n",
      "ep 3880: ep_len:1576 episode reward: total was -18.720000. running mean: -12.850252\n",
      "ep 3880: ep_len:857 episode reward: total was 47.580000. running mean: -12.245950\n",
      "ep 3880: ep_len:983 episode reward: total was 14.130000. running mean: -11.982190\n",
      "ep 3880: ep_len:54 episode reward: total was 25.500000. running mean: -11.607369\n",
      "ep 3880: ep_len:55 episode reward: total was 26.000000. running mean: -11.231295\n",
      "ep 3880: ep_len:1075 episode reward: total was 26.470000. running mean: -10.854282\n",
      "ep 3880: ep_len:2762 episode reward: total was -15.830000. running mean: -10.904039\n",
      "epsilon:0.009992 episode_count: 58352. steps_count: 62917765.000000\n",
      "ep 3881: ep_len:852 episode reward: total was -28.920000. running mean: -11.084199\n",
      "ep 3881: ep_len:793 episode reward: total was 9.830000. running mean: -10.875057\n",
      "ep 3881: ep_len:3059 episode reward: total was 5.520000. running mean: -10.711106\n",
      "ep 3881: ep_len:869 episode reward: total was 12.100000. running mean: -10.482995\n",
      "ep 3881: ep_len:958 episode reward: total was -30.990000. running mean: -10.688065\n",
      "ep 3881: ep_len:4002 episode reward: total was -2811.450000. running mean: -38.695684\n",
      "ep 3881: ep_len:826 episode reward: total was -33.810000. running mean: -38.646828\n",
      "ep 3881: ep_len:830 episode reward: total was 53.240000. running mean: -37.727959\n",
      "ep 3881: ep_len:1097 episode reward: total was -3.340000. running mean: -37.384080\n",
      "ep 3881: ep_len:39 episode reward: total was 18.000000. running mean: -36.830239\n",
      "ep 3881: ep_len:77 episode reward: total was 35.500000. running mean: -36.106937\n",
      "ep 3881: ep_len:743 episode reward: total was -32.130000. running mean: -36.067167\n",
      "ep 3881: ep_len:2824 episode reward: total was -170.470000. running mean: -37.411196\n",
      "ep 3881: ep_len:50 episode reward: total was 23.500000. running mean: -36.802084\n",
      "epsilon:0.009992 episode_count: 58366. steps_count: 62934784.000000\n",
      "ep 3882: ep_len:718 episode reward: total was -60.660000. running mean: -37.040663\n",
      "ep 3882: ep_len:197 episode reward: total was 16.900000. running mean: -36.501256\n",
      "ep 3882: ep_len:3076 episode reward: total was -19.770000. running mean: -36.333944\n",
      "ep 3882: ep_len:703 episode reward: total was 0.130000. running mean: -35.969304\n",
      "ep 3882: ep_len:72 episode reward: total was 33.000000. running mean: -35.279611\n",
      "ep 3882: ep_len:88 episode reward: total was 42.500000. running mean: -34.501815\n",
      "ep 3882: ep_len:650 episode reward: total was -18.410000. running mean: -34.340897\n",
      "ep 3882: ep_len:3776 episode reward: total was -93.830000. running mean: -34.935788\n",
      "ep 3882: ep_len:813 episode reward: total was -54.580000. running mean: -35.132230\n",
      "ep 3882: ep_len:899 episode reward: total was 68.330000. running mean: -34.097608\n",
      "ep 3882: ep_len:640 episode reward: total was -142.590000. running mean: -35.182532\n",
      "ep 3882: ep_len:75 episode reward: total was 36.000000. running mean: -34.470706\n",
      "ep 3882: ep_len:34 episode reward: total was 15.500000. running mean: -33.970999\n",
      "ep 3882: ep_len:1089 episode reward: total was 23.340000. running mean: -33.397889\n",
      "ep 3882: ep_len:2850 episode reward: total was -48.800000. running mean: -33.551910\n",
      "epsilon:0.009992 episode_count: 58381. steps_count: 62950464.000000\n",
      "ep 3883: ep_len:1435 episode reward: total was 2.180000. running mean: -33.194591\n",
      "ep 3883: ep_len:712 episode reward: total was -28.540000. running mean: -33.148045\n",
      "ep 3883: ep_len:2997 episode reward: total was -48.450000. running mean: -33.301065\n",
      "ep 3883: ep_len:524 episode reward: total was -4.020000. running mean: -33.008254\n",
      "ep 3883: ep_len:53 episode reward: total was 25.000000. running mean: -32.428172\n",
      "ep 3883: ep_len:100 episode reward: total was 47.000000. running mean: -31.633890\n",
      "ep 3883: ep_len:72 episode reward: total was 33.000000. running mean: -30.987551\n",
      "ep 3883: ep_len:1033 episode reward: total was 4.100000. running mean: -30.636676\n",
      "ep 3883: ep_len:658 episode reward: total was 14.740000. running mean: -30.182909\n",
      "ep 3883: ep_len:757 episode reward: total was -10.940000. running mean: -29.990480\n",
      "ep 3883: ep_len:794 episode reward: total was 8.490000. running mean: -29.605675\n",
      "ep 3883: ep_len:1051 episode reward: total was 10.340000. running mean: -29.206218\n",
      "ep 3883: ep_len:25 episode reward: total was 11.000000. running mean: -28.804156\n",
      "ep 3883: ep_len:77 episode reward: total was 37.000000. running mean: -28.146114\n",
      "ep 3883: ep_len:1456 episode reward: total was 15.430000. running mean: -27.710353\n",
      "ep 3883: ep_len:32 episode reward: total was 14.500000. running mean: -27.288250\n",
      "ep 3883: ep_len:35 episode reward: total was 16.000000. running mean: -26.855367\n",
      "epsilon:0.009992 episode_count: 58398. steps_count: 62962275.000000\n",
      "ep 3884: ep_len:1111 episode reward: total was 1.850000. running mean: -26.568314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3884: ep_len:1193 episode reward: total was -23.590000. running mean: -26.538530\n",
      "ep 3884: ep_len:3027 episode reward: total was 29.970000. running mean: -25.973445\n",
      "ep 3884: ep_len:1137 episode reward: total was -16.070000. running mean: -25.874411\n",
      "ep 3884: ep_len:35 episode reward: total was 16.000000. running mean: -25.455667\n",
      "ep 3884: ep_len:107 episode reward: total was 49.000000. running mean: -24.711110\n",
      "ep 3884: ep_len:77 episode reward: total was 34.000000. running mean: -24.123999\n",
      "ep 3884: ep_len:1474 episode reward: total was -123.210000. running mean: -25.114859\n",
      "ep 3884: ep_len:3921 episode reward: total was -129.250000. running mean: -26.156210\n",
      "ep 3884: ep_len:1621 episode reward: total was -45.430000. running mean: -26.348948\n",
      "ep 3884: ep_len:829 episode reward: total was 39.450000. running mean: -25.690959\n",
      "ep 3884: ep_len:580 episode reward: total was -2.450000. running mean: -25.458549\n",
      "ep 3884: ep_len:69 episode reward: total was 33.000000. running mean: -24.873964\n",
      "ep 3884: ep_len:33 episode reward: total was 15.000000. running mean: -24.475224\n",
      "ep 3884: ep_len:86 episode reward: total was -33.000000. running mean: -24.560472\n",
      "ep 3884: ep_len:1420 episode reward: total was 5.090000. running mean: -24.263967\n",
      "ep 3884: ep_len:2842 episode reward: total was -13.310000. running mean: -24.154427\n",
      "ep 3884: ep_len:65 episode reward: total was 31.000000. running mean: -23.602883\n",
      "epsilon:0.009992 episode_count: 58416. steps_count: 62981902.000000\n",
      "ep 3885: ep_len:500 episode reward: total was 16.500000. running mean: -23.201854\n",
      "ep 3885: ep_len:938 episode reward: total was 13.400000. running mean: -22.835836\n",
      "ep 3885: ep_len:53 episode reward: total was 23.500000. running mean: -22.372477\n",
      "ep 3885: ep_len:98 episode reward: total was 46.000000. running mean: -21.688753\n",
      "ep 3885: ep_len:1626 episode reward: total was -22.200000. running mean: -21.693865\n",
      "ep 3885: ep_len:53 episode reward: total was 23.500000. running mean: -21.241926\n",
      "ep 3885: ep_len:91 episode reward: total was 44.000000. running mean: -20.589507\n",
      "ep 3885: ep_len:58 episode reward: total was 27.500000. running mean: -20.108612\n",
      "ep 3885: ep_len:1444 episode reward: total was -189.230000. running mean: -21.799826\n",
      "ep 3885: ep_len:3821 episode reward: total was 11.320000. running mean: -21.468628\n",
      "ep 3885: ep_len:557 episode reward: total was -36.010000. running mean: -21.614041\n",
      "ep 3885: ep_len:649 episode reward: total was -3.670000. running mean: -21.434601\n",
      "ep 3885: ep_len:621 episode reward: total was -9.780000. running mean: -21.318055\n",
      "ep 3885: ep_len:51 episode reward: total was 24.000000. running mean: -20.864874\n",
      "ep 3885: ep_len:774 episode reward: total was -27.950000. running mean: -20.935726\n",
      "ep 3885: ep_len:2862 episode reward: total was -19.840000. running mean: -20.924768\n",
      "epsilon:0.009992 episode_count: 58432. steps_count: 62996098.000000\n",
      "ep 3886: ep_len:1125 episode reward: total was 2.080000. running mean: -20.694721\n",
      "ep 3886: ep_len:1621 episode reward: total was -21.330000. running mean: -20.701073\n",
      "ep 3886: ep_len:3107 episode reward: total was -61.760000. running mean: -21.111663\n",
      "ep 3886: ep_len:659 episode reward: total was 19.660000. running mean: -20.703946\n",
      "ep 3886: ep_len:55 episode reward: total was 26.000000. running mean: -20.236907\n",
      "ep 3886: ep_len:748 episode reward: total was -35.940000. running mean: -20.393938\n",
      "ep 3886: ep_len:3886 episode reward: total was 6.770000. running mean: -20.122298\n",
      "ep 3886: ep_len:1979 episode reward: total was -767.660000. running mean: -27.597675\n",
      "ep 3886: ep_len:751 episode reward: total was 36.140000. running mean: -26.960298\n",
      "ep 3886: ep_len:840 episode reward: total was 26.260000. running mean: -26.428096\n",
      "ep 3886: ep_len:174 episode reward: total was 82.500000. running mean: -25.338815\n",
      "ep 3886: ep_len:745 episode reward: total was -85.640000. running mean: -25.941826\n",
      "ep 3886: ep_len:2908 episode reward: total was -0.340000. running mean: -25.685808\n",
      "ep 3886: ep_len:62 episode reward: total was 29.500000. running mean: -25.133950\n",
      "epsilon:0.009992 episode_count: 58446. steps_count: 63014758.000000\n",
      "ep 3887: ep_len:816 episode reward: total was -33.420000. running mean: -25.216811\n",
      "ep 3887: ep_len:793 episode reward: total was -11.420000. running mean: -25.078842\n",
      "ep 3887: ep_len:75 episode reward: total was 34.500000. running mean: -24.483054\n",
      "ep 3887: ep_len:2923 episode reward: total was 0.950000. running mean: -24.228723\n",
      "ep 3887: ep_len:703 episode reward: total was 25.000000. running mean: -23.736436\n",
      "ep 3887: ep_len:61 episode reward: total was 29.000000. running mean: -23.209072\n",
      "ep 3887: ep_len:70 episode reward: total was 32.000000. running mean: -22.656981\n",
      "ep 3887: ep_len:500 episode reward: total was 36.290000. running mean: -22.067511\n",
      "ep 3887: ep_len:3602 episode reward: total was -94.950000. running mean: -22.796336\n",
      "ep 3887: ep_len:1291 episode reward: total was -48.870000. running mean: -23.057073\n",
      "ep 3887: ep_len:809 episode reward: total was 15.060000. running mean: -22.675902\n",
      "ep 3887: ep_len:1062 episode reward: total was -43.550000. running mean: -22.884643\n",
      "ep 3887: ep_len:616 episode reward: total was -5.120000. running mean: -22.706997\n",
      "ep 3887: ep_len:2768 episode reward: total was -37.040000. running mean: -22.850327\n",
      "epsilon:0.009992 episode_count: 58460. steps_count: 63030847.000000\n",
      "ep 3888: ep_len:841 episode reward: total was -17.390000. running mean: -22.795723\n",
      "ep 3888: ep_len:965 episode reward: total was -19.870000. running mean: -22.766466\n",
      "ep 3888: ep_len:3040 episode reward: total was -36.390000. running mean: -22.902702\n",
      "ep 3888: ep_len:1619 episode reward: total was -8.160000. running mean: -22.755275\n",
      "ep 3888: ep_len:762 episode reward: total was -14.770000. running mean: -22.675422\n",
      "ep 3888: ep_len:647 episode reward: total was 25.070000. running mean: -22.197968\n",
      "ep 3888: ep_len:561 episode reward: total was -2.640000. running mean: -22.002388\n",
      "ep 3888: ep_len:605 episode reward: total was -5.780000. running mean: -21.840164\n",
      "ep 3888: ep_len:1460 episode reward: total was 18.070000. running mean: -21.441062\n",
      "ep 3888: ep_len:43 episode reward: total was 20.000000. running mean: -21.026652\n",
      "ep 3888: ep_len:62 episode reward: total was 29.500000. running mean: -20.521385\n",
      "ep 3888: ep_len:1501 episode reward: total was 10.740000. running mean: -20.208771\n",
      "ep 3888: ep_len:2787 episode reward: total was -3.760000. running mean: -20.044284\n",
      "ep 3888: ep_len:51 episode reward: total was 21.000000. running mean: -19.633841\n",
      "epsilon:0.009992 episode_count: 58474. steps_count: 63045791.000000\n",
      "ep 3889: ep_len:642 episode reward: total was -29.200000. running mean: -19.729502\n",
      "ep 3889: ep_len:1189 episode reward: total was -130.280000. running mean: -20.835007\n",
      "ep 3889: ep_len:73 episode reward: total was 32.000000. running mean: -20.306657\n",
      "ep 3889: ep_len:2972 episode reward: total was -35.990000. running mean: -20.463491\n",
      "ep 3889: ep_len:691 episode reward: total was 16.270000. running mean: -20.096156\n",
      "ep 3889: ep_len:110 episode reward: total was 52.000000. running mean: -19.375194\n",
      "ep 3889: ep_len:807 episode reward: total was 22.590000. running mean: -18.955542\n",
      "ep 3889: ep_len:3611 episode reward: total was -145.390000. running mean: -20.219887\n",
      "ep 3889: ep_len:859 episode reward: total was 15.400000. running mean: -19.863688\n",
      "ep 3889: ep_len:656 episode reward: total was 44.580000. running mean: -19.219251\n",
      "ep 3889: ep_len:651 episode reward: total was 4.070000. running mean: -18.986359\n",
      "ep 3889: ep_len:97 episode reward: total was 47.000000. running mean: -18.326495\n",
      "ep 3889: ep_len:199 episode reward: total was 95.000000. running mean: -17.193230\n",
      "ep 3889: ep_len:776 episode reward: total was -39.010000. running mean: -17.411398\n",
      "ep 3889: ep_len:2918 episode reward: total was -21.580000. running mean: -17.453084\n",
      "epsilon:0.009992 episode_count: 58489. steps_count: 63062042.000000\n",
      "ep 3890: ep_len:615 episode reward: total was -0.290000. running mean: -17.281453\n",
      "ep 3890: ep_len:500 episode reward: total was 3.610000. running mean: -17.072538\n",
      "ep 3890: ep_len:2986 episode reward: total was 27.450000. running mean: -16.627313\n",
      "ep 3890: ep_len:645 episode reward: total was 1.590000. running mean: -16.445140\n",
      "ep 3890: ep_len:54 episode reward: total was 25.500000. running mean: -16.025689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3890: ep_len:1048 episode reward: total was -44.230000. running mean: -16.307732\n",
      "ep 3890: ep_len:3866 episode reward: total was -6.310000. running mean: -16.207754\n",
      "ep 3890: ep_len:823 episode reward: total was -30.080000. running mean: -16.346477\n",
      "ep 3890: ep_len:822 episode reward: total was 44.620000. running mean: -15.736812\n",
      "ep 3890: ep_len:934 episode reward: total was 17.260000. running mean: -15.406844\n",
      "ep 3890: ep_len:61 episode reward: total was 29.000000. running mean: -14.962776\n",
      "ep 3890: ep_len:500 episode reward: total was 40.700000. running mean: -14.406148\n",
      "ep 3890: ep_len:2913 episode reward: total was -2.500000. running mean: -14.287086\n",
      "epsilon:0.009992 episode_count: 58502. steps_count: 63077809.000000\n",
      "ep 3891: ep_len:633 episode reward: total was 2.510000. running mean: -14.119115\n",
      "ep 3891: ep_len:1683 episode reward: total was -59.000000. running mean: -14.567924\n",
      "ep 3891: ep_len:61 episode reward: total was 29.000000. running mean: -14.132245\n",
      "ep 3891: ep_len:3029 episode reward: total was -7.740000. running mean: -14.068323\n",
      "ep 3891: ep_len:657 episode reward: total was 17.340000. running mean: -13.754239\n",
      "ep 3891: ep_len:145 episode reward: total was 71.000000. running mean: -12.906697\n",
      "ep 3891: ep_len:114 episode reward: total was 54.000000. running mean: -12.237630\n",
      "ep 3891: ep_len:977 episode reward: total was -56.050000. running mean: -12.675754\n",
      "ep 3891: ep_len:620 episode reward: total was 29.300000. running mean: -12.255996\n",
      "ep 3891: ep_len:659 episode reward: total was -10.750000. running mean: -12.240936\n",
      "ep 3891: ep_len:880 episode reward: total was 60.820000. running mean: -11.510327\n",
      "ep 3891: ep_len:671 episode reward: total was -16.200000. running mean: -11.557224\n",
      "ep 3891: ep_len:174 episode reward: total was 84.000000. running mean: -10.601651\n",
      "ep 3891: ep_len:93 episode reward: total was 43.500000. running mean: -10.060635\n",
      "ep 3891: ep_len:1112 episode reward: total was -20.360000. running mean: -10.163628\n",
      "ep 3891: ep_len:2844 episode reward: total was 9.300000. running mean: -9.968992\n",
      "epsilon:0.009992 episode_count: 58518. steps_count: 63092161.000000\n",
      "ep 3892: ep_len:1499 episode reward: total was 13.380000. running mean: -9.735502\n",
      "ep 3892: ep_len:804 episode reward: total was 14.580000. running mean: -9.492347\n",
      "ep 3892: ep_len:53 episode reward: total was 25.000000. running mean: -9.147424\n",
      "ep 3892: ep_len:2998 episode reward: total was 7.200000. running mean: -8.983950\n",
      "ep 3892: ep_len:893 episode reward: total was 73.610000. running mean: -8.158010\n",
      "ep 3892: ep_len:137 episode reward: total was 67.000000. running mean: -7.406430\n",
      "ep 3892: ep_len:80 episode reward: total was 38.500000. running mean: -6.947366\n",
      "ep 3892: ep_len:517 episode reward: total was -88.810000. running mean: -7.765992\n",
      "ep 3892: ep_len:658 episode reward: total was 29.160000. running mean: -7.396732\n",
      "ep 3892: ep_len:535 episode reward: total was -30.630000. running mean: -7.629065\n",
      "ep 3892: ep_len:858 episode reward: total was 39.870000. running mean: -7.154074\n",
      "ep 3892: ep_len:1501 episode reward: total was 2.290000. running mean: -7.059633\n",
      "ep 3892: ep_len:76 episode reward: total was 36.500000. running mean: -6.624037\n",
      "ep 3892: ep_len:500 episode reward: total was 26.610000. running mean: -6.291697\n",
      "ep 3892: ep_len:2831 episode reward: total was -20.520000. running mean: -6.433980\n",
      "ep 3892: ep_len:43 episode reward: total was 17.000000. running mean: -6.199640\n",
      "epsilon:0.009992 episode_count: 58534. steps_count: 63106144.000000\n",
      "ep 3893: ep_len:677 episode reward: total was -52.590000. running mean: -6.663543\n",
      "ep 3893: ep_len:804 episode reward: total was -6.350000. running mean: -6.660408\n",
      "ep 3893: ep_len:3029 episode reward: total was -9.920000. running mean: -6.693004\n",
      "ep 3893: ep_len:1738 episode reward: total was -30.990000. running mean: -6.935974\n",
      "ep 3893: ep_len:63 episode reward: total was 28.500000. running mean: -6.581614\n",
      "ep 3893: ep_len:661 episode reward: total was -0.630000. running mean: -6.522098\n",
      "ep 3893: ep_len:3526 episode reward: total was -73.090000. running mean: -7.187777\n",
      "ep 3893: ep_len:576 episode reward: total was 3.390000. running mean: -7.081999\n",
      "ep 3893: ep_len:7556 episode reward: total was -140.470000. running mean: -8.415879\n",
      "ep 3893: ep_len:643 episode reward: total was -2.320000. running mean: -8.354920\n",
      "ep 3893: ep_len:60 episode reward: total was 28.500000. running mean: -7.986371\n",
      "ep 3893: ep_len:491 episode reward: total was 41.370000. running mean: -7.492808\n",
      "ep 3893: ep_len:2846 episode reward: total was -12.540000. running mean: -7.543280\n",
      "epsilon:0.009992 episode_count: 58547. steps_count: 63128814.000000\n",
      "ep 3894: ep_len:656 episode reward: total was -1.310000. running mean: -7.480947\n",
      "ep 3894: ep_len:500 episode reward: total was 14.760000. running mean: -7.258537\n",
      "ep 3894: ep_len:95 episode reward: total was 46.000000. running mean: -6.725952\n",
      "ep 3894: ep_len:752 episode reward: total was 1.590000. running mean: -6.642792\n",
      "ep 3894: ep_len:57 episode reward: total was 27.000000. running mean: -6.306364\n",
      "ep 3894: ep_len:500 episode reward: total was 24.220000. running mean: -6.001101\n",
      "ep 3894: ep_len:3658 episode reward: total was -49.060000. running mean: -6.431690\n",
      "ep 3894: ep_len:1580 episode reward: total was -15.470000. running mean: -6.522073\n",
      "ep 3894: ep_len:771 episode reward: total was 64.960000. running mean: -5.807252\n",
      "ep 3894: ep_len:850 episode reward: total was 20.090000. running mean: -5.548280\n",
      "ep 3894: ep_len:108 episode reward: total was 52.500000. running mean: -4.967797\n",
      "ep 3894: ep_len:42 episode reward: total was 19.500000. running mean: -4.723119\n",
      "ep 3894: ep_len:754 episode reward: total was -23.510000. running mean: -4.910988\n",
      "ep 3894: ep_len:2883 episode reward: total was -9.620000. running mean: -4.958078\n",
      "ep 3894: ep_len:53 episode reward: total was 25.000000. running mean: -4.658497\n",
      "epsilon:0.009992 episode_count: 58562. steps_count: 63142073.000000\n",
      "ep 3895: ep_len:1050 episode reward: total was -135.110000. running mean: -5.963012\n",
      "ep 3895: ep_len:500 episode reward: total was 15.500000. running mean: -5.748382\n",
      "ep 3895: ep_len:3049 episode reward: total was -48.970000. running mean: -6.180598\n",
      "ep 3895: ep_len:1427 episode reward: total was 10.090000. running mean: -6.017892\n",
      "ep 3895: ep_len:44 episode reward: total was 20.500000. running mean: -5.752713\n",
      "ep 3895: ep_len:77 episode reward: total was 37.000000. running mean: -5.325186\n",
      "ep 3895: ep_len:97 episode reward: total was 47.000000. running mean: -4.801934\n",
      "ep 3895: ep_len:779 episode reward: total was -4.500000. running mean: -4.798915\n",
      "ep 3895: ep_len:3816 episode reward: total was -12.660000. running mean: -4.877526\n",
      "ep 3895: ep_len:936 episode reward: total was -58.900000. running mean: -5.417750\n",
      "ep 3895: ep_len:765 episode reward: total was 20.190000. running mean: -5.161673\n",
      "ep 3895: ep_len:969 episode reward: total was -20.040000. running mean: -5.310456\n",
      "ep 3895: ep_len:89 episode reward: total was 41.500000. running mean: -4.842352\n",
      "ep 3895: ep_len:1174 episode reward: total was -18.450000. running mean: -4.978428\n",
      "ep 3895: ep_len:2863 episode reward: total was -19.410000. running mean: -5.122744\n",
      "epsilon:0.009992 episode_count: 58577. steps_count: 63159708.000000\n",
      "ep 3896: ep_len:700 episode reward: total was -62.860000. running mean: -5.700116\n",
      "ep 3896: ep_len:206 episode reward: total was 7.500000. running mean: -5.568115\n",
      "ep 3896: ep_len:64 episode reward: total was 29.000000. running mean: -5.222434\n",
      "ep 3896: ep_len:2920 episode reward: total was 2.660000. running mean: -5.143610\n",
      "ep 3896: ep_len:679 episode reward: total was -6.510000. running mean: -5.157274\n",
      "ep 3896: ep_len:40 episode reward: total was 18.500000. running mean: -4.920701\n",
      "ep 3896: ep_len:129 episode reward: total was 60.000000. running mean: -4.271494\n",
      "ep 3896: ep_len:55 episode reward: total was 26.000000. running mean: -3.968779\n",
      "ep 3896: ep_len:80 episode reward: total was 37.000000. running mean: -3.559091\n",
      "ep 3896: ep_len:1488 episode reward: total was -62.120000. running mean: -4.144700\n",
      "ep 3896: ep_len:4112 episode reward: total was -123.510000. running mean: -5.338353\n",
      "ep 3896: ep_len:1593 episode reward: total was -64.690000. running mean: -5.931870\n",
      "ep 3896: ep_len:845 episode reward: total was 55.970000. running mean: -5.312851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3896: ep_len:648 episode reward: total was -70.390000. running mean: -5.963623\n",
      "ep 3896: ep_len:60 episode reward: total was 25.500000. running mean: -5.648986\n",
      "ep 3896: ep_len:1491 episode reward: total was 1.550000. running mean: -5.576996\n",
      "ep 3896: ep_len:2784 episode reward: total was -2.020000. running mean: -5.541426\n",
      "epsilon:0.009992 episode_count: 58594. steps_count: 63177602.000000\n",
      "ep 3897: ep_len:1065 episode reward: total was 13.000000. running mean: -5.356012\n",
      "ep 3897: ep_len:1168 episode reward: total was 1.840000. running mean: -5.284052\n",
      "ep 3897: ep_len:3046 episode reward: total was 13.170000. running mean: -5.099512\n",
      "ep 3897: ep_len:780 episode reward: total was 34.840000. running mean: -4.700116\n",
      "ep 3897: ep_len:40 episode reward: total was 18.500000. running mean: -4.468115\n",
      "ep 3897: ep_len:1396 episode reward: total was -114.630000. running mean: -5.569734\n",
      "ep 3897: ep_len:335 episode reward: total was 17.490000. running mean: -5.339137\n",
      "ep 3897: ep_len:550 episode reward: total was -1.310000. running mean: -5.298845\n",
      "ep 3897: ep_len:819 episode reward: total was 41.340000. running mean: -4.832457\n",
      "ep 3897: ep_len:668 episode reward: total was -7.630000. running mean: -4.860432\n",
      "ep 3897: ep_len:72 episode reward: total was 34.500000. running mean: -4.466828\n",
      "ep 3897: ep_len:57 episode reward: total was 27.000000. running mean: -4.152160\n",
      "ep 3897: ep_len:1017 episode reward: total was 5.110000. running mean: -4.059538\n",
      "ep 3897: ep_len:2811 episode reward: total was 1.560000. running mean: -4.003343\n",
      "epsilon:0.009992 episode_count: 58608. steps_count: 63191426.000000\n",
      "ep 3898: ep_len:1478 episode reward: total was 11.030000. running mean: -3.853009\n",
      "ep 3898: ep_len:175 episode reward: total was -4.380000. running mean: -3.858279\n",
      "ep 3898: ep_len:52 episode reward: total was 24.500000. running mean: -3.574697\n",
      "ep 3898: ep_len:103 episode reward: total was 47.000000. running mean: -3.068950\n",
      "ep 3898: ep_len:630 episode reward: total was 0.590000. running mean: -3.032360\n",
      "ep 3898: ep_len:124 episode reward: total was 54.500000. running mean: -2.457036\n",
      "ep 3898: ep_len:65 episode reward: total was 31.000000. running mean: -2.122466\n",
      "ep 3898: ep_len:1384 episode reward: total was -32.790000. running mean: -2.429141\n",
      "ep 3898: ep_len:3917 episode reward: total was -113.740000. running mean: -3.542250\n",
      "ep 3898: ep_len:604 episode reward: total was 22.010000. running mean: -3.286728\n",
      "ep 3898: ep_len:7389 episode reward: total was 4.980000. running mean: -3.204060\n",
      "ep 3898: ep_len:708 episode reward: total was -36.490000. running mean: -3.536920\n",
      "ep 3898: ep_len:73 episode reward: total was 33.500000. running mean: -3.166550\n",
      "ep 3898: ep_len:1983 episode reward: total was -447.510000. running mean: -7.609985\n",
      "ep 3898: ep_len:2815 episode reward: total was -1.730000. running mean: -7.551185\n",
      "ep 3898: ep_len:48 episode reward: total was 21.000000. running mean: -7.265673\n",
      "epsilon:0.009992 episode_count: 58624. steps_count: 63212974.000000\n",
      "ep 3899: ep_len:1389 episode reward: total was 11.240000. running mean: -7.080617\n",
      "ep 3899: ep_len:764 episode reward: total was -12.090000. running mean: -7.130710\n",
      "ep 3899: ep_len:75 episode reward: total was 34.500000. running mean: -6.714403\n",
      "ep 3899: ep_len:3071 episode reward: total was -2.410000. running mean: -6.671359\n",
      "ep 3899: ep_len:883 episode reward: total was 52.840000. running mean: -6.076246\n",
      "ep 3899: ep_len:61 episode reward: total was 29.000000. running mean: -5.725483\n",
      "ep 3899: ep_len:96 episode reward: total was 45.000000. running mean: -5.218228\n",
      "ep 3899: ep_len:84 episode reward: total was 39.000000. running mean: -4.776046\n",
      "ep 3899: ep_len:500 episode reward: total was 49.030000. running mean: -4.237986\n",
      "ep 3899: ep_len:3798 episode reward: total was -51.670000. running mean: -4.712306\n",
      "ep 3899: ep_len:920 episode reward: total was -18.240000. running mean: -4.847583\n",
      "ep 3899: ep_len:765 episode reward: total was -7.360000. running mean: -4.872707\n",
      "ep 3899: ep_len:2286 episode reward: total was -243.480000. running mean: -7.258780\n",
      "ep 3899: ep_len:150 episode reward: total was 70.500000. running mean: -6.481192\n",
      "ep 3899: ep_len:768 episode reward: total was -12.690000. running mean: -6.543280\n",
      "ep 3899: ep_len:2897 episode reward: total was -10.460000. running mean: -6.582447\n",
      "epsilon:0.009992 episode_count: 58640. steps_count: 63231481.000000\n",
      "ep 3900: ep_len:857 episode reward: total was 24.530000. running mean: -6.271323\n",
      "ep 3900: ep_len:1612 episode reward: total was -70.240000. running mean: -6.911010\n",
      "ep 3900: ep_len:2899 episode reward: total was -29.110000. running mean: -7.132999\n",
      "ep 3900: ep_len:651 episode reward: total was 5.530000. running mean: -7.006369\n",
      "ep 3900: ep_len:163 episode reward: total was 72.500000. running mean: -6.211306\n",
      "ep 3900: ep_len:61 episode reward: total was 27.500000. running mean: -5.874193\n",
      "ep 3900: ep_len:1446 episode reward: total was 0.970000. running mean: -5.805751\n",
      "ep 3900: ep_len:312 episode reward: total was 14.840000. running mean: -5.599293\n",
      "ep 3900: ep_len:2043 episode reward: total was -282.220000. running mean: -8.365500\n",
      "ep 3900: ep_len:7574 episode reward: total was -47.160000. running mean: -8.753445\n",
      "ep 3900: ep_len:1084 episode reward: total was -4.780000. running mean: -8.713711\n",
      "ep 3900: ep_len:59 episode reward: total was 28.000000. running mean: -8.346574\n",
      "ep 3900: ep_len:1131 episode reward: total was -8.050000. running mean: -8.343608\n",
      "ep 3900: ep_len:2768 episode reward: total was -28.070000. running mean: -8.540872\n",
      "ep 3900: ep_len:42 episode reward: total was 18.000000. running mean: -8.275463\n",
      "epsilon:0.009992 episode_count: 58655. steps_count: 63254183.000000\n",
      "ep 3901: ep_len:1424 episode reward: total was 23.400000. running mean: -7.958709\n",
      "ep 3901: ep_len:3694 episode reward: total was -467.340000. running mean: -12.552522\n",
      "ep 3901: ep_len:61 episode reward: total was 29.000000. running mean: -12.136996\n",
      "ep 3901: ep_len:2953 episode reward: total was -32.240000. running mean: -12.338026\n",
      "ep 3901: ep_len:1674 episode reward: total was -23.890000. running mean: -12.453546\n",
      "ep 3901: ep_len:26 episode reward: total was 11.500000. running mean: -12.214011\n",
      "ep 3901: ep_len:84 episode reward: total was 40.500000. running mean: -11.686871\n",
      "ep 3901: ep_len:1002 episode reward: total was 2.780000. running mean: -11.542202\n",
      "ep 3901: ep_len:648 episode reward: total was 25.880000. running mean: -11.167980\n",
      "ep 3901: ep_len:548 episode reward: total was -9.840000. running mean: -11.154700\n",
      "ep 3901: ep_len:704 episode reward: total was 39.460000. running mean: -10.648553\n",
      "ep 3901: ep_len:1511 episode reward: total was 14.450000. running mean: -10.397567\n",
      "ep 3901: ep_len:138 episode reward: total was 67.500000. running mean: -9.618592\n",
      "ep 3901: ep_len:46 episode reward: total was 20.000000. running mean: -9.322406\n",
      "ep 3901: ep_len:103 episode reward: total was 48.500000. running mean: -8.744182\n",
      "ep 3901: ep_len:1169 episode reward: total was -7.360000. running mean: -8.730340\n",
      "ep 3901: ep_len:2797 episode reward: total was -10.880000. running mean: -8.751837\n",
      "epsilon:0.009992 episode_count: 58672. steps_count: 63272765.000000\n",
      "ep 3902: ep_len:885 episode reward: total was -10.900000. running mean: -8.773318\n",
      "ep 3902: ep_len:638 episode reward: total was -15.640000. running mean: -8.841985\n",
      "ep 3902: ep_len:2972 episode reward: total was -30.470000. running mean: -9.058265\n",
      "ep 3902: ep_len:722 episode reward: total was 7.470000. running mean: -8.892983\n",
      "ep 3902: ep_len:98 episode reward: total was 44.500000. running mean: -8.359053\n",
      "ep 3902: ep_len:46 episode reward: total was 20.000000. running mean: -8.075462\n",
      "ep 3902: ep_len:500 episode reward: total was 18.860000. running mean: -7.806108\n",
      "ep 3902: ep_len:3740 episode reward: total was 3.270000. running mean: -7.695346\n",
      "ep 3902: ep_len:935 episode reward: total was -16.060000. running mean: -7.778993\n",
      "ep 3902: ep_len:705 episode reward: total was 27.280000. running mean: -7.428403\n",
      "ep 3902: ep_len:879 episode reward: total was 21.540000. running mean: -7.138719\n",
      "ep 3902: ep_len:643 episode reward: total was -19.910000. running mean: -7.266432\n",
      "ep 3902: ep_len:2858 episode reward: total was -33.410000. running mean: -7.527868\n",
      "ep 3902: ep_len:44 episode reward: total was 19.000000. running mean: -7.262589\n",
      "epsilon:0.009992 episode_count: 58686. steps_count: 63288430.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3903: ep_len:738 episode reward: total was 4.350000. running mean: -7.146463\n",
      "ep 3903: ep_len:793 episode reward: total was 20.180000. running mean: -6.873198\n",
      "ep 3903: ep_len:30 episode reward: total was 12.000000. running mean: -6.684466\n",
      "ep 3903: ep_len:2935 episode reward: total was -71.970000. running mean: -7.337322\n",
      "ep 3903: ep_len:551 episode reward: total was -25.850000. running mean: -7.522448\n",
      "ep 3903: ep_len:64 episode reward: total was 27.500000. running mean: -7.172224\n",
      "ep 3903: ep_len:170 episode reward: total was 80.500000. running mean: -6.295502\n",
      "ep 3903: ep_len:61 episode reward: total was 29.000000. running mean: -5.942547\n",
      "ep 3903: ep_len:74 episode reward: total was 35.500000. running mean: -5.528121\n",
      "ep 3903: ep_len:1455 episode reward: total was -3.710000. running mean: -5.509940\n",
      "ep 3903: ep_len:326 episode reward: total was 6.930000. running mean: -5.385541\n",
      "ep 3903: ep_len:628 episode reward: total was 14.600000. running mean: -5.185685\n",
      "ep 3903: ep_len:768 episode reward: total was 45.430000. running mean: -4.679528\n",
      "ep 3903: ep_len:596 episode reward: total was 8.410000. running mean: -4.548633\n",
      "ep 3903: ep_len:63 episode reward: total was 30.000000. running mean: -4.203147\n",
      "ep 3903: ep_len:141 episode reward: total was 66.000000. running mean: -3.501115\n",
      "ep 3903: ep_len:70 episode reward: total was 30.500000. running mean: -3.161104\n",
      "ep 3903: ep_len:759 episode reward: total was -68.850000. running mean: -3.817993\n",
      "ep 3903: ep_len:2859 episode reward: total was -25.690000. running mean: -4.036713\n",
      "ep 3903: ep_len:40 episode reward: total was 18.500000. running mean: -3.811346\n",
      "epsilon:0.009992 episode_count: 58706. steps_count: 63301551.000000\n",
      "ep 3904: ep_len:727 episode reward: total was -3.750000. running mean: -3.810733\n",
      "ep 3904: ep_len:1647 episode reward: total was -33.180000. running mean: -4.104425\n",
      "ep 3904: ep_len:3021 episode reward: total was -78.000000. running mean: -4.843381\n",
      "ep 3904: ep_len:1118 episode reward: total was -43.530000. running mean: -5.230247\n",
      "ep 3904: ep_len:150 episode reward: total was 73.500000. running mean: -4.442945\n",
      "ep 3904: ep_len:72 episode reward: total was 31.500000. running mean: -4.083515\n",
      "ep 3904: ep_len:996 episode reward: total was -15.670000. running mean: -4.199380\n",
      "ep 3904: ep_len:4018 episode reward: total was -53.490000. running mean: -4.692286\n",
      "ep 3904: ep_len:673 episode reward: total was -3.970000. running mean: -4.685063\n",
      "ep 3904: ep_len:872 episode reward: total was 57.930000. running mean: -4.058913\n",
      "ep 3904: ep_len:599 episode reward: total was -32.440000. running mean: -4.342724\n",
      "ep 3904: ep_len:140 episode reward: total was 62.500000. running mean: -3.674296\n",
      "ep 3904: ep_len:679 episode reward: total was -43.880000. running mean: -4.076354\n",
      "ep 3904: ep_len:2822 episode reward: total was -11.520000. running mean: -4.150790\n",
      "epsilon:0.009992 episode_count: 58720. steps_count: 63319085.000000\n",
      "ep 3905: ep_len:870 episode reward: total was 8.340000. running mean: -4.025882\n",
      "ep 3905: ep_len:931 episode reward: total was 1.830000. running mean: -3.967323\n",
      "ep 3905: ep_len:2865 episode reward: total was -12.240000. running mean: -4.050050\n",
      "ep 3905: ep_len:741 episode reward: total was -15.990000. running mean: -4.169450\n",
      "ep 3905: ep_len:44 episode reward: total was 20.500000. running mean: -3.922755\n",
      "ep 3905: ep_len:770 episode reward: total was -12.700000. running mean: -4.010527\n",
      "ep 3905: ep_len:301 episode reward: total was 20.080000. running mean: -3.769622\n",
      "ep 3905: ep_len:776 episode reward: total was -63.760000. running mean: -4.369526\n",
      "ep 3905: ep_len:879 episode reward: total was 59.740000. running mean: -3.728431\n",
      "ep 3905: ep_len:635 episode reward: total was -6.990000. running mean: -3.761046\n",
      "ep 3905: ep_len:91 episode reward: total was 42.500000. running mean: -3.298436\n",
      "ep 3905: ep_len:170 episode reward: total was 80.500000. running mean: -2.460452\n",
      "ep 3905: ep_len:500 episode reward: total was 50.250000. running mean: -1.933347\n",
      "ep 3905: ep_len:2851 episode reward: total was -4.740000. running mean: -1.961414\n",
      "ep 3905: ep_len:67 episode reward: total was 30.500000. running mean: -1.636799\n",
      "epsilon:0.009992 episode_count: 58735. steps_count: 63331576.000000\n",
      "ep 3906: ep_len:969 episode reward: total was -12.920000. running mean: -1.749631\n",
      "ep 3906: ep_len:618 episode reward: total was 3.990000. running mean: -1.692235\n",
      "ep 3906: ep_len:50 episode reward: total was 23.500000. running mean: -1.440313\n",
      "ep 3906: ep_len:3031 episode reward: total was 5.490000. running mean: -1.371010\n",
      "ep 3906: ep_len:700 episode reward: total was -15.710000. running mean: -1.514400\n",
      "ep 3906: ep_len:875 episode reward: total was 36.350000. running mean: -1.135756\n",
      "ep 3906: ep_len:634 episode reward: total was 23.410000. running mean: -0.890298\n",
      "ep 3906: ep_len:630 episode reward: total was -25.280000. running mean: -1.134195\n",
      "ep 3906: ep_len:672 episode reward: total was 26.640000. running mean: -0.856453\n",
      "ep 3906: ep_len:583 episode reward: total was 56.500000. running mean: -0.282889\n",
      "ep 3906: ep_len:62 episode reward: total was 29.500000. running mean: 0.014940\n",
      "ep 3906: ep_len:45 episode reward: total was 21.000000. running mean: 0.224791\n",
      "ep 3906: ep_len:71 episode reward: total was 34.000000. running mean: 0.562543\n",
      "ep 3906: ep_len:1485 episode reward: total was -25.020000. running mean: 0.306718\n",
      "ep 3906: ep_len:2861 episode reward: total was -21.750000. running mean: 0.086150\n",
      "epsilon:0.009992 episode_count: 58750. steps_count: 63344862.000000\n",
      "ep 3907: ep_len:1137 episode reward: total was -19.180000. running mean: -0.106511\n",
      "ep 3907: ep_len:736 episode reward: total was -12.000000. running mean: -0.225446\n",
      "ep 3907: ep_len:3010 episode reward: total was -50.040000. running mean: -0.723592\n",
      "ep 3907: ep_len:500 episode reward: total was 26.520000. running mean: -0.451156\n",
      "ep 3907: ep_len:55 episode reward: total was 24.500000. running mean: -0.201644\n",
      "ep 3907: ep_len:150 episode reward: total was 73.500000. running mean: 0.535372\n",
      "ep 3907: ep_len:41 episode reward: total was 19.000000. running mean: 0.720019\n",
      "ep 3907: ep_len:943 episode reward: total was 69.580000. running mean: 1.408618\n",
      "ep 3907: ep_len:4161 episode reward: total was -523.890000. running mean: -3.844368\n",
      "ep 3907: ep_len:564 episode reward: total was 7.770000. running mean: -3.728224\n",
      "ep 3907: ep_len:759 episode reward: total was -1.090000. running mean: -3.701842\n",
      "ep 3907: ep_len:702 episode reward: total was 22.270000. running mean: -3.442123\n",
      "ep 3907: ep_len:45 episode reward: total was 15.000000. running mean: -3.257702\n",
      "ep 3907: ep_len:102 episode reward: total was 48.000000. running mean: -2.745125\n",
      "ep 3907: ep_len:762 episode reward: total was -7.700000. running mean: -2.794674\n",
      "ep 3907: ep_len:2789 episode reward: total was -24.580000. running mean: -3.012527\n",
      "ep 3907: ep_len:53 episode reward: total was 23.500000. running mean: -2.747402\n",
      "epsilon:0.009992 episode_count: 58767. steps_count: 63361371.000000\n",
      "ep 3908: ep_len:882 episode reward: total was 6.080000. running mean: -2.659128\n",
      "ep 3908: ep_len:1616 episode reward: total was -83.300000. running mean: -3.465537\n",
      "ep 3908: ep_len:3001 episode reward: total was -16.370000. running mean: -3.594581\n",
      "ep 3908: ep_len:571 episode reward: total was -29.740000. running mean: -3.856035\n",
      "ep 3908: ep_len:56 episode reward: total was 23.500000. running mean: -3.582475\n",
      "ep 3908: ep_len:111 episode reward: total was 54.000000. running mean: -3.006650\n",
      "ep 3908: ep_len:500 episode reward: total was -0.760000. running mean: -2.984184\n",
      "ep 3908: ep_len:3516 episode reward: total was -13.080000. running mean: -3.085142\n",
      "ep 3908: ep_len:3875 episode reward: total was -822.950000. running mean: -11.283791\n",
      "ep 3908: ep_len:651 episode reward: total was 0.710000. running mean: -11.163853\n",
      "ep 3908: ep_len:640 episode reward: total was 16.540000. running mean: -10.886814\n",
      "ep 3908: ep_len:199 episode reward: total was 98.000000. running mean: -9.797946\n",
      "ep 3908: ep_len:1100 episode reward: total was 4.930000. running mean: -9.650666\n",
      "ep 3908: ep_len:2821 episode reward: total was -24.020000. running mean: -9.794360\n",
      "epsilon:0.009992 episode_count: 58781. steps_count: 63380910.000000\n",
      "ep 3909: ep_len:686 episode reward: total was -13.510000. running mean: -9.831516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3909: ep_len:753 episode reward: total was -6.780000. running mean: -9.801001\n",
      "ep 3909: ep_len:3097 episode reward: total was 14.020000. running mean: -9.562791\n",
      "ep 3909: ep_len:688 episode reward: total was 4.530000. running mean: -9.421863\n",
      "ep 3909: ep_len:53 episode reward: total was 23.500000. running mean: -9.092645\n",
      "ep 3909: ep_len:156 episode reward: total was 76.500000. running mean: -8.236718\n",
      "ep 3909: ep_len:58 episode reward: total was 27.500000. running mean: -7.879351\n",
      "ep 3909: ep_len:1100 episode reward: total was -11.390000. running mean: -7.914457\n",
      "ep 3909: ep_len:4070 episode reward: total was -67.570000. running mean: -8.511013\n",
      "ep 3909: ep_len:744 episode reward: total was -16.340000. running mean: -8.589303\n",
      "ep 3909: ep_len:701 episode reward: total was 27.240000. running mean: -8.231010\n",
      "ep 3909: ep_len:688 episode reward: total was -8.790000. running mean: -8.236600\n",
      "ep 3909: ep_len:52 episode reward: total was 24.500000. running mean: -7.909234\n",
      "ep 3909: ep_len:49 episode reward: total was 20.000000. running mean: -7.630141\n",
      "ep 3909: ep_len:624 episode reward: total was -8.270000. running mean: -7.636540\n",
      "ep 3909: ep_len:47 episode reward: total was 22.000000. running mean: -7.340174\n",
      "epsilon:0.009992 episode_count: 58797. steps_count: 63394476.000000\n",
      "ep 3910: ep_len:1121 episode reward: total was 5.390000. running mean: -7.212873\n",
      "ep 3910: ep_len:185 episode reward: total was 6.770000. running mean: -7.073044\n",
      "ep 3910: ep_len:2946 episode reward: total was 7.000000. running mean: -6.932314\n",
      "ep 3910: ep_len:1621 episode reward: total was -75.470000. running mean: -7.617690\n",
      "ep 3910: ep_len:33 episode reward: total was 15.000000. running mean: -7.391513\n",
      "ep 3910: ep_len:89 episode reward: total was 43.000000. running mean: -6.887598\n",
      "ep 3910: ep_len:3352 episode reward: total was -1827.190000. running mean: -25.090622\n",
      "ep 3910: ep_len:650 episode reward: total was 16.500000. running mean: -24.674716\n",
      "ep 3910: ep_len:572 episode reward: total was -1.520000. running mean: -24.443169\n",
      "ep 3910: ep_len:873 episode reward: total was 51.290000. running mean: -23.685837\n",
      "ep 3910: ep_len:500 episode reward: total was -6.860000. running mean: -23.517579\n",
      "ep 3910: ep_len:65 episode reward: total was 29.500000. running mean: -22.987403\n",
      "ep 3910: ep_len:1079 episode reward: total was -25.130000. running mean: -23.008829\n",
      "ep 3910: ep_len:2822 episode reward: total was -29.640000. running mean: -23.075141\n",
      "ep 3910: ep_len:27 episode reward: total was 12.000000. running mean: -22.724389\n",
      "epsilon:0.009992 episode_count: 58812. steps_count: 63410411.000000\n",
      "ep 3911: ep_len:1153 episode reward: total was -33.080000. running mean: -22.827945\n",
      "ep 3911: ep_len:500 episode reward: total was -27.300000. running mean: -22.872666\n",
      "ep 3911: ep_len:2996 episode reward: total was -196.020000. running mean: -24.604139\n",
      "ep 3911: ep_len:538 episode reward: total was -68.520000. running mean: -25.043298\n",
      "ep 3911: ep_len:114 episode reward: total was 54.000000. running mean: -24.252865\n",
      "ep 3911: ep_len:671 episode reward: total was -1.460000. running mean: -24.024936\n",
      "ep 3911: ep_len:4080 episode reward: total was -1407.380000. running mean: -37.858487\n",
      "ep 3911: ep_len:1498 episode reward: total was -74.080000. running mean: -38.220702\n",
      "ep 3911: ep_len:7488 episode reward: total was 26.730000. running mean: -37.571195\n",
      "ep 3911: ep_len:1837 episode reward: total was -157.850000. running mean: -38.773983\n",
      "ep 3911: ep_len:45 episode reward: total was 21.000000. running mean: -38.176243\n",
      "ep 3911: ep_len:764 episode reward: total was 20.800000. running mean: -37.586481\n",
      "ep 3911: ep_len:2830 episode reward: total was -40.580000. running mean: -37.616416\n",
      "ep 3911: ep_len:55 episode reward: total was 24.500000. running mean: -36.995252\n",
      "epsilon:0.009992 episode_count: 58826. steps_count: 63434980.000000\n",
      "ep 3912: ep_len:891 episode reward: total was 19.300000. running mean: -36.432299\n",
      "ep 3912: ep_len:1283 episode reward: total was -41.880000. running mean: -36.486776\n",
      "ep 3912: ep_len:3073 episode reward: total was -54.830000. running mean: -36.670209\n",
      "ep 3912: ep_len:830 episode reward: total was -13.840000. running mean: -36.441907\n",
      "ep 3912: ep_len:147 episode reward: total was 70.500000. running mean: -35.372487\n",
      "ep 3912: ep_len:62 episode reward: total was 26.500000. running mean: -34.753763\n",
      "ep 3912: ep_len:1075 episode reward: total was -13.660000. running mean: -34.542825\n",
      "ep 3912: ep_len:3730 episode reward: total was -21.730000. running mean: -34.414697\n",
      "ep 3912: ep_len:577 episode reward: total was -12.060000. running mean: -34.191150\n",
      "ep 3912: ep_len:613 episode reward: total was 13.250000. running mean: -33.716738\n",
      "ep 3912: ep_len:1517 episode reward: total was 11.050000. running mean: -33.269071\n",
      "ep 3912: ep_len:61 episode reward: total was 26.000000. running mean: -32.676380\n",
      "ep 3912: ep_len:36 episode reward: total was 15.000000. running mean: -32.199616\n",
      "ep 3912: ep_len:80 episode reward: total was 38.500000. running mean: -31.492620\n",
      "ep 3912: ep_len:1433 episode reward: total was 6.660000. running mean: -31.111094\n",
      "ep 3912: ep_len:46 episode reward: total was 21.500000. running mean: -30.584983\n",
      "epsilon:0.009992 episode_count: 58842. steps_count: 63450434.000000\n",
      "ep 3913: ep_len:1393 episode reward: total was 16.270000. running mean: -30.116433\n",
      "ep 3913: ep_len:1002 episode reward: total was 42.950000. running mean: -29.385769\n",
      "ep 3913: ep_len:2979 episode reward: total was -21.760000. running mean: -29.309511\n",
      "ep 3913: ep_len:1448 episode reward: total was 6.170000. running mean: -28.954716\n",
      "ep 3913: ep_len:70 episode reward: total was 33.500000. running mean: -28.330169\n",
      "ep 3913: ep_len:999 episode reward: total was -4.470000. running mean: -28.091567\n",
      "ep 3913: ep_len:3665 episode reward: total was -22.270000. running mean: -28.033352\n",
      "ep 3913: ep_len:651 episode reward: total was 1.290000. running mean: -27.740118\n",
      "ep 3913: ep_len:837 episode reward: total was 41.310000. running mean: -27.049617\n",
      "ep 3913: ep_len:557 episode reward: total was -5.190000. running mean: -26.831021\n",
      "ep 3913: ep_len:31 episode reward: total was 14.000000. running mean: -26.422711\n",
      "ep 3913: ep_len:60 episode reward: total was 28.500000. running mean: -25.873483\n",
      "ep 3913: ep_len:1145 episode reward: total was 12.050000. running mean: -25.494249\n",
      "ep 3913: ep_len:2837 episode reward: total was -40.880000. running mean: -25.648106\n",
      "ep 3913: ep_len:56 episode reward: total was 26.500000. running mean: -25.126625\n",
      "epsilon:0.009992 episode_count: 58857. steps_count: 63468164.000000\n",
      "ep 3914: ep_len:604 episode reward: total was -13.320000. running mean: -25.008559\n",
      "ep 3914: ep_len:1123 episode reward: total was 2.610000. running mean: -24.732373\n",
      "ep 3914: ep_len:3037 episode reward: total was -7.190000. running mean: -24.556949\n",
      "ep 3914: ep_len:559 episode reward: total was -8.290000. running mean: -24.394280\n",
      "ep 3914: ep_len:43 episode reward: total was 18.500000. running mean: -23.965337\n",
      "ep 3914: ep_len:676 episode reward: total was 28.900000. running mean: -23.436684\n",
      "ep 3914: ep_len:3946 episode reward: total was -103.380000. running mean: -24.236117\n",
      "ep 3914: ep_len:780 episode reward: total was -53.690000. running mean: -24.530656\n",
      "ep 3914: ep_len:706 episode reward: total was 36.150000. running mean: -23.923849\n",
      "ep 3914: ep_len:1049 episode reward: total was 28.630000. running mean: -23.398311\n",
      "ep 3914: ep_len:73 episode reward: total was 33.500000. running mean: -22.829328\n",
      "ep 3914: ep_len:1110 episode reward: total was -7.250000. running mean: -22.673534\n",
      "ep 3914: ep_len:2771 episode reward: total was -17.380000. running mean: -22.620599\n",
      "ep 3914: ep_len:44 episode reward: total was 19.000000. running mean: -22.204393\n",
      "epsilon:0.009992 episode_count: 58871. steps_count: 63484685.000000\n",
      "ep 3915: ep_len:631 episode reward: total was 17.470000. running mean: -21.807649\n",
      "ep 3915: ep_len:795 episode reward: total was -23.250000. running mean: -21.822073\n",
      "ep 3915: ep_len:48 episode reward: total was 22.500000. running mean: -21.378852\n",
      "ep 3915: ep_len:2946 episode reward: total was -43.510000. running mean: -21.600163\n",
      "ep 3915: ep_len:624 episode reward: total was 26.010000. running mean: -21.124062\n",
      "ep 3915: ep_len:37 episode reward: total was 17.000000. running mean: -20.742821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3915: ep_len:100 episode reward: total was 45.500000. running mean: -20.080393\n",
      "ep 3915: ep_len:94 episode reward: total was 44.000000. running mean: -19.439589\n",
      "ep 3915: ep_len:976 episode reward: total was 13.390000. running mean: -19.111293\n",
      "ep 3915: ep_len:346 episode reward: total was 3.950000. running mean: -18.880680\n",
      "ep 3915: ep_len:631 episode reward: total was 7.150000. running mean: -18.620373\n",
      "ep 3915: ep_len:662 episode reward: total was 16.100000. running mean: -18.273170\n",
      "ep 3915: ep_len:756 episode reward: total was 10.950000. running mean: -17.980938\n",
      "ep 3915: ep_len:209 episode reward: total was 98.500000. running mean: -16.816129\n",
      "ep 3915: ep_len:38 episode reward: total was 17.500000. running mean: -16.472967\n",
      "ep 3915: ep_len:121 episode reward: total was 57.500000. running mean: -15.733238\n",
      "ep 3915: ep_len:738 episode reward: total was -19.050000. running mean: -15.766405\n",
      "ep 3915: ep_len:2845 episode reward: total was 3.710000. running mean: -15.571641\n",
      "epsilon:0.009992 episode_count: 58889. steps_count: 63497282.000000\n",
      "ep 3916: ep_len:1453 episode reward: total was 10.810000. running mean: -15.307825\n",
      "ep 3916: ep_len:1256 episode reward: total was -64.370000. running mean: -15.798446\n",
      "ep 3916: ep_len:2905 episode reward: total was -24.910000. running mean: -15.889562\n",
      "ep 3916: ep_len:560 episode reward: total was -4.150000. running mean: -15.772166\n",
      "ep 3916: ep_len:1059 episode reward: total was -6.170000. running mean: -15.676145\n",
      "ep 3916: ep_len:320 episode reward: total was -10.820000. running mean: -15.627583\n",
      "ep 3916: ep_len:609 episode reward: total was 14.520000. running mean: -15.326107\n",
      "ep 3916: ep_len:582 episode reward: total was -2.550000. running mean: -15.198346\n",
      "ep 3916: ep_len:579 episode reward: total was -6.500000. running mean: -15.111363\n",
      "ep 3916: ep_len:25 episode reward: total was 11.000000. running mean: -14.850249\n",
      "ep 3916: ep_len:855 episode reward: total was 29.100000. running mean: -14.410747\n",
      "ep 3916: ep_len:2826 episode reward: total was -13.930000. running mean: -14.405939\n",
      "epsilon:0.009992 episode_count: 58901. steps_count: 63510311.000000\n",
      "ep 3917: ep_len:614 episode reward: total was -21.300000. running mean: -14.474880\n",
      "ep 3917: ep_len:770 episode reward: total was -15.180000. running mean: -14.481931\n",
      "ep 3917: ep_len:2966 episode reward: total was -21.180000. running mean: -14.548912\n",
      "ep 3917: ep_len:634 episode reward: total was -5.950000. running mean: -14.462923\n",
      "ep 3917: ep_len:635 episode reward: total was 5.190000. running mean: -14.266393\n",
      "ep 3917: ep_len:635 episode reward: total was 25.380000. running mean: -13.869930\n",
      "ep 3917: ep_len:1601 episode reward: total was -80.670000. running mean: -14.537930\n",
      "ep 3917: ep_len:729 episode reward: total was 25.750000. running mean: -14.135051\n",
      "ep 3917: ep_len:500 episode reward: total was -1.070000. running mean: -14.004400\n",
      "ep 3917: ep_len:82 episode reward: total was 39.500000. running mean: -13.469356\n",
      "ep 3917: ep_len:44 episode reward: total was 20.500000. running mean: -13.129663\n",
      "ep 3917: ep_len:93 episode reward: total was 43.500000. running mean: -12.563366\n",
      "ep 3917: ep_len:713 episode reward: total was -82.930000. running mean: -13.267033\n",
      "ep 3917: ep_len:45 episode reward: total was 21.000000. running mean: -12.924362\n",
      "epsilon:0.009992 episode_count: 58915. steps_count: 63520372.000000\n",
      "ep 3918: ep_len:648 episode reward: total was -20.370000. running mean: -12.998819\n",
      "ep 3918: ep_len:742 episode reward: total was -35.400000. running mean: -13.222830\n",
      "ep 3918: ep_len:2908 episode reward: total was -44.390000. running mean: -13.534502\n",
      "ep 3918: ep_len:515 episode reward: total was 5.990000. running mean: -13.339257\n",
      "ep 3918: ep_len:46 episode reward: total was 21.500000. running mean: -12.990865\n",
      "ep 3918: ep_len:135 episode reward: total was 64.500000. running mean: -12.215956\n",
      "ep 3918: ep_len:500 episode reward: total was 19.230000. running mean: -11.901496\n",
      "ep 3918: ep_len:665 episode reward: total was 24.480000. running mean: -11.537681\n",
      "ep 3918: ep_len:1136 episode reward: total was 12.390000. running mean: -11.298405\n",
      "ep 3918: ep_len:7210 episode reward: total was 31.040000. running mean: -10.875021\n",
      "ep 3918: ep_len:1427 episode reward: total was -1.760000. running mean: -10.783870\n",
      "ep 3918: ep_len:121 episode reward: total was 59.000000. running mean: -10.086032\n",
      "ep 3918: ep_len:124 episode reward: total was 57.500000. running mean: -9.410171\n",
      "ep 3918: ep_len:634 episode reward: total was -26.150000. running mean: -9.577570\n",
      "ep 3918: ep_len:2862 episode reward: total was 2.810000. running mean: -9.453694\n",
      "ep 3918: ep_len:44 episode reward: total was 19.000000. running mean: -9.169157\n",
      "epsilon:0.009992 episode_count: 58931. steps_count: 63540089.000000\n",
      "ep 3919: ep_len:642 episode reward: total was -31.120000. running mean: -9.388665\n",
      "ep 3919: ep_len:624 episode reward: total was 17.450000. running mean: -9.120279\n",
      "ep 3919: ep_len:2986 episode reward: total was -9.080000. running mean: -9.119876\n",
      "ep 3919: ep_len:539 episode reward: total was 13.740000. running mean: -8.891277\n",
      "ep 3919: ep_len:99 episode reward: total was 46.500000. running mean: -8.337364\n",
      "ep 3919: ep_len:1112 episode reward: total was -15.220000. running mean: -8.406191\n",
      "ep 3919: ep_len:332 episode reward: total was 10.170000. running mean: -8.220429\n",
      "ep 3919: ep_len:1294 episode reward: total was -63.990000. running mean: -8.778125\n",
      "ep 3919: ep_len:7373 episode reward: total was 63.990000. running mean: -8.050443\n",
      "ep 3919: ep_len:594 episode reward: total was -14.430000. running mean: -8.114239\n",
      "ep 3919: ep_len:88 episode reward: total was 41.000000. running mean: -7.623096\n",
      "ep 3919: ep_len:55 episode reward: total was 26.000000. running mean: -7.286866\n",
      "ep 3919: ep_len:1438 episode reward: total was -2.970000. running mean: -7.243697\n",
      "ep 3919: ep_len:2748 episode reward: total was -34.450000. running mean: -7.515760\n",
      "ep 3919: ep_len:56 episode reward: total was 26.500000. running mean: -7.175602\n",
      "epsilon:0.009992 episode_count: 58946. steps_count: 63560069.000000\n",
      "ep 3920: ep_len:659 episode reward: total was -54.180000. running mean: -7.645646\n",
      "ep 3920: ep_len:199 episode reward: total was 1.550000. running mean: -7.553690\n",
      "ep 3920: ep_len:72 episode reward: total was 33.000000. running mean: -7.148153\n",
      "ep 3920: ep_len:2980 episode reward: total was 17.100000. running mean: -6.905671\n",
      "ep 3920: ep_len:1671 episode reward: total was -62.460000. running mean: -7.461215\n",
      "ep 3920: ep_len:61 episode reward: total was 29.000000. running mean: -7.096603\n",
      "ep 3920: ep_len:43 episode reward: total was 18.500000. running mean: -6.840637\n",
      "ep 3920: ep_len:887 episode reward: total was 13.570000. running mean: -6.636530\n",
      "ep 3920: ep_len:681 episode reward: total was 25.350000. running mean: -6.316665\n",
      "ep 3920: ep_len:625 episode reward: total was -47.480000. running mean: -6.728298\n",
      "ep 3920: ep_len:7370 episode reward: total was -47.520000. running mean: -7.136215\n",
      "ep 3920: ep_len:967 episode reward: total was 0.940000. running mean: -7.055453\n",
      "ep 3920: ep_len:137 episode reward: total was 61.000000. running mean: -6.374899\n",
      "ep 3920: ep_len:1092 episode reward: total was 16.840000. running mean: -6.142750\n",
      "ep 3920: ep_len:2811 episode reward: total was 10.100000. running mean: -5.980322\n",
      "ep 3920: ep_len:38 episode reward: total was 17.500000. running mean: -5.745519\n",
      "epsilon:0.009992 episode_count: 58962. steps_count: 63580362.000000\n",
      "ep 3921: ep_len:746 episode reward: total was -65.430000. running mean: -6.342364\n",
      "ep 3921: ep_len:500 episode reward: total was -7.440000. running mean: -6.353340\n",
      "ep 3921: ep_len:82 episode reward: total was 39.500000. running mean: -5.894807\n",
      "ep 3921: ep_len:98 episode reward: total was 46.000000. running mean: -5.375859\n",
      "ep 3921: ep_len:776 episode reward: total was -13.620000. running mean: -5.458300\n",
      "ep 3921: ep_len:171 episode reward: total was 79.500000. running mean: -4.608717\n",
      "ep 3921: ep_len:93 episode reward: total was 45.000000. running mean: -4.112630\n",
      "ep 3921: ep_len:3411 episode reward: total was -518.310000. running mean: -9.254603\n",
      "ep 3921: ep_len:500 episode reward: total was 20.070000. running mean: -8.961357\n",
      "ep 3921: ep_len:785 episode reward: total was 2.440000. running mean: -8.847344\n",
      "ep 3921: ep_len:7340 episode reward: total was 50.060000. running mean: -8.258270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3921: ep_len:913 episode reward: total was 9.730000. running mean: -8.078388\n",
      "ep 3921: ep_len:60 episode reward: total was 28.500000. running mean: -7.712604\n",
      "ep 3921: ep_len:793 episode reward: total was -83.530000. running mean: -8.470778\n",
      "ep 3921: ep_len:2929 episode reward: total was -29.000000. running mean: -8.676070\n",
      "epsilon:0.009992 episode_count: 58977. steps_count: 63599559.000000\n",
      "ep 3922: ep_len:672 episode reward: total was -12.640000. running mean: -8.715709\n",
      "ep 3922: ep_len:1565 episode reward: total was -10.560000. running mean: -8.734152\n",
      "ep 3922: ep_len:72 episode reward: total was 33.000000. running mean: -8.316811\n",
      "ep 3922: ep_len:2977 episode reward: total was -40.600000. running mean: -8.639643\n",
      "ep 3922: ep_len:715 episode reward: total was -2.150000. running mean: -8.574746\n",
      "ep 3922: ep_len:101 episode reward: total was 49.000000. running mean: -7.998999\n",
      "ep 3922: ep_len:1123 episode reward: total was -5.100000. running mean: -7.970009\n",
      "ep 3922: ep_len:3858 episode reward: total was -74.420000. running mean: -8.634509\n",
      "ep 3922: ep_len:1287 episode reward: total was -54.970000. running mean: -9.097864\n",
      "ep 3922: ep_len:7341 episode reward: total was 31.010000. running mean: -8.696785\n",
      "ep 3922: ep_len:1074 episode reward: total was 30.560000. running mean: -8.304217\n",
      "ep 3922: ep_len:78 episode reward: total was 36.000000. running mean: -7.861175\n",
      "ep 3922: ep_len:94 episode reward: total was 44.000000. running mean: -7.342563\n",
      "ep 3922: ep_len:759 episode reward: total was -69.340000. running mean: -7.962538\n",
      "ep 3922: ep_len:2860 episode reward: total was -3.790000. running mean: -7.920812\n",
      "epsilon:0.009992 episode_count: 58992. steps_count: 63624135.000000\n",
      "ep 3923: ep_len:1514 episode reward: total was 26.080000. running mean: -7.580804\n",
      "ep 3923: ep_len:730 episode reward: total was -74.910000. running mean: -8.254096\n",
      "ep 3923: ep_len:2945 episode reward: total was -57.090000. running mean: -8.742455\n",
      "ep 3923: ep_len:595 episode reward: total was -0.280000. running mean: -8.657830\n",
      "ep 3923: ep_len:765 episode reward: total was -18.290000. running mean: -8.754152\n",
      "ep 3923: ep_len:3616 episode reward: total was -53.520000. running mean: -9.201811\n",
      "ep 3923: ep_len:672 episode reward: total was 1.500000. running mean: -9.094793\n",
      "ep 3923: ep_len:851 episode reward: total was 56.580000. running mean: -8.438045\n",
      "ep 3923: ep_len:734 episode reward: total was 11.680000. running mean: -8.236864\n",
      "ep 3923: ep_len:85 episode reward: total was 41.000000. running mean: -7.744496\n",
      "ep 3923: ep_len:633 episode reward: total was -8.490000. running mean: -7.751951\n",
      "ep 3923: ep_len:2821 episode reward: total was -28.120000. running mean: -7.955631\n",
      "ep 3923: ep_len:36 episode reward: total was 16.500000. running mean: -7.711075\n",
      "epsilon:0.009992 episode_count: 59005. steps_count: 63640132.000000\n",
      "ep 3924: ep_len:1177 episode reward: total was -22.040000. running mean: -7.854364\n",
      "ep 3924: ep_len:500 episode reward: total was 0.090000. running mean: -7.774920\n",
      "ep 3924: ep_len:2983 episode reward: total was -16.690000. running mean: -7.864071\n",
      "ep 3924: ep_len:500 episode reward: total was 18.800000. running mean: -7.597430\n",
      "ep 3924: ep_len:173 episode reward: total was 80.500000. running mean: -6.716456\n",
      "ep 3924: ep_len:36 episode reward: total was 16.500000. running mean: -6.484292\n",
      "ep 3924: ep_len:963 episode reward: total was -23.530000. running mean: -6.654749\n",
      "ep 3924: ep_len:660 episode reward: total was 31.690000. running mean: -6.271301\n",
      "ep 3924: ep_len:1164 episode reward: total was -10.290000. running mean: -6.311488\n",
      "ep 3924: ep_len:644 episode reward: total was 2.260000. running mean: -6.225773\n",
      "ep 3924: ep_len:524 episode reward: total was 13.840000. running mean: -6.025116\n",
      "ep 3924: ep_len:74 episode reward: total was 35.500000. running mean: -5.609864\n",
      "ep 3924: ep_len:748 episode reward: total was -37.500000. running mean: -5.928766\n",
      "ep 3924: ep_len:2908 episode reward: total was 1.770000. running mean: -5.851778\n",
      "ep 3924: ep_len:63 episode reward: total was 30.000000. running mean: -5.493260\n",
      "epsilon:0.009992 episode_count: 59020. steps_count: 63653249.000000\n",
      "ep 3925: ep_len:1130 episode reward: total was 1.030000. running mean: -5.428028\n",
      "ep 3925: ep_len:790 episode reward: total was 9.730000. running mean: -5.276447\n",
      "ep 3925: ep_len:3004 episode reward: total was 4.600000. running mean: -5.177683\n",
      "ep 3925: ep_len:523 episode reward: total was -11.100000. running mean: -5.236906\n",
      "ep 3925: ep_len:76 episode reward: total was 36.500000. running mean: -4.819537\n",
      "ep 3925: ep_len:500 episode reward: total was 36.910000. running mean: -4.402242\n",
      "ep 3925: ep_len:664 episode reward: total was 24.660000. running mean: -4.111619\n",
      "ep 3925: ep_len:536 episode reward: total was -0.870000. running mean: -4.079203\n",
      "ep 3925: ep_len:903 episode reward: total was -219.760000. running mean: -6.236011\n",
      "ep 3925: ep_len:783 episode reward: total was 1.600000. running mean: -6.157651\n",
      "ep 3925: ep_len:151 episode reward: total was 70.510000. running mean: -5.390974\n",
      "ep 3925: ep_len:61 episode reward: total was 27.500000. running mean: -5.062065\n",
      "ep 3925: ep_len:81 episode reward: total was 36.000000. running mean: -4.651444\n",
      "ep 3925: ep_len:635 episode reward: total was -9.980000. running mean: -4.704730\n",
      "ep 3925: ep_len:2837 episode reward: total was -30.290000. running mean: -4.960582\n",
      "ep 3925: ep_len:41 episode reward: total was 17.500000. running mean: -4.735976\n",
      "epsilon:0.009992 episode_count: 59036. steps_count: 63665964.000000\n",
      "ep 3926: ep_len:959 episode reward: total was -42.800000. running mean: -5.116617\n",
      "ep 3926: ep_len:1629 episode reward: total was -22.050000. running mean: -5.285951\n",
      "ep 3926: ep_len:3025 episode reward: total was -95.390000. running mean: -6.186991\n",
      "ep 3926: ep_len:603 episode reward: total was 6.860000. running mean: -6.056521\n",
      "ep 3926: ep_len:51 episode reward: total was 24.000000. running mean: -5.755956\n",
      "ep 3926: ep_len:1096 episode reward: total was -3.350000. running mean: -5.731896\n",
      "ep 3926: ep_len:665 episode reward: total was 25.190000. running mean: -5.422677\n",
      "ep 3926: ep_len:533 episode reward: total was -0.900000. running mean: -5.377451\n",
      "ep 3926: ep_len:731 episode reward: total was 3.070000. running mean: -5.292976\n",
      "ep 3926: ep_len:500 episode reward: total was 44.950000. running mean: -4.790546\n",
      "ep 3926: ep_len:160 episode reward: total was 75.500000. running mean: -3.987641\n",
      "ep 3926: ep_len:55 episode reward: total was 23.000000. running mean: -3.717764\n",
      "ep 3926: ep_len:1521 episode reward: total was 10.970000. running mean: -3.570887\n",
      "ep 3926: ep_len:2858 episode reward: total was -5.680000. running mean: -3.591978\n",
      "epsilon:0.009992 episode_count: 59050. steps_count: 63680350.000000\n",
      "ep 3927: ep_len:740 episode reward: total was -23.640000. running mean: -3.792458\n",
      "ep 3927: ep_len:771 episode reward: total was -31.820000. running mean: -4.072734\n",
      "ep 3927: ep_len:3035 episode reward: total was -38.490000. running mean: -4.416906\n",
      "ep 3927: ep_len:577 episode reward: total was -18.640000. running mean: -4.559137\n",
      "ep 3927: ep_len:701 episode reward: total was -19.880000. running mean: -4.712346\n",
      "ep 3927: ep_len:3992 episode reward: total was -2007.120000. running mean: -24.736422\n",
      "ep 3927: ep_len:1501 episode reward: total was -13.020000. running mean: -24.619258\n",
      "ep 3927: ep_len:7326 episode reward: total was 63.140000. running mean: -23.741666\n",
      "ep 3927: ep_len:743 episode reward: total was -15.970000. running mean: -23.663949\n",
      "ep 3927: ep_len:194 episode reward: total was 95.500000. running mean: -22.472309\n",
      "ep 3927: ep_len:75 episode reward: total was 36.000000. running mean: -21.887586\n",
      "ep 3927: ep_len:1474 episode reward: total was -55.060000. running mean: -22.219310\n",
      "ep 3927: ep_len:2870 episode reward: total was -10.730000. running mean: -22.104417\n",
      "ep 3927: ep_len:33 episode reward: total was 15.000000. running mean: -21.733373\n",
      "epsilon:0.009992 episode_count: 59064. steps_count: 63704382.000000\n",
      "ep 3928: ep_len:1154 episode reward: total was -17.460000. running mean: -21.690639\n",
      "ep 3928: ep_len:959 episode reward: total was 27.330000. running mean: -21.200433\n",
      "ep 3928: ep_len:2911 episode reward: total was -19.040000. running mean: -21.178829\n",
      "ep 3928: ep_len:1221 episode reward: total was -18.260000. running mean: -21.149640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3928: ep_len:148 episode reward: total was 72.500000. running mean: -20.213144\n",
      "ep 3928: ep_len:73 episode reward: total was 35.000000. running mean: -19.661013\n",
      "ep 3928: ep_len:885 episode reward: total was 13.150000. running mean: -19.332902\n",
      "ep 3928: ep_len:629 episode reward: total was 23.330000. running mean: -18.906273\n",
      "ep 3928: ep_len:1507 episode reward: total was -23.800000. running mean: -18.955211\n",
      "ep 3928: ep_len:678 episode reward: total was 14.240000. running mean: -18.623259\n",
      "ep 3928: ep_len:1004 episode reward: total was 16.700000. running mean: -18.270026\n",
      "ep 3928: ep_len:1513 episode reward: total was -14.640000. running mean: -18.233726\n",
      "ep 3928: ep_len:2702 episode reward: total was 0.070000. running mean: -18.050689\n",
      "epsilon:0.009992 episode_count: 59077. steps_count: 63719766.000000\n",
      "ep 3929: ep_len:586 episode reward: total was -14.510000. running mean: -18.015282\n",
      "ep 3929: ep_len:734 episode reward: total was 12.600000. running mean: -17.709129\n",
      "ep 3929: ep_len:3033 episode reward: total was -34.260000. running mean: -17.874638\n",
      "ep 3929: ep_len:602 episode reward: total was 3.830000. running mean: -17.657591\n",
      "ep 3929: ep_len:90 episode reward: total was 43.500000. running mean: -17.046015\n",
      "ep 3929: ep_len:56 episode reward: total was 26.500000. running mean: -16.610555\n",
      "ep 3929: ep_len:686 episode reward: total was -3.410000. running mean: -16.478550\n",
      "ep 3929: ep_len:307 episode reward: total was 13.750000. running mean: -16.176264\n",
      "ep 3929: ep_len:883 episode reward: total was -16.590000. running mean: -16.180401\n",
      "ep 3929: ep_len:7304 episode reward: total was 24.260000. running mean: -15.775997\n",
      "ep 3929: ep_len:617 episode reward: total was -9.340000. running mean: -15.711637\n",
      "ep 3929: ep_len:93 episode reward: total was 43.500000. running mean: -15.119521\n",
      "ep 3929: ep_len:51 episode reward: total was 24.000000. running mean: -14.728326\n",
      "ep 3929: ep_len:776 episode reward: total was -33.820000. running mean: -14.919243\n",
      "ep 3929: ep_len:2899 episode reward: total was 12.820000. running mean: -14.641850\n",
      "ep 3929: ep_len:52 episode reward: total was 23.000000. running mean: -14.265432\n",
      "epsilon:0.009992 episode_count: 59093. steps_count: 63738535.000000\n",
      "ep 3930: ep_len:1498 episode reward: total was 9.920000. running mean: -14.023577\n",
      "ep 3930: ep_len:962 episode reward: total was 4.830000. running mean: -13.835042\n",
      "ep 3930: ep_len:2882 episode reward: total was -31.090000. running mean: -14.007591\n",
      "ep 3930: ep_len:683 episode reward: total was -8.010000. running mean: -13.947615\n",
      "ep 3930: ep_len:93 episode reward: total was 43.500000. running mean: -13.373139\n",
      "ep 3930: ep_len:40 episode reward: total was 18.500000. running mean: -13.054408\n",
      "ep 3930: ep_len:1449 episode reward: total was -152.330000. running mean: -14.447164\n",
      "ep 3930: ep_len:656 episode reward: total was 23.600000. running mean: -14.066692\n",
      "ep 3930: ep_len:753 episode reward: total was -33.140000. running mean: -14.257425\n",
      "ep 3930: ep_len:822 episode reward: total was 32.030000. running mean: -13.794551\n",
      "ep 3930: ep_len:688 episode reward: total was -5.420000. running mean: -13.710805\n",
      "ep 3930: ep_len:53 episode reward: total was 25.000000. running mean: -13.323697\n",
      "ep 3930: ep_len:1527 episode reward: total was -2.130000. running mean: -13.211760\n",
      "ep 3930: ep_len:2800 episode reward: total was -1.950000. running mean: -13.099143\n",
      "epsilon:0.009992 episode_count: 59107. steps_count: 63753441.000000\n",
      "ep 3931: ep_len:1142 episode reward: total was 8.630000. running mean: -12.881851\n",
      "ep 3931: ep_len:500 episode reward: total was 27.190000. running mean: -12.481133\n",
      "ep 3931: ep_len:3019 episode reward: total was -9.240000. running mean: -12.448721\n",
      "ep 3931: ep_len:534 episode reward: total was -43.310000. running mean: -12.757334\n",
      "ep 3931: ep_len:41 episode reward: total was 19.000000. running mean: -12.439761\n",
      "ep 3931: ep_len:707 episode reward: total was -10.760000. running mean: -12.422963\n",
      "ep 3931: ep_len:309 episode reward: total was 17.470000. running mean: -12.124034\n",
      "ep 3931: ep_len:1607 episode reward: total was -38.900000. running mean: -12.391793\n",
      "ep 3931: ep_len:665 episode reward: total was 6.390000. running mean: -12.203975\n",
      "ep 3931: ep_len:531 episode reward: total was 15.770000. running mean: -11.924236\n",
      "ep 3931: ep_len:79 episode reward: total was 36.500000. running mean: -11.439993\n",
      "ep 3931: ep_len:1476 episode reward: total was 8.530000. running mean: -11.240293\n",
      "ep 3931: ep_len:47 episode reward: total was 20.500000. running mean: -10.922890\n",
      "ep 3931: ep_len:27 episode reward: total was 12.000000. running mean: -10.693661\n",
      "epsilon:0.009992 episode_count: 59121. steps_count: 63764125.000000\n",
      "ep 3932: ep_len:1025 episode reward: total was -101.020000. running mean: -11.596925\n",
      "ep 3932: ep_len:1212 episode reward: total was -121.480000. running mean: -12.695756\n",
      "ep 3932: ep_len:2985 episode reward: total was -25.770000. running mean: -12.826498\n",
      "ep 3932: ep_len:524 episode reward: total was -17.120000. running mean: -12.869433\n",
      "ep 3932: ep_len:35 episode reward: total was 16.000000. running mean: -12.580739\n",
      "ep 3932: ep_len:139 episode reward: total was 68.000000. running mean: -11.774931\n",
      "ep 3932: ep_len:3303 episode reward: total was -467.330000. running mean: -16.330482\n",
      "ep 3932: ep_len:500 episode reward: total was 24.630000. running mean: -15.920877\n",
      "ep 3932: ep_len:1217 episode reward: total was -20.320000. running mean: -15.964868\n",
      "ep 3932: ep_len:697 episode reward: total was 45.940000. running mean: -15.345820\n",
      "ep 3932: ep_len:658 episode reward: total was 7.810000. running mean: -15.114262\n",
      "ep 3932: ep_len:35 episode reward: total was 16.000000. running mean: -14.803119\n",
      "ep 3932: ep_len:851 episode reward: total was 15.310000. running mean: -14.501988\n",
      "ep 3932: ep_len:2827 episode reward: total was 2.850000. running mean: -14.328468\n",
      "epsilon:0.009992 episode_count: 59135. steps_count: 63780133.000000\n",
      "ep 3933: ep_len:1099 episode reward: total was 27.180000. running mean: -13.913383\n",
      "ep 3933: ep_len:1707 episode reward: total was -22.490000. running mean: -13.999149\n",
      "ep 3933: ep_len:48 episode reward: total was 22.500000. running mean: -13.634158\n",
      "ep 3933: ep_len:94 episode reward: total was 45.500000. running mean: -13.042816\n",
      "ep 3933: ep_len:605 episode reward: total was -21.240000. running mean: -13.124788\n",
      "ep 3933: ep_len:82 episode reward: total was 39.500000. running mean: -12.598540\n",
      "ep 3933: ep_len:34 episode reward: total was 15.500000. running mean: -12.317555\n",
      "ep 3933: ep_len:995 episode reward: total was 4.180000. running mean: -12.152579\n",
      "ep 3933: ep_len:334 episode reward: total was -81.500000. running mean: -12.846053\n",
      "ep 3933: ep_len:1193 episode reward: total was -12.480000. running mean: -12.842393\n",
      "ep 3933: ep_len:746 episode reward: total was 47.720000. running mean: -12.236769\n",
      "ep 3933: ep_len:1485 episode reward: total was -0.230000. running mean: -12.116701\n",
      "ep 3933: ep_len:180 episode reward: total was 85.500000. running mean: -11.140534\n",
      "ep 3933: ep_len:1191 episode reward: total was -7.910000. running mean: -11.108229\n",
      "ep 3933: ep_len:2674 episode reward: total was -12.820000. running mean: -11.125347\n",
      "ep 3933: ep_len:68 episode reward: total was 32.500000. running mean: -10.689093\n",
      "epsilon:0.009992 episode_count: 59151. steps_count: 63792668.000000\n",
      "ep 3934: ep_len:1501 episode reward: total was 4.370000. running mean: -10.538502\n",
      "ep 3934: ep_len:1434 episode reward: total was -142.870000. running mean: -11.861817\n",
      "ep 3934: ep_len:60 episode reward: total was 25.500000. running mean: -11.488199\n",
      "ep 3934: ep_len:3029 episode reward: total was -23.030000. running mean: -11.603617\n",
      "ep 3934: ep_len:780 episode reward: total was -15.270000. running mean: -11.640281\n",
      "ep 3934: ep_len:39 episode reward: total was 16.500000. running mean: -11.358878\n",
      "ep 3934: ep_len:590 episode reward: total was 2.700000. running mean: -11.218289\n",
      "ep 3934: ep_len:661 episode reward: total was 7.580000. running mean: -11.030306\n",
      "ep 3934: ep_len:521 episode reward: total was 2.530000. running mean: -10.894703\n",
      "ep 3934: ep_len:840 episode reward: total was 43.300000. running mean: -10.352756\n",
      "ep 3934: ep_len:668 episode reward: total was -16.300000. running mean: -10.412229\n",
      "ep 3934: ep_len:1429 episode reward: total was 12.040000. running mean: -10.187707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3934: ep_len:2824 episode reward: total was 15.530000. running mean: -9.930529\n",
      "epsilon:0.009992 episode_count: 59164. steps_count: 63807044.000000\n",
      "ep 3935: ep_len:1450 episode reward: total was 15.520000. running mean: -9.676024\n",
      "ep 3935: ep_len:661 episode reward: total was -32.950000. running mean: -9.908764\n",
      "ep 3935: ep_len:2939 episode reward: total was 1.520000. running mean: -9.794476\n",
      "ep 3935: ep_len:542 episode reward: total was 3.450000. running mean: -9.662032\n",
      "ep 3935: ep_len:42 episode reward: total was 19.500000. running mean: -9.370411\n",
      "ep 3935: ep_len:676 episode reward: total was -0.480000. running mean: -9.281507\n",
      "ep 3935: ep_len:3566 episode reward: total was 0.150000. running mean: -9.187192\n",
      "ep 3935: ep_len:872 episode reward: total was -1.210000. running mean: -9.107420\n",
      "ep 3935: ep_len:821 episode reward: total was 30.860000. running mean: -8.707746\n",
      "ep 3935: ep_len:500 episode reward: total was 2.520000. running mean: -8.595468\n",
      "ep 3935: ep_len:49 episode reward: total was 20.000000. running mean: -8.309514\n",
      "ep 3935: ep_len:189 episode reward: total was 93.000000. running mean: -7.296419\n",
      "ep 3935: ep_len:91 episode reward: total was 44.000000. running mean: -6.783454\n",
      "ep 3935: ep_len:582 episode reward: total was -0.410000. running mean: -6.719720\n",
      "ep 3935: ep_len:2799 episode reward: total was 10.530000. running mean: -6.547223\n",
      "epsilon:0.009992 episode_count: 59179. steps_count: 63822823.000000\n",
      "ep 3936: ep_len:643 episode reward: total was -19.180000. running mean: -6.673550\n",
      "ep 3936: ep_len:766 episode reward: total was -29.930000. running mean: -6.906115\n",
      "ep 3936: ep_len:2907 episode reward: total was -40.970000. running mean: -7.246754\n",
      "ep 3936: ep_len:560 episode reward: total was 2.570000. running mean: -7.148586\n",
      "ep 3936: ep_len:83 episode reward: total was 35.500000. running mean: -6.722100\n",
      "ep 3936: ep_len:49 episode reward: total was 23.000000. running mean: -6.424879\n",
      "ep 3936: ep_len:1466 episode reward: total was -24.550000. running mean: -6.606131\n",
      "ep 3936: ep_len:3646 episode reward: total was -31.000000. running mean: -6.850069\n",
      "ep 3936: ep_len:1563 episode reward: total was -71.530000. running mean: -7.496869\n",
      "ep 3936: ep_len:891 episode reward: total was 56.950000. running mean: -6.852400\n",
      "ep 3936: ep_len:1424 episode reward: total was 0.110000. running mean: -6.782776\n",
      "ep 3936: ep_len:211 episode reward: total was 102.500000. running mean: -5.689948\n",
      "ep 3936: ep_len:2143 episode reward: total was -197.360000. running mean: -7.606649\n",
      "ep 3936: ep_len:2850 episode reward: total was 4.370000. running mean: -7.486882\n",
      "epsilon:0.009992 episode_count: 59193. steps_count: 63842025.000000\n",
      "ep 3937: ep_len:982 episode reward: total was -88.610000. running mean: -8.298113\n",
      "ep 3937: ep_len:772 episode reward: total was 2.220000. running mean: -8.192932\n",
      "ep 3937: ep_len:2972 episode reward: total was -60.670000. running mean: -8.717703\n",
      "ep 3937: ep_len:1227 episode reward: total was -20.220000. running mean: -8.832726\n",
      "ep 3937: ep_len:97 episode reward: total was 47.000000. running mean: -8.274399\n",
      "ep 3937: ep_len:957 episode reward: total was -3.090000. running mean: -8.222555\n",
      "ep 3937: ep_len:659 episode reward: total was -0.400000. running mean: -8.144329\n",
      "ep 3937: ep_len:1278 episode reward: total was -45.020000. running mean: -8.513086\n",
      "ep 3937: ep_len:744 episode reward: total was 10.550000. running mean: -8.322455\n",
      "ep 3937: ep_len:1175 episode reward: total was -21.750000. running mean: -8.456730\n",
      "ep 3937: ep_len:56 episode reward: total was 25.000000. running mean: -8.122163\n",
      "ep 3937: ep_len:87 episode reward: total was 42.000000. running mean: -7.620941\n",
      "ep 3937: ep_len:893 episode reward: total was -0.210000. running mean: -7.546832\n",
      "ep 3937: ep_len:2762 episode reward: total was 10.070000. running mean: -7.370664\n",
      "epsilon:0.009992 episode_count: 59207. steps_count: 63856686.000000\n",
      "ep 3938: ep_len:673 episode reward: total was -0.570000. running mean: -7.302657\n",
      "ep 3938: ep_len:1659 episode reward: total was -16.420000. running mean: -7.393831\n",
      "ep 3938: ep_len:2821 episode reward: total was -29.960000. running mean: -7.619492\n",
      "ep 3938: ep_len:531 episode reward: total was -9.000000. running mean: -7.633297\n",
      "ep 3938: ep_len:41 episode reward: total was 19.000000. running mean: -7.366964\n",
      "ep 3938: ep_len:838 episode reward: total was 8.390000. running mean: -7.209395\n",
      "ep 3938: ep_len:317 episode reward: total was 7.880000. running mean: -7.058501\n",
      "ep 3938: ep_len:588 episode reward: total was 0.610000. running mean: -6.981816\n",
      "ep 3938: ep_len:693 episode reward: total was 46.940000. running mean: -6.442598\n",
      "ep 3938: ep_len:733 episode reward: total was -0.920000. running mean: -6.387372\n",
      "ep 3938: ep_len:74 episode reward: total was 32.500000. running mean: -5.998498\n",
      "ep 3938: ep_len:171 episode reward: total was 82.500000. running mean: -5.113513\n",
      "ep 3938: ep_len:67 episode reward: total was 30.500000. running mean: -4.757378\n",
      "ep 3938: ep_len:60 episode reward: total was 27.000000. running mean: -4.439804\n",
      "ep 3938: ep_len:644 episode reward: total was -26.050000. running mean: -4.655906\n",
      "ep 3938: ep_len:2800 episode reward: total was -20.500000. running mean: -4.814347\n",
      "ep 3938: ep_len:56 episode reward: total was 26.500000. running mean: -4.501203\n",
      "epsilon:0.009992 episode_count: 59224. steps_count: 63869452.000000\n",
      "ep 3939: ep_len:916 episode reward: total was 33.110000. running mean: -4.125091\n",
      "ep 3939: ep_len:500 episode reward: total was 32.210000. running mean: -3.761740\n",
      "ep 3939: ep_len:58 episode reward: total was 24.500000. running mean: -3.479123\n",
      "ep 3939: ep_len:2987 episode reward: total was 26.900000. running mean: -3.175332\n",
      "ep 3939: ep_len:1139 episode reward: total was 2.130000. running mean: -3.122279\n",
      "ep 3939: ep_len:61 episode reward: total was 29.000000. running mean: -2.801056\n",
      "ep 3939: ep_len:74 episode reward: total was 34.000000. running mean: -2.433045\n",
      "ep 3939: ep_len:1142 episode reward: total was -0.870000. running mean: -2.417415\n",
      "ep 3939: ep_len:674 episode reward: total was 13.370000. running mean: -2.259541\n",
      "ep 3939: ep_len:787 episode reward: total was 23.000000. running mean: -2.006945\n",
      "ep 3939: ep_len:882 episode reward: total was 58.730000. running mean: -1.399576\n",
      "ep 3939: ep_len:519 episode reward: total was 24.410000. running mean: -1.141480\n",
      "ep 3939: ep_len:65 episode reward: total was 31.000000. running mean: -0.820065\n",
      "ep 3939: ep_len:1481 episode reward: total was 15.190000. running mean: -0.659965\n",
      "ep 3939: ep_len:2908 episode reward: total was 8.870000. running mean: -0.564665\n",
      "ep 3939: ep_len:73 episode reward: total was 35.000000. running mean: -0.209018\n",
      "epsilon:0.009992 episode_count: 59240. steps_count: 63883718.000000\n",
      "ep 3940: ep_len:672 episode reward: total was -1.730000. running mean: -0.224228\n",
      "ep 3940: ep_len:928 episode reward: total was 10.860000. running mean: -0.113386\n",
      "ep 3940: ep_len:68 episode reward: total was 32.500000. running mean: 0.212748\n",
      "ep 3940: ep_len:3074 episode reward: total was -15.080000. running mean: 0.059821\n",
      "ep 3940: ep_len:1209 episode reward: total was -305.710000. running mean: -2.997878\n",
      "ep 3940: ep_len:26 episode reward: total was 11.500000. running mean: -2.852899\n",
      "ep 3940: ep_len:73 episode reward: total was 33.500000. running mean: -2.489370\n",
      "ep 3940: ep_len:44 episode reward: total was 20.500000. running mean: -2.259476\n",
      "ep 3940: ep_len:874 episode reward: total was 45.430000. running mean: -1.782581\n",
      "ep 3940: ep_len:3601 episode reward: total was -141.540000. running mean: -3.180156\n",
      "ep 3940: ep_len:951 episode reward: total was -52.540000. running mean: -3.673754\n",
      "ep 3940: ep_len:714 episode reward: total was 30.190000. running mean: -3.335116\n",
      "ep 3940: ep_len:629 episode reward: total was 0.060000. running mean: -3.301165\n",
      "ep 3940: ep_len:67 episode reward: total was 27.500000. running mean: -2.993154\n",
      "ep 3940: ep_len:102 episode reward: total was 48.000000. running mean: -2.483222\n",
      "ep 3940: ep_len:513 episode reward: total was 4.360000. running mean: -2.414790\n",
      "ep 3940: ep_len:2779 episode reward: total was -18.750000. running mean: -2.578142\n",
      "ep 3940: ep_len:57 episode reward: total was 27.000000. running mean: -2.282361\n",
      "epsilon:0.009992 episode_count: 59258. steps_count: 63900099.000000\n",
      "ep 3941: ep_len:500 episode reward: total was 3.000000. running mean: -2.229537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3941: ep_len:660 episode reward: total was -19.830000. running mean: -2.405542\n",
      "ep 3941: ep_len:2940 episode reward: total was -3.210000. running mean: -2.413586\n",
      "ep 3941: ep_len:1061 episode reward: total was -18.850000. running mean: -2.577950\n",
      "ep 3941: ep_len:104 episode reward: total was 50.500000. running mean: -2.047171\n",
      "ep 3941: ep_len:105 episode reward: total was 51.000000. running mean: -1.516699\n",
      "ep 3941: ep_len:659 episode reward: total was -33.440000. running mean: -1.835932\n",
      "ep 3941: ep_len:3906 episode reward: total was 27.540000. running mean: -1.542173\n",
      "ep 3941: ep_len:944 episode reward: total was -12.090000. running mean: -1.647651\n",
      "ep 3941: ep_len:885 episode reward: total was 53.310000. running mean: -1.098075\n",
      "ep 3941: ep_len:500 episode reward: total was 23.950000. running mean: -0.847594\n",
      "ep 3941: ep_len:47 episode reward: total was 20.500000. running mean: -0.634118\n",
      "ep 3941: ep_len:222 episode reward: total was 109.500000. running mean: 0.467223\n",
      "ep 3941: ep_len:790 episode reward: total was -13.810000. running mean: 0.324451\n",
      "ep 3941: ep_len:2686 episode reward: total was 1.780000. running mean: 0.339007\n",
      "epsilon:0.009992 episode_count: 59273. steps_count: 63916108.000000\n",
      "ep 3942: ep_len:614 episode reward: total was 20.660000. running mean: 0.542217\n",
      "ep 3942: ep_len:611 episode reward: total was 16.220000. running mean: 0.698994\n",
      "ep 3942: ep_len:3007 episode reward: total was -7.660000. running mean: 0.615404\n",
      "ep 3942: ep_len:1110 episode reward: total was -6.240000. running mean: 0.546850\n",
      "ep 3942: ep_len:43 episode reward: total was 18.500000. running mean: 0.726382\n",
      "ep 3942: ep_len:132 episode reward: total was 61.500000. running mean: 1.334118\n",
      "ep 3942: ep_len:70 episode reward: total was 33.500000. running mean: 1.655777\n",
      "ep 3942: ep_len:54 episode reward: total was 24.000000. running mean: 1.879219\n",
      "ep 3942: ep_len:594 episode reward: total was 58.980000. running mean: 2.450227\n",
      "ep 3942: ep_len:3780 episode reward: total was 1.790000. running mean: 2.443625\n",
      "ep 3942: ep_len:782 episode reward: total was -62.250000. running mean: 1.796688\n",
      "ep 3942: ep_len:873 episode reward: total was 58.700000. running mean: 2.365721\n",
      "ep 3942: ep_len:888 episode reward: total was 53.840000. running mean: 2.880464\n",
      "ep 3942: ep_len:78 episode reward: total was 36.000000. running mean: 3.211660\n",
      "ep 3942: ep_len:43 episode reward: total was 20.000000. running mean: 3.379543\n",
      "ep 3942: ep_len:111 episode reward: total was 52.500000. running mean: 3.870748\n",
      "ep 3942: ep_len:588 episode reward: total was -10.450000. running mean: 3.727540\n",
      "ep 3942: ep_len:2875 episode reward: total was -1.600000. running mean: 3.674265\n",
      "ep 3942: ep_len:47 episode reward: total was 22.000000. running mean: 3.857522\n",
      "epsilon:0.009992 episode_count: 59292. steps_count: 63932408.000000\n",
      "ep 3943: ep_len:649 episode reward: total was 0.260000. running mean: 3.821547\n",
      "ep 3943: ep_len:788 episode reward: total was 19.080000. running mean: 3.974131\n",
      "ep 3943: ep_len:3078 episode reward: total was 34.490000. running mean: 4.279290\n",
      "ep 3943: ep_len:811 episode reward: total was 5.640000. running mean: 4.292897\n",
      "ep 3943: ep_len:65 episode reward: total was 31.000000. running mean: 4.559968\n",
      "ep 3943: ep_len:68 episode reward: total was 32.500000. running mean: 4.839369\n",
      "ep 3943: ep_len:617 episode reward: total was 5.050000. running mean: 4.841475\n",
      "ep 3943: ep_len:325 episode reward: total was 4.070000. running mean: 4.833760\n",
      "ep 3943: ep_len:1273 episode reward: total was -33.650000. running mean: 4.448922\n",
      "ep 3943: ep_len:740 episode reward: total was -91.010000. running mean: 3.494333\n",
      "ep 3943: ep_len:655 episode reward: total was -0.690000. running mean: 3.452490\n",
      "ep 3943: ep_len:108 episode reward: total was 52.500000. running mean: 3.942965\n",
      "ep 3943: ep_len:73 episode reward: total was 35.000000. running mean: 4.253535\n",
      "ep 3943: ep_len:725 episode reward: total was -33.810000. running mean: 3.872900\n",
      "ep 3943: ep_len:2923 episode reward: total was 2.250000. running mean: 3.856671\n",
      "epsilon:0.009992 episode_count: 59307. steps_count: 63945306.000000\n",
      "ep 3944: ep_len:677 episode reward: total was -5.520000. running mean: 3.762904\n",
      "ep 3944: ep_len:740 episode reward: total was -32.390000. running mean: 3.401375\n",
      "ep 3944: ep_len:44 episode reward: total was 20.500000. running mean: 3.572362\n",
      "ep 3944: ep_len:2954 episode reward: total was 2.850000. running mean: 3.565138\n",
      "ep 3944: ep_len:804 episode reward: total was -17.870000. running mean: 3.350787\n",
      "ep 3944: ep_len:55 episode reward: total was 26.000000. running mean: 3.577279\n",
      "ep 3944: ep_len:921 episode reward: total was 52.950000. running mean: 4.071006\n",
      "ep 3944: ep_len:642 episode reward: total was 14.580000. running mean: 4.176096\n",
      "ep 3944: ep_len:627 episode reward: total was 0.040000. running mean: 4.134735\n",
      "ep 3944: ep_len:674 episode reward: total was 30.430000. running mean: 4.397688\n",
      "ep 3944: ep_len:676 episode reward: total was 11.760000. running mean: 4.471311\n",
      "ep 3944: ep_len:65 episode reward: total was 31.000000. running mean: 4.736598\n",
      "ep 3944: ep_len:592 episode reward: total was -0.610000. running mean: 4.683132\n",
      "ep 3944: ep_len:2858 episode reward: total was 2.460000. running mean: 4.660900\n",
      "epsilon:0.009992 episode_count: 59321. steps_count: 63957635.000000\n",
      "ep 3945: ep_len:1109 episode reward: total was -7.660000. running mean: 4.537691\n",
      "ep 3945: ep_len:935 episode reward: total was 8.480000. running mean: 4.577114\n",
      "ep 3945: ep_len:3010 episode reward: total was -17.800000. running mean: 4.353343\n",
      "ep 3945: ep_len:679 episode reward: total was -14.970000. running mean: 4.160110\n",
      "ep 3945: ep_len:120 episode reward: total was 57.000000. running mean: 4.688509\n",
      "ep 3945: ep_len:942 episode reward: total was 83.560000. running mean: 5.477224\n",
      "ep 3945: ep_len:3687 episode reward: total was -9.380000. running mean: 5.328651\n",
      "ep 3945: ep_len:1294 episode reward: total was -44.800000. running mean: 4.827365\n",
      "ep 3945: ep_len:7290 episode reward: total was 60.700000. running mean: 5.386091\n",
      "ep 3945: ep_len:838 episode reward: total was 38.300000. running mean: 5.715230\n",
      "ep 3945: ep_len:176 episode reward: total was 86.010000. running mean: 6.518178\n",
      "ep 3945: ep_len:47 episode reward: total was 22.000000. running mean: 6.672996\n",
      "ep 3945: ep_len:80 episode reward: total was 35.500000. running mean: 6.961266\n",
      "ep 3945: ep_len:1009 episode reward: total was -29.750000. running mean: 6.594154\n",
      "ep 3945: ep_len:2838 episode reward: total was -8.580000. running mean: 6.442412\n",
      "epsilon:0.009992 episode_count: 59336. steps_count: 63981689.000000\n",
      "ep 3946: ep_len:1148 episode reward: total was 3.790000. running mean: 6.415888\n",
      "ep 3946: ep_len:980 episode reward: total was 2.710000. running mean: 6.378829\n",
      "ep 3946: ep_len:2912 episode reward: total was -25.130000. running mean: 6.063741\n",
      "ep 3946: ep_len:572 episode reward: total was -10.550000. running mean: 5.897603\n",
      "ep 3946: ep_len:129 episode reward: total was 61.500000. running mean: 6.453627\n",
      "ep 3946: ep_len:60 episode reward: total was 28.500000. running mean: 6.674091\n",
      "ep 3946: ep_len:500 episode reward: total was 43.420000. running mean: 7.041550\n",
      "ep 3946: ep_len:351 episode reward: total was 20.250000. running mean: 7.173635\n",
      "ep 3946: ep_len:1301 episode reward: total was -40.690000. running mean: 6.694998\n",
      "ep 3946: ep_len:842 episode reward: total was 49.600000. running mean: 7.124048\n",
      "ep 3946: ep_len:1458 episode reward: total was 7.000000. running mean: 7.122808\n",
      "ep 3946: ep_len:43 episode reward: total was 20.000000. running mean: 7.251580\n",
      "ep 3946: ep_len:500 episode reward: total was 22.660000. running mean: 7.405664\n",
      "ep 3946: ep_len:2801 episode reward: total was -1.200000. running mean: 7.319607\n",
      "epsilon:0.009992 episode_count: 59350. steps_count: 63995286.000000\n",
      "ep 3947: ep_len:1490 episode reward: total was 21.650000. running mean: 7.462911\n",
      "ep 3947: ep_len:721 episode reward: total was -9.120000. running mean: 7.297082\n",
      "ep 3947: ep_len:2886 episode reward: total was -4.540000. running mean: 7.178711\n",
      "ep 3947: ep_len:1652 episode reward: total was -165.450000. running mean: 5.452424\n",
      "ep 3947: ep_len:140 episode reward: total was 67.000000. running mean: 6.067900\n",
      "ep 3947: ep_len:57 episode reward: total was 25.500000. running mean: 6.262221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3947: ep_len:1028 episode reward: total was -27.720000. running mean: 5.922399\n",
      "ep 3947: ep_len:3984 episode reward: total was -95.240000. running mean: 4.910775\n",
      "ep 3947: ep_len:1186 episode reward: total was -2.240000. running mean: 4.839267\n",
      "ep 3947: ep_len:661 episode reward: total was 4.610000. running mean: 4.836974\n",
      "ep 3947: ep_len:609 episode reward: total was 12.080000. running mean: 4.909405\n",
      "ep 3947: ep_len:105 episode reward: total was 49.500000. running mean: 5.355311\n",
      "ep 3947: ep_len:1200 episode reward: total was -8.370000. running mean: 5.218057\n",
      "ep 3947: ep_len:2781 episode reward: total was -3.120000. running mean: 5.134677\n",
      "ep 3947: ep_len:70 episode reward: total was 33.500000. running mean: 5.418330\n",
      "epsilon:0.009992 episode_count: 59365. steps_count: 64013856.000000\n",
      "ep 3948: ep_len:1058 episode reward: total was -4.740000. running mean: 5.316747\n",
      "ep 3948: ep_len:684 episode reward: total was -1.310000. running mean: 5.250479\n",
      "ep 3948: ep_len:47 episode reward: total was 20.500000. running mean: 5.402975\n",
      "ep 3948: ep_len:2964 episode reward: total was -44.720000. running mean: 4.901745\n",
      "ep 3948: ep_len:846 episode reward: total was 33.580000. running mean: 5.188527\n",
      "ep 3948: ep_len:51 episode reward: total was 24.000000. running mean: 5.376642\n",
      "ep 3948: ep_len:119 episode reward: total was 56.500000. running mean: 5.887876\n",
      "ep 3948: ep_len:35 episode reward: total was 14.500000. running mean: 5.973997\n",
      "ep 3948: ep_len:893 episode reward: total was 56.490000. running mean: 6.479157\n",
      "ep 3948: ep_len:3644 episode reward: total was -11.740000. running mean: 6.296965\n",
      "ep 3948: ep_len:749 episode reward: total was 15.700000. running mean: 6.390996\n",
      "ep 3948: ep_len:7471 episode reward: total was -141.770000. running mean: 4.909386\n",
      "ep 3948: ep_len:648 episode reward: total was 32.650000. running mean: 5.186792\n",
      "ep 3948: ep_len:186 episode reward: total was 92.510000. running mean: 6.060024\n",
      "ep 3948: ep_len:84 episode reward: total was 39.000000. running mean: 6.389424\n",
      "ep 3948: ep_len:1391 episode reward: total was 14.230000. running mean: 6.467829\n",
      "ep 3948: ep_len:2772 episode reward: total was -14.930000. running mean: 6.253851\n",
      "ep 3948: ep_len:38 episode reward: total was 17.500000. running mean: 6.366313\n",
      "epsilon:0.009992 episode_count: 59383. steps_count: 64037536.000000\n",
      "ep 3949: ep_len:3649 episode reward: total was -572.360000. running mean: 0.579050\n",
      "ep 3949: ep_len:993 episode reward: total was 28.530000. running mean: 0.858559\n",
      "ep 3949: ep_len:54 episode reward: total was 24.000000. running mean: 1.089973\n",
      "ep 3949: ep_len:2961 episode reward: total was 15.780000. running mean: 1.236874\n",
      "ep 3949: ep_len:699 episode reward: total was -17.420000. running mean: 1.050305\n",
      "ep 3949: ep_len:52 episode reward: total was 24.500000. running mean: 1.284802\n",
      "ep 3949: ep_len:1405 episode reward: total was -134.860000. running mean: -0.076646\n",
      "ep 3949: ep_len:338 episode reward: total was 18.740000. running mean: 0.111520\n",
      "ep 3949: ep_len:3954 episode reward: total was -499.840000. running mean: -4.887995\n",
      "ep 3949: ep_len:722 episode reward: total was 7.180000. running mean: -4.767315\n",
      "ep 3949: ep_len:592 episode reward: total was 55.220000. running mean: -4.167442\n",
      "ep 3949: ep_len:119 episode reward: total was 58.000000. running mean: -3.545767\n",
      "ep 3949: ep_len:37 episode reward: total was 17.000000. running mean: -3.340310\n",
      "ep 3949: ep_len:733 episode reward: total was -51.820000. running mean: -3.825107\n",
      "ep 3949: ep_len:2823 episode reward: total was 5.260000. running mean: -3.734255\n",
      "epsilon:0.009992 episode_count: 59398. steps_count: 64056667.000000\n",
      "ep 3950: ep_len:1050 episode reward: total was 2.250000. running mean: -3.674413\n",
      "ep 3950: ep_len:747 episode reward: total was -17.950000. running mean: -3.817169\n",
      "ep 3950: ep_len:2910 episode reward: total was -0.250000. running mean: -3.781497\n",
      "ep 3950: ep_len:736 episode reward: total was -74.530000. running mean: -4.488982\n",
      "ep 3950: ep_len:91 episode reward: total was 42.500000. running mean: -4.019092\n",
      "ep 3950: ep_len:72 episode reward: total was 32.510000. running mean: -3.653801\n",
      "ep 3950: ep_len:741 episode reward: total was -9.930000. running mean: -3.716563\n",
      "ep 3950: ep_len:610 episode reward: total was 13.860000. running mean: -3.540798\n",
      "ep 3950: ep_len:1613 episode reward: total was -99.190000. running mean: -4.497290\n",
      "ep 3950: ep_len:714 episode reward: total was 29.520000. running mean: -4.157117\n",
      "ep 3950: ep_len:500 episode reward: total was 9.650000. running mean: -4.019046\n",
      "ep 3950: ep_len:167 episode reward: total was 80.500000. running mean: -3.173855\n",
      "ep 3950: ep_len:48 episode reward: total was 22.500000. running mean: -2.917117\n",
      "ep 3950: ep_len:3975 episode reward: total was -343.050000. running mean: -6.318446\n",
      "ep 3950: ep_len:2882 episode reward: total was 7.600000. running mean: -6.179261\n",
      "ep 3950: ep_len:39 episode reward: total was 18.000000. running mean: -5.937468\n",
      "epsilon:0.009992 episode_count: 59414. steps_count: 64073562.000000\n",
      "ep 3951: ep_len:971 episode reward: total was -45.450000. running mean: -6.332594\n",
      "ep 3951: ep_len:749 episode reward: total was -44.880000. running mean: -6.718068\n",
      "ep 3951: ep_len:3023 episode reward: total was -47.700000. running mean: -7.127887\n",
      "ep 3951: ep_len:1128 episode reward: total was -27.760000. running mean: -7.334208\n",
      "ep 3951: ep_len:69 episode reward: total was 30.000000. running mean: -6.960866\n",
      "ep 3951: ep_len:74 episode reward: total was 35.500000. running mean: -6.536258\n",
      "ep 3951: ep_len:710 episode reward: total was 31.400000. running mean: -6.156895\n",
      "ep 3951: ep_len:671 episode reward: total was 23.750000. running mean: -5.857826\n",
      "ep 3951: ep_len:3861 episode reward: total was -300.920000. running mean: -8.808448\n",
      "ep 3951: ep_len:767 episode reward: total was 8.760000. running mean: -8.632763\n",
      "ep 3951: ep_len:3683 episode reward: total was -295.000000. running mean: -11.496436\n",
      "ep 3951: ep_len:591 episode reward: total was -4.790000. running mean: -11.429371\n",
      "ep 3951: ep_len:2827 episode reward: total was -1582.700000. running mean: -27.142078\n",
      "epsilon:0.009992 episode_count: 59427. steps_count: 64092686.000000\n",
      "ep 3952: ep_len:1045 episode reward: total was -20.020000. running mean: -27.070857\n",
      "ep 3952: ep_len:706 episode reward: total was -24.390000. running mean: -27.044048\n",
      "ep 3952: ep_len:77 episode reward: total was 37.000000. running mean: -26.403608\n",
      "ep 3952: ep_len:2976 episode reward: total was -6.850000. running mean: -26.208072\n",
      "ep 3952: ep_len:500 episode reward: total was -7.150000. running mean: -26.017491\n",
      "ep 3952: ep_len:37 episode reward: total was 17.000000. running mean: -25.587316\n",
      "ep 3952: ep_len:68 episode reward: total was 32.500000. running mean: -25.006443\n",
      "ep 3952: ep_len:965 episode reward: total was 7.710000. running mean: -24.679278\n",
      "ep 3952: ep_len:3808 episode reward: total was -126.380000. running mean: -25.696286\n",
      "ep 3952: ep_len:1576 episode reward: total was -81.080000. running mean: -26.250123\n",
      "ep 3952: ep_len:662 episode reward: total was 23.510000. running mean: -25.752522\n",
      "ep 3952: ep_len:918 episode reward: total was -2.160000. running mean: -25.516596\n",
      "ep 3952: ep_len:82 episode reward: total was 39.500000. running mean: -24.866430\n",
      "ep 3952: ep_len:829 episode reward: total was -26.220000. running mean: -24.879966\n",
      "ep 3952: ep_len:45 episode reward: total was 19.500000. running mean: -24.436166\n",
      "epsilon:0.009992 episode_count: 59442. steps_count: 64106980.000000\n",
      "ep 3953: ep_len:1126 episode reward: total was -0.020000. running mean: -24.192005\n",
      "ep 3953: ep_len:728 episode reward: total was -13.090000. running mean: -24.080985\n",
      "ep 3953: ep_len:49 episode reward: total was 23.000000. running mean: -23.610175\n",
      "ep 3953: ep_len:2996 episode reward: total was -0.560000. running mean: -23.379673\n",
      "ep 3953: ep_len:500 episode reward: total was 25.070000. running mean: -22.895176\n",
      "ep 3953: ep_len:70 episode reward: total was 32.000000. running mean: -22.346225\n",
      "ep 3953: ep_len:1401 episode reward: total was -74.230000. running mean: -22.865062\n",
      "ep 3953: ep_len:668 episode reward: total was 19.830000. running mean: -22.438112\n",
      "ep 3953: ep_len:569 episode reward: total was -9.630000. running mean: -22.310031\n",
      "ep 3953: ep_len:7415 episode reward: total was -88.050000. running mean: -22.967430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3953: ep_len:710 episode reward: total was -22.950000. running mean: -22.967256\n",
      "ep 3953: ep_len:180 episode reward: total was 87.000000. running mean: -21.867583\n",
      "ep 3953: ep_len:1516 episode reward: total was -7.050000. running mean: -21.719408\n",
      "ep 3953: ep_len:2759 episode reward: total was -55.000000. running mean: -22.052214\n",
      "ep 3953: ep_len:58 episode reward: total was 27.500000. running mean: -21.556691\n",
      "epsilon:0.009992 episode_count: 59457. steps_count: 64127725.000000\n",
      "ep 3954: ep_len:611 episode reward: total was 10.230000. running mean: -21.238825\n",
      "ep 3954: ep_len:680 episode reward: total was -39.970000. running mean: -21.426136\n",
      "ep 3954: ep_len:3013 episode reward: total was -56.050000. running mean: -21.772375\n",
      "ep 3954: ep_len:824 episode reward: total was 49.680000. running mean: -21.057851\n",
      "ep 3954: ep_len:48 episode reward: total was 22.500000. running mean: -20.622273\n",
      "ep 3954: ep_len:102 episode reward: total was 49.500000. running mean: -19.921050\n",
      "ep 3954: ep_len:500 episode reward: total was 9.350000. running mean: -19.628339\n",
      "ep 3954: ep_len:632 episode reward: total was -3.760000. running mean: -19.469656\n",
      "ep 3954: ep_len:642 episode reward: total was -57.380000. running mean: -19.848759\n",
      "ep 3954: ep_len:7358 episode reward: total was 46.050000. running mean: -19.189772\n",
      "ep 3954: ep_len:989 episode reward: total was 14.990000. running mean: -18.847974\n",
      "ep 3954: ep_len:57 episode reward: total was 27.000000. running mean: -18.389494\n",
      "ep 3954: ep_len:48 episode reward: total was 22.500000. running mean: -17.980599\n",
      "ep 3954: ep_len:1464 episode reward: total was 9.360000. running mean: -17.707193\n",
      "ep 3954: ep_len:2889 episode reward: total was -45.100000. running mean: -17.981122\n",
      "ep 3954: ep_len:63 episode reward: total was 30.000000. running mean: -17.501310\n",
      "epsilon:0.009992 episode_count: 59473. steps_count: 64147645.000000\n",
      "ep 3955: ep_len:1137 episode reward: total was -6.460000. running mean: -17.390897\n",
      "ep 3955: ep_len:764 episode reward: total was -71.970000. running mean: -17.936688\n",
      "ep 3955: ep_len:47 episode reward: total was 20.500000. running mean: -17.552321\n",
      "ep 3955: ep_len:3040 episode reward: total was -51.470000. running mean: -17.891498\n",
      "ep 3955: ep_len:500 episode reward: total was 19.580000. running mean: -17.516783\n",
      "ep 3955: ep_len:71 episode reward: total was 34.000000. running mean: -17.001615\n",
      "ep 3955: ep_len:608 episode reward: total was 42.340000. running mean: -16.408199\n",
      "ep 3955: ep_len:3682 episode reward: total was -188.200000. running mean: -18.126117\n",
      "ep 3955: ep_len:1295 episode reward: total was -25.600000. running mean: -18.200856\n",
      "ep 3955: ep_len:741 episode reward: total was 44.090000. running mean: -17.577947\n",
      "ep 3955: ep_len:861 episode reward: total was 23.260000. running mean: -17.169568\n",
      "ep 3955: ep_len:86 episode reward: total was 41.500000. running mean: -16.582872\n",
      "ep 3955: ep_len:108 episode reward: total was 52.500000. running mean: -15.892044\n",
      "ep 3955: ep_len:648 episode reward: total was -70.940000. running mean: -16.442523\n",
      "ep 3955: ep_len:44 episode reward: total was 20.500000. running mean: -16.073098\n",
      "epsilon:0.009992 episode_count: 59488. steps_count: 64161277.000000\n",
      "ep 3956: ep_len:741 episode reward: total was -85.680000. running mean: -16.769167\n",
      "ep 3956: ep_len:193 episode reward: total was 1.800000. running mean: -16.583475\n",
      "ep 3956: ep_len:66 episode reward: total was 30.000000. running mean: -16.117641\n",
      "ep 3956: ep_len:3018 episode reward: total was -146.700000. running mean: -17.423464\n",
      "ep 3956: ep_len:500 episode reward: total was 28.780000. running mean: -16.961429\n",
      "ep 3956: ep_len:103 episode reward: total was 50.000000. running mean: -16.291815\n",
      "ep 3956: ep_len:1921 episode reward: total was -87.100000. running mean: -16.999897\n",
      "ep 3956: ep_len:3540 episode reward: total was -32.030000. running mean: -17.150198\n",
      "ep 3956: ep_len:946 episode reward: total was -25.050000. running mean: -17.229196\n",
      "ep 3956: ep_len:724 episode reward: total was 28.510000. running mean: -16.771804\n",
      "ep 3956: ep_len:1508 episode reward: total was 6.120000. running mean: -16.542886\n",
      "ep 3956: ep_len:64 episode reward: total was 30.500000. running mean: -16.072457\n",
      "ep 3956: ep_len:1134 episode reward: total was 3.490000. running mean: -15.876833\n",
      "ep 3956: ep_len:2820 episode reward: total was -162.770000. running mean: -17.345764\n",
      "epsilon:0.009992 episode_count: 59502. steps_count: 64178555.000000\n",
      "ep 3957: ep_len:820 episode reward: total was -32.240000. running mean: -17.494707\n",
      "ep 3957: ep_len:681 episode reward: total was 15.160000. running mean: -17.168160\n",
      "ep 3957: ep_len:2974 episode reward: total was -48.720000. running mean: -17.483678\n",
      "ep 3957: ep_len:500 episode reward: total was 4.590000. running mean: -17.262941\n",
      "ep 3957: ep_len:101 episode reward: total was 49.000000. running mean: -16.600312\n",
      "ep 3957: ep_len:639 episode reward: total was 11.300000. running mean: -16.321309\n",
      "ep 3957: ep_len:639 episode reward: total was 26.550000. running mean: -15.892596\n",
      "ep 3957: ep_len:879 episode reward: total was 19.670000. running mean: -15.536970\n",
      "ep 3957: ep_len:619 episode reward: total was 17.750000. running mean: -15.204100\n",
      "ep 3957: ep_len:500 episode reward: total was 11.030000. running mean: -14.941759\n",
      "ep 3957: ep_len:61 episode reward: total was 27.500000. running mean: -14.517341\n",
      "ep 3957: ep_len:126 episode reward: total was 60.000000. running mean: -13.772168\n",
      "ep 3957: ep_len:1034 episode reward: total was 1.670000. running mean: -13.617746\n",
      "ep 3957: ep_len:2946 episode reward: total was -45.020000. running mean: -13.931769\n",
      "epsilon:0.009992 episode_count: 59516. steps_count: 64191074.000000\n",
      "ep 3958: ep_len:609 episode reward: total was 4.290000. running mean: -13.749551\n",
      "ep 3958: ep_len:750 episode reward: total was -6.810000. running mean: -13.680156\n",
      "ep 3958: ep_len:2979 episode reward: total was -103.730000. running mean: -14.580654\n",
      "ep 3958: ep_len:500 episode reward: total was -19.950000. running mean: -14.634348\n",
      "ep 3958: ep_len:57 episode reward: total was 27.000000. running mean: -14.218004\n",
      "ep 3958: ep_len:500 episode reward: total was -12.820000. running mean: -14.204024\n",
      "ep 3958: ep_len:633 episode reward: total was 33.870000. running mean: -13.723284\n",
      "ep 3958: ep_len:529 episode reward: total was 5.550000. running mean: -13.530551\n",
      "ep 3958: ep_len:816 episode reward: total was 45.950000. running mean: -12.935745\n",
      "ep 3958: ep_len:604 episode reward: total was 7.610000. running mean: -12.730288\n",
      "ep 3958: ep_len:811 episode reward: total was 2.200000. running mean: -12.580985\n",
      "ep 3958: ep_len:2933 episode reward: total was -5.970000. running mean: -12.514875\n",
      "epsilon:0.009992 episode_count: 59528. steps_count: 64202795.000000\n",
      "ep 3959: ep_len:999 episode reward: total was -65.880000. running mean: -13.048526\n",
      "ep 3959: ep_len:637 episode reward: total was 19.240000. running mean: -12.725641\n",
      "ep 3959: ep_len:40 episode reward: total was 18.500000. running mean: -12.413385\n",
      "ep 3959: ep_len:3020 episode reward: total was -34.960000. running mean: -12.638851\n",
      "ep 3959: ep_len:563 episode reward: total was -13.940000. running mean: -12.651862\n",
      "ep 3959: ep_len:500 episode reward: total was 48.140000. running mean: -12.043944\n",
      "ep 3959: ep_len:3786 episode reward: total was -3.350000. running mean: -11.957004\n",
      "ep 3959: ep_len:1577 episode reward: total was -73.290000. running mean: -12.570334\n",
      "ep 3959: ep_len:720 episode reward: total was 0.600000. running mean: -12.438631\n",
      "ep 3959: ep_len:519 episode reward: total was 13.170000. running mean: -12.182545\n",
      "ep 3959: ep_len:634 episode reward: total was -7.970000. running mean: -12.140419\n",
      "ep 3959: ep_len:2780 episode reward: total was -24.400000. running mean: -12.263015\n",
      "ep 3959: ep_len:62 episode reward: total was 28.000000. running mean: -11.860385\n",
      "epsilon:0.009992 episode_count: 59541. steps_count: 64218632.000000\n",
      "ep 3960: ep_len:950 episode reward: total was -16.000000. running mean: -11.901781\n",
      "ep 3960: ep_len:744 episode reward: total was -17.270000. running mean: -11.955463\n",
      "ep 3960: ep_len:3041 episode reward: total was -58.410000. running mean: -12.420009\n",
      "ep 3960: ep_len:500 episode reward: total was 16.270000. running mean: -12.133109\n",
      "ep 3960: ep_len:75 episode reward: total was 34.500000. running mean: -11.666777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3960: ep_len:867 episode reward: total was 20.710000. running mean: -11.343010\n",
      "ep 3960: ep_len:342 episode reward: total was 15.660000. running mean: -11.072980\n",
      "ep 3960: ep_len:1169 episode reward: total was -48.070000. running mean: -11.442950\n",
      "ep 3960: ep_len:805 episode reward: total was 6.270000. running mean: -11.265820\n",
      "ep 3960: ep_len:612 episode reward: total was 10.780000. running mean: -11.045362\n",
      "ep 3960: ep_len:74 episode reward: total was 35.500000. running mean: -10.579908\n",
      "ep 3960: ep_len:627 episode reward: total was -4.670000. running mean: -10.520809\n",
      "ep 3960: ep_len:2923 episode reward: total was -28.060000. running mean: -10.696201\n",
      "ep 3960: ep_len:69 episode reward: total was 33.000000. running mean: -10.259239\n",
      "epsilon:0.009992 episode_count: 59555. steps_count: 64231430.000000\n",
      "ep 3961: ep_len:516 episode reward: total was 8.630000. running mean: -10.070347\n",
      "ep 3961: ep_len:1640 episode reward: total was -13.580000. running mean: -10.105443\n",
      "ep 3961: ep_len:73 episode reward: total was 32.000000. running mean: -9.684389\n",
      "ep 3961: ep_len:3043 episode reward: total was -6.760000. running mean: -9.655145\n",
      "ep 3961: ep_len:596 episode reward: total was 3.770000. running mean: -9.520894\n",
      "ep 3961: ep_len:52 episode reward: total was 23.000000. running mean: -9.195685\n",
      "ep 3961: ep_len:1345 episode reward: total was -69.660000. running mean: -9.800328\n",
      "ep 3961: ep_len:3630 episode reward: total was -28.130000. running mean: -9.983625\n",
      "ep 3961: ep_len:1267 episode reward: total was -42.130000. running mean: -10.305088\n",
      "ep 3961: ep_len:763 episode reward: total was 40.880000. running mean: -9.793237\n",
      "ep 3961: ep_len:589 episode reward: total was 15.600000. running mean: -9.539305\n",
      "ep 3961: ep_len:500 episode reward: total was 35.090000. running mean: -9.093012\n",
      "ep 3961: ep_len:2884 episode reward: total was -39.400000. running mean: -9.396082\n",
      "ep 3961: ep_len:39 episode reward: total was 18.000000. running mean: -9.122121\n",
      "epsilon:0.009992 episode_count: 59569. steps_count: 64248367.000000\n",
      "ep 3962: ep_len:2510 episode reward: total was -383.090000. running mean: -12.861800\n",
      "ep 3962: ep_len:3867 episode reward: total was -773.390000. running mean: -20.467082\n",
      "ep 3962: ep_len:2897 episode reward: total was -44.610000. running mean: -20.708511\n",
      "ep 3962: ep_len:682 episode reward: total was 17.630000. running mean: -20.325126\n",
      "ep 3962: ep_len:161 episode reward: total was 79.000000. running mean: -19.331875\n",
      "ep 3962: ep_len:78 episode reward: total was 34.500000. running mean: -18.793556\n",
      "ep 3962: ep_len:56 episode reward: total was 22.000000. running mean: -18.385620\n",
      "ep 3962: ep_len:658 episode reward: total was 2.370000. running mean: -18.178064\n",
      "ep 3962: ep_len:3630 episode reward: total was -3.370000. running mean: -18.029984\n",
      "ep 3962: ep_len:1236 episode reward: total was -18.110000. running mean: -18.030784\n",
      "ep 3962: ep_len:644 episode reward: total was 17.150000. running mean: -17.678976\n",
      "ep 3962: ep_len:1479 episode reward: total was 25.330000. running mean: -17.248886\n",
      "ep 3962: ep_len:41 episode reward: total was 19.000000. running mean: -16.886397\n",
      "ep 3962: ep_len:1479 episode reward: total was 27.050000. running mean: -16.447033\n",
      "ep 3962: ep_len:2817 episode reward: total was -19.250000. running mean: -16.475063\n",
      "ep 3962: ep_len:35 episode reward: total was 13.000000. running mean: -16.180312\n",
      "epsilon:0.009992 episode_count: 59585. steps_count: 64270637.000000\n",
      "ep 3963: ep_len:969 episode reward: total was -41.470000. running mean: -16.433209\n",
      "ep 3963: ep_len:603 episode reward: total was -10.030000. running mean: -16.369177\n",
      "ep 3963: ep_len:46 episode reward: total was 21.500000. running mean: -15.990485\n",
      "ep 3963: ep_len:99 episode reward: total was 46.500000. running mean: -15.365580\n",
      "ep 3963: ep_len:868 episode reward: total was 70.270000. running mean: -14.509225\n",
      "ep 3963: ep_len:40 episode reward: total was 18.500000. running mean: -14.179132\n",
      "ep 3963: ep_len:58 episode reward: total was 27.500000. running mean: -13.762341\n",
      "ep 3963: ep_len:45 episode reward: total was 18.000000. running mean: -13.444718\n",
      "ep 3963: ep_len:929 episode reward: total was 60.100000. running mean: -12.709270\n",
      "ep 3963: ep_len:3851 episode reward: total was -37.450000. running mean: -12.956678\n",
      "ep 3963: ep_len:529 episode reward: total was -3.970000. running mean: -12.866811\n",
      "ep 3963: ep_len:859 episode reward: total was 38.320000. running mean: -12.354943\n",
      "ep 3963: ep_len:735 episode reward: total was -18.170000. running mean: -12.413093\n",
      "ep 3963: ep_len:500 episode reward: total was 11.910000. running mean: -12.169863\n",
      "ep 3963: ep_len:2873 episode reward: total was 8.270000. running mean: -11.965464\n",
      "epsilon:0.009992 episode_count: 59600. steps_count: 64283641.000000\n",
      "ep 3964: ep_len:704 episode reward: total was -28.480000. running mean: -12.130609\n",
      "ep 3964: ep_len:946 episode reward: total was -70.800000. running mean: -12.717303\n",
      "ep 3964: ep_len:2968 episode reward: total was -448.520000. running mean: -17.075330\n",
      "ep 3964: ep_len:610 episode reward: total was -1.140000. running mean: -16.915977\n",
      "ep 3964: ep_len:51 episode reward: total was 24.000000. running mean: -16.506817\n",
      "ep 3964: ep_len:688 episode reward: total was 13.220000. running mean: -16.209549\n",
      "ep 3964: ep_len:660 episode reward: total was 24.920000. running mean: -15.798253\n",
      "ep 3964: ep_len:545 episode reward: total was -50.270000. running mean: -16.142971\n",
      "ep 3964: ep_len:800 episode reward: total was 17.300000. running mean: -15.808541\n",
      "ep 3964: ep_len:596 episode reward: total was 6.150000. running mean: -15.588956\n",
      "ep 3964: ep_len:87 episode reward: total was 40.500000. running mean: -15.028066\n",
      "ep 3964: ep_len:72 episode reward: total was 34.500000. running mean: -14.532786\n",
      "ep 3964: ep_len:1067 episode reward: total was -0.580000. running mean: -14.393258\n",
      "ep 3964: ep_len:2820 episode reward: total was 0.760000. running mean: -14.241725\n",
      "ep 3964: ep_len:65 episode reward: total was 31.000000. running mean: -13.789308\n",
      "epsilon:0.009992 episode_count: 59615. steps_count: 64296320.000000\n",
      "ep 3965: ep_len:609 episode reward: total was 7.840000. running mean: -13.573015\n",
      "ep 3965: ep_len:670 episode reward: total was -2.560000. running mean: -13.462885\n",
      "ep 3965: ep_len:2856 episode reward: total was -195.340000. running mean: -15.281656\n",
      "ep 3965: ep_len:1179 episode reward: total was 1.830000. running mean: -15.110539\n",
      "ep 3965: ep_len:42 episode reward: total was 19.500000. running mean: -14.764434\n",
      "ep 3965: ep_len:146 episode reward: total was 70.000000. running mean: -13.916789\n",
      "ep 3965: ep_len:96 episode reward: total was 46.500000. running mean: -13.312622\n",
      "ep 3965: ep_len:1401 episode reward: total was -184.120000. running mean: -15.020695\n",
      "ep 3965: ep_len:3736 episode reward: total was -36.160000. running mean: -15.232088\n",
      "ep 3965: ep_len:1151 episode reward: total was -14.430000. running mean: -15.224068\n",
      "ep 3965: ep_len:806 episode reward: total was 12.280000. running mean: -14.949027\n",
      "ep 3965: ep_len:714 episode reward: total was 13.680000. running mean: -14.662737\n",
      "ep 3965: ep_len:123 episode reward: total was 58.500000. running mean: -13.931109\n",
      "ep 3965: ep_len:1478 episode reward: total was 22.020000. running mean: -13.571598\n",
      "ep 3965: ep_len:46 episode reward: total was 21.500000. running mean: -13.220882\n",
      "epsilon:0.009992 episode_count: 59630. steps_count: 64311373.000000\n",
      "ep 3966: ep_len:500 episode reward: total was 11.830000. running mean: -12.970373\n",
      "ep 3966: ep_len:784 episode reward: total was -22.660000. running mean: -13.067270\n",
      "ep 3966: ep_len:38 episode reward: total was 17.500000. running mean: -12.761597\n",
      "ep 3966: ep_len:85 episode reward: total was 41.000000. running mean: -12.223981\n",
      "ep 3966: ep_len:868 episode reward: total was 4.070000. running mean: -12.061041\n",
      "ep 3966: ep_len:65 episode reward: total was 31.000000. running mean: -11.630431\n",
      "ep 3966: ep_len:118 episode reward: total was 53.000000. running mean: -10.984126\n",
      "ep 3966: ep_len:736 episode reward: total was -38.540000. running mean: -11.259685\n",
      "ep 3966: ep_len:682 episode reward: total was 2.310000. running mean: -11.123988\n",
      "ep 3966: ep_len:545 episode reward: total was -0.780000. running mean: -11.020548\n",
      "ep 3966: ep_len:693 episode reward: total was 18.400000. running mean: -10.726343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3966: ep_len:679 episode reward: total was 2.230000. running mean: -10.596780\n",
      "ep 3966: ep_len:932 episode reward: total was 23.690000. running mean: -10.253912\n",
      "ep 3966: ep_len:2817 episode reward: total was 2.660000. running mean: -10.124773\n",
      "ep 3966: ep_len:63 episode reward: total was 30.000000. running mean: -9.723525\n",
      "epsilon:0.009992 episode_count: 59645. steps_count: 64320978.000000\n",
      "ep 3967: ep_len:642 episode reward: total was -9.910000. running mean: -9.725390\n",
      "ep 3967: ep_len:724 episode reward: total was -21.870000. running mean: -9.846836\n",
      "ep 3967: ep_len:102 episode reward: total was 48.000000. running mean: -9.268367\n",
      "ep 3967: ep_len:645 episode reward: total was 4.950000. running mean: -9.126184\n",
      "ep 3967: ep_len:46 episode reward: total was 21.500000. running mean: -8.819922\n",
      "ep 3967: ep_len:628 episode reward: total was 4.120000. running mean: -8.690523\n",
      "ep 3967: ep_len:3683 episode reward: total was -5.380000. running mean: -8.657417\n",
      "ep 3967: ep_len:608 episode reward: total was 4.590000. running mean: -8.524943\n",
      "ep 3967: ep_len:669 episode reward: total was 24.600000. running mean: -8.193694\n",
      "ep 3967: ep_len:1046 episode reward: total was 27.070000. running mean: -7.841057\n",
      "ep 3967: ep_len:91 episode reward: total was 44.000000. running mean: -7.322646\n",
      "ep 3967: ep_len:64 episode reward: total was 29.000000. running mean: -6.959420\n",
      "ep 3967: ep_len:700 episode reward: total was 4.510000. running mean: -6.844726\n",
      "ep 3967: ep_len:2772 episode reward: total was 13.630000. running mean: -6.639978\n",
      "epsilon:0.009992 episode_count: 59659. steps_count: 64333398.000000\n",
      "ep 3968: ep_len:1126 episode reward: total was -4.060000. running mean: -6.614179\n",
      "ep 3968: ep_len:500 episode reward: total was 2.080000. running mean: -6.527237\n",
      "ep 3968: ep_len:2944 episode reward: total was -389.100000. running mean: -10.352964\n",
      "ep 3968: ep_len:1209 episode reward: total was -20.400000. running mean: -10.453435\n",
      "ep 3968: ep_len:722 episode reward: total was -8.100000. running mean: -10.429900\n",
      "ep 3968: ep_len:687 episode reward: total was 23.390000. running mean: -10.091701\n",
      "ep 3968: ep_len:543 episode reward: total was -3.830000. running mean: -10.029084\n",
      "ep 3968: ep_len:791 episode reward: total was 36.900000. running mean: -9.559794\n",
      "ep 3968: ep_len:777 episode reward: total was 17.870000. running mean: -9.285496\n",
      "ep 3968: ep_len:513 episode reward: total was 16.240000. running mean: -9.030241\n",
      "ep 3968: ep_len:2880 episode reward: total was -28.840000. running mean: -9.228338\n",
      "ep 3968: ep_len:69 episode reward: total was 30.000000. running mean: -8.836055\n",
      "epsilon:0.009992 episode_count: 59671. steps_count: 64346159.000000\n",
      "ep 3969: ep_len:1139 episode reward: total was 3.780000. running mean: -8.709894\n",
      "ep 3969: ep_len:500 episode reward: total was 26.060000. running mean: -8.362195\n",
      "ep 3969: ep_len:59 episode reward: total was 28.000000. running mean: -7.998573\n",
      "ep 3969: ep_len:2973 episode reward: total was -297.950000. running mean: -10.898088\n",
      "ep 3969: ep_len:519 episode reward: total was -20.230000. running mean: -10.991407\n",
      "ep 3969: ep_len:87 episode reward: total was 39.000000. running mean: -10.491493\n",
      "ep 3969: ep_len:71 episode reward: total was 32.500000. running mean: -10.061578\n",
      "ep 3969: ep_len:687 episode reward: total was 30.130000. running mean: -9.659662\n",
      "ep 3969: ep_len:663 episode reward: total was 22.200000. running mean: -9.341065\n",
      "ep 3969: ep_len:952 episode reward: total was -40.930000. running mean: -9.656955\n",
      "ep 3969: ep_len:847 episode reward: total was 36.360000. running mean: -9.196785\n",
      "ep 3969: ep_len:590 episode reward: total was -38.710000. running mean: -9.491917\n",
      "ep 3969: ep_len:106 episode reward: total was 51.500000. running mean: -8.881998\n",
      "ep 3969: ep_len:1097 episode reward: total was -10.900000. running mean: -8.902178\n",
      "ep 3969: ep_len:2892 episode reward: total was 5.340000. running mean: -8.759756\n",
      "ep 3969: ep_len:74 episode reward: total was 35.500000. running mean: -8.317159\n",
      "epsilon:0.009992 episode_count: 59687. steps_count: 64359415.000000\n",
      "ep 3970: ep_len:740 episode reward: total was -72.560000. running mean: -8.959587\n",
      "ep 3970: ep_len:500 episode reward: total was 14.790000. running mean: -8.722091\n",
      "ep 3970: ep_len:66 episode reward: total was 31.500000. running mean: -8.319871\n",
      "ep 3970: ep_len:3011 episode reward: total was -197.950000. running mean: -10.216172\n",
      "ep 3970: ep_len:1655 episode reward: total was -38.720000. running mean: -10.501210\n",
      "ep 3970: ep_len:65 episode reward: total was 29.500000. running mean: -10.101198\n",
      "ep 3970: ep_len:119 episode reward: total was 56.500000. running mean: -9.435186\n",
      "ep 3970: ep_len:1479 episode reward: total was 39.510000. running mean: -8.945734\n",
      "ep 3970: ep_len:3595 episode reward: total was -82.010000. running mean: -9.676377\n",
      "ep 3970: ep_len:1630 episode reward: total was -27.340000. running mean: -9.853013\n",
      "ep 3970: ep_len:612 episode reward: total was -2.100000. running mean: -9.775483\n",
      "ep 3970: ep_len:714 episode reward: total was -11.210000. running mean: -9.789828\n",
      "ep 3970: ep_len:66 episode reward: total was 31.500000. running mean: -9.376930\n",
      "ep 3970: ep_len:50 episode reward: total was 20.500000. running mean: -9.078161\n",
      "ep 3970: ep_len:642 episode reward: total was 5.430000. running mean: -8.933079\n",
      "ep 3970: ep_len:2857 episode reward: total was -53.620000. running mean: -9.379948\n",
      "epsilon:0.009992 episode_count: 59703. steps_count: 64377216.000000\n",
      "ep 3971: ep_len:1001 episode reward: total was -35.260000. running mean: -9.638749\n",
      "ep 3971: ep_len:959 episode reward: total was 12.700000. running mean: -9.415361\n",
      "ep 3971: ep_len:2936 episode reward: total was -187.520000. running mean: -11.196408\n",
      "ep 3971: ep_len:1565 episode reward: total was -6.520000. running mean: -11.149643\n",
      "ep 3971: ep_len:36 episode reward: total was 15.000000. running mean: -10.888147\n",
      "ep 3971: ep_len:86 episode reward: total was 41.500000. running mean: -10.364266\n",
      "ep 3971: ep_len:1360 episode reward: total was -152.180000. running mean: -11.782423\n",
      "ep 3971: ep_len:3667 episode reward: total was -26.410000. running mean: -11.928699\n",
      "ep 3971: ep_len:637 episode reward: total was 41.800000. running mean: -11.391412\n",
      "ep 3971: ep_len:855 episode reward: total was 62.990000. running mean: -10.647598\n",
      "ep 3971: ep_len:959 episode reward: total was 13.190000. running mean: -10.409222\n",
      "ep 3971: ep_len:77 episode reward: total was 34.000000. running mean: -9.965129\n",
      "ep 3971: ep_len:62 episode reward: total was 29.500000. running mean: -9.570478\n",
      "ep 3971: ep_len:69 episode reward: total was 31.500000. running mean: -9.159773\n",
      "ep 3971: ep_len:1041 episode reward: total was -26.970000. running mean: -9.337876\n",
      "ep 3971: ep_len:2841 episode reward: total was -13.960000. running mean: -9.384097\n",
      "ep 3971: ep_len:25 episode reward: total was 11.000000. running mean: -9.180256\n",
      "epsilon:0.009992 episode_count: 59720. steps_count: 64395392.000000\n",
      "ep 3972: ep_len:620 episode reward: total was 14.930000. running mean: -8.939153\n",
      "ep 3972: ep_len:993 episode reward: total was 30.490000. running mean: -8.544862\n",
      "ep 3972: ep_len:62 episode reward: total was 26.500000. running mean: -8.194413\n",
      "ep 3972: ep_len:2901 episode reward: total was -198.440000. running mean: -10.096869\n",
      "ep 3972: ep_len:1629 episode reward: total was 2.630000. running mean: -9.969600\n",
      "ep 3972: ep_len:54 episode reward: total was 25.500000. running mean: -9.614904\n",
      "ep 3972: ep_len:138 episode reward: total was 67.500000. running mean: -8.843755\n",
      "ep 3972: ep_len:40 episode reward: total was 17.000000. running mean: -8.585318\n",
      "ep 3972: ep_len:1474 episode reward: total was 12.550000. running mean: -8.373965\n",
      "ep 3972: ep_len:349 episode reward: total was 11.260000. running mean: -8.177625\n",
      "ep 3972: ep_len:790 episode reward: total was -37.520000. running mean: -8.471049\n",
      "ep 3972: ep_len:780 episode reward: total was 27.080000. running mean: -8.115538\n",
      "ep 3972: ep_len:1446 episode reward: total was 9.880000. running mean: -7.935583\n",
      "ep 3972: ep_len:60 episode reward: total was 27.000000. running mean: -7.586227\n",
      "ep 3972: ep_len:1088 episode reward: total was 36.760000. running mean: -7.142765\n",
      "ep 3972: ep_len:45 episode reward: total was 21.000000. running mean: -6.861337\n",
      "epsilon:0.009992 episode_count: 59736. steps_count: 64407861.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3973: ep_len:946 episode reward: total was -58.050000. running mean: -7.373224\n",
      "ep 3973: ep_len:1222 episode reward: total was -19.260000. running mean: -7.492091\n",
      "ep 3973: ep_len:3070 episode reward: total was -223.120000. running mean: -9.648371\n",
      "ep 3973: ep_len:1678 episode reward: total was -25.600000. running mean: -9.807887\n",
      "ep 3973: ep_len:108 episode reward: total was 52.500000. running mean: -9.184808\n",
      "ep 3973: ep_len:48 episode reward: total was 19.500000. running mean: -8.897960\n",
      "ep 3973: ep_len:648 episode reward: total was -109.760000. running mean: -9.906580\n",
      "ep 3973: ep_len:652 episode reward: total was 17.590000. running mean: -9.631614\n",
      "ep 3973: ep_len:611 episode reward: total was 41.080000. running mean: -9.124498\n",
      "ep 3973: ep_len:630 episode reward: total was -2.320000. running mean: -9.056453\n",
      "ep 3973: ep_len:775 episode reward: total was -14.640000. running mean: -9.112289\n",
      "ep 3973: ep_len:1465 episode reward: total was 16.190000. running mean: -8.859266\n",
      "ep 3973: ep_len:47 episode reward: total was 20.500000. running mean: -8.565673\n",
      "ep 3973: ep_len:56 episode reward: total was 26.500000. running mean: -8.215017\n",
      "epsilon:0.009992 episode_count: 59750. steps_count: 64419817.000000\n",
      "ep 3974: ep_len:650 episode reward: total was -48.730000. running mean: -8.620166\n",
      "ep 3974: ep_len:1209 episode reward: total was -20.890000. running mean: -8.742865\n",
      "ep 3974: ep_len:34 episode reward: total was 15.500000. running mean: -8.500436\n",
      "ep 3974: ep_len:3022 episode reward: total was -218.240000. running mean: -10.597832\n",
      "ep 3974: ep_len:811 episode reward: total was 17.150000. running mean: -10.320353\n",
      "ep 3974: ep_len:74 episode reward: total was 35.500000. running mean: -9.862150\n",
      "ep 3974: ep_len:72 episode reward: total was 34.500000. running mean: -9.418528\n",
      "ep 3974: ep_len:67 episode reward: total was 30.500000. running mean: -9.019343\n",
      "ep 3974: ep_len:693 episode reward: total was 18.250000. running mean: -8.746650\n",
      "ep 3974: ep_len:3564 episode reward: total was -116.810000. running mean: -9.827283\n",
      "ep 3974: ep_len:3678 episode reward: total was -388.030000. running mean: -13.609310\n",
      "ep 3974: ep_len:7415 episode reward: total was 64.500000. running mean: -12.828217\n",
      "ep 3974: ep_len:1512 episode reward: total was 9.740000. running mean: -12.602535\n",
      "ep 3974: ep_len:92 episode reward: total was 43.000000. running mean: -12.046510\n",
      "ep 3974: ep_len:916 episode reward: total was 0.020000. running mean: -11.925845\n",
      "ep 3974: ep_len:2818 episode reward: total was 1.020000. running mean: -11.796386\n",
      "epsilon:0.009992 episode_count: 59766. steps_count: 64446444.000000\n",
      "ep 3975: ep_len:1107 episode reward: total was -4.250000. running mean: -11.720922\n",
      "ep 3975: ep_len:747 episode reward: total was -13.070000. running mean: -11.734413\n",
      "ep 3975: ep_len:61 episode reward: total was 27.500000. running mean: -11.342069\n",
      "ep 3975: ep_len:3037 episode reward: total was -246.510000. running mean: -13.693748\n",
      "ep 3975: ep_len:530 episode reward: total was -8.000000. running mean: -13.636811\n",
      "ep 3975: ep_len:56 episode reward: total was 25.000000. running mean: -13.250443\n",
      "ep 3975: ep_len:171 episode reward: total was 82.500000. running mean: -12.292938\n",
      "ep 3975: ep_len:102 episode reward: total was 49.500000. running mean: -11.675009\n",
      "ep 3975: ep_len:41 episode reward: total was 19.000000. running mean: -11.368259\n",
      "ep 3975: ep_len:591 episode reward: total was 50.100000. running mean: -10.753576\n",
      "ep 3975: ep_len:3821 episode reward: total was -37.430000. running mean: -11.020340\n",
      "ep 3975: ep_len:1549 episode reward: total was 3.570000. running mean: -10.874437\n",
      "ep 3975: ep_len:780 episode reward: total was 12.870000. running mean: -10.636993\n",
      "ep 3975: ep_len:1377 episode reward: total was -162.220000. running mean: -12.152823\n",
      "ep 3975: ep_len:29 episode reward: total was 13.000000. running mean: -11.901294\n",
      "ep 3975: ep_len:121 episode reward: total was 57.500000. running mean: -11.207282\n",
      "ep 3975: ep_len:915 episode reward: total was -17.160000. running mean: -11.266809\n",
      "ep 3975: ep_len:2860 episode reward: total was 10.040000. running mean: -11.053741\n",
      "ep 3975: ep_len:59 episode reward: total was 26.500000. running mean: -10.678203\n",
      "epsilon:0.009992 episode_count: 59785. steps_count: 64464398.000000\n",
      "ep 3976: ep_len:637 episode reward: total was -3.890000. running mean: -10.610321\n",
      "ep 3976: ep_len:500 episode reward: total was -40.400000. running mean: -10.908218\n",
      "ep 3976: ep_len:101 episode reward: total was 40.000000. running mean: -10.399136\n",
      "ep 3976: ep_len:4593 episode reward: total was -895.680000. running mean: -19.251944\n",
      "ep 3976: ep_len:67 episode reward: total was 30.500000. running mean: -18.754425\n",
      "ep 3976: ep_len:131 episode reward: total was 62.500000. running mean: -17.941881\n",
      "ep 3976: ep_len:49 episode reward: total was 23.000000. running mean: -17.532462\n",
      "ep 3976: ep_len:21 episode reward: total was 9.000000. running mean: -17.267137\n",
      "ep 3976: ep_len:500 episode reward: total was 25.330000. running mean: -16.841166\n",
      "ep 3976: ep_len:3612 episode reward: total was 1.990000. running mean: -16.652854\n",
      "ep 3976: ep_len:1280 episode reward: total was -18.130000. running mean: -16.667626\n",
      "ep 3976: ep_len:915 episode reward: total was 71.830000. running mean: -15.782649\n",
      "ep 3976: ep_len:500 episode reward: total was 14.380000. running mean: -15.481023\n",
      "ep 3976: ep_len:38 episode reward: total was 17.500000. running mean: -15.151213\n",
      "ep 3976: ep_len:166 episode reward: total was 80.000000. running mean: -14.199701\n",
      "ep 3976: ep_len:51 episode reward: total was 24.000000. running mean: -13.817704\n",
      "ep 3976: ep_len:500 episode reward: total was 42.590000. running mean: -13.253627\n",
      "ep 3976: ep_len:2793 episode reward: total was -49.890000. running mean: -13.619990\n",
      "ep 3976: ep_len:67 episode reward: total was 30.500000. running mean: -13.178790\n",
      "epsilon:0.009992 episode_count: 59804. steps_count: 64480919.000000\n",
      "ep 3977: ep_len:918 episode reward: total was -77.910000. running mean: -13.826103\n",
      "ep 3977: ep_len:1646 episode reward: total was 6.900000. running mean: -13.618841\n",
      "ep 3977: ep_len:56 episode reward: total was 26.500000. running mean: -13.217653\n",
      "ep 3977: ep_len:3023 episode reward: total was -287.680000. running mean: -15.962277\n",
      "ep 3977: ep_len:591 episode reward: total was 9.590000. running mean: -15.706754\n",
      "ep 3977: ep_len:63 episode reward: total was 30.000000. running mean: -15.249686\n",
      "ep 3977: ep_len:55 episode reward: total was 24.500000. running mean: -14.852189\n",
      "ep 3977: ep_len:652 episode reward: total was 10.510000. running mean: -14.598567\n",
      "ep 3977: ep_len:3696 episode reward: total was -33.010000. running mean: -14.782682\n",
      "ep 3977: ep_len:597 episode reward: total was -3.130000. running mean: -14.666155\n",
      "ep 3977: ep_len:7038 episode reward: total was 40.080000. running mean: -14.118693\n",
      "ep 3977: ep_len:703 episode reward: total was 0.800000. running mean: -13.969506\n",
      "ep 3977: ep_len:36 episode reward: total was 16.500000. running mean: -13.664811\n",
      "ep 3977: ep_len:1113 episode reward: total was -8.230000. running mean: -13.610463\n",
      "ep 3977: ep_len:2882 episode reward: total was -4.040000. running mean: -13.514759\n",
      "epsilon:0.009992 episode_count: 59819. steps_count: 64503988.000000\n",
      "ep 3978: ep_len:591 episode reward: total was 26.030000. running mean: -13.119311\n",
      "ep 3978: ep_len:704 episode reward: total was -14.490000. running mean: -13.133018\n",
      "ep 3978: ep_len:2877 episode reward: total was -209.450000. running mean: -15.096188\n",
      "ep 3978: ep_len:1189 episode reward: total was -9.490000. running mean: -15.040126\n",
      "ep 3978: ep_len:81 episode reward: total was 33.000000. running mean: -14.559725\n",
      "ep 3978: ep_len:856 episode reward: total was 10.770000. running mean: -14.306427\n",
      "ep 3978: ep_len:3469 episode reward: total was -31.760000. running mean: -14.480963\n",
      "ep 3978: ep_len:1262 episode reward: total was -23.910000. running mean: -14.575254\n",
      "ep 3978: ep_len:683 episode reward: total was 20.260000. running mean: -14.226901\n",
      "ep 3978: ep_len:500 episode reward: total was 28.750000. running mean: -13.797132\n",
      "ep 3978: ep_len:154 episode reward: total was 75.500000. running mean: -12.904161\n",
      "ep 3978: ep_len:70 episode reward: total was 30.500000. running mean: -12.470119\n",
      "ep 3978: ep_len:732 episode reward: total was -42.980000. running mean: -12.775218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3978: ep_len:2812 episode reward: total was -18.880000. running mean: -12.836266\n",
      "epsilon:0.009992 episode_count: 59833. steps_count: 64519968.000000\n",
      "ep 3979: ep_len:656 episode reward: total was -45.000000. running mean: -13.157903\n",
      "ep 3979: ep_len:715 episode reward: total was -18.170000. running mean: -13.208024\n",
      "ep 3979: ep_len:3075 episode reward: total was -119.990000. running mean: -14.275844\n",
      "ep 3979: ep_len:621 episode reward: total was -9.780000. running mean: -14.230885\n",
      "ep 3979: ep_len:62 episode reward: total was 29.500000. running mean: -13.793576\n",
      "ep 3979: ep_len:1504 episode reward: total was -18.600000. running mean: -13.841641\n",
      "ep 3979: ep_len:295 episode reward: total was 21.620000. running mean: -13.487024\n",
      "ep 3979: ep_len:1520 episode reward: total was -21.890000. running mean: -13.571054\n",
      "ep 3979: ep_len:727 episode reward: total was -2.050000. running mean: -13.455844\n",
      "ep 3979: ep_len:668 episode reward: total was 1.460000. running mean: -13.306685\n",
      "ep 3979: ep_len:1372 episode reward: total was 23.590000. running mean: -12.937718\n",
      "ep 3979: ep_len:2888 episode reward: total was 4.110000. running mean: -12.767241\n",
      "epsilon:0.009992 episode_count: 59845. steps_count: 64534071.000000\n",
      "ep 3980: ep_len:659 episode reward: total was -27.830000. running mean: -12.917869\n",
      "ep 3980: ep_len:1605 episode reward: total was -4.650000. running mean: -12.835190\n",
      "ep 3980: ep_len:65 episode reward: total was 31.000000. running mean: -12.396838\n",
      "ep 3980: ep_len:3005 episode reward: total was -80.560000. running mean: -13.078470\n",
      "ep 3980: ep_len:668 episode reward: total was -18.740000. running mean: -13.135085\n",
      "ep 3980: ep_len:61 episode reward: total was 27.500000. running mean: -12.728734\n",
      "ep 3980: ep_len:131 episode reward: total was 61.000000. running mean: -11.991447\n",
      "ep 3980: ep_len:85 episode reward: total was 39.500000. running mean: -11.476532\n",
      "ep 3980: ep_len:1097 episode reward: total was 18.090000. running mean: -11.180867\n",
      "ep 3980: ep_len:619 episode reward: total was 23.960000. running mean: -10.829458\n",
      "ep 3980: ep_len:739 episode reward: total was -41.620000. running mean: -11.137364\n",
      "ep 3980: ep_len:845 episode reward: total was 63.540000. running mean: -10.390590\n",
      "ep 3980: ep_len:764 episode reward: total was -8.690000. running mean: -10.373584\n",
      "ep 3980: ep_len:70 episode reward: total was 32.000000. running mean: -9.949848\n",
      "ep 3980: ep_len:19 episode reward: total was 8.000000. running mean: -9.770350\n",
      "ep 3980: ep_len:500 episode reward: total was 9.340000. running mean: -9.579246\n",
      "ep 3980: ep_len:2867 episode reward: total was -11.610000. running mean: -9.599554\n",
      "epsilon:0.009992 episode_count: 59862. steps_count: 64547870.000000\n",
      "ep 3981: ep_len:1115 episode reward: total was -15.220000. running mean: -9.655758\n",
      "ep 3981: ep_len:675 episode reward: total was -15.640000. running mean: -9.715601\n",
      "ep 3981: ep_len:3021 episode reward: total was -40.960000. running mean: -10.028045\n",
      "ep 3981: ep_len:711 episode reward: total was -8.340000. running mean: -10.011164\n",
      "ep 3981: ep_len:137 episode reward: total was 65.500000. running mean: -9.256053\n",
      "ep 3981: ep_len:500 episode reward: total was 46.550000. running mean: -8.697992\n",
      "ep 3981: ep_len:617 episode reward: total was -153.810000. running mean: -10.149112\n",
      "ep 3981: ep_len:1616 episode reward: total was -928.360000. running mean: -19.331221\n",
      "ep 3981: ep_len:603 episode reward: total was -6.630000. running mean: -19.204209\n",
      "ep 3981: ep_len:943 episode reward: total was -42.520000. running mean: -19.437367\n",
      "ep 3981: ep_len:95 episode reward: total was 43.000000. running mean: -18.812993\n",
      "ep 3981: ep_len:593 episode reward: total was 3.500000. running mean: -18.589863\n",
      "ep 3981: ep_len:2794 episode reward: total was -14.010000. running mean: -18.544065\n",
      "epsilon:0.009992 episode_count: 59875. steps_count: 64561290.000000\n",
      "ep 3982: ep_len:1441 episode reward: total was 10.350000. running mean: -18.255124\n",
      "ep 3982: ep_len:697 episode reward: total was -43.440000. running mean: -18.506973\n",
      "ep 3982: ep_len:3096 episode reward: total was -46.230000. running mean: -18.784203\n",
      "ep 3982: ep_len:839 episode reward: total was 12.530000. running mean: -18.471061\n",
      "ep 3982: ep_len:79 episode reward: total was 38.000000. running mean: -17.906350\n",
      "ep 3982: ep_len:55 episode reward: total was 26.000000. running mean: -17.467287\n",
      "ep 3982: ep_len:2522 episode reward: total was -893.460000. running mean: -26.227214\n",
      "ep 3982: ep_len:3645 episode reward: total was -588.410000. running mean: -31.849042\n",
      "ep 3982: ep_len:500 episode reward: total was -28.770000. running mean: -31.818251\n",
      "ep 3982: ep_len:856 episode reward: total was 44.750000. running mean: -31.052569\n",
      "ep 3982: ep_len:987 episode reward: total was -3.550000. running mean: -30.777543\n",
      "ep 3982: ep_len:26 episode reward: total was 11.500000. running mean: -30.354768\n",
      "ep 3982: ep_len:67 episode reward: total was 30.500000. running mean: -29.746220\n",
      "ep 3982: ep_len:1143 episode reward: total was 1.160000. running mean: -29.437158\n",
      "ep 3982: ep_len:2888 episode reward: total was -6.610000. running mean: -29.208886\n",
      "epsilon:0.009992 episode_count: 59890. steps_count: 64580131.000000\n",
      "ep 3983: ep_len:756 episode reward: total was -45.590000. running mean: -29.372697\n",
      "ep 3983: ep_len:906 episode reward: total was 12.810000. running mean: -28.950870\n",
      "ep 3983: ep_len:2998 episode reward: total was -36.600000. running mean: -29.027362\n",
      "ep 3983: ep_len:606 episode reward: total was -66.220000. running mean: -29.399288\n",
      "ep 3983: ep_len:61 episode reward: total was 29.000000. running mean: -28.815295\n",
      "ep 3983: ep_len:666 episode reward: total was -27.220000. running mean: -28.799342\n",
      "ep 3983: ep_len:304 episode reward: total was 21.000000. running mean: -28.301349\n",
      "ep 3983: ep_len:1615 episode reward: total was -18.970000. running mean: -28.208035\n",
      "ep 3983: ep_len:743 episode reward: total was 15.990000. running mean: -27.766055\n",
      "ep 3983: ep_len:635 episode reward: total was 11.470000. running mean: -27.373694\n",
      "ep 3983: ep_len:125 episode reward: total was 61.000000. running mean: -26.489958\n",
      "ep 3983: ep_len:784 episode reward: total was -26.670000. running mean: -26.491758\n",
      "ep 3983: ep_len:2853 episode reward: total was -1.660000. running mean: -26.243440\n",
      "epsilon:0.009992 episode_count: 59903. steps_count: 64593183.000000\n",
      "ep 3984: ep_len:989 episode reward: total was -8.460000. running mean: -26.065606\n",
      "ep 3984: ep_len:1648 episode reward: total was -24.520000. running mean: -26.050150\n",
      "ep 3984: ep_len:56 episode reward: total was 26.500000. running mean: -25.524648\n",
      "ep 3984: ep_len:2985 episode reward: total was -40.250000. running mean: -25.671902\n",
      "ep 3984: ep_len:503 episode reward: total was -2.210000. running mean: -25.437283\n",
      "ep 3984: ep_len:837 episode reward: total was 14.840000. running mean: -25.034510\n",
      "ep 3984: ep_len:298 episode reward: total was 14.580000. running mean: -24.638365\n",
      "ep 3984: ep_len:863 episode reward: total was 14.890000. running mean: -24.243081\n",
      "ep 3984: ep_len:773 episode reward: total was 4.470000. running mean: -23.955951\n",
      "ep 3984: ep_len:1149 episode reward: total was -11.390000. running mean: -23.830291\n",
      "ep 3984: ep_len:146 episode reward: total was 67.000000. running mean: -22.921988\n",
      "ep 3984: ep_len:56 episode reward: total was 25.000000. running mean: -22.442768\n",
      "ep 3984: ep_len:57 episode reward: total was 25.500000. running mean: -21.963341\n",
      "ep 3984: ep_len:724 episode reward: total was -27.270000. running mean: -22.016407\n",
      "ep 3984: ep_len:2776 episode reward: total was 6.900000. running mean: -21.727243\n",
      "epsilon:0.009992 episode_count: 59918. steps_count: 64607043.000000\n",
      "ep 3985: ep_len:1057 episode reward: total was -5.670000. running mean: -21.566671\n",
      "ep 3985: ep_len:709 episode reward: total was 4.420000. running mean: -21.306804\n",
      "ep 3985: ep_len:3072 episode reward: total was 0.820000. running mean: -21.085536\n",
      "ep 3985: ep_len:650 episode reward: total was 10.070000. running mean: -20.773981\n",
      "ep 3985: ep_len:63 episode reward: total was 27.000000. running mean: -20.296241\n",
      "ep 3985: ep_len:691 episode reward: total was 19.730000. running mean: -19.895978\n",
      "ep 3985: ep_len:319 episode reward: total was 14.790000. running mean: -19.549119\n",
      "ep 3985: ep_len:834 episode reward: total was -10.530000. running mean: -19.458927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3985: ep_len:650 episode reward: total was -6.160000. running mean: -19.325938\n",
      "ep 3985: ep_len:696 episode reward: total was 9.870000. running mean: -19.033979\n",
      "ep 3985: ep_len:211 episode reward: total was 99.500000. running mean: -17.848639\n",
      "ep 3985: ep_len:632 episode reward: total was -14.630000. running mean: -17.816453\n",
      "ep 3985: ep_len:2783 episode reward: total was -1.540000. running mean: -17.653688\n",
      "ep 3985: ep_len:40 episode reward: total was 18.500000. running mean: -17.292151\n",
      "epsilon:0.009992 episode_count: 59932. steps_count: 64619450.000000\n",
      "ep 3986: ep_len:666 episode reward: total was 10.110000. running mean: -17.018130\n",
      "ep 3986: ep_len:957 episode reward: total was -3.390000. running mean: -16.881848\n",
      "ep 3986: ep_len:2923 episode reward: total was -39.070000. running mean: -17.103730\n",
      "ep 3986: ep_len:517 episode reward: total was 25.810000. running mean: -16.674593\n",
      "ep 3986: ep_len:66 episode reward: total was 31.500000. running mean: -16.192847\n",
      "ep 3986: ep_len:652 episode reward: total was 14.070000. running mean: -15.890218\n",
      "ep 3986: ep_len:643 episode reward: total was 22.640000. running mean: -15.504916\n",
      "ep 3986: ep_len:574 episode reward: total was -39.880000. running mean: -15.748667\n",
      "ep 3986: ep_len:880 episode reward: total was 53.590000. running mean: -15.055280\n",
      "ep 3986: ep_len:1080 episode reward: total was 43.110000. running mean: -14.473627\n",
      "ep 3986: ep_len:120 episode reward: total was 57.000000. running mean: -13.758891\n",
      "ep 3986: ep_len:1504 episode reward: total was 7.340000. running mean: -13.547902\n",
      "ep 3986: ep_len:2931 episode reward: total was -12.480000. running mean: -13.537223\n",
      "epsilon:0.009992 episode_count: 59945. steps_count: 64632963.000000\n",
      "ep 3987: ep_len:1052 episode reward: total was -67.420000. running mean: -14.076051\n",
      "ep 3987: ep_len:779 episode reward: total was -25.010000. running mean: -14.185390\n",
      "ep 3987: ep_len:63 episode reward: total was 30.000000. running mean: -13.743536\n",
      "ep 3987: ep_len:2959 episode reward: total was -20.560000. running mean: -13.811701\n",
      "ep 3987: ep_len:638 episode reward: total was 1.160000. running mean: -13.661984\n",
      "ep 3987: ep_len:64 episode reward: total was 29.000000. running mean: -13.235364\n",
      "ep 3987: ep_len:171 episode reward: total was 82.500000. running mean: -12.278011\n",
      "ep 3987: ep_len:96 episode reward: total was 45.000000. running mean: -11.705231\n",
      "ep 3987: ep_len:2501 episode reward: total was -871.420000. running mean: -20.302378\n",
      "ep 3987: ep_len:3730 episode reward: total was -424.060000. running mean: -24.339954\n",
      "ep 3987: ep_len:1568 episode reward: total was -53.340000. running mean: -24.629955\n",
      "ep 3987: ep_len:763 episode reward: total was 9.400000. running mean: -24.289655\n",
      "ep 3987: ep_len:599 episode reward: total was -12.450000. running mean: -24.171259\n",
      "ep 3987: ep_len:174 episode reward: total was 82.500000. running mean: -23.104546\n",
      "ep 3987: ep_len:30 episode reward: total was 13.500000. running mean: -22.738501\n",
      "ep 3987: ep_len:75 episode reward: total was 36.000000. running mean: -22.151116\n",
      "ep 3987: ep_len:990 episode reward: total was -39.800000. running mean: -22.327605\n",
      "ep 3987: ep_len:2836 episode reward: total was -13.950000. running mean: -22.243829\n",
      "epsilon:0.009992 episode_count: 59963. steps_count: 64652051.000000\n",
      "ep 3988: ep_len:1095 episode reward: total was 2.700000. running mean: -21.994390\n",
      "ep 3988: ep_len:1174 episode reward: total was -118.340000. running mean: -22.957846\n",
      "ep 3988: ep_len:2997 episode reward: total was -21.710000. running mean: -22.945368\n",
      "ep 3988: ep_len:679 episode reward: total was -15.180000. running mean: -22.867714\n",
      "ep 3988: ep_len:56 episode reward: total was 26.500000. running mean: -22.374037\n",
      "ep 3988: ep_len:1369 episode reward: total was -45.060000. running mean: -22.600897\n",
      "ep 3988: ep_len:329 episode reward: total was -11.960000. running mean: -22.494488\n",
      "ep 3988: ep_len:792 episode reward: total was -6.220000. running mean: -22.331743\n",
      "ep 3988: ep_len:749 episode reward: total was 25.360000. running mean: -21.854825\n",
      "ep 3988: ep_len:500 episode reward: total was 21.900000. running mean: -21.417277\n",
      "ep 3988: ep_len:944 episode reward: total was -27.090000. running mean: -21.474004\n",
      "ep 3988: ep_len:2901 episode reward: total was 4.700000. running mean: -21.212264\n",
      "ep 3988: ep_len:62 episode reward: total was 29.500000. running mean: -20.705142\n",
      "epsilon:0.009992 episode_count: 59976. steps_count: 64665698.000000\n",
      "ep 3989: ep_len:751 episode reward: total was -62.350000. running mean: -21.121590\n",
      "ep 3989: ep_len:670 episode reward: total was 2.520000. running mean: -20.885174\n",
      "ep 3989: ep_len:3015 episode reward: total was -40.220000. running mean: -21.078523\n",
      "ep 3989: ep_len:838 episode reward: total was 35.330000. running mean: -20.514437\n",
      "ep 3989: ep_len:166 episode reward: total was 77.000000. running mean: -19.539293\n",
      "ep 3989: ep_len:90 episode reward: total was 42.000000. running mean: -18.923900\n",
      "ep 3989: ep_len:500 episode reward: total was 49.270000. running mean: -18.241961\n",
      "ep 3989: ep_len:3916 episode reward: total was -383.860000. running mean: -21.898141\n",
      "ep 3989: ep_len:500 episode reward: total was 25.230000. running mean: -21.426860\n",
      "ep 3989: ep_len:770 episode reward: total was -0.430000. running mean: -21.216891\n",
      "ep 3989: ep_len:500 episode reward: total was -5.300000. running mean: -21.057723\n",
      "ep 3989: ep_len:626 episode reward: total was 10.780000. running mean: -20.739345\n",
      "ep 3989: ep_len:2799 episode reward: total was -21.420000. running mean: -20.746152\n",
      "epsilon:0.009992 episode_count: 59989. steps_count: 64680839.000000\n",
      "ep 3990: ep_len:1371 episode reward: total was 28.260000. running mean: -20.256090\n",
      "ep 3990: ep_len:852 episode reward: total was -4.850000. running mean: -20.102029\n",
      "ep 3990: ep_len:35 episode reward: total was 14.500000. running mean: -19.756009\n",
      "ep 3990: ep_len:2961 episode reward: total was -38.040000. running mean: -19.938849\n",
      "ep 3990: ep_len:611 episode reward: total was -1.630000. running mean: -19.755761\n",
      "ep 3990: ep_len:129 episode reward: total was 63.000000. running mean: -18.928203\n",
      "ep 3990: ep_len:86 episode reward: total was 40.000000. running mean: -18.338921\n",
      "ep 3990: ep_len:1470 episode reward: total was 26.930000. running mean: -17.886232\n",
      "ep 3990: ep_len:337 episode reward: total was 10.440000. running mean: -17.602969\n",
      "ep 3990: ep_len:1288 episode reward: total was -40.820000. running mean: -17.835140\n",
      "ep 3990: ep_len:654 episode reward: total was 0.830000. running mean: -17.648488\n",
      "ep 3990: ep_len:673 episode reward: total was 19.550000. running mean: -17.276503\n",
      "ep 3990: ep_len:115 episode reward: total was 54.500000. running mean: -16.558738\n",
      "ep 3990: ep_len:33 episode reward: total was 13.500000. running mean: -16.258151\n",
      "ep 3990: ep_len:91 episode reward: total was 42.500000. running mean: -15.670570\n",
      "ep 3990: ep_len:654 episode reward: total was 5.800000. running mean: -15.455864\n",
      "ep 3990: ep_len:2867 episode reward: total was 6.500000. running mean: -15.236305\n",
      "epsilon:0.009992 episode_count: 60006. steps_count: 64695066.000000\n",
      "ep 3991: ep_len:2276 episode reward: total was -216.780000. running mean: -17.251742\n",
      "ep 3991: ep_len:809 episode reward: total was 6.680000. running mean: -17.012425\n",
      "ep 3991: ep_len:68 episode reward: total was 31.000000. running mean: -16.532300\n",
      "ep 3991: ep_len:3068 episode reward: total was -34.400000. running mean: -16.710977\n",
      "ep 3991: ep_len:781 episode reward: total was 36.410000. running mean: -16.179768\n",
      "ep 3991: ep_len:47 episode reward: total was 22.000000. running mean: -15.797970\n",
      "ep 3991: ep_len:75 episode reward: total was 34.500000. running mean: -15.294990\n",
      "ep 3991: ep_len:999 episode reward: total was 0.730000. running mean: -15.134740\n",
      "ep 3991: ep_len:3845 episode reward: total was -249.110000. running mean: -17.474493\n",
      "ep 3991: ep_len:4227 episode reward: total was -659.350000. running mean: -23.893248\n",
      "ep 3991: ep_len:672 episode reward: total was -7.640000. running mean: -23.730716\n",
      "ep 3991: ep_len:611 episode reward: total was 27.000000. running mean: -23.223408\n",
      "ep 3991: ep_len:66 episode reward: total was 30.000000. running mean: -22.691174\n",
      "ep 3991: ep_len:763 episode reward: total was -30.310000. running mean: -22.767363\n",
      "ep 3991: ep_len:2714 episode reward: total was -97.630000. running mean: -23.515989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3991: ep_len:37 episode reward: total was 17.000000. running mean: -23.110829\n",
      "epsilon:0.009992 episode_count: 60022. steps_count: 64716124.000000\n",
      "ep 3992: ep_len:1090 episode reward: total was 12.480000. running mean: -22.754921\n",
      "ep 3992: ep_len:746 episode reward: total was -14.960000. running mean: -22.676972\n",
      "ep 3992: ep_len:2834 episode reward: total was -21.260000. running mean: -22.662802\n",
      "ep 3992: ep_len:834 episode reward: total was -31.220000. running mean: -22.748374\n",
      "ep 3992: ep_len:63 episode reward: total was 30.000000. running mean: -22.220890\n",
      "ep 3992: ep_len:45 episode reward: total was 21.000000. running mean: -21.788681\n",
      "ep 3992: ep_len:1010 episode reward: total was -12.930000. running mean: -21.700094\n",
      "ep 3992: ep_len:3607 episode reward: total was -176.280000. running mean: -23.245893\n",
      "ep 3992: ep_len:1369 episode reward: total was -84.640000. running mean: -23.859835\n",
      "ep 3992: ep_len:7504 episode reward: total was -3677.200000. running mean: -60.393236\n",
      "ep 3992: ep_len:1158 episode reward: total was -14.850000. running mean: -59.937804\n",
      "ep 3992: ep_len:500 episode reward: total was 17.640000. running mean: -59.162026\n",
      "ep 3992: ep_len:2862 episode reward: total was 5.530000. running mean: -58.515106\n",
      "epsilon:0.009992 episode_count: 60035. steps_count: 64739746.000000\n",
      "ep 3993: ep_len:689 episode reward: total was -37.720000. running mean: -58.307154\n",
      "ep 3993: ep_len:943 episode reward: total was 18.970000. running mean: -57.534383\n",
      "ep 3993: ep_len:2894 episode reward: total was -8.350000. running mean: -57.042539\n",
      "ep 3993: ep_len:654 episode reward: total was 8.560000. running mean: -56.386514\n",
      "ep 3993: ep_len:46 episode reward: total was 21.500000. running mean: -55.607649\n",
      "ep 3993: ep_len:124 episode reward: total was 59.000000. running mean: -54.461572\n",
      "ep 3993: ep_len:105 episode reward: total was 51.000000. running mean: -53.406956\n",
      "ep 3993: ep_len:71 episode reward: total was 31.000000. running mean: -52.562887\n",
      "ep 3993: ep_len:1099 episode reward: total was -63.920000. running mean: -52.676458\n",
      "ep 3993: ep_len:3901 episode reward: total was -101.120000. running mean: -53.160893\n",
      "ep 3993: ep_len:537 episode reward: total was 11.820000. running mean: -52.511084\n",
      "ep 3993: ep_len:870 episode reward: total was 34.430000. running mean: -51.641674\n",
      "ep 3993: ep_len:1512 episode reward: total was -31.360000. running mean: -51.438857\n",
      "ep 3993: ep_len:35 episode reward: total was 16.000000. running mean: -50.764468\n",
      "ep 3993: ep_len:500 episode reward: total was 17.700000. running mean: -50.079824\n",
      "ep 3993: ep_len:2924 episode reward: total was -3.130000. running mean: -49.610325\n",
      "ep 3993: ep_len:39 episode reward: total was 18.000000. running mean: -48.934222\n",
      "epsilon:0.009992 episode_count: 60052. steps_count: 64756689.000000\n",
      "ep 3994: ep_len:709 episode reward: total was -30.510000. running mean: -48.749980\n",
      "ep 3994: ep_len:702 episode reward: total was -11.330000. running mean: -48.375780\n",
      "ep 3994: ep_len:3067 episode reward: total was -76.210000. running mean: -48.654122\n",
      "ep 3994: ep_len:815 episode reward: total was 25.120000. running mean: -47.916381\n",
      "ep 3994: ep_len:54 episode reward: total was 24.000000. running mean: -47.197217\n",
      "ep 3994: ep_len:145 episode reward: total was 69.500000. running mean: -46.030245\n",
      "ep 3994: ep_len:669 episode reward: total was 12.310000. running mean: -45.446843\n",
      "ep 3994: ep_len:3646 episode reward: total was -69.990000. running mean: -45.692274\n",
      "ep 3994: ep_len:1511 episode reward: total was -24.150000. running mean: -45.476851\n",
      "ep 3994: ep_len:586 episode reward: total was 4.250000. running mean: -44.979583\n",
      "ep 3994: ep_len:656 episode reward: total was 11.370000. running mean: -44.416087\n",
      "ep 3994: ep_len:74 episode reward: total was 34.000000. running mean: -43.631926\n",
      "ep 3994: ep_len:784 episode reward: total was -15.560000. running mean: -43.351207\n",
      "ep 3994: ep_len:2781 episode reward: total was 5.670000. running mean: -42.860995\n",
      "epsilon:0.009992 episode_count: 60066. steps_count: 64772888.000000\n",
      "ep 3995: ep_len:1010 episode reward: total was -79.960000. running mean: -43.231985\n",
      "ep 3995: ep_len:714 episode reward: total was 17.050000. running mean: -42.629165\n",
      "ep 3995: ep_len:72 episode reward: total was 34.500000. running mean: -41.857873\n",
      "ep 3995: ep_len:2990 episode reward: total was -27.040000. running mean: -41.709695\n",
      "ep 3995: ep_len:1140 episode reward: total was 2.140000. running mean: -41.271198\n",
      "ep 3995: ep_len:63 episode reward: total was 28.500000. running mean: -40.573486\n",
      "ep 3995: ep_len:500 episode reward: total was 7.290000. running mean: -40.094851\n",
      "ep 3995: ep_len:344 episode reward: total was 7.480000. running mean: -39.619102\n",
      "ep 3995: ep_len:1568 episode reward: total was -34.080000. running mean: -39.563711\n",
      "ep 3995: ep_len:794 episode reward: total was 1.080000. running mean: -39.157274\n",
      "ep 3995: ep_len:516 episode reward: total was 0.280000. running mean: -38.762902\n",
      "ep 3995: ep_len:68 episode reward: total was 32.500000. running mean: -38.050273\n",
      "ep 3995: ep_len:142 episode reward: total was 66.500000. running mean: -37.004770\n",
      "ep 3995: ep_len:903 episode reward: total was -29.520000. running mean: -36.929922\n",
      "ep 3995: ep_len:2957 episode reward: total was -12.960000. running mean: -36.690223\n",
      "ep 3995: ep_len:38 episode reward: total was 17.500000. running mean: -36.148321\n",
      "epsilon:0.009992 episode_count: 60082. steps_count: 64786707.000000\n",
      "ep 3996: ep_len:636 episode reward: total was 8.450000. running mean: -35.702337\n",
      "ep 3996: ep_len:995 episode reward: total was -29.060000. running mean: -35.635914\n",
      "ep 3996: ep_len:3002 episode reward: total was -76.680000. running mean: -36.046355\n",
      "ep 3996: ep_len:523 episode reward: total was -56.550000. running mean: -36.251391\n",
      "ep 3996: ep_len:72 episode reward: total was 31.500000. running mean: -35.573877\n",
      "ep 3996: ep_len:73 episode reward: total was 35.000000. running mean: -34.868139\n",
      "ep 3996: ep_len:903 episode reward: total was 65.410000. running mean: -33.865357\n",
      "ep 3996: ep_len:631 episode reward: total was 12.240000. running mean: -33.404304\n",
      "ep 3996: ep_len:1592 episode reward: total was -30.720000. running mean: -33.377461\n",
      "ep 3996: ep_len:683 episode reward: total was 37.870000. running mean: -32.664986\n",
      "ep 3996: ep_len:1090 episode reward: total was 29.560000. running mean: -32.042736\n",
      "ep 3996: ep_len:56 episode reward: total was 25.000000. running mean: -31.472309\n",
      "ep 3996: ep_len:164 episode reward: total was 77.500000. running mean: -30.382586\n",
      "ep 3996: ep_len:1469 episode reward: total was 17.520000. running mean: -29.903560\n",
      "ep 3996: ep_len:2761 episode reward: total was -23.430000. running mean: -29.838824\n",
      "epsilon:0.009992 episode_count: 60097. steps_count: 64801357.000000\n",
      "ep 3997: ep_len:1020 episode reward: total was -61.680000. running mean: -30.157236\n",
      "ep 3997: ep_len:755 episode reward: total was -22.920000. running mean: -30.084864\n",
      "ep 3997: ep_len:83 episode reward: total was 40.000000. running mean: -29.384015\n",
      "ep 3997: ep_len:3033 episode reward: total was -0.640000. running mean: -29.096575\n",
      "ep 3997: ep_len:1109 episode reward: total was -1.200000. running mean: -28.817609\n",
      "ep 3997: ep_len:59 episode reward: total was 26.500000. running mean: -28.264433\n",
      "ep 3997: ep_len:500 episode reward: total was 7.470000. running mean: -27.907089\n",
      "ep 3997: ep_len:638 episode reward: total was -0.610000. running mean: -27.634118\n",
      "ep 3997: ep_len:605 episode reward: total was -55.730000. running mean: -27.915077\n",
      "ep 3997: ep_len:7201 episode reward: total was -11.680000. running mean: -27.752726\n",
      "ep 3997: ep_len:1147 episode reward: total was 2.210000. running mean: -27.453099\n",
      "ep 3997: ep_len:69 episode reward: total was 30.000000. running mean: -26.878568\n",
      "ep 3997: ep_len:36 episode reward: total was 16.500000. running mean: -26.444782\n",
      "ep 3997: ep_len:78 episode reward: total was 36.000000. running mean: -25.820334\n",
      "ep 3997: ep_len:1412 episode reward: total was 11.930000. running mean: -25.442831\n",
      "ep 3997: ep_len:2820 episode reward: total was -4.650000. running mean: -25.234903\n",
      "epsilon:0.009992 episode_count: 60113. steps_count: 64821922.000000\n",
      "ep 3998: ep_len:912 episode reward: total was -154.820000. running mean: -26.530754\n",
      "ep 3998: ep_len:211 episode reward: total was -6.190000. running mean: -26.327346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3998: ep_len:61 episode reward: total was 29.000000. running mean: -25.774073\n",
      "ep 3998: ep_len:3017 episode reward: total was -24.210000. running mean: -25.758432\n",
      "ep 3998: ep_len:636 episode reward: total was -11.990000. running mean: -25.620747\n",
      "ep 3998: ep_len:48 episode reward: total was 22.500000. running mean: -25.139540\n",
      "ep 3998: ep_len:500 episode reward: total was 50.340000. running mean: -24.384745\n",
      "ep 3998: ep_len:3867 episode reward: total was -40.030000. running mean: -24.541197\n",
      "ep 3998: ep_len:524 episode reward: total was -22.690000. running mean: -24.522685\n",
      "ep 3998: ep_len:7533 episode reward: total was -127.520000. running mean: -25.552658\n",
      "ep 3998: ep_len:1050 episode reward: total was 55.730000. running mean: -24.739832\n",
      "ep 3998: ep_len:46 episode reward: total was 21.500000. running mean: -24.277433\n",
      "ep 3998: ep_len:109 episode reward: total was 53.000000. running mean: -23.504659\n",
      "ep 3998: ep_len:100 episode reward: total was 48.500000. running mean: -22.784612\n",
      "ep 3998: ep_len:817 episode reward: total was 22.860000. running mean: -22.328166\n",
      "ep 3998: ep_len:2795 episode reward: total was 0.850000. running mean: -22.096385\n",
      "epsilon:0.009992 episode_count: 60129. steps_count: 64844148.000000\n",
      "ep 3999: ep_len:1466 episode reward: total was 17.250000. running mean: -21.702921\n",
      "ep 3999: ep_len:686 episode reward: total was -18.670000. running mean: -21.672592\n",
      "ep 3999: ep_len:46 episode reward: total was 21.500000. running mean: -21.240866\n",
      "ep 3999: ep_len:3036 episode reward: total was -21.470000. running mean: -21.243157\n",
      "ep 3999: ep_len:1163 episode reward: total was -11.770000. running mean: -21.148426\n",
      "ep 3999: ep_len:41 episode reward: total was 19.000000. running mean: -20.746941\n",
      "ep 3999: ep_len:34 episode reward: total was 15.500000. running mean: -20.384472\n",
      "ep 3999: ep_len:1484 episode reward: total was -8.230000. running mean: -20.262927\n",
      "ep 3999: ep_len:3636 episode reward: total was -14.940000. running mean: -20.209698\n",
      "ep 3999: ep_len:653 episode reward: total was 0.300000. running mean: -20.004601\n",
      "ep 3999: ep_len:825 episode reward: total was 34.250000. running mean: -19.462055\n",
      "ep 3999: ep_len:1073 episode reward: total was 21.220000. running mean: -19.055234\n",
      "ep 3999: ep_len:60 episode reward: total was 28.500000. running mean: -18.579682\n",
      "ep 3999: ep_len:1174 episode reward: total was 6.520000. running mean: -18.328685\n",
      "ep 3999: ep_len:2906 episode reward: total was -12.950000. running mean: -18.274898\n",
      "epsilon:0.009992 episode_count: 60144. steps_count: 64862431.000000\n",
      "ep 4000: ep_len:691 episode reward: total was 25.120000. running mean: -17.840949\n",
      "ep 4000: ep_len:682 episode reward: total was -3.710000. running mean: -17.699640\n",
      "ep 4000: ep_len:2945 episode reward: total was -1235.070000. running mean: -29.873343\n",
      "ep 4000: ep_len:639 episode reward: total was -0.800000. running mean: -29.582610\n",
      "ep 4000: ep_len:52 episode reward: total was 23.000000. running mean: -29.056784\n",
      "ep 4000: ep_len:59 episode reward: total was 28.000000. running mean: -28.486216\n",
      "ep 4000: ep_len:1388 episode reward: total was -39.820000. running mean: -28.599554\n",
      "ep 4000: ep_len:618 episode reward: total was 18.320000. running mean: -28.130358\n",
      "ep 4000: ep_len:530 episode reward: total was -2.370000. running mean: -27.872755\n",
      "ep 4000: ep_len:720 episode reward: total was 51.590000. running mean: -27.078127\n",
      "ep 4000: ep_len:1137 episode reward: total was -2.940000. running mean: -26.836746\n",
      "ep 4000: ep_len:162 episode reward: total was 79.500000. running mean: -25.773378\n",
      "ep 4000: ep_len:49 episode reward: total was 23.000000. running mean: -25.285645\n",
      "ep 4000: ep_len:89 episode reward: total was 41.500000. running mean: -24.617788\n",
      "ep 4000: ep_len:503 episode reward: total was -0.060000. running mean: -24.372210\n",
      "ep 4000: ep_len:2821 episode reward: total was -1.770000. running mean: -24.146188\n",
      "ep 4000: ep_len:43 episode reward: total was 20.000000. running mean: -23.704726\n",
      "epsilon:0.009992 episode_count: 60161. steps_count: 64875559.000000\n",
      "ep 4001: ep_len:1084 episode reward: total was -5.550000. running mean: -23.523179\n",
      "ep 4001: ep_len:778 episode reward: total was -33.860000. running mean: -23.626547\n",
      "ep 4001: ep_len:82 episode reward: total was 38.000000. running mean: -23.010282\n",
      "ep 4001: ep_len:3045 episode reward: total was -72.090000. running mean: -23.501079\n",
      "ep 4001: ep_len:622 episode reward: total was -6.420000. running mean: -23.330268\n",
      "ep 4001: ep_len:78 episode reward: total was 37.500000. running mean: -22.721966\n",
      "ep 4001: ep_len:66 episode reward: total was 31.500000. running mean: -22.179746\n",
      "ep 4001: ep_len:1086 episode reward: total was 12.780000. running mean: -21.830148\n",
      "ep 4001: ep_len:329 episode reward: total was 9.470000. running mean: -21.517147\n",
      "ep 4001: ep_len:832 episode reward: total was -41.830000. running mean: -21.720276\n",
      "ep 4001: ep_len:736 episode reward: total was 1.130000. running mean: -21.491773\n",
      "ep 4001: ep_len:988 episode reward: total was 4.940000. running mean: -21.227455\n",
      "ep 4001: ep_len:1184 episode reward: total was 6.780000. running mean: -20.947380\n",
      "ep 4001: ep_len:2865 episode reward: total was 13.000000. running mean: -20.607907\n",
      "ep 4001: ep_len:38 episode reward: total was 17.500000. running mean: -20.226828\n",
      "epsilon:0.009992 episode_count: 60176. steps_count: 64889372.000000\n",
      "ep 4002: ep_len:1116 episode reward: total was -25.830000. running mean: -20.282859\n",
      "ep 4002: ep_len:753 episode reward: total was 0.090000. running mean: -20.079131\n",
      "ep 4002: ep_len:57 episode reward: total was 27.000000. running mean: -19.608339\n",
      "ep 4002: ep_len:2986 episode reward: total was -56.180000. running mean: -19.974056\n",
      "ep 4002: ep_len:902 episode reward: total was 56.680000. running mean: -19.207515\n",
      "ep 4002: ep_len:125 episode reward: total was 61.000000. running mean: -18.405440\n",
      "ep 4002: ep_len:838 episode reward: total was 26.700000. running mean: -17.954386\n",
      "ep 4002: ep_len:3638 episode reward: total was -19.850000. running mean: -17.973342\n",
      "ep 4002: ep_len:628 episode reward: total was 31.130000. running mean: -17.482309\n",
      "ep 4002: ep_len:653 episode reward: total was -30.600000. running mean: -17.613486\n",
      "ep 4002: ep_len:483 episode reward: total was 22.310000. running mean: -17.214251\n",
      "ep 4002: ep_len:169 episode reward: total was 83.000000. running mean: -16.212108\n",
      "ep 4002: ep_len:904 episode reward: total was -67.740000. running mean: -16.727387\n",
      "ep 4002: ep_len:45 episode reward: total was 21.000000. running mean: -16.350113\n",
      "epsilon:0.009992 episode_count: 60190. steps_count: 64902669.000000\n",
      "ep 4003: ep_len:1469 episode reward: total was 4.910000. running mean: -16.137512\n",
      "ep 4003: ep_len:943 episode reward: total was -5.000000. running mean: -16.026137\n",
      "ep 4003: ep_len:2949 episode reward: total was -67.850000. running mean: -16.544376\n",
      "ep 4003: ep_len:500 episode reward: total was 15.250000. running mean: -16.226432\n",
      "ep 4003: ep_len:47 episode reward: total was 22.000000. running mean: -15.844168\n",
      "ep 4003: ep_len:1484 episode reward: total was 25.360000. running mean: -15.432126\n",
      "ep 4003: ep_len:666 episode reward: total was 8.640000. running mean: -15.191405\n",
      "ep 4003: ep_len:1261 episode reward: total was -31.910000. running mean: -15.358591\n",
      "ep 4003: ep_len:632 episode reward: total was -3.640000. running mean: -15.241405\n",
      "ep 4003: ep_len:603 episode reward: total was -0.690000. running mean: -15.095891\n",
      "ep 4003: ep_len:71 episode reward: total was 34.000000. running mean: -14.604932\n",
      "ep 4003: ep_len:505 episode reward: total was 1.770000. running mean: -14.441182\n",
      "ep 4003: ep_len:2789 episode reward: total was -4.470000. running mean: -14.341471\n",
      "ep 4003: ep_len:47 episode reward: total was 22.000000. running mean: -13.978056\n",
      "epsilon:0.009992 episode_count: 60204. steps_count: 64916635.000000\n",
      "ep 4004: ep_len:933 episode reward: total was -77.270000. running mean: -14.610975\n",
      "ep 4004: ep_len:1664 episode reward: total was 0.290000. running mean: -14.461966\n",
      "ep 4004: ep_len:86 episode reward: total was 41.500000. running mean: -13.902346\n",
      "ep 4004: ep_len:794 episode reward: total was 27.910000. running mean: -13.484222\n",
      "ep 4004: ep_len:56 episode reward: total was 25.000000. running mean: -13.099380\n",
      "ep 4004: ep_len:91 episode reward: total was 44.000000. running mean: -12.528386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4004: ep_len:75 episode reward: total was 34.500000. running mean: -12.058103\n",
      "ep 4004: ep_len:500 episode reward: total was 7.540000. running mean: -11.862122\n",
      "ep 4004: ep_len:327 episode reward: total was 14.870000. running mean: -11.594800\n",
      "ep 4004: ep_len:615 episode reward: total was 6.990000. running mean: -11.408952\n",
      "ep 4004: ep_len:678 episode reward: total was 14.060000. running mean: -11.154263\n",
      "ep 4004: ep_len:500 episode reward: total was 15.740000. running mean: -10.885320\n",
      "ep 4004: ep_len:91 episode reward: total was 44.000000. running mean: -10.336467\n",
      "ep 4004: ep_len:3079 episode reward: total was -675.370000. running mean: -16.986802\n",
      "ep 4004: ep_len:2862 episode reward: total was 4.430000. running mean: -16.772634\n",
      "ep 4004: ep_len:48 episode reward: total was 22.500000. running mean: -16.379908\n",
      "epsilon:0.009992 episode_count: 60220. steps_count: 64929034.000000\n",
      "ep 4005: ep_len:655 episode reward: total was -12.570000. running mean: -16.341809\n",
      "ep 4005: ep_len:204 episode reward: total was 10.330000. running mean: -16.075091\n",
      "ep 4005: ep_len:78 episode reward: total was 37.500000. running mean: -15.539340\n",
      "ep 4005: ep_len:3004 episode reward: total was -72.020000. running mean: -16.104146\n",
      "ep 4005: ep_len:511 episode reward: total was -16.270000. running mean: -16.105805\n",
      "ep 4005: ep_len:100 episode reward: total was 47.000000. running mean: -15.474747\n",
      "ep 4005: ep_len:1460 episode reward: total was -126.970000. running mean: -16.589699\n",
      "ep 4005: ep_len:3895 episode reward: total was 20.300000. running mean: -16.220802\n",
      "ep 4005: ep_len:826 episode reward: total was -56.950000. running mean: -16.628094\n",
      "ep 4005: ep_len:621 episode reward: total was -12.020000. running mean: -16.582013\n",
      "ep 4005: ep_len:949 episode reward: total was 34.220000. running mean: -16.073993\n",
      "ep 4005: ep_len:142 episode reward: total was 65.000000. running mean: -15.263253\n",
      "ep 4005: ep_len:1040 episode reward: total was 43.510000. running mean: -14.675521\n",
      "ep 4005: ep_len:2760 episode reward: total was -20.930000. running mean: -14.738066\n",
      "ep 4005: ep_len:68 episode reward: total was 32.500000. running mean: -14.265685\n",
      "epsilon:0.009992 episode_count: 60235. steps_count: 64945347.000000\n",
      "ep 4006: ep_len:679 episode reward: total was 27.100000. running mean: -13.852028\n",
      "ep 4006: ep_len:898 episode reward: total was 1.770000. running mean: -13.695808\n",
      "ep 4006: ep_len:61 episode reward: total was 29.000000. running mean: -13.268850\n",
      "ep 4006: ep_len:101 episode reward: total was 49.000000. running mean: -12.646161\n",
      "ep 4006: ep_len:500 episode reward: total was 25.060000. running mean: -12.269100\n",
      "ep 4006: ep_len:151 episode reward: total was 69.500000. running mean: -11.451409\n",
      "ep 4006: ep_len:72 episode reward: total was 34.500000. running mean: -10.991895\n",
      "ep 4006: ep_len:500 episode reward: total was 8.270000. running mean: -10.799276\n",
      "ep 4006: ep_len:4246 episode reward: total was -105.360000. running mean: -11.744883\n",
      "ep 4006: ep_len:1218 episode reward: total was -5.890000. running mean: -11.686334\n",
      "ep 4006: ep_len:756 episode reward: total was 63.190000. running mean: -10.937571\n",
      "ep 4006: ep_len:670 episode reward: total was 32.930000. running mean: -10.498895\n",
      "ep 4006: ep_len:85 episode reward: total was 39.500000. running mean: -9.998906\n",
      "ep 4006: ep_len:157 episode reward: total was 74.000000. running mean: -9.158917\n",
      "ep 4006: ep_len:1017 episode reward: total was 11.840000. running mean: -8.948928\n",
      "ep 4006: ep_len:2842 episode reward: total was -51.870000. running mean: -9.378139\n",
      "epsilon:0.009992 episode_count: 60251. steps_count: 64959300.000000\n",
      "ep 4007: ep_len:1117 episode reward: total was 8.560000. running mean: -9.198757\n",
      "ep 4007: ep_len:926 episode reward: total was -23.320000. running mean: -9.339970\n",
      "ep 4007: ep_len:100 episode reward: total was 48.500000. running mean: -8.761570\n",
      "ep 4007: ep_len:770 episode reward: total was -13.610000. running mean: -8.810054\n",
      "ep 4007: ep_len:1406 episode reward: total was -81.050000. running mean: -9.532454\n",
      "ep 4007: ep_len:4126 episode reward: total was -49.480000. running mean: -9.931929\n",
      "ep 4007: ep_len:914 episode reward: total was -38.220000. running mean: -10.214810\n",
      "ep 4007: ep_len:633 episode reward: total was -17.070000. running mean: -10.283362\n",
      "ep 4007: ep_len:567 episode reward: total was 40.850000. running mean: -9.772028\n",
      "ep 4007: ep_len:70 episode reward: total was 32.000000. running mean: -9.354308\n",
      "ep 4007: ep_len:732 episode reward: total was -74.630000. running mean: -10.007065\n",
      "ep 4007: ep_len:45 episode reward: total was 19.500000. running mean: -9.711994\n",
      "epsilon:0.009992 episode_count: 60263. steps_count: 64970706.000000\n",
      "ep 4008: ep_len:1028 episode reward: total was -2.010000. running mean: -9.634974\n",
      "ep 4008: ep_len:681 episode reward: total was -3.460000. running mean: -9.573224\n",
      "ep 4008: ep_len:2928 episode reward: total was -166.180000. running mean: -11.139292\n",
      "ep 4008: ep_len:834 episode reward: total was 5.800000. running mean: -10.969899\n",
      "ep 4008: ep_len:29 episode reward: total was 13.000000. running mean: -10.730200\n",
      "ep 4008: ep_len:60 episode reward: total was 28.500000. running mean: -10.337898\n",
      "ep 4008: ep_len:75 episode reward: total was 36.000000. running mean: -9.874519\n",
      "ep 4008: ep_len:1071 episode reward: total was -4.180000. running mean: -9.817574\n",
      "ep 4008: ep_len:4027 episode reward: total was -81.710000. running mean: -10.536498\n",
      "ep 4008: ep_len:1552 episode reward: total was -11.990000. running mean: -10.551033\n",
      "ep 4008: ep_len:840 episode reward: total was 48.510000. running mean: -9.960423\n",
      "ep 4008: ep_len:500 episode reward: total was 9.920000. running mean: -9.761619\n",
      "ep 4008: ep_len:1071 episode reward: total was 15.530000. running mean: -9.508703\n",
      "ep 4008: ep_len:2772 episode reward: total was 0.830000. running mean: -9.405316\n",
      "epsilon:0.009992 episode_count: 60277. steps_count: 64988174.000000\n",
      "ep 4009: ep_len:500 episode reward: total was 39.960000. running mean: -8.911662\n",
      "ep 4009: ep_len:1001 episode reward: total was -12.130000. running mean: -8.943846\n",
      "ep 4009: ep_len:2997 episode reward: total was -110.190000. running mean: -9.956307\n",
      "ep 4009: ep_len:779 episode reward: total was 33.390000. running mean: -9.522844\n",
      "ep 4009: ep_len:66 episode reward: total was 30.000000. running mean: -9.127616\n",
      "ep 4009: ep_len:46 episode reward: total was 21.500000. running mean: -8.821340\n",
      "ep 4009: ep_len:673 episode reward: total was -3.540000. running mean: -8.768526\n",
      "ep 4009: ep_len:4222 episode reward: total was -1392.480000. running mean: -22.605641\n",
      "ep 4009: ep_len:1575 episode reward: total was 3.000000. running mean: -22.349585\n",
      "ep 4009: ep_len:763 episode reward: total was -28.680000. running mean: -22.412889\n",
      "ep 4009: ep_len:893 episode reward: total was 2.550000. running mean: -22.163260\n",
      "ep 4009: ep_len:160 episode reward: total was 77.000000. running mean: -21.171627\n",
      "ep 4009: ep_len:738 episode reward: total was -16.600000. running mean: -21.125911\n",
      "ep 4009: ep_len:2958 episode reward: total was 5.270000. running mean: -20.861952\n",
      "ep 4009: ep_len:52 episode reward: total was 24.500000. running mean: -20.408332\n",
      "epsilon:0.009992 episode_count: 60292. steps_count: 65005597.000000\n",
      "ep 4010: ep_len:1476 episode reward: total was 19.830000. running mean: -20.005949\n",
      "ep 4010: ep_len:756 episode reward: total was 7.190000. running mean: -19.733990\n",
      "ep 4010: ep_len:57 episode reward: total was 27.000000. running mean: -19.266650\n",
      "ep 4010: ep_len:3009 episode reward: total was -29.480000. running mean: -19.368783\n",
      "ep 4010: ep_len:656 episode reward: total was 25.480000. running mean: -18.920295\n",
      "ep 4010: ep_len:1936 episode reward: total was -76.710000. running mean: -19.498192\n",
      "ep 4010: ep_len:3976 episode reward: total was -60.200000. running mean: -19.905210\n",
      "ep 4010: ep_len:868 episode reward: total was -3.180000. running mean: -19.737958\n",
      "ep 4010: ep_len:7260 episode reward: total was 29.520000. running mean: -19.245379\n",
      "ep 4010: ep_len:880 episode reward: total was 35.270000. running mean: -18.700225\n",
      "ep 4010: ep_len:63 episode reward: total was 30.000000. running mean: -18.213223\n",
      "ep 4010: ep_len:39 episode reward: total was 18.000000. running mean: -17.851091\n",
      "ep 4010: ep_len:121 episode reward: total was 59.000000. running mean: -17.082580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4010: ep_len:1127 episode reward: total was -4.050000. running mean: -16.952254\n",
      "ep 4010: ep_len:2804 episode reward: total was -13.290000. running mean: -16.915631\n",
      "ep 4010: ep_len:45 episode reward: total was 19.500000. running mean: -16.551475\n",
      "epsilon:0.009992 episode_count: 60308. steps_count: 65030670.000000\n",
      "ep 4011: ep_len:1083 episode reward: total was 0.560000. running mean: -16.380360\n",
      "ep 4011: ep_len:810 episode reward: total was -5.370000. running mean: -16.270257\n",
      "ep 4011: ep_len:3031 episode reward: total was -73.630000. running mean: -16.843854\n",
      "ep 4011: ep_len:644 episode reward: total was 31.080000. running mean: -16.364616\n",
      "ep 4011: ep_len:150 episode reward: total was 73.500000. running mean: -15.465969\n",
      "ep 4011: ep_len:81 episode reward: total was 37.500000. running mean: -14.936310\n",
      "ep 4011: ep_len:651 episode reward: total was 19.050000. running mean: -14.596447\n",
      "ep 4011: ep_len:4115 episode reward: total was -36.510000. running mean: -14.815582\n",
      "ep 4011: ep_len:583 episode reward: total was -34.740000. running mean: -15.014826\n",
      "ep 4011: ep_len:766 episode reward: total was 16.620000. running mean: -14.698478\n",
      "ep 4011: ep_len:629 episode reward: total was -3.460000. running mean: -14.586093\n",
      "ep 4011: ep_len:575 episode reward: total was -14.620000. running mean: -14.586432\n",
      "ep 4011: ep_len:2799 episode reward: total was -53.840000. running mean: -14.978968\n",
      "ep 4011: ep_len:54 episode reward: total was 25.500000. running mean: -14.574178\n",
      "epsilon:0.009992 episode_count: 60322. steps_count: 65046641.000000\n",
      "ep 4012: ep_len:602 episode reward: total was 22.040000. running mean: -14.208037\n",
      "ep 4012: ep_len:712 episode reward: total was -5.890000. running mean: -14.124856\n",
      "ep 4012: ep_len:2901 episode reward: total was -73.570000. running mean: -14.719308\n",
      "ep 4012: ep_len:523 episode reward: total was -37.360000. running mean: -14.945715\n",
      "ep 4012: ep_len:58 episode reward: total was 27.500000. running mean: -14.521257\n",
      "ep 4012: ep_len:69 episode reward: total was 33.000000. running mean: -14.046045\n",
      "ep 4012: ep_len:1400 episode reward: total was -58.890000. running mean: -14.494484\n",
      "ep 4012: ep_len:3899 episode reward: total was -40.590000. running mean: -14.755439\n",
      "ep 4012: ep_len:1584 episode reward: total was 20.970000. running mean: -14.398185\n",
      "ep 4012: ep_len:7378 episode reward: total was 30.670000. running mean: -13.947503\n",
      "ep 4012: ep_len:699 episode reward: total was -13.380000. running mean: -13.941828\n",
      "ep 4012: ep_len:106 episode reward: total was 50.000000. running mean: -13.302410\n",
      "ep 4012: ep_len:58 episode reward: total was 26.000000. running mean: -12.909386\n",
      "ep 4012: ep_len:1432 episode reward: total was -44.960000. running mean: -13.229892\n",
      "ep 4012: ep_len:2861 episode reward: total was -25.180000. running mean: -13.349393\n",
      "ep 4012: ep_len:44 episode reward: total was 20.500000. running mean: -13.010899\n",
      "epsilon:0.009992 episode_count: 60338. steps_count: 65070967.000000\n",
      "ep 4013: ep_len:635 episode reward: total was -4.930000. running mean: -12.930090\n",
      "ep 4013: ep_len:1244 episode reward: total was -20.050000. running mean: -13.001289\n",
      "ep 4013: ep_len:2965 episode reward: total was -38.820000. running mean: -13.259476\n",
      "ep 4013: ep_len:1117 episode reward: total was -3.140000. running mean: -13.158282\n",
      "ep 4013: ep_len:62 episode reward: total was 29.500000. running mean: -12.731699\n",
      "ep 4013: ep_len:173 episode reward: total was 83.500000. running mean: -11.769382\n",
      "ep 4013: ep_len:677 episode reward: total was 10.080000. running mean: -11.550888\n",
      "ep 4013: ep_len:664 episode reward: total was 21.720000. running mean: -11.218179\n",
      "ep 4013: ep_len:2170 episode reward: total was -156.230000. running mean: -12.668297\n",
      "ep 4013: ep_len:7244 episode reward: total was 38.790000. running mean: -12.153714\n",
      "ep 4013: ep_len:692 episode reward: total was -6.380000. running mean: -12.095977\n",
      "ep 4013: ep_len:65 episode reward: total was 26.500000. running mean: -11.710017\n",
      "ep 4013: ep_len:732 episode reward: total was -55.470000. running mean: -12.147617\n",
      "ep 4013: ep_len:2929 episode reward: total was 3.960000. running mean: -11.986541\n",
      "epsilon:0.009992 episode_count: 60352. steps_count: 65092336.000000\n",
      "ep 4014: ep_len:1436 episode reward: total was 14.130000. running mean: -11.725376\n",
      "ep 4014: ep_len:677 episode reward: total was -57.090000. running mean: -12.179022\n",
      "ep 4014: ep_len:2945 episode reward: total was -57.670000. running mean: -12.633932\n",
      "ep 4014: ep_len:1463 episode reward: total was 20.950000. running mean: -12.298092\n",
      "ep 4014: ep_len:66 episode reward: total was 31.500000. running mean: -11.860111\n",
      "ep 4014: ep_len:161 episode reward: total was 77.500000. running mean: -10.966510\n",
      "ep 4014: ep_len:1094 episode reward: total was -3.370000. running mean: -10.890545\n",
      "ep 4014: ep_len:361 episode reward: total was 12.700000. running mean: -10.654640\n",
      "ep 4014: ep_len:1145 episode reward: total was -21.590000. running mean: -10.763993\n",
      "ep 4014: ep_len:779 episode reward: total was 32.830000. running mean: -10.328053\n",
      "ep 4014: ep_len:500 episode reward: total was -2.200000. running mean: -10.246773\n",
      "ep 4014: ep_len:668 episode reward: total was -9.050000. running mean: -10.234805\n",
      "ep 4014: ep_len:2849 episode reward: total was -25.910000. running mean: -10.391557\n",
      "epsilon:0.009992 episode_count: 60365. steps_count: 65106480.000000\n",
      "ep 4015: ep_len:968 episode reward: total was -19.640000. running mean: -10.484042\n",
      "ep 4015: ep_len:639 episode reward: total was -0.850000. running mean: -10.387701\n",
      "ep 4015: ep_len:2989 episode reward: total was -36.870000. running mean: -10.652524\n",
      "ep 4015: ep_len:500 episode reward: total was 17.580000. running mean: -10.370199\n",
      "ep 4015: ep_len:51 episode reward: total was 24.000000. running mean: -10.026497\n",
      "ep 4015: ep_len:104 episode reward: total was 49.000000. running mean: -9.436232\n",
      "ep 4015: ep_len:93 episode reward: total was 43.500000. running mean: -8.906870\n",
      "ep 4015: ep_len:657 episode reward: total was 3.370000. running mean: -8.784101\n",
      "ep 4015: ep_len:3817 episode reward: total was -78.880000. running mean: -9.485060\n",
      "ep 4015: ep_len:1219 episode reward: total was 27.520000. running mean: -9.115009\n",
      "ep 4015: ep_len:801 episode reward: total was 4.180000. running mean: -8.982059\n",
      "ep 4015: ep_len:640 episode reward: total was 10.980000. running mean: -8.782439\n",
      "ep 4015: ep_len:120 episode reward: total was 57.000000. running mean: -8.124614\n",
      "ep 4015: ep_len:65 episode reward: total was 31.000000. running mean: -7.733368\n",
      "ep 4015: ep_len:766 episode reward: total was 5.480000. running mean: -7.601234\n",
      "ep 4015: ep_len:40 episode reward: total was 18.500000. running mean: -7.340222\n",
      "epsilon:0.009992 episode_count: 60381. steps_count: 65119949.000000\n",
      "ep 4016: ep_len:719 episode reward: total was -14.920000. running mean: -7.416020\n",
      "ep 4016: ep_len:790 episode reward: total was -8.550000. running mean: -7.427360\n",
      "ep 4016: ep_len:2940 episode reward: total was -58.520000. running mean: -7.938286\n",
      "ep 4016: ep_len:851 episode reward: total was 32.680000. running mean: -7.532103\n",
      "ep 4016: ep_len:57 episode reward: total was 27.000000. running mean: -7.186782\n",
      "ep 4016: ep_len:634 episode reward: total was -10.480000. running mean: -7.219714\n",
      "ep 4016: ep_len:617 episode reward: total was 21.860000. running mean: -6.928917\n",
      "ep 4016: ep_len:629 episode reward: total was -23.320000. running mean: -7.092828\n",
      "ep 4016: ep_len:713 episode reward: total was 47.020000. running mean: -6.551700\n",
      "ep 4016: ep_len:639 episode reward: total was 4.160000. running mean: -6.444583\n",
      "ep 4016: ep_len:57 episode reward: total was 27.000000. running mean: -6.110137\n",
      "ep 4016: ep_len:1151 episode reward: total was 24.180000. running mean: -5.807236\n",
      "ep 4016: ep_len:2872 episode reward: total was -0.130000. running mean: -5.750463\n",
      "epsilon:0.009992 episode_count: 60394. steps_count: 65132618.000000\n",
      "ep 4017: ep_len:683 episode reward: total was -3.370000. running mean: -5.726659\n",
      "ep 4017: ep_len:1615 episode reward: total was -43.330000. running mean: -6.102692\n",
      "ep 4017: ep_len:3104 episode reward: total was -9.510000. running mean: -6.136765\n",
      "ep 4017: ep_len:1680 episode reward: total was -326.740000. running mean: -9.342797\n",
      "ep 4017: ep_len:62 episode reward: total was 28.000000. running mean: -8.969369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4017: ep_len:73 episode reward: total was 35.000000. running mean: -8.529676\n",
      "ep 4017: ep_len:60 episode reward: total was 28.500000. running mean: -8.159379\n",
      "ep 4017: ep_len:1518 episode reward: total was 13.450000. running mean: -7.943285\n",
      "ep 4017: ep_len:3543 episode reward: total was -59.790000. running mean: -8.461752\n",
      "ep 4017: ep_len:568 episode reward: total was -7.620000. running mean: -8.453335\n",
      "ep 4017: ep_len:7272 episode reward: total was -56.210000. running mean: -8.930901\n",
      "ep 4017: ep_len:1078 episode reward: total was -7.940000. running mean: -8.920992\n",
      "ep 4017: ep_len:166 episode reward: total was 80.000000. running mean: -8.031783\n",
      "ep 4017: ep_len:55 episode reward: total was 24.500000. running mean: -7.706465\n",
      "ep 4017: ep_len:1512 episode reward: total was 6.840000. running mean: -7.561000\n",
      "ep 4017: ep_len:2858 episode reward: total was -1.120000. running mean: -7.496590\n",
      "ep 4017: ep_len:45 episode reward: total was 21.000000. running mean: -7.211624\n",
      "epsilon:0.009992 episode_count: 60411. steps_count: 65158510.000000\n",
      "ep 4018: ep_len:1426 episode reward: total was -4.040000. running mean: -7.179908\n",
      "ep 4018: ep_len:168 episode reward: total was 10.120000. running mean: -7.006909\n",
      "ep 4018: ep_len:2954 episode reward: total was -5.300000. running mean: -6.989840\n",
      "ep 4018: ep_len:829 episode reward: total was -24.200000. running mean: -7.161941\n",
      "ep 4018: ep_len:46 episode reward: total was 21.500000. running mean: -6.875322\n",
      "ep 4018: ep_len:890 episode reward: total was 36.830000. running mean: -6.438269\n",
      "ep 4018: ep_len:608 episode reward: total was 22.230000. running mean: -6.151586\n",
      "ep 4018: ep_len:550 episode reward: total was -50.590000. running mean: -6.595970\n",
      "ep 4018: ep_len:878 episode reward: total was 59.390000. running mean: -5.936110\n",
      "ep 4018: ep_len:599 episode reward: total was 59.420000. running mean: -5.282549\n",
      "ep 4018: ep_len:52 episode reward: total was 23.000000. running mean: -4.999724\n",
      "ep 4018: ep_len:207 episode reward: total was 102.000000. running mean: -3.929727\n",
      "ep 4018: ep_len:68 episode reward: total was 31.000000. running mean: -3.580429\n",
      "ep 4018: ep_len:801 episode reward: total was 29.410000. running mean: -3.250525\n",
      "ep 4018: ep_len:2918 episode reward: total was -1.740000. running mean: -3.235420\n",
      "epsilon:0.009992 episode_count: 60426. steps_count: 65171504.000000\n",
      "ep 4019: ep_len:581 episode reward: total was -24.110000. running mean: -3.444166\n",
      "ep 4019: ep_len:649 episode reward: total was -24.990000. running mean: -3.659624\n",
      "ep 4019: ep_len:2836 episode reward: total was -31.370000. running mean: -3.936728\n",
      "ep 4019: ep_len:500 episode reward: total was 11.690000. running mean: -3.780460\n",
      "ep 4019: ep_len:55 episode reward: total was 21.500000. running mean: -3.527656\n",
      "ep 4019: ep_len:53 episode reward: total was 23.500000. running mean: -3.257379\n",
      "ep 4019: ep_len:908 episode reward: total was 54.040000. running mean: -2.684405\n",
      "ep 4019: ep_len:660 episode reward: total was 2.180000. running mean: -2.635761\n",
      "ep 4019: ep_len:1197 episode reward: total was -10.970000. running mean: -2.719104\n",
      "ep 4019: ep_len:877 episode reward: total was 74.570000. running mean: -1.946213\n",
      "ep 4019: ep_len:500 episode reward: total was -3.420000. running mean: -1.960951\n",
      "ep 4019: ep_len:755 episode reward: total was -29.640000. running mean: -2.237741\n",
      "ep 4019: ep_len:2803 episode reward: total was -5.870000. running mean: -2.274064\n",
      "epsilon:0.009992 episode_count: 60439. steps_count: 65183878.000000\n",
      "ep 4020: ep_len:718 episode reward: total was -8.140000. running mean: -2.332723\n",
      "ep 4020: ep_len:1664 episode reward: total was -391.880000. running mean: -6.228196\n",
      "ep 4020: ep_len:2817 episode reward: total was -22.160000. running mean: -6.387514\n",
      "ep 4020: ep_len:1426 episode reward: total was -731.290000. running mean: -13.636539\n",
      "ep 4020: ep_len:60 episode reward: total was 28.500000. running mean: -13.215173\n",
      "ep 4020: ep_len:856 episode reward: total was 46.570000. running mean: -12.617322\n",
      "ep 4020: ep_len:3829 episode reward: total was 7.670000. running mean: -12.414448\n",
      "ep 4020: ep_len:2061 episode reward: total was -578.400000. running mean: -18.074304\n",
      "ep 4020: ep_len:648 episode reward: total was 17.800000. running mean: -17.715561\n",
      "ep 4020: ep_len:755 episode reward: total was 3.500000. running mean: -17.503405\n",
      "ep 4020: ep_len:91 episode reward: total was 42.500000. running mean: -16.903371\n",
      "ep 4020: ep_len:145 episode reward: total was 66.500000. running mean: -16.069338\n",
      "ep 4020: ep_len:94 episode reward: total was 42.500000. running mean: -15.483644\n",
      "ep 4020: ep_len:1180 episode reward: total was -9.580000. running mean: -15.424608\n",
      "ep 4020: ep_len:2857 episode reward: total was -35.080000. running mean: -15.621162\n",
      "epsilon:0.009992 episode_count: 60454. steps_count: 65203079.000000\n",
      "ep 4021: ep_len:500 episode reward: total was 13.160000. running mean: -15.333350\n",
      "ep 4021: ep_len:994 episode reward: total was 26.210000. running mean: -14.917917\n",
      "ep 4021: ep_len:2941 episode reward: total was -15.410000. running mean: -14.922837\n",
      "ep 4021: ep_len:537 episode reward: total was 5.640000. running mean: -14.717209\n",
      "ep 4021: ep_len:867 episode reward: total was 37.370000. running mean: -14.196337\n",
      "ep 4021: ep_len:632 episode reward: total was 26.880000. running mean: -13.785574\n",
      "ep 4021: ep_len:529 episode reward: total was 1.630000. running mean: -13.631418\n",
      "ep 4021: ep_len:621 episode reward: total was 4.480000. running mean: -13.450304\n",
      "ep 4021: ep_len:1022 episode reward: total was -67.840000. running mean: -13.994201\n",
      "ep 4021: ep_len:59 episode reward: total was 26.500000. running mean: -13.589259\n",
      "ep 4021: ep_len:66 episode reward: total was 31.500000. running mean: -13.138366\n",
      "ep 4021: ep_len:113 episode reward: total was 52.000000. running mean: -12.486982\n",
      "ep 4021: ep_len:762 episode reward: total was -6.690000. running mean: -12.429012\n",
      "ep 4021: ep_len:2885 episode reward: total was -39.850000. running mean: -12.703222\n",
      "ep 4021: ep_len:69 episode reward: total was 28.500000. running mean: -12.291190\n",
      "epsilon:0.009992 episode_count: 60469. steps_count: 65215676.000000\n",
      "ep 4022: ep_len:1104 episode reward: total was 4.140000. running mean: -12.126878\n",
      "ep 4022: ep_len:500 episode reward: total was 14.390000. running mean: -11.861709\n",
      "ep 4022: ep_len:2983 episode reward: total was 1.060000. running mean: -11.732492\n",
      "ep 4022: ep_len:4273 episode reward: total was -1275.490000. running mean: -24.370067\n",
      "ep 4022: ep_len:136 episode reward: total was 62.000000. running mean: -23.506367\n",
      "ep 4022: ep_len:95 episode reward: total was 43.000000. running mean: -22.841303\n",
      "ep 4022: ep_len:73 episode reward: total was 33.500000. running mean: -22.277890\n",
      "ep 4022: ep_len:1072 episode reward: total was -137.870000. running mean: -23.433811\n",
      "ep 4022: ep_len:666 episode reward: total was 20.730000. running mean: -22.992173\n",
      "ep 4022: ep_len:1623 episode reward: total was 0.910000. running mean: -22.753151\n",
      "ep 4022: ep_len:883 episode reward: total was 62.660000. running mean: -21.899020\n",
      "ep 4022: ep_len:1077 episode reward: total was -5.560000. running mean: -21.735630\n",
      "ep 4022: ep_len:31 episode reward: total was 14.000000. running mean: -21.378273\n",
      "ep 4022: ep_len:674 episode reward: total was 50.440000. running mean: -20.660091\n",
      "ep 4022: ep_len:2870 episode reward: total was -21.780000. running mean: -20.671290\n",
      "ep 4022: ep_len:68 episode reward: total was 32.500000. running mean: -20.139577\n",
      "epsilon:0.009992 episode_count: 60485. steps_count: 65233804.000000\n",
      "ep 4023: ep_len:671 episode reward: total was -42.800000. running mean: -20.366181\n",
      "ep 4023: ep_len:500 episode reward: total was -20.130000. running mean: -20.363819\n",
      "ep 4023: ep_len:3080 episode reward: total was -47.900000. running mean: -20.639181\n",
      "ep 4023: ep_len:668 episode reward: total was -9.940000. running mean: -20.532189\n",
      "ep 4023: ep_len:53 episode reward: total was 25.000000. running mean: -20.076867\n",
      "ep 4023: ep_len:55 episode reward: total was 26.000000. running mean: -19.616099\n",
      "ep 4023: ep_len:858 episode reward: total was 29.100000. running mean: -19.128938\n",
      "ep 4023: ep_len:3677 episode reward: total was -287.230000. running mean: -21.809948\n",
      "ep 4023: ep_len:936 episode reward: total was -29.190000. running mean: -21.883749\n",
      "ep 4023: ep_len:730 episode reward: total was 30.350000. running mean: -21.361411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4023: ep_len:1081 episode reward: total was 43.030000. running mean: -20.717497\n",
      "ep 4023: ep_len:154 episode reward: total was 72.500000. running mean: -19.785322\n",
      "ep 4023: ep_len:665 episode reward: total was -22.740000. running mean: -19.814869\n",
      "ep 4023: ep_len:2791 episode reward: total was -25.420000. running mean: -19.870920\n",
      "epsilon:0.009992 episode_count: 60499. steps_count: 65249723.000000\n",
      "ep 4024: ep_len:1076 episode reward: total was -9.000000. running mean: -19.762211\n",
      "ep 4024: ep_len:729 episode reward: total was -41.590000. running mean: -19.980489\n",
      "ep 4024: ep_len:2966 episode reward: total was -12.340000. running mean: -19.904084\n",
      "ep 4024: ep_len:1114 episode reward: total was -5.190000. running mean: -19.756943\n",
      "ep 4024: ep_len:110 episode reward: total was 50.500000. running mean: -19.054374\n",
      "ep 4024: ep_len:741 episode reward: total was -7.910000. running mean: -18.942930\n",
      "ep 4024: ep_len:3864 episode reward: total was 11.390000. running mean: -18.639601\n",
      "ep 4024: ep_len:554 episode reward: total was 0.320000. running mean: -18.450005\n",
      "ep 4024: ep_len:7260 episode reward: total was 56.760000. running mean: -17.697905\n",
      "ep 4024: ep_len:720 episode reward: total was 4.560000. running mean: -17.475326\n",
      "ep 4024: ep_len:132 episode reward: total was 64.500000. running mean: -16.655572\n",
      "ep 4024: ep_len:104 episode reward: total was 47.500000. running mean: -16.014017\n",
      "ep 4024: ep_len:1093 episode reward: total was 0.660000. running mean: -15.847277\n",
      "ep 4024: ep_len:2776 episode reward: total was -5.460000. running mean: -15.743404\n",
      "ep 4024: ep_len:60 episode reward: total was 28.500000. running mean: -15.300970\n",
      "epsilon:0.009992 episode_count: 60514. steps_count: 65273022.000000\n",
      "ep 4025: ep_len:977 episode reward: total was -156.040000. running mean: -16.708360\n",
      "ep 4025: ep_len:1627 episode reward: total was -45.350000. running mean: -16.994776\n",
      "ep 4025: ep_len:2991 episode reward: total was -1.830000. running mean: -16.843129\n",
      "ep 4025: ep_len:655 episode reward: total was 0.320000. running mean: -16.671497\n",
      "ep 4025: ep_len:51 episode reward: total was 22.500000. running mean: -16.279782\n",
      "ep 4025: ep_len:160 episode reward: total was 75.500000. running mean: -15.361985\n",
      "ep 4025: ep_len:70 episode reward: total was 32.000000. running mean: -14.888365\n",
      "ep 4025: ep_len:500 episode reward: total was 53.710000. running mean: -14.202381\n",
      "ep 4025: ep_len:3830 episode reward: total was -29.540000. running mean: -14.355757\n",
      "ep 4025: ep_len:1220 episode reward: total was -50.590000. running mean: -14.718100\n",
      "ep 4025: ep_len:7279 episode reward: total was 37.700000. running mean: -14.193919\n",
      "ep 4025: ep_len:1117 episode reward: total was 0.900000. running mean: -14.042980\n",
      "ep 4025: ep_len:73 episode reward: total was 33.500000. running mean: -13.567550\n",
      "ep 4025: ep_len:42 episode reward: total was 19.500000. running mean: -13.236874\n",
      "ep 4025: ep_len:792 episode reward: total was -17.710000. running mean: -13.281605\n",
      "ep 4025: ep_len:3005 episode reward: total was -1139.530000. running mean: -24.544089\n",
      "epsilon:0.009992 episode_count: 60530. steps_count: 65297411.000000\n",
      "ep 4026: ep_len:973 episode reward: total was -24.970000. running mean: -24.548349\n",
      "ep 4026: ep_len:761 episode reward: total was -1.580000. running mean: -24.318665\n",
      "ep 4026: ep_len:2971 episode reward: total was 20.530000. running mean: -23.870178\n",
      "ep 4026: ep_len:602 episode reward: total was 13.620000. running mean: -23.495277\n",
      "ep 4026: ep_len:51 episode reward: total was 24.000000. running mean: -23.020324\n",
      "ep 4026: ep_len:158 episode reward: total was 76.000000. running mean: -22.030121\n",
      "ep 4026: ep_len:102 episode reward: total was 48.000000. running mean: -21.329819\n",
      "ep 4026: ep_len:1373 episode reward: total was 13.620000. running mean: -20.980321\n",
      "ep 4026: ep_len:341 episode reward: total was 12.990000. running mean: -20.640618\n",
      "ep 4026: ep_len:588 episode reward: total was -37.720000. running mean: -20.811412\n",
      "ep 4026: ep_len:7423 episode reward: total was 18.210000. running mean: -20.421198\n",
      "ep 4026: ep_len:612 episode reward: total was 4.520000. running mean: -20.171786\n",
      "ep 4026: ep_len:107 episode reward: total was 52.000000. running mean: -19.450068\n",
      "ep 4026: ep_len:50 episode reward: total was 23.500000. running mean: -19.020567\n",
      "ep 4026: ep_len:500 episode reward: total was -3.910000. running mean: -18.869462\n",
      "ep 4026: ep_len:2815 episode reward: total was 17.210000. running mean: -18.508667\n",
      "epsilon:0.009992 episode_count: 60546. steps_count: 65316838.000000\n",
      "ep 4027: ep_len:847 episode reward: total was 16.010000. running mean: -18.163480\n",
      "ep 4027: ep_len:690 episode reward: total was 14.910000. running mean: -17.832745\n",
      "ep 4027: ep_len:2965 episode reward: total was -29.680000. running mean: -17.951218\n",
      "ep 4027: ep_len:622 episode reward: total was 4.030000. running mean: -17.731406\n",
      "ep 4027: ep_len:37 episode reward: total was 17.000000. running mean: -17.384092\n",
      "ep 4027: ep_len:1363 episode reward: total was -63.000000. running mean: -17.840251\n",
      "ep 4027: ep_len:683 episode reward: total was 8.590000. running mean: -17.575948\n",
      "ep 4027: ep_len:1122 episode reward: total was -26.050000. running mean: -17.660689\n",
      "ep 4027: ep_len:864 episode reward: total was 72.020000. running mean: -16.763882\n",
      "ep 4027: ep_len:675 episode reward: total was -9.130000. running mean: -16.687543\n",
      "ep 4027: ep_len:85 episode reward: total was 41.000000. running mean: -16.110668\n",
      "ep 4027: ep_len:100 episode reward: total was 48.500000. running mean: -15.464561\n",
      "ep 4027: ep_len:688 episode reward: total was -4.020000. running mean: -15.350115\n",
      "ep 4027: ep_len:2878 episode reward: total was 2.510000. running mean: -15.171514\n",
      "epsilon:0.009992 episode_count: 60560. steps_count: 65330457.000000\n",
      "ep 4028: ep_len:676 episode reward: total was -0.480000. running mean: -15.024599\n",
      "ep 4028: ep_len:500 episode reward: total was 2.670000. running mean: -14.847653\n",
      "ep 4028: ep_len:48 episode reward: total was 21.000000. running mean: -14.489177\n",
      "ep 4028: ep_len:2848 episode reward: total was -7.470000. running mean: -14.418985\n",
      "ep 4028: ep_len:559 episode reward: total was -90.530000. running mean: -15.180095\n",
      "ep 4028: ep_len:791 episode reward: total was 16.920000. running mean: -14.859094\n",
      "ep 4028: ep_len:3834 episode reward: total was -347.770000. running mean: -18.188203\n",
      "ep 4028: ep_len:780 episode reward: total was -3.880000. running mean: -18.045121\n",
      "ep 4028: ep_len:647 episode reward: total was 0.610000. running mean: -17.858570\n",
      "ep 4028: ep_len:500 episode reward: total was 4.780000. running mean: -17.632184\n",
      "ep 4028: ep_len:54 episode reward: total was 25.500000. running mean: -17.200862\n",
      "ep 4028: ep_len:998 episode reward: total was -62.700000. running mean: -17.655854\n",
      "ep 4028: ep_len:2851 episode reward: total was -16.590000. running mean: -17.645195\n",
      "ep 4028: ep_len:47 episode reward: total was 20.500000. running mean: -17.263743\n",
      "epsilon:0.009992 episode_count: 60574. steps_count: 65345590.000000\n",
      "ep 4029: ep_len:715 episode reward: total was -2.380000. running mean: -17.114906\n",
      "ep 4029: ep_len:983 episode reward: total was 33.630000. running mean: -16.607457\n",
      "ep 4029: ep_len:66 episode reward: total was 30.000000. running mean: -16.141382\n",
      "ep 4029: ep_len:3083 episode reward: total was 15.900000. running mean: -15.820968\n",
      "ep 4029: ep_len:4242 episode reward: total was -861.700000. running mean: -24.279759\n",
      "ep 4029: ep_len:156 episode reward: total was 75.000000. running mean: -23.286961\n",
      "ep 4029: ep_len:66 episode reward: total was 30.000000. running mean: -22.754091\n",
      "ep 4029: ep_len:46 episode reward: total was 21.500000. running mean: -22.311551\n",
      "ep 4029: ep_len:603 episode reward: total was 40.670000. running mean: -21.681735\n",
      "ep 4029: ep_len:3629 episode reward: total was 12.660000. running mean: -21.338318\n",
      "ep 4029: ep_len:1288 episode reward: total was -72.130000. running mean: -21.846234\n",
      "ep 4029: ep_len:627 episode reward: total was 11.310000. running mean: -21.514672\n",
      "ep 4029: ep_len:653 episode reward: total was -5.760000. running mean: -21.357125\n",
      "ep 4029: ep_len:180 episode reward: total was 87.000000. running mean: -20.273554\n",
      "ep 4029: ep_len:127 episode reward: total was 62.000000. running mean: -19.450819\n",
      "ep 4029: ep_len:1218 episode reward: total was -3.100000. running mean: -19.287310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4029: ep_len:2875 episode reward: total was -0.150000. running mean: -19.095937\n",
      "epsilon:0.009992 episode_count: 60591. steps_count: 65366147.000000\n",
      "ep 4030: ep_len:1136 episode reward: total was 21.090000. running mean: -18.694078\n",
      "ep 4030: ep_len:1663 episode reward: total was -42.150000. running mean: -18.928637\n",
      "ep 4030: ep_len:3010 episode reward: total was -16.900000. running mean: -18.908351\n",
      "ep 4030: ep_len:796 episode reward: total was 30.960000. running mean: -18.409667\n",
      "ep 4030: ep_len:95 episode reward: total was 44.500000. running mean: -17.780571\n",
      "ep 4030: ep_len:45 episode reward: total was 21.000000. running mean: -17.392765\n",
      "ep 4030: ep_len:679 episode reward: total was -5.690000. running mean: -17.275737\n",
      "ep 4030: ep_len:345 episode reward: total was 13.640000. running mean: -16.966580\n",
      "ep 4030: ep_len:532 episode reward: total was 0.100000. running mean: -16.795914\n",
      "ep 4030: ep_len:694 episode reward: total was 43.430000. running mean: -16.193655\n",
      "ep 4030: ep_len:702 episode reward: total was 27.800000. running mean: -15.753718\n",
      "ep 4030: ep_len:146 episode reward: total was 70.000000. running mean: -14.896181\n",
      "ep 4030: ep_len:55 episode reward: total was 24.500000. running mean: -14.502219\n",
      "ep 4030: ep_len:658 episode reward: total was -5.940000. running mean: -14.416597\n",
      "ep 4030: ep_len:2830 episode reward: total was -9.270000. running mean: -14.365131\n",
      "epsilon:0.009992 episode_count: 60606. steps_count: 65379533.000000\n",
      "ep 4031: ep_len:618 episode reward: total was -34.760000. running mean: -14.569080\n",
      "ep 4031: ep_len:739 episode reward: total was -201.280000. running mean: -16.436189\n",
      "ep 4031: ep_len:2988 episode reward: total was -31.290000. running mean: -16.584727\n",
      "ep 4031: ep_len:637 episode reward: total was -5.040000. running mean: -16.469280\n",
      "ep 4031: ep_len:500 episode reward: total was 28.670000. running mean: -16.017887\n",
      "ep 4031: ep_len:3756 episode reward: total was -197.560000. running mean: -17.833308\n",
      "ep 4031: ep_len:593 episode reward: total was 20.680000. running mean: -17.448175\n",
      "ep 4031: ep_len:622 episode reward: total was -2.400000. running mean: -17.297693\n",
      "ep 4031: ep_len:593 episode reward: total was 0.710000. running mean: -17.117617\n",
      "ep 4031: ep_len:206 episode reward: total was 101.500000. running mean: -15.931440\n",
      "ep 4031: ep_len:500 episode reward: total was 3.980000. running mean: -15.732326\n",
      "ep 4031: ep_len:2817 episode reward: total was 1.100000. running mean: -15.564003\n",
      "epsilon:0.009992 episode_count: 60618. steps_count: 65394102.000000\n",
      "ep 4032: ep_len:1147 episode reward: total was -0.820000. running mean: -15.416563\n",
      "ep 4032: ep_len:1137 episode reward: total was -105.580000. running mean: -16.318197\n",
      "ep 4032: ep_len:79 episode reward: total was 36.500000. running mean: -15.790015\n",
      "ep 4032: ep_len:3003 episode reward: total was -7.310000. running mean: -15.705215\n",
      "ep 4032: ep_len:4715 episode reward: total was -1724.560000. running mean: -32.793763\n",
      "ep 4032: ep_len:500 episode reward: total was 38.980000. running mean: -32.076025\n",
      "ep 4032: ep_len:3598 episode reward: total was -67.840000. running mean: -32.433665\n",
      "ep 4032: ep_len:602 episode reward: total was 12.950000. running mean: -31.979828\n",
      "ep 4032: ep_len:667 episode reward: total was -4.880000. running mean: -31.708830\n",
      "ep 4032: ep_len:672 episode reward: total was 3.860000. running mean: -31.353142\n",
      "ep 4032: ep_len:69 episode reward: total was 31.500000. running mean: -30.724610\n",
      "ep 4032: ep_len:804 episode reward: total was 3.440000. running mean: -30.382964\n",
      "ep 4032: ep_len:2840 episode reward: total was 1.150000. running mean: -30.067635\n",
      "ep 4032: ep_len:48 episode reward: total was 21.000000. running mean: -29.556958\n",
      "epsilon:0.009992 episode_count: 60632. steps_count: 65413983.000000\n",
      "ep 4033: ep_len:2553 episode reward: total was -286.730000. running mean: -32.128689\n",
      "ep 4033: ep_len:500 episode reward: total was 23.150000. running mean: -31.575902\n",
      "ep 4033: ep_len:3019 episode reward: total was 24.410000. running mean: -31.016043\n",
      "ep 4033: ep_len:634 episode reward: total was -46.670000. running mean: -31.172582\n",
      "ep 4033: ep_len:48 episode reward: total was 22.500000. running mean: -30.635856\n",
      "ep 4033: ep_len:156 episode reward: total was 75.000000. running mean: -29.579498\n",
      "ep 4033: ep_len:62 episode reward: total was 29.500000. running mean: -28.988703\n",
      "ep 4033: ep_len:743 episode reward: total was -0.820000. running mean: -28.707016\n",
      "ep 4033: ep_len:3681 episode reward: total was -24.310000. running mean: -28.663046\n",
      "ep 4033: ep_len:592 episode reward: total was 8.510000. running mean: -28.291315\n",
      "ep 4033: ep_len:823 episode reward: total was 31.100000. running mean: -27.697402\n",
      "ep 4033: ep_len:500 episode reward: total was 6.950000. running mean: -27.350928\n",
      "ep 4033: ep_len:76 episode reward: total was 36.500000. running mean: -26.712419\n",
      "ep 4033: ep_len:692 episode reward: total was -4.360000. running mean: -26.488895\n",
      "ep 4033: ep_len:2865 episode reward: total was 2.380000. running mean: -26.200206\n",
      "epsilon:0.009992 episode_count: 60647. steps_count: 65430927.000000\n",
      "ep 4034: ep_len:1669 episode reward: total was -1113.090000. running mean: -37.069104\n",
      "ep 4034: ep_len:674 episode reward: total was -44.940000. running mean: -37.147813\n",
      "ep 4034: ep_len:46 episode reward: total was 20.000000. running mean: -36.576334\n",
      "ep 4034: ep_len:2960 episode reward: total was -15.500000. running mean: -36.365571\n",
      "ep 4034: ep_len:575 episode reward: total was 17.230000. running mean: -35.829615\n",
      "ep 4034: ep_len:500 episode reward: total was 34.850000. running mean: -35.122819\n",
      "ep 4034: ep_len:3592 episode reward: total was -100.560000. running mean: -35.777191\n",
      "ep 4034: ep_len:535 episode reward: total was -29.160000. running mean: -35.711019\n",
      "ep 4034: ep_len:806 episode reward: total was 18.770000. running mean: -35.166209\n",
      "ep 4034: ep_len:1006 episode reward: total was 15.740000. running mean: -34.657147\n",
      "ep 4034: ep_len:82 episode reward: total was 35.000000. running mean: -33.960575\n",
      "ep 4034: ep_len:500 episode reward: total was 21.660000. running mean: -33.404370\n",
      "ep 4034: ep_len:2834 episode reward: total was 7.420000. running mean: -32.996126\n",
      "epsilon:0.009992 episode_count: 60660. steps_count: 65446706.000000\n",
      "ep 4035: ep_len:747 episode reward: total was -78.550000. running mean: -33.451665\n",
      "ep 4035: ep_len:977 episode reward: total was -88.370000. running mean: -34.000848\n",
      "ep 4035: ep_len:48 episode reward: total was 21.000000. running mean: -33.450840\n",
      "ep 4035: ep_len:3020 episode reward: total was -9.340000. running mean: -33.209731\n",
      "ep 4035: ep_len:904 episode reward: total was 69.770000. running mean: -32.179934\n",
      "ep 4035: ep_len:91 episode reward: total was 41.000000. running mean: -31.448134\n",
      "ep 4035: ep_len:894 episode reward: total was 49.210000. running mean: -30.641553\n",
      "ep 4035: ep_len:639 episode reward: total was 13.970000. running mean: -30.195438\n",
      "ep 4035: ep_len:609 episode reward: total was 8.950000. running mean: -29.803983\n",
      "ep 4035: ep_len:840 episode reward: total was 46.430000. running mean: -29.041643\n",
      "ep 4035: ep_len:500 episode reward: total was 14.210000. running mean: -28.609127\n",
      "ep 4035: ep_len:74 episode reward: total was 35.500000. running mean: -27.968036\n",
      "ep 4035: ep_len:63 episode reward: total was 30.000000. running mean: -27.388355\n",
      "ep 4035: ep_len:564 episode reward: total was -16.750000. running mean: -27.281972\n",
      "ep 4035: ep_len:2806 episode reward: total was -6.880000. running mean: -27.077952\n",
      "epsilon:0.009992 episode_count: 60675. steps_count: 65459482.000000\n",
      "ep 4036: ep_len:624 episode reward: total was 2.270000. running mean: -26.784473\n",
      "ep 4036: ep_len:747 episode reward: total was -59.540000. running mean: -27.112028\n",
      "ep 4036: ep_len:2834 episode reward: total was -51.530000. running mean: -27.356208\n",
      "ep 4036: ep_len:542 episode reward: total was -61.410000. running mean: -27.696745\n",
      "ep 4036: ep_len:53 episode reward: total was 25.000000. running mean: -27.169778\n",
      "ep 4036: ep_len:72 episode reward: total was 31.500000. running mean: -26.583080\n",
      "ep 4036: ep_len:582 episode reward: total was 27.420000. running mean: -26.043049\n",
      "ep 4036: ep_len:638 episode reward: total was 14.510000. running mean: -25.637519\n",
      "ep 4036: ep_len:634 episode reward: total was 19.090000. running mean: -25.190244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4036: ep_len:666 episode reward: total was -5.750000. running mean: -24.995841\n",
      "ep 4036: ep_len:1474 episode reward: total was -25.130000. running mean: -24.997183\n",
      "ep 4036: ep_len:177 episode reward: total was 85.500000. running mean: -23.892211\n",
      "ep 4036: ep_len:32 episode reward: total was 14.500000. running mean: -23.508289\n",
      "ep 4036: ep_len:2443 episode reward: total was -1520.630000. running mean: -38.479506\n",
      "ep 4036: ep_len:2807 episode reward: total was -35.240000. running mean: -38.447111\n",
      "ep 4036: ep_len:45 episode reward: total was 19.500000. running mean: -37.867640\n",
      "epsilon:0.009992 episode_count: 60691. steps_count: 65473852.000000\n",
      "ep 4037: ep_len:1060 episode reward: total was -2.700000. running mean: -37.515964\n",
      "ep 4037: ep_len:500 episode reward: total was 26.240000. running mean: -36.878404\n",
      "ep 4037: ep_len:3067 episode reward: total was -44.190000. running mean: -36.951520\n",
      "ep 4037: ep_len:845 episode reward: total was -45.490000. running mean: -37.036905\n",
      "ep 4037: ep_len:500 episode reward: total was 29.560000. running mean: -36.370936\n",
      "ep 4037: ep_len:302 episode reward: total was 19.670000. running mean: -35.810526\n",
      "ep 4037: ep_len:1570 episode reward: total was -8.770000. running mean: -35.540121\n",
      "ep 4037: ep_len:647 episode reward: total was -3.400000. running mean: -35.218720\n",
      "ep 4037: ep_len:992 episode reward: total was -17.180000. running mean: -35.038333\n",
      "ep 4037: ep_len:608 episode reward: total was 6.700000. running mean: -34.620949\n",
      "ep 4037: ep_len:2902 episode reward: total was -2.700000. running mean: -34.301740\n",
      "ep 4037: ep_len:36 episode reward: total was 13.500000. running mean: -33.823722\n",
      "epsilon:0.009992 episode_count: 60703. steps_count: 65486881.000000\n",
      "ep 4038: ep_len:680 episode reward: total was -17.540000. running mean: -33.660885\n",
      "ep 4038: ep_len:801 episode reward: total was 3.290000. running mean: -33.291376\n",
      "ep 4038: ep_len:25 episode reward: total was 11.000000. running mean: -32.848463\n",
      "ep 4038: ep_len:3094 episode reward: total was -35.840000. running mean: -32.878378\n",
      "ep 4038: ep_len:500 episode reward: total was 50.130000. running mean: -32.048294\n",
      "ep 4038: ep_len:38 episode reward: total was 17.500000. running mean: -31.552811\n",
      "ep 4038: ep_len:71 episode reward: total was 32.500000. running mean: -30.912283\n",
      "ep 4038: ep_len:500 episode reward: total was 1.900000. running mean: -30.584160\n",
      "ep 4038: ep_len:317 episode reward: total was 17.030000. running mean: -30.108019\n",
      "ep 4038: ep_len:592 episode reward: total was -24.610000. running mean: -30.053038\n",
      "ep 4038: ep_len:888 episode reward: total was 56.490000. running mean: -29.187608\n",
      "ep 4038: ep_len:594 episode reward: total was -15.930000. running mean: -29.055032\n",
      "ep 4038: ep_len:85 episode reward: total was 39.500000. running mean: -28.369482\n",
      "ep 4038: ep_len:54 episode reward: total was 24.000000. running mean: -27.845787\n",
      "ep 4038: ep_len:500 episode reward: total was 26.030000. running mean: -27.307029\n",
      "ep 4038: ep_len:2854 episode reward: total was 13.380000. running mean: -26.900159\n",
      "epsilon:0.009992 episode_count: 60719. steps_count: 65498474.000000\n",
      "ep 4039: ep_len:1123 episode reward: total was -3.080000. running mean: -26.661957\n",
      "ep 4039: ep_len:751 episode reward: total was -35.200000. running mean: -26.747338\n",
      "ep 4039: ep_len:71 episode reward: total was 32.500000. running mean: -26.154864\n",
      "ep 4039: ep_len:2879 episode reward: total was -49.610000. running mean: -26.389416\n",
      "ep 4039: ep_len:505 episode reward: total was 12.200000. running mean: -26.003521\n",
      "ep 4039: ep_len:63 episode reward: total was 28.500000. running mean: -25.458486\n",
      "ep 4039: ep_len:500 episode reward: total was 34.520000. running mean: -24.858701\n",
      "ep 4039: ep_len:341 episode reward: total was 16.320000. running mean: -24.446914\n",
      "ep 4039: ep_len:677 episode reward: total was -96.750000. running mean: -25.169945\n",
      "ep 4039: ep_len:675 episode reward: total was 26.400000. running mean: -24.654246\n",
      "ep 4039: ep_len:580 episode reward: total was -25.040000. running mean: -24.658103\n",
      "ep 4039: ep_len:160 episode reward: total was 77.000000. running mean: -23.641522\n",
      "ep 4039: ep_len:943 episode reward: total was -85.930000. running mean: -24.264407\n",
      "ep 4039: ep_len:2842 episode reward: total was -36.660000. running mean: -24.388363\n",
      "ep 4039: ep_len:43 episode reward: total was 20.000000. running mean: -23.944479\n",
      "epsilon:0.009992 episode_count: 60734. steps_count: 65510627.000000\n",
      "ep 4040: ep_len:664 episode reward: total was -53.790000. running mean: -24.242934\n",
      "ep 4040: ep_len:706 episode reward: total was -28.460000. running mean: -24.285105\n",
      "ep 4040: ep_len:47 episode reward: total was 22.000000. running mean: -23.822254\n",
      "ep 4040: ep_len:2943 episode reward: total was -25.830000. running mean: -23.842332\n",
      "ep 4040: ep_len:865 episode reward: total was 54.590000. running mean: -23.058008\n",
      "ep 4040: ep_len:44 episode reward: total was 20.500000. running mean: -22.622428\n",
      "ep 4040: ep_len:598 episode reward: total was -169.430000. running mean: -24.090504\n",
      "ep 4040: ep_len:335 episode reward: total was 29.090000. running mean: -23.558699\n",
      "ep 4040: ep_len:1247 episode reward: total was -41.500000. running mean: -23.738112\n",
      "ep 4040: ep_len:889 episode reward: total was 55.150000. running mean: -22.949231\n",
      "ep 4040: ep_len:897 episode reward: total was 17.770000. running mean: -22.542038\n",
      "ep 4040: ep_len:1134 episode reward: total was -12.060000. running mean: -22.437218\n",
      "ep 4040: ep_len:2731 episode reward: total was 1.190000. running mean: -22.200946\n",
      "ep 4040: ep_len:57 episode reward: total was 27.000000. running mean: -21.708936\n",
      "epsilon:0.009992 episode_count: 60748. steps_count: 65523784.000000\n",
      "ep 4041: ep_len:945 episode reward: total was -78.720000. running mean: -22.279047\n",
      "ep 4041: ep_len:500 episode reward: total was 13.320000. running mean: -21.923057\n",
      "ep 4041: ep_len:54 episode reward: total was 24.000000. running mean: -21.463826\n",
      "ep 4041: ep_len:2920 episode reward: total was -1108.080000. running mean: -32.329988\n",
      "ep 4041: ep_len:1201 episode reward: total was -27.640000. running mean: -32.283088\n",
      "ep 4041: ep_len:680 episode reward: total was 25.130000. running mean: -31.708957\n",
      "ep 4041: ep_len:3615 episode reward: total was -619.130000. running mean: -37.583167\n",
      "ep 4041: ep_len:624 episode reward: total was -72.710000. running mean: -37.934436\n",
      "ep 4041: ep_len:635 episode reward: total was 13.900000. running mean: -37.416091\n",
      "ep 4041: ep_len:973 episode reward: total was 0.200000. running mean: -37.039930\n",
      "ep 4041: ep_len:831 episode reward: total was -15.200000. running mean: -36.821531\n",
      "ep 4041: ep_len:2857 episode reward: total was -88.820000. running mean: -37.341516\n",
      "epsilon:0.009992 episode_count: 60760. steps_count: 65539619.000000\n",
      "ep 4042: ep_len:940 episode reward: total was -77.610000. running mean: -37.744201\n",
      "ep 4042: ep_len:746 episode reward: total was -12.910000. running mean: -37.495859\n",
      "ep 4042: ep_len:2938 episode reward: total was 2.980000. running mean: -37.091100\n",
      "ep 4042: ep_len:814 episode reward: total was 14.610000. running mean: -36.574089\n",
      "ep 4042: ep_len:500 episode reward: total was 5.760000. running mean: -36.150748\n",
      "ep 4042: ep_len:328 episode reward: total was 28.010000. running mean: -35.509141\n",
      "ep 4042: ep_len:1286 episode reward: total was -33.770000. running mean: -35.491749\n",
      "ep 4042: ep_len:635 episode reward: total was 12.060000. running mean: -35.016232\n",
      "ep 4042: ep_len:500 episode reward: total was 4.620000. running mean: -34.619869\n",
      "ep 4042: ep_len:659 episode reward: total was 9.190000. running mean: -34.181771\n",
      "ep 4042: ep_len:2871 episode reward: total was 11.900000. running mean: -33.720953\n",
      "ep 4042: ep_len:43 episode reward: total was 17.000000. running mean: -33.213744\n",
      "epsilon:0.009992 episode_count: 60772. steps_count: 65551879.000000\n",
      "ep 4043: ep_len:1472 episode reward: total was 4.970000. running mean: -32.831906\n",
      "ep 4043: ep_len:500 episode reward: total was 23.670000. running mean: -32.266887\n",
      "ep 4043: ep_len:2950 episode reward: total was -21.440000. running mean: -32.158618\n",
      "ep 4043: ep_len:802 episode reward: total was 65.320000. running mean: -31.183832\n",
      "ep 4043: ep_len:500 episode reward: total was 18.410000. running mean: -30.687894\n",
      "ep 4043: ep_len:573 episode reward: total was 24.880000. running mean: -30.132215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4043: ep_len:3878 episode reward: total was -726.360000. running mean: -37.094493\n",
      "ep 4043: ep_len:7337 episode reward: total was 29.100000. running mean: -36.432548\n",
      "ep 4043: ep_len:1127 episode reward: total was -3.590000. running mean: -36.104122\n",
      "ep 4043: ep_len:77 episode reward: total was 37.000000. running mean: -35.373081\n",
      "ep 4043: ep_len:52 episode reward: total was 24.500000. running mean: -34.774350\n",
      "ep 4043: ep_len:104 episode reward: total was 50.500000. running mean: -33.921607\n",
      "ep 4043: ep_len:511 episode reward: total was 9.910000. running mean: -33.483291\n",
      "ep 4043: ep_len:2903 episode reward: total was 7.350000. running mean: -33.074958\n",
      "epsilon:0.009992 episode_count: 60786. steps_count: 65574665.000000\n",
      "ep 4044: ep_len:1435 episode reward: total was 17.180000. running mean: -32.572408\n",
      "ep 4044: ep_len:617 episode reward: total was 14.150000. running mean: -32.105184\n",
      "ep 4044: ep_len:2866 episode reward: total was -54.270000. running mean: -32.326832\n",
      "ep 4044: ep_len:1196 episode reward: total was -20.530000. running mean: -32.208864\n",
      "ep 4044: ep_len:70 episode reward: total was 33.500000. running mean: -31.551775\n",
      "ep 4044: ep_len:41 episode reward: total was 19.000000. running mean: -31.046257\n",
      "ep 4044: ep_len:1409 episode reward: total was 12.630000. running mean: -30.609495\n",
      "ep 4044: ep_len:3714 episode reward: total was -558.130000. running mean: -35.884700\n",
      "ep 4044: ep_len:759 episode reward: total was -56.560000. running mean: -36.091453\n",
      "ep 4044: ep_len:740 episode reward: total was 2.950000. running mean: -35.701038\n",
      "ep 4044: ep_len:500 episode reward: total was -10.650000. running mean: -35.450528\n",
      "ep 4044: ep_len:123 episode reward: total was 60.000000. running mean: -34.496023\n",
      "ep 4044: ep_len:782 episode reward: total was -9.550000. running mean: -34.246563\n",
      "ep 4044: ep_len:2783 episode reward: total was -38.230000. running mean: -34.286397\n",
      "epsilon:0.009992 episode_count: 60800. steps_count: 65591700.000000\n",
      "ep 4045: ep_len:946 episode reward: total was -210.820000. running mean: -36.051733\n",
      "ep 4045: ep_len:500 episode reward: total was 15.620000. running mean: -35.535016\n",
      "ep 4045: ep_len:58 episode reward: total was 27.500000. running mean: -34.904665\n",
      "ep 4045: ep_len:3077 episode reward: total was -0.780000. running mean: -34.563419\n",
      "ep 4045: ep_len:500 episode reward: total was -31.160000. running mean: -34.529385\n",
      "ep 4045: ep_len:144 episode reward: total was 67.500000. running mean: -33.509091\n",
      "ep 4045: ep_len:74 episode reward: total was 31.000000. running mean: -32.864000\n",
      "ep 4045: ep_len:500 episode reward: total was -1.190000. running mean: -32.547260\n",
      "ep 4045: ep_len:3592 episode reward: total was -220.410000. running mean: -34.425887\n",
      "ep 4045: ep_len:1251 episode reward: total was -187.120000. running mean: -35.952828\n",
      "ep 4045: ep_len:831 episode reward: total was 30.750000. running mean: -35.285800\n",
      "ep 4045: ep_len:665 episode reward: total was -9.600000. running mean: -35.028942\n",
      "ep 4045: ep_len:142 episode reward: total was 68.000000. running mean: -33.998653\n",
      "ep 4045: ep_len:59 episode reward: total was 28.000000. running mean: -33.378666\n",
      "ep 4045: ep_len:1460 episode reward: total was 11.640000. running mean: -32.928479\n",
      "ep 4045: ep_len:2857 episode reward: total was -38.130000. running mean: -32.980495\n",
      "epsilon:0.009992 episode_count: 60816. steps_count: 65608356.000000\n",
      "ep 4046: ep_len:1141 episode reward: total was 4.510000. running mean: -32.605590\n",
      "ep 4046: ep_len:1603 episode reward: total was -14.720000. running mean: -32.426734\n",
      "ep 4046: ep_len:61 episode reward: total was 29.000000. running mean: -31.812467\n",
      "ep 4046: ep_len:2878 episode reward: total was -13.220000. running mean: -31.626542\n",
      "ep 4046: ep_len:1665 episode reward: total was -51.650000. running mean: -31.826776\n",
      "ep 4046: ep_len:500 episode reward: total was 1.840000. running mean: -31.490109\n",
      "ep 4046: ep_len:628 episode reward: total was 15.970000. running mean: -31.015508\n",
      "ep 4046: ep_len:660 episode reward: total was -52.840000. running mean: -31.233753\n",
      "ep 4046: ep_len:830 episode reward: total was 35.330000. running mean: -30.568115\n",
      "ep 4046: ep_len:500 episode reward: total was 42.350000. running mean: -29.838934\n",
      "ep 4046: ep_len:75 episode reward: total was 33.000000. running mean: -29.210544\n",
      "ep 4046: ep_len:137 episode reward: total was 65.500000. running mean: -28.263439\n",
      "ep 4046: ep_len:43 episode reward: total was 20.000000. running mean: -27.780805\n",
      "ep 4046: ep_len:1437 episode reward: total was 27.270000. running mean: -27.230297\n",
      "ep 4046: ep_len:2911 episode reward: total was 6.050000. running mean: -26.897494\n",
      "ep 4046: ep_len:36 episode reward: total was 15.000000. running mean: -26.478519\n",
      "epsilon:0.009992 episode_count: 60832. steps_count: 65623461.000000\n",
      "ep 4047: ep_len:500 episode reward: total was -3.920000. running mean: -26.252934\n",
      "ep 4047: ep_len:798 episode reward: total was -8.730000. running mean: -26.077704\n",
      "ep 4047: ep_len:3126 episode reward: total was -55.440000. running mean: -26.371327\n",
      "ep 4047: ep_len:625 episode reward: total was 1.460000. running mean: -26.093014\n",
      "ep 4047: ep_len:58 episode reward: total was 26.000000. running mean: -25.572084\n",
      "ep 4047: ep_len:1405 episode reward: total was -321.440000. running mean: -28.530763\n",
      "ep 4047: ep_len:3704 episode reward: total was -313.220000. running mean: -31.377655\n",
      "ep 4047: ep_len:1277 episode reward: total was -59.020000. running mean: -31.654079\n",
      "ep 4047: ep_len:876 episode reward: total was 67.820000. running mean: -30.659338\n",
      "ep 4047: ep_len:787 episode reward: total was 10.990000. running mean: -30.242845\n",
      "ep 4047: ep_len:88 episode reward: total was 41.000000. running mean: -29.530416\n",
      "ep 4047: ep_len:39 episode reward: total was 18.000000. running mean: -29.055112\n",
      "ep 4047: ep_len:1152 episode reward: total was 6.950000. running mean: -28.695061\n",
      "ep 4047: ep_len:2828 episode reward: total was -33.400000. running mean: -28.742110\n",
      "ep 4047: ep_len:75 episode reward: total was 36.000000. running mean: -28.094689\n",
      "epsilon:0.009992 episode_count: 60847. steps_count: 65640799.000000\n",
      "ep 4048: ep_len:1486 episode reward: total was 44.140000. running mean: -27.372342\n",
      "ep 4048: ep_len:216 episode reward: total was -11.220000. running mean: -27.210819\n",
      "ep 4048: ep_len:50 episode reward: total was 22.000000. running mean: -26.718711\n",
      "ep 4048: ep_len:2934 episode reward: total was -66.290000. running mean: -27.114424\n",
      "ep 4048: ep_len:1268 episode reward: total was -11.720000. running mean: -26.960479\n",
      "ep 4048: ep_len:42 episode reward: total was 19.500000. running mean: -26.495874\n",
      "ep 4048: ep_len:84 episode reward: total was 40.500000. running mean: -25.825916\n",
      "ep 4048: ep_len:80 episode reward: total was 38.500000. running mean: -25.182657\n",
      "ep 4048: ep_len:1396 episode reward: total was -209.420000. running mean: -27.025030\n",
      "ep 4048: ep_len:3858 episode reward: total was -44.100000. running mean: -27.195780\n",
      "ep 4048: ep_len:859 episode reward: total was -112.880000. running mean: -28.052622\n",
      "ep 4048: ep_len:756 episode reward: total was 7.520000. running mean: -27.696896\n",
      "ep 4048: ep_len:656 episode reward: total was -20.490000. running mean: -27.624827\n",
      "ep 4048: ep_len:66 episode reward: total was 30.000000. running mean: -27.048578\n",
      "ep 4048: ep_len:133 episode reward: total was 62.000000. running mean: -26.158093\n",
      "ep 4048: ep_len:798 episode reward: total was -7.280000. running mean: -25.969312\n",
      "ep 4048: ep_len:2897 episode reward: total was -16.400000. running mean: -25.873619\n",
      "epsilon:0.009992 episode_count: 60864. steps_count: 65658378.000000\n",
      "ep 4049: ep_len:1064 episode reward: total was -0.640000. running mean: -25.621282\n",
      "ep 4049: ep_len:500 episode reward: total was -20.110000. running mean: -25.566170\n",
      "ep 4049: ep_len:2910 episode reward: total was -34.910000. running mean: -25.659608\n",
      "ep 4049: ep_len:745 episode reward: total was -19.990000. running mean: -25.602912\n",
      "ep 4049: ep_len:113 episode reward: total was 53.500000. running mean: -24.811883\n",
      "ep 4049: ep_len:40 episode reward: total was 18.500000. running mean: -24.378764\n",
      "ep 4049: ep_len:1116 episode reward: total was -104.150000. running mean: -25.176476\n",
      "ep 4049: ep_len:3773 episode reward: total was -2402.450000. running mean: -48.949212\n",
      "ep 4049: ep_len:670 episode reward: total was -78.310000. running mean: -49.242819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4049: ep_len:7383 episode reward: total was -2245.390000. running mean: -71.204291\n",
      "ep 4049: ep_len:500 episode reward: total was 11.480000. running mean: -70.377448\n",
      "ep 4049: ep_len:63 episode reward: total was 27.000000. running mean: -69.403674\n",
      "ep 4049: ep_len:1157 episode reward: total was 20.900000. running mean: -68.500637\n",
      "ep 4049: ep_len:2812 episode reward: total was -26.040000. running mean: -68.076031\n",
      "epsilon:0.009992 episode_count: 60878. steps_count: 65681224.000000\n",
      "ep 4050: ep_len:698 episode reward: total was -10.660000. running mean: -67.501870\n",
      "ep 4050: ep_len:937 episode reward: total was -2.700000. running mean: -66.853852\n",
      "ep 4050: ep_len:49 episode reward: total was 23.000000. running mean: -65.955313\n",
      "ep 4050: ep_len:3001 episode reward: total was -20.350000. running mean: -65.499260\n",
      "ep 4050: ep_len:500 episode reward: total was -12.730000. running mean: -64.971567\n",
      "ep 4050: ep_len:59 episode reward: total was 28.000000. running mean: -64.041852\n",
      "ep 4050: ep_len:1854 episode reward: total was -109.320000. running mean: -64.494633\n",
      "ep 4050: ep_len:660 episode reward: total was 14.120000. running mean: -63.708487\n",
      "ep 4050: ep_len:1527 episode reward: total was 17.700000. running mean: -62.894402\n",
      "ep 4050: ep_len:650 episode reward: total was 13.010000. running mean: -62.135358\n",
      "ep 4050: ep_len:500 episode reward: total was 32.890000. running mean: -61.185104\n",
      "ep 4050: ep_len:76 episode reward: total was 35.000000. running mean: -60.223253\n",
      "ep 4050: ep_len:199 episode reward: total was 93.500000. running mean: -58.686021\n",
      "ep 4050: ep_len:97 episode reward: total was 45.500000. running mean: -57.644161\n",
      "ep 4050: ep_len:666 episode reward: total was -7.120000. running mean: -57.138919\n",
      "ep 4050: ep_len:2900 episode reward: total was -56.770000. running mean: -57.135230\n",
      "ep 4050: ep_len:38 episode reward: total was 17.500000. running mean: -56.388878\n",
      "epsilon:0.009992 episode_count: 60895. steps_count: 65695635.000000\n",
      "ep 4051: ep_len:1152 episode reward: total was 16.560000. running mean: -55.659389\n",
      "ep 4051: ep_len:765 episode reward: total was 11.290000. running mean: -54.989895\n",
      "ep 4051: ep_len:2996 episode reward: total was -16.660000. running mean: -54.606596\n",
      "ep 4051: ep_len:527 episode reward: total was -18.130000. running mean: -54.241830\n",
      "ep 4051: ep_len:1415 episode reward: total was -147.590000. running mean: -55.175312\n",
      "ep 4051: ep_len:3593 episode reward: total was -68.900000. running mean: -55.312559\n",
      "ep 4051: ep_len:741 episode reward: total was -25.210000. running mean: -55.011533\n",
      "ep 4051: ep_len:802 episode reward: total was 9.630000. running mean: -54.365118\n",
      "ep 4051: ep_len:500 episode reward: total was -0.080000. running mean: -53.822266\n",
      "ep 4051: ep_len:108 episode reward: total was 51.000000. running mean: -52.774044\n",
      "ep 4051: ep_len:49 episode reward: total was 23.000000. running mean: -52.016303\n",
      "ep 4051: ep_len:1521 episode reward: total was 25.600000. running mean: -51.240140\n",
      "ep 4051: ep_len:2825 episode reward: total was -25.820000. running mean: -50.985939\n",
      "ep 4051: ep_len:47 episode reward: total was 20.500000. running mean: -50.271080\n",
      "epsilon:0.009992 episode_count: 60909. steps_count: 65712676.000000\n",
      "ep 4052: ep_len:988 episode reward: total was -52.550000. running mean: -50.293869\n",
      "ep 4052: ep_len:1265 episode reward: total was -30.950000. running mean: -50.100430\n",
      "ep 4052: ep_len:2942 episode reward: total was -12.030000. running mean: -49.719726\n",
      "ep 4052: ep_len:792 episode reward: total was 8.760000. running mean: -49.134929\n",
      "ep 4052: ep_len:28 episode reward: total was 12.500000. running mean: -48.518579\n",
      "ep 4052: ep_len:679 episode reward: total was -11.560000. running mean: -48.148993\n",
      "ep 4052: ep_len:631 episode reward: total was 23.040000. running mean: -47.437103\n",
      "ep 4052: ep_len:1307 episode reward: total was -20.430000. running mean: -47.167032\n",
      "ep 4052: ep_len:7313 episode reward: total was -124.080000. running mean: -47.936162\n",
      "ep 4052: ep_len:739 episode reward: total was -4.900000. running mean: -47.505801\n",
      "ep 4052: ep_len:79 episode reward: total was 36.500000. running mean: -46.665743\n",
      "ep 4052: ep_len:190 episode reward: total was 92.000000. running mean: -45.279085\n",
      "ep 4052: ep_len:704 episode reward: total was 18.550000. running mean: -44.640794\n",
      "ep 4052: ep_len:2766 episode reward: total was -4.400000. running mean: -44.238386\n",
      "epsilon:0.009992 episode_count: 60923. steps_count: 65733099.000000\n",
      "ep 4053: ep_len:1439 episode reward: total was 8.860000. running mean: -43.707402\n",
      "ep 4053: ep_len:642 episode reward: total was -4.340000. running mean: -43.313728\n",
      "ep 4053: ep_len:51 episode reward: total was 22.500000. running mean: -42.655591\n",
      "ep 4053: ep_len:3049 episode reward: total was -18.050000. running mean: -42.409535\n",
      "ep 4053: ep_len:656 episode reward: total was 7.880000. running mean: -41.906640\n",
      "ep 4053: ep_len:61 episode reward: total was 29.000000. running mean: -41.197573\n",
      "ep 4053: ep_len:96 episode reward: total was 46.500000. running mean: -40.320598\n",
      "ep 4053: ep_len:1397 episode reward: total was -190.740000. running mean: -41.824792\n",
      "ep 4053: ep_len:3708 episode reward: total was -58.660000. running mean: -41.993144\n",
      "ep 4053: ep_len:804 episode reward: total was -42.270000. running mean: -41.995912\n",
      "ep 4053: ep_len:7280 episode reward: total was -2.940000. running mean: -41.605353\n",
      "ep 4053: ep_len:980 episode reward: total was 20.480000. running mean: -40.984500\n",
      "ep 4053: ep_len:61 episode reward: total was 27.500000. running mean: -40.299655\n",
      "ep 4053: ep_len:56 episode reward: total was 26.500000. running mean: -39.631658\n",
      "ep 4053: ep_len:92 episode reward: total was 41.500000. running mean: -38.820342\n",
      "ep 4053: ep_len:1165 episode reward: total was 4.410000. running mean: -38.388038\n",
      "ep 4053: ep_len:2812 episode reward: total was 5.850000. running mean: -37.945658\n",
      "ep 4053: ep_len:53 episode reward: total was 23.500000. running mean: -37.331201\n",
      "epsilon:0.009992 episode_count: 60941. steps_count: 65757501.000000\n",
      "ep 4054: ep_len:815 episode reward: total was 11.440000. running mean: -36.843489\n",
      "ep 4054: ep_len:682 episode reward: total was -11.530000. running mean: -36.590354\n",
      "ep 4054: ep_len:73 episode reward: total was 33.500000. running mean: -35.889451\n",
      "ep 4054: ep_len:2974 episode reward: total was 4.120000. running mean: -35.489356\n",
      "ep 4054: ep_len:500 episode reward: total was 10.840000. running mean: -35.026063\n",
      "ep 4054: ep_len:98 episode reward: total was 47.500000. running mean: -34.200802\n",
      "ep 4054: ep_len:58 episode reward: total was 27.500000. running mean: -33.583794\n",
      "ep 4054: ep_len:1442 episode reward: total was 33.390000. running mean: -32.914056\n",
      "ep 4054: ep_len:3645 episode reward: total was -27.340000. running mean: -32.858316\n",
      "ep 4054: ep_len:500 episode reward: total was -24.610000. running mean: -32.775832\n",
      "ep 4054: ep_len:719 episode reward: total was 40.500000. running mean: -32.043074\n",
      "ep 4054: ep_len:599 episode reward: total was 18.270000. running mean: -31.539943\n",
      "ep 4054: ep_len:63 episode reward: total was 28.500000. running mean: -30.939544\n",
      "ep 4054: ep_len:170 episode reward: total was 80.500000. running mean: -29.825148\n",
      "ep 4054: ep_len:40 episode reward: total was 17.000000. running mean: -29.356897\n",
      "ep 4054: ep_len:1215 episode reward: total was 7.430000. running mean: -28.989028\n",
      "ep 4054: ep_len:2842 episode reward: total was -15.480000. running mean: -28.853938\n",
      "ep 4054: ep_len:52 episode reward: total was 23.000000. running mean: -28.335398\n",
      "epsilon:0.009992 episode_count: 60959. steps_count: 65773988.000000\n",
      "ep 4055: ep_len:1456 episode reward: total was 9.210000. running mean: -27.959944\n",
      "ep 4055: ep_len:739 episode reward: total was -29.370000. running mean: -27.974045\n",
      "ep 4055: ep_len:43 episode reward: total was 18.500000. running mean: -27.509304\n",
      "ep 4055: ep_len:2845 episode reward: total was -27.270000. running mean: -27.506911\n",
      "ep 4055: ep_len:1465 episode reward: total was 17.050000. running mean: -27.061342\n",
      "ep 4055: ep_len:68 episode reward: total was 32.500000. running mean: -26.465729\n",
      "ep 4055: ep_len:83 episode reward: total was 40.000000. running mean: -25.801072\n",
      "ep 4055: ep_len:1397 episode reward: total was -89.220000. running mean: -26.435261\n",
      "ep 4055: ep_len:4035 episode reward: total was -82.090000. running mean: -26.991808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4055: ep_len:810 episode reward: total was -34.380000. running mean: -27.065690\n",
      "ep 4055: ep_len:627 episode reward: total was -1.370000. running mean: -26.808733\n",
      "ep 4055: ep_len:1134 episode reward: total was 0.060000. running mean: -26.540046\n",
      "ep 4055: ep_len:66 episode reward: total was 30.000000. running mean: -25.974646\n",
      "ep 4055: ep_len:36 episode reward: total was 16.500000. running mean: -25.549899\n",
      "ep 4055: ep_len:73 episode reward: total was 35.000000. running mean: -24.944400\n",
      "ep 4055: ep_len:667 episode reward: total was -9.320000. running mean: -24.788156\n",
      "ep 4055: ep_len:2854 episode reward: total was -15.700000. running mean: -24.697274\n",
      "epsilon:0.009992 episode_count: 60976. steps_count: 65792386.000000\n",
      "ep 4056: ep_len:1164 episode reward: total was 17.200000. running mean: -24.278302\n",
      "ep 4056: ep_len:500 episode reward: total was 34.600000. running mean: -23.689519\n",
      "ep 4056: ep_len:50 episode reward: total was 22.000000. running mean: -23.232624\n",
      "ep 4056: ep_len:2994 episode reward: total was -4.230000. running mean: -23.042597\n",
      "ep 4056: ep_len:1643 episode reward: total was -29.290000. running mean: -23.105071\n",
      "ep 4056: ep_len:143 episode reward: total was 67.000000. running mean: -22.204021\n",
      "ep 4056: ep_len:500 episode reward: total was 19.170000. running mean: -21.790280\n",
      "ep 4056: ep_len:647 episode reward: total was 23.510000. running mean: -21.337278\n",
      "ep 4056: ep_len:653 episode reward: total was -47.660000. running mean: -21.600505\n",
      "ep 4056: ep_len:642 episode reward: total was -4.740000. running mean: -21.431900\n",
      "ep 4056: ep_len:649 episode reward: total was 1.720000. running mean: -21.200381\n",
      "ep 4056: ep_len:1218 episode reward: total was 16.520000. running mean: -20.823177\n",
      "ep 4056: ep_len:2877 episode reward: total was -9.380000. running mean: -20.708745\n",
      "epsilon:0.009992 episode_count: 60989. steps_count: 65806066.000000\n",
      "ep 4057: ep_len:1430 episode reward: total was 35.860000. running mean: -20.143058\n",
      "ep 4057: ep_len:736 episode reward: total was -23.800000. running mean: -20.179627\n",
      "ep 4057: ep_len:89 episode reward: total was 43.000000. running mean: -19.547831\n",
      "ep 4057: ep_len:860 episode reward: total was -19.410000. running mean: -19.546453\n",
      "ep 4057: ep_len:146 episode reward: total was 71.500000. running mean: -18.635988\n",
      "ep 4057: ep_len:1946 episode reward: total was -5.380000. running mean: -18.503428\n",
      "ep 4057: ep_len:657 episode reward: total was 18.530000. running mean: -18.133094\n",
      "ep 4057: ep_len:2159 episode reward: total was -566.330000. running mean: -23.615063\n",
      "ep 4057: ep_len:7229 episode reward: total was 39.100000. running mean: -22.987912\n",
      "ep 4057: ep_len:619 episode reward: total was 10.790000. running mean: -22.650133\n",
      "ep 4057: ep_len:711 episode reward: total was -83.010000. running mean: -23.253732\n",
      "ep 4057: ep_len:2849 episode reward: total was -9.840000. running mean: -23.119595\n",
      "epsilon:0.009992 episode_count: 61001. steps_count: 65825497.000000\n",
      "ep 4058: ep_len:1449 episode reward: total was 28.520000. running mean: -22.603199\n",
      "ep 4058: ep_len:1631 episode reward: total was -5.100000. running mean: -22.428167\n",
      "ep 4058: ep_len:57 episode reward: total was 27.000000. running mean: -21.933885\n",
      "ep 4058: ep_len:2967 episode reward: total was -36.150000. running mean: -22.076046\n",
      "ep 4058: ep_len:824 episode reward: total was -3.350000. running mean: -21.888786\n",
      "ep 4058: ep_len:31 episode reward: total was 14.000000. running mean: -21.529898\n",
      "ep 4058: ep_len:80 episode reward: total was 38.500000. running mean: -20.929599\n",
      "ep 4058: ep_len:603 episode reward: total was 22.090000. running mean: -20.499403\n",
      "ep 4058: ep_len:3612 episode reward: total was -27.210000. running mean: -20.566509\n",
      "ep 4058: ep_len:819 episode reward: total was -17.870000. running mean: -20.539544\n",
      "ep 4058: ep_len:725 episode reward: total was 40.710000. running mean: -19.927048\n",
      "ep 4058: ep_len:1438 episode reward: total was 21.920000. running mean: -19.508578\n",
      "ep 4058: ep_len:49 episode reward: total was 23.000000. running mean: -19.083492\n",
      "ep 4058: ep_len:132 episode reward: total was 61.010000. running mean: -18.282557\n",
      "ep 4058: ep_len:35 episode reward: total was 16.000000. running mean: -17.939732\n",
      "ep 4058: ep_len:80 episode reward: total was 37.000000. running mean: -17.390334\n",
      "ep 4058: ep_len:1513 episode reward: total was 8.380000. running mean: -17.132631\n",
      "ep 4058: ep_len:2753 episode reward: total was -20.510000. running mean: -17.166405\n",
      "ep 4058: ep_len:43 episode reward: total was 20.000000. running mean: -16.794741\n",
      "epsilon:0.009992 episode_count: 61020. steps_count: 65844338.000000\n",
      "ep 4059: ep_len:1474 episode reward: total was 11.600000. running mean: -16.510793\n",
      "ep 4059: ep_len:737 episode reward: total was 12.420000. running mean: -16.221485\n",
      "ep 4059: ep_len:98 episode reward: total was 46.000000. running mean: -15.599270\n",
      "ep 4059: ep_len:1214 episode reward: total was -17.320000. running mean: -15.616478\n",
      "ep 4059: ep_len:57 episode reward: total was 25.500000. running mean: -15.205313\n",
      "ep 4059: ep_len:165 episode reward: total was 79.500000. running mean: -14.258260\n",
      "ep 4059: ep_len:64 episode reward: total was 30.500000. running mean: -13.810677\n",
      "ep 4059: ep_len:1135 episode reward: total was -27.180000. running mean: -13.944370\n",
      "ep 4059: ep_len:646 episode reward: total was 19.150000. running mean: -13.613427\n",
      "ep 4059: ep_len:913 episode reward: total was -21.860000. running mean: -13.695892\n",
      "ep 4059: ep_len:647 episode reward: total was -8.730000. running mean: -13.646233\n",
      "ep 4059: ep_len:500 episode reward: total was 3.710000. running mean: -13.472671\n",
      "ep 4059: ep_len:128 episode reward: total was 59.500000. running mean: -12.742944\n",
      "ep 4059: ep_len:714 episode reward: total was 40.830000. running mean: -12.207215\n",
      "ep 4059: ep_len:2870 episode reward: total was 13.940000. running mean: -11.945743\n",
      "ep 4059: ep_len:44 episode reward: total was 20.500000. running mean: -11.621285\n",
      "epsilon:0.009992 episode_count: 61036. steps_count: 65855744.000000\n",
      "ep 4060: ep_len:1447 episode reward: total was 30.460000. running mean: -11.200473\n",
      "ep 4060: ep_len:958 episode reward: total was 2.590000. running mean: -11.062568\n",
      "ep 4060: ep_len:2987 episode reward: total was 15.420000. running mean: -10.797742\n",
      "ep 4060: ep_len:755 episode reward: total was 13.710000. running mean: -10.552665\n",
      "ep 4060: ep_len:88 episode reward: total was 41.000000. running mean: -10.037138\n",
      "ep 4060: ep_len:69 episode reward: total was 33.000000. running mean: -9.606767\n",
      "ep 4060: ep_len:714 episode reward: total was 3.520000. running mean: -9.475499\n",
      "ep 4060: ep_len:3634 episode reward: total was -6.850000. running mean: -9.449244\n",
      "ep 4060: ep_len:918 episode reward: total was -30.380000. running mean: -9.658552\n",
      "ep 4060: ep_len:737 episode reward: total was 45.330000. running mean: -9.108666\n",
      "ep 4060: ep_len:729 episode reward: total was 10.710000. running mean: -8.910479\n",
      "ep 4060: ep_len:1189 episode reward: total was -23.110000. running mean: -9.052475\n",
      "ep 4060: ep_len:2776 episode reward: total was -20.770000. running mean: -9.169650\n",
      "epsilon:0.009992 episode_count: 61049. steps_count: 65872745.000000\n",
      "ep 4061: ep_len:1441 episode reward: total was 28.440000. running mean: -8.793553\n",
      "ep 4061: ep_len:742 episode reward: total was -2.020000. running mean: -8.725818\n",
      "ep 4061: ep_len:2993 episode reward: total was 0.950000. running mean: -8.629060\n",
      "ep 4061: ep_len:500 episode reward: total was 23.610000. running mean: -8.306669\n",
      "ep 4061: ep_len:1498 episode reward: total was -279.060000. running mean: -11.014202\n",
      "ep 4061: ep_len:328 episode reward: total was 21.120000. running mean: -10.692860\n",
      "ep 4061: ep_len:2953 episode reward: total was -166.320000. running mean: -12.249132\n",
      "ep 4061: ep_len:7357 episode reward: total was -7.890000. running mean: -12.205540\n",
      "ep 4061: ep_len:1128 episode reward: total was -9.090000. running mean: -12.174385\n",
      "ep 4061: ep_len:131 episode reward: total was 63.510000. running mean: -11.417541\n",
      "ep 4061: ep_len:26 episode reward: total was 11.500000. running mean: -11.188366\n",
      "ep 4061: ep_len:724 episode reward: total was -20.200000. running mean: -11.278482\n",
      "ep 4061: ep_len:2919 episode reward: total was 20.520000. running mean: -10.960497\n",
      "epsilon:0.009992 episode_count: 61062. steps_count: 65895485.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4062: ep_len:656 episode reward: total was 10.210000. running mean: -10.748792\n",
      "ep 4062: ep_len:619 episode reward: total was -4.940000. running mean: -10.690704\n",
      "ep 4062: ep_len:45 episode reward: total was 21.000000. running mean: -10.373797\n",
      "ep 4062: ep_len:2947 episode reward: total was -51.580000. running mean: -10.785859\n",
      "ep 4062: ep_len:643 episode reward: total was 5.250000. running mean: -10.625501\n",
      "ep 4062: ep_len:167 episode reward: total was 79.000000. running mean: -9.729246\n",
      "ep 4062: ep_len:94 episode reward: total was 44.000000. running mean: -9.191953\n",
      "ep 4062: ep_len:60 episode reward: total was 27.000000. running mean: -8.830034\n",
      "ep 4062: ep_len:911 episode reward: total was 47.730000. running mean: -8.264433\n",
      "ep 4062: ep_len:616 episode reward: total was 8.200000. running mean: -8.099789\n",
      "ep 4062: ep_len:1287 episode reward: total was -27.700000. running mean: -8.295791\n",
      "ep 4062: ep_len:795 episode reward: total was 21.080000. running mean: -8.002033\n",
      "ep 4062: ep_len:730 episode reward: total was 25.150000. running mean: -7.670513\n",
      "ep 4062: ep_len:131 episode reward: total was 62.500000. running mean: -6.968808\n",
      "ep 4062: ep_len:1179 episode reward: total was 12.000000. running mean: -6.779120\n",
      "ep 4062: ep_len:2923 episode reward: total was 11.500000. running mean: -6.596329\n",
      "ep 4062: ep_len:67 episode reward: total was 30.500000. running mean: -6.225365\n",
      "epsilon:0.009992 episode_count: 61079. steps_count: 65909355.000000\n",
      "ep 4063: ep_len:717 episode reward: total was -51.580000. running mean: -6.678912\n",
      "ep 4063: ep_len:1650 episode reward: total was -17.090000. running mean: -6.783023\n",
      "ep 4063: ep_len:56 episode reward: total was 26.500000. running mean: -6.450192\n",
      "ep 4063: ep_len:3092 episode reward: total was -24.420000. running mean: -6.629890\n",
      "ep 4063: ep_len:509 episode reward: total was -51.640000. running mean: -7.079991\n",
      "ep 4063: ep_len:44 episode reward: total was 19.000000. running mean: -6.819192\n",
      "ep 4063: ep_len:36 episode reward: total was 16.500000. running mean: -6.586000\n",
      "ep 4063: ep_len:500 episode reward: total was 10.350000. running mean: -6.416640\n",
      "ep 4063: ep_len:4226 episode reward: total was -484.450000. running mean: -11.196973\n",
      "ep 4063: ep_len:531 episode reward: total was -0.920000. running mean: -11.094204\n",
      "ep 4063: ep_len:844 episode reward: total was 61.990000. running mean: -10.363361\n",
      "ep 4063: ep_len:523 episode reward: total was 28.650000. running mean: -9.973228\n",
      "ep 4063: ep_len:85 episode reward: total was 39.500000. running mean: -9.478496\n",
      "ep 4063: ep_len:1495 episode reward: total was -2.430000. running mean: -9.408011\n",
      "ep 4063: ep_len:2931 episode reward: total was -32.590000. running mean: -9.639831\n",
      "epsilon:0.009992 episode_count: 61094. steps_count: 65926594.000000\n",
      "ep 4064: ep_len:953 episode reward: total was -42.890000. running mean: -9.972332\n",
      "ep 4064: ep_len:725 episode reward: total was -18.570000. running mean: -10.058309\n",
      "ep 4064: ep_len:2897 episode reward: total was -4.400000. running mean: -10.001726\n",
      "ep 4064: ep_len:656 episode reward: total was 1.910000. running mean: -9.882609\n",
      "ep 4064: ep_len:56 episode reward: total was 26.500000. running mean: -9.518782\n",
      "ep 4064: ep_len:166 episode reward: total was 80.000000. running mean: -8.623595\n",
      "ep 4064: ep_len:500 episode reward: total was 22.030000. running mean: -8.317059\n",
      "ep 4064: ep_len:642 episode reward: total was 28.020000. running mean: -7.953688\n",
      "ep 4064: ep_len:2220 episode reward: total was -298.430000. running mean: -10.858451\n",
      "ep 4064: ep_len:842 episode reward: total was 72.630000. running mean: -10.023567\n",
      "ep 4064: ep_len:1109 episode reward: total was -1.200000. running mean: -9.935331\n",
      "ep 4064: ep_len:53 episode reward: total was 23.500000. running mean: -9.600978\n",
      "ep 4064: ep_len:121 episode reward: total was 57.500000. running mean: -8.929968\n",
      "ep 4064: ep_len:637 episode reward: total was 3.110000. running mean: -8.809568\n",
      "ep 4064: ep_len:2812 episode reward: total was 10.450000. running mean: -8.616973\n",
      "epsilon:0.009992 episode_count: 61109. steps_count: 65940983.000000\n",
      "ep 4065: ep_len:579 episode reward: total was 9.680000. running mean: -8.434003\n",
      "ep 4065: ep_len:500 episode reward: total was 21.310000. running mean: -8.136563\n",
      "ep 4065: ep_len:3014 episode reward: total was -0.560000. running mean: -8.060797\n",
      "ep 4065: ep_len:1131 episode reward: total was 2.050000. running mean: -7.959689\n",
      "ep 4065: ep_len:85 episode reward: total was 41.000000. running mean: -7.470092\n",
      "ep 4065: ep_len:500 episode reward: total was -3.090000. running mean: -7.426291\n",
      "ep 4065: ep_len:631 episode reward: total was 27.360000. running mean: -7.078429\n",
      "ep 4065: ep_len:955 episode reward: total was 0.700000. running mean: -7.000644\n",
      "ep 4065: ep_len:782 episode reward: total was 23.980000. running mean: -6.690838\n",
      "ep 4065: ep_len:573 episode reward: total was 0.210000. running mean: -6.621829\n",
      "ep 4065: ep_len:79 episode reward: total was 38.000000. running mean: -6.175611\n",
      "ep 4065: ep_len:1005 episode reward: total was -7.930000. running mean: -6.193155\n",
      "ep 4065: ep_len:2919 episode reward: total was 6.040000. running mean: -6.070823\n",
      "ep 4065: ep_len:41 episode reward: total was 19.000000. running mean: -5.820115\n",
      "epsilon:0.009992 episode_count: 61123. steps_count: 65953777.000000\n",
      "ep 4066: ep_len:1471 episode reward: total was 21.330000. running mean: -5.548614\n",
      "ep 4066: ep_len:828 episode reward: total was 15.750000. running mean: -5.335628\n",
      "ep 4066: ep_len:2882 episode reward: total was 4.100000. running mean: -5.241272\n",
      "ep 4066: ep_len:500 episode reward: total was 25.510000. running mean: -4.933759\n",
      "ep 4066: ep_len:500 episode reward: total was 29.090000. running mean: -4.593521\n",
      "ep 4066: ep_len:3643 episode reward: total was -200.710000. running mean: -6.554686\n",
      "ep 4066: ep_len:4000 episode reward: total was -531.510000. running mean: -11.804239\n",
      "ep 4066: ep_len:7273 episode reward: total was 55.150000. running mean: -11.134697\n",
      "ep 4066: ep_len:969 episode reward: total was 7.630000. running mean: -10.947050\n",
      "ep 4066: ep_len:98 episode reward: total was 46.000000. running mean: -10.377579\n",
      "ep 4066: ep_len:54 episode reward: total was 25.500000. running mean: -10.018804\n",
      "ep 4066: ep_len:125 episode reward: total was 58.000000. running mean: -9.338616\n",
      "ep 4066: ep_len:1072 episode reward: total was -6.620000. running mean: -9.311429\n",
      "ep 4066: ep_len:2842 episode reward: total was -24.670000. running mean: -9.465015\n",
      "ep 4066: ep_len:62 episode reward: total was 29.500000. running mean: -9.075365\n",
      "epsilon:0.009992 episode_count: 61138. steps_count: 65980096.000000\n",
      "ep 4067: ep_len:696 episode reward: total was -2.300000. running mean: -9.007611\n",
      "ep 4067: ep_len:729 episode reward: total was -1.970000. running mean: -8.937235\n",
      "ep 4067: ep_len:2879 episode reward: total was -9.670000. running mean: -8.944563\n",
      "ep 4067: ep_len:500 episode reward: total was -18.330000. running mean: -9.038417\n",
      "ep 4067: ep_len:46 episode reward: total was 21.500000. running mean: -8.733033\n",
      "ep 4067: ep_len:847 episode reward: total was 49.080000. running mean: -8.154903\n",
      "ep 4067: ep_len:652 episode reward: total was -43.560000. running mean: -8.508954\n",
      "ep 4067: ep_len:1142 episode reward: total was -75.890000. running mean: -9.182764\n",
      "ep 4067: ep_len:837 episode reward: total was 38.890000. running mean: -8.702037\n",
      "ep 4067: ep_len:1191 episode reward: total was -16.540000. running mean: -8.780416\n",
      "ep 4067: ep_len:154 episode reward: total was 71.000000. running mean: -7.982612\n",
      "ep 4067: ep_len:500 episode reward: total was 14.330000. running mean: -7.759486\n",
      "ep 4067: ep_len:2853 episode reward: total was 3.630000. running mean: -7.645591\n",
      "epsilon:0.009992 episode_count: 61151. steps_count: 65993122.000000\n",
      "ep 4068: ep_len:1194 episode reward: total was -9.830000. running mean: -7.667435\n",
      "ep 4068: ep_len:1273 episode reward: total was -56.520000. running mean: -8.155961\n",
      "ep 4068: ep_len:41 episode reward: total was 19.000000. running mean: -7.884401\n",
      "ep 4068: ep_len:2984 episode reward: total was 0.060000. running mean: -7.804957\n",
      "ep 4068: ep_len:596 episode reward: total was -31.060000. running mean: -8.037508\n",
      "ep 4068: ep_len:134 episode reward: total was 55.000000. running mean: -7.407132\n",
      "ep 4068: ep_len:41 episode reward: total was 17.500000. running mean: -7.158061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4068: ep_len:1149 episode reward: total was -3.830000. running mean: -7.124781\n",
      "ep 4068: ep_len:3643 episode reward: total was -209.250000. running mean: -9.146033\n",
      "ep 4068: ep_len:643 episode reward: total was -62.420000. running mean: -9.678772\n",
      "ep 4068: ep_len:887 episode reward: total was 63.060000. running mean: -8.951385\n",
      "ep 4068: ep_len:500 episode reward: total was 3.370000. running mean: -8.828171\n",
      "ep 4068: ep_len:73 episode reward: total was 35.000000. running mean: -8.389889\n",
      "ep 4068: ep_len:60 episode reward: total was 27.000000. running mean: -8.035990\n",
      "ep 4068: ep_len:1559 episode reward: total was 4.250000. running mean: -7.913130\n",
      "ep 4068: ep_len:2927 episode reward: total was -45.300000. running mean: -8.286999\n",
      "epsilon:0.009992 episode_count: 61167. steps_count: 66010826.000000\n",
      "ep 4069: ep_len:800 episode reward: total was -10.350000. running mean: -8.307629\n",
      "ep 4069: ep_len:756 episode reward: total was -0.690000. running mean: -8.231453\n",
      "ep 4069: ep_len:3090 episode reward: total was -3.440000. running mean: -8.183538\n",
      "ep 4069: ep_len:1652 episode reward: total was -128.150000. running mean: -9.383203\n",
      "ep 4069: ep_len:46 episode reward: total was 21.500000. running mean: -9.074371\n",
      "ep 4069: ep_len:1436 episode reward: total was 20.780000. running mean: -8.775827\n",
      "ep 4069: ep_len:3727 episode reward: total was -68.970000. running mean: -9.377769\n",
      "ep 4069: ep_len:620 episode reward: total was -3.460000. running mean: -9.318591\n",
      "ep 4069: ep_len:800 episode reward: total was 35.030000. running mean: -8.875105\n",
      "ep 4069: ep_len:735 episode reward: total was -14.950000. running mean: -8.935854\n",
      "ep 4069: ep_len:192 episode reward: total was 91.500000. running mean: -7.931496\n",
      "ep 4069: ep_len:500 episode reward: total was 20.770000. running mean: -7.644481\n",
      "ep 4069: ep_len:2800 episode reward: total was -92.210000. running mean: -8.490136\n",
      "epsilon:0.009992 episode_count: 61180. steps_count: 66027980.000000\n",
      "ep 4070: ep_len:609 episode reward: total was -11.250000. running mean: -8.517735\n",
      "ep 4070: ep_len:974 episode reward: total was 16.400000. running mean: -8.268557\n",
      "ep 4070: ep_len:3104 episode reward: total was 26.670000. running mean: -7.919172\n",
      "ep 4070: ep_len:732 episode reward: total was -10.020000. running mean: -7.940180\n",
      "ep 4070: ep_len:127 episode reward: total was 57.500000. running mean: -7.285778\n",
      "ep 4070: ep_len:684 episode reward: total was -34.780000. running mean: -7.560720\n",
      "ep 4070: ep_len:4391 episode reward: total was -532.320000. running mean: -12.808313\n",
      "ep 4070: ep_len:899 episode reward: total was -35.210000. running mean: -13.032330\n",
      "ep 4070: ep_len:642 episode reward: total was -6.240000. running mean: -12.964407\n",
      "ep 4070: ep_len:689 episode reward: total was 9.530000. running mean: -12.739463\n",
      "ep 4070: ep_len:95 episode reward: total was 44.500000. running mean: -12.167068\n",
      "ep 4070: ep_len:69 episode reward: total was 33.000000. running mean: -11.715397\n",
      "ep 4070: ep_len:757 episode reward: total was -46.130000. running mean: -12.059543\n",
      "ep 4070: ep_len:2827 episode reward: total was -0.330000. running mean: -11.942248\n",
      "ep 4070: ep_len:62 episode reward: total was 29.500000. running mean: -11.527825\n",
      "epsilon:0.009992 episode_count: 61195. steps_count: 66044641.000000\n",
      "ep 4071: ep_len:872 episode reward: total was 19.080000. running mean: -11.221747\n",
      "ep 4071: ep_len:682 episode reward: total was -3.450000. running mean: -11.144030\n",
      "ep 4071: ep_len:2891 episode reward: total was -26.500000. running mean: -11.297589\n",
      "ep 4071: ep_len:872 episode reward: total was 26.890000. running mean: -10.915714\n",
      "ep 4071: ep_len:40 episode reward: total was 18.500000. running mean: -10.621556\n",
      "ep 4071: ep_len:864 episode reward: total was 48.980000. running mean: -10.025541\n",
      "ep 4071: ep_len:3611 episode reward: total was -77.500000. running mean: -10.700285\n",
      "ep 4071: ep_len:644 episode reward: total was -24.090000. running mean: -10.834183\n",
      "ep 4071: ep_len:647 episode reward: total was 21.870000. running mean: -10.507141\n",
      "ep 4071: ep_len:500 episode reward: total was -1.990000. running mean: -10.421969\n",
      "ep 4071: ep_len:135 episode reward: total was -111.490000. running mean: -11.432650\n",
      "ep 4071: ep_len:37 episode reward: total was 17.000000. running mean: -11.148323\n",
      "ep 4071: ep_len:781 episode reward: total was -11.550000. running mean: -11.152340\n",
      "ep 4071: ep_len:2787 episode reward: total was 16.930000. running mean: -10.871516\n",
      "epsilon:0.009992 episode_count: 61209. steps_count: 66060004.000000\n",
      "ep 4072: ep_len:500 episode reward: total was 28.230000. running mean: -10.480501\n",
      "ep 4072: ep_len:755 episode reward: total was -22.570000. running mean: -10.601396\n",
      "ep 4072: ep_len:2923 episode reward: total was -47.330000. running mean: -10.968682\n",
      "ep 4072: ep_len:735 episode reward: total was 17.530000. running mean: -10.683696\n",
      "ep 4072: ep_len:1463 episode reward: total was -222.890000. running mean: -12.805759\n",
      "ep 4072: ep_len:3683 episode reward: total was -6.390000. running mean: -12.741601\n",
      "ep 4072: ep_len:1578 episode reward: total was -145.440000. running mean: -14.068585\n",
      "ep 4072: ep_len:655 episode reward: total was -10.760000. running mean: -14.035499\n",
      "ep 4072: ep_len:624 episode reward: total was 2.450000. running mean: -13.870644\n",
      "ep 4072: ep_len:807 episode reward: total was 23.620000. running mean: -13.495738\n",
      "ep 4072: ep_len:2819 episode reward: total was -16.300000. running mean: -13.523780\n",
      "epsilon:0.009992 episode_count: 61220. steps_count: 66076546.000000\n",
      "ep 4073: ep_len:650 episode reward: total was 17.620000. running mean: -13.212343\n",
      "ep 4073: ep_len:730 episode reward: total was -6.200000. running mean: -13.142219\n",
      "ep 4073: ep_len:66 episode reward: total was 30.000000. running mean: -12.710797\n",
      "ep 4073: ep_len:2962 episode reward: total was -23.190000. running mean: -12.815589\n",
      "ep 4073: ep_len:782 episode reward: total was 26.010000. running mean: -12.427333\n",
      "ep 4073: ep_len:139 episode reward: total was 66.500000. running mean: -11.638060\n",
      "ep 4073: ep_len:75 episode reward: total was 36.000000. running mean: -11.161679\n",
      "ep 4073: ep_len:1492 episode reward: total was 10.190000. running mean: -10.948162\n",
      "ep 4073: ep_len:3607 episode reward: total was -38.460000. running mean: -11.223281\n",
      "ep 4073: ep_len:1627 episode reward: total was -2.480000. running mean: -11.135848\n",
      "ep 4073: ep_len:704 episode reward: total was 34.590000. running mean: -10.678589\n",
      "ep 4073: ep_len:1136 episode reward: total was 3.110000. running mean: -10.540704\n",
      "ep 4073: ep_len:83 episode reward: total was 40.000000. running mean: -10.035296\n",
      "ep 4073: ep_len:103 episode reward: total was 50.000000. running mean: -9.434944\n",
      "ep 4073: ep_len:722 episode reward: total was -33.350000. running mean: -9.674094\n",
      "ep 4073: ep_len:2800 episode reward: total was -54.770000. running mean: -10.125053\n",
      "ep 4073: ep_len:58 episode reward: total was 26.000000. running mean: -9.763803\n",
      "epsilon:0.009992 episode_count: 61237. steps_count: 66094282.000000\n",
      "ep 4074: ep_len:871 episode reward: total was 23.110000. running mean: -9.435065\n",
      "ep 4074: ep_len:721 episode reward: total was 19.110000. running mean: -9.149614\n",
      "ep 4074: ep_len:2904 episode reward: total was -21.170000. running mean: -9.269818\n",
      "ep 4074: ep_len:3991 episode reward: total was -1065.410000. running mean: -19.831220\n",
      "ep 4074: ep_len:48 episode reward: total was 21.000000. running mean: -19.422907\n",
      "ep 4074: ep_len:129 episode reward: total was 63.000000. running mean: -18.598678\n",
      "ep 4074: ep_len:87 episode reward: total was 42.000000. running mean: -17.992692\n",
      "ep 4074: ep_len:840 episode reward: total was 36.850000. running mean: -17.444265\n",
      "ep 4074: ep_len:3657 episode reward: total was -30.190000. running mean: -17.571722\n",
      "ep 4074: ep_len:803 episode reward: total was -26.170000. running mean: -17.657705\n",
      "ep 4074: ep_len:713 episode reward: total was 34.960000. running mean: -17.131528\n",
      "ep 4074: ep_len:976 episode reward: total was 1.360000. running mean: -16.946612\n",
      "ep 4074: ep_len:114 episode reward: total was 54.000000. running mean: -16.237146\n",
      "ep 4074: ep_len:99 episode reward: total was 45.000000. running mean: -15.624775\n",
      "ep 4074: ep_len:802 episode reward: total was -3.590000. running mean: -15.504427\n",
      "ep 4074: ep_len:2864 episode reward: total was 24.530000. running mean: -15.104083\n",
      "epsilon:0.009992 episode_count: 61253. steps_count: 66113901.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4075: ep_len:818 episode reward: total was 26.590000. running mean: -14.687142\n",
      "ep 4075: ep_len:801 episode reward: total was -23.190000. running mean: -14.772171\n",
      "ep 4075: ep_len:76 episode reward: total was 33.500000. running mean: -14.289449\n",
      "ep 4075: ep_len:3072 episode reward: total was -37.130000. running mean: -14.517854\n",
      "ep 4075: ep_len:882 episode reward: total was 1.820000. running mean: -14.354476\n",
      "ep 4075: ep_len:1390 episode reward: total was -201.800000. running mean: -16.228931\n",
      "ep 4075: ep_len:680 episode reward: total was 21.570000. running mean: -15.850942\n",
      "ep 4075: ep_len:1163 episode reward: total was -27.930000. running mean: -15.971732\n",
      "ep 4075: ep_len:647 episode reward: total was 18.460000. running mean: -15.627415\n",
      "ep 4075: ep_len:983 episode reward: total was 62.670000. running mean: -14.844441\n",
      "ep 4075: ep_len:206 episode reward: total was 97.000000. running mean: -13.725996\n",
      "ep 4075: ep_len:55 episode reward: total was 26.000000. running mean: -13.328737\n",
      "ep 4075: ep_len:96 episode reward: total was 46.500000. running mean: -12.730449\n",
      "ep 4075: ep_len:1092 episode reward: total was -12.480000. running mean: -12.727945\n",
      "ep 4075: ep_len:2878 episode reward: total was -1.600000. running mean: -12.616665\n",
      "ep 4075: ep_len:47 episode reward: total was 22.000000. running mean: -12.270499\n",
      "epsilon:0.009992 episode_count: 61269. steps_count: 66128787.000000\n",
      "ep 4076: ep_len:1085 episode reward: total was -12.550000. running mean: -12.273294\n",
      "ep 4076: ep_len:700 episode reward: total was -21.040000. running mean: -12.360961\n",
      "ep 4076: ep_len:2991 episode reward: total was -9.940000. running mean: -12.336751\n",
      "ep 4076: ep_len:605 episode reward: total was -9.680000. running mean: -12.310184\n",
      "ep 4076: ep_len:48 episode reward: total was 22.500000. running mean: -11.962082\n",
      "ep 4076: ep_len:1501 episode reward: total was -121.220000. running mean: -13.054661\n",
      "ep 4076: ep_len:3786 episode reward: total was -6.780000. running mean: -12.991914\n",
      "ep 4076: ep_len:869 episode reward: total was 27.100000. running mean: -12.590995\n",
      "ep 4076: ep_len:768 episode reward: total was 25.650000. running mean: -12.208585\n",
      "ep 4076: ep_len:619 episode reward: total was -8.070000. running mean: -12.167199\n",
      "ep 4076: ep_len:90 episode reward: total was 43.500000. running mean: -11.610527\n",
      "ep 4076: ep_len:149 episode reward: total was 71.500000. running mean: -10.779422\n",
      "ep 4076: ep_len:48 episode reward: total was 22.500000. running mean: -10.446628\n",
      "ep 4076: ep_len:53 episode reward: total was 25.000000. running mean: -10.092162\n",
      "ep 4076: ep_len:659 episode reward: total was -19.840000. running mean: -10.189640\n",
      "ep 4076: ep_len:2837 episode reward: total was -21.260000. running mean: -10.300344\n",
      "epsilon:0.009992 episode_count: 61285. steps_count: 66145595.000000\n",
      "ep 4077: ep_len:817 episode reward: total was -24.320000. running mean: -10.440540\n",
      "ep 4077: ep_len:656 episode reward: total was 0.910000. running mean: -10.327035\n",
      "ep 4077: ep_len:2920 episode reward: total was -32.600000. running mean: -10.549764\n",
      "ep 4077: ep_len:745 episode reward: total was -14.940000. running mean: -10.593667\n",
      "ep 4077: ep_len:37 episode reward: total was 17.000000. running mean: -10.317730\n",
      "ep 4077: ep_len:101 episode reward: total was 47.500000. running mean: -9.739553\n",
      "ep 4077: ep_len:96 episode reward: total was 45.000000. running mean: -9.192157\n",
      "ep 4077: ep_len:47 episode reward: total was 20.500000. running mean: -8.895236\n",
      "ep 4077: ep_len:1369 episode reward: total was 22.060000. running mean: -8.585683\n",
      "ep 4077: ep_len:3623 episode reward: total was -9.130000. running mean: -8.591126\n",
      "ep 4077: ep_len:861 episode reward: total was -41.420000. running mean: -8.919415\n",
      "ep 4077: ep_len:665 episode reward: total was 1.800000. running mean: -8.812221\n",
      "ep 4077: ep_len:971 episode reward: total was 24.600000. running mean: -8.478099\n",
      "ep 4077: ep_len:175 episode reward: total was 80.000000. running mean: -7.593318\n",
      "ep 4077: ep_len:66 episode reward: total was 28.500000. running mean: -7.232385\n",
      "ep 4077: ep_len:1514 episode reward: total was -8.970000. running mean: -7.249761\n",
      "ep 4077: ep_len:2781 episode reward: total was -1.520000. running mean: -7.192463\n",
      "epsilon:0.009992 episode_count: 61302. steps_count: 66163039.000000\n",
      "ep 4078: ep_len:1108 episode reward: total was -5.340000. running mean: -7.173939\n",
      "ep 4078: ep_len:971 episode reward: total was 29.990000. running mean: -6.802299\n",
      "ep 4078: ep_len:54 episode reward: total was 25.500000. running mean: -6.479276\n",
      "ep 4078: ep_len:2965 episode reward: total was -16.010000. running mean: -6.574583\n",
      "ep 4078: ep_len:749 episode reward: total was -26.390000. running mean: -6.772738\n",
      "ep 4078: ep_len:500 episode reward: total was 37.700000. running mean: -6.328010\n",
      "ep 4078: ep_len:3806 episode reward: total was -28.960000. running mean: -6.554330\n",
      "ep 4078: ep_len:886 episode reward: total was -14.140000. running mean: -6.630187\n",
      "ep 4078: ep_len:798 episode reward: total was 7.080000. running mean: -6.493085\n",
      "ep 4078: ep_len:1198 episode reward: total was -34.650000. running mean: -6.774654\n",
      "ep 4078: ep_len:62 episode reward: total was 28.000000. running mean: -6.426908\n",
      "ep 4078: ep_len:65 episode reward: total was 31.000000. running mean: -6.052638\n",
      "ep 4078: ep_len:108 episode reward: total was 49.500000. running mean: -5.497112\n",
      "ep 4078: ep_len:774 episode reward: total was -85.350000. running mean: -6.295641\n",
      "ep 4078: ep_len:2780 episode reward: total was -1.540000. running mean: -6.248085\n",
      "epsilon:0.009992 episode_count: 61317. steps_count: 66179863.000000\n",
      "ep 4079: ep_len:1441 episode reward: total was 32.000000. running mean: -5.865604\n",
      "ep 4079: ep_len:169 episode reward: total was 0.850000. running mean: -5.798448\n",
      "ep 4079: ep_len:86 episode reward: total was 41.500000. running mean: -5.325463\n",
      "ep 4079: ep_len:532 episode reward: total was -25.610000. running mean: -5.528309\n",
      "ep 4079: ep_len:164 episode reward: total was 80.500000. running mean: -4.668025\n",
      "ep 4079: ep_len:725 episode reward: total was -12.110000. running mean: -4.742445\n",
      "ep 4079: ep_len:648 episode reward: total was 20.240000. running mean: -4.492621\n",
      "ep 4079: ep_len:1215 episode reward: total was -43.570000. running mean: -4.883395\n",
      "ep 4079: ep_len:821 episode reward: total was -167.670000. running mean: -6.511261\n",
      "ep 4079: ep_len:603 episode reward: total was 69.500000. running mean: -5.751148\n",
      "ep 4079: ep_len:204 episode reward: total was 96.000000. running mean: -4.733637\n",
      "ep 4079: ep_len:596 episode reward: total was 4.780000. running mean: -4.638500\n",
      "ep 4079: ep_len:2872 episode reward: total was -31.620000. running mean: -4.908315\n",
      "epsilon:0.009992 episode_count: 61330. steps_count: 66189939.000000\n",
      "ep 4080: ep_len:1154 episode reward: total was -2.450000. running mean: -4.883732\n",
      "ep 4080: ep_len:928 episode reward: total was 20.070000. running mean: -4.634195\n",
      "ep 4080: ep_len:2961 episode reward: total was -58.060000. running mean: -5.168453\n",
      "ep 4080: ep_len:664 episode reward: total was -5.540000. running mean: -5.172168\n",
      "ep 4080: ep_len:60 episode reward: total was 28.500000. running mean: -4.835447\n",
      "ep 4080: ep_len:48 episode reward: total was 22.500000. running mean: -4.562092\n",
      "ep 4080: ep_len:1451 episode reward: total was -284.200000. running mean: -7.358471\n",
      "ep 4080: ep_len:3649 episode reward: total was -38.040000. running mean: -7.665286\n",
      "ep 4080: ep_len:603 episode reward: total was -23.430000. running mean: -7.822934\n",
      "ep 4080: ep_len:706 episode reward: total was 41.590000. running mean: -7.328804\n",
      "ep 4080: ep_len:1500 episode reward: total was -23.590000. running mean: -7.491416\n",
      "ep 4080: ep_len:90 episode reward: total was 43.500000. running mean: -6.981502\n",
      "ep 4080: ep_len:1135 episode reward: total was -14.730000. running mean: -7.058987\n",
      "ep 4080: ep_len:2850 episode reward: total was 5.260000. running mean: -6.935797\n",
      "ep 4080: ep_len:46 episode reward: total was 21.500000. running mean: -6.651439\n",
      "epsilon:0.009992 episode_count: 61345. steps_count: 66207784.000000\n",
      "ep 4081: ep_len:1167 episode reward: total was 10.870000. running mean: -6.476225\n",
      "ep 4081: ep_len:750 episode reward: total was -28.130000. running mean: -6.692763\n",
      "ep 4081: ep_len:2950 episode reward: total was -33.160000. running mean: -6.957435\n",
      "ep 4081: ep_len:796 episode reward: total was 3.130000. running mean: -6.856561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4081: ep_len:50 episode reward: total was 23.500000. running mean: -6.552995\n",
      "ep 4081: ep_len:143 episode reward: total was 68.500000. running mean: -5.802465\n",
      "ep 4081: ep_len:739 episode reward: total was -45.150000. running mean: -6.195940\n",
      "ep 4081: ep_len:653 episode reward: total was 24.060000. running mean: -5.893381\n",
      "ep 4081: ep_len:1557 episode reward: total was -55.010000. running mean: -6.384547\n",
      "ep 4081: ep_len:744 episode reward: total was 30.430000. running mean: -6.016402\n",
      "ep 4081: ep_len:867 episode reward: total was 24.170000. running mean: -5.714538\n",
      "ep 4081: ep_len:79 episode reward: total was 38.000000. running mean: -5.277392\n",
      "ep 4081: ep_len:58 episode reward: total was 27.500000. running mean: -4.949618\n",
      "ep 4081: ep_len:1082 episode reward: total was -3.490000. running mean: -4.935022\n",
      "ep 4081: ep_len:2768 episode reward: total was 7.710000. running mean: -4.808572\n",
      "epsilon:0.009992 episode_count: 61360. steps_count: 66222187.000000\n",
      "ep 4082: ep_len:643 episode reward: total was -2.590000. running mean: -4.786386\n",
      "ep 4082: ep_len:998 episode reward: total was 7.640000. running mean: -4.662122\n",
      "ep 4082: ep_len:76 episode reward: total was 35.000000. running mean: -4.265501\n",
      "ep 4082: ep_len:2967 episode reward: total was -30.260000. running mean: -4.525446\n",
      "ep 4082: ep_len:1201 episode reward: total was -22.700000. running mean: -4.707192\n",
      "ep 4082: ep_len:684 episode reward: total was 6.300000. running mean: -4.597120\n",
      "ep 4082: ep_len:3605 episode reward: total was -80.900000. running mean: -5.360149\n",
      "ep 4082: ep_len:841 episode reward: total was -22.920000. running mean: -5.535747\n",
      "ep 4082: ep_len:689 episode reward: total was 18.280000. running mean: -5.297590\n",
      "ep 4082: ep_len:1452 episode reward: total was 13.460000. running mean: -5.110014\n",
      "ep 4082: ep_len:63 episode reward: total was 30.000000. running mean: -4.758914\n",
      "ep 4082: ep_len:128 episode reward: total was 61.000000. running mean: -4.101324\n",
      "ep 4082: ep_len:643 episode reward: total was 9.150000. running mean: -3.968811\n",
      "ep 4082: ep_len:2834 episode reward: total was -15.230000. running mean: -4.081423\n",
      "epsilon:0.009992 episode_count: 61374. steps_count: 66239011.000000\n",
      "ep 4083: ep_len:1449 episode reward: total was 24.940000. running mean: -3.791209\n",
      "ep 4083: ep_len:642 episode reward: total was 21.520000. running mean: -3.538097\n",
      "ep 4083: ep_len:2975 episode reward: total was -73.070000. running mean: -4.233416\n",
      "ep 4083: ep_len:679 episode reward: total was 10.470000. running mean: -4.086382\n",
      "ep 4083: ep_len:63 episode reward: total was 30.000000. running mean: -3.745518\n",
      "ep 4083: ep_len:500 episode reward: total was 22.810000. running mean: -3.479963\n",
      "ep 4083: ep_len:330 episode reward: total was 22.030000. running mean: -3.224863\n",
      "ep 4083: ep_len:1521 episode reward: total was -12.200000. running mean: -3.314614\n",
      "ep 4083: ep_len:791 episode reward: total was 9.370000. running mean: -3.187768\n",
      "ep 4083: ep_len:666 episode reward: total was -1.590000. running mean: -3.171791\n",
      "ep 4083: ep_len:131 episode reward: total was 62.010000. running mean: -2.519973\n",
      "ep 4083: ep_len:595 episode reward: total was -10.380000. running mean: -2.598573\n",
      "ep 4083: ep_len:2824 episode reward: total was 0.990000. running mean: -2.562687\n",
      "epsilon:0.009992 episode_count: 61387. steps_count: 66252177.000000\n",
      "ep 4084: ep_len:1190 episode reward: total was 20.490000. running mean: -2.332160\n",
      "ep 4084: ep_len:737 episode reward: total was 0.550000. running mean: -2.303339\n",
      "ep 4084: ep_len:41 episode reward: total was 19.000000. running mean: -2.090305\n",
      "ep 4084: ep_len:2838 episode reward: total was -45.090000. running mean: -2.520302\n",
      "ep 4084: ep_len:504 episode reward: total was 8.740000. running mean: -2.407699\n",
      "ep 4084: ep_len:43 episode reward: total was 20.000000. running mean: -2.183622\n",
      "ep 4084: ep_len:161 episode reward: total was 76.000000. running mean: -1.401786\n",
      "ep 4084: ep_len:1096 episode reward: total was -0.230000. running mean: -1.390068\n",
      "ep 4084: ep_len:3817 episode reward: total was -85.590000. running mean: -2.232068\n",
      "ep 4084: ep_len:568 episode reward: total was -72.110000. running mean: -2.930847\n",
      "ep 4084: ep_len:654 episode reward: total was -20.350000. running mean: -3.105038\n",
      "ep 4084: ep_len:500 episode reward: total was 43.700000. running mean: -2.636988\n",
      "ep 4084: ep_len:147 episode reward: total was 71.510000. running mean: -1.895518\n",
      "ep 4084: ep_len:1142 episode reward: total was -16.510000. running mean: -2.041663\n",
      "ep 4084: ep_len:2863 episode reward: total was 5.870000. running mean: -1.962546\n",
      "epsilon:0.009992 episode_count: 61402. steps_count: 66268478.000000\n",
      "ep 4085: ep_len:1388 episode reward: total was -4.350000. running mean: -1.986421\n",
      "ep 4085: ep_len:702 episode reward: total was -35.370000. running mean: -2.320257\n",
      "ep 4085: ep_len:3118 episode reward: total was -36.760000. running mean: -2.664654\n",
      "ep 4085: ep_len:1241 episode reward: total was -18.520000. running mean: -2.823208\n",
      "ep 4085: ep_len:500 episode reward: total was 38.770000. running mean: -2.407275\n",
      "ep 4085: ep_len:3590 episode reward: total was -317.400000. running mean: -5.557203\n",
      "ep 4085: ep_len:643 episode reward: total was -50.690000. running mean: -6.008531\n",
      "ep 4085: ep_len:657 episode reward: total was 9.650000. running mean: -5.851945\n",
      "ep 4085: ep_len:570 episode reward: total was -18.190000. running mean: -5.975326\n",
      "ep 4085: ep_len:1125 episode reward: total was -11.200000. running mean: -6.027573\n",
      "ep 4085: ep_len:2768 episode reward: total was -15.950000. running mean: -6.126797\n",
      "epsilon:0.009992 episode_count: 61413. steps_count: 66284780.000000\n",
      "ep 4086: ep_len:1456 episode reward: total was 24.460000. running mean: -5.820929\n",
      "ep 4086: ep_len:674 episode reward: total was -16.800000. running mean: -5.930720\n",
      "ep 4086: ep_len:81 episode reward: total was 37.500000. running mean: -5.496412\n",
      "ep 4086: ep_len:2992 episode reward: total was 12.910000. running mean: -5.312348\n",
      "ep 4086: ep_len:555 episode reward: total was -17.970000. running mean: -5.438925\n",
      "ep 4086: ep_len:146 episode reward: total was 70.000000. running mean: -4.684536\n",
      "ep 4086: ep_len:65 episode reward: total was 28.000000. running mean: -4.357690\n",
      "ep 4086: ep_len:1387 episode reward: total was -81.240000. running mean: -5.126513\n",
      "ep 4086: ep_len:3608 episode reward: total was -61.680000. running mean: -5.692048\n",
      "ep 4086: ep_len:571 episode reward: total was 6.700000. running mean: -5.568128\n",
      "ep 4086: ep_len:658 episode reward: total was -8.030000. running mean: -5.592746\n",
      "ep 4086: ep_len:661 episode reward: total was 20.930000. running mean: -5.327519\n",
      "ep 4086: ep_len:54 episode reward: total was 25.500000. running mean: -5.019244\n",
      "ep 4086: ep_len:1240 episode reward: total was 8.050000. running mean: -4.888551\n",
      "ep 4086: ep_len:2902 episode reward: total was -15.370000. running mean: -4.993366\n",
      "ep 4086: ep_len:37 episode reward: total was 17.000000. running mean: -4.773432\n",
      "epsilon:0.009992 episode_count: 61429. steps_count: 66301867.000000\n",
      "ep 4087: ep_len:751 episode reward: total was -66.390000. running mean: -5.389598\n",
      "ep 4087: ep_len:648 episode reward: total was 14.370000. running mean: -5.192002\n",
      "ep 4087: ep_len:2978 episode reward: total was -72.240000. running mean: -5.862482\n",
      "ep 4087: ep_len:671 episode reward: total was 1.600000. running mean: -5.787857\n",
      "ep 4087: ep_len:38 episode reward: total was 14.500000. running mean: -5.584978\n",
      "ep 4087: ep_len:69 episode reward: total was 33.000000. running mean: -5.199129\n",
      "ep 4087: ep_len:62 episode reward: total was 29.500000. running mean: -4.852137\n",
      "ep 4087: ep_len:1443 episode reward: total was -4.930000. running mean: -4.852916\n",
      "ep 4087: ep_len:641 episode reward: total was 20.510000. running mean: -4.599287\n",
      "ep 4087: ep_len:1535 episode reward: total was -139.990000. running mean: -5.953194\n",
      "ep 4087: ep_len:7528 episode reward: total was -154.440000. running mean: -7.438062\n",
      "ep 4087: ep_len:899 episode reward: total was 58.180000. running mean: -6.781881\n",
      "ep 4087: ep_len:1142 episode reward: total was -76.500000. running mean: -7.479063\n",
      "ep 4087: ep_len:2860 episode reward: total was -81.290000. running mean: -8.217172\n",
      "epsilon:0.009992 episode_count: 61443. steps_count: 66323132.000000\n",
      "ep 4088: ep_len:1112 episode reward: total was 5.010000. running mean: -8.084900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4088: ep_len:921 episode reward: total was 8.550000. running mean: -7.918551\n",
      "ep 4088: ep_len:2930 episode reward: total was -14.500000. running mean: -7.984366\n",
      "ep 4088: ep_len:1247 episode reward: total was -57.970000. running mean: -8.484222\n",
      "ep 4088: ep_len:90 episode reward: total was 40.500000. running mean: -7.994380\n",
      "ep 4088: ep_len:500 episode reward: total was 35.030000. running mean: -7.564136\n",
      "ep 4088: ep_len:3556 episode reward: total was -55.100000. running mean: -8.039495\n",
      "ep 4088: ep_len:1293 episode reward: total was -47.690000. running mean: -8.436000\n",
      "ep 4088: ep_len:710 episode reward: total was 33.550000. running mean: -8.016140\n",
      "ep 4088: ep_len:731 episode reward: total was 25.000000. running mean: -7.685978\n",
      "ep 4088: ep_len:113 episode reward: total was 52.000000. running mean: -7.089119\n",
      "ep 4088: ep_len:1124 episode reward: total was 9.390000. running mean: -6.924327\n",
      "ep 4088: ep_len:2831 episode reward: total was -44.060000. running mean: -7.295684\n",
      "epsilon:0.009992 episode_count: 61456. steps_count: 66340290.000000\n",
      "ep 4089: ep_len:815 episode reward: total was -2.410000. running mean: -7.246827\n",
      "ep 4089: ep_len:696 episode reward: total was -21.150000. running mean: -7.385859\n",
      "ep 4089: ep_len:65 episode reward: total was 29.500000. running mean: -7.017000\n",
      "ep 4089: ep_len:2934 episode reward: total was -32.500000. running mean: -7.271830\n",
      "ep 4089: ep_len:649 episode reward: total was 27.710000. running mean: -6.922012\n",
      "ep 4089: ep_len:1441 episode reward: total was -79.190000. running mean: -7.644692\n",
      "ep 4089: ep_len:3674 episode reward: total was -49.300000. running mean: -8.061245\n",
      "ep 4089: ep_len:1092 episode reward: total was -15.450000. running mean: -8.135133\n",
      "ep 4089: ep_len:636 episode reward: total was -0.970000. running mean: -8.063481\n",
      "ep 4089: ep_len:601 episode reward: total was 22.210000. running mean: -7.760746\n",
      "ep 4089: ep_len:1067 episode reward: total was -37.980000. running mean: -8.062939\n",
      "ep 4089: ep_len:2911 episode reward: total was -5.030000. running mean: -8.032610\n",
      "epsilon:0.009992 episode_count: 61468. steps_count: 66356871.000000\n",
      "ep 4090: ep_len:590 episode reward: total was -24.330000. running mean: -8.195584\n",
      "ep 4090: ep_len:760 episode reward: total was -8.730000. running mean: -8.200928\n",
      "ep 4090: ep_len:85 episode reward: total was 39.500000. running mean: -7.723918\n",
      "ep 4090: ep_len:500 episode reward: total was 31.690000. running mean: -7.329779\n",
      "ep 4090: ep_len:1084 episode reward: total was -21.650000. running mean: -7.472981\n",
      "ep 4090: ep_len:3798 episode reward: total was -24.230000. running mean: -7.640552\n",
      "ep 4090: ep_len:1176 episode reward: total was -3.340000. running mean: -7.597546\n",
      "ep 4090: ep_len:664 episode reward: total was 9.780000. running mean: -7.423771\n",
      "ep 4090: ep_len:891 episode reward: total was 49.370000. running mean: -6.855833\n",
      "ep 4090: ep_len:75 episode reward: total was 36.000000. running mean: -6.427275\n",
      "ep 4090: ep_len:96 episode reward: total was 46.500000. running mean: -5.898002\n",
      "ep 4090: ep_len:1551 episode reward: total was -19.880000. running mean: -6.037822\n",
      "ep 4090: ep_len:2839 episode reward: total was 0.560000. running mean: -5.971844\n",
      "epsilon:0.009992 episode_count: 61481. steps_count: 66370980.000000\n",
      "ep 4091: ep_len:659 episode reward: total was 22.290000. running mean: -5.689225\n",
      "ep 4091: ep_len:965 episode reward: total was 18.300000. running mean: -5.449333\n",
      "ep 4091: ep_len:54 episode reward: total was 25.500000. running mean: -5.139840\n",
      "ep 4091: ep_len:3013 episode reward: total was 0.140000. running mean: -5.087041\n",
      "ep 4091: ep_len:629 episode reward: total was -162.950000. running mean: -6.665671\n",
      "ep 4091: ep_len:48 episode reward: total was 22.500000. running mean: -6.374014\n",
      "ep 4091: ep_len:91 episode reward: total was 44.000000. running mean: -5.870274\n",
      "ep 4091: ep_len:1467 episode reward: total was 2.870000. running mean: -5.782871\n",
      "ep 4091: ep_len:599 episode reward: total was 12.470000. running mean: -5.600343\n",
      "ep 4091: ep_len:791 episode reward: total was -10.080000. running mean: -5.645139\n",
      "ep 4091: ep_len:859 episode reward: total was 61.990000. running mean: -4.968788\n",
      "ep 4091: ep_len:701 episode reward: total was -14.470000. running mean: -5.063800\n",
      "ep 4091: ep_len:93 episode reward: total was 43.500000. running mean: -4.578162\n",
      "ep 4091: ep_len:129 episode reward: total was 57.000000. running mean: -3.962380\n",
      "ep 4091: ep_len:35 episode reward: total was 16.000000. running mean: -3.762756\n",
      "ep 4091: ep_len:1514 episode reward: total was -6.730000. running mean: -3.792429\n",
      "ep 4091: ep_len:2810 episode reward: total was -25.510000. running mean: -4.009605\n",
      "epsilon:0.009992 episode_count: 61498. steps_count: 66385437.000000\n",
      "ep 4092: ep_len:953 episode reward: total was -96.160000. running mean: -4.931109\n",
      "ep 4092: ep_len:500 episode reward: total was 19.660000. running mean: -4.685197\n",
      "ep 4092: ep_len:56 episode reward: total was 26.500000. running mean: -4.373345\n",
      "ep 4092: ep_len:3112 episode reward: total was -5.420000. running mean: -4.383812\n",
      "ep 4092: ep_len:1674 episode reward: total was 6.020000. running mean: -4.279774\n",
      "ep 4092: ep_len:43 episode reward: total was 20.000000. running mean: -4.036976\n",
      "ep 4092: ep_len:1437 episode reward: total was 13.650000. running mean: -3.860106\n",
      "ep 4092: ep_len:351 episode reward: total was 14.280000. running mean: -3.678705\n",
      "ep 4092: ep_len:1275 episode reward: total was -153.240000. running mean: -5.174318\n",
      "ep 4092: ep_len:629 episode reward: total was -8.600000. running mean: -5.208575\n",
      "ep 4092: ep_len:1140 episode reward: total was -31.620000. running mean: -5.472689\n",
      "ep 4092: ep_len:198 episode reward: total was 97.500000. running mean: -4.442962\n",
      "ep 4092: ep_len:55 episode reward: total was 26.000000. running mean: -4.138533\n",
      "ep 4092: ep_len:999 episode reward: total was -14.180000. running mean: -4.238947\n",
      "ep 4092: ep_len:2856 episode reward: total was -47.190000. running mean: -4.668458\n",
      "epsilon:0.009992 episode_count: 61513. steps_count: 66400715.000000\n",
      "ep 4093: ep_len:1439 episode reward: total was 14.740000. running mean: -4.474373\n",
      "ep 4093: ep_len:712 episode reward: total was -27.100000. running mean: -4.700630\n",
      "ep 4093: ep_len:3038 episode reward: total was 12.360000. running mean: -4.530023\n",
      "ep 4093: ep_len:500 episode reward: total was -2.810000. running mean: -4.512823\n",
      "ep 4093: ep_len:56 episode reward: total was 26.500000. running mean: -4.202695\n",
      "ep 4093: ep_len:142 episode reward: total was 69.500000. running mean: -3.465668\n",
      "ep 4093: ep_len:90 episode reward: total was 43.500000. running mean: -2.996011\n",
      "ep 4093: ep_len:790 episode reward: total was -32.030000. running mean: -3.286351\n",
      "ep 4093: ep_len:625 episode reward: total was 23.010000. running mean: -3.023388\n",
      "ep 4093: ep_len:1555 episode reward: total was -38.980000. running mean: -3.382954\n",
      "ep 4093: ep_len:717 episode reward: total was 37.260000. running mean: -2.976524\n",
      "ep 4093: ep_len:579 episode reward: total was 33.560000. running mean: -2.611159\n",
      "ep 4093: ep_len:57 episode reward: total was 27.000000. running mean: -2.315047\n",
      "ep 4093: ep_len:1103 episode reward: total was -2.120000. running mean: -2.313097\n",
      "ep 4093: ep_len:2818 episode reward: total was -53.220000. running mean: -2.822166\n",
      "epsilon:0.009992 episode_count: 61528. steps_count: 66414936.000000\n",
      "ep 4094: ep_len:1045 episode reward: total was -21.030000. running mean: -3.004244\n",
      "ep 4094: ep_len:784 episode reward: total was -0.550000. running mean: -2.979702\n",
      "ep 4094: ep_len:2972 episode reward: total was -10.860000. running mean: -3.058505\n",
      "ep 4094: ep_len:618 episode reward: total was -11.130000. running mean: -3.139220\n",
      "ep 4094: ep_len:1405 episode reward: total was -86.110000. running mean: -3.968928\n",
      "ep 4094: ep_len:3710 episode reward: total was -52.790000. running mean: -4.457138\n",
      "ep 4094: ep_len:688 episode reward: total was -45.900000. running mean: -4.871567\n",
      "ep 4094: ep_len:693 episode reward: total was 36.530000. running mean: -4.457551\n",
      "ep 4094: ep_len:500 episode reward: total was 5.520000. running mean: -4.357776\n",
      "ep 4094: ep_len:1083 episode reward: total was -26.620000. running mean: -4.580398\n",
      "ep 4094: ep_len:2861 episode reward: total was -8.040000. running mean: -4.614994\n",
      "epsilon:0.009992 episode_count: 61539. steps_count: 66431295.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4095: ep_len:2469 episode reward: total was -264.340000. running mean: -7.212244\n",
      "ep 4095: ep_len:500 episode reward: total was 22.840000. running mean: -6.911722\n",
      "ep 4095: ep_len:60 episode reward: total was 28.500000. running mean: -6.557604\n",
      "ep 4095: ep_len:3021 episode reward: total was -47.320000. running mean: -6.965228\n",
      "ep 4095: ep_len:818 episode reward: total was 27.200000. running mean: -6.623576\n",
      "ep 4095: ep_len:50 episode reward: total was 23.500000. running mean: -6.322340\n",
      "ep 4095: ep_len:500 episode reward: total was 4.900000. running mean: -6.210117\n",
      "ep 4095: ep_len:3726 episode reward: total was -35.510000. running mean: -6.503116\n",
      "ep 4095: ep_len:1135 episode reward: total was 13.990000. running mean: -6.298185\n",
      "ep 4095: ep_len:701 episode reward: total was 43.900000. running mean: -5.796203\n",
      "ep 4095: ep_len:1457 episode reward: total was -14.220000. running mean: -5.880441\n",
      "ep 4095: ep_len:53 episode reward: total was 23.500000. running mean: -5.586636\n",
      "ep 4095: ep_len:149 episode reward: total was 72.510000. running mean: -4.805670\n",
      "ep 4095: ep_len:38 episode reward: total was 17.500000. running mean: -4.582613\n",
      "ep 4095: ep_len:1468 episode reward: total was -24.150000. running mean: -4.778287\n",
      "ep 4095: ep_len:2829 episode reward: total was -43.980000. running mean: -5.170304\n",
      "epsilon:0.009992 episode_count: 61555. steps_count: 66450269.000000\n",
      "ep 4096: ep_len:807 episode reward: total was -13.770000. running mean: -5.256301\n",
      "ep 4096: ep_len:500 episode reward: total was 18.490000. running mean: -5.018838\n",
      "ep 4096: ep_len:101 episode reward: total was 49.000000. running mean: -4.478650\n",
      "ep 4096: ep_len:723 episode reward: total was 9.960000. running mean: -4.334263\n",
      "ep 4096: ep_len:146 episode reward: total was 70.000000. running mean: -3.590921\n",
      "ep 4096: ep_len:500 episode reward: total was 29.920000. running mean: -3.255811\n",
      "ep 4096: ep_len:3638 episode reward: total was -26.520000. running mean: -3.488453\n",
      "ep 4096: ep_len:564 episode reward: total was -0.220000. running mean: -3.455769\n",
      "ep 4096: ep_len:754 episode reward: total was 3.330000. running mean: -3.387911\n",
      "ep 4096: ep_len:622 episode reward: total was 11.770000. running mean: -3.236332\n",
      "ep 4096: ep_len:25 episode reward: total was 11.000000. running mean: -3.093969\n",
      "ep 4096: ep_len:111 episode reward: total was 54.000000. running mean: -2.523029\n",
      "ep 4096: ep_len:769 episode reward: total was -104.590000. running mean: -3.543699\n",
      "ep 4096: ep_len:2739 episode reward: total was -4.580000. running mean: -3.554062\n",
      "ep 4096: ep_len:61 episode reward: total was 29.000000. running mean: -3.228521\n",
      "epsilon:0.009992 episode_count: 61570. steps_count: 66462329.000000\n",
      "ep 4097: ep_len:778 episode reward: total was -48.950000. running mean: -3.685736\n",
      "ep 4097: ep_len:647 episode reward: total was -5.790000. running mean: -3.706779\n",
      "ep 4097: ep_len:2995 episode reward: total was 18.080000. running mean: -3.488911\n",
      "ep 4097: ep_len:652 episode reward: total was 14.470000. running mean: -3.309322\n",
      "ep 4097: ep_len:42 episode reward: total was 19.500000. running mean: -3.081228\n",
      "ep 4097: ep_len:65 episode reward: total was 31.000000. running mean: -2.740416\n",
      "ep 4097: ep_len:500 episode reward: total was 27.160000. running mean: -2.441412\n",
      "ep 4097: ep_len:3690 episode reward: total was -14.890000. running mean: -2.565898\n",
      "ep 4097: ep_len:609 episode reward: total was -20.340000. running mean: -2.743639\n",
      "ep 4097: ep_len:714 episode reward: total was 42.190000. running mean: -2.294303\n",
      "ep 4097: ep_len:1482 episode reward: total was -36.740000. running mean: -2.638759\n",
      "ep 4097: ep_len:52 episode reward: total was 23.000000. running mean: -2.382372\n",
      "ep 4097: ep_len:74 episode reward: total was 35.500000. running mean: -2.003548\n",
      "ep 4097: ep_len:1076 episode reward: total was -34.860000. running mean: -2.332113\n",
      "ep 4097: ep_len:2808 episode reward: total was -3.310000. running mean: -2.341892\n",
      "epsilon:0.009992 episode_count: 61585. steps_count: 66478513.000000\n",
      "ep 4098: ep_len:650 episode reward: total was -1.750000. running mean: -2.335973\n",
      "ep 4098: ep_len:765 episode reward: total was -24.320000. running mean: -2.555813\n",
      "ep 4098: ep_len:2913 episode reward: total was -33.460000. running mean: -2.864855\n",
      "ep 4098: ep_len:658 episode reward: total was 21.430000. running mean: -2.621906\n",
      "ep 4098: ep_len:103 episode reward: total was 50.000000. running mean: -2.095687\n",
      "ep 4098: ep_len:669 episode reward: total was -10.140000. running mean: -2.176130\n",
      "ep 4098: ep_len:3643 episode reward: total was 1.810000. running mean: -2.136269\n",
      "ep 4098: ep_len:769 episode reward: total was -7.980000. running mean: -2.194706\n",
      "ep 4098: ep_len:836 episode reward: total was 48.740000. running mean: -1.685359\n",
      "ep 4098: ep_len:1182 episode reward: total was -18.650000. running mean: -1.855006\n",
      "ep 4098: ep_len:43 episode reward: total was 20.000000. running mean: -1.636456\n",
      "ep 4098: ep_len:67 episode reward: total was 32.000000. running mean: -1.300091\n",
      "ep 4098: ep_len:697 episode reward: total was -27.540000. running mean: -1.562490\n",
      "ep 4098: ep_len:2750 episode reward: total was -39.760000. running mean: -1.944465\n",
      "epsilon:0.009992 episode_count: 61599. steps_count: 66494258.000000\n",
      "ep 4099: ep_len:1158 episode reward: total was -7.560000. running mean: -2.000621\n",
      "ep 4099: ep_len:500 episode reward: total was 26.180000. running mean: -1.718814\n",
      "ep 4099: ep_len:2889 episode reward: total was -14.670000. running mean: -1.848326\n",
      "ep 4099: ep_len:653 episode reward: total was -3.190000. running mean: -1.861743\n",
      "ep 4099: ep_len:45 episode reward: total was 21.000000. running mean: -1.633126\n",
      "ep 4099: ep_len:1409 episode reward: total was -18.970000. running mean: -1.806494\n",
      "ep 4099: ep_len:3774 episode reward: total was -21.640000. running mean: -2.004829\n",
      "ep 4099: ep_len:573 episode reward: total was 13.770000. running mean: -1.847081\n",
      "ep 4099: ep_len:754 episode reward: total was 47.860000. running mean: -1.350010\n",
      "ep 4099: ep_len:500 episode reward: total was -7.610000. running mean: -1.412610\n",
      "ep 4099: ep_len:182 episode reward: total was -174.990000. running mean: -3.148384\n",
      "ep 4099: ep_len:61 episode reward: total was 29.000000. running mean: -2.826900\n",
      "ep 4099: ep_len:52 episode reward: total was 24.500000. running mean: -2.553631\n",
      "ep 4099: ep_len:732 episode reward: total was -50.420000. running mean: -3.032295\n",
      "ep 4099: ep_len:2898 episode reward: total was 6.320000. running mean: -2.938772\n",
      "epsilon:0.009992 episode_count: 61614. steps_count: 66510438.000000\n",
      "ep 4100: ep_len:889 episode reward: total was -42.290000. running mean: -3.332284\n",
      "ep 4100: ep_len:1626 episode reward: total was -88.980000. running mean: -4.188761\n",
      "ep 4100: ep_len:55 episode reward: total was 24.500000. running mean: -3.901874\n",
      "ep 4100: ep_len:2995 episode reward: total was 17.490000. running mean: -3.687955\n",
      "ep 4100: ep_len:697 episode reward: total was -6.330000. running mean: -3.714375\n",
      "ep 4100: ep_len:47 episode reward: total was 20.500000. running mean: -3.472232\n",
      "ep 4100: ep_len:76 episode reward: total was 35.000000. running mean: -3.087509\n",
      "ep 4100: ep_len:1023 episode reward: total was -42.440000. running mean: -3.481034\n",
      "ep 4100: ep_len:3769 episode reward: total was 3.920000. running mean: -3.407024\n",
      "ep 4100: ep_len:583 episode reward: total was -4.440000. running mean: -3.417354\n",
      "ep 4100: ep_len:687 episode reward: total was 17.920000. running mean: -3.203980\n",
      "ep 4100: ep_len:1043 episode reward: total was 14.430000. running mean: -3.027640\n",
      "ep 4100: ep_len:121 episode reward: total was 57.500000. running mean: -2.422364\n",
      "ep 4100: ep_len:768 episode reward: total was -15.030000. running mean: -2.548440\n",
      "ep 4100: ep_len:2838 episode reward: total was -4.200000. running mean: -2.564956\n",
      "epsilon:0.009992 episode_count: 61629. steps_count: 66527655.000000\n",
      "ep 4101: ep_len:1172 episode reward: total was 1.830000. running mean: -2.521006\n",
      "ep 4101: ep_len:216 episode reward: total was 6.440000. running mean: -2.431396\n",
      "ep 4101: ep_len:2937 episode reward: total was -25.520000. running mean: -2.662282\n",
      "ep 4101: ep_len:1480 episode reward: total was -36.940000. running mean: -3.005060\n",
      "ep 4101: ep_len:58 episode reward: total was 27.500000. running mean: -2.700009\n",
      "ep 4101: ep_len:67 episode reward: total was 32.000000. running mean: -2.353009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4101: ep_len:540 episode reward: total was -65.240000. running mean: -2.981879\n",
      "ep 4101: ep_len:3613 episode reward: total was -8.100000. running mean: -3.033060\n",
      "ep 4101: ep_len:1199 episode reward: total was -1.860000. running mean: -3.021329\n",
      "ep 4101: ep_len:846 episode reward: total was 59.620000. running mean: -2.394916\n",
      "ep 4101: ep_len:652 episode reward: total was 35.080000. running mean: -2.020167\n",
      "ep 4101: ep_len:90 episode reward: total was 40.500000. running mean: -1.594965\n",
      "ep 4101: ep_len:1425 episode reward: total was -16.310000. running mean: -1.742116\n",
      "ep 4101: ep_len:2900 episode reward: total was 3.770000. running mean: -1.686994\n",
      "ep 4101: ep_len:51 episode reward: total was 22.500000. running mean: -1.445124\n",
      "epsilon:0.009992 episode_count: 61644. steps_count: 66544901.000000\n",
      "ep 4102: ep_len:662 episode reward: total was -27.770000. running mean: -1.708373\n",
      "ep 4102: ep_len:677 episode reward: total was -16.630000. running mean: -1.857590\n",
      "ep 4102: ep_len:2962 episode reward: total was -32.580000. running mean: -2.164814\n",
      "ep 4102: ep_len:500 episode reward: total was 17.090000. running mean: -1.972265\n",
      "ep 4102: ep_len:113 episode reward: total was 55.000000. running mean: -1.402543\n",
      "ep 4102: ep_len:1393 episode reward: total was -34.840000. running mean: -1.736917\n",
      "ep 4102: ep_len:3882 episode reward: total was -18.980000. running mean: -1.909348\n",
      "ep 4102: ep_len:669 episode reward: total was -1.560000. running mean: -1.905855\n",
      "ep 4102: ep_len:673 episode reward: total was -28.300000. running mean: -2.169796\n",
      "ep 4102: ep_len:605 episode reward: total was 32.080000. running mean: -1.827298\n",
      "ep 4102: ep_len:194 episode reward: total was 94.000000. running mean: -0.869025\n",
      "ep 4102: ep_len:1115 episode reward: total was -38.510000. running mean: -1.245435\n",
      "ep 4102: ep_len:2890 episode reward: total was -2.420000. running mean: -1.257181\n",
      "ep 4102: ep_len:53 episode reward: total was 23.500000. running mean: -1.009609\n",
      "epsilon:0.009992 episode_count: 61658. steps_count: 66561289.000000\n",
      "ep 4103: ep_len:1061 episode reward: total was -14.810000. running mean: -1.147613\n",
      "ep 4103: ep_len:936 episode reward: total was -15.050000. running mean: -1.286637\n",
      "ep 4103: ep_len:2954 episode reward: total was -23.820000. running mean: -1.511970\n",
      "ep 4103: ep_len:1226 episode reward: total was -24.600000. running mean: -1.742851\n",
      "ep 4103: ep_len:59 episode reward: total was 28.000000. running mean: -1.445422\n",
      "ep 4103: ep_len:60 episode reward: total was 28.500000. running mean: -1.145968\n",
      "ep 4103: ep_len:1436 episode reward: total was -15.040000. running mean: -1.284908\n",
      "ep 4103: ep_len:3674 episode reward: total was 1.600000. running mean: -1.256059\n",
      "ep 4103: ep_len:1611 episode reward: total was -72.670000. running mean: -1.970198\n",
      "ep 4103: ep_len:755 episode reward: total was 1.290000. running mean: -1.937596\n",
      "ep 4103: ep_len:620 episode reward: total was 7.040000. running mean: -1.847821\n",
      "ep 4103: ep_len:113 episode reward: total was 55.000000. running mean: -1.279342\n",
      "ep 4103: ep_len:593 episode reward: total was -8.380000. running mean: -1.350349\n",
      "ep 4103: ep_len:2916 episode reward: total was 5.000000. running mean: -1.286845\n",
      "epsilon:0.009992 episode_count: 61672. steps_count: 66579303.000000\n",
      "ep 4104: ep_len:1446 episode reward: total was -7.930000. running mean: -1.353277\n",
      "ep 4104: ep_len:634 episode reward: total was 6.260000. running mean: -1.277144\n",
      "ep 4104: ep_len:2973 episode reward: total was 0.560000. running mean: -1.258773\n",
      "ep 4104: ep_len:597 episode reward: total was -5.310000. running mean: -1.299285\n",
      "ep 4104: ep_len:715 episode reward: total was -20.010000. running mean: -1.486392\n",
      "ep 4104: ep_len:354 episode reward: total was 4.580000. running mean: -1.425728\n",
      "ep 4104: ep_len:774 episode reward: total was -10.770000. running mean: -1.519171\n",
      "ep 4104: ep_len:882 episode reward: total was 61.910000. running mean: -0.884879\n",
      "ep 4104: ep_len:726 episode reward: total was -10.080000. running mean: -0.976830\n",
      "ep 4104: ep_len:204 episode reward: total was -166.000000. running mean: -2.627062\n",
      "ep 4104: ep_len:51 episode reward: total was 22.500000. running mean: -2.375792\n",
      "ep 4104: ep_len:500 episode reward: total was 24.560000. running mean: -2.106434\n",
      "ep 4104: ep_len:2773 episode reward: total was -20.340000. running mean: -2.288769\n",
      "ep 4104: ep_len:67 episode reward: total was 30.500000. running mean: -1.960882\n",
      "epsilon:0.009992 episode_count: 61686. steps_count: 66591999.000000\n",
      "ep 4105: ep_len:851 episode reward: total was 31.110000. running mean: -1.630173\n",
      "ep 4105: ep_len:213 episode reward: total was -15.200000. running mean: -1.765871\n",
      "ep 4105: ep_len:62 episode reward: total was 28.000000. running mean: -1.468212\n",
      "ep 4105: ep_len:2926 episode reward: total was -34.840000. running mean: -1.801930\n",
      "ep 4105: ep_len:628 episode reward: total was -6.010000. running mean: -1.844011\n",
      "ep 4105: ep_len:60 episode reward: total was 28.500000. running mean: -1.540571\n",
      "ep 4105: ep_len:500 episode reward: total was -18.030000. running mean: -1.705465\n",
      "ep 4105: ep_len:686 episode reward: total was 12.270000. running mean: -1.565710\n",
      "ep 4105: ep_len:853 episode reward: total was -33.230000. running mean: -1.882353\n",
      "ep 4105: ep_len:7245 episode reward: total was -374.820000. running mean: -5.611730\n",
      "ep 4105: ep_len:3589 episode reward: total was -466.250000. running mean: -10.218113\n",
      "ep 4105: ep_len:50 episode reward: total was 23.500000. running mean: -9.880931\n",
      "ep 4105: ep_len:1134 episode reward: total was -8.140000. running mean: -9.863522\n",
      "ep 4105: ep_len:2901 episode reward: total was -35.300000. running mean: -10.117887\n",
      "ep 4105: ep_len:48 episode reward: total was 22.500000. running mean: -9.791708\n",
      "epsilon:0.009992 episode_count: 61701. steps_count: 66613745.000000\n",
      "ep 4106: ep_len:500 episode reward: total was 10.660000. running mean: -9.587191\n",
      "ep 4106: ep_len:3748 episode reward: total was -447.000000. running mean: -13.961319\n",
      "ep 4106: ep_len:3047 episode reward: total was -1.920000. running mean: -13.840906\n",
      "ep 4106: ep_len:513 episode reward: total was 15.030000. running mean: -13.552197\n",
      "ep 4106: ep_len:768 episode reward: total was -25.670000. running mean: -13.673375\n",
      "ep 4106: ep_len:317 episode reward: total was -14.000000. running mean: -13.676641\n",
      "ep 4106: ep_len:660 episode reward: total was 7.440000. running mean: -13.465475\n",
      "ep 4106: ep_len:922 episode reward: total was 70.610000. running mean: -12.624720\n",
      "ep 4106: ep_len:1531 episode reward: total was -71.930000. running mean: -13.217773\n",
      "ep 4106: ep_len:78 episode reward: total was 34.500000. running mean: -12.740595\n",
      "ep 4106: ep_len:175 episode reward: total was 86.000000. running mean: -11.753189\n",
      "ep 4106: ep_len:36 episode reward: total was 15.000000. running mean: -11.485657\n",
      "ep 4106: ep_len:53 episode reward: total was 25.000000. running mean: -11.120801\n",
      "ep 4106: ep_len:761 episode reward: total was -29.930000. running mean: -11.308893\n",
      "ep 4106: ep_len:2770 episode reward: total was -6.750000. running mean: -11.263304\n",
      "ep 4106: ep_len:57 episode reward: total was 25.500000. running mean: -10.895671\n",
      "epsilon:0.009992 episode_count: 61717. steps_count: 66629681.000000\n",
      "ep 4107: ep_len:1454 episode reward: total was -15.750000. running mean: -10.944214\n",
      "ep 4107: ep_len:656 episode reward: total was -23.360000. running mean: -11.068372\n",
      "ep 4107: ep_len:3113 episode reward: total was 5.420000. running mean: -10.903488\n",
      "ep 4107: ep_len:643 episode reward: total was 6.030000. running mean: -10.734153\n",
      "ep 4107: ep_len:40 episode reward: total was 18.500000. running mean: -10.441812\n",
      "ep 4107: ep_len:124 episode reward: total was 59.000000. running mean: -9.747393\n",
      "ep 4107: ep_len:38 episode reward: total was 16.000000. running mean: -9.489920\n",
      "ep 4107: ep_len:1400 episode reward: total was -53.160000. running mean: -9.926620\n",
      "ep 4107: ep_len:3559 episode reward: total was -21.770000. running mean: -10.045054\n",
      "ep 4107: ep_len:916 episode reward: total was -13.230000. running mean: -10.076904\n",
      "ep 4107: ep_len:850 episode reward: total was 43.770000. running mean: -9.538435\n",
      "ep 4107: ep_len:1414 episode reward: total was -98.300000. running mean: -10.426050\n",
      "ep 4107: ep_len:170 episode reward: total was 82.000000. running mean: -9.501790\n",
      "ep 4107: ep_len:44 episode reward: total was 19.000000. running mean: -9.216772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4107: ep_len:91 episode reward: total was 42.500000. running mean: -8.699604\n",
      "ep 4107: ep_len:784 episode reward: total was -1.940000. running mean: -8.632008\n",
      "ep 4107: ep_len:2907 episode reward: total was -12.540000. running mean: -8.671088\n",
      "ep 4107: ep_len:51 episode reward: total was 22.500000. running mean: -8.359377\n",
      "epsilon:0.009992 episode_count: 61735. steps_count: 66647935.000000\n",
      "ep 4108: ep_len:704 episode reward: total was -2.220000. running mean: -8.297983\n",
      "ep 4108: ep_len:989 episode reward: total was 11.620000. running mean: -8.098804\n",
      "ep 4108: ep_len:2893 episode reward: total was -23.270000. running mean: -8.250515\n",
      "ep 4108: ep_len:817 episode reward: total was -24.400000. running mean: -8.412010\n",
      "ep 4108: ep_len:44 episode reward: total was 20.500000. running mean: -8.122890\n",
      "ep 4108: ep_len:1422 episode reward: total was -22.650000. running mean: -8.268161\n",
      "ep 4108: ep_len:679 episode reward: total was -1.760000. running mean: -8.203080\n",
      "ep 4108: ep_len:806 episode reward: total was -3.560000. running mean: -8.156649\n",
      "ep 4108: ep_len:834 episode reward: total was 51.580000. running mean: -7.559282\n",
      "ep 4108: ep_len:1073 episode reward: total was -21.150000. running mean: -7.695190\n",
      "ep 4108: ep_len:91 episode reward: total was 44.000000. running mean: -7.178238\n",
      "ep 4108: ep_len:48 episode reward: total was 22.500000. running mean: -6.881455\n",
      "ep 4108: ep_len:67 episode reward: total was 30.500000. running mean: -6.507641\n",
      "ep 4108: ep_len:575 episode reward: total was -25.670000. running mean: -6.699264\n",
      "ep 4108: ep_len:2957 episode reward: total was -8.910000. running mean: -6.721372\n",
      "epsilon:0.009992 episode_count: 61750. steps_count: 66661934.000000\n",
      "ep 4109: ep_len:942 episode reward: total was -10.520000. running mean: -6.759358\n",
      "ep 4109: ep_len:730 episode reward: total was -17.830000. running mean: -6.870064\n",
      "ep 4109: ep_len:2937 episode reward: total was 2.730000. running mean: -6.774064\n",
      "ep 4109: ep_len:846 episode reward: total was 12.100000. running mean: -6.585323\n",
      "ep 4109: ep_len:125 episode reward: total was 61.000000. running mean: -5.909470\n",
      "ep 4109: ep_len:60 episode reward: total was 28.500000. running mean: -5.565375\n",
      "ep 4109: ep_len:612 episode reward: total was 40.940000. running mean: -5.100321\n",
      "ep 4109: ep_len:657 episode reward: total was -1.670000. running mean: -5.066018\n",
      "ep 4109: ep_len:1515 episode reward: total was -110.300000. running mean: -6.118358\n",
      "ep 4109: ep_len:779 episode reward: total was 33.540000. running mean: -5.721774\n",
      "ep 4109: ep_len:1453 episode reward: total was -20.410000. running mean: -5.868657\n",
      "ep 4109: ep_len:139 episode reward: total was 65.000000. running mean: -5.159970\n",
      "ep 4109: ep_len:1115 episode reward: total was -18.310000. running mean: -5.291470\n",
      "ep 4109: ep_len:2830 episode reward: total was -22.770000. running mean: -5.466256\n",
      "ep 4109: ep_len:58 episode reward: total was 27.500000. running mean: -5.136593\n",
      "epsilon:0.009992 episode_count: 61765. steps_count: 66676732.000000\n",
      "ep 4110: ep_len:734 episode reward: total was -69.590000. running mean: -5.781127\n",
      "ep 4110: ep_len:708 episode reward: total was -11.500000. running mean: -5.838316\n",
      "ep 4110: ep_len:64 episode reward: total was 29.000000. running mean: -5.489933\n",
      "ep 4110: ep_len:3074 episode reward: total was -22.730000. running mean: -5.662334\n",
      "ep 4110: ep_len:676 episode reward: total was -3.270000. running mean: -5.638410\n",
      "ep 4110: ep_len:72 episode reward: total was 34.500000. running mean: -5.237026\n",
      "ep 4110: ep_len:69 episode reward: total was 33.000000. running mean: -4.854656\n",
      "ep 4110: ep_len:987 episode reward: total was -70.090000. running mean: -5.507009\n",
      "ep 4110: ep_len:3591 episode reward: total was -57.230000. running mean: -6.024239\n",
      "ep 4110: ep_len:561 episode reward: total was 6.240000. running mean: -5.901597\n",
      "ep 4110: ep_len:7433 episode reward: total was 39.880000. running mean: -5.443781\n",
      "ep 4110: ep_len:500 episode reward: total was 0.700000. running mean: -5.382343\n",
      "ep 4110: ep_len:43 episode reward: total was 20.000000. running mean: -5.128520\n",
      "ep 4110: ep_len:89 episode reward: total was 41.500000. running mean: -4.662234\n",
      "ep 4110: ep_len:1478 episode reward: total was -23.250000. running mean: -4.848112\n",
      "ep 4110: ep_len:2772 episode reward: total was -1.030000. running mean: -4.809931\n",
      "epsilon:0.009992 episode_count: 61781. steps_count: 66699583.000000\n",
      "ep 4111: ep_len:594 episode reward: total was -25.910000. running mean: -5.020932\n",
      "ep 4111: ep_len:1637 episode reward: total was -91.380000. running mean: -5.884522\n",
      "ep 4111: ep_len:51 episode reward: total was 24.000000. running mean: -5.585677\n",
      "ep 4111: ep_len:3095 episode reward: total was -9.750000. running mean: -5.627320\n",
      "ep 4111: ep_len:1429 episode reward: total was -53.400000. running mean: -6.105047\n",
      "ep 4111: ep_len:47 episode reward: total was 22.000000. running mean: -5.823997\n",
      "ep 4111: ep_len:1161 episode reward: total was -0.390000. running mean: -5.769657\n",
      "ep 4111: ep_len:3844 episode reward: total was -2.620000. running mean: -5.738160\n",
      "ep 4111: ep_len:611 episode reward: total was -39.510000. running mean: -6.075878\n",
      "ep 4111: ep_len:835 episode reward: total was 14.330000. running mean: -5.871820\n",
      "ep 4111: ep_len:1059 episode reward: total was 29.190000. running mean: -5.521201\n",
      "ep 4111: ep_len:57 episode reward: total was 27.000000. running mean: -5.195989\n",
      "ep 4111: ep_len:145 episode reward: total was 68.000000. running mean: -4.464030\n",
      "ep 4111: ep_len:73 episode reward: total was 35.000000. running mean: -4.069389\n",
      "ep 4111: ep_len:1137 episode reward: total was 3.490000. running mean: -3.993795\n",
      "ep 4111: ep_len:2825 episode reward: total was 4.890000. running mean: -3.904957\n",
      "epsilon:0.009992 episode_count: 61797. steps_count: 66718183.000000\n",
      "ep 4112: ep_len:1078 episode reward: total was -5.550000. running mean: -3.921408\n",
      "ep 4112: ep_len:1600 episode reward: total was -162.760000. running mean: -5.509794\n",
      "ep 4112: ep_len:2959 episode reward: total was -54.440000. running mean: -5.999096\n",
      "ep 4112: ep_len:1640 episode reward: total was -126.120000. running mean: -7.200305\n",
      "ep 4112: ep_len:111 episode reward: total was 52.500000. running mean: -6.603302\n",
      "ep 4112: ep_len:1456 episode reward: total was -21.820000. running mean: -6.755469\n",
      "ep 4112: ep_len:648 episode reward: total was -6.290000. running mean: -6.750814\n",
      "ep 4112: ep_len:535 episode reward: total was -0.880000. running mean: -6.692106\n",
      "ep 4112: ep_len:868 episode reward: total was 60.360000. running mean: -6.021585\n",
      "ep 4112: ep_len:965 episode reward: total was 5.290000. running mean: -5.908469\n",
      "ep 4112: ep_len:43 episode reward: total was 17.000000. running mean: -5.679384\n",
      "ep 4112: ep_len:500 episode reward: total was 41.920000. running mean: -5.203391\n",
      "ep 4112: ep_len:2775 episode reward: total was 5.460000. running mean: -5.096757\n",
      "epsilon:0.009992 episode_count: 61810. steps_count: 66733361.000000\n",
      "ep 4113: ep_len:1458 episode reward: total was -18.620000. running mean: -5.231989\n",
      "ep 4113: ep_len:1069 episode reward: total was -92.890000. running mean: -6.108569\n",
      "ep 4113: ep_len:2948 episode reward: total was -16.110000. running mean: -6.208583\n",
      "ep 4113: ep_len:500 episode reward: total was -3.370000. running mean: -6.180198\n",
      "ep 4113: ep_len:914 episode reward: total was 56.800000. running mean: -5.550396\n",
      "ep 4113: ep_len:350 episode reward: total was -5.930000. running mean: -5.554192\n",
      "ep 4113: ep_len:503 episode reward: total was -22.410000. running mean: -5.722750\n",
      "ep 4113: ep_len:847 episode reward: total was 50.390000. running mean: -5.161622\n",
      "ep 4113: ep_len:614 episode reward: total was -2.110000. running mean: -5.131106\n",
      "ep 4113: ep_len:44 episode reward: total was 20.500000. running mean: -4.874795\n",
      "ep 4113: ep_len:139 episode reward: total was 66.500000. running mean: -4.161047\n",
      "ep 4113: ep_len:1466 episode reward: total was -30.810000. running mean: -4.427537\n",
      "ep 4113: ep_len:2862 episode reward: total was 14.960000. running mean: -4.233661\n",
      "epsilon:0.009992 episode_count: 61823. steps_count: 66747075.000000\n",
      "ep 4114: ep_len:1177 episode reward: total was -25.650000. running mean: -4.447825\n",
      "ep 4114: ep_len:825 episode reward: total was 12.230000. running mean: -4.281046\n",
      "ep 4114: ep_len:2987 episode reward: total was -52.530000. running mean: -4.763536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4114: ep_len:1598 episode reward: total was -65.420000. running mean: -5.370101\n",
      "ep 4114: ep_len:25 episode reward: total was 11.000000. running mean: -5.206400\n",
      "ep 4114: ep_len:1028 episode reward: total was -59.960000. running mean: -5.753936\n",
      "ep 4114: ep_len:316 episode reward: total was 5.180000. running mean: -5.644596\n",
      "ep 4114: ep_len:1200 episode reward: total was -51.800000. running mean: -6.106150\n",
      "ep 4114: ep_len:867 episode reward: total was 32.960000. running mean: -5.715489\n",
      "ep 4114: ep_len:682 episode reward: total was 18.720000. running mean: -5.471134\n",
      "ep 4114: ep_len:68 episode reward: total was 32.500000. running mean: -5.091423\n",
      "ep 4114: ep_len:136 episode reward: total was 66.500000. running mean: -4.375508\n",
      "ep 4114: ep_len:1460 episode reward: total was 4.330000. running mean: -4.288453\n",
      "ep 4114: ep_len:2787 episode reward: total was -28.560000. running mean: -4.531169\n",
      "epsilon:0.009992 episode_count: 61837. steps_count: 66762231.000000\n",
      "ep 4115: ep_len:2384 episode reward: total was -993.890000. running mean: -14.424757\n",
      "ep 4115: ep_len:770 episode reward: total was -8.840000. running mean: -14.368909\n",
      "ep 4115: ep_len:3018 episode reward: total was -17.680000. running mean: -14.402020\n",
      "ep 4115: ep_len:647 episode reward: total was -10.840000. running mean: -14.366400\n",
      "ep 4115: ep_len:38 episode reward: total was 17.500000. running mean: -14.047736\n",
      "ep 4115: ep_len:142 episode reward: total was 68.000000. running mean: -13.227259\n",
      "ep 4115: ep_len:1376 episode reward: total was -7.470000. running mean: -13.169686\n",
      "ep 4115: ep_len:3792 episode reward: total was -24.530000. running mean: -13.283289\n",
      "ep 4115: ep_len:743 episode reward: total was 1.530000. running mean: -13.135156\n",
      "ep 4115: ep_len:684 episode reward: total was 13.350000. running mean: -12.870305\n",
      "ep 4115: ep_len:644 episode reward: total was 13.580000. running mean: -12.605802\n",
      "ep 4115: ep_len:201 episode reward: total was -132.490000. running mean: -13.804644\n",
      "ep 4115: ep_len:664 episode reward: total was 15.830000. running mean: -13.508297\n",
      "ep 4115: ep_len:2822 episode reward: total was -6.780000. running mean: -13.441014\n",
      "ep 4115: ep_len:48 episode reward: total was 21.000000. running mean: -13.096604\n",
      "epsilon:0.009992 episode_count: 61852. steps_count: 66780204.000000\n",
      "ep 4116: ep_len:833 episode reward: total was 16.260000. running mean: -12.803038\n",
      "ep 4116: ep_len:500 episode reward: total was 10.260000. running mean: -12.572408\n",
      "ep 4116: ep_len:43 episode reward: total was 18.500000. running mean: -12.261684\n",
      "ep 4116: ep_len:3049 episode reward: total was -0.880000. running mean: -12.147867\n",
      "ep 4116: ep_len:653 episode reward: total was 18.440000. running mean: -11.841988\n",
      "ep 4116: ep_len:42 episode reward: total was 19.500000. running mean: -11.528568\n",
      "ep 4116: ep_len:500 episode reward: total was 29.340000. running mean: -11.119883\n",
      "ep 4116: ep_len:3973 episode reward: total was -94.390000. running mean: -11.952584\n",
      "ep 4116: ep_len:529 episode reward: total was -9.390000. running mean: -11.926958\n",
      "ep 4116: ep_len:837 episode reward: total was 37.210000. running mean: -11.435588\n",
      "ep 4116: ep_len:1121 episode reward: total was 6.120000. running mean: -11.260033\n",
      "ep 4116: ep_len:859 episode reward: total was -19.130000. running mean: -11.338732\n",
      "ep 4116: ep_len:2824 episode reward: total was -33.360000. running mean: -11.558945\n",
      "epsilon:0.009992 episode_count: 61865. steps_count: 66795967.000000\n",
      "ep 4117: ep_len:3519 episode reward: total was -1148.200000. running mean: -22.925355\n",
      "ep 4117: ep_len:500 episode reward: total was -26.470000. running mean: -22.960802\n",
      "ep 4117: ep_len:3093 episode reward: total was 4.540000. running mean: -22.685794\n",
      "ep 4117: ep_len:550 episode reward: total was -32.220000. running mean: -22.781136\n",
      "ep 4117: ep_len:927 episode reward: total was 56.560000. running mean: -21.987725\n",
      "ep 4117: ep_len:632 episode reward: total was -10.250000. running mean: -21.870347\n",
      "ep 4117: ep_len:500 episode reward: total was 29.730000. running mean: -21.354344\n",
      "ep 4117: ep_len:668 episode reward: total was 27.520000. running mean: -20.865600\n",
      "ep 4117: ep_len:889 episode reward: total was 43.350000. running mean: -20.223444\n",
      "ep 4117: ep_len:101 episode reward: total was 49.000000. running mean: -19.531210\n",
      "ep 4117: ep_len:58 episode reward: total was 27.500000. running mean: -19.060898\n",
      "ep 4117: ep_len:765 episode reward: total was -36.960000. running mean: -19.239889\n",
      "ep 4117: ep_len:2798 episode reward: total was -34.380000. running mean: -19.391290\n",
      "ep 4117: ep_len:62 episode reward: total was 29.500000. running mean: -18.902377\n",
      "epsilon:0.009992 episode_count: 61879. steps_count: 66811029.000000\n",
      "ep 4118: ep_len:874 episode reward: total was 4.650000. running mean: -18.666853\n",
      "ep 4118: ep_len:680 episode reward: total was -17.520000. running mean: -18.655385\n",
      "ep 4118: ep_len:2919 episode reward: total was -51.200000. running mean: -18.980831\n",
      "ep 4118: ep_len:500 episode reward: total was 7.820000. running mean: -18.712823\n",
      "ep 4118: ep_len:58 episode reward: total was 27.500000. running mean: -18.250694\n",
      "ep 4118: ep_len:84 episode reward: total was 40.500000. running mean: -17.663187\n",
      "ep 4118: ep_len:30 episode reward: total was 13.500000. running mean: -17.351556\n",
      "ep 4118: ep_len:500 episode reward: total was 14.680000. running mean: -17.031240\n",
      "ep 4118: ep_len:359 episode reward: total was -2.870000. running mean: -16.889628\n",
      "ep 4118: ep_len:553 episode reward: total was -12.910000. running mean: -16.849831\n",
      "ep 4118: ep_len:7345 episode reward: total was -136.770000. running mean: -18.049033\n",
      "ep 4118: ep_len:540 episode reward: total was 19.390000. running mean: -17.674643\n",
      "ep 4118: ep_len:140 episode reward: total was 68.010000. running mean: -16.817796\n",
      "ep 4118: ep_len:1075 episode reward: total was -7.600000. running mean: -16.725618\n",
      "ep 4118: ep_len:2840 episode reward: total was 2.830000. running mean: -16.530062\n",
      "epsilon:0.009992 episode_count: 61894. steps_count: 66829526.000000\n",
      "ep 4119: ep_len:805 episode reward: total was -14.340000. running mean: -16.508162\n",
      "ep 4119: ep_len:728 episode reward: total was 13.800000. running mean: -16.205080\n",
      "ep 4119: ep_len:3061 episode reward: total was 35.390000. running mean: -15.689129\n",
      "ep 4119: ep_len:643 episode reward: total was 16.520000. running mean: -15.367038\n",
      "ep 4119: ep_len:500 episode reward: total was 51.080000. running mean: -14.702567\n",
      "ep 4119: ep_len:3491 episode reward: total was -123.330000. running mean: -15.788842\n",
      "ep 4119: ep_len:909 episode reward: total was -81.980000. running mean: -16.450753\n",
      "ep 4119: ep_len:884 episode reward: total was 55.380000. running mean: -15.732446\n",
      "ep 4119: ep_len:624 episode reward: total was -20.190000. running mean: -15.777021\n",
      "ep 4119: ep_len:52 episode reward: total was 24.500000. running mean: -15.374251\n",
      "ep 4119: ep_len:44 episode reward: total was 20.500000. running mean: -15.015509\n",
      "ep 4119: ep_len:1433 episode reward: total was -8.640000. running mean: -14.951754\n",
      "ep 4119: ep_len:2745 episode reward: total was -16.760000. running mean: -14.969836\n",
      "epsilon:0.009992 episode_count: 61907. steps_count: 66845445.000000\n",
      "ep 4120: ep_len:2909 episode reward: total was -1049.420000. running mean: -25.314338\n",
      "ep 4120: ep_len:1623 episode reward: total was -76.680000. running mean: -25.827994\n",
      "ep 4120: ep_len:65 episode reward: total was 31.000000. running mean: -25.259714\n",
      "ep 4120: ep_len:3100 episode reward: total was -8.940000. running mean: -25.096517\n",
      "ep 4120: ep_len:516 episode reward: total was -31.370000. running mean: -25.159252\n",
      "ep 4120: ep_len:45 episode reward: total was 21.000000. running mean: -24.697659\n",
      "ep 4120: ep_len:95 episode reward: total was 44.500000. running mean: -24.005683\n",
      "ep 4120: ep_len:80 episode reward: total was 35.500000. running mean: -23.410626\n",
      "ep 4120: ep_len:1411 episode reward: total was -13.820000. running mean: -23.314720\n",
      "ep 4120: ep_len:324 episode reward: total was -13.660000. running mean: -23.218173\n",
      "ep 4120: ep_len:1166 episode reward: total was -21.250000. running mean: -23.198491\n",
      "ep 4120: ep_len:762 episode reward: total was 36.520000. running mean: -22.601306\n",
      "ep 4120: ep_len:583 episode reward: total was 1.620000. running mean: -22.359093\n",
      "ep 4120: ep_len:41 episode reward: total was 19.000000. running mean: -21.945502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4120: ep_len:36 episode reward: total was 16.500000. running mean: -21.561047\n",
      "ep 4120: ep_len:516 episode reward: total was 6.440000. running mean: -21.281036\n",
      "ep 4120: ep_len:2878 episode reward: total was -29.290000. running mean: -21.361126\n",
      "epsilon:0.009992 episode_count: 61924. steps_count: 66861595.000000\n",
      "ep 4121: ep_len:892 episode reward: total was 32.900000. running mean: -20.818515\n",
      "ep 4121: ep_len:757 episode reward: total was -21.890000. running mean: -20.829230\n",
      "ep 4121: ep_len:2922 episode reward: total was -34.900000. running mean: -20.969937\n",
      "ep 4121: ep_len:777 episode reward: total was 32.790000. running mean: -20.432338\n",
      "ep 4121: ep_len:42 episode reward: total was 19.500000. running mean: -20.033015\n",
      "ep 4121: ep_len:500 episode reward: total was -2.010000. running mean: -19.852785\n",
      "ep 4121: ep_len:3796 episode reward: total was -54.550000. running mean: -20.199757\n",
      "ep 4121: ep_len:875 episode reward: total was -108.580000. running mean: -21.083559\n",
      "ep 4121: ep_len:640 episode reward: total was 5.160000. running mean: -20.821124\n",
      "ep 4121: ep_len:695 episode reward: total was -19.480000. running mean: -20.807712\n",
      "ep 4121: ep_len:110 episode reward: total was 52.000000. running mean: -20.079635\n",
      "ep 4121: ep_len:1117 episode reward: total was -19.270000. running mean: -20.071539\n",
      "ep 4121: ep_len:2823 episode reward: total was 3.150000. running mean: -19.839323\n",
      "epsilon:0.009992 episode_count: 61937. steps_count: 66877541.000000\n",
      "ep 4122: ep_len:3487 episode reward: total was -549.650000. running mean: -25.137430\n",
      "ep 4122: ep_len:683 episode reward: total was 12.120000. running mean: -24.764856\n",
      "ep 4122: ep_len:56 episode reward: total was 26.500000. running mean: -24.252207\n",
      "ep 4122: ep_len:3006 episode reward: total was 9.230000. running mean: -23.917385\n",
      "ep 4122: ep_len:682 episode reward: total was -17.590000. running mean: -23.854111\n",
      "ep 4122: ep_len:95 episode reward: total was 46.000000. running mean: -23.155570\n",
      "ep 4122: ep_len:101 episode reward: total was 49.000000. running mean: -22.434015\n",
      "ep 4122: ep_len:819 episode reward: total was 23.720000. running mean: -21.972474\n",
      "ep 4122: ep_len:332 episode reward: total was -42.440000. running mean: -22.177150\n",
      "ep 4122: ep_len:682 episode reward: total was -43.850000. running mean: -22.393878\n",
      "ep 4122: ep_len:807 episode reward: total was 27.260000. running mean: -21.897339\n",
      "ep 4122: ep_len:676 episode reward: total was -23.320000. running mean: -21.911566\n",
      "ep 4122: ep_len:61 episode reward: total was 27.500000. running mean: -21.417450\n",
      "ep 4122: ep_len:68 episode reward: total was 31.000000. running mean: -20.893276\n",
      "ep 4122: ep_len:833 episode reward: total was -37.460000. running mean: -21.058943\n",
      "ep 4122: ep_len:2942 episode reward: total was 8.960000. running mean: -20.758754\n",
      "ep 4122: ep_len:47 episode reward: total was 22.000000. running mean: -20.331166\n",
      "epsilon:0.009992 episode_count: 61954. steps_count: 66892918.000000\n",
      "ep 4123: ep_len:1113 episode reward: total was -21.390000. running mean: -20.341754\n",
      "ep 4123: ep_len:680 episode reward: total was 8.750000. running mean: -20.050837\n",
      "ep 4123: ep_len:43 episode reward: total was 18.500000. running mean: -19.665329\n",
      "ep 4123: ep_len:2985 episode reward: total was -21.950000. running mean: -19.688175\n",
      "ep 4123: ep_len:621 episode reward: total was -12.140000. running mean: -19.612694\n",
      "ep 4123: ep_len:53 episode reward: total was 22.000000. running mean: -19.196567\n",
      "ep 4123: ep_len:63 episode reward: total was 30.000000. running mean: -18.704601\n",
      "ep 4123: ep_len:40 episode reward: total was 18.500000. running mean: -18.332555\n",
      "ep 4123: ep_len:1442 episode reward: total was -13.970000. running mean: -18.288929\n",
      "ep 4123: ep_len:642 episode reward: total was 6.780000. running mean: -18.038240\n",
      "ep 4123: ep_len:822 episode reward: total was -28.810000. running mean: -18.145958\n",
      "ep 4123: ep_len:826 episode reward: total was 43.160000. running mean: -17.532898\n",
      "ep 4123: ep_len:1016 episode reward: total was -6.040000. running mean: -17.417969\n",
      "ep 4123: ep_len:58 episode reward: total was 27.500000. running mean: -16.968789\n",
      "ep 4123: ep_len:1052 episode reward: total was -40.340000. running mean: -17.202502\n",
      "ep 4123: ep_len:2867 episode reward: total was -42.840000. running mean: -17.458877\n",
      "epsilon:0.009992 episode_count: 61970. steps_count: 66907241.000000\n",
      "ep 4124: ep_len:1101 episode reward: total was 13.630000. running mean: -17.147988\n",
      "ep 4124: ep_len:183 episode reward: total was 15.260000. running mean: -16.823908\n",
      "ep 4124: ep_len:2950 episode reward: total was -58.260000. running mean: -17.238269\n",
      "ep 4124: ep_len:571 episode reward: total was -23.230000. running mean: -17.298186\n",
      "ep 4124: ep_len:48 episode reward: total was 22.500000. running mean: -16.900204\n",
      "ep 4124: ep_len:112 episode reward: total was 51.500000. running mean: -16.216202\n",
      "ep 4124: ep_len:1403 episode reward: total was -147.740000. running mean: -17.531440\n",
      "ep 4124: ep_len:355 episode reward: total was -6.550000. running mean: -17.421626\n",
      "ep 4124: ep_len:532 episode reward: total was -20.100000. running mean: -17.448410\n",
      "ep 4124: ep_len:734 episode reward: total was 35.140000. running mean: -16.922525\n",
      "ep 4124: ep_len:701 episode reward: total was -6.880000. running mean: -16.822100\n",
      "ep 4124: ep_len:69 episode reward: total was 31.500000. running mean: -16.338879\n",
      "ep 4124: ep_len:116 episode reward: total was 56.500000. running mean: -15.610490\n",
      "ep 4124: ep_len:500 episode reward: total was 52.270000. running mean: -14.931685\n",
      "ep 4124: ep_len:2917 episode reward: total was 7.270000. running mean: -14.709669\n",
      "ep 4124: ep_len:58 episode reward: total was 27.500000. running mean: -14.287572\n",
      "epsilon:0.009992 episode_count: 61986. steps_count: 66919591.000000\n",
      "ep 4125: ep_len:1142 episode reward: total was -42.010000. running mean: -14.564796\n",
      "ep 4125: ep_len:917 episode reward: total was -36.330000. running mean: -14.782448\n",
      "ep 4125: ep_len:35 episode reward: total was 16.000000. running mean: -14.474624\n",
      "ep 4125: ep_len:3095 episode reward: total was 5.150000. running mean: -14.278378\n",
      "ep 4125: ep_len:707 episode reward: total was -1.450000. running mean: -14.150094\n",
      "ep 4125: ep_len:95 episode reward: total was 41.500000. running mean: -13.593593\n",
      "ep 4125: ep_len:1503 episode reward: total was -15.350000. running mean: -13.611157\n",
      "ep 4125: ep_len:3584 episode reward: total was -103.820000. running mean: -14.513245\n",
      "ep 4125: ep_len:795 episode reward: total was -11.790000. running mean: -14.486013\n",
      "ep 4125: ep_len:763 episode reward: total was 8.840000. running mean: -14.252753\n",
      "ep 4125: ep_len:663 episode reward: total was 0.480000. running mean: -14.105425\n",
      "ep 4125: ep_len:45 episode reward: total was 19.500000. running mean: -13.769371\n",
      "ep 4125: ep_len:34 episode reward: total was -29.990000. running mean: -13.931577\n",
      "ep 4125: ep_len:591 episode reward: total was -6.350000. running mean: -13.855761\n",
      "ep 4125: ep_len:2849 episode reward: total was 0.380000. running mean: -13.713404\n",
      "epsilon:0.009992 episode_count: 62001. steps_count: 66936409.000000\n",
      "ep 4126: ep_len:892 episode reward: total was 14.780000. running mean: -13.428470\n",
      "ep 4126: ep_len:745 episode reward: total was -5.770000. running mean: -13.351885\n",
      "ep 4126: ep_len:3018 episode reward: total was -1511.570000. running mean: -28.334066\n",
      "ep 4126: ep_len:500 episode reward: total was 9.860000. running mean: -27.952126\n",
      "ep 4126: ep_len:50 episode reward: total was 23.500000. running mean: -27.437604\n",
      "ep 4126: ep_len:159 episode reward: total was 75.000000. running mean: -26.413228\n",
      "ep 4126: ep_len:1056 episode reward: total was -2.740000. running mean: -26.176496\n",
      "ep 4126: ep_len:3855 episode reward: total was -36.210000. running mean: -26.276831\n",
      "ep 4126: ep_len:617 episode reward: total was -32.380000. running mean: -26.337863\n",
      "ep 4126: ep_len:732 episode reward: total was 6.170000. running mean: -26.012784\n",
      "ep 4126: ep_len:1017 episode reward: total was 24.700000. running mean: -25.505656\n",
      "ep 4126: ep_len:109 episode reward: total was 51.500000. running mean: -24.735600\n",
      "ep 4126: ep_len:1510 episode reward: total was -24.460000. running mean: -24.732844\n",
      "ep 4126: ep_len:2914 episode reward: total was -9.100000. running mean: -24.576515\n",
      "epsilon:0.009992 episode_count: 62015. steps_count: 66953583.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4127: ep_len:1122 episode reward: total was -11.170000. running mean: -24.442450\n",
      "ep 4127: ep_len:196 episode reward: total was 20.350000. running mean: -23.994526\n",
      "ep 4127: ep_len:66 episode reward: total was 31.500000. running mean: -23.439580\n",
      "ep 4127: ep_len:3026 episode reward: total was 6.880000. running mean: -23.136385\n",
      "ep 4127: ep_len:703 episode reward: total was -4.350000. running mean: -22.948521\n",
      "ep 4127: ep_len:129 episode reward: total was 61.500000. running mean: -22.104036\n",
      "ep 4127: ep_len:55 episode reward: total was 26.000000. running mean: -21.622995\n",
      "ep 4127: ep_len:753 episode reward: total was -29.460000. running mean: -21.701365\n",
      "ep 4127: ep_len:3710 episode reward: total was -243.410000. running mean: -23.918452\n",
      "ep 4127: ep_len:629 episode reward: total was -0.950000. running mean: -23.688767\n",
      "ep 4127: ep_len:760 episode reward: total was -1.160000. running mean: -23.463479\n",
      "ep 4127: ep_len:961 episode reward: total was 8.800000. running mean: -23.140845\n",
      "ep 4127: ep_len:760 episode reward: total was -44.080000. running mean: -23.350236\n",
      "ep 4127: ep_len:2826 episode reward: total was -30.860000. running mean: -23.425334\n",
      "ep 4127: ep_len:44 episode reward: total was 20.500000. running mean: -22.986080\n",
      "epsilon:0.009992 episode_count: 62030. steps_count: 66969323.000000\n",
      "ep 4128: ep_len:1103 episode reward: total was -0.710000. running mean: -22.763320\n",
      "ep 4128: ep_len:711 episode reward: total was -6.640000. running mean: -22.602086\n",
      "ep 4128: ep_len:2962 episode reward: total was -1.510000. running mean: -22.391166\n",
      "ep 4128: ep_len:885 episode reward: total was 10.270000. running mean: -22.064554\n",
      "ep 4128: ep_len:826 episode reward: total was -2.750000. running mean: -21.871408\n",
      "ep 4128: ep_len:3886 episode reward: total was -115.460000. running mean: -22.807294\n",
      "ep 4128: ep_len:742 episode reward: total was 12.140000. running mean: -22.457821\n",
      "ep 4128: ep_len:856 episode reward: total was 51.490000. running mean: -21.718343\n",
      "ep 4128: ep_len:711 episode reward: total was -18.410000. running mean: -21.685260\n",
      "ep 4128: ep_len:173 episode reward: total was 85.000000. running mean: -20.618407\n",
      "ep 4128: ep_len:77 episode reward: total was 35.500000. running mean: -20.057223\n",
      "ep 4128: ep_len:677 episode reward: total was -19.140000. running mean: -20.048051\n",
      "ep 4128: ep_len:2828 episode reward: total was -11.740000. running mean: -19.964970\n",
      "ep 4128: ep_len:52 episode reward: total was 24.500000. running mean: -19.520321\n",
      "epsilon:0.009992 episode_count: 62044. steps_count: 66985812.000000\n",
      "ep 4129: ep_len:999 episode reward: total was -114.610000. running mean: -20.471217\n",
      "ep 4129: ep_len:677 episode reward: total was -16.050000. running mean: -20.427005\n",
      "ep 4129: ep_len:2828 episode reward: total was -90.550000. running mean: -21.128235\n",
      "ep 4129: ep_len:657 episode reward: total was 5.530000. running mean: -20.861653\n",
      "ep 4129: ep_len:40 episode reward: total was 18.500000. running mean: -20.468036\n",
      "ep 4129: ep_len:80 episode reward: total was 37.000000. running mean: -19.893356\n",
      "ep 4129: ep_len:644 episode reward: total was -1.080000. running mean: -19.705222\n",
      "ep 4129: ep_len:298 episode reward: total was -8.470000. running mean: -19.592870\n",
      "ep 4129: ep_len:693 episode reward: total was -30.730000. running mean: -19.704241\n",
      "ep 4129: ep_len:707 episode reward: total was -15.620000. running mean: -19.663399\n",
      "ep 4129: ep_len:707 episode reward: total was -14.680000. running mean: -19.613565\n",
      "ep 4129: ep_len:44 episode reward: total was 19.000000. running mean: -19.227429\n",
      "ep 4129: ep_len:46 episode reward: total was 21.500000. running mean: -18.820155\n",
      "ep 4129: ep_len:1510 episode reward: total was -72.850000. running mean: -19.360454\n",
      "ep 4129: ep_len:2814 episode reward: total was -29.630000. running mean: -19.463149\n",
      "ep 4129: ep_len:25 episode reward: total was 11.000000. running mean: -19.158518\n",
      "epsilon:0.009992 episode_count: 62060. steps_count: 66998581.000000\n",
      "ep 4130: ep_len:1092 episode reward: total was -2.380000. running mean: -18.990732\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-25915784d303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# make next_state vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;31m#print (x_t.size(),h_t.size(),enc_history.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mnext_state_xt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_tau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-020464c72e25>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#embedded = [src len, batch size, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#outputs = [src len, batch size, hid dim * n directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 522\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_ep_len = 200\n",
    "seq_len = 20\n",
    "h_len = seq_len\n",
    "\n",
    "while epoch < numEpoch:\n",
    "#for epoch in range(numEpoch):\n",
    "    # repeat for all pedestrians\n",
    "    #disp(pALL)\n",
    "    \n",
    "    policy_net.train()\n",
    "    \n",
    "    for p in range(pALL.shape[0]): #range(pInLoop.shape[0])\n",
    "        \n",
    "        # load p'th person data\n",
    "        ped = np.copy(pALL[p])\n",
    "\n",
    "        # camera index and frame index starts from zero\n",
    "        ped[:,0] -= 1\n",
    "        ped[:,1] -= 1\n",
    "        #print (np.unique(ped[:,0]))\n",
    "        \n",
    "        # check if camera number is correct\n",
    "        if (ped[:,0] >= num_camera).any():\n",
    "            print ('Error in person ', p)\n",
    "            break\n",
    "            \n",
    "        if ped.shape[0] < max_ep_len/5:\n",
    "            continue\n",
    "        \n",
    "        # select a camera uniformly\n",
    "        uniq_cam = np.unique(ped[:,0])\n",
    "        if len(uniq_cam) < 2 and np.random.rand() > 0.4:\n",
    "            if len(np.where(ped[1:,1]-ped[0:-1,1] != 1)[0]) == 0:\n",
    "                continue\n",
    "        rand_cam = uniq_cam[np.random.randint(len(uniq_cam))]\n",
    "        index_of_rand_cam = np.nonzero( ped[:,0]==rand_cam )[0]\n",
    "        len_indices_rand_cam = len(index_of_rand_cam)\n",
    "        \n",
    "        # Initialize with current state with start frame\n",
    "        tranIDX = np.where(ped[1:,0]-ped[0:-1,0])[0]\n",
    "        if len(uniq_cam) < 2 or np.random.rand() < 0.4:\n",
    "            startIDX = np.random.randint( 0,int(ped.shape[0]-ped.shape[0]/2) )\n",
    "        else:\n",
    "            startIDX = np.random.choice(tranIDX)-20\n",
    "        #startIDX = np.random.choice(tranIDX)-20 if np.random.rand(1) < 0.6 else np.random.randint( 0,ped.shape[0]-max_ep_len/2 )\n",
    "        #startIDX = index_of_rand_cam[np.random.randint(len_indices_rand_cam/10)]\n",
    "        myPos = ped[startIDX,0:]\n",
    "        #print (myPos)\n",
    "        \n",
    "        curr_camera = myPos[0]\n",
    "        curr_frame = myPos[1]\n",
    "        \n",
    "        # Initialize history variable (one-hot encoding)\n",
    "        ch = np.zeros((h_len,cityflow_cam))\n",
    "        prev_ch = ch\n",
    "        \n",
    "        # initialize total time target was occluded\n",
    "        num_steps = 0\n",
    "        occ_len = 0.01\n",
    "        hcount = np.array(10*np.log(occ_len))\n",
    "        CDataEp = []\n",
    "        inCDataEp = []\n",
    "        EpData = []\n",
    "        episodic_seq = []\n",
    "        \n",
    "        # create initial state (ct,rt,tau_t)\n",
    "        #bbox = myPos[2:]\n",
    "        #rt = afc.find_curr_rt(bbox)\n",
    "        x_t,c_t,te_tau,r_t = make_state_vector(ped, curr_camera,curr_frame,ch,occ_len)\n",
    "        prev_rt = r_t[0:4]\n",
    "        stCam = curr_camera\n",
    "        expStC = curr_camera\n",
    "        count_curr_c = 0\n",
    "        prev_camera = curr_camera\n",
    "\n",
    "        if render: # show current location\n",
    "            plt.imshow(x.reshape(input_size))\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "        episode_count += 1\n",
    "        if epsilon > finalEpsilon:\n",
    "            epsilon -= (initialEpsilon - finalEpsilon)/20000\n",
    "        \n",
    "        # select an action from the current state\n",
    "        hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "        #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "        state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "        #state = torch.cat([state_xt, hidden.detach().flatten().view(1,-1)], dim=1)\n",
    "        state = torch.cat([state_xt, hidden[1,].detach()], dim=1)\n",
    "        #print ('State size: ', state.size())\n",
    "        \n",
    "        while(curr_frame <= ped[-1,1]):\n",
    "        \n",
    "            state_in = Variable(state)\n",
    "            value_c = policy_net(state_in)\n",
    "\n",
    "            steps_count += 1\n",
    "            \n",
    "            # generate random steps\n",
    "            if np.random.rand(1) < 0.01:\n",
    "                rsteps = np.random.randint(fpsc,20,1)\n",
    "            else:\n",
    "                rsteps = 1\n",
    "            curr_frame += rsteps*fpsc if rsteps > 1 else fpsc\n",
    "            num_steps += 1\n",
    "                \n",
    "            # initialize action\n",
    "            one_hot_action = torch.zeros([num_camera], dtype=torch.float32)\n",
    "            if use_cuda:  # put on GPU if CUDA is available\n",
    "                one_hot_action = one_hot_action.cuda()\n",
    "\n",
    "            # epsilon greedy exploration\n",
    "            random_action = np.random.random() <= epsilon\n",
    "            camera_index = [torch.randint(num_camera, torch.Size([]), dtype=torch.int)\n",
    "                           if random_action else torch.argmax(value_c)][0]\n",
    "            \n",
    "            if use_cuda:  # put on GPU if CUDA is available\n",
    "                camera_index = camera_index.cuda()\n",
    "\n",
    "            one_hot_action[camera_index] = 1\n",
    "            one_hot_action = one_hot_action.unsqueeze(0)\n",
    "            c = camera_index.detach().cpu().numpy()\n",
    "            \n",
    "            # Store the transition explored\n",
    "            #M[stCam,c] += 1\n",
    "            \n",
    "            # get the current bounding box\n",
    "            bbox = ped[ np.logical_and(ped[:,0]==c,ped[:,1]==curr_frame),2:]\n",
    "            if bbox.shape[0] > 0:\n",
    "                #rt = afc.find_curr_rt(bbox[0]) \n",
    "                bbox = bbox[0]\n",
    "                rt = np.zeros((8))\n",
    "                rt[0] = bbox[0]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[1] = bbox[1]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[2] = bbox[2]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[3] = bbox[3]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[4] = rt[0] - prev_rt[0] if occ_len < 0.2 else 0\n",
    "                rt[5] = rt[1] - prev_rt[1] if occ_len < 0.2 else 0\n",
    "                rt[6] = rt[2] - prev_rt[2] if occ_len < 0.2 else 0\n",
    "                rt[7] = rt[3] - prev_rt[3] if occ_len < 0.2 else 0\n",
    "                \n",
    "                curr_camera = c\n",
    "                # make next_state vector\n",
    "                this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "                x_t = np.concatenate((this_cam, rt.ravel()))\n",
    "                x_t[x_t==0] = -10\n",
    "                x_t[x_t==1] = 10\n",
    "                x_t = x_t.reshape(1,-1)\n",
    "                if use_cuda:\n",
    "                    x_t = torch.from_numpy(x_t).float().cuda()\n",
    "                \n",
    "                #num_steps = 0\n",
    "                ispresent = 1\n",
    "                stCam = c\n",
    "                \n",
    "                prev_rt = rt[0:4]\n",
    "            else:\n",
    "                ispresent = 0\n",
    "            \n",
    "            #if ispresent and expStC != num_camera-1 and c != num_camera-1:\n",
    "            #    trExplored[str(expStC)+'-'+str(c)].append(occ_len)\n",
    "            #    expStC = c\n",
    "            \n",
    "            # get correct label from ground truth\n",
    "            y = afc.find_target_camera(ped, curr_frame)\n",
    "            # get reward (give reward at end of episode)\n",
    "            if y == num_camera-1 and y == c:\n",
    "                reward = 0.01\n",
    "            elif y == c and occ_len< 20:\n",
    "                reward = 0.5\n",
    "                wt = 1\n",
    "            elif y == c:\n",
    "                reward = 1\n",
    "                wt = 10\n",
    "            else:\n",
    "                reward = -1\n",
    "            reward_sum += reward\n",
    "            rs.append(reward)\n",
    "            EpData.append((list(value_c.detach().cpu().numpy()[0]),hcount.ravel()[0],reward,random_action,y,c,episode_reward))\n",
    "            \n",
    "            #print (np.array([rt, ispresent,c]))\n",
    "            ######################## prepare the next state  #############################\n",
    "            # count the time of prev_camera selection\n",
    "            if ispresent:\n",
    "                occ_len = 0.01\n",
    "            else:\n",
    "                occ_len += rsteps\n",
    "            #hcount = np.array(-occ_max_val + (occ_len/500)*(occ_max_val-(-occ_max_val)))\n",
    "            hcount = np.array(10*np.log(occ_len))\n",
    "            \n",
    "            # get next camera using policy network\n",
    "            this_cam = afc.make_one_hot_camera(c)\n",
    "            c_t = this_cam.reshape(1,-1)\n",
    "            \n",
    "            # update current state and history\n",
    "            prev_ch = ch\n",
    "            ch[1:,] = ch[0:-1,]\n",
    "            ch[0,0:num_camera] = afc.make_one_hot_camera(c)\n",
    "            ch[0,num_camera:] = 0\n",
    "            \n",
    "            if use_cuda:\n",
    "                c_t = torch.from_numpy(c_t).float().cuda()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float().cuda()\n",
    "            else:\n",
    "                c_t = torch.from_numpy(c_t).float()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float()\n",
    "            episodic_seq.append((c_t))\n",
    "            \n",
    "            # make next_state vector\n",
    "            hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "            #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "            next_state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "            #next_state = torch.cat([next_state_xt, hidden.detach().flatten().view(1,-1)], dim=1)\n",
    "            next_state = torch.cat([next_state_xt, hidden[1,].detach()], dim=1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).cuda()\n",
    "            else:\n",
    "                reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "            \n",
    "            #replay_memory.append((state_xt,prev_ch, one_hot_action, reward, next_state_xt,ch, ispresent))\n",
    "            \n",
    "            # save transition to replay memory\n",
    "            if reward > 0.2:\n",
    "                #replay_memory_pos.append((state_xt,prev_ch, one_hot_action, reward, next_state_xt,ch, ispresent))\n",
    "                replay_memory_pos.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "                pos_prob.append(wt)\n",
    "            elif reward == 0.01:\n",
    "                #replay_memory_cx.append((state_xt,prev_ch, one_hot_action, reward, next_state_xt,ch, ispresent))\n",
    "                replay_memory_cx.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "            else:\n",
    "                #replay_memory_neg.append((state_xt,prev_ch, one_hot_action, reward, next_state_xt,ch, ispresent))\n",
    "                replay_memory_neg.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "            \n",
    "            # if replay memory is full, remove the oldest transition\n",
    "            if len(replay_memory_pos) > replay_memory_size:\n",
    "                replay_memory_pos.pop(0)\n",
    "                pos_prob.pop(0)\n",
    "            if len(replay_memory_neg) > replay_memory_size:\n",
    "                replay_memory_neg.pop(0)\n",
    "            if len(replay_memory_cx) > replay_memory_size:\n",
    "                replay_memory_cx.pop(0)\n",
    "            \n",
    "            #state_xt = next_state_xt\n",
    "            state = next_state #torch.cat([state_xt, hidden.detach()], dim=1)\n",
    "            #state = torch.cat([next_state_xt, enc_history], dim=1)\n",
    "            prev_camera = c\n",
    "            \n",
    "            if num_steps >= max_ep_len and y!=num_camera-1 and y==c:  # break the episode\n",
    "                #print ('')\n",
    "                #print (epoch, p, random_action, rsteps)\n",
    "                #print ('x_t: ', c,rt)\n",
    "                ##print ( np.where(ch)[1])\n",
    "                #print ('Q values: ', value_c)\n",
    "                #print (y,c, curr_frame,ped[-1,1], num_steps, hcount)\n",
    "                #print ('isPresent', ispresent)\n",
    "                #print ('Pos Replay length: ', len(replay_memory_pos))\n",
    "                \n",
    "                #print (pos_prob[:])\n",
    "                break\n",
    "        \n",
    "        # update value_function\n",
    "        loss,rrr,update_criteria = backward_network(replay_memory_pos, pos_prob[:], replay_memory_neg,replay_memory_cx, update_criteria)\n",
    "        numUpdateRew.append(rrr)\n",
    "        if np.random.rand() < 0.05:\n",
    "            allEpData.append((np.stack(EpData)))\n",
    "        \n",
    "        # store episodic reward\n",
    "        #numRew.append((sum(np.stack(rs)==100),sum(np.stack(rs)==-100),sum(np.stack(rs)==0.1)))\n",
    "        rs = append_reward(rs,num_steps)\n",
    "        \n",
    "        # boring book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print ('ep %d: ep_len:%d episode reward: total was %f. running mean: %f' % (epoch, num_steps, reward_sum, running_reward))\n",
    "        reward_sum = 0\n",
    "        num_steps = 0\n",
    "        rs = []\n",
    "    \n",
    "    if epoch % 20 == 100000: # test on validation set\n",
    "        _,accV,qv,numTR = test_func(pTest[1:2],iloc='first',eloc='last')\n",
    "        av = np.stack(accV[0])\n",
    "        av = sum(av[av[:,0]!=(num_camera-1),0] == av[av[:,0]!=(num_camera-1),1])/sum(av[:,0]!=(num_camera-1))\n",
    "        validation_reward.append((qv,av,numTR)) \n",
    "    #print (M)\n",
    "    epoch += 1\n",
    "    print ('epsilon:%f episode_count: %d. steps_count: %f' % (epsilon, episode_count,steps_count))\n",
    "    if epoch % 50 == 1: \n",
    "        torch.save({'state_dict': policy_net.state_dict()}, './models/policy_MM_db4_seq20_'+str(epoch))\n",
    "    #if epoch %200 == 100:\n",
    "    #    np.save('./EpData/allEpData_ECCV_db4_pretrAE_seq50_rp20K_'+str(epoch),np.array(allEpData),allow_pickle=True)\n",
    "    #    np.save('./EpData/episode_reward_ECCV_db4_pretrAE_seq50_rp20K_'+str(epoch), (episode_reward,validation_reward,epsilon,episode_count, steps_count,running_reward))\n",
    "    allEpData = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-400, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reward received during training\n",
    "rpR = np.vstack(episode_reward)\n",
    "from scipy.signal import savgol_filter\n",
    "yhat = savgol_filter(rpR[:,2], 361, 2) # window size 51, polynomial order 3\n",
    "#plt.plot(rpR[:,4])\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(yhat)\n",
    "plt.ylim([-400,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vvr = np.stack(validation_reward)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(vvr[:,1])\n",
    "plt.ylabel('Validation Acc (non-Cx)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of Q values last 10 episodes\n",
    "epn = -1\n",
    "mmat = allEpData\n",
    "episQ = np.stack(mmat[epn][:,0])[:,]\n",
    "episY = mmat[epn][:,4]*10+10\n",
    "episC = mmat[epn][:,5]*10+10\n",
    "episTau = mmat[epn][:,1]\n",
    "# print (episY)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(episY, label='GT')\n",
    "ax1.plot(episTau, label='Tau', color='black')\n",
    "#ax1.plot(episC+1, label='pi(s)', color='cyan')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.plot(episQ[:,0], label='A1')\n",
    "ax2.plot(episQ[:,1], label='A2')\n",
    "ax2.plot(episQ[:,2], label='A3')\n",
    "ax2.plot(episQ[:,3], label='A4')\n",
    "ax2.plot(episQ[:,4], label='A5')\n",
    "ax2.plot(episQ[:,5], label='Ax')\n",
    "ax2.set_ylim([-4,5])\n",
    "ax2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(arr): \n",
    "    final_list = [] \n",
    "    gt_tr = []\n",
    "    final_list.append(arr[0])     \n",
    "    for i in range(1,arr.shape[0]): \n",
    "        if arr[i] != arr[i-1]:\n",
    "            final_list.append(arr[i])     \n",
    "            if arr[i] != num_camera-1:\n",
    "                gt_tr.append(arr[i])\n",
    "    return final_list, gt_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "h_len = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#policy_net = torch.load('./EpData/policy_db4_3rep_del_rareFreq_ep5k_1201')\n",
    "policy_net.load_state_dict(torch.load('./models/policy_MM_db4_seq20_3201')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial position:  [    4 25201    10    93    37    70]\n",
      "Initial position:  [  0 932 291  90  27  86]\n",
      "Initial position:  [    2 23661   253    93    36    68]\n",
      "Initial position:  [   3 2762   43   54   16   58]\n",
      "Initial position:  [  3   2 278  79  35 106]\n",
      "Initial position:  [    4 32592   150    72     9    25]\n",
      "Initial position:  [   3 1342  253   84   48  115]\n",
      "Initial position:  [    4 27637   293    89    22    75]\n",
      "Initial position:  [    3 18785   244    88    67   113]\n",
      "Initial position:  [   4 8473  154   74   14   43]\n",
      "Initial position:  [    2 23111   173    97    46   142]\n",
      "Initial position:  [   1 8880  282   72   35  113]\n",
      "Initial position:  [   4 6779  289   82   24   74]\n",
      "Initial position:  [   4 8297  134   68   21   43]\n",
      "Initial position:  [   0 3838  288   98   30   87]\n",
      "Initial position:  [    0 13822   300    93    18    57]\n",
      "Initial position:  [    4 12824   101    87    29    81]\n",
      "Initial position:  [    4 20574   146    66    11    31]\n",
      "Initial position:  [    4 34365    43    89    34    61]\n",
      "Initial position:  [  4   2 131  76  16  42]\n",
      "Initial position:  [   4 2795  150   68   11   33]\n",
      "Initial position:  [   3 3331   20   60   30   75]\n",
      "Initial position:  [  0 548 301  87  18  84]\n",
      "Initial position:  [   2 6404   64   96   16   46]\n",
      "Person:  0\n",
      "Transitions:  [4, 5, 4, 5]\n",
      "GT transitions:  1\n",
      "Transitions captured:  0\n",
      "A,P,R:  0.16828478964401294 0.96 0.15841584158415842\n",
      "Num frames:  (50, 2)\n",
      "Accuracy:  0.16828478964401294\n",
      "Person:  1\n",
      "Transitions:  [0, 5, 0, 5, 1, 5, 2, 5, 3, 5, 4, 5, 3, 5, 2, 5, 1, 5, 0, 5, 1, 5, 2, 5, 3, 5, 4, 5, 3, 5, 2, 5, 1, 5, 1, 5, 0, 5, 1, 5, 2, 5, 3, 5, 4, 5, 4, 5]\n",
      "GT transitions:  23\n",
      "Transitions captured:  15\n",
      "A,P,R:  0.795632151828077 0.29874776386404295 0.6080097087378641\n",
      "Num frames:  (3354, 2283)\n",
      "Accuracy:  0.795632151828077\n",
      "Person:  2\n",
      "Transitions:  [2, 5, 3, 5, 4, 5, 4, 5, 3, 5, 3, 5, 3, 5, 2, 5, 1, 5, 1, 5, 2, 5, 3, 5]\n",
      "GT transitions:  11\n",
      "Transitions captured:  2\n",
      "A,P,R:  0.9271447282252783 0.8230088495575221 0.3147208121827411\n",
      "Num frames:  (226, 40)\n",
      "Accuracy:  0.9271447282252783\n",
      "Person:  3\n",
      "Transitions:  [3, 5, 3, 5, 3, 5, 3, 5, 3, 5]\n",
      "GT transitions:  4\n",
      "Transitions captured:  0\n",
      "A,P,R:  0.9104912572855953 0.25 0.04020100502512563\n",
      "Num frames:  (32, 24)\n",
      "Accuracy:  0.9104912572855953\n",
      "Person:  4\n",
      "Transitions:  [3, 5, 2, 5, 1, 5, 0, 5, 1, 5, 2, 5, 3, 5, 4, 5, 4, 5, 3, 5, 2, 5, 1, 5, 1, 5, 2, 5, 3, 5, 4, 5, 3, 5, 2, 5, 1, 5, 0, 5, 0, 5]\n",
      "GT transitions:  20\n",
      "Transitions captured:  9\n",
      "A,P,R:  0.777364045722203 0.26789059572873736 0.5099857346647646\n",
      "Num frames:  (2669, 1884)\n",
      "Accuracy:  0.777364045722203\n",
      "Person:  5\n",
      "Transitions:  [4, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.9921259842519685 0.9921259842519685 1.0\n",
      "Num frames:  (127, 1)\n",
      "Accuracy:  0.9921259842519685\n",
      "Person:  6\n",
      "Transitions:  [3, 5, 3, 5]\n",
      "GT transitions:  1\n",
      "Transitions captured:  0\n",
      "A,P,R:  0.9426264343391415 0.7476635514018691 0.425531914893617\n",
      "Num frames:  (107, 27)\n",
      "Accuracy:  0.9426264343391415\n",
      "Person:  7\n",
      "Transitions:  [4, 5, 3, 5, 3, 5, 2, 5, 1, 5, 1, 5, 2, 5]\n",
      "GT transitions:  6\n",
      "Transitions captured:  4\n",
      "A,P,R:  0.7789031620553359 0.16244725738396623 0.5579710144927537\n",
      "Num frames:  (948, 773)\n",
      "Accuracy:  0.7789031620553359\n",
      "Person:  8\n",
      "Transitions:  [3, 5, 3, 5]\n",
      "GT transitions:  1\n",
      "Transitions captured:  0\n",
      "A,P,R:  0.9290989660265879 0.5268817204301075 0.48514851485148514\n",
      "Num frames:  (93, 44)\n",
      "Accuracy:  0.9290989660265879\n",
      "Person:  9\n",
      "Transitions:  [4, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.9897959183673469 0.9897959183673469 1.0\n",
      "Num frames:  (98, 1)\n",
      "Accuracy:  0.9897959183673469\n",
      "Person:  10\n",
      "Transitions:  [2, 5, 1, 5, 0, 5, 1, 5, 2, 5, 2, 5, 3, 5, 4, 5, 3, 5, 2, 5, 1, 5, 1, 5]\n",
      "GT transitions:  11\n",
      "Transitions captured:  9\n",
      "A,P,R:  0.921411387329591 0.6485623003194888 0.7748091603053435\n",
      "Num frames:  (939, 313)\n",
      "Accuracy:  0.921411387329591\n",
      "Person:  11\n",
      "Transitions:  [1, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.9850746268656716 0.9850746268656716 1.0\n",
      "Num frames:  (67, 1)\n",
      "Accuracy:  0.9850746268656716\n",
      "Person:  12\n",
      "Transitions:  [4, 5, 4, 5, 4, 5, 4, 5, 4, 5]\n",
      "GT transitions:  4\n",
      "Transitions captured:  1\n",
      "A,P,R:  0.45106589147286824 0.24815724815724816 0.27900552486187846\n",
      "Num frames:  (814, 611)\n",
      "Accuracy:  0.45106589147286824\n",
      "Person:  13\n",
      "Transitions:  [4, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.989010989010989 0.989010989010989 1.0\n",
      "Num frames:  (91, 1)\n",
      "Accuracy:  0.989010989010989\n",
      "Person:  14\n",
      "Transitions:  [0, 5, 0, 5]\n",
      "GT transitions:  1\n",
      "Transitions captured:  1\n",
      "A,P,R:  0.8888888888888888 0.8888888888888888 1.0\n",
      "Num frames:  (36, 4)\n",
      "Accuracy:  0.8888888888888888\n",
      "Person:  15\n",
      "Transitions:  [0, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.9523809523809523 0.9523809523809523 1.0\n",
      "Num frames:  (21, 1)\n",
      "Accuracy:  0.9523809523809523\n",
      "Person:  16\n",
      "Transitions:  [4, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.9904761904761905 0.9904761904761905 1.0\n",
      "Num frames:  (105, 1)\n",
      "Accuracy:  0.9904761904761905\n",
      "Person:  17\n",
      "Transitions:  [4, 5, 3, 5, 3, 5, 4, 5, 4, 5, 3, 5, 2, 5, 1, 5, 0, 5, 1, 5, 2, 5, 3, 5, 4, 5, 3, 5]\n",
      "GT transitions:  13\n",
      "Transitions captured:  5\n",
      "A,P,R:  0.6808400311122634 0.1700122900450635 0.4125248508946322\n",
      "Num frames:  (2441, 1871)\n",
      "Accuracy:  0.6808400311122634\n",
      "Person:  18\n",
      "Transitions:  [4, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.9852941176470589 0.9852941176470589 1.0\n",
      "Num frames:  (68, 1)\n",
      "Accuracy:  0.9852941176470589\n",
      "Person:  19\n",
      "Transitions:  [4, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.9855072463768116 0.9855072463768116 1.0\n",
      "Num frames:  (69, 1)\n",
      "Accuracy:  0.9855072463768116\n",
      "Person:  20\n",
      "Transitions:  [4, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.9931506849315068 0.9931506849315068 1.0\n",
      "Num frames:  (146, 1)\n",
      "Accuracy:  0.9931506849315068\n",
      "Person:  21\n",
      "Transitions:  [3, 5]\n",
      "GT transitions:  0\n",
      "Transitions captured:  0\n",
      "\n",
      "A,P,R:  0.987012987012987 0.987012987012987 1.0\n",
      "Num frames:  (77, 1)\n",
      "Accuracy:  0.987012987012987\n",
      "Person:  22\n",
      "Transitions:  [0, 5, 1, 5, 2, 5, 3, 5, 4, 5, 3, 5, 3, 5, 3, 5, 2, 5, 0, 5, 0, 5, 1, 5, 2, 5, 2, 5, 3, 5, 4, 5, 4, 5, 3, 5, 3, 5, 2, 5, 2, 5, 1, 5, 1, 5, 0, 5, 0, 5, 1, 5, 2, 5, 3, 5, 4, 5]\n",
      "GT transitions:  28\n",
      "Transitions captured:  16\n",
      "A,P,R:  0.7541199279878134 0.25338541666666664 0.5337356006582556\n",
      "Num frames:  (3840, 2701)\n",
      "Accuracy:  0.7541199279878134\n",
      "Person:  23\n",
      "Transitions:  [2, 5, 1, 5]\n",
      "GT transitions:  1\n",
      "Transitions captured:  1\n",
      "A,P,R:  0.7974683544303798 0.5979899497487438 1.0\n",
      "Num frames:  (199, 80)\n",
      "Accuracy:  0.7974683544303798\n",
      "0.85721540473623\n",
      "Average (only transitions) A,P,R 0.7659528583105741 0.4888311308708818 0.5071471202251872\n",
      "Average (all targets) A,P,R,F, ttr 0.8572154047362299 0.6955610637297429 0.7125024867980257 465.25 [3, 47, 23, 9, 41, 1, 3, 13, 3, 1, 23, 1, 9, 1, 3, 1, 1, 27, 1, 1, 1, 1, 57, 3]\n",
      "27284\n"
     ]
    }
   ],
   "source": [
    "policy_net.eval()\n",
    "req_inc = 0\n",
    "render = False\n",
    "_,acc,_,numTR = test_func(pTest,iloc='fix',eloc='last', fixLoc=2, isdebug=0, req_inc=req_inc)\n",
    "tr_acc = 0\n",
    "avg_tr_captured = []\n",
    "A,P,R,F, ttr = [],[],[],[],[]\n",
    "A_onlytr,P_onlytr,R_onlytr = [],[],[]\n",
    "nfr = []\n",
    "for i in range(len(acc)):\n",
    "    print ('Person: ',i)\n",
    "    gt = np.array([d[0] for d in acc[i]])\n",
    "    pr = np.array([d[1] for d in acc[i]])\n",
    "    g = gt #t[gt != num_camera-1]\n",
    "    p = pr #r[gt != num_camera-1]\n",
    "    \n",
    "    dups,gt_tr = remove_duplicates(g)\n",
    "    print ('Transitions: ', dups)\n",
    "    print ('GT transitions: ', len(gt_tr))\n",
    "    print ('Transitions captured: ', numTR[i])\n",
    "    if len(gt_tr) != 0:\n",
    "        avg_tr_captured.append((numTR[i],len(gt_tr)))\n",
    "        contains_tr = 1\n",
    "    else:\n",
    "        print ('')\n",
    "        contains_tr = 0\n",
    "        #continue\n",
    "    \n",
    "    # plot transitions\n",
    "    #afc.plot_color_transitions(p,g)\n",
    "    # MCTA and number of frames\n",
    "    if req_inc == 1:\n",
    "        ac,pr,re,fr,tr = afc.compute_APRF_one_person_sct_ict(p,g)\n",
    "    else:\n",
    "        ac,pr,re,fr,tr = afc.compute_APRF_one_person_sct_ict(p,g)\n",
    "        \n",
    "    if contains_tr == 1:\n",
    "        A_onlytr.append(ac)\n",
    "        P_onlytr.append(pr)\n",
    "        R_onlytr.append(re)\n",
    "    A.append(ac)\n",
    "    P.append(pr)\n",
    "    R.append(re)\n",
    "    F.append(fr)\n",
    "    ttr.append(tr)\n",
    "    print ('A,P,R: ', ac,pr,re)\n",
    "    f = afc.compute_num_frames(p,g)\n",
    "    nfr.append(f)\n",
    "    print ('Num frames: ', f)\n",
    "    # Accuracy\n",
    "    tacc = np.sum(g==p, dtype=np.float)/g.shape[0]\n",
    "    tr_acc += tacc\n",
    "    print ('Accuracy: ',tacc)\n",
    "print (tr_acc/len(A))\n",
    "print ('Average (only transitions) A,P,R', np.mean(A_onlytr),np.mean(P_onlytr),np.mean(R_onlytr))\n",
    "print ('Average (all targets) A,P,R,F, ttr', np.mean(A),np.mean(P),np.mean(R),np.mean(F), ttr)\n",
    "print (np.sum(nfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4267774668395787\n",
      "0.504\n"
     ]
    }
   ],
   "source": [
    "a = np.stack(avg_tr_captured)\n",
    "print (np.mean(a[:,0]/a[:,1]))\n",
    "print (sum(a[:,0])/sum(a[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_fname = '/media/win/HRLhkl/Q_CamSel_3L_l4_st200_db3_1tCont_2'\n",
    "hkl.dump([[episode_reward, running_reward]], backup_fname+'_variables.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/np.log(600*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 1\n",
    "np.max(pTest[pp][1:,1] - pTest[pp][0:-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
