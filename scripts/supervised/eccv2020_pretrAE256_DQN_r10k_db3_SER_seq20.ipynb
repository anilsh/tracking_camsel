{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "#from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2 as cv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time, math\n",
    " \n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import collections\n",
    "import hickle as hkl\n",
    "import ttictoc as tt\n",
    "\n",
    "sys.path.insert(0, '../data/')\n",
    "import get_pid_train_test as db\n",
    "import auxiliary as af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0,'../py-MDNet/modules')\n",
    "# from sample_generator import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def plot_current_state(ped, c,fno):\n",
    "    # load image for current location\n",
    "    img,bb = load_image(ped,c,fno,db_no)\n",
    "\n",
    "    dpi = 80.0\n",
    "    #figsize = (img.size[0]/dpi, img.size[1]/dpi)\n",
    "    figsize = (img.shape[0]/dpi, img.shape[1]/dpi)\n",
    "    fig = plt.figure(frameon=False, figsize=figsize, dpi=dpi)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "\n",
    "    # get image and rect handle\n",
    "    imAX = ax.imshow(img, aspect='normal')\n",
    "    rect = plt.Rectangle(tuple(bb[0,:2]),bb[0,2],bb[0,3], \n",
    "        linewidth=3, edgecolor=\"#ff0000\", zorder=1, fill=False)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    plt.pause(.01)\n",
    "    plt.draw()\n",
    "    #fig.savefig(os.path.join(savefig_dir,'0000.jpg'),dpi=dpi)\n",
    "    \n",
    "    return imAX, rect\n",
    "    \n",
    "def plot_second(ped,c,curr_frame, imAX,rect):\n",
    "    img,bb =  load_image(ped,c,curr_frame,db_no)\n",
    "    #if np.array(img).shape[0] > 0:\n",
    "    if img != []:\n",
    "        imAX.set_data(img)\n",
    "    #print (bb)\n",
    "\n",
    "    #if bb.shape[0] > 0:\n",
    "    if bb != []:\n",
    "        rect.set_xy(bb[0,:2])\n",
    "        rect.set_width(bb[0,2])\n",
    "        rect.set_height(bb[0,3])\n",
    "        print ('Correct camera')\n",
    "    elif c!= num_camera-1:\n",
    "        print ('Wrong camera')\n",
    "\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    plt.pause(1)\n",
    "    plt.draw()\n",
    "    #fig.savefig(os.path.join(savefig_dir,'%04d.jpg'%(i)),dpi=dpi)\n",
    "\n",
    "def get_reward_gt(ped, curr_frame, c):\n",
    "    y = afc.find_target_camera(ped,curr_frame)\n",
    "    # get reward (give reward at end of episode)\n",
    "    if y == num_camera-1 and y == c:\n",
    "        reward = 0\n",
    "    elif y == c:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "        \n",
    "    return reward,y\n",
    "\n",
    "def get_next_step(ped,c,curr_frame, state):\n",
    "    # update current state and history\n",
    "    ispresent,this_state = get_state_vector(ped, c,curr_frame)\n",
    "    if ispresent:\n",
    "        next_state = this_state\n",
    "    else:\n",
    "        # use previous state\n",
    "        next_state = state\n",
    "    \n",
    "    # get correct label from ground truth\n",
    "    reward,y = get_reward_gt(ped, curr_frame,c)\n",
    "\n",
    "    return next_state,reward,y,ispresent\n",
    "\n",
    "def test_func(pTest, iloc='first', eloc='last', fixLoc=-1, isdebug=0, req_inc=1):\n",
    "    policy_net.eval()\n",
    "    rsT,accT = [],[]\n",
    "    Qvalues = []\n",
    "    numTrAllP = []\n",
    "    \n",
    "    for p in range(pTest.shape[0]): \n",
    "        reward_sum = 0\n",
    "        accP = []\n",
    "        inc = 1\n",
    "        aaa = 1\n",
    "        Qval_1p= []\n",
    "        numTr = 0\n",
    "        \n",
    "        # load p'th person data\n",
    "        ped = np.copy(pTest[p])\n",
    "        # camera index and frame index starts from zero\n",
    "        ped[:,0] -= 1\n",
    "        ped[:,1] -= 1\n",
    "        \n",
    "        # Initialize with current state with start frame\n",
    "        if iloc == 'first':\n",
    "            startIDX = 0\n",
    "        elif iloc == 'rand':\n",
    "            startIDX = np.random.randint( 0,ped.shape[0]-20 )\n",
    "        elif iloc == 'fix':\n",
    "            startIDX = fixLoc\n",
    "        if startIDX > ped.shape[0]:\n",
    "            continue\n",
    "        myPos = ped[startIDX,0:]\n",
    "        print ('Initial position: ',myPos)\n",
    "        \n",
    "        curr_camera = myPos[0]\n",
    "        curr_frame = myPos[1]\n",
    "        \n",
    "        # Initialize history variable (one-hot encoding)\n",
    "        ch = np.zeros((h_len,duke_cam))\n",
    "        occ_len = 0.01\n",
    "        # Make initial state\n",
    "        x_t,c_t,te_tau,r_t = make_state_vector(ped, curr_camera,curr_frame,ch,occ_len)\n",
    "        prev_rt = r_t[0:4]\n",
    "        #print (state.size())\n",
    "        num_steps = 0\n",
    "        prev_camera = curr_camera\n",
    "        count_curr_c = 0\n",
    "        \n",
    "        if render: # show current location\n",
    "            plot_current_state(ped, curr_camera,curr_frame)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "        # select an action from the current state\n",
    "        hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "        #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "        state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "        state = torch.cat([state_xt, hidden[1,].detach()], dim=1)\n",
    "        \n",
    "        \n",
    "        while(curr_frame <= ped[-1,1]): # alltime-6):\n",
    "            \n",
    "            if use_cuda:\n",
    "                state_in = Variable(state)\n",
    "                value_c = policy_net(state_in)\n",
    "            else:\n",
    "                state_in = Variable(state)\n",
    "                value_c = policy_net(state_in)\n",
    "                \n",
    "            # Only exploitation for testing\n",
    "            camera_index = torch.argmax(value_c)\n",
    "            c = camera_index.detach().cpu().numpy()\n",
    "            \n",
    "            occ_max_val = 12000000\n",
    "            aaa += 1\n",
    "            if aaa > 1 and occ_len > occ_max_val:\n",
    "                c = c #np.array(num_camera-1)\n",
    "            if occ_len > occ_max_val and aaa%50 == 0:\n",
    "                aaa = 1\n",
    "                c = np.array(np.random.randint(num_camera))\n",
    "\n",
    "            # find target for the next frame\n",
    "            curr_frame += fpsc\n",
    "            num_steps += 1\n",
    "            \n",
    "            # get correct label from ground truth\n",
    "            reward,y = get_reward_gt(ped, curr_frame,c)\n",
    "            #if req_inc:\n",
    "            if inc==1 and y!=num_camera-1:\n",
    "                # inside a camera\n",
    "                if req_inc:\n",
    "                    accP.append((y,y))\n",
    "                    c = y\n",
    "                else:\n",
    "                    accP.append((y,c.item(0)))\n",
    "            elif inc==0 and y==c.item(0) and y!=num_camera-1:\n",
    "                # transitioning to second camera\n",
    "                accP.append((y,c.item(0)))\n",
    "                inc = 1\n",
    "                numTr += 1\n",
    "            elif inc==1 and y==num_camera-1:\n",
    "                # moving out of a camera FOV\n",
    "                inc = 0\n",
    "                accP.append((y,c.item(0)))\n",
    "            else:\n",
    "                # Making transition\n",
    "                accP.append((y,c.item(0)))\n",
    "                #print ('Another case',y,c.item(0))\n",
    "                    \n",
    "            #else:\n",
    "            #    accP.append((y,c.item(0)))\n",
    "            \n",
    "            # get the current bounding box\n",
    "            bbox = ped[ np.logical_and(ped[:,0]==c,ped[:,1]==curr_frame),2:]\n",
    "            if bbox.shape[0] > 0: # and np.random.rand < 0.95:\n",
    "                bbox = bbox[0]\n",
    "                rt = np.zeros((8))\n",
    "                rt[0] = bbox[0]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[1] = bbox[1]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[2] = bbox[2]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[3] = bbox[3]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[4] = rt[0] - prev_rt[0] if occ_len < 0.2 else 0\n",
    "                rt[5] = rt[1] - prev_rt[1] if occ_len < 0.2 else 0\n",
    "                rt[6] = rt[2] - prev_rt[2] if occ_len < 0.2 else 0\n",
    "                rt[7] = rt[3] - prev_rt[3] if occ_len < 0.2 else 0\n",
    "                curr_camera = c\n",
    "                \n",
    "                # make next_state vector\n",
    "                this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "                x_t = np.concatenate((this_cam, rt.ravel()))\n",
    "                x_t[x_t==0] = -10\n",
    "                x_t[x_t==1] = 10\n",
    "                x_t = x_t.reshape(1,-1)\n",
    "                if use_cuda:\n",
    "                    x_t = torch.from_numpy(x_t).float().cuda()\n",
    "                \n",
    "                \n",
    "                ispresent = 1\n",
    "                prev_rt = rt[0:4]\n",
    "                    \n",
    "            else:\n",
    "                ispresent = 0\n",
    "                \n",
    "            # count the time of prev_camera selection\n",
    "            if ispresent:\n",
    "                occ_len = 0.01\n",
    "            else:\n",
    "                occ_len += 1\n",
    "            #hcount = np.array(-occ_max_val + (occ_len/500)*(occ_max_val-(-occ_max_val)))\n",
    "            hcount = np.array(10*np.log(occ_len))\n",
    "            \n",
    "            # update current state and history\n",
    "            ch[1:,] = ch[0:-1,]\n",
    "            ch[0,0:num_camera] = afc.make_one_hot_camera(c)\n",
    "            ch[0,num_camera:] = 0\n",
    "            this_cam = afc.make_one_hot_camera(c)\n",
    "            c_t = this_cam.reshape(1,-1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                c_t = torch.from_numpy(c_t).float().cuda()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float().cuda()\n",
    "            else:\n",
    "                c_t = torch.from_numpy(c_t).float()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float()\n",
    "                \n",
    "            if isdebug:\n",
    "                print ( np.where(rt.ravel()))\n",
    "                print ( np.where(ch))\n",
    "                print (c, curr_frame)\n",
    "                print ('isPresent', ispresent)\n",
    "                \n",
    "            # make next_state vector\n",
    "            hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "            #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "            next_state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "            next_state = torch.cat([next_state_xt, hidden[1,].detach()], dim=1)\n",
    "            \n",
    "            # store current reward\n",
    "            reward_sum += reward\n",
    "            Qval_1p.append((list(value_c.detach().cpu().numpy()[0]),hcount.ravel()[0],reward,False,y,c,state.detach().cpu().numpy()))\n",
    "                        \n",
    "            #state = next_state\n",
    "            #state_xt = next_state_xt\n",
    "            state = next_state #torch.cat([state_xt, enc_history], dim=1)\n",
    "            prev_camera = c\n",
    "            \n",
    "            if render:\n",
    "                plot_second()\n",
    "            if eloc != 'last':\n",
    "                if num_steps > eloc:\n",
    "                    break\n",
    "            \n",
    "        # stack episodic reward \n",
    "        Qvalues.append((np.stack(Qval_1p)))\n",
    "        rsT.append((reward_sum,num_steps))\n",
    "        accT.append(accP)\n",
    "        numTrAllP.append(numTr)\n",
    "        \n",
    "    return rsT, accT, Qvalues, numTrAllP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 1500\n",
    "replay_memory_size = 20000\n",
    "#epsilon = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n",
    "eps = np.finfo(np.float32).eps.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of person in data set:  (1, 14)\n",
      "Total number of person in data set:  (1, 14)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "db_no = 3\n",
    "[pALL,num_camera,alltime,fps] = db.get_pid(set_no=db_no, train_flag='train')\n",
    "num_camera += 1  # occlusion is also considered as a FOV\n",
    "fpsc = 2\n",
    "pALL = np.array(pALL)\n",
    "\n",
    "# load test set for current data set\n",
    "[pTest,num_camera,alltime,fps] = db.get_pid(set_no=db_no, train_flag='test')\n",
    "num_camera += 1  # occlusion is also considered as a FOV\n",
    "fpsc = 2\n",
    "pTest = np.array(pTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpoch = 100000\n",
    "d = 10\n",
    "region_size = (d,d)\n",
    "\n",
    "h_len = 10\n",
    "\n",
    "# Load auxiliary functions using an object\n",
    "afc = af.AuxiliaryFunction(num_camera=num_camera, d=d, h_len=h_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize required parameters\n",
    "lstm_size = 256\n",
    "hidden_size1 = 4096\n",
    "hidden_size2 = 2048\n",
    "hidden_size3 = 256\n",
    "\n",
    "input_size = lstm_size + num_camera+ 4*2 +1\n",
    "\n",
    "# Required network\n",
    "class NextCamera(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NextCamera, self).__init__()\n",
    "        \n",
    "        # make decoder layers\n",
    "        self.fch1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fch2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fch3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fco = nn.Linear(hidden_size3, num_camera)\n",
    "        \n",
    "        # Activation function \n",
    "        self.tanh = nn.Tanh() #ReLU()\n",
    "        self.relu = nn.ReLU() #ReLU()\n",
    "        #self.linear = nn.Linear() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.relu(self.fch1(x))\n",
    "        x = self.relu(self.fch2(x))\n",
    "        x = self.relu(self.fch3(x))\n",
    "        x = self.fco(x)\n",
    "            \n",
    "        return x # nn.functional.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "if use_cuda:\n",
    "    policy_net = NextCamera().float().cuda()\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    \n",
    "else:\n",
    "    policy_net = NextCamera().float()\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "if use_cuda:\n",
    "    target_net = NextCamera().cuda()\n",
    "    target_net.float().cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# use ADAM as optimizer since we can load the whole data to train\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_network(replay_memory_pos,pos_prob, replay_memory_neg, replay_memory_cx, update_criteria):\n",
    "\n",
    "    # sample random minibatch\n",
    "    minibatch_pos = random.choices(replay_memory_pos, k=min(len(replay_memory_pos), 500), weights=pos_prob)\n",
    "    #minibatch_pos = random.sample(replay_memory_pos, min(len(replay_memory_pos), 300)) #int(batch_size/3)))\n",
    "    minibatch_posneg = minibatch_pos + random.sample(replay_memory_neg, min(len(replay_memory_neg), 500)) # int(batch_size/3)))\n",
    "    minibatch = minibatch_posneg + random.sample(replay_memory_cx, min(len(replay_memory_cx), 500)) #int(batch_size/3)))\n",
    "    \n",
    "    # unpack minibatch\n",
    "    #state_xt = tuple(d[0] for d in minibatch)\n",
    "    state = torch.cat(tuple(d[0] for d in minibatch))\n",
    "    #prev_ch = tuple(d[1] for d in minibatch)\n",
    "    action = torch.cat(tuple(d[1] for d in minibatch))\n",
    "    reward = torch.cat(tuple(d[2] for d in minibatch))\n",
    "    #next_state_xt = tuple(d[4] for d in minibatch)\n",
    "    next_state = torch.cat(tuple(d[3] for d in minibatch))\n",
    "    #ch = tuple(d[5] for d in minibatch)\n",
    "    \n",
    "    # num samples of different categories\n",
    "    numRew = torch.stack([torch.sum(reward>=0.2),torch.sum(reward==-1),torch.sum(reward==0.01)]).data.cpu().numpy()\n",
    "    \n",
    "    # get output for the next state\n",
    "    next_output = target_net(next_state)\n",
    "\n",
    "    # set y_j to r_j for terminal state, otherwise to r_j + gamma*max(Q)\n",
    "    y = torch.cat(tuple(reward[i] if minibatch[i][4] \\\n",
    "                        else reward[i] + gamma * torch.max(next_output[i]) \\\n",
    "                        for i in range(len(minibatch))))\n",
    "\n",
    "    # extract Q-value\n",
    "    q_value = torch.sum(policy_net(state) * action, dim=1)\n",
    "\n",
    "    # PyTorch accumulates gradients by default, so they need to be reset in each pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # returns a new Tensor, detached from the current graph, the result will never require gradient\n",
    "    y = y.detach()\n",
    "\n",
    "    #print (y, q_value)\n",
    "    # calculate loss\n",
    "    loss = criterion(q_value, y)\n",
    "\n",
    "    # do backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # copy weights from policy_net to target_net\n",
    "    if update_criteria == 10:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        update_criteria = 0\n",
    "    update_criteria += 1\n",
    "    \n",
    "    return loss.data,numRew,update_criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = src #self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size, dim]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size, dim]\n",
    "        \n",
    "        embedded = input #self.dropout(self.embedding(input))\n",
    "        #embedded[np.arange(embedded.size),a] = 1\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.1):\n",
    "        \n",
    "        #src = [src len, batch size, dim]\n",
    "        #trg = [trg len, batch size, dim]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output #.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "duke_cam = 9\n",
    "INPUT_DIM = duke_cam\n",
    "OUTPUT_DIM = duke_cam\n",
    "ENC_EMB_DIM = duke_cam\n",
    "DEC_EMB_DIM = duke_cam\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).float().cuda()\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).float().cuda()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Seq2Seq(enc, dec, device).float().to(device)\n",
    "criterion_ae = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(9, 9)\n",
       "    (rnn): LSTM(9, 256, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(9, 9)\n",
       "    (rnn): LSTM(9, 256, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=256, out_features=9, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load encoder model\n",
    "#enc.load_state_dict(torch.load('enc-model_manyDB_state64.pt'))\n",
    "#enc.load_state_dict(torch.load('enc-model_manyDB.pt'))\n",
    "#enc.eval()\n",
    "#dec.load_state_dict(torch.load('dec-model_manyDB_state64.pt'))\n",
    "#dec.eval()\n",
    "model.load_state_dict(torch.load('tut1-model_duke_lstmSize128_manyDB_2.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "running_reward = None\n",
    "xs,rs,cprs = [],[],[]\n",
    "episode_number = 0\n",
    "episode_durations = []\n",
    "episode_reward = []\n",
    "validation_reward= []\n",
    "replay_memory_pos = []\n",
    "pos_prob = []\n",
    "replay_memory_neg = []\n",
    "replay_memory_cx = []\n",
    "M = np.zeros((num_camera,num_camera))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state_vector(ped, curr_camera,curr_frame, ch,occ_len):\n",
    "    numSamples = 30\n",
    "    overlap_thres = [0.9, 1]\n",
    "        \n",
    "    # read image\n",
    "    img,bbox,p = afc.load_image(ped,curr_camera,curr_frame,db_no)\n",
    "    imw, imh = (320,240) #img.size\n",
    "    hc = np.array(10*np.log(occ_len))\n",
    "    \n",
    "    if p:\n",
    "        rt = np.zeros((8))\n",
    "        rt[0] = bbox[0]/imw -(np.random.rand()-0.5)/100\n",
    "        rt[1] = bbox[1]/imh -(np.random.rand()-0.5)/100\n",
    "        rt[2] = bbox[2]/imw -(np.random.rand()-0.5)/100\n",
    "        rt[3] = bbox[3]/imh -(np.random.rand()-0.5)/100\n",
    "        rt[4] = 0\n",
    "        rt[5] = 0\n",
    "        rt[6] = 0\n",
    "        rt[7] = 0\n",
    "        #print (np.where(rt.ravel()))\n",
    "        \n",
    "        # make next_state vector\n",
    "        #this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "        #state = np.concatenate((this_cam, rt.ravel()))\n",
    "        #state = np.concatenate((state, hc.ravel()))\n",
    "        #state = np.concatenate((state, ch.ravel()))\n",
    "        #state = state.reshape(1,-1)\n",
    "        #state[state==0] = -10\n",
    "        #state[state==1] = 10\n",
    "        \n",
    "        # make next_state vector\n",
    "        this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "        xt = np.concatenate((this_cam, rt.ravel()))\n",
    "        xt[xt==0] = -10\n",
    "        xt[xt==1] = 10\n",
    "        xt = xt.reshape(1,-1)\n",
    "        \n",
    "        # make history vector\n",
    "        c_t = this_cam.reshape(1,-1)\n",
    "        \n",
    "        if use_cuda:\n",
    "            xt = torch.from_numpy(xt).float().cuda()\n",
    "            c_t = torch.from_numpy(c_t).float().cuda()\n",
    "            hc = torch.from_numpy(hc.reshape(1,-1)).float().cuda()\n",
    "        else:\n",
    "            xt = torch.from_numpy(xt).float()\n",
    "            c_t = torch.from_numpy(c_t).float()\n",
    "            hc = torch.from_numpy(hc.reshape(1,-1)).float()\n",
    "    else:\n",
    "        print ('Target is not present in ',c,curr_frame)\n",
    "        xt,h_t = [],[]\n",
    "    \n",
    "    return xt,c_t,hc,rt #p,state,rt\n",
    "\n",
    "def append_reward(rs,num_steps):\n",
    "    if len(rs) > 0:\n",
    "        # stack episodic reward \n",
    "        epR = np.vstack(rs)\n",
    "        rs = []\n",
    "\n",
    "        # append the episodic reward\n",
    "        #episode_number += 1\n",
    "        #episode_durations.append(num_steps)\n",
    "        reward_stat = [num_steps,np.std(epR),np.sum(epR)]\n",
    "        episode_reward.append(reward_stat)\n",
    "    \n",
    "    return rs\n",
    "\n",
    "def reinit_ae(ch):\n",
    "    # Initialize history variable (one-hot encoding)\n",
    "    if use_cuda:\n",
    "        ch = torch.from_numpy(ch).float().cuda()\n",
    "        enc_h = torch.zeros(1,lstm_size).float().cuda()\n",
    "        enc_c = torch.zeros(1,lstm_size).float().cuda()\n",
    "    else:\n",
    "        enc_h = torch.zeros(1,lstm_size).float()\n",
    "        enc_c = torch.zeros(1,lstm_size).float()\n",
    "        \n",
    "    # encode whole camera history\n",
    "    for i in range(seq_len-1,-1,-1):\n",
    "        #print (ch[i,:])\n",
    "        x = ch[i,:].view(1,-1)\n",
    "        h_lstm,enc = ae_enc((enc_h,enc_c), x)\n",
    "        (enc_h,enc_c) = h_lstm\n",
    "\n",
    "    return h_lstm,enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "occ_max_val = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpData = []\n",
    "allEpData = []\n",
    "numRew=[]\n",
    "\n",
    "numUpdateRew=[]\n",
    "update_criteria = 0\n",
    "episode_count = 0\n",
    "steps_count = 0\n",
    "initialEpsilon = 0.2\n",
    "finalEpsilon = 0.01\n",
    "epsilon = initialEpsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trExplored = {}\n",
    "for i in range(num_camera):\n",
    "    for j in range(num_camera):\n",
    "        trExplored[str(i)+'-'+str(j)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "if resume:\n",
    "    epoch = 600\n",
    "    epsilon = 0.170962\n",
    "    aaa = np.load('./EpData/.npy', allow_pickle=True)\n",
    "    episode_count = aaa[2]\n",
    "    steps_count = aaa[3]\n",
    "    episode_reward_pre = aaa[0]\n",
    "    validation_reward_pre = aaa[1]\n",
    "    policy_net.load_state_dict(torch.load('./models/')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0: ep_len:505 episode reward: total was -461.790000. running mean: -461.790000\n",
      "ep 0: ep_len:513 episode reward: total was -283.400000. running mean: -460.006100\n",
      "ep 0: ep_len:799 episode reward: total was -348.810000. running mean: -458.894139\n",
      "ep 0: ep_len:138 episode reward: total was -68.940000. running mean: -454.994598\n",
      "ep 0: ep_len:50 episode reward: total was -47.000000. running mean: -450.914652\n",
      "ep 0: ep_len:703 episode reward: total was -156.410000. running mean: -447.969605\n",
      "ep 0: ep_len:596 episode reward: total was -205.920000. running mean: -445.549109\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.199956 episode_count: 7. steps_count: 3304.000000\n",
      "Time elapsed:  17.53948950767517\n",
      "ep 1: ep_len:694 episode reward: total was -284.280000. running mean: -443.936418\n",
      "ep 1: ep_len:500 episode reward: total was -305.430000. running mean: -442.551354\n",
      "ep 1: ep_len:538 episode reward: total was -374.890000. running mean: -441.874740\n",
      "ep 1: ep_len:773 episode reward: total was -518.390000. running mean: -442.639893\n",
      "ep 1: ep_len:115 episode reward: total was -101.980000. running mean: -439.233294\n",
      "ep 1: ep_len:603 episode reward: total was -565.760000. running mean: -440.498561\n",
      "ep 1: ep_len:525 episode reward: total was -469.340000. running mean: -440.786975\n",
      "epsilon:0.199911 episode_count: 14. steps_count: 7052.000000\n",
      "Time elapsed:  27.03497886657715\n",
      "ep 2: ep_len:528 episode reward: total was -496.350000. running mean: -441.342606\n",
      "ep 2: ep_len:814 episode reward: total was -386.350000. running mean: -440.792680\n",
      "ep 2: ep_len:512 episode reward: total was -462.340000. running mean: -441.008153\n",
      "ep 2: ep_len:612 episode reward: total was -467.810000. running mean: -441.276171\n",
      "ep 2: ep_len:3 episode reward: total was -3.000000. running mean: -436.893410\n",
      "ep 2: ep_len:530 episode reward: total was -219.590000. running mean: -434.720375\n",
      "ep 2: ep_len:501 episode reward: total was -195.870000. running mean: -432.331872\n",
      "epsilon:0.199867 episode_count: 21. steps_count: 10552.000000\n",
      "Time elapsed:  36.06586718559265\n",
      "ep 3: ep_len:954 episode reward: total was -325.500000. running mean: -431.263553\n",
      "ep 3: ep_len:502 episode reward: total was -187.070000. running mean: -428.821617\n",
      "ep 3: ep_len:512 episode reward: total was -413.770000. running mean: -428.671101\n",
      "ep 3: ep_len:355 episode reward: total was -260.430000. running mean: -426.988690\n",
      "ep 3: ep_len:3 episode reward: total was -3.000000. running mean: -422.748803\n",
      "ep 3: ep_len:178 episode reward: total was -86.310000. running mean: -419.384415\n",
      "ep 3: ep_len:564 episode reward: total was -269.750000. running mean: -417.888071\n",
      "epsilon:0.199823 episode_count: 28. steps_count: 13620.000000\n",
      "Time elapsed:  44.279144525527954\n",
      "ep 4: ep_len:914 episode reward: total was -259.760000. running mean: -416.306790\n",
      "ep 4: ep_len:737 episode reward: total was -228.090000. running mean: -414.424623\n",
      "ep 4: ep_len:604 episode reward: total was -214.490000. running mean: -412.425276\n",
      "ep 4: ep_len:614 episode reward: total was -219.680000. running mean: -410.497824\n",
      "ep 4: ep_len:134 episode reward: total was -83.500000. running mean: -407.227845\n",
      "ep 4: ep_len:507 episode reward: total was -354.420000. running mean: -406.699767\n",
      "ep 4: ep_len:732 episode reward: total was -474.280000. running mean: -407.375569\n",
      "epsilon:0.199778 episode_count: 35. steps_count: 17862.000000\n",
      "Time elapsed:  55.07374405860901\n",
      "ep 5: ep_len:606 episode reward: total was -300.930000. running mean: -406.311114\n",
      "ep 5: ep_len:519 episode reward: total was -175.790000. running mean: -404.005902\n",
      "ep 5: ep_len:381 episode reward: total was -167.940000. running mean: -401.645243\n",
      "ep 5: ep_len:546 episode reward: total was -302.430000. running mean: -400.653091\n",
      "ep 5: ep_len:107 episode reward: total was -26.850000. running mean: -396.915060\n",
      "ep 5: ep_len:533 episode reward: total was -170.340000. running mean: -394.649309\n",
      "ep 5: ep_len:700 episode reward: total was -131.130000. running mean: -392.014116\n",
      "epsilon:0.199734 episode_count: 42. steps_count: 21254.000000\n",
      "Time elapsed:  63.90609288215637\n",
      "ep 6: ep_len:638 episode reward: total was -275.180000. running mean: -390.845775\n",
      "ep 6: ep_len:501 episode reward: total was -239.890000. running mean: -389.336217\n",
      "ep 6: ep_len:500 episode reward: total was -174.420000. running mean: -387.187055\n",
      "ep 6: ep_len:651 episode reward: total was -230.590000. running mean: -385.621085\n",
      "ep 6: ep_len:3 episode reward: total was 0.000000. running mean: -381.764874\n",
      "ep 6: ep_len:161 episode reward: total was -23.950000. running mean: -378.186725\n",
      "ep 6: ep_len:619 episode reward: total was -229.710000. running mean: -376.701958\n",
      "epsilon:0.199690 episode_count: 49. steps_count: 24327.000000\n",
      "Time elapsed:  72.04163384437561\n",
      "ep 7: ep_len:665 episode reward: total was -198.110000. running mean: -374.916038\n",
      "ep 7: ep_len:526 episode reward: total was -117.610000. running mean: -372.342978\n",
      "ep 7: ep_len:500 episode reward: total was -229.000000. running mean: -370.909548\n",
      "ep 7: ep_len:163 episode reward: total was -74.730000. running mean: -367.947753\n",
      "ep 7: ep_len:116 episode reward: total was -63.980000. running mean: -364.908075\n",
      "ep 7: ep_len:500 episode reward: total was -221.540000. running mean: -363.474394\n",
      "ep 7: ep_len:186 episode reward: total was -131.170000. running mean: -361.151350\n",
      "epsilon:0.199645 episode_count: 56. steps_count: 26983.000000\n",
      "Time elapsed:  79.20412993431091\n",
      "ep 8: ep_len:508 episode reward: total was -141.690000. running mean: -358.956737\n",
      "ep 8: ep_len:339 episode reward: total was -82.110000. running mean: -356.188270\n",
      "ep 8: ep_len:566 episode reward: total was -302.130000. running mean: -355.647687\n",
      "ep 8: ep_len:590 episode reward: total was -200.900000. running mean: -354.100210\n",
      "ep 8: ep_len:131 episode reward: total was -24.830000. running mean: -350.807508\n",
      "ep 8: ep_len:544 episode reward: total was -393.460000. running mean: -351.234033\n",
      "ep 8: ep_len:538 episode reward: total was -143.860000. running mean: -349.160292\n",
      "epsilon:0.199601 episode_count: 63. steps_count: 30199.000000\n",
      "Time elapsed:  87.54234027862549\n",
      "ep 9: ep_len:547 episode reward: total was -173.290000. running mean: -347.401590\n",
      "ep 9: ep_len:653 episode reward: total was -117.960000. running mean: -345.107174\n",
      "ep 9: ep_len:550 episode reward: total was -211.400000. running mean: -343.770102\n",
      "ep 9: ep_len:618 episode reward: total was -371.060000. running mean: -344.043001\n",
      "ep 9: ep_len:79 episode reward: total was -30.440000. running mean: -340.906971\n",
      "ep 9: ep_len:577 episode reward: total was -208.990000. running mean: -339.587801\n",
      "ep 9: ep_len:732 episode reward: total was -295.420000. running mean: -339.146123\n",
      "epsilon:0.199557 episode_count: 70. steps_count: 33955.000000\n",
      "Time elapsed:  97.39020299911499\n",
      "ep 10: ep_len:658 episode reward: total was -311.310000. running mean: -338.867762\n",
      "ep 10: ep_len:307 episode reward: total was -132.170000. running mean: -336.800784\n",
      "ep 10: ep_len:538 episode reward: total was -197.780000. running mean: -335.410576\n",
      "ep 10: ep_len:509 episode reward: total was -223.270000. running mean: -334.289171\n",
      "ep 10: ep_len:3 episode reward: total was 0.000000. running mean: -330.946279\n",
      "ep 10: ep_len:566 episode reward: total was -95.510000. running mean: -328.591916\n",
      "ep 10: ep_len:740 episode reward: total was -208.730000. running mean: -327.393297\n",
      "epsilon:0.199512 episode_count: 77. steps_count: 37276.000000\n",
      "Time elapsed:  106.25677704811096\n",
      "ep 11: ep_len:514 episode reward: total was -107.500000. running mean: -325.194364\n",
      "ep 11: ep_len:717 episode reward: total was -174.580000. running mean: -323.688220\n",
      "ep 11: ep_len:500 episode reward: total was -146.700000. running mean: -321.918338\n",
      "ep 11: ep_len:500 episode reward: total was -181.940000. running mean: -320.518555\n",
      "ep 11: ep_len:122 episode reward: total was -70.720000. running mean: -318.020569\n",
      "ep 11: ep_len:500 episode reward: total was -108.620000. running mean: -315.926564\n",
      "ep 11: ep_len:605 episode reward: total was -185.640000. running mean: -314.623698\n",
      "epsilon:0.199468 episode_count: 84. steps_count: 40734.000000\n",
      "Time elapsed:  115.16989064216614\n",
      "ep 12: ep_len:539 episode reward: total was -292.210000. running mean: -314.399561\n",
      "ep 12: ep_len:531 episode reward: total was -91.320000. running mean: -312.168765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 12: ep_len:79 episode reward: total was -20.310000. running mean: -309.250178\n",
      "ep 12: ep_len:533 episode reward: total was -147.400000. running mean: -307.631676\n",
      "ep 12: ep_len:3 episode reward: total was 0.000000. running mean: -304.555359\n",
      "ep 12: ep_len:502 episode reward: total was -122.710000. running mean: -302.736906\n",
      "ep 12: ep_len:585 episode reward: total was -172.310000. running mean: -301.432637\n",
      "epsilon:0.199424 episode_count: 91. steps_count: 43506.000000\n",
      "Time elapsed:  122.58621215820312\n",
      "ep 13: ep_len:507 episode reward: total was -116.180000. running mean: -299.580110\n",
      "ep 13: ep_len:201 episode reward: total was -96.500000. running mean: -297.549309\n",
      "ep 13: ep_len:555 episode reward: total was -162.090000. running mean: -296.194716\n",
      "ep 13: ep_len:627 episode reward: total was -143.240000. running mean: -294.665169\n",
      "ep 13: ep_len:3 episode reward: total was 0.000000. running mean: -291.718517\n",
      "ep 13: ep_len:224 episode reward: total was -37.990000. running mean: -289.181232\n",
      "ep 13: ep_len:669 episode reward: total was -279.090000. running mean: -289.080320\n",
      "epsilon:0.199379 episode_count: 98. steps_count: 46292.000000\n",
      "Time elapsed:  130.21932697296143\n",
      "ep 14: ep_len:500 episode reward: total was -124.060000. running mean: -287.430116\n",
      "ep 14: ep_len:500 episode reward: total was -186.970000. running mean: -286.425515\n",
      "ep 14: ep_len:511 episode reward: total was -141.630000. running mean: -284.977560\n",
      "ep 14: ep_len:132 episode reward: total was -23.100000. running mean: -282.358785\n",
      "ep 14: ep_len:3 episode reward: total was 1.010000. running mean: -279.525097\n",
      "ep 14: ep_len:524 episode reward: total was -202.960000. running mean: -278.759446\n",
      "ep 14: ep_len:550 episode reward: total was -148.120000. running mean: -277.453051\n",
      "epsilon:0.199335 episode_count: 105. steps_count: 49012.000000\n",
      "Time elapsed:  137.53999066352844\n",
      "ep 15: ep_len:134 episode reward: total was -35.480000. running mean: -275.033321\n",
      "ep 15: ep_len:510 episode reward: total was -128.480000. running mean: -273.567788\n",
      "ep 15: ep_len:595 episode reward: total was -212.980000. running mean: -272.961910\n",
      "ep 15: ep_len:556 episode reward: total was -150.010000. running mean: -271.732391\n",
      "ep 15: ep_len:3 episode reward: total was 0.000000. running mean: -269.015067\n",
      "ep 15: ep_len:612 episode reward: total was -189.150000. running mean: -268.216416\n",
      "ep 15: ep_len:766 episode reward: total was -304.110000. running mean: -268.575352\n",
      "epsilon:0.199291 episode_count: 112. steps_count: 52188.000000\n",
      "Time elapsed:  145.98658442497253\n",
      "ep 16: ep_len:250 episode reward: total was -45.830000. running mean: -266.347898\n",
      "ep 16: ep_len:500 episode reward: total was -13.430000. running mean: -263.818719\n",
      "ep 16: ep_len:428 episode reward: total was -45.080000. running mean: -261.631332\n",
      "ep 16: ep_len:501 episode reward: total was -127.250000. running mean: -260.287519\n",
      "ep 16: ep_len:54 episode reward: total was 9.000000. running mean: -257.594644\n",
      "ep 16: ep_len:638 episode reward: total was -195.500000. running mean: -256.973697\n",
      "ep 16: ep_len:603 episode reward: total was -182.020000. running mean: -256.224160\n",
      "epsilon:0.199246 episode_count: 119. steps_count: 55162.000000\n",
      "Time elapsed:  153.88512563705444\n",
      "ep 17: ep_len:532 episode reward: total was -177.980000. running mean: -255.441719\n",
      "ep 17: ep_len:500 episode reward: total was -43.160000. running mean: -253.318901\n",
      "ep 17: ep_len:500 episode reward: total was -103.130000. running mean: -251.817012\n",
      "ep 17: ep_len:575 episode reward: total was -114.830000. running mean: -250.447142\n",
      "ep 17: ep_len:3 episode reward: total was -1.500000. running mean: -247.957671\n",
      "ep 17: ep_len:505 episode reward: total was -123.490000. running mean: -246.712994\n",
      "ep 17: ep_len:554 episode reward: total was -207.240000. running mean: -246.318264\n",
      "epsilon:0.199202 episode_count: 126. steps_count: 58331.000000\n",
      "Time elapsed:  162.16352438926697\n",
      "ep 18: ep_len:590 episode reward: total was -242.460000. running mean: -246.279682\n",
      "ep 18: ep_len:183 episode reward: total was -108.480000. running mean: -244.901685\n",
      "ep 18: ep_len:502 episode reward: total was -107.530000. running mean: -243.527968\n",
      "ep 18: ep_len:515 episode reward: total was -135.660000. running mean: -242.449288\n",
      "ep 18: ep_len:3 episode reward: total was 0.000000. running mean: -240.024795\n",
      "ep 18: ep_len:500 episode reward: total was -131.040000. running mean: -238.934947\n",
      "ep 18: ep_len:555 episode reward: total was -249.590000. running mean: -239.041498\n",
      "epsilon:0.199158 episode_count: 133. steps_count: 61179.000000\n",
      "Time elapsed:  169.87487769126892\n",
      "ep 19: ep_len:565 episode reward: total was -263.820000. running mean: -239.289283\n",
      "ep 19: ep_len:284 episode reward: total was -111.870000. running mean: -238.015090\n",
      "ep 19: ep_len:516 episode reward: total was -152.080000. running mean: -237.155739\n",
      "ep 19: ep_len:546 episode reward: total was -114.510000. running mean: -235.929282\n",
      "ep 19: ep_len:117 episode reward: total was -10.790000. running mean: -233.677889\n",
      "ep 19: ep_len:585 episode reward: total was -191.500000. running mean: -233.256110\n",
      "ep 19: ep_len:589 episode reward: total was -186.950000. running mean: -232.793049\n",
      "epsilon:0.199113 episode_count: 140. steps_count: 64381.000000\n",
      "Time elapsed:  178.4179425239563\n",
      "ep 20: ep_len:130 episode reward: total was -28.020000. running mean: -230.745319\n",
      "ep 20: ep_len:602 episode reward: total was -191.790000. running mean: -230.355765\n",
      "ep 20: ep_len:501 episode reward: total was -171.870000. running mean: -229.770908\n",
      "ep 20: ep_len:553 episode reward: total was -90.880000. running mean: -228.381999\n",
      "ep 20: ep_len:94 episode reward: total was -22.760000. running mean: -226.325779\n",
      "ep 20: ep_len:518 episode reward: total was -138.840000. running mean: -225.450921\n",
      "ep 20: ep_len:536 episode reward: total was -143.220000. running mean: -224.628612\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.199069 episode_count: 147. steps_count: 67315.000000\n",
      "Time elapsed:  190.97543859481812\n",
      "ep 21: ep_len:506 episode reward: total was -160.720000. running mean: -223.989526\n",
      "ep 21: ep_len:370 episode reward: total was -143.200000. running mean: -223.181630\n",
      "ep 21: ep_len:72 episode reward: total was -12.820000. running mean: -221.078014\n",
      "ep 21: ep_len:56 episode reward: total was -11.210000. running mean: -218.979334\n",
      "ep 21: ep_len:3 episode reward: total was 0.000000. running mean: -216.789540\n",
      "ep 21: ep_len:503 episode reward: total was -163.740000. running mean: -216.259045\n",
      "ep 21: ep_len:594 episode reward: total was -237.360000. running mean: -216.470055\n",
      "epsilon:0.199025 episode_count: 154. steps_count: 69419.000000\n",
      "Time elapsed:  196.95377445220947\n",
      "ep 22: ep_len:672 episode reward: total was -247.160000. running mean: -216.776954\n",
      "ep 22: ep_len:270 episode reward: total was -108.900000. running mean: -215.698185\n",
      "ep 22: ep_len:579 episode reward: total was -160.310000. running mean: -215.144303\n",
      "ep 22: ep_len:501 episode reward: total was -56.680000. running mean: -213.559660\n",
      "ep 22: ep_len:3 episode reward: total was 0.000000. running mean: -211.424063\n",
      "ep 22: ep_len:501 episode reward: total was -77.630000. running mean: -210.086122\n",
      "ep 22: ep_len:568 episode reward: total was -145.630000. running mean: -209.441561\n",
      "epsilon:0.198980 episode_count: 161. steps_count: 72513.000000\n",
      "Time elapsed:  205.10820269584656\n",
      "ep 23: ep_len:502 episode reward: total was -159.780000. running mean: -208.944946\n",
      "ep 23: ep_len:500 episode reward: total was -193.730000. running mean: -208.792796\n",
      "ep 23: ep_len:457 episode reward: total was -65.610000. running mean: -207.360968\n",
      "ep 23: ep_len:516 episode reward: total was -188.490000. running mean: -207.172258\n",
      "ep 23: ep_len:52 episode reward: total was 6.500000. running mean: -205.035536\n",
      "ep 23: ep_len:553 episode reward: total was -125.650000. running mean: -204.241681\n",
      "ep 23: ep_len:619 episode reward: total was -142.460000. running mean: -203.623864\n",
      "epsilon:0.198936 episode_count: 168. steps_count: 75712.000000\n",
      "Time elapsed:  213.6605679988861\n",
      "ep 24: ep_len:541 episode reward: total was -119.360000. running mean: -202.781225\n",
      "ep 24: ep_len:524 episode reward: total was -145.770000. running mean: -202.211113\n",
      "ep 24: ep_len:639 episode reward: total was -254.820000. running mean: -202.737202\n",
      "ep 24: ep_len:47 episode reward: total was -17.830000. running mean: -200.888130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 24: ep_len:108 episode reward: total was -15.330000. running mean: -199.032548\n",
      "ep 24: ep_len:553 episode reward: total was -169.820000. running mean: -198.740423\n",
      "ep 24: ep_len:500 episode reward: total was -140.520000. running mean: -198.158219\n",
      "epsilon:0.198892 episode_count: 175. steps_count: 78624.000000\n",
      "Time elapsed:  221.45384907722473\n",
      "ep 25: ep_len:500 episode reward: total was -107.740000. running mean: -197.254037\n",
      "ep 25: ep_len:500 episode reward: total was -147.150000. running mean: -196.752996\n",
      "ep 25: ep_len:637 episode reward: total was -221.610000. running mean: -197.001566\n",
      "ep 25: ep_len:532 episode reward: total was -122.070000. running mean: -196.252251\n",
      "ep 25: ep_len:3 episode reward: total was 0.000000. running mean: -194.289728\n",
      "ep 25: ep_len:511 episode reward: total was -238.350000. running mean: -194.730331\n",
      "ep 25: ep_len:558 episode reward: total was -207.430000. running mean: -194.857327\n",
      "epsilon:0.198847 episode_count: 182. steps_count: 81865.000000\n",
      "Time elapsed:  230.19536972045898\n",
      "ep 26: ep_len:748 episode reward: total was -358.320000. running mean: -196.491954\n",
      "ep 26: ep_len:565 episode reward: total was -86.730000. running mean: -195.394335\n",
      "ep 26: ep_len:579 episode reward: total was -165.350000. running mean: -195.093891\n",
      "ep 26: ep_len:500 episode reward: total was -82.390000. running mean: -193.966852\n",
      "ep 26: ep_len:3 episode reward: total was 0.000000. running mean: -192.027184\n",
      "ep 26: ep_len:557 episode reward: total was -163.660000. running mean: -191.743512\n",
      "ep 26: ep_len:325 episode reward: total was -125.080000. running mean: -191.076877\n",
      "epsilon:0.198803 episode_count: 189. steps_count: 85142.000000\n",
      "Time elapsed:  238.76146245002747\n",
      "ep 27: ep_len:540 episode reward: total was -179.540000. running mean: -190.961508\n",
      "ep 27: ep_len:540 episode reward: total was -140.690000. running mean: -190.458793\n",
      "ep 27: ep_len:572 episode reward: total was -159.420000. running mean: -190.148405\n",
      "ep 27: ep_len:554 episode reward: total was -143.230000. running mean: -189.679221\n",
      "ep 27: ep_len:74 episode reward: total was -33.830000. running mean: -188.120729\n",
      "ep 27: ep_len:635 episode reward: total was -125.820000. running mean: -187.497722\n",
      "ep 27: ep_len:516 episode reward: total was -162.330000. running mean: -187.246044\n",
      "epsilon:0.198759 episode_count: 196. steps_count: 88573.000000\n",
      "Time elapsed:  247.7838478088379\n",
      "ep 28: ep_len:500 episode reward: total was -135.550000. running mean: -186.729084\n",
      "ep 28: ep_len:500 episode reward: total was -54.710000. running mean: -185.408893\n",
      "ep 28: ep_len:587 episode reward: total was -155.480000. running mean: -185.109604\n",
      "ep 28: ep_len:500 episode reward: total was -95.570000. running mean: -184.214208\n",
      "ep 28: ep_len:130 episode reward: total was -49.690000. running mean: -182.868966\n",
      "ep 28: ep_len:572 episode reward: total was -153.490000. running mean: -182.575176\n",
      "ep 28: ep_len:571 episode reward: total was -201.560000. running mean: -182.765025\n",
      "epsilon:0.198714 episode_count: 203. steps_count: 91933.000000\n",
      "Time elapsed:  256.6879472732544\n",
      "ep 29: ep_len:547 episode reward: total was -196.430000. running mean: -182.901674\n",
      "ep 29: ep_len:616 episode reward: total was -149.200000. running mean: -182.564658\n",
      "ep 29: ep_len:500 episode reward: total was -163.020000. running mean: -182.369211\n",
      "ep 29: ep_len:508 episode reward: total was -144.620000. running mean: -181.991719\n",
      "ep 29: ep_len:119 episode reward: total was -38.210000. running mean: -180.553902\n",
      "ep 29: ep_len:547 episode reward: total was -149.220000. running mean: -180.240563\n",
      "ep 29: ep_len:655 episode reward: total was -188.920000. running mean: -180.327357\n",
      "epsilon:0.198670 episode_count: 210. steps_count: 95425.000000\n",
      "Time elapsed:  265.69204354286194\n",
      "ep 30: ep_len:533 episode reward: total was -134.180000. running mean: -179.865883\n",
      "ep 30: ep_len:643 episode reward: total was -107.420000. running mean: -179.141425\n",
      "ep 30: ep_len:500 episode reward: total was -122.620000. running mean: -178.576210\n",
      "ep 30: ep_len:571 episode reward: total was -149.290000. running mean: -178.283348\n",
      "ep 30: ep_len:88 episode reward: total was -23.310000. running mean: -176.733615\n",
      "ep 30: ep_len:604 episode reward: total was -175.060000. running mean: -176.716879\n",
      "ep 30: ep_len:617 episode reward: total was -136.110000. running mean: -176.310810\n",
      "epsilon:0.198626 episode_count: 217. steps_count: 98981.000000\n",
      "Time elapsed:  275.01870608329773\n",
      "ep 31: ep_len:500 episode reward: total was -108.820000. running mean: -175.635902\n",
      "ep 31: ep_len:500 episode reward: total was -38.270000. running mean: -174.262243\n",
      "ep 31: ep_len:574 episode reward: total was -148.660000. running mean: -174.006220\n",
      "ep 31: ep_len:132 episode reward: total was -17.480000. running mean: -172.440958\n",
      "ep 31: ep_len:3 episode reward: total was 0.000000. running mean: -170.716549\n",
      "ep 31: ep_len:560 episode reward: total was -142.890000. running mean: -170.438283\n",
      "ep 31: ep_len:501 episode reward: total was -184.130000. running mean: -170.575200\n",
      "epsilon:0.198581 episode_count: 224. steps_count: 101751.000000\n",
      "Time elapsed:  282.5176990032196\n",
      "ep 32: ep_len:585 episode reward: total was -235.850000. running mean: -171.227948\n",
      "ep 32: ep_len:501 episode reward: total was -100.020000. running mean: -170.515869\n",
      "ep 32: ep_len:547 episode reward: total was -107.680000. running mean: -169.887510\n",
      "ep 32: ep_len:518 episode reward: total was -142.920000. running mean: -169.617835\n",
      "ep 32: ep_len:3 episode reward: total was 0.000000. running mean: -167.921657\n",
      "ep 32: ep_len:505 episode reward: total was -138.790000. running mean: -167.630340\n",
      "ep 32: ep_len:596 episode reward: total was -118.310000. running mean: -167.137137\n",
      "epsilon:0.198537 episode_count: 231. steps_count: 105006.000000\n",
      "Time elapsed:  291.20907068252563\n",
      "ep 33: ep_len:601 episode reward: total was -125.930000. running mean: -166.725065\n",
      "ep 33: ep_len:524 episode reward: total was -205.950000. running mean: -167.117315\n",
      "ep 33: ep_len:565 episode reward: total was -119.500000. running mean: -166.641141\n",
      "ep 33: ep_len:551 episode reward: total was -121.220000. running mean: -166.186930\n",
      "ep 33: ep_len:3 episode reward: total was 0.000000. running mean: -164.525061\n",
      "ep 33: ep_len:544 episode reward: total was -131.220000. running mean: -164.192010\n",
      "ep 33: ep_len:620 episode reward: total was -173.090000. running mean: -164.280990\n",
      "epsilon:0.198493 episode_count: 238. steps_count: 108414.000000\n",
      "Time elapsed:  300.2826874256134\n",
      "ep 34: ep_len:620 episode reward: total was -127.700000. running mean: -163.915180\n",
      "ep 34: ep_len:500 episode reward: total was -80.680000. running mean: -163.082828\n",
      "ep 34: ep_len:560 episode reward: total was -139.180000. running mean: -162.843800\n",
      "ep 34: ep_len:533 episode reward: total was -162.100000. running mean: -162.836362\n",
      "ep 34: ep_len:3 episode reward: total was 0.000000. running mean: -161.207998\n",
      "ep 34: ep_len:531 episode reward: total was -187.810000. running mean: -161.474018\n",
      "ep 34: ep_len:505 episode reward: total was -139.650000. running mean: -161.255778\n",
      "epsilon:0.198448 episode_count: 245. steps_count: 111666.000000\n",
      "Time elapsed:  308.89597153663635\n",
      "ep 35: ep_len:588 episode reward: total was -118.990000. running mean: -160.833121\n",
      "ep 35: ep_len:557 episode reward: total was -87.170000. running mean: -160.096489\n",
      "ep 35: ep_len:594 episode reward: total was -214.300000. running mean: -160.638524\n",
      "ep 35: ep_len:500 episode reward: total was -130.450000. running mean: -160.336639\n",
      "ep 35: ep_len:3 episode reward: total was 0.000000. running mean: -158.733273\n",
      "ep 35: ep_len:544 episode reward: total was -130.270000. running mean: -158.448640\n",
      "ep 35: ep_len:605 episode reward: total was -160.310000. running mean: -158.467254\n",
      "epsilon:0.198404 episode_count: 252. steps_count: 115057.000000\n",
      "Time elapsed:  317.83414030075073\n",
      "ep 36: ep_len:207 episode reward: total was -1.280000. running mean: -156.895381\n",
      "ep 36: ep_len:184 episode reward: total was -56.030000. running mean: -155.886727\n",
      "ep 36: ep_len:577 episode reward: total was -159.100000. running mean: -155.918860\n",
      "ep 36: ep_len:170 episode reward: total was -20.950000. running mean: -154.569171\n",
      "ep 36: ep_len:100 episode reward: total was -31.330000. running mean: -153.336780\n",
      "ep 36: ep_len:526 episode reward: total was -128.390000. running mean: -153.087312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 36: ep_len:630 episode reward: total was -177.320000. running mean: -153.329639\n",
      "epsilon:0.198360 episode_count: 259. steps_count: 117451.000000\n",
      "Time elapsed:  324.3915026187897\n",
      "ep 37: ep_len:564 episode reward: total was -163.180000. running mean: -153.428142\n",
      "ep 37: ep_len:594 episode reward: total was -126.600000. running mean: -153.159861\n",
      "ep 37: ep_len:446 episode reward: total was -61.450000. running mean: -152.242762\n",
      "ep 37: ep_len:500 episode reward: total was -117.800000. running mean: -151.898335\n",
      "ep 37: ep_len:3 episode reward: total was 0.000000. running mean: -150.379351\n",
      "ep 37: ep_len:531 episode reward: total was -155.970000. running mean: -150.435258\n",
      "ep 37: ep_len:358 episode reward: total was -164.210000. running mean: -150.573005\n",
      "epsilon:0.198315 episode_count: 266. steps_count: 120447.000000\n",
      "Time elapsed:  332.3329703807831\n",
      "ep 38: ep_len:500 episode reward: total was -83.130000. running mean: -149.898575\n",
      "ep 38: ep_len:520 episode reward: total was -166.500000. running mean: -150.064589\n",
      "ep 38: ep_len:501 episode reward: total was -134.180000. running mean: -149.905744\n",
      "ep 38: ep_len:170 episode reward: total was -1.910000. running mean: -148.425786\n",
      "ep 38: ep_len:3 episode reward: total was 0.000000. running mean: -146.941528\n",
      "ep 38: ep_len:552 episode reward: total was -182.080000. running mean: -147.292913\n",
      "ep 38: ep_len:544 episode reward: total was -94.670000. running mean: -146.766684\n",
      "epsilon:0.198271 episode_count: 273. steps_count: 123237.000000\n",
      "Time elapsed:  339.89720368385315\n",
      "ep 39: ep_len:529 episode reward: total was -103.090000. running mean: -146.329917\n",
      "ep 39: ep_len:501 episode reward: total was -41.490000. running mean: -145.281518\n",
      "ep 39: ep_len:623 episode reward: total was -186.210000. running mean: -145.690803\n",
      "ep 39: ep_len:615 episode reward: total was -98.880000. running mean: -145.222695\n",
      "ep 39: ep_len:108 episode reward: total was 9.250000. running mean: -143.677968\n",
      "ep 39: ep_len:513 episode reward: total was -115.430000. running mean: -143.395488\n",
      "ep 39: ep_len:285 episode reward: total was -131.180000. running mean: -143.273333\n",
      "epsilon:0.198227 episode_count: 280. steps_count: 126411.000000\n",
      "Time elapsed:  348.2143156528473\n",
      "ep 40: ep_len:589 episode reward: total was -114.150000. running mean: -142.982100\n",
      "ep 40: ep_len:516 episode reward: total was -34.620000. running mean: -141.898479\n",
      "ep 40: ep_len:348 episode reward: total was -56.670000. running mean: -141.046194\n",
      "ep 40: ep_len:501 episode reward: total was -100.110000. running mean: -140.636832\n",
      "ep 40: ep_len:3 episode reward: total was 0.000000. running mean: -139.230464\n",
      "ep 40: ep_len:632 episode reward: total was -122.550000. running mean: -139.063659\n",
      "ep 40: ep_len:558 episode reward: total was -107.720000. running mean: -138.750223\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.198182 episode_count: 287. steps_count: 129558.000000\n",
      "Time elapsed:  361.46141386032104\n",
      "ep 41: ep_len:124 episode reward: total was -12.560000. running mean: -137.488320\n",
      "ep 41: ep_len:525 episode reward: total was -115.070000. running mean: -137.264137\n",
      "ep 41: ep_len:572 episode reward: total was -210.530000. running mean: -137.996796\n",
      "ep 41: ep_len:500 episode reward: total was -164.360000. running mean: -138.260428\n",
      "ep 41: ep_len:2 episode reward: total was -0.500000. running mean: -136.882824\n",
      "ep 41: ep_len:523 episode reward: total was -164.930000. running mean: -137.163295\n",
      "ep 41: ep_len:211 episode reward: total was -73.180000. running mean: -136.523462\n",
      "epsilon:0.198138 episode_count: 294. steps_count: 132015.000000\n",
      "Time elapsed:  368.16827940940857\n",
      "ep 42: ep_len:527 episode reward: total was -128.650000. running mean: -136.444728\n",
      "ep 42: ep_len:509 episode reward: total was -124.450000. running mean: -136.324780\n",
      "ep 42: ep_len:540 episode reward: total was -192.570000. running mean: -136.887233\n",
      "ep 42: ep_len:562 episode reward: total was -77.320000. running mean: -136.291560\n",
      "ep 42: ep_len:99 episode reward: total was -33.760000. running mean: -135.266245\n",
      "ep 42: ep_len:586 episode reward: total was -141.890000. running mean: -135.332482\n",
      "ep 42: ep_len:581 episode reward: total was -123.180000. running mean: -135.210957\n",
      "epsilon:0.198094 episode_count: 301. steps_count: 135419.000000\n",
      "Time elapsed:  377.2627737522125\n",
      "ep 43: ep_len:258 episode reward: total was -27.240000. running mean: -134.131248\n",
      "ep 43: ep_len:520 episode reward: total was -157.140000. running mean: -134.361335\n",
      "ep 43: ep_len:503 episode reward: total was -125.780000. running mean: -134.275522\n",
      "ep 43: ep_len:500 episode reward: total was -68.400000. running mean: -133.616767\n",
      "ep 43: ep_len:126 episode reward: total was 7.810000. running mean: -132.202499\n",
      "ep 43: ep_len:599 episode reward: total was -152.390000. running mean: -132.404374\n",
      "ep 43: ep_len:316 episode reward: total was -110.940000. running mean: -132.189730\n",
      "epsilon:0.198049 episode_count: 308. steps_count: 138241.000000\n",
      "Time elapsed:  384.922287940979\n",
      "ep 44: ep_len:500 episode reward: total was -176.250000. running mean: -132.630333\n",
      "ep 44: ep_len:500 episode reward: total was -147.320000. running mean: -132.777230\n",
      "ep 44: ep_len:551 episode reward: total was -160.420000. running mean: -133.053657\n",
      "ep 44: ep_len:516 episode reward: total was -55.440000. running mean: -132.277521\n",
      "ep 44: ep_len:3 episode reward: total was 0.000000. running mean: -130.954746\n",
      "ep 44: ep_len:599 episode reward: total was -156.490000. running mean: -131.210098\n",
      "ep 44: ep_len:301 episode reward: total was -119.550000. running mean: -131.093497\n",
      "epsilon:0.198005 episode_count: 315. steps_count: 141211.000000\n",
      "Time elapsed:  392.7530851364136\n",
      "ep 45: ep_len:255 episode reward: total was -72.520000. running mean: -130.507762\n",
      "ep 45: ep_len:603 episode reward: total was -94.720000. running mean: -130.149885\n",
      "ep 45: ep_len:566 episode reward: total was -117.950000. running mean: -130.027886\n",
      "ep 45: ep_len:522 episode reward: total was -161.700000. running mean: -130.344607\n",
      "ep 45: ep_len:86 episode reward: total was -2.790000. running mean: -129.069061\n",
      "ep 45: ep_len:500 episode reward: total was -97.630000. running mean: -128.754670\n",
      "ep 45: ep_len:607 episode reward: total was -110.870000. running mean: -128.575824\n",
      "epsilon:0.197961 episode_count: 322. steps_count: 144350.000000\n",
      "Time elapsed:  400.9619708061218\n",
      "ep 46: ep_len:518 episode reward: total was -144.550000. running mean: -128.735565\n",
      "ep 46: ep_len:505 episode reward: total was -77.810000. running mean: -128.226310\n",
      "ep 46: ep_len:567 episode reward: total was -96.900000. running mean: -127.913047\n",
      "ep 46: ep_len:155 episode reward: total was -22.560000. running mean: -126.859516\n",
      "ep 46: ep_len:89 episode reward: total was -13.380000. running mean: -125.724721\n",
      "ep 46: ep_len:532 episode reward: total was -141.550000. running mean: -125.882974\n",
      "ep 46: ep_len:211 episode reward: total was -101.030000. running mean: -125.634444\n",
      "epsilon:0.197916 episode_count: 329. steps_count: 146927.000000\n",
      "Time elapsed:  407.91958475112915\n",
      "ep 47: ep_len:686 episode reward: total was -137.930000. running mean: -125.757400\n",
      "ep 47: ep_len:604 episode reward: total was -192.150000. running mean: -126.421326\n",
      "ep 47: ep_len:566 episode reward: total was -250.840000. running mean: -127.665512\n",
      "ep 47: ep_len:500 episode reward: total was -100.530000. running mean: -127.394157\n",
      "ep 47: ep_len:78 episode reward: total was -5.840000. running mean: -126.178616\n",
      "ep 47: ep_len:186 episode reward: total was -5.880000. running mean: -124.975629\n",
      "ep 47: ep_len:549 episode reward: total was -71.430000. running mean: -124.440173\n",
      "epsilon:0.197872 episode_count: 336. steps_count: 150096.000000\n",
      "Time elapsed:  416.16340160369873\n",
      "ep 48: ep_len:576 episode reward: total was -102.350000. running mean: -124.219271\n",
      "ep 48: ep_len:523 episode reward: total was -148.610000. running mean: -124.463179\n",
      "ep 48: ep_len:532 episode reward: total was -129.490000. running mean: -124.513447\n",
      "ep 48: ep_len:500 episode reward: total was -92.950000. running mean: -124.197812\n",
      "ep 48: ep_len:99 episode reward: total was 4.290000. running mean: -122.912934\n",
      "ep 48: ep_len:591 episode reward: total was -167.790000. running mean: -123.361705\n",
      "ep 48: ep_len:500 episode reward: total was -136.010000. running mean: -123.488188\n",
      "epsilon:0.197828 episode_count: 343. steps_count: 153417.000000\n",
      "Time elapsed:  425.0206973552704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 49: ep_len:500 episode reward: total was -119.240000. running mean: -123.445706\n",
      "ep 49: ep_len:500 episode reward: total was -34.850000. running mean: -122.559749\n",
      "ep 49: ep_len:78 episode reward: total was -2.750000. running mean: -121.361652\n",
      "ep 49: ep_len:547 episode reward: total was -74.330000. running mean: -120.891335\n",
      "ep 49: ep_len:50 episode reward: total was 11.500000. running mean: -119.567422\n",
      "ep 49: ep_len:615 episode reward: total was -73.450000. running mean: -119.106247\n",
      "ep 49: ep_len:542 episode reward: total was -202.980000. running mean: -119.944985\n",
      "epsilon:0.197783 episode_count: 350. steps_count: 156249.000000\n",
      "Time elapsed:  432.555499792099\n",
      "ep 50: ep_len:134 episode reward: total was -9.560000. running mean: -118.841135\n",
      "ep 50: ep_len:514 episode reward: total was -136.010000. running mean: -119.012824\n",
      "ep 50: ep_len:622 episode reward: total was -161.120000. running mean: -119.433896\n",
      "ep 50: ep_len:500 episode reward: total was -146.900000. running mean: -119.708557\n",
      "ep 50: ep_len:3 episode reward: total was 0.000000. running mean: -118.511471\n",
      "ep 50: ep_len:500 episode reward: total was -71.720000. running mean: -118.043556\n",
      "ep 50: ep_len:603 episode reward: total was -149.910000. running mean: -118.362221\n",
      "epsilon:0.197739 episode_count: 357. steps_count: 159125.000000\n",
      "Time elapsed:  440.8422820568085\n",
      "ep 51: ep_len:649 episode reward: total was -240.800000. running mean: -119.586599\n",
      "ep 51: ep_len:500 episode reward: total was -68.900000. running mean: -119.079733\n",
      "ep 51: ep_len:618 episode reward: total was -222.100000. running mean: -120.109935\n",
      "ep 51: ep_len:496 episode reward: total was -77.280000. running mean: -119.681636\n",
      "ep 51: ep_len:53 episode reward: total was 11.500000. running mean: -118.369820\n",
      "ep 51: ep_len:550 episode reward: total was -100.580000. running mean: -118.191921\n",
      "ep 51: ep_len:334 episode reward: total was -115.190000. running mean: -118.161902\n",
      "epsilon:0.197695 episode_count: 364. steps_count: 162325.000000\n",
      "Time elapsed:  449.42080664634705\n",
      "ep 52: ep_len:500 episode reward: total was -59.190000. running mean: -117.572183\n",
      "ep 52: ep_len:541 episode reward: total was -70.920000. running mean: -117.105661\n",
      "ep 52: ep_len:500 episode reward: total was -56.160000. running mean: -116.496205\n",
      "ep 52: ep_len:500 episode reward: total was -85.770000. running mean: -116.188943\n",
      "ep 52: ep_len:3 episode reward: total was 0.000000. running mean: -115.027053\n",
      "ep 52: ep_len:500 episode reward: total was -79.410000. running mean: -114.670883\n",
      "ep 52: ep_len:618 episode reward: total was -104.830000. running mean: -114.572474\n",
      "epsilon:0.197650 episode_count: 371. steps_count: 165487.000000\n",
      "Time elapsed:  457.85337829589844\n",
      "ep 53: ep_len:596 episode reward: total was -67.330000. running mean: -114.100049\n",
      "ep 53: ep_len:748 episode reward: total was -288.620000. running mean: -115.845249\n",
      "ep 53: ep_len:351 episode reward: total was -38.000000. running mean: -115.066796\n",
      "ep 53: ep_len:513 episode reward: total was -118.230000. running mean: -115.098428\n",
      "ep 53: ep_len:74 episode reward: total was -23.390000. running mean: -114.181344\n",
      "ep 53: ep_len:533 episode reward: total was -103.990000. running mean: -114.079430\n",
      "ep 53: ep_len:553 episode reward: total was -88.260000. running mean: -113.821236\n",
      "epsilon:0.197606 episode_count: 378. steps_count: 168855.000000\n",
      "Time elapsed:  466.6714551448822\n",
      "ep 54: ep_len:577 episode reward: total was -76.630000. running mean: -113.449324\n",
      "ep 54: ep_len:507 episode reward: total was -19.420000. running mean: -112.509031\n",
      "ep 54: ep_len:646 episode reward: total was -149.950000. running mean: -112.883440\n",
      "ep 54: ep_len:56 episode reward: total was -15.230000. running mean: -111.906906\n",
      "ep 54: ep_len:3 episode reward: total was 0.000000. running mean: -110.787837\n",
      "ep 54: ep_len:676 episode reward: total was -127.340000. running mean: -110.953358\n",
      "ep 54: ep_len:548 episode reward: total was -115.650000. running mean: -111.000325\n",
      "epsilon:0.197562 episode_count: 385. steps_count: 171868.000000\n",
      "Time elapsed:  474.81588768959045\n",
      "ep 55: ep_len:500 episode reward: total was -97.790000. running mean: -110.868222\n",
      "ep 55: ep_len:500 episode reward: total was -20.570000. running mean: -109.965239\n",
      "ep 55: ep_len:639 episode reward: total was -236.850000. running mean: -111.234087\n",
      "ep 55: ep_len:500 episode reward: total was -70.000000. running mean: -110.821746\n",
      "ep 55: ep_len:3 episode reward: total was -1.500000. running mean: -109.728529\n",
      "ep 55: ep_len:539 episode reward: total was -121.540000. running mean: -109.846643\n",
      "ep 55: ep_len:554 episode reward: total was -146.410000. running mean: -110.212277\n",
      "epsilon:0.197517 episode_count: 392. steps_count: 175103.000000\n",
      "Time elapsed:  483.33707213401794\n",
      "ep 56: ep_len:134 episode reward: total was -24.990000. running mean: -109.360054\n",
      "ep 56: ep_len:201 episode reward: total was -80.250000. running mean: -109.068954\n",
      "ep 56: ep_len:551 episode reward: total was -133.420000. running mean: -109.312464\n",
      "ep 56: ep_len:500 episode reward: total was -153.190000. running mean: -109.751239\n",
      "ep 56: ep_len:3 episode reward: total was 0.000000. running mean: -108.653727\n",
      "ep 56: ep_len:571 episode reward: total was -80.940000. running mean: -108.376590\n",
      "ep 56: ep_len:612 episode reward: total was -194.710000. running mean: -109.239924\n",
      "epsilon:0.197473 episode_count: 399. steps_count: 177675.000000\n",
      "Time elapsed:  490.3716993331909\n",
      "ep 57: ep_len:500 episode reward: total was -126.550000. running mean: -109.413025\n",
      "ep 57: ep_len:285 episode reward: total was -116.370000. running mean: -109.482594\n",
      "ep 57: ep_len:434 episode reward: total was -98.990000. running mean: -109.377668\n",
      "ep 57: ep_len:547 episode reward: total was -120.210000. running mean: -109.485992\n",
      "ep 57: ep_len:3 episode reward: total was -1.500000. running mean: -108.406132\n",
      "ep 57: ep_len:175 episode reward: total was -15.570000. running mean: -107.477770\n",
      "ep 57: ep_len:602 episode reward: total was -140.400000. running mean: -107.806993\n",
      "epsilon:0.197429 episode_count: 406. steps_count: 180221.000000\n",
      "Time elapsed:  497.3950116634369\n",
      "ep 58: ep_len:211 episode reward: total was -21.260000. running mean: -106.941523\n",
      "ep 58: ep_len:182 episode reward: total was -61.070000. running mean: -106.482808\n",
      "ep 58: ep_len:573 episode reward: total was -143.500000. running mean: -106.852980\n",
      "ep 58: ep_len:502 episode reward: total was -184.580000. running mean: -107.630250\n",
      "ep 58: ep_len:3 episode reward: total was 0.000000. running mean: -106.553947\n",
      "ep 58: ep_len:513 episode reward: total was -112.360000. running mean: -106.612008\n",
      "ep 58: ep_len:537 episode reward: total was -125.760000. running mean: -106.803488\n",
      "epsilon:0.197384 episode_count: 413. steps_count: 182742.000000\n",
      "Time elapsed:  504.2303762435913\n",
      "ep 59: ep_len:580 episode reward: total was -194.280000. running mean: -107.678253\n",
      "ep 59: ep_len:500 episode reward: total was -33.270000. running mean: -106.934170\n",
      "ep 59: ep_len:500 episode reward: total was -101.510000. running mean: -106.879929\n",
      "ep 59: ep_len:47 episode reward: total was -3.200000. running mean: -105.843129\n",
      "ep 59: ep_len:3 episode reward: total was 0.000000. running mean: -104.784698\n",
      "ep 59: ep_len:505 episode reward: total was -137.590000. running mean: -105.112751\n",
      "ep 59: ep_len:555 episode reward: total was -149.830000. running mean: -105.559924\n",
      "epsilon:0.197340 episode_count: 420. steps_count: 185432.000000\n",
      "Time elapsed:  511.53362488746643\n",
      "ep 60: ep_len:578 episode reward: total was -107.650000. running mean: -105.580824\n",
      "ep 60: ep_len:627 episode reward: total was -176.340000. running mean: -106.288416\n",
      "ep 60: ep_len:500 episode reward: total was -122.890000. running mean: -106.454432\n",
      "ep 60: ep_len:526 episode reward: total was -119.260000. running mean: -106.582488\n",
      "ep 60: ep_len:52 episode reward: total was -10.000000. running mean: -105.616663\n",
      "ep 60: ep_len:528 episode reward: total was -93.400000. running mean: -105.494496\n",
      "ep 60: ep_len:525 episode reward: total was -141.240000. running mean: -105.851951\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.197296 episode_count: 427. steps_count: 188768.000000\n",
      "Time elapsed:  525.0729897022247\n",
      "ep 61: ep_len:562 episode reward: total was -91.610000. running mean: -105.709532\n",
      "ep 61: ep_len:500 episode reward: total was -37.800000. running mean: -105.030436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 61: ep_len:501 episode reward: total was -75.820000. running mean: -104.738332\n",
      "ep 61: ep_len:501 episode reward: total was -136.130000. running mean: -105.052249\n",
      "ep 61: ep_len:3 episode reward: total was 0.000000. running mean: -104.001726\n",
      "ep 61: ep_len:505 episode reward: total was -88.390000. running mean: -103.845609\n",
      "ep 61: ep_len:508 episode reward: total was -104.750000. running mean: -103.854653\n",
      "epsilon:0.197251 episode_count: 434. steps_count: 191848.000000\n",
      "Time elapsed:  533.2734034061432\n",
      "ep 62: ep_len:607 episode reward: total was -107.120000. running mean: -103.887306\n",
      "ep 62: ep_len:671 episode reward: total was -112.450000. running mean: -103.972933\n",
      "ep 62: ep_len:582 episode reward: total was -135.100000. running mean: -104.284204\n",
      "ep 62: ep_len:543 episode reward: total was -61.970000. running mean: -103.861062\n",
      "ep 62: ep_len:103 episode reward: total was -31.670000. running mean: -103.139151\n",
      "ep 62: ep_len:582 episode reward: total was -101.210000. running mean: -103.119860\n",
      "ep 62: ep_len:586 episode reward: total was -115.220000. running mean: -103.240861\n",
      "epsilon:0.197207 episode_count: 441. steps_count: 195522.000000\n",
      "Time elapsed:  542.9551982879639\n",
      "ep 63: ep_len:583 episode reward: total was -70.330000. running mean: -102.911752\n",
      "ep 63: ep_len:608 episode reward: total was -116.110000. running mean: -103.043735\n",
      "ep 63: ep_len:531 episode reward: total was -200.410000. running mean: -104.017398\n",
      "ep 63: ep_len:517 episode reward: total was -148.200000. running mean: -104.459224\n",
      "ep 63: ep_len:3 episode reward: total was 0.000000. running mean: -103.414631\n",
      "ep 63: ep_len:500 episode reward: total was -104.260000. running mean: -103.423085\n",
      "ep 63: ep_len:607 episode reward: total was -196.860000. running mean: -104.357454\n",
      "epsilon:0.197163 episode_count: 448. steps_count: 198871.000000\n",
      "Time elapsed:  551.7864298820496\n",
      "ep 64: ep_len:608 episode reward: total was -213.490000. running mean: -105.448780\n",
      "ep 64: ep_len:554 episode reward: total was -113.890000. running mean: -105.533192\n",
      "ep 64: ep_len:514 episode reward: total was -110.470000. running mean: -105.582560\n",
      "ep 64: ep_len:373 episode reward: total was -69.520000. running mean: -105.221934\n",
      "ep 64: ep_len:3 episode reward: total was 0.000000. running mean: -104.169715\n",
      "ep 64: ep_len:500 episode reward: total was -87.860000. running mean: -104.006618\n",
      "ep 64: ep_len:619 episode reward: total was -143.410000. running mean: -104.400652\n",
      "epsilon:0.197118 episode_count: 455. steps_count: 202042.000000\n",
      "Time elapsed:  560.3545908927917\n",
      "ep 65: ep_len:665 episode reward: total was -161.000000. running mean: -104.966645\n",
      "ep 65: ep_len:591 episode reward: total was -95.870000. running mean: -104.875679\n",
      "ep 65: ep_len:501 episode reward: total was -101.630000. running mean: -104.843222\n",
      "ep 65: ep_len:562 episode reward: total was -121.950000. running mean: -105.014290\n",
      "ep 65: ep_len:3 episode reward: total was 0.000000. running mean: -103.964147\n",
      "ep 65: ep_len:500 episode reward: total was -119.190000. running mean: -104.116405\n",
      "ep 65: ep_len:582 episode reward: total was -136.240000. running mean: -104.437641\n",
      "epsilon:0.197074 episode_count: 462. steps_count: 205446.000000\n",
      "Time elapsed:  569.4062783718109\n",
      "ep 66: ep_len:613 episode reward: total was -98.600000. running mean: -104.379265\n",
      "ep 66: ep_len:501 episode reward: total was -18.660000. running mean: -103.522072\n",
      "ep 66: ep_len:500 episode reward: total was -109.240000. running mean: -103.579252\n",
      "ep 66: ep_len:517 episode reward: total was -99.100000. running mean: -103.534459\n",
      "ep 66: ep_len:121 episode reward: total was -1.780000. running mean: -102.516914\n",
      "ep 66: ep_len:581 episode reward: total was -154.150000. running mean: -103.033245\n",
      "ep 66: ep_len:336 episode reward: total was -117.360000. running mean: -103.176513\n",
      "epsilon:0.197030 episode_count: 469. steps_count: 208615.000000\n",
      "Time elapsed:  578.053840637207\n",
      "ep 67: ep_len:550 episode reward: total was -73.030000. running mean: -102.875048\n",
      "ep 67: ep_len:588 episode reward: total was -129.820000. running mean: -103.144497\n",
      "ep 67: ep_len:527 episode reward: total was -120.310000. running mean: -103.316152\n",
      "ep 67: ep_len:515 episode reward: total was -101.870000. running mean: -103.301691\n",
      "ep 67: ep_len:48 episode reward: total was 13.500000. running mean: -102.133674\n",
      "ep 67: ep_len:552 episode reward: total was -151.420000. running mean: -102.626537\n",
      "ep 67: ep_len:194 episode reward: total was -70.290000. running mean: -102.303172\n",
      "epsilon:0.196985 episode_count: 476. steps_count: 211589.000000\n",
      "Time elapsed:  586.0406472682953\n",
      "ep 68: ep_len:645 episode reward: total was -215.130000. running mean: -103.431440\n",
      "ep 68: ep_len:647 episode reward: total was -26.370000. running mean: -102.660826\n",
      "ep 68: ep_len:500 episode reward: total was -109.940000. running mean: -102.733617\n",
      "ep 68: ep_len:56 episode reward: total was -16.240000. running mean: -101.868681\n",
      "ep 68: ep_len:89 episode reward: total was -37.780000. running mean: -101.227794\n",
      "ep 68: ep_len:616 episode reward: total was -130.310000. running mean: -101.518616\n",
      "ep 68: ep_len:608 episode reward: total was -109.540000. running mean: -101.598830\n",
      "epsilon:0.196941 episode_count: 483. steps_count: 214750.000000\n",
      "Time elapsed:  594.5259430408478\n",
      "ep 69: ep_len:500 episode reward: total was -88.020000. running mean: -101.463042\n",
      "ep 69: ep_len:500 episode reward: total was -32.400000. running mean: -100.772412\n",
      "ep 69: ep_len:521 episode reward: total was -139.970000. running mean: -101.164387\n",
      "ep 69: ep_len:510 episode reward: total was -106.720000. running mean: -101.219944\n",
      "ep 69: ep_len:3 episode reward: total was 0.000000. running mean: -100.207744\n",
      "ep 69: ep_len:522 episode reward: total was -130.750000. running mean: -100.513167\n",
      "ep 69: ep_len:326 episode reward: total was -97.320000. running mean: -100.481235\n",
      "epsilon:0.196897 episode_count: 490. steps_count: 217632.000000\n",
      "Time elapsed:  602.2606105804443\n",
      "ep 70: ep_len:500 episode reward: total was -133.660000. running mean: -100.813023\n",
      "ep 70: ep_len:502 episode reward: total was -70.510000. running mean: -100.509992\n",
      "ep 70: ep_len:500 episode reward: total was -81.620000. running mean: -100.321092\n",
      "ep 70: ep_len:645 episode reward: total was -101.120000. running mean: -100.329082\n",
      "ep 70: ep_len:88 episode reward: total was -0.780000. running mean: -99.333591\n",
      "ep 70: ep_len:244 episode reward: total was -71.130000. running mean: -99.051555\n",
      "ep 70: ep_len:500 episode reward: total was -130.820000. running mean: -99.369239\n",
      "epsilon:0.196852 episode_count: 497. steps_count: 220611.000000\n",
      "Time elapsed:  610.365142583847\n",
      "ep 71: ep_len:566 episode reward: total was -86.810000. running mean: -99.243647\n",
      "ep 71: ep_len:500 episode reward: total was -95.260000. running mean: -99.203810\n",
      "ep 71: ep_len:507 episode reward: total was -134.220000. running mean: -99.553972\n",
      "ep 71: ep_len:611 episode reward: total was -67.610000. running mean: -99.234533\n",
      "ep 71: ep_len:3 episode reward: total was -1.500000. running mean: -98.257187\n",
      "ep 71: ep_len:248 episode reward: total was -7.170000. running mean: -97.346315\n",
      "ep 71: ep_len:545 episode reward: total was -122.120000. running mean: -97.594052\n",
      "epsilon:0.196808 episode_count: 504. steps_count: 223591.000000\n",
      "Time elapsed:  618.3851337432861\n",
      "ep 72: ep_len:500 episode reward: total was -64.050000. running mean: -97.258612\n",
      "ep 72: ep_len:531 episode reward: total was -123.030000. running mean: -97.516326\n",
      "ep 72: ep_len:585 episode reward: total was -181.160000. running mean: -98.352762\n",
      "ep 72: ep_len:533 episode reward: total was -87.750000. running mean: -98.246735\n",
      "ep 72: ep_len:3 episode reward: total was 0.000000. running mean: -97.264267\n",
      "ep 72: ep_len:525 episode reward: total was -108.870000. running mean: -97.380325\n",
      "ep 72: ep_len:500 episode reward: total was -178.500000. running mean: -98.191521\n",
      "epsilon:0.196764 episode_count: 511. steps_count: 226768.000000\n",
      "Time elapsed:  626.888653755188\n",
      "ep 73: ep_len:665 episode reward: total was -272.600000. running mean: -99.935606\n",
      "ep 73: ep_len:549 episode reward: total was -136.290000. running mean: -100.299150\n",
      "ep 73: ep_len:648 episode reward: total was -161.620000. running mean: -100.912359\n",
      "ep 73: ep_len:394 episode reward: total was -77.050000. running mean: -100.673735\n",
      "ep 73: ep_len:3 episode reward: total was 0.000000. running mean: -99.666998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 73: ep_len:584 episode reward: total was -121.510000. running mean: -99.885428\n",
      "ep 73: ep_len:197 episode reward: total was -55.650000. running mean: -99.443073\n",
      "epsilon:0.196719 episode_count: 518. steps_count: 229808.000000\n",
      "Time elapsed:  635.0445146560669\n",
      "ep 74: ep_len:572 episode reward: total was -82.130000. running mean: -99.269943\n",
      "ep 74: ep_len:500 episode reward: total was -78.280000. running mean: -99.060043\n",
      "ep 74: ep_len:522 episode reward: total was -124.230000. running mean: -99.311743\n",
      "ep 74: ep_len:504 episode reward: total was -47.330000. running mean: -98.791925\n",
      "ep 74: ep_len:3 episode reward: total was -1.500000. running mean: -97.819006\n",
      "ep 74: ep_len:518 episode reward: total was -132.220000. running mean: -98.163016\n",
      "ep 74: ep_len:625 episode reward: total was -179.170000. running mean: -98.973086\n",
      "epsilon:0.196675 episode_count: 525. steps_count: 233052.000000\n",
      "Time elapsed:  643.7830729484558\n",
      "ep 75: ep_len:525 episode reward: total was -75.280000. running mean: -98.736155\n",
      "ep 75: ep_len:500 episode reward: total was -52.350000. running mean: -98.272294\n",
      "ep 75: ep_len:641 episode reward: total was -139.260000. running mean: -98.682171\n",
      "ep 75: ep_len:517 episode reward: total was -124.570000. running mean: -98.941049\n",
      "ep 75: ep_len:107 episode reward: total was 14.230000. running mean: -97.809338\n",
      "ep 75: ep_len:602 episode reward: total was -146.700000. running mean: -98.298245\n",
      "ep 75: ep_len:527 episode reward: total was -139.630000. running mean: -98.711563\n",
      "epsilon:0.196631 episode_count: 532. steps_count: 236471.000000\n",
      "Time elapsed:  652.6507771015167\n",
      "ep 76: ep_len:627 episode reward: total was -136.990000. running mean: -99.094347\n",
      "ep 76: ep_len:500 episode reward: total was -74.650000. running mean: -98.849904\n",
      "ep 76: ep_len:504 episode reward: total was -203.320000. running mean: -99.894604\n",
      "ep 76: ep_len:594 episode reward: total was -86.270000. running mean: -99.758358\n",
      "ep 76: ep_len:3 episode reward: total was -1.500000. running mean: -98.775775\n",
      "ep 76: ep_len:530 episode reward: total was -98.660000. running mean: -98.774617\n",
      "ep 76: ep_len:345 episode reward: total was -92.100000. running mean: -98.707871\n",
      "epsilon:0.196586 episode_count: 539. steps_count: 239574.000000\n",
      "Time elapsed:  661.080732345581\n",
      "ep 77: ep_len:501 episode reward: total was -111.550000. running mean: -98.836292\n",
      "ep 77: ep_len:522 episode reward: total was -107.750000. running mean: -98.925429\n",
      "ep 77: ep_len:612 episode reward: total was -186.230000. running mean: -99.798475\n",
      "ep 77: ep_len:612 episode reward: total was -107.170000. running mean: -99.872190\n",
      "ep 77: ep_len:3 episode reward: total was 0.000000. running mean: -98.873468\n",
      "ep 77: ep_len:512 episode reward: total was -129.220000. running mean: -99.176934\n",
      "ep 77: ep_len:602 episode reward: total was -84.980000. running mean: -99.034964\n",
      "epsilon:0.196542 episode_count: 546. steps_count: 242938.000000\n",
      "Time elapsed:  670.0267086029053\n",
      "ep 78: ep_len:536 episode reward: total was -143.810000. running mean: -99.482715\n",
      "ep 78: ep_len:583 episode reward: total was -128.540000. running mean: -99.773288\n",
      "ep 78: ep_len:530 episode reward: total was -146.460000. running mean: -100.240155\n",
      "ep 78: ep_len:500 episode reward: total was -68.840000. running mean: -99.926153\n",
      "ep 78: ep_len:82 episode reward: total was -27.290000. running mean: -99.199792\n",
      "ep 78: ep_len:602 episode reward: total was -197.900000. running mean: -100.186794\n",
      "ep 78: ep_len:335 episode reward: total was -113.180000. running mean: -100.316726\n",
      "epsilon:0.196498 episode_count: 553. steps_count: 246106.000000\n",
      "Time elapsed:  678.4476764202118\n",
      "ep 79: ep_len:500 episode reward: total was -94.000000. running mean: -100.253558\n",
      "ep 79: ep_len:298 episode reward: total was -45.650000. running mean: -99.707523\n",
      "ep 79: ep_len:64 episode reward: total was -3.410000. running mean: -98.744548\n",
      "ep 79: ep_len:613 episode reward: total was -139.050000. running mean: -99.147602\n",
      "ep 79: ep_len:3 episode reward: total was 0.000000. running mean: -98.156126\n",
      "ep 79: ep_len:502 episode reward: total was -74.430000. running mean: -97.918865\n",
      "ep 79: ep_len:529 episode reward: total was -117.970000. running mean: -98.119376\n",
      "epsilon:0.196453 episode_count: 560. steps_count: 248615.000000\n",
      "Time elapsed:  685.3909261226654\n",
      "ep 80: ep_len:500 episode reward: total was -59.710000. running mean: -97.735282\n",
      "ep 80: ep_len:574 episode reward: total was -109.180000. running mean: -97.849730\n",
      "ep 80: ep_len:500 episode reward: total was -131.190000. running mean: -98.183132\n",
      "ep 80: ep_len:503 episode reward: total was -61.740000. running mean: -97.818701\n",
      "ep 80: ep_len:3 episode reward: total was -1.500000. running mean: -96.855514\n",
      "ep 80: ep_len:557 episode reward: total was -133.050000. running mean: -97.217459\n",
      "ep 80: ep_len:530 episode reward: total was -165.580000. running mean: -97.901084\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.196409 episode_count: 567. steps_count: 251782.000000\n",
      "Time elapsed:  698.6017401218414\n",
      "ep 81: ep_len:546 episode reward: total was -153.100000. running mean: -98.453073\n",
      "ep 81: ep_len:272 episode reward: total was -115.490000. running mean: -98.623443\n",
      "ep 81: ep_len:594 episode reward: total was -112.320000. running mean: -98.760408\n",
      "ep 81: ep_len:561 episode reward: total was -83.310000. running mean: -98.605904\n",
      "ep 81: ep_len:3 episode reward: total was 0.000000. running mean: -97.619845\n",
      "ep 81: ep_len:622 episode reward: total was -119.200000. running mean: -97.835647\n",
      "ep 81: ep_len:568 episode reward: total was -138.320000. running mean: -98.240490\n",
      "epsilon:0.196365 episode_count: 574. steps_count: 254948.000000\n",
      "Time elapsed:  707.2370178699493\n",
      "ep 82: ep_len:249 episode reward: total was -39.790000. running mean: -97.655985\n",
      "ep 82: ep_len:501 episode reward: total was -102.410000. running mean: -97.703525\n",
      "ep 82: ep_len:637 episode reward: total was -148.390000. running mean: -98.210390\n",
      "ep 82: ep_len:56 episode reward: total was -6.250000. running mean: -97.290786\n",
      "ep 82: ep_len:91 episode reward: total was -18.260000. running mean: -96.500478\n",
      "ep 82: ep_len:501 episode reward: total was -129.460000. running mean: -96.830074\n",
      "ep 82: ep_len:308 episode reward: total was -55.430000. running mean: -96.416073\n",
      "epsilon:0.196320 episode_count: 581. steps_count: 257291.000000\n",
      "Time elapsed:  713.784093618393\n",
      "ep 83: ep_len:580 episode reward: total was -149.800000. running mean: -96.949912\n",
      "ep 83: ep_len:575 episode reward: total was -107.880000. running mean: -97.059213\n",
      "ep 83: ep_len:500 episode reward: total was -123.420000. running mean: -97.322821\n",
      "ep 83: ep_len:524 episode reward: total was -44.850000. running mean: -96.798093\n",
      "ep 83: ep_len:3 episode reward: total was 0.000000. running mean: -95.830112\n",
      "ep 83: ep_len:500 episode reward: total was -51.750000. running mean: -95.389311\n",
      "ep 83: ep_len:605 episode reward: total was -164.890000. running mean: -96.084318\n",
      "epsilon:0.196276 episode_count: 588. steps_count: 260578.000000\n",
      "Time elapsed:  722.4490077495575\n",
      "ep 84: ep_len:506 episode reward: total was -158.170000. running mean: -96.705174\n",
      "ep 84: ep_len:543 episode reward: total was -30.850000. running mean: -96.046623\n",
      "ep 84: ep_len:593 episode reward: total was -101.590000. running mean: -96.102056\n",
      "ep 84: ep_len:507 episode reward: total was -84.630000. running mean: -95.987336\n",
      "ep 84: ep_len:86 episode reward: total was -2.300000. running mean: -95.050463\n",
      "ep 84: ep_len:585 episode reward: total was -136.690000. running mean: -95.466858\n",
      "ep 84: ep_len:628 episode reward: total was -153.650000. running mean: -96.048689\n",
      "epsilon:0.196232 episode_count: 595. steps_count: 264026.000000\n",
      "Time elapsed:  731.6736149787903\n",
      "ep 85: ep_len:265 episode reward: total was -47.000000. running mean: -95.558202\n",
      "ep 85: ep_len:500 episode reward: total was -121.150000. running mean: -95.814120\n",
      "ep 85: ep_len:557 episode reward: total was -222.020000. running mean: -97.076179\n",
      "ep 85: ep_len:500 episode reward: total was -46.140000. running mean: -96.566817\n",
      "ep 85: ep_len:3 episode reward: total was 0.000000. running mean: -95.601149\n",
      "ep 85: ep_len:166 episode reward: total was -13.610000. running mean: -94.781238\n",
      "ep 85: ep_len:580 episode reward: total was -180.080000. running mean: -95.634225\n",
      "epsilon:0.196187 episode_count: 602. steps_count: 266597.000000\n",
      "Time elapsed:  738.5988609790802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 86: ep_len:593 episode reward: total was -129.380000. running mean: -95.971683\n",
      "ep 86: ep_len:521 episode reward: total was -126.680000. running mean: -96.278766\n",
      "ep 86: ep_len:529 episode reward: total was -154.360000. running mean: -96.859579\n",
      "ep 86: ep_len:528 episode reward: total was -149.810000. running mean: -97.389083\n",
      "ep 86: ep_len:3 episode reward: total was 0.000000. running mean: -96.415192\n",
      "ep 86: ep_len:500 episode reward: total was -188.150000. running mean: -97.332540\n",
      "ep 86: ep_len:608 episode reward: total was -124.290000. running mean: -97.602115\n",
      "epsilon:0.196143 episode_count: 609. steps_count: 269879.000000\n",
      "Time elapsed:  747.18630194664\n",
      "ep 87: ep_len:620 episode reward: total was -123.220000. running mean: -97.858294\n",
      "ep 87: ep_len:588 episode reward: total was -131.430000. running mean: -98.194011\n",
      "ep 87: ep_len:571 episode reward: total was -91.960000. running mean: -98.131671\n",
      "ep 87: ep_len:591 episode reward: total was -153.390000. running mean: -98.684254\n",
      "ep 87: ep_len:3 episode reward: total was -1.500000. running mean: -97.712411\n",
      "ep 87: ep_len:509 episode reward: total was -121.650000. running mean: -97.951787\n",
      "ep 87: ep_len:576 episode reward: total was -50.540000. running mean: -97.477669\n",
      "epsilon:0.196099 episode_count: 616. steps_count: 273337.000000\n",
      "Time elapsed:  756.2082939147949\n",
      "ep 88: ep_len:500 episode reward: total was -88.850000. running mean: -97.391393\n",
      "ep 88: ep_len:518 episode reward: total was -96.990000. running mean: -97.387379\n",
      "ep 88: ep_len:585 episode reward: total was -165.990000. running mean: -98.073405\n",
      "ep 88: ep_len:500 episode reward: total was -59.190000. running mean: -97.684571\n",
      "ep 88: ep_len:3 episode reward: total was -1.500000. running mean: -96.722725\n",
      "ep 88: ep_len:562 episode reward: total was -105.000000. running mean: -96.805498\n",
      "ep 88: ep_len:500 episode reward: total was -110.190000. running mean: -96.939343\n",
      "epsilon:0.196054 episode_count: 623. steps_count: 276505.000000\n",
      "Time elapsed:  764.8980267047882\n",
      "ep 89: ep_len:500 episode reward: total was -87.230000. running mean: -96.842249\n",
      "ep 89: ep_len:511 episode reward: total was -27.540000. running mean: -96.149227\n",
      "ep 89: ep_len:645 episode reward: total was -137.540000. running mean: -96.563135\n",
      "ep 89: ep_len:500 episode reward: total was -132.570000. running mean: -96.923203\n",
      "ep 89: ep_len:3 episode reward: total was -1.500000. running mean: -95.968971\n",
      "ep 89: ep_len:683 episode reward: total was -118.570000. running mean: -96.194982\n",
      "ep 89: ep_len:202 episode reward: total was -69.900000. running mean: -95.932032\n",
      "epsilon:0.196010 episode_count: 630. steps_count: 279549.000000\n",
      "Time elapsed:  773.053542137146\n",
      "ep 90: ep_len:543 episode reward: total was -53.360000. running mean: -95.506311\n",
      "ep 90: ep_len:563 episode reward: total was 21.900000. running mean: -94.332248\n",
      "ep 90: ep_len:658 episode reward: total was -158.630000. running mean: -94.975226\n",
      "ep 90: ep_len:548 episode reward: total was -119.400000. running mean: -95.219474\n",
      "ep 90: ep_len:3 episode reward: total was -1.500000. running mean: -94.282279\n",
      "ep 90: ep_len:533 episode reward: total was -103.040000. running mean: -94.369856\n",
      "ep 90: ep_len:560 episode reward: total was -179.630000. running mean: -95.222458\n",
      "epsilon:0.195966 episode_count: 637. steps_count: 282957.000000\n",
      "Time elapsed:  782.1146006584167\n",
      "ep 91: ep_len:182 episode reward: total was -5.070000. running mean: -94.320933\n",
      "ep 91: ep_len:582 episode reward: total was -59.590000. running mean: -93.973624\n",
      "ep 91: ep_len:397 episode reward: total was -49.630000. running mean: -93.530187\n",
      "ep 91: ep_len:565 episode reward: total was -118.320000. running mean: -93.778086\n",
      "ep 91: ep_len:3 episode reward: total was 0.000000. running mean: -92.840305\n",
      "ep 91: ep_len:500 episode reward: total was -82.140000. running mean: -92.733302\n",
      "ep 91: ep_len:538 episode reward: total was -157.960000. running mean: -93.385569\n",
      "epsilon:0.195921 episode_count: 644. steps_count: 285724.000000\n",
      "Time elapsed:  789.6590650081635\n",
      "ep 92: ep_len:501 episode reward: total was -198.230000. running mean: -94.434013\n",
      "ep 92: ep_len:500 episode reward: total was -19.040000. running mean: -93.680073\n",
      "ep 92: ep_len:573 episode reward: total was -168.990000. running mean: -94.433172\n",
      "ep 92: ep_len:502 episode reward: total was -152.440000. running mean: -95.013240\n",
      "ep 92: ep_len:83 episode reward: total was -4.260000. running mean: -94.105708\n",
      "ep 92: ep_len:519 episode reward: total was -42.680000. running mean: -93.591451\n",
      "ep 92: ep_len:555 episode reward: total was -62.860000. running mean: -93.284136\n",
      "epsilon:0.195877 episode_count: 651. steps_count: 288957.000000\n",
      "Time elapsed:  798.4582722187042\n",
      "ep 93: ep_len:620 episode reward: total was -113.440000. running mean: -93.485695\n",
      "ep 93: ep_len:379 episode reward: total was -148.470000. running mean: -94.035538\n",
      "ep 93: ep_len:601 episode reward: total was -115.760000. running mean: -94.252783\n",
      "ep 93: ep_len:500 episode reward: total was -30.970000. running mean: -93.619955\n",
      "ep 93: ep_len:3 episode reward: total was -1.500000. running mean: -92.698755\n",
      "ep 93: ep_len:569 episode reward: total was -165.370000. running mean: -93.425468\n",
      "ep 93: ep_len:577 episode reward: total was -80.610000. running mean: -93.297313\n",
      "epsilon:0.195833 episode_count: 658. steps_count: 292206.000000\n",
      "Time elapsed:  807.1648032665253\n",
      "ep 94: ep_len:506 episode reward: total was -115.250000. running mean: -93.516840\n",
      "ep 94: ep_len:500 episode reward: total was -121.400000. running mean: -93.795672\n",
      "ep 94: ep_len:389 episode reward: total was -35.960000. running mean: -93.217315\n",
      "ep 94: ep_len:579 episode reward: total was -44.610000. running mean: -92.731242\n",
      "ep 94: ep_len:89 episode reward: total was 7.220000. running mean: -91.731729\n",
      "ep 94: ep_len:500 episode reward: total was -97.540000. running mean: -91.789812\n",
      "ep 94: ep_len:511 episode reward: total was -112.590000. running mean: -91.997814\n",
      "epsilon:0.195788 episode_count: 665. steps_count: 295280.000000\n",
      "Time elapsed:  815.5281364917755\n",
      "ep 95: ep_len:501 episode reward: total was -93.480000. running mean: -92.012636\n",
      "ep 95: ep_len:604 episode reward: total was -98.030000. running mean: -92.072809\n",
      "ep 95: ep_len:370 episode reward: total was -50.420000. running mean: -91.656281\n",
      "ep 95: ep_len:500 episode reward: total was -49.940000. running mean: -91.239118\n",
      "ep 95: ep_len:117 episode reward: total was -23.260000. running mean: -90.559327\n",
      "ep 95: ep_len:515 episode reward: total was -152.940000. running mean: -91.183134\n",
      "ep 95: ep_len:313 episode reward: total was -93.530000. running mean: -91.206603\n",
      "epsilon:0.195744 episode_count: 672. steps_count: 298200.000000\n",
      "Time elapsed:  823.5376446247101\n",
      "ep 96: ep_len:233 episode reward: total was -26.320000. running mean: -90.557737\n",
      "ep 96: ep_len:577 episode reward: total was -48.280000. running mean: -90.134959\n",
      "ep 96: ep_len:841 episode reward: total was -270.910000. running mean: -91.942710\n",
      "ep 96: ep_len:500 episode reward: total was -115.790000. running mean: -92.181183\n",
      "ep 96: ep_len:93 episode reward: total was 7.260000. running mean: -91.186771\n",
      "ep 96: ep_len:537 episode reward: total was -124.830000. running mean: -91.523203\n",
      "ep 96: ep_len:575 episode reward: total was -99.920000. running mean: -91.607171\n",
      "epsilon:0.195700 episode_count: 679. steps_count: 301556.000000\n",
      "Time elapsed:  832.5309300422668\n",
      "ep 97: ep_len:656 episode reward: total was -127.360000. running mean: -91.964699\n",
      "ep 97: ep_len:500 episode reward: total was -23.930000. running mean: -91.284352\n",
      "ep 97: ep_len:637 episode reward: total was -162.600000. running mean: -91.997509\n",
      "ep 97: ep_len:587 episode reward: total was -158.600000. running mean: -92.663534\n",
      "ep 97: ep_len:55 episode reward: total was 14.000000. running mean: -91.596898\n",
      "ep 97: ep_len:579 episode reward: total was -65.620000. running mean: -91.337129\n",
      "ep 97: ep_len:530 episode reward: total was -132.140000. running mean: -91.745158\n",
      "epsilon:0.195655 episode_count: 686. steps_count: 305100.000000\n",
      "Time elapsed:  841.708987236023\n",
      "ep 98: ep_len:592 episode reward: total was -196.700000. running mean: -92.794706\n",
      "ep 98: ep_len:608 episode reward: total was -143.370000. running mean: -93.300459\n",
      "ep 98: ep_len:529 episode reward: total was -202.670000. running mean: -94.394155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 98: ep_len:542 episode reward: total was -159.640000. running mean: -95.046613\n",
      "ep 98: ep_len:3 episode reward: total was -1.500000. running mean: -94.111147\n",
      "ep 98: ep_len:611 episode reward: total was -133.710000. running mean: -94.507136\n",
      "ep 98: ep_len:591 episode reward: total was -81.090000. running mean: -94.372964\n",
      "epsilon:0.195611 episode_count: 693. steps_count: 308576.000000\n",
      "Time elapsed:  850.8335959911346\n",
      "ep 99: ep_len:642 episode reward: total was -114.740000. running mean: -94.576635\n",
      "ep 99: ep_len:532 episode reward: total was -149.930000. running mean: -95.130168\n",
      "ep 99: ep_len:500 episode reward: total was -135.310000. running mean: -95.531967\n",
      "ep 99: ep_len:541 episode reward: total was -51.800000. running mean: -95.094647\n",
      "ep 99: ep_len:96 episode reward: total was -10.800000. running mean: -94.251700\n",
      "ep 99: ep_len:229 episode reward: total was -17.680000. running mean: -93.485983\n",
      "ep 99: ep_len:501 episode reward: total was -94.160000. running mean: -93.492724\n",
      "epsilon:0.195567 episode_count: 700. steps_count: 311617.000000\n",
      "Time elapsed:  858.8226625919342\n",
      "ep 100: ep_len:517 episode reward: total was -140.050000. running mean: -93.958296\n",
      "ep 100: ep_len:644 episode reward: total was -50.390000. running mean: -93.522613\n",
      "ep 100: ep_len:515 episode reward: total was -105.220000. running mean: -93.639587\n",
      "ep 100: ep_len:500 episode reward: total was -46.210000. running mean: -93.165291\n",
      "ep 100: ep_len:3 episode reward: total was 0.000000. running mean: -92.233639\n",
      "ep 100: ep_len:540 episode reward: total was -132.800000. running mean: -92.639302\n",
      "ep 100: ep_len:554 episode reward: total was -178.140000. running mean: -93.494309\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.195522 episode_count: 707. steps_count: 314890.000000\n",
      "Time elapsed:  872.5619685649872\n",
      "ep 101: ep_len:608 episode reward: total was -154.350000. running mean: -94.102866\n",
      "ep 101: ep_len:607 episode reward: total was -93.720000. running mean: -94.099037\n",
      "ep 101: ep_len:805 episode reward: total was -249.020000. running mean: -95.648247\n",
      "ep 101: ep_len:534 episode reward: total was -119.700000. running mean: -95.888765\n",
      "ep 101: ep_len:3 episode reward: total was 0.000000. running mean: -94.929877\n",
      "ep 101: ep_len:611 episode reward: total was -115.550000. running mean: -95.136078\n",
      "ep 101: ep_len:500 episode reward: total was -78.720000. running mean: -94.971917\n",
      "epsilon:0.195478 episode_count: 714. steps_count: 318558.000000\n",
      "Time elapsed:  882.2060821056366\n",
      "ep 102: ep_len:500 episode reward: total was -94.140000. running mean: -94.963598\n",
      "ep 102: ep_len:500 episode reward: total was -93.030000. running mean: -94.944262\n",
      "ep 102: ep_len:537 episode reward: total was -161.130000. running mean: -95.606120\n",
      "ep 102: ep_len:153 episode reward: total was -8.070000. running mean: -94.730758\n",
      "ep 102: ep_len:96 episode reward: total was -14.290000. running mean: -93.926351\n",
      "ep 102: ep_len:545 episode reward: total was -138.320000. running mean: -94.370287\n",
      "ep 102: ep_len:594 episode reward: total was -128.520000. running mean: -94.711784\n",
      "epsilon:0.195434 episode_count: 721. steps_count: 321483.000000\n",
      "Time elapsed:  890.082222700119\n",
      "ep 103: ep_len:606 episode reward: total was -86.800000. running mean: -94.632667\n",
      "ep 103: ep_len:500 episode reward: total was -90.030000. running mean: -94.586640\n",
      "ep 103: ep_len:449 episode reward: total was -20.070000. running mean: -93.841473\n",
      "ep 103: ep_len:571 episode reward: total was -74.630000. running mean: -93.649359\n",
      "ep 103: ep_len:85 episode reward: total was 3.720000. running mean: -92.675665\n",
      "ep 103: ep_len:595 episode reward: total was -173.210000. running mean: -93.481009\n",
      "ep 103: ep_len:576 episode reward: total was -147.990000. running mean: -94.026098\n",
      "epsilon:0.195389 episode_count: 728. steps_count: 324865.000000\n",
      "Time elapsed:  899.129885673523\n",
      "ep 104: ep_len:126 episode reward: total was -27.050000. running mean: -93.356337\n",
      "ep 104: ep_len:371 episode reward: total was -150.140000. running mean: -93.924174\n",
      "ep 104: ep_len:500 episode reward: total was -68.610000. running mean: -93.671032\n",
      "ep 104: ep_len:600 episode reward: total was -73.290000. running mean: -93.467222\n",
      "ep 104: ep_len:3 episode reward: total was 0.000000. running mean: -92.532550\n",
      "ep 104: ep_len:573 episode reward: total was -179.000000. running mean: -93.397224\n",
      "ep 104: ep_len:500 episode reward: total was -74.290000. running mean: -93.206152\n",
      "epsilon:0.195345 episode_count: 735. steps_count: 327538.000000\n",
      "Time elapsed:  906.3425648212433\n",
      "ep 105: ep_len:501 episode reward: total was -89.600000. running mean: -93.170091\n",
      "ep 105: ep_len:500 episode reward: total was -38.070000. running mean: -92.619090\n",
      "ep 105: ep_len:79 episode reward: total was -9.780000. running mean: -91.790699\n",
      "ep 105: ep_len:512 episode reward: total was -154.850000. running mean: -92.421292\n",
      "ep 105: ep_len:133 episode reward: total was 7.300000. running mean: -91.424079\n",
      "ep 105: ep_len:540 episode reward: total was -81.550000. running mean: -91.325338\n",
      "ep 105: ep_len:525 episode reward: total was -137.360000. running mean: -91.785685\n",
      "epsilon:0.195301 episode_count: 742. steps_count: 330328.000000\n",
      "Time elapsed:  914.15584897995\n",
      "ep 106: ep_len:582 episode reward: total was -101.190000. running mean: -91.879728\n",
      "ep 106: ep_len:575 episode reward: total was -68.440000. running mean: -91.645331\n",
      "ep 106: ep_len:500 episode reward: total was -100.300000. running mean: -91.731877\n",
      "ep 106: ep_len:554 episode reward: total was -76.120000. running mean: -91.575758\n",
      "ep 106: ep_len:3 episode reward: total was 0.000000. running mean: -90.660001\n",
      "ep 106: ep_len:534 episode reward: total was -119.430000. running mean: -90.947701\n",
      "ep 106: ep_len:523 episode reward: total was -143.470000. running mean: -91.472924\n",
      "epsilon:0.195256 episode_count: 749. steps_count: 333599.000000\n",
      "Time elapsed:  922.8436579704285\n",
      "ep 107: ep_len:624 episode reward: total was -195.050000. running mean: -92.508695\n",
      "ep 107: ep_len:500 episode reward: total was -67.740000. running mean: -92.261008\n",
      "ep 107: ep_len:616 episode reward: total was -145.420000. running mean: -92.792598\n",
      "ep 107: ep_len:589 episode reward: total was -72.080000. running mean: -92.585472\n",
      "ep 107: ep_len:43 episode reward: total was 9.010000. running mean: -91.569517\n",
      "ep 107: ep_len:518 episode reward: total was -94.830000. running mean: -91.602122\n",
      "ep 107: ep_len:309 episode reward: total was -117.500000. running mean: -91.861101\n",
      "epsilon:0.195212 episode_count: 756. steps_count: 336798.000000\n",
      "Time elapsed:  931.3340749740601\n",
      "ep 108: ep_len:572 episode reward: total was -106.980000. running mean: -92.012290\n",
      "ep 108: ep_len:509 episode reward: total was -189.290000. running mean: -92.985067\n",
      "ep 108: ep_len:1019 episode reward: total was -488.040000. running mean: -96.935616\n",
      "ep 108: ep_len:565 episode reward: total was -76.070000. running mean: -96.726960\n",
      "ep 108: ep_len:3 episode reward: total was -3.000000. running mean: -95.789690\n",
      "ep 108: ep_len:533 episode reward: total was -67.960000. running mean: -95.511393\n",
      "ep 108: ep_len:326 episode reward: total was -111.770000. running mean: -95.673979\n",
      "epsilon:0.195168 episode_count: 763. steps_count: 340325.000000\n",
      "Time elapsed:  940.5862226486206\n",
      "ep 109: ep_len:574 episode reward: total was -142.410000. running mean: -96.141340\n",
      "ep 109: ep_len:500 episode reward: total was -27.710000. running mean: -95.457026\n",
      "ep 109: ep_len:553 episode reward: total was -114.890000. running mean: -95.651356\n",
      "ep 109: ep_len:503 episode reward: total was -105.750000. running mean: -95.752342\n",
      "ep 109: ep_len:3 episode reward: total was -1.500000. running mean: -94.809819\n",
      "ep 109: ep_len:603 episode reward: total was -135.820000. running mean: -95.219921\n",
      "ep 109: ep_len:336 episode reward: total was -73.710000. running mean: -95.004822\n",
      "epsilon:0.195123 episode_count: 770. steps_count: 343397.000000\n",
      "Time elapsed:  948.8323698043823\n",
      "ep 110: ep_len:500 episode reward: total was -71.130000. running mean: -94.766073\n",
      "ep 110: ep_len:502 episode reward: total was -127.780000. running mean: -95.096213\n",
      "ep 110: ep_len:612 episode reward: total was -185.080000. running mean: -95.996050\n",
      "ep 110: ep_len:520 episode reward: total was -119.630000. running mean: -96.232390\n",
      "ep 110: ep_len:43 episode reward: total was 6.500000. running mean: -95.205066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 110: ep_len:512 episode reward: total was -94.890000. running mean: -95.201915\n",
      "ep 110: ep_len:583 episode reward: total was -183.530000. running mean: -96.085196\n",
      "epsilon:0.195079 episode_count: 777. steps_count: 346669.000000\n",
      "Time elapsed:  957.3871779441833\n",
      "ep 111: ep_len:220 episode reward: total was -60.690000. running mean: -95.731244\n",
      "ep 111: ep_len:186 episode reward: total was -35.690000. running mean: -95.130832\n",
      "ep 111: ep_len:552 episode reward: total was -98.370000. running mean: -95.163224\n",
      "ep 111: ep_len:500 episode reward: total was -30.310000. running mean: -94.514691\n",
      "ep 111: ep_len:100 episode reward: total was -33.180000. running mean: -93.901344\n",
      "ep 111: ep_len:539 episode reward: total was -102.230000. running mean: -93.984631\n",
      "ep 111: ep_len:580 episode reward: total was -111.450000. running mean: -94.159285\n",
      "epsilon:0.195035 episode_count: 784. steps_count: 349346.000000\n",
      "Time elapsed:  964.9386467933655\n",
      "ep 112: ep_len:627 episode reward: total was -166.590000. running mean: -94.883592\n",
      "ep 112: ep_len:292 episode reward: total was -64.970000. running mean: -94.584456\n",
      "ep 112: ep_len:549 episode reward: total was -127.710000. running mean: -94.915711\n",
      "ep 112: ep_len:500 episode reward: total was -56.900000. running mean: -94.535554\n",
      "ep 112: ep_len:3 episode reward: total was 0.000000. running mean: -93.590199\n",
      "ep 112: ep_len:606 episode reward: total was -154.450000. running mean: -94.198797\n",
      "ep 112: ep_len:529 episode reward: total was -72.670000. running mean: -93.983509\n",
      "epsilon:0.194990 episode_count: 791. steps_count: 352452.000000\n",
      "Time elapsed:  973.434987783432\n",
      "ep 113: ep_len:548 episode reward: total was -68.140000. running mean: -93.725074\n",
      "ep 113: ep_len:500 episode reward: total was -83.840000. running mean: -93.626223\n",
      "ep 113: ep_len:551 episode reward: total was -118.460000. running mean: -93.874561\n",
      "ep 113: ep_len:507 episode reward: total was -136.380000. running mean: -94.299615\n",
      "ep 113: ep_len:93 episode reward: total was -26.310000. running mean: -93.619719\n",
      "ep 113: ep_len:737 episode reward: total was -203.670000. running mean: -94.720222\n",
      "ep 113: ep_len:607 episode reward: total was -92.360000. running mean: -94.696619\n",
      "epsilon:0.194946 episode_count: 798. steps_count: 355995.000000\n",
      "Time elapsed:  982.777651309967\n",
      "ep 114: ep_len:628 episode reward: total was -77.330000. running mean: -94.522953\n",
      "ep 114: ep_len:500 episode reward: total was -92.160000. running mean: -94.499324\n",
      "ep 114: ep_len:500 episode reward: total was -81.550000. running mean: -94.369830\n",
      "ep 114: ep_len:591 episode reward: total was -85.530000. running mean: -94.281432\n",
      "ep 114: ep_len:77 episode reward: total was -37.840000. running mean: -93.717018\n",
      "ep 114: ep_len:552 episode reward: total was -72.530000. running mean: -93.505148\n",
      "ep 114: ep_len:553 episode reward: total was -97.490000. running mean: -93.544996\n",
      "epsilon:0.194902 episode_count: 805. steps_count: 359396.000000\n",
      "Time elapsed:  991.946935415268\n",
      "ep 115: ep_len:121 episode reward: total was -5.470000. running mean: -92.664246\n",
      "ep 115: ep_len:515 episode reward: total was -28.510000. running mean: -92.022704\n",
      "ep 115: ep_len:550 episode reward: total was -85.390000. running mean: -91.956377\n",
      "ep 115: ep_len:610 episode reward: total was -112.120000. running mean: -92.158013\n",
      "ep 115: ep_len:3 episode reward: total was 0.000000. running mean: -91.236433\n",
      "ep 115: ep_len:593 episode reward: total was -140.440000. running mean: -91.728469\n",
      "ep 115: ep_len:630 episode reward: total was -100.450000. running mean: -91.815684\n",
      "epsilon:0.194857 episode_count: 812. steps_count: 362418.000000\n",
      "Time elapsed:  1000.2103500366211\n",
      "ep 116: ep_len:507 episode reward: total was -123.400000. running mean: -92.131527\n",
      "ep 116: ep_len:590 episode reward: total was -95.720000. running mean: -92.167412\n",
      "ep 116: ep_len:767 episode reward: total was -239.610000. running mean: -93.641838\n",
      "ep 116: ep_len:505 episode reward: total was -71.040000. running mean: -93.415819\n",
      "ep 116: ep_len:99 episode reward: total was -31.310000. running mean: -92.794761\n",
      "ep 116: ep_len:591 episode reward: total was -178.960000. running mean: -93.656413\n",
      "ep 116: ep_len:609 episode reward: total was -137.880000. running mean: -94.098649\n",
      "epsilon:0.194813 episode_count: 819. steps_count: 366086.000000\n",
      "Time elapsed:  1009.838928937912\n",
      "ep 117: ep_len:561 episode reward: total was -132.930000. running mean: -94.486963\n",
      "ep 117: ep_len:500 episode reward: total was 23.360000. running mean: -93.308493\n",
      "ep 117: ep_len:500 episode reward: total was -51.950000. running mean: -92.894908\n",
      "ep 117: ep_len:578 episode reward: total was -70.180000. running mean: -92.667759\n",
      "ep 117: ep_len:3 episode reward: total was 1.010000. running mean: -91.730982\n",
      "ep 117: ep_len:616 episode reward: total was -104.850000. running mean: -91.862172\n",
      "ep 117: ep_len:573 episode reward: total was -149.160000. running mean: -92.435150\n",
      "epsilon:0.194769 episode_count: 826. steps_count: 369417.000000\n",
      "Time elapsed:  1018.8424491882324\n",
      "ep 118: ep_len:624 episode reward: total was -155.520000. running mean: -93.065999\n",
      "ep 118: ep_len:526 episode reward: total was -190.470000. running mean: -94.040039\n",
      "ep 118: ep_len:583 episode reward: total was -136.020000. running mean: -94.459838\n",
      "ep 118: ep_len:563 episode reward: total was -80.440000. running mean: -94.319640\n",
      "ep 118: ep_len:3 episode reward: total was 0.000000. running mean: -93.376443\n",
      "ep 118: ep_len:506 episode reward: total was -77.550000. running mean: -93.218179\n",
      "ep 118: ep_len:264 episode reward: total was -102.950000. running mean: -93.315497\n",
      "epsilon:0.194724 episode_count: 833. steps_count: 372486.000000\n",
      "Time elapsed:  1027.342405796051\n",
      "ep 119: ep_len:162 episode reward: total was -5.130000. running mean: -92.433642\n",
      "ep 119: ep_len:687 episode reward: total was -135.930000. running mean: -92.868606\n",
      "ep 119: ep_len:593 episode reward: total was -115.670000. running mean: -93.096620\n",
      "ep 119: ep_len:515 episode reward: total was -122.340000. running mean: -93.389054\n",
      "ep 119: ep_len:94 episode reward: total was -12.290000. running mean: -92.578063\n",
      "ep 119: ep_len:502 episode reward: total was -93.070000. running mean: -92.582982\n",
      "ep 119: ep_len:315 episode reward: total was -102.540000. running mean: -92.682553\n",
      "epsilon:0.194680 episode_count: 840. steps_count: 375354.000000\n",
      "Time elapsed:  1035.021425485611\n",
      "ep 120: ep_len:500 episode reward: total was -124.330000. running mean: -92.999027\n",
      "ep 120: ep_len:500 episode reward: total was -26.400000. running mean: -92.333037\n",
      "ep 120: ep_len:625 episode reward: total was -134.840000. running mean: -92.758106\n",
      "ep 120: ep_len:600 episode reward: total was -68.890000. running mean: -92.519425\n",
      "ep 120: ep_len:3 episode reward: total was 0.000000. running mean: -91.594231\n",
      "ep 120: ep_len:567 episode reward: total was -92.250000. running mean: -91.600789\n",
      "ep 120: ep_len:589 episode reward: total was -109.210000. running mean: -91.776881\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.194636 episode_count: 847. steps_count: 378738.000000\n",
      "Time elapsed:  1048.8970742225647\n",
      "ep 121: ep_len:235 episode reward: total was -7.790000. running mean: -90.937012\n",
      "ep 121: ep_len:565 episode reward: total was -89.860000. running mean: -90.926242\n",
      "ep 121: ep_len:654 episode reward: total was -108.410000. running mean: -91.101080\n",
      "ep 121: ep_len:552 episode reward: total was -125.760000. running mean: -91.447669\n",
      "ep 121: ep_len:91 episode reward: total was -17.280000. running mean: -90.705992\n",
      "ep 121: ep_len:648 episode reward: total was -95.060000. running mean: -90.749532\n",
      "ep 121: ep_len:577 episode reward: total was -107.890000. running mean: -90.920937\n",
      "epsilon:0.194591 episode_count: 854. steps_count: 382060.000000\n",
      "Time elapsed:  1057.9485166072845\n",
      "ep 122: ep_len:108 episode reward: total was -14.050000. running mean: -90.152227\n",
      "ep 122: ep_len:500 episode reward: total was -53.600000. running mean: -89.786705\n",
      "ep 122: ep_len:380 episode reward: total was -60.110000. running mean: -89.489938\n",
      "ep 122: ep_len:565 episode reward: total was -63.450000. running mean: -89.229539\n",
      "ep 122: ep_len:3 episode reward: total was 0.000000. running mean: -88.337243\n",
      "ep 122: ep_len:563 episode reward: total was -131.660000. running mean: -88.770471\n",
      "ep 122: ep_len:590 episode reward: total was -75.530000. running mean: -88.638066\n",
      "epsilon:0.194547 episode_count: 861. steps_count: 384769.000000\n",
      "Time elapsed:  1065.2184643745422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 123: ep_len:593 episode reward: total was -61.400000. running mean: -88.365686\n",
      "ep 123: ep_len:500 episode reward: total was -56.760000. running mean: -88.049629\n",
      "ep 123: ep_len:500 episode reward: total was -115.750000. running mean: -88.326632\n",
      "ep 123: ep_len:515 episode reward: total was -66.450000. running mean: -88.107866\n",
      "ep 123: ep_len:3 episode reward: total was 0.000000. running mean: -87.226787\n",
      "ep 123: ep_len:522 episode reward: total was -45.960000. running mean: -86.814120\n",
      "ep 123: ep_len:578 episode reward: total was -121.560000. running mean: -87.161578\n",
      "epsilon:0.194503 episode_count: 868. steps_count: 387980.000000\n",
      "Time elapsed:  1073.7783229351044\n",
      "ep 124: ep_len:643 episode reward: total was -105.030000. running mean: -87.340263\n",
      "ep 124: ep_len:500 episode reward: total was -2.900000. running mean: -86.495860\n",
      "ep 124: ep_len:544 episode reward: total was -70.090000. running mean: -86.331801\n",
      "ep 124: ep_len:500 episode reward: total was -37.530000. running mean: -85.843783\n",
      "ep 124: ep_len:98 episode reward: total was 5.750000. running mean: -84.927845\n",
      "ep 124: ep_len:543 episode reward: total was -142.580000. running mean: -85.504367\n",
      "ep 124: ep_len:288 episode reward: total was -126.950000. running mean: -85.918823\n",
      "epsilon:0.194458 episode_count: 875. steps_count: 391096.000000\n",
      "Time elapsed:  1082.1597893238068\n",
      "ep 125: ep_len:655 episode reward: total was -163.920000. running mean: -86.698835\n",
      "ep 125: ep_len:577 episode reward: total was -125.310000. running mean: -87.084947\n",
      "ep 125: ep_len:411 episode reward: total was -70.870000. running mean: -86.922797\n",
      "ep 125: ep_len:500 episode reward: total was -119.560000. running mean: -87.249169\n",
      "ep 125: ep_len:3 episode reward: total was -1.500000. running mean: -86.391678\n",
      "ep 125: ep_len:579 episode reward: total was -91.270000. running mean: -86.440461\n",
      "ep 125: ep_len:565 episode reward: total was -70.630000. running mean: -86.282356\n",
      "epsilon:0.194414 episode_count: 882. steps_count: 394386.000000\n",
      "Time elapsed:  1090.9114952087402\n",
      "ep 126: ep_len:233 episode reward: total was -31.710000. running mean: -85.736633\n",
      "ep 126: ep_len:633 episode reward: total was -66.180000. running mean: -85.541066\n",
      "ep 126: ep_len:626 episode reward: total was -137.050000. running mean: -86.056156\n",
      "ep 126: ep_len:96 episode reward: total was -8.690000. running mean: -85.282494\n",
      "ep 126: ep_len:3 episode reward: total was 0.000000. running mean: -84.429669\n",
      "ep 126: ep_len:565 episode reward: total was -97.030000. running mean: -84.555672\n",
      "ep 126: ep_len:553 episode reward: total was -91.000000. running mean: -84.620116\n",
      "epsilon:0.194370 episode_count: 889. steps_count: 397095.000000\n",
      "Time elapsed:  1098.2683131694794\n",
      "ep 127: ep_len:258 episode reward: total was -47.680000. running mean: -84.250715\n",
      "ep 127: ep_len:516 episode reward: total was -42.720000. running mean: -83.835407\n",
      "ep 127: ep_len:590 episode reward: total was -138.840000. running mean: -84.385453\n",
      "ep 127: ep_len:500 episode reward: total was -116.380000. running mean: -84.705399\n",
      "ep 127: ep_len:3 episode reward: total was 0.000000. running mean: -83.858345\n",
      "ep 127: ep_len:500 episode reward: total was -103.690000. running mean: -84.056661\n",
      "ep 127: ep_len:500 episode reward: total was -110.750000. running mean: -84.323595\n",
      "epsilon:0.194325 episode_count: 896. steps_count: 399962.000000\n",
      "Time elapsed:  1105.9315712451935\n",
      "ep 128: ep_len:254 episode reward: total was 0.210000. running mean: -83.478259\n",
      "ep 128: ep_len:757 episode reward: total was -185.350000. running mean: -84.496976\n",
      "ep 128: ep_len:683 episode reward: total was -136.170000. running mean: -85.013707\n",
      "ep 128: ep_len:148 episode reward: total was -39.510000. running mean: -84.558669\n",
      "ep 128: ep_len:3 episode reward: total was -1.500000. running mean: -83.728083\n",
      "ep 128: ep_len:591 episode reward: total was -124.410000. running mean: -84.134902\n",
      "ep 128: ep_len:311 episode reward: total was -127.690000. running mean: -84.570453\n",
      "epsilon:0.194281 episode_count: 903. steps_count: 402709.000000\n",
      "Time elapsed:  1113.1897196769714\n",
      "ep 129: ep_len:229 episode reward: total was -42.260000. running mean: -84.147348\n",
      "ep 129: ep_len:583 episode reward: total was -271.450000. running mean: -86.020375\n",
      "ep 129: ep_len:559 episode reward: total was -165.500000. running mean: -86.815171\n",
      "ep 129: ep_len:525 episode reward: total was -118.630000. running mean: -87.133319\n",
      "ep 129: ep_len:3 episode reward: total was 0.000000. running mean: -86.261986\n",
      "ep 129: ep_len:501 episode reward: total was -134.140000. running mean: -86.740766\n",
      "ep 129: ep_len:598 episode reward: total was -140.740000. running mean: -87.280759\n",
      "epsilon:0.194237 episode_count: 910. steps_count: 405707.000000\n",
      "Time elapsed:  1121.135815858841\n",
      "ep 130: ep_len:602 episode reward: total was -140.460000. running mean: -87.812551\n",
      "ep 130: ep_len:579 episode reward: total was -130.850000. running mean: -88.242926\n",
      "ep 130: ep_len:558 episode reward: total was -114.980000. running mean: -88.510296\n",
      "ep 130: ep_len:501 episode reward: total was -145.610000. running mean: -89.081293\n",
      "ep 130: ep_len:3 episode reward: total was 0.000000. running mean: -88.190480\n",
      "ep 130: ep_len:534 episode reward: total was -74.290000. running mean: -88.051476\n",
      "ep 130: ep_len:617 episode reward: total was -125.460000. running mean: -88.425561\n",
      "epsilon:0.194192 episode_count: 917. steps_count: 409101.000000\n",
      "Time elapsed:  1130.2383601665497\n",
      "ep 131: ep_len:584 episode reward: total was -69.140000. running mean: -88.232705\n",
      "ep 131: ep_len:616 episode reward: total was -129.980000. running mean: -88.650178\n",
      "ep 131: ep_len:500 episode reward: total was -82.810000. running mean: -88.591776\n",
      "ep 131: ep_len:535 episode reward: total was -71.990000. running mean: -88.425759\n",
      "ep 131: ep_len:3 episode reward: total was -1.500000. running mean: -87.556501\n",
      "ep 131: ep_len:585 episode reward: total was -142.440000. running mean: -88.105336\n",
      "ep 131: ep_len:328 episode reward: total was -77.230000. running mean: -87.996583\n",
      "epsilon:0.194148 episode_count: 924. steps_count: 412252.000000\n",
      "Time elapsed:  1138.7696723937988\n",
      "ep 132: ep_len:500 episode reward: total was -88.630000. running mean: -88.002917\n",
      "ep 132: ep_len:666 episode reward: total was -40.040000. running mean: -87.523288\n",
      "ep 132: ep_len:507 episode reward: total was -145.490000. running mean: -88.102955\n",
      "ep 132: ep_len:500 episode reward: total was -63.850000. running mean: -87.860425\n",
      "ep 132: ep_len:91 episode reward: total was -18.750000. running mean: -87.169321\n",
      "ep 132: ep_len:556 episode reward: total was -117.510000. running mean: -87.472728\n",
      "ep 132: ep_len:500 episode reward: total was -145.680000. running mean: -88.054801\n",
      "epsilon:0.194104 episode_count: 931. steps_count: 415572.000000\n",
      "Time elapsed:  1147.6095855236053\n",
      "ep 133: ep_len:578 episode reward: total was -118.370000. running mean: -88.357953\n",
      "ep 133: ep_len:639 episode reward: total was -84.030000. running mean: -88.314673\n",
      "ep 133: ep_len:647 episode reward: total was -182.280000. running mean: -89.254326\n",
      "ep 133: ep_len:605 episode reward: total was -63.500000. running mean: -88.996783\n",
      "ep 133: ep_len:3 episode reward: total was 0.000000. running mean: -88.106815\n",
      "ep 133: ep_len:582 episode reward: total was -109.060000. running mean: -88.316347\n",
      "ep 133: ep_len:187 episode reward: total was -47.280000. running mean: -87.905984\n",
      "epsilon:0.194059 episode_count: 938. steps_count: 418813.000000\n",
      "Time elapsed:  1156.1744141578674\n",
      "ep 134: ep_len:536 episode reward: total was -84.340000. running mean: -87.870324\n",
      "ep 134: ep_len:523 episode reward: total was -108.640000. running mean: -88.078021\n",
      "ep 134: ep_len:528 episode reward: total was -113.860000. running mean: -88.335840\n",
      "ep 134: ep_len:406 episode reward: total was -77.720000. running mean: -88.229682\n",
      "ep 134: ep_len:3 episode reward: total was -1.500000. running mean: -87.362385\n",
      "ep 134: ep_len:631 episode reward: total was -94.800000. running mean: -87.436761\n",
      "ep 134: ep_len:572 episode reward: total was -108.270000. running mean: -87.645094\n",
      "epsilon:0.194015 episode_count: 945. steps_count: 422012.000000\n",
      "Time elapsed:  1164.639066696167\n",
      "ep 135: ep_len:500 episode reward: total was -51.110000. running mean: -87.279743\n",
      "ep 135: ep_len:505 episode reward: total was -71.900000. running mean: -87.125945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 135: ep_len:500 episode reward: total was -99.490000. running mean: -87.249586\n",
      "ep 135: ep_len:621 episode reward: total was -47.730000. running mean: -86.854390\n",
      "ep 135: ep_len:3 episode reward: total was 0.000000. running mean: -85.985846\n",
      "ep 135: ep_len:657 episode reward: total was -153.480000. running mean: -86.660788\n",
      "ep 135: ep_len:509 episode reward: total was -70.610000. running mean: -86.500280\n",
      "epsilon:0.193971 episode_count: 952. steps_count: 425307.000000\n",
      "Time elapsed:  1173.0627739429474\n",
      "ep 136: ep_len:616 episode reward: total was -84.210000. running mean: -86.477377\n",
      "ep 136: ep_len:500 episode reward: total was -57.030000. running mean: -86.182903\n",
      "ep 136: ep_len:53 episode reward: total was -7.320000. running mean: -85.394274\n",
      "ep 136: ep_len:500 episode reward: total was -46.590000. running mean: -85.006231\n",
      "ep 136: ep_len:3 episode reward: total was 0.000000. running mean: -84.156169\n",
      "ep 136: ep_len:500 episode reward: total was -197.000000. running mean: -85.284607\n",
      "ep 136: ep_len:528 episode reward: total was -127.650000. running mean: -85.708261\n",
      "epsilon:0.193926 episode_count: 959. steps_count: 428007.000000\n",
      "Time elapsed:  1180.551117181778\n",
      "ep 137: ep_len:543 episode reward: total was -288.140000. running mean: -87.732579\n",
      "ep 137: ep_len:627 episode reward: total was -69.470000. running mean: -87.549953\n",
      "ep 137: ep_len:579 episode reward: total was -129.800000. running mean: -87.972453\n",
      "ep 137: ep_len:516 episode reward: total was -10.740000. running mean: -87.200129\n",
      "ep 137: ep_len:3 episode reward: total was 0.000000. running mean: -86.328128\n",
      "ep 137: ep_len:594 episode reward: total was -94.300000. running mean: -86.407846\n",
      "ep 137: ep_len:573 episode reward: total was -142.450000. running mean: -86.968268\n",
      "epsilon:0.193882 episode_count: 966. steps_count: 431442.000000\n",
      "Time elapsed:  1191.136269569397\n",
      "ep 138: ep_len:535 episode reward: total was -81.370000. running mean: -86.912285\n",
      "ep 138: ep_len:500 episode reward: total was -100.240000. running mean: -87.045562\n",
      "ep 138: ep_len:637 episode reward: total was -180.030000. running mean: -87.975407\n",
      "ep 138: ep_len:56 episode reward: total was -3.220000. running mean: -87.127853\n",
      "ep 138: ep_len:3 episode reward: total was 0.000000. running mean: -86.256574\n",
      "ep 138: ep_len:538 episode reward: total was -120.360000. running mean: -86.597608\n",
      "ep 138: ep_len:192 episode reward: total was -5.450000. running mean: -85.786132\n",
      "epsilon:0.193838 episode_count: 973. steps_count: 433903.000000\n",
      "Time elapsed:  1198.854323387146\n",
      "ep 139: ep_len:780 episode reward: total was -194.510000. running mean: -86.873371\n",
      "ep 139: ep_len:500 episode reward: total was -78.320000. running mean: -86.787837\n",
      "ep 139: ep_len:597 episode reward: total was -72.930000. running mean: -86.649259\n",
      "ep 139: ep_len:545 episode reward: total was -114.600000. running mean: -86.928766\n",
      "ep 139: ep_len:108 episode reward: total was -21.760000. running mean: -86.277079\n",
      "ep 139: ep_len:317 episode reward: total was -52.280000. running mean: -85.937108\n",
      "ep 139: ep_len:193 episode reward: total was -49.660000. running mean: -85.574337\n",
      "epsilon:0.193793 episode_count: 980. steps_count: 436943.000000\n",
      "Time elapsed:  1208.2959005832672\n",
      "ep 140: ep_len:501 episode reward: total was -75.750000. running mean: -85.476093\n",
      "ep 140: ep_len:500 episode reward: total was -14.650000. running mean: -84.767832\n",
      "ep 140: ep_len:418 episode reward: total was -40.960000. running mean: -84.329754\n",
      "ep 140: ep_len:500 episode reward: total was -55.300000. running mean: -84.039457\n",
      "ep 140: ep_len:3 episode reward: total was -1.500000. running mean: -83.214062\n",
      "ep 140: ep_len:582 episode reward: total was -120.030000. running mean: -83.582221\n",
      "ep 140: ep_len:193 episode reward: total was -70.910000. running mean: -83.455499\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.193749 episode_count: 987. steps_count: 439640.000000\n",
      "Time elapsed:  1221.732521533966\n",
      "ep 141: ep_len:502 episode reward: total was -219.350000. running mean: -84.814444\n",
      "ep 141: ep_len:633 episode reward: total was -76.590000. running mean: -84.732200\n",
      "ep 141: ep_len:583 episode reward: total was -96.150000. running mean: -84.846378\n",
      "ep 141: ep_len:587 episode reward: total was -86.210000. running mean: -84.860014\n",
      "ep 141: ep_len:3 episode reward: total was -1.500000. running mean: -84.026414\n",
      "ep 141: ep_len:536 episode reward: total was -92.840000. running mean: -84.114550\n",
      "ep 141: ep_len:510 episode reward: total was -153.000000. running mean: -84.803404\n",
      "epsilon:0.193705 episode_count: 994. steps_count: 442994.000000\n",
      "Time elapsed:  1231.751441001892\n",
      "ep 142: ep_len:595 episode reward: total was -67.100000. running mean: -84.626370\n",
      "ep 142: ep_len:171 episode reward: total was -40.760000. running mean: -84.187706\n",
      "ep 142: ep_len:396 episode reward: total was -25.360000. running mean: -83.599429\n",
      "ep 142: ep_len:587 episode reward: total was -142.650000. running mean: -84.189935\n",
      "ep 142: ep_len:3 episode reward: total was 0.000000. running mean: -83.348036\n",
      "ep 142: ep_len:533 episode reward: total was -91.050000. running mean: -83.425055\n",
      "ep 142: ep_len:562 episode reward: total was -187.020000. running mean: -84.461005\n",
      "epsilon:0.193660 episode_count: 1001. steps_count: 445841.000000\n",
      "Time elapsed:  1240.641756772995\n",
      "ep 143: ep_len:105 episode reward: total was -19.190000. running mean: -83.808295\n",
      "ep 143: ep_len:523 episode reward: total was -94.350000. running mean: -83.913712\n",
      "ep 143: ep_len:599 episode reward: total was -120.590000. running mean: -84.280475\n",
      "ep 143: ep_len:503 episode reward: total was -60.470000. running mean: -84.042370\n",
      "ep 143: ep_len:3 episode reward: total was -1.500000. running mean: -83.216946\n",
      "ep 143: ep_len:636 episode reward: total was -206.190000. running mean: -84.446677\n",
      "ep 143: ep_len:500 episode reward: total was -79.560000. running mean: -84.397810\n",
      "epsilon:0.193616 episode_count: 1008. steps_count: 448710.000000\n",
      "Time elapsed:  1249.4706809520721\n",
      "ep 144: ep_len:628 episode reward: total was -143.440000. running mean: -84.988232\n",
      "ep 144: ep_len:500 episode reward: total was -67.260000. running mean: -84.810950\n",
      "ep 144: ep_len:656 episode reward: total was -256.070000. running mean: -86.523540\n",
      "ep 144: ep_len:500 episode reward: total was -71.880000. running mean: -86.377105\n",
      "ep 144: ep_len:92 episode reward: total was -22.220000. running mean: -85.735534\n",
      "ep 144: ep_len:500 episode reward: total was -46.250000. running mean: -85.340678\n",
      "ep 144: ep_len:519 episode reward: total was -183.350000. running mean: -86.320772\n",
      "epsilon:0.193572 episode_count: 1015. steps_count: 452105.000000\n",
      "Time elapsed:  1259.9491322040558\n",
      "ep 145: ep_len:798 episode reward: total was -651.790000. running mean: -91.975464\n",
      "ep 145: ep_len:544 episode reward: total was -123.290000. running mean: -92.288609\n",
      "ep 145: ep_len:594 episode reward: total was -141.380000. running mean: -92.779523\n",
      "ep 145: ep_len:507 episode reward: total was -18.510000. running mean: -92.036828\n",
      "ep 145: ep_len:96 episode reward: total was -38.270000. running mean: -91.499160\n",
      "ep 145: ep_len:541 episode reward: total was -82.020000. running mean: -91.404368\n",
      "ep 145: ep_len:572 episode reward: total was -52.350000. running mean: -91.013824\n",
      "epsilon:0.193527 episode_count: 1022. steps_count: 455757.000000\n",
      "Time elapsed:  1270.6953241825104\n",
      "ep 146: ep_len:500 episode reward: total was -106.450000. running mean: -91.168186\n",
      "ep 146: ep_len:609 episode reward: total was -109.310000. running mean: -91.349604\n",
      "ep 146: ep_len:434 episode reward: total was -30.630000. running mean: -90.742408\n",
      "ep 146: ep_len:500 episode reward: total was -32.020000. running mean: -90.155184\n",
      "ep 146: ep_len:3 episode reward: total was -1.500000. running mean: -89.268632\n",
      "ep 146: ep_len:500 episode reward: total was -55.710000. running mean: -88.933046\n",
      "ep 146: ep_len:529 episode reward: total was -80.140000. running mean: -88.845115\n",
      "epsilon:0.193483 episode_count: 1029. steps_count: 458832.000000\n",
      "Time elapsed:  1279.9961276054382\n",
      "ep 147: ep_len:577 episode reward: total was -131.760000. running mean: -89.274264\n",
      "ep 147: ep_len:639 episode reward: total was -93.600000. running mean: -89.317522\n",
      "ep 147: ep_len:896 episode reward: total was -343.440000. running mean: -91.858746\n",
      "ep 147: ep_len:500 episode reward: total was -110.520000. running mean: -92.045359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 147: ep_len:78 episode reward: total was -25.310000. running mean: -91.378005\n",
      "ep 147: ep_len:528 episode reward: total was -59.490000. running mean: -91.059125\n",
      "ep 147: ep_len:568 episode reward: total was -68.080000. running mean: -90.829334\n",
      "epsilon:0.193439 episode_count: 1036. steps_count: 462618.000000\n",
      "Time elapsed:  1291.506807088852\n",
      "ep 148: ep_len:186 episode reward: total was -28.030000. running mean: -90.201341\n",
      "ep 148: ep_len:525 episode reward: total was -99.960000. running mean: -90.298927\n",
      "ep 148: ep_len:501 episode reward: total was -131.390000. running mean: -90.709838\n",
      "ep 148: ep_len:500 episode reward: total was -141.970000. running mean: -91.222440\n",
      "ep 148: ep_len:104 episode reward: total was -63.320000. running mean: -90.943415\n",
      "ep 148: ep_len:667 episode reward: total was -137.990000. running mean: -91.413881\n",
      "ep 148: ep_len:285 episode reward: total was -96.880000. running mean: -91.468542\n",
      "epsilon:0.193394 episode_count: 1043. steps_count: 465386.000000\n",
      "Time elapsed:  1300.0917279720306\n",
      "ep 149: ep_len:500 episode reward: total was -53.070000. running mean: -91.084557\n",
      "ep 149: ep_len:568 episode reward: total was -47.660000. running mean: -90.650311\n",
      "ep 149: ep_len:552 episode reward: total was -126.010000. running mean: -91.003908\n",
      "ep 149: ep_len:413 episode reward: total was -50.230000. running mean: -90.596169\n",
      "ep 149: ep_len:83 episode reward: total was 12.180000. running mean: -89.568407\n",
      "ep 149: ep_len:562 episode reward: total was -163.920000. running mean: -90.311923\n",
      "ep 149: ep_len:294 episode reward: total was -91.730000. running mean: -90.326104\n",
      "epsilon:0.193350 episode_count: 1050. steps_count: 468358.000000\n",
      "Time elapsed:  1309.110969543457\n",
      "ep 150: ep_len:134 episode reward: total was -17.520000. running mean: -89.598043\n",
      "ep 150: ep_len:288 episode reward: total was -165.240000. running mean: -90.354463\n",
      "ep 150: ep_len:562 episode reward: total was -86.920000. running mean: -90.320118\n",
      "ep 150: ep_len:501 episode reward: total was -57.630000. running mean: -89.993217\n",
      "ep 150: ep_len:3 episode reward: total was 0.000000. running mean: -89.093285\n",
      "ep 150: ep_len:319 episode reward: total was -19.480000. running mean: -88.397152\n",
      "ep 150: ep_len:504 episode reward: total was -98.420000. running mean: -88.497380\n",
      "epsilon:0.193306 episode_count: 1057. steps_count: 470669.000000\n",
      "Time elapsed:  1316.8002796173096\n",
      "ep 151: ep_len:526 episode reward: total was -100.980000. running mean: -88.622207\n",
      "ep 151: ep_len:501 episode reward: total was -24.720000. running mean: -87.983184\n",
      "ep 151: ep_len:591 episode reward: total was -126.520000. running mean: -88.368553\n",
      "ep 151: ep_len:149 episode reward: total was -9.420000. running mean: -87.579067\n",
      "ep 151: ep_len:3 episode reward: total was 0.000000. running mean: -86.703276\n",
      "ep 151: ep_len:501 episode reward: total was -59.940000. running mean: -86.435644\n",
      "ep 151: ep_len:546 episode reward: total was -108.560000. running mean: -86.656887\n",
      "epsilon:0.193261 episode_count: 1064. steps_count: 473486.000000\n",
      "Time elapsed:  1324.463695049286\n",
      "ep 152: ep_len:661 episode reward: total was -133.750000. running mean: -87.127818\n",
      "ep 152: ep_len:186 episode reward: total was -58.360000. running mean: -86.840140\n",
      "ep 152: ep_len:387 episode reward: total was -39.360000. running mean: -86.365339\n",
      "ep 152: ep_len:509 episode reward: total was -108.420000. running mean: -86.585885\n",
      "ep 152: ep_len:122 episode reward: total was -1.220000. running mean: -85.732227\n",
      "ep 152: ep_len:523 episode reward: total was -65.910000. running mean: -85.534004\n",
      "ep 152: ep_len:533 episode reward: total was -88.040000. running mean: -85.559064\n",
      "epsilon:0.193217 episode_count: 1071. steps_count: 476407.000000\n",
      "Time elapsed:  1332.1631290912628\n",
      "ep 153: ep_len:118 episode reward: total was -0.660000. running mean: -84.710074\n",
      "ep 153: ep_len:322 episode reward: total was -106.490000. running mean: -84.927873\n",
      "ep 153: ep_len:627 episode reward: total was -184.950000. running mean: -85.928094\n",
      "ep 153: ep_len:532 episode reward: total was -78.840000. running mean: -85.857213\n",
      "ep 153: ep_len:89 episode reward: total was 6.730000. running mean: -84.931341\n",
      "ep 153: ep_len:598 episode reward: total was -143.590000. running mean: -85.517928\n",
      "ep 153: ep_len:201 episode reward: total was -36.030000. running mean: -85.023048\n",
      "epsilon:0.193173 episode_count: 1078. steps_count: 478894.000000\n",
      "Time elapsed:  1338.979742527008\n",
      "ep 154: ep_len:603 episode reward: total was -102.070000. running mean: -85.193518\n",
      "ep 154: ep_len:538 episode reward: total was -55.950000. running mean: -84.901083\n",
      "ep 154: ep_len:623 episode reward: total was -94.370000. running mean: -84.995772\n",
      "ep 154: ep_len:503 episode reward: total was -86.010000. running mean: -85.005914\n",
      "ep 154: ep_len:3 episode reward: total was 0.000000. running mean: -84.155855\n",
      "ep 154: ep_len:702 episode reward: total was -84.100000. running mean: -84.155296\n",
      "ep 154: ep_len:597 episode reward: total was -85.100000. running mean: -84.164743\n",
      "epsilon:0.193128 episode_count: 1085. steps_count: 482463.000000\n",
      "Time elapsed:  1348.5353407859802\n",
      "ep 155: ep_len:517 episode reward: total was -67.470000. running mean: -83.997796\n",
      "ep 155: ep_len:620 episode reward: total was -118.910000. running mean: -84.346918\n",
      "ep 155: ep_len:565 episode reward: total was -103.540000. running mean: -84.538849\n",
      "ep 155: ep_len:528 episode reward: total was -69.910000. running mean: -84.392560\n",
      "ep 155: ep_len:3 episode reward: total was 0.000000. running mean: -83.548635\n",
      "ep 155: ep_len:637 episode reward: total was -125.670000. running mean: -83.969848\n",
      "ep 155: ep_len:574 episode reward: total was -92.060000. running mean: -84.050750\n",
      "epsilon:0.193084 episode_count: 1092. steps_count: 485907.000000\n",
      "Time elapsed:  1357.749351978302\n",
      "ep 156: ep_len:228 episode reward: total was -29.370000. running mean: -83.503942\n",
      "ep 156: ep_len:534 episode reward: total was -92.450000. running mean: -83.593403\n",
      "ep 156: ep_len:542 episode reward: total was -186.320000. running mean: -84.620669\n",
      "ep 156: ep_len:500 episode reward: total was -90.430000. running mean: -84.678762\n",
      "ep 156: ep_len:99 episode reward: total was -23.780000. running mean: -84.069775\n",
      "ep 156: ep_len:564 episode reward: total was -95.280000. running mean: -84.181877\n",
      "ep 156: ep_len:541 episode reward: total was -85.330000. running mean: -84.193358\n",
      "epsilon:0.193040 episode_count: 1099. steps_count: 488915.000000\n",
      "Time elapsed:  1365.8121826648712\n",
      "ep 157: ep_len:190 episode reward: total was -31.900000. running mean: -83.670425\n",
      "ep 157: ep_len:500 episode reward: total was -64.200000. running mean: -83.475720\n",
      "ep 157: ep_len:513 episode reward: total was -108.400000. running mean: -83.724963\n",
      "ep 157: ep_len:500 episode reward: total was -50.780000. running mean: -83.395514\n",
      "ep 157: ep_len:3 episode reward: total was 0.000000. running mean: -82.561558\n",
      "ep 157: ep_len:526 episode reward: total was -103.040000. running mean: -82.766343\n",
      "ep 157: ep_len:580 episode reward: total was -93.030000. running mean: -82.868979\n",
      "epsilon:0.192995 episode_count: 1106. steps_count: 491727.000000\n",
      "Time elapsed:  1373.5755875110626\n",
      "ep 158: ep_len:253 episode reward: total was -27.320000. running mean: -82.313490\n",
      "ep 158: ep_len:500 episode reward: total was -60.670000. running mean: -82.097055\n",
      "ep 158: ep_len:510 episode reward: total was -60.670000. running mean: -81.882784\n",
      "ep 158: ep_len:501 episode reward: total was -80.340000. running mean: -81.867356\n",
      "ep 158: ep_len:85 episode reward: total was -41.290000. running mean: -81.461583\n",
      "ep 158: ep_len:630 episode reward: total was -118.390000. running mean: -81.830867\n",
      "ep 158: ep_len:520 episode reward: total was -107.650000. running mean: -82.089058\n",
      "epsilon:0.192951 episode_count: 1113. steps_count: 494726.000000\n",
      "Time elapsed:  1381.3800115585327\n",
      "ep 159: ep_len:658 episode reward: total was -141.450000. running mean: -82.682668\n",
      "ep 159: ep_len:599 episode reward: total was -140.990000. running mean: -83.265741\n",
      "ep 159: ep_len:608 episode reward: total was -96.390000. running mean: -83.396984\n",
      "ep 159: ep_len:500 episode reward: total was -51.790000. running mean: -83.080914\n",
      "ep 159: ep_len:3 episode reward: total was -1.500000. running mean: -82.265105\n",
      "ep 159: ep_len:520 episode reward: total was -87.790000. running mean: -82.320354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 159: ep_len:547 episode reward: total was -138.610000. running mean: -82.883250\n",
      "epsilon:0.192907 episode_count: 1120. steps_count: 498161.000000\n",
      "Time elapsed:  1390.4031128883362\n",
      "ep 160: ep_len:548 episode reward: total was -94.640000. running mean: -83.000818\n",
      "ep 160: ep_len:564 episode reward: total was -92.560000. running mean: -83.096409\n",
      "ep 160: ep_len:558 episode reward: total was -60.970000. running mean: -82.875145\n",
      "ep 160: ep_len:522 episode reward: total was -80.490000. running mean: -82.851294\n",
      "ep 160: ep_len:3 episode reward: total was 0.000000. running mean: -82.022781\n",
      "ep 160: ep_len:517 episode reward: total was -138.230000. running mean: -82.584853\n",
      "ep 160: ep_len:571 episode reward: total was -86.580000. running mean: -82.624805\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.192862 episode_count: 1127. steps_count: 501444.000000\n",
      "Time elapsed:  1404.2207815647125\n",
      "ep 161: ep_len:580 episode reward: total was -120.240000. running mean: -83.000956\n",
      "ep 161: ep_len:586 episode reward: total was -99.330000. running mean: -83.164247\n",
      "ep 161: ep_len:646 episode reward: total was -312.070000. running mean: -85.453304\n",
      "ep 161: ep_len:527 episode reward: total was -54.700000. running mean: -85.145771\n",
      "ep 161: ep_len:3 episode reward: total was 1.010000. running mean: -84.284214\n",
      "ep 161: ep_len:503 episode reward: total was -100.860000. running mean: -84.449972\n",
      "ep 161: ep_len:500 episode reward: total was -129.640000. running mean: -84.901872\n",
      "epsilon:0.192818 episode_count: 1134. steps_count: 504789.000000\n",
      "Time elapsed:  1413.0379939079285\n",
      "ep 162: ep_len:501 episode reward: total was -76.460000. running mean: -84.817453\n",
      "ep 162: ep_len:532 episode reward: total was -87.240000. running mean: -84.841679\n",
      "ep 162: ep_len:699 episode reward: total was -175.690000. running mean: -85.750162\n",
      "ep 162: ep_len:501 episode reward: total was -35.500000. running mean: -85.247660\n",
      "ep 162: ep_len:102 episode reward: total was 4.260000. running mean: -84.352584\n",
      "ep 162: ep_len:500 episode reward: total was -106.430000. running mean: -84.573358\n",
      "ep 162: ep_len:340 episode reward: total was -91.100000. running mean: -84.638624\n",
      "epsilon:0.192774 episode_count: 1141. steps_count: 507964.000000\n",
      "Time elapsed:  1421.6882269382477\n",
      "ep 163: ep_len:254 episode reward: total was -15.620000. running mean: -83.948438\n",
      "ep 163: ep_len:571 episode reward: total was -3.580000. running mean: -83.144754\n",
      "ep 163: ep_len:599 episode reward: total was -94.320000. running mean: -83.256506\n",
      "ep 163: ep_len:507 episode reward: total was -105.840000. running mean: -83.482341\n",
      "ep 163: ep_len:3 episode reward: total was -1.500000. running mean: -82.662518\n",
      "ep 163: ep_len:660 episode reward: total was -88.810000. running mean: -82.723992\n",
      "ep 163: ep_len:589 episode reward: total was -118.490000. running mean: -83.081652\n",
      "epsilon:0.192729 episode_count: 1148. steps_count: 511147.000000\n",
      "Time elapsed:  1430.177702665329\n",
      "ep 164: ep_len:535 episode reward: total was -129.000000. running mean: -83.540836\n",
      "ep 164: ep_len:179 episode reward: total was -33.800000. running mean: -83.043428\n",
      "ep 164: ep_len:557 episode reward: total was -150.380000. running mean: -83.716793\n",
      "ep 164: ep_len:505 episode reward: total was -135.850000. running mean: -84.238125\n",
      "ep 164: ep_len:3 episode reward: total was 0.000000. running mean: -83.395744\n",
      "ep 164: ep_len:600 episode reward: total was -123.100000. running mean: -83.792787\n",
      "ep 164: ep_len:584 episode reward: total was -104.980000. running mean: -84.004659\n",
      "epsilon:0.192685 episode_count: 1155. steps_count: 514110.000000\n",
      "Time elapsed:  1438.0770275592804\n",
      "ep 165: ep_len:552 episode reward: total was -146.970000. running mean: -84.634312\n",
      "ep 165: ep_len:520 episode reward: total was -116.370000. running mean: -84.951669\n",
      "ep 165: ep_len:516 episode reward: total was -73.080000. running mean: -84.832952\n",
      "ep 165: ep_len:540 episode reward: total was -54.890000. running mean: -84.533523\n",
      "ep 165: ep_len:50 episode reward: total was 16.000000. running mean: -83.528188\n",
      "ep 165: ep_len:231 episode reward: total was -55.700000. running mean: -83.249906\n",
      "ep 165: ep_len:500 episode reward: total was -71.450000. running mean: -83.131907\n",
      "epsilon:0.192641 episode_count: 1162. steps_count: 517019.000000\n",
      "Time elapsed:  1445.912185907364\n",
      "ep 166: ep_len:503 episode reward: total was -112.610000. running mean: -83.426688\n",
      "ep 166: ep_len:577 episode reward: total was -181.820000. running mean: -84.410621\n",
      "ep 166: ep_len:532 episode reward: total was -140.330000. running mean: -84.969815\n",
      "ep 166: ep_len:534 episode reward: total was -104.580000. running mean: -85.165916\n",
      "ep 166: ep_len:3 episode reward: total was -1.500000. running mean: -84.329257\n",
      "ep 166: ep_len:518 episode reward: total was -81.490000. running mean: -84.300865\n",
      "ep 166: ep_len:506 episode reward: total was -72.300000. running mean: -84.180856\n",
      "epsilon:0.192596 episode_count: 1169. steps_count: 520192.000000\n",
      "Time elapsed:  1454.4971759319305\n",
      "ep 167: ep_len:538 episode reward: total was -110.760000. running mean: -84.446647\n",
      "ep 167: ep_len:510 episode reward: total was -114.270000. running mean: -84.744881\n",
      "ep 167: ep_len:649 episode reward: total was -140.140000. running mean: -85.298832\n",
      "ep 167: ep_len:557 episode reward: total was -82.800000. running mean: -85.273844\n",
      "ep 167: ep_len:3 episode reward: total was 0.000000. running mean: -84.421105\n",
      "ep 167: ep_len:500 episode reward: total was -118.410000. running mean: -84.760994\n",
      "ep 167: ep_len:500 episode reward: total was -46.540000. running mean: -84.378784\n",
      "epsilon:0.192552 episode_count: 1176. steps_count: 523449.000000\n",
      "Time elapsed:  1463.0669765472412\n",
      "ep 168: ep_len:556 episode reward: total was -43.900000. running mean: -83.973997\n",
      "ep 168: ep_len:305 episode reward: total was -62.410000. running mean: -83.758357\n",
      "ep 168: ep_len:410 episode reward: total was -62.750000. running mean: -83.548273\n",
      "ep 168: ep_len:513 episode reward: total was -38.640000. running mean: -83.099190\n",
      "ep 168: ep_len:117 episode reward: total was -49.710000. running mean: -82.765298\n",
      "ep 168: ep_len:594 episode reward: total was -84.140000. running mean: -82.779045\n",
      "ep 168: ep_len:588 episode reward: total was -119.500000. running mean: -83.146255\n",
      "epsilon:0.192508 episode_count: 1183. steps_count: 526532.000000\n",
      "Time elapsed:  1471.4247212409973\n",
      "ep 169: ep_len:103 episode reward: total was -18.140000. running mean: -82.496192\n",
      "ep 169: ep_len:519 episode reward: total was -140.660000. running mean: -83.077830\n",
      "ep 169: ep_len:500 episode reward: total was -119.970000. running mean: -83.446752\n",
      "ep 169: ep_len:500 episode reward: total was -46.280000. running mean: -83.075085\n",
      "ep 169: ep_len:3 episode reward: total was 0.000000. running mean: -82.244334\n",
      "ep 169: ep_len:630 episode reward: total was -136.710000. running mean: -82.788990\n",
      "ep 169: ep_len:568 episode reward: total was -137.700000. running mean: -83.338101\n",
      "epsilon:0.192463 episode_count: 1190. steps_count: 529355.000000\n",
      "Time elapsed:  1478.9304325580597\n",
      "ep 170: ep_len:226 episode reward: total was -41.250000. running mean: -82.917220\n",
      "ep 170: ep_len:595 episode reward: total was -153.880000. running mean: -83.626847\n",
      "ep 170: ep_len:567 episode reward: total was -132.490000. running mean: -84.115479\n",
      "ep 170: ep_len:514 episode reward: total was -185.310000. running mean: -85.127424\n",
      "ep 170: ep_len:3 episode reward: total was 0.000000. running mean: -84.276150\n",
      "ep 170: ep_len:516 episode reward: total was -108.130000. running mean: -84.514688\n",
      "ep 170: ep_len:524 episode reward: total was -99.080000. running mean: -84.660341\n",
      "epsilon:0.192419 episode_count: 1197. steps_count: 532300.000000\n",
      "Time elapsed:  1486.9709994792938\n",
      "ep 171: ep_len:678 episode reward: total was -164.130000. running mean: -85.455038\n",
      "ep 171: ep_len:589 episode reward: total was -56.100000. running mean: -85.161488\n",
      "ep 171: ep_len:557 episode reward: total was -157.460000. running mean: -85.884473\n",
      "ep 171: ep_len:511 episode reward: total was -120.690000. running mean: -86.232528\n",
      "ep 171: ep_len:3 episode reward: total was 0.000000. running mean: -85.370203\n",
      "ep 171: ep_len:163 episode reward: total was -55.080000. running mean: -85.067301\n",
      "ep 171: ep_len:576 episode reward: total was -157.450000. running mean: -85.791128\n",
      "epsilon:0.192375 episode_count: 1204. steps_count: 535377.000000\n",
      "Time elapsed:  1494.808803319931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 172: ep_len:197 episode reward: total was -17.480000. running mean: -85.108016\n",
      "ep 172: ep_len:506 episode reward: total was -151.010000. running mean: -85.767036\n",
      "ep 172: ep_len:667 episode reward: total was -130.920000. running mean: -86.218566\n",
      "ep 172: ep_len:500 episode reward: total was -132.570000. running mean: -86.682080\n",
      "ep 172: ep_len:3 episode reward: total was -1.500000. running mean: -85.830260\n",
      "ep 172: ep_len:555 episode reward: total was -58.930000. running mean: -85.561257\n",
      "ep 172: ep_len:605 episode reward: total was -80.100000. running mean: -85.506644\n",
      "epsilon:0.192330 episode_count: 1211. steps_count: 538410.000000\n",
      "Time elapsed:  1502.9579544067383\n",
      "ep 173: ep_len:662 episode reward: total was -122.860000. running mean: -85.880178\n",
      "ep 173: ep_len:542 episode reward: total was -73.710000. running mean: -85.758476\n",
      "ep 173: ep_len:558 episode reward: total was -112.120000. running mean: -86.022091\n",
      "ep 173: ep_len:501 episode reward: total was -23.560000. running mean: -85.397470\n",
      "ep 173: ep_len:3 episode reward: total was 0.000000. running mean: -84.543496\n",
      "ep 173: ep_len:636 episode reward: total was -93.240000. running mean: -84.630461\n",
      "ep 173: ep_len:539 episode reward: total was -115.490000. running mean: -84.939056\n",
      "epsilon:0.192286 episode_count: 1218. steps_count: 541851.000000\n",
      "Time elapsed:  1512.014092206955\n",
      "ep 174: ep_len:229 episode reward: total was -19.270000. running mean: -84.282366\n",
      "ep 174: ep_len:535 episode reward: total was -233.480000. running mean: -85.774342\n",
      "ep 174: ep_len:77 episode reward: total was -6.250000. running mean: -84.979099\n",
      "ep 174: ep_len:512 episode reward: total was -63.740000. running mean: -84.766708\n",
      "ep 174: ep_len:3 episode reward: total was 0.000000. running mean: -83.919040\n",
      "ep 174: ep_len:577 episode reward: total was -124.650000. running mean: -84.326350\n",
      "ep 174: ep_len:519 episode reward: total was -103.320000. running mean: -84.516287\n",
      "epsilon:0.192242 episode_count: 1225. steps_count: 544303.000000\n",
      "Time elapsed:  1518.9647212028503\n",
      "ep 175: ep_len:248 episode reward: total was -42.670000. running mean: -84.097824\n",
      "ep 175: ep_len:502 episode reward: total was -11.340000. running mean: -83.370245\n",
      "ep 175: ep_len:587 episode reward: total was -104.920000. running mean: -83.585743\n",
      "ep 175: ep_len:502 episode reward: total was -96.060000. running mean: -83.710486\n",
      "ep 175: ep_len:86 episode reward: total was -61.270000. running mean: -83.486081\n",
      "ep 175: ep_len:580 episode reward: total was -108.580000. running mean: -83.737020\n",
      "ep 175: ep_len:333 episode reward: total was -94.510000. running mean: -83.844750\n",
      "epsilon:0.192197 episode_count: 1232. steps_count: 547141.000000\n",
      "Time elapsed:  1526.8965349197388\n",
      "ep 176: ep_len:500 episode reward: total was -114.880000. running mean: -84.155102\n",
      "ep 176: ep_len:516 episode reward: total was -89.690000. running mean: -84.210451\n",
      "ep 176: ep_len:500 episode reward: total was -87.540000. running mean: -84.243747\n",
      "ep 176: ep_len:501 episode reward: total was -104.270000. running mean: -84.444009\n",
      "ep 176: ep_len:3 episode reward: total was 0.000000. running mean: -83.599569\n",
      "ep 176: ep_len:287 episode reward: total was -12.150000. running mean: -82.885073\n",
      "ep 176: ep_len:500 episode reward: total was -83.980000. running mean: -82.896023\n",
      "epsilon:0.192153 episode_count: 1239. steps_count: 549948.000000\n",
      "Time elapsed:  1534.7694449424744\n",
      "ep 177: ep_len:533 episode reward: total was -144.260000. running mean: -83.509662\n",
      "ep 177: ep_len:500 episode reward: total was -131.660000. running mean: -83.991166\n",
      "ep 177: ep_len:644 episode reward: total was -203.540000. running mean: -85.186654\n",
      "ep 177: ep_len:532 episode reward: total was -117.360000. running mean: -85.508388\n",
      "ep 177: ep_len:133 episode reward: total was -7.700000. running mean: -84.730304\n",
      "ep 177: ep_len:313 episode reward: total was -54.740000. running mean: -84.430401\n",
      "ep 177: ep_len:513 episode reward: total was -63.290000. running mean: -84.218997\n",
      "epsilon:0.192109 episode_count: 1246. steps_count: 553116.000000\n",
      "Time elapsed:  1543.1577517986298\n",
      "ep 178: ep_len:636 episode reward: total was -99.890000. running mean: -84.375707\n",
      "ep 178: ep_len:501 episode reward: total was -58.880000. running mean: -84.120750\n",
      "ep 178: ep_len:500 episode reward: total was -89.860000. running mean: -84.178142\n",
      "ep 178: ep_len:569 episode reward: total was -73.830000. running mean: -84.074661\n",
      "ep 178: ep_len:112 episode reward: total was -75.240000. running mean: -83.986314\n",
      "ep 178: ep_len:616 episode reward: total was -121.940000. running mean: -84.365851\n",
      "ep 178: ep_len:500 episode reward: total was -135.260000. running mean: -84.874793\n",
      "epsilon:0.192064 episode_count: 1253. steps_count: 556550.000000\n",
      "Time elapsed:  1552.5118374824524\n",
      "ep 179: ep_len:612 episode reward: total was -157.700000. running mean: -85.603045\n",
      "ep 179: ep_len:625 episode reward: total was -163.650000. running mean: -86.383514\n",
      "ep 179: ep_len:368 episode reward: total was -23.560000. running mean: -85.755279\n",
      "ep 179: ep_len:500 episode reward: total was -131.720000. running mean: -86.214926\n",
      "ep 179: ep_len:3 episode reward: total was 0.000000. running mean: -85.352777\n",
      "ep 179: ep_len:518 episode reward: total was -113.170000. running mean: -85.630949\n",
      "ep 179: ep_len:193 episode reward: total was -72.800000. running mean: -85.502640\n",
      "epsilon:0.192020 episode_count: 1260. steps_count: 559369.000000\n",
      "Time elapsed:  1560.0845420360565\n",
      "ep 180: ep_len:500 episode reward: total was -80.120000. running mean: -85.448813\n",
      "ep 180: ep_len:273 episode reward: total was -49.980000. running mean: -85.094125\n",
      "ep 180: ep_len:500 episode reward: total was -109.290000. running mean: -85.336084\n",
      "ep 180: ep_len:412 episode reward: total was -64.290000. running mean: -85.125623\n",
      "ep 180: ep_len:97 episode reward: total was -46.760000. running mean: -84.741967\n",
      "ep 180: ep_len:500 episode reward: total was -72.150000. running mean: -84.616047\n",
      "ep 180: ep_len:534 episode reward: total was -117.790000. running mean: -84.947787\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.191976 episode_count: 1267. steps_count: 562185.000000\n",
      "Time elapsed:  1572.88143658638\n",
      "ep 181: ep_len:500 episode reward: total was -93.740000. running mean: -85.035709\n",
      "ep 181: ep_len:530 episode reward: total was 0.620000. running mean: -84.179152\n",
      "ep 181: ep_len:459 episode reward: total was -58.320000. running mean: -83.920560\n",
      "ep 181: ep_len:500 episode reward: total was -110.440000. running mean: -84.185755\n",
      "ep 181: ep_len:3 episode reward: total was 0.000000. running mean: -83.343897\n",
      "ep 181: ep_len:580 episode reward: total was -107.190000. running mean: -83.582358\n",
      "ep 181: ep_len:512 episode reward: total was -102.800000. running mean: -83.774535\n",
      "epsilon:0.191931 episode_count: 1274. steps_count: 565269.000000\n",
      "Time elapsed:  1581.0983970165253\n",
      "ep 182: ep_len:500 episode reward: total was -35.650000. running mean: -83.293289\n",
      "ep 182: ep_len:653 episode reward: total was -75.760000. running mean: -83.217956\n",
      "ep 182: ep_len:559 episode reward: total was -111.860000. running mean: -83.504377\n",
      "ep 182: ep_len:504 episode reward: total was -136.110000. running mean: -84.030433\n",
      "ep 182: ep_len:3 episode reward: total was 0.000000. running mean: -83.190129\n",
      "ep 182: ep_len:211 episode reward: total was -1.360000. running mean: -82.371827\n",
      "ep 182: ep_len:618 episode reward: total was -264.830000. running mean: -84.196409\n",
      "epsilon:0.191887 episode_count: 1281. steps_count: 568317.000000\n",
      "Time elapsed:  1589.5652828216553\n",
      "ep 183: ep_len:592 episode reward: total was -109.200000. running mean: -84.446445\n",
      "ep 183: ep_len:522 episode reward: total was -90.480000. running mean: -84.506781\n",
      "ep 183: ep_len:500 episode reward: total was -112.170000. running mean: -84.783413\n",
      "ep 183: ep_len:510 episode reward: total was -143.790000. running mean: -85.373479\n",
      "ep 183: ep_len:102 episode reward: total was -33.210000. running mean: -84.851844\n",
      "ep 183: ep_len:629 episode reward: total was -106.270000. running mean: -85.066025\n",
      "ep 183: ep_len:282 episode reward: total was -71.010000. running mean: -84.925465\n",
      "epsilon:0.191843 episode_count: 1288. steps_count: 571454.000000\n",
      "Time elapsed:  1598.1335628032684\n",
      "ep 184: ep_len:546 episode reward: total was -107.030000. running mean: -85.146510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 184: ep_len:513 episode reward: total was -95.330000. running mean: -85.248345\n",
      "ep 184: ep_len:399 episode reward: total was -25.450000. running mean: -84.650362\n",
      "ep 184: ep_len:378 episode reward: total was -53.830000. running mean: -84.342158\n",
      "ep 184: ep_len:3 episode reward: total was 0.000000. running mean: -83.498737\n",
      "ep 184: ep_len:501 episode reward: total was -102.370000. running mean: -83.687449\n",
      "ep 184: ep_len:598 episode reward: total was -122.560000. running mean: -84.076175\n",
      "epsilon:0.191798 episode_count: 1295. steps_count: 574392.000000\n",
      "Time elapsed:  1606.0006494522095\n",
      "ep 185: ep_len:129 episode reward: total was -13.160000. running mean: -83.367013\n",
      "ep 185: ep_len:184 episode reward: total was -52.150000. running mean: -83.054843\n",
      "ep 185: ep_len:647 episode reward: total was -163.370000. running mean: -83.857995\n",
      "ep 185: ep_len:501 episode reward: total was -37.020000. running mean: -83.389615\n",
      "ep 185: ep_len:110 episode reward: total was 2.290000. running mean: -82.532818\n",
      "ep 185: ep_len:516 episode reward: total was -109.320000. running mean: -82.800690\n",
      "ep 185: ep_len:534 episode reward: total was -112.590000. running mean: -83.098583\n",
      "epsilon:0.191754 episode_count: 1302. steps_count: 577013.000000\n",
      "Time elapsed:  1613.156772851944\n",
      "ep 186: ep_len:583 episode reward: total was -123.220000. running mean: -83.499798\n",
      "ep 186: ep_len:586 episode reward: total was -55.250000. running mean: -83.217300\n",
      "ep 186: ep_len:500 episode reward: total was -57.130000. running mean: -82.956427\n",
      "ep 186: ep_len:575 episode reward: total was -67.340000. running mean: -82.800262\n",
      "ep 186: ep_len:63 episode reward: total was -7.800000. running mean: -82.050260\n",
      "ep 186: ep_len:505 episode reward: total was -94.380000. running mean: -82.173557\n",
      "ep 186: ep_len:310 episode reward: total was -91.770000. running mean: -82.269521\n",
      "epsilon:0.191710 episode_count: 1309. steps_count: 580135.000000\n",
      "Time elapsed:  1621.44730758667\n",
      "ep 187: ep_len:643 episode reward: total was -197.710000. running mean: -83.423926\n",
      "ep 187: ep_len:516 episode reward: total was -49.390000. running mean: -83.083587\n",
      "ep 187: ep_len:616 episode reward: total was -149.780000. running mean: -83.750551\n",
      "ep 187: ep_len:523 episode reward: total was -78.460000. running mean: -83.697646\n",
      "ep 187: ep_len:88 episode reward: total was -14.770000. running mean: -83.008369\n",
      "ep 187: ep_len:501 episode reward: total was -82.210000. running mean: -83.000385\n",
      "ep 187: ep_len:573 episode reward: total was -90.940000. running mean: -83.079782\n",
      "epsilon:0.191665 episode_count: 1316. steps_count: 583595.000000\n",
      "Time elapsed:  1630.7255573272705\n",
      "ep 188: ep_len:653 episode reward: total was -97.800000. running mean: -83.226984\n",
      "ep 188: ep_len:543 episode reward: total was -129.580000. running mean: -83.690514\n",
      "ep 188: ep_len:504 episode reward: total was -114.000000. running mean: -83.993609\n",
      "ep 188: ep_len:110 episode reward: total was 5.870000. running mean: -83.094973\n",
      "ep 188: ep_len:3 episode reward: total was 0.000000. running mean: -82.264023\n",
      "ep 188: ep_len:515 episode reward: total was -99.580000. running mean: -82.437183\n",
      "ep 188: ep_len:182 episode reward: total was -35.260000. running mean: -81.965411\n",
      "epsilon:0.191621 episode_count: 1323. steps_count: 586105.000000\n",
      "Time elapsed:  1637.87904214859\n",
      "ep 189: ep_len:259 episode reward: total was -29.820000. running mean: -81.443957\n",
      "ep 189: ep_len:505 episode reward: total was -73.970000. running mean: -81.369217\n",
      "ep 189: ep_len:531 episode reward: total was -188.040000. running mean: -82.435925\n",
      "ep 189: ep_len:513 episode reward: total was -149.930000. running mean: -83.110866\n",
      "ep 189: ep_len:85 episode reward: total was -53.750000. running mean: -82.817257\n",
      "ep 189: ep_len:632 episode reward: total was -76.850000. running mean: -82.757585\n",
      "ep 189: ep_len:608 episode reward: total was -109.820000. running mean: -83.028209\n",
      "epsilon:0.191577 episode_count: 1330. steps_count: 589238.000000\n",
      "Time elapsed:  1646.2668991088867\n",
      "ep 190: ep_len:525 episode reward: total was -239.860000. running mean: -84.596527\n",
      "ep 190: ep_len:587 episode reward: total was -85.390000. running mean: -84.604461\n",
      "ep 190: ep_len:687 episode reward: total was -191.810000. running mean: -85.676517\n",
      "ep 190: ep_len:501 episode reward: total was -126.010000. running mean: -86.079852\n",
      "ep 190: ep_len:3 episode reward: total was 0.000000. running mean: -85.219053\n",
      "ep 190: ep_len:500 episode reward: total was -114.960000. running mean: -85.516463\n",
      "ep 190: ep_len:586 episode reward: total was -71.040000. running mean: -85.371698\n",
      "epsilon:0.191532 episode_count: 1337. steps_count: 592627.000000\n",
      "Time elapsed:  1655.2385907173157\n",
      "ep 191: ep_len:125 episode reward: total was -7.660000. running mean: -84.594581\n",
      "ep 191: ep_len:268 episode reward: total was -53.510000. running mean: -84.283735\n",
      "ep 191: ep_len:557 episode reward: total was -148.790000. running mean: -84.928798\n",
      "ep 191: ep_len:555 episode reward: total was -38.000000. running mean: -84.459510\n",
      "ep 191: ep_len:55 episode reward: total was 6.500000. running mean: -83.549915\n",
      "ep 191: ep_len:539 episode reward: total was -83.000000. running mean: -83.544416\n",
      "ep 191: ep_len:554 episode reward: total was -102.280000. running mean: -83.731771\n",
      "epsilon:0.191488 episode_count: 1344. steps_count: 595280.000000\n",
      "Time elapsed:  1662.5650486946106\n",
      "ep 192: ep_len:217 episode reward: total was -23.490000. running mean: -83.129354\n",
      "ep 192: ep_len:581 episode reward: total was -120.500000. running mean: -83.503060\n",
      "ep 192: ep_len:544 episode reward: total was -108.180000. running mean: -83.749830\n",
      "ep 192: ep_len:587 episode reward: total was -151.130000. running mean: -84.423631\n",
      "ep 192: ep_len:133 episode reward: total was 5.860000. running mean: -83.520795\n",
      "ep 192: ep_len:555 episode reward: total was -73.760000. running mean: -83.423187\n",
      "ep 192: ep_len:571 episode reward: total was -108.630000. running mean: -83.675255\n",
      "epsilon:0.191444 episode_count: 1351. steps_count: 598468.000000\n",
      "Time elapsed:  1671.1768386363983\n",
      "ep 193: ep_len:555 episode reward: total was -68.440000. running mean: -83.522903\n",
      "ep 193: ep_len:520 episode reward: total was -3.970000. running mean: -82.727374\n",
      "ep 193: ep_len:637 episode reward: total was -87.270000. running mean: -82.772800\n",
      "ep 193: ep_len:369 episode reward: total was -75.650000. running mean: -82.701572\n",
      "ep 193: ep_len:109 episode reward: total was -4.300000. running mean: -81.917556\n",
      "ep 193: ep_len:501 episode reward: total was -71.660000. running mean: -81.814981\n",
      "ep 193: ep_len:568 episode reward: total was -117.620000. running mean: -82.173031\n",
      "epsilon:0.191399 episode_count: 1358. steps_count: 601727.000000\n",
      "Time elapsed:  1679.8998651504517\n",
      "ep 194: ep_len:500 episode reward: total was -77.950000. running mean: -82.130800\n",
      "ep 194: ep_len:540 episode reward: total was -65.770000. running mean: -81.967192\n",
      "ep 194: ep_len:572 episode reward: total was -119.440000. running mean: -82.341921\n",
      "ep 194: ep_len:520 episode reward: total was -84.440000. running mean: -82.362901\n",
      "ep 194: ep_len:99 episode reward: total was 9.740000. running mean: -81.441872\n",
      "ep 194: ep_len:500 episode reward: total was -93.740000. running mean: -81.564854\n",
      "ep 194: ep_len:319 episode reward: total was -103.390000. running mean: -81.783105\n",
      "epsilon:0.191355 episode_count: 1365. steps_count: 604777.000000\n",
      "Time elapsed:  1688.1007969379425\n",
      "ep 195: ep_len:597 episode reward: total was -91.450000. running mean: -81.879774\n",
      "ep 195: ep_len:500 episode reward: total was -140.690000. running mean: -82.467876\n",
      "ep 195: ep_len:500 episode reward: total was -103.640000. running mean: -82.679598\n",
      "ep 195: ep_len:510 episode reward: total was -118.690000. running mean: -83.039702\n",
      "ep 195: ep_len:3 episode reward: total was 0.000000. running mean: -82.209305\n",
      "ep 195: ep_len:605 episode reward: total was -88.800000. running mean: -82.275211\n",
      "ep 195: ep_len:500 episode reward: total was -122.700000. running mean: -82.679459\n",
      "epsilon:0.191311 episode_count: 1372. steps_count: 607992.000000\n",
      "Time elapsed:  1696.3413815498352\n",
      "ep 196: ep_len:598 episode reward: total was -101.260000. running mean: -82.865265\n",
      "ep 196: ep_len:596 episode reward: total was -98.500000. running mean: -83.021612\n",
      "ep 196: ep_len:397 episode reward: total was -29.850000. running mean: -82.489896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 196: ep_len:527 episode reward: total was -112.520000. running mean: -82.790197\n",
      "ep 196: ep_len:3 episode reward: total was -1.500000. running mean: -81.977295\n",
      "ep 196: ep_len:500 episode reward: total was -82.290000. running mean: -81.980422\n",
      "ep 196: ep_len:500 episode reward: total was -79.700000. running mean: -81.957618\n",
      "epsilon:0.191266 episode_count: 1379. steps_count: 611113.000000\n",
      "Time elapsed:  1704.787085533142\n",
      "ep 197: ep_len:542 episode reward: total was -69.130000. running mean: -81.829342\n",
      "ep 197: ep_len:617 episode reward: total was -122.390000. running mean: -82.234948\n",
      "ep 197: ep_len:507 episode reward: total was -29.080000. running mean: -81.703399\n",
      "ep 197: ep_len:500 episode reward: total was -72.100000. running mean: -81.607365\n",
      "ep 197: ep_len:3 episode reward: total was 0.000000. running mean: -80.791291\n",
      "ep 197: ep_len:500 episode reward: total was -98.710000. running mean: -80.970478\n",
      "ep 197: ep_len:326 episode reward: total was -90.970000. running mean: -81.070473\n",
      "epsilon:0.191222 episode_count: 1386. steps_count: 614108.000000\n",
      "Time elapsed:  1712.9139285087585\n",
      "ep 198: ep_len:220 episode reward: total was 1.210000. running mean: -80.247669\n",
      "ep 198: ep_len:169 episode reward: total was -20.490000. running mean: -79.650092\n",
      "ep 198: ep_len:503 episode reward: total was -44.820000. running mean: -79.301791\n",
      "ep 198: ep_len:530 episode reward: total was -127.040000. running mean: -79.779173\n",
      "ep 198: ep_len:3 episode reward: total was 0.000000. running mean: -78.981381\n",
      "ep 198: ep_len:235 episode reward: total was -16.850000. running mean: -78.360068\n",
      "ep 198: ep_len:603 episode reward: total was -89.140000. running mean: -78.467867\n",
      "epsilon:0.191178 episode_count: 1393. steps_count: 616371.000000\n",
      "Time elapsed:  1719.0731179714203\n",
      "ep 199: ep_len:536 episode reward: total was -113.680000. running mean: -78.819988\n",
      "ep 199: ep_len:517 episode reward: total was -16.670000. running mean: -78.198488\n",
      "ep 199: ep_len:537 episode reward: total was -118.260000. running mean: -78.599104\n",
      "ep 199: ep_len:56 episode reward: total was -6.220000. running mean: -77.875313\n",
      "ep 199: ep_len:3 episode reward: total was -3.000000. running mean: -77.126559\n",
      "ep 199: ep_len:572 episode reward: total was -74.320000. running mean: -77.098494\n",
      "ep 199: ep_len:526 episode reward: total was -75.030000. running mean: -77.077809\n",
      "epsilon:0.191133 episode_count: 1400. steps_count: 619118.000000\n",
      "Time elapsed:  1726.7599506378174\n",
      "ep 200: ep_len:555 episode reward: total was -134.980000. running mean: -77.656831\n",
      "ep 200: ep_len:507 episode reward: total was -58.060000. running mean: -77.460862\n",
      "ep 200: ep_len:556 episode reward: total was -122.200000. running mean: -77.908254\n",
      "ep 200: ep_len:510 episode reward: total was -135.280000. running mean: -78.481971\n",
      "ep 200: ep_len:3 episode reward: total was 0.000000. running mean: -77.697152\n",
      "ep 200: ep_len:500 episode reward: total was -74.670000. running mean: -77.666880\n",
      "ep 200: ep_len:500 episode reward: total was -125.630000. running mean: -78.146511\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.191089 episode_count: 1407. steps_count: 622249.000000\n",
      "Time elapsed:  1740.3692638874054\n",
      "ep 201: ep_len:215 episode reward: total was -24.920000. running mean: -77.614246\n",
      "ep 201: ep_len:500 episode reward: total was -61.610000. running mean: -77.454204\n",
      "ep 201: ep_len:590 episode reward: total was -166.790000. running mean: -78.347562\n",
      "ep 201: ep_len:526 episode reward: total was -125.620000. running mean: -78.820286\n",
      "ep 201: ep_len:3 episode reward: total was 0.000000. running mean: -78.032083\n",
      "ep 201: ep_len:500 episode reward: total was -72.110000. running mean: -77.972862\n",
      "ep 201: ep_len:574 episode reward: total was -86.660000. running mean: -78.059734\n",
      "epsilon:0.191045 episode_count: 1414. steps_count: 625157.000000\n",
      "Time elapsed:  1748.7266545295715\n",
      "ep 202: ep_len:546 episode reward: total was -96.260000. running mean: -78.241736\n",
      "ep 202: ep_len:574 episode reward: total was -106.990000. running mean: -78.529219\n",
      "ep 202: ep_len:562 episode reward: total was -73.290000. running mean: -78.476827\n",
      "ep 202: ep_len:500 episode reward: total was -91.640000. running mean: -78.608459\n",
      "ep 202: ep_len:3 episode reward: total was 0.000000. running mean: -77.822374\n",
      "ep 202: ep_len:538 episode reward: total was -70.450000. running mean: -77.748650\n",
      "ep 202: ep_len:559 episode reward: total was -119.530000. running mean: -78.166464\n",
      "epsilon:0.191000 episode_count: 1421. steps_count: 628439.000000\n",
      "Time elapsed:  1758.0292139053345\n",
      "ep 203: ep_len:657 episode reward: total was -151.160000. running mean: -78.896399\n",
      "ep 203: ep_len:500 episode reward: total was -25.000000. running mean: -78.357435\n",
      "ep 203: ep_len:568 episode reward: total was -102.960000. running mean: -78.603461\n",
      "ep 203: ep_len:500 episode reward: total was -22.500000. running mean: -78.042426\n",
      "ep 203: ep_len:3 episode reward: total was -1.500000. running mean: -77.277002\n",
      "ep 203: ep_len:628 episode reward: total was -78.560000. running mean: -77.289832\n",
      "ep 203: ep_len:612 episode reward: total was -48.610000. running mean: -77.003034\n",
      "epsilon:0.190956 episode_count: 1428. steps_count: 631907.000000\n",
      "Time elapsed:  1767.1738839149475\n",
      "ep 204: ep_len:522 episode reward: total was -84.290000. running mean: -77.075903\n",
      "ep 204: ep_len:370 episode reward: total was -112.800000. running mean: -77.433144\n",
      "ep 204: ep_len:619 episode reward: total was -95.070000. running mean: -77.609513\n",
      "ep 204: ep_len:500 episode reward: total was -39.840000. running mean: -77.231818\n",
      "ep 204: ep_len:3 episode reward: total was 0.000000. running mean: -76.459499\n",
      "ep 204: ep_len:500 episode reward: total was -151.790000. running mean: -77.212804\n",
      "ep 204: ep_len:511 episode reward: total was -118.230000. running mean: -77.622976\n",
      "epsilon:0.190912 episode_count: 1435. steps_count: 634932.000000\n",
      "Time elapsed:  1775.3461606502533\n",
      "ep 205: ep_len:500 episode reward: total was -87.850000. running mean: -77.725247\n",
      "ep 205: ep_len:687 episode reward: total was -133.470000. running mean: -78.282694\n",
      "ep 205: ep_len:55 episode reward: total was -1.820000. running mean: -77.518067\n",
      "ep 205: ep_len:428 episode reward: total was -44.540000. running mean: -77.188287\n",
      "ep 205: ep_len:3 episode reward: total was 0.000000. running mean: -76.416404\n",
      "ep 205: ep_len:574 episode reward: total was -146.010000. running mean: -77.112340\n",
      "ep 205: ep_len:609 episode reward: total was -202.120000. running mean: -78.362416\n",
      "epsilon:0.190867 episode_count: 1442. steps_count: 637788.000000\n",
      "Time elapsed:  1782.743143081665\n",
      "ep 206: ep_len:500 episode reward: total was -69.390000. running mean: -78.272692\n",
      "ep 206: ep_len:505 episode reward: total was -111.120000. running mean: -78.601165\n",
      "ep 206: ep_len:577 episode reward: total was -135.890000. running mean: -79.174054\n",
      "ep 206: ep_len:500 episode reward: total was -80.960000. running mean: -79.191913\n",
      "ep 206: ep_len:3 episode reward: total was 0.000000. running mean: -78.399994\n",
      "ep 206: ep_len:505 episode reward: total was -59.560000. running mean: -78.211594\n",
      "ep 206: ep_len:557 episode reward: total was -75.110000. running mean: -78.180578\n",
      "epsilon:0.190823 episode_count: 1449. steps_count: 640935.000000\n",
      "Time elapsed:  1790.960577249527\n",
      "ep 207: ep_len:235 episode reward: total was -35.910000. running mean: -77.757872\n",
      "ep 207: ep_len:500 episode reward: total was -171.860000. running mean: -78.698893\n",
      "ep 207: ep_len:585 episode reward: total was -107.090000. running mean: -78.982805\n",
      "ep 207: ep_len:526 episode reward: total was -57.960000. running mean: -78.772577\n",
      "ep 207: ep_len:3 episode reward: total was -1.500000. running mean: -77.999851\n",
      "ep 207: ep_len:563 episode reward: total was -129.080000. running mean: -78.510652\n",
      "ep 207: ep_len:546 episode reward: total was -190.250000. running mean: -79.628046\n",
      "epsilon:0.190779 episode_count: 1456. steps_count: 643893.000000\n",
      "Time elapsed:  1798.8747646808624\n",
      "ep 208: ep_len:530 episode reward: total was -110.660000. running mean: -79.938365\n",
      "ep 208: ep_len:352 episode reward: total was -51.800000. running mean: -79.656982\n",
      "ep 208: ep_len:538 episode reward: total was -72.200000. running mean: -79.582412\n",
      "ep 208: ep_len:132 episode reward: total was -1.980000. running mean: -78.806388\n",
      "ep 208: ep_len:3 episode reward: total was 0.000000. running mean: -78.018324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 208: ep_len:500 episode reward: total was -51.100000. running mean: -77.749141\n",
      "ep 208: ep_len:540 episode reward: total was -105.650000. running mean: -78.028149\n",
      "epsilon:0.190734 episode_count: 1463. steps_count: 646488.000000\n",
      "Time elapsed:  1805.9149515628815\n",
      "ep 209: ep_len:594 episode reward: total was -92.000000. running mean: -78.167868\n",
      "ep 209: ep_len:279 episode reward: total was -73.030000. running mean: -78.116489\n",
      "ep 209: ep_len:687 episode reward: total was -137.630000. running mean: -78.711624\n",
      "ep 209: ep_len:363 episode reward: total was -110.530000. running mean: -79.029808\n",
      "ep 209: ep_len:3 episode reward: total was 0.000000. running mean: -78.239510\n",
      "ep 209: ep_len:539 episode reward: total was -102.510000. running mean: -78.482215\n",
      "ep 209: ep_len:516 episode reward: total was -80.520000. running mean: -78.502593\n",
      "epsilon:0.190690 episode_count: 1470. steps_count: 649469.000000\n",
      "Time elapsed:  1813.774399280548\n",
      "ep 210: ep_len:228 episode reward: total was -23.260000. running mean: -77.950167\n",
      "ep 210: ep_len:571 episode reward: total was -76.800000. running mean: -77.938665\n",
      "ep 210: ep_len:533 episode reward: total was -72.150000. running mean: -77.880778\n",
      "ep 210: ep_len:501 episode reward: total was -87.410000. running mean: -77.976071\n",
      "ep 210: ep_len:3 episode reward: total was 0.000000. running mean: -77.196310\n",
      "ep 210: ep_len:500 episode reward: total was -57.560000. running mean: -76.999947\n",
      "ep 210: ep_len:572 episode reward: total was -75.830000. running mean: -76.988247\n",
      "epsilon:0.190646 episode_count: 1477. steps_count: 652377.000000\n",
      "Time elapsed:  1821.8612534999847\n",
      "ep 211: ep_len:564 episode reward: total was -120.460000. running mean: -77.422965\n",
      "ep 211: ep_len:508 episode reward: total was -106.670000. running mean: -77.715435\n",
      "ep 211: ep_len:457 episode reward: total was -60.910000. running mean: -77.547381\n",
      "ep 211: ep_len:501 episode reward: total was -156.130000. running mean: -78.333207\n",
      "ep 211: ep_len:74 episode reward: total was -27.370000. running mean: -77.823575\n",
      "ep 211: ep_len:507 episode reward: total was -159.110000. running mean: -78.636439\n",
      "ep 211: ep_len:500 episode reward: total was -94.620000. running mean: -78.796275\n",
      "epsilon:0.190601 episode_count: 1484. steps_count: 655488.000000\n",
      "Time elapsed:  1830.145889043808\n",
      "ep 212: ep_len:593 episode reward: total was -111.780000. running mean: -79.126112\n",
      "ep 212: ep_len:278 episode reward: total was -22.470000. running mean: -78.559551\n",
      "ep 212: ep_len:507 episode reward: total was -167.740000. running mean: -79.451355\n",
      "ep 212: ep_len:524 episode reward: total was -59.730000. running mean: -79.254142\n",
      "ep 212: ep_len:3 episode reward: total was -3.000000. running mean: -78.491600\n",
      "ep 212: ep_len:550 episode reward: total was -140.890000. running mean: -79.115584\n",
      "ep 212: ep_len:302 episode reward: total was -99.950000. running mean: -79.323929\n",
      "epsilon:0.190557 episode_count: 1491. steps_count: 658245.000000\n",
      "Time elapsed:  1837.4969747066498\n",
      "ep 213: ep_len:639 episode reward: total was -127.410000. running mean: -79.804789\n",
      "ep 213: ep_len:521 episode reward: total was -130.130000. running mean: -80.308041\n",
      "ep 213: ep_len:542 episode reward: total was -80.130000. running mean: -80.306261\n",
      "ep 213: ep_len:570 episode reward: total was -63.370000. running mean: -80.136898\n",
      "ep 213: ep_len:3 episode reward: total was 0.000000. running mean: -79.335529\n",
      "ep 213: ep_len:509 episode reward: total was -87.550000. running mean: -79.417674\n",
      "ep 213: ep_len:500 episode reward: total was -210.970000. running mean: -80.733197\n",
      "epsilon:0.190513 episode_count: 1498. steps_count: 661529.000000\n",
      "Time elapsed:  1846.2528800964355\n",
      "ep 214: ep_len:538 episode reward: total was -98.420000. running mean: -80.910065\n",
      "ep 214: ep_len:500 episode reward: total was 3.710000. running mean: -80.063865\n",
      "ep 214: ep_len:500 episode reward: total was -89.000000. running mean: -80.153226\n",
      "ep 214: ep_len:601 episode reward: total was -47.360000. running mean: -79.825294\n",
      "ep 214: ep_len:3 episode reward: total was 0.000000. running mean: -79.027041\n",
      "ep 214: ep_len:535 episode reward: total was -62.400000. running mean: -78.860770\n",
      "ep 214: ep_len:500 episode reward: total was -107.390000. running mean: -79.146063\n",
      "epsilon:0.190468 episode_count: 1505. steps_count: 664706.000000\n",
      "Time elapsed:  1854.605990409851\n",
      "ep 215: ep_len:542 episode reward: total was -152.790000. running mean: -79.882502\n",
      "ep 215: ep_len:591 episode reward: total was -151.850000. running mean: -80.602177\n",
      "ep 215: ep_len:576 episode reward: total was -298.680000. running mean: -82.782955\n",
      "ep 215: ep_len:602 episode reward: total was -38.410000. running mean: -82.339226\n",
      "ep 215: ep_len:87 episode reward: total was 1.660000. running mean: -81.499234\n",
      "ep 215: ep_len:603 episode reward: total was -136.200000. running mean: -82.046241\n",
      "ep 215: ep_len:208 episode reward: total was -39.470000. running mean: -81.620479\n",
      "epsilon:0.190424 episode_count: 1512. steps_count: 667915.000000\n",
      "Time elapsed:  1863.0588281154633\n",
      "ep 216: ep_len:570 episode reward: total was -82.750000. running mean: -81.631774\n",
      "ep 216: ep_len:516 episode reward: total was -105.580000. running mean: -81.871256\n",
      "ep 216: ep_len:556 episode reward: total was -195.620000. running mean: -83.008744\n",
      "ep 216: ep_len:520 episode reward: total was -136.740000. running mean: -83.546056\n",
      "ep 216: ep_len:3 episode reward: total was -1.500000. running mean: -82.725596\n",
      "ep 216: ep_len:552 episode reward: total was -148.030000. running mean: -83.378640\n",
      "ep 216: ep_len:628 episode reward: total was -92.490000. running mean: -83.469753\n",
      "epsilon:0.190380 episode_count: 1519. steps_count: 671260.000000\n",
      "Time elapsed:  1872.0257019996643\n",
      "ep 217: ep_len:600 episode reward: total was -63.020000. running mean: -83.265256\n",
      "ep 217: ep_len:512 episode reward: total was -59.240000. running mean: -83.025003\n",
      "ep 217: ep_len:668 episode reward: total was -141.750000. running mean: -83.612253\n",
      "ep 217: ep_len:569 episode reward: total was -123.510000. running mean: -84.011231\n",
      "ep 217: ep_len:107 episode reward: total was -7.230000. running mean: -83.243418\n",
      "ep 217: ep_len:606 episode reward: total was -70.270000. running mean: -83.113684\n",
      "ep 217: ep_len:586 episode reward: total was -96.290000. running mean: -83.245447\n",
      "epsilon:0.190335 episode_count: 1526. steps_count: 674908.000000\n",
      "Time elapsed:  1881.7143216133118\n",
      "ep 218: ep_len:611 episode reward: total was -65.990000. running mean: -83.072893\n",
      "ep 218: ep_len:505 episode reward: total was -47.560000. running mean: -82.717764\n",
      "ep 218: ep_len:643 episode reward: total was -199.570000. running mean: -83.886286\n",
      "ep 218: ep_len:525 episode reward: total was -91.050000. running mean: -83.957923\n",
      "ep 218: ep_len:46 episode reward: total was 11.000000. running mean: -83.008344\n",
      "ep 218: ep_len:596 episode reward: total was -105.880000. running mean: -83.237061\n",
      "ep 218: ep_len:540 episode reward: total was -91.940000. running mean: -83.324090\n",
      "epsilon:0.190291 episode_count: 1533. steps_count: 678374.000000\n",
      "Time elapsed:  1890.316260099411\n",
      "ep 219: ep_len:591 episode reward: total was -107.940000. running mean: -83.570249\n",
      "ep 219: ep_len:640 episode reward: total was -40.180000. running mean: -83.136347\n",
      "ep 219: ep_len:505 episode reward: total was -46.380000. running mean: -82.768783\n",
      "ep 219: ep_len:500 episode reward: total was -44.400000. running mean: -82.385095\n",
      "ep 219: ep_len:3 episode reward: total was 0.000000. running mean: -81.561244\n",
      "ep 219: ep_len:591 episode reward: total was -82.820000. running mean: -81.573832\n",
      "ep 219: ep_len:577 episode reward: total was -81.210000. running mean: -81.570194\n",
      "epsilon:0.190247 episode_count: 1540. steps_count: 681781.000000\n",
      "Time elapsed:  1899.2545862197876\n",
      "ep 220: ep_len:548 episode reward: total was -115.190000. running mean: -81.906392\n",
      "ep 220: ep_len:510 episode reward: total was -79.450000. running mean: -81.881828\n",
      "ep 220: ep_len:580 episode reward: total was -111.570000. running mean: -82.178710\n",
      "ep 220: ep_len:614 episode reward: total was -72.780000. running mean: -82.084723\n",
      "ep 220: ep_len:3 episode reward: total was -1.500000. running mean: -81.278875\n",
      "ep 220: ep_len:500 episode reward: total was -125.240000. running mean: -81.718487\n",
      "ep 220: ep_len:528 episode reward: total was -91.350000. running mean: -81.814802\n",
      "Initial position:  [  2   0  17  87  63 149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:0.190202 episode_count: 1547. steps_count: 685064.000000\n",
      "Time elapsed:  1912.6756377220154\n",
      "ep 221: ep_len:257 episode reward: total was -13.900000. running mean: -81.135654\n",
      "ep 221: ep_len:596 episode reward: total was -12.950000. running mean: -80.453797\n",
      "ep 221: ep_len:500 episode reward: total was -95.360000. running mean: -80.602859\n",
      "ep 221: ep_len:501 episode reward: total was -26.720000. running mean: -80.064031\n",
      "ep 221: ep_len:121 episode reward: total was -59.220000. running mean: -79.855590\n",
      "ep 221: ep_len:624 episode reward: total was -131.570000. running mean: -80.372734\n",
      "ep 221: ep_len:315 episode reward: total was -78.720000. running mean: -80.356207\n",
      "epsilon:0.190158 episode_count: 1554. steps_count: 687978.000000\n",
      "Time elapsed:  1920.362148284912\n",
      "ep 222: ep_len:586 episode reward: total was -123.630000. running mean: -80.788945\n",
      "ep 222: ep_len:501 episode reward: total was -8.250000. running mean: -80.063555\n",
      "ep 222: ep_len:555 episode reward: total was -85.910000. running mean: -80.122020\n",
      "ep 222: ep_len:130 episode reward: total was 1.460000. running mean: -79.306200\n",
      "ep 222: ep_len:99 episode reward: total was -57.280000. running mean: -79.085938\n",
      "ep 222: ep_len:500 episode reward: total was -78.480000. running mean: -79.079878\n",
      "ep 222: ep_len:500 episode reward: total was -84.250000. running mean: -79.131580\n",
      "epsilon:0.190114 episode_count: 1561. steps_count: 690849.000000\n",
      "Time elapsed:  1928.1287205219269\n",
      "ep 223: ep_len:134 episode reward: total was -22.540000. running mean: -78.565664\n",
      "ep 223: ep_len:509 episode reward: total was -121.830000. running mean: -78.998307\n",
      "ep 223: ep_len:508 episode reward: total was -100.780000. running mean: -79.216124\n",
      "ep 223: ep_len:167 episode reward: total was -13.950000. running mean: -78.563463\n",
      "ep 223: ep_len:2 episode reward: total was -0.500000. running mean: -77.782828\n",
      "ep 223: ep_len:702 episode reward: total was -261.170000. running mean: -79.616700\n",
      "ep 223: ep_len:500 episode reward: total was -124.010000. running mean: -80.060633\n",
      "epsilon:0.190069 episode_count: 1568. steps_count: 693371.000000\n",
      "Time elapsed:  1935.0350031852722\n",
      "ep 224: ep_len:568 episode reward: total was -108.730000. running mean: -80.347327\n",
      "ep 224: ep_len:500 episode reward: total was -83.520000. running mean: -80.379053\n",
      "ep 224: ep_len:366 episode reward: total was -5.860000. running mean: -79.633863\n",
      "ep 224: ep_len:565 episode reward: total was -48.160000. running mean: -79.319124\n",
      "ep 224: ep_len:3 episode reward: total was -1.500000. running mean: -78.540933\n",
      "ep 224: ep_len:502 episode reward: total was -115.460000. running mean: -78.910124\n",
      "ep 224: ep_len:563 episode reward: total was -104.190000. running mean: -79.162922\n",
      "epsilon:0.190025 episode_count: 1575. steps_count: 696438.000000\n",
      "Time elapsed:  1943.2289457321167\n",
      "ep 225: ep_len:634 episode reward: total was -146.680000. running mean: -79.838093\n",
      "ep 225: ep_len:500 episode reward: total was -58.060000. running mean: -79.620312\n",
      "ep 225: ep_len:573 episode reward: total was -86.120000. running mean: -79.685309\n",
      "ep 225: ep_len:569 episode reward: total was -77.140000. running mean: -79.659856\n",
      "ep 225: ep_len:109 episode reward: total was -55.770000. running mean: -79.420957\n",
      "ep 225: ep_len:500 episode reward: total was -45.600000. running mean: -79.082748\n",
      "ep 225: ep_len:506 episode reward: total was -65.800000. running mean: -78.949920\n",
      "epsilon:0.189981 episode_count: 1582. steps_count: 699829.000000\n",
      "Time elapsed:  1952.155309677124\n",
      "ep 226: ep_len:229 episode reward: total was -21.840000. running mean: -78.378821\n",
      "ep 226: ep_len:596 episode reward: total was -64.810000. running mean: -78.243133\n",
      "ep 226: ep_len:578 episode reward: total was -73.170000. running mean: -78.192402\n",
      "ep 226: ep_len:502 episode reward: total was -105.570000. running mean: -78.466178\n",
      "ep 226: ep_len:3 episode reward: total was 0.000000. running mean: -77.681516\n",
      "ep 226: ep_len:592 episode reward: total was -76.020000. running mean: -77.664901\n",
      "ep 226: ep_len:623 episode reward: total was -88.540000. running mean: -77.773652\n",
      "epsilon:0.189936 episode_count: 1589. steps_count: 702952.000000\n",
      "Time elapsed:  1960.5128967761993\n",
      "ep 227: ep_len:629 episode reward: total was -158.950000. running mean: -78.585415\n",
      "ep 227: ep_len:500 episode reward: total was -90.640000. running mean: -78.705961\n",
      "ep 227: ep_len:530 episode reward: total was -147.860000. running mean: -79.397501\n",
      "ep 227: ep_len:513 episode reward: total was -40.000000. running mean: -79.003526\n",
      "ep 227: ep_len:3 episode reward: total was 0.000000. running mean: -78.213491\n",
      "ep 227: ep_len:577 episode reward: total was -99.590000. running mean: -78.427256\n",
      "ep 227: ep_len:547 episode reward: total was -81.100000. running mean: -78.453984\n",
      "epsilon:0.189892 episode_count: 1596. steps_count: 706251.000000\n",
      "Time elapsed:  1969.1404218673706\n",
      "ep 228: ep_len:611 episode reward: total was -135.880000. running mean: -79.028244\n",
      "ep 228: ep_len:536 episode reward: total was -59.970000. running mean: -78.837661\n",
      "ep 228: ep_len:576 episode reward: total was -102.260000. running mean: -79.071885\n",
      "ep 228: ep_len:502 episode reward: total was -130.640000. running mean: -79.587566\n",
      "ep 228: ep_len:112 episode reward: total was 5.250000. running mean: -78.739190\n",
      "ep 228: ep_len:500 episode reward: total was -93.970000. running mean: -78.891498\n",
      "ep 228: ep_len:563 episode reward: total was -89.310000. running mean: -78.995683\n",
      "epsilon:0.189848 episode_count: 1603. steps_count: 709651.000000\n",
      "Time elapsed:  1978.2614011764526\n",
      "ep 229: ep_len:637 episode reward: total was -145.740000. running mean: -79.663127\n",
      "ep 229: ep_len:615 episode reward: total was -107.590000. running mean: -79.942395\n",
      "ep 229: ep_len:508 episode reward: total was -143.380000. running mean: -80.576771\n",
      "ep 229: ep_len:589 episode reward: total was -36.400000. running mean: -80.135004\n",
      "ep 229: ep_len:3 episode reward: total was 0.000000. running mean: -79.333654\n",
      "ep 229: ep_len:500 episode reward: total was -67.110000. running mean: -79.211417\n",
      "ep 229: ep_len:578 episode reward: total was -90.750000. running mean: -79.326803\n",
      "epsilon:0.189803 episode_count: 1610. steps_count: 713081.000000\n",
      "Time elapsed:  1987.3836879730225\n",
      "ep 230: ep_len:588 episode reward: total was -86.830000. running mean: -79.401835\n",
      "ep 230: ep_len:553 episode reward: total was -190.710000. running mean: -80.514916\n",
      "ep 230: ep_len:650 episode reward: total was -197.510000. running mean: -81.684867\n",
      "ep 230: ep_len:500 episode reward: total was -154.200000. running mean: -82.410019\n",
      "ep 230: ep_len:3 episode reward: total was 0.000000. running mean: -81.585918\n",
      "ep 230: ep_len:500 episode reward: total was -60.350000. running mean: -81.373559\n",
      "ep 230: ep_len:551 episode reward: total was -147.440000. running mean: -82.034224\n",
      "epsilon:0.189759 episode_count: 1617. steps_count: 716426.000000\n",
      "Time elapsed:  1996.4905560016632\n",
      "ep 231: ep_len:500 episode reward: total was -126.320000. running mean: -82.477081\n",
      "ep 231: ep_len:574 episode reward: total was -51.060000. running mean: -82.162911\n",
      "ep 231: ep_len:79 episode reward: total was -8.310000. running mean: -81.424382\n",
      "ep 231: ep_len:500 episode reward: total was -102.300000. running mean: -81.633138\n",
      "ep 231: ep_len:3 episode reward: total was 0.000000. running mean: -80.816806\n",
      "ep 231: ep_len:506 episode reward: total was -97.730000. running mean: -80.985938\n",
      "ep 231: ep_len:512 episode reward: total was -77.430000. running mean: -80.950379\n",
      "epsilon:0.189715 episode_count: 1624. steps_count: 719100.000000\n",
      "Time elapsed:  2003.8748285770416\n",
      "ep 232: ep_len:617 episode reward: total was -69.320000. running mean: -80.834075\n",
      "ep 232: ep_len:500 episode reward: total was -200.210000. running mean: -82.027834\n",
      "ep 232: ep_len:526 episode reward: total was -103.640000. running mean: -82.243956\n",
      "ep 232: ep_len:500 episode reward: total was -115.590000. running mean: -82.577416\n",
      "ep 232: ep_len:3 episode reward: total was 0.000000. running mean: -81.751642\n",
      "ep 232: ep_len:667 episode reward: total was -92.510000. running mean: -81.859226\n",
      "ep 232: ep_len:623 episode reward: total was -67.910000. running mean: -81.719734\n",
      "epsilon:0.189670 episode_count: 1631. steps_count: 722536.000000\n",
      "Time elapsed:  2012.9982633590698\n",
      "ep 233: ep_len:213 episode reward: total was -14.320000. running mean: -81.045736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 233: ep_len:546 episode reward: total was -105.710000. running mean: -81.292379\n",
      "ep 233: ep_len:568 episode reward: total was -146.870000. running mean: -81.948155\n",
      "ep 233: ep_len:500 episode reward: total was -36.150000. running mean: -81.490174\n",
      "ep 233: ep_len:82 episode reward: total was -40.760000. running mean: -81.082872\n",
      "ep 233: ep_len:186 episode reward: total was -28.040000. running mean: -80.552443\n",
      "ep 233: ep_len:210 episode reward: total was -53.720000. running mean: -80.284119\n",
      "epsilon:0.189626 episode_count: 1638. steps_count: 724841.000000\n",
      "Time elapsed:  2019.456460237503\n",
      "ep 234: ep_len:558 episode reward: total was -37.130000. running mean: -79.852577\n",
      "ep 234: ep_len:577 episode reward: total was -150.810000. running mean: -80.562152\n",
      "ep 234: ep_len:506 episode reward: total was -99.890000. running mean: -80.755430\n",
      "ep 234: ep_len:500 episode reward: total was 12.830000. running mean: -79.819576\n",
      "ep 234: ep_len:3 episode reward: total was -1.500000. running mean: -79.036380\n",
      "ep 234: ep_len:508 episode reward: total was -141.180000. running mean: -79.657816\n",
      "ep 234: ep_len:558 episode reward: total was -90.320000. running mean: -79.764438\n",
      "epsilon:0.189582 episode_count: 1645. steps_count: 728051.000000\n",
      "Time elapsed:  2027.9973528385162\n",
      "ep 235: ep_len:628 episode reward: total was -75.400000. running mean: -79.720794\n",
      "ep 235: ep_len:528 episode reward: total was -53.960000. running mean: -79.463186\n",
      "ep 235: ep_len:632 episode reward: total was -127.660000. running mean: -79.945154\n",
      "ep 235: ep_len:520 episode reward: total was -105.860000. running mean: -80.204302\n",
      "ep 235: ep_len:47 episode reward: total was 11.500000. running mean: -79.287259\n",
      "ep 235: ep_len:537 episode reward: total was -139.110000. running mean: -79.885487\n",
      "ep 235: ep_len:207 episode reward: total was -76.430000. running mean: -79.850932\n",
      "epsilon:0.189537 episode_count: 1652. steps_count: 731150.000000\n",
      "Time elapsed:  2036.2831718921661\n",
      "ep 236: ep_len:578 episode reward: total was -79.620000. running mean: -79.848623\n",
      "ep 236: ep_len:518 episode reward: total was -132.490000. running mean: -80.375036\n",
      "ep 236: ep_len:645 episode reward: total was -59.310000. running mean: -80.164386\n",
      "ep 236: ep_len:515 episode reward: total was -79.100000. running mean: -80.153742\n",
      "ep 236: ep_len:3 episode reward: total was 0.000000. running mean: -79.352205\n",
      "ep 236: ep_len:542 episode reward: total was -88.370000. running mean: -79.442383\n",
      "ep 236: ep_len:595 episode reward: total was -122.390000. running mean: -79.871859\n",
      "epsilon:0.189493 episode_count: 1659. steps_count: 734546.000000\n",
      "Time elapsed:  2045.256709098816\n",
      "ep 237: ep_len:548 episode reward: total was -128.720000. running mean: -80.360340\n",
      "ep 237: ep_len:516 episode reward: total was -125.040000. running mean: -80.807137\n",
      "ep 237: ep_len:519 episode reward: total was -83.770000. running mean: -80.836766\n",
      "ep 237: ep_len:505 episode reward: total was -144.450000. running mean: -81.472898\n",
      "ep 237: ep_len:3 episode reward: total was 0.000000. running mean: -80.658169\n",
      "ep 237: ep_len:510 episode reward: total was -104.880000. running mean: -80.900387\n",
      "ep 237: ep_len:502 episode reward: total was -113.270000. running mean: -81.224083\n",
      "epsilon:0.189449 episode_count: 1666. steps_count: 737649.000000\n",
      "Time elapsed:  2053.826056241989\n",
      "ep 238: ep_len:265 episode reward: total was -27.800000. running mean: -80.689842\n",
      "ep 238: ep_len:559 episode reward: total was -89.370000. running mean: -80.776644\n",
      "ep 238: ep_len:516 episode reward: total was -114.890000. running mean: -81.117778\n",
      "ep 238: ep_len:500 episode reward: total was -61.240000. running mean: -80.919000\n",
      "ep 238: ep_len:3 episode reward: total was -1.500000. running mean: -80.124810\n",
      "ep 238: ep_len:501 episode reward: total was -84.650000. running mean: -80.170062\n",
      "ep 238: ep_len:636 episode reward: total was -95.600000. running mean: -80.324361\n",
      "epsilon:0.189404 episode_count: 1673. steps_count: 740629.000000\n",
      "Time elapsed:  2062.1324410438538\n",
      "ep 239: ep_len:641 episode reward: total was -101.960000. running mean: -80.540718\n",
      "ep 239: ep_len:616 episode reward: total was -130.430000. running mean: -81.039610\n",
      "ep 239: ep_len:383 episode reward: total was -42.250000. running mean: -80.651714\n",
      "ep 239: ep_len:518 episode reward: total was -28.530000. running mean: -80.130497\n",
      "ep 239: ep_len:3 episode reward: total was 0.000000. running mean: -79.329192\n",
      "ep 239: ep_len:599 episode reward: total was -96.660000. running mean: -79.502500\n",
      "ep 239: ep_len:174 episode reward: total was -40.660000. running mean: -79.114075\n",
      "epsilon:0.189360 episode_count: 1680. steps_count: 743563.000000\n",
      "Time elapsed:  2069.543800354004\n",
      "ep 240: ep_len:650 episode reward: total was -211.260000. running mean: -80.435534\n",
      "ep 240: ep_len:600 episode reward: total was -95.080000. running mean: -80.581979\n",
      "ep 240: ep_len:566 episode reward: total was -108.140000. running mean: -80.857559\n",
      "ep 240: ep_len:582 episode reward: total was -85.680000. running mean: -80.905784\n",
      "ep 240: ep_len:128 episode reward: total was -62.220000. running mean: -80.718926\n",
      "ep 240: ep_len:501 episode reward: total was -138.500000. running mean: -81.296737\n",
      "ep 240: ep_len:606 episode reward: total was -153.770000. running mean: -82.021469\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.189316 episode_count: 1687. steps_count: 747196.000000\n",
      "Time elapsed:  2083.584831237793\n",
      "ep 241: ep_len:574 episode reward: total was -81.620000. running mean: -82.017455\n",
      "ep 241: ep_len:595 episode reward: total was -65.400000. running mean: -81.851280\n",
      "ep 241: ep_len:516 episode reward: total was -57.800000. running mean: -81.610767\n",
      "ep 241: ep_len:625 episode reward: total was -72.330000. running mean: -81.517960\n",
      "ep 241: ep_len:3 episode reward: total was 0.000000. running mean: -80.702780\n",
      "ep 241: ep_len:515 episode reward: total was -132.250000. running mean: -81.218252\n",
      "ep 241: ep_len:516 episode reward: total was -137.420000. running mean: -81.780270\n",
      "epsilon:0.189271 episode_count: 1694. steps_count: 750540.000000\n",
      "Time elapsed:  2092.4604058265686\n",
      "ep 242: ep_len:619 episode reward: total was -128.540000. running mean: -82.247867\n",
      "ep 242: ep_len:550 episode reward: total was -149.090000. running mean: -82.916288\n",
      "ep 242: ep_len:542 episode reward: total was -120.950000. running mean: -83.296625\n",
      "ep 242: ep_len:500 episode reward: total was -49.390000. running mean: -82.957559\n",
      "ep 242: ep_len:3 episode reward: total was 0.000000. running mean: -82.127984\n",
      "ep 242: ep_len:604 episode reward: total was -147.840000. running mean: -82.785104\n",
      "ep 242: ep_len:502 episode reward: total was -47.140000. running mean: -82.428653\n",
      "epsilon:0.189227 episode_count: 1701. steps_count: 753860.000000\n",
      "Time elapsed:  2101.3853368759155\n",
      "ep 243: ep_len:576 episode reward: total was -86.650000. running mean: -82.470866\n",
      "ep 243: ep_len:559 episode reward: total was -109.610000. running mean: -82.742257\n",
      "ep 243: ep_len:602 episode reward: total was -149.860000. running mean: -83.413435\n",
      "ep 243: ep_len:500 episode reward: total was -74.430000. running mean: -83.323601\n",
      "ep 243: ep_len:3 episode reward: total was 0.000000. running mean: -82.490365\n",
      "ep 243: ep_len:529 episode reward: total was -88.730000. running mean: -82.552761\n",
      "ep 243: ep_len:569 episode reward: total was -81.650000. running mean: -82.543733\n",
      "epsilon:0.189183 episode_count: 1708. steps_count: 757198.000000\n",
      "Time elapsed:  2110.2439630031586\n",
      "ep 244: ep_len:500 episode reward: total was -67.710000. running mean: -82.395396\n",
      "ep 244: ep_len:640 episode reward: total was -266.750000. running mean: -84.238942\n",
      "ep 244: ep_len:615 episode reward: total was -102.030000. running mean: -84.416853\n",
      "ep 244: ep_len:56 episode reward: total was -16.350000. running mean: -83.736184\n",
      "ep 244: ep_len:126 episode reward: total was -6.730000. running mean: -82.966122\n",
      "ep 244: ep_len:185 episode reward: total was -27.550000. running mean: -82.411961\n",
      "ep 244: ep_len:506 episode reward: total was -88.820000. running mean: -82.476041\n",
      "epsilon:0.189138 episode_count: 1715. steps_count: 759826.000000\n",
      "Time elapsed:  2117.9501740932465\n",
      "ep 245: ep_len:500 episode reward: total was -34.140000. running mean: -81.992681\n",
      "ep 245: ep_len:607 episode reward: total was -136.410000. running mean: -82.536854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 245: ep_len:629 episode reward: total was -117.230000. running mean: -82.883786\n",
      "ep 245: ep_len:169 episode reward: total was 2.070000. running mean: -82.034248\n",
      "ep 245: ep_len:3 episode reward: total was 0.000000. running mean: -81.213905\n",
      "ep 245: ep_len:590 episode reward: total was -178.110000. running mean: -82.182866\n",
      "ep 245: ep_len:573 episode reward: total was -152.520000. running mean: -82.886238\n",
      "epsilon:0.189094 episode_count: 1722. steps_count: 762897.000000\n",
      "Time elapsed:  2126.3503546714783\n",
      "ep 246: ep_len:537 episode reward: total was -83.460000. running mean: -82.891975\n",
      "ep 246: ep_len:181 episode reward: total was -56.940000. running mean: -82.632455\n",
      "ep 246: ep_len:545 episode reward: total was -76.860000. running mean: -82.574731\n",
      "ep 246: ep_len:510 episode reward: total was -67.570000. running mean: -82.424684\n",
      "ep 246: ep_len:94 episode reward: total was -30.290000. running mean: -81.903337\n",
      "ep 246: ep_len:535 episode reward: total was -56.580000. running mean: -81.650103\n",
      "ep 246: ep_len:520 episode reward: total was -104.840000. running mean: -81.882002\n",
      "epsilon:0.189050 episode_count: 1729. steps_count: 765819.000000\n",
      "Time elapsed:  2134.179770708084\n",
      "ep 247: ep_len:559 episode reward: total was -108.740000. running mean: -82.150582\n",
      "ep 247: ep_len:501 episode reward: total was -87.620000. running mean: -82.205277\n",
      "ep 247: ep_len:548 episode reward: total was -83.230000. running mean: -82.215524\n",
      "ep 247: ep_len:500 episode reward: total was -39.210000. running mean: -81.785469\n",
      "ep 247: ep_len:3 episode reward: total was 0.000000. running mean: -80.967614\n",
      "ep 247: ep_len:671 episode reward: total was -101.850000. running mean: -81.176438\n",
      "ep 247: ep_len:280 episode reward: total was -82.840000. running mean: -81.193073\n",
      "epsilon:0.189005 episode_count: 1736. steps_count: 768881.000000\n",
      "Time elapsed:  2142.5455570220947\n",
      "ep 248: ep_len:233 episode reward: total was -21.360000. running mean: -80.594743\n",
      "ep 248: ep_len:574 episode reward: total was -158.140000. running mean: -81.370195\n",
      "ep 248: ep_len:539 episode reward: total was -175.160000. running mean: -82.308093\n",
      "ep 248: ep_len:614 episode reward: total was -39.420000. running mean: -81.879212\n",
      "ep 248: ep_len:109 episode reward: total was -49.300000. running mean: -81.553420\n",
      "ep 248: ep_len:519 episode reward: total was -123.080000. running mean: -81.968686\n",
      "ep 248: ep_len:510 episode reward: total was -137.830000. running mean: -82.527299\n",
      "epsilon:0.188961 episode_count: 1743. steps_count: 771979.000000\n",
      "Time elapsed:  2151.0742180347443\n",
      "ep 249: ep_len:534 episode reward: total was -93.910000. running mean: -82.641126\n",
      "ep 249: ep_len:530 episode reward: total was -184.930000. running mean: -83.664015\n",
      "ep 249: ep_len:572 episode reward: total was -129.540000. running mean: -84.122775\n",
      "ep 249: ep_len:525 episode reward: total was -87.320000. running mean: -84.154747\n",
      "ep 249: ep_len:126 episode reward: total was -59.140000. running mean: -83.904599\n",
      "ep 249: ep_len:261 episode reward: total was -22.210000. running mean: -83.287653\n",
      "ep 249: ep_len:558 episode reward: total was -78.570000. running mean: -83.240477\n",
      "epsilon:0.188917 episode_count: 1750. steps_count: 775085.000000\n",
      "Time elapsed:  2159.386048078537\n",
      "ep 250: ep_len:233 episode reward: total was -30.420000. running mean: -82.712272\n",
      "ep 250: ep_len:606 episode reward: total was -111.940000. running mean: -83.004549\n",
      "ep 250: ep_len:548 episode reward: total was -77.130000. running mean: -82.945804\n",
      "ep 250: ep_len:500 episode reward: total was -109.080000. running mean: -83.207146\n",
      "ep 250: ep_len:52 episode reward: total was 12.500000. running mean: -82.250074\n",
      "ep 250: ep_len:500 episode reward: total was -88.820000. running mean: -82.315774\n",
      "ep 250: ep_len:593 episode reward: total was -88.790000. running mean: -82.380516\n",
      "epsilon:0.188872 episode_count: 1757. steps_count: 778117.000000\n",
      "Time elapsed:  2167.9648098945618\n",
      "ep 251: ep_len:180 episode reward: total was -39.070000. running mean: -81.947411\n",
      "ep 251: ep_len:688 episode reward: total was -190.840000. running mean: -83.036337\n",
      "ep 251: ep_len:361 episode reward: total was -55.930000. running mean: -82.765273\n",
      "ep 251: ep_len:539 episode reward: total was -46.530000. running mean: -82.402921\n",
      "ep 251: ep_len:104 episode reward: total was 14.200000. running mean: -81.436891\n",
      "ep 251: ep_len:580 episode reward: total was -137.690000. running mean: -81.999422\n",
      "ep 251: ep_len:500 episode reward: total was -91.970000. running mean: -82.099128\n",
      "epsilon:0.188828 episode_count: 1764. steps_count: 781069.000000\n",
      "Time elapsed:  2177.374752998352\n",
      "ep 252: ep_len:586 episode reward: total was -62.180000. running mean: -81.899937\n",
      "ep 252: ep_len:500 episode reward: total was -8.980000. running mean: -81.170738\n",
      "ep 252: ep_len:500 episode reward: total was -98.410000. running mean: -81.343130\n",
      "ep 252: ep_len:500 episode reward: total was -128.550000. running mean: -81.815199\n",
      "ep 252: ep_len:3 episode reward: total was -1.500000. running mean: -81.012047\n",
      "ep 252: ep_len:523 episode reward: total was -59.800000. running mean: -80.799926\n",
      "ep 252: ep_len:595 episode reward: total was -168.370000. running mean: -81.675627\n",
      "epsilon:0.188784 episode_count: 1771. steps_count: 784276.000000\n",
      "Time elapsed:  2187.0411694049835\n",
      "ep 253: ep_len:125 episode reward: total was -11.000000. running mean: -80.968871\n",
      "ep 253: ep_len:516 episode reward: total was -110.390000. running mean: -81.263082\n",
      "ep 253: ep_len:597 episode reward: total was -105.090000. running mean: -81.501351\n",
      "ep 253: ep_len:619 episode reward: total was -57.270000. running mean: -81.259038\n",
      "ep 253: ep_len:3 episode reward: total was 0.000000. running mean: -80.446448\n",
      "ep 253: ep_len:507 episode reward: total was -81.260000. running mean: -80.454583\n",
      "ep 253: ep_len:611 episode reward: total was -76.020000. running mean: -80.410237\n",
      "epsilon:0.188739 episode_count: 1778. steps_count: 787254.000000\n",
      "Time elapsed:  2196.032175540924\n",
      "ep 254: ep_len:501 episode reward: total was -39.430000. running mean: -80.000435\n",
      "ep 254: ep_len:585 episode reward: total was -140.220000. running mean: -80.602630\n",
      "ep 254: ep_len:443 episode reward: total was -52.920000. running mean: -80.325804\n",
      "ep 254: ep_len:591 episode reward: total was -81.000000. running mean: -80.332546\n",
      "ep 254: ep_len:3 episode reward: total was 0.000000. running mean: -79.529221\n",
      "ep 254: ep_len:588 episode reward: total was -85.060000. running mean: -79.584528\n",
      "ep 254: ep_len:608 episode reward: total was -74.720000. running mean: -79.535883\n",
      "epsilon:0.188695 episode_count: 1785. steps_count: 790573.000000\n",
      "Time elapsed:  2205.9122335910797\n",
      "ep 255: ep_len:657 episode reward: total was -141.280000. running mean: -80.153324\n",
      "ep 255: ep_len:276 episode reward: total was -70.450000. running mean: -80.056291\n",
      "ep 255: ep_len:500 episode reward: total was -29.700000. running mean: -79.552728\n",
      "ep 255: ep_len:591 episode reward: total was -73.220000. running mean: -79.489401\n",
      "ep 255: ep_len:3 episode reward: total was 0.000000. running mean: -78.694507\n",
      "ep 255: ep_len:679 episode reward: total was -125.350000. running mean: -79.161062\n",
      "ep 255: ep_len:534 episode reward: total was -101.940000. running mean: -79.388851\n",
      "epsilon:0.188651 episode_count: 1792. steps_count: 793813.000000\n",
      "Time elapsed:  2216.137304544449\n",
      "ep 256: ep_len:537 episode reward: total was -128.970000. running mean: -79.884663\n",
      "ep 256: ep_len:579 episode reward: total was -102.320000. running mean: -80.109016\n",
      "ep 256: ep_len:631 episode reward: total was -77.350000. running mean: -80.081426\n",
      "ep 256: ep_len:500 episode reward: total was -54.550000. running mean: -79.826112\n",
      "ep 256: ep_len:3 episode reward: total was 0.000000. running mean: -79.027851\n",
      "ep 256: ep_len:502 episode reward: total was -141.100000. running mean: -79.648572\n",
      "ep 256: ep_len:500 episode reward: total was -72.370000. running mean: -79.575786\n",
      "epsilon:0.188606 episode_count: 1799. steps_count: 797065.000000\n",
      "Time elapsed:  2226.21937251091\n",
      "ep 257: ep_len:663 episode reward: total was -116.790000. running mean: -79.947928\n",
      "ep 257: ep_len:501 episode reward: total was -39.110000. running mean: -79.539549\n",
      "ep 257: ep_len:79 episode reward: total was -4.270000. running mean: -78.786854\n",
      "ep 257: ep_len:500 episode reward: total was -21.630000. running mean: -78.215285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 257: ep_len:98 episode reward: total was -3.800000. running mean: -77.471132\n",
      "ep 257: ep_len:172 episode reward: total was 6.100000. running mean: -76.635421\n",
      "ep 257: ep_len:196 episode reward: total was -59.740000. running mean: -76.466467\n",
      "epsilon:0.188562 episode_count: 1806. steps_count: 799274.000000\n",
      "Time elapsed:  2234.018864631653\n",
      "ep 258: ep_len:593 episode reward: total was -91.750000. running mean: -76.619302\n",
      "ep 258: ep_len:537 episode reward: total was -110.860000. running mean: -76.961709\n",
      "ep 258: ep_len:461 episode reward: total was -2.930000. running mean: -76.221392\n",
      "ep 258: ep_len:539 episode reward: total was -29.420000. running mean: -75.753378\n",
      "ep 258: ep_len:3 episode reward: total was 0.000000. running mean: -74.995844\n",
      "ep 258: ep_len:573 episode reward: total was -96.410000. running mean: -75.209986\n",
      "ep 258: ep_len:588 episode reward: total was -140.110000. running mean: -75.858986\n",
      "epsilon:0.188518 episode_count: 1813. steps_count: 802568.000000\n",
      "Time elapsed:  2244.292371749878\n",
      "ep 259: ep_len:609 episode reward: total was -175.950000. running mean: -76.859896\n",
      "ep 259: ep_len:602 episode reward: total was -119.910000. running mean: -77.290397\n",
      "ep 259: ep_len:622 episode reward: total was -96.330000. running mean: -77.480793\n",
      "ep 259: ep_len:502 episode reward: total was -21.030000. running mean: -76.916285\n",
      "ep 259: ep_len:3 episode reward: total was 0.000000. running mean: -76.147122\n",
      "ep 259: ep_len:649 episode reward: total was -85.400000. running mean: -76.239651\n",
      "ep 259: ep_len:572 episode reward: total was -95.720000. running mean: -76.434455\n",
      "epsilon:0.188473 episode_count: 1820. steps_count: 806127.000000\n",
      "Time elapsed:  2255.1535193920135\n",
      "ep 260: ep_len:500 episode reward: total was -160.730000. running mean: -77.277410\n",
      "ep 260: ep_len:515 episode reward: total was -157.380000. running mean: -78.078436\n",
      "ep 260: ep_len:583 episode reward: total was -143.200000. running mean: -78.729652\n",
      "ep 260: ep_len:517 episode reward: total was -115.470000. running mean: -79.097055\n",
      "ep 260: ep_len:81 episode reward: total was -2.750000. running mean: -78.333585\n",
      "ep 260: ep_len:541 episode reward: total was -96.530000. running mean: -78.515549\n",
      "ep 260: ep_len:278 episode reward: total was -66.220000. running mean: -78.392593\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.188429 episode_count: 1827. steps_count: 809142.000000\n",
      "Time elapsed:  2269.577799797058\n",
      "ep 261: ep_len:593 episode reward: total was -120.720000. running mean: -78.815867\n",
      "ep 261: ep_len:563 episode reward: total was -61.790000. running mean: -78.645609\n",
      "ep 261: ep_len:626 episode reward: total was -117.490000. running mean: -79.034053\n",
      "ep 261: ep_len:573 episode reward: total was -53.860000. running mean: -78.782312\n",
      "ep 261: ep_len:133 episode reward: total was -11.680000. running mean: -78.111289\n",
      "ep 261: ep_len:625 episode reward: total was -88.840000. running mean: -78.218576\n",
      "ep 261: ep_len:262 episode reward: total was -96.580000. running mean: -78.402190\n",
      "epsilon:0.188385 episode_count: 1834. steps_count: 812517.000000\n",
      "Time elapsed:  2280.0200095176697\n",
      "ep 262: ep_len:536 episode reward: total was -110.350000. running mean: -78.721668\n",
      "ep 262: ep_len:589 episode reward: total was -88.810000. running mean: -78.822552\n",
      "ep 262: ep_len:510 episode reward: total was -116.410000. running mean: -79.198426\n",
      "ep 262: ep_len:500 episode reward: total was -99.300000. running mean: -79.399442\n",
      "ep 262: ep_len:90 episode reward: total was 6.220000. running mean: -78.543247\n",
      "ep 262: ep_len:500 episode reward: total was -106.190000. running mean: -78.819715\n",
      "ep 262: ep_len:514 episode reward: total was -108.530000. running mean: -79.116818\n",
      "epsilon:0.188340 episode_count: 1841. steps_count: 815756.000000\n",
      "Time elapsed:  2289.7613055706024\n",
      "ep 263: ep_len:696 episode reward: total was -240.070000. running mean: -80.726350\n",
      "ep 263: ep_len:500 episode reward: total was -128.460000. running mean: -81.203686\n",
      "ep 263: ep_len:444 episode reward: total was -47.060000. running mean: -80.862249\n",
      "ep 263: ep_len:629 episode reward: total was -75.080000. running mean: -80.804427\n",
      "ep 263: ep_len:121 episode reward: total was -10.270000. running mean: -80.099083\n",
      "ep 263: ep_len:548 episode reward: total was -125.890000. running mean: -80.556992\n",
      "ep 263: ep_len:304 episode reward: total was -50.770000. running mean: -80.259122\n",
      "epsilon:0.188296 episode_count: 1848. steps_count: 818998.000000\n",
      "Time elapsed:  2299.6542253494263\n",
      "ep 264: ep_len:512 episode reward: total was -142.900000. running mean: -80.885531\n",
      "ep 264: ep_len:587 episode reward: total was -82.200000. running mean: -80.898675\n",
      "ep 264: ep_len:551 episode reward: total was -85.830000. running mean: -80.947989\n",
      "ep 264: ep_len:522 episode reward: total was -24.350000. running mean: -80.382009\n",
      "ep 264: ep_len:3 episode reward: total was 0.000000. running mean: -79.578189\n",
      "ep 264: ep_len:569 episode reward: total was -129.200000. running mean: -80.074407\n",
      "ep 264: ep_len:604 episode reward: total was -118.200000. running mean: -80.455663\n",
      "epsilon:0.188252 episode_count: 1855. steps_count: 822346.000000\n",
      "Time elapsed:  2309.911843061447\n",
      "ep 265: ep_len:584 episode reward: total was -126.250000. running mean: -80.913606\n",
      "ep 265: ep_len:170 episode reward: total was -47.410000. running mean: -80.578570\n",
      "ep 265: ep_len:585 episode reward: total was -100.040000. running mean: -80.773184\n",
      "ep 265: ep_len:578 episode reward: total was -141.320000. running mean: -81.378652\n",
      "ep 265: ep_len:128 episode reward: total was 13.310000. running mean: -80.431766\n",
      "ep 265: ep_len:541 episode reward: total was -168.130000. running mean: -81.308748\n",
      "ep 265: ep_len:650 episode reward: total was -100.640000. running mean: -81.502061\n",
      "epsilon:0.188207 episode_count: 1862. steps_count: 825582.000000\n",
      "Time elapsed:  2320.0228219032288\n",
      "ep 266: ep_len:205 episode reward: total was -21.960000. running mean: -80.906640\n",
      "ep 266: ep_len:511 episode reward: total was -71.860000. running mean: -80.816174\n",
      "ep 266: ep_len:552 episode reward: total was -125.710000. running mean: -81.265112\n",
      "ep 266: ep_len:508 episode reward: total was -113.870000. running mean: -81.591161\n",
      "ep 266: ep_len:3 episode reward: total was -1.500000. running mean: -80.790249\n",
      "ep 266: ep_len:629 episode reward: total was -118.590000. running mean: -81.168247\n",
      "ep 266: ep_len:541 episode reward: total was -189.930000. running mean: -82.255864\n",
      "epsilon:0.188163 episode_count: 1869. steps_count: 828531.000000\n",
      "Time elapsed:  2328.977449655533\n",
      "ep 267: ep_len:612 episode reward: total was -80.780000. running mean: -82.241106\n",
      "ep 267: ep_len:531 episode reward: total was -128.360000. running mean: -82.702295\n",
      "ep 267: ep_len:594 episode reward: total was -80.150000. running mean: -82.676772\n",
      "ep 267: ep_len:504 episode reward: total was -113.060000. running mean: -82.980604\n",
      "ep 267: ep_len:56 episode reward: total was -2.000000. running mean: -82.170798\n",
      "ep 267: ep_len:501 episode reward: total was -84.840000. running mean: -82.197490\n",
      "ep 267: ep_len:522 episode reward: total was -55.930000. running mean: -81.934815\n",
      "epsilon:0.188119 episode_count: 1876. steps_count: 831851.000000\n",
      "Time elapsed:  2339.1470975875854\n",
      "ep 268: ep_len:123 episode reward: total was -22.710000. running mean: -81.342567\n",
      "ep 268: ep_len:500 episode reward: total was -68.390000. running mean: -81.213041\n",
      "ep 268: ep_len:680 episode reward: total was -131.710000. running mean: -81.718011\n",
      "ep 268: ep_len:542 episode reward: total was -37.550000. running mean: -81.276331\n",
      "ep 268: ep_len:3 episode reward: total was 0.000000. running mean: -80.463567\n",
      "ep 268: ep_len:504 episode reward: total was -110.610000. running mean: -80.765032\n",
      "ep 268: ep_len:567 episode reward: total was -72.020000. running mean: -80.677581\n",
      "epsilon:0.188074 episode_count: 1883. steps_count: 834770.000000\n",
      "Time elapsed:  2348.0404930114746\n",
      "ep 269: ep_len:664 episode reward: total was -73.810000. running mean: -80.608906\n",
      "ep 269: ep_len:500 episode reward: total was -38.940000. running mean: -80.192217\n",
      "ep 269: ep_len:650 episode reward: total was -143.660000. running mean: -80.826894\n",
      "ep 269: ep_len:503 episode reward: total was -74.350000. running mean: -80.762125\n",
      "ep 269: ep_len:68 episode reward: total was -3.890000. running mean: -79.993404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 269: ep_len:633 episode reward: total was -83.290000. running mean: -80.026370\n",
      "ep 269: ep_len:299 episode reward: total was -52.870000. running mean: -79.754806\n",
      "epsilon:0.188030 episode_count: 1890. steps_count: 838087.000000\n",
      "Time elapsed:  2357.9006803035736\n",
      "ep 270: ep_len:546 episode reward: total was -132.980000. running mean: -80.287058\n",
      "ep 270: ep_len:528 episode reward: total was -61.270000. running mean: -80.096888\n",
      "ep 270: ep_len:569 episode reward: total was -79.350000. running mean: -80.089419\n",
      "ep 270: ep_len:131 episode reward: total was -2.980000. running mean: -79.318325\n",
      "ep 270: ep_len:3 episode reward: total was -1.500000. running mean: -78.540141\n",
      "ep 270: ep_len:222 episode reward: total was -13.220000. running mean: -77.886940\n",
      "ep 270: ep_len:560 episode reward: total was -113.470000. running mean: -78.242771\n",
      "epsilon:0.187986 episode_count: 1897. steps_count: 840646.000000\n",
      "Time elapsed:  2365.9332976341248\n",
      "ep 271: ep_len:658 episode reward: total was -189.910000. running mean: -79.359443\n",
      "ep 271: ep_len:557 episode reward: total was -39.900000. running mean: -78.964848\n",
      "ep 271: ep_len:559 episode reward: total was -184.410000. running mean: -80.019300\n",
      "ep 271: ep_len:518 episode reward: total was -142.920000. running mean: -80.648307\n",
      "ep 271: ep_len:3 episode reward: total was 0.000000. running mean: -79.841824\n",
      "ep 271: ep_len:602 episode reward: total was -98.770000. running mean: -80.031106\n",
      "ep 271: ep_len:532 episode reward: total was -61.200000. running mean: -79.842795\n",
      "epsilon:0.187941 episode_count: 1904. steps_count: 844075.000000\n",
      "Time elapsed:  2376.322625398636\n",
      "ep 272: ep_len:575 episode reward: total was -52.110000. running mean: -79.565467\n",
      "ep 272: ep_len:500 episode reward: total was -75.360000. running mean: -79.523412\n",
      "ep 272: ep_len:541 episode reward: total was -110.510000. running mean: -79.833278\n",
      "ep 272: ep_len:501 episode reward: total was -44.240000. running mean: -79.477345\n",
      "ep 272: ep_len:79 episode reward: total was -39.870000. running mean: -79.081272\n",
      "ep 272: ep_len:522 episode reward: total was -160.340000. running mean: -79.893859\n",
      "ep 272: ep_len:602 episode reward: total was -46.050000. running mean: -79.555420\n",
      "epsilon:0.187897 episode_count: 1911. steps_count: 847395.000000\n",
      "Time elapsed:  2386.302102327347\n",
      "ep 273: ep_len:500 episode reward: total was -154.080000. running mean: -80.300666\n",
      "ep 273: ep_len:500 episode reward: total was -68.690000. running mean: -80.184560\n",
      "ep 273: ep_len:533 episode reward: total was -104.650000. running mean: -80.429214\n",
      "ep 273: ep_len:615 episode reward: total was -74.150000. running mean: -80.366422\n",
      "ep 273: ep_len:3 episode reward: total was 0.000000. running mean: -79.562758\n",
      "ep 273: ep_len:523 episode reward: total was -119.080000. running mean: -79.957930\n",
      "ep 273: ep_len:617 episode reward: total was -54.210000. running mean: -79.700451\n",
      "epsilon:0.187853 episode_count: 1918. steps_count: 850686.000000\n",
      "Time elapsed:  2396.014499902725\n",
      "ep 274: ep_len:592 episode reward: total was -144.190000. running mean: -80.345346\n",
      "ep 274: ep_len:600 episode reward: total was -153.010000. running mean: -81.071993\n",
      "ep 274: ep_len:599 episode reward: total was -106.300000. running mean: -81.324273\n",
      "ep 274: ep_len:535 episode reward: total was -55.870000. running mean: -81.069730\n",
      "ep 274: ep_len:3 episode reward: total was 0.000000. running mean: -80.259033\n",
      "ep 274: ep_len:232 episode reward: total was 0.940000. running mean: -79.447042\n",
      "ep 274: ep_len:557 episode reward: total was -53.910000. running mean: -79.191672\n",
      "epsilon:0.187808 episode_count: 1925. steps_count: 853804.000000\n",
      "Time elapsed:  2405.4836797714233\n",
      "ep 275: ep_len:611 episode reward: total was -77.740000. running mean: -79.177155\n",
      "ep 275: ep_len:189 episode reward: total was -42.970000. running mean: -78.815084\n",
      "ep 275: ep_len:568 episode reward: total was -133.830000. running mean: -79.365233\n",
      "ep 275: ep_len:611 episode reward: total was -94.540000. running mean: -79.516981\n",
      "ep 275: ep_len:3 episode reward: total was 0.000000. running mean: -78.721811\n",
      "ep 275: ep_len:571 episode reward: total was -79.660000. running mean: -78.731193\n",
      "ep 275: ep_len:327 episode reward: total was -138.770000. running mean: -79.331581\n",
      "epsilon:0.187764 episode_count: 1932. steps_count: 856684.000000\n",
      "Time elapsed:  2414.7969913482666\n",
      "ep 276: ep_len:562 episode reward: total was -48.190000. running mean: -79.020165\n",
      "ep 276: ep_len:595 episode reward: total was -88.270000. running mean: -79.112663\n",
      "ep 276: ep_len:634 episode reward: total was -83.450000. running mean: -79.156037\n",
      "ep 276: ep_len:568 episode reward: total was -34.190000. running mean: -78.706376\n",
      "ep 276: ep_len:3 episode reward: total was -1.500000. running mean: -77.934313\n",
      "ep 276: ep_len:542 episode reward: total was -65.620000. running mean: -77.811169\n",
      "ep 276: ep_len:543 episode reward: total was -81.990000. running mean: -77.852958\n",
      "epsilon:0.187720 episode_count: 1939. steps_count: 860131.000000\n",
      "Time elapsed:  2425.1843371391296\n",
      "ep 277: ep_len:190 episode reward: total was 3.570000. running mean: -77.038728\n",
      "ep 277: ep_len:500 episode reward: total was -67.320000. running mean: -76.941541\n",
      "ep 277: ep_len:570 episode reward: total was -97.170000. running mean: -77.143825\n",
      "ep 277: ep_len:503 episode reward: total was -96.700000. running mean: -77.339387\n",
      "ep 277: ep_len:3 episode reward: total was 0.000000. running mean: -76.565993\n",
      "ep 277: ep_len:716 episode reward: total was -67.920000. running mean: -76.479533\n",
      "ep 277: ep_len:211 episode reward: total was -56.280000. running mean: -76.277538\n",
      "epsilon:0.187675 episode_count: 1946. steps_count: 862824.000000\n",
      "Time elapsed:  2433.709952354431\n",
      "ep 278: ep_len:252 episode reward: total was -7.760000. running mean: -75.592363\n",
      "ep 278: ep_len:644 episode reward: total was -88.010000. running mean: -75.716539\n",
      "ep 278: ep_len:533 episode reward: total was -84.330000. running mean: -75.802674\n",
      "ep 278: ep_len:537 episode reward: total was -114.710000. running mean: -76.191747\n",
      "ep 278: ep_len:105 episode reward: total was -10.800000. running mean: -75.537829\n",
      "ep 278: ep_len:551 episode reward: total was -87.930000. running mean: -75.661751\n",
      "ep 278: ep_len:621 episode reward: total was -68.670000. running mean: -75.591834\n",
      "epsilon:0.187631 episode_count: 1953. steps_count: 866067.000000\n",
      "Time elapsed:  2443.776304244995\n",
      "ep 279: ep_len:500 episode reward: total was -75.610000. running mean: -75.592015\n",
      "ep 279: ep_len:600 episode reward: total was -31.920000. running mean: -75.155295\n",
      "ep 279: ep_len:546 episode reward: total was -100.440000. running mean: -75.408142\n",
      "ep 279: ep_len:641 episode reward: total was -69.470000. running mean: -75.348761\n",
      "ep 279: ep_len:3 episode reward: total was 0.000000. running mean: -74.595273\n",
      "ep 279: ep_len:627 episode reward: total was -107.020000. running mean: -74.919520\n",
      "ep 279: ep_len:192 episode reward: total was -62.810000. running mean: -74.798425\n",
      "epsilon:0.187587 episode_count: 1960. steps_count: 869176.000000\n",
      "Time elapsed:  2453.2654304504395\n",
      "ep 280: ep_len:609 episode reward: total was -26.310000. running mean: -74.313541\n",
      "ep 280: ep_len:347 episode reward: total was -95.620000. running mean: -74.526606\n",
      "ep 280: ep_len:503 episode reward: total was -108.630000. running mean: -74.867639\n",
      "ep 280: ep_len:561 episode reward: total was -138.460000. running mean: -75.503563\n",
      "ep 280: ep_len:3 episode reward: total was 0.000000. running mean: -74.748527\n",
      "ep 280: ep_len:520 episode reward: total was -105.720000. running mean: -75.058242\n",
      "ep 280: ep_len:595 episode reward: total was -64.370000. running mean: -74.951360\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.187542 episode_count: 1967. steps_count: 872314.000000\n",
      "Time elapsed:  2467.1935334205627\n",
      "ep 281: ep_len:510 episode reward: total was -93.280000. running mean: -75.134646\n",
      "ep 281: ep_len:550 episode reward: total was -10.740000. running mean: -74.490700\n",
      "ep 281: ep_len:79 episode reward: total was -6.290000. running mean: -73.808693\n",
      "ep 281: ep_len:500 episode reward: total was -101.550000. running mean: -74.086106\n",
      "ep 281: ep_len:131 episode reward: total was 2.290000. running mean: -73.322345\n",
      "ep 281: ep_len:602 episode reward: total was -59.210000. running mean: -73.181221\n",
      "ep 281: ep_len:211 episode reward: total was -42.600000. running mean: -72.875409\n",
      "epsilon:0.187498 episode_count: 1974. steps_count: 874897.000000\n",
      "Time elapsed:  2474.989033937454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 282: ep_len:229 episode reward: total was -31.780000. running mean: -72.464455\n",
      "ep 282: ep_len:523 episode reward: total was -113.360000. running mean: -72.873410\n",
      "ep 282: ep_len:545 episode reward: total was -68.720000. running mean: -72.831876\n",
      "ep 282: ep_len:44 episode reward: total was -0.740000. running mean: -72.110958\n",
      "ep 282: ep_len:3 episode reward: total was 0.000000. running mean: -71.389848\n",
      "ep 282: ep_len:500 episode reward: total was -105.920000. running mean: -71.735150\n",
      "ep 282: ep_len:529 episode reward: total was -74.280000. running mean: -71.760598\n",
      "epsilon:0.187454 episode_count: 1981. steps_count: 877270.000000\n",
      "Time elapsed:  2482.735431909561\n",
      "ep 283: ep_len:133 episode reward: total was -10.090000. running mean: -71.143892\n",
      "ep 283: ep_len:500 episode reward: total was -0.590000. running mean: -70.438353\n",
      "ep 283: ep_len:528 episode reward: total was -105.590000. running mean: -70.789870\n",
      "ep 283: ep_len:546 episode reward: total was -60.840000. running mean: -70.690371\n",
      "ep 283: ep_len:3 episode reward: total was 0.000000. running mean: -69.983467\n",
      "ep 283: ep_len:648 episode reward: total was -165.160000. running mean: -70.935233\n",
      "ep 283: ep_len:642 episode reward: total was -76.290000. running mean: -70.988780\n",
      "epsilon:0.187409 episode_count: 1988. steps_count: 880270.000000\n",
      "Time elapsed:  2492.39888548851\n",
      "ep 284: ep_len:525 episode reward: total was -81.680000. running mean: -71.095692\n",
      "ep 284: ep_len:567 episode reward: total was -128.480000. running mean: -71.669535\n",
      "ep 284: ep_len:330 episode reward: total was -43.510000. running mean: -71.387940\n",
      "ep 284: ep_len:500 episode reward: total was -23.570000. running mean: -70.909761\n",
      "ep 284: ep_len:3 episode reward: total was 0.000000. running mean: -70.200663\n",
      "ep 284: ep_len:500 episode reward: total was -93.510000. running mean: -70.433756\n",
      "ep 284: ep_len:520 episode reward: total was -94.390000. running mean: -70.673319\n",
      "epsilon:0.187365 episode_count: 1995. steps_count: 883215.000000\n",
      "Time elapsed:  2500.9525077342987\n",
      "ep 285: ep_len:123 episode reward: total was -9.550000. running mean: -70.062086\n",
      "ep 285: ep_len:500 episode reward: total was -28.750000. running mean: -69.648965\n",
      "ep 285: ep_len:500 episode reward: total was -95.670000. running mean: -69.909175\n",
      "ep 285: ep_len:530 episode reward: total was -79.770000. running mean: -70.007783\n",
      "ep 285: ep_len:3 episode reward: total was 0.000000. running mean: -69.307706\n",
      "ep 285: ep_len:585 episode reward: total was -84.390000. running mean: -69.458529\n",
      "ep 285: ep_len:585 episode reward: total was -112.560000. running mean: -69.889543\n",
      "epsilon:0.187321 episode_count: 2002. steps_count: 886041.000000\n",
      "Time elapsed:  2508.947196006775\n",
      "ep 286: ep_len:500 episode reward: total was -118.570000. running mean: -70.376348\n",
      "ep 286: ep_len:613 episode reward: total was -56.060000. running mean: -70.233184\n",
      "ep 286: ep_len:500 episode reward: total was -97.670000. running mean: -70.507553\n",
      "ep 286: ep_len:501 episode reward: total was -72.780000. running mean: -70.530277\n",
      "ep 286: ep_len:3 episode reward: total was 0.000000. running mean: -69.824974\n",
      "ep 286: ep_len:304 episode reward: total was -8.310000. running mean: -69.209824\n",
      "ep 286: ep_len:594 episode reward: total was -65.250000. running mean: -69.170226\n",
      "epsilon:0.187276 episode_count: 2009. steps_count: 889056.000000\n",
      "Time elapsed:  2517.360331296921\n",
      "ep 287: ep_len:619 episode reward: total was -72.470000. running mean: -69.203224\n",
      "ep 287: ep_len:634 episode reward: total was -130.780000. running mean: -69.818992\n",
      "ep 287: ep_len:628 episode reward: total was -93.120000. running mean: -70.052002\n",
      "ep 287: ep_len:510 episode reward: total was -45.070000. running mean: -69.802182\n",
      "ep 287: ep_len:3 episode reward: total was 0.000000. running mean: -69.104160\n",
      "ep 287: ep_len:556 episode reward: total was -101.540000. running mean: -69.428518\n",
      "ep 287: ep_len:612 episode reward: total was -105.530000. running mean: -69.789533\n",
      "epsilon:0.187232 episode_count: 2016. steps_count: 892618.000000\n",
      "Time elapsed:  2526.5175392627716\n",
      "ep 288: ep_len:500 episode reward: total was -13.120000. running mean: -69.222838\n",
      "ep 288: ep_len:328 episode reward: total was -83.530000. running mean: -69.365909\n",
      "ep 288: ep_len:642 episode reward: total was -108.000000. running mean: -69.752250\n",
      "ep 288: ep_len:512 episode reward: total was -38.620000. running mean: -69.440928\n",
      "ep 288: ep_len:92 episode reward: total was 2.720000. running mean: -68.719319\n",
      "ep 288: ep_len:592 episode reward: total was -78.680000. running mean: -68.818925\n",
      "ep 288: ep_len:566 episode reward: total was -75.030000. running mean: -68.881036\n",
      "epsilon:0.187188 episode_count: 2023. steps_count: 895850.000000\n",
      "Time elapsed:  2534.777267217636\n",
      "ep 289: ep_len:570 episode reward: total was -25.310000. running mean: -68.445326\n",
      "ep 289: ep_len:549 episode reward: total was -87.570000. running mean: -68.636573\n",
      "ep 289: ep_len:637 episode reward: total was -102.460000. running mean: -68.974807\n",
      "ep 289: ep_len:509 episode reward: total was -136.110000. running mean: -69.646159\n",
      "ep 289: ep_len:3 episode reward: total was 0.000000. running mean: -68.949697\n",
      "ep 289: ep_len:500 episode reward: total was -99.290000. running mean: -69.253100\n",
      "ep 289: ep_len:500 episode reward: total was -123.430000. running mean: -69.794869\n",
      "epsilon:0.187143 episode_count: 2030. steps_count: 899118.000000\n",
      "Time elapsed:  2543.809878349304\n",
      "ep 290: ep_len:615 episode reward: total was -200.410000. running mean: -71.101021\n",
      "ep 290: ep_len:516 episode reward: total was -88.380000. running mean: -71.273810\n",
      "ep 290: ep_len:525 episode reward: total was -89.210000. running mean: -71.453172\n",
      "ep 290: ep_len:170 episode reward: total was -12.390000. running mean: -70.862540\n",
      "ep 290: ep_len:86 episode reward: total was -32.800000. running mean: -70.481915\n",
      "ep 290: ep_len:551 episode reward: total was -137.910000. running mean: -71.156196\n",
      "ep 290: ep_len:558 episode reward: total was -158.910000. running mean: -72.033734\n",
      "epsilon:0.187099 episode_count: 2037. steps_count: 902139.000000\n",
      "Time elapsed:  2551.611988544464\n",
      "ep 291: ep_len:535 episode reward: total was -39.980000. running mean: -71.713197\n",
      "ep 291: ep_len:501 episode reward: total was -78.110000. running mean: -71.777165\n",
      "ep 291: ep_len:500 episode reward: total was -58.040000. running mean: -71.639793\n",
      "ep 291: ep_len:510 episode reward: total was -97.230000. running mean: -71.895695\n",
      "ep 291: ep_len:83 episode reward: total was -5.300000. running mean: -71.229738\n",
      "ep 291: ep_len:535 episode reward: total was -111.720000. running mean: -71.634641\n",
      "ep 291: ep_len:169 episode reward: total was -39.900000. running mean: -71.317294\n",
      "epsilon:0.187055 episode_count: 2044. steps_count: 904972.000000\n",
      "Time elapsed:  2559.283629179001\n",
      "ep 292: ep_len:502 episode reward: total was -81.950000. running mean: -71.423621\n",
      "ep 292: ep_len:556 episode reward: total was -123.080000. running mean: -71.940185\n",
      "ep 292: ep_len:570 episode reward: total was -74.800000. running mean: -71.968783\n",
      "ep 292: ep_len:500 episode reward: total was -73.750000. running mean: -71.986595\n",
      "ep 292: ep_len:118 episode reward: total was -29.240000. running mean: -71.559130\n",
      "ep 292: ep_len:227 episode reward: total was -3.190000. running mean: -70.875438\n",
      "ep 292: ep_len:590 episode reward: total was -78.180000. running mean: -70.948484\n",
      "epsilon:0.187010 episode_count: 2051. steps_count: 908035.000000\n",
      "Time elapsed:  2567.4378299713135\n",
      "ep 293: ep_len:642 episode reward: total was -97.660000. running mean: -71.215599\n",
      "ep 293: ep_len:507 episode reward: total was 16.720000. running mean: -70.336243\n",
      "ep 293: ep_len:638 episode reward: total was -150.610000. running mean: -71.138981\n",
      "ep 293: ep_len:500 episode reward: total was -34.100000. running mean: -70.768591\n",
      "ep 293: ep_len:3 episode reward: total was 0.000000. running mean: -70.060905\n",
      "ep 293: ep_len:537 episode reward: total was -100.970000. running mean: -70.369996\n",
      "ep 293: ep_len:628 episode reward: total was -115.150000. running mean: -70.817796\n",
      "epsilon:0.186966 episode_count: 2058. steps_count: 911490.000000\n",
      "Time elapsed:  2576.976326227188\n",
      "ep 294: ep_len:216 episode reward: total was -1.340000. running mean: -70.123018\n",
      "ep 294: ep_len:594 episode reward: total was -175.790000. running mean: -71.179688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 294: ep_len:645 episode reward: total was -116.230000. running mean: -71.630191\n",
      "ep 294: ep_len:500 episode reward: total was 4.990000. running mean: -70.863989\n",
      "ep 294: ep_len:81 episode reward: total was -5.320000. running mean: -70.208549\n",
      "ep 294: ep_len:186 episode reward: total was 1.070000. running mean: -69.495764\n",
      "ep 294: ep_len:571 episode reward: total was -74.760000. running mean: -69.548406\n",
      "epsilon:0.186922 episode_count: 2065. steps_count: 914283.000000\n",
      "Time elapsed:  2584.4651148319244\n",
      "ep 295: ep_len:500 episode reward: total was -26.410000. running mean: -69.117022\n",
      "ep 295: ep_len:551 episode reward: total was -54.060000. running mean: -68.966452\n",
      "ep 295: ep_len:562 episode reward: total was -118.870000. running mean: -69.465487\n",
      "ep 295: ep_len:534 episode reward: total was -76.070000. running mean: -69.531532\n",
      "ep 295: ep_len:3 episode reward: total was 0.000000. running mean: -68.836217\n",
      "ep 295: ep_len:500 episode reward: total was -124.130000. running mean: -69.389155\n",
      "ep 295: ep_len:325 episode reward: total was -90.160000. running mean: -69.596863\n",
      "epsilon:0.186877 episode_count: 2072. steps_count: 917258.000000\n",
      "Time elapsed:  2592.713290452957\n",
      "ep 296: ep_len:502 episode reward: total was -20.230000. running mean: -69.103195\n",
      "ep 296: ep_len:500 episode reward: total was -61.550000. running mean: -69.027663\n",
      "ep 296: ep_len:385 episode reward: total was -27.650000. running mean: -68.613886\n",
      "ep 296: ep_len:572 episode reward: total was -35.080000. running mean: -68.278547\n",
      "ep 296: ep_len:78 episode reward: total was -4.310000. running mean: -67.638862\n",
      "ep 296: ep_len:500 episode reward: total was -92.920000. running mean: -67.891673\n",
      "ep 296: ep_len:500 episode reward: total was -87.370000. running mean: -68.086456\n",
      "epsilon:0.186833 episode_count: 2079. steps_count: 920295.000000\n",
      "Time elapsed:  2600.7413725852966\n",
      "ep 297: ep_len:500 episode reward: total was -52.330000. running mean: -67.928892\n",
      "ep 297: ep_len:556 episode reward: total was -173.210000. running mean: -68.981703\n",
      "ep 297: ep_len:524 episode reward: total was -48.650000. running mean: -68.778386\n",
      "ep 297: ep_len:512 episode reward: total was -98.840000. running mean: -69.079002\n",
      "ep 297: ep_len:104 episode reward: total was -28.720000. running mean: -68.675412\n",
      "ep 297: ep_len:631 episode reward: total was -90.820000. running mean: -68.896858\n",
      "ep 297: ep_len:501 episode reward: total was -81.080000. running mean: -69.018689\n",
      "epsilon:0.186789 episode_count: 2086. steps_count: 923623.000000\n",
      "Time elapsed:  2609.4632580280304\n",
      "ep 298: ep_len:533 episode reward: total was -194.030000. running mean: -70.268802\n",
      "ep 298: ep_len:501 episode reward: total was -100.330000. running mean: -70.569414\n",
      "ep 298: ep_len:636 episode reward: total was -90.690000. running mean: -70.770620\n",
      "ep 298: ep_len:511 episode reward: total was -65.310000. running mean: -70.716014\n",
      "ep 298: ep_len:3 episode reward: total was -1.500000. running mean: -70.023854\n",
      "ep 298: ep_len:667 episode reward: total was -150.330000. running mean: -70.826915\n",
      "ep 298: ep_len:578 episode reward: total was -125.430000. running mean: -71.372946\n",
      "epsilon:0.186744 episode_count: 2093. steps_count: 927052.000000\n",
      "Time elapsed:  2618.4548325538635\n",
      "ep 299: ep_len:500 episode reward: total was -41.070000. running mean: -71.069917\n",
      "ep 299: ep_len:652 episode reward: total was -56.800000. running mean: -70.927218\n",
      "ep 299: ep_len:79 episode reward: total was 1.790000. running mean: -70.200045\n",
      "ep 299: ep_len:124 episode reward: total was 3.880000. running mean: -69.459245\n",
      "ep 299: ep_len:3 episode reward: total was -1.500000. running mean: -68.779652\n",
      "ep 299: ep_len:624 episode reward: total was -99.570000. running mean: -69.087556\n",
      "ep 299: ep_len:542 episode reward: total was -85.750000. running mean: -69.254180\n",
      "epsilon:0.186700 episode_count: 2100. steps_count: 929576.000000\n",
      "Time elapsed:  2625.435359954834\n",
      "ep 300: ep_len:261 episode reward: total was -5.750000. running mean: -68.619139\n",
      "ep 300: ep_len:832 episode reward: total was -308.540000. running mean: -71.018347\n",
      "ep 300: ep_len:500 episode reward: total was -85.900000. running mean: -71.167164\n",
      "ep 300: ep_len:515 episode reward: total was -94.430000. running mean: -71.399792\n",
      "ep 300: ep_len:3 episode reward: total was -1.500000. running mean: -70.700794\n",
      "ep 300: ep_len:635 episode reward: total was -127.800000. running mean: -71.271786\n",
      "ep 300: ep_len:500 episode reward: total was -113.920000. running mean: -71.698268\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.186656 episode_count: 2107. steps_count: 932822.000000\n",
      "Time elapsed:  2639.5206170082092\n",
      "ep 301: ep_len:500 episode reward: total was -33.470000. running mean: -71.315986\n",
      "ep 301: ep_len:502 episode reward: total was -47.360000. running mean: -71.076426\n",
      "ep 301: ep_len:515 episode reward: total was -98.530000. running mean: -71.350962\n",
      "ep 301: ep_len:545 episode reward: total was -67.850000. running mean: -71.315952\n",
      "ep 301: ep_len:73 episode reward: total was 7.730000. running mean: -70.525492\n",
      "ep 301: ep_len:577 episode reward: total was -95.120000. running mean: -70.771437\n",
      "ep 301: ep_len:543 episode reward: total was -151.110000. running mean: -71.574823\n",
      "epsilon:0.186611 episode_count: 2114. steps_count: 936077.000000\n",
      "Time elapsed:  2647.7800948619843\n",
      "ep 302: ep_len:662 episode reward: total was -124.850000. running mean: -72.107575\n",
      "ep 302: ep_len:511 episode reward: total was -56.620000. running mean: -71.952699\n",
      "ep 302: ep_len:500 episode reward: total was -89.780000. running mean: -72.130972\n",
      "ep 302: ep_len:150 episode reward: total was -8.510000. running mean: -71.494762\n",
      "ep 302: ep_len:3 episode reward: total was -3.000000. running mean: -70.809815\n",
      "ep 302: ep_len:531 episode reward: total was -96.970000. running mean: -71.071417\n",
      "ep 302: ep_len:591 episode reward: total was -65.260000. running mean: -71.013302\n",
      "epsilon:0.186567 episode_count: 2121. steps_count: 939025.000000\n",
      "Time elapsed:  2655.8128213882446\n",
      "ep 303: ep_len:134 episode reward: total was -15.990000. running mean: -70.463069\n",
      "ep 303: ep_len:355 episode reward: total was -133.360000. running mean: -71.092039\n",
      "ep 303: ep_len:595 episode reward: total was -88.580000. running mean: -71.266918\n",
      "ep 303: ep_len:550 episode reward: total was -92.720000. running mean: -71.481449\n",
      "ep 303: ep_len:3 episode reward: total was 0.000000. running mean: -70.766635\n",
      "ep 303: ep_len:502 episode reward: total was -110.270000. running mean: -71.161668\n",
      "ep 303: ep_len:537 episode reward: total was -84.150000. running mean: -71.291552\n",
      "epsilon:0.186523 episode_count: 2128. steps_count: 941701.000000\n",
      "Time elapsed:  2662.9502389431\n",
      "ep 304: ep_len:543 episode reward: total was -111.680000. running mean: -71.695436\n",
      "ep 304: ep_len:611 episode reward: total was -130.770000. running mean: -72.286182\n",
      "ep 304: ep_len:644 episode reward: total was -61.510000. running mean: -72.178420\n",
      "ep 304: ep_len:375 episode reward: total was -53.840000. running mean: -71.995036\n",
      "ep 304: ep_len:91 episode reward: total was 5.710000. running mean: -71.217985\n",
      "ep 304: ep_len:602 episode reward: total was -121.370000. running mean: -71.719506\n",
      "ep 304: ep_len:540 episode reward: total was -109.620000. running mean: -72.098511\n",
      "epsilon:0.186478 episode_count: 2135. steps_count: 945107.000000\n",
      "Time elapsed:  2672.509186267853\n",
      "ep 305: ep_len:595 episode reward: total was -153.580000. running mean: -72.913325\n",
      "ep 305: ep_len:520 episode reward: total was -106.580000. running mean: -73.249992\n",
      "ep 305: ep_len:500 episode reward: total was -71.780000. running mean: -73.235292\n",
      "ep 305: ep_len:500 episode reward: total was -62.190000. running mean: -73.124839\n",
      "ep 305: ep_len:3 episode reward: total was -1.500000. running mean: -72.408591\n",
      "ep 305: ep_len:628 episode reward: total was -73.170000. running mean: -72.416205\n",
      "ep 305: ep_len:535 episode reward: total was -125.230000. running mean: -72.944343\n",
      "epsilon:0.186434 episode_count: 2142. steps_count: 948388.000000\n",
      "Time elapsed:  2681.8950362205505\n",
      "ep 306: ep_len:579 episode reward: total was -126.990000. running mean: -73.484800\n",
      "ep 306: ep_len:541 episode reward: total was -63.730000. running mean: -73.387252\n",
      "ep 306: ep_len:501 episode reward: total was -97.700000. running mean: -73.630379\n",
      "ep 306: ep_len:501 episode reward: total was -30.540000. running mean: -73.199475\n",
      "ep 306: ep_len:3 episode reward: total was -3.000000. running mean: -72.497480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 306: ep_len:587 episode reward: total was -75.110000. running mean: -72.523606\n",
      "ep 306: ep_len:345 episode reward: total was -57.930000. running mean: -72.377670\n",
      "epsilon:0.186390 episode_count: 2149. steps_count: 951445.000000\n",
      "Time elapsed:  2690.1317224502563\n",
      "ep 307: ep_len:604 episode reward: total was -95.780000. running mean: -72.611693\n",
      "ep 307: ep_len:500 episode reward: total was -31.830000. running mean: -72.203876\n",
      "ep 307: ep_len:501 episode reward: total was -77.780000. running mean: -72.259637\n",
      "ep 307: ep_len:151 episode reward: total was -9.460000. running mean: -71.631641\n",
      "ep 307: ep_len:112 episode reward: total was -14.800000. running mean: -71.063324\n",
      "ep 307: ep_len:530 episode reward: total was -126.140000. running mean: -71.614091\n",
      "ep 307: ep_len:511 episode reward: total was -103.050000. running mean: -71.928450\n",
      "epsilon:0.186345 episode_count: 2156. steps_count: 954354.000000\n",
      "Time elapsed:  2698.3663482666016\n",
      "ep 308: ep_len:223 episode reward: total was -27.350000. running mean: -71.482666\n",
      "ep 308: ep_len:565 episode reward: total was -96.860000. running mean: -71.736439\n",
      "ep 308: ep_len:67 episode reward: total was -10.390000. running mean: -71.122975\n",
      "ep 308: ep_len:500 episode reward: total was -33.050000. running mean: -70.742245\n",
      "ep 308: ep_len:3 episode reward: total was 0.000000. running mean: -70.034823\n",
      "ep 308: ep_len:500 episode reward: total was -98.830000. running mean: -70.322774\n",
      "ep 308: ep_len:500 episode reward: total was -74.090000. running mean: -70.360447\n",
      "epsilon:0.186301 episode_count: 2163. steps_count: 956712.000000\n",
      "Time elapsed:  2704.7320473194122\n",
      "ep 309: ep_len:214 episode reward: total was -13.420000. running mean: -69.791042\n",
      "ep 309: ep_len:526 episode reward: total was 16.760000. running mean: -68.925532\n",
      "ep 309: ep_len:556 episode reward: total was -76.290000. running mean: -68.999176\n",
      "ep 309: ep_len:544 episode reward: total was -109.970000. running mean: -69.408885\n",
      "ep 309: ep_len:3 episode reward: total was 1.010000. running mean: -68.704696\n",
      "ep 309: ep_len:500 episode reward: total was -73.930000. running mean: -68.756949\n",
      "ep 309: ep_len:325 episode reward: total was -87.550000. running mean: -68.944879\n",
      "epsilon:0.186257 episode_count: 2170. steps_count: 959380.000000\n",
      "Time elapsed:  2711.9484508037567\n",
      "ep 310: ep_len:621 episode reward: total was -172.560000. running mean: -69.981031\n",
      "ep 310: ep_len:500 episode reward: total was -66.000000. running mean: -69.941220\n",
      "ep 310: ep_len:500 episode reward: total was -87.010000. running mean: -70.111908\n",
      "ep 310: ep_len:124 episode reward: total was -9.740000. running mean: -69.508189\n",
      "ep 310: ep_len:3 episode reward: total was -3.000000. running mean: -68.843107\n",
      "ep 310: ep_len:548 episode reward: total was -92.370000. running mean: -69.078376\n",
      "ep 310: ep_len:603 episode reward: total was -96.800000. running mean: -69.355592\n",
      "epsilon:0.186212 episode_count: 2177. steps_count: 962279.000000\n",
      "Time elapsed:  2719.8002409934998\n",
      "ep 311: ep_len:568 episode reward: total was -70.930000. running mean: -69.371336\n",
      "ep 311: ep_len:500 episode reward: total was -132.570000. running mean: -70.003323\n",
      "ep 311: ep_len:580 episode reward: total was -75.330000. running mean: -70.056590\n",
      "ep 311: ep_len:518 episode reward: total was -113.590000. running mean: -70.491924\n",
      "ep 311: ep_len:49 episode reward: total was -14.000000. running mean: -69.927005\n",
      "ep 311: ep_len:564 episode reward: total was -78.720000. running mean: -70.014935\n",
      "ep 311: ep_len:564 episode reward: total was -76.740000. running mean: -70.082185\n",
      "epsilon:0.186168 episode_count: 2184. steps_count: 965622.000000\n",
      "Time elapsed:  2728.6057271957397\n",
      "ep 312: ep_len:605 episode reward: total was -28.530000. running mean: -69.666663\n",
      "ep 312: ep_len:500 episode reward: total was -29.660000. running mean: -69.266597\n",
      "ep 312: ep_len:616 episode reward: total was -209.700000. running mean: -70.670931\n",
      "ep 312: ep_len:528 episode reward: total was -40.950000. running mean: -70.373721\n",
      "ep 312: ep_len:106 episode reward: total was -9.780000. running mean: -69.767784\n",
      "ep 312: ep_len:600 episode reward: total was -98.870000. running mean: -70.058806\n",
      "ep 312: ep_len:549 episode reward: total was -63.800000. running mean: -69.996218\n",
      "epsilon:0.186124 episode_count: 2191. steps_count: 969126.000000\n",
      "Time elapsed:  2737.699556350708\n",
      "ep 313: ep_len:624 episode reward: total was -122.030000. running mean: -70.516556\n",
      "ep 313: ep_len:519 episode reward: total was -86.930000. running mean: -70.680691\n",
      "ep 313: ep_len:673 episode reward: total was -128.340000. running mean: -71.257284\n",
      "ep 313: ep_len:536 episode reward: total was -34.000000. running mean: -70.884711\n",
      "ep 313: ep_len:104 episode reward: total was -2.270000. running mean: -70.198564\n",
      "ep 313: ep_len:525 episode reward: total was -83.050000. running mean: -70.327078\n",
      "ep 313: ep_len:500 episode reward: total was -86.280000. running mean: -70.486607\n",
      "epsilon:0.186079 episode_count: 2198. steps_count: 972607.000000\n",
      "Time elapsed:  2746.924257993698\n",
      "ep 314: ep_len:247 episode reward: total was -32.400000. running mean: -70.105741\n",
      "ep 314: ep_len:515 episode reward: total was -73.610000. running mean: -70.140784\n",
      "ep 314: ep_len:622 episode reward: total was -136.110000. running mean: -70.800476\n",
      "ep 314: ep_len:583 episode reward: total was -68.730000. running mean: -70.779771\n",
      "ep 314: ep_len:52 episode reward: total was 23.000000. running mean: -69.841973\n",
      "ep 314: ep_len:500 episode reward: total was -115.480000. running mean: -70.298354\n",
      "ep 314: ep_len:269 episode reward: total was -25.900000. running mean: -69.854370\n",
      "epsilon:0.186035 episode_count: 2205. steps_count: 975395.000000\n",
      "Time elapsed:  2754.402093887329\n",
      "ep 315: ep_len:500 episode reward: total was -51.440000. running mean: -69.670227\n",
      "ep 315: ep_len:500 episode reward: total was -73.610000. running mean: -69.709624\n",
      "ep 315: ep_len:64 episode reward: total was -17.800000. running mean: -69.190528\n",
      "ep 315: ep_len:386 episode reward: total was -68.640000. running mean: -69.185023\n",
      "ep 315: ep_len:49 episode reward: total was 13.510000. running mean: -68.358072\n",
      "ep 315: ep_len:186 episode reward: total was -6.830000. running mean: -67.742792\n",
      "ep 315: ep_len:595 episode reward: total was -173.830000. running mean: -68.803664\n",
      "epsilon:0.185991 episode_count: 2212. steps_count: 977675.000000\n",
      "Time elapsed:  2760.3786160945892\n",
      "ep 316: ep_len:559 episode reward: total was -78.280000. running mean: -68.898427\n",
      "ep 316: ep_len:514 episode reward: total was -97.510000. running mean: -69.184543\n",
      "ep 316: ep_len:525 episode reward: total was -108.830000. running mean: -69.580998\n",
      "ep 316: ep_len:500 episode reward: total was -74.260000. running mean: -69.627788\n",
      "ep 316: ep_len:3 episode reward: total was 0.000000. running mean: -68.931510\n",
      "ep 316: ep_len:621 episode reward: total was -89.630000. running mean: -69.138495\n",
      "ep 316: ep_len:323 episode reward: total was -76.130000. running mean: -69.208410\n",
      "epsilon:0.185946 episode_count: 2219. steps_count: 980720.000000\n",
      "Time elapsed:  2768.133626937866\n",
      "ep 317: ep_len:658 episode reward: total was -167.810000. running mean: -70.194426\n",
      "ep 317: ep_len:500 episode reward: total was -60.230000. running mean: -70.094781\n",
      "ep 317: ep_len:390 episode reward: total was -49.700000. running mean: -69.890833\n",
      "ep 317: ep_len:613 episode reward: total was -27.460000. running mean: -69.466525\n",
      "ep 317: ep_len:3 episode reward: total was 0.000000. running mean: -68.771860\n",
      "ep 317: ep_len:642 episode reward: total was -119.160000. running mean: -69.275741\n",
      "ep 317: ep_len:284 episode reward: total was -48.430000. running mean: -69.067284\n",
      "epsilon:0.185902 episode_count: 2226. steps_count: 983810.000000\n",
      "Time elapsed:  2776.423844099045\n",
      "ep 318: ep_len:666 episode reward: total was -168.500000. running mean: -70.061611\n",
      "ep 318: ep_len:561 episode reward: total was -188.380000. running mean: -71.244795\n",
      "ep 318: ep_len:79 episode reward: total was -18.840000. running mean: -70.720747\n",
      "ep 318: ep_len:625 episode reward: total was -137.570000. running mean: -71.389239\n",
      "ep 318: ep_len:3 episode reward: total was 0.000000. running mean: -70.675347\n",
      "ep 318: ep_len:500 episode reward: total was -91.260000. running mean: -70.881194\n",
      "ep 318: ep_len:526 episode reward: total was -40.250000. running mean: -70.574882\n",
      "epsilon:0.185858 episode_count: 2233. steps_count: 986770.000000\n",
      "Time elapsed:  2784.5248861312866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 319: ep_len:667 episode reward: total was -85.170000. running mean: -70.720833\n",
      "ep 319: ep_len:636 episode reward: total was -110.080000. running mean: -71.114425\n",
      "ep 319: ep_len:592 episode reward: total was -98.700000. running mean: -71.390280\n",
      "ep 319: ep_len:500 episode reward: total was -47.000000. running mean: -71.146377\n",
      "ep 319: ep_len:3 episode reward: total was 0.000000. running mean: -70.434914\n",
      "ep 319: ep_len:500 episode reward: total was -57.750000. running mean: -70.308065\n",
      "ep 319: ep_len:547 episode reward: total was -82.120000. running mean: -70.426184\n",
      "epsilon:0.185813 episode_count: 2240. steps_count: 990215.000000\n",
      "Time elapsed:  2793.6050260066986\n",
      "ep 320: ep_len:265 episode reward: total was -30.800000. running mean: -70.029922\n",
      "ep 320: ep_len:500 episode reward: total was -55.310000. running mean: -69.882723\n",
      "ep 320: ep_len:384 episode reward: total was -55.560000. running mean: -69.739496\n",
      "ep 320: ep_len:588 episode reward: total was -35.680000. running mean: -69.398901\n",
      "ep 320: ep_len:128 episode reward: total was -27.670000. running mean: -68.981612\n",
      "ep 320: ep_len:553 episode reward: total was -75.480000. running mean: -69.046596\n",
      "ep 320: ep_len:500 episode reward: total was -117.280000. running mean: -69.528930\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.185769 episode_count: 2247. steps_count: 993133.000000\n",
      "Time elapsed:  2806.1094098091125\n",
      "ep 321: ep_len:531 episode reward: total was -12.340000. running mean: -68.957040\n",
      "ep 321: ep_len:553 episode reward: total was -82.530000. running mean: -69.092770\n",
      "ep 321: ep_len:525 episode reward: total was -106.380000. running mean: -69.465642\n",
      "ep 321: ep_len:518 episode reward: total was -83.440000. running mean: -69.605386\n",
      "ep 321: ep_len:69 episode reward: total was -35.340000. running mean: -69.262732\n",
      "ep 321: ep_len:512 episode reward: total was -104.560000. running mean: -69.615705\n",
      "ep 321: ep_len:534 episode reward: total was -60.740000. running mean: -69.526948\n",
      "epsilon:0.185725 episode_count: 2254. steps_count: 996375.000000\n",
      "Time elapsed:  2815.0920429229736\n",
      "ep 322: ep_len:119 episode reward: total was -7.080000. running mean: -68.902478\n",
      "ep 322: ep_len:645 episode reward: total was -153.980000. running mean: -69.753253\n",
      "ep 322: ep_len:443 episode reward: total was -63.480000. running mean: -69.690521\n",
      "ep 322: ep_len:500 episode reward: total was -88.440000. running mean: -69.878016\n",
      "ep 322: ep_len:3 episode reward: total was -1.500000. running mean: -69.194235\n",
      "ep 322: ep_len:503 episode reward: total was -90.930000. running mean: -69.411593\n",
      "ep 322: ep_len:532 episode reward: total was -101.390000. running mean: -69.731377\n",
      "epsilon:0.185680 episode_count: 2261. steps_count: 999120.000000\n",
      "Time elapsed:  2822.5883190631866\n",
      "ep 323: ep_len:545 episode reward: total was -78.660000. running mean: -69.820663\n",
      "ep 323: ep_len:692 episode reward: total was -94.100000. running mean: -70.063457\n",
      "ep 323: ep_len:513 episode reward: total was -53.570000. running mean: -69.898522\n",
      "ep 323: ep_len:581 episode reward: total was -75.320000. running mean: -69.952737\n",
      "ep 323: ep_len:112 episode reward: total was 12.230000. running mean: -69.130910\n",
      "ep 323: ep_len:624 episode reward: total was -115.980000. running mean: -69.599400\n",
      "ep 323: ep_len:500 episode reward: total was -100.630000. running mean: -69.909706\n",
      "epsilon:0.185636 episode_count: 2268. steps_count: 1002687.000000\n",
      "Time elapsed:  2832.0003168582916\n",
      "ep 324: ep_len:590 episode reward: total was -34.250000. running mean: -69.553109\n",
      "ep 324: ep_len:539 episode reward: total was -42.150000. running mean: -69.279078\n",
      "ep 324: ep_len:500 episode reward: total was -64.310000. running mean: -69.229388\n",
      "ep 324: ep_len:500 episode reward: total was -54.570000. running mean: -69.082794\n",
      "ep 324: ep_len:3 episode reward: total was -1.500000. running mean: -68.406966\n",
      "ep 324: ep_len:550 episode reward: total was -57.960000. running mean: -68.302496\n",
      "ep 324: ep_len:541 episode reward: total was -106.190000. running mean: -68.681371\n",
      "epsilon:0.185592 episode_count: 2275. steps_count: 1005910.000000\n",
      "Time elapsed:  2840.6078686714172\n",
      "ep 325: ep_len:508 episode reward: total was -103.990000. running mean: -69.034457\n",
      "ep 325: ep_len:508 episode reward: total was -62.910000. running mean: -68.973213\n",
      "ep 325: ep_len:542 episode reward: total was -101.200000. running mean: -69.295481\n",
      "ep 325: ep_len:500 episode reward: total was -49.440000. running mean: -69.096926\n",
      "ep 325: ep_len:3 episode reward: total was 0.000000. running mean: -68.405957\n",
      "ep 325: ep_len:603 episode reward: total was -91.950000. running mean: -68.641397\n",
      "ep 325: ep_len:626 episode reward: total was -83.140000. running mean: -68.786383\n",
      "epsilon:0.185547 episode_count: 2282. steps_count: 1009200.000000\n",
      "Time elapsed:  2849.313858270645\n",
      "ep 326: ep_len:578 episode reward: total was -40.280000. running mean: -68.501319\n",
      "ep 326: ep_len:501 episode reward: total was -48.190000. running mean: -68.298206\n",
      "ep 326: ep_len:360 episode reward: total was -1.240000. running mean: -67.627624\n",
      "ep 326: ep_len:376 episode reward: total was -32.790000. running mean: -67.279248\n",
      "ep 326: ep_len:84 episode reward: total was -30.790000. running mean: -66.914355\n",
      "ep 326: ep_len:657 episode reward: total was -145.560000. running mean: -67.700812\n",
      "ep 326: ep_len:286 episode reward: total was -80.770000. running mean: -67.831504\n",
      "epsilon:0.185503 episode_count: 2289. steps_count: 1012042.000000\n",
      "Time elapsed:  2857.0020055770874\n",
      "ep 327: ep_len:500 episode reward: total was -35.060000. running mean: -67.503789\n",
      "ep 327: ep_len:591 episode reward: total was -75.170000. running mean: -67.580451\n",
      "ep 327: ep_len:501 episode reward: total was -55.250000. running mean: -67.457146\n",
      "ep 327: ep_len:554 episode reward: total was -67.910000. running mean: -67.461675\n",
      "ep 327: ep_len:33 episode reward: total was 7.500000. running mean: -66.712058\n",
      "ep 327: ep_len:522 episode reward: total was -85.350000. running mean: -66.898437\n",
      "ep 327: ep_len:315 episode reward: total was -86.840000. running mean: -67.097853\n",
      "epsilon:0.185459 episode_count: 2296. steps_count: 1015058.000000\n",
      "Time elapsed:  2865.021165370941\n",
      "ep 328: ep_len:131 episode reward: total was -0.990000. running mean: -66.436774\n",
      "ep 328: ep_len:579 episode reward: total was -90.440000. running mean: -66.676807\n",
      "ep 328: ep_len:500 episode reward: total was -107.170000. running mean: -67.081739\n",
      "ep 328: ep_len:516 episode reward: total was -173.700000. running mean: -68.147921\n",
      "ep 328: ep_len:3 episode reward: total was 0.000000. running mean: -67.466442\n",
      "ep 328: ep_len:510 episode reward: total was -98.520000. running mean: -67.776978\n",
      "ep 328: ep_len:192 episode reward: total was -33.030000. running mean: -67.429508\n",
      "epsilon:0.185414 episode_count: 2303. steps_count: 1017489.000000\n",
      "Time elapsed:  2871.714696407318\n",
      "ep 329: ep_len:541 episode reward: total was -119.250000. running mean: -67.947713\n",
      "ep 329: ep_len:302 episode reward: total was -39.460000. running mean: -67.662836\n",
      "ep 329: ep_len:500 episode reward: total was -95.660000. running mean: -67.942807\n",
      "ep 329: ep_len:522 episode reward: total was -112.510000. running mean: -68.388479\n",
      "ep 329: ep_len:3 episode reward: total was -1.500000. running mean: -67.719594\n",
      "ep 329: ep_len:180 episode reward: total was -34.830000. running mean: -67.390699\n",
      "ep 329: ep_len:500 episode reward: total was -46.610000. running mean: -67.182892\n",
      "epsilon:0.185370 episode_count: 2310. steps_count: 1020037.000000\n",
      "Time elapsed:  2878.5670309066772\n",
      "ep 330: ep_len:548 episode reward: total was -37.410000. running mean: -66.885163\n",
      "ep 330: ep_len:585 episode reward: total was -21.310000. running mean: -66.429411\n",
      "ep 330: ep_len:530 episode reward: total was -63.060000. running mean: -66.395717\n",
      "ep 330: ep_len:401 episode reward: total was -27.130000. running mean: -66.003060\n",
      "ep 330: ep_len:3 episode reward: total was 0.000000. running mean: -65.343029\n",
      "ep 330: ep_len:518 episode reward: total was -95.400000. running mean: -65.643599\n",
      "ep 330: ep_len:564 episode reward: total was -38.040000. running mean: -65.367563\n",
      "epsilon:0.185326 episode_count: 2317. steps_count: 1023186.000000\n",
      "Time elapsed:  2887.2518441677094\n",
      "ep 331: ep_len:500 episode reward: total was -105.920000. running mean: -65.773087\n",
      "ep 331: ep_len:536 episode reward: total was -97.860000. running mean: -66.093956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 331: ep_len:533 episode reward: total was -159.470000. running mean: -67.027717\n",
      "ep 331: ep_len:500 episode reward: total was -45.330000. running mean: -66.810740\n",
      "ep 331: ep_len:3 episode reward: total was 0.000000. running mean: -66.142632\n",
      "ep 331: ep_len:598 episode reward: total was -96.530000. running mean: -66.446506\n",
      "ep 331: ep_len:500 episode reward: total was -50.610000. running mean: -66.288141\n",
      "epsilon:0.185281 episode_count: 2324. steps_count: 1026356.000000\n",
      "Time elapsed:  2895.605192899704\n",
      "ep 332: ep_len:194 episode reward: total was -0.860000. running mean: -65.633859\n",
      "ep 332: ep_len:549 episode reward: total was -110.620000. running mean: -66.083721\n",
      "ep 332: ep_len:405 episode reward: total was -93.600000. running mean: -66.358884\n",
      "ep 332: ep_len:589 episode reward: total was -36.640000. running mean: -66.061695\n",
      "ep 332: ep_len:3 episode reward: total was 0.000000. running mean: -65.401078\n",
      "ep 332: ep_len:502 episode reward: total was -118.230000. running mean: -65.929367\n",
      "ep 332: ep_len:502 episode reward: total was -61.180000. running mean: -65.881873\n",
      "epsilon:0.185237 episode_count: 2331. steps_count: 1029100.000000\n",
      "Time elapsed:  2903.1145730018616\n",
      "ep 333: ep_len:531 episode reward: total was -18.270000. running mean: -65.405755\n",
      "ep 333: ep_len:500 episode reward: total was -99.960000. running mean: -65.751297\n",
      "ep 333: ep_len:446 episode reward: total was -26.990000. running mean: -65.363684\n",
      "ep 333: ep_len:560 episode reward: total was -65.660000. running mean: -65.366647\n",
      "ep 333: ep_len:94 episode reward: total was -28.270000. running mean: -64.995681\n",
      "ep 333: ep_len:500 episode reward: total was -88.670000. running mean: -65.232424\n",
      "ep 333: ep_len:586 episode reward: total was -49.870000. running mean: -65.078800\n",
      "epsilon:0.185193 episode_count: 2338. steps_count: 1032317.000000\n",
      "Time elapsed:  2911.5359559059143\n",
      "ep 334: ep_len:624 episode reward: total was -184.560000. running mean: -66.273612\n",
      "ep 334: ep_len:520 episode reward: total was -116.190000. running mean: -66.772776\n",
      "ep 334: ep_len:76 episode reward: total was -3.290000. running mean: -66.137948\n",
      "ep 334: ep_len:514 episode reward: total was -97.650000. running mean: -66.453068\n",
      "ep 334: ep_len:116 episode reward: total was -17.700000. running mean: -65.965538\n",
      "ep 334: ep_len:500 episode reward: total was -121.720000. running mean: -66.523082\n",
      "ep 334: ep_len:526 episode reward: total was -112.480000. running mean: -66.982652\n",
      "epsilon:0.185148 episode_count: 2345. steps_count: 1035193.000000\n",
      "Time elapsed:  2919.1962983608246\n",
      "ep 335: ep_len:500 episode reward: total was -14.190000. running mean: -66.454725\n",
      "ep 335: ep_len:542 episode reward: total was -93.470000. running mean: -66.724878\n",
      "ep 335: ep_len:677 episode reward: total was -134.070000. running mean: -67.398329\n",
      "ep 335: ep_len:512 episode reward: total was -110.470000. running mean: -67.829046\n",
      "ep 335: ep_len:34 episode reward: total was -20.000000. running mean: -67.350755\n",
      "ep 335: ep_len:500 episode reward: total was -41.790000. running mean: -67.095148\n",
      "ep 335: ep_len:539 episode reward: total was -58.070000. running mean: -67.004896\n",
      "epsilon:0.185104 episode_count: 2352. steps_count: 1038497.000000\n",
      "Time elapsed:  2928.4368262290955\n",
      "ep 336: ep_len:604 episode reward: total was -126.510000. running mean: -67.599947\n",
      "ep 336: ep_len:506 episode reward: total was -67.560000. running mean: -67.599548\n",
      "ep 336: ep_len:581 episode reward: total was -121.760000. running mean: -68.141152\n",
      "ep 336: ep_len:527 episode reward: total was -127.660000. running mean: -68.736341\n",
      "ep 336: ep_len:3 episode reward: total was 0.000000. running mean: -68.048977\n",
      "ep 336: ep_len:500 episode reward: total was -97.120000. running mean: -68.339688\n",
      "ep 336: ep_len:210 episode reward: total was -50.780000. running mean: -68.164091\n",
      "epsilon:0.185060 episode_count: 2359. steps_count: 1041428.000000\n",
      "Time elapsed:  2936.030569791794\n",
      "ep 337: ep_len:588 episode reward: total was -127.170000. running mean: -68.754150\n",
      "ep 337: ep_len:500 episode reward: total was -92.710000. running mean: -68.993708\n",
      "ep 337: ep_len:64 episode reward: total was -2.890000. running mean: -68.332671\n",
      "ep 337: ep_len:511 episode reward: total was -67.310000. running mean: -68.322445\n",
      "ep 337: ep_len:88 episode reward: total was -32.280000. running mean: -67.962020\n",
      "ep 337: ep_len:557 episode reward: total was -97.390000. running mean: -68.256300\n",
      "ep 337: ep_len:500 episode reward: total was -42.340000. running mean: -67.997137\n",
      "epsilon:0.185015 episode_count: 2366. steps_count: 1044236.000000\n",
      "Time elapsed:  2943.8594892024994\n",
      "ep 338: ep_len:514 episode reward: total was -60.060000. running mean: -67.917766\n",
      "ep 338: ep_len:590 episode reward: total was -88.530000. running mean: -68.123888\n",
      "ep 338: ep_len:500 episode reward: total was -68.210000. running mean: -68.124749\n",
      "ep 338: ep_len:502 episode reward: total was -79.570000. running mean: -68.239201\n",
      "ep 338: ep_len:3 episode reward: total was -3.000000. running mean: -67.586809\n",
      "ep 338: ep_len:519 episode reward: total was -151.970000. running mean: -68.430641\n",
      "ep 338: ep_len:332 episode reward: total was -103.490000. running mean: -68.781235\n",
      "epsilon:0.184971 episode_count: 2373. steps_count: 1047196.000000\n",
      "Time elapsed:  2951.975729942322\n",
      "ep 339: ep_len:500 episode reward: total was -30.540000. running mean: -68.398823\n",
      "ep 339: ep_len:549 episode reward: total was -2.640000. running mean: -67.741234\n",
      "ep 339: ep_len:500 episode reward: total was -67.400000. running mean: -67.737822\n",
      "ep 339: ep_len:500 episode reward: total was -167.010000. running mean: -68.730544\n",
      "ep 339: ep_len:91 episode reward: total was 8.250000. running mean: -67.960738\n",
      "ep 339: ep_len:500 episode reward: total was -63.010000. running mean: -67.911231\n",
      "ep 339: ep_len:342 episode reward: total was -60.070000. running mean: -67.832819\n",
      "epsilon:0.184927 episode_count: 2380. steps_count: 1050178.000000\n",
      "Time elapsed:  2960.4243907928467\n",
      "ep 340: ep_len:643 episode reward: total was -94.290000. running mean: -68.097390\n",
      "ep 340: ep_len:301 episode reward: total was -30.900000. running mean: -67.725417\n",
      "ep 340: ep_len:609 episode reward: total was -128.820000. running mean: -68.336362\n",
      "ep 340: ep_len:529 episode reward: total was -86.770000. running mean: -68.520699\n",
      "ep 340: ep_len:3 episode reward: total was 0.000000. running mean: -67.835492\n",
      "ep 340: ep_len:500 episode reward: total was -103.580000. running mean: -68.192937\n",
      "ep 340: ep_len:508 episode reward: total was -81.790000. running mean: -68.328908\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.184882 episode_count: 2387. steps_count: 1053271.000000\n",
      "Time elapsed:  2973.4488134384155\n",
      "ep 341: ep_len:226 episode reward: total was 5.590000. running mean: -67.589718\n",
      "ep 341: ep_len:501 episode reward: total was -72.420000. running mean: -67.638021\n",
      "ep 341: ep_len:631 episode reward: total was -172.660000. running mean: -68.688241\n",
      "ep 341: ep_len:51 episode reward: total was -9.300000. running mean: -68.094359\n",
      "ep 341: ep_len:2 episode reward: total was -0.500000. running mean: -67.418415\n",
      "ep 341: ep_len:544 episode reward: total was -159.530000. running mean: -68.339531\n",
      "ep 341: ep_len:502 episode reward: total was -123.980000. running mean: -68.895936\n",
      "epsilon:0.184838 episode_count: 2394. steps_count: 1055728.000000\n",
      "Time elapsed:  2980.455952167511\n",
      "ep 342: ep_len:533 episode reward: total was -70.020000. running mean: -68.907176\n",
      "ep 342: ep_len:524 episode reward: total was -15.250000. running mean: -68.370604\n",
      "ep 342: ep_len:500 episode reward: total was -45.520000. running mean: -68.142098\n",
      "ep 342: ep_len:500 episode reward: total was -35.170000. running mean: -67.812377\n",
      "ep 342: ep_len:81 episode reward: total was 0.190000. running mean: -67.132354\n",
      "ep 342: ep_len:500 episode reward: total was -66.800000. running mean: -67.129030\n",
      "ep 342: ep_len:538 episode reward: total was -91.430000. running mean: -67.372040\n",
      "epsilon:0.184794 episode_count: 2401. steps_count: 1058904.000000\n",
      "Time elapsed:  2988.8301627635956\n",
      "ep 343: ep_len:553 episode reward: total was -41.800000. running mean: -67.116319\n",
      "ep 343: ep_len:586 episode reward: total was -74.420000. running mean: -67.189356\n",
      "ep 343: ep_len:500 episode reward: total was -77.210000. running mean: -67.289563\n",
      "ep 343: ep_len:505 episode reward: total was -72.090000. running mean: -67.337567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 343: ep_len:101 episode reward: total was -15.740000. running mean: -66.821591\n",
      "ep 343: ep_len:670 episode reward: total was -98.690000. running mean: -67.140275\n",
      "ep 343: ep_len:516 episode reward: total was -64.160000. running mean: -67.110473\n",
      "epsilon:0.184749 episode_count: 2408. steps_count: 1062335.000000\n",
      "Time elapsed:  2998.336267232895\n",
      "ep 344: ep_len:551 episode reward: total was -91.430000. running mean: -67.353668\n",
      "ep 344: ep_len:642 episode reward: total was -13.160000. running mean: -66.811731\n",
      "ep 344: ep_len:422 episode reward: total was -39.780000. running mean: -66.541414\n",
      "ep 344: ep_len:500 episode reward: total was -55.840000. running mean: -66.434400\n",
      "ep 344: ep_len:3 episode reward: total was 0.000000. running mean: -65.770056\n",
      "ep 344: ep_len:575 episode reward: total was -69.810000. running mean: -65.810455\n",
      "ep 344: ep_len:536 episode reward: total was -84.790000. running mean: -66.000251\n",
      "epsilon:0.184705 episode_count: 2415. steps_count: 1065564.000000\n",
      "Time elapsed:  3007.038836956024\n",
      "ep 345: ep_len:500 episode reward: total was -13.980000. running mean: -65.480048\n",
      "ep 345: ep_len:500 episode reward: total was -122.280000. running mean: -66.048048\n",
      "ep 345: ep_len:639 episode reward: total was -82.910000. running mean: -66.216667\n",
      "ep 345: ep_len:505 episode reward: total was -43.080000. running mean: -65.985301\n",
      "ep 345: ep_len:94 episode reward: total was 0.630000. running mean: -65.319148\n",
      "ep 345: ep_len:607 episode reward: total was -116.040000. running mean: -65.826356\n",
      "ep 345: ep_len:521 episode reward: total was -82.240000. running mean: -65.990493\n",
      "epsilon:0.184661 episode_count: 2422. steps_count: 1068930.000000\n",
      "Time elapsed:  3016.274822950363\n",
      "ep 346: ep_len:501 episode reward: total was -138.290000. running mean: -66.713488\n",
      "ep 346: ep_len:528 episode reward: total was -103.560000. running mean: -67.081953\n",
      "ep 346: ep_len:548 episode reward: total was -108.100000. running mean: -67.492133\n",
      "ep 346: ep_len:597 episode reward: total was -33.770000. running mean: -67.154912\n",
      "ep 346: ep_len:80 episode reward: total was 2.660000. running mean: -66.456763\n",
      "ep 346: ep_len:612 episode reward: total was -159.250000. running mean: -67.384695\n",
      "ep 346: ep_len:527 episode reward: total was -45.270000. running mean: -67.163548\n",
      "epsilon:0.184616 episode_count: 2429. steps_count: 1072323.000000\n",
      "Time elapsed:  3025.0896756649017\n",
      "ep 347: ep_len:539 episode reward: total was -68.140000. running mean: -67.173313\n",
      "ep 347: ep_len:554 episode reward: total was 1.180000. running mean: -66.489780\n",
      "ep 347: ep_len:573 episode reward: total was -79.700000. running mean: -66.621882\n",
      "ep 347: ep_len:506 episode reward: total was -45.420000. running mean: -66.409863\n",
      "ep 347: ep_len:3 episode reward: total was 0.000000. running mean: -65.745764\n",
      "ep 347: ep_len:588 episode reward: total was -97.700000. running mean: -66.065307\n",
      "ep 347: ep_len:507 episode reward: total was -128.190000. running mean: -66.686554\n",
      "epsilon:0.184572 episode_count: 2436. steps_count: 1075593.000000\n",
      "Time elapsed:  3033.81049990654\n",
      "ep 348: ep_len:654 episode reward: total was -91.390000. running mean: -66.933588\n",
      "ep 348: ep_len:518 episode reward: total was -37.090000. running mean: -66.635152\n",
      "ep 348: ep_len:440 episode reward: total was -22.980000. running mean: -66.198601\n",
      "ep 348: ep_len:56 episode reward: total was -11.680000. running mean: -65.653415\n",
      "ep 348: ep_len:3 episode reward: total was 0.000000. running mean: -64.996881\n",
      "ep 348: ep_len:504 episode reward: total was -70.390000. running mean: -65.050812\n",
      "ep 348: ep_len:298 episode reward: total was -64.990000. running mean: -65.050204\n",
      "epsilon:0.184528 episode_count: 2443. steps_count: 1078066.000000\n",
      "Time elapsed:  3040.8548533916473\n",
      "ep 349: ep_len:547 episode reward: total was -103.750000. running mean: -65.437202\n",
      "ep 349: ep_len:500 episode reward: total was -33.290000. running mean: -65.115730\n",
      "ep 349: ep_len:549 episode reward: total was -129.010000. running mean: -65.754672\n",
      "ep 349: ep_len:614 episode reward: total was -61.980000. running mean: -65.716926\n",
      "ep 349: ep_len:3 episode reward: total was 0.000000. running mean: -65.059756\n",
      "ep 349: ep_len:542 episode reward: total was -79.830000. running mean: -65.207459\n",
      "ep 349: ep_len:602 episode reward: total was -61.370000. running mean: -65.169084\n",
      "epsilon:0.184483 episode_count: 2450. steps_count: 1081423.000000\n",
      "Time elapsed:  3049.5564839839935\n",
      "ep 350: ep_len:648 episode reward: total was -94.820000. running mean: -65.465593\n",
      "ep 350: ep_len:519 episode reward: total was -54.460000. running mean: -65.355537\n",
      "ep 350: ep_len:500 episode reward: total was -86.960000. running mean: -65.571582\n",
      "ep 350: ep_len:500 episode reward: total was -66.620000. running mean: -65.582066\n",
      "ep 350: ep_len:119 episode reward: total was -7.260000. running mean: -64.998846\n",
      "ep 350: ep_len:500 episode reward: total was -40.690000. running mean: -64.755757\n",
      "ep 350: ep_len:552 episode reward: total was -135.080000. running mean: -65.458999\n",
      "epsilon:0.184439 episode_count: 2457. steps_count: 1084761.000000\n",
      "Time elapsed:  3058.5106337070465\n",
      "ep 351: ep_len:584 episode reward: total was -134.660000. running mean: -66.151009\n",
      "ep 351: ep_len:500 episode reward: total was -53.710000. running mean: -66.026599\n",
      "ep 351: ep_len:590 episode reward: total was -103.780000. running mean: -66.404133\n",
      "ep 351: ep_len:506 episode reward: total was -41.310000. running mean: -66.153192\n",
      "ep 351: ep_len:3 episode reward: total was 0.000000. running mean: -65.491660\n",
      "ep 351: ep_len:631 episode reward: total was -89.750000. running mean: -65.734244\n",
      "ep 351: ep_len:535 episode reward: total was -78.850000. running mean: -65.865401\n",
      "epsilon:0.184395 episode_count: 2464. steps_count: 1088110.000000\n",
      "Time elapsed:  3067.427674293518\n",
      "ep 352: ep_len:590 episode reward: total was -86.570000. running mean: -66.072447\n",
      "ep 352: ep_len:596 episode reward: total was -55.470000. running mean: -65.966423\n",
      "ep 352: ep_len:521 episode reward: total was -59.960000. running mean: -65.906358\n",
      "ep 352: ep_len:596 episode reward: total was -36.010000. running mean: -65.607395\n",
      "ep 352: ep_len:3 episode reward: total was -1.500000. running mean: -64.966321\n",
      "ep 352: ep_len:654 episode reward: total was -94.250000. running mean: -65.259158\n",
      "ep 352: ep_len:633 episode reward: total was -141.450000. running mean: -66.021066\n",
      "epsilon:0.184350 episode_count: 2471. steps_count: 1091703.000000\n",
      "Time elapsed:  3077.747591495514\n",
      "ep 353: ep_len:546 episode reward: total was -125.280000. running mean: -66.613655\n",
      "ep 353: ep_len:617 episode reward: total was -49.220000. running mean: -66.439719\n",
      "ep 353: ep_len:470 episode reward: total was -65.930000. running mean: -66.434622\n",
      "ep 353: ep_len:500 episode reward: total was -81.880000. running mean: -66.589075\n",
      "ep 353: ep_len:3 episode reward: total was 0.000000. running mean: -65.923185\n",
      "ep 353: ep_len:604 episode reward: total was -90.010000. running mean: -66.164053\n",
      "ep 353: ep_len:567 episode reward: total was -54.180000. running mean: -66.044212\n",
      "epsilon:0.184306 episode_count: 2478. steps_count: 1095010.000000\n",
      "Time elapsed:  3086.328586101532\n",
      "ep 354: ep_len:544 episode reward: total was -70.540000. running mean: -66.089170\n",
      "ep 354: ep_len:617 episode reward: total was -51.630000. running mean: -65.944579\n",
      "ep 354: ep_len:609 episode reward: total was -63.860000. running mean: -65.923733\n",
      "ep 354: ep_len:170 episode reward: total was -6.880000. running mean: -65.333295\n",
      "ep 354: ep_len:3 episode reward: total was -1.500000. running mean: -64.694962\n",
      "ep 354: ep_len:538 episode reward: total was -23.000000. running mean: -64.278013\n",
      "ep 354: ep_len:177 episode reward: total was -27.300000. running mean: -63.908233\n",
      "epsilon:0.184262 episode_count: 2485. steps_count: 1097668.000000\n",
      "Time elapsed:  3093.3076622486115\n",
      "ep 355: ep_len:647 episode reward: total was -118.860000. running mean: -64.457750\n",
      "ep 355: ep_len:655 episode reward: total was -27.990000. running mean: -64.093073\n",
      "ep 355: ep_len:67 episode reward: total was -2.370000. running mean: -63.475842\n",
      "ep 355: ep_len:500 episode reward: total was -48.330000. running mean: -63.324384\n",
      "ep 355: ep_len:96 episode reward: total was -16.740000. running mean: -62.858540\n",
      "ep 355: ep_len:593 episode reward: total was -70.380000. running mean: -62.933754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 355: ep_len:563 episode reward: total was -66.630000. running mean: -62.970717\n",
      "epsilon:0.184217 episode_count: 2492. steps_count: 1100789.000000\n",
      "Time elapsed:  3101.79647231102\n",
      "ep 356: ep_len:577 episode reward: total was -83.420000. running mean: -63.175210\n",
      "ep 356: ep_len:500 episode reward: total was 28.960000. running mean: -62.253858\n",
      "ep 356: ep_len:592 episode reward: total was -108.930000. running mean: -62.720619\n",
      "ep 356: ep_len:513 episode reward: total was -98.390000. running mean: -63.077313\n",
      "ep 356: ep_len:94 episode reward: total was 11.740000. running mean: -62.329140\n",
      "ep 356: ep_len:533 episode reward: total was -101.680000. running mean: -62.722648\n",
      "ep 356: ep_len:639 episode reward: total was -125.390000. running mean: -63.349322\n",
      "epsilon:0.184173 episode_count: 2499. steps_count: 1104237.000000\n",
      "Time elapsed:  3110.7901685237885\n",
      "ep 357: ep_len:535 episode reward: total was -116.140000. running mean: -63.877229\n",
      "ep 357: ep_len:500 episode reward: total was -71.640000. running mean: -63.954856\n",
      "ep 357: ep_len:446 episode reward: total was -53.100000. running mean: -63.846308\n",
      "ep 357: ep_len:508 episode reward: total was -64.230000. running mean: -63.850145\n",
      "ep 357: ep_len:60 episode reward: total was -38.420000. running mean: -63.595843\n",
      "ep 357: ep_len:162 episode reward: total was -23.510000. running mean: -63.194985\n",
      "ep 357: ep_len:604 episode reward: total was -83.850000. running mean: -63.401535\n",
      "epsilon:0.184129 episode_count: 2506. steps_count: 1107052.000000\n",
      "Time elapsed:  3118.2217490673065\n",
      "ep 358: ep_len:636 episode reward: total was -119.080000. running mean: -63.958320\n",
      "ep 358: ep_len:606 episode reward: total was -117.380000. running mean: -64.492536\n",
      "ep 358: ep_len:530 episode reward: total was -88.430000. running mean: -64.731911\n",
      "ep 358: ep_len:511 episode reward: total was -39.210000. running mean: -64.476692\n",
      "ep 358: ep_len:44 episode reward: total was -34.990000. running mean: -64.181825\n",
      "ep 358: ep_len:315 episode reward: total was -34.860000. running mean: -63.888607\n",
      "ep 358: ep_len:530 episode reward: total was -60.590000. running mean: -63.855621\n",
      "epsilon:0.184084 episode_count: 2513. steps_count: 1110224.000000\n",
      "Time elapsed:  3126.855781555176\n",
      "ep 359: ep_len:229 episode reward: total was -21.230000. running mean: -63.429365\n",
      "ep 359: ep_len:514 episode reward: total was 20.000000. running mean: -62.595071\n",
      "ep 359: ep_len:527 episode reward: total was -95.280000. running mean: -62.921920\n",
      "ep 359: ep_len:500 episode reward: total was -66.570000. running mean: -62.958401\n",
      "ep 359: ep_len:87 episode reward: total was -8.810000. running mean: -62.416917\n",
      "ep 359: ep_len:531 episode reward: total was -74.740000. running mean: -62.540148\n",
      "ep 359: ep_len:555 episode reward: total was -96.780000. running mean: -62.882546\n",
      "epsilon:0.184040 episode_count: 2520. steps_count: 1113167.000000\n",
      "Time elapsed:  3134.802323818207\n",
      "ep 360: ep_len:134 episode reward: total was -10.570000. running mean: -62.359421\n",
      "ep 360: ep_len:339 episode reward: total was -82.790000. running mean: -62.563727\n",
      "ep 360: ep_len:367 episode reward: total was -40.810000. running mean: -62.346189\n",
      "ep 360: ep_len:524 episode reward: total was -93.270000. running mean: -62.655427\n",
      "ep 360: ep_len:3 episode reward: total was -1.500000. running mean: -62.043873\n",
      "ep 360: ep_len:500 episode reward: total was -86.460000. running mean: -62.288034\n",
      "ep 360: ep_len:517 episode reward: total was -64.490000. running mean: -62.310054\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.183996 episode_count: 2527. steps_count: 1115551.000000\n",
      "Time elapsed:  3146.333565711975\n",
      "ep 361: ep_len:527 episode reward: total was -90.910000. running mean: -62.596054\n",
      "ep 361: ep_len:548 episode reward: total was -157.030000. running mean: -63.540393\n",
      "ep 361: ep_len:449 episode reward: total was -5.010000. running mean: -62.955089\n",
      "ep 361: ep_len:554 episode reward: total was -18.910000. running mean: -62.514638\n",
      "ep 361: ep_len:88 episode reward: total was -3.260000. running mean: -61.922092\n",
      "ep 361: ep_len:637 episode reward: total was -87.170000. running mean: -62.174571\n",
      "ep 361: ep_len:602 episode reward: total was -146.610000. running mean: -63.018925\n",
      "epsilon:0.183951 episode_count: 2534. steps_count: 1118956.000000\n",
      "Time elapsed:  3155.529310464859\n",
      "ep 362: ep_len:519 episode reward: total was -31.220000. running mean: -62.700936\n",
      "ep 362: ep_len:500 episode reward: total was -9.330000. running mean: -62.167227\n",
      "ep 362: ep_len:595 episode reward: total was -96.710000. running mean: -62.512654\n",
      "ep 362: ep_len:118 episode reward: total was -9.970000. running mean: -61.987228\n",
      "ep 362: ep_len:1 episode reward: total was -1.000000. running mean: -61.377356\n",
      "ep 362: ep_len:500 episode reward: total was -62.320000. running mean: -61.386782\n",
      "ep 362: ep_len:500 episode reward: total was -94.800000. running mean: -61.720914\n",
      "epsilon:0.183907 episode_count: 2541. steps_count: 1121689.000000\n",
      "Time elapsed:  3162.858656644821\n",
      "ep 363: ep_len:562 episode reward: total was -151.620000. running mean: -62.619905\n",
      "ep 363: ep_len:663 episode reward: total was -56.130000. running mean: -62.555006\n",
      "ep 363: ep_len:428 episode reward: total was -67.990000. running mean: -62.609356\n",
      "ep 363: ep_len:515 episode reward: total was -127.090000. running mean: -63.254162\n",
      "ep 363: ep_len:107 episode reward: total was -23.820000. running mean: -62.859821\n",
      "ep 363: ep_len:261 episode reward: total was -42.790000. running mean: -62.659123\n",
      "ep 363: ep_len:329 episode reward: total was -126.890000. running mean: -63.301431\n",
      "epsilon:0.183863 episode_count: 2548. steps_count: 1124554.000000\n",
      "Time elapsed:  3170.4081642627716\n",
      "ep 364: ep_len:501 episode reward: total was -49.690000. running mean: -63.165317\n",
      "ep 364: ep_len:517 episode reward: total was -104.610000. running mean: -63.579764\n",
      "ep 364: ep_len:802 episode reward: total was -241.530000. running mean: -65.359266\n",
      "ep 364: ep_len:626 episode reward: total was -76.770000. running mean: -65.473374\n",
      "ep 364: ep_len:101 episode reward: total was -72.290000. running mean: -65.541540\n",
      "ep 364: ep_len:500 episode reward: total was -49.110000. running mean: -65.377224\n",
      "ep 364: ep_len:203 episode reward: total was -45.650000. running mean: -65.179952\n",
      "epsilon:0.183818 episode_count: 2555. steps_count: 1127804.000000\n",
      "Time elapsed:  3178.8713324069977\n",
      "ep 365: ep_len:500 episode reward: total was -30.840000. running mean: -64.836553\n",
      "ep 365: ep_len:266 episode reward: total was -56.100000. running mean: -64.749187\n",
      "ep 365: ep_len:502 episode reward: total was -60.230000. running mean: -64.703995\n",
      "ep 365: ep_len:500 episode reward: total was -16.470000. running mean: -64.221655\n",
      "ep 365: ep_len:127 episode reward: total was -15.260000. running mean: -63.732039\n",
      "ep 365: ep_len:517 episode reward: total was -71.270000. running mean: -63.807418\n",
      "ep 365: ep_len:176 episode reward: total was -18.710000. running mean: -63.356444\n",
      "epsilon:0.183774 episode_count: 2562. steps_count: 1130392.000000\n",
      "Time elapsed:  3185.8835730552673\n",
      "ep 366: ep_len:623 episode reward: total was -97.690000. running mean: -63.699780\n",
      "ep 366: ep_len:503 episode reward: total was -11.400000. running mean: -63.176782\n",
      "ep 366: ep_len:500 episode reward: total was -77.980000. running mean: -63.324814\n",
      "ep 366: ep_len:511 episode reward: total was -41.630000. running mean: -63.107866\n",
      "ep 366: ep_len:105 episode reward: total was -13.760000. running mean: -62.614387\n",
      "ep 366: ep_len:656 episode reward: total was -81.260000. running mean: -62.800843\n",
      "ep 366: ep_len:202 episode reward: total was -77.400000. running mean: -62.946835\n",
      "epsilon:0.183730 episode_count: 2569. steps_count: 1133492.000000\n",
      "Time elapsed:  3194.618486404419\n",
      "ep 367: ep_len:500 episode reward: total was -55.200000. running mean: -62.869367\n",
      "ep 367: ep_len:500 episode reward: total was -4.040000. running mean: -62.281073\n",
      "ep 367: ep_len:784 episode reward: total was -177.680000. running mean: -63.435062\n",
      "ep 367: ep_len:132 episode reward: total was -12.430000. running mean: -62.925012\n",
      "ep 367: ep_len:105 episode reward: total was -29.750000. running mean: -62.593261\n",
      "ep 367: ep_len:636 episode reward: total was -124.340000. running mean: -63.210729\n",
      "ep 367: ep_len:500 episode reward: total was -40.700000. running mean: -62.985622\n",
      "epsilon:0.183685 episode_count: 2576. steps_count: 1136649.000000\n",
      "Time elapsed:  3203.3156213760376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 368: ep_len:553 episode reward: total was -103.190000. running mean: -63.387665\n",
      "ep 368: ep_len:152 episode reward: total was -35.590000. running mean: -63.109689\n",
      "ep 368: ep_len:633 episode reward: total was -87.440000. running mean: -63.352992\n",
      "ep 368: ep_len:501 episode reward: total was -81.630000. running mean: -63.535762\n",
      "ep 368: ep_len:94 episode reward: total was -61.770000. running mean: -63.518104\n",
      "ep 368: ep_len:527 episode reward: total was -26.640000. running mean: -63.149323\n",
      "ep 368: ep_len:332 episode reward: total was -108.740000. running mean: -63.605230\n",
      "epsilon:0.183641 episode_count: 2583. steps_count: 1139441.000000\n",
      "Time elapsed:  3210.8054492473602\n",
      "ep 369: ep_len:587 episode reward: total was -93.620000. running mean: -63.905378\n",
      "ep 369: ep_len:504 episode reward: total was -56.700000. running mean: -63.833324\n",
      "ep 369: ep_len:79 episode reward: total was -9.780000. running mean: -63.292791\n",
      "ep 369: ep_len:518 episode reward: total was -35.880000. running mean: -63.018663\n",
      "ep 369: ep_len:129 episode reward: total was -17.690000. running mean: -62.565376\n",
      "ep 369: ep_len:315 episode reward: total was -48.660000. running mean: -62.426322\n",
      "ep 369: ep_len:510 episode reward: total was -81.070000. running mean: -62.612759\n",
      "epsilon:0.183597 episode_count: 2590. steps_count: 1142083.000000\n",
      "Time elapsed:  3217.9009368419647\n",
      "ep 370: ep_len:588 episode reward: total was -95.760000. running mean: -62.944232\n",
      "ep 370: ep_len:500 episode reward: total was -64.670000. running mean: -62.961489\n",
      "ep 370: ep_len:505 episode reward: total was -96.780000. running mean: -63.299674\n",
      "ep 370: ep_len:153 episode reward: total was -14.980000. running mean: -62.816478\n",
      "ep 370: ep_len:88 episode reward: total was 14.190000. running mean: -62.046413\n",
      "ep 370: ep_len:321 episode reward: total was -33.070000. running mean: -61.756649\n",
      "ep 370: ep_len:500 episode reward: total was -77.240000. running mean: -61.911482\n",
      "epsilon:0.183552 episode_count: 2597. steps_count: 1144738.000000\n",
      "Time elapsed:  3225.380974292755\n",
      "ep 371: ep_len:586 episode reward: total was -89.150000. running mean: -62.183867\n",
      "ep 371: ep_len:500 episode reward: total was -104.730000. running mean: -62.609329\n",
      "ep 371: ep_len:643 episode reward: total was -83.970000. running mean: -62.822935\n",
      "ep 371: ep_len:564 episode reward: total was -76.010000. running mean: -62.954806\n",
      "ep 371: ep_len:3 episode reward: total was 0.000000. running mean: -62.325258\n",
      "ep 371: ep_len:620 episode reward: total was -64.760000. running mean: -62.349605\n",
      "ep 371: ep_len:634 episode reward: total was -47.720000. running mean: -62.203309\n",
      "epsilon:0.183508 episode_count: 2604. steps_count: 1148288.000000\n",
      "Time elapsed:  3234.778205394745\n",
      "ep 372: ep_len:627 episode reward: total was -89.060000. running mean: -62.471876\n",
      "ep 372: ep_len:500 episode reward: total was -143.800000. running mean: -63.285158\n",
      "ep 372: ep_len:589 episode reward: total was -250.110000. running mean: -65.153406\n",
      "ep 372: ep_len:500 episode reward: total was -30.120000. running mean: -64.803072\n",
      "ep 372: ep_len:3 episode reward: total was -1.500000. running mean: -64.170041\n",
      "ep 372: ep_len:507 episode reward: total was -73.320000. running mean: -64.261541\n",
      "ep 372: ep_len:315 episode reward: total was -48.610000. running mean: -64.105025\n",
      "epsilon:0.183464 episode_count: 2611. steps_count: 1151329.000000\n",
      "Time elapsed:  3243.133592605591\n",
      "ep 373: ep_len:512 episode reward: total was -141.580000. running mean: -64.879775\n",
      "ep 373: ep_len:174 episode reward: total was -61.060000. running mean: -64.841577\n",
      "ep 373: ep_len:613 episode reward: total was -100.140000. running mean: -65.194562\n",
      "ep 373: ep_len:500 episode reward: total was -174.100000. running mean: -66.283616\n",
      "ep 373: ep_len:3 episode reward: total was 0.000000. running mean: -65.620780\n",
      "ep 373: ep_len:631 episode reward: total was -132.820000. running mean: -66.292772\n",
      "ep 373: ep_len:630 episode reward: total was -66.050000. running mean: -66.290344\n",
      "epsilon:0.183419 episode_count: 2618. steps_count: 1154392.000000\n",
      "Time elapsed:  3251.880854845047\n",
      "ep 374: ep_len:500 episode reward: total was -42.390000. running mean: -66.051341\n",
      "ep 374: ep_len:507 episode reward: total was -101.010000. running mean: -66.400927\n",
      "ep 374: ep_len:500 episode reward: total was -79.850000. running mean: -66.535418\n",
      "ep 374: ep_len:500 episode reward: total was -36.220000. running mean: -66.232264\n",
      "ep 374: ep_len:3 episode reward: total was 0.000000. running mean: -65.569941\n",
      "ep 374: ep_len:501 episode reward: total was -52.740000. running mean: -65.441642\n",
      "ep 374: ep_len:351 episode reward: total was -115.100000. running mean: -65.938226\n",
      "epsilon:0.183375 episode_count: 2625. steps_count: 1157254.000000\n",
      "Time elapsed:  3259.712674856186\n",
      "ep 375: ep_len:500 episode reward: total was -109.450000. running mean: -66.373343\n",
      "ep 375: ep_len:567 episode reward: total was -152.590000. running mean: -67.235510\n",
      "ep 375: ep_len:647 episode reward: total was -98.860000. running mean: -67.551755\n",
      "ep 375: ep_len:505 episode reward: total was -35.280000. running mean: -67.229037\n",
      "ep 375: ep_len:98 episode reward: total was -62.270000. running mean: -67.179447\n",
      "ep 375: ep_len:560 episode reward: total was -63.240000. running mean: -67.140052\n",
      "ep 375: ep_len:300 episode reward: total was -59.590000. running mean: -67.064552\n",
      "epsilon:0.183331 episode_count: 2632. steps_count: 1160431.000000\n",
      "Time elapsed:  3268.584532022476\n",
      "ep 376: ep_len:604 episode reward: total was -64.920000. running mean: -67.043106\n",
      "ep 376: ep_len:501 episode reward: total was -108.780000. running mean: -67.460475\n",
      "ep 376: ep_len:573 episode reward: total was -122.800000. running mean: -68.013870\n",
      "ep 376: ep_len:500 episode reward: total was -57.500000. running mean: -67.908732\n",
      "ep 376: ep_len:100 episode reward: total was 9.690000. running mean: -67.132744\n",
      "ep 376: ep_len:500 episode reward: total was -98.930000. running mean: -67.450717\n",
      "ep 376: ep_len:541 episode reward: total was -90.350000. running mean: -67.679710\n",
      "epsilon:0.183286 episode_count: 2639. steps_count: 1163750.000000\n",
      "Time elapsed:  3277.5048866271973\n",
      "ep 377: ep_len:666 episode reward: total was -96.590000. running mean: -67.968813\n",
      "ep 377: ep_len:503 episode reward: total was -130.070000. running mean: -68.589825\n",
      "ep 377: ep_len:626 episode reward: total was -124.330000. running mean: -69.147226\n",
      "ep 377: ep_len:505 episode reward: total was -65.540000. running mean: -69.111154\n",
      "ep 377: ep_len:3 episode reward: total was 0.000000. running mean: -68.420043\n",
      "ep 377: ep_len:500 episode reward: total was -114.510000. running mean: -68.880942\n",
      "ep 377: ep_len:500 episode reward: total was -58.690000. running mean: -68.779033\n",
      "epsilon:0.183242 episode_count: 2646. steps_count: 1167053.000000\n",
      "Time elapsed:  3286.212236404419\n",
      "ep 378: ep_len:588 episode reward: total was -15.210000. running mean: -68.243342\n",
      "ep 378: ep_len:500 episode reward: total was -152.020000. running mean: -69.081109\n",
      "ep 378: ep_len:561 episode reward: total was -37.300000. running mean: -68.763298\n",
      "ep 378: ep_len:516 episode reward: total was -92.460000. running mean: -69.000265\n",
      "ep 378: ep_len:89 episode reward: total was -4.290000. running mean: -68.353162\n",
      "ep 378: ep_len:629 episode reward: total was -168.580000. running mean: -69.355431\n",
      "ep 378: ep_len:599 episode reward: total was -85.850000. running mean: -69.520376\n",
      "epsilon:0.183198 episode_count: 2653. steps_count: 1170535.000000\n",
      "Time elapsed:  3295.0164279937744\n",
      "ep 379: ep_len:134 episode reward: total was -25.780000. running mean: -69.082973\n",
      "ep 379: ep_len:500 episode reward: total was -78.050000. running mean: -69.172643\n",
      "ep 379: ep_len:646 episode reward: total was -105.820000. running mean: -69.539116\n",
      "ep 379: ep_len:541 episode reward: total was -70.990000. running mean: -69.553625\n",
      "ep 379: ep_len:3 episode reward: total was 0.000000. running mean: -68.858089\n",
      "ep 379: ep_len:595 episode reward: total was -114.560000. running mean: -69.315108\n",
      "ep 379: ep_len:500 episode reward: total was -60.400000. running mean: -69.225957\n",
      "epsilon:0.183153 episode_count: 2660. steps_count: 1173454.000000\n",
      "Time elapsed:  3302.8101131916046\n",
      "ep 380: ep_len:265 episode reward: total was -44.800000. running mean: -68.981697\n",
      "ep 380: ep_len:507 episode reward: total was -113.490000. running mean: -69.426780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 380: ep_len:607 episode reward: total was -134.500000. running mean: -70.077513\n",
      "ep 380: ep_len:500 episode reward: total was -101.010000. running mean: -70.386838\n",
      "ep 380: ep_len:92 episode reward: total was -12.310000. running mean: -69.806069\n",
      "ep 380: ep_len:577 episode reward: total was -123.000000. running mean: -70.338008\n",
      "ep 380: ep_len:542 episode reward: total was -94.050000. running mean: -70.575128\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.183109 episode_count: 2667. steps_count: 1176544.000000\n",
      "Time elapsed:  3315.8482007980347\n",
      "ep 381: ep_len:535 episode reward: total was -29.620000. running mean: -70.165577\n",
      "ep 381: ep_len:500 episode reward: total was -58.440000. running mean: -70.048321\n",
      "ep 381: ep_len:524 episode reward: total was -72.480000. running mean: -70.072638\n",
      "ep 381: ep_len:561 episode reward: total was -40.970000. running mean: -69.781612\n",
      "ep 381: ep_len:92 episode reward: total was -7.260000. running mean: -69.156396\n",
      "ep 381: ep_len:638 episode reward: total was -85.540000. running mean: -69.320232\n",
      "ep 381: ep_len:199 episode reward: total was -40.150000. running mean: -69.028529\n",
      "epsilon:0.183065 episode_count: 2674. steps_count: 1179593.000000\n",
      "Time elapsed:  3323.8501658439636\n",
      "ep 382: ep_len:570 episode reward: total was -1.650000. running mean: -68.354744\n",
      "ep 382: ep_len:268 episode reward: total was -67.970000. running mean: -68.350897\n",
      "ep 382: ep_len:502 episode reward: total was -70.920000. running mean: -68.376588\n",
      "ep 382: ep_len:523 episode reward: total was -125.530000. running mean: -68.948122\n",
      "ep 382: ep_len:3 episode reward: total was 0.000000. running mean: -68.258641\n",
      "ep 382: ep_len:545 episode reward: total was -124.130000. running mean: -68.817354\n",
      "ep 382: ep_len:507 episode reward: total was -115.870000. running mean: -69.287881\n",
      "epsilon:0.183020 episode_count: 2681. steps_count: 1182511.000000\n",
      "Time elapsed:  3331.8954961299896\n",
      "ep 383: ep_len:585 episode reward: total was -56.250000. running mean: -69.157502\n",
      "ep 383: ep_len:501 episode reward: total was -83.800000. running mean: -69.303927\n",
      "ep 383: ep_len:500 episode reward: total was -82.950000. running mean: -69.440388\n",
      "ep 383: ep_len:510 episode reward: total was -49.970000. running mean: -69.245684\n",
      "ep 383: ep_len:3 episode reward: total was -1.500000. running mean: -68.568227\n",
      "ep 383: ep_len:501 episode reward: total was -53.080000. running mean: -68.413345\n",
      "ep 383: ep_len:573 episode reward: total was -57.590000. running mean: -68.305111\n",
      "epsilon:0.182976 episode_count: 2688. steps_count: 1185684.000000\n",
      "Time elapsed:  3340.2355647087097\n",
      "ep 384: ep_len:530 episode reward: total was -70.310000. running mean: -68.325160\n",
      "ep 384: ep_len:500 episode reward: total was -150.110000. running mean: -69.143008\n",
      "ep 384: ep_len:375 episode reward: total was -42.450000. running mean: -68.876078\n",
      "ep 384: ep_len:46 episode reward: total was 3.200000. running mean: -68.155318\n",
      "ep 384: ep_len:88 episode reward: total was 6.170000. running mean: -67.412064\n",
      "ep 384: ep_len:579 episode reward: total was -134.090000. running mean: -68.078844\n",
      "ep 384: ep_len:600 episode reward: total was -70.910000. running mean: -68.107155\n",
      "epsilon:0.182932 episode_count: 2695. steps_count: 1188402.000000\n",
      "Time elapsed:  3348.1342928409576\n",
      "ep 385: ep_len:129 episode reward: total was -7.040000. running mean: -67.496484\n",
      "ep 385: ep_len:500 episode reward: total was -28.150000. running mean: -67.103019\n",
      "ep 385: ep_len:500 episode reward: total was -40.010000. running mean: -66.832089\n",
      "ep 385: ep_len:570 episode reward: total was -60.650000. running mean: -66.770268\n",
      "ep 385: ep_len:106 episode reward: total was -18.250000. running mean: -66.285065\n",
      "ep 385: ep_len:559 episode reward: total was -100.560000. running mean: -66.627814\n",
      "ep 385: ep_len:576 episode reward: total was -108.870000. running mean: -67.050236\n",
      "epsilon:0.182887 episode_count: 2702. steps_count: 1191342.000000\n",
      "Time elapsed:  3356.048225402832\n",
      "ep 386: ep_len:516 episode reward: total was -55.290000. running mean: -66.932634\n",
      "ep 386: ep_len:523 episode reward: total was -81.150000. running mean: -67.074808\n",
      "ep 386: ep_len:619 episode reward: total was -96.340000. running mean: -67.367460\n",
      "ep 386: ep_len:512 episode reward: total was -39.030000. running mean: -67.084085\n",
      "ep 386: ep_len:3 episode reward: total was 0.000000. running mean: -66.413244\n",
      "ep 386: ep_len:500 episode reward: total was -70.350000. running mean: -66.452612\n",
      "ep 386: ep_len:574 episode reward: total was -61.170000. running mean: -66.399786\n",
      "epsilon:0.182843 episode_count: 2709. steps_count: 1194589.000000\n",
      "Time elapsed:  3364.6649162769318\n",
      "ep 387: ep_len:180 episode reward: total was -17.440000. running mean: -65.910188\n",
      "ep 387: ep_len:500 episode reward: total was -69.240000. running mean: -65.943486\n",
      "ep 387: ep_len:432 episode reward: total was -74.360000. running mean: -66.027651\n",
      "ep 387: ep_len:500 episode reward: total was -38.090000. running mean: -65.748274\n",
      "ep 387: ep_len:3 episode reward: total was 0.000000. running mean: -65.090792\n",
      "ep 387: ep_len:500 episode reward: total was -36.980000. running mean: -64.809684\n",
      "ep 387: ep_len:606 episode reward: total was -70.420000. running mean: -64.865787\n",
      "epsilon:0.182799 episode_count: 2716. steps_count: 1197310.000000\n",
      "Time elapsed:  3371.972190141678\n",
      "ep 388: ep_len:675 episode reward: total was -192.270000. running mean: -66.139829\n",
      "ep 388: ep_len:500 episode reward: total was -54.670000. running mean: -66.025131\n",
      "ep 388: ep_len:504 episode reward: total was -97.740000. running mean: -66.342279\n",
      "ep 388: ep_len:573 episode reward: total was -31.700000. running mean: -65.995857\n",
      "ep 388: ep_len:3 episode reward: total was 0.000000. running mean: -65.335898\n",
      "ep 388: ep_len:507 episode reward: total was -78.520000. running mean: -65.467739\n",
      "ep 388: ep_len:283 episode reward: total was -46.760000. running mean: -65.280662\n",
      "epsilon:0.182754 episode_count: 2723. steps_count: 1200355.000000\n",
      "Time elapsed:  3379.575855731964\n",
      "ep 389: ep_len:541 episode reward: total was -76.940000. running mean: -65.397255\n",
      "ep 389: ep_len:520 episode reward: total was -81.500000. running mean: -65.558283\n",
      "ep 389: ep_len:565 episode reward: total was -165.540000. running mean: -66.558100\n",
      "ep 389: ep_len:506 episode reward: total was -119.860000. running mean: -67.091119\n",
      "ep 389: ep_len:100 episode reward: total was -8.250000. running mean: -66.502708\n",
      "ep 389: ep_len:592 episode reward: total was -102.720000. running mean: -66.864880\n",
      "ep 389: ep_len:300 episode reward: total was -68.280000. running mean: -66.879032\n",
      "epsilon:0.182710 episode_count: 2730. steps_count: 1203479.000000\n",
      "Time elapsed:  3387.9647641181946\n",
      "ep 390: ep_len:509 episode reward: total was -42.780000. running mean: -66.638041\n",
      "ep 390: ep_len:500 episode reward: total was -38.970000. running mean: -66.361361\n",
      "ep 390: ep_len:617 episode reward: total was -134.170000. running mean: -67.039447\n",
      "ep 390: ep_len:500 episode reward: total was -82.830000. running mean: -67.197353\n",
      "ep 390: ep_len:3 episode reward: total was 0.000000. running mean: -66.525379\n",
      "ep 390: ep_len:501 episode reward: total was -98.820000. running mean: -66.848326\n",
      "ep 390: ep_len:592 episode reward: total was -95.250000. running mean: -67.132342\n",
      "epsilon:0.182666 episode_count: 2737. steps_count: 1206701.000000\n",
      "Time elapsed:  3396.221407175064\n",
      "ep 391: ep_len:614 episode reward: total was -88.820000. running mean: -67.349219\n",
      "ep 391: ep_len:559 episode reward: total was -13.770000. running mean: -66.813427\n",
      "ep 391: ep_len:79 episode reward: total was -2.770000. running mean: -66.172992\n",
      "ep 391: ep_len:56 episode reward: total was -1.200000. running mean: -65.523262\n",
      "ep 391: ep_len:99 episode reward: total was -42.280000. running mean: -65.290830\n",
      "ep 391: ep_len:500 episode reward: total was -134.970000. running mean: -65.987622\n",
      "ep 391: ep_len:511 episode reward: total was -129.900000. running mean: -66.626745\n",
      "epsilon:0.182621 episode_count: 2744. steps_count: 1209119.000000\n",
      "Time elapsed:  3403.260491371155\n",
      "ep 392: ep_len:555 episode reward: total was -207.090000. running mean: -68.031378\n",
      "ep 392: ep_len:629 episode reward: total was -93.210000. running mean: -68.283164\n",
      "ep 392: ep_len:589 episode reward: total was -80.850000. running mean: -68.408832\n",
      "ep 392: ep_len:572 episode reward: total was -44.630000. running mean: -68.171044\n",
      "ep 392: ep_len:3 episode reward: total was -1.500000. running mean: -67.504334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 392: ep_len:542 episode reward: total was -66.370000. running mean: -67.492990\n",
      "ep 392: ep_len:567 episode reward: total was -74.310000. running mean: -67.561160\n",
      "epsilon:0.182577 episode_count: 2751. steps_count: 1212576.000000\n",
      "Time elapsed:  3412.730614900589\n",
      "ep 393: ep_len:573 episode reward: total was -85.940000. running mean: -67.744949\n",
      "ep 393: ep_len:341 episode reward: total was -71.590000. running mean: -67.783399\n",
      "ep 393: ep_len:570 episode reward: total was -79.670000. running mean: -67.902265\n",
      "ep 393: ep_len:625 episode reward: total was -28.470000. running mean: -67.507943\n",
      "ep 393: ep_len:3 episode reward: total was 0.000000. running mean: -66.832863\n",
      "ep 393: ep_len:631 episode reward: total was -82.350000. running mean: -66.988035\n",
      "ep 393: ep_len:500 episode reward: total was -76.730000. running mean: -67.085454\n",
      "epsilon:0.182533 episode_count: 2758. steps_count: 1215819.000000\n",
      "Time elapsed:  3421.5491814613342\n",
      "ep 394: ep_len:580 episode reward: total was -130.410000. running mean: -67.718700\n",
      "ep 394: ep_len:589 episode reward: total was -97.630000. running mean: -68.017813\n",
      "ep 394: ep_len:502 episode reward: total was -75.880000. running mean: -68.096435\n",
      "ep 394: ep_len:531 episode reward: total was -104.090000. running mean: -68.456370\n",
      "ep 394: ep_len:100 episode reward: total was -0.290000. running mean: -67.774707\n",
      "ep 394: ep_len:516 episode reward: total was -76.210000. running mean: -67.859060\n",
      "ep 394: ep_len:517 episode reward: total was -96.620000. running mean: -68.146669\n",
      "epsilon:0.182488 episode_count: 2765. steps_count: 1219154.000000\n",
      "Time elapsed:  3430.527958869934\n",
      "ep 395: ep_len:671 episode reward: total was -103.970000. running mean: -68.504902\n",
      "ep 395: ep_len:566 episode reward: total was -57.960000. running mean: -68.399453\n",
      "ep 395: ep_len:705 episode reward: total was -89.100000. running mean: -68.606459\n",
      "ep 395: ep_len:501 episode reward: total was -89.440000. running mean: -68.814794\n",
      "ep 395: ep_len:3 episode reward: total was 0.000000. running mean: -68.126646\n",
      "ep 395: ep_len:617 episode reward: total was -125.770000. running mean: -68.703080\n",
      "ep 395: ep_len:536 episode reward: total was -100.040000. running mean: -69.016449\n",
      "epsilon:0.182444 episode_count: 2772. steps_count: 1222753.000000\n",
      "Time elapsed:  3439.861964225769\n",
      "ep 396: ep_len:629 episode reward: total was -68.540000. running mean: -69.011684\n",
      "ep 396: ep_len:631 episode reward: total was -73.390000. running mean: -69.055468\n",
      "ep 396: ep_len:79 episode reward: total was -17.860000. running mean: -68.543513\n",
      "ep 396: ep_len:500 episode reward: total was -54.730000. running mean: -68.405378\n",
      "ep 396: ep_len:3 episode reward: total was 0.000000. running mean: -67.721324\n",
      "ep 396: ep_len:652 episode reward: total was -106.760000. running mean: -68.111711\n",
      "ep 396: ep_len:581 episode reward: total was -30.770000. running mean: -67.738294\n",
      "epsilon:0.182400 episode_count: 2779. steps_count: 1225828.000000\n",
      "Time elapsed:  3448.050588130951\n",
      "ep 397: ep_len:208 episode reward: total was -23.430000. running mean: -67.295211\n",
      "ep 397: ep_len:500 episode reward: total was -112.130000. running mean: -67.743559\n",
      "ep 397: ep_len:562 episode reward: total was -104.750000. running mean: -68.113623\n",
      "ep 397: ep_len:530 episode reward: total was -26.470000. running mean: -67.697187\n",
      "ep 397: ep_len:3 episode reward: total was 0.000000. running mean: -67.020215\n",
      "ep 397: ep_len:608 episode reward: total was -104.570000. running mean: -67.395713\n",
      "ep 397: ep_len:593 episode reward: total was -65.550000. running mean: -67.377256\n",
      "epsilon:0.182355 episode_count: 2786. steps_count: 1228832.000000\n",
      "Time elapsed:  3455.7698311805725\n",
      "ep 398: ep_len:665 episode reward: total was -112.890000. running mean: -67.832383\n",
      "ep 398: ep_len:582 episode reward: total was -134.330000. running mean: -68.497359\n",
      "ep 398: ep_len:551 episode reward: total was -78.960000. running mean: -68.601986\n",
      "ep 398: ep_len:578 episode reward: total was -23.440000. running mean: -68.150366\n",
      "ep 398: ep_len:100 episode reward: total was 12.230000. running mean: -67.346562\n",
      "ep 398: ep_len:503 episode reward: total was -112.410000. running mean: -67.797197\n",
      "ep 398: ep_len:581 episode reward: total was -165.410000. running mean: -68.773325\n",
      "epsilon:0.182311 episode_count: 2793. steps_count: 1232392.000000\n",
      "Time elapsed:  3464.9777779579163\n",
      "ep 399: ep_len:653 episode reward: total was -142.050000. running mean: -69.506091\n",
      "ep 399: ep_len:501 episode reward: total was -50.110000. running mean: -69.312130\n",
      "ep 399: ep_len:563 episode reward: total was -67.340000. running mean: -69.292409\n",
      "ep 399: ep_len:401 episode reward: total was -116.730000. running mean: -69.766785\n",
      "ep 399: ep_len:3 episode reward: total was 0.000000. running mean: -69.069117\n",
      "ep 399: ep_len:641 episode reward: total was -108.580000. running mean: -69.464226\n",
      "ep 399: ep_len:552 episode reward: total was -91.170000. running mean: -69.681284\n",
      "epsilon:0.182267 episode_count: 2800. steps_count: 1235706.000000\n",
      "Time elapsed:  3473.8162944316864\n",
      "ep 400: ep_len:645 episode reward: total was -105.350000. running mean: -70.037971\n",
      "ep 400: ep_len:501 episode reward: total was -150.290000. running mean: -70.840491\n",
      "ep 400: ep_len:659 episode reward: total was -88.430000. running mean: -71.016386\n",
      "ep 400: ep_len:500 episode reward: total was -120.530000. running mean: -71.511522\n",
      "ep 400: ep_len:3 episode reward: total was 0.000000. running mean: -70.796407\n",
      "ep 400: ep_len:171 episode reward: total was 15.460000. running mean: -69.933843\n",
      "ep 400: ep_len:570 episode reward: total was -135.690000. running mean: -70.591405\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.182222 episode_count: 2807. steps_count: 1238755.000000\n",
      "Time elapsed:  3487.213791847229\n",
      "ep 401: ep_len:500 episode reward: total was 13.170000. running mean: -69.753791\n",
      "ep 401: ep_len:192 episode reward: total was -36.910000. running mean: -69.425353\n",
      "ep 401: ep_len:505 episode reward: total was -68.600000. running mean: -69.417099\n",
      "ep 401: ep_len:533 episode reward: total was -56.160000. running mean: -69.284528\n",
      "ep 401: ep_len:84 episode reward: total was -7.340000. running mean: -68.665083\n",
      "ep 401: ep_len:568 episode reward: total was -85.350000. running mean: -68.831932\n",
      "ep 401: ep_len:603 episode reward: total was -70.400000. running mean: -68.847613\n",
      "epsilon:0.182178 episode_count: 2814. steps_count: 1241740.000000\n",
      "Time elapsed:  3496.3114972114563\n",
      "ep 402: ep_len:544 episode reward: total was -33.100000. running mean: -68.490137\n",
      "ep 402: ep_len:584 episode reward: total was -63.950000. running mean: -68.444735\n",
      "ep 402: ep_len:582 episode reward: total was -94.230000. running mean: -68.702588\n",
      "ep 402: ep_len:603 episode reward: total was -51.830000. running mean: -68.533862\n",
      "ep 402: ep_len:92 episode reward: total was -15.800000. running mean: -68.006523\n",
      "ep 402: ep_len:609 episode reward: total was -60.130000. running mean: -67.927758\n",
      "ep 402: ep_len:580 episode reward: total was -48.140000. running mean: -67.729881\n",
      "epsilon:0.182134 episode_count: 2821. steps_count: 1245334.000000\n",
      "Time elapsed:  3507.9878420829773\n",
      "ep 403: ep_len:500 episode reward: total was -96.450000. running mean: -68.017082\n",
      "ep 403: ep_len:500 episode reward: total was -33.470000. running mean: -67.671611\n",
      "ep 403: ep_len:73 episode reward: total was -7.820000. running mean: -67.073095\n",
      "ep 403: ep_len:575 episode reward: total was -35.160000. running mean: -66.753964\n",
      "ep 403: ep_len:3 episode reward: total was 0.000000. running mean: -66.086424\n",
      "ep 403: ep_len:600 episode reward: total was -77.900000. running mean: -66.204560\n",
      "ep 403: ep_len:540 episode reward: total was -120.560000. running mean: -66.748114\n",
      "epsilon:0.182089 episode_count: 2828. steps_count: 1248125.000000\n",
      "Time elapsed:  3515.7774176597595\n",
      "ep 404: ep_len:553 episode reward: total was -113.480000. running mean: -67.215433\n",
      "ep 404: ep_len:556 episode reward: total was -43.450000. running mean: -66.977779\n",
      "ep 404: ep_len:559 episode reward: total was -62.030000. running mean: -66.928301\n",
      "ep 404: ep_len:538 episode reward: total was -45.390000. running mean: -66.712918\n",
      "ep 404: ep_len:3 episode reward: total was 0.000000. running mean: -66.045789\n",
      "ep 404: ep_len:666 episode reward: total was -109.140000. running mean: -66.476731\n",
      "ep 404: ep_len:194 episode reward: total was -40.690000. running mean: -66.218864\n",
      "epsilon:0.182045 episode_count: 2835. steps_count: 1251194.000000\n",
      "Time elapsed:  3525.0094747543335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 405: ep_len:242 episode reward: total was -20.790000. running mean: -65.764575\n",
      "ep 405: ep_len:560 episode reward: total was -158.490000. running mean: -66.691829\n",
      "ep 405: ep_len:528 episode reward: total was -95.160000. running mean: -66.976511\n",
      "ep 405: ep_len:500 episode reward: total was -76.820000. running mean: -67.074946\n",
      "ep 405: ep_len:3 episode reward: total was -1.500000. running mean: -66.419197\n",
      "ep 405: ep_len:635 episode reward: total was -114.730000. running mean: -66.902305\n",
      "ep 405: ep_len:527 episode reward: total was -95.490000. running mean: -67.188182\n",
      "epsilon:0.182001 episode_count: 2842. steps_count: 1254189.000000\n",
      "Time elapsed:  3533.7007410526276\n",
      "ep 406: ep_len:122 episode reward: total was -1.630000. running mean: -66.532600\n",
      "ep 406: ep_len:294 episode reward: total was -58.610000. running mean: -66.453374\n",
      "ep 406: ep_len:500 episode reward: total was -94.660000. running mean: -66.735440\n",
      "ep 406: ep_len:166 episode reward: total was -11.900000. running mean: -66.187086\n",
      "ep 406: ep_len:3 episode reward: total was -1.500000. running mean: -65.540215\n",
      "ep 406: ep_len:608 episode reward: total was -68.670000. running mean: -65.571513\n",
      "ep 406: ep_len:568 episode reward: total was -96.310000. running mean: -65.878897\n",
      "epsilon:0.181956 episode_count: 2849. steps_count: 1256450.000000\n",
      "Time elapsed:  3541.091229200363\n",
      "ep 407: ep_len:507 episode reward: total was -50.150000. running mean: -65.721608\n",
      "ep 407: ep_len:577 episode reward: total was -69.440000. running mean: -65.758792\n",
      "ep 407: ep_len:444 episode reward: total was -62.770000. running mean: -65.728904\n",
      "ep 407: ep_len:525 episode reward: total was -92.240000. running mean: -65.994015\n",
      "ep 407: ep_len:3 episode reward: total was -1.500000. running mean: -65.349075\n",
      "ep 407: ep_len:500 episode reward: total was -49.110000. running mean: -65.186685\n",
      "ep 407: ep_len:504 episode reward: total was -81.240000. running mean: -65.347218\n",
      "epsilon:0.181912 episode_count: 2856. steps_count: 1259510.000000\n",
      "Time elapsed:  3550.425260782242\n",
      "ep 408: ep_len:588 episode reward: total was -132.170000. running mean: -66.015445\n",
      "ep 408: ep_len:568 episode reward: total was -92.700000. running mean: -66.282291\n",
      "ep 408: ep_len:432 episode reward: total was -29.180000. running mean: -65.911268\n",
      "ep 408: ep_len:381 episode reward: total was -26.620000. running mean: -65.518355\n",
      "ep 408: ep_len:3 episode reward: total was 0.000000. running mean: -64.863172\n",
      "ep 408: ep_len:652 episode reward: total was -138.680000. running mean: -65.601340\n",
      "ep 408: ep_len:354 episode reward: total was -91.700000. running mean: -65.862327\n",
      "epsilon:0.181868 episode_count: 2863. steps_count: 1262488.000000\n",
      "Time elapsed:  3559.8334772586823\n",
      "ep 409: ep_len:590 episode reward: total was -53.940000. running mean: -65.743103\n",
      "ep 409: ep_len:587 episode reward: total was -20.630000. running mean: -65.291972\n",
      "ep 409: ep_len:650 episode reward: total was -74.920000. running mean: -65.388253\n",
      "ep 409: ep_len:156 episode reward: total was -12.660000. running mean: -64.860970\n",
      "ep 409: ep_len:3 episode reward: total was -1.500000. running mean: -64.227361\n",
      "ep 409: ep_len:500 episode reward: total was -51.790000. running mean: -64.102987\n",
      "ep 409: ep_len:519 episode reward: total was -64.970000. running mean: -64.111657\n",
      "epsilon:0.181823 episode_count: 2870. steps_count: 1265493.000000\n",
      "Time elapsed:  3569.035155057907\n",
      "ep 410: ep_len:539 episode reward: total was -198.560000. running mean: -65.456140\n",
      "ep 410: ep_len:545 episode reward: total was -84.600000. running mean: -65.647579\n",
      "ep 410: ep_len:500 episode reward: total was -102.880000. running mean: -66.019903\n",
      "ep 410: ep_len:500 episode reward: total was -26.660000. running mean: -65.626304\n",
      "ep 410: ep_len:103 episode reward: total was 7.730000. running mean: -64.892741\n",
      "ep 410: ep_len:501 episode reward: total was -90.550000. running mean: -65.149314\n",
      "ep 410: ep_len:618 episode reward: total was -70.380000. running mean: -65.201621\n",
      "epsilon:0.181779 episode_count: 2877. steps_count: 1268799.000000\n",
      "Time elapsed:  3579.391870498657\n",
      "ep 411: ep_len:183 episode reward: total was -27.050000. running mean: -64.820104\n",
      "ep 411: ep_len:514 episode reward: total was -89.920000. running mean: -65.071103\n",
      "ep 411: ep_len:500 episode reward: total was -75.260000. running mean: -65.172992\n",
      "ep 411: ep_len:509 episode reward: total was -21.220000. running mean: -64.733462\n",
      "ep 411: ep_len:33 episode reward: total was 10.500000. running mean: -63.981128\n",
      "ep 411: ep_len:518 episode reward: total was -62.530000. running mean: -63.966617\n",
      "ep 411: ep_len:630 episode reward: total was -64.380000. running mean: -63.970750\n",
      "epsilon:0.181735 episode_count: 2884. steps_count: 1271686.000000\n",
      "Time elapsed:  3588.2492413520813\n",
      "ep 412: ep_len:539 episode reward: total was -70.860000. running mean: -64.039643\n",
      "ep 412: ep_len:199 episode reward: total was -21.290000. running mean: -63.612146\n",
      "ep 412: ep_len:545 episode reward: total was -66.630000. running mean: -63.642325\n",
      "ep 412: ep_len:518 episode reward: total was -24.920000. running mean: -63.255102\n",
      "ep 412: ep_len:94 episode reward: total was -51.240000. running mean: -63.134951\n",
      "ep 412: ep_len:500 episode reward: total was -128.730000. running mean: -63.790901\n",
      "ep 412: ep_len:538 episode reward: total was -167.260000. running mean: -64.825592\n",
      "epsilon:0.181690 episode_count: 2891. steps_count: 1274619.000000\n",
      "Time elapsed:  3597.6507704257965\n",
      "ep 413: ep_len:628 episode reward: total was -40.050000. running mean: -64.577836\n",
      "ep 413: ep_len:500 episode reward: total was -15.470000. running mean: -64.086758\n",
      "ep 413: ep_len:530 episode reward: total was -136.820000. running mean: -64.814090\n",
      "ep 413: ep_len:520 episode reward: total was -72.770000. running mean: -64.893649\n",
      "ep 413: ep_len:3 episode reward: total was -1.500000. running mean: -64.259713\n",
      "ep 413: ep_len:292 episode reward: total was -29.120000. running mean: -63.908316\n",
      "ep 413: ep_len:554 episode reward: total was -69.120000. running mean: -63.960433\n",
      "epsilon:0.181646 episode_count: 2898. steps_count: 1277646.000000\n",
      "Time elapsed:  3606.8522679805756\n",
      "ep 414: ep_len:645 episode reward: total was -92.180000. running mean: -64.242628\n",
      "ep 414: ep_len:620 episode reward: total was -85.250000. running mean: -64.452702\n",
      "ep 414: ep_len:507 episode reward: total was -65.040000. running mean: -64.458575\n",
      "ep 414: ep_len:596 episode reward: total was -59.630000. running mean: -64.410289\n",
      "ep 414: ep_len:3 episode reward: total was 0.000000. running mean: -63.766186\n",
      "ep 414: ep_len:500 episode reward: total was -96.330000. running mean: -64.091825\n",
      "ep 414: ep_len:583 episode reward: total was -51.300000. running mean: -63.963906\n",
      "epsilon:0.181602 episode_count: 2905. steps_count: 1281100.000000\n",
      "Time elapsed:  3617.2027657032013\n",
      "ep 415: ep_len:622 episode reward: total was -82.990000. running mean: -64.154167\n",
      "ep 415: ep_len:569 episode reward: total was -94.150000. running mean: -64.454126\n",
      "ep 415: ep_len:616 episode reward: total was -85.000000. running mean: -64.659584\n",
      "ep 415: ep_len:500 episode reward: total was -96.880000. running mean: -64.981788\n",
      "ep 415: ep_len:86 episode reward: total was -1.780000. running mean: -64.349771\n",
      "ep 415: ep_len:536 episode reward: total was -79.090000. running mean: -64.497173\n",
      "ep 415: ep_len:532 episode reward: total was -48.850000. running mean: -64.340701\n",
      "epsilon:0.181557 episode_count: 2912. steps_count: 1284561.000000\n",
      "Time elapsed:  3627.491358280182\n",
      "ep 416: ep_len:559 episode reward: total was -26.400000. running mean: -63.961294\n",
      "ep 416: ep_len:539 episode reward: total was 1.430000. running mean: -63.307381\n",
      "ep 416: ep_len:518 episode reward: total was -100.020000. running mean: -63.674507\n",
      "ep 416: ep_len:500 episode reward: total was -69.980000. running mean: -63.737562\n",
      "ep 416: ep_len:99 episode reward: total was -15.760000. running mean: -63.257787\n",
      "ep 416: ep_len:540 episode reward: total was -71.980000. running mean: -63.345009\n",
      "ep 416: ep_len:549 episode reward: total was -70.820000. running mean: -63.419759\n",
      "epsilon:0.181513 episode_count: 2919. steps_count: 1287865.000000\n",
      "Time elapsed:  3636.9961397647858\n",
      "ep 417: ep_len:542 episode reward: total was -132.260000. running mean: -64.108161\n",
      "ep 417: ep_len:540 episode reward: total was -99.420000. running mean: -64.461279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 417: ep_len:554 episode reward: total was -110.470000. running mean: -64.921367\n",
      "ep 417: ep_len:502 episode reward: total was -63.020000. running mean: -64.902353\n",
      "ep 417: ep_len:38 episode reward: total was 11.500000. running mean: -64.138330\n",
      "ep 417: ep_len:525 episode reward: total was -100.850000. running mean: -64.505446\n",
      "ep 417: ep_len:555 episode reward: total was -142.460000. running mean: -65.284992\n",
      "epsilon:0.181469 episode_count: 2926. steps_count: 1291121.000000\n",
      "Time elapsed:  3645.7705500125885\n",
      "ep 418: ep_len:615 episode reward: total was -92.550000. running mean: -65.557642\n",
      "ep 418: ep_len:609 episode reward: total was -139.680000. running mean: -66.298865\n",
      "ep 418: ep_len:838 episode reward: total was -349.890000. running mean: -69.134777\n",
      "ep 418: ep_len:500 episode reward: total was -58.870000. running mean: -69.032129\n",
      "ep 418: ep_len:111 episode reward: total was -28.800000. running mean: -68.629808\n",
      "ep 418: ep_len:651 episode reward: total was -100.870000. running mean: -68.952210\n",
      "ep 418: ep_len:540 episode reward: total was -121.210000. running mean: -69.474788\n",
      "epsilon:0.181424 episode_count: 2933. steps_count: 1294985.000000\n",
      "Time elapsed:  3657.119049310684\n",
      "ep 419: ep_len:229 episode reward: total was -15.870000. running mean: -68.938740\n",
      "ep 419: ep_len:559 episode reward: total was -2.630000. running mean: -68.275652\n",
      "ep 419: ep_len:500 episode reward: total was -38.120000. running mean: -67.974096\n",
      "ep 419: ep_len:122 episode reward: total was -4.080000. running mean: -67.335155\n",
      "ep 419: ep_len:3 episode reward: total was -1.500000. running mean: -66.676803\n",
      "ep 419: ep_len:686 episode reward: total was -100.820000. running mean: -67.018235\n",
      "ep 419: ep_len:597 episode reward: total was -87.610000. running mean: -67.224153\n",
      "epsilon:0.181380 episode_count: 2940. steps_count: 1297681.000000\n",
      "Time elapsed:  3665.449292898178\n",
      "ep 420: ep_len:500 episode reward: total was -16.550000. running mean: -66.717411\n",
      "ep 420: ep_len:513 episode reward: total was -74.700000. running mean: -66.797237\n",
      "ep 420: ep_len:600 episode reward: total was -73.450000. running mean: -66.863765\n",
      "ep 420: ep_len:509 episode reward: total was -5.850000. running mean: -66.253627\n",
      "ep 420: ep_len:3 episode reward: total was 0.000000. running mean: -65.591091\n",
      "ep 420: ep_len:500 episode reward: total was -30.670000. running mean: -65.241880\n",
      "ep 420: ep_len:543 episode reward: total was -145.150000. running mean: -66.040961\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.181336 episode_count: 2947. steps_count: 1300849.000000\n",
      "Time elapsed:  3679.588688135147\n",
      "ep 421: ep_len:500 episode reward: total was -28.530000. running mean: -65.665852\n",
      "ep 421: ep_len:601 episode reward: total was -78.600000. running mean: -65.795193\n",
      "ep 421: ep_len:501 episode reward: total was -295.710000. running mean: -68.094341\n",
      "ep 421: ep_len:529 episode reward: total was -14.040000. running mean: -67.553798\n",
      "ep 421: ep_len:3 episode reward: total was 0.000000. running mean: -66.878260\n",
      "ep 421: ep_len:551 episode reward: total was -34.190000. running mean: -66.551377\n",
      "ep 421: ep_len:534 episode reward: total was -78.370000. running mean: -66.669563\n",
      "epsilon:0.181291 episode_count: 2954. steps_count: 1304068.000000\n",
      "Time elapsed:  3689.814397096634\n",
      "ep 422: ep_len:563 episode reward: total was -116.070000. running mean: -67.163568\n",
      "ep 422: ep_len:581 episode reward: total was -81.460000. running mean: -67.306532\n",
      "ep 422: ep_len:501 episode reward: total was -41.100000. running mean: -67.044467\n",
      "ep 422: ep_len:56 episode reward: total was -19.380000. running mean: -66.567822\n",
      "ep 422: ep_len:3 episode reward: total was 0.000000. running mean: -65.902144\n",
      "ep 422: ep_len:531 episode reward: total was -108.210000. running mean: -66.325222\n",
      "ep 422: ep_len:308 episode reward: total was -75.630000. running mean: -66.418270\n",
      "epsilon:0.181247 episode_count: 2961. steps_count: 1306611.000000\n",
      "Time elapsed:  3698.033762216568\n",
      "ep 423: ep_len:579 episode reward: total was -69.170000. running mean: -66.445787\n",
      "ep 423: ep_len:500 episode reward: total was -145.650000. running mean: -67.237830\n",
      "ep 423: ep_len:523 episode reward: total was -90.160000. running mean: -67.467051\n",
      "ep 423: ep_len:533 episode reward: total was -99.510000. running mean: -67.787481\n",
      "ep 423: ep_len:85 episode reward: total was -22.880000. running mean: -67.338406\n",
      "ep 423: ep_len:605 episode reward: total was -56.230000. running mean: -67.227322\n",
      "ep 423: ep_len:536 episode reward: total was -47.820000. running mean: -67.033249\n",
      "epsilon:0.181203 episode_count: 2968. steps_count: 1309972.000000\n",
      "Time elapsed:  3709.2316048145294\n",
      "ep 424: ep_len:605 episode reward: total was -75.400000. running mean: -67.116916\n",
      "ep 424: ep_len:510 episode reward: total was -60.960000. running mean: -67.055347\n",
      "ep 424: ep_len:501 episode reward: total was -71.700000. running mean: -67.101794\n",
      "ep 424: ep_len:37 episode reward: total was 0.690000. running mean: -66.423876\n",
      "ep 424: ep_len:109 episode reward: total was -2.760000. running mean: -65.787237\n",
      "ep 424: ep_len:593 episode reward: total was -135.190000. running mean: -66.481265\n",
      "ep 424: ep_len:545 episode reward: total was -92.520000. running mean: -66.741652\n",
      "epsilon:0.181158 episode_count: 2975. steps_count: 1312872.000000\n",
      "Time elapsed:  3718.039665222168\n",
      "ep 425: ep_len:566 episode reward: total was -11.760000. running mean: -66.191835\n",
      "ep 425: ep_len:501 episode reward: total was -21.650000. running mean: -65.746417\n",
      "ep 425: ep_len:606 episode reward: total was -69.430000. running mean: -65.783253\n",
      "ep 425: ep_len:500 episode reward: total was -69.710000. running mean: -65.822520\n",
      "ep 425: ep_len:3 episode reward: total was 0.000000. running mean: -65.164295\n",
      "ep 425: ep_len:645 episode reward: total was -127.380000. running mean: -65.786452\n",
      "ep 425: ep_len:203 episode reward: total was -32.730000. running mean: -65.455888\n",
      "epsilon:0.181114 episode_count: 2982. steps_count: 1315896.000000\n",
      "Time elapsed:  3727.2729074954987\n",
      "ep 426: ep_len:525 episode reward: total was -64.640000. running mean: -65.447729\n",
      "ep 426: ep_len:505 episode reward: total was -83.490000. running mean: -65.628151\n",
      "ep 426: ep_len:630 episode reward: total was -84.810000. running mean: -65.819970\n",
      "ep 426: ep_len:572 episode reward: total was -26.430000. running mean: -65.426070\n",
      "ep 426: ep_len:3 episode reward: total was 0.000000. running mean: -64.771810\n",
      "ep 426: ep_len:523 episode reward: total was -63.620000. running mean: -64.760291\n",
      "ep 426: ep_len:591 episode reward: total was -220.980000. running mean: -66.322489\n",
      "epsilon:0.181070 episode_count: 2989. steps_count: 1319245.000000\n",
      "Time elapsed:  3737.0996465682983\n",
      "ep 427: ep_len:111 episode reward: total was -18.580000. running mean: -65.845064\n",
      "ep 427: ep_len:500 episode reward: total was -135.510000. running mean: -66.541713\n",
      "ep 427: ep_len:549 episode reward: total was -134.230000. running mean: -67.218596\n",
      "ep 427: ep_len:576 episode reward: total was -21.740000. running mean: -66.763810\n",
      "ep 427: ep_len:3 episode reward: total was 0.000000. running mean: -66.096172\n",
      "ep 427: ep_len:613 episode reward: total was -83.030000. running mean: -66.265510\n",
      "ep 427: ep_len:580 episode reward: total was -96.590000. running mean: -66.568755\n",
      "epsilon:0.181025 episode_count: 2996. steps_count: 1322177.000000\n",
      "Time elapsed:  3746.7228343486786\n",
      "ep 428: ep_len:500 episode reward: total was -18.750000. running mean: -66.090567\n",
      "ep 428: ep_len:500 episode reward: total was -167.140000. running mean: -67.101062\n",
      "ep 428: ep_len:500 episode reward: total was -45.740000. running mean: -66.887451\n",
      "ep 428: ep_len:569 episode reward: total was -85.050000. running mean: -67.069077\n",
      "ep 428: ep_len:3 episode reward: total was 0.000000. running mean: -66.398386\n",
      "ep 428: ep_len:525 episode reward: total was -42.960000. running mean: -66.164002\n",
      "ep 428: ep_len:638 episode reward: total was -85.790000. running mean: -66.360262\n",
      "epsilon:0.180981 episode_count: 3003. steps_count: 1325412.000000\n",
      "Time elapsed:  3757.1114728450775\n",
      "ep 429: ep_len:501 episode reward: total was -87.860000. running mean: -66.575259\n",
      "ep 429: ep_len:588 episode reward: total was -124.420000. running mean: -67.153707\n",
      "ep 429: ep_len:648 episode reward: total was -135.550000. running mean: -67.837670\n",
      "ep 429: ep_len:533 episode reward: total was -83.320000. running mean: -67.992493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 429: ep_len:53 episode reward: total was 1.000000. running mean: -67.302568\n",
      "ep 429: ep_len:641 episode reward: total was -88.240000. running mean: -67.511942\n",
      "ep 429: ep_len:513 episode reward: total was -102.580000. running mean: -67.862623\n",
      "epsilon:0.180937 episode_count: 3010. steps_count: 1328889.000000\n",
      "Time elapsed:  3767.2986800670624\n",
      "ep 430: ep_len:525 episode reward: total was -30.070000. running mean: -67.484697\n",
      "ep 430: ep_len:511 episode reward: total was -106.510000. running mean: -67.874950\n",
      "ep 430: ep_len:605 episode reward: total was -118.750000. running mean: -68.383700\n",
      "ep 430: ep_len:128 episode reward: total was 7.530000. running mean: -67.624563\n",
      "ep 430: ep_len:3 episode reward: total was -1.500000. running mean: -66.963318\n",
      "ep 430: ep_len:500 episode reward: total was -114.050000. running mean: -67.434185\n",
      "ep 430: ep_len:338 episode reward: total was -75.010000. running mean: -67.509943\n",
      "epsilon:0.180892 episode_count: 3017. steps_count: 1331499.000000\n",
      "Time elapsed:  3775.760635614395\n",
      "ep 431: ep_len:571 episode reward: total was -43.330000. running mean: -67.268143\n",
      "ep 431: ep_len:501 episode reward: total was -105.100000. running mean: -67.646462\n",
      "ep 431: ep_len:451 episode reward: total was -53.170000. running mean: -67.501697\n",
      "ep 431: ep_len:513 episode reward: total was -134.290000. running mean: -68.169580\n",
      "ep 431: ep_len:125 episode reward: total was 3.760000. running mean: -67.450284\n",
      "ep 431: ep_len:306 episode reward: total was -59.670000. running mean: -67.372482\n",
      "ep 431: ep_len:509 episode reward: total was -124.140000. running mean: -67.940157\n",
      "epsilon:0.180848 episode_count: 3024. steps_count: 1334475.000000\n",
      "Time elapsed:  3784.7934465408325\n",
      "ep 432: ep_len:647 episode reward: total was -56.320000. running mean: -67.823955\n",
      "ep 432: ep_len:533 episode reward: total was -100.020000. running mean: -68.145916\n",
      "ep 432: ep_len:501 episode reward: total was -28.630000. running mean: -67.750756\n",
      "ep 432: ep_len:40 episode reward: total was -15.910000. running mean: -67.232349\n",
      "ep 432: ep_len:74 episode reward: total was 6.150000. running mean: -66.498525\n",
      "ep 432: ep_len:568 episode reward: total was -122.450000. running mean: -67.058040\n",
      "ep 432: ep_len:606 episode reward: total was -99.190000. running mean: -67.379360\n",
      "epsilon:0.180804 episode_count: 3031. steps_count: 1337444.000000\n",
      "Time elapsed:  3793.9029717445374\n",
      "ep 433: ep_len:123 episode reward: total was -18.700000. running mean: -66.892566\n",
      "ep 433: ep_len:500 episode reward: total was -73.400000. running mean: -66.957641\n",
      "ep 433: ep_len:541 episode reward: total was -98.900000. running mean: -67.277064\n",
      "ep 433: ep_len:627 episode reward: total was -82.810000. running mean: -67.432393\n",
      "ep 433: ep_len:3 episode reward: total was 0.000000. running mean: -66.758070\n",
      "ep 433: ep_len:607 episode reward: total was -60.230000. running mean: -66.692789\n",
      "ep 433: ep_len:556 episode reward: total was -102.270000. running mean: -67.048561\n",
      "epsilon:0.180759 episode_count: 3038. steps_count: 1340401.000000\n",
      "Time elapsed:  3803.455602645874\n",
      "ep 434: ep_len:265 episode reward: total was -13.290000. running mean: -66.510975\n",
      "ep 434: ep_len:512 episode reward: total was -78.400000. running mean: -66.629866\n",
      "ep 434: ep_len:437 episode reward: total was -68.470000. running mean: -66.648267\n",
      "ep 434: ep_len:502 episode reward: total was -55.770000. running mean: -66.539484\n",
      "ep 434: ep_len:85 episode reward: total was -10.300000. running mean: -65.977089\n",
      "ep 434: ep_len:530 episode reward: total was -87.890000. running mean: -66.196219\n",
      "ep 434: ep_len:539 episode reward: total was -72.960000. running mean: -66.263856\n",
      "epsilon:0.180715 episode_count: 3045. steps_count: 1343271.000000\n",
      "Time elapsed:  3812.7205893993378\n",
      "ep 435: ep_len:552 episode reward: total was -50.070000. running mean: -66.101918\n",
      "ep 435: ep_len:500 episode reward: total was -99.010000. running mean: -66.430999\n",
      "ep 435: ep_len:448 episode reward: total was -37.770000. running mean: -66.144389\n",
      "ep 435: ep_len:507 episode reward: total was -93.590000. running mean: -66.418845\n",
      "ep 435: ep_len:3 episode reward: total was 0.000000. running mean: -65.754656\n",
      "ep 435: ep_len:141 episode reward: total was -2.600000. running mean: -65.123110\n",
      "ep 435: ep_len:541 episode reward: total was -146.600000. running mean: -65.937879\n",
      "epsilon:0.180671 episode_count: 3052. steps_count: 1345963.000000\n",
      "Time elapsed:  3821.1790294647217\n",
      "ep 436: ep_len:500 episode reward: total was -10.540000. running mean: -65.383900\n",
      "ep 436: ep_len:501 episode reward: total was -80.890000. running mean: -65.538961\n",
      "ep 436: ep_len:500 episode reward: total was -71.220000. running mean: -65.595771\n",
      "ep 436: ep_len:500 episode reward: total was -66.870000. running mean: -65.608514\n",
      "ep 436: ep_len:3 episode reward: total was 0.000000. running mean: -64.952428\n",
      "ep 436: ep_len:576 episode reward: total was -126.850000. running mean: -65.571404\n",
      "ep 436: ep_len:188 episode reward: total was -19.660000. running mean: -65.112290\n",
      "epsilon:0.180626 episode_count: 3059. steps_count: 1348731.000000\n",
      "Time elapsed:  3829.610957622528\n",
      "ep 437: ep_len:616 episode reward: total was -92.360000. running mean: -65.384767\n",
      "ep 437: ep_len:361 episode reward: total was -145.160000. running mean: -66.182519\n",
      "ep 437: ep_len:500 episode reward: total was -133.400000. running mean: -66.854694\n",
      "ep 437: ep_len:126 episode reward: total was -8.040000. running mean: -66.266547\n",
      "ep 437: ep_len:3 episode reward: total was 0.000000. running mean: -65.603882\n",
      "ep 437: ep_len:500 episode reward: total was -101.680000. running mean: -65.964643\n",
      "ep 437: ep_len:511 episode reward: total was -91.540000. running mean: -66.220397\n",
      "epsilon:0.180582 episode_count: 3066. steps_count: 1351348.000000\n",
      "Time elapsed:  3837.689774990082\n",
      "ep 438: ep_len:592 episode reward: total was -7.020000. running mean: -65.628393\n",
      "ep 438: ep_len:251 episode reward: total was -71.830000. running mean: -65.690409\n",
      "ep 438: ep_len:510 episode reward: total was -77.350000. running mean: -65.807005\n",
      "ep 438: ep_len:128 episode reward: total was -3.520000. running mean: -65.184135\n",
      "ep 438: ep_len:3 episode reward: total was 0.000000. running mean: -64.532293\n",
      "ep 438: ep_len:316 episode reward: total was -26.000000. running mean: -64.146970\n",
      "ep 438: ep_len:577 episode reward: total was -83.830000. running mean: -64.343801\n",
      "epsilon:0.180538 episode_count: 3073. steps_count: 1353725.000000\n",
      "Time elapsed:  3844.2802278995514\n",
      "ep 439: ep_len:576 episode reward: total was -138.710000. running mean: -65.087463\n",
      "ep 439: ep_len:500 episode reward: total was -59.670000. running mean: -65.033288\n",
      "ep 439: ep_len:522 episode reward: total was -129.850000. running mean: -65.681455\n",
      "ep 439: ep_len:511 episode reward: total was -70.400000. running mean: -65.728641\n",
      "ep 439: ep_len:3 episode reward: total was -1.500000. running mean: -65.086354\n",
      "ep 439: ep_len:500 episode reward: total was -86.980000. running mean: -65.305291\n",
      "ep 439: ep_len:598 episode reward: total was -78.610000. running mean: -65.438338\n",
      "epsilon:0.180493 episode_count: 3080. steps_count: 1356935.000000\n",
      "Time elapsed:  3852.9171979427338\n",
      "ep 440: ep_len:608 episode reward: total was -32.170000. running mean: -65.105654\n",
      "ep 440: ep_len:544 episode reward: total was -83.760000. running mean: -65.292198\n",
      "ep 440: ep_len:557 episode reward: total was -66.370000. running mean: -65.302976\n",
      "ep 440: ep_len:114 episode reward: total was -16.680000. running mean: -64.816746\n",
      "ep 440: ep_len:107 episode reward: total was -4.290000. running mean: -64.211479\n",
      "ep 440: ep_len:532 episode reward: total was -59.350000. running mean: -64.162864\n",
      "ep 440: ep_len:298 episode reward: total was -87.810000. running mean: -64.399335\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.180449 episode_count: 3087. steps_count: 1359695.000000\n",
      "Time elapsed:  3865.302042722702\n",
      "ep 441: ep_len:585 episode reward: total was -47.490000. running mean: -64.230242\n",
      "ep 441: ep_len:600 episode reward: total was -124.180000. running mean: -64.829739\n",
      "ep 441: ep_len:511 episode reward: total was -74.090000. running mean: -64.922342\n",
      "ep 441: ep_len:502 episode reward: total was -81.400000. running mean: -65.087119\n",
      "ep 441: ep_len:91 episode reward: total was 3.200000. running mean: -64.404247\n",
      "ep 441: ep_len:501 episode reward: total was -117.980000. running mean: -64.940005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 441: ep_len:543 episode reward: total was -77.810000. running mean: -65.068705\n",
      "epsilon:0.180405 episode_count: 3094. steps_count: 1363028.000000\n",
      "Time elapsed:  3874.1084372997284\n",
      "ep 442: ep_len:501 episode reward: total was -17.210000. running mean: -64.590118\n",
      "ep 442: ep_len:500 episode reward: total was -126.480000. running mean: -65.209017\n",
      "ep 442: ep_len:546 episode reward: total was -75.580000. running mean: -65.312726\n",
      "ep 442: ep_len:522 episode reward: total was -87.560000. running mean: -65.535199\n",
      "ep 442: ep_len:3 episode reward: total was 0.000000. running mean: -64.879847\n",
      "ep 442: ep_len:521 episode reward: total was -92.530000. running mean: -65.156349\n",
      "ep 442: ep_len:582 episode reward: total was -62.830000. running mean: -65.133085\n",
      "epsilon:0.180360 episode_count: 3101. steps_count: 1366203.000000\n",
      "Time elapsed:  3882.4538114070892\n",
      "ep 443: ep_len:550 episode reward: total was -85.350000. running mean: -65.335254\n",
      "ep 443: ep_len:526 episode reward: total was -81.560000. running mean: -65.497502\n",
      "ep 443: ep_len:576 episode reward: total was -110.180000. running mean: -65.944327\n",
      "ep 443: ep_len:562 episode reward: total was -4.200000. running mean: -65.326884\n",
      "ep 443: ep_len:3 episode reward: total was 0.000000. running mean: -64.673615\n",
      "ep 443: ep_len:699 episode reward: total was -92.250000. running mean: -64.949379\n",
      "ep 443: ep_len:598 episode reward: total was -88.640000. running mean: -65.186285\n",
      "epsilon:0.180316 episode_count: 3108. steps_count: 1369717.000000\n",
      "Time elapsed:  3891.6509289741516\n",
      "ep 444: ep_len:241 episode reward: total was 3.180000. running mean: -64.502622\n",
      "ep 444: ep_len:533 episode reward: total was -48.020000. running mean: -64.337796\n",
      "ep 444: ep_len:72 episode reward: total was -14.840000. running mean: -63.842818\n",
      "ep 444: ep_len:500 episode reward: total was -73.520000. running mean: -63.939590\n",
      "ep 444: ep_len:3 episode reward: total was -1.500000. running mean: -63.315194\n",
      "ep 444: ep_len:503 episode reward: total was -34.490000. running mean: -63.026942\n",
      "ep 444: ep_len:622 episode reward: total was -45.710000. running mean: -62.853772\n",
      "epsilon:0.180272 episode_count: 3115. steps_count: 1372191.000000\n",
      "Time elapsed:  3898.5432217121124\n",
      "ep 445: ep_len:109 episode reward: total was -13.270000. running mean: -62.357935\n",
      "ep 445: ep_len:589 episode reward: total was -34.250000. running mean: -62.076855\n",
      "ep 445: ep_len:569 episode reward: total was -143.950000. running mean: -62.895587\n",
      "ep 445: ep_len:123 episode reward: total was -5.070000. running mean: -62.317331\n",
      "ep 445: ep_len:3 episode reward: total was -1.500000. running mean: -61.709158\n",
      "ep 445: ep_len:515 episode reward: total was -137.550000. running mean: -62.467566\n",
      "ep 445: ep_len:566 episode reward: total was -83.410000. running mean: -62.676990\n",
      "epsilon:0.180227 episode_count: 3122. steps_count: 1374665.000000\n",
      "Time elapsed:  3905.3768842220306\n",
      "ep 446: ep_len:606 episode reward: total was -80.300000. running mean: -62.853220\n",
      "ep 446: ep_len:500 episode reward: total was -119.530000. running mean: -63.419988\n",
      "ep 446: ep_len:633 episode reward: total was -95.120000. running mean: -63.736988\n",
      "ep 446: ep_len:500 episode reward: total was -71.700000. running mean: -63.816618\n",
      "ep 446: ep_len:127 episode reward: total was 16.820000. running mean: -63.010252\n",
      "ep 446: ep_len:506 episode reward: total was -82.820000. running mean: -63.208350\n",
      "ep 446: ep_len:642 episode reward: total was -171.270000. running mean: -64.288966\n",
      "epsilon:0.180183 episode_count: 3129. steps_count: 1378179.000000\n",
      "Time elapsed:  3914.6016716957092\n",
      "ep 447: ep_len:586 episode reward: total was -87.970000. running mean: -64.525777\n",
      "ep 447: ep_len:500 episode reward: total was -128.560000. running mean: -65.166119\n",
      "ep 447: ep_len:379 episode reward: total was -8.270000. running mean: -64.597158\n",
      "ep 447: ep_len:510 episode reward: total was -25.460000. running mean: -64.205786\n",
      "ep 447: ep_len:3 episode reward: total was 0.000000. running mean: -63.563728\n",
      "ep 447: ep_len:644 episode reward: total was -98.490000. running mean: -63.912991\n",
      "ep 447: ep_len:615 episode reward: total was -57.660000. running mean: -63.850461\n",
      "epsilon:0.180139 episode_count: 3136. steps_count: 1381416.000000\n",
      "Time elapsed:  3923.347363471985\n",
      "ep 448: ep_len:570 episode reward: total was -147.780000. running mean: -64.689756\n",
      "ep 448: ep_len:500 episode reward: total was -4.060000. running mean: -64.083459\n",
      "ep 448: ep_len:434 episode reward: total was -57.690000. running mean: -64.019524\n",
      "ep 448: ep_len:509 episode reward: total was -73.830000. running mean: -64.117629\n",
      "ep 448: ep_len:3 episode reward: total was 0.000000. running mean: -63.476453\n",
      "ep 448: ep_len:601 episode reward: total was -117.460000. running mean: -64.016288\n",
      "ep 448: ep_len:594 episode reward: total was -60.180000. running mean: -63.977925\n",
      "epsilon:0.180094 episode_count: 3143. steps_count: 1384627.000000\n",
      "Time elapsed:  3932.176598548889\n",
      "ep 449: ep_len:572 episode reward: total was -28.960000. running mean: -63.627746\n",
      "ep 449: ep_len:500 episode reward: total was -59.480000. running mean: -63.586269\n",
      "ep 449: ep_len:555 episode reward: total was -98.890000. running mean: -63.939306\n",
      "ep 449: ep_len:519 episode reward: total was -90.720000. running mean: -64.207113\n",
      "ep 449: ep_len:81 episode reward: total was 8.730000. running mean: -63.477742\n",
      "ep 449: ep_len:627 episode reward: total was -63.030000. running mean: -63.473264\n",
      "ep 449: ep_len:500 episode reward: total was -118.640000. running mean: -64.024932\n",
      "epsilon:0.180050 episode_count: 3150. steps_count: 1387981.000000\n",
      "Time elapsed:  3941.288362979889\n",
      "ep 450: ep_len:556 episode reward: total was -50.080000. running mean: -63.885482\n",
      "ep 450: ep_len:503 episode reward: total was -122.210000. running mean: -64.468728\n",
      "ep 450: ep_len:605 episode reward: total was -119.810000. running mean: -65.022140\n",
      "ep 450: ep_len:577 episode reward: total was -68.420000. running mean: -65.056119\n",
      "ep 450: ep_len:3 episode reward: total was 0.000000. running mean: -64.405558\n",
      "ep 450: ep_len:514 episode reward: total was -183.480000. running mean: -65.596302\n",
      "ep 450: ep_len:565 episode reward: total was -66.610000. running mean: -65.606439\n",
      "epsilon:0.180006 episode_count: 3157. steps_count: 1391304.000000\n",
      "Time elapsed:  3949.9446411132812\n",
      "ep 451: ep_len:507 episode reward: total was -55.360000. running mean: -65.503975\n",
      "ep 451: ep_len:507 episode reward: total was -81.490000. running mean: -65.663835\n",
      "ep 451: ep_len:586 episode reward: total was -32.690000. running mean: -65.334097\n",
      "ep 451: ep_len:544 episode reward: total was -74.310000. running mean: -65.423856\n",
      "ep 451: ep_len:3 episode reward: total was 0.000000. running mean: -64.769617\n",
      "ep 451: ep_len:688 episode reward: total was -69.430000. running mean: -64.816221\n",
      "ep 451: ep_len:592 episode reward: total was -97.320000. running mean: -65.141259\n",
      "epsilon:0.179961 episode_count: 3164. steps_count: 1394731.000000\n",
      "Time elapsed:  3959.4295296669006\n",
      "ep 452: ep_len:647 episode reward: total was -139.910000. running mean: -65.888946\n",
      "ep 452: ep_len:631 episode reward: total was -56.680000. running mean: -65.796857\n",
      "ep 452: ep_len:619 episode reward: total was -149.490000. running mean: -66.633788\n",
      "ep 452: ep_len:604 episode reward: total was -6.770000. running mean: -66.035150\n",
      "ep 452: ep_len:3 episode reward: total was 0.000000. running mean: -65.374799\n",
      "ep 452: ep_len:522 episode reward: total was -94.260000. running mean: -65.663651\n",
      "ep 452: ep_len:579 episode reward: total was -82.800000. running mean: -65.835014\n",
      "epsilon:0.179917 episode_count: 3171. steps_count: 1398336.000000\n",
      "Time elapsed:  3968.800306558609\n",
      "ep 453: ep_len:673 episode reward: total was -253.290000. running mean: -67.709564\n",
      "ep 453: ep_len:500 episode reward: total was -85.630000. running mean: -67.888768\n",
      "ep 453: ep_len:67 episode reward: total was -10.270000. running mean: -67.312581\n",
      "ep 453: ep_len:530 episode reward: total was -73.230000. running mean: -67.371755\n",
      "ep 453: ep_len:36 episode reward: total was 12.000000. running mean: -66.578037\n",
      "ep 453: ep_len:599 episode reward: total was -71.350000. running mean: -66.625757\n",
      "ep 453: ep_len:518 episode reward: total was -87.570000. running mean: -66.835199\n",
      "epsilon:0.179873 episode_count: 3178. steps_count: 1401259.000000\n",
      "Time elapsed:  3976.7675862312317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 454: ep_len:510 episode reward: total was -69.980000. running mean: -66.866647\n",
      "ep 454: ep_len:334 episode reward: total was -97.040000. running mean: -67.168381\n",
      "ep 454: ep_len:581 episode reward: total was -80.840000. running mean: -67.305097\n",
      "ep 454: ep_len:594 episode reward: total was -133.690000. running mean: -67.968946\n",
      "ep 454: ep_len:3 episode reward: total was 1.010000. running mean: -67.279157\n",
      "ep 454: ep_len:603 episode reward: total was -83.620000. running mean: -67.442565\n",
      "ep 454: ep_len:203 episode reward: total was -69.740000. running mean: -67.465539\n",
      "epsilon:0.179828 episode_count: 3185. steps_count: 1404087.000000\n",
      "Time elapsed:  3984.5649330615997\n",
      "ep 455: ep_len:547 episode reward: total was -39.380000. running mean: -67.184684\n",
      "ep 455: ep_len:574 episode reward: total was -55.220000. running mean: -67.065037\n",
      "ep 455: ep_len:570 episode reward: total was -112.500000. running mean: -67.519387\n",
      "ep 455: ep_len:500 episode reward: total was -24.490000. running mean: -67.089093\n",
      "ep 455: ep_len:92 episode reward: total was 13.740000. running mean: -66.280802\n",
      "ep 455: ep_len:500 episode reward: total was -88.670000. running mean: -66.504694\n",
      "ep 455: ep_len:558 episode reward: total was -97.580000. running mean: -66.815447\n",
      "epsilon:0.179784 episode_count: 3192. steps_count: 1407428.000000\n",
      "Time elapsed:  3993.4117221832275\n",
      "ep 456: ep_len:599 episode reward: total was -56.810000. running mean: -66.715393\n",
      "ep 456: ep_len:506 episode reward: total was -62.200000. running mean: -66.670239\n",
      "ep 456: ep_len:557 episode reward: total was -90.070000. running mean: -66.904236\n",
      "ep 456: ep_len:42 episode reward: total was -11.290000. running mean: -66.348094\n",
      "ep 456: ep_len:97 episode reward: total was 12.630000. running mean: -65.558313\n",
      "ep 456: ep_len:174 episode reward: total was -9.430000. running mean: -64.997030\n",
      "ep 456: ep_len:500 episode reward: total was -86.870000. running mean: -65.215760\n",
      "epsilon:0.179740 episode_count: 3199. steps_count: 1409903.000000\n",
      "Time elapsed:  4000.3011865615845\n",
      "ep 457: ep_len:572 episode reward: total was -90.860000. running mean: -65.472202\n",
      "ep 457: ep_len:528 episode reward: total was -90.620000. running mean: -65.723680\n",
      "ep 457: ep_len:540 episode reward: total was -96.130000. running mean: -66.027743\n",
      "ep 457: ep_len:506 episode reward: total was -128.130000. running mean: -66.648766\n",
      "ep 457: ep_len:1 episode reward: total was -1.000000. running mean: -65.992278\n",
      "ep 457: ep_len:513 episode reward: total was -106.630000. running mean: -66.398655\n",
      "ep 457: ep_len:522 episode reward: total was -92.040000. running mean: -66.655069\n",
      "epsilon:0.179695 episode_count: 3206. steps_count: 1413085.000000\n",
      "Time elapsed:  4008.8130667209625\n",
      "ep 458: ep_len:624 episode reward: total was -30.200000. running mean: -66.290518\n",
      "ep 458: ep_len:582 episode reward: total was -96.570000. running mean: -66.593313\n",
      "ep 458: ep_len:363 episode reward: total was -36.200000. running mean: -66.289380\n",
      "ep 458: ep_len:569 episode reward: total was -26.960000. running mean: -65.896086\n",
      "ep 458: ep_len:3 episode reward: total was 0.000000. running mean: -65.237125\n",
      "ep 458: ep_len:319 episode reward: total was -48.830000. running mean: -65.073054\n",
      "ep 458: ep_len:500 episode reward: total was -60.500000. running mean: -65.027323\n",
      "epsilon:0.179651 episode_count: 3213. steps_count: 1416045.000000\n",
      "Time elapsed:  4016.6231603622437\n",
      "ep 459: ep_len:659 episode reward: total was -116.740000. running mean: -65.544450\n",
      "ep 459: ep_len:500 episode reward: total was -24.730000. running mean: -65.136306\n",
      "ep 459: ep_len:540 episode reward: total was -94.170000. running mean: -65.426643\n",
      "ep 459: ep_len:502 episode reward: total was -44.540000. running mean: -65.217776\n",
      "ep 459: ep_len:92 episode reward: total was -8.300000. running mean: -64.648598\n",
      "ep 459: ep_len:501 episode reward: total was -102.890000. running mean: -65.031012\n",
      "ep 459: ep_len:500 episode reward: total was -93.720000. running mean: -65.317902\n",
      "epsilon:0.179607 episode_count: 3220. steps_count: 1419339.000000\n",
      "Time elapsed:  4025.2393386363983\n",
      "ep 460: ep_len:588 episode reward: total was -100.400000. running mean: -65.668723\n",
      "ep 460: ep_len:527 episode reward: total was -66.270000. running mean: -65.674736\n",
      "ep 460: ep_len:64 episode reward: total was 2.190000. running mean: -64.996089\n",
      "ep 460: ep_len:143 episode reward: total was -60.480000. running mean: -64.950928\n",
      "ep 460: ep_len:3 episode reward: total was 0.000000. running mean: -64.301418\n",
      "ep 460: ep_len:183 episode reward: total was 7.990000. running mean: -63.578504\n",
      "ep 460: ep_len:597 episode reward: total was -77.090000. running mean: -63.713619\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.179562 episode_count: 3227. steps_count: 1421444.000000\n",
      "Time elapsed:  4035.9439404010773\n",
      "ep 461: ep_len:606 episode reward: total was -127.290000. running mean: -64.349383\n",
      "ep 461: ep_len:500 episode reward: total was -8.320000. running mean: -63.789089\n",
      "ep 461: ep_len:634 episode reward: total was -120.060000. running mean: -64.351798\n",
      "ep 461: ep_len:595 episode reward: total was -33.310000. running mean: -64.041380\n",
      "ep 461: ep_len:3 episode reward: total was 0.000000. running mean: -63.400967\n",
      "ep 461: ep_len:643 episode reward: total was -74.660000. running mean: -63.513557\n",
      "ep 461: ep_len:515 episode reward: total was -87.630000. running mean: -63.754721\n",
      "epsilon:0.179518 episode_count: 3234. steps_count: 1424940.000000\n",
      "Time elapsed:  4044.955926179886\n",
      "ep 462: ep_len:507 episode reward: total was 4.340000. running mean: -63.073774\n",
      "ep 462: ep_len:511 episode reward: total was -4.860000. running mean: -62.491636\n",
      "ep 462: ep_len:500 episode reward: total was -39.160000. running mean: -62.258320\n",
      "ep 462: ep_len:500 episode reward: total was -109.620000. running mean: -62.731937\n",
      "ep 462: ep_len:97 episode reward: total was -3.290000. running mean: -62.137517\n",
      "ep 462: ep_len:643 episode reward: total was -129.580000. running mean: -62.811942\n",
      "ep 462: ep_len:571 episode reward: total was -52.010000. running mean: -62.703923\n",
      "epsilon:0.179474 episode_count: 3241. steps_count: 1428269.000000\n",
      "Time elapsed:  4054.08909368515\n",
      "ep 463: ep_len:528 episode reward: total was -22.930000. running mean: -62.306184\n",
      "ep 463: ep_len:500 episode reward: total was -46.780000. running mean: -62.150922\n",
      "ep 463: ep_len:608 episode reward: total was -48.000000. running mean: -62.009413\n",
      "ep 463: ep_len:553 episode reward: total was -43.960000. running mean: -61.828918\n",
      "ep 463: ep_len:75 episode reward: total was 3.710000. running mean: -61.173529\n",
      "ep 463: ep_len:664 episode reward: total was -165.620000. running mean: -62.217994\n",
      "ep 463: ep_len:572 episode reward: total was -125.440000. running mean: -62.850214\n",
      "epsilon:0.179429 episode_count: 3248. steps_count: 1431769.000000\n",
      "Time elapsed:  4063.2888226509094\n",
      "ep 464: ep_len:589 episode reward: total was -72.130000. running mean: -62.943012\n",
      "ep 464: ep_len:583 episode reward: total was -66.440000. running mean: -62.977982\n",
      "ep 464: ep_len:79 episode reward: total was -13.820000. running mean: -62.486402\n",
      "ep 464: ep_len:500 episode reward: total was -126.810000. running mean: -63.129638\n",
      "ep 464: ep_len:87 episode reward: total was -11.290000. running mean: -62.611242\n",
      "ep 464: ep_len:562 episode reward: total was -57.130000. running mean: -62.556429\n",
      "ep 464: ep_len:513 episode reward: total was -72.440000. running mean: -62.655265\n",
      "epsilon:0.179385 episode_count: 3255. steps_count: 1434682.000000\n",
      "Time elapsed:  4071.40793299675\n",
      "ep 465: ep_len:621 episode reward: total was -135.920000. running mean: -63.387912\n",
      "ep 465: ep_len:500 episode reward: total was -63.480000. running mean: -63.388833\n",
      "ep 465: ep_len:530 episode reward: total was -100.690000. running mean: -63.761845\n",
      "ep 465: ep_len:500 episode reward: total was -8.290000. running mean: -63.207126\n",
      "ep 465: ep_len:3 episode reward: total was 0.000000. running mean: -62.575055\n",
      "ep 465: ep_len:500 episode reward: total was -45.700000. running mean: -62.406304\n",
      "ep 465: ep_len:500 episode reward: total was -124.050000. running mean: -63.022741\n",
      "epsilon:0.179341 episode_count: 3262. steps_count: 1437836.000000\n",
      "Time elapsed:  4079.7882380485535\n",
      "ep 466: ep_len:121 episode reward: total was -2.070000. running mean: -62.413214\n",
      "ep 466: ep_len:500 episode reward: total was -11.410000. running mean: -61.903182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 466: ep_len:500 episode reward: total was -87.560000. running mean: -62.159750\n",
      "ep 466: ep_len:505 episode reward: total was -144.540000. running mean: -62.983553\n",
      "ep 466: ep_len:3 episode reward: total was 0.000000. running mean: -62.353717\n",
      "ep 466: ep_len:533 episode reward: total was -107.110000. running mean: -62.801280\n",
      "ep 466: ep_len:332 episode reward: total was -131.930000. running mean: -63.492567\n",
      "epsilon:0.179296 episode_count: 3269. steps_count: 1440330.000000\n",
      "Time elapsed:  4086.595160961151\n",
      "ep 467: ep_len:207 episode reward: total was 1.170000. running mean: -62.845941\n",
      "ep 467: ep_len:526 episode reward: total was -81.170000. running mean: -63.029182\n",
      "ep 467: ep_len:510 episode reward: total was -50.480000. running mean: -62.903690\n",
      "ep 467: ep_len:391 episode reward: total was -49.280000. running mean: -62.767453\n",
      "ep 467: ep_len:3 episode reward: total was -1.500000. running mean: -62.154779\n",
      "ep 467: ep_len:570 episode reward: total was -68.580000. running mean: -62.219031\n",
      "ep 467: ep_len:519 episode reward: total was -74.390000. running mean: -62.340741\n",
      "epsilon:0.179252 episode_count: 3276. steps_count: 1443056.000000\n",
      "Time elapsed:  4093.9230065345764\n",
      "ep 468: ep_len:558 episode reward: total was -17.630000. running mean: -61.893633\n",
      "ep 468: ep_len:500 episode reward: total was -63.900000. running mean: -61.913697\n",
      "ep 468: ep_len:591 episode reward: total was -110.700000. running mean: -62.401560\n",
      "ep 468: ep_len:534 episode reward: total was -27.390000. running mean: -62.051444\n",
      "ep 468: ep_len:112 episode reward: total was -6.750000. running mean: -61.498430\n",
      "ep 468: ep_len:500 episode reward: total was -119.290000. running mean: -62.076346\n",
      "ep 468: ep_len:541 episode reward: total was -163.880000. running mean: -63.094382\n",
      "epsilon:0.179208 episode_count: 3283. steps_count: 1446392.000000\n",
      "Time elapsed:  4102.9528086185455\n",
      "ep 469: ep_len:569 episode reward: total was -89.360000. running mean: -63.357038\n",
      "ep 469: ep_len:512 episode reward: total was -146.790000. running mean: -64.191368\n",
      "ep 469: ep_len:557 episode reward: total was -101.110000. running mean: -64.560554\n",
      "ep 469: ep_len:604 episode reward: total was -49.370000. running mean: -64.408649\n",
      "ep 469: ep_len:100 episode reward: total was 1.300000. running mean: -63.751562\n",
      "ep 469: ep_len:575 episode reward: total was -86.990000. running mean: -63.983947\n",
      "ep 469: ep_len:194 episode reward: total was -38.180000. running mean: -63.725907\n",
      "epsilon:0.179163 episode_count: 3290. steps_count: 1449503.000000\n",
      "Time elapsed:  4111.73371386528\n",
      "ep 470: ep_len:501 episode reward: total was -104.440000. running mean: -64.133048\n",
      "ep 470: ep_len:653 episode reward: total was -150.220000. running mean: -64.993918\n",
      "ep 470: ep_len:446 episode reward: total was -12.750000. running mean: -64.471478\n",
      "ep 470: ep_len:508 episode reward: total was -95.370000. running mean: -64.780464\n",
      "ep 470: ep_len:49 episode reward: total was 11.000000. running mean: -64.022659\n",
      "ep 470: ep_len:500 episode reward: total was -97.460000. running mean: -64.357032\n",
      "ep 470: ep_len:546 episode reward: total was -105.120000. running mean: -64.764662\n",
      "epsilon:0.179119 episode_count: 3297. steps_count: 1452706.000000\n",
      "Time elapsed:  4120.7120571136475\n",
      "ep 471: ep_len:574 episode reward: total was -96.530000. running mean: -65.082315\n",
      "ep 471: ep_len:515 episode reward: total was -87.140000. running mean: -65.302892\n",
      "ep 471: ep_len:376 episode reward: total was -29.390000. running mean: -64.943763\n",
      "ep 471: ep_len:540 episode reward: total was -33.840000. running mean: -64.632726\n",
      "ep 471: ep_len:94 episode reward: total was -8.340000. running mean: -64.069798\n",
      "ep 471: ep_len:289 episode reward: total was -41.700000. running mean: -63.846100\n",
      "ep 471: ep_len:556 episode reward: total was -43.870000. running mean: -63.646339\n",
      "epsilon:0.179075 episode_count: 3304. steps_count: 1455650.000000\n",
      "Time elapsed:  4129.366320133209\n",
      "ep 472: ep_len:508 episode reward: total was -66.220000. running mean: -63.672076\n",
      "ep 472: ep_len:620 episode reward: total was -80.990000. running mean: -63.845255\n",
      "ep 472: ep_len:540 episode reward: total was -91.850000. running mean: -64.125303\n",
      "ep 472: ep_len:529 episode reward: total was -121.590000. running mean: -64.699950\n",
      "ep 472: ep_len:3 episode reward: total was 0.000000. running mean: -64.052950\n",
      "ep 472: ep_len:615 episode reward: total was -190.620000. running mean: -65.318621\n",
      "ep 472: ep_len:314 episode reward: total was -72.060000. running mean: -65.386035\n",
      "epsilon:0.179030 episode_count: 3311. steps_count: 1458779.000000\n",
      "Time elapsed:  4137.706413030624\n",
      "ep 473: ep_len:500 episode reward: total was -49.760000. running mean: -65.229774\n",
      "ep 473: ep_len:533 episode reward: total was -70.260000. running mean: -65.280076\n",
      "ep 473: ep_len:590 episode reward: total was -59.460000. running mean: -65.221876\n",
      "ep 473: ep_len:540 episode reward: total was -56.280000. running mean: -65.132457\n",
      "ep 473: ep_len:3 episode reward: total was 0.000000. running mean: -64.481132\n",
      "ep 473: ep_len:540 episode reward: total was -87.380000. running mean: -64.710121\n",
      "ep 473: ep_len:596 episode reward: total was -112.870000. running mean: -65.191720\n",
      "epsilon:0.178986 episode_count: 3318. steps_count: 1462081.000000\n",
      "Time elapsed:  4145.755677700043\n",
      "ep 474: ep_len:645 episode reward: total was -156.120000. running mean: -66.101003\n",
      "ep 474: ep_len:500 episode reward: total was -10.410000. running mean: -65.544093\n",
      "ep 474: ep_len:557 episode reward: total was -99.180000. running mean: -65.880452\n",
      "ep 474: ep_len:531 episode reward: total was -49.280000. running mean: -65.714447\n",
      "ep 474: ep_len:3 episode reward: total was 0.000000. running mean: -65.057303\n",
      "ep 474: ep_len:527 episode reward: total was -117.110000. running mean: -65.577830\n",
      "ep 474: ep_len:521 episode reward: total was -95.340000. running mean: -65.875451\n",
      "epsilon:0.178942 episode_count: 3325. steps_count: 1465365.000000\n",
      "Time elapsed:  4154.248684644699\n",
      "ep 475: ep_len:626 episode reward: total was -224.940000. running mean: -67.466097\n",
      "ep 475: ep_len:500 episode reward: total was -77.070000. running mean: -67.562136\n",
      "ep 475: ep_len:57 episode reward: total was -3.880000. running mean: -66.925315\n",
      "ep 475: ep_len:512 episode reward: total was -68.160000. running mean: -66.937661\n",
      "ep 475: ep_len:50 episode reward: total was -16.500000. running mean: -66.433285\n",
      "ep 475: ep_len:650 episode reward: total was -80.100000. running mean: -66.569952\n",
      "ep 475: ep_len:198 episode reward: total was -39.620000. running mean: -66.300452\n",
      "epsilon:0.178897 episode_count: 3332. steps_count: 1467958.000000\n",
      "Time elapsed:  4161.257748603821\n",
      "ep 476: ep_len:595 episode reward: total was -88.100000. running mean: -66.518448\n",
      "ep 476: ep_len:596 episode reward: total was 9.580000. running mean: -65.757463\n",
      "ep 476: ep_len:328 episode reward: total was -44.460000. running mean: -65.544489\n",
      "ep 476: ep_len:502 episode reward: total was -97.650000. running mean: -65.865544\n",
      "ep 476: ep_len:3 episode reward: total was 0.000000. running mean: -65.206888\n",
      "ep 476: ep_len:639 episode reward: total was -44.360000. running mean: -64.998420\n",
      "ep 476: ep_len:587 episode reward: total was -59.490000. running mean: -64.943335\n",
      "epsilon:0.178853 episode_count: 3339. steps_count: 1471208.000000\n",
      "Time elapsed:  4169.907579421997\n",
      "ep 477: ep_len:654 episode reward: total was -107.710000. running mean: -65.371002\n",
      "ep 477: ep_len:525 episode reward: total was -111.860000. running mean: -65.835892\n",
      "ep 477: ep_len:587 episode reward: total was -103.710000. running mean: -66.214633\n",
      "ep 477: ep_len:517 episode reward: total was -54.830000. running mean: -66.100787\n",
      "ep 477: ep_len:3 episode reward: total was -0.490000. running mean: -65.444679\n",
      "ep 477: ep_len:500 episode reward: total was -51.130000. running mean: -65.301532\n",
      "ep 477: ep_len:607 episode reward: total was -94.890000. running mean: -65.597417\n",
      "epsilon:0.178809 episode_count: 3346. steps_count: 1474601.000000\n",
      "Time elapsed:  4179.145528078079\n",
      "ep 478: ep_len:556 episode reward: total was -74.300000. running mean: -65.684443\n",
      "ep 478: ep_len:524 episode reward: total was -67.220000. running mean: -65.699798\n",
      "ep 478: ep_len:537 episode reward: total was -90.180000. running mean: -65.944600\n",
      "ep 478: ep_len:40 episode reward: total was -16.400000. running mean: -65.449154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 478: ep_len:96 episode reward: total was -18.270000. running mean: -64.977363\n",
      "ep 478: ep_len:574 episode reward: total was -152.660000. running mean: -65.854189\n",
      "ep 478: ep_len:552 episode reward: total was -66.110000. running mean: -65.856747\n",
      "epsilon:0.178764 episode_count: 3353. steps_count: 1477480.000000\n",
      "Time elapsed:  4186.837096691132\n",
      "ep 479: ep_len:500 episode reward: total was -19.790000. running mean: -65.396080\n",
      "ep 479: ep_len:543 episode reward: total was -57.200000. running mean: -65.314119\n",
      "ep 479: ep_len:500 episode reward: total was -44.960000. running mean: -65.110578\n",
      "ep 479: ep_len:503 episode reward: total was -43.010000. running mean: -64.889572\n",
      "ep 479: ep_len:3 episode reward: total was 0.000000. running mean: -64.240676\n",
      "ep 479: ep_len:147 episode reward: total was 8.420000. running mean: -63.514069\n",
      "ep 479: ep_len:616 episode reward: total was -59.740000. running mean: -63.476329\n",
      "epsilon:0.178720 episode_count: 3360. steps_count: 1480292.000000\n",
      "Time elapsed:  4194.495791196823\n",
      "ep 480: ep_len:585 episode reward: total was -25.890000. running mean: -63.100465\n",
      "ep 480: ep_len:185 episode reward: total was -13.380000. running mean: -62.603261\n",
      "ep 480: ep_len:501 episode reward: total was -88.220000. running mean: -62.859428\n",
      "ep 480: ep_len:528 episode reward: total was -46.200000. running mean: -62.692834\n",
      "ep 480: ep_len:3 episode reward: total was 0.000000. running mean: -62.065906\n",
      "ep 480: ep_len:514 episode reward: total was -62.840000. running mean: -62.073646\n",
      "ep 480: ep_len:552 episode reward: total was -76.620000. running mean: -62.219110\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.178676 episode_count: 3367. steps_count: 1483160.000000\n",
      "Time elapsed:  4207.037486791611\n",
      "ep 481: ep_len:591 episode reward: total was -71.090000. running mean: -62.307819\n",
      "ep 481: ep_len:550 episode reward: total was -48.190000. running mean: -62.166641\n",
      "ep 481: ep_len:620 episode reward: total was -79.940000. running mean: -62.344374\n",
      "ep 481: ep_len:432 episode reward: total was -58.640000. running mean: -62.307331\n",
      "ep 481: ep_len:3 episode reward: total was 0.000000. running mean: -61.684257\n",
      "ep 481: ep_len:531 episode reward: total was -127.790000. running mean: -62.345315\n",
      "ep 481: ep_len:513 episode reward: total was -96.520000. running mean: -62.687062\n",
      "epsilon:0.178631 episode_count: 3374. steps_count: 1486400.000000\n",
      "Time elapsed:  4215.811841011047\n",
      "ep 482: ep_len:547 episode reward: total was -95.340000. running mean: -63.013591\n",
      "ep 482: ep_len:510 episode reward: total was -33.030000. running mean: -62.713755\n",
      "ep 482: ep_len:393 episode reward: total was -13.170000. running mean: -62.218317\n",
      "ep 482: ep_len:500 episode reward: total was -21.520000. running mean: -61.811334\n",
      "ep 482: ep_len:3 episode reward: total was 0.000000. running mean: -61.193221\n",
      "ep 482: ep_len:716 episode reward: total was -203.450000. running mean: -62.615789\n",
      "ep 482: ep_len:286 episode reward: total was -28.510000. running mean: -62.274731\n",
      "epsilon:0.178587 episode_count: 3381. steps_count: 1489355.000000\n",
      "Time elapsed:  4223.551254987717\n",
      "ep 483: ep_len:500 episode reward: total was 9.320000. running mean: -61.558784\n",
      "ep 483: ep_len:171 episode reward: total was -52.490000. running mean: -61.468096\n",
      "ep 483: ep_len:532 episode reward: total was -56.370000. running mean: -61.417115\n",
      "ep 483: ep_len:500 episode reward: total was -73.860000. running mean: -61.541544\n",
      "ep 483: ep_len:103 episode reward: total was -14.370000. running mean: -61.069828\n",
      "ep 483: ep_len:523 episode reward: total was -79.830000. running mean: -61.257430\n",
      "ep 483: ep_len:584 episode reward: total was -72.720000. running mean: -61.372056\n",
      "epsilon:0.178543 episode_count: 3388. steps_count: 1492268.000000\n",
      "Time elapsed:  4231.181364774704\n",
      "ep 484: ep_len:501 episode reward: total was -107.470000. running mean: -61.833035\n",
      "ep 484: ep_len:655 episode reward: total was -58.170000. running mean: -61.796405\n",
      "ep 484: ep_len:641 episode reward: total was -81.360000. running mean: -61.992041\n",
      "ep 484: ep_len:500 episode reward: total was -97.310000. running mean: -62.345220\n",
      "ep 484: ep_len:3 episode reward: total was 0.000000. running mean: -61.721768\n",
      "ep 484: ep_len:540 episode reward: total was -50.800000. running mean: -61.612550\n",
      "ep 484: ep_len:500 episode reward: total was -101.980000. running mean: -62.016225\n",
      "epsilon:0.178498 episode_count: 3395. steps_count: 1495608.000000\n",
      "Time elapsed:  4239.788673400879\n",
      "ep 485: ep_len:229 episode reward: total was 7.670000. running mean: -61.319363\n",
      "ep 485: ep_len:508 episode reward: total was -35.630000. running mean: -61.062469\n",
      "ep 485: ep_len:500 episode reward: total was -74.330000. running mean: -61.195144\n",
      "ep 485: ep_len:566 episode reward: total was -32.900000. running mean: -60.912193\n",
      "ep 485: ep_len:3 episode reward: total was 0.000000. running mean: -60.303071\n",
      "ep 485: ep_len:520 episode reward: total was -102.330000. running mean: -60.723340\n",
      "ep 485: ep_len:604 episode reward: total was -105.200000. running mean: -61.168107\n",
      "epsilon:0.178454 episode_count: 3402. steps_count: 1498538.000000\n",
      "Time elapsed:  4247.810576677322\n",
      "ep 486: ep_len:500 episode reward: total was -54.960000. running mean: -61.106026\n",
      "ep 486: ep_len:607 episode reward: total was -66.030000. running mean: -61.155265\n",
      "ep 486: ep_len:349 episode reward: total was -13.010000. running mean: -60.673813\n",
      "ep 486: ep_len:574 episode reward: total was -27.150000. running mean: -60.338575\n",
      "ep 486: ep_len:75 episode reward: total was -2.440000. running mean: -59.759589\n",
      "ep 486: ep_len:500 episode reward: total was -69.740000. running mean: -59.859393\n",
      "ep 486: ep_len:329 episode reward: total was -55.670000. running mean: -59.817499\n",
      "epsilon:0.178410 episode_count: 3409. steps_count: 1501472.000000\n",
      "Time elapsed:  4255.609129667282\n",
      "ep 487: ep_len:600 episode reward: total was -64.740000. running mean: -59.866724\n",
      "ep 487: ep_len:558 episode reward: total was -79.470000. running mean: -60.062757\n",
      "ep 487: ep_len:620 episode reward: total was -54.110000. running mean: -60.003229\n",
      "ep 487: ep_len:517 episode reward: total was -87.860000. running mean: -60.281797\n",
      "ep 487: ep_len:3 episode reward: total was 0.000000. running mean: -59.678979\n",
      "ep 487: ep_len:566 episode reward: total was -45.710000. running mean: -59.539289\n",
      "ep 487: ep_len:627 episode reward: total was -65.820000. running mean: -59.602096\n",
      "epsilon:0.178365 episode_count: 3416. steps_count: 1504963.000000\n",
      "Time elapsed:  4264.831829071045\n",
      "ep 488: ep_len:591 episode reward: total was -128.080000. running mean: -60.286875\n",
      "ep 488: ep_len:500 episode reward: total was -99.730000. running mean: -60.681307\n",
      "ep 488: ep_len:534 episode reward: total was -153.300000. running mean: -61.607494\n",
      "ep 488: ep_len:500 episode reward: total was -67.360000. running mean: -61.665019\n",
      "ep 488: ep_len:102 episode reward: total was -4.280000. running mean: -61.091168\n",
      "ep 488: ep_len:636 episode reward: total was -131.080000. running mean: -61.791057\n",
      "ep 488: ep_len:585 episode reward: total was -77.250000. running mean: -61.945646\n",
      "epsilon:0.178321 episode_count: 3423. steps_count: 1508411.000000\n",
      "Time elapsed:  4274.503914356232\n",
      "ep 489: ep_len:212 episode reward: total was 0.490000. running mean: -61.321290\n",
      "ep 489: ep_len:518 episode reward: total was -113.260000. running mean: -61.840677\n",
      "ep 489: ep_len:534 episode reward: total was -61.070000. running mean: -61.832970\n",
      "ep 489: ep_len:500 episode reward: total was -86.870000. running mean: -62.083340\n",
      "ep 489: ep_len:3 episode reward: total was 0.000000. running mean: -61.462507\n",
      "ep 489: ep_len:518 episode reward: total was -114.720000. running mean: -61.995082\n",
      "ep 489: ep_len:500 episode reward: total was -91.720000. running mean: -62.292331\n",
      "epsilon:0.178277 episode_count: 3430. steps_count: 1511196.000000\n",
      "Time elapsed:  4282.346565961838\n",
      "ep 490: ep_len:134 episode reward: total was -22.140000. running mean: -61.890808\n",
      "ep 490: ep_len:500 episode reward: total was -33.440000. running mean: -61.606300\n",
      "ep 490: ep_len:522 episode reward: total was -131.940000. running mean: -62.309637\n",
      "ep 490: ep_len:132 episode reward: total was -2.010000. running mean: -61.706640\n",
      "ep 490: ep_len:3 episode reward: total was -1.500000. running mean: -61.104574\n",
      "ep 490: ep_len:304 episode reward: total was -30.250000. running mean: -60.796028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 490: ep_len:562 episode reward: total was -131.450000. running mean: -61.502568\n",
      "epsilon:0.178232 episode_count: 3437. steps_count: 1513353.000000\n",
      "Time elapsed:  4288.413744926453\n",
      "ep 491: ep_len:626 episode reward: total was -69.020000. running mean: -61.577742\n",
      "ep 491: ep_len:500 episode reward: total was -34.400000. running mean: -61.305965\n",
      "ep 491: ep_len:513 episode reward: total was -83.490000. running mean: -61.527805\n",
      "ep 491: ep_len:501 episode reward: total was -91.050000. running mean: -61.823027\n",
      "ep 491: ep_len:3 episode reward: total was -1.500000. running mean: -61.219797\n",
      "ep 491: ep_len:566 episode reward: total was -29.050000. running mean: -60.898099\n",
      "ep 491: ep_len:613 episode reward: total was -85.990000. running mean: -61.149018\n",
      "epsilon:0.178188 episode_count: 3444. steps_count: 1516675.000000\n",
      "Time elapsed:  4296.925763368607\n",
      "ep 492: ep_len:607 episode reward: total was -125.380000. running mean: -61.791328\n",
      "ep 492: ep_len:633 episode reward: total was -15.810000. running mean: -61.331514\n",
      "ep 492: ep_len:502 episode reward: total was -94.850000. running mean: -61.666699\n",
      "ep 492: ep_len:508 episode reward: total was -59.290000. running mean: -61.642932\n",
      "ep 492: ep_len:72 episode reward: total was 2.180000. running mean: -61.004703\n",
      "ep 492: ep_len:506 episode reward: total was -163.070000. running mean: -62.025356\n",
      "ep 492: ep_len:605 episode reward: total was -68.400000. running mean: -62.089102\n",
      "epsilon:0.178144 episode_count: 3451. steps_count: 1520108.000000\n",
      "Time elapsed:  4305.912974357605\n",
      "ep 493: ep_len:571 episode reward: total was -69.770000. running mean: -62.165911\n",
      "ep 493: ep_len:500 episode reward: total was -56.670000. running mean: -62.110952\n",
      "ep 493: ep_len:540 episode reward: total was -130.650000. running mean: -62.796343\n",
      "ep 493: ep_len:586 episode reward: total was -16.300000. running mean: -62.331379\n",
      "ep 493: ep_len:3 episode reward: total was -1.500000. running mean: -61.723066\n",
      "ep 493: ep_len:638 episode reward: total was -67.340000. running mean: -61.779235\n",
      "ep 493: ep_len:548 episode reward: total was -157.340000. running mean: -62.734842\n",
      "epsilon:0.178099 episode_count: 3458. steps_count: 1523494.000000\n",
      "Time elapsed:  4314.808148145676\n",
      "ep 494: ep_len:230 episode reward: total was -25.370000. running mean: -62.361194\n",
      "ep 494: ep_len:585 episode reward: total was -107.610000. running mean: -62.813682\n",
      "ep 494: ep_len:518 episode reward: total was -77.690000. running mean: -62.962445\n",
      "ep 494: ep_len:502 episode reward: total was -70.530000. running mean: -63.038121\n",
      "ep 494: ep_len:3 episode reward: total was 0.000000. running mean: -62.407740\n",
      "ep 494: ep_len:500 episode reward: total was -73.320000. running mean: -62.516862\n",
      "ep 494: ep_len:509 episode reward: total was -81.600000. running mean: -62.707694\n",
      "epsilon:0.178055 episode_count: 3465. steps_count: 1526341.000000\n",
      "Time elapsed:  4322.56049489975\n",
      "ep 495: ep_len:651 episode reward: total was -130.020000. running mean: -63.380817\n",
      "ep 495: ep_len:197 episode reward: total was -36.890000. running mean: -63.115909\n",
      "ep 495: ep_len:648 episode reward: total was -88.810000. running mean: -63.372849\n",
      "ep 495: ep_len:500 episode reward: total was -90.140000. running mean: -63.640521\n",
      "ep 495: ep_len:97 episode reward: total was -2.400000. running mean: -63.028116\n",
      "ep 495: ep_len:514 episode reward: total was -94.280000. running mean: -63.340635\n",
      "ep 495: ep_len:500 episode reward: total was -31.150000. running mean: -63.018728\n",
      "epsilon:0.178011 episode_count: 3472. steps_count: 1529448.000000\n",
      "Time elapsed:  4330.813519716263\n",
      "ep 496: ep_len:524 episode reward: total was -170.060000. running mean: -64.089141\n",
      "ep 496: ep_len:352 episode reward: total was -50.760000. running mean: -63.955850\n",
      "ep 496: ep_len:735 episode reward: total was -146.760000. running mean: -64.783891\n",
      "ep 496: ep_len:50 episode reward: total was -14.730000. running mean: -64.283352\n",
      "ep 496: ep_len:125 episode reward: total was -37.680000. running mean: -64.017319\n",
      "ep 496: ep_len:500 episode reward: total was -46.860000. running mean: -63.845745\n",
      "ep 496: ep_len:524 episode reward: total was -54.420000. running mean: -63.751488\n",
      "epsilon:0.177966 episode_count: 3479. steps_count: 1532258.000000\n",
      "Time elapsed:  4338.639755487442\n",
      "ep 497: ep_len:670 episode reward: total was -88.130000. running mean: -63.995273\n",
      "ep 497: ep_len:517 episode reward: total was -30.390000. running mean: -63.659220\n",
      "ep 497: ep_len:626 episode reward: total was -70.080000. running mean: -63.723428\n",
      "ep 497: ep_len:508 episode reward: total was -40.010000. running mean: -63.486294\n",
      "ep 497: ep_len:33 episode reward: total was -3.490000. running mean: -62.886331\n",
      "ep 497: ep_len:500 episode reward: total was -47.670000. running mean: -62.734168\n",
      "ep 497: ep_len:307 episode reward: total was -54.520000. running mean: -62.652026\n",
      "epsilon:0.177922 episode_count: 3486. steps_count: 1535419.000000\n",
      "Time elapsed:  4347.079658746719\n",
      "ep 498: ep_len:500 episode reward: total was 0.010000. running mean: -62.025406\n",
      "ep 498: ep_len:512 episode reward: total was -102.420000. running mean: -62.429352\n",
      "ep 498: ep_len:628 episode reward: total was -204.370000. running mean: -63.848758\n",
      "ep 498: ep_len:53 episode reward: total was -4.200000. running mean: -63.252271\n",
      "ep 498: ep_len:102 episode reward: total was -37.740000. running mean: -62.997148\n",
      "ep 498: ep_len:500 episode reward: total was -26.610000. running mean: -62.633276\n",
      "ep 498: ep_len:500 episode reward: total was -63.930000. running mean: -62.646244\n",
      "epsilon:0.177878 episode_count: 3493. steps_count: 1538214.000000\n",
      "Time elapsed:  4354.431915283203\n",
      "ep 499: ep_len:636 episode reward: total was -74.980000. running mean: -62.769581\n",
      "ep 499: ep_len:500 episode reward: total was -86.850000. running mean: -63.010385\n",
      "ep 499: ep_len:606 episode reward: total was -127.450000. running mean: -63.654781\n",
      "ep 499: ep_len:502 episode reward: total was -100.780000. running mean: -64.026034\n",
      "ep 499: ep_len:3 episode reward: total was 0.000000. running mean: -63.385773\n",
      "ep 499: ep_len:609 episode reward: total was -68.930000. running mean: -63.441216\n",
      "ep 499: ep_len:621 episode reward: total was -65.700000. running mean: -63.463803\n",
      "epsilon:0.177833 episode_count: 3500. steps_count: 1541691.000000\n",
      "Time elapsed:  4363.400288581848\n",
      "ep 500: ep_len:561 episode reward: total was -63.320000. running mean: -63.462365\n",
      "ep 500: ep_len:671 episode reward: total was -132.070000. running mean: -64.148442\n",
      "ep 500: ep_len:451 episode reward: total was -30.400000. running mean: -63.810957\n",
      "ep 500: ep_len:511 episode reward: total was -80.670000. running mean: -63.979548\n",
      "ep 500: ep_len:78 episode reward: total was -41.440000. running mean: -63.754152\n",
      "ep 500: ep_len:500 episode reward: total was -59.000000. running mean: -63.706611\n",
      "ep 500: ep_len:195 episode reward: total was -34.800000. running mean: -63.417545\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.177789 episode_count: 3507. steps_count: 1544658.000000\n",
      "Time elapsed:  4376.339445352554\n",
      "ep 501: ep_len:184 episode reward: total was -4.570000. running mean: -62.829069\n",
      "ep 501: ep_len:515 episode reward: total was -22.450000. running mean: -62.425279\n",
      "ep 501: ep_len:562 episode reward: total was -87.990000. running mean: -62.680926\n",
      "ep 501: ep_len:500 episode reward: total was -74.230000. running mean: -62.796416\n",
      "ep 501: ep_len:102 episode reward: total was -2.290000. running mean: -62.191352\n",
      "ep 501: ep_len:520 episode reward: total was -108.180000. running mean: -62.651239\n",
      "ep 501: ep_len:594 episode reward: total was -125.740000. running mean: -63.282126\n",
      "epsilon:0.177745 episode_count: 3514. steps_count: 1547635.000000\n",
      "Time elapsed:  4384.704277515411\n",
      "ep 502: ep_len:584 episode reward: total was -116.550000. running mean: -63.814805\n",
      "ep 502: ep_len:580 episode reward: total was -156.130000. running mean: -64.737957\n",
      "ep 502: ep_len:548 episode reward: total was -99.220000. running mean: -65.082778\n",
      "ep 502: ep_len:505 episode reward: total was -82.530000. running mean: -65.257250\n",
      "ep 502: ep_len:3 episode reward: total was -1.500000. running mean: -64.619677\n",
      "ep 502: ep_len:544 episode reward: total was -105.270000. running mean: -65.026180\n",
      "ep 502: ep_len:586 episode reward: total was -99.150000. running mean: -65.367419\n",
      "epsilon:0.177700 episode_count: 3521. steps_count: 1550985.000000\n",
      "Time elapsed:  4393.586030006409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 503: ep_len:503 episode reward: total was -49.510000. running mean: -65.208844\n",
      "ep 503: ep_len:185 episode reward: total was -33.890000. running mean: -64.895656\n",
      "ep 503: ep_len:577 episode reward: total was -127.810000. running mean: -65.524799\n",
      "ep 503: ep_len:615 episode reward: total was -5.250000. running mean: -64.922051\n",
      "ep 503: ep_len:3 episode reward: total was 0.000000. running mean: -64.272831\n",
      "ep 503: ep_len:651 episode reward: total was -120.960000. running mean: -64.839703\n",
      "ep 503: ep_len:595 episode reward: total was -58.340000. running mean: -64.774706\n",
      "epsilon:0.177656 episode_count: 3528. steps_count: 1554114.000000\n",
      "Time elapsed:  4401.896535396576\n",
      "ep 504: ep_len:652 episode reward: total was -210.470000. running mean: -66.231659\n",
      "ep 504: ep_len:529 episode reward: total was -125.310000. running mean: -66.822442\n",
      "ep 504: ep_len:590 episode reward: total was -83.650000. running mean: -66.990718\n",
      "ep 504: ep_len:146 episode reward: total was -18.960000. running mean: -66.510410\n",
      "ep 504: ep_len:3 episode reward: total was 0.000000. running mean: -65.845306\n",
      "ep 504: ep_len:541 episode reward: total was -148.580000. running mean: -66.672653\n",
      "ep 504: ep_len:179 episode reward: total was -32.670000. running mean: -66.332627\n",
      "epsilon:0.177612 episode_count: 3535. steps_count: 1556754.000000\n",
      "Time elapsed:  4409.282939910889\n",
      "ep 505: ep_len:500 episode reward: total was -12.550000. running mean: -65.794800\n",
      "ep 505: ep_len:555 episode reward: total was -58.070000. running mean: -65.717552\n",
      "ep 505: ep_len:691 episode reward: total was -64.500000. running mean: -65.705377\n",
      "ep 505: ep_len:506 episode reward: total was -76.860000. running mean: -65.816923\n",
      "ep 505: ep_len:3 episode reward: total was 0.000000. running mean: -65.158754\n",
      "ep 505: ep_len:577 episode reward: total was -81.890000. running mean: -65.326066\n",
      "ep 505: ep_len:501 episode reward: total was -58.690000. running mean: -65.259706\n",
      "epsilon:0.177567 episode_count: 3542. steps_count: 1560087.000000\n",
      "Time elapsed:  4417.842979192734\n",
      "ep 506: ep_len:567 episode reward: total was -96.970000. running mean: -65.576809\n",
      "ep 506: ep_len:529 episode reward: total was -21.170000. running mean: -65.132741\n",
      "ep 506: ep_len:435 episode reward: total was -37.660000. running mean: -64.858013\n",
      "ep 506: ep_len:500 episode reward: total was -19.370000. running mean: -64.403133\n",
      "ep 506: ep_len:87 episode reward: total was 3.740000. running mean: -63.721702\n",
      "ep 506: ep_len:529 episode reward: total was -46.540000. running mean: -63.549885\n",
      "ep 506: ep_len:500 episode reward: total was -55.180000. running mean: -63.466186\n",
      "epsilon:0.177523 episode_count: 3549. steps_count: 1563234.000000\n",
      "Time elapsed:  4426.2851305007935\n",
      "ep 507: ep_len:631 episode reward: total was -149.780000. running mean: -64.329324\n",
      "ep 507: ep_len:533 episode reward: total was -49.580000. running mean: -64.181831\n",
      "ep 507: ep_len:539 episode reward: total was -104.660000. running mean: -64.586612\n",
      "ep 507: ep_len:500 episode reward: total was -45.520000. running mean: -64.395946\n",
      "ep 507: ep_len:50 episode reward: total was 2.500000. running mean: -63.726987\n",
      "ep 507: ep_len:646 episode reward: total was -97.520000. running mean: -64.064917\n",
      "ep 507: ep_len:525 episode reward: total was -66.960000. running mean: -64.093868\n",
      "epsilon:0.177479 episode_count: 3556. steps_count: 1566658.000000\n",
      "Time elapsed:  4435.009348154068\n",
      "ep 508: ep_len:569 episode reward: total was -88.500000. running mean: -64.337929\n",
      "ep 508: ep_len:500 episode reward: total was -4.990000. running mean: -63.744450\n",
      "ep 508: ep_len:575 episode reward: total was -78.250000. running mean: -63.889505\n",
      "ep 508: ep_len:574 episode reward: total was -0.670000. running mean: -63.257310\n",
      "ep 508: ep_len:3 episode reward: total was 0.000000. running mean: -62.624737\n",
      "ep 508: ep_len:500 episode reward: total was -84.700000. running mean: -62.845490\n",
      "ep 508: ep_len:527 episode reward: total was -68.630000. running mean: -62.903335\n",
      "epsilon:0.177434 episode_count: 3563. steps_count: 1569906.000000\n",
      "Time elapsed:  4443.555036067963\n",
      "ep 509: ep_len:594 episode reward: total was -104.020000. running mean: -63.314502\n",
      "ep 509: ep_len:501 episode reward: total was -42.830000. running mean: -63.109657\n",
      "ep 509: ep_len:646 episode reward: total was -159.260000. running mean: -64.071160\n",
      "ep 509: ep_len:516 episode reward: total was -54.820000. running mean: -63.978648\n",
      "ep 509: ep_len:123 episode reward: total was 6.830000. running mean: -63.270562\n",
      "ep 509: ep_len:624 episode reward: total was -77.550000. running mean: -63.413356\n",
      "ep 509: ep_len:633 episode reward: total was -61.780000. running mean: -63.397023\n",
      "epsilon:0.177390 episode_count: 3570. steps_count: 1573543.000000\n",
      "Time elapsed:  4453.063481807709\n",
      "ep 510: ep_len:521 episode reward: total was -69.410000. running mean: -63.457152\n",
      "ep 510: ep_len:500 episode reward: total was -77.340000. running mean: -63.595981\n",
      "ep 510: ep_len:594 episode reward: total was -50.710000. running mean: -63.467121\n",
      "ep 510: ep_len:543 episode reward: total was -40.360000. running mean: -63.236050\n",
      "ep 510: ep_len:3 episode reward: total was 1.010000. running mean: -62.593589\n",
      "ep 510: ep_len:261 episode reward: total was 2.790000. running mean: -61.939754\n",
      "ep 510: ep_len:307 episode reward: total was -57.500000. running mean: -61.895356\n",
      "epsilon:0.177346 episode_count: 3577. steps_count: 1576272.000000\n",
      "Time elapsed:  4460.4883868694305\n",
      "ep 511: ep_len:525 episode reward: total was -70.840000. running mean: -61.984802\n",
      "ep 511: ep_len:185 episode reward: total was -41.350000. running mean: -61.778454\n",
      "ep 511: ep_len:451 episode reward: total was -12.430000. running mean: -61.284970\n",
      "ep 511: ep_len:500 episode reward: total was -60.340000. running mean: -61.275520\n",
      "ep 511: ep_len:3 episode reward: total was 0.000000. running mean: -60.662765\n",
      "ep 511: ep_len:535 episode reward: total was -155.150000. running mean: -61.607637\n",
      "ep 511: ep_len:292 episode reward: total was -95.290000. running mean: -61.944461\n",
      "epsilon:0.177301 episode_count: 3584. steps_count: 1578763.000000\n",
      "Time elapsed:  4467.606222629547\n",
      "ep 512: ep_len:501 episode reward: total was -33.130000. running mean: -61.656316\n",
      "ep 512: ep_len:504 episode reward: total was -66.340000. running mean: -61.703153\n",
      "ep 512: ep_len:566 episode reward: total was -88.400000. running mean: -61.970122\n",
      "ep 512: ep_len:500 episode reward: total was -49.950000. running mean: -61.849920\n",
      "ep 512: ep_len:3 episode reward: total was -1.500000. running mean: -61.246421\n",
      "ep 512: ep_len:500 episode reward: total was -91.140000. running mean: -61.545357\n",
      "ep 512: ep_len:500 episode reward: total was -61.130000. running mean: -61.541203\n",
      "epsilon:0.177257 episode_count: 3591. steps_count: 1581837.000000\n",
      "Time elapsed:  4477.827979803085\n",
      "ep 513: ep_len:513 episode reward: total was -65.590000. running mean: -61.581691\n",
      "ep 513: ep_len:193 episode reward: total was -13.270000. running mean: -61.098574\n",
      "ep 513: ep_len:660 episode reward: total was -233.220000. running mean: -62.819789\n",
      "ep 513: ep_len:163 episode reward: total was -20.370000. running mean: -62.395291\n",
      "ep 513: ep_len:107 episode reward: total was -21.250000. running mean: -61.983838\n",
      "ep 513: ep_len:618 episode reward: total was -106.280000. running mean: -62.426800\n",
      "ep 513: ep_len:556 episode reward: total was -103.430000. running mean: -62.836832\n",
      "epsilon:0.177213 episode_count: 3598. steps_count: 1584647.000000\n",
      "Time elapsed:  4486.436659574509\n",
      "ep 514: ep_len:226 episode reward: total was -13.760000. running mean: -62.346063\n",
      "ep 514: ep_len:500 episode reward: total was -56.260000. running mean: -62.285203\n",
      "ep 514: ep_len:550 episode reward: total was -78.220000. running mean: -62.444551\n",
      "ep 514: ep_len:51 episode reward: total was -15.680000. running mean: -61.976905\n",
      "ep 514: ep_len:3 episode reward: total was 0.000000. running mean: -61.357136\n",
      "ep 514: ep_len:580 episode reward: total was -105.790000. running mean: -61.801465\n",
      "ep 514: ep_len:358 episode reward: total was -88.380000. running mean: -62.067250\n",
      "epsilon:0.177168 episode_count: 3605. steps_count: 1586915.000000\n",
      "Time elapsed:  4493.636964082718\n",
      "ep 515: ep_len:134 episode reward: total was -7.110000. running mean: -61.517678\n",
      "ep 515: ep_len:565 episode reward: total was -34.890000. running mean: -61.251401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 515: ep_len:562 episode reward: total was -134.550000. running mean: -61.984387\n",
      "ep 515: ep_len:546 episode reward: total was -67.450000. running mean: -62.039043\n",
      "ep 515: ep_len:3 episode reward: total was -1.500000. running mean: -61.433652\n",
      "ep 515: ep_len:513 episode reward: total was -83.580000. running mean: -61.655116\n",
      "ep 515: ep_len:589 episode reward: total was -45.730000. running mean: -61.495865\n",
      "epsilon:0.177124 episode_count: 3612. steps_count: 1589827.000000\n",
      "Time elapsed:  4503.853528022766\n",
      "ep 516: ep_len:600 episode reward: total was -81.610000. running mean: -61.697006\n",
      "ep 516: ep_len:528 episode reward: total was -105.530000. running mean: -62.135336\n",
      "ep 516: ep_len:79 episode reward: total was -0.200000. running mean: -61.515983\n",
      "ep 516: ep_len:503 episode reward: total was -85.310000. running mean: -61.753923\n",
      "ep 516: ep_len:108 episode reward: total was 19.230000. running mean: -60.944084\n",
      "ep 516: ep_len:505 episode reward: total was -115.530000. running mean: -61.489943\n",
      "ep 516: ep_len:616 episode reward: total was -96.510000. running mean: -61.840143\n",
      "epsilon:0.177080 episode_count: 3619. steps_count: 1592766.000000\n",
      "Time elapsed:  4513.6681599617\n",
      "ep 517: ep_len:109 episode reward: total was -7.180000. running mean: -61.293542\n",
      "ep 517: ep_len:500 episode reward: total was -72.210000. running mean: -61.402707\n",
      "ep 517: ep_len:682 episode reward: total was -119.500000. running mean: -61.983679\n",
      "ep 517: ep_len:583 episode reward: total was -28.020000. running mean: -61.644043\n",
      "ep 517: ep_len:3 episode reward: total was 0.000000. running mean: -61.027602\n",
      "ep 517: ep_len:537 episode reward: total was -84.450000. running mean: -61.261826\n",
      "ep 517: ep_len:543 episode reward: total was -118.280000. running mean: -61.832008\n",
      "epsilon:0.177035 episode_count: 3626. steps_count: 1595723.000000\n",
      "Time elapsed:  4523.177780866623\n",
      "ep 518: ep_len:621 episode reward: total was -43.240000. running mean: -61.646088\n",
      "ep 518: ep_len:523 episode reward: total was -80.840000. running mean: -61.838027\n",
      "ep 518: ep_len:79 episode reward: total was -0.660000. running mean: -61.226247\n",
      "ep 518: ep_len:501 episode reward: total was -91.360000. running mean: -61.527584\n",
      "ep 518: ep_len:73 episode reward: total was 8.590000. running mean: -60.826408\n",
      "ep 518: ep_len:585 episode reward: total was -81.380000. running mean: -61.031944\n",
      "ep 518: ep_len:337 episode reward: total was -53.120000. running mean: -60.952825\n",
      "epsilon:0.176991 episode_count: 3633. steps_count: 1598442.000000\n",
      "Time elapsed:  4532.146684408188\n",
      "ep 519: ep_len:500 episode reward: total was -34.430000. running mean: -60.687597\n",
      "ep 519: ep_len:500 episode reward: total was -146.990000. running mean: -61.550621\n",
      "ep 519: ep_len:75 episode reward: total was 2.760000. running mean: -60.907514\n",
      "ep 519: ep_len:595 episode reward: total was -29.560000. running mean: -60.594039\n",
      "ep 519: ep_len:3 episode reward: total was 0.000000. running mean: -59.988099\n",
      "ep 519: ep_len:720 episode reward: total was -90.200000. running mean: -60.290218\n",
      "ep 519: ep_len:500 episode reward: total was -77.360000. running mean: -60.460916\n",
      "epsilon:0.176947 episode_count: 3640. steps_count: 1601335.000000\n",
      "Time elapsed:  4540.987104654312\n",
      "ep 520: ep_len:573 episode reward: total was -46.120000. running mean: -60.317507\n",
      "ep 520: ep_len:501 episode reward: total was -89.000000. running mean: -60.604332\n",
      "ep 520: ep_len:554 episode reward: total was -114.820000. running mean: -61.146488\n",
      "ep 520: ep_len:531 episode reward: total was -22.890000. running mean: -60.763923\n",
      "ep 520: ep_len:120 episode reward: total was -9.700000. running mean: -60.253284\n",
      "ep 520: ep_len:509 episode reward: total was -70.440000. running mean: -60.355151\n",
      "ep 520: ep_len:572 episode reward: total was -58.710000. running mean: -60.338700\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.176902 episode_count: 3647. steps_count: 1604695.000000\n",
      "Time elapsed:  4556.095819473267\n",
      "ep 521: ep_len:500 episode reward: total was -132.420000. running mean: -61.059513\n",
      "ep 521: ep_len:501 episode reward: total was -31.970000. running mean: -60.768618\n",
      "ep 521: ep_len:571 episode reward: total was -42.160000. running mean: -60.582531\n",
      "ep 521: ep_len:500 episode reward: total was -76.960000. running mean: -60.746306\n",
      "ep 521: ep_len:3 episode reward: total was -1.500000. running mean: -60.153843\n",
      "ep 521: ep_len:501 episode reward: total was -58.070000. running mean: -60.133005\n",
      "ep 521: ep_len:538 episode reward: total was -80.560000. running mean: -60.337275\n",
      "epsilon:0.176858 episode_count: 3654. steps_count: 1607809.000000\n",
      "Time elapsed:  4565.670308351517\n",
      "ep 522: ep_len:549 episode reward: total was -263.920000. running mean: -62.373102\n",
      "ep 522: ep_len:549 episode reward: total was -148.810000. running mean: -63.237471\n",
      "ep 522: ep_len:543 episode reward: total was -79.630000. running mean: -63.401396\n",
      "ep 522: ep_len:143 episode reward: total was -7.470000. running mean: -62.842082\n",
      "ep 522: ep_len:3 episode reward: total was 0.000000. running mean: -62.213661\n",
      "ep 522: ep_len:517 episode reward: total was -86.220000. running mean: -62.453725\n",
      "ep 522: ep_len:295 episode reward: total was -87.040000. running mean: -62.699587\n",
      "epsilon:0.176814 episode_count: 3661. steps_count: 1610408.000000\n",
      "Time elapsed:  4573.221483230591\n",
      "ep 523: ep_len:650 episode reward: total was -123.250000. running mean: -63.305092\n",
      "ep 523: ep_len:500 episode reward: total was -77.720000. running mean: -63.449241\n",
      "ep 523: ep_len:589 episode reward: total was -154.540000. running mean: -64.360148\n",
      "ep 523: ep_len:584 episode reward: total was -60.120000. running mean: -64.317747\n",
      "ep 523: ep_len:36 episode reward: total was -6.000000. running mean: -63.734569\n",
      "ep 523: ep_len:584 episode reward: total was -54.460000. running mean: -63.641824\n",
      "ep 523: ep_len:627 episode reward: total was -92.180000. running mean: -63.927205\n",
      "epsilon:0.176769 episode_count: 3668. steps_count: 1613978.000000\n",
      "Time elapsed:  4584.393600702286\n",
      "ep 524: ep_len:560 episode reward: total was -19.110000. running mean: -63.479033\n",
      "ep 524: ep_len:500 episode reward: total was 4.450000. running mean: -62.799743\n",
      "ep 524: ep_len:516 episode reward: total was -59.010000. running mean: -62.761846\n",
      "ep 524: ep_len:500 episode reward: total was -86.250000. running mean: -62.996727\n",
      "ep 524: ep_len:3 episode reward: total was 0.000000. running mean: -62.366760\n",
      "ep 524: ep_len:293 episode reward: total was -7.320000. running mean: -61.816292\n",
      "ep 524: ep_len:581 episode reward: total was -45.400000. running mean: -61.652129\n",
      "epsilon:0.176725 episode_count: 3675. steps_count: 1616931.000000\n",
      "Time elapsed:  4593.410710334778\n",
      "ep 525: ep_len:576 episode reward: total was -91.800000. running mean: -61.953608\n",
      "ep 525: ep_len:593 episode reward: total was -50.730000. running mean: -61.841372\n",
      "ep 525: ep_len:542 episode reward: total was -51.460000. running mean: -61.737558\n",
      "ep 525: ep_len:583 episode reward: total was -20.600000. running mean: -61.326183\n",
      "ep 525: ep_len:3 episode reward: total was 0.000000. running mean: -60.712921\n",
      "ep 525: ep_len:510 episode reward: total was -42.230000. running mean: -60.528092\n",
      "ep 525: ep_len:504 episode reward: total was -121.990000. running mean: -61.142711\n",
      "epsilon:0.176681 episode_count: 3682. steps_count: 1620242.000000\n",
      "Time elapsed:  4603.810728311539\n",
      "ep 526: ep_len:599 episode reward: total was -94.730000. running mean: -61.478584\n",
      "ep 526: ep_len:549 episode reward: total was -115.280000. running mean: -62.016598\n",
      "ep 526: ep_len:550 episode reward: total was -82.660000. running mean: -62.223032\n",
      "ep 526: ep_len:527 episode reward: total was -25.960000. running mean: -61.860401\n",
      "ep 526: ep_len:3 episode reward: total was -3.000000. running mean: -61.271797\n",
      "ep 526: ep_len:500 episode reward: total was -70.890000. running mean: -61.367979\n",
      "ep 526: ep_len:211 episode reward: total was -23.530000. running mean: -60.989600\n",
      "epsilon:0.176636 episode_count: 3689. steps_count: 1623181.000000\n",
      "Time elapsed:  4612.805818319321\n",
      "ep 527: ep_len:212 episode reward: total was -8.780000. running mean: -60.467504\n",
      "ep 527: ep_len:500 episode reward: total was -59.840000. running mean: -60.461229\n",
      "ep 527: ep_len:704 episode reward: total was -139.700000. running mean: -61.253616\n",
      "ep 527: ep_len:510 episode reward: total was -86.700000. running mean: -61.508080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 527: ep_len:77 episode reward: total was -0.800000. running mean: -60.900999\n",
      "ep 527: ep_len:150 episode reward: total was -11.710000. running mean: -60.409089\n",
      "ep 527: ep_len:194 episode reward: total was -35.300000. running mean: -60.157999\n",
      "epsilon:0.176592 episode_count: 3696. steps_count: 1625528.000000\n",
      "Time elapsed:  4620.281230688095\n",
      "ep 528: ep_len:500 episode reward: total was -36.340000. running mean: -59.919819\n",
      "ep 528: ep_len:275 episode reward: total was -164.650000. running mean: -60.967120\n",
      "ep 528: ep_len:569 episode reward: total was -44.610000. running mean: -60.803549\n",
      "ep 528: ep_len:119 episode reward: total was -2.600000. running mean: -60.221514\n",
      "ep 528: ep_len:94 episode reward: total was 9.750000. running mean: -59.521799\n",
      "ep 528: ep_len:604 episode reward: total was -136.260000. running mean: -60.289181\n",
      "ep 528: ep_len:295 episode reward: total was -57.500000. running mean: -60.261289\n",
      "epsilon:0.176548 episode_count: 3703. steps_count: 1627984.000000\n",
      "Time elapsed:  4628.818112850189\n",
      "ep 529: ep_len:249 episode reward: total was -28.940000. running mean: -59.948076\n",
      "ep 529: ep_len:533 episode reward: total was -73.700000. running mean: -60.085595\n",
      "ep 529: ep_len:627 episode reward: total was -127.780000. running mean: -60.762539\n",
      "ep 529: ep_len:523 episode reward: total was -119.350000. running mean: -61.348414\n",
      "ep 529: ep_len:3 episode reward: total was -1.500000. running mean: -60.749930\n",
      "ep 529: ep_len:574 episode reward: total was -82.370000. running mean: -60.966130\n",
      "ep 529: ep_len:553 episode reward: total was -95.140000. running mean: -61.307869\n",
      "epsilon:0.176503 episode_count: 3710. steps_count: 1631046.000000\n",
      "Time elapsed:  4639.002273797989\n",
      "ep 530: ep_len:530 episode reward: total was -55.990000. running mean: -61.254690\n",
      "ep 530: ep_len:536 episode reward: total was -60.020000. running mean: -61.242343\n",
      "ep 530: ep_len:500 episode reward: total was -35.600000. running mean: -60.985920\n",
      "ep 530: ep_len:500 episode reward: total was -6.580000. running mean: -60.441861\n",
      "ep 530: ep_len:94 episode reward: total was 10.270000. running mean: -59.734742\n",
      "ep 530: ep_len:512 episode reward: total was -63.290000. running mean: -59.770295\n",
      "ep 530: ep_len:500 episode reward: total was -39.310000. running mean: -59.565692\n",
      "epsilon:0.176459 episode_count: 3717. steps_count: 1634218.000000\n",
      "Time elapsed:  4648.717576503754\n",
      "ep 531: ep_len:581 episode reward: total was -9.560000. running mean: -59.065635\n",
      "ep 531: ep_len:574 episode reward: total was -48.940000. running mean: -58.964379\n",
      "ep 531: ep_len:500 episode reward: total was -81.760000. running mean: -59.192335\n",
      "ep 531: ep_len:132 episode reward: total was -1.430000. running mean: -58.614711\n",
      "ep 531: ep_len:42 episode reward: total was 13.500000. running mean: -57.893564\n",
      "ep 531: ep_len:500 episode reward: total was -141.940000. running mean: -58.734029\n",
      "ep 531: ep_len:599 episode reward: total was -75.250000. running mean: -58.899188\n",
      "epsilon:0.176415 episode_count: 3724. steps_count: 1637146.000000\n",
      "Time elapsed:  4657.770889759064\n",
      "ep 532: ep_len:500 episode reward: total was -30.870000. running mean: -58.618896\n",
      "ep 532: ep_len:580 episode reward: total was -20.460000. running mean: -58.237308\n",
      "ep 532: ep_len:576 episode reward: total was -38.020000. running mean: -58.035134\n",
      "ep 532: ep_len:382 episode reward: total was -37.080000. running mean: -57.825583\n",
      "ep 532: ep_len:3 episode reward: total was 1.010000. running mean: -57.237227\n",
      "ep 532: ep_len:186 episode reward: total was -2.480000. running mean: -56.689655\n",
      "ep 532: ep_len:549 episode reward: total was -79.340000. running mean: -56.916158\n",
      "epsilon:0.176370 episode_count: 3731. steps_count: 1639922.000000\n",
      "Time elapsed:  4666.210262060165\n",
      "ep 533: ep_len:606 episode reward: total was -115.120000. running mean: -57.498197\n",
      "ep 533: ep_len:600 episode reward: total was -112.230000. running mean: -58.045515\n",
      "ep 533: ep_len:583 episode reward: total was -112.160000. running mean: -58.586660\n",
      "ep 533: ep_len:501 episode reward: total was -9.790000. running mean: -58.098693\n",
      "ep 533: ep_len:3 episode reward: total was -1.500000. running mean: -57.532706\n",
      "ep 533: ep_len:167 episode reward: total was -16.620000. running mean: -57.123579\n",
      "ep 533: ep_len:606 episode reward: total was -54.130000. running mean: -57.093643\n",
      "epsilon:0.176326 episode_count: 3738. steps_count: 1642988.000000\n",
      "Time elapsed:  4675.536652565002\n",
      "ep 534: ep_len:555 episode reward: total was -53.410000. running mean: -57.056807\n",
      "ep 534: ep_len:551 episode reward: total was -93.380000. running mean: -57.420039\n",
      "ep 534: ep_len:674 episode reward: total was -89.500000. running mean: -57.740838\n",
      "ep 534: ep_len:170 episode reward: total was -13.900000. running mean: -57.302430\n",
      "ep 534: ep_len:3 episode reward: total was 0.000000. running mean: -56.729406\n",
      "ep 534: ep_len:691 episode reward: total was -54.040000. running mean: -56.702512\n",
      "ep 534: ep_len:500 episode reward: total was -34.640000. running mean: -56.481887\n",
      "epsilon:0.176282 episode_count: 3745. steps_count: 1646132.000000\n",
      "Time elapsed:  4685.513969421387\n",
      "ep 535: ep_len:214 episode reward: total was -15.840000. running mean: -56.075468\n",
      "ep 535: ep_len:554 episode reward: total was -195.850000. running mean: -57.473213\n",
      "ep 535: ep_len:608 episode reward: total was -104.000000. running mean: -57.938481\n",
      "ep 535: ep_len:500 episode reward: total was -10.560000. running mean: -57.464696\n",
      "ep 535: ep_len:3 episode reward: total was 0.000000. running mean: -56.890049\n",
      "ep 535: ep_len:506 episode reward: total was -81.850000. running mean: -57.139649\n",
      "ep 535: ep_len:569 episode reward: total was -41.120000. running mean: -56.979452\n",
      "epsilon:0.176237 episode_count: 3752. steps_count: 1649086.000000\n",
      "Time elapsed:  4696.293385744095\n",
      "ep 536: ep_len:581 episode reward: total was -153.230000. running mean: -57.941958\n",
      "ep 536: ep_len:501 episode reward: total was -85.430000. running mean: -58.216838\n",
      "ep 536: ep_len:557 episode reward: total was -106.620000. running mean: -58.700870\n",
      "ep 536: ep_len:524 episode reward: total was -117.820000. running mean: -59.292061\n",
      "ep 536: ep_len:3 episode reward: total was -1.500000. running mean: -58.714140\n",
      "ep 536: ep_len:500 episode reward: total was -73.730000. running mean: -58.864299\n",
      "ep 536: ep_len:310 episode reward: total was -54.960000. running mean: -58.825256\n",
      "epsilon:0.176193 episode_count: 3759. steps_count: 1652062.000000\n",
      "Time elapsed:  4705.232776403427\n",
      "ep 537: ep_len:638 episode reward: total was -70.700000. running mean: -58.944003\n",
      "ep 537: ep_len:587 episode reward: total was -41.530000. running mean: -58.769863\n",
      "ep 537: ep_len:614 episode reward: total was -81.050000. running mean: -58.992665\n",
      "ep 537: ep_len:526 episode reward: total was -84.160000. running mean: -59.244338\n",
      "ep 537: ep_len:3 episode reward: total was 0.000000. running mean: -58.651895\n",
      "ep 537: ep_len:557 episode reward: total was -84.880000. running mean: -58.914176\n",
      "ep 537: ep_len:604 episode reward: total was -37.830000. running mean: -58.703334\n",
      "epsilon:0.176149 episode_count: 3766. steps_count: 1655591.000000\n",
      "Time elapsed:  4715.753517627716\n",
      "ep 538: ep_len:588 episode reward: total was -21.020000. running mean: -58.326501\n",
      "ep 538: ep_len:270 episode reward: total was -85.420000. running mean: -58.597436\n",
      "ep 538: ep_len:665 episode reward: total was -117.600000. running mean: -59.187461\n",
      "ep 538: ep_len:500 episode reward: total was -71.190000. running mean: -59.307487\n",
      "ep 538: ep_len:3 episode reward: total was -3.000000. running mean: -58.744412\n",
      "ep 538: ep_len:500 episode reward: total was -16.490000. running mean: -58.321868\n",
      "ep 538: ep_len:606 episode reward: total was -86.570000. running mean: -58.604349\n",
      "epsilon:0.176104 episode_count: 3773. steps_count: 1658723.000000\n",
      "Time elapsed:  4725.2316999435425\n",
      "ep 539: ep_len:547 episode reward: total was -64.960000. running mean: -58.667906\n",
      "ep 539: ep_len:563 episode reward: total was -54.980000. running mean: -58.631027\n",
      "ep 539: ep_len:659 episode reward: total was -125.550000. running mean: -59.300216\n",
      "ep 539: ep_len:500 episode reward: total was -30.140000. running mean: -59.008614\n",
      "ep 539: ep_len:53 episode reward: total was 17.500000. running mean: -58.243528\n",
      "ep 539: ep_len:649 episode reward: total was -72.910000. running mean: -58.390193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 539: ep_len:200 episode reward: total was -20.640000. running mean: -58.012691\n",
      "epsilon:0.176060 episode_count: 3780. steps_count: 1661894.000000\n",
      "Time elapsed:  4735.120292901993\n",
      "ep 540: ep_len:642 episode reward: total was -58.910000. running mean: -58.021664\n",
      "ep 540: ep_len:500 episode reward: total was 26.260000. running mean: -57.178847\n",
      "ep 540: ep_len:64 episode reward: total was -6.900000. running mean: -56.676059\n",
      "ep 540: ep_len:591 episode reward: total was -69.680000. running mean: -56.806098\n",
      "ep 540: ep_len:3 episode reward: total was -1.500000. running mean: -56.253037\n",
      "ep 540: ep_len:568 episode reward: total was -83.070000. running mean: -56.521207\n",
      "ep 540: ep_len:614 episode reward: total was -146.800000. running mean: -57.423995\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.176016 episode_count: 3787. steps_count: 1664876.000000\n",
      "Time elapsed:  4749.4986453056335\n",
      "ep 541: ep_len:552 episode reward: total was -99.140000. running mean: -57.841155\n",
      "ep 541: ep_len:500 episode reward: total was -117.660000. running mean: -58.439343\n",
      "ep 541: ep_len:78 episode reward: total was -1.770000. running mean: -57.872650\n",
      "ep 541: ep_len:48 episode reward: total was -4.740000. running mean: -57.341323\n",
      "ep 541: ep_len:3 episode reward: total was 0.000000. running mean: -56.767910\n",
      "ep 541: ep_len:678 episode reward: total was -76.850000. running mean: -56.968731\n",
      "ep 541: ep_len:567 episode reward: total was -75.270000. running mean: -57.151744\n",
      "epsilon:0.175971 episode_count: 3794. steps_count: 1667302.000000\n",
      "Time elapsed:  4757.825868844986\n",
      "ep 542: ep_len:506 episode reward: total was -30.320000. running mean: -56.883426\n",
      "ep 542: ep_len:500 episode reward: total was -15.080000. running mean: -56.465392\n",
      "ep 542: ep_len:529 episode reward: total was -75.860000. running mean: -56.659338\n",
      "ep 542: ep_len:500 episode reward: total was -33.600000. running mean: -56.428745\n",
      "ep 542: ep_len:3 episode reward: total was 0.000000. running mean: -55.864457\n",
      "ep 542: ep_len:557 episode reward: total was -108.290000. running mean: -56.388713\n",
      "ep 542: ep_len:504 episode reward: total was -80.210000. running mean: -56.626925\n",
      "epsilon:0.175927 episode_count: 3801. steps_count: 1670401.000000\n",
      "Time elapsed:  4767.069123268127\n",
      "ep 543: ep_len:240 episode reward: total was -6.820000. running mean: -56.128856\n",
      "ep 543: ep_len:500 episode reward: total was -56.530000. running mean: -56.132868\n",
      "ep 543: ep_len:628 episode reward: total was -91.480000. running mean: -56.486339\n",
      "ep 543: ep_len:534 episode reward: total was -127.180000. running mean: -57.193276\n",
      "ep 543: ep_len:3 episode reward: total was 0.000000. running mean: -56.621343\n",
      "ep 543: ep_len:500 episode reward: total was -124.880000. running mean: -57.303929\n",
      "ep 543: ep_len:500 episode reward: total was -75.780000. running mean: -57.488690\n",
      "epsilon:0.175883 episode_count: 3808. steps_count: 1673306.000000\n",
      "Time elapsed:  4775.869563817978\n",
      "ep 544: ep_len:569 episode reward: total was -31.210000. running mean: -57.225903\n",
      "ep 544: ep_len:522 episode reward: total was 1.310000. running mean: -56.640544\n",
      "ep 544: ep_len:567 episode reward: total was -95.360000. running mean: -57.027739\n",
      "ep 544: ep_len:500 episode reward: total was -7.010000. running mean: -56.527561\n",
      "ep 544: ep_len:108 episode reward: total was -25.760000. running mean: -56.219886\n",
      "ep 544: ep_len:178 episode reward: total was -10.980000. running mean: -55.767487\n",
      "ep 544: ep_len:312 episode reward: total was -57.440000. running mean: -55.784212\n",
      "epsilon:0.175838 episode_count: 3815. steps_count: 1676062.000000\n",
      "Time elapsed:  4784.410282373428\n",
      "ep 545: ep_len:560 episode reward: total was -20.390000. running mean: -55.430270\n",
      "ep 545: ep_len:541 episode reward: total was -6.200000. running mean: -54.937967\n",
      "ep 545: ep_len:500 episode reward: total was -79.860000. running mean: -55.187188\n",
      "ep 545: ep_len:500 episode reward: total was -81.050000. running mean: -55.445816\n",
      "ep 545: ep_len:90 episode reward: total was 9.710000. running mean: -54.794258\n",
      "ep 545: ep_len:506 episode reward: total was -39.320000. running mean: -54.639515\n",
      "ep 545: ep_len:500 episode reward: total was -59.360000. running mean: -54.686720\n",
      "epsilon:0.175794 episode_count: 3822. steps_count: 1679259.000000\n",
      "Time elapsed:  4793.774389266968\n",
      "ep 546: ep_len:653 episode reward: total was -131.550000. running mean: -55.455353\n",
      "ep 546: ep_len:531 episode reward: total was -95.080000. running mean: -55.851599\n",
      "ep 546: ep_len:640 episode reward: total was -63.740000. running mean: -55.930483\n",
      "ep 546: ep_len:500 episode reward: total was -75.250000. running mean: -56.123678\n",
      "ep 546: ep_len:42 episode reward: total was 15.000000. running mean: -55.412441\n",
      "ep 546: ep_len:510 episode reward: total was -103.070000. running mean: -55.889017\n",
      "ep 546: ep_len:569 episode reward: total was -106.310000. running mean: -56.393227\n",
      "epsilon:0.175750 episode_count: 3829. steps_count: 1682704.000000\n",
      "Time elapsed:  4804.168087720871\n",
      "ep 547: ep_len:632 episode reward: total was -51.450000. running mean: -56.343795\n",
      "ep 547: ep_len:500 episode reward: total was 2.300000. running mean: -55.757357\n",
      "ep 547: ep_len:517 episode reward: total was -92.900000. running mean: -56.128783\n",
      "ep 547: ep_len:517 episode reward: total was -24.560000. running mean: -55.813095\n",
      "ep 547: ep_len:3 episode reward: total was 0.000000. running mean: -55.254964\n",
      "ep 547: ep_len:530 episode reward: total was -71.740000. running mean: -55.419815\n",
      "ep 547: ep_len:556 episode reward: total was -75.310000. running mean: -55.618717\n",
      "epsilon:0.175705 episode_count: 3836. steps_count: 1685959.000000\n",
      "Time elapsed:  4814.21576499939\n",
      "ep 548: ep_len:643 episode reward: total was -136.800000. running mean: -56.430529\n",
      "ep 548: ep_len:631 episode reward: total was -175.420000. running mean: -57.620424\n",
      "ep 548: ep_len:67 episode reward: total was -10.910000. running mean: -57.153320\n",
      "ep 548: ep_len:500 episode reward: total was -65.400000. running mean: -57.235787\n",
      "ep 548: ep_len:3 episode reward: total was 0.000000. running mean: -56.663429\n",
      "ep 548: ep_len:500 episode reward: total was -67.340000. running mean: -56.770194\n",
      "ep 548: ep_len:504 episode reward: total was -88.480000. running mean: -57.087293\n",
      "epsilon:0.175661 episode_count: 3843. steps_count: 1688807.000000\n",
      "Time elapsed:  4823.036179304123\n",
      "ep 549: ep_len:584 episode reward: total was -24.450000. running mean: -56.760920\n",
      "ep 549: ep_len:523 episode reward: total was -40.760000. running mean: -56.600910\n",
      "ep 549: ep_len:574 episode reward: total was -121.720000. running mean: -57.252101\n",
      "ep 549: ep_len:170 episode reward: total was -17.920000. running mean: -56.858780\n",
      "ep 549: ep_len:3 episode reward: total was -1.500000. running mean: -56.305192\n",
      "ep 549: ep_len:177 episode reward: total was 9.120000. running mean: -55.650941\n",
      "ep 549: ep_len:556 episode reward: total was -61.050000. running mean: -55.704931\n",
      "epsilon:0.175617 episode_count: 3850. steps_count: 1691394.000000\n",
      "Time elapsed:  4830.744450092316\n",
      "ep 550: ep_len:587 episode reward: total was -61.290000. running mean: -55.760782\n",
      "ep 550: ep_len:507 episode reward: total was -84.830000. running mean: -56.051474\n",
      "ep 550: ep_len:397 episode reward: total was -12.680000. running mean: -55.617759\n",
      "ep 550: ep_len:500 episode reward: total was -98.710000. running mean: -56.048682\n",
      "ep 550: ep_len:3 episode reward: total was -1.500000. running mean: -55.503195\n",
      "ep 550: ep_len:587 episode reward: total was -44.190000. running mean: -55.390063\n",
      "ep 550: ep_len:580 episode reward: total was -62.590000. running mean: -55.462062\n",
      "epsilon:0.175572 episode_count: 3857. steps_count: 1694555.000000\n",
      "Time elapsed:  4840.6362657547\n",
      "ep 551: ep_len:571 episode reward: total was -101.490000. running mean: -55.922342\n",
      "ep 551: ep_len:569 episode reward: total was -75.930000. running mean: -56.122418\n",
      "ep 551: ep_len:632 episode reward: total was -60.540000. running mean: -56.166594\n",
      "ep 551: ep_len:516 episode reward: total was -91.390000. running mean: -56.518828\n",
      "ep 551: ep_len:3 episode reward: total was 0.000000. running mean: -55.953640\n",
      "ep 551: ep_len:500 episode reward: total was -28.550000. running mean: -55.679603\n",
      "ep 551: ep_len:609 episode reward: total was -71.760000. running mean: -55.840407\n",
      "epsilon:0.175528 episode_count: 3864. steps_count: 1697955.000000\n",
      "Time elapsed:  4850.791071176529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 552: ep_len:500 episode reward: total was -36.930000. running mean: -55.651303\n",
      "ep 552: ep_len:500 episode reward: total was -46.440000. running mean: -55.559190\n",
      "ep 552: ep_len:516 episode reward: total was -189.300000. running mean: -56.896598\n",
      "ep 552: ep_len:544 episode reward: total was -18.950000. running mean: -56.517132\n",
      "ep 552: ep_len:3 episode reward: total was 1.010000. running mean: -55.941861\n",
      "ep 552: ep_len:500 episode reward: total was -93.570000. running mean: -56.318142\n",
      "ep 552: ep_len:615 episode reward: total was -106.940000. running mean: -56.824361\n",
      "epsilon:0.175484 episode_count: 3871. steps_count: 1701133.000000\n",
      "Time elapsed:  4860.368264198303\n",
      "ep 553: ep_len:596 episode reward: total was -47.790000. running mean: -56.734017\n",
      "ep 553: ep_len:553 episode reward: total was -70.590000. running mean: -56.872577\n",
      "ep 553: ep_len:397 episode reward: total was 7.720000. running mean: -56.226652\n",
      "ep 553: ep_len:527 episode reward: total was -69.640000. running mean: -56.360785\n",
      "ep 553: ep_len:85 episode reward: total was -55.330000. running mean: -56.350477\n",
      "ep 553: ep_len:515 episode reward: total was -111.940000. running mean: -56.906372\n",
      "ep 553: ep_len:614 episode reward: total was -114.020000. running mean: -57.477509\n",
      "epsilon:0.175439 episode_count: 3878. steps_count: 1704420.000000\n",
      "Time elapsed:  4870.173429727554\n",
      "ep 554: ep_len:590 episode reward: total was -137.140000. running mean: -58.274134\n",
      "ep 554: ep_len:288 episode reward: total was -20.410000. running mean: -57.895492\n",
      "ep 554: ep_len:632 episode reward: total was -99.230000. running mean: -58.308837\n",
      "ep 554: ep_len:536 episode reward: total was -99.580000. running mean: -58.721549\n",
      "ep 554: ep_len:3 episode reward: total was 0.000000. running mean: -58.134333\n",
      "ep 554: ep_len:557 episode reward: total was -77.340000. running mean: -58.326390\n",
      "ep 554: ep_len:325 episode reward: total was -104.330000. running mean: -58.786426\n",
      "epsilon:0.175395 episode_count: 3885. steps_count: 1707351.000000\n",
      "Time elapsed:  4879.233766078949\n",
      "ep 555: ep_len:125 episode reward: total was -14.610000. running mean: -58.344662\n",
      "ep 555: ep_len:623 episode reward: total was -83.460000. running mean: -58.595815\n",
      "ep 555: ep_len:536 episode reward: total was -76.860000. running mean: -58.778457\n",
      "ep 555: ep_len:500 episode reward: total was -6.430000. running mean: -58.254973\n",
      "ep 555: ep_len:98 episode reward: total was -50.740000. running mean: -58.179823\n",
      "ep 555: ep_len:579 episode reward: total was -22.670000. running mean: -57.824725\n",
      "ep 555: ep_len:518 episode reward: total was -67.130000. running mean: -57.917777\n",
      "epsilon:0.175351 episode_count: 3892. steps_count: 1710330.000000\n",
      "Time elapsed:  4888.29132938385\n",
      "ep 556: ep_len:614 episode reward: total was -56.880000. running mean: -57.907400\n",
      "ep 556: ep_len:594 episode reward: total was -172.790000. running mean: -59.056226\n",
      "ep 556: ep_len:580 episode reward: total was -85.630000. running mean: -59.321963\n",
      "ep 556: ep_len:500 episode reward: total was -95.750000. running mean: -59.686244\n",
      "ep 556: ep_len:113 episode reward: total was 12.150000. running mean: -58.967881\n",
      "ep 556: ep_len:549 episode reward: total was -61.450000. running mean: -58.992702\n",
      "ep 556: ep_len:500 episode reward: total was -71.480000. running mean: -59.117575\n",
      "epsilon:0.175306 episode_count: 3899. steps_count: 1713780.000000\n",
      "Time elapsed:  4899.520092964172\n",
      "ep 557: ep_len:583 episode reward: total was -87.600000. running mean: -59.402400\n",
      "ep 557: ep_len:609 episode reward: total was -86.380000. running mean: -59.672176\n",
      "ep 557: ep_len:671 episode reward: total was -77.990000. running mean: -59.855354\n",
      "ep 557: ep_len:512 episode reward: total was -123.870000. running mean: -60.495500\n",
      "ep 557: ep_len:3 episode reward: total was 0.000000. running mean: -59.890545\n",
      "ep 557: ep_len:500 episode reward: total was -91.580000. running mean: -60.207440\n",
      "ep 557: ep_len:508 episode reward: total was -63.530000. running mean: -60.240666\n",
      "epsilon:0.175262 episode_count: 3906. steps_count: 1717166.000000\n",
      "Time elapsed:  4909.600078344345\n",
      "ep 558: ep_len:510 episode reward: total was -67.480000. running mean: -60.313059\n",
      "ep 558: ep_len:500 episode reward: total was -123.240000. running mean: -60.942328\n",
      "ep 558: ep_len:608 episode reward: total was -77.860000. running mean: -61.111505\n",
      "ep 558: ep_len:519 episode reward: total was -68.960000. running mean: -61.189990\n",
      "ep 558: ep_len:2 episode reward: total was -0.500000. running mean: -60.583090\n",
      "ep 558: ep_len:509 episode reward: total was -75.790000. running mean: -60.735159\n",
      "ep 558: ep_len:525 episode reward: total was -144.020000. running mean: -61.568008\n",
      "epsilon:0.175218 episode_count: 3913. steps_count: 1720339.000000\n",
      "Time elapsed:  4919.095410823822\n",
      "ep 559: ep_len:590 episode reward: total was -15.930000. running mean: -61.111628\n",
      "ep 559: ep_len:529 episode reward: total was 0.320000. running mean: -60.497311\n",
      "ep 559: ep_len:79 episode reward: total was -3.260000. running mean: -59.924938\n",
      "ep 559: ep_len:530 episode reward: total was -24.150000. running mean: -59.567189\n",
      "ep 559: ep_len:3 episode reward: total was 0.000000. running mean: -58.971517\n",
      "ep 559: ep_len:500 episode reward: total was -86.990000. running mean: -59.251702\n",
      "ep 559: ep_len:519 episode reward: total was -85.160000. running mean: -59.510785\n",
      "epsilon:0.175173 episode_count: 3920. steps_count: 1723089.000000\n",
      "Time elapsed:  4927.681115627289\n",
      "ep 560: ep_len:568 episode reward: total was -126.830000. running mean: -60.183977\n",
      "ep 560: ep_len:540 episode reward: total was -83.190000. running mean: -60.414037\n",
      "ep 560: ep_len:500 episode reward: total was -92.640000. running mean: -60.736297\n",
      "ep 560: ep_len:522 episode reward: total was -32.150000. running mean: -60.450434\n",
      "ep 560: ep_len:3 episode reward: total was 0.000000. running mean: -59.845929\n",
      "ep 560: ep_len:179 episode reward: total was 10.640000. running mean: -59.141070\n",
      "ep 560: ep_len:579 episode reward: total was -76.570000. running mean: -59.315359\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.175129 episode_count: 3927. steps_count: 1725980.000000\n",
      "Time elapsed:  4941.7859563827515\n",
      "ep 561: ep_len:554 episode reward: total was -98.230000. running mean: -59.704506\n",
      "ep 561: ep_len:500 episode reward: total was -9.110000. running mean: -59.198561\n",
      "ep 561: ep_len:411 episode reward: total was -2.190000. running mean: -58.628475\n",
      "ep 561: ep_len:503 episode reward: total was -20.500000. running mean: -58.247190\n",
      "ep 561: ep_len:54 episode reward: total was 12.000000. running mean: -57.544718\n",
      "ep 561: ep_len:609 episode reward: total was -66.130000. running mean: -57.630571\n",
      "ep 561: ep_len:595 episode reward: total was -85.220000. running mean: -57.906466\n",
      "epsilon:0.175085 episode_count: 3934. steps_count: 1729206.000000\n",
      "Time elapsed:  4950.096333503723\n",
      "ep 562: ep_len:598 episode reward: total was -37.550000. running mean: -57.702901\n",
      "ep 562: ep_len:576 episode reward: total was -33.080000. running mean: -57.456672\n",
      "ep 562: ep_len:556 episode reward: total was -94.190000. running mean: -57.824005\n",
      "ep 562: ep_len:552 episode reward: total was -47.530000. running mean: -57.721065\n",
      "ep 562: ep_len:3 episode reward: total was -1.500000. running mean: -57.158854\n",
      "ep 562: ep_len:550 episode reward: total was -160.430000. running mean: -58.191566\n",
      "ep 562: ep_len:570 episode reward: total was -92.110000. running mean: -58.530750\n",
      "epsilon:0.175040 episode_count: 3941. steps_count: 1732611.000000\n",
      "Time elapsed:  4958.47193479538\n",
      "ep 563: ep_len:562 episode reward: total was -5.160000. running mean: -57.997043\n",
      "ep 563: ep_len:625 episode reward: total was -65.860000. running mean: -58.075672\n",
      "ep 563: ep_len:613 episode reward: total was -186.440000. running mean: -59.359316\n",
      "ep 563: ep_len:132 episode reward: total was -12.970000. running mean: -58.895422\n",
      "ep 563: ep_len:3 episode reward: total was 0.000000. running mean: -58.306468\n",
      "ep 563: ep_len:596 episode reward: total was -228.190000. running mean: -60.005304\n",
      "ep 563: ep_len:564 episode reward: total was -75.020000. running mean: -60.155451\n",
      "epsilon:0.174996 episode_count: 3948. steps_count: 1735706.000000\n",
      "Time elapsed:  4966.493857860565\n",
      "ep 564: ep_len:589 episode reward: total was -108.840000. running mean: -60.642296\n",
      "ep 564: ep_len:343 episode reward: total was -43.310000. running mean: -60.468973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 564: ep_len:570 episode reward: total was -190.260000. running mean: -61.766883\n",
      "ep 564: ep_len:514 episode reward: total was -0.900000. running mean: -61.158214\n",
      "ep 564: ep_len:3 episode reward: total was 0.000000. running mean: -60.546632\n",
      "ep 564: ep_len:500 episode reward: total was -54.200000. running mean: -60.483166\n",
      "ep 564: ep_len:610 episode reward: total was -51.690000. running mean: -60.395234\n",
      "epsilon:0.174952 episode_count: 3955. steps_count: 1738835.000000\n",
      "Time elapsed:  4975.040009260178\n",
      "ep 565: ep_len:618 episode reward: total was -125.330000. running mean: -61.044582\n",
      "ep 565: ep_len:500 episode reward: total was -70.320000. running mean: -61.137336\n",
      "ep 565: ep_len:580 episode reward: total was -123.180000. running mean: -61.757763\n",
      "ep 565: ep_len:500 episode reward: total was -72.400000. running mean: -61.864185\n",
      "ep 565: ep_len:97 episode reward: total was -26.770000. running mean: -61.513243\n",
      "ep 565: ep_len:548 episode reward: total was -115.300000. running mean: -62.051111\n",
      "ep 565: ep_len:201 episode reward: total was -58.930000. running mean: -62.019900\n",
      "epsilon:0.174907 episode_count: 3962. steps_count: 1741879.000000\n",
      "Time elapsed:  4983.5766134262085\n",
      "ep 566: ep_len:564 episode reward: total was -83.880000. running mean: -62.238501\n",
      "ep 566: ep_len:625 episode reward: total was -49.510000. running mean: -62.111216\n",
      "ep 566: ep_len:620 episode reward: total was -114.230000. running mean: -62.632404\n",
      "ep 566: ep_len:500 episode reward: total was -69.100000. running mean: -62.697080\n",
      "ep 566: ep_len:40 episode reward: total was 11.000000. running mean: -61.960109\n",
      "ep 566: ep_len:575 episode reward: total was -94.500000. running mean: -62.285508\n",
      "ep 566: ep_len:529 episode reward: total was -101.440000. running mean: -62.677053\n",
      "epsilon:0.174863 episode_count: 3969. steps_count: 1745332.000000\n",
      "Time elapsed:  4992.530254364014\n",
      "ep 567: ep_len:555 episode reward: total was -105.290000. running mean: -63.103182\n",
      "ep 567: ep_len:500 episode reward: total was -65.880000. running mean: -63.130950\n",
      "ep 567: ep_len:575 episode reward: total was -44.730000. running mean: -62.946941\n",
      "ep 567: ep_len:99 episode reward: total was 0.750000. running mean: -62.309971\n",
      "ep 567: ep_len:3 episode reward: total was -1.500000. running mean: -61.701872\n",
      "ep 567: ep_len:500 episode reward: total was -36.100000. running mean: -61.445853\n",
      "ep 567: ep_len:164 episode reward: total was -20.210000. running mean: -61.033494\n",
      "epsilon:0.174819 episode_count: 3976. steps_count: 1747728.000000\n",
      "Time elapsed:  4999.033478498459\n",
      "ep 568: ep_len:500 episode reward: total was 9.560000. running mean: -60.327559\n",
      "ep 568: ep_len:553 episode reward: total was 16.840000. running mean: -59.555884\n",
      "ep 568: ep_len:515 episode reward: total was -86.070000. running mean: -59.821025\n",
      "ep 568: ep_len:500 episode reward: total was -25.970000. running mean: -59.482515\n",
      "ep 568: ep_len:90 episode reward: total was 12.220000. running mean: -58.765490\n",
      "ep 568: ep_len:529 episode reward: total was -106.920000. running mean: -59.247035\n",
      "ep 568: ep_len:500 episode reward: total was -51.570000. running mean: -59.170264\n",
      "epsilon:0.174774 episode_count: 3983. steps_count: 1750915.000000\n",
      "Time elapsed:  5007.966673135757\n",
      "ep 569: ep_len:639 episode reward: total was -110.920000. running mean: -59.687762\n",
      "ep 569: ep_len:501 episode reward: total was -154.640000. running mean: -60.637284\n",
      "ep 569: ep_len:539 episode reward: total was -117.140000. running mean: -61.202311\n",
      "ep 569: ep_len:539 episode reward: total was -88.940000. running mean: -61.479688\n",
      "ep 569: ep_len:102 episode reward: total was 5.240000. running mean: -60.812491\n",
      "ep 569: ep_len:500 episode reward: total was -65.220000. running mean: -60.856566\n",
      "ep 569: ep_len:553 episode reward: total was -100.170000. running mean: -61.249701\n",
      "epsilon:0.174730 episode_count: 3990. steps_count: 1754288.000000\n",
      "Time elapsed:  5017.030992746353\n",
      "ep 570: ep_len:607 episode reward: total was 3.930000. running mean: -60.597904\n",
      "ep 570: ep_len:576 episode reward: total was -89.270000. running mean: -60.884625\n",
      "ep 570: ep_len:443 episode reward: total was -29.100000. running mean: -60.566778\n",
      "ep 570: ep_len:515 episode reward: total was -31.670000. running mean: -60.277811\n",
      "ep 570: ep_len:78 episode reward: total was -30.740000. running mean: -59.982433\n",
      "ep 570: ep_len:573 episode reward: total was -79.100000. running mean: -60.173608\n",
      "ep 570: ep_len:567 episode reward: total was -124.880000. running mean: -60.820672\n",
      "epsilon:0.174686 episode_count: 3997. steps_count: 1757647.000000\n",
      "Time elapsed:  5025.7205991744995\n",
      "ep 571: ep_len:585 episode reward: total was -24.840000. running mean: -60.460865\n",
      "ep 571: ep_len:510 episode reward: total was -66.570000. running mean: -60.521957\n",
      "ep 571: ep_len:528 episode reward: total was -72.870000. running mean: -60.645437\n",
      "ep 571: ep_len:508 episode reward: total was -43.430000. running mean: -60.473283\n",
      "ep 571: ep_len:3 episode reward: total was 0.000000. running mean: -59.868550\n",
      "ep 571: ep_len:185 episode reward: total was 4.670000. running mean: -59.223165\n",
      "ep 571: ep_len:509 episode reward: total was -110.170000. running mean: -59.732633\n",
      "epsilon:0.174641 episode_count: 4004. steps_count: 1760475.000000\n",
      "Time elapsed:  5033.26767539978\n",
      "ep 572: ep_len:521 episode reward: total was -52.190000. running mean: -59.657207\n",
      "ep 572: ep_len:500 episode reward: total was -73.220000. running mean: -59.792834\n",
      "ep 572: ep_len:581 episode reward: total was -81.030000. running mean: -60.005206\n",
      "ep 572: ep_len:500 episode reward: total was -93.790000. running mean: -60.343054\n",
      "ep 572: ep_len:86 episode reward: total was -8.300000. running mean: -59.822624\n",
      "ep 572: ep_len:540 episode reward: total was -57.810000. running mean: -59.802497\n",
      "ep 572: ep_len:594 episode reward: total was -64.590000. running mean: -59.850372\n",
      "epsilon:0.174597 episode_count: 4011. steps_count: 1763797.000000\n",
      "Time elapsed:  5042.072768688202\n",
      "ep 573: ep_len:134 episode reward: total was -5.460000. running mean: -59.306469\n",
      "ep 573: ep_len:611 episode reward: total was -46.830000. running mean: -59.181704\n",
      "ep 573: ep_len:500 episode reward: total was -105.270000. running mean: -59.642587\n",
      "ep 573: ep_len:546 episode reward: total was -74.950000. running mean: -59.795661\n",
      "ep 573: ep_len:3 episode reward: total was 0.000000. running mean: -59.197704\n",
      "ep 573: ep_len:254 episode reward: total was -9.740000. running mean: -58.703127\n",
      "ep 573: ep_len:500 episode reward: total was -46.200000. running mean: -58.578096\n",
      "epsilon:0.174553 episode_count: 4018. steps_count: 1766345.000000\n",
      "Time elapsed:  5049.255538702011\n",
      "ep 574: ep_len:531 episode reward: total was -80.730000. running mean: -58.799615\n",
      "ep 574: ep_len:523 episode reward: total was -19.040000. running mean: -58.402019\n",
      "ep 574: ep_len:61 episode reward: total was 0.080000. running mean: -57.817199\n",
      "ep 574: ep_len:500 episode reward: total was -44.030000. running mean: -57.679327\n",
      "ep 574: ep_len:44 episode reward: total was -2.000000. running mean: -57.122534\n",
      "ep 574: ep_len:504 episode reward: total was -67.960000. running mean: -57.230908\n",
      "ep 574: ep_len:595 episode reward: total was -46.390000. running mean: -57.122499\n",
      "epsilon:0.174508 episode_count: 4025. steps_count: 1769103.000000\n",
      "Time elapsed:  5056.851055383682\n",
      "ep 575: ep_len:500 episode reward: total was -27.210000. running mean: -56.823374\n",
      "ep 575: ep_len:500 episode reward: total was -49.900000. running mean: -56.754140\n",
      "ep 575: ep_len:521 episode reward: total was -94.850000. running mean: -57.135099\n",
      "ep 575: ep_len:503 episode reward: total was -59.750000. running mean: -57.161248\n",
      "ep 575: ep_len:104 episode reward: total was -2.780000. running mean: -56.617435\n",
      "ep 575: ep_len:162 episode reward: total was -4.900000. running mean: -56.100261\n",
      "ep 575: ep_len:547 episode reward: total was -59.190000. running mean: -56.131159\n",
      "epsilon:0.174464 episode_count: 4032. steps_count: 1771940.000000\n",
      "Time elapsed:  5064.534435033798\n",
      "ep 576: ep_len:528 episode reward: total was -44.500000. running mean: -56.014847\n",
      "ep 576: ep_len:500 episode reward: total was -125.200000. running mean: -56.706698\n",
      "ep 576: ep_len:530 episode reward: total was -59.900000. running mean: -56.738631\n",
      "ep 576: ep_len:420 episode reward: total was -24.120000. running mean: -56.412445\n",
      "ep 576: ep_len:3 episode reward: total was -1.500000. running mean: -55.863321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 576: ep_len:605 episode reward: total was -56.060000. running mean: -55.865288\n",
      "ep 576: ep_len:519 episode reward: total was -69.620000. running mean: -56.002835\n",
      "epsilon:0.174420 episode_count: 4039. steps_count: 1775045.000000\n",
      "Time elapsed:  5072.510979175568\n",
      "ep 577: ep_len:571 episode reward: total was -116.950000. running mean: -56.612306\n",
      "ep 577: ep_len:568 episode reward: total was -44.100000. running mean: -56.487183\n",
      "ep 577: ep_len:599 episode reward: total was -68.020000. running mean: -56.602511\n",
      "ep 577: ep_len:562 episode reward: total was -1.660000. running mean: -56.053086\n",
      "ep 577: ep_len:3 episode reward: total was -1.500000. running mean: -55.507555\n",
      "ep 577: ep_len:500 episode reward: total was -25.620000. running mean: -55.208680\n",
      "ep 577: ep_len:515 episode reward: total was -78.500000. running mean: -55.441593\n",
      "epsilon:0.174375 episode_count: 4046. steps_count: 1778363.000000\n",
      "Time elapsed:  5081.425217151642\n",
      "ep 578: ep_len:220 episode reward: total was -13.390000. running mean: -55.021077\n",
      "ep 578: ep_len:598 episode reward: total was -120.100000. running mean: -55.671866\n",
      "ep 578: ep_len:425 episode reward: total was -34.310000. running mean: -55.458248\n",
      "ep 578: ep_len:590 episode reward: total was -23.500000. running mean: -55.138665\n",
      "ep 578: ep_len:92 episode reward: total was -14.270000. running mean: -54.729979\n",
      "ep 578: ep_len:525 episode reward: total was -87.530000. running mean: -55.057979\n",
      "ep 578: ep_len:501 episode reward: total was -42.340000. running mean: -54.930799\n",
      "epsilon:0.174331 episode_count: 4053. steps_count: 1781314.000000\n",
      "Time elapsed:  5089.218185186386\n",
      "ep 579: ep_len:501 episode reward: total was -99.310000. running mean: -55.374591\n",
      "ep 579: ep_len:580 episode reward: total was -69.930000. running mean: -55.520145\n",
      "ep 579: ep_len:564 episode reward: total was -90.130000. running mean: -55.866244\n",
      "ep 579: ep_len:500 episode reward: total was -64.940000. running mean: -55.956981\n",
      "ep 579: ep_len:3 episode reward: total was 0.000000. running mean: -55.397411\n",
      "ep 579: ep_len:592 episode reward: total was -164.820000. running mean: -56.491637\n",
      "ep 579: ep_len:331 episode reward: total was -72.830000. running mean: -56.655021\n",
      "epsilon:0.174287 episode_count: 4060. steps_count: 1784385.000000\n",
      "Time elapsed:  5097.405615329742\n",
      "ep 580: ep_len:593 episode reward: total was -136.870000. running mean: -57.457171\n",
      "ep 580: ep_len:270 episode reward: total was -82.760000. running mean: -57.710199\n",
      "ep 580: ep_len:500 episode reward: total was -47.920000. running mean: -57.612297\n",
      "ep 580: ep_len:399 episode reward: total was -39.070000. running mean: -57.426874\n",
      "ep 580: ep_len:100 episode reward: total was -12.260000. running mean: -56.975205\n",
      "ep 580: ep_len:519 episode reward: total was -80.100000. running mean: -57.206453\n",
      "ep 580: ep_len:614 episode reward: total was -81.460000. running mean: -57.448989\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.174242 episode_count: 4067. steps_count: 1787380.000000\n",
      "Time elapsed:  5110.314666986465\n",
      "ep 581: ep_len:500 episode reward: total was -33.870000. running mean: -57.213199\n",
      "ep 581: ep_len:201 episode reward: total was -55.950000. running mean: -57.200567\n",
      "ep 581: ep_len:543 episode reward: total was -87.440000. running mean: -57.502961\n",
      "ep 581: ep_len:594 episode reward: total was -42.040000. running mean: -57.348332\n",
      "ep 581: ep_len:44 episode reward: total was 13.000000. running mean: -56.644848\n",
      "ep 581: ep_len:500 episode reward: total was -119.030000. running mean: -57.268700\n",
      "ep 581: ep_len:258 episode reward: total was -30.910000. running mean: -57.005113\n",
      "epsilon:0.174198 episode_count: 4074. steps_count: 1790020.000000\n",
      "Time elapsed:  5117.462707519531\n",
      "ep 582: ep_len:577 episode reward: total was -8.160000. running mean: -56.516662\n",
      "ep 582: ep_len:612 episode reward: total was -52.750000. running mean: -56.478995\n",
      "ep 582: ep_len:516 episode reward: total was -111.540000. running mean: -57.029605\n",
      "ep 582: ep_len:500 episode reward: total was -47.620000. running mean: -56.935509\n",
      "ep 582: ep_len:3 episode reward: total was -1.500000. running mean: -56.381154\n",
      "ep 582: ep_len:584 episode reward: total was -74.990000. running mean: -56.567242\n",
      "ep 582: ep_len:514 episode reward: total was -72.520000. running mean: -56.726770\n",
      "epsilon:0.174154 episode_count: 4081. steps_count: 1793326.000000\n",
      "Time elapsed:  5126.125898361206\n",
      "ep 583: ep_len:534 episode reward: total was -77.710000. running mean: -56.936602\n",
      "ep 583: ep_len:500 episode reward: total was -23.260000. running mean: -56.599836\n",
      "ep 583: ep_len:608 episode reward: total was -110.950000. running mean: -57.143338\n",
      "ep 583: ep_len:113 episode reward: total was 1.930000. running mean: -56.552604\n",
      "ep 583: ep_len:101 episode reward: total was -15.770000. running mean: -56.144778\n",
      "ep 583: ep_len:538 episode reward: total was -80.490000. running mean: -56.388231\n",
      "ep 583: ep_len:500 episode reward: total was -43.110000. running mean: -56.255448\n",
      "epsilon:0.174109 episode_count: 4088. steps_count: 1796220.000000\n",
      "Time elapsed:  5134.178024053574\n",
      "ep 584: ep_len:665 episode reward: total was -92.110000. running mean: -56.613994\n",
      "ep 584: ep_len:520 episode reward: total was -57.770000. running mean: -56.625554\n",
      "ep 584: ep_len:674 episode reward: total was -107.920000. running mean: -57.138498\n",
      "ep 584: ep_len:117 episode reward: total was -5.070000. running mean: -56.617813\n",
      "ep 584: ep_len:3 episode reward: total was 0.000000. running mean: -56.051635\n",
      "ep 584: ep_len:502 episode reward: total was -106.400000. running mean: -56.555119\n",
      "ep 584: ep_len:512 episode reward: total was -72.620000. running mean: -56.715768\n",
      "epsilon:0.174065 episode_count: 4095. steps_count: 1799213.000000\n",
      "Time elapsed:  5142.053665399551\n",
      "ep 585: ep_len:544 episode reward: total was -130.040000. running mean: -57.449010\n",
      "ep 585: ep_len:369 episode reward: total was -105.640000. running mean: -57.930920\n",
      "ep 585: ep_len:577 episode reward: total was -96.500000. running mean: -58.316611\n",
      "ep 585: ep_len:500 episode reward: total was -38.130000. running mean: -58.114745\n",
      "ep 585: ep_len:3 episode reward: total was -1.500000. running mean: -57.548597\n",
      "ep 585: ep_len:511 episode reward: total was -103.640000. running mean: -58.009511\n",
      "ep 585: ep_len:539 episode reward: total was -48.130000. running mean: -57.910716\n",
      "epsilon:0.174021 episode_count: 4102. steps_count: 1802256.000000\n",
      "Time elapsed:  5150.04639673233\n",
      "ep 586: ep_len:619 episode reward: total was -67.010000. running mean: -58.001709\n",
      "ep 586: ep_len:592 episode reward: total was -50.300000. running mean: -57.924692\n",
      "ep 586: ep_len:525 episode reward: total was -97.020000. running mean: -58.315645\n",
      "ep 586: ep_len:13 episode reward: total was -2.900000. running mean: -57.761489\n",
      "ep 586: ep_len:3 episode reward: total was 0.000000. running mean: -57.183874\n",
      "ep 586: ep_len:601 episode reward: total was -93.780000. running mean: -57.549835\n",
      "ep 586: ep_len:304 episode reward: total was -87.490000. running mean: -57.849237\n",
      "epsilon:0.173976 episode_count: 4109. steps_count: 1804913.000000\n",
      "Time elapsed:  5157.159956455231\n",
      "ep 587: ep_len:615 episode reward: total was -24.890000. running mean: -57.519644\n",
      "ep 587: ep_len:523 episode reward: total was -50.870000. running mean: -57.453148\n",
      "ep 587: ep_len:621 episode reward: total was -161.630000. running mean: -58.494916\n",
      "ep 587: ep_len:56 episode reward: total was -5.670000. running mean: -57.966667\n",
      "ep 587: ep_len:3 episode reward: total was 0.000000. running mean: -57.387000\n",
      "ep 587: ep_len:554 episode reward: total was -74.830000. running mean: -57.561430\n",
      "ep 587: ep_len:500 episode reward: total was -60.100000. running mean: -57.586816\n",
      "epsilon:0.173932 episode_count: 4116. steps_count: 1807785.000000\n",
      "Time elapsed:  5165.025143146515\n",
      "ep 588: ep_len:500 episode reward: total was -110.840000. running mean: -58.119348\n",
      "ep 588: ep_len:581 episode reward: total was -79.030000. running mean: -58.328454\n",
      "ep 588: ep_len:500 episode reward: total was -39.340000. running mean: -58.138570\n",
      "ep 588: ep_len:564 episode reward: total was -7.300000. running mean: -57.630184\n",
      "ep 588: ep_len:3 episode reward: total was 0.000000. running mean: -57.053882\n",
      "ep 588: ep_len:514 episode reward: total was -141.050000. running mean: -57.893844\n",
      "ep 588: ep_len:500 episode reward: total was -98.610000. running mean: -58.301005\n",
      "epsilon:0.173888 episode_count: 4123. steps_count: 1810947.000000\n",
      "Time elapsed:  5173.99936914444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 589: ep_len:604 episode reward: total was -141.080000. running mean: -59.128795\n",
      "ep 589: ep_len:640 episode reward: total was -54.080000. running mean: -59.078307\n",
      "ep 589: ep_len:556 episode reward: total was -53.310000. running mean: -59.020624\n",
      "ep 589: ep_len:570 episode reward: total was -54.860000. running mean: -58.979018\n",
      "ep 589: ep_len:3 episode reward: total was 0.000000. running mean: -58.389228\n",
      "ep 589: ep_len:237 episode reward: total was -1.250000. running mean: -57.817835\n",
      "ep 589: ep_len:208 episode reward: total was -35.940000. running mean: -57.599057\n",
      "epsilon:0.173843 episode_count: 4130. steps_count: 1813765.000000\n",
      "Time elapsed:  5181.658444881439\n",
      "ep 590: ep_len:501 episode reward: total was 2.590000. running mean: -56.997166\n",
      "ep 590: ep_len:501 episode reward: total was -14.010000. running mean: -56.567295\n",
      "ep 590: ep_len:528 episode reward: total was -95.060000. running mean: -56.952222\n",
      "ep 590: ep_len:521 episode reward: total was -31.480000. running mean: -56.697500\n",
      "ep 590: ep_len:55 episode reward: total was 19.510000. running mean: -55.935425\n",
      "ep 590: ep_len:631 episode reward: total was -157.350000. running mean: -56.949570\n",
      "ep 590: ep_len:500 episode reward: total was -99.070000. running mean: -57.370775\n",
      "epsilon:0.173799 episode_count: 4137. steps_count: 1817002.000000\n",
      "Time elapsed:  5190.350925922394\n",
      "ep 591: ep_len:632 episode reward: total was -107.060000. running mean: -57.867667\n",
      "ep 591: ep_len:500 episode reward: total was -95.020000. running mean: -58.239190\n",
      "ep 591: ep_len:458 episode reward: total was -17.470000. running mean: -57.831498\n",
      "ep 591: ep_len:56 episode reward: total was -15.720000. running mean: -57.410383\n",
      "ep 591: ep_len:47 episode reward: total was 11.500000. running mean: -56.721280\n",
      "ep 591: ep_len:617 episode reward: total was -100.710000. running mean: -57.161167\n",
      "ep 591: ep_len:501 episode reward: total was -63.940000. running mean: -57.228955\n",
      "epsilon:0.173755 episode_count: 4144. steps_count: 1819813.000000\n",
      "Time elapsed:  5198.632165670395\n",
      "ep 592: ep_len:500 episode reward: total was -6.750000. running mean: -56.724166\n",
      "ep 592: ep_len:644 episode reward: total was -77.180000. running mean: -56.928724\n",
      "ep 592: ep_len:641 episode reward: total was -92.040000. running mean: -57.279837\n",
      "ep 592: ep_len:143 episode reward: total was -9.980000. running mean: -56.806838\n",
      "ep 592: ep_len:103 episode reward: total was 21.260000. running mean: -56.026170\n",
      "ep 592: ep_len:186 episode reward: total was 21.210000. running mean: -55.253808\n",
      "ep 592: ep_len:180 episode reward: total was -21.300000. running mean: -54.914270\n",
      "epsilon:0.173710 episode_count: 4151. steps_count: 1822210.000000\n",
      "Time elapsed:  5205.178590536118\n",
      "ep 593: ep_len:578 episode reward: total was -110.200000. running mean: -55.467127\n",
      "ep 593: ep_len:556 episode reward: total was 10.250000. running mean: -54.809956\n",
      "ep 593: ep_len:429 episode reward: total was -43.470000. running mean: -54.696557\n",
      "ep 593: ep_len:510 episode reward: total was -11.530000. running mean: -54.264891\n",
      "ep 593: ep_len:3 episode reward: total was 0.000000. running mean: -53.722242\n",
      "ep 593: ep_len:590 episode reward: total was -74.420000. running mean: -53.929220\n",
      "ep 593: ep_len:534 episode reward: total was -74.580000. running mean: -54.135727\n",
      "epsilon:0.173666 episode_count: 4158. steps_count: 1825410.000000\n",
      "Time elapsed:  5213.614196538925\n",
      "ep 594: ep_len:237 episode reward: total was -5.340000. running mean: -53.647770\n",
      "ep 594: ep_len:500 episode reward: total was -128.760000. running mean: -54.398892\n",
      "ep 594: ep_len:680 episode reward: total was -142.180000. running mean: -55.276704\n",
      "ep 594: ep_len:386 episode reward: total was -65.730000. running mean: -55.381237\n",
      "ep 594: ep_len:90 episode reward: total was 5.210000. running mean: -54.775324\n",
      "ep 594: ep_len:587 episode reward: total was -117.030000. running mean: -55.397871\n",
      "ep 594: ep_len:626 episode reward: total was -61.150000. running mean: -55.455392\n",
      "epsilon:0.173622 episode_count: 4165. steps_count: 1828516.000000\n",
      "Time elapsed:  5221.902090072632\n",
      "ep 595: ep_len:668 episode reward: total was -81.790000. running mean: -55.718738\n",
      "ep 595: ep_len:582 episode reward: total was -144.160000. running mean: -56.603151\n",
      "ep 595: ep_len:591 episode reward: total was -84.260000. running mean: -56.879719\n",
      "ep 595: ep_len:169 episode reward: total was -11.340000. running mean: -56.424322\n",
      "ep 595: ep_len:95 episode reward: total was -8.790000. running mean: -55.947979\n",
      "ep 595: ep_len:320 episode reward: total was -14.880000. running mean: -55.537299\n",
      "ep 595: ep_len:508 episode reward: total was -80.700000. running mean: -55.788926\n",
      "epsilon:0.173577 episode_count: 4172. steps_count: 1831449.000000\n",
      "Time elapsed:  5229.926369190216\n",
      "ep 596: ep_len:628 episode reward: total was -212.990000. running mean: -57.360937\n",
      "ep 596: ep_len:500 episode reward: total was -6.480000. running mean: -56.852128\n",
      "ep 596: ep_len:594 episode reward: total was -68.680000. running mean: -56.970406\n",
      "ep 596: ep_len:501 episode reward: total was -123.190000. running mean: -57.632602\n",
      "ep 596: ep_len:88 episode reward: total was 0.230000. running mean: -57.053976\n",
      "ep 596: ep_len:518 episode reward: total was -70.930000. running mean: -57.192736\n",
      "ep 596: ep_len:600 episode reward: total was -80.770000. running mean: -57.428509\n",
      "epsilon:0.173533 episode_count: 4179. steps_count: 1834878.000000\n",
      "Time elapsed:  5239.473129987717\n",
      "ep 597: ep_len:500 episode reward: total was -28.910000. running mean: -57.143324\n",
      "ep 597: ep_len:320 episode reward: total was -138.370000. running mean: -57.955591\n",
      "ep 597: ep_len:501 episode reward: total was -70.900000. running mean: -58.085035\n",
      "ep 597: ep_len:533 episode reward: total was -33.790000. running mean: -57.842084\n",
      "ep 597: ep_len:3 episode reward: total was 0.000000. running mean: -57.263664\n",
      "ep 597: ep_len:563 episode reward: total was -89.050000. running mean: -57.581527\n",
      "ep 597: ep_len:506 episode reward: total was -118.570000. running mean: -58.191412\n",
      "epsilon:0.173489 episode_count: 4186. steps_count: 1837804.000000\n",
      "Time elapsed:  5247.28074836731\n",
      "ep 598: ep_len:500 episode reward: total was -139.480000. running mean: -59.004298\n",
      "ep 598: ep_len:572 episode reward: total was -15.830000. running mean: -58.572555\n",
      "ep 598: ep_len:587 episode reward: total was -133.780000. running mean: -59.324629\n",
      "ep 598: ep_len:520 episode reward: total was -56.980000. running mean: -59.301183\n",
      "ep 598: ep_len:3 episode reward: total was -1.500000. running mean: -58.723171\n",
      "ep 598: ep_len:614 episode reward: total was -71.910000. running mean: -58.855039\n",
      "ep 598: ep_len:551 episode reward: total was -48.770000. running mean: -58.754189\n",
      "epsilon:0.173444 episode_count: 4193. steps_count: 1841151.000000\n",
      "Time elapsed:  5256.09708571434\n",
      "ep 599: ep_len:588 episode reward: total was -65.400000. running mean: -58.820647\n",
      "ep 599: ep_len:505 episode reward: total was -81.090000. running mean: -59.043341\n",
      "ep 599: ep_len:500 episode reward: total was -48.470000. running mean: -58.937607\n",
      "ep 599: ep_len:418 episode reward: total was -15.540000. running mean: -58.503631\n",
      "ep 599: ep_len:3 episode reward: total was 0.000000. running mean: -57.918595\n",
      "ep 599: ep_len:530 episode reward: total was -52.140000. running mean: -57.860809\n",
      "ep 599: ep_len:613 episode reward: total was -72.520000. running mean: -58.007401\n",
      "epsilon:0.173400 episode_count: 4200. steps_count: 1844308.000000\n",
      "Time elapsed:  5264.535559892654\n",
      "ep 600: ep_len:526 episode reward: total was -132.960000. running mean: -58.756927\n",
      "ep 600: ep_len:524 episode reward: total was -8.740000. running mean: -58.256757\n",
      "ep 600: ep_len:436 episode reward: total was 10.130000. running mean: -57.572890\n",
      "ep 600: ep_len:154 episode reward: total was -5.510000. running mean: -57.052261\n",
      "ep 600: ep_len:116 episode reward: total was 10.800000. running mean: -56.373738\n",
      "ep 600: ep_len:672 episode reward: total was -70.270000. running mean: -56.512701\n",
      "ep 600: ep_len:543 episode reward: total was -82.530000. running mean: -56.772874\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.173356 episode_count: 4207. steps_count: 1847279.000000\n",
      "Time elapsed:  5277.628219127655\n",
      "ep 601: ep_len:500 episode reward: total was -104.990000. running mean: -57.255045\n",
      "ep 601: ep_len:266 episode reward: total was -85.470000. running mean: -57.537195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 601: ep_len:505 episode reward: total was -88.220000. running mean: -57.844023\n",
      "ep 601: ep_len:547 episode reward: total was -12.270000. running mean: -57.388283\n",
      "ep 601: ep_len:3 episode reward: total was 0.000000. running mean: -56.814400\n",
      "ep 601: ep_len:500 episode reward: total was -80.500000. running mean: -57.051256\n",
      "ep 601: ep_len:556 episode reward: total was -98.070000. running mean: -57.461443\n",
      "epsilon:0.173311 episode_count: 4214. steps_count: 1850156.000000\n",
      "Time elapsed:  5286.280788183212\n",
      "ep 602: ep_len:218 episode reward: total was -35.910000. running mean: -57.245929\n",
      "ep 602: ep_len:500 episode reward: total was -71.940000. running mean: -57.392869\n",
      "ep 602: ep_len:629 episode reward: total was -111.080000. running mean: -57.929741\n",
      "ep 602: ep_len:554 episode reward: total was -40.460000. running mean: -57.755043\n",
      "ep 602: ep_len:92 episode reward: total was -19.290000. running mean: -57.370393\n",
      "ep 602: ep_len:500 episode reward: total was -119.630000. running mean: -57.992989\n",
      "ep 602: ep_len:500 episode reward: total was -64.740000. running mean: -58.060459\n",
      "epsilon:0.173267 episode_count: 4221. steps_count: 1853149.000000\n",
      "Time elapsed:  5295.420659303665\n",
      "ep 603: ep_len:237 episode reward: total was -13.770000. running mean: -57.617555\n",
      "ep 603: ep_len:650 episode reward: total was -108.420000. running mean: -58.125579\n",
      "ep 603: ep_len:79 episode reward: total was -4.760000. running mean: -57.591923\n",
      "ep 603: ep_len:528 episode reward: total was -104.960000. running mean: -58.065604\n",
      "ep 603: ep_len:3 episode reward: total was -1.500000. running mean: -57.499948\n",
      "ep 603: ep_len:528 episode reward: total was -53.990000. running mean: -57.464848\n",
      "ep 603: ep_len:193 episode reward: total was -25.270000. running mean: -57.142900\n",
      "epsilon:0.173223 episode_count: 4228. steps_count: 1855367.000000\n",
      "Time elapsed:  5302.620283126831\n",
      "ep 604: ep_len:531 episode reward: total was -56.310000. running mean: -57.134571\n",
      "ep 604: ep_len:592 episode reward: total was -32.040000. running mean: -56.883625\n",
      "ep 604: ep_len:550 episode reward: total was -87.370000. running mean: -57.188489\n",
      "ep 604: ep_len:501 episode reward: total was -75.260000. running mean: -57.369204\n",
      "ep 604: ep_len:3 episode reward: total was 0.000000. running mean: -56.795512\n",
      "ep 604: ep_len:503 episode reward: total was -97.160000. running mean: -57.199157\n",
      "ep 604: ep_len:595 episode reward: total was -136.460000. running mean: -57.991765\n",
      "epsilon:0.173178 episode_count: 4235. steps_count: 1858642.000000\n",
      "Time elapsed:  5312.220087766647\n",
      "ep 605: ep_len:515 episode reward: total was -53.260000. running mean: -57.944448\n",
      "ep 605: ep_len:588 episode reward: total was -66.250000. running mean: -58.027503\n",
      "ep 605: ep_len:581 episode reward: total was -78.670000. running mean: -58.233928\n",
      "ep 605: ep_len:54 episode reward: total was -7.190000. running mean: -57.723489\n",
      "ep 605: ep_len:3 episode reward: total was 0.000000. running mean: -57.146254\n",
      "ep 605: ep_len:500 episode reward: total was -57.660000. running mean: -57.151392\n",
      "ep 605: ep_len:546 episode reward: total was -97.880000. running mean: -57.558678\n",
      "epsilon:0.173134 episode_count: 4242. steps_count: 1861429.000000\n",
      "Time elapsed:  5320.894465208054\n",
      "ep 606: ep_len:500 episode reward: total was -30.690000. running mean: -57.289991\n",
      "ep 606: ep_len:500 episode reward: total was -30.540000. running mean: -57.022491\n",
      "ep 606: ep_len:621 episode reward: total was -82.060000. running mean: -57.272866\n",
      "ep 606: ep_len:550 episode reward: total was -27.110000. running mean: -56.971237\n",
      "ep 606: ep_len:81 episode reward: total was 9.640000. running mean: -56.305125\n",
      "ep 606: ep_len:615 episode reward: total was -69.830000. running mean: -56.440374\n",
      "ep 606: ep_len:528 episode reward: total was -59.420000. running mean: -56.470170\n",
      "epsilon:0.173090 episode_count: 4249. steps_count: 1864824.000000\n",
      "Time elapsed:  5330.990955352783\n",
      "ep 607: ep_len:501 episode reward: total was -58.380000. running mean: -56.489268\n",
      "ep 607: ep_len:514 episode reward: total was -49.820000. running mean: -56.422576\n",
      "ep 607: ep_len:500 episode reward: total was -40.020000. running mean: -56.258550\n",
      "ep 607: ep_len:500 episode reward: total was -19.590000. running mean: -55.891864\n",
      "ep 607: ep_len:64 episode reward: total was -29.310000. running mean: -55.626046\n",
      "ep 607: ep_len:185 episode reward: total was 6.080000. running mean: -55.008985\n",
      "ep 607: ep_len:599 episode reward: total was -69.470000. running mean: -55.153595\n",
      "epsilon:0.173045 episode_count: 4256. steps_count: 1867687.000000\n",
      "Time elapsed:  5339.8113925457\n",
      "ep 608: ep_len:550 episode reward: total was -34.670000. running mean: -54.948759\n",
      "ep 608: ep_len:562 episode reward: total was -74.670000. running mean: -55.145972\n",
      "ep 608: ep_len:603 episode reward: total was -157.460000. running mean: -56.169112\n",
      "ep 608: ep_len:501 episode reward: total was -110.730000. running mean: -56.714721\n",
      "ep 608: ep_len:3 episode reward: total was 0.000000. running mean: -56.147574\n",
      "ep 608: ep_len:575 episode reward: total was -62.820000. running mean: -56.214298\n",
      "ep 608: ep_len:503 episode reward: total was -73.340000. running mean: -56.385555\n",
      "epsilon:0.173001 episode_count: 4263. steps_count: 1870984.000000\n",
      "Time elapsed:  5351.390187740326\n",
      "ep 609: ep_len:516 episode reward: total was -26.550000. running mean: -56.087200\n",
      "ep 609: ep_len:500 episode reward: total was -81.260000. running mean: -56.338928\n",
      "ep 609: ep_len:637 episode reward: total was -134.620000. running mean: -57.121738\n",
      "ep 609: ep_len:503 episode reward: total was -38.050000. running mean: -56.931021\n",
      "ep 609: ep_len:3 episode reward: total was 0.000000. running mean: -56.361711\n",
      "ep 609: ep_len:601 episode reward: total was -104.240000. running mean: -56.840494\n",
      "ep 609: ep_len:503 episode reward: total was -51.950000. running mean: -56.791589\n",
      "epsilon:0.172957 episode_count: 4270. steps_count: 1874247.000000\n",
      "Time elapsed:  5361.520463228226\n",
      "ep 610: ep_len:500 episode reward: total was -1.060000. running mean: -56.234273\n",
      "ep 610: ep_len:518 episode reward: total was -98.280000. running mean: -56.654730\n",
      "ep 610: ep_len:626 episode reward: total was -103.880000. running mean: -57.126983\n",
      "ep 610: ep_len:130 episode reward: total was 1.490000. running mean: -56.540813\n",
      "ep 610: ep_len:3 episode reward: total was 0.000000. running mean: -55.975405\n",
      "ep 610: ep_len:323 episode reward: total was -59.260000. running mean: -56.008251\n",
      "ep 610: ep_len:307 episode reward: total was -32.610000. running mean: -55.774268\n",
      "epsilon:0.172912 episode_count: 4277. steps_count: 1876654.000000\n",
      "Time elapsed:  5368.943578243256\n",
      "ep 611: ep_len:575 episode reward: total was -22.650000. running mean: -55.443026\n",
      "ep 611: ep_len:543 episode reward: total was -20.240000. running mean: -55.090995\n",
      "ep 611: ep_len:500 episode reward: total was -29.980000. running mean: -54.839885\n",
      "ep 611: ep_len:500 episode reward: total was -16.730000. running mean: -54.458786\n",
      "ep 611: ep_len:41 episode reward: total was 11.500000. running mean: -53.799199\n",
      "ep 611: ep_len:599 episode reward: total was -50.570000. running mean: -53.766907\n",
      "ep 611: ep_len:525 episode reward: total was -88.020000. running mean: -54.109438\n",
      "epsilon:0.172868 episode_count: 4284. steps_count: 1879937.000000\n",
      "Time elapsed:  5378.760901689529\n",
      "ep 612: ep_len:525 episode reward: total was -128.450000. running mean: -54.852843\n",
      "ep 612: ep_len:591 episode reward: total was -56.180000. running mean: -54.866115\n",
      "ep 612: ep_len:620 episode reward: total was -111.380000. running mean: -55.431254\n",
      "ep 612: ep_len:607 episode reward: total was -166.130000. running mean: -56.538241\n",
      "ep 612: ep_len:102 episode reward: total was 11.760000. running mean: -55.855259\n",
      "ep 612: ep_len:641 episode reward: total was -57.520000. running mean: -55.871906\n",
      "ep 612: ep_len:514 episode reward: total was -75.320000. running mean: -56.066387\n",
      "epsilon:0.172824 episode_count: 4291. steps_count: 1883537.000000\n",
      "Time elapsed:  5389.557667970657\n",
      "ep 613: ep_len:635 episode reward: total was -57.790000. running mean: -56.083623\n",
      "ep 613: ep_len:537 episode reward: total was -46.640000. running mean: -55.989187\n",
      "ep 613: ep_len:532 episode reward: total was -109.790000. running mean: -56.527195\n",
      "ep 613: ep_len:533 episode reward: total was -76.910000. running mean: -56.731023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 613: ep_len:55 episode reward: total was -21.500000. running mean: -56.378713\n",
      "ep 613: ep_len:530 episode reward: total was -58.810000. running mean: -56.403026\n",
      "ep 613: ep_len:510 episode reward: total was -39.210000. running mean: -56.231095\n",
      "epsilon:0.172779 episode_count: 4298. steps_count: 1886869.000000\n",
      "Time elapsed:  5398.7122848033905\n",
      "ep 614: ep_len:500 episode reward: total was 18.620000. running mean: -55.482584\n",
      "ep 614: ep_len:560 episode reward: total was -11.300000. running mean: -55.040759\n",
      "ep 614: ep_len:349 episode reward: total was -32.990000. running mean: -54.820251\n",
      "ep 614: ep_len:533 episode reward: total was -25.060000. running mean: -54.522649\n",
      "ep 614: ep_len:3 episode reward: total was 0.000000. running mean: -53.977422\n",
      "ep 614: ep_len:505 episode reward: total was -81.920000. running mean: -54.256848\n",
      "ep 614: ep_len:503 episode reward: total was -75.370000. running mean: -54.467979\n",
      "epsilon:0.172735 episode_count: 4305. steps_count: 1889822.000000\n",
      "Time elapsed:  5407.6671459674835\n",
      "ep 615: ep_len:211 episode reward: total was -4.360000. running mean: -53.966900\n",
      "ep 615: ep_len:532 episode reward: total was -24.380000. running mean: -53.671031\n",
      "ep 615: ep_len:620 episode reward: total was -102.050000. running mean: -54.154820\n",
      "ep 615: ep_len:56 episode reward: total was -6.220000. running mean: -53.675472\n",
      "ep 615: ep_len:50 episode reward: total was 10.000000. running mean: -53.038717\n",
      "ep 615: ep_len:304 episode reward: total was -19.630000. running mean: -52.704630\n",
      "ep 615: ep_len:503 episode reward: total was -87.040000. running mean: -53.047984\n",
      "epsilon:0.172691 episode_count: 4312. steps_count: 1892098.000000\n",
      "Time elapsed:  5414.4442439079285\n",
      "ep 616: ep_len:528 episode reward: total was -13.950000. running mean: -52.657004\n",
      "ep 616: ep_len:561 episode reward: total was 8.270000. running mean: -52.047734\n",
      "ep 616: ep_len:408 episode reward: total was -13.380000. running mean: -51.661057\n",
      "ep 616: ep_len:500 episode reward: total was -74.440000. running mean: -51.888846\n",
      "ep 616: ep_len:3 episode reward: total was 0.000000. running mean: -51.369958\n",
      "ep 616: ep_len:500 episode reward: total was -65.480000. running mean: -51.511058\n",
      "ep 616: ep_len:288 episode reward: total was -41.360000. running mean: -51.409547\n",
      "epsilon:0.172646 episode_count: 4319. steps_count: 1894886.000000\n",
      "Time elapsed:  5422.02817773819\n",
      "ep 617: ep_len:599 episode reward: total was -15.500000. running mean: -51.050452\n",
      "ep 617: ep_len:294 episode reward: total was -57.200000. running mean: -51.111947\n",
      "ep 617: ep_len:634 episode reward: total was -59.440000. running mean: -51.195228\n",
      "ep 617: ep_len:508 episode reward: total was -111.610000. running mean: -51.799376\n",
      "ep 617: ep_len:3 episode reward: total was -1.500000. running mean: -51.296382\n",
      "ep 617: ep_len:674 episode reward: total was -69.420000. running mean: -51.477618\n",
      "ep 617: ep_len:579 episode reward: total was -51.630000. running mean: -51.479142\n",
      "epsilon:0.172602 episode_count: 4326. steps_count: 1898177.000000\n",
      "Time elapsed:  5430.611618757248\n",
      "ep 618: ep_len:504 episode reward: total was -108.170000. running mean: -52.046051\n",
      "ep 618: ep_len:294 episode reward: total was -41.040000. running mean: -51.935990\n",
      "ep 618: ep_len:511 episode reward: total was -72.990000. running mean: -52.146530\n",
      "ep 618: ep_len:56 episode reward: total was -14.710000. running mean: -51.772165\n",
      "ep 618: ep_len:3 episode reward: total was 0.000000. running mean: -51.254443\n",
      "ep 618: ep_len:500 episode reward: total was -52.530000. running mean: -51.267199\n",
      "ep 618: ep_len:613 episode reward: total was -71.710000. running mean: -51.471627\n",
      "epsilon:0.172558 episode_count: 4333. steps_count: 1900658.000000\n",
      "Time elapsed:  5437.519434928894\n",
      "ep 619: ep_len:544 episode reward: total was -38.180000. running mean: -51.338711\n",
      "ep 619: ep_len:581 episode reward: total was -85.510000. running mean: -51.680423\n",
      "ep 619: ep_len:627 episode reward: total was -91.640000. running mean: -52.080019\n",
      "ep 619: ep_len:88 episode reward: total was -7.230000. running mean: -51.631519\n",
      "ep 619: ep_len:3 episode reward: total was -3.000000. running mean: -51.145204\n",
      "ep 619: ep_len:684 episode reward: total was -57.930000. running mean: -51.213052\n",
      "ep 619: ep_len:564 episode reward: total was -57.280000. running mean: -51.273721\n",
      "epsilon:0.172513 episode_count: 4340. steps_count: 1903749.000000\n",
      "Time elapsed:  5446.2255845069885\n",
      "ep 620: ep_len:500 episode reward: total was 0.500000. running mean: -50.755984\n",
      "ep 620: ep_len:500 episode reward: total was -101.070000. running mean: -51.259124\n",
      "ep 620: ep_len:635 episode reward: total was -54.790000. running mean: -51.294433\n",
      "ep 620: ep_len:500 episode reward: total was -30.020000. running mean: -51.081689\n",
      "ep 620: ep_len:3 episode reward: total was -1.500000. running mean: -50.585872\n",
      "ep 620: ep_len:530 episode reward: total was -57.150000. running mean: -50.651513\n",
      "ep 620: ep_len:575 episode reward: total was -50.920000. running mean: -50.654198\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.172469 episode_count: 4347. steps_count: 1906992.000000\n",
      "Time elapsed:  5459.728956699371\n",
      "ep 621: ep_len:131 episode reward: total was -4.110000. running mean: -50.188756\n",
      "ep 621: ep_len:509 episode reward: total was -77.160000. running mean: -50.458468\n",
      "ep 621: ep_len:501 episode reward: total was -37.350000. running mean: -50.327384\n",
      "ep 621: ep_len:128 episode reward: total was -9.000000. running mean: -49.914110\n",
      "ep 621: ep_len:3 episode reward: total was 0.000000. running mean: -49.414969\n",
      "ep 621: ep_len:519 episode reward: total was -100.970000. running mean: -49.930519\n",
      "ep 621: ep_len:508 episode reward: total was -70.030000. running mean: -50.131514\n",
      "epsilon:0.172425 episode_count: 4354. steps_count: 1909291.000000\n",
      "Time elapsed:  5466.0694398880005\n",
      "ep 622: ep_len:664 episode reward: total was -102.710000. running mean: -50.657299\n",
      "ep 622: ep_len:628 episode reward: total was -17.730000. running mean: -50.328026\n",
      "ep 622: ep_len:500 episode reward: total was -64.030000. running mean: -50.465045\n",
      "ep 622: ep_len:589 episode reward: total was -30.860000. running mean: -50.268995\n",
      "ep 622: ep_len:48 episode reward: total was 15.000000. running mean: -49.616305\n",
      "ep 622: ep_len:500 episode reward: total was -80.760000. running mean: -49.927742\n",
      "ep 622: ep_len:529 episode reward: total was -57.340000. running mean: -50.001865\n",
      "epsilon:0.172380 episode_count: 4361. steps_count: 1912749.000000\n",
      "Time elapsed:  5476.232243776321\n",
      "ep 623: ep_len:628 episode reward: total was -135.920000. running mean: -50.861046\n",
      "ep 623: ep_len:500 episode reward: total was -53.920000. running mean: -50.891635\n",
      "ep 623: ep_len:642 episode reward: total was -140.540000. running mean: -51.788119\n",
      "ep 623: ep_len:513 episode reward: total was -59.710000. running mean: -51.867338\n",
      "ep 623: ep_len:109 episode reward: total was -22.760000. running mean: -51.576265\n",
      "ep 623: ep_len:562 episode reward: total was -49.360000. running mean: -51.554102\n",
      "ep 623: ep_len:514 episode reward: total was -60.550000. running mean: -51.644061\n",
      "epsilon:0.172336 episode_count: 4368. steps_count: 1916217.000000\n",
      "Time elapsed:  5485.806401014328\n",
      "ep 624: ep_len:644 episode reward: total was -149.000000. running mean: -52.617620\n",
      "ep 624: ep_len:604 episode reward: total was -140.950000. running mean: -53.500944\n",
      "ep 624: ep_len:647 episode reward: total was -107.250000. running mean: -54.038435\n",
      "ep 624: ep_len:572 episode reward: total was -24.230000. running mean: -53.740350\n",
      "ep 624: ep_len:3 episode reward: total was 0.000000. running mean: -53.202947\n",
      "ep 624: ep_len:600 episode reward: total was -115.600000. running mean: -53.826917\n",
      "ep 624: ep_len:305 episode reward: total was -57.730000. running mean: -53.865948\n",
      "epsilon:0.172292 episode_count: 4375. steps_count: 1919592.000000\n",
      "Time elapsed:  5495.006273269653\n",
      "ep 625: ep_len:265 episode reward: total was -21.290000. running mean: -53.540189\n",
      "ep 625: ep_len:610 episode reward: total was 2.200000. running mean: -52.982787\n",
      "ep 625: ep_len:579 episode reward: total was -92.320000. running mean: -53.376159\n",
      "ep 625: ep_len:534 episode reward: total was -59.030000. running mean: -53.432697\n",
      "ep 625: ep_len:3 episode reward: total was 0.000000. running mean: -52.898370\n",
      "ep 625: ep_len:500 episode reward: total was -43.180000. running mean: -52.801187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 625: ep_len:500 episode reward: total was -27.690000. running mean: -52.550075\n",
      "epsilon:0.172247 episode_count: 4382. steps_count: 1922583.000000\n",
      "Time elapsed:  5502.810426950455\n",
      "ep 626: ep_len:519 episode reward: total was -33.390000. running mean: -52.358474\n",
      "ep 626: ep_len:529 episode reward: total was -46.430000. running mean: -52.299189\n",
      "ep 626: ep_len:556 episode reward: total was -95.450000. running mean: -52.730697\n",
      "ep 626: ep_len:586 episode reward: total was -15.310000. running mean: -52.356490\n",
      "ep 626: ep_len:3 episode reward: total was -1.500000. running mean: -51.847926\n",
      "ep 626: ep_len:513 episode reward: total was -58.240000. running mean: -51.911846\n",
      "ep 626: ep_len:570 episode reward: total was -69.060000. running mean: -52.083328\n",
      "epsilon:0.172203 episode_count: 4389. steps_count: 1925859.000000\n",
      "Time elapsed:  5511.456627368927\n",
      "ep 627: ep_len:213 episode reward: total was -10.000000. running mean: -51.662495\n",
      "ep 627: ep_len:500 episode reward: total was -76.570000. running mean: -51.911570\n",
      "ep 627: ep_len:539 episode reward: total was -125.660000. running mean: -52.649054\n",
      "ep 627: ep_len:618 episode reward: total was -91.410000. running mean: -53.036663\n",
      "ep 627: ep_len:3 episode reward: total was 0.000000. running mean: -52.506297\n",
      "ep 627: ep_len:505 episode reward: total was -93.050000. running mean: -52.911734\n",
      "ep 627: ep_len:555 episode reward: total was -62.380000. running mean: -53.006416\n",
      "epsilon:0.172159 episode_count: 4396. steps_count: 1928792.000000\n",
      "Time elapsed:  5519.155301094055\n",
      "ep 628: ep_len:265 episode reward: total was -21.890000. running mean: -52.695252\n",
      "ep 628: ep_len:610 episode reward: total was -67.050000. running mean: -52.838800\n",
      "ep 628: ep_len:593 episode reward: total was -114.350000. running mean: -53.453912\n",
      "ep 628: ep_len:116 episode reward: total was 7.470000. running mean: -52.844673\n",
      "ep 628: ep_len:54 episode reward: total was 14.510000. running mean: -52.171126\n",
      "ep 628: ep_len:500 episode reward: total was -84.070000. running mean: -52.490115\n",
      "ep 628: ep_len:285 episode reward: total was -38.320000. running mean: -52.348413\n",
      "epsilon:0.172114 episode_count: 4403. steps_count: 1931215.000000\n",
      "Time elapsed:  5526.065616130829\n",
      "ep 629: ep_len:512 episode reward: total was -96.680000. running mean: -52.791729\n",
      "ep 629: ep_len:193 episode reward: total was -26.190000. running mean: -52.525712\n",
      "ep 629: ep_len:535 episode reward: total was -111.920000. running mean: -53.119655\n",
      "ep 629: ep_len:515 episode reward: total was -38.630000. running mean: -52.974758\n",
      "ep 629: ep_len:92 episode reward: total was 9.240000. running mean: -52.352611\n",
      "ep 629: ep_len:500 episode reward: total was -20.080000. running mean: -52.029885\n",
      "ep 629: ep_len:532 episode reward: total was -82.940000. running mean: -52.338986\n",
      "epsilon:0.172070 episode_count: 4410. steps_count: 1934094.000000\n",
      "Time elapsed:  5533.792235136032\n",
      "ep 630: ep_len:535 episode reward: total was -59.090000. running mean: -52.406496\n",
      "ep 630: ep_len:510 episode reward: total was -82.800000. running mean: -52.710431\n",
      "ep 630: ep_len:543 episode reward: total was -59.140000. running mean: -52.774727\n",
      "ep 630: ep_len:500 episode reward: total was -73.970000. running mean: -52.986679\n",
      "ep 630: ep_len:3 episode reward: total was 0.000000. running mean: -52.456813\n",
      "ep 630: ep_len:170 episode reward: total was 2.040000. running mean: -51.911845\n",
      "ep 630: ep_len:615 episode reward: total was -64.720000. running mean: -52.039926\n",
      "epsilon:0.172026 episode_count: 4417. steps_count: 1936970.000000\n",
      "Time elapsed:  5541.445154666901\n",
      "ep 631: ep_len:134 episode reward: total was -20.150000. running mean: -51.721027\n",
      "ep 631: ep_len:500 episode reward: total was -45.220000. running mean: -51.656017\n",
      "ep 631: ep_len:581 episode reward: total was -110.170000. running mean: -52.241156\n",
      "ep 631: ep_len:500 episode reward: total was 26.770000. running mean: -51.451045\n",
      "ep 631: ep_len:46 episode reward: total was 12.500000. running mean: -50.811534\n",
      "ep 631: ep_len:500 episode reward: total was -51.590000. running mean: -50.819319\n",
      "ep 631: ep_len:282 episode reward: total was -81.790000. running mean: -51.129026\n",
      "epsilon:0.171981 episode_count: 4424. steps_count: 1939513.000000\n",
      "Time elapsed:  5548.557371139526\n",
      "ep 632: ep_len:628 episode reward: total was -36.960000. running mean: -50.987336\n",
      "ep 632: ep_len:500 episode reward: total was -26.330000. running mean: -50.740762\n",
      "ep 632: ep_len:514 episode reward: total was -68.570000. running mean: -50.919055\n",
      "ep 632: ep_len:582 episode reward: total was -38.580000. running mean: -50.795664\n",
      "ep 632: ep_len:3 episode reward: total was 0.000000. running mean: -50.287707\n",
      "ep 632: ep_len:644 episode reward: total was -147.560000. running mean: -51.260430\n",
      "ep 632: ep_len:500 episode reward: total was -27.710000. running mean: -51.024926\n",
      "epsilon:0.171937 episode_count: 4431. steps_count: 1942884.000000\n",
      "Time elapsed:  5557.279135227203\n",
      "ep 633: ep_len:500 episode reward: total was -42.470000. running mean: -50.939377\n",
      "ep 633: ep_len:500 episode reward: total was -122.620000. running mean: -51.656183\n",
      "ep 633: ep_len:502 episode reward: total was -37.930000. running mean: -51.518921\n",
      "ep 633: ep_len:500 episode reward: total was 25.240000. running mean: -50.751332\n",
      "ep 633: ep_len:3 episode reward: total was 0.000000. running mean: -50.243819\n",
      "ep 633: ep_len:260 episode reward: total was 2.200000. running mean: -49.719380\n",
      "ep 633: ep_len:293 episode reward: total was -57.300000. running mean: -49.795187\n",
      "epsilon:0.171893 episode_count: 4438. steps_count: 1945442.000000\n",
      "Time elapsed:  5564.340331792831\n",
      "ep 634: ep_len:514 episode reward: total was -13.560000. running mean: -49.432835\n",
      "ep 634: ep_len:579 episode reward: total was -9.420000. running mean: -49.032706\n",
      "ep 634: ep_len:567 episode reward: total was -77.160000. running mean: -49.313979\n",
      "ep 634: ep_len:508 episode reward: total was -15.850000. running mean: -48.979340\n",
      "ep 634: ep_len:3 episode reward: total was -1.500000. running mean: -48.504546\n",
      "ep 634: ep_len:500 episode reward: total was -77.520000. running mean: -48.794701\n",
      "ep 634: ep_len:210 episode reward: total was -40.620000. running mean: -48.712954\n",
      "epsilon:0.171848 episode_count: 4445. steps_count: 1948323.000000\n",
      "Time elapsed:  5571.977568387985\n",
      "ep 635: ep_len:500 episode reward: total was -25.480000. running mean: -48.480624\n",
      "ep 635: ep_len:533 episode reward: total was -82.420000. running mean: -48.820018\n",
      "ep 635: ep_len:598 episode reward: total was -72.400000. running mean: -49.055818\n",
      "ep 635: ep_len:532 episode reward: total was -68.390000. running mean: -49.249160\n",
      "ep 635: ep_len:99 episode reward: total was -46.800000. running mean: -49.224668\n",
      "ep 635: ep_len:569 episode reward: total was -102.380000. running mean: -49.756221\n",
      "ep 635: ep_len:500 episode reward: total was -125.320000. running mean: -50.511859\n",
      "epsilon:0.171804 episode_count: 4452. steps_count: 1951654.000000\n",
      "Time elapsed:  5580.700523614883\n",
      "ep 636: ep_len:575 episode reward: total was -68.660000. running mean: -50.693340\n",
      "ep 636: ep_len:533 episode reward: total was -57.420000. running mean: -50.760607\n",
      "ep 636: ep_len:500 episode reward: total was -101.030000. running mean: -51.263301\n",
      "ep 636: ep_len:378 episode reward: total was -50.810000. running mean: -51.258768\n",
      "ep 636: ep_len:84 episode reward: total was 3.740000. running mean: -50.708780\n",
      "ep 636: ep_len:633 episode reward: total was -56.310000. running mean: -50.764793\n",
      "ep 636: ep_len:502 episode reward: total was -66.530000. running mean: -50.922445\n",
      "epsilon:0.171760 episode_count: 4459. steps_count: 1954859.000000\n",
      "Time elapsed:  5589.192056179047\n",
      "ep 637: ep_len:660 episode reward: total was -92.710000. running mean: -51.340320\n",
      "ep 637: ep_len:559 episode reward: total was -85.940000. running mean: -51.686317\n",
      "ep 637: ep_len:584 episode reward: total was -106.430000. running mean: -52.233754\n",
      "ep 637: ep_len:511 episode reward: total was -94.870000. running mean: -52.660116\n",
      "ep 637: ep_len:97 episode reward: total was -65.260000. running mean: -52.786115\n",
      "ep 637: ep_len:500 episode reward: total was -44.490000. running mean: -52.703154\n",
      "ep 637: ep_len:516 episode reward: total was -54.730000. running mean: -52.723422\n",
      "epsilon:0.171715 episode_count: 4466. steps_count: 1958286.000000\n",
      "Time elapsed:  5598.525159597397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 638: ep_len:246 episode reward: total was -16.830000. running mean: -52.364488\n",
      "ep 638: ep_len:500 episode reward: total was -43.690000. running mean: -52.277743\n",
      "ep 638: ep_len:563 episode reward: total was -75.610000. running mean: -52.511066\n",
      "ep 638: ep_len:510 episode reward: total was -59.370000. running mean: -52.579655\n",
      "ep 638: ep_len:53 episode reward: total was 13.000000. running mean: -51.923859\n",
      "ep 638: ep_len:501 episode reward: total was -63.240000. running mean: -52.037020\n",
      "ep 638: ep_len:503 episode reward: total was -49.790000. running mean: -52.014550\n",
      "epsilon:0.171671 episode_count: 4473. steps_count: 1961162.000000\n",
      "Time elapsed:  5606.335381031036\n",
      "ep 639: ep_len:178 episode reward: total was 4.550000. running mean: -51.448904\n",
      "ep 639: ep_len:188 episode reward: total was -18.160000. running mean: -51.116015\n",
      "ep 639: ep_len:539 episode reward: total was -65.690000. running mean: -51.261755\n",
      "ep 639: ep_len:522 episode reward: total was -47.880000. running mean: -51.227938\n",
      "ep 639: ep_len:3 episode reward: total was 0.000000. running mean: -50.715658\n",
      "ep 639: ep_len:510 episode reward: total was -82.380000. running mean: -51.032302\n",
      "ep 639: ep_len:528 episode reward: total was -139.900000. running mean: -51.920979\n",
      "epsilon:0.171627 episode_count: 4480. steps_count: 1963630.000000\n",
      "Time elapsed:  5612.212673902512\n",
      "ep 640: ep_len:564 episode reward: total was -129.260000. running mean: -52.694369\n",
      "ep 640: ep_len:571 episode reward: total was -142.240000. running mean: -53.589825\n",
      "ep 640: ep_len:409 episode reward: total was -1.530000. running mean: -53.069227\n",
      "ep 640: ep_len:501 episode reward: total was -12.860000. running mean: -52.667135\n",
      "ep 640: ep_len:3 episode reward: total was 0.000000. running mean: -52.140463\n",
      "ep 640: ep_len:638 episode reward: total was -75.070000. running mean: -52.369759\n",
      "ep 640: ep_len:625 episode reward: total was -81.950000. running mean: -52.665561\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.171582 episode_count: 4487. steps_count: 1966941.000000\n",
      "Time elapsed:  5625.7137179374695\n",
      "ep 641: ep_len:552 episode reward: total was -99.630000. running mean: -53.135205\n",
      "ep 641: ep_len:569 episode reward: total was -20.480000. running mean: -52.808653\n",
      "ep 641: ep_len:419 episode reward: total was -42.600000. running mean: -52.706567\n",
      "ep 641: ep_len:510 episode reward: total was -131.420000. running mean: -53.493701\n",
      "ep 641: ep_len:3 episode reward: total was 0.000000. running mean: -52.958764\n",
      "ep 641: ep_len:537 episode reward: total was -31.210000. running mean: -52.741277\n",
      "ep 641: ep_len:511 episode reward: total was -61.960000. running mean: -52.833464\n",
      "epsilon:0.171538 episode_count: 4494. steps_count: 1970042.000000\n",
      "Time elapsed:  5634.397451400757\n",
      "ep 642: ep_len:500 episode reward: total was -20.380000. running mean: -52.508929\n",
      "ep 642: ep_len:531 episode reward: total was -60.070000. running mean: -52.584540\n",
      "ep 642: ep_len:59 episode reward: total was 1.070000. running mean: -52.047994\n",
      "ep 642: ep_len:588 episode reward: total was -37.350000. running mean: -51.901015\n",
      "ep 642: ep_len:55 episode reward: total was 14.000000. running mean: -51.242004\n",
      "ep 642: ep_len:680 episode reward: total was -60.140000. running mean: -51.330984\n",
      "ep 642: ep_len:504 episode reward: total was -126.880000. running mean: -52.086474\n",
      "epsilon:0.171494 episode_count: 4501. steps_count: 1972959.000000\n",
      "Time elapsed:  5641.939159631729\n",
      "ep 643: ep_len:512 episode reward: total was -72.380000. running mean: -52.289410\n",
      "ep 643: ep_len:500 episode reward: total was -40.790000. running mean: -52.174416\n",
      "ep 643: ep_len:560 episode reward: total was -50.280000. running mean: -52.155471\n",
      "ep 643: ep_len:161 episode reward: total was -7.470000. running mean: -51.708617\n",
      "ep 643: ep_len:3 episode reward: total was 0.000000. running mean: -51.191531\n",
      "ep 643: ep_len:500 episode reward: total was -77.520000. running mean: -51.454815\n",
      "ep 643: ep_len:530 episode reward: total was -57.390000. running mean: -51.514167\n",
      "epsilon:0.171449 episode_count: 4508. steps_count: 1975725.000000\n",
      "Time elapsed:  5650.094811201096\n",
      "ep 644: ep_len:193 episode reward: total was -13.080000. running mean: -51.129825\n",
      "ep 644: ep_len:500 episode reward: total was 3.490000. running mean: -50.583627\n",
      "ep 644: ep_len:538 episode reward: total was -93.910000. running mean: -51.016891\n",
      "ep 644: ep_len:397 episode reward: total was -71.290000. running mean: -51.219622\n",
      "ep 644: ep_len:3 episode reward: total was -3.000000. running mean: -50.737426\n",
      "ep 644: ep_len:530 episode reward: total was -88.070000. running mean: -51.110752\n",
      "ep 644: ep_len:577 episode reward: total was -95.730000. running mean: -51.556944\n",
      "epsilon:0.171405 episode_count: 4515. steps_count: 1978463.000000\n",
      "Time elapsed:  5657.812314033508\n",
      "ep 645: ep_len:511 episode reward: total was -148.890000. running mean: -52.530275\n",
      "ep 645: ep_len:501 episode reward: total was -88.880000. running mean: -52.893772\n",
      "ep 645: ep_len:556 episode reward: total was -71.910000. running mean: -53.083934\n",
      "ep 645: ep_len:500 episode reward: total was -24.960000. running mean: -52.802695\n",
      "ep 645: ep_len:94 episode reward: total was -4.790000. running mean: -52.322568\n",
      "ep 645: ep_len:519 episode reward: total was -55.580000. running mean: -52.355142\n",
      "ep 645: ep_len:632 episode reward: total was -53.710000. running mean: -52.368691\n",
      "epsilon:0.171361 episode_count: 4522. steps_count: 1981776.000000\n",
      "Time elapsed:  5667.165876865387\n",
      "ep 646: ep_len:229 episode reward: total was -10.850000. running mean: -51.953504\n",
      "ep 646: ep_len:608 episode reward: total was -38.650000. running mean: -51.820469\n",
      "ep 646: ep_len:501 episode reward: total was -39.430000. running mean: -51.696564\n",
      "ep 646: ep_len:502 episode reward: total was -88.440000. running mean: -52.063998\n",
      "ep 646: ep_len:100 episode reward: total was -0.810000. running mean: -51.551458\n",
      "ep 646: ep_len:502 episode reward: total was -136.960000. running mean: -52.405544\n",
      "ep 646: ep_len:500 episode reward: total was -83.870000. running mean: -52.720188\n",
      "epsilon:0.171316 episode_count: 4529. steps_count: 1984718.000000\n",
      "Time elapsed:  5675.220338106155\n",
      "ep 647: ep_len:500 episode reward: total was -22.610000. running mean: -52.419087\n",
      "ep 647: ep_len:530 episode reward: total was -74.260000. running mean: -52.637496\n",
      "ep 647: ep_len:605 episode reward: total was -95.580000. running mean: -53.066921\n",
      "ep 647: ep_len:519 episode reward: total was -59.890000. running mean: -53.135152\n",
      "ep 647: ep_len:111 episode reward: total was -2.780000. running mean: -52.631600\n",
      "ep 647: ep_len:155 episode reward: total was 1.950000. running mean: -52.085784\n",
      "ep 647: ep_len:559 episode reward: total was -56.940000. running mean: -52.134326\n",
      "epsilon:0.171272 episode_count: 4536. steps_count: 1987697.000000\n",
      "Time elapsed:  5683.208874702454\n",
      "ep 648: ep_len:646 episode reward: total was -49.260000. running mean: -52.105583\n",
      "ep 648: ep_len:368 episode reward: total was -85.210000. running mean: -52.436627\n",
      "ep 648: ep_len:542 episode reward: total was -152.380000. running mean: -53.436061\n",
      "ep 648: ep_len:531 episode reward: total was -80.770000. running mean: -53.709400\n",
      "ep 648: ep_len:3 episode reward: total was -1.500000. running mean: -53.187306\n",
      "ep 648: ep_len:614 episode reward: total was -64.780000. running mean: -53.303233\n",
      "ep 648: ep_len:510 episode reward: total was -67.260000. running mean: -53.442801\n",
      "epsilon:0.171228 episode_count: 4543. steps_count: 1990911.000000\n",
      "Time elapsed:  5691.754508256912\n",
      "ep 649: ep_len:500 episode reward: total was -28.910000. running mean: -53.197473\n",
      "ep 649: ep_len:588 episode reward: total was -62.350000. running mean: -53.288998\n",
      "ep 649: ep_len:582 episode reward: total was -157.270000. running mean: -54.328808\n",
      "ep 649: ep_len:534 episode reward: total was -54.110000. running mean: -54.326620\n",
      "ep 649: ep_len:42 episode reward: total was 9.000000. running mean: -53.693354\n",
      "ep 649: ep_len:501 episode reward: total was -41.390000. running mean: -53.570320\n",
      "ep 649: ep_len:567 episode reward: total was -93.330000. running mean: -53.967917\n",
      "epsilon:0.171183 episode_count: 4550. steps_count: 1994225.000000\n",
      "Time elapsed:  5700.398869752884\n",
      "ep 650: ep_len:134 episode reward: total was -12.160000. running mean: -53.549838\n",
      "ep 650: ep_len:500 episode reward: total was -61.550000. running mean: -53.629840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 650: ep_len:672 episode reward: total was -86.640000. running mean: -53.959941\n",
      "ep 650: ep_len:509 episode reward: total was -65.200000. running mean: -54.072342\n",
      "ep 650: ep_len:74 episode reward: total was -36.770000. running mean: -53.899318\n",
      "ep 650: ep_len:633 episode reward: total was -82.050000. running mean: -54.180825\n",
      "ep 650: ep_len:557 episode reward: total was -92.010000. running mean: -54.559117\n",
      "epsilon:0.171139 episode_count: 4557. steps_count: 1997304.000000\n",
      "Time elapsed:  5708.853116750717\n",
      "ep 651: ep_len:563 episode reward: total was -132.810000. running mean: -55.341626\n",
      "ep 651: ep_len:599 episode reward: total was -129.050000. running mean: -56.078709\n",
      "ep 651: ep_len:79 episode reward: total was -4.760000. running mean: -55.565522\n",
      "ep 651: ep_len:500 episode reward: total was -12.860000. running mean: -55.138467\n",
      "ep 651: ep_len:3 episode reward: total was 1.010000. running mean: -54.576982\n",
      "ep 651: ep_len:559 episode reward: total was -109.480000. running mean: -55.126013\n",
      "ep 651: ep_len:549 episode reward: total was -64.320000. running mean: -55.217953\n",
      "epsilon:0.171095 episode_count: 4564. steps_count: 2000156.000000\n",
      "Time elapsed:  5716.510495662689\n",
      "ep 652: ep_len:564 episode reward: total was -47.900000. running mean: -55.144773\n",
      "ep 652: ep_len:553 episode reward: total was -61.080000. running mean: -55.204125\n",
      "ep 652: ep_len:500 episode reward: total was -85.530000. running mean: -55.507384\n",
      "ep 652: ep_len:625 episode reward: total was -36.280000. running mean: -55.315110\n",
      "ep 652: ep_len:108 episode reward: total was 11.210000. running mean: -54.649859\n",
      "ep 652: ep_len:615 episode reward: total was -90.020000. running mean: -55.003560\n",
      "ep 652: ep_len:500 episode reward: total was -42.290000. running mean: -54.876425\n",
      "epsilon:0.171050 episode_count: 4571. steps_count: 2003621.000000\n",
      "Time elapsed:  5725.755711078644\n",
      "ep 653: ep_len:121 episode reward: total was -19.090000. running mean: -54.518561\n",
      "ep 653: ep_len:549 episode reward: total was -73.730000. running mean: -54.710675\n",
      "ep 653: ep_len:616 episode reward: total was -90.650000. running mean: -55.070068\n",
      "ep 653: ep_len:516 episode reward: total was -58.950000. running mean: -55.108868\n",
      "ep 653: ep_len:111 episode reward: total was 16.750000. running mean: -54.390279\n",
      "ep 653: ep_len:616 episode reward: total was -59.930000. running mean: -54.445676\n",
      "ep 653: ep_len:354 episode reward: total was -63.690000. running mean: -54.538119\n",
      "epsilon:0.171006 episode_count: 4578. steps_count: 2006504.000000\n",
      "Time elapsed:  5733.437720537186\n",
      "ep 654: ep_len:112 episode reward: total was 1.360000. running mean: -53.979138\n",
      "ep 654: ep_len:552 episode reward: total was -3.040000. running mean: -53.469747\n",
      "ep 654: ep_len:500 episode reward: total was -57.690000. running mean: -53.511949\n",
      "ep 654: ep_len:570 episode reward: total was -21.260000. running mean: -53.189430\n",
      "ep 654: ep_len:3 episode reward: total was 0.000000. running mean: -52.657536\n",
      "ep 654: ep_len:592 episode reward: total was -122.820000. running mean: -53.359160\n",
      "ep 654: ep_len:535 episode reward: total was -118.460000. running mean: -54.010169\n",
      "epsilon:0.170962 episode_count: 4585. steps_count: 2009368.000000\n",
      "Time elapsed:  5741.056505441666\n",
      "ep 655: ep_len:522 episode reward: total was 3.640000. running mean: -53.433667\n",
      "ep 655: ep_len:579 episode reward: total was -63.470000. running mean: -53.534030\n",
      "ep 655: ep_len:529 episode reward: total was -61.340000. running mean: -53.612090\n",
      "ep 655: ep_len:515 episode reward: total was -58.680000. running mean: -53.662769\n",
      "ep 655: ep_len:110 episode reward: total was -21.250000. running mean: -53.338641\n",
      "ep 655: ep_len:500 episode reward: total was -53.710000. running mean: -53.342355\n",
      "ep 655: ep_len:540 episode reward: total was -73.840000. running mean: -53.547331\n",
      "epsilon:0.170917 episode_count: 4592. steps_count: 2012663.000000\n",
      "Time elapsed:  5750.6296536922455\n",
      "ep 656: ep_len:615 episode reward: total was -145.500000. running mean: -54.466858\n",
      "ep 656: ep_len:547 episode reward: total was -96.190000. running mean: -54.884089\n",
      "ep 656: ep_len:659 episode reward: total was -121.300000. running mean: -55.548249\n",
      "ep 656: ep_len:539 episode reward: total was -39.390000. running mean: -55.386666\n",
      "ep 656: ep_len:76 episode reward: total was -9.260000. running mean: -54.925399\n",
      "ep 656: ep_len:630 episode reward: total was -72.000000. running mean: -55.096145\n",
      "ep 656: ep_len:565 episode reward: total was -64.930000. running mean: -55.194484\n",
      "epsilon:0.170873 episode_count: 4599. steps_count: 2016294.000000\n",
      "Time elapsed:  5760.099087238312\n",
      "ep 657: ep_len:615 episode reward: total was -100.720000. running mean: -55.649739\n",
      "ep 657: ep_len:531 episode reward: total was -78.440000. running mean: -55.877642\n",
      "ep 657: ep_len:513 episode reward: total was -47.280000. running mean: -55.791665\n",
      "ep 657: ep_len:113 episode reward: total was -13.560000. running mean: -55.369349\n",
      "ep 657: ep_len:3 episode reward: total was 0.000000. running mean: -54.815655\n",
      "ep 657: ep_len:500 episode reward: total was -36.600000. running mean: -54.633499\n",
      "ep 657: ep_len:332 episode reward: total was -50.780000. running mean: -54.594964\n",
      "epsilon:0.170829 episode_count: 4606. steps_count: 2018901.000000\n",
      "Time elapsed:  5767.344025373459\n",
      "ep 658: ep_len:533 episode reward: total was -108.890000. running mean: -55.137914\n",
      "ep 658: ep_len:616 episode reward: total was -178.830000. running mean: -56.374835\n",
      "ep 658: ep_len:384 episode reward: total was -14.680000. running mean: -55.957887\n",
      "ep 658: ep_len:514 episode reward: total was -72.700000. running mean: -56.125308\n",
      "ep 658: ep_len:3 episode reward: total was -1.500000. running mean: -55.579055\n",
      "ep 658: ep_len:536 episode reward: total was -93.230000. running mean: -55.955564\n",
      "ep 658: ep_len:525 episode reward: total was -57.120000. running mean: -55.967208\n",
      "epsilon:0.170784 episode_count: 4613. steps_count: 2022012.000000\n",
      "Time elapsed:  5775.875613689423\n",
      "ep 659: ep_len:557 episode reward: total was -23.720000. running mean: -55.644736\n",
      "ep 659: ep_len:545 episode reward: total was -49.210000. running mean: -55.580389\n",
      "ep 659: ep_len:583 episode reward: total was -53.210000. running mean: -55.556685\n",
      "ep 659: ep_len:512 episode reward: total was -38.360000. running mean: -55.384718\n",
      "ep 659: ep_len:3 episode reward: total was 0.000000. running mean: -54.830871\n",
      "ep 659: ep_len:259 episode reward: total was -1.210000. running mean: -54.294662\n",
      "ep 659: ep_len:511 episode reward: total was -111.480000. running mean: -54.866516\n",
      "epsilon:0.170740 episode_count: 4620. steps_count: 2024982.000000\n",
      "Time elapsed:  5784.068286895752\n",
      "ep 660: ep_len:601 episode reward: total was -73.710000. running mean: -55.054951\n",
      "ep 660: ep_len:602 episode reward: total was 7.220000. running mean: -54.432201\n",
      "ep 660: ep_len:560 episode reward: total was -76.770000. running mean: -54.655579\n",
      "ep 660: ep_len:500 episode reward: total was -47.700000. running mean: -54.586023\n",
      "ep 660: ep_len:3 episode reward: total was -1.500000. running mean: -54.055163\n",
      "ep 660: ep_len:556 episode reward: total was -65.320000. running mean: -54.167811\n",
      "ep 660: ep_len:596 episode reward: total was -59.250000. running mean: -54.218633\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.170696 episode_count: 4627. steps_count: 2028400.000000\n",
      "Time elapsed:  5798.500341176987\n",
      "ep 661: ep_len:579 episode reward: total was -35.530000. running mean: -54.031747\n",
      "ep 661: ep_len:554 episode reward: total was -110.250000. running mean: -54.593929\n",
      "ep 661: ep_len:500 episode reward: total was -70.540000. running mean: -54.753390\n",
      "ep 661: ep_len:500 episode reward: total was -49.140000. running mean: -54.697256\n",
      "ep 661: ep_len:3 episode reward: total was 0.000000. running mean: -54.150284\n",
      "ep 661: ep_len:252 episode reward: total was -13.680000. running mean: -53.745581\n",
      "ep 661: ep_len:592 episode reward: total was -76.060000. running mean: -53.968725\n",
      "epsilon:0.170651 episode_count: 4634. steps_count: 2031380.000000\n",
      "Time elapsed:  5806.669280052185\n",
      "ep 662: ep_len:255 episode reward: total was -38.830000. running mean: -53.817338\n",
      "ep 662: ep_len:617 episode reward: total was -26.680000. running mean: -53.545964\n",
      "ep 662: ep_len:517 episode reward: total was -99.630000. running mean: -54.006805\n",
      "ep 662: ep_len:116 episode reward: total was -25.490000. running mean: -53.721637\n",
      "ep 662: ep_len:3 episode reward: total was 0.000000. running mean: -53.184420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 662: ep_len:570 episode reward: total was -48.250000. running mean: -53.135076\n",
      "ep 662: ep_len:529 episode reward: total was -59.610000. running mean: -53.199825\n",
      "epsilon:0.170607 episode_count: 4641. steps_count: 2033987.000000\n",
      "Time elapsed:  5813.781964540482\n",
      "ep 663: ep_len:580 episode reward: total was -1.950000. running mean: -52.687327\n",
      "ep 663: ep_len:544 episode reward: total was -38.920000. running mean: -52.549654\n",
      "ep 663: ep_len:381 episode reward: total was -2.280000. running mean: -52.046957\n",
      "ep 663: ep_len:511 episode reward: total was -38.450000. running mean: -51.910988\n",
      "ep 663: ep_len:92 episode reward: total was -11.300000. running mean: -51.504878\n",
      "ep 663: ep_len:572 episode reward: total was -97.270000. running mean: -51.962529\n",
      "ep 663: ep_len:550 episode reward: total was -88.070000. running mean: -52.323604\n",
      "epsilon:0.170563 episode_count: 4648. steps_count: 2037217.000000\n",
      "Time elapsed:  5822.33070063591\n",
      "ep 664: ep_len:117 episode reward: total was 0.310000. running mean: -51.797268\n",
      "ep 664: ep_len:501 episode reward: total was -60.620000. running mean: -51.885495\n",
      "ep 664: ep_len:500 episode reward: total was -36.010000. running mean: -51.726740\n",
      "ep 664: ep_len:56 episode reward: total was -3.710000. running mean: -51.246573\n",
      "ep 664: ep_len:3 episode reward: total was -1.500000. running mean: -50.749107\n",
      "ep 664: ep_len:500 episode reward: total was -75.710000. running mean: -50.998716\n",
      "ep 664: ep_len:201 episode reward: total was -61.890000. running mean: -51.107629\n",
      "epsilon:0.170518 episode_count: 4655. steps_count: 2039095.000000\n",
      "Time elapsed:  5827.897462129593\n",
      "ep 665: ep_len:205 episode reward: total was -118.800000. running mean: -51.784552\n",
      "ep 665: ep_len:500 episode reward: total was -30.020000. running mean: -51.566907\n",
      "ep 665: ep_len:614 episode reward: total was -63.600000. running mean: -51.687238\n",
      "ep 665: ep_len:500 episode reward: total was -69.550000. running mean: -51.865866\n",
      "ep 665: ep_len:55 episode reward: total was 14.000000. running mean: -51.207207\n",
      "ep 665: ep_len:613 episode reward: total was -64.700000. running mean: -51.342135\n",
      "ep 665: ep_len:605 episode reward: total was -52.730000. running mean: -51.356013\n",
      "epsilon:0.170474 episode_count: 4662. steps_count: 2042187.000000\n",
      "Time elapsed:  5836.191558122635\n",
      "ep 666: ep_len:559 episode reward: total was -106.180000. running mean: -51.904253\n",
      "ep 666: ep_len:501 episode reward: total was -15.880000. running mean: -51.544011\n",
      "ep 666: ep_len:418 episode reward: total was -36.640000. running mean: -51.394971\n",
      "ep 666: ep_len:617 episode reward: total was -28.950000. running mean: -51.170521\n",
      "ep 666: ep_len:3 episode reward: total was 0.000000. running mean: -50.658816\n",
      "ep 666: ep_len:501 episode reward: total was -68.790000. running mean: -50.840128\n",
      "ep 666: ep_len:565 episode reward: total was -88.680000. running mean: -51.218526\n",
      "epsilon:0.170430 episode_count: 4669. steps_count: 2045351.000000\n",
      "Time elapsed:  5845.035551548004\n",
      "ep 667: ep_len:558 episode reward: total was -57.110000. running mean: -51.277441\n",
      "ep 667: ep_len:500 episode reward: total was -3.660000. running mean: -50.801267\n",
      "ep 667: ep_len:558 episode reward: total was -58.740000. running mean: -50.880654\n",
      "ep 667: ep_len:511 episode reward: total was -33.350000. running mean: -50.705347\n",
      "ep 667: ep_len:81 episode reward: total was 3.710000. running mean: -50.161194\n",
      "ep 667: ep_len:661 episode reward: total was -81.370000. running mean: -50.473282\n",
      "ep 667: ep_len:588 episode reward: total was -52.750000. running mean: -50.496049\n",
      "epsilon:0.170385 episode_count: 4676. steps_count: 2048808.000000\n",
      "Time elapsed:  5854.429643630981\n",
      "ep 668: ep_len:238 episode reward: total was -16.810000. running mean: -50.159189\n",
      "ep 668: ep_len:502 episode reward: total was -44.210000. running mean: -50.099697\n",
      "ep 668: ep_len:500 episode reward: total was -89.950000. running mean: -50.498200\n",
      "ep 668: ep_len:595 episode reward: total was -35.760000. running mean: -50.350818\n",
      "ep 668: ep_len:80 episode reward: total was -3.280000. running mean: -49.880110\n",
      "ep 668: ep_len:316 episode reward: total was -25.940000. running mean: -49.640709\n",
      "ep 668: ep_len:517 episode reward: total was -96.400000. running mean: -50.108302\n",
      "epsilon:0.170341 episode_count: 4683. steps_count: 2051556.000000\n",
      "Time elapsed:  5861.838271617889\n",
      "ep 669: ep_len:500 episode reward: total was -92.480000. running mean: -50.532018\n",
      "ep 669: ep_len:342 episode reward: total was -41.400000. running mean: -50.440698\n",
      "ep 669: ep_len:594 episode reward: total was -65.710000. running mean: -50.593391\n",
      "ep 669: ep_len:613 episode reward: total was -36.830000. running mean: -50.455757\n",
      "ep 669: ep_len:77 episode reward: total was 10.680000. running mean: -49.844400\n",
      "ep 669: ep_len:159 episode reward: total was -5.660000. running mean: -49.402556\n",
      "ep 669: ep_len:519 episode reward: total was -91.670000. running mean: -49.825230\n",
      "epsilon:0.170297 episode_count: 4690. steps_count: 2054360.000000\n",
      "Time elapsed:  5869.439842224121\n",
      "ep 670: ep_len:500 episode reward: total was 5.270000. running mean: -49.274278\n",
      "ep 670: ep_len:500 episode reward: total was -34.070000. running mean: -49.122235\n",
      "ep 670: ep_len:512 episode reward: total was -25.900000. running mean: -48.890013\n",
      "ep 670: ep_len:515 episode reward: total was -106.480000. running mean: -49.465913\n",
      "ep 670: ep_len:3 episode reward: total was 0.000000. running mean: -48.971254\n",
      "ep 670: ep_len:551 episode reward: total was -65.810000. running mean: -49.139641\n",
      "ep 670: ep_len:500 episode reward: total was -80.510000. running mean: -49.453345\n",
      "epsilon:0.170252 episode_count: 4697. steps_count: 2057441.000000\n",
      "Time elapsed:  5877.514770746231\n",
      "ep 671: ep_len:608 episode reward: total was -21.410000. running mean: -49.172911\n",
      "ep 671: ep_len:501 episode reward: total was 33.160000. running mean: -48.349582\n",
      "ep 671: ep_len:636 episode reward: total was -100.020000. running mean: -48.866286\n",
      "ep 671: ep_len:527 episode reward: total was -246.280000. running mean: -50.840423\n",
      "ep 671: ep_len:3 episode reward: total was -1.500000. running mean: -50.347019\n",
      "ep 671: ep_len:294 episode reward: total was -20.650000. running mean: -50.050049\n",
      "ep 671: ep_len:523 episode reward: total was -80.140000. running mean: -50.350948\n",
      "epsilon:0.170208 episode_count: 4704. steps_count: 2060533.000000\n",
      "Time elapsed:  5885.654987335205\n",
      "ep 672: ep_len:636 episode reward: total was -107.640000. running mean: -50.923839\n",
      "ep 672: ep_len:546 episode reward: total was -27.810000. running mean: -50.692701\n",
      "ep 672: ep_len:523 episode reward: total was -46.540000. running mean: -50.651174\n",
      "ep 672: ep_len:567 episode reward: total was -46.110000. running mean: -50.605762\n",
      "ep 672: ep_len:89 episode reward: total was 6.240000. running mean: -50.037304\n",
      "ep 672: ep_len:500 episode reward: total was -20.150000. running mean: -49.738431\n",
      "ep 672: ep_len:335 episode reward: total was -48.300000. running mean: -49.724047\n",
      "epsilon:0.170164 episode_count: 4711. steps_count: 2063729.000000\n",
      "Time elapsed:  5894.569578409195\n",
      "ep 673: ep_len:597 episode reward: total was -61.050000. running mean: -49.837306\n",
      "ep 673: ep_len:500 episode reward: total was -32.330000. running mean: -49.662233\n",
      "ep 673: ep_len:637 episode reward: total was -47.150000. running mean: -49.637111\n",
      "ep 673: ep_len:519 episode reward: total was -56.230000. running mean: -49.703040\n",
      "ep 673: ep_len:3 episode reward: total was 0.000000. running mean: -49.206010\n",
      "ep 673: ep_len:523 episode reward: total was -74.230000. running mean: -49.456249\n",
      "ep 673: ep_len:522 episode reward: total was -63.920000. running mean: -49.600887\n",
      "epsilon:0.170119 episode_count: 4718. steps_count: 2067030.000000\n",
      "Time elapsed:  5903.174002408981\n",
      "ep 674: ep_len:500 episode reward: total was -104.240000. running mean: -50.147278\n",
      "ep 674: ep_len:201 episode reward: total was -35.780000. running mean: -50.003605\n",
      "ep 674: ep_len:626 episode reward: total was -41.050000. running mean: -49.914069\n",
      "ep 674: ep_len:500 episode reward: total was -49.310000. running mean: -49.908029\n",
      "ep 674: ep_len:74 episode reward: total was 2.200000. running mean: -49.386948\n",
      "ep 674: ep_len:307 episode reward: total was -50.660000. running mean: -49.399679\n",
      "ep 674: ep_len:349 episode reward: total was -79.370000. running mean: -49.699382\n",
      "epsilon:0.170075 episode_count: 4725. steps_count: 2069587.000000\n",
      "Time elapsed:  5910.235331296921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 675: ep_len:228 episode reward: total was -9.330000. running mean: -49.295688\n",
      "ep 675: ep_len:623 episode reward: total was -75.140000. running mean: -49.554131\n",
      "ep 675: ep_len:374 episode reward: total was -10.770000. running mean: -49.166290\n",
      "ep 675: ep_len:518 episode reward: total was -23.810000. running mean: -48.912727\n",
      "ep 675: ep_len:3 episode reward: total was 0.000000. running mean: -48.423600\n",
      "ep 675: ep_len:169 episode reward: total was 4.480000. running mean: -47.894564\n",
      "ep 675: ep_len:505 episode reward: total was -65.230000. running mean: -48.067918\n",
      "epsilon:0.170031 episode_count: 4732. steps_count: 2072007.000000\n",
      "Time elapsed:  5917.276844024658\n",
      "ep 676: ep_len:500 episode reward: total was -67.600000. running mean: -48.263239\n",
      "ep 676: ep_len:500 episode reward: total was 6.090000. running mean: -47.719707\n",
      "ep 676: ep_len:429 episode reward: total was -19.260000. running mean: -47.435110\n",
      "ep 676: ep_len:123 episode reward: total was -5.560000. running mean: -47.016358\n",
      "ep 676: ep_len:3 episode reward: total was 0.000000. running mean: -46.546195\n",
      "ep 676: ep_len:315 episode reward: total was 2.580000. running mean: -46.054933\n",
      "ep 676: ep_len:618 episode reward: total was -82.570000. running mean: -46.420084\n",
      "epsilon:0.169986 episode_count: 4739. steps_count: 2074495.000000\n",
      "Time elapsed:  5924.185394525528\n",
      "ep 677: ep_len:569 episode reward: total was 6.940000. running mean: -45.886483\n",
      "ep 677: ep_len:500 episode reward: total was -63.550000. running mean: -46.063118\n",
      "ep 677: ep_len:598 episode reward: total was -50.810000. running mean: -46.110587\n",
      "ep 677: ep_len:500 episode reward: total was -73.480000. running mean: -46.384281\n",
      "ep 677: ep_len:3 episode reward: total was 0.000000. running mean: -45.920438\n",
      "ep 677: ep_len:689 episode reward: total was -76.360000. running mean: -46.224834\n",
      "ep 677: ep_len:641 episode reward: total was -21.630000. running mean: -45.978885\n",
      "epsilon:0.169942 episode_count: 4746. steps_count: 2077995.000000\n",
      "Time elapsed:  5933.016511440277\n",
      "ep 678: ep_len:583 episode reward: total was -23.450000. running mean: -45.753596\n",
      "ep 678: ep_len:598 episode reward: total was -64.310000. running mean: -45.939161\n",
      "ep 678: ep_len:360 episode reward: total was -6.990000. running mean: -45.549669\n",
      "ep 678: ep_len:575 episode reward: total was -17.130000. running mean: -45.265472\n",
      "ep 678: ep_len:93 episode reward: total was -2.750000. running mean: -44.840317\n",
      "ep 678: ep_len:500 episode reward: total was -75.360000. running mean: -45.145514\n",
      "ep 678: ep_len:534 episode reward: total was -54.490000. running mean: -45.238959\n",
      "epsilon:0.169898 episode_count: 4753. steps_count: 2081238.000000\n",
      "Time elapsed:  5941.928308963776\n",
      "ep 679: ep_len:534 episode reward: total was -86.340000. running mean: -45.649970\n",
      "ep 679: ep_len:186 episode reward: total was -52.460000. running mean: -45.718070\n",
      "ep 679: ep_len:390 episode reward: total was -41.350000. running mean: -45.674389\n",
      "ep 679: ep_len:500 episode reward: total was -91.850000. running mean: -46.136145\n",
      "ep 679: ep_len:3 episode reward: total was 0.000000. running mean: -45.674784\n",
      "ep 679: ep_len:606 episode reward: total was -34.460000. running mean: -45.562636\n",
      "ep 679: ep_len:501 episode reward: total was -68.660000. running mean: -45.793610\n",
      "epsilon:0.169853 episode_count: 4760. steps_count: 2083958.000000\n",
      "Time elapsed:  5949.720294475555\n",
      "ep 680: ep_len:609 episode reward: total was -214.680000. running mean: -47.482474\n",
      "ep 680: ep_len:597 episode reward: total was -80.470000. running mean: -47.812349\n",
      "ep 680: ep_len:562 episode reward: total was -99.970000. running mean: -48.333925\n",
      "ep 680: ep_len:501 episode reward: total was -30.570000. running mean: -48.156286\n",
      "ep 680: ep_len:120 episode reward: total was -18.240000. running mean: -47.857123\n",
      "ep 680: ep_len:295 episode reward: total was -9.220000. running mean: -47.470752\n",
      "ep 680: ep_len:618 episode reward: total was -95.980000. running mean: -47.955844\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.169809 episode_count: 4767. steps_count: 2087260.000000\n",
      "Time elapsed:  5963.993267297745\n",
      "ep 681: ep_len:645 episode reward: total was -89.190000. running mean: -48.368186\n",
      "ep 681: ep_len:500 episode reward: total was -23.090000. running mean: -48.115404\n",
      "ep 681: ep_len:557 episode reward: total was -47.310000. running mean: -48.107350\n",
      "ep 681: ep_len:511 episode reward: total was -68.150000. running mean: -48.307777\n",
      "ep 681: ep_len:102 episode reward: total was 9.230000. running mean: -47.732399\n",
      "ep 681: ep_len:556 episode reward: total was -150.800000. running mean: -48.763075\n",
      "ep 681: ep_len:523 episode reward: total was -75.790000. running mean: -49.033344\n",
      "epsilon:0.169765 episode_count: 4774. steps_count: 2090654.000000\n",
      "Time elapsed:  5972.880555868149\n",
      "ep 682: ep_len:131 episode reward: total was -12.560000. running mean: -48.668611\n",
      "ep 682: ep_len:372 episode reward: total was -121.400000. running mean: -49.395925\n",
      "ep 682: ep_len:646 episode reward: total was -60.130000. running mean: -49.503265\n",
      "ep 682: ep_len:526 episode reward: total was -9.750000. running mean: -49.105733\n",
      "ep 682: ep_len:95 episode reward: total was -59.310000. running mean: -49.207775\n",
      "ep 682: ep_len:261 episode reward: total was -11.650000. running mean: -48.832198\n",
      "ep 682: ep_len:540 episode reward: total was -48.080000. running mean: -48.824676\n",
      "epsilon:0.169720 episode_count: 4781. steps_count: 2093225.000000\n",
      "Time elapsed:  5979.967990159988\n",
      "ep 683: ep_len:592 episode reward: total was -2.670000. running mean: -48.363129\n",
      "ep 683: ep_len:500 episode reward: total was -39.620000. running mean: -48.275698\n",
      "ep 683: ep_len:544 episode reward: total was -99.170000. running mean: -48.784641\n",
      "ep 683: ep_len:597 episode reward: total was -29.370000. running mean: -48.590494\n",
      "ep 683: ep_len:3 episode reward: total was -1.500000. running mean: -48.119589\n",
      "ep 683: ep_len:538 episode reward: total was -53.910000. running mean: -48.177493\n",
      "ep 683: ep_len:521 episode reward: total was -150.040000. running mean: -49.196118\n",
      "epsilon:0.169676 episode_count: 4788. steps_count: 2096520.000000\n",
      "Time elapsed:  5989.472936868668\n",
      "ep 684: ep_len:577 episode reward: total was -1.400000. running mean: -48.718157\n",
      "ep 684: ep_len:657 episode reward: total was -114.980000. running mean: -49.380776\n",
      "ep 684: ep_len:572 episode reward: total was -101.720000. running mean: -49.904168\n",
      "ep 684: ep_len:500 episode reward: total was -27.950000. running mean: -49.684626\n",
      "ep 684: ep_len:93 episode reward: total was 8.240000. running mean: -49.105380\n",
      "ep 684: ep_len:540 episode reward: total was -72.170000. running mean: -49.336026\n",
      "ep 684: ep_len:596 episode reward: total was -55.850000. running mean: -49.401166\n",
      "epsilon:0.169632 episode_count: 4795. steps_count: 2100055.000000\n",
      "Time elapsed:  5998.700899839401\n",
      "ep 685: ep_len:535 episode reward: total was -170.130000. running mean: -50.608454\n",
      "ep 685: ep_len:267 episode reward: total was -45.110000. running mean: -50.553470\n",
      "ep 685: ep_len:642 episode reward: total was -164.220000. running mean: -51.690135\n",
      "ep 685: ep_len:601 episode reward: total was -48.490000. running mean: -51.658134\n",
      "ep 685: ep_len:3 episode reward: total was 0.000000. running mean: -51.141552\n",
      "ep 685: ep_len:536 episode reward: total was -91.160000. running mean: -51.541737\n",
      "ep 685: ep_len:533 episode reward: total was -122.290000. running mean: -52.249219\n",
      "epsilon:0.169587 episode_count: 4802. steps_count: 2103172.000000\n",
      "Time elapsed:  6006.883432626724\n",
      "ep 686: ep_len:564 episode reward: total was -49.330000. running mean: -52.220027\n",
      "ep 686: ep_len:236 episode reward: total was -28.220000. running mean: -51.980027\n",
      "ep 686: ep_len:465 episode reward: total was -21.560000. running mean: -51.675827\n",
      "ep 686: ep_len:500 episode reward: total was -79.720000. running mean: -51.956268\n",
      "ep 686: ep_len:3 episode reward: total was 0.000000. running mean: -51.436706\n",
      "ep 686: ep_len:552 episode reward: total was -62.920000. running mean: -51.551539\n",
      "ep 686: ep_len:352 episode reward: total was -41.410000. running mean: -51.450123\n",
      "epsilon:0.169543 episode_count: 4809. steps_count: 2105844.000000\n",
      "Time elapsed:  6014.111178159714\n",
      "ep 687: ep_len:603 episode reward: total was -20.450000. running mean: -51.140122\n",
      "ep 687: ep_len:524 episode reward: total was -13.110000. running mean: -50.759821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 687: ep_len:542 episode reward: total was -105.450000. running mean: -51.306723\n",
      "ep 687: ep_len:515 episode reward: total was -15.910000. running mean: -50.952755\n",
      "ep 687: ep_len:86 episode reward: total was -51.250000. running mean: -50.955728\n",
      "ep 687: ep_len:635 episode reward: total was -105.220000. running mean: -51.498371\n",
      "ep 687: ep_len:564 episode reward: total was -92.660000. running mean: -51.909987\n",
      "epsilon:0.169499 episode_count: 4816. steps_count: 2109313.000000\n",
      "Time elapsed:  6023.249637842178\n",
      "ep 688: ep_len:604 episode reward: total was 7.570000. running mean: -51.315187\n",
      "ep 688: ep_len:531 episode reward: total was -92.690000. running mean: -51.728935\n",
      "ep 688: ep_len:619 episode reward: total was -114.110000. running mean: -52.352746\n",
      "ep 688: ep_len:521 episode reward: total was -110.090000. running mean: -52.930118\n",
      "ep 688: ep_len:3 episode reward: total was 0.000000. running mean: -52.400817\n",
      "ep 688: ep_len:501 episode reward: total was -151.010000. running mean: -53.386909\n",
      "ep 688: ep_len:511 episode reward: total was -79.300000. running mean: -53.646040\n",
      "epsilon:0.169454 episode_count: 4823. steps_count: 2112603.000000\n",
      "Time elapsed:  6031.989600896835\n",
      "ep 689: ep_len:130 episode reward: total was -5.130000. running mean: -53.160879\n",
      "ep 689: ep_len:201 episode reward: total was -20.750000. running mean: -52.836771\n",
      "ep 689: ep_len:500 episode reward: total was -224.090000. running mean: -54.549303\n",
      "ep 689: ep_len:556 episode reward: total was -98.020000. running mean: -54.984010\n",
      "ep 689: ep_len:89 episode reward: total was -56.890000. running mean: -55.003070\n",
      "ep 689: ep_len:166 episode reward: total was 6.500000. running mean: -54.388039\n",
      "ep 689: ep_len:162 episode reward: total was -52.120000. running mean: -54.365359\n",
      "epsilon:0.169410 episode_count: 4830. steps_count: 2114407.000000\n",
      "Time elapsed:  6037.211600065231\n",
      "ep 690: ep_len:549 episode reward: total was -142.480000. running mean: -55.246505\n",
      "ep 690: ep_len:539 episode reward: total was -113.060000. running mean: -55.824640\n",
      "ep 690: ep_len:607 episode reward: total was -77.130000. running mean: -56.037694\n",
      "ep 690: ep_len:398 episode reward: total was -45.330000. running mean: -55.930617\n",
      "ep 690: ep_len:3 episode reward: total was 0.000000. running mean: -55.371311\n",
      "ep 690: ep_len:575 episode reward: total was -95.420000. running mean: -55.771798\n",
      "ep 690: ep_len:565 episode reward: total was -60.480000. running mean: -55.818880\n",
      "epsilon:0.169366 episode_count: 4837. steps_count: 2117643.000000\n",
      "Time elapsed:  6045.76198554039\n",
      "ep 691: ep_len:590 episode reward: total was -92.120000. running mean: -56.181891\n",
      "ep 691: ep_len:500 episode reward: total was 7.850000. running mean: -55.541572\n",
      "ep 691: ep_len:74 episode reward: total was -0.740000. running mean: -54.993556\n",
      "ep 691: ep_len:609 episode reward: total was -68.610000. running mean: -55.129721\n",
      "ep 691: ep_len:3 episode reward: total was 0.000000. running mean: -54.578423\n",
      "ep 691: ep_len:523 episode reward: total was -93.170000. running mean: -54.964339\n",
      "ep 691: ep_len:316 episode reward: total was -37.780000. running mean: -54.792496\n",
      "epsilon:0.169321 episode_count: 4844. steps_count: 2120258.000000\n",
      "Time elapsed:  6052.498490810394\n",
      "ep 692: ep_len:501 episode reward: total was 4.670000. running mean: -54.197871\n",
      "ep 692: ep_len:500 episode reward: total was -41.860000. running mean: -54.074492\n",
      "ep 692: ep_len:635 episode reward: total was -57.960000. running mean: -54.113347\n",
      "ep 692: ep_len:586 episode reward: total was -44.540000. running mean: -54.017614\n",
      "ep 692: ep_len:120 episode reward: total was 9.800000. running mean: -53.379438\n",
      "ep 692: ep_len:568 episode reward: total was -59.060000. running mean: -53.436243\n",
      "ep 692: ep_len:620 episode reward: total was -95.060000. running mean: -53.852481\n",
      "epsilon:0.169277 episode_count: 4851. steps_count: 2123788.000000\n",
      "Time elapsed:  6060.2930743694305\n",
      "ep 693: ep_len:673 episode reward: total was -162.850000. running mean: -54.942456\n",
      "ep 693: ep_len:501 episode reward: total was -109.890000. running mean: -55.491931\n",
      "ep 693: ep_len:501 episode reward: total was -71.790000. running mean: -55.654912\n",
      "ep 693: ep_len:500 episode reward: total was -57.790000. running mean: -55.676263\n",
      "ep 693: ep_len:3 episode reward: total was -1.500000. running mean: -55.134500\n",
      "ep 693: ep_len:560 episode reward: total was -72.150000. running mean: -55.304655\n",
      "ep 693: ep_len:534 episode reward: total was -66.810000. running mean: -55.419709\n",
      "epsilon:0.169233 episode_count: 4858. steps_count: 2127060.000000\n",
      "Time elapsed:  6068.248844861984\n",
      "ep 694: ep_len:625 episode reward: total was -83.940000. running mean: -55.704912\n",
      "ep 694: ep_len:596 episode reward: total was -66.780000. running mean: -55.815663\n",
      "ep 694: ep_len:440 episode reward: total was -3.200000. running mean: -55.289506\n",
      "ep 694: ep_len:154 episode reward: total was -36.360000. running mean: -55.100211\n",
      "ep 694: ep_len:3 episode reward: total was 0.000000. running mean: -54.549209\n",
      "ep 694: ep_len:619 episode reward: total was -85.870000. running mean: -54.862417\n",
      "ep 694: ep_len:623 episode reward: total was -101.400000. running mean: -55.327792\n",
      "epsilon:0.169188 episode_count: 4865. steps_count: 2130120.000000\n",
      "Time elapsed:  6076.398725748062\n",
      "ep 695: ep_len:544 episode reward: total was -19.510000. running mean: -54.969615\n",
      "ep 695: ep_len:585 episode reward: total was -65.850000. running mean: -55.078418\n",
      "ep 695: ep_len:561 episode reward: total was -70.060000. running mean: -55.228234\n",
      "ep 695: ep_len:162 episode reward: total was -9.560000. running mean: -54.771552\n",
      "ep 695: ep_len:96 episode reward: total was 11.240000. running mean: -54.111436\n",
      "ep 695: ep_len:242 episode reward: total was 4.530000. running mean: -53.525022\n",
      "ep 695: ep_len:580 episode reward: total was -117.890000. running mean: -54.168672\n",
      "epsilon:0.169144 episode_count: 4872. steps_count: 2132890.000000\n",
      "Time elapsed:  6083.847081899643\n",
      "ep 696: ep_len:198 episode reward: total was -30.050000. running mean: -53.927485\n",
      "ep 696: ep_len:630 episode reward: total was -74.790000. running mean: -54.136110\n",
      "ep 696: ep_len:552 episode reward: total was -78.630000. running mean: -54.381049\n",
      "ep 696: ep_len:540 episode reward: total was -20.170000. running mean: -54.038939\n",
      "ep 696: ep_len:3 episode reward: total was 0.000000. running mean: -53.498549\n",
      "ep 696: ep_len:551 episode reward: total was -76.280000. running mean: -53.726364\n",
      "ep 696: ep_len:500 episode reward: total was -40.780000. running mean: -53.596900\n",
      "epsilon:0.169100 episode_count: 4879. steps_count: 2135864.000000\n",
      "Time elapsed:  6093.098143577576\n",
      "ep 697: ep_len:500 episode reward: total was -14.470000. running mean: -53.205631\n",
      "ep 697: ep_len:573 episode reward: total was -130.740000. running mean: -53.980975\n",
      "ep 697: ep_len:435 episode reward: total was -35.240000. running mean: -53.793565\n",
      "ep 697: ep_len:605 episode reward: total was -21.910000. running mean: -53.474729\n",
      "ep 697: ep_len:3 episode reward: total was 0.000000. running mean: -52.939982\n",
      "ep 697: ep_len:510 episode reward: total was -28.630000. running mean: -52.696882\n",
      "ep 697: ep_len:553 episode reward: total was -146.780000. running mean: -53.637713\n",
      "epsilon:0.169055 episode_count: 4886. steps_count: 2139043.000000\n",
      "Time elapsed:  6101.408488273621\n",
      "ep 698: ep_len:617 episode reward: total was -143.470000. running mean: -54.536036\n",
      "ep 698: ep_len:538 episode reward: total was 8.090000. running mean: -53.909776\n",
      "ep 698: ep_len:575 episode reward: total was -64.070000. running mean: -54.011378\n",
      "ep 698: ep_len:406 episode reward: total was -32.830000. running mean: -53.799564\n",
      "ep 698: ep_len:3 episode reward: total was 0.000000. running mean: -53.261569\n",
      "ep 698: ep_len:696 episode reward: total was -54.960000. running mean: -53.278553\n",
      "ep 698: ep_len:204 episode reward: total was -36.610000. running mean: -53.111868\n",
      "epsilon:0.169011 episode_count: 4893. steps_count: 2142082.000000\n",
      "Time elapsed:  6109.584776639938\n",
      "ep 699: ep_len:577 episode reward: total was -9.650000. running mean: -52.677249\n",
      "ep 699: ep_len:537 episode reward: total was -2.420000. running mean: -52.174676\n",
      "ep 699: ep_len:618 episode reward: total was -23.860000. running mean: -51.891530\n",
      "ep 699: ep_len:521 episode reward: total was -56.050000. running mean: -51.933114\n",
      "ep 699: ep_len:3 episode reward: total was 0.000000. running mean: -51.413783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 699: ep_len:540 episode reward: total was -69.660000. running mean: -51.596245\n",
      "ep 699: ep_len:537 episode reward: total was -102.260000. running mean: -52.102883\n",
      "epsilon:0.168967 episode_count: 4900. steps_count: 2145415.000000\n",
      "Time elapsed:  6118.195261955261\n",
      "ep 700: ep_len:674 episode reward: total was -106.670000. running mean: -52.648554\n",
      "ep 700: ep_len:503 episode reward: total was -101.350000. running mean: -53.135569\n",
      "ep 700: ep_len:580 episode reward: total was -179.230000. running mean: -54.396513\n",
      "ep 700: ep_len:501 episode reward: total was -41.240000. running mean: -54.264948\n",
      "ep 700: ep_len:102 episode reward: total was -47.780000. running mean: -54.200098\n",
      "ep 700: ep_len:569 episode reward: total was -50.140000. running mean: -54.159497\n",
      "ep 700: ep_len:627 episode reward: total was -76.720000. running mean: -54.385102\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.168922 episode_count: 4907. steps_count: 2148971.000000\n",
      "Time elapsed:  6132.853661298752\n",
      "ep 701: ep_len:645 episode reward: total was -45.810000. running mean: -54.299351\n",
      "ep 701: ep_len:272 episode reward: total was -43.070000. running mean: -54.187058\n",
      "ep 701: ep_len:564 episode reward: total was -121.530000. running mean: -54.860487\n",
      "ep 701: ep_len:501 episode reward: total was -65.800000. running mean: -54.969882\n",
      "ep 701: ep_len:50 episode reward: total was 17.500000. running mean: -54.245183\n",
      "ep 701: ep_len:196 episode reward: total was -0.350000. running mean: -53.706232\n",
      "ep 701: ep_len:606 episode reward: total was -45.590000. running mean: -53.625069\n",
      "epsilon:0.168878 episode_count: 4914. steps_count: 2151805.000000\n",
      "Time elapsed:  6141.549324989319\n",
      "ep 702: ep_len:200 episode reward: total was -10.290000. running mean: -53.191719\n",
      "ep 702: ep_len:621 episode reward: total was -59.320000. running mean: -53.253001\n",
      "ep 702: ep_len:543 episode reward: total was -99.170000. running mean: -53.712171\n",
      "ep 702: ep_len:500 episode reward: total was -74.810000. running mean: -53.923150\n",
      "ep 702: ep_len:3 episode reward: total was 0.000000. running mean: -53.383918\n",
      "ep 702: ep_len:714 episode reward: total was -74.590000. running mean: -53.595979\n",
      "ep 702: ep_len:532 episode reward: total was -81.570000. running mean: -53.875719\n",
      "epsilon:0.168834 episode_count: 4921. steps_count: 2154918.000000\n",
      "Time elapsed:  6150.682012557983\n",
      "ep 703: ep_len:501 episode reward: total was 12.350000. running mean: -53.213462\n",
      "ep 703: ep_len:554 episode reward: total was -47.530000. running mean: -53.156627\n",
      "ep 703: ep_len:503 episode reward: total was -133.510000. running mean: -53.960161\n",
      "ep 703: ep_len:381 episode reward: total was -58.420000. running mean: -54.004760\n",
      "ep 703: ep_len:83 episode reward: total was 4.160000. running mean: -53.423112\n",
      "ep 703: ep_len:500 episode reward: total was -45.720000. running mean: -53.346081\n",
      "ep 703: ep_len:500 episode reward: total was -72.750000. running mean: -53.540120\n",
      "epsilon:0.168789 episode_count: 4928. steps_count: 2157940.000000\n",
      "Time elapsed:  6158.74992275238\n",
      "ep 704: ep_len:569 episode reward: total was -66.670000. running mean: -53.671419\n",
      "ep 704: ep_len:646 episode reward: total was -48.360000. running mean: -53.618305\n",
      "ep 704: ep_len:613 episode reward: total was -101.490000. running mean: -54.097022\n",
      "ep 704: ep_len:504 episode reward: total was -40.420000. running mean: -53.960251\n",
      "ep 704: ep_len:3 episode reward: total was -1.500000. running mean: -53.435649\n",
      "ep 704: ep_len:500 episode reward: total was -81.450000. running mean: -53.715792\n",
      "ep 704: ep_len:509 episode reward: total was -67.330000. running mean: -53.851934\n",
      "epsilon:0.168745 episode_count: 4935. steps_count: 2161284.000000\n",
      "Time elapsed:  6168.124768257141\n",
      "ep 705: ep_len:219 episode reward: total was -8.440000. running mean: -53.397815\n",
      "ep 705: ep_len:501 episode reward: total was -22.270000. running mean: -53.086537\n",
      "ep 705: ep_len:589 episode reward: total was -168.670000. running mean: -54.242372\n",
      "ep 705: ep_len:501 episode reward: total was -51.540000. running mean: -54.215348\n",
      "ep 705: ep_len:95 episode reward: total was 19.220000. running mean: -53.480994\n",
      "ep 705: ep_len:167 episode reward: total was -21.000000. running mean: -53.156184\n",
      "ep 705: ep_len:500 episode reward: total was -76.540000. running mean: -53.390023\n",
      "epsilon:0.168701 episode_count: 4942. steps_count: 2163856.000000\n",
      "Time elapsed:  6175.165535926819\n",
      "ep 706: ep_len:500 episode reward: total was -50.250000. running mean: -53.358622\n",
      "ep 706: ep_len:520 episode reward: total was -64.800000. running mean: -53.473036\n",
      "ep 706: ep_len:609 episode reward: total was -86.960000. running mean: -53.807906\n",
      "ep 706: ep_len:597 episode reward: total was -19.300000. running mean: -53.462827\n",
      "ep 706: ep_len:105 episode reward: total was -1.280000. running mean: -52.940998\n",
      "ep 706: ep_len:500 episode reward: total was -77.580000. running mean: -53.187388\n",
      "ep 706: ep_len:500 episode reward: total was -48.260000. running mean: -53.138115\n",
      "epsilon:0.168656 episode_count: 4949. steps_count: 2167187.000000\n",
      "Time elapsed:  6183.691168785095\n",
      "ep 707: ep_len:624 episode reward: total was -107.360000. running mean: -53.680333\n",
      "ep 707: ep_len:500 episode reward: total was -30.010000. running mean: -53.443630\n",
      "ep 707: ep_len:69 episode reward: total was -0.820000. running mean: -52.917394\n",
      "ep 707: ep_len:500 episode reward: total was -72.630000. running mean: -53.114520\n",
      "ep 707: ep_len:91 episode reward: total was 8.190000. running mean: -52.501475\n",
      "ep 707: ep_len:319 episode reward: total was -119.530000. running mean: -53.171760\n",
      "ep 707: ep_len:315 episode reward: total was -43.790000. running mean: -53.077942\n",
      "epsilon:0.168612 episode_count: 4956. steps_count: 2169605.000000\n",
      "Time elapsed:  6190.305502414703\n",
      "ep 708: ep_len:501 episode reward: total was -22.330000. running mean: -52.770463\n",
      "ep 708: ep_len:534 episode reward: total was -42.560000. running mean: -52.668358\n",
      "ep 708: ep_len:662 episode reward: total was -88.270000. running mean: -53.024375\n",
      "ep 708: ep_len:504 episode reward: total was -28.660000. running mean: -52.780731\n",
      "ep 708: ep_len:3 episode reward: total was -1.500000. running mean: -52.267924\n",
      "ep 708: ep_len:500 episode reward: total was -81.210000. running mean: -52.557344\n",
      "ep 708: ep_len:507 episode reward: total was -83.830000. running mean: -52.870071\n",
      "epsilon:0.168568 episode_count: 4963. steps_count: 2172816.000000\n",
      "Time elapsed:  6198.863413333893\n",
      "ep 709: ep_len:500 episode reward: total was -6.420000. running mean: -52.405570\n",
      "ep 709: ep_len:501 episode reward: total was -73.490000. running mean: -52.616415\n",
      "ep 709: ep_len:524 episode reward: total was -56.910000. running mean: -52.659350\n",
      "ep 709: ep_len:536 episode reward: total was -18.470000. running mean: -52.317457\n",
      "ep 709: ep_len:84 episode reward: total was -19.310000. running mean: -51.987382\n",
      "ep 709: ep_len:500 episode reward: total was -99.530000. running mean: -52.462809\n",
      "ep 709: ep_len:585 episode reward: total was -67.840000. running mean: -52.616580\n",
      "epsilon:0.168523 episode_count: 4970. steps_count: 2176046.000000\n",
      "Time elapsed:  6207.363990545273\n",
      "ep 710: ep_len:594 episode reward: total was -204.710000. running mean: -54.137515\n",
      "ep 710: ep_len:201 episode reward: total was -42.710000. running mean: -54.023239\n",
      "ep 710: ep_len:64 episode reward: total was -4.910000. running mean: -53.532107\n",
      "ep 710: ep_len:501 episode reward: total was -26.250000. running mean: -53.259286\n",
      "ep 710: ep_len:1 episode reward: total was -1.000000. running mean: -52.736693\n",
      "ep 710: ep_len:633 episode reward: total was -56.930000. running mean: -52.778626\n",
      "ep 710: ep_len:500 episode reward: total was -4.920000. running mean: -52.300040\n",
      "epsilon:0.168479 episode_count: 4977. steps_count: 2178540.000000\n",
      "Time elapsed:  6214.236261844635\n",
      "ep 711: ep_len:654 episode reward: total was -129.520000. running mean: -53.072240\n",
      "ep 711: ep_len:568 episode reward: total was -3.780000. running mean: -52.579317\n",
      "ep 711: ep_len:622 episode reward: total was -102.740000. running mean: -53.080924\n",
      "ep 711: ep_len:102 episode reward: total was 5.370000. running mean: -52.496415\n",
      "ep 711: ep_len:3 episode reward: total was 0.000000. running mean: -51.971451\n",
      "ep 711: ep_len:500 episode reward: total was -94.870000. running mean: -52.400436\n",
      "ep 711: ep_len:169 episode reward: total was -47.800000. running mean: -52.354432\n",
      "epsilon:0.168435 episode_count: 4984. steps_count: 2181158.000000\n",
      "Time elapsed:  6221.903964281082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 712: ep_len:501 episode reward: total was -105.070000. running mean: -52.881587\n",
      "ep 712: ep_len:577 episode reward: total was -25.170000. running mean: -52.604472\n",
      "ep 712: ep_len:521 episode reward: total was -36.860000. running mean: -52.447027\n",
      "ep 712: ep_len:515 episode reward: total was -130.590000. running mean: -53.228457\n",
      "ep 712: ep_len:3 episode reward: total was 0.000000. running mean: -52.696172\n",
      "ep 712: ep_len:558 episode reward: total was -68.640000. running mean: -52.855610\n",
      "ep 712: ep_len:619 episode reward: total was -63.050000. running mean: -52.957554\n",
      "epsilon:0.168390 episode_count: 4991. steps_count: 2184452.000000\n",
      "Time elapsed:  6230.231214761734\n",
      "ep 713: ep_len:121 episode reward: total was -0.660000. running mean: -52.434579\n",
      "ep 713: ep_len:338 episode reward: total was -50.370000. running mean: -52.413933\n",
      "ep 713: ep_len:500 episode reward: total was -27.930000. running mean: -52.169094\n",
      "ep 713: ep_len:432 episode reward: total was -43.970000. running mean: -52.087103\n",
      "ep 713: ep_len:3 episode reward: total was -3.000000. running mean: -51.596232\n",
      "ep 713: ep_len:584 episode reward: total was -62.170000. running mean: -51.701969\n",
      "ep 713: ep_len:545 episode reward: total was -56.740000. running mean: -51.752350\n",
      "epsilon:0.168346 episode_count: 4998. steps_count: 2186975.000000\n",
      "Time elapsed:  6237.043991327286\n",
      "ep 714: ep_len:590 episode reward: total was -68.720000. running mean: -51.922026\n",
      "ep 714: ep_len:546 episode reward: total was -56.490000. running mean: -51.967706\n",
      "ep 714: ep_len:563 episode reward: total was -96.500000. running mean: -52.413029\n",
      "ep 714: ep_len:544 episode reward: total was -62.530000. running mean: -52.514198\n",
      "ep 714: ep_len:3 episode reward: total was -1.500000. running mean: -52.004056\n",
      "ep 714: ep_len:244 episode reward: total was 7.360000. running mean: -51.410416\n",
      "ep 714: ep_len:298 episode reward: total was -63.310000. running mean: -51.529412\n",
      "epsilon:0.168302 episode_count: 5005. steps_count: 2189763.000000\n",
      "Time elapsed:  6244.455968618393\n",
      "ep 715: ep_len:641 episode reward: total was -58.680000. running mean: -51.600918\n",
      "ep 715: ep_len:500 episode reward: total was -18.230000. running mean: -51.267208\n",
      "ep 715: ep_len:504 episode reward: total was -26.790000. running mean: -51.022436\n",
      "ep 715: ep_len:599 episode reward: total was -79.150000. running mean: -51.303712\n",
      "ep 715: ep_len:3 episode reward: total was 0.000000. running mean: -50.790675\n",
      "ep 715: ep_len:500 episode reward: total was -23.690000. running mean: -50.519668\n",
      "ep 715: ep_len:520 episode reward: total was -67.130000. running mean: -50.685771\n",
      "epsilon:0.168257 episode_count: 5012. steps_count: 2193030.000000\n",
      "Time elapsed:  6253.117985725403\n",
      "ep 716: ep_len:208 episode reward: total was -10.790000. running mean: -50.286814\n",
      "ep 716: ep_len:500 episode reward: total was -34.570000. running mean: -50.129646\n",
      "ep 716: ep_len:610 episode reward: total was -169.960000. running mean: -51.327949\n",
      "ep 716: ep_len:512 episode reward: total was -46.170000. running mean: -51.276370\n",
      "ep 716: ep_len:98 episode reward: total was 13.740000. running mean: -50.626206\n",
      "ep 716: ep_len:608 episode reward: total was -87.740000. running mean: -50.997344\n",
      "ep 716: ep_len:541 episode reward: total was -88.920000. running mean: -51.376570\n",
      "epsilon:0.168213 episode_count: 5019. steps_count: 2196107.000000\n",
      "Time elapsed:  6261.838012456894\n",
      "ep 717: ep_len:505 episode reward: total was -123.030000. running mean: -52.093105\n",
      "ep 717: ep_len:500 episode reward: total was -55.680000. running mean: -52.128974\n",
      "ep 717: ep_len:500 episode reward: total was -46.860000. running mean: -52.076284\n",
      "ep 717: ep_len:500 episode reward: total was -68.150000. running mean: -52.237021\n",
      "ep 717: ep_len:3 episode reward: total was -1.500000. running mean: -51.729651\n",
      "ep 717: ep_len:500 episode reward: total was -97.930000. running mean: -52.191654\n",
      "ep 717: ep_len:501 episode reward: total was -43.930000. running mean: -52.109038\n",
      "epsilon:0.168169 episode_count: 5026. steps_count: 2199116.000000\n",
      "Time elapsed:  6269.866824626923\n",
      "ep 718: ep_len:226 episode reward: total was -18.900000. running mean: -51.776947\n",
      "ep 718: ep_len:585 episode reward: total was -80.160000. running mean: -52.060778\n",
      "ep 718: ep_len:510 episode reward: total was -139.520000. running mean: -52.935370\n",
      "ep 718: ep_len:520 episode reward: total was -14.890000. running mean: -52.554917\n",
      "ep 718: ep_len:111 episode reward: total was -16.210000. running mean: -52.191467\n",
      "ep 718: ep_len:633 episode reward: total was -74.570000. running mean: -52.415253\n",
      "ep 718: ep_len:500 episode reward: total was -32.370000. running mean: -52.214800\n",
      "epsilon:0.168124 episode_count: 5033. steps_count: 2202201.000000\n",
      "Time elapsed:  6278.149383544922\n",
      "ep 719: ep_len:509 episode reward: total was -5.840000. running mean: -51.751052\n",
      "ep 719: ep_len:500 episode reward: total was -137.730000. running mean: -52.610842\n",
      "ep 719: ep_len:670 episode reward: total was -44.180000. running mean: -52.526533\n",
      "ep 719: ep_len:515 episode reward: total was -33.300000. running mean: -52.334268\n",
      "ep 719: ep_len:84 episode reward: total was 4.630000. running mean: -51.764625\n",
      "ep 719: ep_len:648 episode reward: total was -76.200000. running mean: -52.008979\n",
      "ep 719: ep_len:211 episode reward: total was -53.800000. running mean: -52.026889\n",
      "epsilon:0.168080 episode_count: 5040. steps_count: 2205338.000000\n",
      "Time elapsed:  6286.544778108597\n",
      "ep 720: ep_len:558 episode reward: total was -160.660000. running mean: -53.113220\n",
      "ep 720: ep_len:551 episode reward: total was -42.780000. running mean: -53.009888\n",
      "ep 720: ep_len:564 episode reward: total was -63.780000. running mean: -53.117589\n",
      "ep 720: ep_len:501 episode reward: total was -60.870000. running mean: -53.195113\n",
      "ep 720: ep_len:3 episode reward: total was 0.000000. running mean: -52.663162\n",
      "ep 720: ep_len:627 episode reward: total was -71.990000. running mean: -52.856431\n",
      "ep 720: ep_len:188 episode reward: total was -66.370000. running mean: -52.991566\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.168036 episode_count: 5047. steps_count: 2208330.000000\n",
      "Time elapsed:  6299.505854845047\n",
      "ep 721: ep_len:551 episode reward: total was -30.920000. running mean: -52.770851\n",
      "ep 721: ep_len:500 episode reward: total was 27.350000. running mean: -51.969642\n",
      "ep 721: ep_len:500 episode reward: total was -96.780000. running mean: -52.417746\n",
      "ep 721: ep_len:546 episode reward: total was -51.010000. running mean: -52.403668\n",
      "ep 721: ep_len:87 episode reward: total was -3.760000. running mean: -51.917232\n",
      "ep 721: ep_len:605 episode reward: total was -96.010000. running mean: -52.358159\n",
      "ep 721: ep_len:500 episode reward: total was -37.890000. running mean: -52.213478\n",
      "epsilon:0.167991 episode_count: 5054. steps_count: 2211619.000000\n",
      "Time elapsed:  6308.51417350769\n",
      "ep 722: ep_len:565 episode reward: total was -35.710000. running mean: -52.048443\n",
      "ep 722: ep_len:354 episode reward: total was -60.720000. running mean: -52.135158\n",
      "ep 722: ep_len:79 episode reward: total was 2.830000. running mean: -51.585507\n",
      "ep 722: ep_len:101 episode reward: total was -11.580000. running mean: -51.185452\n",
      "ep 722: ep_len:87 episode reward: total was 12.710000. running mean: -50.546497\n",
      "ep 722: ep_len:571 episode reward: total was -55.970000. running mean: -50.600732\n",
      "ep 722: ep_len:580 episode reward: total was -68.350000. running mean: -50.778225\n",
      "epsilon:0.167947 episode_count: 5061. steps_count: 2213956.000000\n",
      "Time elapsed:  6315.517531871796\n",
      "ep 723: ep_len:546 episode reward: total was -37.680000. running mean: -50.647243\n",
      "ep 723: ep_len:505 episode reward: total was -70.080000. running mean: -50.841570\n",
      "ep 723: ep_len:70 episode reward: total was -2.770000. running mean: -50.360855\n",
      "ep 723: ep_len:46 episode reward: total was -6.780000. running mean: -49.925046\n",
      "ep 723: ep_len:3 episode reward: total was -1.500000. running mean: -49.440796\n",
      "ep 723: ep_len:500 episode reward: total was -62.310000. running mean: -49.569488\n",
      "ep 723: ep_len:528 episode reward: total was -54.860000. running mean: -49.622393\n",
      "epsilon:0.167903 episode_count: 5068. steps_count: 2216154.000000\n",
      "Time elapsed:  6322.278675556183\n",
      "ep 724: ep_len:564 episode reward: total was -51.570000. running mean: -49.641869\n",
      "ep 724: ep_len:500 episode reward: total was -66.230000. running mean: -49.807750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 724: ep_len:547 episode reward: total was -32.250000. running mean: -49.632173\n",
      "ep 724: ep_len:520 episode reward: total was -7.240000. running mean: -49.208251\n",
      "ep 724: ep_len:87 episode reward: total was 7.720000. running mean: -48.638968\n",
      "ep 724: ep_len:603 episode reward: total was -72.480000. running mean: -48.877379\n",
      "ep 724: ep_len:505 episode reward: total was -116.390000. running mean: -49.552505\n",
      "epsilon:0.167858 episode_count: 5075. steps_count: 2219480.000000\n",
      "Time elapsed:  6330.6408133506775\n",
      "ep 725: ep_len:247 episode reward: total was -14.730000. running mean: -49.204280\n",
      "ep 725: ep_len:536 episode reward: total was -51.240000. running mean: -49.224637\n",
      "ep 725: ep_len:535 episode reward: total was -59.490000. running mean: -49.327291\n",
      "ep 725: ep_len:500 episode reward: total was -38.470000. running mean: -49.218718\n",
      "ep 725: ep_len:3 episode reward: total was 0.000000. running mean: -48.726531\n",
      "ep 725: ep_len:500 episode reward: total was -63.000000. running mean: -48.869265\n",
      "ep 725: ep_len:315 episode reward: total was -51.180000. running mean: -48.892373\n",
      "epsilon:0.167814 episode_count: 5082. steps_count: 2222116.000000\n",
      "Time elapsed:  6337.584750413895\n",
      "ep 726: ep_len:538 episode reward: total was -126.160000. running mean: -49.665049\n",
      "ep 726: ep_len:521 episode reward: total was -11.830000. running mean: -49.286698\n",
      "ep 726: ep_len:501 episode reward: total was -26.260000. running mean: -49.056431\n",
      "ep 726: ep_len:510 episode reward: total was -13.220000. running mean: -48.698067\n",
      "ep 726: ep_len:3 episode reward: total was 0.000000. running mean: -48.211086\n",
      "ep 726: ep_len:580 episode reward: total was -157.750000. running mean: -49.306476\n",
      "ep 726: ep_len:538 episode reward: total was -66.680000. running mean: -49.480211\n",
      "epsilon:0.167770 episode_count: 5089. steps_count: 2225307.000000\n",
      "Time elapsed:  6346.073157310486\n",
      "ep 727: ep_len:501 episode reward: total was -85.330000. running mean: -49.838709\n",
      "ep 727: ep_len:500 episode reward: total was -12.060000. running mean: -49.460922\n",
      "ep 727: ep_len:444 episode reward: total was -25.350000. running mean: -49.219812\n",
      "ep 727: ep_len:511 episode reward: total was -73.660000. running mean: -49.464214\n",
      "ep 727: ep_len:3 episode reward: total was 0.000000. running mean: -48.969572\n",
      "ep 727: ep_len:546 episode reward: total was -119.940000. running mean: -49.679276\n",
      "ep 727: ep_len:607 episode reward: total was -72.270000. running mean: -49.905184\n",
      "epsilon:0.167725 episode_count: 5096. steps_count: 2228419.000000\n",
      "Time elapsed:  6354.215656518936\n",
      "ep 728: ep_len:256 episode reward: total was -2.710000. running mean: -49.433232\n",
      "ep 728: ep_len:602 episode reward: total was -47.780000. running mean: -49.416700\n",
      "ep 728: ep_len:541 episode reward: total was -94.350000. running mean: -49.866033\n",
      "ep 728: ep_len:501 episode reward: total was -44.850000. running mean: -49.815872\n",
      "ep 728: ep_len:3 episode reward: total was 0.000000. running mean: -49.317713\n",
      "ep 728: ep_len:316 episode reward: total was -22.480000. running mean: -49.049336\n",
      "ep 728: ep_len:676 episode reward: total was -170.880000. running mean: -50.267643\n",
      "epsilon:0.167681 episode_count: 5103. steps_count: 2231314.000000\n",
      "Time elapsed:  6361.978799819946\n",
      "ep 729: ep_len:540 episode reward: total was -125.380000. running mean: -51.018767\n",
      "ep 729: ep_len:576 episode reward: total was -73.910000. running mean: -51.247679\n",
      "ep 729: ep_len:570 episode reward: total was -97.580000. running mean: -51.711002\n",
      "ep 729: ep_len:513 episode reward: total was -22.690000. running mean: -51.420792\n",
      "ep 729: ep_len:3 episode reward: total was 0.000000. running mean: -50.906584\n",
      "ep 729: ep_len:500 episode reward: total was -81.580000. running mean: -51.213318\n",
      "ep 729: ep_len:594 episode reward: total was -45.310000. running mean: -51.154285\n",
      "epsilon:0.167637 episode_count: 5110. steps_count: 2234610.000000\n",
      "Time elapsed:  6370.708680152893\n",
      "ep 730: ep_len:500 episode reward: total was -17.580000. running mean: -50.818542\n",
      "ep 730: ep_len:500 episode reward: total was -29.530000. running mean: -50.605657\n",
      "ep 730: ep_len:396 episode reward: total was -42.750000. running mean: -50.527100\n",
      "ep 730: ep_len:500 episode reward: total was -10.620000. running mean: -50.128029\n",
      "ep 730: ep_len:3 episode reward: total was -1.500000. running mean: -49.641749\n",
      "ep 730: ep_len:607 episode reward: total was -41.700000. running mean: -49.562331\n",
      "ep 730: ep_len:501 episode reward: total was -58.770000. running mean: -49.654408\n",
      "epsilon:0.167592 episode_count: 5117. steps_count: 2237617.000000\n",
      "Time elapsed:  6378.688711881638\n",
      "ep 731: ep_len:533 episode reward: total was -48.430000. running mean: -49.642164\n",
      "ep 731: ep_len:500 episode reward: total was -26.810000. running mean: -49.413842\n",
      "ep 731: ep_len:556 episode reward: total was -104.590000. running mean: -49.965604\n",
      "ep 731: ep_len:521 episode reward: total was -91.920000. running mean: -50.385148\n",
      "ep 731: ep_len:3 episode reward: total was -1.500000. running mean: -49.896297\n",
      "ep 731: ep_len:500 episode reward: total was -54.090000. running mean: -49.938234\n",
      "ep 731: ep_len:500 episode reward: total was -59.560000. running mean: -50.034451\n",
      "epsilon:0.167548 episode_count: 5124. steps_count: 2240730.000000\n",
      "Time elapsed:  6386.7929129600525\n",
      "ep 732: ep_len:121 episode reward: total was -2.130000. running mean: -49.555407\n",
      "ep 732: ep_len:272 episode reward: total was -108.210000. running mean: -50.141953\n",
      "ep 732: ep_len:541 episode reward: total was -74.910000. running mean: -50.389633\n",
      "ep 732: ep_len:394 episode reward: total was -30.600000. running mean: -50.191737\n",
      "ep 732: ep_len:3 episode reward: total was -1.500000. running mean: -49.704819\n",
      "ep 732: ep_len:167 episode reward: total was -16.530000. running mean: -49.373071\n",
      "ep 732: ep_len:575 episode reward: total was -81.260000. running mean: -49.691940\n",
      "epsilon:0.167504 episode_count: 5131. steps_count: 2242803.000000\n",
      "Time elapsed:  6393.061089038849\n",
      "ep 733: ep_len:602 episode reward: total was -2.050000. running mean: -49.215521\n",
      "ep 733: ep_len:300 episode reward: total was -69.080000. running mean: -49.414166\n",
      "ep 733: ep_len:578 episode reward: total was -113.450000. running mean: -50.054524\n",
      "ep 733: ep_len:403 episode reward: total was -43.460000. running mean: -49.988579\n",
      "ep 733: ep_len:99 episode reward: total was -8.780000. running mean: -49.576493\n",
      "ep 733: ep_len:501 episode reward: total was -25.380000. running mean: -49.334528\n",
      "ep 733: ep_len:500 episode reward: total was -85.180000. running mean: -49.692983\n",
      "epsilon:0.167459 episode_count: 5138. steps_count: 2245786.000000\n",
      "Time elapsed:  6400.945026636124\n",
      "ep 734: ep_len:663 episode reward: total was -116.250000. running mean: -50.358553\n",
      "ep 734: ep_len:500 episode reward: total was -35.150000. running mean: -50.206468\n",
      "ep 734: ep_len:515 episode reward: total was -70.310000. running mean: -50.407503\n",
      "ep 734: ep_len:513 episode reward: total was -142.730000. running mean: -51.330728\n",
      "ep 734: ep_len:3 episode reward: total was 0.000000. running mean: -50.817421\n",
      "ep 734: ep_len:507 episode reward: total was -64.410000. running mean: -50.953346\n",
      "ep 734: ep_len:291 episode reward: total was -41.710000. running mean: -50.860913\n",
      "epsilon:0.167415 episode_count: 5145. steps_count: 2248778.000000\n",
      "Time elapsed:  6409.5581386089325\n",
      "ep 735: ep_len:528 episode reward: total was -28.190000. running mean: -50.634204\n",
      "ep 735: ep_len:610 episode reward: total was -82.720000. running mean: -50.955062\n",
      "ep 735: ep_len:566 episode reward: total was -104.810000. running mean: -51.493611\n",
      "ep 735: ep_len:500 episode reward: total was -63.180000. running mean: -51.610475\n",
      "ep 735: ep_len:3 episode reward: total was 0.000000. running mean: -51.094370\n",
      "ep 735: ep_len:247 episode reward: total was -0.140000. running mean: -50.584827\n",
      "ep 735: ep_len:548 episode reward: total was -39.560000. running mean: -50.474578\n",
      "epsilon:0.167371 episode_count: 5152. steps_count: 2251780.000000\n",
      "Time elapsed:  6417.650706291199\n",
      "ep 736: ep_len:573 episode reward: total was -26.840000. running mean: -50.238233\n",
      "ep 736: ep_len:500 episode reward: total was -87.530000. running mean: -50.611150\n",
      "ep 736: ep_len:512 episode reward: total was -83.990000. running mean: -50.944939\n",
      "ep 736: ep_len:500 episode reward: total was -32.380000. running mean: -50.759289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 736: ep_len:86 episode reward: total was -10.260000. running mean: -50.354296\n",
      "ep 736: ep_len:177 episode reward: total was 4.010000. running mean: -49.810653\n",
      "ep 736: ep_len:503 episode reward: total was -51.000000. running mean: -49.822547\n",
      "epsilon:0.167326 episode_count: 5159. steps_count: 2254631.000000\n",
      "Time elapsed:  6426.0077250003815\n",
      "ep 737: ep_len:501 episode reward: total was -31.870000. running mean: -49.643021\n",
      "ep 737: ep_len:500 episode reward: total was -0.760000. running mean: -49.154191\n",
      "ep 737: ep_len:527 episode reward: total was -72.790000. running mean: -49.390549\n",
      "ep 737: ep_len:544 episode reward: total was -48.020000. running mean: -49.376844\n",
      "ep 737: ep_len:52 episode reward: total was 9.500000. running mean: -48.788075\n",
      "ep 737: ep_len:527 episode reward: total was -102.950000. running mean: -49.329695\n",
      "ep 737: ep_len:527 episode reward: total was -51.620000. running mean: -49.352598\n",
      "epsilon:0.167282 episode_count: 5166. steps_count: 2257809.000000\n",
      "Time elapsed:  6434.916793107986\n",
      "ep 738: ep_len:599 episode reward: total was -71.250000. running mean: -49.571572\n",
      "ep 738: ep_len:556 episode reward: total was -50.230000. running mean: -49.578156\n",
      "ep 738: ep_len:60 episode reward: total was -0.300000. running mean: -49.085374\n",
      "ep 738: ep_len:387 episode reward: total was -71.680000. running mean: -49.311321\n",
      "ep 738: ep_len:3 episode reward: total was 0.000000. running mean: -48.818208\n",
      "ep 738: ep_len:552 episode reward: total was -88.410000. running mean: -49.214125\n",
      "ep 738: ep_len:500 episode reward: total was -102.410000. running mean: -49.746084\n",
      "epsilon:0.167238 episode_count: 5173. steps_count: 2260466.000000\n",
      "Time elapsed:  6442.595337867737\n",
      "ep 739: ep_len:646 episode reward: total was -144.600000. running mean: -50.694623\n",
      "ep 739: ep_len:500 episode reward: total was -99.380000. running mean: -51.181477\n",
      "ep 739: ep_len:504 episode reward: total was -75.990000. running mean: -51.429562\n",
      "ep 739: ep_len:523 episode reward: total was -54.300000. running mean: -51.458267\n",
      "ep 739: ep_len:3 episode reward: total was 0.000000. running mean: -50.943684\n",
      "ep 739: ep_len:628 episode reward: total was -92.130000. running mean: -51.355547\n",
      "ep 739: ep_len:631 episode reward: total was -71.660000. running mean: -51.558592\n",
      "epsilon:0.167193 episode_count: 5180. steps_count: 2263901.000000\n",
      "Time elapsed:  6451.661213636398\n",
      "ep 740: ep_len:507 episode reward: total was -65.880000. running mean: -51.701806\n",
      "ep 740: ep_len:559 episode reward: total was -64.430000. running mean: -51.829088\n",
      "ep 740: ep_len:673 episode reward: total was -69.920000. running mean: -52.009997\n",
      "ep 740: ep_len:500 episode reward: total was -60.520000. running mean: -52.095097\n",
      "ep 740: ep_len:2 episode reward: total was -0.500000. running mean: -51.579146\n",
      "ep 740: ep_len:508 episode reward: total was -133.700000. running mean: -52.400354\n",
      "ep 740: ep_len:500 episode reward: total was -39.000000. running mean: -52.266351\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.167149 episode_count: 5187. steps_count: 2267150.000000\n",
      "Time elapsed:  6465.078574419022\n",
      "ep 741: ep_len:610 episode reward: total was -111.330000. running mean: -52.856987\n",
      "ep 741: ep_len:578 episode reward: total was -79.690000. running mean: -53.125318\n",
      "ep 741: ep_len:678 episode reward: total was -80.150000. running mean: -53.395564\n",
      "ep 741: ep_len:500 episode reward: total was -28.870000. running mean: -53.150309\n",
      "ep 741: ep_len:3 episode reward: total was 0.000000. running mean: -52.618806\n",
      "ep 741: ep_len:558 episode reward: total was -99.340000. running mean: -53.086018\n",
      "ep 741: ep_len:547 episode reward: total was -96.250000. running mean: -53.517657\n",
      "epsilon:0.167105 episode_count: 5194. steps_count: 2270624.000000\n",
      "Time elapsed:  6474.154683828354\n",
      "ep 742: ep_len:565 episode reward: total was -110.920000. running mean: -54.091681\n",
      "ep 742: ep_len:543 episode reward: total was 19.810000. running mean: -53.352664\n",
      "ep 742: ep_len:79 episode reward: total was -10.300000. running mean: -52.922137\n",
      "ep 742: ep_len:503 episode reward: total was -11.480000. running mean: -52.507716\n",
      "ep 742: ep_len:124 episode reward: total was -36.710000. running mean: -52.349739\n",
      "ep 742: ep_len:538 episode reward: total was -67.750000. running mean: -52.503741\n",
      "ep 742: ep_len:501 episode reward: total was -44.670000. running mean: -52.425404\n",
      "epsilon:0.167060 episode_count: 5201. steps_count: 2273477.000000\n",
      "Time elapsed:  6482.474409580231\n",
      "ep 743: ep_len:500 episode reward: total was -130.940000. running mean: -53.210550\n",
      "ep 743: ep_len:633 episode reward: total was -70.340000. running mean: -53.381845\n",
      "ep 743: ep_len:500 episode reward: total was -32.470000. running mean: -53.172726\n",
      "ep 743: ep_len:532 episode reward: total was -41.240000. running mean: -53.053399\n",
      "ep 743: ep_len:79 episode reward: total was 6.260000. running mean: -52.460265\n",
      "ep 743: ep_len:500 episode reward: total was -54.570000. running mean: -52.481362\n",
      "ep 743: ep_len:500 episode reward: total was -62.620000. running mean: -52.582749\n",
      "epsilon:0.167016 episode_count: 5208. steps_count: 2276721.000000\n",
      "Time elapsed:  6491.089430093765\n",
      "ep 744: ep_len:594 episode reward: total was -44.650000. running mean: -52.503421\n",
      "ep 744: ep_len:575 episode reward: total was -27.200000. running mean: -52.250387\n",
      "ep 744: ep_len:528 episode reward: total was -66.350000. running mean: -52.391383\n",
      "ep 744: ep_len:500 episode reward: total was -9.310000. running mean: -51.960569\n",
      "ep 744: ep_len:3 episode reward: total was 0.000000. running mean: -51.440963\n",
      "ep 744: ep_len:239 episode reward: total was 5.350000. running mean: -50.873054\n",
      "ep 744: ep_len:517 episode reward: total was -100.920000. running mean: -51.373523\n",
      "epsilon:0.166972 episode_count: 5215. steps_count: 2279677.000000\n",
      "Time elapsed:  6498.608116388321\n",
      "ep 745: ep_len:202 episode reward: total was 10.580000. running mean: -50.753988\n",
      "ep 745: ep_len:564 episode reward: total was -94.440000. running mean: -51.190848\n",
      "ep 745: ep_len:548 episode reward: total was -79.000000. running mean: -51.468940\n",
      "ep 745: ep_len:530 episode reward: total was -89.590000. running mean: -51.850150\n",
      "ep 745: ep_len:118 episode reward: total was -27.160000. running mean: -51.603249\n",
      "ep 745: ep_len:510 episode reward: total was -125.270000. running mean: -52.339916\n",
      "ep 745: ep_len:551 episode reward: total was -56.990000. running mean: -52.386417\n",
      "epsilon:0.166927 episode_count: 5222. steps_count: 2282700.000000\n",
      "Time elapsed:  6507.051591396332\n",
      "ep 746: ep_len:259 episode reward: total was 2.310000. running mean: -51.839453\n",
      "ep 746: ep_len:524 episode reward: total was -68.090000. running mean: -52.001958\n",
      "ep 746: ep_len:610 episode reward: total was -72.560000. running mean: -52.207539\n",
      "ep 746: ep_len:520 episode reward: total was -77.420000. running mean: -52.459663\n",
      "ep 746: ep_len:3 episode reward: total was 0.000000. running mean: -51.935067\n",
      "ep 746: ep_len:500 episode reward: total was -56.460000. running mean: -51.980316\n",
      "ep 746: ep_len:503 episode reward: total was -67.870000. running mean: -52.139213\n",
      "epsilon:0.166883 episode_count: 5229. steps_count: 2285619.000000\n",
      "Time elapsed:  6514.866478919983\n",
      "ep 747: ep_len:500 episode reward: total was -114.630000. running mean: -52.764121\n",
      "ep 747: ep_len:500 episode reward: total was -170.280000. running mean: -53.939280\n",
      "ep 747: ep_len:467 episode reward: total was -2.010000. running mean: -53.419987\n",
      "ep 747: ep_len:605 episode reward: total was -44.130000. running mean: -53.327087\n",
      "ep 747: ep_len:115 episode reward: total was -6.750000. running mean: -52.861316\n",
      "ep 747: ep_len:500 episode reward: total was -10.580000. running mean: -52.438503\n",
      "ep 747: ep_len:602 episode reward: total was -67.980000. running mean: -52.593918\n",
      "epsilon:0.166839 episode_count: 5236. steps_count: 2288908.000000\n",
      "Time elapsed:  6524.327311038971\n",
      "ep 748: ep_len:134 episode reward: total was 0.940000. running mean: -52.058579\n",
      "ep 748: ep_len:570 episode reward: total was -32.400000. running mean: -51.861993\n",
      "ep 748: ep_len:565 episode reward: total was -55.720000. running mean: -51.900573\n",
      "ep 748: ep_len:501 episode reward: total was -50.230000. running mean: -51.883867\n",
      "ep 748: ep_len:3 episode reward: total was 0.000000. running mean: -51.365029\n",
      "ep 748: ep_len:570 episode reward: total was -70.240000. running mean: -51.553778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 748: ep_len:501 episode reward: total was -85.530000. running mean: -51.893541\n",
      "epsilon:0.166794 episode_count: 5243. steps_count: 2291752.000000\n",
      "Time elapsed:  6531.435955286026\n",
      "ep 749: ep_len:544 episode reward: total was -28.390000. running mean: -51.658505\n",
      "ep 749: ep_len:648 episode reward: total was -27.500000. running mean: -51.416920\n",
      "ep 749: ep_len:362 episode reward: total was -8.040000. running mean: -50.983151\n",
      "ep 749: ep_len:543 episode reward: total was 5.880000. running mean: -50.414519\n",
      "ep 749: ep_len:3 episode reward: total was 0.000000. running mean: -49.910374\n",
      "ep 749: ep_len:668 episode reward: total was -41.040000. running mean: -49.821670\n",
      "ep 749: ep_len:517 episode reward: total was -53.080000. running mean: -49.854254\n",
      "epsilon:0.166750 episode_count: 5250. steps_count: 2295037.000000\n",
      "Time elapsed:  6539.2745106220245\n",
      "ep 750: ep_len:500 episode reward: total was -77.840000. running mean: -50.134111\n",
      "ep 750: ep_len:624 episode reward: total was 13.990000. running mean: -49.492870\n",
      "ep 750: ep_len:658 episode reward: total was -139.190000. running mean: -50.389841\n",
      "ep 750: ep_len:500 episode reward: total was -57.240000. running mean: -50.458343\n",
      "ep 750: ep_len:83 episode reward: total was 5.200000. running mean: -49.901760\n",
      "ep 750: ep_len:538 episode reward: total was -67.310000. running mean: -50.075842\n",
      "ep 750: ep_len:203 episode reward: total was -45.220000. running mean: -50.027284\n",
      "epsilon:0.166706 episode_count: 5257. steps_count: 2298143.000000\n",
      "Time elapsed:  6548.041281223297\n",
      "ep 751: ep_len:502 episode reward: total was -76.860000. running mean: -50.295611\n",
      "ep 751: ep_len:500 episode reward: total was -2.070000. running mean: -49.813355\n",
      "ep 751: ep_len:521 episode reward: total was -40.450000. running mean: -49.719721\n",
      "ep 751: ep_len:555 episode reward: total was -6.750000. running mean: -49.290024\n",
      "ep 751: ep_len:3 episode reward: total was 0.000000. running mean: -48.797124\n",
      "ep 751: ep_len:517 episode reward: total was -24.540000. running mean: -48.554552\n",
      "ep 751: ep_len:329 episode reward: total was -59.530000. running mean: -48.664307\n",
      "epsilon:0.166661 episode_count: 5264. steps_count: 2301070.000000\n",
      "Time elapsed:  6556.091914176941\n",
      "ep 752: ep_len:587 episode reward: total was -74.780000. running mean: -48.925464\n",
      "ep 752: ep_len:500 episode reward: total was -98.810000. running mean: -49.424309\n",
      "ep 752: ep_len:578 episode reward: total was -85.890000. running mean: -49.788966\n",
      "ep 752: ep_len:607 episode reward: total was -52.590000. running mean: -49.816976\n",
      "ep 752: ep_len:110 episode reward: total was -6.800000. running mean: -49.386807\n",
      "ep 752: ep_len:172 episode reward: total was 4.570000. running mean: -48.847239\n",
      "ep 752: ep_len:339 episode reward: total was -55.370000. running mean: -48.912466\n",
      "epsilon:0.166617 episode_count: 5271. steps_count: 2303963.000000\n",
      "Time elapsed:  6563.782431602478\n",
      "ep 753: ep_len:509 episode reward: total was -68.780000. running mean: -49.111142\n",
      "ep 753: ep_len:514 episode reward: total was -76.970000. running mean: -49.389730\n",
      "ep 753: ep_len:621 episode reward: total was -95.390000. running mean: -49.849733\n",
      "ep 753: ep_len:624 episode reward: total was -48.950000. running mean: -49.840735\n",
      "ep 753: ep_len:3 episode reward: total was 0.000000. running mean: -49.342328\n",
      "ep 753: ep_len:509 episode reward: total was -44.030000. running mean: -49.289205\n",
      "ep 753: ep_len:569 episode reward: total was -98.130000. running mean: -49.777613\n",
      "epsilon:0.166573 episode_count: 5278. steps_count: 2307312.000000\n",
      "Time elapsed:  6572.569027423859\n",
      "ep 754: ep_len:533 episode reward: total was -205.740000. running mean: -51.337237\n",
      "ep 754: ep_len:512 episode reward: total was -4.410000. running mean: -50.867964\n",
      "ep 754: ep_len:510 episode reward: total was -71.510000. running mean: -51.074385\n",
      "ep 754: ep_len:673 episode reward: total was -194.720000. running mean: -52.510841\n",
      "ep 754: ep_len:3 episode reward: total was 0.000000. running mean: -51.985732\n",
      "ep 754: ep_len:501 episode reward: total was -68.520000. running mean: -52.151075\n",
      "ep 754: ep_len:534 episode reward: total was -46.900000. running mean: -52.098564\n",
      "epsilon:0.166528 episode_count: 5285. steps_count: 2310578.000000\n",
      "Time elapsed:  6581.2889132499695\n",
      "ep 755: ep_len:621 episode reward: total was -45.510000. running mean: -52.032679\n",
      "ep 755: ep_len:593 episode reward: total was -47.630000. running mean: -51.988652\n",
      "ep 755: ep_len:597 episode reward: total was -76.500000. running mean: -52.233765\n",
      "ep 755: ep_len:149 episode reward: total was 8.510000. running mean: -51.626328\n",
      "ep 755: ep_len:83 episode reward: total was 9.730000. running mean: -51.012764\n",
      "ep 755: ep_len:615 episode reward: total was -99.210000. running mean: -51.494737\n",
      "ep 755: ep_len:597 episode reward: total was -45.660000. running mean: -51.436389\n",
      "epsilon:0.166484 episode_count: 5292. steps_count: 2313833.000000\n",
      "Time elapsed:  6590.022431612015\n",
      "ep 756: ep_len:617 episode reward: total was -46.740000. running mean: -51.389426\n",
      "ep 756: ep_len:500 episode reward: total was 23.400000. running mean: -50.641531\n",
      "ep 756: ep_len:532 episode reward: total was -35.550000. running mean: -50.490616\n",
      "ep 756: ep_len:548 episode reward: total was -72.360000. running mean: -50.709310\n",
      "ep 756: ep_len:3 episode reward: total was -1.500000. running mean: -50.217217\n",
      "ep 756: ep_len:530 episode reward: total was -95.980000. running mean: -50.674845\n",
      "ep 756: ep_len:500 episode reward: total was -62.190000. running mean: -50.789996\n",
      "epsilon:0.166440 episode_count: 5299. steps_count: 2317063.000000\n",
      "Time elapsed:  6598.770201921463\n",
      "ep 757: ep_len:556 episode reward: total was -20.000000. running mean: -50.482096\n",
      "ep 757: ep_len:626 episode reward: total was -84.170000. running mean: -50.818975\n",
      "ep 757: ep_len:523 episode reward: total was -71.690000. running mean: -51.027685\n",
      "ep 757: ep_len:500 episode reward: total was -35.760000. running mean: -50.875009\n",
      "ep 757: ep_len:112 episode reward: total was 1.240000. running mean: -50.353858\n",
      "ep 757: ep_len:543 episode reward: total was -89.210000. running mean: -50.742420\n",
      "ep 757: ep_len:500 episode reward: total was -48.350000. running mean: -50.718496\n",
      "epsilon:0.166395 episode_count: 5306. steps_count: 2320423.000000\n",
      "Time elapsed:  6607.85545706749\n",
      "ep 758: ep_len:603 episode reward: total was -71.120000. running mean: -50.922511\n",
      "ep 758: ep_len:536 episode reward: total was -66.650000. running mean: -51.079786\n",
      "ep 758: ep_len:618 episode reward: total was -74.060000. running mean: -51.309588\n",
      "ep 758: ep_len:502 episode reward: total was -104.140000. running mean: -51.837892\n",
      "ep 758: ep_len:3 episode reward: total was 0.000000. running mean: -51.319513\n",
      "ep 758: ep_len:500 episode reward: total was -63.910000. running mean: -51.445418\n",
      "ep 758: ep_len:542 episode reward: total was -68.460000. running mean: -51.615564\n",
      "epsilon:0.166351 episode_count: 5313. steps_count: 2323727.000000\n",
      "Time elapsed:  6616.628052949905\n",
      "ep 759: ep_len:500 episode reward: total was -104.590000. running mean: -52.145308\n",
      "ep 759: ep_len:501 episode reward: total was -38.660000. running mean: -52.010455\n",
      "ep 759: ep_len:422 episode reward: total was -13.160000. running mean: -51.621950\n",
      "ep 759: ep_len:501 episode reward: total was -48.230000. running mean: -51.588031\n",
      "ep 759: ep_len:3 episode reward: total was 0.000000. running mean: -51.072151\n",
      "ep 759: ep_len:500 episode reward: total was 1.610000. running mean: -50.545329\n",
      "ep 759: ep_len:500 episode reward: total was -86.510000. running mean: -50.904976\n",
      "epsilon:0.166307 episode_count: 5320. steps_count: 2326654.000000\n",
      "Time elapsed:  6624.261461019516\n",
      "ep 760: ep_len:547 episode reward: total was -25.240000. running mean: -50.648326\n",
      "ep 760: ep_len:500 episode reward: total was -66.940000. running mean: -50.811243\n",
      "ep 760: ep_len:461 episode reward: total was -31.980000. running mean: -50.622930\n",
      "ep 760: ep_len:545 episode reward: total was -12.420000. running mean: -50.240901\n",
      "ep 760: ep_len:3 episode reward: total was 1.010000. running mean: -49.728392\n",
      "ep 760: ep_len:533 episode reward: total was -78.730000. running mean: -50.018408\n",
      "ep 760: ep_len:569 episode reward: total was -62.900000. running mean: -50.147224\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.166262 episode_count: 5327. steps_count: 2329812.000000\n",
      "Time elapsed:  6637.190435886383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 761: ep_len:121 episode reward: total was -8.040000. running mean: -49.726152\n",
      "ep 761: ep_len:501 episode reward: total was -16.000000. running mean: -49.388890\n",
      "ep 761: ep_len:605 episode reward: total was -72.410000. running mean: -49.619101\n",
      "ep 761: ep_len:520 episode reward: total was -54.760000. running mean: -49.670510\n",
      "ep 761: ep_len:3 episode reward: total was 0.000000. running mean: -49.173805\n",
      "ep 761: ep_len:500 episode reward: total was -97.570000. running mean: -49.657767\n",
      "ep 761: ep_len:500 episode reward: total was -109.740000. running mean: -50.258590\n",
      "epsilon:0.166218 episode_count: 5334. steps_count: 2332562.000000\n",
      "Time elapsed:  6644.949112653732\n",
      "ep 762: ep_len:543 episode reward: total was -97.330000. running mean: -50.729304\n",
      "ep 762: ep_len:187 episode reward: total was -45.530000. running mean: -50.677311\n",
      "ep 762: ep_len:570 episode reward: total was -74.570000. running mean: -50.916237\n",
      "ep 762: ep_len:505 episode reward: total was -62.700000. running mean: -51.034075\n",
      "ep 762: ep_len:3 episode reward: total was 0.000000. running mean: -50.523734\n",
      "ep 762: ep_len:305 episode reward: total was -13.310000. running mean: -50.151597\n",
      "ep 762: ep_len:542 episode reward: total was -101.500000. running mean: -50.665081\n",
      "epsilon:0.166174 episode_count: 5341. steps_count: 2335217.000000\n",
      "Time elapsed:  6652.481127738953\n",
      "ep 763: ep_len:500 episode reward: total was 0.290000. running mean: -50.155530\n",
      "ep 763: ep_len:523 episode reward: total was -6.850000. running mean: -49.722475\n",
      "ep 763: ep_len:576 episode reward: total was -88.420000. running mean: -50.109450\n",
      "ep 763: ep_len:500 episode reward: total was -55.920000. running mean: -50.167556\n",
      "ep 763: ep_len:3 episode reward: total was -1.500000. running mean: -49.680880\n",
      "ep 763: ep_len:500 episode reward: total was -36.500000. running mean: -49.549071\n",
      "ep 763: ep_len:332 episode reward: total was -28.560000. running mean: -49.339181\n",
      "epsilon:0.166129 episode_count: 5348. steps_count: 2338151.000000\n",
      "Time elapsed:  6660.324013948441\n",
      "ep 764: ep_len:666 episode reward: total was -87.170000. running mean: -49.717489\n",
      "ep 764: ep_len:663 episode reward: total was -69.180000. running mean: -49.912114\n",
      "ep 764: ep_len:563 episode reward: total was -48.170000. running mean: -49.894693\n",
      "ep 764: ep_len:500 episode reward: total was -35.470000. running mean: -49.750446\n",
      "ep 764: ep_len:88 episode reward: total was -7.270000. running mean: -49.325641\n",
      "ep 764: ep_len:513 episode reward: total was -107.000000. running mean: -49.902385\n",
      "ep 764: ep_len:581 episode reward: total was -56.340000. running mean: -49.966761\n",
      "epsilon:0.166085 episode_count: 5355. steps_count: 2341725.000000\n",
      "Time elapsed:  6669.573424816132\n",
      "ep 765: ep_len:571 episode reward: total was -3.970000. running mean: -49.506794\n",
      "ep 765: ep_len:500 episode reward: total was -48.620000. running mean: -49.497926\n",
      "ep 765: ep_len:665 episode reward: total was -68.870000. running mean: -49.691646\n",
      "ep 765: ep_len:511 episode reward: total was 0.720000. running mean: -49.187530\n",
      "ep 765: ep_len:3 episode reward: total was 0.000000. running mean: -48.695655\n",
      "ep 765: ep_len:601 episode reward: total was -96.060000. running mean: -49.169298\n",
      "ep 765: ep_len:580 episode reward: total was -74.940000. running mean: -49.427005\n",
      "epsilon:0.166041 episode_count: 5362. steps_count: 2345156.000000\n",
      "Time elapsed:  6678.675634860992\n",
      "ep 766: ep_len:128 episode reward: total was -0.040000. running mean: -48.933135\n",
      "ep 766: ep_len:500 episode reward: total was -83.210000. running mean: -49.275904\n",
      "ep 766: ep_len:542 episode reward: total was -45.890000. running mean: -49.242045\n",
      "ep 766: ep_len:500 episode reward: total was -32.340000. running mean: -49.073024\n",
      "ep 766: ep_len:3 episode reward: total was -1.500000. running mean: -48.597294\n",
      "ep 766: ep_len:500 episode reward: total was -55.490000. running mean: -48.666221\n",
      "ep 766: ep_len:556 episode reward: total was -59.660000. running mean: -48.776159\n",
      "epsilon:0.165996 episode_count: 5369. steps_count: 2347885.000000\n",
      "Time elapsed:  6686.146113872528\n",
      "ep 767: ep_len:126 episode reward: total was -5.170000. running mean: -48.340097\n",
      "ep 767: ep_len:500 episode reward: total was -29.420000. running mean: -48.150896\n",
      "ep 767: ep_len:526 episode reward: total was -26.830000. running mean: -47.937687\n",
      "ep 767: ep_len:500 episode reward: total was -7.810000. running mean: -47.536410\n",
      "ep 767: ep_len:81 episode reward: total was 12.130000. running mean: -46.939746\n",
      "ep 767: ep_len:531 episode reward: total was -39.760000. running mean: -46.867949\n",
      "ep 767: ep_len:588 episode reward: total was -38.130000. running mean: -46.780569\n",
      "epsilon:0.165952 episode_count: 5376. steps_count: 2350737.000000\n",
      "Time elapsed:  6693.816313028336\n",
      "ep 768: ep_len:221 episode reward: total was -13.380000. running mean: -46.446564\n",
      "ep 768: ep_len:500 episode reward: total was -73.010000. running mean: -46.712198\n",
      "ep 768: ep_len:362 episode reward: total was 13.750000. running mean: -46.107576\n",
      "ep 768: ep_len:109 episode reward: total was -1.950000. running mean: -45.666000\n",
      "ep 768: ep_len:94 episode reward: total was 6.200000. running mean: -45.147340\n",
      "ep 768: ep_len:531 episode reward: total was -53.860000. running mean: -45.234467\n",
      "ep 768: ep_len:608 episode reward: total was -200.790000. running mean: -46.790022\n",
      "epsilon:0.165908 episode_count: 5383. steps_count: 2353162.000000\n",
      "Time elapsed:  6700.315870761871\n",
      "ep 769: ep_len:221 episode reward: total was -5.300000. running mean: -46.375122\n",
      "ep 769: ep_len:251 episode reward: total was -38.810000. running mean: -46.299471\n",
      "ep 769: ep_len:622 episode reward: total was -87.070000. running mean: -46.707176\n",
      "ep 769: ep_len:421 episode reward: total was -27.860000. running mean: -46.518704\n",
      "ep 769: ep_len:83 episode reward: total was 1.650000. running mean: -46.037017\n",
      "ep 769: ep_len:517 episode reward: total was -132.890000. running mean: -46.905547\n",
      "ep 769: ep_len:300 episode reward: total was -24.240000. running mean: -46.678892\n",
      "epsilon:0.165863 episode_count: 5390. steps_count: 2355577.000000\n",
      "Time elapsed:  6706.899016618729\n",
      "ep 770: ep_len:592 episode reward: total was -29.910000. running mean: -46.511203\n",
      "ep 770: ep_len:631 episode reward: total was -24.940000. running mean: -46.295491\n",
      "ep 770: ep_len:567 episode reward: total was -87.600000. running mean: -46.708536\n",
      "ep 770: ep_len:501 episode reward: total was -36.050000. running mean: -46.601950\n",
      "ep 770: ep_len:111 episode reward: total was -29.680000. running mean: -46.432731\n",
      "ep 770: ep_len:687 episode reward: total was -78.410000. running mean: -46.752504\n",
      "ep 770: ep_len:280 episode reward: total was -37.330000. running mean: -46.658279\n",
      "epsilon:0.165819 episode_count: 5397. steps_count: 2358946.000000\n",
      "Time elapsed:  6715.660174846649\n",
      "ep 771: ep_len:500 episode reward: total was -38.080000. running mean: -46.572496\n",
      "ep 771: ep_len:631 episode reward: total was -145.760000. running mean: -47.564371\n",
      "ep 771: ep_len:395 episode reward: total was -69.760000. running mean: -47.786327\n",
      "ep 771: ep_len:163 episode reward: total was 0.110000. running mean: -47.307364\n",
      "ep 771: ep_len:3 episode reward: total was 0.000000. running mean: -46.834290\n",
      "ep 771: ep_len:500 episode reward: total was -107.230000. running mean: -47.438247\n",
      "ep 771: ep_len:513 episode reward: total was -80.980000. running mean: -47.773665\n",
      "epsilon:0.165775 episode_count: 5404. steps_count: 2361651.000000\n",
      "Time elapsed:  6723.682447195053\n",
      "ep 772: ep_len:238 episode reward: total was -29.970000. running mean: -47.595628\n",
      "ep 772: ep_len:169 episode reward: total was -28.360000. running mean: -47.403272\n",
      "ep 772: ep_len:650 episode reward: total was -210.460000. running mean: -49.033839\n",
      "ep 772: ep_len:500 episode reward: total was -55.870000. running mean: -49.102201\n",
      "ep 772: ep_len:3 episode reward: total was 0.000000. running mean: -48.611179\n",
      "ep 772: ep_len:527 episode reward: total was -76.930000. running mean: -48.894367\n",
      "ep 772: ep_len:353 episode reward: total was -38.080000. running mean: -48.786223\n",
      "epsilon:0.165730 episode_count: 5411. steps_count: 2364091.000000\n",
      "Time elapsed:  6731.178000688553\n",
      "ep 773: ep_len:229 episode reward: total was -13.420000. running mean: -48.432561\n",
      "ep 773: ep_len:277 episode reward: total was -10.330000. running mean: -48.051535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 773: ep_len:598 episode reward: total was -67.200000. running mean: -48.243020\n",
      "ep 773: ep_len:563 episode reward: total was -22.150000. running mean: -47.982090\n",
      "ep 773: ep_len:3 episode reward: total was 0.000000. running mean: -47.502269\n",
      "ep 773: ep_len:555 episode reward: total was -126.530000. running mean: -48.292546\n",
      "ep 773: ep_len:566 episode reward: total was -23.790000. running mean: -48.047521\n",
      "epsilon:0.165686 episode_count: 5418. steps_count: 2366882.000000\n",
      "Time elapsed:  6739.353814840317\n",
      "ep 774: ep_len:603 episode reward: total was 17.200000. running mean: -47.395046\n",
      "ep 774: ep_len:620 episode reward: total was -252.960000. running mean: -49.450695\n",
      "ep 774: ep_len:560 episode reward: total was -85.080000. running mean: -49.806988\n",
      "ep 774: ep_len:501 episode reward: total was 16.080000. running mean: -49.148118\n",
      "ep 774: ep_len:126 episode reward: total was -16.700000. running mean: -48.823637\n",
      "ep 774: ep_len:561 episode reward: total was -90.970000. running mean: -49.245101\n",
      "ep 774: ep_len:286 episode reward: total was -64.040000. running mean: -49.393050\n",
      "epsilon:0.165642 episode_count: 5425. steps_count: 2370139.000000\n",
      "Time elapsed:  6747.758064746857\n",
      "ep 775: ep_len:500 episode reward: total was -17.260000. running mean: -49.071719\n",
      "ep 775: ep_len:500 episode reward: total was -39.650000. running mean: -48.977502\n",
      "ep 775: ep_len:688 episode reward: total was -111.370000. running mean: -49.601427\n",
      "ep 775: ep_len:533 episode reward: total was -75.630000. running mean: -49.861713\n",
      "ep 775: ep_len:3 episode reward: total was -1.500000. running mean: -49.378096\n",
      "ep 775: ep_len:501 episode reward: total was -43.010000. running mean: -49.314415\n",
      "ep 775: ep_len:597 episode reward: total was -74.730000. running mean: -49.568571\n",
      "epsilon:0.165597 episode_count: 5432. steps_count: 2373461.000000\n",
      "Time elapsed:  6756.5718784332275\n",
      "ep 776: ep_len:219 episode reward: total was 11.240000. running mean: -48.960485\n",
      "ep 776: ep_len:500 episode reward: total was -86.080000. running mean: -49.331680\n",
      "ep 776: ep_len:652 episode reward: total was -95.360000. running mean: -49.791963\n",
      "ep 776: ep_len:147 episode reward: total was 8.100000. running mean: -49.213044\n",
      "ep 776: ep_len:3 episode reward: total was 0.000000. running mean: -48.720913\n",
      "ep 776: ep_len:500 episode reward: total was -59.390000. running mean: -48.827604\n",
      "ep 776: ep_len:501 episode reward: total was -81.290000. running mean: -49.152228\n",
      "epsilon:0.165553 episode_count: 5439. steps_count: 2375983.000000\n",
      "Time elapsed:  6763.361509561539\n",
      "ep 777: ep_len:522 episode reward: total was -50.900000. running mean: -49.169706\n",
      "ep 777: ep_len:568 episode reward: total was -86.270000. running mean: -49.540709\n",
      "ep 777: ep_len:79 episode reward: total was -7.790000. running mean: -49.123202\n",
      "ep 777: ep_len:627 episode reward: total was -40.070000. running mean: -49.032670\n",
      "ep 777: ep_len:3 episode reward: total was 0.000000. running mean: -48.542343\n",
      "ep 777: ep_len:500 episode reward: total was -59.770000. running mean: -48.654619\n",
      "ep 777: ep_len:201 episode reward: total was -43.710000. running mean: -48.605173\n",
      "epsilon:0.165509 episode_count: 5446. steps_count: 2378483.000000\n",
      "Time elapsed:  6770.879918336868\n",
      "ep 778: ep_len:663 episode reward: total was -135.590000. running mean: -49.475021\n",
      "ep 778: ep_len:501 episode reward: total was -84.530000. running mean: -49.825571\n",
      "ep 778: ep_len:500 episode reward: total was -6.940000. running mean: -49.396716\n",
      "ep 778: ep_len:558 episode reward: total was -40.660000. running mean: -49.309348\n",
      "ep 778: ep_len:108 episode reward: total was 2.190000. running mean: -48.794355\n",
      "ep 778: ep_len:612 episode reward: total was -34.910000. running mean: -48.655511\n",
      "ep 778: ep_len:558 episode reward: total was -61.580000. running mean: -48.784756\n",
      "epsilon:0.165464 episode_count: 5453. steps_count: 2381983.000000\n",
      "Time elapsed:  6779.742474317551\n",
      "ep 779: ep_len:649 episode reward: total was -120.210000. running mean: -49.499009\n",
      "ep 779: ep_len:556 episode reward: total was -39.320000. running mean: -49.397219\n",
      "ep 779: ep_len:500 episode reward: total was -36.020000. running mean: -49.263446\n",
      "ep 779: ep_len:130 episode reward: total was 10.000000. running mean: -48.670812\n",
      "ep 779: ep_len:3 episode reward: total was 0.000000. running mean: -48.184104\n",
      "ep 779: ep_len:550 episode reward: total was -64.410000. running mean: -48.346363\n",
      "ep 779: ep_len:623 episode reward: total was -71.140000. running mean: -48.574299\n",
      "epsilon:0.165420 episode_count: 5460. steps_count: 2384994.000000\n",
      "Time elapsed:  6788.407357931137\n",
      "ep 780: ep_len:500 episode reward: total was -94.940000. running mean: -49.037956\n",
      "ep 780: ep_len:359 episode reward: total was -140.930000. running mean: -49.956877\n",
      "ep 780: ep_len:615 episode reward: total was -87.230000. running mean: -50.329608\n",
      "ep 780: ep_len:170 episode reward: total was -16.930000. running mean: -49.995612\n",
      "ep 780: ep_len:3 episode reward: total was -1.500000. running mean: -49.510656\n",
      "ep 780: ep_len:611 episode reward: total was -48.520000. running mean: -49.500749\n",
      "ep 780: ep_len:343 episode reward: total was -67.710000. running mean: -49.682842\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.165376 episode_count: 5467. steps_count: 2387595.000000\n",
      "Time elapsed:  6800.462787866592\n",
      "ep 781: ep_len:779 episode reward: total was -182.010000. running mean: -51.006113\n",
      "ep 781: ep_len:596 episode reward: total was -139.350000. running mean: -51.889552\n",
      "ep 781: ep_len:445 episode reward: total was -29.690000. running mean: -51.667557\n",
      "ep 781: ep_len:146 episode reward: total was -11.630000. running mean: -51.267181\n",
      "ep 781: ep_len:103 episode reward: total was -5.770000. running mean: -50.812209\n",
      "ep 781: ep_len:628 episode reward: total was -90.420000. running mean: -51.208287\n",
      "ep 781: ep_len:207 episode reward: total was -30.180000. running mean: -50.998004\n",
      "epsilon:0.165331 episode_count: 5474. steps_count: 2390499.000000\n",
      "Time elapsed:  6809.1961896419525\n",
      "ep 782: ep_len:104 episode reward: total was -16.660000. running mean: -50.654624\n",
      "ep 782: ep_len:608 episode reward: total was -139.830000. running mean: -51.546378\n",
      "ep 782: ep_len:500 episode reward: total was -112.350000. running mean: -52.154414\n",
      "ep 782: ep_len:511 episode reward: total was -109.350000. running mean: -52.726370\n",
      "ep 782: ep_len:47 episode reward: total was 10.000000. running mean: -52.099106\n",
      "ep 782: ep_len:571 episode reward: total was -68.510000. running mean: -52.263215\n",
      "ep 782: ep_len:542 episode reward: total was -233.110000. running mean: -54.071683\n",
      "epsilon:0.165287 episode_count: 5481. steps_count: 2393382.000000\n",
      "Time elapsed:  6816.442514657974\n",
      "ep 783: ep_len:529 episode reward: total was -1.880000. running mean: -53.549766\n",
      "ep 783: ep_len:177 episode reward: total was -16.490000. running mean: -53.179169\n",
      "ep 783: ep_len:436 episode reward: total was -36.180000. running mean: -53.009177\n",
      "ep 783: ep_len:510 episode reward: total was -69.720000. running mean: -53.176285\n",
      "ep 783: ep_len:102 episode reward: total was 14.760000. running mean: -52.496922\n",
      "ep 783: ep_len:518 episode reward: total was -133.510000. running mean: -53.307053\n",
      "ep 783: ep_len:501 episode reward: total was -142.460000. running mean: -54.198583\n",
      "epsilon:0.165243 episode_count: 5488. steps_count: 2396155.000000\n",
      "Time elapsed:  6824.408298492432\n",
      "ep 784: ep_len:617 episode reward: total was -142.880000. running mean: -55.085397\n",
      "ep 784: ep_len:345 episode reward: total was -66.130000. running mean: -55.195843\n",
      "ep 784: ep_len:431 episode reward: total was -53.860000. running mean: -55.182484\n",
      "ep 784: ep_len:500 episode reward: total was -92.720000. running mean: -55.557859\n",
      "ep 784: ep_len:98 episode reward: total was 1.740000. running mean: -54.984881\n",
      "ep 784: ep_len:500 episode reward: total was -89.320000. running mean: -55.328232\n",
      "ep 784: ep_len:565 episode reward: total was -53.040000. running mean: -55.305350\n",
      "epsilon:0.165198 episode_count: 5495. steps_count: 2399211.000000\n",
      "Time elapsed:  6832.65736579895\n",
      "ep 785: ep_len:187 episode reward: total was -13.140000. running mean: -54.883696\n",
      "ep 785: ep_len:501 episode reward: total was -33.230000. running mean: -54.667159\n",
      "ep 785: ep_len:584 episode reward: total was -62.600000. running mean: -54.746488\n",
      "ep 785: ep_len:525 episode reward: total was -96.780000. running mean: -55.166823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 785: ep_len:99 episode reward: total was -8.780000. running mean: -54.702955\n",
      "ep 785: ep_len:500 episode reward: total was -81.350000. running mean: -54.969425\n",
      "ep 785: ep_len:515 episode reward: total was -73.640000. running mean: -55.156131\n",
      "epsilon:0.165154 episode_count: 5502. steps_count: 2402122.000000\n",
      "Time elapsed:  6840.534403800964\n",
      "ep 786: ep_len:591 episode reward: total was -159.460000. running mean: -56.199169\n",
      "ep 786: ep_len:678 episode reward: total was -114.650000. running mean: -56.783678\n",
      "ep 786: ep_len:525 episode reward: total was -82.630000. running mean: -57.042141\n",
      "ep 786: ep_len:154 episode reward: total was -8.880000. running mean: -56.560520\n",
      "ep 786: ep_len:55 episode reward: total was 17.000000. running mean: -55.824914\n",
      "ep 786: ep_len:620 episode reward: total was -83.070000. running mean: -56.097365\n",
      "ep 786: ep_len:590 episode reward: total was -71.420000. running mean: -56.250592\n",
      "epsilon:0.165110 episode_count: 5509. steps_count: 2405335.000000\n",
      "Time elapsed:  6849.107499837875\n",
      "ep 787: ep_len:665 episode reward: total was -116.930000. running mean: -56.857386\n",
      "ep 787: ep_len:500 episode reward: total was -57.170000. running mean: -56.860512\n",
      "ep 787: ep_len:569 episode reward: total was -96.500000. running mean: -57.256907\n",
      "ep 787: ep_len:500 episode reward: total was -81.870000. running mean: -57.503038\n",
      "ep 787: ep_len:3 episode reward: total was -1.500000. running mean: -56.943007\n",
      "ep 787: ep_len:602 episode reward: total was -72.720000. running mean: -57.100777\n",
      "ep 787: ep_len:570 episode reward: total was -69.040000. running mean: -57.220169\n",
      "epsilon:0.165065 episode_count: 5516. steps_count: 2408744.000000\n",
      "Time elapsed:  6857.989734649658\n",
      "ep 788: ep_len:605 episode reward: total was -37.110000. running mean: -57.019068\n",
      "ep 788: ep_len:525 episode reward: total was -81.400000. running mean: -57.262877\n",
      "ep 788: ep_len:516 episode reward: total was -168.210000. running mean: -58.372348\n",
      "ep 788: ep_len:500 episode reward: total was -48.210000. running mean: -58.270725\n",
      "ep 788: ep_len:3 episode reward: total was 0.000000. running mean: -57.688018\n",
      "ep 788: ep_len:304 episode reward: total was -44.980000. running mean: -57.560937\n",
      "ep 788: ep_len:211 episode reward: total was -42.690000. running mean: -57.412228\n",
      "epsilon:0.165021 episode_count: 5523. steps_count: 2411408.000000\n",
      "Time elapsed:  6866.662518978119\n",
      "ep 789: ep_len:500 episode reward: total was -35.500000. running mean: -57.193106\n",
      "ep 789: ep_len:566 episode reward: total was 23.220000. running mean: -56.388975\n",
      "ep 789: ep_len:500 episode reward: total was -80.850000. running mean: -56.633585\n",
      "ep 789: ep_len:501 episode reward: total was -72.740000. running mean: -56.794649\n",
      "ep 789: ep_len:3 episode reward: total was 0.000000. running mean: -56.226703\n",
      "ep 789: ep_len:586 episode reward: total was -94.730000. running mean: -56.611736\n",
      "ep 789: ep_len:286 episode reward: total was -39.950000. running mean: -56.445118\n",
      "epsilon:0.164977 episode_count: 5530. steps_count: 2414350.000000\n",
      "Time elapsed:  6875.275733709335\n",
      "ep 790: ep_len:500 episode reward: total was -55.180000. running mean: -56.432467\n",
      "ep 790: ep_len:592 episode reward: total was -86.030000. running mean: -56.728442\n",
      "ep 790: ep_len:576 episode reward: total was -73.910000. running mean: -56.900258\n",
      "ep 790: ep_len:500 episode reward: total was -67.370000. running mean: -57.004955\n",
      "ep 790: ep_len:3 episode reward: total was 0.000000. running mean: -56.434906\n",
      "ep 790: ep_len:599 episode reward: total was -70.220000. running mean: -56.572757\n",
      "ep 790: ep_len:300 episode reward: total was -43.760000. running mean: -56.444629\n",
      "epsilon:0.164932 episode_count: 5537. steps_count: 2417420.000000\n",
      "Time elapsed:  6884.678032159805\n",
      "ep 791: ep_len:586 episode reward: total was -32.740000. running mean: -56.207583\n",
      "ep 791: ep_len:504 episode reward: total was -82.670000. running mean: -56.472207\n",
      "ep 791: ep_len:557 episode reward: total was -85.710000. running mean: -56.764585\n",
      "ep 791: ep_len:500 episode reward: total was -22.610000. running mean: -56.423039\n",
      "ep 791: ep_len:105 episode reward: total was 8.240000. running mean: -55.776409\n",
      "ep 791: ep_len:580 episode reward: total was -80.430000. running mean: -56.022945\n",
      "ep 791: ep_len:586 episode reward: total was -54.170000. running mean: -56.004415\n",
      "epsilon:0.164888 episode_count: 5544. steps_count: 2420838.000000\n",
      "Time elapsed:  6893.632982969284\n",
      "ep 792: ep_len:560 episode reward: total was -7.280000. running mean: -55.517171\n",
      "ep 792: ep_len:582 episode reward: total was -147.910000. running mean: -56.441099\n",
      "ep 792: ep_len:628 episode reward: total was -85.590000. running mean: -56.732588\n",
      "ep 792: ep_len:533 episode reward: total was -40.120000. running mean: -56.566462\n",
      "ep 792: ep_len:84 episode reward: total was -20.840000. running mean: -56.209198\n",
      "ep 792: ep_len:500 episode reward: total was -88.080000. running mean: -56.527906\n",
      "ep 792: ep_len:515 episode reward: total was -69.110000. running mean: -56.653727\n",
      "epsilon:0.164844 episode_count: 5551. steps_count: 2424240.000000\n",
      "Time elapsed:  6902.810130357742\n",
      "ep 793: ep_len:579 episode reward: total was 3.100000. running mean: -56.056190\n",
      "ep 793: ep_len:520 episode reward: total was -88.230000. running mean: -56.377928\n",
      "ep 793: ep_len:567 episode reward: total was -63.360000. running mean: -56.447748\n",
      "ep 793: ep_len:40 episode reward: total was 3.660000. running mean: -55.846671\n",
      "ep 793: ep_len:96 episode reward: total was 12.250000. running mean: -55.165704\n",
      "ep 793: ep_len:542 episode reward: total was -108.920000. running mean: -55.703247\n",
      "ep 793: ep_len:520 episode reward: total was -90.210000. running mean: -56.048315\n",
      "epsilon:0.164799 episode_count: 5558. steps_count: 2427104.000000\n",
      "Time elapsed:  6910.46981549263\n",
      "ep 794: ep_len:551 episode reward: total was -66.840000. running mean: -56.156232\n",
      "ep 794: ep_len:581 episode reward: total was -65.430000. running mean: -56.248969\n",
      "ep 794: ep_len:500 episode reward: total was -31.540000. running mean: -56.001879\n",
      "ep 794: ep_len:500 episode reward: total was -27.550000. running mean: -55.717361\n",
      "ep 794: ep_len:3 episode reward: total was -1.500000. running mean: -55.175187\n",
      "ep 794: ep_len:623 episode reward: total was -72.240000. running mean: -55.345835\n",
      "ep 794: ep_len:613 episode reward: total was -71.070000. running mean: -55.503077\n",
      "epsilon:0.164755 episode_count: 5565. steps_count: 2430475.000000\n",
      "Time elapsed:  6919.549000501633\n",
      "ep 795: ep_len:516 episode reward: total was -115.040000. running mean: -56.098446\n",
      "ep 795: ep_len:588 episode reward: total was -172.180000. running mean: -57.259262\n",
      "ep 795: ep_len:645 episode reward: total was -115.080000. running mean: -57.837469\n",
      "ep 795: ep_len:500 episode reward: total was -40.400000. running mean: -57.663094\n",
      "ep 795: ep_len:3 episode reward: total was 0.000000. running mean: -57.086463\n",
      "ep 795: ep_len:612 episode reward: total was -82.600000. running mean: -57.341599\n",
      "ep 795: ep_len:572 episode reward: total was -69.600000. running mean: -57.464183\n",
      "epsilon:0.164711 episode_count: 5572. steps_count: 2433911.000000\n",
      "Time elapsed:  6927.6411826610565\n",
      "ep 796: ep_len:625 episode reward: total was -49.200000. running mean: -57.381541\n",
      "ep 796: ep_len:508 episode reward: total was -97.520000. running mean: -57.782926\n",
      "ep 796: ep_len:587 episode reward: total was -101.780000. running mean: -58.222896\n",
      "ep 796: ep_len:502 episode reward: total was -55.570000. running mean: -58.196367\n",
      "ep 796: ep_len:3 episode reward: total was 0.000000. running mean: -57.614404\n",
      "ep 796: ep_len:274 episode reward: total was -26.730000. running mean: -57.305560\n",
      "ep 796: ep_len:532 episode reward: total was -110.570000. running mean: -57.838204\n",
      "epsilon:0.164666 episode_count: 5579. steps_count: 2436942.000000\n",
      "Time elapsed:  6935.604727983475\n",
      "ep 797: ep_len:631 episode reward: total was -83.970000. running mean: -58.099522\n",
      "ep 797: ep_len:500 episode reward: total was -47.330000. running mean: -57.991827\n",
      "ep 797: ep_len:554 episode reward: total was -39.210000. running mean: -57.804008\n",
      "ep 797: ep_len:128 episode reward: total was -5.050000. running mean: -57.276468\n",
      "ep 797: ep_len:68 episode reward: total was -45.320000. running mean: -57.156904\n",
      "ep 797: ep_len:256 episode reward: total was 0.170000. running mean: -56.583635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 797: ep_len:311 episode reward: total was -89.200000. running mean: -56.909798\n",
      "epsilon:0.164622 episode_count: 5586. steps_count: 2439390.000000\n",
      "Time elapsed:  6942.988794565201\n",
      "ep 798: ep_len:566 episode reward: total was -74.660000. running mean: -57.087300\n",
      "ep 798: ep_len:760 episode reward: total was -232.740000. running mean: -58.843827\n",
      "ep 798: ep_len:500 episode reward: total was -41.810000. running mean: -58.673489\n",
      "ep 798: ep_len:500 episode reward: total was 9.040000. running mean: -57.996354\n",
      "ep 798: ep_len:130 episode reward: total was 0.290000. running mean: -57.413491\n",
      "ep 798: ep_len:500 episode reward: total was -22.170000. running mean: -57.061056\n",
      "ep 798: ep_len:622 episode reward: total was -119.750000. running mean: -57.687945\n",
      "epsilon:0.164578 episode_count: 5593. steps_count: 2442968.000000\n",
      "Time elapsed:  6952.220747709274\n",
      "ep 799: ep_len:672 episode reward: total was -110.270000. running mean: -58.213766\n",
      "ep 799: ep_len:500 episode reward: total was -24.170000. running mean: -57.873328\n",
      "ep 799: ep_len:538 episode reward: total was -86.630000. running mean: -58.160895\n",
      "ep 799: ep_len:543 episode reward: total was -16.360000. running mean: -57.742886\n",
      "ep 799: ep_len:55 episode reward: total was 18.500000. running mean: -56.980457\n",
      "ep 799: ep_len:500 episode reward: total was -73.340000. running mean: -57.144052\n",
      "ep 799: ep_len:535 episode reward: total was -95.860000. running mean: -57.531212\n",
      "epsilon:0.164533 episode_count: 5600. steps_count: 2446311.000000\n",
      "Time elapsed:  6960.9623165130615\n",
      "ep 800: ep_len:588 episode reward: total was -36.480000. running mean: -57.320700\n",
      "ep 800: ep_len:539 episode reward: total was -21.200000. running mean: -56.959493\n",
      "ep 800: ep_len:596 episode reward: total was -78.430000. running mean: -57.174198\n",
      "ep 800: ep_len:501 episode reward: total was -33.290000. running mean: -56.935356\n",
      "ep 800: ep_len:108 episode reward: total was -41.690000. running mean: -56.782902\n",
      "ep 800: ep_len:500 episode reward: total was -39.540000. running mean: -56.610473\n",
      "ep 800: ep_len:500 episode reward: total was -94.110000. running mean: -56.985469\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.164489 episode_count: 5607. steps_count: 2449643.000000\n",
      "Time elapsed:  6974.985905647278\n",
      "ep 801: ep_len:500 episode reward: total was -40.660000. running mean: -56.822214\n",
      "ep 801: ep_len:595 episode reward: total was -54.670000. running mean: -56.800692\n",
      "ep 801: ep_len:597 episode reward: total was -77.040000. running mean: -57.003085\n",
      "ep 801: ep_len:132 episode reward: total was -3.540000. running mean: -56.468454\n",
      "ep 801: ep_len:1 episode reward: total was -1.000000. running mean: -55.913769\n",
      "ep 801: ep_len:505 episode reward: total was -51.070000. running mean: -55.865332\n",
      "ep 801: ep_len:500 episode reward: total was -90.700000. running mean: -56.213678\n",
      "epsilon:0.164445 episode_count: 5614. steps_count: 2452473.000000\n",
      "Time elapsed:  6982.771551609039\n",
      "ep 802: ep_len:576 episode reward: total was -57.390000. running mean: -56.225442\n",
      "ep 802: ep_len:500 episode reward: total was -51.540000. running mean: -56.178587\n",
      "ep 802: ep_len:600 episode reward: total was -85.260000. running mean: -56.469401\n",
      "ep 802: ep_len:546 episode reward: total was -19.410000. running mean: -56.098807\n",
      "ep 802: ep_len:3 episode reward: total was 1.010000. running mean: -55.527719\n",
      "ep 802: ep_len:529 episode reward: total was -132.220000. running mean: -56.294642\n",
      "ep 802: ep_len:547 episode reward: total was -84.090000. running mean: -56.572596\n",
      "epsilon:0.164400 episode_count: 5621. steps_count: 2455774.000000\n",
      "Time elapsed:  6992.02904343605\n",
      "ep 803: ep_len:103 episode reward: total was -14.190000. running mean: -56.148770\n",
      "ep 803: ep_len:592 episode reward: total was -87.950000. running mean: -56.466782\n",
      "ep 803: ep_len:568 episode reward: total was -112.770000. running mean: -57.029814\n",
      "ep 803: ep_len:502 episode reward: total was -62.330000. running mean: -57.082816\n",
      "ep 803: ep_len:3 episode reward: total was -3.000000. running mean: -56.541988\n",
      "ep 803: ep_len:582 episode reward: total was -55.280000. running mean: -56.529368\n",
      "ep 803: ep_len:504 episode reward: total was -86.630000. running mean: -56.830374\n",
      "epsilon:0.164356 episode_count: 5628. steps_count: 2458628.000000\n",
      "Time elapsed:  6999.722989082336\n",
      "ep 804: ep_len:229 episode reward: total was -9.840000. running mean: -56.360471\n",
      "ep 804: ep_len:500 episode reward: total was -23.440000. running mean: -56.031266\n",
      "ep 804: ep_len:575 episode reward: total was -67.860000. running mean: -56.149553\n",
      "ep 804: ep_len:503 episode reward: total was -37.280000. running mean: -55.960858\n",
      "ep 804: ep_len:3 episode reward: total was 0.000000. running mean: -55.401249\n",
      "ep 804: ep_len:540 episode reward: total was -68.330000. running mean: -55.530537\n",
      "ep 804: ep_len:511 episode reward: total was -53.410000. running mean: -55.509331\n",
      "epsilon:0.164312 episode_count: 5635. steps_count: 2461489.000000\n",
      "Time elapsed:  7007.807117938995\n",
      "ep 805: ep_len:549 episode reward: total was -56.010000. running mean: -55.514338\n",
      "ep 805: ep_len:500 episode reward: total was -100.230000. running mean: -55.961495\n",
      "ep 805: ep_len:500 episode reward: total was -53.150000. running mean: -55.933380\n",
      "ep 805: ep_len:611 episode reward: total was -17.990000. running mean: -55.553946\n",
      "ep 805: ep_len:2 episode reward: total was -0.500000. running mean: -55.003406\n",
      "ep 805: ep_len:512 episode reward: total was -134.250000. running mean: -55.795872\n",
      "ep 805: ep_len:536 episode reward: total was -80.750000. running mean: -56.045414\n",
      "epsilon:0.164267 episode_count: 5642. steps_count: 2464699.000000\n",
      "Time elapsed:  7016.281816005707\n",
      "ep 806: ep_len:531 episode reward: total was -141.120000. running mean: -56.896159\n",
      "ep 806: ep_len:500 episode reward: total was -29.880000. running mean: -56.625998\n",
      "ep 806: ep_len:500 episode reward: total was -89.090000. running mean: -56.950638\n",
      "ep 806: ep_len:500 episode reward: total was -41.050000. running mean: -56.791631\n",
      "ep 806: ep_len:3 episode reward: total was 0.000000. running mean: -56.223715\n",
      "ep 806: ep_len:244 episode reward: total was 11.250000. running mean: -55.548978\n",
      "ep 806: ep_len:632 episode reward: total was -80.740000. running mean: -55.800888\n",
      "epsilon:0.164223 episode_count: 5649. steps_count: 2467609.000000\n",
      "Time elapsed:  7024.748404502869\n",
      "ep 807: ep_len:711 episode reward: total was -180.610000. running mean: -57.048979\n",
      "ep 807: ep_len:519 episode reward: total was -46.170000. running mean: -56.940190\n",
      "ep 807: ep_len:598 episode reward: total was -67.570000. running mean: -57.046488\n",
      "ep 807: ep_len:502 episode reward: total was -2.220000. running mean: -56.498223\n",
      "ep 807: ep_len:91 episode reward: total was -42.820000. running mean: -56.361441\n",
      "ep 807: ep_len:687 episode reward: total was -43.580000. running mean: -56.233626\n",
      "ep 807: ep_len:575 episode reward: total was -43.390000. running mean: -56.105190\n",
      "epsilon:0.164179 episode_count: 5656. steps_count: 2471292.000000\n",
      "Time elapsed:  7035.096317768097\n",
      "ep 808: ep_len:535 episode reward: total was -82.570000. running mean: -56.369838\n",
      "ep 808: ep_len:645 episode reward: total was -41.130000. running mean: -56.217440\n",
      "ep 808: ep_len:602 episode reward: total was -79.100000. running mean: -56.446265\n",
      "ep 808: ep_len:166 episode reward: total was 0.170000. running mean: -55.880103\n",
      "ep 808: ep_len:3 episode reward: total was 0.000000. running mean: -55.321302\n",
      "ep 808: ep_len:500 episode reward: total was -94.290000. running mean: -55.710989\n",
      "ep 808: ep_len:553 episode reward: total was -89.880000. running mean: -56.052679\n",
      "epsilon:0.164134 episode_count: 5663. steps_count: 2474296.000000\n",
      "Time elapsed:  7043.088388204575\n",
      "ep 809: ep_len:504 episode reward: total was -100.750000. running mean: -56.499652\n",
      "ep 809: ep_len:547 episode reward: total was -41.980000. running mean: -56.354455\n",
      "ep 809: ep_len:500 episode reward: total was -31.880000. running mean: -56.109711\n",
      "ep 809: ep_len:594 episode reward: total was -24.650000. running mean: -55.795114\n",
      "ep 809: ep_len:84 episode reward: total was -5.350000. running mean: -55.290663\n",
      "ep 809: ep_len:500 episode reward: total was -12.980000. running mean: -54.867556\n",
      "ep 809: ep_len:323 episode reward: total was -48.880000. running mean: -54.807680\n",
      "epsilon:0.164090 episode_count: 5670. steps_count: 2477348.000000\n",
      "Time elapsed:  7051.066034793854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 810: ep_len:500 episode reward: total was 1.940000. running mean: -54.240204\n",
      "ep 810: ep_len:555 episode reward: total was 1.960000. running mean: -53.678202\n",
      "ep 810: ep_len:535 episode reward: total was -18.600000. running mean: -53.327419\n",
      "ep 810: ep_len:562 episode reward: total was -104.980000. running mean: -53.843945\n",
      "ep 810: ep_len:3 episode reward: total was -1.500000. running mean: -53.320506\n",
      "ep 810: ep_len:550 episode reward: total was -120.760000. running mean: -53.994901\n",
      "ep 810: ep_len:600 episode reward: total was -84.440000. running mean: -54.299352\n",
      "epsilon:0.164046 episode_count: 5677. steps_count: 2480653.000000\n",
      "Time elapsed:  7059.816360473633\n",
      "ep 811: ep_len:545 episode reward: total was 6.670000. running mean: -53.689658\n",
      "ep 811: ep_len:235 episode reward: total was -25.780000. running mean: -53.410562\n",
      "ep 811: ep_len:455 episode reward: total was 5.270000. running mean: -52.823756\n",
      "ep 811: ep_len:583 episode reward: total was -86.010000. running mean: -53.155618\n",
      "ep 811: ep_len:3 episode reward: total was 0.000000. running mean: -52.624062\n",
      "ep 811: ep_len:500 episode reward: total was -68.260000. running mean: -52.780422\n",
      "ep 811: ep_len:546 episode reward: total was -50.960000. running mean: -52.762217\n",
      "epsilon:0.164001 episode_count: 5684. steps_count: 2483520.000000\n",
      "Time elapsed:  7067.719419717789\n",
      "ep 812: ep_len:500 episode reward: total was -5.880000. running mean: -52.293395\n",
      "ep 812: ep_len:500 episode reward: total was -64.490000. running mean: -52.415361\n",
      "ep 812: ep_len:545 episode reward: total was -213.810000. running mean: -54.029308\n",
      "ep 812: ep_len:500 episode reward: total was -6.490000. running mean: -53.553915\n",
      "ep 812: ep_len:3 episode reward: total was -1.500000. running mean: -53.033376\n",
      "ep 812: ep_len:551 episode reward: total was -34.160000. running mean: -52.844642\n",
      "ep 812: ep_len:540 episode reward: total was -71.860000. running mean: -53.034795\n",
      "epsilon:0.163957 episode_count: 5691. steps_count: 2486659.000000\n",
      "Time elapsed:  7075.903436899185\n",
      "ep 813: ep_len:511 episode reward: total was 5.870000. running mean: -52.445747\n",
      "ep 813: ep_len:594 episode reward: total was 36.890000. running mean: -51.552390\n",
      "ep 813: ep_len:602 episode reward: total was -79.710000. running mean: -51.833966\n",
      "ep 813: ep_len:601 episode reward: total was -9.220000. running mean: -51.407826\n",
      "ep 813: ep_len:3 episode reward: total was 0.000000. running mean: -50.893748\n",
      "ep 813: ep_len:500 episode reward: total was -45.960000. running mean: -50.844411\n",
      "ep 813: ep_len:560 episode reward: total was -50.160000. running mean: -50.837566\n",
      "epsilon:0.163913 episode_count: 5698. steps_count: 2490030.000000\n",
      "Time elapsed:  7085.45222401619\n",
      "ep 814: ep_len:568 episode reward: total was -27.780000. running mean: -50.606991\n",
      "ep 814: ep_len:575 episode reward: total was -78.920000. running mean: -50.890121\n",
      "ep 814: ep_len:653 episode reward: total was -77.470000. running mean: -51.155920\n",
      "ep 814: ep_len:514 episode reward: total was -50.370000. running mean: -51.148061\n",
      "ep 814: ep_len:128 episode reward: total was 6.300000. running mean: -50.573580\n",
      "ep 814: ep_len:227 episode reward: total was -4.350000. running mean: -50.111344\n",
      "ep 814: ep_len:513 episode reward: total was -70.590000. running mean: -50.316131\n",
      "epsilon:0.163868 episode_count: 5705. steps_count: 2493208.000000\n",
      "Time elapsed:  7094.024796485901\n",
      "ep 815: ep_len:569 episode reward: total was -118.410000. running mean: -50.997069\n",
      "ep 815: ep_len:506 episode reward: total was -12.960000. running mean: -50.616699\n",
      "ep 815: ep_len:782 episode reward: total was -276.480000. running mean: -52.875332\n",
      "ep 815: ep_len:562 episode reward: total was -42.130000. running mean: -52.767878\n",
      "ep 815: ep_len:93 episode reward: total was -11.720000. running mean: -52.357400\n",
      "ep 815: ep_len:660 episode reward: total was -51.500000. running mean: -52.348826\n",
      "ep 815: ep_len:547 episode reward: total was -41.450000. running mean: -52.239837\n",
      "epsilon:0.163824 episode_count: 5712. steps_count: 2496927.000000\n",
      "Time elapsed:  7103.604652643204\n",
      "ep 816: ep_len:512 episode reward: total was 13.260000. running mean: -51.584839\n",
      "ep 816: ep_len:188 episode reward: total was -20.240000. running mean: -51.271391\n",
      "ep 816: ep_len:673 episode reward: total was -86.870000. running mean: -51.627377\n",
      "ep 816: ep_len:502 episode reward: total was -85.080000. running mean: -51.961903\n",
      "ep 816: ep_len:3 episode reward: total was -1.500000. running mean: -51.457284\n",
      "ep 816: ep_len:503 episode reward: total was -44.450000. running mean: -51.387211\n",
      "ep 816: ep_len:501 episode reward: total was -27.070000. running mean: -51.144039\n",
      "epsilon:0.163780 episode_count: 5719. steps_count: 2499809.000000\n",
      "Time elapsed:  7111.101050853729\n",
      "ep 817: ep_len:586 episode reward: total was 5.870000. running mean: -50.573899\n",
      "ep 817: ep_len:520 episode reward: total was -88.920000. running mean: -50.957360\n",
      "ep 817: ep_len:633 episode reward: total was -95.270000. running mean: -51.400486\n",
      "ep 817: ep_len:500 episode reward: total was -48.840000. running mean: -51.374881\n",
      "ep 817: ep_len:56 episode reward: total was 11.500000. running mean: -50.746132\n",
      "ep 817: ep_len:688 episode reward: total was -80.270000. running mean: -51.041371\n",
      "ep 817: ep_len:593 episode reward: total was -43.060000. running mean: -50.961557\n",
      "epsilon:0.163735 episode_count: 5726. steps_count: 2503385.000000\n",
      "Time elapsed:  7119.9766545295715\n",
      "ep 818: ep_len:500 episode reward: total was -68.050000. running mean: -51.132442\n",
      "ep 818: ep_len:516 episode reward: total was -20.990000. running mean: -50.831017\n",
      "ep 818: ep_len:610 episode reward: total was -59.670000. running mean: -50.919407\n",
      "ep 818: ep_len:508 episode reward: total was -12.290000. running mean: -50.533113\n",
      "ep 818: ep_len:133 episode reward: total was -5.680000. running mean: -50.084582\n",
      "ep 818: ep_len:500 episode reward: total was -62.530000. running mean: -50.209036\n",
      "ep 818: ep_len:543 episode reward: total was -60.630000. running mean: -50.313246\n",
      "epsilon:0.163691 episode_count: 5733. steps_count: 2506695.000000\n",
      "Time elapsed:  7128.635336637497\n",
      "ep 819: ep_len:500 episode reward: total was -48.070000. running mean: -50.290813\n",
      "ep 819: ep_len:177 episode reward: total was -32.230000. running mean: -50.110205\n",
      "ep 819: ep_len:586 episode reward: total was -64.020000. running mean: -50.249303\n",
      "ep 819: ep_len:517 episode reward: total was -80.150000. running mean: -50.548310\n",
      "ep 819: ep_len:114 episode reward: total was -8.250000. running mean: -50.125327\n",
      "ep 819: ep_len:502 episode reward: total was -53.730000. running mean: -50.161374\n",
      "ep 819: ep_len:297 episode reward: total was -43.490000. running mean: -50.094660\n",
      "epsilon:0.163647 episode_count: 5740. steps_count: 2509388.000000\n",
      "Time elapsed:  7135.915898323059\n",
      "ep 820: ep_len:576 episode reward: total was -51.390000. running mean: -50.107613\n",
      "ep 820: ep_len:618 episode reward: total was -65.430000. running mean: -50.260837\n",
      "ep 820: ep_len:500 episode reward: total was -69.040000. running mean: -50.448629\n",
      "ep 820: ep_len:500 episode reward: total was -30.200000. running mean: -50.246143\n",
      "ep 820: ep_len:43 episode reward: total was 12.500000. running mean: -49.618681\n",
      "ep 820: ep_len:500 episode reward: total was -84.650000. running mean: -49.968994\n",
      "ep 820: ep_len:629 episode reward: total was -121.320000. running mean: -50.682504\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.163602 episode_count: 5747. steps_count: 2512754.000000\n",
      "Time elapsed:  7149.588993310928\n",
      "ep 821: ep_len:500 episode reward: total was -19.770000. running mean: -50.373379\n",
      "ep 821: ep_len:533 episode reward: total was -64.920000. running mean: -50.518846\n",
      "ep 821: ep_len:522 episode reward: total was -52.370000. running mean: -50.537357\n",
      "ep 821: ep_len:500 episode reward: total was -29.730000. running mean: -50.329283\n",
      "ep 821: ep_len:3 episode reward: total was -1.500000. running mean: -49.840991\n",
      "ep 821: ep_len:500 episode reward: total was -74.490000. running mean: -50.087481\n",
      "ep 821: ep_len:531 episode reward: total was -53.750000. running mean: -50.124106\n",
      "epsilon:0.163558 episode_count: 5754. steps_count: 2515843.000000\n",
      "Time elapsed:  7158.027211427689\n",
      "ep 822: ep_len:500 episode reward: total was -6.780000. running mean: -49.690665\n",
      "ep 822: ep_len:540 episode reward: total was -115.590000. running mean: -50.349658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 822: ep_len:387 episode reward: total was -31.790000. running mean: -50.164062\n",
      "ep 822: ep_len:534 episode reward: total was -22.640000. running mean: -49.888821\n",
      "ep 822: ep_len:3 episode reward: total was 0.000000. running mean: -49.389933\n",
      "ep 822: ep_len:510 episode reward: total was -64.240000. running mean: -49.538433\n",
      "ep 822: ep_len:577 episode reward: total was -54.310000. running mean: -49.586149\n",
      "epsilon:0.163514 episode_count: 5761. steps_count: 2518894.000000\n",
      "Time elapsed:  7166.185187339783\n",
      "ep 823: ep_len:500 episode reward: total was 4.540000. running mean: -49.044888\n",
      "ep 823: ep_len:518 episode reward: total was -40.250000. running mean: -48.956939\n",
      "ep 823: ep_len:551 episode reward: total was -85.810000. running mean: -49.325469\n",
      "ep 823: ep_len:502 episode reward: total was -47.500000. running mean: -49.307215\n",
      "ep 823: ep_len:100 episode reward: total was 7.240000. running mean: -48.741743\n",
      "ep 823: ep_len:516 episode reward: total was -56.430000. running mean: -48.818625\n",
      "ep 823: ep_len:511 episode reward: total was -59.020000. running mean: -48.920639\n",
      "epsilon:0.163469 episode_count: 5768. steps_count: 2522092.000000\n",
      "Time elapsed:  7174.564410448074\n",
      "ep 824: ep_len:588 episode reward: total was 4.480000. running mean: -48.386632\n",
      "ep 824: ep_len:508 episode reward: total was -40.920000. running mean: -48.311966\n",
      "ep 824: ep_len:500 episode reward: total was -26.520000. running mean: -48.094047\n",
      "ep 824: ep_len:500 episode reward: total was -50.930000. running mean: -48.122406\n",
      "ep 824: ep_len:3 episode reward: total was 0.000000. running mean: -47.641182\n",
      "ep 824: ep_len:643 episode reward: total was -91.270000. running mean: -48.077470\n",
      "ep 824: ep_len:207 episode reward: total was -74.410000. running mean: -48.340795\n",
      "epsilon:0.163425 episode_count: 5775. steps_count: 2525041.000000\n",
      "Time elapsed:  7182.572629213333\n",
      "ep 825: ep_len:508 episode reward: total was -44.770000. running mean: -48.305088\n",
      "ep 825: ep_len:500 episode reward: total was -24.700000. running mean: -48.069037\n",
      "ep 825: ep_len:500 episode reward: total was -64.460000. running mean: -48.232946\n",
      "ep 825: ep_len:536 episode reward: total was -7.990000. running mean: -47.830517\n",
      "ep 825: ep_len:32 episode reward: total was 11.500000. running mean: -47.237212\n",
      "ep 825: ep_len:186 episode reward: total was 5.080000. running mean: -46.714040\n",
      "ep 825: ep_len:500 episode reward: total was -52.340000. running mean: -46.770299\n",
      "epsilon:0.163381 episode_count: 5782. steps_count: 2527803.000000\n",
      "Time elapsed:  7190.117477655411\n",
      "ep 826: ep_len:678 episode reward: total was -75.110000. running mean: -47.053696\n",
      "ep 826: ep_len:594 episode reward: total was -47.630000. running mean: -47.059459\n",
      "ep 826: ep_len:521 episode reward: total was -81.360000. running mean: -47.402465\n",
      "ep 826: ep_len:56 episode reward: total was -1.200000. running mean: -46.940440\n",
      "ep 826: ep_len:3 episode reward: total was 0.000000. running mean: -46.471036\n",
      "ep 826: ep_len:566 episode reward: total was -50.620000. running mean: -46.512525\n",
      "ep 826: ep_len:541 episode reward: total was -36.040000. running mean: -46.407800\n",
      "epsilon:0.163336 episode_count: 5789. steps_count: 2530762.000000\n",
      "Time elapsed:  7198.830612182617\n",
      "ep 827: ep_len:500 episode reward: total was -16.330000. running mean: -46.107022\n",
      "ep 827: ep_len:500 episode reward: total was -13.460000. running mean: -45.780552\n",
      "ep 827: ep_len:374 episode reward: total was -15.290000. running mean: -45.475646\n",
      "ep 827: ep_len:170 episode reward: total was 8.630000. running mean: -44.934590\n",
      "ep 827: ep_len:3 episode reward: total was 0.000000. running mean: -44.485244\n",
      "ep 827: ep_len:521 episode reward: total was -35.070000. running mean: -44.391091\n",
      "ep 827: ep_len:505 episode reward: total was -82.050000. running mean: -44.767680\n",
      "epsilon:0.163292 episode_count: 5796. steps_count: 2533335.000000\n",
      "Time elapsed:  7205.870558977127\n",
      "ep 828: ep_len:588 episode reward: total was -85.270000. running mean: -45.172704\n",
      "ep 828: ep_len:619 episode reward: total was 56.890000. running mean: -44.152077\n",
      "ep 828: ep_len:624 episode reward: total was -95.090000. running mean: -44.661456\n",
      "ep 828: ep_len:532 episode reward: total was -35.120000. running mean: -44.566041\n",
      "ep 828: ep_len:41 episode reward: total was 14.500000. running mean: -43.975381\n",
      "ep 828: ep_len:625 episode reward: total was -63.990000. running mean: -44.175527\n",
      "ep 828: ep_len:177 episode reward: total was -51.480000. running mean: -44.248572\n",
      "epsilon:0.163248 episode_count: 5803. steps_count: 2536541.000000\n",
      "Time elapsed:  7214.6154017448425\n",
      "ep 829: ep_len:500 episode reward: total was -64.920000. running mean: -44.455286\n",
      "ep 829: ep_len:501 episode reward: total was -63.410000. running mean: -44.644833\n",
      "ep 829: ep_len:380 episode reward: total was -8.750000. running mean: -44.285885\n",
      "ep 829: ep_len:502 episode reward: total was -42.390000. running mean: -44.266926\n",
      "ep 829: ep_len:3 episode reward: total was 0.000000. running mean: -43.824257\n",
      "ep 829: ep_len:531 episode reward: total was -39.110000. running mean: -43.777114\n",
      "ep 829: ep_len:590 episode reward: total was -44.760000. running mean: -43.786943\n",
      "epsilon:0.163203 episode_count: 5810. steps_count: 2539548.000000\n",
      "Time elapsed:  7222.582725763321\n",
      "ep 830: ep_len:562 episode reward: total was -40.140000. running mean: -43.750474\n",
      "ep 830: ep_len:500 episode reward: total was -29.390000. running mean: -43.606869\n",
      "ep 830: ep_len:592 episode reward: total was -37.170000. running mean: -43.542500\n",
      "ep 830: ep_len:114 episode reward: total was -1.670000. running mean: -43.123775\n",
      "ep 830: ep_len:90 episode reward: total was 1.110000. running mean: -42.681437\n",
      "ep 830: ep_len:604 episode reward: total was -55.790000. running mean: -42.812523\n",
      "ep 830: ep_len:250 episode reward: total was -23.120000. running mean: -42.615598\n",
      "epsilon:0.163159 episode_count: 5817. steps_count: 2542260.000000\n",
      "Time elapsed:  7229.803780794144\n",
      "ep 831: ep_len:553 episode reward: total was -103.170000. running mean: -43.221142\n",
      "ep 831: ep_len:525 episode reward: total was -16.130000. running mean: -42.950230\n",
      "ep 831: ep_len:594 episode reward: total was -54.510000. running mean: -43.065828\n",
      "ep 831: ep_len:608 episode reward: total was -10.410000. running mean: -42.739270\n",
      "ep 831: ep_len:3 episode reward: total was 0.000000. running mean: -42.311877\n",
      "ep 831: ep_len:670 episode reward: total was -101.540000. running mean: -42.904158\n",
      "ep 831: ep_len:186 episode reward: total was -9.640000. running mean: -42.571517\n",
      "epsilon:0.163115 episode_count: 5824. steps_count: 2545399.000000\n",
      "Time elapsed:  7238.95875287056\n",
      "ep 832: ep_len:500 episode reward: total was -85.220000. running mean: -42.998002\n",
      "ep 832: ep_len:623 episode reward: total was -30.640000. running mean: -42.874422\n",
      "ep 832: ep_len:654 episode reward: total was -61.030000. running mean: -43.055977\n",
      "ep 832: ep_len:565 episode reward: total was -70.160000. running mean: -43.327018\n",
      "ep 832: ep_len:3 episode reward: total was 0.000000. running mean: -42.893747\n",
      "ep 832: ep_len:678 episode reward: total was -170.500000. running mean: -44.169810\n",
      "ep 832: ep_len:202 episode reward: total was -54.810000. running mean: -44.276212\n",
      "epsilon:0.163070 episode_count: 5831. steps_count: 2548624.000000\n",
      "Time elapsed:  7246.934903383255\n",
      "ep 833: ep_len:637 episode reward: total was -88.900000. running mean: -44.722450\n",
      "ep 833: ep_len:552 episode reward: total was 0.220000. running mean: -44.273025\n",
      "ep 833: ep_len:674 episode reward: total was -311.190000. running mean: -46.942195\n",
      "ep 833: ep_len:617 episode reward: total was -115.160000. running mean: -47.624373\n",
      "ep 833: ep_len:3 episode reward: total was 0.000000. running mean: -47.148129\n",
      "ep 833: ep_len:508 episode reward: total was -52.370000. running mean: -47.200348\n",
      "ep 833: ep_len:194 episode reward: total was -29.670000. running mean: -47.025045\n",
      "epsilon:0.163026 episode_count: 5838. steps_count: 2551809.000000\n",
      "Time elapsed:  7254.790358781815\n",
      "ep 834: ep_len:241 episode reward: total was -15.250000. running mean: -46.707294\n",
      "ep 834: ep_len:636 episode reward: total was -71.990000. running mean: -46.960121\n",
      "ep 834: ep_len:500 episode reward: total was -66.930000. running mean: -47.159820\n",
      "ep 834: ep_len:530 episode reward: total was -74.630000. running mean: -47.434522\n",
      "ep 834: ep_len:3 episode reward: total was 0.000000. running mean: -46.960177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 834: ep_len:331 episode reward: total was -7.420000. running mean: -46.564775\n",
      "ep 834: ep_len:596 episode reward: total was -38.950000. running mean: -46.488627\n",
      "epsilon:0.162982 episode_count: 5845. steps_count: 2554646.000000\n",
      "Time elapsed:  7262.445932865143\n",
      "ep 835: ep_len:628 episode reward: total was -100.410000. running mean: -47.027841\n",
      "ep 835: ep_len:616 episode reward: total was 31.820000. running mean: -46.239362\n",
      "ep 835: ep_len:79 episode reward: total was -1.240000. running mean: -45.789369\n",
      "ep 835: ep_len:56 episode reward: total was -8.650000. running mean: -45.417975\n",
      "ep 835: ep_len:3 episode reward: total was 0.000000. running mean: -44.963795\n",
      "ep 835: ep_len:500 episode reward: total was -51.740000. running mean: -45.031557\n",
      "ep 835: ep_len:500 episode reward: total was -51.840000. running mean: -45.099642\n",
      "epsilon:0.162937 episode_count: 5852. steps_count: 2557028.000000\n",
      "Time elapsed:  7269.1653916835785\n",
      "ep 836: ep_len:508 episode reward: total was -12.730000. running mean: -44.775945\n",
      "ep 836: ep_len:500 episode reward: total was -67.160000. running mean: -44.999786\n",
      "ep 836: ep_len:639 episode reward: total was -136.250000. running mean: -45.912288\n",
      "ep 836: ep_len:508 episode reward: total was -60.280000. running mean: -46.055965\n",
      "ep 836: ep_len:3 episode reward: total was 0.000000. running mean: -45.595406\n",
      "ep 836: ep_len:575 episode reward: total was -33.520000. running mean: -45.474651\n",
      "ep 836: ep_len:201 episode reward: total was -57.270000. running mean: -45.592605\n",
      "epsilon:0.162893 episode_count: 5859. steps_count: 2559962.000000\n",
      "Time elapsed:  7275.918993711472\n",
      "ep 837: ep_len:615 episode reward: total was -3.570000. running mean: -45.172379\n",
      "ep 837: ep_len:549 episode reward: total was -8.070000. running mean: -44.801355\n",
      "ep 837: ep_len:437 episode reward: total was 1.140000. running mean: -44.341942\n",
      "ep 837: ep_len:500 episode reward: total was -79.500000. running mean: -44.693522\n",
      "ep 837: ep_len:3 episode reward: total was -1.500000. running mean: -44.261587\n",
      "ep 837: ep_len:500 episode reward: total was -34.010000. running mean: -44.159071\n",
      "ep 837: ep_len:198 episode reward: total was -59.780000. running mean: -44.315280\n",
      "epsilon:0.162849 episode_count: 5866. steps_count: 2562764.000000\n",
      "Time elapsed:  7283.227263212204\n",
      "ep 838: ep_len:530 episode reward: total was -87.610000. running mean: -44.748228\n",
      "ep 838: ep_len:591 episode reward: total was -110.470000. running mean: -45.405445\n",
      "ep 838: ep_len:441 episode reward: total was -26.610000. running mean: -45.217491\n",
      "ep 838: ep_len:589 episode reward: total was -46.990000. running mean: -45.235216\n",
      "ep 838: ep_len:3 episode reward: total was 0.000000. running mean: -44.782864\n",
      "ep 838: ep_len:226 episode reward: total was -1.210000. running mean: -44.347135\n",
      "ep 838: ep_len:517 episode reward: total was -59.110000. running mean: -44.494764\n",
      "epsilon:0.162804 episode_count: 5873. steps_count: 2565661.000000\n",
      "Time elapsed:  7291.111671924591\n",
      "ep 839: ep_len:539 episode reward: total was -74.020000. running mean: -44.790016\n",
      "ep 839: ep_len:500 episode reward: total was -45.020000. running mean: -44.792316\n",
      "ep 839: ep_len:550 episode reward: total was -28.050000. running mean: -44.624893\n",
      "ep 839: ep_len:526 episode reward: total was -30.000000. running mean: -44.478644\n",
      "ep 839: ep_len:3 episode reward: total was 0.000000. running mean: -44.033857\n",
      "ep 839: ep_len:514 episode reward: total was -55.020000. running mean: -44.143719\n",
      "ep 839: ep_len:504 episode reward: total was -40.170000. running mean: -44.103982\n",
      "epsilon:0.162760 episode_count: 5880. steps_count: 2568797.000000\n",
      "Time elapsed:  7299.393758773804\n",
      "ep 840: ep_len:500 episode reward: total was 16.050000. running mean: -43.502442\n",
      "ep 840: ep_len:542 episode reward: total was -8.700000. running mean: -43.154417\n",
      "ep 840: ep_len:500 episode reward: total was -40.970000. running mean: -43.132573\n",
      "ep 840: ep_len:832 episode reward: total was -355.950000. running mean: -46.260748\n",
      "ep 840: ep_len:3 episode reward: total was 0.000000. running mean: -45.798140\n",
      "ep 840: ep_len:500 episode reward: total was -119.360000. running mean: -46.533759\n",
      "ep 840: ep_len:547 episode reward: total was -67.050000. running mean: -46.738921\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.162716 episode_count: 5887. steps_count: 2572221.000000\n",
      "Time elapsed:  7313.9112849235535\n",
      "ep 841: ep_len:597 episode reward: total was -42.760000. running mean: -46.699132\n",
      "ep 841: ep_len:500 episode reward: total was 3.610000. running mean: -46.196041\n",
      "ep 841: ep_len:721 episode reward: total was -212.920000. running mean: -47.863280\n",
      "ep 841: ep_len:507 episode reward: total was -67.420000. running mean: -48.058847\n",
      "ep 841: ep_len:90 episode reward: total was -2.750000. running mean: -47.605759\n",
      "ep 841: ep_len:500 episode reward: total was -85.080000. running mean: -47.980501\n",
      "ep 841: ep_len:591 episode reward: total was -70.930000. running mean: -48.209996\n",
      "epsilon:0.162671 episode_count: 5894. steps_count: 2575727.000000\n",
      "Time elapsed:  7323.158084630966\n",
      "ep 842: ep_len:536 episode reward: total was -90.670000. running mean: -48.634596\n",
      "ep 842: ep_len:549 episode reward: total was 0.340000. running mean: -48.144850\n",
      "ep 842: ep_len:390 episode reward: total was 15.230000. running mean: -47.511102\n",
      "ep 842: ep_len:500 episode reward: total was -29.790000. running mean: -47.333891\n",
      "ep 842: ep_len:3 episode reward: total was -1.500000. running mean: -46.875552\n",
      "ep 842: ep_len:500 episode reward: total was -181.480000. running mean: -48.221596\n",
      "ep 842: ep_len:296 episode reward: total was -67.920000. running mean: -48.418580\n",
      "epsilon:0.162627 episode_count: 5901. steps_count: 2578501.000000\n",
      "Time elapsed:  7331.258024692535\n",
      "ep 843: ep_len:510 episode reward: total was -17.270000. running mean: -48.107095\n",
      "ep 843: ep_len:521 episode reward: total was 4.100000. running mean: -47.585024\n",
      "ep 843: ep_len:596 episode reward: total was -59.380000. running mean: -47.702973\n",
      "ep 843: ep_len:500 episode reward: total was -53.720000. running mean: -47.763144\n",
      "ep 843: ep_len:3 episode reward: total was -0.490000. running mean: -47.290412\n",
      "ep 843: ep_len:748 episode reward: total was -182.470000. running mean: -48.642208\n",
      "ep 843: ep_len:588 episode reward: total was -54.850000. running mean: -48.704286\n",
      "epsilon:0.162583 episode_count: 5908. steps_count: 2581967.000000\n",
      "Time elapsed:  7340.490723133087\n",
      "ep 844: ep_len:265 episode reward: total was -1.700000. running mean: -48.234243\n",
      "ep 844: ep_len:548 episode reward: total was -18.170000. running mean: -47.933601\n",
      "ep 844: ep_len:581 episode reward: total was -28.590000. running mean: -47.740165\n",
      "ep 844: ep_len:170 episode reward: total was -22.930000. running mean: -47.492063\n",
      "ep 844: ep_len:50 episode reward: total was 16.000000. running mean: -46.857142\n",
      "ep 844: ep_len:547 episode reward: total was -55.770000. running mean: -46.946271\n",
      "ep 844: ep_len:551 episode reward: total was -94.250000. running mean: -47.419308\n",
      "epsilon:0.162538 episode_count: 5915. steps_count: 2584679.000000\n",
      "Time elapsed:  7347.824901103973\n",
      "ep 845: ep_len:560 episode reward: total was -67.930000. running mean: -47.624415\n",
      "ep 845: ep_len:268 episode reward: total was -51.560000. running mean: -47.663771\n",
      "ep 845: ep_len:389 episode reward: total was -13.250000. running mean: -47.319633\n",
      "ep 845: ep_len:561 episode reward: total was -0.970000. running mean: -46.856137\n",
      "ep 845: ep_len:84 episode reward: total was 5.760000. running mean: -46.329976\n",
      "ep 845: ep_len:517 episode reward: total was -95.440000. running mean: -46.821076\n",
      "ep 845: ep_len:524 episode reward: total was -97.670000. running mean: -47.329565\n",
      "epsilon:0.162494 episode_count: 5922. steps_count: 2587582.000000\n",
      "Time elapsed:  7355.886367797852\n",
      "ep 846: ep_len:552 episode reward: total was -15.240000. running mean: -47.008670\n",
      "ep 846: ep_len:622 episode reward: total was -149.630000. running mean: -48.034883\n",
      "ep 846: ep_len:515 episode reward: total was -15.430000. running mean: -47.708834\n",
      "ep 846: ep_len:114 episode reward: total was 1.410000. running mean: -47.217646\n",
      "ep 846: ep_len:3 episode reward: total was 0.000000. running mean: -46.745469\n",
      "ep 846: ep_len:527 episode reward: total was -60.120000. running mean: -46.879215\n",
      "ep 846: ep_len:506 episode reward: total was -51.560000. running mean: -46.926022\n",
      "epsilon:0.162450 episode_count: 5929. steps_count: 2590421.000000\n",
      "Time elapsed:  7363.629565954208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 847: ep_len:568 episode reward: total was -6.360000. running mean: -46.520362\n",
      "ep 847: ep_len:584 episode reward: total was -7.980000. running mean: -46.134959\n",
      "ep 847: ep_len:623 episode reward: total was -139.770000. running mean: -47.071309\n",
      "ep 847: ep_len:500 episode reward: total was -58.620000. running mean: -47.186796\n",
      "ep 847: ep_len:3 episode reward: total was 0.000000. running mean: -46.714928\n",
      "ep 847: ep_len:709 episode reward: total was -218.350000. running mean: -48.431279\n",
      "ep 847: ep_len:535 episode reward: total was -58.140000. running mean: -48.528366\n",
      "epsilon:0.162405 episode_count: 5936. steps_count: 2593943.000000\n",
      "Time elapsed:  7373.604250669479\n",
      "ep 848: ep_len:615 episode reward: total was -95.430000. running mean: -48.997382\n",
      "ep 848: ep_len:500 episode reward: total was -69.260000. running mean: -49.200008\n",
      "ep 848: ep_len:339 episode reward: total was -89.130000. running mean: -49.599308\n",
      "ep 848: ep_len:528 episode reward: total was -1.540000. running mean: -49.118715\n",
      "ep 848: ep_len:53 episode reward: total was 13.000000. running mean: -48.497528\n",
      "ep 848: ep_len:609 episode reward: total was -79.480000. running mean: -48.807353\n",
      "ep 848: ep_len:502 episode reward: total was -69.100000. running mean: -49.010279\n",
      "epsilon:0.162361 episode_count: 5943. steps_count: 2597089.000000\n",
      "Time elapsed:  7381.866579532623\n",
      "ep 849: ep_len:104 episode reward: total was -2.640000. running mean: -48.546576\n",
      "ep 849: ep_len:548 episode reward: total was -2.270000. running mean: -48.083811\n",
      "ep 849: ep_len:500 episode reward: total was -20.340000. running mean: -47.806373\n",
      "ep 849: ep_len:629 episode reward: total was -55.830000. running mean: -47.886609\n",
      "ep 849: ep_len:3 episode reward: total was 0.000000. running mean: -47.407743\n",
      "ep 849: ep_len:555 episode reward: total was -47.770000. running mean: -47.411365\n",
      "ep 849: ep_len:531 episode reward: total was -39.920000. running mean: -47.336452\n",
      "epsilon:0.162317 episode_count: 5950. steps_count: 2599959.000000\n",
      "Time elapsed:  7389.73631644249\n",
      "ep 850: ep_len:621 episode reward: total was -27.290000. running mean: -47.135987\n",
      "ep 850: ep_len:506 episode reward: total was -14.300000. running mean: -46.807627\n",
      "ep 850: ep_len:677 episode reward: total was -144.440000. running mean: -47.783951\n",
      "ep 850: ep_len:592 episode reward: total was -59.680000. running mean: -47.902911\n",
      "ep 850: ep_len:3 episode reward: total was -1.500000. running mean: -47.438882\n",
      "ep 850: ep_len:620 episode reward: total was -95.560000. running mean: -47.920094\n",
      "ep 850: ep_len:507 episode reward: total was -79.350000. running mean: -48.234393\n",
      "epsilon:0.162272 episode_count: 5957. steps_count: 2603485.000000\n",
      "Time elapsed:  7399.740961074829\n",
      "ep 851: ep_len:500 episode reward: total was -10.580000. running mean: -47.857849\n",
      "ep 851: ep_len:503 episode reward: total was -64.410000. running mean: -48.023370\n",
      "ep 851: ep_len:578 episode reward: total was -49.690000. running mean: -48.040036\n",
      "ep 851: ep_len:509 episode reward: total was -2.910000. running mean: -47.588736\n",
      "ep 851: ep_len:3 episode reward: total was 0.000000. running mean: -47.112849\n",
      "ep 851: ep_len:556 episode reward: total was -129.700000. running mean: -47.938720\n",
      "ep 851: ep_len:305 episode reward: total was -78.540000. running mean: -48.244733\n",
      "epsilon:0.162228 episode_count: 5964. steps_count: 2606439.000000\n",
      "Time elapsed:  7408.010243177414\n",
      "ep 852: ep_len:645 episode reward: total was -166.870000. running mean: -49.430986\n",
      "ep 852: ep_len:517 episode reward: total was -84.800000. running mean: -49.784676\n",
      "ep 852: ep_len:67 episode reward: total was -2.340000. running mean: -49.310229\n",
      "ep 852: ep_len:508 episode reward: total was -30.400000. running mean: -49.121127\n",
      "ep 852: ep_len:3 episode reward: total was -1.500000. running mean: -48.644916\n",
      "ep 852: ep_len:585 episode reward: total was -36.260000. running mean: -48.521066\n",
      "ep 852: ep_len:297 episode reward: total was -61.970000. running mean: -48.655556\n",
      "epsilon:0.162184 episode_count: 5971. steps_count: 2609061.000000\n",
      "Time elapsed:  7415.971769809723\n",
      "ep 853: ep_len:209 episode reward: total was -5.450000. running mean: -48.223500\n",
      "ep 853: ep_len:506 episode reward: total was -21.810000. running mean: -47.959365\n",
      "ep 853: ep_len:602 episode reward: total was -62.680000. running mean: -48.106572\n",
      "ep 853: ep_len:132 episode reward: total was 9.040000. running mean: -47.535106\n",
      "ep 853: ep_len:3 episode reward: total was 0.000000. running mean: -47.059755\n",
      "ep 853: ep_len:512 episode reward: total was -10.730000. running mean: -46.696457\n",
      "ep 853: ep_len:547 episode reward: total was -105.460000. running mean: -47.284093\n",
      "epsilon:0.162139 episode_count: 5978. steps_count: 2611572.000000\n",
      "Time elapsed:  7422.927927732468\n",
      "ep 854: ep_len:501 episode reward: total was -75.350000. running mean: -47.564752\n",
      "ep 854: ep_len:500 episode reward: total was -89.520000. running mean: -47.984304\n",
      "ep 854: ep_len:535 episode reward: total was -65.880000. running mean: -48.163261\n",
      "ep 854: ep_len:529 episode reward: total was -49.470000. running mean: -48.176329\n",
      "ep 854: ep_len:3 episode reward: total was 0.000000. running mean: -47.694565\n",
      "ep 854: ep_len:536 episode reward: total was -56.420000. running mean: -47.781820\n",
      "ep 854: ep_len:500 episode reward: total was -78.980000. running mean: -48.093801\n",
      "epsilon:0.162095 episode_count: 5985. steps_count: 2614676.000000\n",
      "Time elapsed:  7430.82950425148\n",
      "ep 855: ep_len:571 episode reward: total was -104.490000. running mean: -48.657763\n",
      "ep 855: ep_len:604 episode reward: total was -19.730000. running mean: -48.368486\n",
      "ep 855: ep_len:500 episode reward: total was -57.590000. running mean: -48.460701\n",
      "ep 855: ep_len:512 episode reward: total was -10.120000. running mean: -48.077294\n",
      "ep 855: ep_len:3 episode reward: total was 0.000000. running mean: -47.596521\n",
      "ep 855: ep_len:622 episode reward: total was -70.610000. running mean: -47.826656\n",
      "ep 855: ep_len:326 episode reward: total was -48.180000. running mean: -47.830189\n",
      "epsilon:0.162051 episode_count: 5992. steps_count: 2617814.000000\n",
      "Time elapsed:  7439.201662540436\n",
      "ep 856: ep_len:500 episode reward: total was -93.890000. running mean: -48.290787\n",
      "ep 856: ep_len:542 episode reward: total was 6.750000. running mean: -47.740379\n",
      "ep 856: ep_len:536 episode reward: total was -76.460000. running mean: -48.027576\n",
      "ep 856: ep_len:520 episode reward: total was -10.390000. running mean: -47.651200\n",
      "ep 856: ep_len:3 episode reward: total was -1.500000. running mean: -47.189688\n",
      "ep 856: ep_len:500 episode reward: total was -105.180000. running mean: -47.769591\n",
      "ep 856: ep_len:195 episode reward: total was -39.700000. running mean: -47.688895\n",
      "epsilon:0.162006 episode_count: 5999. steps_count: 2620610.000000\n",
      "Time elapsed:  7446.741805791855\n",
      "ep 857: ep_len:573 episode reward: total was -49.130000. running mean: -47.703306\n",
      "ep 857: ep_len:172 episode reward: total was -26.950000. running mean: -47.495773\n",
      "ep 857: ep_len:553 episode reward: total was -63.390000. running mean: -47.654715\n",
      "ep 857: ep_len:55 episode reward: total was -2.190000. running mean: -47.200068\n",
      "ep 857: ep_len:3 episode reward: total was 0.000000. running mean: -46.728068\n",
      "ep 857: ep_len:550 episode reward: total was 1.130000. running mean: -46.249487\n",
      "ep 857: ep_len:552 episode reward: total was -133.720000. running mean: -47.124192\n",
      "epsilon:0.161962 episode_count: 6006. steps_count: 2623068.000000\n",
      "Time elapsed:  7453.438484668732\n",
      "ep 858: ep_len:117 episode reward: total was -14.260000. running mean: -46.795550\n",
      "ep 858: ep_len:500 episode reward: total was 11.830000. running mean: -46.209295\n",
      "ep 858: ep_len:669 episode reward: total was -122.460000. running mean: -46.971802\n",
      "ep 858: ep_len:118 episode reward: total was 8.040000. running mean: -46.421684\n",
      "ep 858: ep_len:3 episode reward: total was 0.000000. running mean: -45.957467\n",
      "ep 858: ep_len:500 episode reward: total was -42.590000. running mean: -45.923792\n",
      "ep 858: ep_len:500 episode reward: total was -47.990000. running mean: -45.944454\n",
      "epsilon:0.161918 episode_count: 6013. steps_count: 2625475.000000\n",
      "Time elapsed:  7459.969995260239\n",
      "ep 859: ep_len:250 episode reward: total was -12.890000. running mean: -45.613910\n",
      "ep 859: ep_len:578 episode reward: total was -94.450000. running mean: -46.102271\n",
      "ep 859: ep_len:500 episode reward: total was -51.100000. running mean: -46.152248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 859: ep_len:520 episode reward: total was -80.580000. running mean: -46.496525\n",
      "ep 859: ep_len:99 episode reward: total was -43.230000. running mean: -46.463860\n",
      "ep 859: ep_len:184 episode reward: total was 4.970000. running mean: -45.949521\n",
      "ep 859: ep_len:538 episode reward: total was -78.660000. running mean: -46.276626\n",
      "epsilon:0.161873 episode_count: 6020. steps_count: 2628144.000000\n",
      "Time elapsed:  7467.01570391655\n",
      "ep 860: ep_len:130 episode reward: total was -19.210000. running mean: -46.005960\n",
      "ep 860: ep_len:514 episode reward: total was -42.290000. running mean: -45.968800\n",
      "ep 860: ep_len:551 episode reward: total was -101.060000. running mean: -46.519712\n",
      "ep 860: ep_len:501 episode reward: total was 11.980000. running mean: -45.934715\n",
      "ep 860: ep_len:3 episode reward: total was 0.000000. running mean: -45.475368\n",
      "ep 860: ep_len:500 episode reward: total was -51.110000. running mean: -45.531714\n",
      "ep 860: ep_len:319 episode reward: total was -57.930000. running mean: -45.655697\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.161829 episode_count: 6027. steps_count: 2630662.000000\n",
      "Time elapsed:  7478.818422794342\n",
      "ep 861: ep_len:646 episode reward: total was -146.600000. running mean: -46.665140\n",
      "ep 861: ep_len:565 episode reward: total was -103.950000. running mean: -47.237989\n",
      "ep 861: ep_len:438 episode reward: total was -65.820000. running mean: -47.423809\n",
      "ep 861: ep_len:402 episode reward: total was -80.060000. running mean: -47.750171\n",
      "ep 861: ep_len:96 episode reward: total was 6.250000. running mean: -47.210169\n",
      "ep 861: ep_len:697 episode reward: total was -105.610000. running mean: -47.794168\n",
      "ep 861: ep_len:500 episode reward: total was -67.600000. running mean: -47.992226\n",
      "epsilon:0.161785 episode_count: 6034. steps_count: 2634006.000000\n",
      "Time elapsed:  7487.394382476807\n",
      "ep 862: ep_len:210 episode reward: total was -35.560000. running mean: -47.867904\n",
      "ep 862: ep_len:501 episode reward: total was -25.540000. running mean: -47.644625\n",
      "ep 862: ep_len:536 episode reward: total was -21.860000. running mean: -47.386778\n",
      "ep 862: ep_len:500 episode reward: total was -51.210000. running mean: -47.425011\n",
      "ep 862: ep_len:54 episode reward: total was 13.500000. running mean: -46.815760\n",
      "ep 862: ep_len:591 episode reward: total was -63.700000. running mean: -46.984603\n",
      "ep 862: ep_len:519 episode reward: total was -66.920000. running mean: -47.183957\n",
      "epsilon:0.161740 episode_count: 6041. steps_count: 2636917.000000\n",
      "Time elapsed:  7495.072019338608\n",
      "ep 863: ep_len:221 episode reward: total was -9.340000. running mean: -46.805517\n",
      "ep 863: ep_len:619 episode reward: total was -73.580000. running mean: -47.073262\n",
      "ep 863: ep_len:555 episode reward: total was -53.460000. running mean: -47.137129\n",
      "ep 863: ep_len:573 episode reward: total was -88.730000. running mean: -47.553058\n",
      "ep 863: ep_len:3 episode reward: total was 0.000000. running mean: -47.077528\n",
      "ep 863: ep_len:500 episode reward: total was -47.060000. running mean: -47.077352\n",
      "ep 863: ep_len:611 episode reward: total was -76.020000. running mean: -47.366779\n",
      "epsilon:0.161696 episode_count: 6048. steps_count: 2639999.000000\n",
      "Time elapsed:  7503.282510757446\n",
      "ep 864: ep_len:578 episode reward: total was -16.440000. running mean: -47.057511\n",
      "ep 864: ep_len:374 episode reward: total was -82.340000. running mean: -47.410336\n",
      "ep 864: ep_len:535 episode reward: total was -23.710000. running mean: -47.173333\n",
      "ep 864: ep_len:502 episode reward: total was -76.820000. running mean: -47.469799\n",
      "ep 864: ep_len:121 episode reward: total was -1.240000. running mean: -47.007501\n",
      "ep 864: ep_len:600 episode reward: total was -38.080000. running mean: -46.918226\n",
      "ep 864: ep_len:606 episode reward: total was -85.580000. running mean: -47.304844\n",
      "epsilon:0.161652 episode_count: 6055. steps_count: 2643315.000000\n",
      "Time elapsed:  7512.147074222565\n",
      "ep 865: ep_len:556 episode reward: total was -59.430000. running mean: -47.426095\n",
      "ep 865: ep_len:635 episode reward: total was -74.820000. running mean: -47.700035\n",
      "ep 865: ep_len:79 episode reward: total was -1.760000. running mean: -47.240634\n",
      "ep 865: ep_len:570 episode reward: total was -58.480000. running mean: -47.353028\n",
      "ep 865: ep_len:82 episode reward: total was 4.270000. running mean: -46.836798\n",
      "ep 865: ep_len:506 episode reward: total was -46.880000. running mean: -46.837230\n",
      "ep 865: ep_len:543 episode reward: total was -53.800000. running mean: -46.906857\n",
      "epsilon:0.161607 episode_count: 6062. steps_count: 2646286.000000\n",
      "Time elapsed:  7520.103894472122\n",
      "ep 866: ep_len:229 episode reward: total was -2.190000. running mean: -46.459689\n",
      "ep 866: ep_len:500 episode reward: total was -43.660000. running mean: -46.431692\n",
      "ep 866: ep_len:500 episode reward: total was -35.300000. running mean: -46.320375\n",
      "ep 866: ep_len:509 episode reward: total was 1.050000. running mean: -45.846671\n",
      "ep 866: ep_len:3 episode reward: total was 0.000000. running mean: -45.388204\n",
      "ep 866: ep_len:584 episode reward: total was -63.120000. running mean: -45.565522\n",
      "ep 866: ep_len:500 episode reward: total was -49.300000. running mean: -45.602867\n",
      "epsilon:0.161563 episode_count: 6069. steps_count: 2649111.000000\n",
      "Time elapsed:  7527.726359605789\n",
      "ep 867: ep_len:134 episode reward: total was 0.940000. running mean: -45.137439\n",
      "ep 867: ep_len:599 episode reward: total was -57.340000. running mean: -45.259464\n",
      "ep 867: ep_len:59 episode reward: total was 3.730000. running mean: -44.769569\n",
      "ep 867: ep_len:502 episode reward: total was -57.470000. running mean: -44.896574\n",
      "ep 867: ep_len:3 episode reward: total was 0.000000. running mean: -44.447608\n",
      "ep 867: ep_len:238 episode reward: total was 2.860000. running mean: -43.974532\n",
      "ep 867: ep_len:310 episode reward: total was -89.700000. running mean: -44.431787\n",
      "epsilon:0.161519 episode_count: 6076. steps_count: 2650956.000000\n",
      "Time elapsed:  7532.956384420395\n",
      "ep 868: ep_len:689 episode reward: total was -120.110000. running mean: -45.188569\n",
      "ep 868: ep_len:501 episode reward: total was 7.030000. running mean: -44.666383\n",
      "ep 868: ep_len:442 episode reward: total was -92.900000. running mean: -45.148719\n",
      "ep 868: ep_len:501 episode reward: total was -27.890000. running mean: -44.976132\n",
      "ep 868: ep_len:95 episode reward: total was 5.230000. running mean: -44.474071\n",
      "ep 868: ep_len:678 episode reward: total was -87.180000. running mean: -44.901130\n",
      "ep 868: ep_len:610 episode reward: total was -53.740000. running mean: -44.989519\n",
      "epsilon:0.161474 episode_count: 6083. steps_count: 2654472.000000\n",
      "Time elapsed:  7541.878091573715\n",
      "ep 869: ep_len:501 episode reward: total was -4.600000. running mean: -44.585624\n",
      "ep 869: ep_len:585 episode reward: total was -193.540000. running mean: -46.075167\n",
      "ep 869: ep_len:549 episode reward: total was -34.340000. running mean: -45.957816\n",
      "ep 869: ep_len:56 episode reward: total was 2.320000. running mean: -45.475037\n",
      "ep 869: ep_len:3 episode reward: total was -1.500000. running mean: -45.035287\n",
      "ep 869: ep_len:559 episode reward: total was -86.190000. running mean: -45.446834\n",
      "ep 869: ep_len:541 episode reward: total was -38.060000. running mean: -45.372966\n",
      "epsilon:0.161430 episode_count: 6090. steps_count: 2657266.000000\n",
      "Time elapsed:  7549.379108905792\n",
      "ep 870: ep_len:568 episode reward: total was -35.120000. running mean: -45.270436\n",
      "ep 870: ep_len:519 episode reward: total was -130.590000. running mean: -46.123632\n",
      "ep 870: ep_len:668 episode reward: total was -86.920000. running mean: -46.531596\n",
      "ep 870: ep_len:500 episode reward: total was -19.280000. running mean: -46.259080\n",
      "ep 870: ep_len:3 episode reward: total was 0.000000. running mean: -45.796489\n",
      "ep 870: ep_len:500 episode reward: total was -55.620000. running mean: -45.894724\n",
      "ep 870: ep_len:505 episode reward: total was -87.500000. running mean: -46.310777\n",
      "epsilon:0.161386 episode_count: 6097. steps_count: 2660529.000000\n",
      "Time elapsed:  7557.9984974861145\n",
      "ep 871: ep_len:575 episode reward: total was -61.430000. running mean: -46.461969\n",
      "ep 871: ep_len:500 episode reward: total was -76.380000. running mean: -46.761149\n",
      "ep 871: ep_len:571 episode reward: total was -97.000000. running mean: -47.263538\n",
      "ep 871: ep_len:516 episode reward: total was -4.830000. running mean: -46.839202\n",
      "ep 871: ep_len:3 episode reward: total was 0.000000. running mean: -46.370810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 871: ep_len:500 episode reward: total was -123.350000. running mean: -47.140602\n",
      "ep 871: ep_len:171 episode reward: total was -23.720000. running mean: -46.906396\n",
      "epsilon:0.161341 episode_count: 6104. steps_count: 2663365.000000\n",
      "Time elapsed:  7565.196254491806\n",
      "ep 872: ep_len:238 episode reward: total was -35.420000. running mean: -46.791532\n",
      "ep 872: ep_len:500 episode reward: total was -29.690000. running mean: -46.620517\n",
      "ep 872: ep_len:564 episode reward: total was -110.340000. running mean: -47.257712\n",
      "ep 872: ep_len:524 episode reward: total was -35.720000. running mean: -47.142335\n",
      "ep 872: ep_len:3 episode reward: total was 0.000000. running mean: -46.670911\n",
      "ep 872: ep_len:520 episode reward: total was -96.420000. running mean: -47.168402\n",
      "ep 872: ep_len:503 episode reward: total was -78.240000. running mean: -47.479118\n",
      "epsilon:0.161297 episode_count: 6111. steps_count: 2666217.000000\n",
      "Time elapsed:  7572.961782217026\n",
      "ep 873: ep_len:500 episode reward: total was -14.750000. running mean: -47.151827\n",
      "ep 873: ep_len:608 episode reward: total was -19.780000. running mean: -46.878109\n",
      "ep 873: ep_len:600 episode reward: total was -248.580000. running mean: -48.895128\n",
      "ep 873: ep_len:589 episode reward: total was -63.270000. running mean: -49.038876\n",
      "ep 873: ep_len:97 episode reward: total was -0.750000. running mean: -48.555988\n",
      "ep 873: ep_len:529 episode reward: total was -74.570000. running mean: -48.816128\n",
      "ep 873: ep_len:587 episode reward: total was -83.100000. running mean: -49.158966\n",
      "epsilon:0.161253 episode_count: 6118. steps_count: 2669727.000000\n",
      "Time elapsed:  7582.531166791916\n",
      "ep 874: ep_len:536 episode reward: total was -131.740000. running mean: -49.984777\n",
      "ep 874: ep_len:501 episode reward: total was -20.760000. running mean: -49.692529\n",
      "ep 874: ep_len:457 episode reward: total was -29.540000. running mean: -49.491004\n",
      "ep 874: ep_len:500 episode reward: total was -46.280000. running mean: -49.458894\n",
      "ep 874: ep_len:3 episode reward: total was 0.000000. running mean: -48.964305\n",
      "ep 874: ep_len:709 episode reward: total was -92.210000. running mean: -49.396762\n",
      "ep 874: ep_len:500 episode reward: total was -85.790000. running mean: -49.760694\n",
      "epsilon:0.161208 episode_count: 6125. steps_count: 2672933.000000\n",
      "Time elapsed:  7591.023424386978\n",
      "ep 875: ep_len:551 episode reward: total was -167.250000. running mean: -50.935587\n",
      "ep 875: ep_len:532 episode reward: total was -94.130000. running mean: -51.367531\n",
      "ep 875: ep_len:500 episode reward: total was -45.960000. running mean: -51.313456\n",
      "ep 875: ep_len:510 episode reward: total was -67.360000. running mean: -51.473921\n",
      "ep 875: ep_len:75 episode reward: total was 2.730000. running mean: -50.931882\n",
      "ep 875: ep_len:607 episode reward: total was -68.710000. running mean: -51.109663\n",
      "ep 875: ep_len:312 episode reward: total was -41.230000. running mean: -51.010867\n",
      "epsilon:0.161164 episode_count: 6132. steps_count: 2676020.000000\n",
      "Time elapsed:  7599.356601476669\n",
      "ep 876: ep_len:665 episode reward: total was -74.290000. running mean: -51.243658\n",
      "ep 876: ep_len:534 episode reward: total was -55.440000. running mean: -51.285621\n",
      "ep 876: ep_len:578 episode reward: total was -80.500000. running mean: -51.577765\n",
      "ep 876: ep_len:612 episode reward: total was -20.890000. running mean: -51.270888\n",
      "ep 876: ep_len:73 episode reward: total was 14.620000. running mean: -50.611979\n",
      "ep 876: ep_len:597 episode reward: total was -120.750000. running mean: -51.313359\n",
      "ep 876: ep_len:197 episode reward: total was -48.950000. running mean: -51.289725\n",
      "epsilon:0.161120 episode_count: 6139. steps_count: 2679276.000000\n",
      "Time elapsed:  7608.068199157715\n",
      "ep 877: ep_len:221 episode reward: total was 4.310000. running mean: -50.733728\n",
      "ep 877: ep_len:500 episode reward: total was -41.420000. running mean: -50.640591\n",
      "ep 877: ep_len:573 episode reward: total was -103.110000. running mean: -51.165285\n",
      "ep 877: ep_len:514 episode reward: total was -38.840000. running mean: -51.042032\n",
      "ep 877: ep_len:3 episode reward: total was 1.010000. running mean: -50.521512\n",
      "ep 877: ep_len:500 episode reward: total was -100.120000. running mean: -51.017497\n",
      "ep 877: ep_len:551 episode reward: total was -43.240000. running mean: -50.939722\n",
      "epsilon:0.161075 episode_count: 6146. steps_count: 2682138.000000\n",
      "Time elapsed:  7615.736342906952\n",
      "ep 878: ep_len:598 episode reward: total was -26.990000. running mean: -50.700224\n",
      "ep 878: ep_len:506 episode reward: total was -47.360000. running mean: -50.666822\n",
      "ep 878: ep_len:511 episode reward: total was -74.050000. running mean: -50.900654\n",
      "ep 878: ep_len:550 episode reward: total was -52.580000. running mean: -50.917447\n",
      "ep 878: ep_len:3 episode reward: total was 0.000000. running mean: -50.408273\n",
      "ep 878: ep_len:184 episode reward: total was -0.510000. running mean: -49.909290\n",
      "ep 878: ep_len:616 episode reward: total was -90.180000. running mean: -50.311997\n",
      "epsilon:0.161031 episode_count: 6153. steps_count: 2685106.000000\n",
      "Time elapsed:  7624.31232714653\n",
      "ep 879: ep_len:605 episode reward: total was 11.650000. running mean: -49.692377\n",
      "ep 879: ep_len:516 episode reward: total was -43.420000. running mean: -49.629654\n",
      "ep 879: ep_len:452 episode reward: total was -37.640000. running mean: -49.509757\n",
      "ep 879: ep_len:504 episode reward: total was -81.140000. running mean: -49.826059\n",
      "ep 879: ep_len:3 episode reward: total was 0.000000. running mean: -49.327799\n",
      "ep 879: ep_len:500 episode reward: total was -56.810000. running mean: -49.402621\n",
      "ep 879: ep_len:597 episode reward: total was -57.300000. running mean: -49.481595\n",
      "epsilon:0.160987 episode_count: 6160. steps_count: 2688283.000000\n",
      "Time elapsed:  7632.751910448074\n",
      "ep 880: ep_len:500 episode reward: total was -12.390000. running mean: -49.110679\n",
      "ep 880: ep_len:500 episode reward: total was -35.370000. running mean: -48.973272\n",
      "ep 880: ep_len:540 episode reward: total was -31.490000. running mean: -48.798439\n",
      "ep 880: ep_len:528 episode reward: total was -43.430000. running mean: -48.744755\n",
      "ep 880: ep_len:3 episode reward: total was 0.000000. running mean: -48.257307\n",
      "ep 880: ep_len:544 episode reward: total was -79.470000. running mean: -48.569434\n",
      "ep 880: ep_len:196 episode reward: total was -20.220000. running mean: -48.285940\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.160942 episode_count: 6167. steps_count: 2691094.000000\n",
      "Time elapsed:  7646.354643344879\n",
      "ep 881: ep_len:615 episode reward: total was -27.210000. running mean: -48.075180\n",
      "ep 881: ep_len:539 episode reward: total was -69.470000. running mean: -48.289129\n",
      "ep 881: ep_len:663 episode reward: total was -87.650000. running mean: -48.682737\n",
      "ep 881: ep_len:501 episode reward: total was -73.760000. running mean: -48.933510\n",
      "ep 881: ep_len:90 episode reward: total was 5.120000. running mean: -48.392975\n",
      "ep 881: ep_len:578 episode reward: total was -66.180000. running mean: -48.570845\n",
      "ep 881: ep_len:500 episode reward: total was -55.290000. running mean: -48.638037\n",
      "epsilon:0.160898 episode_count: 6174. steps_count: 2694580.000000\n",
      "Time elapsed:  7655.420479774475\n",
      "ep 882: ep_len:584 episode reward: total was -16.490000. running mean: -48.316556\n",
      "ep 882: ep_len:282 episode reward: total was -73.640000. running mean: -48.569791\n",
      "ep 882: ep_len:615 episode reward: total was -65.330000. running mean: -48.737393\n",
      "ep 882: ep_len:574 episode reward: total was -14.020000. running mean: -48.390219\n",
      "ep 882: ep_len:105 episode reward: total was 1.690000. running mean: -47.889417\n",
      "ep 882: ep_len:547 episode reward: total was -81.240000. running mean: -48.222923\n",
      "ep 882: ep_len:500 episode reward: total was -85.490000. running mean: -48.595593\n",
      "epsilon:0.160854 episode_count: 6181. steps_count: 2697787.000000\n",
      "Time elapsed:  7663.855076551437\n",
      "ep 883: ep_len:628 episode reward: total was -135.390000. running mean: -49.463537\n",
      "ep 883: ep_len:500 episode reward: total was -56.110000. running mean: -49.530002\n",
      "ep 883: ep_len:544 episode reward: total was -25.700000. running mean: -49.291702\n",
      "ep 883: ep_len:537 episode reward: total was -36.380000. running mean: -49.162585\n",
      "ep 883: ep_len:3 episode reward: total was 0.000000. running mean: -48.670959\n",
      "ep 883: ep_len:535 episode reward: total was -62.860000. running mean: -48.812850\n",
      "ep 883: ep_len:500 episode reward: total was -71.400000. running mean: -49.038721\n",
      "epsilon:0.160809 episode_count: 6188. steps_count: 2701034.000000\n",
      "Time elapsed:  7673.417439222336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 884: ep_len:598 episode reward: total was -26.380000. running mean: -48.812134\n",
      "ep 884: ep_len:500 episode reward: total was -78.180000. running mean: -49.105813\n",
      "ep 884: ep_len:659 episode reward: total was -67.590000. running mean: -49.290654\n",
      "ep 884: ep_len:543 episode reward: total was -23.640000. running mean: -49.034148\n",
      "ep 884: ep_len:3 episode reward: total was 0.000000. running mean: -48.543806\n",
      "ep 884: ep_len:583 episode reward: total was -44.750000. running mean: -48.505868\n",
      "ep 884: ep_len:532 episode reward: total was -77.430000. running mean: -48.795110\n",
      "epsilon:0.160765 episode_count: 6195. steps_count: 2704452.000000\n",
      "Time elapsed:  7682.461017608643\n",
      "ep 885: ep_len:568 episode reward: total was -29.860000. running mean: -48.605759\n",
      "ep 885: ep_len:506 episode reward: total was -11.050000. running mean: -48.230201\n",
      "ep 885: ep_len:576 episode reward: total was -65.160000. running mean: -48.399499\n",
      "ep 885: ep_len:584 episode reward: total was -15.610000. running mean: -48.071604\n",
      "ep 885: ep_len:3 episode reward: total was -3.000000. running mean: -47.620888\n",
      "ep 885: ep_len:602 episode reward: total was -56.730000. running mean: -47.711979\n",
      "ep 885: ep_len:544 episode reward: total was -56.430000. running mean: -47.799159\n",
      "epsilon:0.160721 episode_count: 6202. steps_count: 2707835.000000\n",
      "Time elapsed:  7687.94845366478\n",
      "ep 886: ep_len:500 episode reward: total was -4.360000. running mean: -47.364768\n",
      "ep 886: ep_len:500 episode reward: total was -43.790000. running mean: -47.329020\n",
      "ep 886: ep_len:501 episode reward: total was -67.640000. running mean: -47.532130\n",
      "ep 886: ep_len:500 episode reward: total was -11.910000. running mean: -47.175908\n",
      "ep 886: ep_len:108 episode reward: total was -17.290000. running mean: -46.877049\n",
      "ep 886: ep_len:500 episode reward: total was -15.290000. running mean: -46.561179\n",
      "ep 886: ep_len:528 episode reward: total was -66.160000. running mean: -46.757167\n",
      "epsilon:0.160676 episode_count: 6209. steps_count: 2710972.000000\n",
      "Time elapsed:  7696.228720188141\n",
      "ep 887: ep_len:511 episode reward: total was 9.280000. running mean: -46.196795\n",
      "ep 887: ep_len:327 episode reward: total was -40.840000. running mean: -46.143227\n",
      "ep 887: ep_len:636 episode reward: total was -88.300000. running mean: -46.564795\n",
      "ep 887: ep_len:500 episode reward: total was -84.880000. running mean: -46.947947\n",
      "ep 887: ep_len:3 episode reward: total was 0.000000. running mean: -46.478468\n",
      "ep 887: ep_len:500 episode reward: total was -50.200000. running mean: -46.515683\n",
      "ep 887: ep_len:652 episode reward: total was -73.890000. running mean: -46.789426\n",
      "epsilon:0.160632 episode_count: 6216. steps_count: 2714101.000000\n",
      "Time elapsed:  7704.453733205795\n",
      "ep 888: ep_len:523 episode reward: total was -21.380000. running mean: -46.535332\n",
      "ep 888: ep_len:535 episode reward: total was 7.020000. running mean: -45.999779\n",
      "ep 888: ep_len:500 episode reward: total was -52.700000. running mean: -46.066781\n",
      "ep 888: ep_len:509 episode reward: total was -13.200000. running mean: -45.738113\n",
      "ep 888: ep_len:3 episode reward: total was 0.000000. running mean: -45.280732\n",
      "ep 888: ep_len:559 episode reward: total was -64.750000. running mean: -45.475425\n",
      "ep 888: ep_len:610 episode reward: total was -92.740000. running mean: -45.948070\n",
      "epsilon:0.160588 episode_count: 6223. steps_count: 2717340.000000\n",
      "Time elapsed:  7713.186394929886\n",
      "ep 889: ep_len:582 episode reward: total was -17.900000. running mean: -45.667590\n",
      "ep 889: ep_len:608 episode reward: total was -71.210000. running mean: -45.923014\n",
      "ep 889: ep_len:434 episode reward: total was -18.200000. running mean: -45.645784\n",
      "ep 889: ep_len:500 episode reward: total was -60.760000. running mean: -45.796926\n",
      "ep 889: ep_len:3 episode reward: total was 0.000000. running mean: -45.338957\n",
      "ep 889: ep_len:184 episode reward: total was -0.480000. running mean: -44.890367\n",
      "ep 889: ep_len:185 episode reward: total was -27.740000. running mean: -44.718863\n",
      "epsilon:0.160543 episode_count: 6230. steps_count: 2719836.000000\n",
      "Time elapsed:  7719.957506656647\n",
      "ep 890: ep_len:761 episode reward: total was -168.990000. running mean: -45.961575\n",
      "ep 890: ep_len:500 episode reward: total was 22.540000. running mean: -45.276559\n",
      "ep 890: ep_len:577 episode reward: total was -152.980000. running mean: -46.353593\n",
      "ep 890: ep_len:529 episode reward: total was -69.150000. running mean: -46.581557\n",
      "ep 890: ep_len:3 episode reward: total was 0.000000. running mean: -46.115742\n",
      "ep 890: ep_len:596 episode reward: total was -47.290000. running mean: -46.127484\n",
      "ep 890: ep_len:588 episode reward: total was -60.970000. running mean: -46.275910\n",
      "epsilon:0.160499 episode_count: 6237. steps_count: 2723390.000000\n",
      "Time elapsed:  7729.958460092545\n",
      "ep 891: ep_len:598 episode reward: total was -61.840000. running mean: -46.431550\n",
      "ep 891: ep_len:500 episode reward: total was -84.620000. running mean: -46.813435\n",
      "ep 891: ep_len:540 episode reward: total was -97.290000. running mean: -47.318201\n",
      "ep 891: ep_len:510 episode reward: total was -45.020000. running mean: -47.295219\n",
      "ep 891: ep_len:48 episode reward: total was 15.000000. running mean: -46.672266\n",
      "ep 891: ep_len:557 episode reward: total was -45.580000. running mean: -46.661344\n",
      "ep 891: ep_len:287 episode reward: total was -50.260000. running mean: -46.697330\n",
      "epsilon:0.160455 episode_count: 6244. steps_count: 2726430.000000\n",
      "Time elapsed:  7739.259872674942\n",
      "ep 892: ep_len:613 episode reward: total was -57.520000. running mean: -46.805557\n",
      "ep 892: ep_len:536 episode reward: total was -54.360000. running mean: -46.881101\n",
      "ep 892: ep_len:554 episode reward: total was -214.720000. running mean: -48.559490\n",
      "ep 892: ep_len:515 episode reward: total was -45.220000. running mean: -48.526096\n",
      "ep 892: ep_len:3 episode reward: total was -1.500000. running mean: -48.055835\n",
      "ep 892: ep_len:501 episode reward: total was -52.100000. running mean: -48.096276\n",
      "ep 892: ep_len:509 episode reward: total was -40.050000. running mean: -48.015813\n",
      "epsilon:0.160410 episode_count: 6251. steps_count: 2729661.000000\n",
      "Time elapsed:  7748.526403188705\n",
      "ep 893: ep_len:516 episode reward: total was -40.460000. running mean: -47.940255\n",
      "ep 893: ep_len:293 episode reward: total was -25.100000. running mean: -47.711853\n",
      "ep 893: ep_len:568 episode reward: total was -35.800000. running mean: -47.592734\n",
      "ep 893: ep_len:555 episode reward: total was -26.830000. running mean: -47.385107\n",
      "ep 893: ep_len:3 episode reward: total was 0.000000. running mean: -46.911256\n",
      "ep 893: ep_len:507 episode reward: total was -82.140000. running mean: -47.263543\n",
      "ep 893: ep_len:606 episode reward: total was -70.450000. running mean: -47.495408\n",
      "epsilon:0.160366 episode_count: 6258. steps_count: 2732709.000000\n",
      "Time elapsed:  7756.813109874725\n",
      "ep 894: ep_len:500 episode reward: total was 2.620000. running mean: -46.994254\n",
      "ep 894: ep_len:580 episode reward: total was -53.030000. running mean: -47.054611\n",
      "ep 894: ep_len:500 episode reward: total was -76.730000. running mean: -47.351365\n",
      "ep 894: ep_len:132 episode reward: total was -2.960000. running mean: -46.907451\n",
      "ep 894: ep_len:51 episode reward: total was 22.500000. running mean: -46.213377\n",
      "ep 894: ep_len:169 episode reward: total was -2.470000. running mean: -45.775943\n",
      "ep 894: ep_len:613 episode reward: total was -45.680000. running mean: -45.774984\n",
      "epsilon:0.160322 episode_count: 6265. steps_count: 2735254.000000\n",
      "Time elapsed:  7763.387271404266\n",
      "ep 895: ep_len:227 episode reward: total was -33.400000. running mean: -45.651234\n",
      "ep 895: ep_len:640 episode reward: total was -15.000000. running mean: -45.344722\n",
      "ep 895: ep_len:500 episode reward: total was -33.570000. running mean: -45.226974\n",
      "ep 895: ep_len:500 episode reward: total was -91.400000. running mean: -45.688705\n",
      "ep 895: ep_len:50 episode reward: total was 10.000000. running mean: -45.131818\n",
      "ep 895: ep_len:500 episode reward: total was -30.770000. running mean: -44.988199\n",
      "ep 895: ep_len:593 episode reward: total was -41.470000. running mean: -44.953017\n",
      "epsilon:0.160277 episode_count: 6272. steps_count: 2738264.000000\n",
      "Time elapsed:  7771.158144712448\n",
      "ep 896: ep_len:575 episode reward: total was -35.300000. running mean: -44.856487\n",
      "ep 896: ep_len:553 episode reward: total was -84.190000. running mean: -45.249822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 896: ep_len:627 episode reward: total was -113.140000. running mean: -45.928724\n",
      "ep 896: ep_len:376 episode reward: total was -82.130000. running mean: -46.290737\n",
      "ep 896: ep_len:3 episode reward: total was 0.000000. running mean: -45.827830\n",
      "ep 896: ep_len:590 episode reward: total was -84.650000. running mean: -46.216051\n",
      "ep 896: ep_len:506 episode reward: total was -57.050000. running mean: -46.324391\n",
      "epsilon:0.160233 episode_count: 6279. steps_count: 2741494.000000\n",
      "Time elapsed:  7780.455767393112\n",
      "ep 897: ep_len:117 episode reward: total was -2.020000. running mean: -45.881347\n",
      "ep 897: ep_len:590 episode reward: total was -39.950000. running mean: -45.822033\n",
      "ep 897: ep_len:440 episode reward: total was -22.400000. running mean: -45.587813\n",
      "ep 897: ep_len:500 episode reward: total was -24.770000. running mean: -45.379635\n",
      "ep 897: ep_len:123 episode reward: total was -28.740000. running mean: -45.213239\n",
      "ep 897: ep_len:577 episode reward: total was -53.360000. running mean: -45.294706\n",
      "ep 897: ep_len:563 episode reward: total was -96.390000. running mean: -45.805659\n",
      "epsilon:0.160189 episode_count: 6286. steps_count: 2744404.000000\n",
      "Time elapsed:  7788.159789562225\n",
      "ep 898: ep_len:116 episode reward: total was -3.070000. running mean: -45.378302\n",
      "ep 898: ep_len:616 episode reward: total was -40.420000. running mean: -45.328719\n",
      "ep 898: ep_len:673 episode reward: total was -128.070000. running mean: -46.156132\n",
      "ep 898: ep_len:523 episode reward: total was -125.530000. running mean: -46.949871\n",
      "ep 898: ep_len:3 episode reward: total was 0.000000. running mean: -46.480372\n",
      "ep 898: ep_len:500 episode reward: total was -46.870000. running mean: -46.484269\n",
      "ep 898: ep_len:524 episode reward: total was -42.880000. running mean: -46.448226\n",
      "epsilon:0.160144 episode_count: 6293. steps_count: 2747359.000000\n",
      "Time elapsed:  7796.036171197891\n",
      "ep 899: ep_len:673 episode reward: total was -76.350000. running mean: -46.747244\n",
      "ep 899: ep_len:510 episode reward: total was -171.680000. running mean: -47.996571\n",
      "ep 899: ep_len:576 episode reward: total was -81.830000. running mean: -48.334905\n",
      "ep 899: ep_len:595 episode reward: total was -21.070000. running mean: -48.062256\n",
      "ep 899: ep_len:3 episode reward: total was -1.500000. running mean: -47.596634\n",
      "ep 899: ep_len:664 episode reward: total was -86.300000. running mean: -47.983667\n",
      "ep 899: ep_len:534 episode reward: total was -69.150000. running mean: -48.195331\n",
      "epsilon:0.160100 episode_count: 6300. steps_count: 2750914.000000\n",
      "Time elapsed:  7806.024647474289\n",
      "ep 900: ep_len:530 episode reward: total was -1.350000. running mean: -47.726877\n",
      "ep 900: ep_len:641 episode reward: total was -37.480000. running mean: -47.624409\n",
      "ep 900: ep_len:79 episode reward: total was -10.790000. running mean: -47.256065\n",
      "ep 900: ep_len:161 episode reward: total was -4.920000. running mean: -46.832704\n",
      "ep 900: ep_len:100 episode reward: total was 4.700000. running mean: -46.317377\n",
      "ep 900: ep_len:646 episode reward: total was -93.510000. running mean: -46.789303\n",
      "ep 900: ep_len:587 episode reward: total was -52.440000. running mean: -46.845810\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.160056 episode_count: 6307. steps_count: 2753658.000000\n",
      "Time elapsed:  7818.5413229465485\n",
      "ep 901: ep_len:525 episode reward: total was -9.360000. running mean: -46.470952\n",
      "ep 901: ep_len:599 episode reward: total was -57.820000. running mean: -46.584443\n",
      "ep 901: ep_len:605 episode reward: total was -114.420000. running mean: -47.262798\n",
      "ep 901: ep_len:500 episode reward: total was -24.860000. running mean: -47.038770\n",
      "ep 901: ep_len:55 episode reward: total was 14.000000. running mean: -46.428382\n",
      "ep 901: ep_len:675 episode reward: total was -41.620000. running mean: -46.380299\n",
      "ep 901: ep_len:304 episode reward: total was -75.310000. running mean: -46.669596\n",
      "epsilon:0.160011 episode_count: 6314. steps_count: 2756921.000000\n",
      "Time elapsed:  7827.7513909339905\n",
      "ep 902: ep_len:550 episode reward: total was 1.120000. running mean: -46.191700\n",
      "ep 902: ep_len:500 episode reward: total was -25.040000. running mean: -45.980183\n",
      "ep 902: ep_len:573 episode reward: total was -72.330000. running mean: -46.243681\n",
      "ep 902: ep_len:530 episode reward: total was -7.340000. running mean: -45.854644\n",
      "ep 902: ep_len:85 episode reward: total was 9.690000. running mean: -45.299198\n",
      "ep 902: ep_len:524 episode reward: total was -115.540000. running mean: -46.001606\n",
      "ep 902: ep_len:525 episode reward: total was -72.620000. running mean: -46.267790\n",
      "epsilon:0.159967 episode_count: 6321. steps_count: 2760208.000000\n",
      "Time elapsed:  7836.984111070633\n",
      "ep 903: ep_len:578 episode reward: total was -138.760000. running mean: -47.192712\n",
      "ep 903: ep_len:569 episode reward: total was 0.030000. running mean: -46.720485\n",
      "ep 903: ep_len:500 episode reward: total was -36.750000. running mean: -46.620780\n",
      "ep 903: ep_len:500 episode reward: total was 4.990000. running mean: -46.104672\n",
      "ep 903: ep_len:3 episode reward: total was -1.500000. running mean: -45.658625\n",
      "ep 903: ep_len:554 episode reward: total was -96.110000. running mean: -46.163139\n",
      "ep 903: ep_len:502 episode reward: total was -46.470000. running mean: -46.166208\n",
      "epsilon:0.159923 episode_count: 6328. steps_count: 2763414.000000\n",
      "Time elapsed:  7846.157826185226\n",
      "ep 904: ep_len:554 episode reward: total was -37.650000. running mean: -46.081045\n",
      "ep 904: ep_len:512 episode reward: total was -108.160000. running mean: -46.701835\n",
      "ep 904: ep_len:411 episode reward: total was -19.270000. running mean: -46.427517\n",
      "ep 904: ep_len:576 episode reward: total was -64.980000. running mean: -46.613041\n",
      "ep 904: ep_len:3 episode reward: total was -1.500000. running mean: -46.161911\n",
      "ep 904: ep_len:500 episode reward: total was -32.950000. running mean: -46.029792\n",
      "ep 904: ep_len:601 episode reward: total was -40.620000. running mean: -45.975694\n",
      "epsilon:0.159878 episode_count: 6335. steps_count: 2766571.000000\n",
      "Time elapsed:  7855.169366121292\n",
      "ep 905: ep_len:512 episode reward: total was -7.700000. running mean: -45.592937\n",
      "ep 905: ep_len:500 episode reward: total was -34.680000. running mean: -45.483808\n",
      "ep 905: ep_len:631 episode reward: total was -113.700000. running mean: -46.165970\n",
      "ep 905: ep_len:56 episode reward: total was -5.240000. running mean: -45.756710\n",
      "ep 905: ep_len:3 episode reward: total was -1.500000. running mean: -45.314143\n",
      "ep 905: ep_len:696 episode reward: total was -136.110000. running mean: -46.222101\n",
      "ep 905: ep_len:565 episode reward: total was -51.990000. running mean: -46.279780\n",
      "epsilon:0.159834 episode_count: 6342. steps_count: 2769534.000000\n",
      "Time elapsed:  7863.14840722084\n",
      "ep 906: ep_len:520 episode reward: total was -46.420000. running mean: -46.281183\n",
      "ep 906: ep_len:500 episode reward: total was -21.770000. running mean: -46.036071\n",
      "ep 906: ep_len:648 episode reward: total was -116.020000. running mean: -46.735910\n",
      "ep 906: ep_len:125 episode reward: total was -8.490000. running mean: -46.353451\n",
      "ep 906: ep_len:3 episode reward: total was -3.000000. running mean: -45.919916\n",
      "ep 906: ep_len:595 episode reward: total was -95.360000. running mean: -46.414317\n",
      "ep 906: ep_len:307 episode reward: total was -55.880000. running mean: -46.508974\n",
      "epsilon:0.159790 episode_count: 6349. steps_count: 2772232.000000\n",
      "Time elapsed:  7870.501391410828\n",
      "ep 907: ep_len:626 episode reward: total was -130.750000. running mean: -47.351384\n",
      "ep 907: ep_len:544 episode reward: total was -74.080000. running mean: -47.618671\n",
      "ep 907: ep_len:594 episode reward: total was -85.060000. running mean: -47.993084\n",
      "ep 907: ep_len:507 episode reward: total was -4.280000. running mean: -47.555953\n",
      "ep 907: ep_len:3 episode reward: total was 0.000000. running mean: -47.080393\n",
      "ep 907: ep_len:507 episode reward: total was -111.930000. running mean: -47.728890\n",
      "ep 907: ep_len:500 episode reward: total was -89.610000. running mean: -48.147701\n",
      "epsilon:0.159745 episode_count: 6356. steps_count: 2775513.000000\n",
      "Time elapsed:  7879.285222768784\n",
      "ep 908: ep_len:514 episode reward: total was -84.830000. running mean: -48.514524\n",
      "ep 908: ep_len:500 episode reward: total was -50.890000. running mean: -48.538278\n",
      "ep 908: ep_len:513 episode reward: total was -71.190000. running mean: -48.764796\n",
      "ep 908: ep_len:588 episode reward: total was -36.320000. running mean: -48.640348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 908: ep_len:121 episode reward: total was -4.730000. running mean: -48.201244\n",
      "ep 908: ep_len:500 episode reward: total was -31.850000. running mean: -48.037732\n",
      "ep 908: ep_len:646 episode reward: total was -62.840000. running mean: -48.185754\n",
      "epsilon:0.159701 episode_count: 6363. steps_count: 2778895.000000\n",
      "Time elapsed:  7888.0824337005615\n",
      "ep 909: ep_len:544 episode reward: total was 0.930000. running mean: -47.694597\n",
      "ep 909: ep_len:500 episode reward: total was -47.320000. running mean: -47.690851\n",
      "ep 909: ep_len:551 episode reward: total was -71.870000. running mean: -47.932642\n",
      "ep 909: ep_len:500 episode reward: total was 8.910000. running mean: -47.364216\n",
      "ep 909: ep_len:3 episode reward: total was 0.000000. running mean: -46.890574\n",
      "ep 909: ep_len:590 episode reward: total was -78.920000. running mean: -47.210868\n",
      "ep 909: ep_len:584 episode reward: total was -101.630000. running mean: -47.755059\n",
      "epsilon:0.159657 episode_count: 6370. steps_count: 2782167.000000\n",
      "Time elapsed:  7897.49854516983\n",
      "ep 910: ep_len:563 episode reward: total was -38.050000. running mean: -47.658009\n",
      "ep 910: ep_len:542 episode reward: total was -2.680000. running mean: -47.208229\n",
      "ep 910: ep_len:601 episode reward: total was -78.390000. running mean: -47.520046\n",
      "ep 910: ep_len:500 episode reward: total was -48.300000. running mean: -47.527846\n",
      "ep 910: ep_len:3 episode reward: total was 0.000000. running mean: -47.052568\n",
      "ep 910: ep_len:500 episode reward: total was -57.850000. running mean: -47.160542\n",
      "ep 910: ep_len:308 episode reward: total was -78.690000. running mean: -47.475836\n",
      "epsilon:0.159612 episode_count: 6377. steps_count: 2785184.000000\n",
      "Time elapsed:  7905.603478908539\n",
      "ep 911: ep_len:501 episode reward: total was -37.970000. running mean: -47.380778\n",
      "ep 911: ep_len:201 episode reward: total was -2.690000. running mean: -46.933870\n",
      "ep 911: ep_len:587 episode reward: total was -78.540000. running mean: -47.249932\n",
      "ep 911: ep_len:607 episode reward: total was -48.330000. running mean: -47.260732\n",
      "ep 911: ep_len:99 episode reward: total was -0.270000. running mean: -46.790825\n",
      "ep 911: ep_len:536 episode reward: total was -77.740000. running mean: -47.100317\n",
      "ep 911: ep_len:587 episode reward: total was -73.600000. running mean: -47.365314\n",
      "epsilon:0.159568 episode_count: 6384. steps_count: 2788302.000000\n",
      "Time elapsed:  7914.737080812454\n",
      "ep 912: ep_len:528 episode reward: total was -123.610000. running mean: -48.127760\n",
      "ep 912: ep_len:514 episode reward: total was -16.030000. running mean: -47.806783\n",
      "ep 912: ep_len:500 episode reward: total was -46.270000. running mean: -47.791415\n",
      "ep 912: ep_len:524 episode reward: total was -19.710000. running mean: -47.510601\n",
      "ep 912: ep_len:86 episode reward: total was -26.240000. running mean: -47.297895\n",
      "ep 912: ep_len:500 episode reward: total was -34.020000. running mean: -47.165116\n",
      "ep 912: ep_len:500 episode reward: total was -68.310000. running mean: -47.376565\n",
      "epsilon:0.159524 episode_count: 6391. steps_count: 2791454.000000\n",
      "Time elapsed:  7922.650045633316\n",
      "ep 913: ep_len:593 episode reward: total was -102.890000. running mean: -47.931699\n",
      "ep 913: ep_len:500 episode reward: total was -56.360000. running mean: -48.015982\n",
      "ep 913: ep_len:529 episode reward: total was -85.730000. running mean: -48.393122\n",
      "ep 913: ep_len:170 episode reward: total was -2.410000. running mean: -47.933291\n",
      "ep 913: ep_len:78 episode reward: total was 0.250000. running mean: -47.451458\n",
      "ep 913: ep_len:513 episode reward: total was -46.290000. running mean: -47.439844\n",
      "ep 913: ep_len:606 episode reward: total was -68.360000. running mean: -47.649045\n",
      "epsilon:0.159479 episode_count: 6398. steps_count: 2794443.000000\n",
      "Time elapsed:  7930.66371679306\n",
      "ep 914: ep_len:623 episode reward: total was -17.870000. running mean: -47.351255\n",
      "ep 914: ep_len:528 episode reward: total was -33.440000. running mean: -47.212142\n",
      "ep 914: ep_len:529 episode reward: total was -75.540000. running mean: -47.495421\n",
      "ep 914: ep_len:125 episode reward: total was -4.970000. running mean: -47.070166\n",
      "ep 914: ep_len:3 episode reward: total was -1.500000. running mean: -46.614465\n",
      "ep 914: ep_len:514 episode reward: total was -101.160000. running mean: -47.159920\n",
      "ep 914: ep_len:542 episode reward: total was -61.510000. running mean: -47.303421\n",
      "epsilon:0.159435 episode_count: 6405. steps_count: 2797307.000000\n",
      "Time elapsed:  7938.722482442856\n",
      "ep 915: ep_len:548 episode reward: total was -58.410000. running mean: -47.414487\n",
      "ep 915: ep_len:638 episode reward: total was -38.750000. running mean: -47.327842\n",
      "ep 915: ep_len:628 episode reward: total was -96.180000. running mean: -47.816363\n",
      "ep 915: ep_len:144 episode reward: total was -17.370000. running mean: -47.511900\n",
      "ep 915: ep_len:82 episode reward: total was 15.720000. running mean: -46.879581\n",
      "ep 915: ep_len:244 episode reward: total was 3.230000. running mean: -46.378485\n",
      "ep 915: ep_len:513 episode reward: total was -153.480000. running mean: -47.449500\n",
      "epsilon:0.159391 episode_count: 6412. steps_count: 2800104.000000\n",
      "Time elapsed:  7947.255787372589\n",
      "ep 916: ep_len:119 episode reward: total was -11.120000. running mean: -47.086205\n",
      "ep 916: ep_len:588 episode reward: total was 35.240000. running mean: -46.262943\n",
      "ep 916: ep_len:634 episode reward: total was -76.650000. running mean: -46.566814\n",
      "ep 916: ep_len:553 episode reward: total was -18.250000. running mean: -46.283646\n",
      "ep 916: ep_len:3 episode reward: total was 0.000000. running mean: -45.820809\n",
      "ep 916: ep_len:572 episode reward: total was -61.420000. running mean: -45.976801\n",
      "ep 916: ep_len:584 episode reward: total was -69.570000. running mean: -46.212733\n",
      "epsilon:0.159346 episode_count: 6419. steps_count: 2803157.000000\n",
      "Time elapsed:  7956.303797006607\n",
      "ep 917: ep_len:615 episode reward: total was -10.890000. running mean: -45.859506\n",
      "ep 917: ep_len:500 episode reward: total was -38.150000. running mean: -45.782411\n",
      "ep 917: ep_len:629 episode reward: total was -65.570000. running mean: -45.980286\n",
      "ep 917: ep_len:504 episode reward: total was -75.840000. running mean: -46.278884\n",
      "ep 917: ep_len:93 episode reward: total was -15.240000. running mean: -45.968495\n",
      "ep 917: ep_len:500 episode reward: total was -107.370000. running mean: -46.582510\n",
      "ep 917: ep_len:514 episode reward: total was -156.810000. running mean: -47.684785\n",
      "epsilon:0.159302 episode_count: 6426. steps_count: 2806512.000000\n",
      "Time elapsed:  7964.4899780750275\n",
      "ep 918: ep_len:581 episode reward: total was 5.130000. running mean: -47.156637\n",
      "ep 918: ep_len:580 episode reward: total was -92.000000. running mean: -47.605071\n",
      "ep 918: ep_len:598 episode reward: total was -173.260000. running mean: -48.861620\n",
      "ep 918: ep_len:53 episode reward: total was -16.730000. running mean: -48.540304\n",
      "ep 918: ep_len:112 episode reward: total was -6.260000. running mean: -48.117501\n",
      "ep 918: ep_len:500 episode reward: total was -44.990000. running mean: -48.086226\n",
      "ep 918: ep_len:525 episode reward: total was -63.090000. running mean: -48.236263\n",
      "epsilon:0.159258 episode_count: 6433. steps_count: 2809461.000000\n",
      "Time elapsed:  7973.121807575226\n",
      "ep 919: ep_len:111 episode reward: total was -3.120000. running mean: -47.785101\n",
      "ep 919: ep_len:501 episode reward: total was -118.850000. running mean: -48.495750\n",
      "ep 919: ep_len:588 episode reward: total was -96.110000. running mean: -48.971892\n",
      "ep 919: ep_len:574 episode reward: total was -119.490000. running mean: -49.677073\n",
      "ep 919: ep_len:3 episode reward: total was -1.500000. running mean: -49.195303\n",
      "ep 919: ep_len:700 episode reward: total was -90.250000. running mean: -49.605849\n",
      "ep 919: ep_len:500 episode reward: total was -52.230000. running mean: -49.632091\n",
      "epsilon:0.159213 episode_count: 6440. steps_count: 2812438.000000\n",
      "Time elapsed:  7980.654603719711\n",
      "ep 920: ep_len:598 episode reward: total was -43.840000. running mean: -49.574170\n",
      "ep 920: ep_len:582 episode reward: total was -59.990000. running mean: -49.678328\n",
      "ep 920: ep_len:564 episode reward: total was -107.820000. running mean: -50.259745\n",
      "ep 920: ep_len:550 episode reward: total was -86.210000. running mean: -50.619248\n",
      "ep 920: ep_len:109 episode reward: total was 2.710000. running mean: -50.085955\n",
      "ep 920: ep_len:500 episode reward: total was 0.150000. running mean: -49.583596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 920: ep_len:500 episode reward: total was -55.240000. running mean: -49.640160\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.159169 episode_count: 6447. steps_count: 2815841.000000\n",
      "Time elapsed:  7995.063953399658\n",
      "ep 921: ep_len:615 episode reward: total was -21.040000. running mean: -49.354158\n",
      "ep 921: ep_len:500 episode reward: total was -80.850000. running mean: -49.669116\n",
      "ep 921: ep_len:375 episode reward: total was -9.260000. running mean: -49.265025\n",
      "ep 921: ep_len:500 episode reward: total was -68.270000. running mean: -49.455075\n",
      "ep 921: ep_len:3 episode reward: total was 0.000000. running mean: -48.960524\n",
      "ep 921: ep_len:500 episode reward: total was -122.840000. running mean: -49.699319\n",
      "ep 921: ep_len:597 episode reward: total was -57.780000. running mean: -49.780126\n",
      "epsilon:0.159125 episode_count: 6454. steps_count: 2818931.000000\n",
      "Time elapsed:  8003.17343711853\n",
      "ep 922: ep_len:163 episode reward: total was -21.040000. running mean: -49.492725\n",
      "ep 922: ep_len:500 episode reward: total was -0.980000. running mean: -49.007597\n",
      "ep 922: ep_len:596 episode reward: total was -55.690000. running mean: -49.074421\n",
      "ep 922: ep_len:386 episode reward: total was -36.210000. running mean: -48.945777\n",
      "ep 922: ep_len:100 episode reward: total was 2.740000. running mean: -48.428919\n",
      "ep 922: ep_len:570 episode reward: total was -88.850000. running mean: -48.833130\n",
      "ep 922: ep_len:500 episode reward: total was -77.340000. running mean: -49.118199\n",
      "epsilon:0.159080 episode_count: 6461. steps_count: 2821746.000000\n",
      "Time elapsed:  8010.9408922195435\n",
      "ep 923: ep_len:501 episode reward: total was -30.290000. running mean: -48.929917\n",
      "ep 923: ep_len:500 episode reward: total was 1.530000. running mean: -48.425318\n",
      "ep 923: ep_len:542 episode reward: total was -33.240000. running mean: -48.273465\n",
      "ep 923: ep_len:511 episode reward: total was -63.710000. running mean: -48.427830\n",
      "ep 923: ep_len:3 episode reward: total was 0.000000. running mean: -47.943552\n",
      "ep 923: ep_len:506 episode reward: total was -9.380000. running mean: -47.557916\n",
      "ep 923: ep_len:500 episode reward: total was -58.850000. running mean: -47.670837\n",
      "epsilon:0.159036 episode_count: 6468. steps_count: 2824809.000000\n",
      "Time elapsed:  8019.75196313858\n",
      "ep 924: ep_len:679 episode reward: total was -120.020000. running mean: -48.394329\n",
      "ep 924: ep_len:500 episode reward: total was -0.030000. running mean: -47.910685\n",
      "ep 924: ep_len:609 episode reward: total was -81.040000. running mean: -48.241978\n",
      "ep 924: ep_len:528 episode reward: total was -9.580000. running mean: -47.855359\n",
      "ep 924: ep_len:52 episode reward: total was 9.500000. running mean: -47.281805\n",
      "ep 924: ep_len:609 episode reward: total was -42.370000. running mean: -47.232687\n",
      "ep 924: ep_len:209 episode reward: total was -23.060000. running mean: -46.990960\n",
      "epsilon:0.158992 episode_count: 6475. steps_count: 2827995.000000\n",
      "Time elapsed:  8028.358388900757\n",
      "ep 925: ep_len:252 episode reward: total was -20.770000. running mean: -46.728751\n",
      "ep 925: ep_len:500 episode reward: total was -61.560000. running mean: -46.877063\n",
      "ep 925: ep_len:563 episode reward: total was -104.670000. running mean: -47.454992\n",
      "ep 925: ep_len:500 episode reward: total was -81.360000. running mean: -47.794043\n",
      "ep 925: ep_len:125 episode reward: total was 10.750000. running mean: -47.208602\n",
      "ep 925: ep_len:639 episode reward: total was -83.970000. running mean: -47.576216\n",
      "ep 925: ep_len:612 episode reward: total was -64.230000. running mean: -47.742754\n",
      "epsilon:0.158947 episode_count: 6482. steps_count: 2831186.000000\n",
      "Time elapsed:  8036.903768777847\n",
      "ep 926: ep_len:506 episode reward: total was -34.380000. running mean: -47.609126\n",
      "ep 926: ep_len:500 episode reward: total was -51.760000. running mean: -47.650635\n",
      "ep 926: ep_len:523 episode reward: total was -43.520000. running mean: -47.609329\n",
      "ep 926: ep_len:576 episode reward: total was -1.760000. running mean: -47.150835\n",
      "ep 926: ep_len:103 episode reward: total was 3.720000. running mean: -46.642127\n",
      "ep 926: ep_len:161 episode reward: total was 3.540000. running mean: -46.140306\n",
      "ep 926: ep_len:561 episode reward: total was -88.830000. running mean: -46.567203\n",
      "epsilon:0.158903 episode_count: 6489. steps_count: 2834116.000000\n",
      "Time elapsed:  8044.8101279735565\n",
      "ep 927: ep_len:676 episode reward: total was -103.110000. running mean: -47.132631\n",
      "ep 927: ep_len:501 episode reward: total was -65.630000. running mean: -47.317604\n",
      "ep 927: ep_len:693 episode reward: total was -116.920000. running mean: -48.013628\n",
      "ep 927: ep_len:557 episode reward: total was -8.290000. running mean: -47.616392\n",
      "ep 927: ep_len:3 episode reward: total was 0.000000. running mean: -47.140228\n",
      "ep 927: ep_len:186 episode reward: total was -3.490000. running mean: -46.703726\n",
      "ep 927: ep_len:585 episode reward: total was -74.380000. running mean: -46.980489\n",
      "epsilon:0.158859 episode_count: 6496. steps_count: 2837317.000000\n",
      "Time elapsed:  8053.402721643448\n",
      "ep 928: ep_len:535 episode reward: total was -150.440000. running mean: -48.015084\n",
      "ep 928: ep_len:262 episode reward: total was -38.090000. running mean: -47.915833\n",
      "ep 928: ep_len:500 episode reward: total was -56.490000. running mean: -48.001575\n",
      "ep 928: ep_len:500 episode reward: total was -53.530000. running mean: -48.056859\n",
      "ep 928: ep_len:3 episode reward: total was 0.000000. running mean: -47.576290\n",
      "ep 928: ep_len:618 episode reward: total was -48.730000. running mean: -47.587827\n",
      "ep 928: ep_len:501 episode reward: total was -50.700000. running mean: -47.618949\n",
      "epsilon:0.158814 episode_count: 6503. steps_count: 2840236.000000\n",
      "Time elapsed:  8061.218698263168\n",
      "ep 929: ep_len:190 episode reward: total was 1.150000. running mean: -47.131260\n",
      "ep 929: ep_len:550 episode reward: total was -74.470000. running mean: -47.404647\n",
      "ep 929: ep_len:452 episode reward: total was -11.560000. running mean: -47.046201\n",
      "ep 929: ep_len:501 episode reward: total was -11.840000. running mean: -46.694139\n",
      "ep 929: ep_len:3 episode reward: total was 0.000000. running mean: -46.227197\n",
      "ep 929: ep_len:255 episode reward: total was -1.620000. running mean: -45.781125\n",
      "ep 929: ep_len:173 episode reward: total was -36.950000. running mean: -45.692814\n",
      "epsilon:0.158770 episode_count: 6510. steps_count: 2842360.000000\n",
      "Time elapsed:  8067.954626560211\n",
      "ep 930: ep_len:629 episode reward: total was -107.580000. running mean: -46.311686\n",
      "ep 930: ep_len:550 episode reward: total was -52.550000. running mean: -46.374069\n",
      "ep 930: ep_len:500 episode reward: total was -66.230000. running mean: -46.572628\n",
      "ep 930: ep_len:108 episode reward: total was -13.100000. running mean: -46.237902\n",
      "ep 930: ep_len:80 episode reward: total was -18.250000. running mean: -45.958023\n",
      "ep 930: ep_len:529 episode reward: total was -62.810000. running mean: -46.126543\n",
      "ep 930: ep_len:541 episode reward: total was -61.910000. running mean: -46.284377\n",
      "epsilon:0.158726 episode_count: 6517. steps_count: 2845297.000000\n",
      "Time elapsed:  8075.904131889343\n",
      "ep 931: ep_len:500 episode reward: total was 29.150000. running mean: -45.530033\n",
      "ep 931: ep_len:500 episode reward: total was -27.650000. running mean: -45.351233\n",
      "ep 931: ep_len:548 episode reward: total was -81.330000. running mean: -45.711021\n",
      "ep 931: ep_len:500 episode reward: total was -128.550000. running mean: -46.539411\n",
      "ep 931: ep_len:3 episode reward: total was -1.500000. running mean: -46.089017\n",
      "ep 931: ep_len:564 episode reward: total was -27.300000. running mean: -45.901126\n",
      "ep 931: ep_len:193 episode reward: total was -47.890000. running mean: -45.921015\n",
      "epsilon:0.158681 episode_count: 6524. steps_count: 2848105.000000\n",
      "Time elapsed:  8083.474062204361\n",
      "ep 932: ep_len:576 episode reward: total was -1.980000. running mean: -45.481605\n",
      "ep 932: ep_len:500 episode reward: total was -50.110000. running mean: -45.527889\n",
      "ep 932: ep_len:623 episode reward: total was -50.360000. running mean: -45.576210\n",
      "ep 932: ep_len:548 episode reward: total was -46.010000. running mean: -45.580548\n",
      "ep 932: ep_len:3 episode reward: total was 0.000000. running mean: -45.124742\n",
      "ep 932: ep_len:670 episode reward: total was -63.610000. running mean: -45.309595\n",
      "ep 932: ep_len:299 episode reward: total was -24.710000. running mean: -45.103599\n",
      "epsilon:0.158637 episode_count: 6531. steps_count: 2851324.000000\n",
      "Time elapsed:  8091.995001316071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 933: ep_len:505 episode reward: total was 9.360000. running mean: -44.558963\n",
      "ep 933: ep_len:575 episode reward: total was 37.340000. running mean: -43.739973\n",
      "ep 933: ep_len:453 episode reward: total was -11.640000. running mean: -43.418974\n",
      "ep 933: ep_len:500 episode reward: total was -52.290000. running mean: -43.507684\n",
      "ep 933: ep_len:115 episode reward: total was 1.210000. running mean: -43.060507\n",
      "ep 933: ep_len:571 episode reward: total was 3.150000. running mean: -42.598402\n",
      "ep 933: ep_len:500 episode reward: total was -36.220000. running mean: -42.534618\n",
      "epsilon:0.158593 episode_count: 6538. steps_count: 2854543.000000\n",
      "Time elapsed:  8100.507526636124\n",
      "ep 934: ep_len:234 episode reward: total was 0.740000. running mean: -42.101872\n",
      "ep 934: ep_len:599 episode reward: total was -27.630000. running mean: -41.957153\n",
      "ep 934: ep_len:558 episode reward: total was -96.590000. running mean: -42.503482\n",
      "ep 934: ep_len:508 episode reward: total was -39.130000. running mean: -42.469747\n",
      "ep 934: ep_len:3 episode reward: total was 0.000000. running mean: -42.045049\n",
      "ep 934: ep_len:619 episode reward: total was -40.380000. running mean: -42.028399\n",
      "ep 934: ep_len:205 episode reward: total was -55.360000. running mean: -42.161715\n",
      "epsilon:0.158548 episode_count: 6545. steps_count: 2857269.000000\n",
      "Time elapsed:  8107.972016334534\n",
      "ep 935: ep_len:538 episode reward: total was -126.640000. running mean: -43.006498\n",
      "ep 935: ep_len:578 episode reward: total was -167.020000. running mean: -44.246633\n",
      "ep 935: ep_len:390 episode reward: total was -0.780000. running mean: -43.811966\n",
      "ep 935: ep_len:121 episode reward: total was 3.020000. running mean: -43.343647\n",
      "ep 935: ep_len:3 episode reward: total was -3.000000. running mean: -42.940210\n",
      "ep 935: ep_len:500 episode reward: total was -14.400000. running mean: -42.654808\n",
      "ep 935: ep_len:585 episode reward: total was -68.440000. running mean: -42.912660\n",
      "epsilon:0.158504 episode_count: 6552. steps_count: 2859984.000000\n",
      "Time elapsed:  8115.360373020172\n",
      "ep 936: ep_len:621 episode reward: total was -96.130000. running mean: -43.444833\n",
      "ep 936: ep_len:560 episode reward: total was 23.480000. running mean: -42.775585\n",
      "ep 936: ep_len:651 episode reward: total was -75.490000. running mean: -43.102729\n",
      "ep 936: ep_len:530 episode reward: total was -7.090000. running mean: -42.742602\n",
      "ep 936: ep_len:87 episode reward: total was 7.750000. running mean: -42.237676\n",
      "ep 936: ep_len:579 episode reward: total was -71.680000. running mean: -42.532099\n",
      "ep 936: ep_len:556 episode reward: total was -27.250000. running mean: -42.379278\n",
      "epsilon:0.158460 episode_count: 6559. steps_count: 2863568.000000\n",
      "Time elapsed:  8123.574470996857\n",
      "ep 937: ep_len:612 episode reward: total was -129.160000. running mean: -43.247085\n",
      "ep 937: ep_len:524 episode reward: total was -68.950000. running mean: -43.504115\n",
      "ep 937: ep_len:500 episode reward: total was -27.780000. running mean: -43.346873\n",
      "ep 937: ep_len:500 episode reward: total was -22.900000. running mean: -43.142405\n",
      "ep 937: ep_len:49 episode reward: total was 11.000000. running mean: -42.600981\n",
      "ep 937: ep_len:583 episode reward: total was -74.150000. running mean: -42.916471\n",
      "ep 937: ep_len:292 episode reward: total was -25.880000. running mean: -42.746106\n",
      "epsilon:0.158415 episode_count: 6566. steps_count: 2866628.000000\n",
      "Time elapsed:  8131.724853277206\n",
      "ep 938: ep_len:131 episode reward: total was -0.590000. running mean: -42.324545\n",
      "ep 938: ep_len:533 episode reward: total was -1.640000. running mean: -41.917700\n",
      "ep 938: ep_len:581 episode reward: total was -39.700000. running mean: -41.895523\n",
      "ep 938: ep_len:500 episode reward: total was -64.780000. running mean: -42.124367\n",
      "ep 938: ep_len:3 episode reward: total was -1.500000. running mean: -41.718124\n",
      "ep 938: ep_len:310 episode reward: total was -18.440000. running mean: -41.485342\n",
      "ep 938: ep_len:516 episode reward: total was -66.530000. running mean: -41.735789\n",
      "epsilon:0.158371 episode_count: 6573. steps_count: 2869202.000000\n",
      "Time elapsed:  8138.617473363876\n",
      "ep 939: ep_len:500 episode reward: total was -11.980000. running mean: -41.438231\n",
      "ep 939: ep_len:609 episode reward: total was -86.170000. running mean: -41.885549\n",
      "ep 939: ep_len:460 episode reward: total was -23.050000. running mean: -41.697193\n",
      "ep 939: ep_len:567 episode reward: total was -9.880000. running mean: -41.379021\n",
      "ep 939: ep_len:3 episode reward: total was 0.000000. running mean: -40.965231\n",
      "ep 939: ep_len:672 episode reward: total was -40.460000. running mean: -40.960179\n",
      "ep 939: ep_len:520 episode reward: total was -62.880000. running mean: -41.179377\n",
      "epsilon:0.158327 episode_count: 6580. steps_count: 2872533.000000\n",
      "Time elapsed:  8147.553917884827\n",
      "ep 940: ep_len:602 episode reward: total was -104.830000. running mean: -41.815883\n",
      "ep 940: ep_len:569 episode reward: total was -40.860000. running mean: -41.806325\n",
      "ep 940: ep_len:576 episode reward: total was -69.520000. running mean: -42.083461\n",
      "ep 940: ep_len:588 episode reward: total was -106.510000. running mean: -42.727727\n",
      "ep 940: ep_len:3 episode reward: total was 0.000000. running mean: -42.300449\n",
      "ep 940: ep_len:541 episode reward: total was -36.360000. running mean: -42.241045\n",
      "ep 940: ep_len:308 episode reward: total was -85.670000. running mean: -42.675334\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.158282 episode_count: 6587. steps_count: 2875720.000000\n",
      "Time elapsed:  8161.051038742065\n",
      "ep 941: ep_len:513 episode reward: total was -32.790000. running mean: -42.576481\n",
      "ep 941: ep_len:556 episode reward: total was -30.660000. running mean: -42.457316\n",
      "ep 941: ep_len:546 episode reward: total was -63.840000. running mean: -42.671143\n",
      "ep 941: ep_len:568 episode reward: total was -31.560000. running mean: -42.560032\n",
      "ep 941: ep_len:107 episode reward: total was 15.240000. running mean: -41.982031\n",
      "ep 941: ep_len:234 episode reward: total was 0.950000. running mean: -41.552711\n",
      "ep 941: ep_len:590 episode reward: total was -93.460000. running mean: -42.071784\n",
      "epsilon:0.158238 episode_count: 6594. steps_count: 2878834.000000\n",
      "Time elapsed:  8169.2656836509705\n",
      "ep 942: ep_len:219 episode reward: total was 9.310000. running mean: -41.557966\n",
      "ep 942: ep_len:590 episode reward: total was -68.300000. running mean: -41.825386\n",
      "ep 942: ep_len:611 episode reward: total was -70.360000. running mean: -42.110733\n",
      "ep 942: ep_len:132 episode reward: total was -7.000000. running mean: -41.759625\n",
      "ep 942: ep_len:86 episode reward: total was 4.770000. running mean: -41.294329\n",
      "ep 942: ep_len:521 episode reward: total was -41.280000. running mean: -41.294186\n",
      "ep 942: ep_len:549 episode reward: total was -104.840000. running mean: -41.929644\n",
      "epsilon:0.158194 episode_count: 6601. steps_count: 2881542.000000\n",
      "Time elapsed:  8176.607665777206\n",
      "ep 943: ep_len:184 episode reward: total was -2.920000. running mean: -41.539547\n",
      "ep 943: ep_len:505 episode reward: total was -153.020000. running mean: -42.654352\n",
      "ep 943: ep_len:500 episode reward: total was -51.290000. running mean: -42.740708\n",
      "ep 943: ep_len:531 episode reward: total was -25.190000. running mean: -42.565201\n",
      "ep 943: ep_len:91 episode reward: total was -17.800000. running mean: -42.317549\n",
      "ep 943: ep_len:507 episode reward: total was -70.050000. running mean: -42.594874\n",
      "ep 943: ep_len:525 episode reward: total was -47.980000. running mean: -42.648725\n",
      "epsilon:0.158149 episode_count: 6608. steps_count: 2884385.000000\n",
      "Time elapsed:  8183.382970333099\n",
      "ep 944: ep_len:569 episode reward: total was -56.690000. running mean: -42.789138\n",
      "ep 944: ep_len:194 episode reward: total was -15.280000. running mean: -42.514046\n",
      "ep 944: ep_len:555 episode reward: total was -44.220000. running mean: -42.531106\n",
      "ep 944: ep_len:570 episode reward: total was -40.640000. running mean: -42.512195\n",
      "ep 944: ep_len:93 episode reward: total was 16.780000. running mean: -41.919273\n",
      "ep 944: ep_len:326 episode reward: total was -21.520000. running mean: -41.715280\n",
      "ep 944: ep_len:614 episode reward: total was -78.510000. running mean: -42.083227\n",
      "epsilon:0.158105 episode_count: 6615. steps_count: 2887306.000000\n",
      "Time elapsed:  8192.188516139984\n",
      "ep 945: ep_len:590 episode reward: total was 35.190000. running mean: -41.310495\n",
      "ep 945: ep_len:517 episode reward: total was -72.320000. running mean: -41.620590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 945: ep_len:579 episode reward: total was -60.970000. running mean: -41.814084\n",
      "ep 945: ep_len:500 episode reward: total was -27.190000. running mean: -41.667843\n",
      "ep 945: ep_len:3 episode reward: total was 0.000000. running mean: -41.251165\n",
      "ep 945: ep_len:502 episode reward: total was -56.820000. running mean: -41.406853\n",
      "ep 945: ep_len:152 episode reward: total was -35.200000. running mean: -41.344785\n",
      "epsilon:0.158061 episode_count: 6622. steps_count: 2890149.000000\n",
      "Time elapsed:  8200.822602033615\n",
      "ep 946: ep_len:519 episode reward: total was -49.840000. running mean: -41.429737\n",
      "ep 946: ep_len:331 episode reward: total was -90.310000. running mean: -41.918540\n",
      "ep 946: ep_len:896 episode reward: total was -276.910000. running mean: -44.268454\n",
      "ep 946: ep_len:544 episode reward: total was -84.040000. running mean: -44.666170\n",
      "ep 946: ep_len:67 episode reward: total was 2.650000. running mean: -44.193008\n",
      "ep 946: ep_len:500 episode reward: total was -38.630000. running mean: -44.137378\n",
      "ep 946: ep_len:539 episode reward: total was -76.290000. running mean: -44.458904\n",
      "epsilon:0.158016 episode_count: 6629. steps_count: 2893545.000000\n",
      "Time elapsed:  8210.633422136307\n",
      "ep 947: ep_len:504 episode reward: total was -48.350000. running mean: -44.497815\n",
      "ep 947: ep_len:642 episode reward: total was 8.610000. running mean: -43.966737\n",
      "ep 947: ep_len:518 episode reward: total was -68.440000. running mean: -44.211470\n",
      "ep 947: ep_len:585 episode reward: total was -15.810000. running mean: -43.927455\n",
      "ep 947: ep_len:97 episode reward: total was 10.210000. running mean: -43.386080\n",
      "ep 947: ep_len:238 episode reward: total was -11.800000. running mean: -43.070220\n",
      "ep 947: ep_len:304 episode reward: total was -32.420000. running mean: -42.963717\n",
      "epsilon:0.157972 episode_count: 6636. steps_count: 2896433.000000\n",
      "Time elapsed:  8218.130604028702\n",
      "ep 948: ep_len:500 episode reward: total was -44.280000. running mean: -42.976880\n",
      "ep 948: ep_len:556 episode reward: total was 25.290000. running mean: -42.294211\n",
      "ep 948: ep_len:562 episode reward: total was -33.260000. running mean: -42.203869\n",
      "ep 948: ep_len:601 episode reward: total was -21.340000. running mean: -41.995231\n",
      "ep 948: ep_len:3 episode reward: total was 0.000000. running mean: -41.575278\n",
      "ep 948: ep_len:501 episode reward: total was -4.830000. running mean: -41.207825\n",
      "ep 948: ep_len:205 episode reward: total was -34.530000. running mean: -41.141047\n",
      "epsilon:0.157928 episode_count: 6643. steps_count: 2899361.000000\n",
      "Time elapsed:  8225.969712495804\n",
      "ep 949: ep_len:500 episode reward: total was -11.380000. running mean: -40.843437\n",
      "ep 949: ep_len:500 episode reward: total was -42.290000. running mean: -40.857902\n",
      "ep 949: ep_len:556 episode reward: total was -40.210000. running mean: -40.851423\n",
      "ep 949: ep_len:501 episode reward: total was 2.670000. running mean: -40.416209\n",
      "ep 949: ep_len:91 episode reward: total was 13.730000. running mean: -39.874747\n",
      "ep 949: ep_len:572 episode reward: total was -30.560000. running mean: -39.781600\n",
      "ep 949: ep_len:299 episode reward: total was -36.220000. running mean: -39.745984\n",
      "epsilon:0.157883 episode_count: 6650. steps_count: 2902380.000000\n",
      "Time elapsed:  8234.078956365585\n",
      "ep 950: ep_len:500 episode reward: total was -35.430000. running mean: -39.702824\n",
      "ep 950: ep_len:502 episode reward: total was -47.330000. running mean: -39.779096\n",
      "ep 950: ep_len:378 episode reward: total was -28.940000. running mean: -39.670705\n",
      "ep 950: ep_len:500 episode reward: total was -30.980000. running mean: -39.583798\n",
      "ep 950: ep_len:3 episode reward: total was 0.000000. running mean: -39.187960\n",
      "ep 950: ep_len:501 episode reward: total was -45.230000. running mean: -39.248380\n",
      "ep 950: ep_len:634 episode reward: total was -124.850000. running mean: -40.104396\n",
      "epsilon:0.157839 episode_count: 6657. steps_count: 2905398.000000\n",
      "Time elapsed:  8241.705883741379\n",
      "ep 951: ep_len:523 episode reward: total was -73.780000. running mean: -40.441152\n",
      "ep 951: ep_len:500 episode reward: total was 11.920000. running mean: -39.917541\n",
      "ep 951: ep_len:366 episode reward: total was -21.810000. running mean: -39.736465\n",
      "ep 951: ep_len:595 episode reward: total was -13.810000. running mean: -39.477201\n",
      "ep 951: ep_len:79 episode reward: total was -15.750000. running mean: -39.239929\n",
      "ep 951: ep_len:519 episode reward: total was -77.410000. running mean: -39.621629\n",
      "ep 951: ep_len:189 episode reward: total was -14.200000. running mean: -39.367413\n",
      "epsilon:0.157795 episode_count: 6664. steps_count: 2908169.000000\n",
      "Time elapsed:  8250.746360301971\n",
      "ep 952: ep_len:631 episode reward: total was -102.060000. running mean: -39.994339\n",
      "ep 952: ep_len:581 episode reward: total was -89.560000. running mean: -40.489995\n",
      "ep 952: ep_len:595 episode reward: total was -82.410000. running mean: -40.909196\n",
      "ep 952: ep_len:590 episode reward: total was -14.720000. running mean: -40.647304\n",
      "ep 952: ep_len:107 episode reward: total was 15.240000. running mean: -40.088431\n",
      "ep 952: ep_len:550 episode reward: total was -47.320000. running mean: -40.160746\n",
      "ep 952: ep_len:506 episode reward: total was -66.300000. running mean: -40.422139\n",
      "epsilon:0.157750 episode_count: 6671. steps_count: 2911729.000000\n",
      "Time elapsed:  8259.978967666626\n",
      "ep 953: ep_len:585 episode reward: total was -22.740000. running mean: -40.245317\n",
      "ep 953: ep_len:504 episode reward: total was -85.720000. running mean: -40.700064\n",
      "ep 953: ep_len:661 episode reward: total was -63.220000. running mean: -40.925264\n",
      "ep 953: ep_len:129 episode reward: total was 0.500000. running mean: -40.511011\n",
      "ep 953: ep_len:3 episode reward: total was 0.000000. running mean: -40.105901\n",
      "ep 953: ep_len:664 episode reward: total was -151.670000. running mean: -41.221542\n",
      "ep 953: ep_len:291 episode reward: total was -35.810000. running mean: -41.167426\n",
      "epsilon:0.157706 episode_count: 6678. steps_count: 2914566.000000\n",
      "Time elapsed:  8269.843437194824\n",
      "ep 954: ep_len:610 episode reward: total was -73.040000. running mean: -41.486152\n",
      "ep 954: ep_len:618 episode reward: total was -131.140000. running mean: -42.382691\n",
      "ep 954: ep_len:439 episode reward: total was -5.750000. running mean: -42.016364\n",
      "ep 954: ep_len:503 episode reward: total was -67.370000. running mean: -42.269900\n",
      "ep 954: ep_len:92 episode reward: total was 0.730000. running mean: -41.839901\n",
      "ep 954: ep_len:601 episode reward: total was -50.530000. running mean: -41.926802\n",
      "ep 954: ep_len:589 episode reward: total was -72.460000. running mean: -42.232134\n",
      "epsilon:0.157662 episode_count: 6685. steps_count: 2918018.000000\n",
      "Time elapsed:  8279.525722503662\n",
      "ep 955: ep_len:567 episode reward: total was -45.570000. running mean: -42.265513\n",
      "ep 955: ep_len:500 episode reward: total was -119.380000. running mean: -43.036658\n",
      "ep 955: ep_len:610 episode reward: total was -62.400000. running mean: -43.230291\n",
      "ep 955: ep_len:500 episode reward: total was -103.640000. running mean: -43.834388\n",
      "ep 955: ep_len:112 episode reward: total was -26.730000. running mean: -43.663344\n",
      "ep 955: ep_len:600 episode reward: total was -95.220000. running mean: -44.178911\n",
      "ep 955: ep_len:592 episode reward: total was -46.950000. running mean: -44.206622\n",
      "epsilon:0.157617 episode_count: 6692. steps_count: 2921499.000000\n",
      "Time elapsed:  8289.986575841904\n",
      "ep 956: ep_len:550 episode reward: total was -0.480000. running mean: -43.769355\n",
      "ep 956: ep_len:628 episode reward: total was -92.700000. running mean: -44.258662\n",
      "ep 956: ep_len:649 episode reward: total was -55.690000. running mean: -44.372975\n",
      "ep 956: ep_len:500 episode reward: total was -65.120000. running mean: -44.580446\n",
      "ep 956: ep_len:3 episode reward: total was -1.500000. running mean: -44.149641\n",
      "ep 956: ep_len:545 episode reward: total was -68.550000. running mean: -44.393645\n",
      "ep 956: ep_len:574 episode reward: total was -86.030000. running mean: -44.810008\n",
      "epsilon:0.157573 episode_count: 6699. steps_count: 2924948.000000\n",
      "Time elapsed:  8299.532547712326\n",
      "ep 957: ep_len:532 episode reward: total was -30.410000. running mean: -44.666008\n",
      "ep 957: ep_len:521 episode reward: total was 10.670000. running mean: -44.112648\n",
      "ep 957: ep_len:406 episode reward: total was -42.890000. running mean: -44.100422\n",
      "ep 957: ep_len:599 episode reward: total was -12.670000. running mean: -43.786117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 957: ep_len:82 episode reward: total was 10.730000. running mean: -43.240956\n",
      "ep 957: ep_len:527 episode reward: total was -48.670000. running mean: -43.295247\n",
      "ep 957: ep_len:549 episode reward: total was -74.810000. running mean: -43.610394\n",
      "epsilon:0.157529 episode_count: 6706. steps_count: 2928164.000000\n",
      "Time elapsed:  8310.716059446335\n",
      "ep 958: ep_len:584 episode reward: total was -40.060000. running mean: -43.574890\n",
      "ep 958: ep_len:500 episode reward: total was -76.410000. running mean: -43.903241\n",
      "ep 958: ep_len:613 episode reward: total was -90.740000. running mean: -44.371609\n",
      "ep 958: ep_len:502 episode reward: total was -48.480000. running mean: -44.412693\n",
      "ep 958: ep_len:3 episode reward: total was 0.000000. running mean: -43.968566\n",
      "ep 958: ep_len:617 episode reward: total was -77.540000. running mean: -44.304280\n",
      "ep 958: ep_len:550 episode reward: total was -64.250000. running mean: -44.503737\n",
      "epsilon:0.157484 episode_count: 6713. steps_count: 2931533.000000\n",
      "Time elapsed:  8321.853491306305\n",
      "ep 959: ep_len:621 episode reward: total was -99.070000. running mean: -45.049400\n",
      "ep 959: ep_len:500 episode reward: total was -20.100000. running mean: -44.799906\n",
      "ep 959: ep_len:500 episode reward: total was -79.400000. running mean: -45.145907\n",
      "ep 959: ep_len:56 episode reward: total was -5.180000. running mean: -44.746248\n",
      "ep 959: ep_len:3 episode reward: total was -1.500000. running mean: -44.313785\n",
      "ep 959: ep_len:648 episode reward: total was -73.840000. running mean: -44.609048\n",
      "ep 959: ep_len:609 episode reward: total was -49.210000. running mean: -44.655057\n",
      "epsilon:0.157440 episode_count: 6720. steps_count: 2934470.000000\n",
      "Time elapsed:  8332.066441774368\n",
      "ep 960: ep_len:198 episode reward: total was -9.930000. running mean: -44.307807\n",
      "ep 960: ep_len:345 episode reward: total was -103.230000. running mean: -44.897028\n",
      "ep 960: ep_len:613 episode reward: total was -155.150000. running mean: -45.999558\n",
      "ep 960: ep_len:500 episode reward: total was -66.600000. running mean: -46.205563\n",
      "ep 960: ep_len:3 episode reward: total was 0.000000. running mean: -45.743507\n",
      "ep 960: ep_len:500 episode reward: total was -83.410000. running mean: -46.120172\n",
      "ep 960: ep_len:173 episode reward: total was -25.750000. running mean: -45.916470\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.157396 episode_count: 6727. steps_count: 2936802.000000\n",
      "Time elapsed:  8345.012707710266\n",
      "ep 961: ep_len:224 episode reward: total was -24.460000. running mean: -45.701905\n",
      "ep 961: ep_len:349 episode reward: total was -26.910000. running mean: -45.513986\n",
      "ep 961: ep_len:581 episode reward: total was -103.740000. running mean: -46.096247\n",
      "ep 961: ep_len:502 episode reward: total was -33.620000. running mean: -45.971484\n",
      "ep 961: ep_len:3 episode reward: total was 0.000000. running mean: -45.511769\n",
      "ep 961: ep_len:673 episode reward: total was -52.900000. running mean: -45.585652\n",
      "ep 961: ep_len:500 episode reward: total was -57.620000. running mean: -45.705995\n",
      "epsilon:0.157351 episode_count: 6734. steps_count: 2939634.000000\n",
      "Time elapsed:  8355.109488248825\n",
      "ep 962: ep_len:521 episode reward: total was -70.640000. running mean: -45.955335\n",
      "ep 962: ep_len:201 episode reward: total was -7.160000. running mean: -45.567382\n",
      "ep 962: ep_len:559 episode reward: total was -84.620000. running mean: -45.957908\n",
      "ep 962: ep_len:56 episode reward: total was -1.720000. running mean: -45.515529\n",
      "ep 962: ep_len:3 episode reward: total was 0.000000. running mean: -45.060374\n",
      "ep 962: ep_len:541 episode reward: total was -126.210000. running mean: -45.871870\n",
      "ep 962: ep_len:706 episode reward: total was -302.590000. running mean: -48.439051\n",
      "epsilon:0.157307 episode_count: 6741. steps_count: 2942221.000000\n",
      "Time elapsed:  8363.251898050308\n",
      "ep 963: ep_len:666 episode reward: total was -139.230000. running mean: -49.346961\n",
      "ep 963: ep_len:552 episode reward: total was -11.180000. running mean: -48.965291\n",
      "ep 963: ep_len:411 episode reward: total was -38.430000. running mean: -48.859938\n",
      "ep 963: ep_len:500 episode reward: total was -71.910000. running mean: -49.090439\n",
      "ep 963: ep_len:3 episode reward: total was 0.000000. running mean: -48.599534\n",
      "ep 963: ep_len:563 episode reward: total was -40.440000. running mean: -48.517939\n",
      "ep 963: ep_len:500 episode reward: total was -51.750000. running mean: -48.550260\n",
      "epsilon:0.157263 episode_count: 6748. steps_count: 2945416.000000\n",
      "Time elapsed:  8372.870610952377\n",
      "ep 964: ep_len:624 episode reward: total was -38.740000. running mean: -48.452157\n",
      "ep 964: ep_len:527 episode reward: total was -4.850000. running mean: -48.016135\n",
      "ep 964: ep_len:654 episode reward: total was -64.690000. running mean: -48.182874\n",
      "ep 964: ep_len:500 episode reward: total was -76.260000. running mean: -48.463645\n",
      "ep 964: ep_len:96 episode reward: total was 12.740000. running mean: -47.851609\n",
      "ep 964: ep_len:167 episode reward: total was 9.910000. running mean: -47.273993\n",
      "ep 964: ep_len:503 episode reward: total was -68.090000. running mean: -47.482153\n",
      "epsilon:0.157218 episode_count: 6755. steps_count: 2948487.000000\n",
      "Time elapsed:  8382.121113061905\n",
      "ep 965: ep_len:204 episode reward: total was -14.530000. running mean: -47.152631\n",
      "ep 965: ep_len:501 episode reward: total was -43.870000. running mean: -47.119805\n",
      "ep 965: ep_len:72 episode reward: total was -5.810000. running mean: -46.706707\n",
      "ep 965: ep_len:554 episode reward: total was -30.930000. running mean: -46.548940\n",
      "ep 965: ep_len:3 episode reward: total was 0.000000. running mean: -46.083450\n",
      "ep 965: ep_len:510 episode reward: total was -193.800000. running mean: -47.560616\n",
      "ep 965: ep_len:588 episode reward: total was -58.360000. running mean: -47.668610\n",
      "epsilon:0.157174 episode_count: 6762. steps_count: 2950919.000000\n",
      "Time elapsed:  8389.40039229393\n",
      "ep 966: ep_len:566 episode reward: total was 5.770000. running mean: -47.134224\n",
      "ep 966: ep_len:512 episode reward: total was -40.160000. running mean: -47.064481\n",
      "ep 966: ep_len:79 episode reward: total was -0.720000. running mean: -46.601037\n",
      "ep 966: ep_len:523 episode reward: total was 11.710000. running mean: -46.017926\n",
      "ep 966: ep_len:100 episode reward: total was 9.230000. running mean: -45.465447\n",
      "ep 966: ep_len:514 episode reward: total was -33.820000. running mean: -45.348993\n",
      "ep 966: ep_len:605 episode reward: total was -70.420000. running mean: -45.599703\n",
      "epsilon:0.157130 episode_count: 6769. steps_count: 2953818.000000\n",
      "Time elapsed:  8399.919241189957\n",
      "ep 967: ep_len:618 episode reward: total was -131.480000. running mean: -46.458506\n",
      "ep 967: ep_len:592 episode reward: total was -140.620000. running mean: -47.400121\n",
      "ep 967: ep_len:569 episode reward: total was -94.950000. running mean: -47.875619\n",
      "ep 967: ep_len:515 episode reward: total was -44.040000. running mean: -47.837263\n",
      "ep 967: ep_len:127 episode reward: total was 4.760000. running mean: -47.311291\n",
      "ep 967: ep_len:500 episode reward: total was -90.380000. running mean: -47.741978\n",
      "ep 967: ep_len:501 episode reward: total was -83.840000. running mean: -48.102958\n",
      "epsilon:0.157085 episode_count: 6776. steps_count: 2957240.000000\n",
      "Time elapsed:  8410.186130285263\n",
      "ep 968: ep_len:658 episode reward: total was -122.600000. running mean: -48.847928\n",
      "ep 968: ep_len:513 episode reward: total was -12.330000. running mean: -48.482749\n",
      "ep 968: ep_len:569 episode reward: total was -106.370000. running mean: -49.061622\n",
      "ep 968: ep_len:124 episode reward: total was -2.980000. running mean: -48.600805\n",
      "ep 968: ep_len:3 episode reward: total was 0.000000. running mean: -48.114797\n",
      "ep 968: ep_len:508 episode reward: total was -28.280000. running mean: -47.916449\n",
      "ep 968: ep_len:506 episode reward: total was -42.150000. running mean: -47.858785\n",
      "epsilon:0.157041 episode_count: 6783. steps_count: 2960121.000000\n",
      "Time elapsed:  8419.247737884521\n",
      "ep 969: ep_len:583 episode reward: total was -19.210000. running mean: -47.572297\n",
      "ep 969: ep_len:541 episode reward: total was -30.920000. running mean: -47.405774\n",
      "ep 969: ep_len:859 episode reward: total was -252.470000. running mean: -49.456416\n",
      "ep 969: ep_len:522 episode reward: total was -35.100000. running mean: -49.312852\n",
      "ep 969: ep_len:84 episode reward: total was 1.750000. running mean: -48.802224\n",
      "ep 969: ep_len:556 episode reward: total was -105.270000. running mean: -49.366901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 969: ep_len:211 episode reward: total was -42.630000. running mean: -49.299532\n",
      "epsilon:0.156997 episode_count: 6790. steps_count: 2963477.000000\n",
      "Time elapsed:  8429.279751300812\n",
      "ep 970: ep_len:511 episode reward: total was -51.560000. running mean: -49.322137\n",
      "ep 970: ep_len:527 episode reward: total was -93.820000. running mean: -49.767116\n",
      "ep 970: ep_len:79 episode reward: total was 3.780000. running mean: -49.231644\n",
      "ep 970: ep_len:512 episode reward: total was -47.600000. running mean: -49.215328\n",
      "ep 970: ep_len:86 episode reward: total was 9.730000. running mean: -48.625875\n",
      "ep 970: ep_len:557 episode reward: total was -83.100000. running mean: -48.970616\n",
      "ep 970: ep_len:500 episode reward: total was -94.030000. running mean: -49.421210\n",
      "epsilon:0.156952 episode_count: 6797. steps_count: 2966249.000000\n",
      "Time elapsed:  8439.158572912216\n",
      "ep 971: ep_len:509 episode reward: total was -101.940000. running mean: -49.946398\n",
      "ep 971: ep_len:500 episode reward: total was -45.090000. running mean: -49.897834\n",
      "ep 971: ep_len:564 episode reward: total was -80.460000. running mean: -50.203455\n",
      "ep 971: ep_len:521 episode reward: total was 4.250000. running mean: -49.658921\n",
      "ep 971: ep_len:3 episode reward: total was 0.000000. running mean: -49.162332\n",
      "ep 971: ep_len:588 episode reward: total was -38.260000. running mean: -49.053308\n",
      "ep 971: ep_len:500 episode reward: total was -81.560000. running mean: -49.378375\n",
      "epsilon:0.156908 episode_count: 6804. steps_count: 2969434.000000\n",
      "Time elapsed:  8448.676915884018\n",
      "ep 972: ep_len:562 episode reward: total was 26.280000. running mean: -48.621791\n",
      "ep 972: ep_len:513 episode reward: total was -97.620000. running mean: -49.111774\n",
      "ep 972: ep_len:551 episode reward: total was -79.250000. running mean: -49.413156\n",
      "ep 972: ep_len:528 episode reward: total was -47.620000. running mean: -49.395224\n",
      "ep 972: ep_len:120 episode reward: total was 0.250000. running mean: -48.898772\n",
      "ep 972: ep_len:570 episode reward: total was -54.690000. running mean: -48.956684\n",
      "ep 972: ep_len:533 episode reward: total was -68.250000. running mean: -49.149617\n",
      "epsilon:0.156864 episode_count: 6811. steps_count: 2972811.000000\n",
      "Time elapsed:  8458.774881839752\n",
      "ep 973: ep_len:648 episode reward: total was -75.450000. running mean: -49.412621\n",
      "ep 973: ep_len:500 episode reward: total was -43.040000. running mean: -49.348895\n",
      "ep 973: ep_len:558 episode reward: total was -80.920000. running mean: -49.664606\n",
      "ep 973: ep_len:550 episode reward: total was 5.750000. running mean: -49.110460\n",
      "ep 973: ep_len:3 episode reward: total was 0.000000. running mean: -48.619355\n",
      "ep 973: ep_len:507 episode reward: total was -65.380000. running mean: -48.786962\n",
      "ep 973: ep_len:608 episode reward: total was -79.310000. running mean: -49.092192\n",
      "epsilon:0.156819 episode_count: 6818. steps_count: 2976185.000000\n",
      "Time elapsed:  8468.889777421951\n",
      "ep 974: ep_len:500 episode reward: total was 17.430000. running mean: -48.426970\n",
      "ep 974: ep_len:500 episode reward: total was -85.660000. running mean: -48.799301\n",
      "ep 974: ep_len:600 episode reward: total was -74.770000. running mean: -49.059008\n",
      "ep 974: ep_len:132 episode reward: total was -12.570000. running mean: -48.694118\n",
      "ep 974: ep_len:82 episode reward: total was 6.230000. running mean: -48.144876\n",
      "ep 974: ep_len:637 episode reward: total was -28.300000. running mean: -47.946428\n",
      "ep 974: ep_len:530 episode reward: total was -79.110000. running mean: -48.258063\n",
      "epsilon:0.156775 episode_count: 6825. steps_count: 2979166.000000\n",
      "Time elapsed:  8477.942962169647\n",
      "ep 975: ep_len:170 episode reward: total was -21.030000. running mean: -47.985783\n",
      "ep 975: ep_len:500 episode reward: total was -11.510000. running mean: -47.621025\n",
      "ep 975: ep_len:452 episode reward: total was -7.120000. running mean: -47.216015\n",
      "ep 975: ep_len:608 episode reward: total was -14.440000. running mean: -46.888255\n",
      "ep 975: ep_len:3 episode reward: total was 0.000000. running mean: -46.419372\n",
      "ep 975: ep_len:593 episode reward: total was -46.870000. running mean: -46.423878\n",
      "ep 975: ep_len:183 episode reward: total was -29.900000. running mean: -46.258639\n",
      "epsilon:0.156731 episode_count: 6832. steps_count: 2981675.000000\n",
      "Time elapsed:  8487.782755851746\n",
      "ep 976: ep_len:559 episode reward: total was -77.430000. running mean: -46.570353\n",
      "ep 976: ep_len:602 episode reward: total was -41.660000. running mean: -46.521250\n",
      "ep 976: ep_len:678 episode reward: total was -96.990000. running mean: -47.025937\n",
      "ep 976: ep_len:38 episode reward: total was -1.350000. running mean: -46.569178\n",
      "ep 976: ep_len:3 episode reward: total was 0.000000. running mean: -46.103486\n",
      "ep 976: ep_len:526 episode reward: total was -62.500000. running mean: -46.267451\n",
      "ep 976: ep_len:588 episode reward: total was -51.290000. running mean: -46.317677\n",
      "epsilon:0.156686 episode_count: 6839. steps_count: 2984669.000000\n",
      "Time elapsed:  8498.407594442368\n",
      "ep 977: ep_len:134 episode reward: total was -5.520000. running mean: -45.909700\n",
      "ep 977: ep_len:375 episode reward: total was -52.000000. running mean: -45.970603\n",
      "ep 977: ep_len:559 episode reward: total was -13.180000. running mean: -45.642697\n",
      "ep 977: ep_len:393 episode reward: total was -74.150000. running mean: -45.927770\n",
      "ep 977: ep_len:3 episode reward: total was 0.000000. running mean: -45.468492\n",
      "ep 977: ep_len:537 episode reward: total was -67.670000. running mean: -45.690507\n",
      "ep 977: ep_len:555 episode reward: total was -45.880000. running mean: -45.692402\n",
      "epsilon:0.156642 episode_count: 6846. steps_count: 2987225.000000\n",
      "Time elapsed:  8506.525935173035\n",
      "ep 978: ep_len:620 episode reward: total was -81.790000. running mean: -46.053378\n",
      "ep 978: ep_len:501 episode reward: total was -19.090000. running mean: -45.783744\n",
      "ep 978: ep_len:507 episode reward: total was -71.670000. running mean: -46.042607\n",
      "ep 978: ep_len:116 episode reward: total was -6.010000. running mean: -45.642281\n",
      "ep 978: ep_len:3 episode reward: total was 0.000000. running mean: -45.185858\n",
      "ep 978: ep_len:581 episode reward: total was -50.510000. running mean: -45.239099\n",
      "ep 978: ep_len:311 episode reward: total was -72.440000. running mean: -45.511108\n",
      "epsilon:0.156598 episode_count: 6853. steps_count: 2989864.000000\n",
      "Time elapsed:  8515.993902683258\n",
      "ep 979: ep_len:500 episode reward: total was -2.810000. running mean: -45.084097\n",
      "ep 979: ep_len:183 episode reward: total was -52.060000. running mean: -45.153856\n",
      "ep 979: ep_len:465 episode reward: total was -17.490000. running mean: -44.877218\n",
      "ep 979: ep_len:501 episode reward: total was -43.280000. running mean: -44.861246\n",
      "ep 979: ep_len:114 episode reward: total was 7.810000. running mean: -44.334533\n",
      "ep 979: ep_len:503 episode reward: total was -27.270000. running mean: -44.163888\n",
      "ep 979: ep_len:537 episode reward: total was -54.780000. running mean: -44.270049\n",
      "epsilon:0.156553 episode_count: 6860. steps_count: 2992667.000000\n",
      "Time elapsed:  8524.53925895691\n",
      "ep 980: ep_len:529 episode reward: total was -1.040000. running mean: -43.837748\n",
      "ep 980: ep_len:500 episode reward: total was -28.640000. running mean: -43.685771\n",
      "ep 980: ep_len:622 episode reward: total was -74.140000. running mean: -43.990313\n",
      "ep 980: ep_len:500 episode reward: total was 8.570000. running mean: -43.464710\n",
      "ep 980: ep_len:3 episode reward: total was 0.000000. running mean: -43.030063\n",
      "ep 980: ep_len:336 episode reward: total was -16.460000. running mean: -42.764362\n",
      "ep 980: ep_len:500 episode reward: total was -73.890000. running mean: -43.075619\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.156509 episode_count: 6867. steps_count: 2995657.000000\n",
      "Time elapsed:  8540.850591659546\n",
      "ep 981: ep_len:503 episode reward: total was -31.390000. running mean: -42.958763\n",
      "ep 981: ep_len:501 episode reward: total was -34.150000. running mean: -42.870675\n",
      "ep 981: ep_len:500 episode reward: total was -39.430000. running mean: -42.836268\n",
      "ep 981: ep_len:56 episode reward: total was -13.690000. running mean: -42.544806\n",
      "ep 981: ep_len:92 episode reward: total was -57.750000. running mean: -42.696857\n",
      "ep 981: ep_len:695 episode reward: total was -170.820000. running mean: -43.978089\n",
      "ep 981: ep_len:603 episode reward: total was -37.080000. running mean: -43.909108\n",
      "epsilon:0.156465 episode_count: 6874. steps_count: 2998607.000000\n",
      "Time elapsed:  8549.738931179047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 982: ep_len:522 episode reward: total was -86.310000. running mean: -44.333117\n",
      "ep 982: ep_len:500 episode reward: total was -15.820000. running mean: -44.047986\n",
      "ep 982: ep_len:591 episode reward: total was -67.840000. running mean: -44.285906\n",
      "ep 982: ep_len:564 episode reward: total was 22.760000. running mean: -43.615447\n",
      "ep 982: ep_len:95 episode reward: total was 15.730000. running mean: -43.021992\n",
      "ep 982: ep_len:628 episode reward: total was -66.010000. running mean: -43.251872\n",
      "ep 982: ep_len:558 episode reward: total was -45.920000. running mean: -43.278554\n",
      "epsilon:0.156420 episode_count: 6881. steps_count: 3002065.000000\n",
      "Time elapsed:  8560.176358699799\n",
      "ep 983: ep_len:548 episode reward: total was -106.220000. running mean: -43.907968\n",
      "ep 983: ep_len:500 episode reward: total was -52.150000. running mean: -43.990388\n",
      "ep 983: ep_len:581 episode reward: total was -65.590000. running mean: -44.206385\n",
      "ep 983: ep_len:501 episode reward: total was -28.000000. running mean: -44.044321\n",
      "ep 983: ep_len:3 episode reward: total was 0.000000. running mean: -43.603878\n",
      "ep 983: ep_len:598 episode reward: total was -126.310000. running mean: -44.430939\n",
      "ep 983: ep_len:573 episode reward: total was -36.840000. running mean: -44.355029\n",
      "epsilon:0.156376 episode_count: 6888. steps_count: 3005369.000000\n",
      "Time elapsed:  8566.916008710861\n",
      "ep 984: ep_len:562 episode reward: total was -88.540000. running mean: -44.796879\n",
      "ep 984: ep_len:501 episode reward: total was -55.480000. running mean: -44.903710\n",
      "ep 984: ep_len:556 episode reward: total was -73.390000. running mean: -45.188573\n",
      "ep 984: ep_len:363 episode reward: total was -40.120000. running mean: -45.137887\n",
      "ep 984: ep_len:3 episode reward: total was 0.000000. running mean: -44.686509\n",
      "ep 984: ep_len:500 episode reward: total was -69.920000. running mean: -44.938844\n",
      "ep 984: ep_len:564 episode reward: total was -101.150000. running mean: -45.500955\n",
      "epsilon:0.156332 episode_count: 6895. steps_count: 3008418.000000\n",
      "Time elapsed:  8576.411549806595\n",
      "ep 985: ep_len:570 episode reward: total was -107.620000. running mean: -46.122146\n",
      "ep 985: ep_len:637 episode reward: total was -38.480000. running mean: -46.045724\n",
      "ep 985: ep_len:563 episode reward: total was -19.170000. running mean: -45.776967\n",
      "ep 985: ep_len:516 episode reward: total was -1.800000. running mean: -45.337197\n",
      "ep 985: ep_len:3 episode reward: total was -1.500000. running mean: -44.898825\n",
      "ep 985: ep_len:624 episode reward: total was -24.930000. running mean: -44.699137\n",
      "ep 985: ep_len:587 episode reward: total was -56.000000. running mean: -44.812146\n",
      "epsilon:0.156287 episode_count: 6902. steps_count: 3011918.000000\n",
      "Time elapsed:  8586.822655439377\n",
      "ep 986: ep_len:624 episode reward: total was -45.900000. running mean: -44.823024\n",
      "ep 986: ep_len:579 episode reward: total was 17.330000. running mean: -44.201494\n",
      "ep 986: ep_len:582 episode reward: total was -83.830000. running mean: -44.597779\n",
      "ep 986: ep_len:505 episode reward: total was 4.000000. running mean: -44.111801\n",
      "ep 986: ep_len:3 episode reward: total was -1.500000. running mean: -43.685683\n",
      "ep 986: ep_len:177 episode reward: total was 11.140000. running mean: -43.137426\n",
      "ep 986: ep_len:195 episode reward: total was -36.240000. running mean: -43.068452\n",
      "epsilon:0.156243 episode_count: 6909. steps_count: 3014583.000000\n",
      "Time elapsed:  8595.05583357811\n",
      "ep 987: ep_len:653 episode reward: total was -76.650000. running mean: -43.404268\n",
      "ep 987: ep_len:531 episode reward: total was -69.140000. running mean: -43.661625\n",
      "ep 987: ep_len:500 episode reward: total was -67.080000. running mean: -43.895809\n",
      "ep 987: ep_len:500 episode reward: total was -38.720000. running mean: -43.844051\n",
      "ep 987: ep_len:109 episode reward: total was -44.250000. running mean: -43.848110\n",
      "ep 987: ep_len:560 episode reward: total was -69.110000. running mean: -44.100729\n",
      "ep 987: ep_len:618 episode reward: total was -60.300000. running mean: -44.262722\n",
      "epsilon:0.156199 episode_count: 6916. steps_count: 3018054.000000\n",
      "Time elapsed:  8605.155307769775\n",
      "ep 988: ep_len:134 episode reward: total was 2.010000. running mean: -43.799994\n",
      "ep 988: ep_len:595 episode reward: total was -56.670000. running mean: -43.928694\n",
      "ep 988: ep_len:617 episode reward: total was -62.440000. running mean: -44.113808\n",
      "ep 988: ep_len:518 episode reward: total was -21.880000. running mean: -43.891469\n",
      "ep 988: ep_len:103 episode reward: total was -17.740000. running mean: -43.629955\n",
      "ep 988: ep_len:500 episode reward: total was -31.090000. running mean: -43.504555\n",
      "ep 988: ep_len:208 episode reward: total was -49.150000. running mean: -43.561010\n",
      "epsilon:0.156154 episode_count: 6923. steps_count: 3020729.000000\n",
      "Time elapsed:  8613.32632637024\n",
      "ep 989: ep_len:550 episode reward: total was -46.030000. running mean: -43.585700\n",
      "ep 989: ep_len:631 episode reward: total was -46.970000. running mean: -43.619543\n",
      "ep 989: ep_len:513 episode reward: total was -56.380000. running mean: -43.747147\n",
      "ep 989: ep_len:501 episode reward: total was -74.350000. running mean: -44.053176\n",
      "ep 989: ep_len:3 episode reward: total was 0.000000. running mean: -43.612644\n",
      "ep 989: ep_len:503 episode reward: total was -44.820000. running mean: -43.624717\n",
      "ep 989: ep_len:296 episode reward: total was -64.360000. running mean: -43.832070\n",
      "epsilon:0.156110 episode_count: 6930. steps_count: 3023726.000000\n",
      "Time elapsed:  8622.270372390747\n",
      "ep 990: ep_len:243 episode reward: total was 0.250000. running mean: -43.391250\n",
      "ep 990: ep_len:357 episode reward: total was -91.610000. running mean: -43.873437\n",
      "ep 990: ep_len:71 episode reward: total was -2.240000. running mean: -43.457103\n",
      "ep 990: ep_len:53 episode reward: total was -3.710000. running mean: -43.059632\n",
      "ep 990: ep_len:2 episode reward: total was -0.500000. running mean: -42.634035\n",
      "ep 990: ep_len:500 episode reward: total was -32.460000. running mean: -42.532295\n",
      "ep 990: ep_len:567 episode reward: total was -80.660000. running mean: -42.913572\n",
      "epsilon:0.156066 episode_count: 6937. steps_count: 3025519.000000\n",
      "Time elapsed:  8628.316219568253\n",
      "ep 991: ep_len:597 episode reward: total was -29.060000. running mean: -42.775036\n",
      "ep 991: ep_len:632 episode reward: total was -94.460000. running mean: -43.291886\n",
      "ep 991: ep_len:534 episode reward: total was -62.310000. running mean: -43.482067\n",
      "ep 991: ep_len:600 episode reward: total was -20.090000. running mean: -43.248146\n",
      "ep 991: ep_len:3 episode reward: total was 0.000000. running mean: -42.815665\n",
      "ep 991: ep_len:218 episode reward: total was 15.610000. running mean: -42.231408\n",
      "ep 991: ep_len:585 episode reward: total was -40.780000. running mean: -42.216894\n",
      "epsilon:0.156021 episode_count: 6944. steps_count: 3028688.000000\n",
      "Time elapsed:  8639.7408618927\n",
      "ep 992: ep_len:533 episode reward: total was -9.620000. running mean: -41.890925\n",
      "ep 992: ep_len:364 episode reward: total was -58.180000. running mean: -42.053816\n",
      "ep 992: ep_len:558 episode reward: total was -56.720000. running mean: -42.200478\n",
      "ep 992: ep_len:500 episode reward: total was -85.650000. running mean: -42.634973\n",
      "ep 992: ep_len:3 episode reward: total was 0.000000. running mean: -42.208623\n",
      "ep 992: ep_len:500 episode reward: total was -26.890000. running mean: -42.055437\n",
      "ep 992: ep_len:552 episode reward: total was -69.100000. running mean: -42.325883\n",
      "epsilon:0.155977 episode_count: 6951. steps_count: 3031698.000000\n",
      "Time elapsed:  8648.687740802765\n",
      "ep 993: ep_len:500 episode reward: total was 1.970000. running mean: -41.882924\n",
      "ep 993: ep_len:501 episode reward: total was -86.020000. running mean: -42.324295\n",
      "ep 993: ep_len:500 episode reward: total was -49.590000. running mean: -42.396952\n",
      "ep 993: ep_len:170 episode reward: total was -11.440000. running mean: -42.087382\n",
      "ep 993: ep_len:3 episode reward: total was 0.000000. running mean: -41.666508\n",
      "ep 993: ep_len:304 episode reward: total was -14.090000. running mean: -41.390743\n",
      "ep 993: ep_len:564 episode reward: total was -74.880000. running mean: -41.725636\n",
      "epsilon:0.155933 episode_count: 6958. steps_count: 3034240.000000\n",
      "Time elapsed:  8656.688656568527\n",
      "ep 994: ep_len:561 episode reward: total was 7.190000. running mean: -41.236480\n",
      "ep 994: ep_len:500 episode reward: total was -117.710000. running mean: -42.001215\n",
      "ep 994: ep_len:500 episode reward: total was -28.340000. running mean: -41.864603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 994: ep_len:501 episode reward: total was 0.720000. running mean: -41.438757\n",
      "ep 994: ep_len:3 episode reward: total was 1.010000. running mean: -41.014269\n",
      "ep 994: ep_len:511 episode reward: total was -59.950000. running mean: -41.203626\n",
      "ep 994: ep_len:305 episode reward: total was -47.200000. running mean: -41.263590\n",
      "epsilon:0.155888 episode_count: 6965. steps_count: 3037121.000000\n",
      "Time elapsed:  8665.460592508316\n",
      "ep 995: ep_len:195 episode reward: total was -7.890000. running mean: -40.929854\n",
      "ep 995: ep_len:500 episode reward: total was -70.510000. running mean: -41.225656\n",
      "ep 995: ep_len:593 episode reward: total was -54.030000. running mean: -41.353699\n",
      "ep 995: ep_len:500 episode reward: total was -34.080000. running mean: -41.280962\n",
      "ep 995: ep_len:43 episode reward: total was 14.000000. running mean: -40.728152\n",
      "ep 995: ep_len:181 episode reward: total was -1.920000. running mean: -40.340071\n",
      "ep 995: ep_len:299 episode reward: total was -75.450000. running mean: -40.691170\n",
      "epsilon:0.155844 episode_count: 6972. steps_count: 3039432.000000\n",
      "Time elapsed:  8672.852596282959\n",
      "ep 996: ep_len:509 episode reward: total was -67.810000. running mean: -40.962359\n",
      "ep 996: ep_len:625 episode reward: total was -26.790000. running mean: -40.820635\n",
      "ep 996: ep_len:501 episode reward: total was -44.180000. running mean: -40.854229\n",
      "ep 996: ep_len:514 episode reward: total was -72.560000. running mean: -41.171286\n",
      "ep 996: ep_len:108 episode reward: total was 14.270000. running mean: -40.616873\n",
      "ep 996: ep_len:509 episode reward: total was -42.110000. running mean: -40.631805\n",
      "ep 996: ep_len:500 episode reward: total was -67.730000. running mean: -40.902787\n",
      "epsilon:0.155800 episode_count: 6979. steps_count: 3042698.000000\n",
      "Time elapsed:  8684.119195699692\n",
      "ep 997: ep_len:593 episode reward: total was 28.060000. running mean: -40.213159\n",
      "ep 997: ep_len:515 episode reward: total was -135.570000. running mean: -41.166727\n",
      "ep 997: ep_len:545 episode reward: total was -64.990000. running mean: -41.404960\n",
      "ep 997: ep_len:533 episode reward: total was -49.980000. running mean: -41.490710\n",
      "ep 997: ep_len:84 episode reward: total was 13.690000. running mean: -40.938903\n",
      "ep 997: ep_len:180 episode reward: total was 15.060000. running mean: -40.378914\n",
      "ep 997: ep_len:511 episode reward: total was -55.990000. running mean: -40.535025\n",
      "epsilon:0.155755 episode_count: 6986. steps_count: 3045659.000000\n",
      "Time elapsed:  8693.482731103897\n",
      "ep 998: ep_len:595 episode reward: total was -42.990000. running mean: -40.559575\n",
      "ep 998: ep_len:174 episode reward: total was -2.720000. running mean: -40.181179\n",
      "ep 998: ep_len:540 episode reward: total was -85.490000. running mean: -40.634267\n",
      "ep 998: ep_len:525 episode reward: total was 11.360000. running mean: -40.114325\n",
      "ep 998: ep_len:3 episode reward: total was -1.500000. running mean: -39.728181\n",
      "ep 998: ep_len:552 episode reward: total was -68.050000. running mean: -40.011400\n",
      "ep 998: ep_len:543 episode reward: total was -61.000000. running mean: -40.221286\n",
      "epsilon:0.155711 episode_count: 6993. steps_count: 3048591.000000\n",
      "Time elapsed:  8702.485473155975\n",
      "ep 999: ep_len:563 episode reward: total was -11.640000. running mean: -39.935473\n",
      "ep 999: ep_len:595 episode reward: total was -87.410000. running mean: -40.410218\n",
      "ep 999: ep_len:500 episode reward: total was -29.580000. running mean: -40.301916\n",
      "ep 999: ep_len:525 episode reward: total was -53.470000. running mean: -40.433597\n",
      "ep 999: ep_len:87 episode reward: total was 0.740000. running mean: -40.021861\n",
      "ep 999: ep_len:252 episode reward: total was 9.460000. running mean: -39.527042\n",
      "ep 999: ep_len:607 episode reward: total was -12.390000. running mean: -39.255672\n",
      "epsilon:0.155667 episode_count: 7000. steps_count: 3051720.000000\n",
      "Time elapsed:  8712.68236207962\n",
      "ep 1000: ep_len:601 episode reward: total was -8.090000. running mean: -38.944015\n",
      "ep 1000: ep_len:500 episode reward: total was -5.290000. running mean: -38.607475\n",
      "ep 1000: ep_len:403 episode reward: total was -28.600000. running mean: -38.507400\n",
      "ep 1000: ep_len:502 episode reward: total was -61.690000. running mean: -38.739226\n",
      "ep 1000: ep_len:3 episode reward: total was 0.000000. running mean: -38.351834\n",
      "ep 1000: ep_len:648 episode reward: total was -92.670000. running mean: -38.895015\n",
      "ep 1000: ep_len:500 episode reward: total was -73.850000. running mean: -39.244565\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.155622 episode_count: 7007. steps_count: 3054877.000000\n",
      "Time elapsed:  8727.495621442795\n",
      "ep 1001: ep_len:532 episode reward: total was -19.030000. running mean: -39.042420\n",
      "ep 1001: ep_len:500 episode reward: total was -29.690000. running mean: -38.948895\n",
      "ep 1001: ep_len:635 episode reward: total was -79.520000. running mean: -39.354606\n",
      "ep 1001: ep_len:612 episode reward: total was -32.300000. running mean: -39.284060\n",
      "ep 1001: ep_len:107 episode reward: total was 17.260000. running mean: -38.718620\n",
      "ep 1001: ep_len:679 episode reward: total was -88.380000. running mean: -39.215234\n",
      "ep 1001: ep_len:590 episode reward: total was -71.800000. running mean: -39.541081\n",
      "epsilon:0.155578 episode_count: 7014. steps_count: 3058532.000000\n",
      "Time elapsed:  8738.518542528152\n",
      "ep 1002: ep_len:134 episode reward: total was -10.600000. running mean: -39.251670\n",
      "ep 1002: ep_len:596 episode reward: total was -153.050000. running mean: -40.389654\n",
      "ep 1002: ep_len:464 episode reward: total was -16.370000. running mean: -40.149457\n",
      "ep 1002: ep_len:511 episode reward: total was -52.660000. running mean: -40.274563\n",
      "ep 1002: ep_len:110 episode reward: total was -12.250000. running mean: -39.994317\n",
      "ep 1002: ep_len:576 episode reward: total was -102.310000. running mean: -40.617474\n",
      "ep 1002: ep_len:557 episode reward: total was -45.440000. running mean: -40.665699\n",
      "epsilon:0.155534 episode_count: 7021. steps_count: 3061480.000000\n",
      "Time elapsed:  8747.657736063004\n",
      "ep 1003: ep_len:581 episode reward: total was -37.220000. running mean: -40.631242\n",
      "ep 1003: ep_len:556 episode reward: total was -61.990000. running mean: -40.844830\n",
      "ep 1003: ep_len:542 episode reward: total was -116.260000. running mean: -41.598981\n",
      "ep 1003: ep_len:510 episode reward: total was 15.590000. running mean: -41.027092\n",
      "ep 1003: ep_len:3 episode reward: total was 0.000000. running mean: -40.616821\n",
      "ep 1003: ep_len:500 episode reward: total was -38.430000. running mean: -40.594952\n",
      "ep 1003: ep_len:630 episode reward: total was -78.830000. running mean: -40.977303\n",
      "epsilon:0.155489 episode_count: 7028. steps_count: 3064802.000000\n",
      "Time elapsed:  8757.792236566544\n",
      "ep 1004: ep_len:237 episode reward: total was -12.870000. running mean: -40.696230\n",
      "ep 1004: ep_len:525 episode reward: total was -16.930000. running mean: -40.458568\n",
      "ep 1004: ep_len:551 episode reward: total was -87.760000. running mean: -40.931582\n",
      "ep 1004: ep_len:500 episode reward: total was -57.290000. running mean: -41.095166\n",
      "ep 1004: ep_len:3 episode reward: total was 0.000000. running mean: -40.684214\n",
      "ep 1004: ep_len:500 episode reward: total was -49.000000. running mean: -40.767372\n",
      "ep 1004: ep_len:555 episode reward: total was -116.810000. running mean: -41.527799\n",
      "epsilon:0.155445 episode_count: 7035. steps_count: 3067673.000000\n",
      "Time elapsed:  8766.618014335632\n",
      "ep 1005: ep_len:518 episode reward: total was 14.300000. running mean: -40.969521\n",
      "ep 1005: ep_len:500 episode reward: total was -47.660000. running mean: -41.036425\n",
      "ep 1005: ep_len:369 episode reward: total was -24.470000. running mean: -40.870761\n",
      "ep 1005: ep_len:515 episode reward: total was -55.520000. running mean: -41.017254\n",
      "ep 1005: ep_len:3 episode reward: total was 0.000000. running mean: -40.607081\n",
      "ep 1005: ep_len:560 episode reward: total was -36.920000. running mean: -40.570210\n",
      "ep 1005: ep_len:584 episode reward: total was -95.890000. running mean: -41.123408\n",
      "epsilon:0.155401 episode_count: 7042. steps_count: 3070722.000000\n",
      "Time elapsed:  8775.779341936111\n",
      "ep 1006: ep_len:547 episode reward: total was -67.600000. running mean: -41.388174\n",
      "ep 1006: ep_len:626 episode reward: total was -90.220000. running mean: -41.876492\n",
      "ep 1006: ep_len:442 episode reward: total was -0.980000. running mean: -41.467527\n",
      "ep 1006: ep_len:512 episode reward: total was -11.840000. running mean: -41.171252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1006: ep_len:125 episode reward: total was 0.300000. running mean: -40.756540\n",
      "ep 1006: ep_len:534 episode reward: total was -136.400000. running mean: -41.712974\n",
      "ep 1006: ep_len:590 episode reward: total was -55.970000. running mean: -41.855544\n",
      "epsilon:0.155356 episode_count: 7049. steps_count: 3074098.000000\n",
      "Time elapsed:  8785.850514173508\n",
      "ep 1007: ep_len:514 episode reward: total was 8.750000. running mean: -41.349489\n",
      "ep 1007: ep_len:500 episode reward: total was -8.930000. running mean: -41.025294\n",
      "ep 1007: ep_len:571 episode reward: total was -73.810000. running mean: -41.353141\n",
      "ep 1007: ep_len:591 episode reward: total was -33.020000. running mean: -41.269810\n",
      "ep 1007: ep_len:3 episode reward: total was 0.000000. running mean: -40.857112\n",
      "ep 1007: ep_len:532 episode reward: total was -71.290000. running mean: -41.161440\n",
      "ep 1007: ep_len:531 episode reward: total was -33.420000. running mean: -41.084026\n",
      "epsilon:0.155312 episode_count: 7056. steps_count: 3077340.000000\n",
      "Time elapsed:  8795.639101982117\n",
      "ep 1008: ep_len:600 episode reward: total was -8.020000. running mean: -40.753386\n",
      "ep 1008: ep_len:613 episode reward: total was -32.210000. running mean: -40.667952\n",
      "ep 1008: ep_len:584 episode reward: total was -45.730000. running mean: -40.718572\n",
      "ep 1008: ep_len:500 episode reward: total was -44.520000. running mean: -40.756587\n",
      "ep 1008: ep_len:3 episode reward: total was 0.000000. running mean: -40.349021\n",
      "ep 1008: ep_len:515 episode reward: total was -84.890000. running mean: -40.794431\n",
      "ep 1008: ep_len:335 episode reward: total was -64.310000. running mean: -41.029586\n",
      "epsilon:0.155268 episode_count: 7063. steps_count: 3080490.000000\n",
      "Time elapsed:  8805.18236041069\n",
      "ep 1009: ep_len:501 episode reward: total was -1.260000. running mean: -40.631890\n",
      "ep 1009: ep_len:609 episode reward: total was -35.220000. running mean: -40.577772\n",
      "ep 1009: ep_len:617 episode reward: total was -21.550000. running mean: -40.387494\n",
      "ep 1009: ep_len:558 episode reward: total was 19.360000. running mean: -39.790019\n",
      "ep 1009: ep_len:3 episode reward: total was 0.000000. running mean: -39.392119\n",
      "ep 1009: ep_len:215 episode reward: total was 11.170000. running mean: -38.886498\n",
      "ep 1009: ep_len:547 episode reward: total was -87.250000. running mean: -39.370133\n",
      "epsilon:0.155223 episode_count: 7070. steps_count: 3083540.000000\n",
      "Time elapsed:  8814.246459007263\n",
      "ep 1010: ep_len:245 episode reward: total was -2.780000. running mean: -39.004231\n",
      "ep 1010: ep_len:593 episode reward: total was -106.890000. running mean: -39.683089\n",
      "ep 1010: ep_len:526 episode reward: total was -77.730000. running mean: -40.063558\n",
      "ep 1010: ep_len:507 episode reward: total was -158.950000. running mean: -41.252422\n",
      "ep 1010: ep_len:52 episode reward: total was 15.500000. running mean: -40.684898\n",
      "ep 1010: ep_len:573 episode reward: total was -27.110000. running mean: -40.549149\n",
      "ep 1010: ep_len:561 episode reward: total was -73.920000. running mean: -40.882858\n",
      "epsilon:0.155179 episode_count: 7077. steps_count: 3086597.000000\n",
      "Time elapsed:  8825.160890102386\n",
      "ep 1011: ep_len:645 episode reward: total was -7.980000. running mean: -40.553829\n",
      "ep 1011: ep_len:604 episode reward: total was -114.090000. running mean: -41.289191\n",
      "ep 1011: ep_len:584 episode reward: total was -71.340000. running mean: -41.589699\n",
      "ep 1011: ep_len:124 episode reward: total was -6.020000. running mean: -41.234002\n",
      "ep 1011: ep_len:3 episode reward: total was -3.000000. running mean: -40.851662\n",
      "ep 1011: ep_len:503 episode reward: total was -70.660000. running mean: -41.149745\n",
      "ep 1011: ep_len:560 episode reward: total was -92.820000. running mean: -41.666448\n",
      "epsilon:0.155135 episode_count: 7084. steps_count: 3089620.000000\n",
      "Time elapsed:  8835.902555465698\n",
      "ep 1012: ep_len:130 episode reward: total was -1.030000. running mean: -41.260083\n",
      "ep 1012: ep_len:500 episode reward: total was -46.750000. running mean: -41.314983\n",
      "ep 1012: ep_len:577 episode reward: total was -66.070000. running mean: -41.562533\n",
      "ep 1012: ep_len:503 episode reward: total was 6.580000. running mean: -41.081107\n",
      "ep 1012: ep_len:3 episode reward: total was 0.000000. running mean: -40.670296\n",
      "ep 1012: ep_len:592 episode reward: total was -45.380000. running mean: -40.717393\n",
      "ep 1012: ep_len:293 episode reward: total was -40.910000. running mean: -40.719319\n",
      "epsilon:0.155090 episode_count: 7091. steps_count: 3092218.000000\n",
      "Time elapsed:  8843.927869796753\n",
      "ep 1013: ep_len:208 episode reward: total was -16.330000. running mean: -40.475426\n",
      "ep 1013: ep_len:552 episode reward: total was -64.640000. running mean: -40.717072\n",
      "ep 1013: ep_len:554 episode reward: total was -27.010000. running mean: -40.580001\n",
      "ep 1013: ep_len:123 episode reward: total was 9.470000. running mean: -40.079501\n",
      "ep 1013: ep_len:3 episode reward: total was 0.000000. running mean: -39.678706\n",
      "ep 1013: ep_len:639 episode reward: total was -23.390000. running mean: -39.515819\n",
      "ep 1013: ep_len:575 episode reward: total was -101.380000. running mean: -40.134461\n",
      "epsilon:0.155046 episode_count: 7098. steps_count: 3094872.000000\n",
      "Time elapsed:  8855.39427781105\n",
      "ep 1014: ep_len:241 episode reward: total was -5.250000. running mean: -39.785616\n",
      "ep 1014: ep_len:500 episode reward: total was -80.970000. running mean: -40.197460\n",
      "ep 1014: ep_len:607 episode reward: total was -51.220000. running mean: -40.307686\n",
      "ep 1014: ep_len:520 episode reward: total was -60.170000. running mean: -40.506309\n",
      "ep 1014: ep_len:3 episode reward: total was 0.000000. running mean: -40.101246\n",
      "ep 1014: ep_len:500 episode reward: total was -64.030000. running mean: -40.340533\n",
      "ep 1014: ep_len:545 episode reward: total was -58.410000. running mean: -40.521228\n",
      "epsilon:0.155002 episode_count: 7105. steps_count: 3097788.000000\n",
      "Time elapsed:  8864.685307264328\n",
      "ep 1015: ep_len:689 episode reward: total was -285.830000. running mean: -42.974316\n",
      "ep 1015: ep_len:500 episode reward: total was -21.300000. running mean: -42.757572\n",
      "ep 1015: ep_len:500 episode reward: total was -69.290000. running mean: -43.022897\n",
      "ep 1015: ep_len:117 episode reward: total was 4.940000. running mean: -42.543268\n",
      "ep 1015: ep_len:3 episode reward: total was 0.000000. running mean: -42.117835\n",
      "ep 1015: ep_len:636 episode reward: total was -73.080000. running mean: -42.427457\n",
      "ep 1015: ep_len:287 episode reward: total was -71.550000. running mean: -42.718682\n",
      "epsilon:0.154957 episode_count: 7112. steps_count: 3100520.000000\n",
      "Time elapsed:  8873.19407916069\n",
      "ep 1016: ep_len:540 episode reward: total was -243.280000. running mean: -44.724295\n",
      "ep 1016: ep_len:588 episode reward: total was -86.030000. running mean: -45.137352\n",
      "ep 1016: ep_len:582 episode reward: total was -49.130000. running mean: -45.177279\n",
      "ep 1016: ep_len:502 episode reward: total was -30.000000. running mean: -45.025506\n",
      "ep 1016: ep_len:32 episode reward: total was -3.500000. running mean: -44.610251\n",
      "ep 1016: ep_len:180 episode reward: total was 14.110000. running mean: -44.023049\n",
      "ep 1016: ep_len:579 episode reward: total was -77.980000. running mean: -44.362618\n",
      "epsilon:0.154913 episode_count: 7119. steps_count: 3103523.000000\n",
      "Time elapsed:  8882.321434497833\n",
      "ep 1017: ep_len:593 episode reward: total was -36.110000. running mean: -44.280092\n",
      "ep 1017: ep_len:500 episode reward: total was -26.630000. running mean: -44.103591\n",
      "ep 1017: ep_len:566 episode reward: total was -79.900000. running mean: -44.461555\n",
      "ep 1017: ep_len:132 episode reward: total was -0.940000. running mean: -44.026339\n",
      "ep 1017: ep_len:3 episode reward: total was 0.000000. running mean: -43.586076\n",
      "ep 1017: ep_len:323 episode reward: total was -9.980000. running mean: -43.250015\n",
      "ep 1017: ep_len:559 episode reward: total was -80.670000. running mean: -43.624215\n",
      "epsilon:0.154869 episode_count: 7126. steps_count: 3106199.000000\n",
      "Time elapsed:  8891.085689544678\n",
      "ep 1018: ep_len:249 episode reward: total was -2.140000. running mean: -43.209373\n",
      "ep 1018: ep_len:534 episode reward: total was 7.950000. running mean: -42.697779\n",
      "ep 1018: ep_len:556 episode reward: total was -117.160000. running mean: -43.442401\n",
      "ep 1018: ep_len:514 episode reward: total was -115.910000. running mean: -44.167077\n",
      "ep 1018: ep_len:3 episode reward: total was 0.000000. running mean: -43.725407\n",
      "ep 1018: ep_len:511 episode reward: total was -50.470000. running mean: -43.792853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1018: ep_len:577 episode reward: total was -76.480000. running mean: -44.119724\n",
      "epsilon:0.154824 episode_count: 7133. steps_count: 3109143.000000\n",
      "Time elapsed:  8902.251120090485\n",
      "ep 1019: ep_len:583 episode reward: total was -60.070000. running mean: -44.279227\n",
      "ep 1019: ep_len:516 episode reward: total was -23.610000. running mean: -44.072535\n",
      "ep 1019: ep_len:534 episode reward: total was -43.330000. running mean: -44.065109\n",
      "ep 1019: ep_len:521 episode reward: total was -32.750000. running mean: -43.951958\n",
      "ep 1019: ep_len:3 episode reward: total was 0.000000. running mean: -43.512439\n",
      "ep 1019: ep_len:582 episode reward: total was -53.130000. running mean: -43.608614\n",
      "ep 1019: ep_len:500 episode reward: total was -45.740000. running mean: -43.629928\n",
      "epsilon:0.154780 episode_count: 7140. steps_count: 3112382.000000\n",
      "Time elapsed:  8912.063577651978\n",
      "ep 1020: ep_len:554 episode reward: total was -79.100000. running mean: -43.984629\n",
      "ep 1020: ep_len:540 episode reward: total was -9.620000. running mean: -43.640982\n",
      "ep 1020: ep_len:510 episode reward: total was -43.050000. running mean: -43.635073\n",
      "ep 1020: ep_len:500 episode reward: total was -44.960000. running mean: -43.648322\n",
      "ep 1020: ep_len:3 episode reward: total was 0.000000. running mean: -43.211839\n",
      "ep 1020: ep_len:257 episode reward: total was 17.440000. running mean: -42.605320\n",
      "ep 1020: ep_len:500 episode reward: total was -53.280000. running mean: -42.712067\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.154736 episode_count: 7147. steps_count: 3115246.000000\n",
      "Time elapsed:  8927.001246452332\n",
      "ep 1021: ep_len:111 episode reward: total was -1.580000. running mean: -42.300746\n",
      "ep 1021: ep_len:532 episode reward: total was -47.140000. running mean: -42.349139\n",
      "ep 1021: ep_len:500 episode reward: total was -155.540000. running mean: -43.481048\n",
      "ep 1021: ep_len:593 episode reward: total was -53.440000. running mean: -43.580637\n",
      "ep 1021: ep_len:70 episode reward: total was -28.420000. running mean: -43.429031\n",
      "ep 1021: ep_len:146 episode reward: total was -0.590000. running mean: -43.000640\n",
      "ep 1021: ep_len:521 episode reward: total was -37.540000. running mean: -42.946034\n",
      "epsilon:0.154691 episode_count: 7154. steps_count: 3117719.000000\n",
      "Time elapsed:  8935.017213821411\n",
      "ep 1022: ep_len:500 episode reward: total was -10.910000. running mean: -42.625674\n",
      "ep 1022: ep_len:520 episode reward: total was -69.820000. running mean: -42.897617\n",
      "ep 1022: ep_len:573 episode reward: total was -89.420000. running mean: -43.362841\n",
      "ep 1022: ep_len:132 episode reward: total was 0.990000. running mean: -42.919312\n",
      "ep 1022: ep_len:3 episode reward: total was 0.000000. running mean: -42.490119\n",
      "ep 1022: ep_len:545 episode reward: total was -64.060000. running mean: -42.705818\n",
      "ep 1022: ep_len:588 episode reward: total was -57.650000. running mean: -42.855260\n",
      "epsilon:0.154647 episode_count: 7161. steps_count: 3120580.000000\n",
      "Time elapsed:  8941.940799713135\n",
      "ep 1023: ep_len:226 episode reward: total was 7.240000. running mean: -42.354307\n",
      "ep 1023: ep_len:500 episode reward: total was -8.170000. running mean: -42.012464\n",
      "ep 1023: ep_len:546 episode reward: total was -32.660000. running mean: -41.918940\n",
      "ep 1023: ep_len:500 episode reward: total was -38.130000. running mean: -41.881050\n",
      "ep 1023: ep_len:3 episode reward: total was 0.000000. running mean: -41.462240\n",
      "ep 1023: ep_len:576 episode reward: total was -43.580000. running mean: -41.483417\n",
      "ep 1023: ep_len:583 episode reward: total was -63.310000. running mean: -41.701683\n",
      "epsilon:0.154603 episode_count: 7168. steps_count: 3123514.000000\n",
      "Time elapsed:  8950.83150267601\n",
      "ep 1024: ep_len:502 episode reward: total was 4.720000. running mean: -41.237466\n",
      "ep 1024: ep_len:500 episode reward: total was 5.730000. running mean: -40.767792\n",
      "ep 1024: ep_len:554 episode reward: total was -60.470000. running mean: -40.964814\n",
      "ep 1024: ep_len:530 episode reward: total was -15.620000. running mean: -40.711366\n",
      "ep 1024: ep_len:3 episode reward: total was 0.000000. running mean: -40.304252\n",
      "ep 1024: ep_len:250 episode reward: total was 2.770000. running mean: -39.873509\n",
      "ep 1024: ep_len:549 episode reward: total was -28.280000. running mean: -39.757574\n",
      "epsilon:0.154558 episode_count: 7175. steps_count: 3126402.000000\n",
      "Time elapsed:  8961.009632587433\n",
      "ep 1025: ep_len:537 episode reward: total was -137.790000. running mean: -40.737899\n",
      "ep 1025: ep_len:531 episode reward: total was -25.100000. running mean: -40.581520\n",
      "ep 1025: ep_len:540 episode reward: total was -78.810000. running mean: -40.963804\n",
      "ep 1025: ep_len:556 episode reward: total was -39.400000. running mean: -40.948166\n",
      "ep 1025: ep_len:90 episode reward: total was -6.240000. running mean: -40.601085\n",
      "ep 1025: ep_len:500 episode reward: total was -23.320000. running mean: -40.428274\n",
      "ep 1025: ep_len:520 episode reward: total was -58.240000. running mean: -40.606391\n",
      "epsilon:0.154514 episode_count: 7182. steps_count: 3129676.000000\n",
      "Time elapsed:  8971.280005931854\n",
      "ep 1026: ep_len:500 episode reward: total was 22.750000. running mean: -39.972827\n",
      "ep 1026: ep_len:588 episode reward: total was -66.760000. running mean: -40.240699\n",
      "ep 1026: ep_len:515 episode reward: total was -39.170000. running mean: -40.229992\n",
      "ep 1026: ep_len:534 episode reward: total was -92.650000. running mean: -40.754192\n",
      "ep 1026: ep_len:112 episode reward: total was 8.740000. running mean: -40.259250\n",
      "ep 1026: ep_len:505 episode reward: total was -10.800000. running mean: -39.964658\n",
      "ep 1026: ep_len:533 episode reward: total was -39.930000. running mean: -39.964311\n",
      "epsilon:0.154470 episode_count: 7189. steps_count: 3132963.000000\n",
      "Time elapsed:  8981.114836215973\n",
      "ep 1027: ep_len:625 episode reward: total was -114.120000. running mean: -40.705868\n",
      "ep 1027: ep_len:500 episode reward: total was 2.400000. running mean: -40.274809\n",
      "ep 1027: ep_len:588 episode reward: total was -47.470000. running mean: -40.346761\n",
      "ep 1027: ep_len:500 episode reward: total was -2.850000. running mean: -39.971793\n",
      "ep 1027: ep_len:3 episode reward: total was -1.500000. running mean: -39.587076\n",
      "ep 1027: ep_len:505 episode reward: total was -87.430000. running mean: -40.065505\n",
      "ep 1027: ep_len:530 episode reward: total was -57.190000. running mean: -40.236750\n",
      "epsilon:0.154425 episode_count: 7196. steps_count: 3136214.000000\n",
      "Time elapsed:  8993.616151809692\n",
      "ep 1028: ep_len:134 episode reward: total was -0.040000. running mean: -39.834782\n",
      "ep 1028: ep_len:615 episode reward: total was -104.930000. running mean: -40.485734\n",
      "ep 1028: ep_len:500 episode reward: total was -70.710000. running mean: -40.787977\n",
      "ep 1028: ep_len:170 episode reward: total was 1.660000. running mean: -40.363497\n",
      "ep 1028: ep_len:3 episode reward: total was -1.500000. running mean: -39.974862\n",
      "ep 1028: ep_len:502 episode reward: total was -41.860000. running mean: -39.993714\n",
      "ep 1028: ep_len:206 episode reward: total was -56.580000. running mean: -40.159577\n",
      "epsilon:0.154381 episode_count: 7203. steps_count: 3138344.000000\n",
      "Time elapsed:  9002.380653381348\n",
      "ep 1029: ep_len:564 episode reward: total was -90.970000. running mean: -40.667681\n",
      "ep 1029: ep_len:500 episode reward: total was -11.090000. running mean: -40.371904\n",
      "ep 1029: ep_len:668 episode reward: total was -101.560000. running mean: -40.983785\n",
      "ep 1029: ep_len:503 episode reward: total was -20.120000. running mean: -40.775147\n",
      "ep 1029: ep_len:3 episode reward: total was 0.000000. running mean: -40.367396\n",
      "ep 1029: ep_len:591 episode reward: total was -66.360000. running mean: -40.627322\n",
      "ep 1029: ep_len:504 episode reward: total was -47.570000. running mean: -40.696748\n",
      "epsilon:0.154337 episode_count: 7210. steps_count: 3141677.000000\n",
      "Time elapsed:  9012.199082612991\n",
      "ep 1030: ep_len:246 episode reward: total was -6.230000. running mean: -40.352081\n",
      "ep 1030: ep_len:570 episode reward: total was -23.830000. running mean: -40.186860\n",
      "ep 1030: ep_len:549 episode reward: total was -40.280000. running mean: -40.187792\n",
      "ep 1030: ep_len:500 episode reward: total was -51.470000. running mean: -40.300614\n",
      "ep 1030: ep_len:3 episode reward: total was -1.500000. running mean: -39.912608\n",
      "ep 1030: ep_len:583 episode reward: total was -40.910000. running mean: -39.922581\n",
      "ep 1030: ep_len:291 episode reward: total was -51.700000. running mean: -40.040356\n",
      "epsilon:0.154292 episode_count: 7217. steps_count: 3144419.000000\n",
      "Time elapsed:  9020.61725306511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1031: ep_len:125 episode reward: total was -6.590000. running mean: -39.705852\n",
      "ep 1031: ep_len:501 episode reward: total was -111.540000. running mean: -40.424194\n",
      "ep 1031: ep_len:500 episode reward: total was -35.920000. running mean: -40.379152\n",
      "ep 1031: ep_len:539 episode reward: total was -31.590000. running mean: -40.291260\n",
      "ep 1031: ep_len:3 episode reward: total was 0.000000. running mean: -39.888348\n",
      "ep 1031: ep_len:640 episode reward: total was -55.440000. running mean: -40.043864\n",
      "ep 1031: ep_len:574 episode reward: total was -78.880000. running mean: -40.432225\n",
      "epsilon:0.154248 episode_count: 7224. steps_count: 3147301.000000\n",
      "Time elapsed:  9029.256990909576\n",
      "ep 1032: ep_len:513 episode reward: total was -4.840000. running mean: -40.076303\n",
      "ep 1032: ep_len:510 episode reward: total was -100.590000. running mean: -40.681440\n",
      "ep 1032: ep_len:578 episode reward: total was -70.560000. running mean: -40.980226\n",
      "ep 1032: ep_len:539 episode reward: total was -73.620000. running mean: -41.306623\n",
      "ep 1032: ep_len:3 episode reward: total was 0.000000. running mean: -40.893557\n",
      "ep 1032: ep_len:600 episode reward: total was -97.290000. running mean: -41.457522\n",
      "ep 1032: ep_len:518 episode reward: total was -26.500000. running mean: -41.307946\n",
      "epsilon:0.154204 episode_count: 7231. steps_count: 3150562.000000\n",
      "Time elapsed:  9040.459347248077\n",
      "ep 1033: ep_len:183 episode reward: total was -17.350000. running mean: -41.068367\n",
      "ep 1033: ep_len:545 episode reward: total was -77.260000. running mean: -41.430283\n",
      "ep 1033: ep_len:384 episode reward: total was -63.060000. running mean: -41.646580\n",
      "ep 1033: ep_len:542 episode reward: total was -15.900000. running mean: -41.389115\n",
      "ep 1033: ep_len:3 episode reward: total was 0.000000. running mean: -40.975224\n",
      "ep 1033: ep_len:698 episode reward: total was -108.440000. running mean: -41.649871\n",
      "ep 1033: ep_len:577 episode reward: total was -93.040000. running mean: -42.163773\n",
      "epsilon:0.154159 episode_count: 7238. steps_count: 3153494.000000\n",
      "Time elapsed:  9049.011859893799\n",
      "ep 1034: ep_len:571 episode reward: total was -118.140000. running mean: -42.923535\n",
      "ep 1034: ep_len:630 episode reward: total was -68.530000. running mean: -43.179599\n",
      "ep 1034: ep_len:417 episode reward: total was -14.540000. running mean: -42.893203\n",
      "ep 1034: ep_len:153 episode reward: total was 9.040000. running mean: -42.373871\n",
      "ep 1034: ep_len:85 episode reward: total was -2.280000. running mean: -41.972933\n",
      "ep 1034: ep_len:593 episode reward: total was -138.470000. running mean: -42.937903\n",
      "ep 1034: ep_len:571 episode reward: total was -128.510000. running mean: -43.793624\n",
      "epsilon:0.154115 episode_count: 7245. steps_count: 3156514.000000\n",
      "Time elapsed:  9058.121001958847\n",
      "ep 1035: ep_len:223 episode reward: total was -3.440000. running mean: -43.390088\n",
      "ep 1035: ep_len:501 episode reward: total was -37.500000. running mean: -43.331187\n",
      "ep 1035: ep_len:576 episode reward: total was -28.240000. running mean: -43.180275\n",
      "ep 1035: ep_len:48 episode reward: total was -2.200000. running mean: -42.770473\n",
      "ep 1035: ep_len:64 episode reward: total was -36.390000. running mean: -42.706668\n",
      "ep 1035: ep_len:619 episode reward: total was -114.100000. running mean: -43.420601\n",
      "ep 1035: ep_len:500 episode reward: total was -59.070000. running mean: -43.577095\n",
      "epsilon:0.154071 episode_count: 7252. steps_count: 3159045.000000\n",
      "Time elapsed:  9065.977643251419\n",
      "ep 1036: ep_len:501 episode reward: total was 11.130000. running mean: -43.030024\n",
      "ep 1036: ep_len:645 episode reward: total was -2.410000. running mean: -42.623824\n",
      "ep 1036: ep_len:621 episode reward: total was -36.020000. running mean: -42.557786\n",
      "ep 1036: ep_len:159 episode reward: total was -0.810000. running mean: -42.140308\n",
      "ep 1036: ep_len:84 episode reward: total was 8.240000. running mean: -41.636505\n",
      "ep 1036: ep_len:306 episode reward: total was -19.520000. running mean: -41.415340\n",
      "ep 1036: ep_len:556 episode reward: total was -41.240000. running mean: -41.413586\n",
      "epsilon:0.154026 episode_count: 7259. steps_count: 3161917.000000\n",
      "Time elapsed:  9074.888234853745\n",
      "ep 1037: ep_len:531 episode reward: total was -25.980000. running mean: -41.259251\n",
      "ep 1037: ep_len:515 episode reward: total was -99.010000. running mean: -41.836758\n",
      "ep 1037: ep_len:546 episode reward: total was -69.980000. running mean: -42.118190\n",
      "ep 1037: ep_len:140 episode reward: total was 7.050000. running mean: -41.626509\n",
      "ep 1037: ep_len:64 episode reward: total was -26.800000. running mean: -41.478243\n",
      "ep 1037: ep_len:314 episode reward: total was -11.850000. running mean: -41.181961\n",
      "ep 1037: ep_len:562 episode reward: total was -84.760000. running mean: -41.617741\n",
      "epsilon:0.153982 episode_count: 7266. steps_count: 3164589.000000\n",
      "Time elapsed:  9083.198053836823\n",
      "ep 1038: ep_len:511 episode reward: total was 2.390000. running mean: -41.177664\n",
      "ep 1038: ep_len:543 episode reward: total was 13.720000. running mean: -40.628687\n",
      "ep 1038: ep_len:79 episode reward: total was -0.230000. running mean: -40.224700\n",
      "ep 1038: ep_len:620 episode reward: total was -18.330000. running mean: -40.005753\n",
      "ep 1038: ep_len:3 episode reward: total was -1.500000. running mean: -39.620696\n",
      "ep 1038: ep_len:168 episode reward: total was -8.050000. running mean: -39.304989\n",
      "ep 1038: ep_len:637 episode reward: total was -45.090000. running mean: -39.362839\n",
      "epsilon:0.153938 episode_count: 7273. steps_count: 3167150.000000\n",
      "Time elapsed:  9091.589911222458\n",
      "ep 1039: ep_len:628 episode reward: total was -49.510000. running mean: -39.464311\n",
      "ep 1039: ep_len:614 episode reward: total was -19.260000. running mean: -39.262268\n",
      "ep 1039: ep_len:661 episode reward: total was -81.400000. running mean: -39.683645\n",
      "ep 1039: ep_len:520 episode reward: total was -67.230000. running mean: -39.959108\n",
      "ep 1039: ep_len:81 episode reward: total was -38.270000. running mean: -39.942217\n",
      "ep 1039: ep_len:320 episode reward: total was -29.050000. running mean: -39.833295\n",
      "ep 1039: ep_len:508 episode reward: total was -87.350000. running mean: -40.308462\n",
      "epsilon:0.153893 episode_count: 7280. steps_count: 3170482.000000\n",
      "Time elapsed:  9102.75935959816\n",
      "ep 1040: ep_len:562 episode reward: total was -15.720000. running mean: -40.062578\n",
      "ep 1040: ep_len:531 episode reward: total was -134.540000. running mean: -41.007352\n",
      "ep 1040: ep_len:636 episode reward: total was -33.690000. running mean: -40.934178\n",
      "ep 1040: ep_len:517 episode reward: total was -33.220000. running mean: -40.857037\n",
      "ep 1040: ep_len:107 episode reward: total was 5.750000. running mean: -40.390966\n",
      "ep 1040: ep_len:545 episode reward: total was -50.850000. running mean: -40.495557\n",
      "ep 1040: ep_len:238 episode reward: total was -105.060000. running mean: -41.141201\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.153849 episode_count: 7287. steps_count: 3173618.000000\n",
      "Time elapsed:  9118.171661615372\n",
      "ep 1041: ep_len:228 episode reward: total was -23.440000. running mean: -40.964189\n",
      "ep 1041: ep_len:193 episode reward: total was -20.100000. running mean: -40.755547\n",
      "ep 1041: ep_len:569 episode reward: total was -101.100000. running mean: -41.358992\n",
      "ep 1041: ep_len:507 episode reward: total was -90.310000. running mean: -41.848502\n",
      "ep 1041: ep_len:46 episode reward: total was 17.000000. running mean: -41.260017\n",
      "ep 1041: ep_len:500 episode reward: total was -24.010000. running mean: -41.087516\n",
      "ep 1041: ep_len:597 episode reward: total was -44.360000. running mean: -41.120241\n",
      "epsilon:0.153805 episode_count: 7294. steps_count: 3176258.000000\n",
      "Time elapsed:  9126.485262870789\n",
      "ep 1042: ep_len:211 episode reward: total was -128.810000. running mean: -41.997139\n",
      "ep 1042: ep_len:533 episode reward: total was -7.960000. running mean: -41.656768\n",
      "ep 1042: ep_len:556 episode reward: total was -87.010000. running mean: -42.110300\n",
      "ep 1042: ep_len:547 episode reward: total was -94.280000. running mean: -42.631997\n",
      "ep 1042: ep_len:3 episode reward: total was 0.000000. running mean: -42.205677\n",
      "ep 1042: ep_len:560 episode reward: total was -52.590000. running mean: -42.309520\n",
      "ep 1042: ep_len:605 episode reward: total was -112.250000. running mean: -43.008925\n",
      "epsilon:0.153760 episode_count: 7301. steps_count: 3179273.000000\n",
      "Time elapsed:  9136.394721984863\n",
      "ep 1043: ep_len:225 episode reward: total was -21.820000. running mean: -42.797036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1043: ep_len:500 episode reward: total was -39.940000. running mean: -42.768465\n",
      "ep 1043: ep_len:500 episode reward: total was -54.660000. running mean: -42.887381\n",
      "ep 1043: ep_len:500 episode reward: total was -59.540000. running mean: -43.053907\n",
      "ep 1043: ep_len:3 episode reward: total was 0.000000. running mean: -42.623368\n",
      "ep 1043: ep_len:570 episode reward: total was 0.410000. running mean: -42.193034\n",
      "ep 1043: ep_len:328 episode reward: total was -45.750000. running mean: -42.228604\n",
      "epsilon:0.153716 episode_count: 7308. steps_count: 3181899.000000\n",
      "Time elapsed:  9146.801998853683\n",
      "ep 1044: ep_len:525 episode reward: total was -67.620000. running mean: -42.482518\n",
      "ep 1044: ep_len:625 episode reward: total was -16.360000. running mean: -42.221293\n",
      "ep 1044: ep_len:555 episode reward: total was -49.490000. running mean: -42.293980\n",
      "ep 1044: ep_len:541 episode reward: total was -95.750000. running mean: -42.828540\n",
      "ep 1044: ep_len:91 episode reward: total was 3.720000. running mean: -42.363054\n",
      "ep 1044: ep_len:555 episode reward: total was -108.680000. running mean: -43.026224\n",
      "ep 1044: ep_len:562 episode reward: total was -91.420000. running mean: -43.510162\n",
      "epsilon:0.153672 episode_count: 7315. steps_count: 3185353.000000\n",
      "Time elapsed:  9157.105098247528\n",
      "ep 1045: ep_len:105 episode reward: total was -9.330000. running mean: -43.168360\n",
      "ep 1045: ep_len:537 episode reward: total was -32.960000. running mean: -43.066276\n",
      "ep 1045: ep_len:583 episode reward: total was -89.840000. running mean: -43.534014\n",
      "ep 1045: ep_len:536 episode reward: total was -50.790000. running mean: -43.606574\n",
      "ep 1045: ep_len:3 episode reward: total was -1.500000. running mean: -43.185508\n",
      "ep 1045: ep_len:500 episode reward: total was -37.210000. running mean: -43.125753\n",
      "ep 1045: ep_len:562 episode reward: total was -49.290000. running mean: -43.187395\n",
      "epsilon:0.153627 episode_count: 7322. steps_count: 3188179.000000\n",
      "Time elapsed:  9165.836482286453\n",
      "ep 1046: ep_len:624 episode reward: total was -120.160000. running mean: -43.957121\n",
      "ep 1046: ep_len:556 episode reward: total was -5.710000. running mean: -43.574650\n",
      "ep 1046: ep_len:535 episode reward: total was -14.250000. running mean: -43.281404\n",
      "ep 1046: ep_len:583 episode reward: total was -3.130000. running mean: -42.879889\n",
      "ep 1046: ep_len:3 episode reward: total was 0.000000. running mean: -42.451091\n",
      "ep 1046: ep_len:501 episode reward: total was -129.960000. running mean: -43.326180\n",
      "ep 1046: ep_len:210 episode reward: total was -69.140000. running mean: -43.584318\n",
      "epsilon:0.153583 episode_count: 7329. steps_count: 3191191.000000\n",
      "Time elapsed:  9175.61473941803\n",
      "ep 1047: ep_len:623 episode reward: total was -97.000000. running mean: -44.118475\n",
      "ep 1047: ep_len:500 episode reward: total was -29.720000. running mean: -43.974490\n",
      "ep 1047: ep_len:635 episode reward: total was -58.520000. running mean: -44.119945\n",
      "ep 1047: ep_len:386 episode reward: total was -34.850000. running mean: -44.027246\n",
      "ep 1047: ep_len:3 episode reward: total was 0.000000. running mean: -43.586973\n",
      "ep 1047: ep_len:533 episode reward: total was -52.870000. running mean: -43.679803\n",
      "ep 1047: ep_len:517 episode reward: total was -98.090000. running mean: -44.223905\n",
      "epsilon:0.153539 episode_count: 7336. steps_count: 3194388.000000\n",
      "Time elapsed:  9185.231522798538\n",
      "ep 1048: ep_len:239 episode reward: total was -14.750000. running mean: -43.929166\n",
      "ep 1048: ep_len:500 episode reward: total was 4.460000. running mean: -43.445275\n",
      "ep 1048: ep_len:428 episode reward: total was -17.510000. running mean: -43.185922\n",
      "ep 1048: ep_len:48 episode reward: total was -4.250000. running mean: -42.796563\n",
      "ep 1048: ep_len:35 episode reward: total was 5.010000. running mean: -42.318497\n",
      "ep 1048: ep_len:500 episode reward: total was -31.600000. running mean: -42.211312\n",
      "ep 1048: ep_len:504 episode reward: total was -52.370000. running mean: -42.312899\n",
      "epsilon:0.153494 episode_count: 7343. steps_count: 3196642.000000\n",
      "Time elapsed:  9192.45533156395\n",
      "ep 1049: ep_len:584 episode reward: total was -122.530000. running mean: -43.115070\n",
      "ep 1049: ep_len:500 episode reward: total was 8.910000. running mean: -42.594819\n",
      "ep 1049: ep_len:54 episode reward: total was -9.880000. running mean: -42.267671\n",
      "ep 1049: ep_len:500 episode reward: total was -55.870000. running mean: -42.403694\n",
      "ep 1049: ep_len:3 episode reward: total was -1.500000. running mean: -41.994657\n",
      "ep 1049: ep_len:564 episode reward: total was -155.960000. running mean: -43.134311\n",
      "ep 1049: ep_len:532 episode reward: total was -203.650000. running mean: -44.739468\n",
      "epsilon:0.153450 episode_count: 7350. steps_count: 3199379.000000\n",
      "Time elapsed:  9201.253188848495\n",
      "ep 1050: ep_len:500 episode reward: total was 35.760000. running mean: -43.934473\n",
      "ep 1050: ep_len:629 episode reward: total was -218.750000. running mean: -45.682628\n",
      "ep 1050: ep_len:532 episode reward: total was -66.010000. running mean: -45.885902\n",
      "ep 1050: ep_len:164 episode reward: total was -2.450000. running mean: -45.451543\n",
      "ep 1050: ep_len:127 episode reward: total was 12.320000. running mean: -44.873828\n",
      "ep 1050: ep_len:623 episode reward: total was -112.790000. running mean: -45.552989\n",
      "ep 1050: ep_len:510 episode reward: total was -72.390000. running mean: -45.821359\n",
      "epsilon:0.153406 episode_count: 7357. steps_count: 3202464.000000\n",
      "Time elapsed:  9212.017408370972\n",
      "ep 1051: ep_len:557 episode reward: total was -44.810000. running mean: -45.811246\n",
      "ep 1051: ep_len:508 episode reward: total was 13.570000. running mean: -45.217433\n",
      "ep 1051: ep_len:708 episode reward: total was -151.220000. running mean: -46.277459\n",
      "ep 1051: ep_len:508 episode reward: total was -30.040000. running mean: -46.115084\n",
      "ep 1051: ep_len:3 episode reward: total was 0.000000. running mean: -45.653934\n",
      "ep 1051: ep_len:501 episode reward: total was -91.750000. running mean: -46.114894\n",
      "ep 1051: ep_len:608 episode reward: total was -84.050000. running mean: -46.494245\n",
      "epsilon:0.153361 episode_count: 7364. steps_count: 3205857.000000\n",
      "Time elapsed:  9222.147937059402\n",
      "ep 1052: ep_len:265 episode reward: total was -8.770000. running mean: -46.117003\n",
      "ep 1052: ep_len:513 episode reward: total was 6.500000. running mean: -45.590833\n",
      "ep 1052: ep_len:602 episode reward: total was -198.830000. running mean: -47.123225\n",
      "ep 1052: ep_len:502 episode reward: total was -78.830000. running mean: -47.440292\n",
      "ep 1052: ep_len:66 episode reward: total was 3.620000. running mean: -46.929689\n",
      "ep 1052: ep_len:542 episode reward: total was -41.690000. running mean: -46.877292\n",
      "ep 1052: ep_len:307 episode reward: total was -108.010000. running mean: -47.488620\n",
      "epsilon:0.153317 episode_count: 7371. steps_count: 3208654.000000\n",
      "Time elapsed:  9232.34061717987\n",
      "ep 1053: ep_len:640 episode reward: total was -126.180000. running mean: -48.275533\n",
      "ep 1053: ep_len:552 episode reward: total was -148.820000. running mean: -49.280978\n",
      "ep 1053: ep_len:396 episode reward: total was -5.850000. running mean: -48.846668\n",
      "ep 1053: ep_len:512 episode reward: total was -63.010000. running mean: -48.988302\n",
      "ep 1053: ep_len:1 episode reward: total was -1.000000. running mean: -48.508419\n",
      "ep 1053: ep_len:500 episode reward: total was -38.880000. running mean: -48.412134\n",
      "ep 1053: ep_len:500 episode reward: total was -156.520000. running mean: -49.493213\n",
      "epsilon:0.153273 episode_count: 7378. steps_count: 3211755.000000\n",
      "Time elapsed:  9241.796133756638\n",
      "ep 1054: ep_len:500 episode reward: total was 3.650000. running mean: -48.961781\n",
      "ep 1054: ep_len:552 episode reward: total was -4.770000. running mean: -48.519863\n",
      "ep 1054: ep_len:595 episode reward: total was -87.680000. running mean: -48.911464\n",
      "ep 1054: ep_len:56 episode reward: total was -6.710000. running mean: -48.489450\n",
      "ep 1054: ep_len:3 episode reward: total was 0.000000. running mean: -48.004555\n",
      "ep 1054: ep_len:298 episode reward: total was -33.210000. running mean: -47.856610\n",
      "ep 1054: ep_len:550 episode reward: total was -55.560000. running mean: -47.933644\n",
      "epsilon:0.153228 episode_count: 7385. steps_count: 3214309.000000\n",
      "Time elapsed:  9249.844224691391\n",
      "ep 1055: ep_len:512 episode reward: total was -97.150000. running mean: -48.425807\n",
      "ep 1055: ep_len:512 episode reward: total was -13.850000. running mean: -48.080049\n",
      "ep 1055: ep_len:526 episode reward: total was -94.570000. running mean: -48.544949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1055: ep_len:500 episode reward: total was -45.610000. running mean: -48.515599\n",
      "ep 1055: ep_len:3 episode reward: total was -1.500000. running mean: -48.045443\n",
      "ep 1055: ep_len:529 episode reward: total was -86.840000. running mean: -48.433389\n",
      "ep 1055: ep_len:614 episode reward: total was -41.810000. running mean: -48.367155\n",
      "epsilon:0.153184 episode_count: 7392. steps_count: 3217505.000000\n",
      "Time elapsed:  9259.620168924332\n",
      "ep 1056: ep_len:134 episode reward: total was -13.200000. running mean: -48.015483\n",
      "ep 1056: ep_len:500 episode reward: total was -64.750000. running mean: -48.182828\n",
      "ep 1056: ep_len:579 episode reward: total was -70.660000. running mean: -48.407600\n",
      "ep 1056: ep_len:503 episode reward: total was -3.310000. running mean: -47.956624\n",
      "ep 1056: ep_len:3 episode reward: total was 0.000000. running mean: -47.477058\n",
      "ep 1056: ep_len:705 episode reward: total was -67.970000. running mean: -47.681987\n",
      "ep 1056: ep_len:561 episode reward: total was -120.810000. running mean: -48.413267\n",
      "epsilon:0.153140 episode_count: 7399. steps_count: 3220490.000000\n",
      "Time elapsed:  9268.824870586395\n",
      "ep 1057: ep_len:596 episode reward: total was -33.600000. running mean: -48.265135\n",
      "ep 1057: ep_len:347 episode reward: total was -73.550000. running mean: -48.517983\n",
      "ep 1057: ep_len:508 episode reward: total was -61.040000. running mean: -48.643204\n",
      "ep 1057: ep_len:515 episode reward: total was -50.610000. running mean: -48.662872\n",
      "ep 1057: ep_len:3 episode reward: total was 0.000000. running mean: -48.176243\n",
      "ep 1057: ep_len:500 episode reward: total was -116.310000. running mean: -48.857580\n",
      "ep 1057: ep_len:197 episode reward: total was -30.040000. running mean: -48.669405\n",
      "epsilon:0.153095 episode_count: 7406. steps_count: 3223156.000000\n",
      "Time elapsed:  9277.070021390915\n",
      "ep 1058: ep_len:535 episode reward: total was -57.410000. running mean: -48.756811\n",
      "ep 1058: ep_len:536 episode reward: total was -106.980000. running mean: -49.339042\n",
      "ep 1058: ep_len:500 episode reward: total was -47.460000. running mean: -49.320252\n",
      "ep 1058: ep_len:56 episode reward: total was -12.770000. running mean: -48.954750\n",
      "ep 1058: ep_len:3 episode reward: total was -3.000000. running mean: -48.495202\n",
      "ep 1058: ep_len:261 episode reward: total was -45.460000. running mean: -48.464850\n",
      "ep 1058: ep_len:589 episode reward: total was -52.100000. running mean: -48.501202\n",
      "epsilon:0.153051 episode_count: 7413. steps_count: 3225636.000000\n",
      "Time elapsed:  9284.96176981926\n",
      "ep 1059: ep_len:501 episode reward: total was -8.280000. running mean: -48.098990\n",
      "ep 1059: ep_len:500 episode reward: total was -79.510000. running mean: -48.413100\n",
      "ep 1059: ep_len:525 episode reward: total was -46.030000. running mean: -48.389269\n",
      "ep 1059: ep_len:523 episode reward: total was -437.610000. running mean: -52.281476\n",
      "ep 1059: ep_len:120 episode reward: total was 7.270000. running mean: -51.685961\n",
      "ep 1059: ep_len:507 episode reward: total was -28.560000. running mean: -51.454702\n",
      "ep 1059: ep_len:563 episode reward: total was -78.120000. running mean: -51.721355\n",
      "epsilon:0.153007 episode_count: 7420. steps_count: 3228875.000000\n",
      "Time elapsed:  9295.103993177414\n",
      "ep 1060: ep_len:258 episode reward: total was -10.700000. running mean: -51.311141\n",
      "ep 1060: ep_len:500 episode reward: total was 7.820000. running mean: -50.719830\n",
      "ep 1060: ep_len:635 episode reward: total was -61.400000. running mean: -50.826631\n",
      "ep 1060: ep_len:51 episode reward: total was -13.230000. running mean: -50.450665\n",
      "ep 1060: ep_len:3 episode reward: total was 0.000000. running mean: -49.946158\n",
      "ep 1060: ep_len:550 episode reward: total was -62.840000. running mean: -50.075097\n",
      "ep 1060: ep_len:509 episode reward: total was -70.470000. running mean: -50.279046\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.152962 episode_count: 7427. steps_count: 3231381.000000\n",
      "Time elapsed:  9308.931977510452\n",
      "ep 1061: ep_len:134 episode reward: total was -3.100000. running mean: -49.807255\n",
      "ep 1061: ep_len:355 episode reward: total was -21.220000. running mean: -49.521383\n",
      "ep 1061: ep_len:580 episode reward: total was -71.840000. running mean: -49.744569\n",
      "ep 1061: ep_len:560 episode reward: total was -11.290000. running mean: -49.360023\n",
      "ep 1061: ep_len:103 episode reward: total was -13.330000. running mean: -48.999723\n",
      "ep 1061: ep_len:500 episode reward: total was -24.870000. running mean: -48.758426\n",
      "ep 1061: ep_len:509 episode reward: total was -64.580000. running mean: -48.916642\n",
      "epsilon:0.152918 episode_count: 7434. steps_count: 3234122.000000\n",
      "Time elapsed:  9317.5785343647\n",
      "ep 1062: ep_len:509 episode reward: total was -11.960000. running mean: -48.547075\n",
      "ep 1062: ep_len:604 episode reward: total was -138.240000. running mean: -49.444004\n",
      "ep 1062: ep_len:563 episode reward: total was -93.470000. running mean: -49.884264\n",
      "ep 1062: ep_len:599 episode reward: total was -23.500000. running mean: -49.620422\n",
      "ep 1062: ep_len:3 episode reward: total was 0.000000. running mean: -49.124217\n",
      "ep 1062: ep_len:538 episode reward: total was -61.410000. running mean: -49.247075\n",
      "ep 1062: ep_len:613 episode reward: total was -77.660000. running mean: -49.531205\n",
      "epsilon:0.152874 episode_count: 7441. steps_count: 3237551.000000\n",
      "Time elapsed:  9326.447699785233\n",
      "ep 1063: ep_len:500 episode reward: total was -53.940000. running mean: -49.575292\n",
      "ep 1063: ep_len:500 episode reward: total was -16.990000. running mean: -49.249440\n",
      "ep 1063: ep_len:578 episode reward: total was -55.370000. running mean: -49.310645\n",
      "ep 1063: ep_len:500 episode reward: total was -19.260000. running mean: -49.010139\n",
      "ep 1063: ep_len:3 episode reward: total was 0.000000. running mean: -48.520037\n",
      "ep 1063: ep_len:537 episode reward: total was -39.380000. running mean: -48.428637\n",
      "ep 1063: ep_len:585 episode reward: total was -61.680000. running mean: -48.561151\n",
      "epsilon:0.152829 episode_count: 7448. steps_count: 3240754.000000\n",
      "Time elapsed:  9334.715199232101\n",
      "ep 1064: ep_len:516 episode reward: total was -153.800000. running mean: -49.613539\n",
      "ep 1064: ep_len:533 episode reward: total was -32.070000. running mean: -49.438104\n",
      "ep 1064: ep_len:500 episode reward: total was -44.450000. running mean: -49.388223\n",
      "ep 1064: ep_len:505 episode reward: total was -34.710000. running mean: -49.241440\n",
      "ep 1064: ep_len:3 episode reward: total was 0.000000. running mean: -48.749026\n",
      "ep 1064: ep_len:525 episode reward: total was -153.760000. running mean: -49.799136\n",
      "ep 1064: ep_len:567 episode reward: total was -36.420000. running mean: -49.665344\n",
      "epsilon:0.152785 episode_count: 7455. steps_count: 3243903.000000\n",
      "Time elapsed:  9343.892928123474\n",
      "ep 1065: ep_len:500 episode reward: total was -155.190000. running mean: -50.720591\n",
      "ep 1065: ep_len:500 episode reward: total was -24.850000. running mean: -50.461885\n",
      "ep 1065: ep_len:588 episode reward: total was -75.680000. running mean: -50.714066\n",
      "ep 1065: ep_len:500 episode reward: total was -1.500000. running mean: -50.221926\n",
      "ep 1065: ep_len:3 episode reward: total was 0.000000. running mean: -49.719706\n",
      "ep 1065: ep_len:178 episode reward: total was 14.120000. running mean: -49.081309\n",
      "ep 1065: ep_len:573 episode reward: total was -52.000000. running mean: -49.110496\n",
      "epsilon:0.152741 episode_count: 7462. steps_count: 3246745.000000\n",
      "Time elapsed:  9351.530506372452\n",
      "ep 1066: ep_len:514 episode reward: total was -20.750000. running mean: -48.826891\n",
      "ep 1066: ep_len:545 episode reward: total was -88.550000. running mean: -49.224122\n",
      "ep 1066: ep_len:501 episode reward: total was -24.200000. running mean: -48.973881\n",
      "ep 1066: ep_len:608 episode reward: total was -28.820000. running mean: -48.772342\n",
      "ep 1066: ep_len:105 episode reward: total was 16.200000. running mean: -48.122619\n",
      "ep 1066: ep_len:535 episode reward: total was -77.750000. running mean: -48.418893\n",
      "ep 1066: ep_len:585 episode reward: total was -80.180000. running mean: -48.736504\n",
      "epsilon:0.152696 episode_count: 7469. steps_count: 3250138.000000\n",
      "Time elapsed:  9360.508559465408\n",
      "ep 1067: ep_len:129 episode reward: total was 5.020000. running mean: -48.198939\n",
      "ep 1067: ep_len:514 episode reward: total was -32.420000. running mean: -48.041149\n",
      "ep 1067: ep_len:405 episode reward: total was -8.220000. running mean: -47.642938\n",
      "ep 1067: ep_len:500 episode reward: total was -54.360000. running mean: -47.710108\n",
      "ep 1067: ep_len:3 episode reward: total was 0.000000. running mean: -47.233007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1067: ep_len:509 episode reward: total was -101.110000. running mean: -47.771777\n",
      "ep 1067: ep_len:565 episode reward: total was -68.620000. running mean: -47.980259\n",
      "epsilon:0.152652 episode_count: 7476. steps_count: 3252763.000000\n",
      "Time elapsed:  9367.537402391434\n",
      "ep 1068: ep_len:500 episode reward: total was -0.260000. running mean: -47.503057\n",
      "ep 1068: ep_len:514 episode reward: total was -40.060000. running mean: -47.428626\n",
      "ep 1068: ep_len:507 episode reward: total was -75.230000. running mean: -47.706640\n",
      "ep 1068: ep_len:500 episode reward: total was -39.950000. running mean: -47.629074\n",
      "ep 1068: ep_len:50 episode reward: total was 14.500000. running mean: -47.007783\n",
      "ep 1068: ep_len:275 episode reward: total was -52.250000. running mean: -47.060205\n",
      "ep 1068: ep_len:315 episode reward: total was -42.640000. running mean: -47.016003\n",
      "epsilon:0.152608 episode_count: 7483. steps_count: 3255424.000000\n",
      "Time elapsed:  9375.45735168457\n",
      "ep 1069: ep_len:587 episode reward: total was -15.180000. running mean: -46.697643\n",
      "ep 1069: ep_len:534 episode reward: total was -67.700000. running mean: -46.907667\n",
      "ep 1069: ep_len:574 episode reward: total was -96.290000. running mean: -47.401490\n",
      "ep 1069: ep_len:532 episode reward: total was -12.630000. running mean: -47.053775\n",
      "ep 1069: ep_len:3 episode reward: total was 0.000000. running mean: -46.583237\n",
      "ep 1069: ep_len:538 episode reward: total was -96.820000. running mean: -47.085605\n",
      "ep 1069: ep_len:500 episode reward: total was -36.670000. running mean: -46.981449\n",
      "epsilon:0.152563 episode_count: 7490. steps_count: 3258692.000000\n",
      "Time elapsed:  9383.980639696121\n",
      "ep 1070: ep_len:500 episode reward: total was -67.730000. running mean: -47.188934\n",
      "ep 1070: ep_len:565 episode reward: total was -151.820000. running mean: -48.235245\n",
      "ep 1070: ep_len:616 episode reward: total was -73.420000. running mean: -48.487093\n",
      "ep 1070: ep_len:513 episode reward: total was -59.490000. running mean: -48.597122\n",
      "ep 1070: ep_len:3 episode reward: total was 0.000000. running mean: -48.111150\n",
      "ep 1070: ep_len:501 episode reward: total was -75.480000. running mean: -48.384839\n",
      "ep 1070: ep_len:506 episode reward: total was -89.600000. running mean: -48.796991\n",
      "epsilon:0.152519 episode_count: 7497. steps_count: 3261896.000000\n",
      "Time elapsed:  9393.513617753983\n",
      "ep 1071: ep_len:620 episode reward: total was -80.220000. running mean: -49.111221\n",
      "ep 1071: ep_len:568 episode reward: total was -39.900000. running mean: -49.019108\n",
      "ep 1071: ep_len:775 episode reward: total was -158.670000. running mean: -50.115617\n",
      "ep 1071: ep_len:525 episode reward: total was -0.730000. running mean: -49.621761\n",
      "ep 1071: ep_len:3 episode reward: total was 0.000000. running mean: -49.125544\n",
      "ep 1071: ep_len:500 episode reward: total was -20.010000. running mean: -48.834388\n",
      "ep 1071: ep_len:563 episode reward: total was -47.020000. running mean: -48.816244\n",
      "epsilon:0.152475 episode_count: 7504. steps_count: 3265450.000000\n",
      "Time elapsed:  9402.781080961227\n",
      "ep 1072: ep_len:500 episode reward: total was -14.890000. running mean: -48.476982\n",
      "ep 1072: ep_len:340 episode reward: total was -51.800000. running mean: -48.510212\n",
      "ep 1072: ep_len:560 episode reward: total was -30.890000. running mean: -48.334010\n",
      "ep 1072: ep_len:500 episode reward: total was -87.980000. running mean: -48.730470\n",
      "ep 1072: ep_len:3 episode reward: total was -1.500000. running mean: -48.258165\n",
      "ep 1072: ep_len:501 episode reward: total was -37.700000. running mean: -48.152583\n",
      "ep 1072: ep_len:525 episode reward: total was -73.140000. running mean: -48.402458\n",
      "epsilon:0.152430 episode_count: 7511. steps_count: 3268379.000000\n",
      "Time elapsed:  9410.547747373581\n",
      "ep 1073: ep_len:134 episode reward: total was -3.530000. running mean: -47.953733\n",
      "ep 1073: ep_len:586 episode reward: total was -61.990000. running mean: -48.094096\n",
      "ep 1073: ep_len:500 episode reward: total was -60.850000. running mean: -48.221655\n",
      "ep 1073: ep_len:500 episode reward: total was -52.770000. running mean: -48.267138\n",
      "ep 1073: ep_len:89 episode reward: total was 19.190000. running mean: -47.592567\n",
      "ep 1073: ep_len:647 episode reward: total was -57.450000. running mean: -47.691141\n",
      "ep 1073: ep_len:559 episode reward: total was -76.660000. running mean: -47.980830\n",
      "epsilon:0.152386 episode_count: 7518. steps_count: 3271394.000000\n",
      "Time elapsed:  9418.646081924438\n",
      "ep 1074: ep_len:621 episode reward: total was -76.910000. running mean: -48.270121\n",
      "ep 1074: ep_len:605 episode reward: total was -116.860000. running mean: -48.956020\n",
      "ep 1074: ep_len:572 episode reward: total was -28.800000. running mean: -48.754460\n",
      "ep 1074: ep_len:591 episode reward: total was -25.080000. running mean: -48.517715\n",
      "ep 1074: ep_len:3 episode reward: total was 0.000000. running mean: -48.032538\n",
      "ep 1074: ep_len:506 episode reward: total was -74.830000. running mean: -48.300513\n",
      "ep 1074: ep_len:522 episode reward: total was -113.220000. running mean: -48.949708\n",
      "epsilon:0.152342 episode_count: 7525. steps_count: 3274814.000000\n",
      "Time elapsed:  9428.749726057053\n",
      "ep 1075: ep_len:674 episode reward: total was -76.550000. running mean: -49.225711\n",
      "ep 1075: ep_len:598 episode reward: total was -104.880000. running mean: -49.782254\n",
      "ep 1075: ep_len:594 episode reward: total was -42.100000. running mean: -49.705431\n",
      "ep 1075: ep_len:510 episode reward: total was -47.130000. running mean: -49.679677\n",
      "ep 1075: ep_len:129 episode reward: total was -30.170000. running mean: -49.484580\n",
      "ep 1075: ep_len:501 episode reward: total was -60.440000. running mean: -49.594134\n",
      "ep 1075: ep_len:529 episode reward: total was -105.430000. running mean: -50.152493\n",
      "epsilon:0.152297 episode_count: 7532. steps_count: 3278349.000000\n",
      "Time elapsed:  9437.840916156769\n",
      "ep 1076: ep_len:557 episode reward: total was -58.980000. running mean: -50.240768\n",
      "ep 1076: ep_len:579 episode reward: total was -46.010000. running mean: -50.198460\n",
      "ep 1076: ep_len:388 episode reward: total was -22.470000. running mean: -49.921176\n",
      "ep 1076: ep_len:532 episode reward: total was -74.520000. running mean: -50.167164\n",
      "ep 1076: ep_len:50 episode reward: total was 13.000000. running mean: -49.535492\n",
      "ep 1076: ep_len:500 episode reward: total was -18.260000. running mean: -49.222737\n",
      "ep 1076: ep_len:563 episode reward: total was -48.840000. running mean: -49.218910\n",
      "epsilon:0.152253 episode_count: 7539. steps_count: 3281518.000000\n",
      "Time elapsed:  9446.936720371246\n",
      "ep 1077: ep_len:568 episode reward: total was -5.640000. running mean: -48.783121\n",
      "ep 1077: ep_len:547 episode reward: total was -32.790000. running mean: -48.623190\n",
      "ep 1077: ep_len:393 episode reward: total was -15.800000. running mean: -48.294958\n",
      "ep 1077: ep_len:500 episode reward: total was -17.610000. running mean: -47.988108\n",
      "ep 1077: ep_len:3 episode reward: total was -3.000000. running mean: -47.538227\n",
      "ep 1077: ep_len:500 episode reward: total was -56.790000. running mean: -47.630745\n",
      "ep 1077: ep_len:517 episode reward: total was -40.200000. running mean: -47.556437\n",
      "epsilon:0.152209 episode_count: 7546. steps_count: 3284546.000000\n",
      "Time elapsed:  9454.899548530579\n",
      "ep 1078: ep_len:234 episode reward: total was -32.410000. running mean: -47.404973\n",
      "ep 1078: ep_len:369 episode reward: total was -143.070000. running mean: -48.361623\n",
      "ep 1078: ep_len:633 episode reward: total was -71.550000. running mean: -48.593507\n",
      "ep 1078: ep_len:565 episode reward: total was -113.600000. running mean: -49.243572\n",
      "ep 1078: ep_len:89 episode reward: total was -3.280000. running mean: -48.783936\n",
      "ep 1078: ep_len:334 episode reward: total was -31.570000. running mean: -48.611797\n",
      "ep 1078: ep_len:517 episode reward: total was -95.490000. running mean: -49.080579\n",
      "epsilon:0.152164 episode_count: 7553. steps_count: 3287287.000000\n",
      "Time elapsed:  9462.467689752579\n",
      "ep 1079: ep_len:565 episode reward: total was -21.720000. running mean: -48.806973\n",
      "ep 1079: ep_len:501 episode reward: total was 24.930000. running mean: -48.069603\n",
      "ep 1079: ep_len:635 episode reward: total was -73.560000. running mean: -48.324507\n",
      "ep 1079: ep_len:617 episode reward: total was 3.930000. running mean: -47.801962\n",
      "ep 1079: ep_len:54 episode reward: total was -31.000000. running mean: -47.633943\n",
      "ep 1079: ep_len:607 episode reward: total was -73.230000. running mean: -47.889903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1079: ep_len:517 episode reward: total was -48.820000. running mean: -47.899204\n",
      "epsilon:0.152120 episode_count: 7560. steps_count: 3290783.000000\n",
      "Time elapsed:  9471.632677555084\n",
      "ep 1080: ep_len:590 episode reward: total was -63.000000. running mean: -48.050212\n",
      "ep 1080: ep_len:649 episode reward: total was -229.630000. running mean: -49.866010\n",
      "ep 1080: ep_len:500 episode reward: total was -39.720000. running mean: -49.764550\n",
      "ep 1080: ep_len:507 episode reward: total was -83.520000. running mean: -50.102104\n",
      "ep 1080: ep_len:3 episode reward: total was 0.000000. running mean: -49.601083\n",
      "ep 1080: ep_len:517 episode reward: total was -103.310000. running mean: -50.138173\n",
      "ep 1080: ep_len:600 episode reward: total was -84.110000. running mean: -50.477891\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.152076 episode_count: 7567. steps_count: 3294149.000000\n",
      "Time elapsed:  9485.089941501617\n",
      "ep 1081: ep_len:560 episode reward: total was -179.220000. running mean: -51.765312\n",
      "ep 1081: ep_len:532 episode reward: total was 11.100000. running mean: -51.136659\n",
      "ep 1081: ep_len:509 episode reward: total was -42.410000. running mean: -51.049392\n",
      "ep 1081: ep_len:504 episode reward: total was -85.240000. running mean: -51.391298\n",
      "ep 1081: ep_len:97 episode reward: total was -40.790000. running mean: -51.285285\n",
      "ep 1081: ep_len:500 episode reward: total was -42.310000. running mean: -51.195532\n",
      "ep 1081: ep_len:503 episode reward: total was -129.800000. running mean: -51.981577\n",
      "epsilon:0.152031 episode_count: 7574. steps_count: 3297354.000000\n",
      "Time elapsed:  9494.222160339355\n",
      "ep 1082: ep_len:691 episode reward: total was -122.940000. running mean: -52.691161\n",
      "ep 1082: ep_len:589 episode reward: total was -72.320000. running mean: -52.887450\n",
      "ep 1082: ep_len:557 episode reward: total was -51.100000. running mean: -52.869575\n",
      "ep 1082: ep_len:527 episode reward: total was -33.760000. running mean: -52.678479\n",
      "ep 1082: ep_len:3 episode reward: total was 0.000000. running mean: -52.151695\n",
      "ep 1082: ep_len:526 episode reward: total was -88.030000. running mean: -52.510478\n",
      "ep 1082: ep_len:308 episode reward: total was -18.270000. running mean: -52.168073\n",
      "epsilon:0.151987 episode_count: 7581. steps_count: 3300555.000000\n",
      "Time elapsed:  9502.748946428299\n",
      "ep 1083: ep_len:534 episode reward: total was 2.310000. running mean: -51.623292\n",
      "ep 1083: ep_len:373 episode reward: total was -36.100000. running mean: -51.468059\n",
      "ep 1083: ep_len:658 episode reward: total was -54.890000. running mean: -51.502279\n",
      "ep 1083: ep_len:500 episode reward: total was -65.730000. running mean: -51.644556\n",
      "ep 1083: ep_len:3 episode reward: total was 0.000000. running mean: -51.128110\n",
      "ep 1083: ep_len:505 episode reward: total was -52.000000. running mean: -51.136829\n",
      "ep 1083: ep_len:515 episode reward: total was -77.850000. running mean: -51.403961\n",
      "epsilon:0.151943 episode_count: 7588. steps_count: 3303643.000000\n",
      "Time elapsed:  9511.29853796959\n",
      "ep 1084: ep_len:501 episode reward: total was -33.240000. running mean: -51.222321\n",
      "ep 1084: ep_len:515 episode reward: total was -40.550000. running mean: -51.115598\n",
      "ep 1084: ep_len:547 episode reward: total was -70.190000. running mean: -51.306342\n",
      "ep 1084: ep_len:500 episode reward: total was -31.600000. running mean: -51.109279\n",
      "ep 1084: ep_len:107 episode reward: total was 15.240000. running mean: -50.445786\n",
      "ep 1084: ep_len:598 episode reward: total was -129.660000. running mean: -51.237928\n",
      "ep 1084: ep_len:500 episode reward: total was -84.240000. running mean: -51.567949\n",
      "epsilon:0.151898 episode_count: 7595. steps_count: 3306911.000000\n",
      "Time elapsed:  9519.805513381958\n",
      "ep 1085: ep_len:507 episode reward: total was -53.030000. running mean: -51.582569\n",
      "ep 1085: ep_len:506 episode reward: total was -32.210000. running mean: -51.388844\n",
      "ep 1085: ep_len:631 episode reward: total was -51.400000. running mean: -51.388955\n",
      "ep 1085: ep_len:500 episode reward: total was 14.570000. running mean: -50.729366\n",
      "ep 1085: ep_len:81 episode reward: total was 7.690000. running mean: -50.145172\n",
      "ep 1085: ep_len:500 episode reward: total was -75.660000. running mean: -50.400320\n",
      "ep 1085: ep_len:537 episode reward: total was -180.450000. running mean: -51.700817\n",
      "epsilon:0.151854 episode_count: 7602. steps_count: 3310173.000000\n",
      "Time elapsed:  9530.168749809265\n",
      "ep 1086: ep_len:241 episode reward: total was -26.740000. running mean: -51.451209\n",
      "ep 1086: ep_len:581 episode reward: total was -38.270000. running mean: -51.319397\n",
      "ep 1086: ep_len:500 episode reward: total was -57.320000. running mean: -51.379403\n",
      "ep 1086: ep_len:500 episode reward: total was -39.660000. running mean: -51.262209\n",
      "ep 1086: ep_len:3 episode reward: total was 0.000000. running mean: -50.749587\n",
      "ep 1086: ep_len:675 episode reward: total was -74.800000. running mean: -50.990091\n",
      "ep 1086: ep_len:515 episode reward: total was -84.510000. running mean: -51.325290\n",
      "epsilon:0.151810 episode_count: 7609. steps_count: 3313188.000000\n",
      "Time elapsed:  9538.193758487701\n",
      "ep 1087: ep_len:516 episode reward: total was -66.360000. running mean: -51.475637\n",
      "ep 1087: ep_len:500 episode reward: total was -45.470000. running mean: -51.415581\n",
      "ep 1087: ep_len:594 episode reward: total was -106.150000. running mean: -51.962925\n",
      "ep 1087: ep_len:501 episode reward: total was -26.840000. running mean: -51.711696\n",
      "ep 1087: ep_len:3 episode reward: total was 0.000000. running mean: -51.194579\n",
      "ep 1087: ep_len:538 episode reward: total was -72.850000. running mean: -51.411133\n",
      "ep 1087: ep_len:523 episode reward: total was -43.560000. running mean: -51.332622\n",
      "epsilon:0.151765 episode_count: 7616. steps_count: 3316363.000000\n",
      "Time elapsed:  9545.712454080582\n",
      "ep 1088: ep_len:698 episode reward: total was -93.420000. running mean: -51.753495\n",
      "ep 1088: ep_len:605 episode reward: total was -44.280000. running mean: -51.678760\n",
      "ep 1088: ep_len:561 episode reward: total was -52.850000. running mean: -51.690473\n",
      "ep 1088: ep_len:114 episode reward: total was -2.570000. running mean: -51.199268\n",
      "ep 1088: ep_len:3 episode reward: total was 0.000000. running mean: -50.687275\n",
      "ep 1088: ep_len:684 episode reward: total was -30.140000. running mean: -50.481803\n",
      "ep 1088: ep_len:500 episode reward: total was -35.710000. running mean: -50.334085\n",
      "epsilon:0.151721 episode_count: 7623. steps_count: 3319528.000000\n",
      "Time elapsed:  9556.199949741364\n",
      "ep 1089: ep_len:500 episode reward: total was -1.670000. running mean: -49.847444\n",
      "ep 1089: ep_len:500 episode reward: total was -82.540000. running mean: -50.174369\n",
      "ep 1089: ep_len:651 episode reward: total was -84.840000. running mean: -50.521026\n",
      "ep 1089: ep_len:170 episode reward: total was 7.140000. running mean: -49.944415\n",
      "ep 1089: ep_len:3 episode reward: total was 0.000000. running mean: -49.444971\n",
      "ep 1089: ep_len:591 episode reward: total was -57.280000. running mean: -49.523321\n",
      "ep 1089: ep_len:532 episode reward: total was -75.920000. running mean: -49.787288\n",
      "epsilon:0.151677 episode_count: 7630. steps_count: 3322475.000000\n",
      "Time elapsed:  9564.065626859665\n",
      "ep 1090: ep_len:500 episode reward: total was -55.590000. running mean: -49.845315\n",
      "ep 1090: ep_len:500 episode reward: total was -34.030000. running mean: -49.687162\n",
      "ep 1090: ep_len:671 episode reward: total was -73.310000. running mean: -49.923391\n",
      "ep 1090: ep_len:524 episode reward: total was 18.160000. running mean: -49.242557\n",
      "ep 1090: ep_len:3 episode reward: total was -1.500000. running mean: -48.765131\n",
      "ep 1090: ep_len:566 episode reward: total was -86.110000. running mean: -49.138580\n",
      "ep 1090: ep_len:585 episode reward: total was -67.970000. running mean: -49.326894\n",
      "epsilon:0.151632 episode_count: 7637. steps_count: 3325824.000000\n",
      "Time elapsed:  9574.146661996841\n",
      "ep 1091: ep_len:647 episode reward: total was -165.960000. running mean: -50.493225\n",
      "ep 1091: ep_len:548 episode reward: total was 0.910000. running mean: -49.979193\n",
      "ep 1091: ep_len:569 episode reward: total was -78.610000. running mean: -50.265501\n",
      "ep 1091: ep_len:502 episode reward: total was -2.050000. running mean: -49.783346\n",
      "ep 1091: ep_len:3 episode reward: total was -1.500000. running mean: -49.300512\n",
      "ep 1091: ep_len:636 episode reward: total was -114.480000. running mean: -49.952307\n",
      "ep 1091: ep_len:284 episode reward: total was -14.070000. running mean: -49.593484\n",
      "epsilon:0.151588 episode_count: 7644. steps_count: 3329013.000000\n",
      "Time elapsed:  9582.450377225876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1092: ep_len:229 episode reward: total was -8.800000. running mean: -49.185549\n",
      "ep 1092: ep_len:588 episode reward: total was -51.450000. running mean: -49.208194\n",
      "ep 1092: ep_len:540 episode reward: total was -71.710000. running mean: -49.433212\n",
      "ep 1092: ep_len:500 episode reward: total was -11.170000. running mean: -49.050580\n",
      "ep 1092: ep_len:110 episode reward: total was 25.250000. running mean: -48.307574\n",
      "ep 1092: ep_len:609 episode reward: total was -26.670000. running mean: -48.091198\n",
      "ep 1092: ep_len:615 episode reward: total was -40.020000. running mean: -48.010486\n",
      "epsilon:0.151544 episode_count: 7651. steps_count: 3332204.000000\n",
      "Time elapsed:  9590.951993703842\n",
      "ep 1093: ep_len:561 episode reward: total was -22.930000. running mean: -47.759681\n",
      "ep 1093: ep_len:621 episode reward: total was -72.090000. running mean: -48.002985\n",
      "ep 1093: ep_len:628 episode reward: total was -61.970000. running mean: -48.142655\n",
      "ep 1093: ep_len:500 episode reward: total was -123.590000. running mean: -48.897128\n",
      "ep 1093: ep_len:3 episode reward: total was -1.500000. running mean: -48.423157\n",
      "ep 1093: ep_len:168 episode reward: total was 5.020000. running mean: -47.888725\n",
      "ep 1093: ep_len:589 episode reward: total was -52.800000. running mean: -47.937838\n",
      "epsilon:0.151499 episode_count: 7658. steps_count: 3335274.000000\n",
      "Time elapsed:  9600.206514596939\n",
      "ep 1094: ep_len:504 episode reward: total was -67.670000. running mean: -48.135160\n",
      "ep 1094: ep_len:598 episode reward: total was -82.650000. running mean: -48.480308\n",
      "ep 1094: ep_len:79 episode reward: total was -3.230000. running mean: -48.027805\n",
      "ep 1094: ep_len:501 episode reward: total was -95.150000. running mean: -48.499027\n",
      "ep 1094: ep_len:53 episode reward: total was 19.000000. running mean: -47.824037\n",
      "ep 1094: ep_len:528 episode reward: total was -42.910000. running mean: -47.774896\n",
      "ep 1094: ep_len:528 episode reward: total was -65.460000. running mean: -47.951747\n",
      "epsilon:0.151455 episode_count: 7665. steps_count: 3338065.000000\n",
      "Time elapsed:  9607.906677484512\n",
      "ep 1095: ep_len:123 episode reward: total was -13.710000. running mean: -47.609330\n",
      "ep 1095: ep_len:516 episode reward: total was -46.730000. running mean: -47.600537\n",
      "ep 1095: ep_len:616 episode reward: total was -88.110000. running mean: -48.005631\n",
      "ep 1095: ep_len:544 episode reward: total was -168.510000. running mean: -49.210675\n",
      "ep 1095: ep_len:127 episode reward: total was -1.270000. running mean: -48.731268\n",
      "ep 1095: ep_len:523 episode reward: total was -64.380000. running mean: -48.887756\n",
      "ep 1095: ep_len:508 episode reward: total was -140.160000. running mean: -49.800478\n",
      "epsilon:0.151411 episode_count: 7672. steps_count: 3341022.000000\n",
      "Time elapsed:  9615.64046573639\n",
      "ep 1096: ep_len:573 episode reward: total was -2.850000. running mean: -49.330973\n",
      "ep 1096: ep_len:600 episode reward: total was -44.600000. running mean: -49.283663\n",
      "ep 1096: ep_len:652 episode reward: total was -125.560000. running mean: -50.046427\n",
      "ep 1096: ep_len:600 episode reward: total was 36.500000. running mean: -49.180963\n",
      "ep 1096: ep_len:3 episode reward: total was 0.000000. running mean: -48.689153\n",
      "ep 1096: ep_len:541 episode reward: total was -100.040000. running mean: -49.202661\n",
      "ep 1096: ep_len:552 episode reward: total was -62.820000. running mean: -49.338835\n",
      "epsilon:0.151366 episode_count: 7679. steps_count: 3344543.000000\n",
      "Time elapsed:  9624.688513755798\n",
      "ep 1097: ep_len:591 episode reward: total was -29.840000. running mean: -49.143846\n",
      "ep 1097: ep_len:530 episode reward: total was -1.380000. running mean: -48.666208\n",
      "ep 1097: ep_len:500 episode reward: total was -35.520000. running mean: -48.534746\n",
      "ep 1097: ep_len:501 episode reward: total was -47.650000. running mean: -48.525898\n",
      "ep 1097: ep_len:3 episode reward: total was 0.000000. running mean: -48.040639\n",
      "ep 1097: ep_len:661 episode reward: total was -71.240000. running mean: -48.272633\n",
      "ep 1097: ep_len:533 episode reward: total was -89.620000. running mean: -48.686107\n",
      "epsilon:0.151322 episode_count: 7686. steps_count: 3347862.000000\n",
      "Time elapsed:  9633.440421104431\n",
      "ep 1098: ep_len:265 episode reward: total was -15.230000. running mean: -48.351546\n",
      "ep 1098: ep_len:537 episode reward: total was -157.400000. running mean: -49.442030\n",
      "ep 1098: ep_len:500 episode reward: total was -56.980000. running mean: -49.517410\n",
      "ep 1098: ep_len:506 episode reward: total was -40.230000. running mean: -49.424536\n",
      "ep 1098: ep_len:3 episode reward: total was -1.500000. running mean: -48.945290\n",
      "ep 1098: ep_len:518 episode reward: total was -71.600000. running mean: -49.171838\n",
      "ep 1098: ep_len:519 episode reward: total was -97.430000. running mean: -49.654419\n",
      "epsilon:0.151278 episode_count: 7693. steps_count: 3350710.000000\n",
      "Time elapsed:  9641.181602954865\n",
      "ep 1099: ep_len:500 episode reward: total was 3.850000. running mean: -49.119375\n",
      "ep 1099: ep_len:500 episode reward: total was -44.030000. running mean: -49.068481\n",
      "ep 1099: ep_len:500 episode reward: total was -27.500000. running mean: -48.852796\n",
      "ep 1099: ep_len:56 episode reward: total was -7.200000. running mean: -48.436268\n",
      "ep 1099: ep_len:3 episode reward: total was 0.000000. running mean: -47.951906\n",
      "ep 1099: ep_len:581 episode reward: total was -49.350000. running mean: -47.965887\n",
      "ep 1099: ep_len:513 episode reward: total was -31.030000. running mean: -47.796528\n",
      "epsilon:0.151233 episode_count: 7700. steps_count: 3353363.000000\n",
      "Time elapsed:  9649.211734771729\n",
      "ep 1100: ep_len:527 episode reward: total was -119.170000. running mean: -48.510263\n",
      "ep 1100: ep_len:576 episode reward: total was -11.960000. running mean: -48.144760\n",
      "ep 1100: ep_len:500 episode reward: total was -84.710000. running mean: -48.510412\n",
      "ep 1100: ep_len:529 episode reward: total was -56.990000. running mean: -48.595208\n",
      "ep 1100: ep_len:3 episode reward: total was 0.000000. running mean: -48.109256\n",
      "ep 1100: ep_len:500 episode reward: total was -34.120000. running mean: -47.969364\n",
      "ep 1100: ep_len:500 episode reward: total was -57.970000. running mean: -48.069370\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.151189 episode_count: 7707. steps_count: 3356498.000000\n",
      "Time elapsed:  9662.401577949524\n",
      "ep 1101: ep_len:132 episode reward: total was -2.110000. running mean: -47.609776\n",
      "ep 1101: ep_len:500 episode reward: total was -1.040000. running mean: -47.144078\n",
      "ep 1101: ep_len:362 episode reward: total was 14.760000. running mean: -46.525038\n",
      "ep 1101: ep_len:560 episode reward: total was -43.430000. running mean: -46.494087\n",
      "ep 1101: ep_len:3 episode reward: total was -1.500000. running mean: -46.044146\n",
      "ep 1101: ep_len:313 episode reward: total was -16.570000. running mean: -45.749405\n",
      "ep 1101: ep_len:564 episode reward: total was -58.840000. running mean: -45.880311\n",
      "epsilon:0.151145 episode_count: 7714. steps_count: 3358932.000000\n",
      "Time elapsed:  9669.001543283463\n",
      "ep 1102: ep_len:641 episode reward: total was -35.080000. running mean: -45.772308\n",
      "ep 1102: ep_len:692 episode reward: total was -216.540000. running mean: -47.479985\n",
      "ep 1102: ep_len:570 episode reward: total was -55.830000. running mean: -47.563485\n",
      "ep 1102: ep_len:51 episode reward: total was -5.720000. running mean: -47.145050\n",
      "ep 1102: ep_len:48 episode reward: total was 10.500000. running mean: -46.568600\n",
      "ep 1102: ep_len:500 episode reward: total was -85.410000. running mean: -46.957014\n",
      "ep 1102: ep_len:548 episode reward: total was -90.180000. running mean: -47.389243\n",
      "epsilon:0.151100 episode_count: 7721. steps_count: 3361982.000000\n",
      "Time elapsed:  9676.875853061676\n",
      "ep 1103: ep_len:234 episode reward: total was -3.880000. running mean: -46.954151\n",
      "ep 1103: ep_len:500 episode reward: total was -42.800000. running mean: -46.912609\n",
      "ep 1103: ep_len:603 episode reward: total was -48.170000. running mean: -46.925183\n",
      "ep 1103: ep_len:500 episode reward: total was -53.670000. running mean: -46.992632\n",
      "ep 1103: ep_len:69 episode reward: total was 8.180000. running mean: -46.440905\n",
      "ep 1103: ep_len:589 episode reward: total was -54.310000. running mean: -46.519596\n",
      "ep 1103: ep_len:593 episode reward: total was -50.120000. running mean: -46.555600\n",
      "epsilon:0.151056 episode_count: 7728. steps_count: 3365070.000000\n",
      "Time elapsed:  9685.221188783646\n",
      "ep 1104: ep_len:500 episode reward: total was 3.720000. running mean: -46.052844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1104: ep_len:549 episode reward: total was -94.780000. running mean: -46.540116\n",
      "ep 1104: ep_len:520 episode reward: total was -78.870000. running mean: -46.863415\n",
      "ep 1104: ep_len:576 episode reward: total was -28.030000. running mean: -46.675080\n",
      "ep 1104: ep_len:3 episode reward: total was 0.000000. running mean: -46.208330\n",
      "ep 1104: ep_len:695 episode reward: total was -40.560000. running mean: -46.151846\n",
      "ep 1104: ep_len:511 episode reward: total was -63.170000. running mean: -46.322028\n",
      "epsilon:0.151012 episode_count: 7735. steps_count: 3368424.000000\n",
      "Time elapsed:  9694.02399301529\n",
      "ep 1105: ep_len:500 episode reward: total was 5.710000. running mean: -45.801708\n",
      "ep 1105: ep_len:603 episode reward: total was -69.580000. running mean: -46.039491\n",
      "ep 1105: ep_len:461 episode reward: total was -4.800000. running mean: -45.627096\n",
      "ep 1105: ep_len:500 episode reward: total was -21.250000. running mean: -45.383325\n",
      "ep 1105: ep_len:100 episode reward: total was 15.720000. running mean: -44.772291\n",
      "ep 1105: ep_len:637 episode reward: total was -115.170000. running mean: -45.476269\n",
      "ep 1105: ep_len:537 episode reward: total was -51.130000. running mean: -45.532806\n",
      "epsilon:0.150967 episode_count: 7742. steps_count: 3371762.000000\n",
      "Time elapsed:  9701.060148954391\n",
      "ep 1106: ep_len:586 episode reward: total was -19.090000. running mean: -45.268378\n",
      "ep 1106: ep_len:500 episode reward: total was -15.790000. running mean: -44.973594\n",
      "ep 1106: ep_len:383 episode reward: total was -24.790000. running mean: -44.771758\n",
      "ep 1106: ep_len:170 episode reward: total was -14.050000. running mean: -44.464540\n",
      "ep 1106: ep_len:3 episode reward: total was 0.000000. running mean: -44.019895\n",
      "ep 1106: ep_len:528 episode reward: total was -49.080000. running mean: -44.070496\n",
      "ep 1106: ep_len:586 episode reward: total was -70.940000. running mean: -44.339191\n",
      "epsilon:0.150923 episode_count: 7749. steps_count: 3374518.000000\n",
      "Time elapsed:  9708.743732213974\n",
      "ep 1107: ep_len:619 episode reward: total was -126.570000. running mean: -45.161499\n",
      "ep 1107: ep_len:548 episode reward: total was -59.340000. running mean: -45.303284\n",
      "ep 1107: ep_len:618 episode reward: total was -61.020000. running mean: -45.460451\n",
      "ep 1107: ep_len:582 episode reward: total was -19.730000. running mean: -45.203147\n",
      "ep 1107: ep_len:37 episode reward: total was 11.000000. running mean: -44.641115\n",
      "ep 1107: ep_len:518 episode reward: total was -116.340000. running mean: -45.358104\n",
      "ep 1107: ep_len:579 episode reward: total was -76.400000. running mean: -45.668523\n",
      "epsilon:0.150879 episode_count: 7756. steps_count: 3378019.000000\n",
      "Time elapsed:  9717.840827941895\n",
      "ep 1108: ep_len:614 episode reward: total was -37.460000. running mean: -45.586438\n",
      "ep 1108: ep_len:500 episode reward: total was 13.960000. running mean: -44.990974\n",
      "ep 1108: ep_len:533 episode reward: total was -35.500000. running mean: -44.896064\n",
      "ep 1108: ep_len:582 episode reward: total was 8.530000. running mean: -44.361803\n",
      "ep 1108: ep_len:3 episode reward: total was 0.000000. running mean: -43.918185\n",
      "ep 1108: ep_len:500 episode reward: total was -56.320000. running mean: -44.042203\n",
      "ep 1108: ep_len:598 episode reward: total was -38.750000. running mean: -43.989281\n",
      "epsilon:0.150834 episode_count: 7763. steps_count: 3381349.000000\n",
      "Time elapsed:  9728.508036375046\n",
      "ep 1109: ep_len:622 episode reward: total was -171.790000. running mean: -45.267289\n",
      "ep 1109: ep_len:189 episode reward: total was -36.420000. running mean: -45.178816\n",
      "ep 1109: ep_len:548 episode reward: total was -58.410000. running mean: -45.311127\n",
      "ep 1109: ep_len:502 episode reward: total was -378.160000. running mean: -48.639616\n",
      "ep 1109: ep_len:102 episode reward: total was 21.160000. running mean: -47.941620\n",
      "ep 1109: ep_len:539 episode reward: total was -61.370000. running mean: -48.075904\n",
      "ep 1109: ep_len:589 episode reward: total was -55.850000. running mean: -48.153645\n",
      "epsilon:0.150790 episode_count: 7770. steps_count: 3384440.000000\n",
      "Time elapsed:  9736.56820845604\n",
      "ep 1110: ep_len:600 episode reward: total was -224.930000. running mean: -49.921408\n",
      "ep 1110: ep_len:573 episode reward: total was -19.840000. running mean: -49.620594\n",
      "ep 1110: ep_len:426 episode reward: total was -26.460000. running mean: -49.388988\n",
      "ep 1110: ep_len:405 episode reward: total was -38.080000. running mean: -49.275898\n",
      "ep 1110: ep_len:3 episode reward: total was 0.000000. running mean: -48.783139\n",
      "ep 1110: ep_len:503 episode reward: total was -121.800000. running mean: -49.513308\n",
      "ep 1110: ep_len:560 episode reward: total was -45.980000. running mean: -49.477975\n",
      "epsilon:0.150746 episode_count: 7777. steps_count: 3387510.000000\n",
      "Time elapsed:  9744.755362272263\n",
      "ep 1111: ep_len:501 episode reward: total was -6.070000. running mean: -49.043895\n",
      "ep 1111: ep_len:506 episode reward: total was -68.260000. running mean: -49.236056\n",
      "ep 1111: ep_len:350 episode reward: total was -20.140000. running mean: -48.945096\n",
      "ep 1111: ep_len:589 episode reward: total was -27.280000. running mean: -48.728445\n",
      "ep 1111: ep_len:3 episode reward: total was 0.000000. running mean: -48.241160\n",
      "ep 1111: ep_len:531 episode reward: total was -92.600000. running mean: -48.684749\n",
      "ep 1111: ep_len:589 episode reward: total was -47.440000. running mean: -48.672301\n",
      "epsilon:0.150701 episode_count: 7784. steps_count: 3390579.000000\n",
      "Time elapsed:  9753.05317568779\n",
      "ep 1112: ep_len:500 episode reward: total was -87.340000. running mean: -49.058978\n",
      "ep 1112: ep_len:501 episode reward: total was -53.820000. running mean: -49.106588\n",
      "ep 1112: ep_len:578 episode reward: total was -98.720000. running mean: -49.602723\n",
      "ep 1112: ep_len:386 episode reward: total was -77.220000. running mean: -49.878895\n",
      "ep 1112: ep_len:3 episode reward: total was 0.000000. running mean: -49.380106\n",
      "ep 1112: ep_len:521 episode reward: total was -17.550000. running mean: -49.061805\n",
      "ep 1112: ep_len:588 episode reward: total was -54.730000. running mean: -49.118487\n",
      "epsilon:0.150657 episode_count: 7791. steps_count: 3393656.000000\n",
      "Time elapsed:  9760.66327214241\n",
      "ep 1113: ep_len:500 episode reward: total was -26.400000. running mean: -48.891302\n",
      "ep 1113: ep_len:544 episode reward: total was 45.160000. running mean: -47.950789\n",
      "ep 1113: ep_len:359 episode reward: total was 21.160000. running mean: -47.259681\n",
      "ep 1113: ep_len:141 episode reward: total was 6.900000. running mean: -46.718085\n",
      "ep 1113: ep_len:93 episode reward: total was 11.240000. running mean: -46.138504\n",
      "ep 1113: ep_len:500 episode reward: total was -10.550000. running mean: -45.782619\n",
      "ep 1113: ep_len:514 episode reward: total was -77.040000. running mean: -46.095193\n",
      "epsilon:0.150613 episode_count: 7798. steps_count: 3396307.000000\n",
      "Time elapsed:  9767.878427743912\n",
      "ep 1114: ep_len:500 episode reward: total was 5.790000. running mean: -45.576341\n",
      "ep 1114: ep_len:527 episode reward: total was -10.310000. running mean: -45.223677\n",
      "ep 1114: ep_len:595 episode reward: total was -51.350000. running mean: -45.284940\n",
      "ep 1114: ep_len:538 episode reward: total was -2.130000. running mean: -44.853391\n",
      "ep 1114: ep_len:121 episode reward: total was 6.810000. running mean: -44.336757\n",
      "ep 1114: ep_len:520 episode reward: total was -68.420000. running mean: -44.577590\n",
      "ep 1114: ep_len:592 episode reward: total was -70.990000. running mean: -44.841714\n",
      "epsilon:0.150568 episode_count: 7805. steps_count: 3399700.000000\n",
      "Time elapsed:  9777.843548297882\n",
      "ep 1115: ep_len:605 episode reward: total was -5.140000. running mean: -44.444697\n",
      "ep 1115: ep_len:607 episode reward: total was -81.280000. running mean: -44.813050\n",
      "ep 1115: ep_len:500 episode reward: total was -61.370000. running mean: -44.978619\n",
      "ep 1115: ep_len:505 episode reward: total was -75.190000. running mean: -45.280733\n",
      "ep 1115: ep_len:3 episode reward: total was 0.000000. running mean: -44.827926\n",
      "ep 1115: ep_len:589 episode reward: total was -131.050000. running mean: -45.690146\n",
      "ep 1115: ep_len:537 episode reward: total was -72.770000. running mean: -45.960945\n",
      "epsilon:0.150524 episode_count: 7812. steps_count: 3403046.000000\n",
      "Time elapsed:  9786.715359687805\n",
      "ep 1116: ep_len:619 episode reward: total was -138.110000. running mean: -46.882435\n",
      "ep 1116: ep_len:609 episode reward: total was -64.770000. running mean: -47.061311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1116: ep_len:62 episode reward: total was 10.190000. running mean: -46.488798\n",
      "ep 1116: ep_len:521 episode reward: total was -62.370000. running mean: -46.647610\n",
      "ep 1116: ep_len:3 episode reward: total was 0.000000. running mean: -46.181134\n",
      "ep 1116: ep_len:630 episode reward: total was -45.270000. running mean: -46.172023\n",
      "ep 1116: ep_len:211 episode reward: total was -25.990000. running mean: -45.970202\n",
      "epsilon:0.150480 episode_count: 7819. steps_count: 3405701.000000\n",
      "Time elapsed:  9793.777381896973\n",
      "ep 1117: ep_len:592 episode reward: total was 9.540000. running mean: -45.415100\n",
      "ep 1117: ep_len:531 episode reward: total was 24.690000. running mean: -44.714049\n",
      "ep 1117: ep_len:500 episode reward: total was -45.410000. running mean: -44.721009\n",
      "ep 1117: ep_len:508 episode reward: total was -74.710000. running mean: -45.020899\n",
      "ep 1117: ep_len:3 episode reward: total was -1.500000. running mean: -44.585690\n",
      "ep 1117: ep_len:598 episode reward: total was -52.110000. running mean: -44.660933\n",
      "ep 1117: ep_len:528 episode reward: total was -20.260000. running mean: -44.416923\n",
      "epsilon:0.150435 episode_count: 7826. steps_count: 3408961.000000\n",
      "Time elapsed:  9802.555888414383\n",
      "ep 1118: ep_len:551 episode reward: total was -23.660000. running mean: -44.209354\n",
      "ep 1118: ep_len:339 episode reward: total was -53.400000. running mean: -44.301261\n",
      "ep 1118: ep_len:562 episode reward: total was -101.850000. running mean: -44.876748\n",
      "ep 1118: ep_len:531 episode reward: total was -17.580000. running mean: -44.603781\n",
      "ep 1118: ep_len:122 episode reward: total was 12.820000. running mean: -44.029543\n",
      "ep 1118: ep_len:501 episode reward: total was -112.320000. running mean: -44.712447\n",
      "ep 1118: ep_len:500 episode reward: total was -8.150000. running mean: -44.346823\n",
      "epsilon:0.150391 episode_count: 7833. steps_count: 3412067.000000\n",
      "Time elapsed:  9810.732981443405\n",
      "ep 1119: ep_len:248 episode reward: total was 7.230000. running mean: -43.831055\n",
      "ep 1119: ep_len:572 episode reward: total was -63.460000. running mean: -44.027344\n",
      "ep 1119: ep_len:605 episode reward: total was -82.450000. running mean: -44.411571\n",
      "ep 1119: ep_len:503 episode reward: total was -61.770000. running mean: -44.585155\n",
      "ep 1119: ep_len:88 episode reward: total was -37.740000. running mean: -44.516703\n",
      "ep 1119: ep_len:500 episode reward: total was -82.100000. running mean: -44.892536\n",
      "ep 1119: ep_len:538 episode reward: total was -41.770000. running mean: -44.861311\n",
      "epsilon:0.150347 episode_count: 7840. steps_count: 3415121.000000\n",
      "Time elapsed:  9818.324121952057\n",
      "ep 1120: ep_len:500 episode reward: total was -2.740000. running mean: -44.440098\n",
      "ep 1120: ep_len:608 episode reward: total was -83.740000. running mean: -44.833097\n",
      "ep 1120: ep_len:500 episode reward: total was -58.910000. running mean: -44.973866\n",
      "ep 1120: ep_len:500 episode reward: total was -30.620000. running mean: -44.830327\n",
      "ep 1120: ep_len:3 episode reward: total was -3.000000. running mean: -44.412024\n",
      "ep 1120: ep_len:631 episode reward: total was -109.320000. running mean: -45.061104\n",
      "ep 1120: ep_len:501 episode reward: total was -41.210000. running mean: -45.022593\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.150302 episode_count: 7847. steps_count: 3418364.000000\n",
      "Time elapsed:  9831.068535089493\n",
      "ep 1121: ep_len:558 episode reward: total was -131.340000. running mean: -45.885767\n",
      "ep 1121: ep_len:509 episode reward: total was -82.640000. running mean: -46.253309\n",
      "ep 1121: ep_len:574 episode reward: total was -81.710000. running mean: -46.607876\n",
      "ep 1121: ep_len:56 episode reward: total was -5.210000. running mean: -46.193897\n",
      "ep 1121: ep_len:131 episode reward: total was 2.330000. running mean: -45.708658\n",
      "ep 1121: ep_len:500 episode reward: total was -65.200000. running mean: -45.903572\n",
      "ep 1121: ep_len:599 episode reward: total was -33.610000. running mean: -45.780636\n",
      "epsilon:0.150258 episode_count: 7854. steps_count: 3421291.000000\n",
      "Time elapsed:  9839.430175065994\n",
      "ep 1122: ep_len:254 episode reward: total was -33.920000. running mean: -45.662030\n",
      "ep 1122: ep_len:517 episode reward: total was -39.510000. running mean: -45.600509\n",
      "ep 1122: ep_len:577 episode reward: total was -69.100000. running mean: -45.835504\n",
      "ep 1122: ep_len:502 episode reward: total was -59.090000. running mean: -45.968049\n",
      "ep 1122: ep_len:85 episode reward: total was -16.330000. running mean: -45.671669\n",
      "ep 1122: ep_len:520 episode reward: total was -67.010000. running mean: -45.885052\n",
      "ep 1122: ep_len:634 episode reward: total was -155.650000. running mean: -46.982702\n",
      "epsilon:0.150214 episode_count: 7861. steps_count: 3424380.000000\n",
      "Time elapsed:  9848.23048377037\n",
      "ep 1123: ep_len:500 episode reward: total was 4.080000. running mean: -46.472075\n",
      "ep 1123: ep_len:201 episode reward: total was -19.710000. running mean: -46.204454\n",
      "ep 1123: ep_len:584 episode reward: total was -79.480000. running mean: -46.537209\n",
      "ep 1123: ep_len:500 episode reward: total was 3.340000. running mean: -46.038437\n",
      "ep 1123: ep_len:100 episode reward: total was 8.310000. running mean: -45.494953\n",
      "ep 1123: ep_len:568 episode reward: total was -56.230000. running mean: -45.602303\n",
      "ep 1123: ep_len:543 episode reward: total was 9.190000. running mean: -45.054380\n",
      "epsilon:0.150169 episode_count: 7868. steps_count: 3427376.000000\n",
      "Time elapsed:  9856.119994878769\n",
      "ep 1124: ep_len:500 episode reward: total was -35.010000. running mean: -44.953936\n",
      "ep 1124: ep_len:500 episode reward: total was 1.580000. running mean: -44.488597\n",
      "ep 1124: ep_len:642 episode reward: total was -83.920000. running mean: -44.882911\n",
      "ep 1124: ep_len:527 episode reward: total was -38.460000. running mean: -44.818682\n",
      "ep 1124: ep_len:79 episode reward: total was 5.710000. running mean: -44.313395\n",
      "ep 1124: ep_len:248 episode reward: total was -4.220000. running mean: -43.912461\n",
      "ep 1124: ep_len:305 episode reward: total was -73.690000. running mean: -44.210237\n",
      "epsilon:0.150125 episode_count: 7875. steps_count: 3430177.000000\n",
      "Time elapsed:  9863.565348148346\n",
      "ep 1125: ep_len:565 episode reward: total was 7.440000. running mean: -43.693734\n",
      "ep 1125: ep_len:500 episode reward: total was -22.600000. running mean: -43.482797\n",
      "ep 1125: ep_len:500 episode reward: total was -0.730000. running mean: -43.055269\n",
      "ep 1125: ep_len:589 episode reward: total was -17.420000. running mean: -42.798916\n",
      "ep 1125: ep_len:99 episode reward: total was 18.710000. running mean: -42.183827\n",
      "ep 1125: ep_len:500 episode reward: total was -43.010000. running mean: -42.192089\n",
      "ep 1125: ep_len:600 episode reward: total was -75.520000. running mean: -42.525368\n",
      "epsilon:0.150081 episode_count: 7882. steps_count: 3433530.000000\n",
      "Time elapsed:  9872.248332738876\n",
      "ep 1126: ep_len:500 episode reward: total was 21.140000. running mean: -41.888714\n",
      "ep 1126: ep_len:500 episode reward: total was -96.680000. running mean: -42.436627\n",
      "ep 1126: ep_len:79 episode reward: total was -7.760000. running mean: -42.089861\n",
      "ep 1126: ep_len:625 episode reward: total was -65.630000. running mean: -42.325262\n",
      "ep 1126: ep_len:55 episode reward: total was 11.000000. running mean: -41.792010\n",
      "ep 1126: ep_len:500 episode reward: total was -88.090000. running mean: -42.254989\n",
      "ep 1126: ep_len:563 episode reward: total was -79.340000. running mean: -42.625840\n",
      "epsilon:0.150036 episode_count: 7889. steps_count: 3436352.000000\n",
      "Time elapsed:  9879.920018196106\n",
      "ep 1127: ep_len:134 episode reward: total was -10.570000. running mean: -42.305281\n",
      "ep 1127: ep_len:529 episode reward: total was -63.240000. running mean: -42.514628\n",
      "ep 1127: ep_len:500 episode reward: total was -62.560000. running mean: -42.715082\n",
      "ep 1127: ep_len:500 episode reward: total was -12.890000. running mean: -42.416831\n",
      "ep 1127: ep_len:48 episode reward: total was 6.000000. running mean: -41.932663\n",
      "ep 1127: ep_len:261 episode reward: total was 5.300000. running mean: -41.460336\n",
      "ep 1127: ep_len:578 episode reward: total was -94.340000. running mean: -41.989133\n",
      "epsilon:0.149992 episode_count: 7896. steps_count: 3438902.000000\n",
      "Time elapsed:  9886.961234092712\n",
      "ep 1128: ep_len:134 episode reward: total was -8.060000. running mean: -41.649842\n",
      "ep 1128: ep_len:532 episode reward: total was -100.860000. running mean: -42.241943\n",
      "ep 1128: ep_len:637 episode reward: total was -35.290000. running mean: -42.172424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1128: ep_len:56 episode reward: total was -11.730000. running mean: -41.868000\n",
      "ep 1128: ep_len:101 episode reward: total was 6.250000. running mean: -41.386820\n",
      "ep 1128: ep_len:553 episode reward: total was -37.840000. running mean: -41.351351\n",
      "ep 1128: ep_len:500 episode reward: total was -42.260000. running mean: -41.360438\n",
      "epsilon:0.149948 episode_count: 7903. steps_count: 3441415.000000\n",
      "Time elapsed:  9894.066148042679\n",
      "ep 1129: ep_len:542 episode reward: total was 25.250000. running mean: -40.694333\n",
      "ep 1129: ep_len:500 episode reward: total was -59.260000. running mean: -40.879990\n",
      "ep 1129: ep_len:545 episode reward: total was -93.770000. running mean: -41.408890\n",
      "ep 1129: ep_len:132 episode reward: total was 8.580000. running mean: -40.909001\n",
      "ep 1129: ep_len:3 episode reward: total was -1.500000. running mean: -40.514911\n",
      "ep 1129: ep_len:528 episode reward: total was -61.880000. running mean: -40.728562\n",
      "ep 1129: ep_len:185 episode reward: total was -18.560000. running mean: -40.506877\n",
      "epsilon:0.149903 episode_count: 7910. steps_count: 3443850.000000\n",
      "Time elapsed:  9900.890521764755\n",
      "ep 1130: ep_len:651 episode reward: total was -68.400000. running mean: -40.785808\n",
      "ep 1130: ep_len:540 episode reward: total was -34.920000. running mean: -40.727150\n",
      "ep 1130: ep_len:538 episode reward: total was -57.900000. running mean: -40.898878\n",
      "ep 1130: ep_len:500 episode reward: total was -54.730000. running mean: -41.037189\n",
      "ep 1130: ep_len:86 episode reward: total was -50.800000. running mean: -41.134818\n",
      "ep 1130: ep_len:502 episode reward: total was -47.420000. running mean: -41.197669\n",
      "ep 1130: ep_len:501 episode reward: total was -71.020000. running mean: -41.495893\n",
      "epsilon:0.149859 episode_count: 7917. steps_count: 3447168.000000\n",
      "Time elapsed:  9909.536810159683\n",
      "ep 1131: ep_len:230 episode reward: total was -22.560000. running mean: -41.306534\n",
      "ep 1131: ep_len:550 episode reward: total was -54.430000. running mean: -41.437768\n",
      "ep 1131: ep_len:373 episode reward: total was -10.440000. running mean: -41.127791\n",
      "ep 1131: ep_len:508 episode reward: total was -88.750000. running mean: -41.604013\n",
      "ep 1131: ep_len:3 episode reward: total was -1.500000. running mean: -41.202973\n",
      "ep 1131: ep_len:500 episode reward: total was -45.530000. running mean: -41.246243\n",
      "ep 1131: ep_len:500 episode reward: total was -34.270000. running mean: -41.176481\n",
      "epsilon:0.149815 episode_count: 7924. steps_count: 3449832.000000\n",
      "Time elapsed:  9917.797838926315\n",
      "ep 1132: ep_len:660 episode reward: total was -155.300000. running mean: -42.317716\n",
      "ep 1132: ep_len:500 episode reward: total was -55.370000. running mean: -42.448239\n",
      "ep 1132: ep_len:382 episode reward: total was -9.890000. running mean: -42.122656\n",
      "ep 1132: ep_len:541 episode reward: total was 9.330000. running mean: -41.608130\n",
      "ep 1132: ep_len:104 episode reward: total was 10.250000. running mean: -41.089548\n",
      "ep 1132: ep_len:501 episode reward: total was -86.670000. running mean: -41.545353\n",
      "ep 1132: ep_len:198 episode reward: total was -27.090000. running mean: -41.400799\n",
      "epsilon:0.149770 episode_count: 7931. steps_count: 3452718.000000\n",
      "Time elapsed:  9926.626569986343\n",
      "ep 1133: ep_len:573 episode reward: total was -78.880000. running mean: -41.775591\n",
      "ep 1133: ep_len:594 episode reward: total was -95.950000. running mean: -42.317335\n",
      "ep 1133: ep_len:614 episode reward: total was -51.640000. running mean: -42.410562\n",
      "ep 1133: ep_len:510 episode reward: total was -74.120000. running mean: -42.727656\n",
      "ep 1133: ep_len:3 episode reward: total was 0.000000. running mean: -42.300380\n",
      "ep 1133: ep_len:735 episode reward: total was -241.980000. running mean: -44.297176\n",
      "ep 1133: ep_len:201 episode reward: total was -38.600000. running mean: -44.240204\n",
      "epsilon:0.149726 episode_count: 7938. steps_count: 3455948.000000\n",
      "Time elapsed:  9936.497982740402\n",
      "ep 1134: ep_len:501 episode reward: total was -16.350000. running mean: -43.961302\n",
      "ep 1134: ep_len:543 episode reward: total was -35.070000. running mean: -43.872389\n",
      "ep 1134: ep_len:449 episode reward: total was -0.110000. running mean: -43.434765\n",
      "ep 1134: ep_len:526 episode reward: total was 0.030000. running mean: -43.000118\n",
      "ep 1134: ep_len:3 episode reward: total was 0.000000. running mean: -42.570117\n",
      "ep 1134: ep_len:573 episode reward: total was -47.200000. running mean: -42.616415\n",
      "ep 1134: ep_len:510 episode reward: total was -47.950000. running mean: -42.669751\n",
      "epsilon:0.149682 episode_count: 7945. steps_count: 3459053.000000\n",
      "Time elapsed:  9944.887038707733\n",
      "ep 1135: ep_len:571 episode reward: total was -147.980000. running mean: -43.722854\n",
      "ep 1135: ep_len:584 episode reward: total was -21.970000. running mean: -43.505325\n",
      "ep 1135: ep_len:371 episode reward: total was -8.170000. running mean: -43.151972\n",
      "ep 1135: ep_len:378 episode reward: total was -54.260000. running mean: -43.263052\n",
      "ep 1135: ep_len:55 episode reward: total was 18.500000. running mean: -42.645422\n",
      "ep 1135: ep_len:535 episode reward: total was -32.290000. running mean: -42.541867\n",
      "ep 1135: ep_len:601 episode reward: total was -93.490000. running mean: -43.051349\n",
      "epsilon:0.149637 episode_count: 7952. steps_count: 3462148.000000\n",
      "Time elapsed:  9953.035809278488\n",
      "ep 1136: ep_len:212 episode reward: total was 0.090000. running mean: -42.619935\n",
      "ep 1136: ep_len:500 episode reward: total was -25.110000. running mean: -42.444836\n",
      "ep 1136: ep_len:79 episode reward: total was 0.750000. running mean: -42.012888\n",
      "ep 1136: ep_len:605 episode reward: total was -30.240000. running mean: -41.895159\n",
      "ep 1136: ep_len:111 episode reward: total was 20.240000. running mean: -41.273807\n",
      "ep 1136: ep_len:500 episode reward: total was -44.150000. running mean: -41.302569\n",
      "ep 1136: ep_len:344 episode reward: total was -101.170000. running mean: -41.901243\n",
      "epsilon:0.149593 episode_count: 7959. steps_count: 3464499.000000\n",
      "Time elapsed:  9959.53784108162\n",
      "ep 1137: ep_len:500 episode reward: total was -100.260000. running mean: -42.484831\n",
      "ep 1137: ep_len:619 episode reward: total was -53.720000. running mean: -42.597183\n",
      "ep 1137: ep_len:500 episode reward: total was -97.190000. running mean: -43.143111\n",
      "ep 1137: ep_len:151 episode reward: total was 11.590000. running mean: -42.595780\n",
      "ep 1137: ep_len:3 episode reward: total was -1.500000. running mean: -42.184822\n",
      "ep 1137: ep_len:644 episode reward: total was -66.470000. running mean: -42.427674\n",
      "ep 1137: ep_len:299 episode reward: total was -59.380000. running mean: -42.597197\n",
      "epsilon:0.149549 episode_count: 7966. steps_count: 3467215.000000\n",
      "Time elapsed:  9967.951662063599\n",
      "ep 1138: ep_len:619 episode reward: total was -45.490000. running mean: -42.626125\n",
      "ep 1138: ep_len:500 episode reward: total was -34.490000. running mean: -42.544764\n",
      "ep 1138: ep_len:579 episode reward: total was -52.200000. running mean: -42.641316\n",
      "ep 1138: ep_len:600 episode reward: total was 19.420000. running mean: -42.020703\n",
      "ep 1138: ep_len:3 episode reward: total was 0.000000. running mean: -41.600496\n",
      "ep 1138: ep_len:519 episode reward: total was -89.220000. running mean: -42.076691\n",
      "ep 1138: ep_len:608 episode reward: total was -24.750000. running mean: -41.903424\n",
      "epsilon:0.149504 episode_count: 7973. steps_count: 3470643.000000\n",
      "Time elapsed:  9977.001764774323\n",
      "ep 1139: ep_len:521 episode reward: total was -41.510000. running mean: -41.899490\n",
      "ep 1139: ep_len:289 episode reward: total was -90.440000. running mean: -42.384895\n",
      "ep 1139: ep_len:56 episode reward: total was -11.420000. running mean: -42.075246\n",
      "ep 1139: ep_len:565 episode reward: total was 1.460000. running mean: -41.639893\n",
      "ep 1139: ep_len:3 episode reward: total was 0.000000. running mean: -41.223495\n",
      "ep 1139: ep_len:515 episode reward: total was -84.790000. running mean: -41.659160\n",
      "ep 1139: ep_len:520 episode reward: total was -83.480000. running mean: -42.077368\n",
      "epsilon:0.149460 episode_count: 7980. steps_count: 3473112.000000\n",
      "Time elapsed:  9983.742535591125\n",
      "ep 1140: ep_len:580 episode reward: total was 6.380000. running mean: -41.592794\n",
      "ep 1140: ep_len:654 episode reward: total was -110.890000. running mean: -42.285766\n",
      "ep 1140: ep_len:400 episode reward: total was -54.790000. running mean: -42.410809\n",
      "ep 1140: ep_len:500 episode reward: total was 9.880000. running mean: -41.887901\n",
      "ep 1140: ep_len:3 episode reward: total was -0.490000. running mean: -41.473922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1140: ep_len:509 episode reward: total was -95.710000. running mean: -42.016282\n",
      "ep 1140: ep_len:577 episode reward: total was -167.900000. running mean: -43.275120\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.149416 episode_count: 7987. steps_count: 3476335.000000\n",
      "Time elapsed:  9997.264499664307\n",
      "ep 1141: ep_len:549 episode reward: total was -47.260000. running mean: -43.314968\n",
      "ep 1141: ep_len:554 episode reward: total was -50.720000. running mean: -43.389019\n",
      "ep 1141: ep_len:336 episode reward: total was 6.170000. running mean: -42.893429\n",
      "ep 1141: ep_len:500 episode reward: total was -113.060000. running mean: -43.595094\n",
      "ep 1141: ep_len:129 episode reward: total was 2.270000. running mean: -43.136443\n",
      "ep 1141: ep_len:621 episode reward: total was -45.220000. running mean: -43.157279\n",
      "ep 1141: ep_len:623 episode reward: total was -94.420000. running mean: -43.669906\n",
      "epsilon:0.149371 episode_count: 7994. steps_count: 3479647.000000\n",
      "Time elapsed:  10007.001438379288\n",
      "ep 1142: ep_len:622 episode reward: total was -42.670000. running mean: -43.659907\n",
      "ep 1142: ep_len:599 episode reward: total was -45.690000. running mean: -43.680208\n",
      "ep 1142: ep_len:500 episode reward: total was -71.690000. running mean: -43.960306\n",
      "ep 1142: ep_len:500 episode reward: total was -32.850000. running mean: -43.849203\n",
      "ep 1142: ep_len:90 episode reward: total was 16.750000. running mean: -43.243211\n",
      "ep 1142: ep_len:586 episode reward: total was -50.040000. running mean: -43.311179\n",
      "ep 1142: ep_len:571 episode reward: total was -216.810000. running mean: -45.046167\n",
      "epsilon:0.149327 episode_count: 8001. steps_count: 3483115.000000\n",
      "Time elapsed:  10016.128470182419\n",
      "ep 1143: ep_len:500 episode reward: total was 6.080000. running mean: -44.534905\n",
      "ep 1143: ep_len:500 episode reward: total was -51.230000. running mean: -44.601856\n",
      "ep 1143: ep_len:521 episode reward: total was -89.300000. running mean: -45.048838\n",
      "ep 1143: ep_len:536 episode reward: total was -80.800000. running mean: -45.406349\n",
      "ep 1143: ep_len:71 episode reward: total was 3.730000. running mean: -44.914986\n",
      "ep 1143: ep_len:534 episode reward: total was -56.870000. running mean: -45.034536\n",
      "ep 1143: ep_len:543 episode reward: total was -57.100000. running mean: -45.155191\n",
      "epsilon:0.149283 episode_count: 8008. steps_count: 3486320.000000\n",
      "Time elapsed:  10024.498089075089\n",
      "ep 1144: ep_len:532 episode reward: total was -108.980000. running mean: -45.793439\n",
      "ep 1144: ep_len:500 episode reward: total was 0.830000. running mean: -45.327204\n",
      "ep 1144: ep_len:526 episode reward: total was -9.410000. running mean: -44.968032\n",
      "ep 1144: ep_len:543 episode reward: total was 2.420000. running mean: -44.494152\n",
      "ep 1144: ep_len:40 episode reward: total was 8.000000. running mean: -43.969210\n",
      "ep 1144: ep_len:531 episode reward: total was -64.310000. running mean: -44.172618\n",
      "ep 1144: ep_len:592 episode reward: total was -62.110000. running mean: -44.351992\n",
      "epsilon:0.149238 episode_count: 8015. steps_count: 3489584.000000\n",
      "Time elapsed:  10034.010999679565\n",
      "ep 1145: ep_len:586 episode reward: total was -9.090000. running mean: -43.999372\n",
      "ep 1145: ep_len:565 episode reward: total was -28.300000. running mean: -43.842378\n",
      "ep 1145: ep_len:586 episode reward: total was -45.120000. running mean: -43.855155\n",
      "ep 1145: ep_len:500 episode reward: total was -51.610000. running mean: -43.932703\n",
      "ep 1145: ep_len:3 episode reward: total was 0.000000. running mean: -43.493376\n",
      "ep 1145: ep_len:639 episode reward: total was -43.140000. running mean: -43.489842\n",
      "ep 1145: ep_len:578 episode reward: total was -164.260000. running mean: -44.697544\n",
      "epsilon:0.149194 episode_count: 8022. steps_count: 3493041.000000\n",
      "Time elapsed:  10043.16170668602\n",
      "ep 1146: ep_len:518 episode reward: total was -99.290000. running mean: -45.243468\n",
      "ep 1146: ep_len:577 episode reward: total was -17.710000. running mean: -44.968134\n",
      "ep 1146: ep_len:415 episode reward: total was 9.740000. running mean: -44.421052\n",
      "ep 1146: ep_len:500 episode reward: total was -83.610000. running mean: -44.812942\n",
      "ep 1146: ep_len:85 episode reward: total was -10.240000. running mean: -44.467212\n",
      "ep 1146: ep_len:528 episode reward: total was -35.930000. running mean: -44.381840\n",
      "ep 1146: ep_len:512 episode reward: total was -38.640000. running mean: -44.324422\n",
      "epsilon:0.149150 episode_count: 8029. steps_count: 3496176.000000\n",
      "Time elapsed:  10051.444010972977\n",
      "ep 1147: ep_len:600 episode reward: total was 6.960000. running mean: -43.811578\n",
      "ep 1147: ep_len:555 episode reward: total was -26.400000. running mean: -43.637462\n",
      "ep 1147: ep_len:500 episode reward: total was -25.090000. running mean: -43.451987\n",
      "ep 1147: ep_len:500 episode reward: total was -15.570000. running mean: -43.173167\n",
      "ep 1147: ep_len:3 episode reward: total was 0.000000. running mean: -42.741436\n",
      "ep 1147: ep_len:500 episode reward: total was -45.100000. running mean: -42.765021\n",
      "ep 1147: ep_len:538 episode reward: total was -72.130000. running mean: -43.058671\n",
      "epsilon:0.149105 episode_count: 8036. steps_count: 3499372.000000\n",
      "Time elapsed:  10059.923903465271\n",
      "ep 1148: ep_len:574 episode reward: total was -65.870000. running mean: -43.286784\n",
      "ep 1148: ep_len:633 episode reward: total was 9.570000. running mean: -42.758217\n",
      "ep 1148: ep_len:572 episode reward: total was -44.180000. running mean: -42.772434\n",
      "ep 1148: ep_len:500 episode reward: total was 23.490000. running mean: -42.109810\n",
      "ep 1148: ep_len:98 episode reward: total was 10.220000. running mean: -41.586512\n",
      "ep 1148: ep_len:582 episode reward: total was -9.970000. running mean: -41.270347\n",
      "ep 1148: ep_len:511 episode reward: total was -64.520000. running mean: -41.502843\n",
      "epsilon:0.149061 episode_count: 8043. steps_count: 3502842.000000\n",
      "Time elapsed:  10070.220552682877\n",
      "ep 1149: ep_len:647 episode reward: total was -91.610000. running mean: -42.003915\n",
      "ep 1149: ep_len:580 episode reward: total was -75.950000. running mean: -42.343376\n",
      "ep 1149: ep_len:538 episode reward: total was -45.980000. running mean: -42.379742\n",
      "ep 1149: ep_len:513 episode reward: total was 4.660000. running mean: -41.909345\n",
      "ep 1149: ep_len:3 episode reward: total was 0.000000. running mean: -41.490251\n",
      "ep 1149: ep_len:667 episode reward: total was -50.050000. running mean: -41.575849\n",
      "ep 1149: ep_len:508 episode reward: total was -57.640000. running mean: -41.736490\n",
      "epsilon:0.149017 episode_count: 8050. steps_count: 3506298.000000\n",
      "Time elapsed:  10079.01594209671\n",
      "ep 1150: ep_len:500 episode reward: total was -43.050000. running mean: -41.749625\n",
      "ep 1150: ep_len:500 episode reward: total was -36.620000. running mean: -41.698329\n",
      "ep 1150: ep_len:396 episode reward: total was -19.760000. running mean: -41.478946\n",
      "ep 1150: ep_len:544 episode reward: total was -66.030000. running mean: -41.724456\n",
      "ep 1150: ep_len:3 episode reward: total was 0.000000. running mean: -41.307212\n",
      "ep 1150: ep_len:633 episode reward: total was -138.920000. running mean: -42.283340\n",
      "ep 1150: ep_len:524 episode reward: total was -60.730000. running mean: -42.467806\n",
      "epsilon:0.148972 episode_count: 8057. steps_count: 3509398.000000\n",
      "Time elapsed:  10089.650667905807\n",
      "ep 1151: ep_len:511 episode reward: total was -64.620000. running mean: -42.689328\n",
      "ep 1151: ep_len:505 episode reward: total was -43.160000. running mean: -42.694035\n",
      "ep 1151: ep_len:636 episode reward: total was -88.590000. running mean: -43.152995\n",
      "ep 1151: ep_len:575 episode reward: total was -39.610000. running mean: -43.117565\n",
      "ep 1151: ep_len:104 episode reward: total was 0.680000. running mean: -42.679589\n",
      "ep 1151: ep_len:554 episode reward: total was -73.330000. running mean: -42.986093\n",
      "ep 1151: ep_len:500 episode reward: total was -23.530000. running mean: -42.791532\n",
      "epsilon:0.148928 episode_count: 8064. steps_count: 3512783.000000\n",
      "Time elapsed:  10098.738445520401\n",
      "ep 1152: ep_len:509 episode reward: total was 5.700000. running mean: -42.306617\n",
      "ep 1152: ep_len:570 episode reward: total was -3.040000. running mean: -41.913951\n",
      "ep 1152: ep_len:511 episode reward: total was -59.520000. running mean: -42.090011\n",
      "ep 1152: ep_len:614 episode reward: total was -14.470000. running mean: -41.813811\n",
      "ep 1152: ep_len:40 episode reward: total was 12.500000. running mean: -41.270673\n",
      "ep 1152: ep_len:500 episode reward: total was -55.600000. running mean: -41.413966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1152: ep_len:553 episode reward: total was -24.780000. running mean: -41.247627\n",
      "epsilon:0.148884 episode_count: 8071. steps_count: 3516080.000000\n",
      "Time elapsed:  10108.549946308136\n",
      "ep 1153: ep_len:260 episode reward: total was -5.720000. running mean: -40.892350\n",
      "ep 1153: ep_len:500 episode reward: total was -13.690000. running mean: -40.620327\n",
      "ep 1153: ep_len:436 episode reward: total was -53.140000. running mean: -40.745523\n",
      "ep 1153: ep_len:500 episode reward: total was 12.310000. running mean: -40.214968\n",
      "ep 1153: ep_len:3 episode reward: total was 0.000000. running mean: -39.812819\n",
      "ep 1153: ep_len:544 episode reward: total was -65.170000. running mean: -40.066390\n",
      "ep 1153: ep_len:500 episode reward: total was -54.470000. running mean: -40.210426\n",
      "epsilon:0.148839 episode_count: 8078. steps_count: 3518823.000000\n",
      "Time elapsed:  10117.039432764053\n",
      "ep 1154: ep_len:500 episode reward: total was 15.150000. running mean: -39.656822\n",
      "ep 1154: ep_len:190 episode reward: total was -32.460000. running mean: -39.584854\n",
      "ep 1154: ep_len:562 episode reward: total was -94.500000. running mean: -40.134005\n",
      "ep 1154: ep_len:500 episode reward: total was -31.680000. running mean: -40.049465\n",
      "ep 1154: ep_len:47 episode reward: total was 14.500000. running mean: -39.503971\n",
      "ep 1154: ep_len:212 episode reward: total was 6.640000. running mean: -39.042531\n",
      "ep 1154: ep_len:598 episode reward: total was -61.590000. running mean: -39.268006\n",
      "epsilon:0.148795 episode_count: 8085. steps_count: 3521432.000000\n",
      "Time elapsed:  10125.119755506516\n",
      "ep 1155: ep_len:678 episode reward: total was -216.310000. running mean: -41.038426\n",
      "ep 1155: ep_len:500 episode reward: total was -57.780000. running mean: -41.205841\n",
      "ep 1155: ep_len:533 episode reward: total was -73.110000. running mean: -41.524883\n",
      "ep 1155: ep_len:500 episode reward: total was 14.260000. running mean: -40.967034\n",
      "ep 1155: ep_len:3 episode reward: total was 0.000000. running mean: -40.557364\n",
      "ep 1155: ep_len:638 episode reward: total was -74.960000. running mean: -40.901390\n",
      "ep 1155: ep_len:350 episode reward: total was -66.980000. running mean: -41.162176\n",
      "epsilon:0.148751 episode_count: 8092. steps_count: 3524634.000000\n",
      "Time elapsed:  10134.413858890533\n",
      "ep 1156: ep_len:624 episode reward: total was -26.370000. running mean: -41.014255\n",
      "ep 1156: ep_len:512 episode reward: total was -27.710000. running mean: -40.881212\n",
      "ep 1156: ep_len:570 episode reward: total was -103.270000. running mean: -41.505100\n",
      "ep 1156: ep_len:391 episode reward: total was -17.460000. running mean: -41.264649\n",
      "ep 1156: ep_len:3 episode reward: total was 0.000000. running mean: -40.852002\n",
      "ep 1156: ep_len:695 episode reward: total was -55.190000. running mean: -40.995382\n",
      "ep 1156: ep_len:543 episode reward: total was -46.020000. running mean: -41.045629\n",
      "epsilon:0.148706 episode_count: 8099. steps_count: 3527972.000000\n",
      "Time elapsed:  10143.205652236938\n",
      "ep 1157: ep_len:546 episode reward: total was -35.620000. running mean: -40.991372\n",
      "ep 1157: ep_len:631 episode reward: total was -20.880000. running mean: -40.790259\n",
      "ep 1157: ep_len:528 episode reward: total was -39.760000. running mean: -40.779956\n",
      "ep 1157: ep_len:520 episode reward: total was -58.690000. running mean: -40.959056\n",
      "ep 1157: ep_len:55 episode reward: total was 18.500000. running mean: -40.364466\n",
      "ep 1157: ep_len:513 episode reward: total was -74.060000. running mean: -40.701421\n",
      "ep 1157: ep_len:539 episode reward: total was -53.810000. running mean: -40.832507\n",
      "epsilon:0.148662 episode_count: 8106. steps_count: 3531304.000000\n",
      "Time elapsed:  10153.345217704773\n",
      "ep 1158: ep_len:503 episode reward: total was -97.270000. running mean: -41.396882\n",
      "ep 1158: ep_len:529 episode reward: total was -14.070000. running mean: -41.123613\n",
      "ep 1158: ep_len:79 episode reward: total was -0.230000. running mean: -40.714677\n",
      "ep 1158: ep_len:40 episode reward: total was -0.210000. running mean: -40.309630\n",
      "ep 1158: ep_len:3 episode reward: total was -1.500000. running mean: -39.921534\n",
      "ep 1158: ep_len:619 episode reward: total was -77.190000. running mean: -40.294219\n",
      "ep 1158: ep_len:531 episode reward: total was -55.620000. running mean: -40.447476\n",
      "epsilon:0.148618 episode_count: 8113. steps_count: 3533608.000000\n",
      "Time elapsed:  10159.810206413269\n",
      "ep 1159: ep_len:616 episode reward: total was -78.850000. running mean: -40.831502\n",
      "ep 1159: ep_len:500 episode reward: total was -52.410000. running mean: -40.947287\n",
      "ep 1159: ep_len:426 episode reward: total was -29.980000. running mean: -40.837614\n",
      "ep 1159: ep_len:500 episode reward: total was -42.260000. running mean: -40.851838\n",
      "ep 1159: ep_len:3 episode reward: total was 0.000000. running mean: -40.443319\n",
      "ep 1159: ep_len:510 episode reward: total was -64.390000. running mean: -40.682786\n",
      "ep 1159: ep_len:649 episode reward: total was -178.160000. running mean: -42.057558\n",
      "epsilon:0.148573 episode_count: 8120. steps_count: 3536812.000000\n",
      "Time elapsed:  10168.379258155823\n",
      "ep 1160: ep_len:117 episode reward: total was 0.920000. running mean: -41.627783\n",
      "ep 1160: ep_len:500 episode reward: total was -49.190000. running mean: -41.703405\n",
      "ep 1160: ep_len:568 episode reward: total was 0.470000. running mean: -41.281671\n",
      "ep 1160: ep_len:527 episode reward: total was -37.580000. running mean: -41.244654\n",
      "ep 1160: ep_len:3 episode reward: total was 0.000000. running mean: -40.832207\n",
      "ep 1160: ep_len:690 episode reward: total was -51.160000. running mean: -40.935485\n",
      "ep 1160: ep_len:503 episode reward: total was -72.830000. running mean: -41.254430\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.148529 episode_count: 8127. steps_count: 3539720.000000\n",
      "Time elapsed:  10181.06852388382\n",
      "ep 1161: ep_len:611 episode reward: total was -129.590000. running mean: -42.137786\n",
      "ep 1161: ep_len:500 episode reward: total was 11.400000. running mean: -41.602408\n",
      "ep 1161: ep_len:70 episode reward: total was -6.230000. running mean: -41.248684\n",
      "ep 1161: ep_len:500 episode reward: total was 2.690000. running mean: -40.809297\n",
      "ep 1161: ep_len:3 episode reward: total was -1.500000. running mean: -40.416204\n",
      "ep 1161: ep_len:652 episode reward: total was -26.420000. running mean: -40.276242\n",
      "ep 1161: ep_len:587 episode reward: total was -36.960000. running mean: -40.243080\n",
      "epsilon:0.148485 episode_count: 8134. steps_count: 3542643.000000\n",
      "Time elapsed:  10188.380739688873\n",
      "ep 1162: ep_len:242 episode reward: total was -24.310000. running mean: -40.083749\n",
      "ep 1162: ep_len:527 episode reward: total was -41.170000. running mean: -40.094612\n",
      "ep 1162: ep_len:508 episode reward: total was -93.310000. running mean: -40.626766\n",
      "ep 1162: ep_len:593 episode reward: total was -25.180000. running mean: -40.472298\n",
      "ep 1162: ep_len:85 episode reward: total was 7.730000. running mean: -39.990275\n",
      "ep 1162: ep_len:511 episode reward: total was -61.440000. running mean: -40.204772\n",
      "ep 1162: ep_len:530 episode reward: total was -63.420000. running mean: -40.436924\n",
      "epsilon:0.148440 episode_count: 8141. steps_count: 3545639.000000\n",
      "Time elapsed:  10197.352116823196\n",
      "ep 1163: ep_len:647 episode reward: total was -17.990000. running mean: -40.212455\n",
      "ep 1163: ep_len:572 episode reward: total was -19.810000. running mean: -40.008431\n",
      "ep 1163: ep_len:623 episode reward: total was -45.540000. running mean: -40.063746\n",
      "ep 1163: ep_len:118 episode reward: total was 2.990000. running mean: -39.633209\n",
      "ep 1163: ep_len:90 episode reward: total was 7.170000. running mean: -39.165177\n",
      "ep 1163: ep_len:500 episode reward: total was -47.650000. running mean: -39.250025\n",
      "ep 1163: ep_len:575 episode reward: total was -41.480000. running mean: -39.272325\n",
      "epsilon:0.148396 episode_count: 8148. steps_count: 3548764.000000\n",
      "Time elapsed:  10205.712954759598\n",
      "ep 1164: ep_len:219 episode reward: total was -22.460000. running mean: -39.104202\n",
      "ep 1164: ep_len:357 episode reward: total was -73.960000. running mean: -39.452760\n",
      "ep 1164: ep_len:68 episode reward: total was 3.180000. running mean: -39.026432\n",
      "ep 1164: ep_len:501 episode reward: total was -59.770000. running mean: -39.233868\n",
      "ep 1164: ep_len:116 episode reward: total was 12.310000. running mean: -38.718429\n",
      "ep 1164: ep_len:509 episode reward: total was -38.770000. running mean: -38.718945\n",
      "ep 1164: ep_len:508 episode reward: total was -93.970000. running mean: -39.271455\n",
      "epsilon:0.148352 episode_count: 8155. steps_count: 3551042.000000\n",
      "Time elapsed:  10212.120304584503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1165: ep_len:500 episode reward: total was 7.660000. running mean: -38.802141\n",
      "ep 1165: ep_len:500 episode reward: total was -44.010000. running mean: -38.854219\n",
      "ep 1165: ep_len:617 episode reward: total was -44.170000. running mean: -38.907377\n",
      "ep 1165: ep_len:127 episode reward: total was 2.070000. running mean: -38.497603\n",
      "ep 1165: ep_len:3 episode reward: total was 0.000000. running mean: -38.112627\n",
      "ep 1165: ep_len:518 episode reward: total was -61.170000. running mean: -38.343201\n",
      "ep 1165: ep_len:507 episode reward: total was -48.560000. running mean: -38.445369\n",
      "epsilon:0.148307 episode_count: 8162. steps_count: 3553814.000000\n",
      "Time elapsed:  10221.555788755417\n",
      "ep 1166: ep_len:622 episode reward: total was -38.940000. running mean: -38.450315\n",
      "ep 1166: ep_len:500 episode reward: total was -35.940000. running mean: -38.425212\n",
      "ep 1166: ep_len:647 episode reward: total was -134.380000. running mean: -39.384760\n",
      "ep 1166: ep_len:514 episode reward: total was -38.860000. running mean: -39.379512\n",
      "ep 1166: ep_len:3 episode reward: total was 0.000000. running mean: -38.985717\n",
      "ep 1166: ep_len:555 episode reward: total was -47.280000. running mean: -39.068660\n",
      "ep 1166: ep_len:536 episode reward: total was -67.840000. running mean: -39.356373\n",
      "epsilon:0.148263 episode_count: 8169. steps_count: 3557191.000000\n",
      "Time elapsed:  10231.7278444767\n",
      "ep 1167: ep_len:512 episode reward: total was -34.650000. running mean: -39.309310\n",
      "ep 1167: ep_len:591 episode reward: total was -50.720000. running mean: -39.423417\n",
      "ep 1167: ep_len:385 episode reward: total was -10.140000. running mean: -39.130582\n",
      "ep 1167: ep_len:407 episode reward: total was -5.080000. running mean: -38.790077\n",
      "ep 1167: ep_len:3 episode reward: total was -1.500000. running mean: -38.417176\n",
      "ep 1167: ep_len:503 episode reward: total was -19.320000. running mean: -38.226204\n",
      "ep 1167: ep_len:523 episode reward: total was -70.730000. running mean: -38.551242\n",
      "epsilon:0.148219 episode_count: 8176. steps_count: 3560115.000000\n",
      "Time elapsed:  10239.582817792892\n",
      "ep 1168: ep_len:501 episode reward: total was 24.510000. running mean: -37.920630\n",
      "ep 1168: ep_len:574 episode reward: total was -6.410000. running mean: -37.605523\n",
      "ep 1168: ep_len:531 episode reward: total was -70.140000. running mean: -37.930868\n",
      "ep 1168: ep_len:530 episode reward: total was -33.060000. running mean: -37.882159\n",
      "ep 1168: ep_len:3 episode reward: total was -1.500000. running mean: -37.518338\n",
      "ep 1168: ep_len:560 episode reward: total was -33.920000. running mean: -37.482354\n",
      "ep 1168: ep_len:506 episode reward: total was -104.110000. running mean: -38.148631\n",
      "epsilon:0.148174 episode_count: 8183. steps_count: 3563320.000000\n",
      "Time elapsed:  10249.171042919159\n",
      "ep 1169: ep_len:660 episode reward: total was -134.030000. running mean: -39.107445\n",
      "ep 1169: ep_len:287 episode reward: total was -63.660000. running mean: -39.352970\n",
      "ep 1169: ep_len:527 episode reward: total was -37.290000. running mean: -39.332340\n",
      "ep 1169: ep_len:547 episode reward: total was -88.710000. running mean: -39.826117\n",
      "ep 1169: ep_len:3 episode reward: total was 0.000000. running mean: -39.427856\n",
      "ep 1169: ep_len:536 episode reward: total was -49.740000. running mean: -39.530977\n",
      "ep 1169: ep_len:532 episode reward: total was -73.100000. running mean: -39.866668\n",
      "epsilon:0.148130 episode_count: 8190. steps_count: 3566412.000000\n",
      "Time elapsed:  10258.49515748024\n",
      "ep 1170: ep_len:590 episode reward: total was 9.080000. running mean: -39.377201\n",
      "ep 1170: ep_len:500 episode reward: total was -56.700000. running mean: -39.550429\n",
      "ep 1170: ep_len:461 episode reward: total was -0.880000. running mean: -39.163725\n",
      "ep 1170: ep_len:500 episode reward: total was -38.810000. running mean: -39.160187\n",
      "ep 1170: ep_len:106 episode reward: total was 16.270000. running mean: -38.605885\n",
      "ep 1170: ep_len:534 episode reward: total was -50.130000. running mean: -38.721127\n",
      "ep 1170: ep_len:586 episode reward: total was -67.800000. running mean: -39.011915\n",
      "epsilon:0.148086 episode_count: 8197. steps_count: 3569689.000000\n",
      "Time elapsed:  10267.974256038666\n",
      "ep 1171: ep_len:645 episode reward: total was -80.400000. running mean: -39.425796\n",
      "ep 1171: ep_len:517 episode reward: total was 1.110000. running mean: -39.020438\n",
      "ep 1171: ep_len:611 episode reward: total was -92.070000. running mean: -39.550934\n",
      "ep 1171: ep_len:518 episode reward: total was -60.700000. running mean: -39.762425\n",
      "ep 1171: ep_len:3 episode reward: total was -1.500000. running mean: -39.379800\n",
      "ep 1171: ep_len:668 episode reward: total was -54.640000. running mean: -39.532402\n",
      "ep 1171: ep_len:556 episode reward: total was -32.620000. running mean: -39.463278\n",
      "epsilon:0.148041 episode_count: 8204. steps_count: 3573207.000000\n",
      "Time elapsed:  10277.104794740677\n",
      "ep 1172: ep_len:543 episode reward: total was -11.540000. running mean: -39.184045\n",
      "ep 1172: ep_len:593 episode reward: total was -81.230000. running mean: -39.604505\n",
      "ep 1172: ep_len:621 episode reward: total was -103.380000. running mean: -40.242260\n",
      "ep 1172: ep_len:507 episode reward: total was -24.350000. running mean: -40.083337\n",
      "ep 1172: ep_len:100 episode reward: total was 18.750000. running mean: -39.495004\n",
      "ep 1172: ep_len:586 episode reward: total was -24.990000. running mean: -39.349954\n",
      "ep 1172: ep_len:567 episode reward: total was -45.840000. running mean: -39.414854\n",
      "epsilon:0.147997 episode_count: 8211. steps_count: 3576724.000000\n",
      "Time elapsed:  10286.181682348251\n",
      "ep 1173: ep_len:509 episode reward: total was 0.230000. running mean: -39.018406\n",
      "ep 1173: ep_len:506 episode reward: total was -25.030000. running mean: -38.878522\n",
      "ep 1173: ep_len:623 episode reward: total was -73.020000. running mean: -39.219937\n",
      "ep 1173: ep_len:566 episode reward: total was -70.890000. running mean: -39.536637\n",
      "ep 1173: ep_len:49 episode reward: total was 18.500000. running mean: -38.956271\n",
      "ep 1173: ep_len:500 episode reward: total was 0.170000. running mean: -38.565008\n",
      "ep 1173: ep_len:526 episode reward: total was -12.750000. running mean: -38.306858\n",
      "epsilon:0.147953 episode_count: 8218. steps_count: 3580003.000000\n",
      "Time elapsed:  10294.901890039444\n",
      "ep 1174: ep_len:563 episode reward: total was 25.490000. running mean: -37.668889\n",
      "ep 1174: ep_len:512 episode reward: total was -61.680000. running mean: -37.909001\n",
      "ep 1174: ep_len:500 episode reward: total was -22.640000. running mean: -37.756311\n",
      "ep 1174: ep_len:162 episode reward: total was 2.130000. running mean: -37.357447\n",
      "ep 1174: ep_len:3 episode reward: total was -1.500000. running mean: -36.998873\n",
      "ep 1174: ep_len:524 episode reward: total was -124.870000. running mean: -37.877584\n",
      "ep 1174: ep_len:207 episode reward: total was -30.640000. running mean: -37.805208\n",
      "epsilon:0.147908 episode_count: 8225. steps_count: 3582474.000000\n",
      "Time elapsed:  10301.697017908096\n",
      "ep 1175: ep_len:500 episode reward: total was 7.700000. running mean: -37.350156\n",
      "ep 1175: ep_len:647 episode reward: total was -64.350000. running mean: -37.620155\n",
      "ep 1175: ep_len:547 episode reward: total was -73.260000. running mean: -37.976553\n",
      "ep 1175: ep_len:515 episode reward: total was -40.160000. running mean: -37.998388\n",
      "ep 1175: ep_len:112 episode reward: total was 12.750000. running mean: -37.490904\n",
      "ep 1175: ep_len:500 episode reward: total was -77.070000. running mean: -37.886695\n",
      "ep 1175: ep_len:500 episode reward: total was -65.480000. running mean: -38.162628\n",
      "epsilon:0.147864 episode_count: 8232. steps_count: 3585795.000000\n",
      "Time elapsed:  10310.27313375473\n",
      "ep 1176: ep_len:237 episode reward: total was -4.340000. running mean: -37.824402\n",
      "ep 1176: ep_len:292 episode reward: total was -16.050000. running mean: -37.606658\n",
      "ep 1176: ep_len:553 episode reward: total was -69.680000. running mean: -37.927391\n",
      "ep 1176: ep_len:509 episode reward: total was -19.640000. running mean: -37.744517\n",
      "ep 1176: ep_len:3 episode reward: total was 0.000000. running mean: -37.367072\n",
      "ep 1176: ep_len:500 episode reward: total was -75.520000. running mean: -37.748601\n",
      "ep 1176: ep_len:600 episode reward: total was -42.200000. running mean: -37.793115\n",
      "epsilon:0.147820 episode_count: 8239. steps_count: 3588489.000000\n",
      "Time elapsed:  10317.606673240662\n",
      "ep 1177: ep_len:623 episode reward: total was -0.240000. running mean: -37.417584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1177: ep_len:501 episode reward: total was 1.000000. running mean: -37.033408\n",
      "ep 1177: ep_len:500 episode reward: total was -58.320000. running mean: -37.246274\n",
      "ep 1177: ep_len:504 episode reward: total was -35.730000. running mean: -37.231111\n",
      "ep 1177: ep_len:3 episode reward: total was -1.500000. running mean: -36.873800\n",
      "ep 1177: ep_len:500 episode reward: total was -22.080000. running mean: -36.725862\n",
      "ep 1177: ep_len:566 episode reward: total was -71.720000. running mean: -37.075804\n",
      "epsilon:0.147775 episode_count: 8246. steps_count: 3591686.000000\n",
      "Time elapsed:  10326.066425323486\n",
      "ep 1178: ep_len:580 episode reward: total was -27.480000. running mean: -36.979846\n",
      "ep 1178: ep_len:500 episode reward: total was -10.320000. running mean: -36.713247\n",
      "ep 1178: ep_len:501 episode reward: total was -34.840000. running mean: -36.694515\n",
      "ep 1178: ep_len:121 episode reward: total was -4.570000. running mean: -36.373269\n",
      "ep 1178: ep_len:3 episode reward: total was -1.500000. running mean: -36.024537\n",
      "ep 1178: ep_len:500 episode reward: total was -39.170000. running mean: -36.055991\n",
      "ep 1178: ep_len:598 episode reward: total was -18.960000. running mean: -35.885032\n",
      "epsilon:0.147731 episode_count: 8253. steps_count: 3594489.000000\n",
      "Time elapsed:  10334.562268257141\n",
      "ep 1179: ep_len:535 episode reward: total was 13.390000. running mean: -35.392281\n",
      "ep 1179: ep_len:319 episode reward: total was -36.490000. running mean: -35.403258\n",
      "ep 1179: ep_len:635 episode reward: total was -148.970000. running mean: -36.538926\n",
      "ep 1179: ep_len:338 episode reward: total was 3.080000. running mean: -36.142737\n",
      "ep 1179: ep_len:96 episode reward: total was -59.760000. running mean: -36.378909\n",
      "ep 1179: ep_len:636 episode reward: total was -16.740000. running mean: -36.182520\n",
      "ep 1179: ep_len:315 episode reward: total was -43.250000. running mean: -36.253195\n",
      "epsilon:0.147687 episode_count: 8260. steps_count: 3597363.000000\n",
      "Time elapsed:  10342.179510116577\n",
      "ep 1180: ep_len:625 episode reward: total was -3.920000. running mean: -35.929863\n",
      "ep 1180: ep_len:584 episode reward: total was -64.450000. running mean: -36.215064\n",
      "ep 1180: ep_len:612 episode reward: total was -70.000000. running mean: -36.552914\n",
      "ep 1180: ep_len:505 episode reward: total was -23.690000. running mean: -36.424285\n",
      "ep 1180: ep_len:3 episode reward: total was 0.000000. running mean: -36.060042\n",
      "ep 1180: ep_len:610 episode reward: total was -62.530000. running mean: -36.324741\n",
      "ep 1180: ep_len:508 episode reward: total was -75.270000. running mean: -36.714194\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.147642 episode_count: 8267. steps_count: 3600810.000000\n",
      "Time elapsed:  10356.15026307106\n",
      "ep 1181: ep_len:549 episode reward: total was -98.680000. running mean: -37.333852\n",
      "ep 1181: ep_len:353 episode reward: total was -20.660000. running mean: -37.167113\n",
      "ep 1181: ep_len:535 episode reward: total was -131.720000. running mean: -38.112642\n",
      "ep 1181: ep_len:625 episode reward: total was -15.650000. running mean: -37.888016\n",
      "ep 1181: ep_len:84 episode reward: total was 11.180000. running mean: -37.397336\n",
      "ep 1181: ep_len:598 episode reward: total was -93.770000. running mean: -37.961062\n",
      "ep 1181: ep_len:558 episode reward: total was -53.920000. running mean: -38.120652\n",
      "epsilon:0.147598 episode_count: 8274. steps_count: 3604112.000000\n",
      "Time elapsed:  10364.736181974411\n",
      "ep 1182: ep_len:559 episode reward: total was -40.540000. running mean: -38.144845\n",
      "ep 1182: ep_len:540 episode reward: total was -47.530000. running mean: -38.238697\n",
      "ep 1182: ep_len:500 episode reward: total was -35.610000. running mean: -38.212410\n",
      "ep 1182: ep_len:607 episode reward: total was -20.570000. running mean: -38.035986\n",
      "ep 1182: ep_len:79 episode reward: total was -37.310000. running mean: -38.028726\n",
      "ep 1182: ep_len:500 episode reward: total was -89.160000. running mean: -38.540039\n",
      "ep 1182: ep_len:510 episode reward: total was -115.970000. running mean: -39.314338\n",
      "epsilon:0.147554 episode_count: 8281. steps_count: 3607407.000000\n",
      "Time elapsed:  10373.492918252945\n",
      "ep 1183: ep_len:654 episode reward: total was -111.530000. running mean: -40.036495\n",
      "ep 1183: ep_len:508 episode reward: total was -55.090000. running mean: -40.187030\n",
      "ep 1183: ep_len:424 episode reward: total was -12.540000. running mean: -39.910560\n",
      "ep 1183: ep_len:526 episode reward: total was -4.150000. running mean: -39.552954\n",
      "ep 1183: ep_len:3 episode reward: total was 0.000000. running mean: -39.157424\n",
      "ep 1183: ep_len:562 episode reward: total was -76.780000. running mean: -39.533650\n",
      "ep 1183: ep_len:596 episode reward: total was -20.610000. running mean: -39.344414\n",
      "epsilon:0.147509 episode_count: 8288. steps_count: 3610680.000000\n",
      "Time elapsed:  10382.125646591187\n",
      "ep 1184: ep_len:659 episode reward: total was -98.690000. running mean: -39.937870\n",
      "ep 1184: ep_len:500 episode reward: total was -63.830000. running mean: -40.176791\n",
      "ep 1184: ep_len:70 episode reward: total was 2.710000. running mean: -39.747923\n",
      "ep 1184: ep_len:524 episode reward: total was -80.460000. running mean: -40.155044\n",
      "ep 1184: ep_len:84 episode reward: total was -31.250000. running mean: -40.065993\n",
      "ep 1184: ep_len:500 episode reward: total was -66.610000. running mean: -40.331433\n",
      "ep 1184: ep_len:500 episode reward: total was -138.250000. running mean: -41.310619\n",
      "epsilon:0.147465 episode_count: 8295. steps_count: 3613517.000000\n",
      "Time elapsed:  10389.816545724869\n",
      "ep 1185: ep_len:229 episode reward: total was -16.880000. running mean: -41.066313\n",
      "ep 1185: ep_len:500 episode reward: total was -54.520000. running mean: -41.200850\n",
      "ep 1185: ep_len:565 episode reward: total was -90.160000. running mean: -41.690441\n",
      "ep 1185: ep_len:500 episode reward: total was -41.570000. running mean: -41.689237\n",
      "ep 1185: ep_len:45 episode reward: total was 19.500000. running mean: -41.077344\n",
      "ep 1185: ep_len:627 episode reward: total was -80.180000. running mean: -41.468371\n",
      "ep 1185: ep_len:622 episode reward: total was -28.810000. running mean: -41.341787\n",
      "epsilon:0.147421 episode_count: 8302. steps_count: 3616605.000000\n",
      "Time elapsed:  10397.611090898514\n",
      "ep 1186: ep_len:256 episode reward: total was -3.350000. running mean: -40.961869\n",
      "ep 1186: ep_len:538 episode reward: total was -66.190000. running mean: -41.214151\n",
      "ep 1186: ep_len:500 episode reward: total was -12.470000. running mean: -40.926709\n",
      "ep 1186: ep_len:508 episode reward: total was -48.640000. running mean: -41.003842\n",
      "ep 1186: ep_len:3 episode reward: total was 0.000000. running mean: -40.593804\n",
      "ep 1186: ep_len:500 episode reward: total was -59.800000. running mean: -40.785866\n",
      "ep 1186: ep_len:500 episode reward: total was -39.020000. running mean: -40.768207\n",
      "epsilon:0.147376 episode_count: 8309. steps_count: 3619410.000000\n",
      "Time elapsed:  10404.874337911606\n",
      "ep 1187: ep_len:546 episode reward: total was -167.760000. running mean: -42.038125\n",
      "ep 1187: ep_len:351 episode reward: total was -83.360000. running mean: -42.451344\n",
      "ep 1187: ep_len:543 episode reward: total was -78.220000. running mean: -42.809030\n",
      "ep 1187: ep_len:502 episode reward: total was 7.830000. running mean: -42.302640\n",
      "ep 1187: ep_len:3 episode reward: total was -1.500000. running mean: -41.894614\n",
      "ep 1187: ep_len:520 episode reward: total was -63.460000. running mean: -42.110267\n",
      "ep 1187: ep_len:310 episode reward: total was -17.620000. running mean: -41.865365\n",
      "epsilon:0.147332 episode_count: 8316. steps_count: 3622185.000000\n",
      "Time elapsed:  10412.72458577156\n",
      "ep 1188: ep_len:500 episode reward: total was -151.750000. running mean: -42.964211\n",
      "ep 1188: ep_len:606 episode reward: total was -34.700000. running mean: -42.881569\n",
      "ep 1188: ep_len:605 episode reward: total was -51.180000. running mean: -42.964553\n",
      "ep 1188: ep_len:500 episode reward: total was -18.030000. running mean: -42.715208\n",
      "ep 1188: ep_len:3 episode reward: total was 0.000000. running mean: -42.288056\n",
      "ep 1188: ep_len:500 episode reward: total was -70.600000. running mean: -42.571175\n",
      "ep 1188: ep_len:601 episode reward: total was -62.970000. running mean: -42.775163\n",
      "epsilon:0.147288 episode_count: 8323. steps_count: 3625500.000000\n",
      "Time elapsed:  10420.966584444046\n",
      "ep 1189: ep_len:598 episode reward: total was -21.140000. running mean: -42.558812\n",
      "ep 1189: ep_len:606 episode reward: total was -98.740000. running mean: -43.120624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1189: ep_len:639 episode reward: total was -61.110000. running mean: -43.300517\n",
      "ep 1189: ep_len:501 episode reward: total was -12.030000. running mean: -42.987812\n",
      "ep 1189: ep_len:94 episode reward: total was 6.750000. running mean: -42.490434\n",
      "ep 1189: ep_len:501 episode reward: total was -2.390000. running mean: -42.089430\n",
      "ep 1189: ep_len:503 episode reward: total was -78.440000. running mean: -42.452935\n",
      "epsilon:0.147243 episode_count: 8330. steps_count: 3628942.000000\n",
      "Time elapsed:  10431.109462022781\n",
      "ep 1190: ep_len:560 episode reward: total was -19.450000. running mean: -42.222906\n",
      "ep 1190: ep_len:500 episode reward: total was -22.160000. running mean: -42.022277\n",
      "ep 1190: ep_len:628 episode reward: total was -77.930000. running mean: -42.381354\n",
      "ep 1190: ep_len:500 episode reward: total was -45.760000. running mean: -42.415141\n",
      "ep 1190: ep_len:3 episode reward: total was 0.000000. running mean: -41.990989\n",
      "ep 1190: ep_len:501 episode reward: total was -59.300000. running mean: -42.164079\n",
      "ep 1190: ep_len:508 episode reward: total was -77.280000. running mean: -42.515239\n",
      "epsilon:0.147199 episode_count: 8337. steps_count: 3632142.000000\n",
      "Time elapsed:  10439.402940273285\n",
      "ep 1191: ep_len:501 episode reward: total was -65.590000. running mean: -42.745986\n",
      "ep 1191: ep_len:285 episode reward: total was -14.870000. running mean: -42.467226\n",
      "ep 1191: ep_len:630 episode reward: total was -42.500000. running mean: -42.467554\n",
      "ep 1191: ep_len:562 episode reward: total was -20.810000. running mean: -42.250979\n",
      "ep 1191: ep_len:3 episode reward: total was 0.000000. running mean: -41.828469\n",
      "ep 1191: ep_len:734 episode reward: total was -229.330000. running mean: -43.703484\n",
      "ep 1191: ep_len:191 episode reward: total was -38.330000. running mean: -43.649749\n",
      "epsilon:0.147155 episode_count: 8344. steps_count: 3635048.000000\n",
      "Time elapsed:  10447.700129032135\n",
      "ep 1192: ep_len:540 episode reward: total was 16.960000. running mean: -43.043652\n",
      "ep 1192: ep_len:509 episode reward: total was -43.330000. running mean: -43.046515\n",
      "ep 1192: ep_len:545 episode reward: total was -74.380000. running mean: -43.359850\n",
      "ep 1192: ep_len:154 episode reward: total was -6.380000. running mean: -42.990052\n",
      "ep 1192: ep_len:3 episode reward: total was 0.000000. running mean: -42.560151\n",
      "ep 1192: ep_len:564 episode reward: total was -32.120000. running mean: -42.455750\n",
      "ep 1192: ep_len:535 episode reward: total was -65.360000. running mean: -42.684792\n",
      "epsilon:0.147110 episode_count: 8351. steps_count: 3637898.000000\n",
      "Time elapsed:  10455.363965034485\n",
      "ep 1193: ep_len:640 episode reward: total was -102.830000. running mean: -43.286244\n",
      "ep 1193: ep_len:500 episode reward: total was -18.260000. running mean: -43.035982\n",
      "ep 1193: ep_len:606 episode reward: total was -57.160000. running mean: -43.177222\n",
      "ep 1193: ep_len:517 episode reward: total was -33.720000. running mean: -43.082650\n",
      "ep 1193: ep_len:3 episode reward: total was 0.000000. running mean: -42.651823\n",
      "ep 1193: ep_len:652 episode reward: total was -82.930000. running mean: -43.054605\n",
      "ep 1193: ep_len:549 episode reward: total was -83.070000. running mean: -43.454759\n",
      "epsilon:0.147066 episode_count: 8358. steps_count: 3641365.000000\n",
      "Time elapsed:  10465.317525625229\n",
      "ep 1194: ep_len:561 episode reward: total was -98.560000. running mean: -44.005811\n",
      "ep 1194: ep_len:500 episode reward: total was 1.400000. running mean: -43.551753\n",
      "ep 1194: ep_len:607 episode reward: total was -43.600000. running mean: -43.552236\n",
      "ep 1194: ep_len:170 episode reward: total was -6.330000. running mean: -43.180013\n",
      "ep 1194: ep_len:3 episode reward: total was 0.000000. running mean: -42.748213\n",
      "ep 1194: ep_len:602 episode reward: total was -104.440000. running mean: -43.365131\n",
      "ep 1194: ep_len:531 episode reward: total was -105.620000. running mean: -43.987680\n",
      "epsilon:0.147022 episode_count: 8365. steps_count: 3644339.000000\n",
      "Time elapsed:  10473.31225848198\n",
      "ep 1195: ep_len:500 episode reward: total was -36.850000. running mean: -43.916303\n",
      "ep 1195: ep_len:511 episode reward: total was -85.550000. running mean: -44.332640\n",
      "ep 1195: ep_len:582 episode reward: total was -70.280000. running mean: -44.592113\n",
      "ep 1195: ep_len:142 episode reward: total was 2.540000. running mean: -44.120792\n",
      "ep 1195: ep_len:75 episode reward: total was -41.260000. running mean: -44.092184\n",
      "ep 1195: ep_len:511 episode reward: total was -41.900000. running mean: -44.070263\n",
      "ep 1195: ep_len:500 episode reward: total was -50.980000. running mean: -44.139360\n",
      "epsilon:0.146977 episode_count: 8372. steps_count: 3647160.000000\n",
      "Time elapsed:  10480.80569267273\n",
      "ep 1196: ep_len:178 episode reward: total was 1.990000. running mean: -43.678066\n",
      "ep 1196: ep_len:500 episode reward: total was -10.410000. running mean: -43.345386\n",
      "ep 1196: ep_len:367 episode reward: total was -4.720000. running mean: -42.959132\n",
      "ep 1196: ep_len:500 episode reward: total was -56.800000. running mean: -43.097541\n",
      "ep 1196: ep_len:95 episode reward: total was 19.220000. running mean: -42.474365\n",
      "ep 1196: ep_len:612 episode reward: total was -58.790000. running mean: -42.637521\n",
      "ep 1196: ep_len:542 episode reward: total was -81.230000. running mean: -43.023446\n",
      "epsilon:0.146933 episode_count: 8379. steps_count: 3649954.000000\n",
      "Time elapsed:  10488.365541696548\n",
      "ep 1197: ep_len:550 episode reward: total was -94.630000. running mean: -43.539512\n",
      "ep 1197: ep_len:516 episode reward: total was -29.130000. running mean: -43.395417\n",
      "ep 1197: ep_len:66 episode reward: total was -10.860000. running mean: -43.070062\n",
      "ep 1197: ep_len:504 episode reward: total was -3.270000. running mean: -42.672062\n",
      "ep 1197: ep_len:3 episode reward: total was 0.000000. running mean: -42.245341\n",
      "ep 1197: ep_len:620 episode reward: total was -65.340000. running mean: -42.476288\n",
      "ep 1197: ep_len:211 episode reward: total was -63.930000. running mean: -42.690825\n",
      "epsilon:0.146889 episode_count: 8386. steps_count: 3652424.000000\n",
      "Time elapsed:  10494.271102428436\n",
      "ep 1198: ep_len:252 episode reward: total was 0.310000. running mean: -42.260817\n",
      "ep 1198: ep_len:517 episode reward: total was -12.280000. running mean: -41.961009\n",
      "ep 1198: ep_len:379 episode reward: total was 6.790000. running mean: -41.473498\n",
      "ep 1198: ep_len:599 episode reward: total was -122.550000. running mean: -42.284263\n",
      "ep 1198: ep_len:3 episode reward: total was 0.000000. running mean: -41.861421\n",
      "ep 1198: ep_len:500 episode reward: total was -28.580000. running mean: -41.728607\n",
      "ep 1198: ep_len:321 episode reward: total was -64.390000. running mean: -41.955221\n",
      "epsilon:0.146844 episode_count: 8393. steps_count: 3654995.000000\n",
      "Time elapsed:  10501.256999731064\n",
      "ep 1199: ep_len:500 episode reward: total was -138.250000. running mean: -42.918168\n",
      "ep 1199: ep_len:500 episode reward: total was 25.970000. running mean: -42.229287\n",
      "ep 1199: ep_len:521 episode reward: total was -81.890000. running mean: -42.625894\n",
      "ep 1199: ep_len:523 episode reward: total was -29.620000. running mean: -42.495835\n",
      "ep 1199: ep_len:3 episode reward: total was 0.000000. running mean: -42.070877\n",
      "ep 1199: ep_len:643 episode reward: total was -47.240000. running mean: -42.122568\n",
      "ep 1199: ep_len:567 episode reward: total was -57.450000. running mean: -42.275842\n",
      "epsilon:0.146800 episode_count: 8400. steps_count: 3658252.000000\n",
      "Time elapsed:  10512.518104553223\n",
      "ep 1200: ep_len:500 episode reward: total was -66.010000. running mean: -42.513184\n",
      "ep 1200: ep_len:616 episode reward: total was -1.760000. running mean: -42.105652\n",
      "ep 1200: ep_len:528 episode reward: total was -42.940000. running mean: -42.113995\n",
      "ep 1200: ep_len:600 episode reward: total was -19.480000. running mean: -41.887655\n",
      "ep 1200: ep_len:3 episode reward: total was 0.000000. running mean: -41.468779\n",
      "ep 1200: ep_len:520 episode reward: total was -43.100000. running mean: -41.485091\n",
      "ep 1200: ep_len:283 episode reward: total was -39.070000. running mean: -41.460940\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.146756 episode_count: 8407. steps_count: 3661302.000000\n",
      "Time elapsed:  10525.331302404404\n",
      "ep 1201: ep_len:567 episode reward: total was -28.160000. running mean: -41.327931\n",
      "ep 1201: ep_len:556 episode reward: total was -85.080000. running mean: -41.765451\n",
      "ep 1201: ep_len:551 episode reward: total was -95.510000. running mean: -42.302897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1201: ep_len:533 episode reward: total was 0.450000. running mean: -41.875368\n",
      "ep 1201: ep_len:40 episode reward: total was 14.000000. running mean: -41.316614\n",
      "ep 1201: ep_len:501 episode reward: total was -57.960000. running mean: -41.483048\n",
      "ep 1201: ep_len:204 episode reward: total was -29.230000. running mean: -41.360518\n",
      "epsilon:0.146711 episode_count: 8414. steps_count: 3664254.000000\n",
      "Time elapsed:  10532.782527446747\n",
      "ep 1202: ep_len:619 episode reward: total was -40.200000. running mean: -41.348912\n",
      "ep 1202: ep_len:574 episode reward: total was -84.120000. running mean: -41.776623\n",
      "ep 1202: ep_len:75 episode reward: total was -10.250000. running mean: -41.461357\n",
      "ep 1202: ep_len:542 episode reward: total was -15.760000. running mean: -41.204344\n",
      "ep 1202: ep_len:3 episode reward: total was -1.500000. running mean: -40.807300\n",
      "ep 1202: ep_len:503 episode reward: total was -47.990000. running mean: -40.879127\n",
      "ep 1202: ep_len:554 episode reward: total was -34.380000. running mean: -40.814136\n",
      "epsilon:0.146667 episode_count: 8421. steps_count: 3667124.000000\n",
      "Time elapsed:  10541.703053236008\n",
      "ep 1203: ep_len:536 episode reward: total was -45.940000. running mean: -40.865394\n",
      "ep 1203: ep_len:500 episode reward: total was 15.670000. running mean: -40.300040\n",
      "ep 1203: ep_len:564 episode reward: total was -55.070000. running mean: -40.447740\n",
      "ep 1203: ep_len:520 episode reward: total was -56.350000. running mean: -40.606763\n",
      "ep 1203: ep_len:3 episode reward: total was 0.000000. running mean: -40.200695\n",
      "ep 1203: ep_len:513 episode reward: total was -17.190000. running mean: -39.970588\n",
      "ep 1203: ep_len:586 episode reward: total was -51.170000. running mean: -40.082582\n",
      "epsilon:0.146623 episode_count: 8428. steps_count: 3670346.000000\n",
      "Time elapsed:  10550.358569383621\n",
      "ep 1204: ep_len:500 episode reward: total was -6.940000. running mean: -39.751156\n",
      "ep 1204: ep_len:530 episode reward: total was -82.090000. running mean: -40.174545\n",
      "ep 1204: ep_len:537 episode reward: total was -70.850000. running mean: -40.481299\n",
      "ep 1204: ep_len:500 episode reward: total was -11.330000. running mean: -40.189786\n",
      "ep 1204: ep_len:124 episode reward: total was -13.670000. running mean: -39.924589\n",
      "ep 1204: ep_len:518 episode reward: total was -45.180000. running mean: -39.977143\n",
      "ep 1204: ep_len:564 episode reward: total was -111.610000. running mean: -40.693471\n",
      "epsilon:0.146578 episode_count: 8435. steps_count: 3673619.000000\n",
      "Time elapsed:  10558.897156476974\n",
      "ep 1205: ep_len:548 episode reward: total was -121.830000. running mean: -41.504837\n",
      "ep 1205: ep_len:197 episode reward: total was -46.410000. running mean: -41.553888\n",
      "ep 1205: ep_len:654 episode reward: total was -81.870000. running mean: -41.957049\n",
      "ep 1205: ep_len:528 episode reward: total was -25.820000. running mean: -41.795679\n",
      "ep 1205: ep_len:90 episode reward: total was -4.770000. running mean: -41.425422\n",
      "ep 1205: ep_len:530 episode reward: total was -52.110000. running mean: -41.532268\n",
      "ep 1205: ep_len:558 episode reward: total was -44.550000. running mean: -41.562445\n",
      "epsilon:0.146534 episode_count: 8442. steps_count: 3676724.000000\n",
      "Time elapsed:  10566.977500200272\n",
      "ep 1206: ep_len:664 episode reward: total was -156.240000. running mean: -42.709221\n",
      "ep 1206: ep_len:614 episode reward: total was -40.440000. running mean: -42.686528\n",
      "ep 1206: ep_len:613 episode reward: total was -72.070000. running mean: -42.980363\n",
      "ep 1206: ep_len:524 episode reward: total was -60.980000. running mean: -43.160360\n",
      "ep 1206: ep_len:3 episode reward: total was 0.000000. running mean: -42.728756\n",
      "ep 1206: ep_len:500 episode reward: total was -152.760000. running mean: -43.829068\n",
      "ep 1206: ep_len:562 episode reward: total was -46.840000. running mean: -43.859178\n",
      "epsilon:0.146490 episode_count: 8449. steps_count: 3680204.000000\n",
      "Time elapsed:  10577.544775724411\n",
      "ep 1207: ep_len:563 episode reward: total was -68.910000. running mean: -44.109686\n",
      "ep 1207: ep_len:545 episode reward: total was -52.040000. running mean: -44.188989\n",
      "ep 1207: ep_len:546 episode reward: total was -73.360000. running mean: -44.480699\n",
      "ep 1207: ep_len:603 episode reward: total was -33.560000. running mean: -44.371492\n",
      "ep 1207: ep_len:85 episode reward: total was 7.180000. running mean: -43.855977\n",
      "ep 1207: ep_len:500 episode reward: total was -53.920000. running mean: -43.956617\n",
      "ep 1207: ep_len:631 episode reward: total was -49.230000. running mean: -44.009351\n",
      "epsilon:0.146445 episode_count: 8456. steps_count: 3683677.000000\n",
      "Time elapsed:  10586.547658205032\n",
      "ep 1208: ep_len:196 episode reward: total was 7.060000. running mean: -43.498658\n",
      "ep 1208: ep_len:618 episode reward: total was -2.590000. running mean: -43.089571\n",
      "ep 1208: ep_len:532 episode reward: total was -88.960000. running mean: -43.548275\n",
      "ep 1208: ep_len:566 episode reward: total was -67.110000. running mean: -43.783893\n",
      "ep 1208: ep_len:3 episode reward: total was 0.000000. running mean: -43.346054\n",
      "ep 1208: ep_len:580 episode reward: total was -30.310000. running mean: -43.215693\n",
      "ep 1208: ep_len:515 episode reward: total was -139.780000. running mean: -44.181336\n",
      "epsilon:0.146401 episode_count: 8463. steps_count: 3686687.000000\n",
      "Time elapsed:  10594.57993721962\n",
      "ep 1209: ep_len:214 episode reward: total was -7.820000. running mean: -43.817723\n",
      "ep 1209: ep_len:500 episode reward: total was -30.150000. running mean: -43.681046\n",
      "ep 1209: ep_len:448 episode reward: total was -5.570000. running mean: -43.299935\n",
      "ep 1209: ep_len:508 episode reward: total was 6.700000. running mean: -42.799936\n",
      "ep 1209: ep_len:102 episode reward: total was 10.260000. running mean: -42.269337\n",
      "ep 1209: ep_len:253 episode reward: total was -2.680000. running mean: -41.873443\n",
      "ep 1209: ep_len:525 episode reward: total was -110.940000. running mean: -42.564109\n",
      "epsilon:0.146357 episode_count: 8470. steps_count: 3689237.000000\n",
      "Time elapsed:  10601.470048189163\n",
      "ep 1210: ep_len:500 episode reward: total was 7.060000. running mean: -42.067868\n",
      "ep 1210: ep_len:500 episode reward: total was -61.790000. running mean: -42.265089\n",
      "ep 1210: ep_len:673 episode reward: total was -75.430000. running mean: -42.596738\n",
      "ep 1210: ep_len:500 episode reward: total was 6.860000. running mean: -42.102171\n",
      "ep 1210: ep_len:3 episode reward: total was 0.000000. running mean: -41.681149\n",
      "ep 1210: ep_len:547 episode reward: total was -69.120000. running mean: -41.955538\n",
      "ep 1210: ep_len:560 episode reward: total was -40.330000. running mean: -41.939282\n",
      "epsilon:0.146312 episode_count: 8477. steps_count: 3692520.000000\n",
      "Time elapsed:  10610.307130098343\n",
      "ep 1211: ep_len:500 episode reward: total was 14.130000. running mean: -41.378589\n",
      "ep 1211: ep_len:530 episode reward: total was -72.670000. running mean: -41.691503\n",
      "ep 1211: ep_len:596 episode reward: total was -64.040000. running mean: -41.914988\n",
      "ep 1211: ep_len:46 episode reward: total was -2.800000. running mean: -41.523839\n",
      "ep 1211: ep_len:3 episode reward: total was 0.000000. running mean: -41.108600\n",
      "ep 1211: ep_len:542 episode reward: total was -88.240000. running mean: -41.579914\n",
      "ep 1211: ep_len:614 episode reward: total was -38.570000. running mean: -41.549815\n",
      "epsilon:0.146268 episode_count: 8484. steps_count: 3695351.000000\n",
      "Time elapsed:  10617.900494098663\n",
      "ep 1212: ep_len:216 episode reward: total was -4.830000. running mean: -41.182617\n",
      "ep 1212: ep_len:561 episode reward: total was -77.210000. running mean: -41.542891\n",
      "ep 1212: ep_len:574 episode reward: total was -13.120000. running mean: -41.258662\n",
      "ep 1212: ep_len:521 episode reward: total was -52.500000. running mean: -41.371075\n",
      "ep 1212: ep_len:84 episode reward: total was 5.760000. running mean: -40.899764\n",
      "ep 1212: ep_len:293 episode reward: total was -11.200000. running mean: -40.602767\n",
      "ep 1212: ep_len:552 episode reward: total was -61.680000. running mean: -40.813539\n",
      "epsilon:0.146224 episode_count: 8491. steps_count: 3698152.000000\n",
      "Time elapsed:  10623.3882522583\n",
      "ep 1213: ep_len:583 episode reward: total was -109.970000. running mean: -41.505104\n",
      "ep 1213: ep_len:510 episode reward: total was -60.390000. running mean: -41.693953\n",
      "ep 1213: ep_len:500 episode reward: total was -49.970000. running mean: -41.776713\n",
      "ep 1213: ep_len:517 episode reward: total was -48.070000. running mean: -41.839646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1213: ep_len:61 episode reward: total was 4.150000. running mean: -41.379750\n",
      "ep 1213: ep_len:557 episode reward: total was -144.620000. running mean: -42.412152\n",
      "ep 1213: ep_len:588 episode reward: total was -76.080000. running mean: -42.748831\n",
      "epsilon:0.146179 episode_count: 8498. steps_count: 3701468.000000\n",
      "Time elapsed:  10632.231747150421\n",
      "ep 1214: ep_len:589 episode reward: total was -30.320000. running mean: -42.624542\n",
      "ep 1214: ep_len:547 episode reward: total was -13.580000. running mean: -42.334097\n",
      "ep 1214: ep_len:562 episode reward: total was -72.720000. running mean: -42.637956\n",
      "ep 1214: ep_len:518 episode reward: total was -36.640000. running mean: -42.577976\n",
      "ep 1214: ep_len:3 episode reward: total was 0.000000. running mean: -42.152197\n",
      "ep 1214: ep_len:541 episode reward: total was -87.160000. running mean: -42.602275\n",
      "ep 1214: ep_len:500 episode reward: total was -65.550000. running mean: -42.831752\n",
      "epsilon:0.146135 episode_count: 8505. steps_count: 3704728.000000\n",
      "Time elapsed:  10640.893732309341\n",
      "ep 1215: ep_len:685 episode reward: total was -85.840000. running mean: -43.261834\n",
      "ep 1215: ep_len:500 episode reward: total was -20.160000. running mean: -43.030816\n",
      "ep 1215: ep_len:633 episode reward: total was -62.610000. running mean: -43.226608\n",
      "ep 1215: ep_len:500 episode reward: total was -56.400000. running mean: -43.358342\n",
      "ep 1215: ep_len:3 episode reward: total was 1.010000. running mean: -42.914658\n",
      "ep 1215: ep_len:526 episode reward: total was -87.260000. running mean: -43.358112\n",
      "ep 1215: ep_len:593 episode reward: total was -53.700000. running mean: -43.461531\n",
      "epsilon:0.146091 episode_count: 8512. steps_count: 3708168.000000\n",
      "Time elapsed:  10651.29135632515\n",
      "ep 1216: ep_len:527 episode reward: total was 7.860000. running mean: -42.948315\n",
      "ep 1216: ep_len:501 episode reward: total was -81.860000. running mean: -43.337432\n",
      "ep 1216: ep_len:650 episode reward: total was -44.770000. running mean: -43.351758\n",
      "ep 1216: ep_len:500 episode reward: total was 14.540000. running mean: -42.772840\n",
      "ep 1216: ep_len:133 episode reward: total was -2.680000. running mean: -42.371912\n",
      "ep 1216: ep_len:515 episode reward: total was -81.530000. running mean: -42.763493\n",
      "ep 1216: ep_len:528 episode reward: total was -53.740000. running mean: -42.873258\n",
      "epsilon:0.146046 episode_count: 8519. steps_count: 3711522.000000\n",
      "Time elapsed:  10660.008793115616\n",
      "ep 1217: ep_len:653 episode reward: total was -48.240000. running mean: -42.926925\n",
      "ep 1217: ep_len:560 episode reward: total was -90.090000. running mean: -43.398556\n",
      "ep 1217: ep_len:500 episode reward: total was -59.280000. running mean: -43.557370\n",
      "ep 1217: ep_len:500 episode reward: total was -72.210000. running mean: -43.843897\n",
      "ep 1217: ep_len:3 episode reward: total was -1.500000. running mean: -43.420458\n",
      "ep 1217: ep_len:505 episode reward: total was -87.790000. running mean: -43.864153\n",
      "ep 1217: ep_len:631 episode reward: total was -32.920000. running mean: -43.754712\n",
      "epsilon:0.146002 episode_count: 8526. steps_count: 3714874.000000\n",
      "Time elapsed:  10668.881599187851\n",
      "ep 1218: ep_len:189 episode reward: total was -13.580000. running mean: -43.452965\n",
      "ep 1218: ep_len:554 episode reward: total was -66.830000. running mean: -43.686735\n",
      "ep 1218: ep_len:535 episode reward: total was -44.000000. running mean: -43.689868\n",
      "ep 1218: ep_len:500 episode reward: total was -26.460000. running mean: -43.517569\n",
      "ep 1218: ep_len:3 episode reward: total was 0.000000. running mean: -43.082393\n",
      "ep 1218: ep_len:170 episode reward: total was -1.420000. running mean: -42.665769\n",
      "ep 1218: ep_len:512 episode reward: total was -65.060000. running mean: -42.889712\n",
      "epsilon:0.145958 episode_count: 8533. steps_count: 3717337.000000\n",
      "Time elapsed:  10675.505190372467\n",
      "ep 1219: ep_len:579 episode reward: total was -20.470000. running mean: -42.665514\n",
      "ep 1219: ep_len:183 episode reward: total was -25.190000. running mean: -42.490759\n",
      "ep 1219: ep_len:557 episode reward: total was -66.120000. running mean: -42.727052\n",
      "ep 1219: ep_len:540 episode reward: total was 13.270000. running mean: -42.167081\n",
      "ep 1219: ep_len:3 episode reward: total was 0.000000. running mean: -41.745410\n",
      "ep 1219: ep_len:518 episode reward: total was -71.130000. running mean: -42.039256\n",
      "ep 1219: ep_len:572 episode reward: total was -25.950000. running mean: -41.878364\n",
      "epsilon:0.145913 episode_count: 8540. steps_count: 3720289.000000\n",
      "Time elapsed:  10683.409806251526\n",
      "ep 1220: ep_len:538 episode reward: total was -93.250000. running mean: -42.392080\n",
      "ep 1220: ep_len:500 episode reward: total was -32.480000. running mean: -42.292959\n",
      "ep 1220: ep_len:52 episode reward: total was 7.120000. running mean: -41.798830\n",
      "ep 1220: ep_len:412 episode reward: total was -27.710000. running mean: -41.657941\n",
      "ep 1220: ep_len:3 episode reward: total was -1.500000. running mean: -41.256362\n",
      "ep 1220: ep_len:601 episode reward: total was -54.910000. running mean: -41.392898\n",
      "ep 1220: ep_len:600 episode reward: total was -27.420000. running mean: -41.253169\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.145869 episode_count: 8547. steps_count: 3722995.000000\n",
      "Time elapsed:  10694.173771381378\n",
      "ep 1221: ep_len:538 episode reward: total was -65.370000. running mean: -41.494338\n",
      "ep 1221: ep_len:545 episode reward: total was -31.970000. running mean: -41.399094\n",
      "ep 1221: ep_len:553 episode reward: total was -101.060000. running mean: -41.995703\n",
      "ep 1221: ep_len:500 episode reward: total was -3.550000. running mean: -41.611246\n",
      "ep 1221: ep_len:51 episode reward: total was 13.500000. running mean: -41.060134\n",
      "ep 1221: ep_len:500 episode reward: total was -63.050000. running mean: -41.280032\n",
      "ep 1221: ep_len:532 episode reward: total was -56.230000. running mean: -41.429532\n",
      "epsilon:0.145825 episode_count: 8554. steps_count: 3726214.000000\n",
      "Time elapsed:  10702.902816057205\n",
      "ep 1222: ep_len:588 episode reward: total was -5.480000. running mean: -41.070037\n",
      "ep 1222: ep_len:542 episode reward: total was -73.360000. running mean: -41.392936\n",
      "ep 1222: ep_len:537 episode reward: total was -64.040000. running mean: -41.619407\n",
      "ep 1222: ep_len:170 episode reward: total was 13.130000. running mean: -41.071913\n",
      "ep 1222: ep_len:87 episode reward: total was 12.250000. running mean: -40.538694\n",
      "ep 1222: ep_len:617 episode reward: total was -81.490000. running mean: -40.948207\n",
      "ep 1222: ep_len:549 episode reward: total was -78.740000. running mean: -41.326125\n",
      "epsilon:0.145780 episode_count: 8561. steps_count: 3729304.000000\n",
      "Time elapsed:  10711.559783935547\n",
      "ep 1223: ep_len:520 episode reward: total was -8.380000. running mean: -40.996664\n",
      "ep 1223: ep_len:590 episode reward: total was -59.880000. running mean: -41.185497\n",
      "ep 1223: ep_len:462 episode reward: total was -17.120000. running mean: -40.944842\n",
      "ep 1223: ep_len:518 episode reward: total was -40.010000. running mean: -40.935494\n",
      "ep 1223: ep_len:98 episode reward: total was 18.760000. running mean: -40.338539\n",
      "ep 1223: ep_len:618 episode reward: total was -18.430000. running mean: -40.119453\n",
      "ep 1223: ep_len:518 episode reward: total was -21.870000. running mean: -39.936959\n",
      "epsilon:0.145736 episode_count: 8568. steps_count: 3732628.000000\n",
      "Time elapsed:  10721.528272628784\n",
      "ep 1224: ep_len:238 episode reward: total was -3.440000. running mean: -39.571989\n",
      "ep 1224: ep_len:500 episode reward: total was -46.030000. running mean: -39.636569\n",
      "ep 1224: ep_len:63 episode reward: total was 7.690000. running mean: -39.163304\n",
      "ep 1224: ep_len:541 episode reward: total was -103.700000. running mean: -39.808671\n",
      "ep 1224: ep_len:3 episode reward: total was 0.000000. running mean: -39.410584\n",
      "ep 1224: ep_len:586 episode reward: total was -76.610000. running mean: -39.782578\n",
      "ep 1224: ep_len:255 episode reward: total was -31.320000. running mean: -39.697952\n",
      "epsilon:0.145692 episode_count: 8575. steps_count: 3734814.000000\n",
      "Time elapsed:  10728.120068073273\n",
      "ep 1225: ep_len:500 episode reward: total was 31.350000. running mean: -38.987473\n",
      "ep 1225: ep_len:531 episode reward: total was -73.750000. running mean: -39.335098\n",
      "ep 1225: ep_len:593 episode reward: total was -54.460000. running mean: -39.486347\n",
      "ep 1225: ep_len:500 episode reward: total was -18.400000. running mean: -39.275484\n",
      "ep 1225: ep_len:3 episode reward: total was 0.000000. running mean: -38.882729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1225: ep_len:155 episode reward: total was -8.490000. running mean: -38.578801\n",
      "ep 1225: ep_len:500 episode reward: total was -59.330000. running mean: -38.786313\n",
      "epsilon:0.145647 episode_count: 8582. steps_count: 3737596.000000\n",
      "Time elapsed:  10735.653561115265\n",
      "ep 1226: ep_len:601 episode reward: total was -107.290000. running mean: -39.471350\n",
      "ep 1226: ep_len:559 episode reward: total was -58.740000. running mean: -39.664037\n",
      "ep 1226: ep_len:79 episode reward: total was -0.720000. running mean: -39.274596\n",
      "ep 1226: ep_len:516 episode reward: total was -2.600000. running mean: -38.907850\n",
      "ep 1226: ep_len:3 episode reward: total was 0.000000. running mean: -38.518772\n",
      "ep 1226: ep_len:500 episode reward: total was -20.670000. running mean: -38.340284\n",
      "ep 1226: ep_len:500 episode reward: total was -20.520000. running mean: -38.162081\n",
      "epsilon:0.145603 episode_count: 8589. steps_count: 3740354.000000\n",
      "Time elapsed:  10743.234825134277\n",
      "ep 1227: ep_len:713 episode reward: total was -95.750000. running mean: -38.737961\n",
      "ep 1227: ep_len:513 episode reward: total was -70.200000. running mean: -39.052581\n",
      "ep 1227: ep_len:501 episode reward: total was -47.120000. running mean: -39.133255\n",
      "ep 1227: ep_len:520 episode reward: total was -6.680000. running mean: -38.808723\n",
      "ep 1227: ep_len:74 episode reward: total was 5.690000. running mean: -38.363735\n",
      "ep 1227: ep_len:577 episode reward: total was -37.150000. running mean: -38.351598\n",
      "ep 1227: ep_len:205 episode reward: total was -23.620000. running mean: -38.204282\n",
      "epsilon:0.145559 episode_count: 8596. steps_count: 3743457.000000\n",
      "Time elapsed:  10751.412243366241\n",
      "ep 1228: ep_len:500 episode reward: total was -17.870000. running mean: -38.000939\n",
      "ep 1228: ep_len:550 episode reward: total was -43.660000. running mean: -38.057530\n",
      "ep 1228: ep_len:580 episode reward: total was -73.310000. running mean: -38.410055\n",
      "ep 1228: ep_len:391 episode reward: total was -159.260000. running mean: -39.618554\n",
      "ep 1228: ep_len:3 episode reward: total was -1.500000. running mean: -39.237368\n",
      "ep 1228: ep_len:598 episode reward: total was -7.330000. running mean: -38.918295\n",
      "ep 1228: ep_len:549 episode reward: total was -73.860000. running mean: -39.267712\n",
      "epsilon:0.145514 episode_count: 8603. steps_count: 3746628.000000\n",
      "Time elapsed:  10759.187271118164\n",
      "ep 1229: ep_len:127 episode reward: total was -4.580000. running mean: -38.920835\n",
      "ep 1229: ep_len:199 episode reward: total was -39.960000. running mean: -38.931226\n",
      "ep 1229: ep_len:531 episode reward: total was -12.690000. running mean: -38.668814\n",
      "ep 1229: ep_len:372 episode reward: total was 3.190000. running mean: -38.250226\n",
      "ep 1229: ep_len:3 episode reward: total was -0.490000. running mean: -37.872624\n",
      "ep 1229: ep_len:629 episode reward: total was -44.870000. running mean: -37.942597\n",
      "ep 1229: ep_len:575 episode reward: total was -28.290000. running mean: -37.846071\n",
      "epsilon:0.145470 episode_count: 8610. steps_count: 3749064.000000\n",
      "Time elapsed:  10765.911926031113\n",
      "ep 1230: ep_len:602 episode reward: total was -38.780000. running mean: -37.855411\n",
      "ep 1230: ep_len:556 episode reward: total was 6.440000. running mean: -37.412457\n",
      "ep 1230: ep_len:622 episode reward: total was -49.840000. running mean: -37.536732\n",
      "ep 1230: ep_len:500 episode reward: total was -41.620000. running mean: -37.577565\n",
      "ep 1230: ep_len:3 episode reward: total was 1.010000. running mean: -37.191689\n",
      "ep 1230: ep_len:617 episode reward: total was -98.090000. running mean: -37.800672\n",
      "ep 1230: ep_len:558 episode reward: total was -37.110000. running mean: -37.793765\n",
      "epsilon:0.145426 episode_count: 8617. steps_count: 3752522.000000\n",
      "Time elapsed:  10774.691010475159\n",
      "ep 1231: ep_len:114 episode reward: total was -6.700000. running mean: -37.482828\n",
      "ep 1231: ep_len:500 episode reward: total was -13.490000. running mean: -37.242900\n",
      "ep 1231: ep_len:500 episode reward: total was -58.990000. running mean: -37.460371\n",
      "ep 1231: ep_len:117 episode reward: total was 10.500000. running mean: -36.980767\n",
      "ep 1231: ep_len:3 episode reward: total was 1.010000. running mean: -36.600859\n",
      "ep 1231: ep_len:532 episode reward: total was -45.310000. running mean: -36.687951\n",
      "ep 1231: ep_len:337 episode reward: total was -86.050000. running mean: -37.181571\n",
      "epsilon:0.145381 episode_count: 8624. steps_count: 3754625.000000\n",
      "Time elapsed:  10780.630270957947\n",
      "ep 1232: ep_len:545 episode reward: total was -50.540000. running mean: -37.315155\n",
      "ep 1232: ep_len:561 episode reward: total was -56.700000. running mean: -37.509004\n",
      "ep 1232: ep_len:611 episode reward: total was -37.560000. running mean: -37.509514\n",
      "ep 1232: ep_len:500 episode reward: total was -65.260000. running mean: -37.787019\n",
      "ep 1232: ep_len:3 episode reward: total was 0.000000. running mean: -37.409148\n",
      "ep 1232: ep_len:515 episode reward: total was -61.950000. running mean: -37.654557\n",
      "ep 1232: ep_len:596 episode reward: total was -139.350000. running mean: -38.671511\n",
      "epsilon:0.145337 episode_count: 8631. steps_count: 3757956.000000\n",
      "Time elapsed:  10790.658754825592\n",
      "ep 1233: ep_len:500 episode reward: total was -101.440000. running mean: -39.299196\n",
      "ep 1233: ep_len:543 episode reward: total was -55.980000. running mean: -39.466004\n",
      "ep 1233: ep_len:79 episode reward: total was -2.250000. running mean: -39.093844\n",
      "ep 1233: ep_len:552 episode reward: total was -72.110000. running mean: -39.424006\n",
      "ep 1233: ep_len:72 episode reward: total was 13.600000. running mean: -38.893766\n",
      "ep 1233: ep_len:555 episode reward: total was -80.610000. running mean: -39.310928\n",
      "ep 1233: ep_len:209 episode reward: total was -51.890000. running mean: -39.436719\n",
      "epsilon:0.145293 episode_count: 8638. steps_count: 3760466.000000\n",
      "Time elapsed:  10797.490862369537\n",
      "ep 1234: ep_len:500 episode reward: total was -13.020000. running mean: -39.172552\n",
      "ep 1234: ep_len:529 episode reward: total was -21.850000. running mean: -38.999326\n",
      "ep 1234: ep_len:76 episode reward: total was -5.310000. running mean: -38.662433\n",
      "ep 1234: ep_len:500 episode reward: total was -19.360000. running mean: -38.469409\n",
      "ep 1234: ep_len:3 episode reward: total was -1.500000. running mean: -38.099714\n",
      "ep 1234: ep_len:610 episode reward: total was -20.440000. running mean: -37.923117\n",
      "ep 1234: ep_len:519 episode reward: total was -20.530000. running mean: -37.749186\n",
      "epsilon:0.145248 episode_count: 8645. steps_count: 3763203.000000\n",
      "Time elapsed:  10804.827809810638\n",
      "ep 1235: ep_len:516 episode reward: total was 34.000000. running mean: -37.031694\n",
      "ep 1235: ep_len:500 episode reward: total was -22.430000. running mean: -36.885677\n",
      "ep 1235: ep_len:651 episode reward: total was -83.580000. running mean: -37.352621\n",
      "ep 1235: ep_len:528 episode reward: total was -20.480000. running mean: -37.183894\n",
      "ep 1235: ep_len:3 episode reward: total was 0.000000. running mean: -36.812055\n",
      "ep 1235: ep_len:509 episode reward: total was -88.610000. running mean: -37.330035\n",
      "ep 1235: ep_len:554 episode reward: total was -55.490000. running mean: -37.511635\n",
      "epsilon:0.145204 episode_count: 8652. steps_count: 3766464.000000\n",
      "Time elapsed:  10813.58328294754\n",
      "ep 1236: ep_len:122 episode reward: total was 1.490000. running mean: -37.121618\n",
      "ep 1236: ep_len:506 episode reward: total was -0.510000. running mean: -36.755502\n",
      "ep 1236: ep_len:574 episode reward: total was -56.900000. running mean: -36.956947\n",
      "ep 1236: ep_len:530 episode reward: total was -23.600000. running mean: -36.823378\n",
      "ep 1236: ep_len:69 episode reward: total was 6.130000. running mean: -36.393844\n",
      "ep 1236: ep_len:218 episode reward: total was 4.220000. running mean: -35.987705\n",
      "ep 1236: ep_len:585 episode reward: total was -63.870000. running mean: -36.266528\n",
      "epsilon:0.145160 episode_count: 8659. steps_count: 3769068.000000\n",
      "Time elapsed:  10820.690623521805\n",
      "ep 1237: ep_len:529 episode reward: total was -17.950000. running mean: -36.083363\n",
      "ep 1237: ep_len:500 episode reward: total was -50.890000. running mean: -36.231429\n",
      "ep 1237: ep_len:643 episode reward: total was -74.700000. running mean: -36.616115\n",
      "ep 1237: ep_len:589 episode reward: total was -17.980000. running mean: -36.429754\n",
      "ep 1237: ep_len:3 episode reward: total was 0.000000. running mean: -36.065456\n",
      "ep 1237: ep_len:547 episode reward: total was -74.040000. running mean: -36.445202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1237: ep_len:500 episode reward: total was -50.170000. running mean: -36.582450\n",
      "epsilon:0.145115 episode_count: 8666. steps_count: 3772379.000000\n",
      "Time elapsed:  10829.20532631874\n",
      "ep 1238: ep_len:547 episode reward: total was -26.000000. running mean: -36.476625\n",
      "ep 1238: ep_len:533 episode reward: total was 10.240000. running mean: -36.009459\n",
      "ep 1238: ep_len:548 episode reward: total was -95.620000. running mean: -36.605564\n",
      "ep 1238: ep_len:508 episode reward: total was 2.860000. running mean: -36.210909\n",
      "ep 1238: ep_len:129 episode reward: total was 11.360000. running mean: -35.735200\n",
      "ep 1238: ep_len:500 episode reward: total was -75.870000. running mean: -36.136548\n",
      "ep 1238: ep_len:508 episode reward: total was -51.840000. running mean: -36.293582\n",
      "epsilon:0.145071 episode_count: 8673. steps_count: 3775652.000000\n",
      "Time elapsed:  10837.76848936081\n",
      "ep 1239: ep_len:134 episode reward: total was -6.160000. running mean: -35.992246\n",
      "ep 1239: ep_len:509 episode reward: total was -92.860000. running mean: -36.560924\n",
      "ep 1239: ep_len:624 episode reward: total was -38.320000. running mean: -36.578515\n",
      "ep 1239: ep_len:625 episode reward: total was -34.410000. running mean: -36.556830\n",
      "ep 1239: ep_len:88 episode reward: total was 3.750000. running mean: -36.153761\n",
      "ep 1239: ep_len:621 episode reward: total was -22.350000. running mean: -36.015724\n",
      "ep 1239: ep_len:550 episode reward: total was -55.830000. running mean: -36.213866\n",
      "epsilon:0.145027 episode_count: 8680. steps_count: 3778803.000000\n",
      "Time elapsed:  10845.563615083694\n",
      "ep 1240: ep_len:510 episode reward: total was -108.010000. running mean: -36.931828\n",
      "ep 1240: ep_len:500 episode reward: total was -45.330000. running mean: -37.015809\n",
      "ep 1240: ep_len:678 episode reward: total was -66.470000. running mean: -37.310351\n",
      "ep 1240: ep_len:500 episode reward: total was -31.500000. running mean: -37.252248\n",
      "ep 1240: ep_len:3 episode reward: total was 0.000000. running mean: -36.879725\n",
      "ep 1240: ep_len:500 episode reward: total was -37.410000. running mean: -36.885028\n",
      "ep 1240: ep_len:534 episode reward: total was -56.250000. running mean: -37.078678\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.144982 episode_count: 8687. steps_count: 3782028.000000\n",
      "Time elapsed:  10859.821828842163\n",
      "ep 1241: ep_len:531 episode reward: total was -30.350000. running mean: -37.011391\n",
      "ep 1241: ep_len:500 episode reward: total was 10.480000. running mean: -36.536477\n",
      "ep 1241: ep_len:578 episode reward: total was -41.140000. running mean: -36.582512\n",
      "ep 1241: ep_len:506 episode reward: total was -32.480000. running mean: -36.541487\n",
      "ep 1241: ep_len:3 episode reward: total was -1.500000. running mean: -36.191072\n",
      "ep 1241: ep_len:298 episode reward: total was -13.290000. running mean: -35.962062\n",
      "ep 1241: ep_len:526 episode reward: total was -74.560000. running mean: -36.348041\n",
      "epsilon:0.144938 episode_count: 8694. steps_count: 3784970.000000\n",
      "Time elapsed:  10867.639130353928\n",
      "ep 1242: ep_len:856 episode reward: total was -423.950000. running mean: -40.224061\n",
      "ep 1242: ep_len:500 episode reward: total was -47.260000. running mean: -40.294420\n",
      "ep 1242: ep_len:515 episode reward: total was -54.070000. running mean: -40.432176\n",
      "ep 1242: ep_len:579 episode reward: total was -73.260000. running mean: -40.760454\n",
      "ep 1242: ep_len:3 episode reward: total was 0.000000. running mean: -40.352850\n",
      "ep 1242: ep_len:620 episode reward: total was -66.560000. running mean: -40.614921\n",
      "ep 1242: ep_len:578 episode reward: total was -103.290000. running mean: -41.241672\n",
      "epsilon:0.144894 episode_count: 8701. steps_count: 3788621.000000\n",
      "Time elapsed:  10877.241348266602\n",
      "ep 1243: ep_len:611 episode reward: total was 3.340000. running mean: -40.795855\n",
      "ep 1243: ep_len:545 episode reward: total was -121.630000. running mean: -41.604197\n",
      "ep 1243: ep_len:602 episode reward: total was -43.930000. running mean: -41.627455\n",
      "ep 1243: ep_len:56 episode reward: total was -12.690000. running mean: -41.338080\n",
      "ep 1243: ep_len:3 episode reward: total was 0.000000. running mean: -40.924699\n",
      "ep 1243: ep_len:537 episode reward: total was -49.230000. running mean: -41.007752\n",
      "ep 1243: ep_len:598 episode reward: total was -73.260000. running mean: -41.330275\n",
      "epsilon:0.144849 episode_count: 8708. steps_count: 3791573.000000\n",
      "Time elapsed:  10885.287041664124\n",
      "ep 1244: ep_len:221 episode reward: total was -10.320000. running mean: -41.020172\n",
      "ep 1244: ep_len:588 episode reward: total was 18.340000. running mean: -40.426570\n",
      "ep 1244: ep_len:577 episode reward: total was -65.670000. running mean: -40.679005\n",
      "ep 1244: ep_len:500 episode reward: total was -25.580000. running mean: -40.528015\n",
      "ep 1244: ep_len:78 episode reward: total was 11.760000. running mean: -40.005134\n",
      "ep 1244: ep_len:588 episode reward: total was 4.590000. running mean: -39.559183\n",
      "ep 1244: ep_len:616 episode reward: total was -61.840000. running mean: -39.781991\n",
      "epsilon:0.144805 episode_count: 8715. steps_count: 3794741.000000\n",
      "Time elapsed:  10893.527715921402\n",
      "ep 1245: ep_len:538 episode reward: total was -203.600000. running mean: -41.420171\n",
      "ep 1245: ep_len:504 episode reward: total was -31.060000. running mean: -41.316570\n",
      "ep 1245: ep_len:661 episode reward: total was -115.070000. running mean: -42.054104\n",
      "ep 1245: ep_len:500 episode reward: total was 11.950000. running mean: -41.514063\n",
      "ep 1245: ep_len:53 episode reward: total was 20.500000. running mean: -40.893922\n",
      "ep 1245: ep_len:588 episode reward: total was -34.430000. running mean: -40.829283\n",
      "ep 1245: ep_len:502 episode reward: total was -42.140000. running mean: -40.842390\n",
      "epsilon:0.144761 episode_count: 8722. steps_count: 3798087.000000\n",
      "Time elapsed:  10902.137529850006\n",
      "ep 1246: ep_len:638 episode reward: total was -35.140000. running mean: -40.785366\n",
      "ep 1246: ep_len:500 episode reward: total was 2.330000. running mean: -40.354213\n",
      "ep 1246: ep_len:507 episode reward: total was -73.160000. running mean: -40.682270\n",
      "ep 1246: ep_len:501 episode reward: total was -77.360000. running mean: -41.049048\n",
      "ep 1246: ep_len:104 episode reward: total was -25.750000. running mean: -40.896057\n",
      "ep 1246: ep_len:524 episode reward: total was -14.410000. running mean: -40.631197\n",
      "ep 1246: ep_len:211 episode reward: total was -36.630000. running mean: -40.591185\n",
      "epsilon:0.144716 episode_count: 8729. steps_count: 3801072.000000\n",
      "Time elapsed:  10910.066022634506\n",
      "ep 1247: ep_len:631 episode reward: total was -98.940000. running mean: -41.174673\n",
      "ep 1247: ep_len:500 episode reward: total was -8.020000. running mean: -40.843126\n",
      "ep 1247: ep_len:684 episode reward: total was -85.570000. running mean: -41.290395\n",
      "ep 1247: ep_len:524 episode reward: total was -8.100000. running mean: -40.958491\n",
      "ep 1247: ep_len:3 episode reward: total was 0.000000. running mean: -40.548906\n",
      "ep 1247: ep_len:567 episode reward: total was -11.840000. running mean: -40.261817\n",
      "ep 1247: ep_len:513 episode reward: total was -54.500000. running mean: -40.404199\n",
      "epsilon:0.144672 episode_count: 8736. steps_count: 3804494.000000\n",
      "Time elapsed:  10919.282464265823\n",
      "ep 1248: ep_len:500 episode reward: total was 58.200000. running mean: -39.418157\n",
      "ep 1248: ep_len:538 episode reward: total was -16.050000. running mean: -39.184475\n",
      "ep 1248: ep_len:630 episode reward: total was -191.020000. running mean: -40.702831\n",
      "ep 1248: ep_len:500 episode reward: total was -121.310000. running mean: -41.508902\n",
      "ep 1248: ep_len:81 episode reward: total was 7.230000. running mean: -41.021513\n",
      "ep 1248: ep_len:597 episode reward: total was -33.830000. running mean: -40.949598\n",
      "ep 1248: ep_len:599 episode reward: total was -49.920000. running mean: -41.039302\n",
      "epsilon:0.144628 episode_count: 8743. steps_count: 3807939.000000\n",
      "Time elapsed:  10928.379697084427\n",
      "ep 1249: ep_len:512 episode reward: total was -94.500000. running mean: -41.573909\n",
      "ep 1249: ep_len:513 episode reward: total was -12.870000. running mean: -41.286870\n",
      "ep 1249: ep_len:637 episode reward: total was -111.480000. running mean: -41.988801\n",
      "ep 1249: ep_len:56 episode reward: total was -17.220000. running mean: -41.741113\n",
      "ep 1249: ep_len:107 episode reward: total was -52.760000. running mean: -41.851302\n",
      "ep 1249: ep_len:500 episode reward: total was -23.850000. running mean: -41.671289\n",
      "ep 1249: ep_len:549 episode reward: total was -41.530000. running mean: -41.669876\n",
      "epsilon:0.144583 episode_count: 8750. steps_count: 3810813.000000\n",
      "Time elapsed:  10936.025580644608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1250: ep_len:573 episode reward: total was 7.500000. running mean: -41.178177\n",
      "ep 1250: ep_len:594 episode reward: total was -49.090000. running mean: -41.257296\n",
      "ep 1250: ep_len:516 episode reward: total was -63.150000. running mean: -41.476223\n",
      "ep 1250: ep_len:500 episode reward: total was -40.320000. running mean: -41.464660\n",
      "ep 1250: ep_len:119 episode reward: total was 22.750000. running mean: -40.822514\n",
      "ep 1250: ep_len:500 episode reward: total was -82.270000. running mean: -41.236989\n",
      "ep 1250: ep_len:500 episode reward: total was -61.160000. running mean: -41.436219\n",
      "epsilon:0.144539 episode_count: 8757. steps_count: 3814115.000000\n",
      "Time elapsed:  10944.94961476326\n",
      "ep 1251: ep_len:582 episode reward: total was -186.530000. running mean: -42.887157\n",
      "ep 1251: ep_len:500 episode reward: total was 2.930000. running mean: -42.428985\n",
      "ep 1251: ep_len:79 episode reward: total was 1.790000. running mean: -41.986795\n",
      "ep 1251: ep_len:393 episode reward: total was -37.080000. running mean: -41.937727\n",
      "ep 1251: ep_len:3 episode reward: total was -1.500000. running mean: -41.533350\n",
      "ep 1251: ep_len:508 episode reward: total was -16.400000. running mean: -41.282017\n",
      "ep 1251: ep_len:510 episode reward: total was -36.980000. running mean: -41.238996\n",
      "epsilon:0.144495 episode_count: 8764. steps_count: 3816690.000000\n",
      "Time elapsed:  10953.005742311478\n",
      "ep 1252: ep_len:501 episode reward: total was -29.160000. running mean: -41.118206\n",
      "ep 1252: ep_len:521 episode reward: total was -23.450000. running mean: -40.941524\n",
      "ep 1252: ep_len:517 episode reward: total was -72.460000. running mean: -41.256709\n",
      "ep 1252: ep_len:517 episode reward: total was 16.150000. running mean: -40.682642\n",
      "ep 1252: ep_len:55 episode reward: total was 17.000000. running mean: -40.105816\n",
      "ep 1252: ep_len:532 episode reward: total was -35.710000. running mean: -40.061857\n",
      "ep 1252: ep_len:500 episode reward: total was -114.130000. running mean: -40.802539\n",
      "epsilon:0.144450 episode_count: 8771. steps_count: 3819833.000000\n",
      "Time elapsed:  10963.809064388275\n",
      "ep 1253: ep_len:540 episode reward: total was -46.730000. running mean: -40.861813\n",
      "ep 1253: ep_len:528 episode reward: total was -8.360000. running mean: -40.536795\n",
      "ep 1253: ep_len:624 episode reward: total was -83.050000. running mean: -40.961927\n",
      "ep 1253: ep_len:501 episode reward: total was 0.930000. running mean: -40.543008\n",
      "ep 1253: ep_len:3 episode reward: total was 0.000000. running mean: -40.137578\n",
      "ep 1253: ep_len:318 episode reward: total was -25.580000. running mean: -39.992002\n",
      "ep 1253: ep_len:552 episode reward: total was -4.760000. running mean: -39.639682\n",
      "epsilon:0.144406 episode_count: 8778. steps_count: 3822899.000000\n",
      "Time elapsed:  10972.015112638474\n",
      "ep 1254: ep_len:101 episode reward: total was 2.850000. running mean: -39.214785\n",
      "ep 1254: ep_len:504 episode reward: total was -126.580000. running mean: -40.088438\n",
      "ep 1254: ep_len:63 episode reward: total was 4.110000. running mean: -39.646453\n",
      "ep 1254: ep_len:580 episode reward: total was -19.070000. running mean: -39.440689\n",
      "ep 1254: ep_len:3 episode reward: total was 0.000000. running mean: -39.046282\n",
      "ep 1254: ep_len:650 episode reward: total was -24.580000. running mean: -38.901619\n",
      "ep 1254: ep_len:507 episode reward: total was -84.230000. running mean: -39.354903\n",
      "epsilon:0.144362 episode_count: 8785. steps_count: 3825307.000000\n",
      "Time elapsed:  10978.580081224442\n",
      "ep 1255: ep_len:130 episode reward: total was 9.990000. running mean: -38.861454\n",
      "ep 1255: ep_len:500 episode reward: total was 0.430000. running mean: -38.468539\n",
      "ep 1255: ep_len:432 episode reward: total was -11.580000. running mean: -38.199654\n",
      "ep 1255: ep_len:160 episode reward: total was -4.020000. running mean: -37.857857\n",
      "ep 1255: ep_len:83 episode reward: total was 20.080000. running mean: -37.278479\n",
      "ep 1255: ep_len:550 episode reward: total was -88.670000. running mean: -37.792394\n",
      "ep 1255: ep_len:508 episode reward: total was -80.890000. running mean: -38.223370\n",
      "epsilon:0.144317 episode_count: 8792. steps_count: 3827670.000000\n",
      "Time elapsed:  10985.050536632538\n",
      "ep 1256: ep_len:560 episode reward: total was -110.810000. running mean: -38.949236\n",
      "ep 1256: ep_len:201 episode reward: total was -13.100000. running mean: -38.690744\n",
      "ep 1256: ep_len:652 episode reward: total was -61.530000. running mean: -38.919136\n",
      "ep 1256: ep_len:505 episode reward: total was -70.600000. running mean: -39.235945\n",
      "ep 1256: ep_len:89 episode reward: total was 12.730000. running mean: -38.716286\n",
      "ep 1256: ep_len:646 episode reward: total was -24.400000. running mean: -38.573123\n",
      "ep 1256: ep_len:503 episode reward: total was -68.230000. running mean: -38.869692\n",
      "epsilon:0.144273 episode_count: 8799. steps_count: 3830826.000000\n",
      "Time elapsed:  10992.720345020294\n",
      "ep 1257: ep_len:576 episode reward: total was 18.180000. running mean: -38.299195\n",
      "ep 1257: ep_len:500 episode reward: total was -6.240000. running mean: -37.978603\n",
      "ep 1257: ep_len:615 episode reward: total was -60.230000. running mean: -38.201117\n",
      "ep 1257: ep_len:542 episode reward: total was -3.210000. running mean: -37.851205\n",
      "ep 1257: ep_len:68 episode reward: total was -35.890000. running mean: -37.831593\n",
      "ep 1257: ep_len:504 episode reward: total was -65.590000. running mean: -38.109177\n",
      "ep 1257: ep_len:500 episode reward: total was -61.700000. running mean: -38.345086\n",
      "epsilon:0.144229 episode_count: 8806. steps_count: 3834131.000000\n",
      "Time elapsed:  11001.285603761673\n",
      "ep 1258: ep_len:576 episode reward: total was 3.550000. running mean: -37.926135\n",
      "ep 1258: ep_len:595 episode reward: total was -34.960000. running mean: -37.896474\n",
      "ep 1258: ep_len:613 episode reward: total was -58.980000. running mean: -38.107309\n",
      "ep 1258: ep_len:500 episode reward: total was -42.990000. running mean: -38.156136\n",
      "ep 1258: ep_len:3 episode reward: total was -1.500000. running mean: -37.789574\n",
      "ep 1258: ep_len:500 episode reward: total was -28.880000. running mean: -37.700479\n",
      "ep 1258: ep_len:211 episode reward: total was -26.040000. running mean: -37.583874\n",
      "epsilon:0.144184 episode_count: 8813. steps_count: 3837129.000000\n",
      "Time elapsed:  11010.627152442932\n",
      "ep 1259: ep_len:533 episode reward: total was -5.140000. running mean: -37.259435\n",
      "ep 1259: ep_len:500 episode reward: total was -6.710000. running mean: -36.953941\n",
      "ep 1259: ep_len:518 episode reward: total was -59.780000. running mean: -37.182201\n",
      "ep 1259: ep_len:500 episode reward: total was -10.090000. running mean: -36.911279\n",
      "ep 1259: ep_len:48 episode reward: total was 13.500000. running mean: -36.407167\n",
      "ep 1259: ep_len:680 episode reward: total was -35.660000. running mean: -36.399695\n",
      "ep 1259: ep_len:563 episode reward: total was -53.750000. running mean: -36.573198\n",
      "epsilon:0.144140 episode_count: 8820. steps_count: 3840471.000000\n",
      "Time elapsed:  11019.467475652695\n",
      "ep 1260: ep_len:206 episode reward: total was 0.520000. running mean: -36.202266\n",
      "ep 1260: ep_len:500 episode reward: total was -16.280000. running mean: -36.003043\n",
      "ep 1260: ep_len:570 episode reward: total was -57.360000. running mean: -36.216613\n",
      "ep 1260: ep_len:500 episode reward: total was 30.350000. running mean: -35.550947\n",
      "ep 1260: ep_len:3 episode reward: total was -1.500000. running mean: -35.210437\n",
      "ep 1260: ep_len:590 episode reward: total was -55.440000. running mean: -35.412733\n",
      "ep 1260: ep_len:290 episode reward: total was -52.300000. running mean: -35.581606\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.144096 episode_count: 8827. steps_count: 3843130.000000\n",
      "Time elapsed:  11031.464069366455\n",
      "ep 1261: ep_len:552 episode reward: total was -1.250000. running mean: -35.238289\n",
      "ep 1261: ep_len:555 episode reward: total was -48.130000. running mean: -35.367207\n",
      "ep 1261: ep_len:559 episode reward: total was -41.680000. running mean: -35.430335\n",
      "ep 1261: ep_len:56 episode reward: total was -4.690000. running mean: -35.122931\n",
      "ep 1261: ep_len:50 episode reward: total was 17.500000. running mean: -34.596702\n",
      "ep 1261: ep_len:610 episode reward: total was -39.500000. running mean: -34.645735\n",
      "ep 1261: ep_len:627 episode reward: total was -54.600000. running mean: -34.845277\n",
      "epsilon:0.144051 episode_count: 8834. steps_count: 3846139.000000\n",
      "Time elapsed:  11040.801409244537\n",
      "ep 1262: ep_len:661 episode reward: total was -91.930000. running mean: -35.416125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1262: ep_len:336 episode reward: total was -49.360000. running mean: -35.555563\n",
      "ep 1262: ep_len:538 episode reward: total was -51.970000. running mean: -35.719708\n",
      "ep 1262: ep_len:605 episode reward: total was 0.680000. running mean: -35.355711\n",
      "ep 1262: ep_len:125 episode reward: total was 12.300000. running mean: -34.879154\n",
      "ep 1262: ep_len:626 episode reward: total was -118.220000. running mean: -35.712562\n",
      "ep 1262: ep_len:302 episode reward: total was -63.940000. running mean: -35.994836\n",
      "epsilon:0.144007 episode_count: 8841. steps_count: 3849332.000000\n",
      "Time elapsed:  11051.903001785278\n",
      "ep 1263: ep_len:595 episode reward: total was -32.300000. running mean: -35.957888\n",
      "ep 1263: ep_len:517 episode reward: total was 10.930000. running mean: -35.489009\n",
      "ep 1263: ep_len:373 episode reward: total was -7.170000. running mean: -35.205819\n",
      "ep 1263: ep_len:117 episode reward: total was -15.660000. running mean: -35.010361\n",
      "ep 1263: ep_len:3 episode reward: total was -1.500000. running mean: -34.675257\n",
      "ep 1263: ep_len:714 episode reward: total was -54.390000. running mean: -34.872405\n",
      "ep 1263: ep_len:580 episode reward: total was -58.700000. running mean: -35.110681\n",
      "epsilon:0.143963 episode_count: 8848. steps_count: 3852231.000000\n",
      "Time elapsed:  11059.71877026558\n",
      "ep 1264: ep_len:500 episode reward: total was -7.430000. running mean: -34.833874\n",
      "ep 1264: ep_len:500 episode reward: total was 41.640000. running mean: -34.069135\n",
      "ep 1264: ep_len:547 episode reward: total was -63.070000. running mean: -34.359144\n",
      "ep 1264: ep_len:500 episode reward: total was -25.840000. running mean: -34.273952\n",
      "ep 1264: ep_len:3 episode reward: total was 1.010000. running mean: -33.921113\n",
      "ep 1264: ep_len:251 episode reward: total was -0.770000. running mean: -33.589602\n",
      "ep 1264: ep_len:335 episode reward: total was -30.730000. running mean: -33.561006\n",
      "epsilon:0.143918 episode_count: 8855. steps_count: 3854867.000000\n",
      "Time elapsed:  11066.961814880371\n",
      "ep 1265: ep_len:646 episode reward: total was -38.580000. running mean: -33.611196\n",
      "ep 1265: ep_len:289 episode reward: total was -27.380000. running mean: -33.548884\n",
      "ep 1265: ep_len:558 episode reward: total was -97.270000. running mean: -34.186095\n",
      "ep 1265: ep_len:531 episode reward: total was -31.400000. running mean: -34.158234\n",
      "ep 1265: ep_len:48 episode reward: total was 18.000000. running mean: -33.636652\n",
      "ep 1265: ep_len:500 episode reward: total was -67.810000. running mean: -33.978385\n",
      "ep 1265: ep_len:500 episode reward: total was -42.190000. running mean: -34.060501\n",
      "epsilon:0.143874 episode_count: 8862. steps_count: 3857939.000000\n",
      "Time elapsed:  11074.883178710938\n",
      "ep 1266: ep_len:507 episode reward: total was -85.710000. running mean: -34.576996\n",
      "ep 1266: ep_len:582 episode reward: total was -20.580000. running mean: -34.437026\n",
      "ep 1266: ep_len:529 episode reward: total was -44.340000. running mean: -34.536056\n",
      "ep 1266: ep_len:563 episode reward: total was -23.900000. running mean: -34.429695\n",
      "ep 1266: ep_len:3 episode reward: total was 0.000000. running mean: -34.085398\n",
      "ep 1266: ep_len:549 episode reward: total was -50.770000. running mean: -34.252244\n",
      "ep 1266: ep_len:563 episode reward: total was -83.770000. running mean: -34.747422\n",
      "epsilon:0.143830 episode_count: 8869. steps_count: 3861235.000000\n",
      "Time elapsed:  11083.652420282364\n",
      "ep 1267: ep_len:207 episode reward: total was -11.870000. running mean: -34.518648\n",
      "ep 1267: ep_len:603 episode reward: total was -73.580000. running mean: -34.909261\n",
      "ep 1267: ep_len:327 episode reward: total was -16.370000. running mean: -34.723869\n",
      "ep 1267: ep_len:519 episode reward: total was -63.380000. running mean: -35.010430\n",
      "ep 1267: ep_len:89 episode reward: total was 7.710000. running mean: -34.583226\n",
      "ep 1267: ep_len:674 episode reward: total was -44.540000. running mean: -34.682793\n",
      "ep 1267: ep_len:554 episode reward: total was -6.470000. running mean: -34.400666\n",
      "epsilon:0.143785 episode_count: 8876. steps_count: 3864208.000000\n",
      "Time elapsed:  11091.103749752045\n",
      "ep 1268: ep_len:501 episode reward: total was 35.980000. running mean: -33.696859\n",
      "ep 1268: ep_len:618 episode reward: total was 0.420000. running mean: -33.355690\n",
      "ep 1268: ep_len:540 episode reward: total was -38.050000. running mean: -33.402633\n",
      "ep 1268: ep_len:145 episode reward: total was -8.950000. running mean: -33.158107\n",
      "ep 1268: ep_len:3 episode reward: total was 0.000000. running mean: -32.826526\n",
      "ep 1268: ep_len:540 episode reward: total was 7.450000. running mean: -32.423761\n",
      "ep 1268: ep_len:500 episode reward: total was -22.610000. running mean: -32.325623\n",
      "epsilon:0.143741 episode_count: 8883. steps_count: 3867055.000000\n",
      "Time elapsed:  11098.76151061058\n",
      "ep 1269: ep_len:500 episode reward: total was -65.240000. running mean: -32.654767\n",
      "ep 1269: ep_len:500 episode reward: total was 1.420000. running mean: -32.314019\n",
      "ep 1269: ep_len:457 episode reward: total was 10.100000. running mean: -31.889879\n",
      "ep 1269: ep_len:563 episode reward: total was -7.210000. running mean: -31.643080\n",
      "ep 1269: ep_len:3 episode reward: total was 0.000000. running mean: -31.326649\n",
      "ep 1269: ep_len:195 episode reward: total was 18.740000. running mean: -30.825983\n",
      "ep 1269: ep_len:557 episode reward: total was -56.640000. running mean: -31.084123\n",
      "epsilon:0.143697 episode_count: 8890. steps_count: 3869830.000000\n",
      "Time elapsed:  11106.318381786346\n",
      "ep 1270: ep_len:608 episode reward: total was -84.850000. running mean: -31.621782\n",
      "ep 1270: ep_len:634 episode reward: total was 7.140000. running mean: -31.234164\n",
      "ep 1270: ep_len:564 episode reward: total was -64.120000. running mean: -31.563022\n",
      "ep 1270: ep_len:513 episode reward: total was -6.810000. running mean: -31.315492\n",
      "ep 1270: ep_len:3 episode reward: total was 0.000000. running mean: -31.002337\n",
      "ep 1270: ep_len:500 episode reward: total was -59.870000. running mean: -31.291014\n",
      "ep 1270: ep_len:536 episode reward: total was -55.960000. running mean: -31.537704\n",
      "epsilon:0.143652 episode_count: 8897. steps_count: 3873188.000000\n",
      "Time elapsed:  11115.273975133896\n",
      "ep 1271: ep_len:197 episode reward: total was -0.400000. running mean: -31.226327\n",
      "ep 1271: ep_len:344 episode reward: total was -48.910000. running mean: -31.403163\n",
      "ep 1271: ep_len:359 episode reward: total was 0.740000. running mean: -31.081732\n",
      "ep 1271: ep_len:500 episode reward: total was -67.830000. running mean: -31.449215\n",
      "ep 1271: ep_len:23 episode reward: total was 10.000000. running mean: -31.034722\n",
      "ep 1271: ep_len:502 episode reward: total was -60.650000. running mean: -31.330875\n",
      "ep 1271: ep_len:521 episode reward: total was -69.690000. running mean: -31.714466\n",
      "epsilon:0.143608 episode_count: 8904. steps_count: 3875634.000000\n",
      "Time elapsed:  11121.909712553024\n",
      "ep 1272: ep_len:500 episode reward: total was 6.130000. running mean: -31.336022\n",
      "ep 1272: ep_len:177 episode reward: total was -23.900000. running mean: -31.261662\n",
      "ep 1272: ep_len:569 episode reward: total was -39.090000. running mean: -31.339945\n",
      "ep 1272: ep_len:591 episode reward: total was 1.000000. running mean: -31.016545\n",
      "ep 1272: ep_len:3 episode reward: total was -1.500000. running mean: -30.721380\n",
      "ep 1272: ep_len:517 episode reward: total was -96.230000. running mean: -31.376466\n",
      "ep 1272: ep_len:600 episode reward: total was -49.850000. running mean: -31.561202\n",
      "epsilon:0.143564 episode_count: 8911. steps_count: 3878591.000000\n",
      "Time elapsed:  11130.625241994858\n",
      "ep 1273: ep_len:566 episode reward: total was -4.740000. running mean: -31.292990\n",
      "ep 1273: ep_len:279 episode reward: total was -85.430000. running mean: -31.834360\n",
      "ep 1273: ep_len:404 episode reward: total was -13.150000. running mean: -31.647516\n",
      "ep 1273: ep_len:598 episode reward: total was -149.340000. running mean: -32.824441\n",
      "ep 1273: ep_len:3 episode reward: total was 0.000000. running mean: -32.496196\n",
      "ep 1273: ep_len:522 episode reward: total was -57.740000. running mean: -32.748634\n",
      "ep 1273: ep_len:310 episode reward: total was -60.800000. running mean: -33.029148\n",
      "epsilon:0.143519 episode_count: 8918. steps_count: 3881273.000000\n",
      "Time elapsed:  11138.008261919022\n",
      "ep 1274: ep_len:562 episode reward: total was -4.150000. running mean: -32.740357\n",
      "ep 1274: ep_len:611 episode reward: total was -3.640000. running mean: -32.449353\n",
      "ep 1274: ep_len:552 episode reward: total was -100.730000. running mean: -33.132160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1274: ep_len:501 episode reward: total was -29.070000. running mean: -33.091538\n",
      "ep 1274: ep_len:83 episode reward: total was -4.840000. running mean: -32.809023\n",
      "ep 1274: ep_len:529 episode reward: total was -43.880000. running mean: -32.919732\n",
      "ep 1274: ep_len:500 episode reward: total was -146.570000. running mean: -34.056235\n",
      "epsilon:0.143475 episode_count: 8925. steps_count: 3884611.000000\n",
      "Time elapsed:  11146.68254494667\n",
      "ep 1275: ep_len:506 episode reward: total was -29.480000. running mean: -34.010473\n",
      "ep 1275: ep_len:500 episode reward: total was -15.250000. running mean: -33.822868\n",
      "ep 1275: ep_len:500 episode reward: total was -49.590000. running mean: -33.980539\n",
      "ep 1275: ep_len:130 episode reward: total was 10.550000. running mean: -33.535234\n",
      "ep 1275: ep_len:3 episode reward: total was 0.000000. running mean: -33.199882\n",
      "ep 1275: ep_len:599 episode reward: total was -27.120000. running mean: -33.139083\n",
      "ep 1275: ep_len:500 episode reward: total was -31.410000. running mean: -33.121792\n",
      "epsilon:0.143431 episode_count: 8932. steps_count: 3887349.000000\n",
      "Time elapsed:  11153.971816062927\n",
      "ep 1276: ep_len:626 episode reward: total was -21.100000. running mean: -33.001574\n",
      "ep 1276: ep_len:581 episode reward: total was -122.210000. running mean: -33.893658\n",
      "ep 1276: ep_len:579 episode reward: total was -86.330000. running mean: -34.418022\n",
      "ep 1276: ep_len:500 episode reward: total was 7.660000. running mean: -33.997241\n",
      "ep 1276: ep_len:127 episode reward: total was 0.330000. running mean: -33.653969\n",
      "ep 1276: ep_len:558 episode reward: total was -80.370000. running mean: -34.121129\n",
      "ep 1276: ep_len:555 episode reward: total was -47.710000. running mean: -34.257018\n",
      "epsilon:0.143386 episode_count: 8939. steps_count: 3890875.000000\n",
      "Time elapsed:  11163.192269325256\n",
      "ep 1277: ep_len:609 episode reward: total was -38.650000. running mean: -34.300948\n",
      "ep 1277: ep_len:500 episode reward: total was -39.810000. running mean: -34.356038\n",
      "ep 1277: ep_len:566 episode reward: total was -170.370000. running mean: -35.716178\n",
      "ep 1277: ep_len:525 episode reward: total was -8.890000. running mean: -35.447916\n",
      "ep 1277: ep_len:81 episode reward: total was 6.680000. running mean: -35.026637\n",
      "ep 1277: ep_len:500 episode reward: total was -41.610000. running mean: -35.092471\n",
      "ep 1277: ep_len:589 episode reward: total was -17.730000. running mean: -34.918846\n",
      "epsilon:0.143342 episode_count: 8946. steps_count: 3894245.000000\n",
      "Time elapsed:  11172.0751247406\n",
      "ep 1278: ep_len:256 episode reward: total was 3.300000. running mean: -34.536658\n",
      "ep 1278: ep_len:535 episode reward: total was -81.540000. running mean: -35.006691\n",
      "ep 1278: ep_len:764 episode reward: total was -290.750000. running mean: -37.564124\n",
      "ep 1278: ep_len:83 episode reward: total was 3.270000. running mean: -37.155783\n",
      "ep 1278: ep_len:3 episode reward: total was 0.000000. running mean: -36.784225\n",
      "ep 1278: ep_len:255 episode reward: total was 6.400000. running mean: -36.352383\n",
      "ep 1278: ep_len:629 episode reward: total was -35.480000. running mean: -36.343659\n",
      "epsilon:0.143298 episode_count: 8953. steps_count: 3896770.000000\n",
      "Time elapsed:  11178.161374807358\n",
      "ep 1279: ep_len:532 episode reward: total was -27.320000. running mean: -36.253422\n",
      "ep 1279: ep_len:503 episode reward: total was -3.980000. running mean: -35.930688\n",
      "ep 1279: ep_len:500 episode reward: total was -72.840000. running mean: -36.299781\n",
      "ep 1279: ep_len:385 episode reward: total was -98.540000. running mean: -36.922183\n",
      "ep 1279: ep_len:3 episode reward: total was 0.000000. running mean: -36.552962\n",
      "ep 1279: ep_len:648 episode reward: total was -165.380000. running mean: -37.841232\n",
      "ep 1279: ep_len:324 episode reward: total was -36.110000. running mean: -37.823920\n",
      "epsilon:0.143253 episode_count: 8960. steps_count: 3899665.000000\n",
      "Time elapsed:  11185.772032260895\n",
      "ep 1280: ep_len:637 episode reward: total was -140.080000. running mean: -38.846480\n",
      "ep 1280: ep_len:522 episode reward: total was 17.360000. running mean: -38.284416\n",
      "ep 1280: ep_len:78 episode reward: total was -5.750000. running mean: -37.959071\n",
      "ep 1280: ep_len:500 episode reward: total was -40.800000. running mean: -37.987481\n",
      "ep 1280: ep_len:99 episode reward: total was -4.920000. running mean: -37.656806\n",
      "ep 1280: ep_len:504 episode reward: total was -63.080000. running mean: -37.911038\n",
      "ep 1280: ep_len:500 episode reward: total was -53.090000. running mean: -38.062828\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.143209 episode_count: 8967. steps_count: 3902505.000000\n",
      "Time elapsed:  11198.144669055939\n",
      "ep 1281: ep_len:536 episode reward: total was -3.950000. running mean: -37.721699\n",
      "ep 1281: ep_len:519 episode reward: total was -67.260000. running mean: -38.017082\n",
      "ep 1281: ep_len:650 episode reward: total was -141.370000. running mean: -39.050611\n",
      "ep 1281: ep_len:500 episode reward: total was -114.290000. running mean: -39.803005\n",
      "ep 1281: ep_len:3 episode reward: total was 0.000000. running mean: -39.404975\n",
      "ep 1281: ep_len:639 episode reward: total was -87.260000. running mean: -39.883525\n",
      "ep 1281: ep_len:577 episode reward: total was -27.810000. running mean: -39.762790\n",
      "epsilon:0.143165 episode_count: 8974. steps_count: 3905929.000000\n",
      "Time elapsed:  11208.393050432205\n",
      "ep 1282: ep_len:532 episode reward: total was -73.450000. running mean: -40.099662\n",
      "ep 1282: ep_len:500 episode reward: total was -60.280000. running mean: -40.301466\n",
      "ep 1282: ep_len:546 episode reward: total was -49.540000. running mean: -40.393851\n",
      "ep 1282: ep_len:387 episode reward: total was -24.270000. running mean: -40.232613\n",
      "ep 1282: ep_len:3 episode reward: total was 0.000000. running mean: -39.830286\n",
      "ep 1282: ep_len:602 episode reward: total was -54.630000. running mean: -39.978284\n",
      "ep 1282: ep_len:550 episode reward: total was -27.130000. running mean: -39.849801\n",
      "epsilon:0.143120 episode_count: 8981. steps_count: 3909049.000000\n",
      "Time elapsed:  11216.592559814453\n",
      "ep 1283: ep_len:575 episode reward: total was -25.810000. running mean: -39.709403\n",
      "ep 1283: ep_len:501 episode reward: total was -53.230000. running mean: -39.844609\n",
      "ep 1283: ep_len:500 episode reward: total was -61.960000. running mean: -40.065763\n",
      "ep 1283: ep_len:587 episode reward: total was -7.530000. running mean: -39.740405\n",
      "ep 1283: ep_len:3 episode reward: total was 0.000000. running mean: -39.343001\n",
      "ep 1283: ep_len:501 episode reward: total was -50.920000. running mean: -39.458771\n",
      "ep 1283: ep_len:550 episode reward: total was -31.840000. running mean: -39.382583\n",
      "epsilon:0.143076 episode_count: 8988. steps_count: 3912266.000000\n",
      "Time elapsed:  11225.192463636398\n",
      "ep 1284: ep_len:212 episode reward: total was -37.960000. running mean: -39.368357\n",
      "ep 1284: ep_len:585 episode reward: total was 52.130000. running mean: -38.453374\n",
      "ep 1284: ep_len:474 episode reward: total was -8.800000. running mean: -38.156840\n",
      "ep 1284: ep_len:500 episode reward: total was -47.840000. running mean: -38.253672\n",
      "ep 1284: ep_len:100 episode reward: total was 2.710000. running mean: -37.844035\n",
      "ep 1284: ep_len:500 episode reward: total was -58.150000. running mean: -38.047095\n",
      "ep 1284: ep_len:549 episode reward: total was -51.930000. running mean: -38.185924\n",
      "epsilon:0.143032 episode_count: 8995. steps_count: 3915186.000000\n",
      "Time elapsed:  11233.127028465271\n",
      "ep 1285: ep_len:580 episode reward: total was -21.780000. running mean: -38.021864\n",
      "ep 1285: ep_len:547 episode reward: total was 19.450000. running mean: -37.447146\n",
      "ep 1285: ep_len:601 episode reward: total was -48.210000. running mean: -37.554774\n",
      "ep 1285: ep_len:135 episode reward: total was 15.560000. running mean: -37.023627\n",
      "ep 1285: ep_len:3 episode reward: total was 0.000000. running mean: -36.653390\n",
      "ep 1285: ep_len:500 episode reward: total was -36.700000. running mean: -36.653856\n",
      "ep 1285: ep_len:197 episode reward: total was -52.260000. running mean: -36.809918\n",
      "epsilon:0.142987 episode_count: 9002. steps_count: 3917749.000000\n",
      "Time elapsed:  11241.72125172615\n",
      "ep 1286: ep_len:562 episode reward: total was -104.730000. running mean: -37.489119\n",
      "ep 1286: ep_len:570 episode reward: total was -46.480000. running mean: -37.579027\n",
      "ep 1286: ep_len:500 episode reward: total was -57.240000. running mean: -37.775637\n",
      "ep 1286: ep_len:523 episode reward: total was -18.200000. running mean: -37.579881\n",
      "ep 1286: ep_len:3 episode reward: total was -1.500000. running mean: -37.219082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1286: ep_len:644 episode reward: total was -43.250000. running mean: -37.279391\n",
      "ep 1286: ep_len:512 episode reward: total was -50.020000. running mean: -37.406797\n",
      "epsilon:0.142943 episode_count: 9009. steps_count: 3921063.000000\n",
      "Time elapsed:  11250.715379476547\n",
      "ep 1287: ep_len:99 episode reward: total was -4.160000. running mean: -37.074329\n",
      "ep 1287: ep_len:605 episode reward: total was -75.330000. running mean: -37.456886\n",
      "ep 1287: ep_len:560 episode reward: total was -37.170000. running mean: -37.454017\n",
      "ep 1287: ep_len:500 episode reward: total was -75.970000. running mean: -37.839177\n",
      "ep 1287: ep_len:52 episode reward: total was 17.000000. running mean: -37.290785\n",
      "ep 1287: ep_len:673 episode reward: total was -191.830000. running mean: -38.836177\n",
      "ep 1287: ep_len:339 episode reward: total was -56.610000. running mean: -39.013916\n",
      "epsilon:0.142899 episode_count: 9016. steps_count: 3923891.000000\n",
      "Time elapsed:  11258.61303639412\n",
      "ep 1288: ep_len:577 episode reward: total was -153.170000. running mean: -40.155476\n",
      "ep 1288: ep_len:529 episode reward: total was -59.250000. running mean: -40.346422\n",
      "ep 1288: ep_len:565 episode reward: total was -78.700000. running mean: -40.729957\n",
      "ep 1288: ep_len:130 episode reward: total was 2.560000. running mean: -40.297058\n",
      "ep 1288: ep_len:3 episode reward: total was -1.500000. running mean: -39.909087\n",
      "ep 1288: ep_len:503 episode reward: total was -49.270000. running mean: -40.002696\n",
      "ep 1288: ep_len:584 episode reward: total was -40.460000. running mean: -40.007269\n",
      "epsilon:0.142854 episode_count: 9023. steps_count: 3926782.000000\n",
      "Time elapsed:  11266.40656375885\n",
      "ep 1289: ep_len:551 episode reward: total was -82.130000. running mean: -40.428497\n",
      "ep 1289: ep_len:599 episode reward: total was -66.780000. running mean: -40.692012\n",
      "ep 1289: ep_len:518 episode reward: total was -82.540000. running mean: -41.110492\n",
      "ep 1289: ep_len:148 episode reward: total was 10.440000. running mean: -40.594987\n",
      "ep 1289: ep_len:110 episode reward: total was -1.720000. running mean: -40.206237\n",
      "ep 1289: ep_len:564 episode reward: total was -70.230000. running mean: -40.506475\n",
      "ep 1289: ep_len:632 episode reward: total was -87.070000. running mean: -40.972110\n",
      "epsilon:0.142810 episode_count: 9030. steps_count: 3929904.000000\n",
      "Time elapsed:  11276.199099779129\n",
      "ep 1290: ep_len:586 episode reward: total was 5.120000. running mean: -40.511189\n",
      "ep 1290: ep_len:500 episode reward: total was -25.290000. running mean: -40.358977\n",
      "ep 1290: ep_len:438 episode reward: total was -7.390000. running mean: -40.029287\n",
      "ep 1290: ep_len:515 episode reward: total was -22.340000. running mean: -39.852394\n",
      "ep 1290: ep_len:3 episode reward: total was 0.000000. running mean: -39.453870\n",
      "ep 1290: ep_len:239 episode reward: total was 19.340000. running mean: -38.865932\n",
      "ep 1290: ep_len:575 episode reward: total was -52.480000. running mean: -39.002072\n",
      "epsilon:0.142766 episode_count: 9037. steps_count: 3932760.000000\n",
      "Time elapsed:  11284.028192281723\n",
      "ep 1291: ep_len:632 episode reward: total was -133.820000. running mean: -39.950251\n",
      "ep 1291: ep_len:500 episode reward: total was -105.790000. running mean: -40.608649\n",
      "ep 1291: ep_len:594 episode reward: total was -103.760000. running mean: -41.240162\n",
      "ep 1291: ep_len:500 episode reward: total was 3.670000. running mean: -40.791061\n",
      "ep 1291: ep_len:3 episode reward: total was 0.000000. running mean: -40.383150\n",
      "ep 1291: ep_len:500 episode reward: total was 18.140000. running mean: -39.797919\n",
      "ep 1291: ep_len:500 episode reward: total was -36.170000. running mean: -39.761640\n",
      "epsilon:0.142721 episode_count: 9044. steps_count: 3935989.000000\n",
      "Time elapsed:  11292.49184012413\n",
      "ep 1292: ep_len:567 episode reward: total was -10.850000. running mean: -39.472523\n",
      "ep 1292: ep_len:293 episode reward: total was -28.930000. running mean: -39.367098\n",
      "ep 1292: ep_len:564 episode reward: total was -59.840000. running mean: -39.571827\n",
      "ep 1292: ep_len:554 episode reward: total was -103.580000. running mean: -40.211909\n",
      "ep 1292: ep_len:34 episode reward: total was 8.000000. running mean: -39.729790\n",
      "ep 1292: ep_len:564 episode reward: total was -10.460000. running mean: -39.437092\n",
      "ep 1292: ep_len:501 episode reward: total was -19.220000. running mean: -39.234921\n",
      "epsilon:0.142677 episode_count: 9051. steps_count: 3939066.000000\n",
      "Time elapsed:  11298.714702367783\n",
      "ep 1293: ep_len:500 episode reward: total was -40.840000. running mean: -39.250972\n",
      "ep 1293: ep_len:615 episode reward: total was -50.160000. running mean: -39.360062\n",
      "ep 1293: ep_len:544 episode reward: total was -67.540000. running mean: -39.641861\n",
      "ep 1293: ep_len:582 episode reward: total was -16.660000. running mean: -39.412043\n",
      "ep 1293: ep_len:3 episode reward: total was 0.000000. running mean: -39.017922\n",
      "ep 1293: ep_len:531 episode reward: total was -23.480000. running mean: -38.862543\n",
      "ep 1293: ep_len:279 episode reward: total was -27.510000. running mean: -38.749018\n",
      "epsilon:0.142633 episode_count: 9058. steps_count: 3942120.000000\n",
      "Time elapsed:  11306.788042783737\n",
      "ep 1294: ep_len:634 episode reward: total was -79.230000. running mean: -39.153827\n",
      "ep 1294: ep_len:542 episode reward: total was -46.080000. running mean: -39.223089\n",
      "ep 1294: ep_len:606 episode reward: total was -97.560000. running mean: -39.806458\n",
      "ep 1294: ep_len:56 episode reward: total was -4.690000. running mean: -39.455294\n",
      "ep 1294: ep_len:3 episode reward: total was 0.000000. running mean: -39.060741\n",
      "ep 1294: ep_len:221 episode reward: total was 5.750000. running mean: -38.612633\n",
      "ep 1294: ep_len:511 episode reward: total was -22.050000. running mean: -38.447007\n",
      "epsilon:0.142588 episode_count: 9065. steps_count: 3944693.000000\n",
      "Time elapsed:  11313.942757368088\n",
      "ep 1295: ep_len:594 episode reward: total was -39.390000. running mean: -38.456437\n",
      "ep 1295: ep_len:557 episode reward: total was -13.720000. running mean: -38.209073\n",
      "ep 1295: ep_len:79 episode reward: total was -0.230000. running mean: -37.829282\n",
      "ep 1295: ep_len:56 episode reward: total was -9.220000. running mean: -37.543189\n",
      "ep 1295: ep_len:3 episode reward: total was 0.000000. running mean: -37.167757\n",
      "ep 1295: ep_len:682 episode reward: total was -60.370000. running mean: -37.399780\n",
      "ep 1295: ep_len:524 episode reward: total was -52.930000. running mean: -37.555082\n",
      "epsilon:0.142544 episode_count: 9072. steps_count: 3947188.000000\n",
      "Time elapsed:  11320.855211019516\n",
      "ep 1296: ep_len:213 episode reward: total was -6.270000. running mean: -37.242231\n",
      "ep 1296: ep_len:624 episode reward: total was -17.140000. running mean: -37.041209\n",
      "ep 1296: ep_len:557 episode reward: total was -92.780000. running mean: -37.598596\n",
      "ep 1296: ep_len:108 episode reward: total was 6.440000. running mean: -37.158211\n",
      "ep 1296: ep_len:76 episode reward: total was 6.260000. running mean: -36.724028\n",
      "ep 1296: ep_len:500 episode reward: total was -46.460000. running mean: -36.821388\n",
      "ep 1296: ep_len:673 episode reward: total was -296.390000. running mean: -39.417074\n",
      "epsilon:0.142500 episode_count: 9079. steps_count: 3949939.000000\n",
      "Time elapsed:  11328.291142940521\n",
      "ep 1297: ep_len:194 episode reward: total was -2.910000. running mean: -39.052004\n",
      "ep 1297: ep_len:513 episode reward: total was 14.200000. running mean: -38.519483\n",
      "ep 1297: ep_len:565 episode reward: total was -62.330000. running mean: -38.757589\n",
      "ep 1297: ep_len:500 episode reward: total was -8.400000. running mean: -38.454013\n",
      "ep 1297: ep_len:3 episode reward: total was 0.000000. running mean: -38.069473\n",
      "ep 1297: ep_len:558 episode reward: total was -70.120000. running mean: -38.389978\n",
      "ep 1297: ep_len:521 episode reward: total was -65.810000. running mean: -38.664178\n",
      "epsilon:0.142455 episode_count: 9086. steps_count: 3952793.000000\n",
      "Time elapsed:  11337.420405864716\n",
      "ep 1298: ep_len:588 episode reward: total was -97.250000. running mean: -39.250036\n",
      "ep 1298: ep_len:500 episode reward: total was -17.730000. running mean: -39.034836\n",
      "ep 1298: ep_len:612 episode reward: total was -87.750000. running mean: -39.521988\n",
      "ep 1298: ep_len:123 episode reward: total was 7.050000. running mean: -39.056268\n",
      "ep 1298: ep_len:3 episode reward: total was 1.010000. running mean: -38.655605\n",
      "ep 1298: ep_len:500 episode reward: total was -33.140000. running mean: -38.600449\n",
      "ep 1298: ep_len:558 episode reward: total was -32.970000. running mean: -38.544145\n",
      "epsilon:0.142411 episode_count: 9093. steps_count: 3955677.000000\n",
      "Time elapsed:  11345.535714387894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1299: ep_len:500 episode reward: total was 30.130000. running mean: -37.857403\n",
      "ep 1299: ep_len:500 episode reward: total was -9.640000. running mean: -37.575229\n",
      "ep 1299: ep_len:563 episode reward: total was -95.120000. running mean: -38.150677\n",
      "ep 1299: ep_len:346 episode reward: total was -81.780000. running mean: -38.586970\n",
      "ep 1299: ep_len:124 episode reward: total was -61.190000. running mean: -38.813000\n",
      "ep 1299: ep_len:537 episode reward: total was -92.500000. running mean: -39.349870\n",
      "ep 1299: ep_len:501 episode reward: total was -78.390000. running mean: -39.740272\n",
      "epsilon:0.142367 episode_count: 9100. steps_count: 3958748.000000\n",
      "Time elapsed:  11355.296717643738\n",
      "ep 1300: ep_len:580 episode reward: total was -17.800000. running mean: -39.520869\n",
      "ep 1300: ep_len:500 episode reward: total was -28.550000. running mean: -39.411160\n",
      "ep 1300: ep_len:593 episode reward: total was -132.510000. running mean: -40.342149\n",
      "ep 1300: ep_len:500 episode reward: total was 20.360000. running mean: -39.735127\n",
      "ep 1300: ep_len:132 episode reward: total was -15.670000. running mean: -39.494476\n",
      "ep 1300: ep_len:642 episode reward: total was -70.300000. running mean: -39.802531\n",
      "ep 1300: ep_len:500 episode reward: total was -66.260000. running mean: -40.067106\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.142322 episode_count: 9107. steps_count: 3962195.000000\n",
      "Time elapsed:  11369.58221745491\n",
      "ep 1301: ep_len:554 episode reward: total was -29.260000. running mean: -39.959035\n",
      "ep 1301: ep_len:539 episode reward: total was -63.110000. running mean: -40.190544\n",
      "ep 1301: ep_len:543 episode reward: total was -55.920000. running mean: -40.347839\n",
      "ep 1301: ep_len:510 episode reward: total was -36.860000. running mean: -40.312961\n",
      "ep 1301: ep_len:90 episode reward: total was 5.240000. running mean: -39.857431\n",
      "ep 1301: ep_len:581 episode reward: total was -76.340000. running mean: -40.222257\n",
      "ep 1301: ep_len:596 episode reward: total was -48.810000. running mean: -40.308134\n",
      "epsilon:0.142278 episode_count: 9114. steps_count: 3965608.000000\n",
      "Time elapsed:  11379.917849779129\n",
      "ep 1302: ep_len:211 episode reward: total was 3.020000. running mean: -39.874853\n",
      "ep 1302: ep_len:171 episode reward: total was -31.220000. running mean: -39.788304\n",
      "ep 1302: ep_len:543 episode reward: total was -75.020000. running mean: -40.140621\n",
      "ep 1302: ep_len:509 episode reward: total was -35.200000. running mean: -40.091215\n",
      "ep 1302: ep_len:3 episode reward: total was 0.000000. running mean: -39.690303\n",
      "ep 1302: ep_len:506 episode reward: total was -101.810000. running mean: -40.311500\n",
      "ep 1302: ep_len:594 episode reward: total was -54.120000. running mean: -40.449585\n",
      "epsilon:0.142234 episode_count: 9121. steps_count: 3968145.000000\n",
      "Time elapsed:  11388.257157087326\n",
      "ep 1303: ep_len:592 episode reward: total was -18.020000. running mean: -40.225289\n",
      "ep 1303: ep_len:502 episode reward: total was -64.460000. running mean: -40.467636\n",
      "ep 1303: ep_len:500 episode reward: total was -13.940000. running mean: -40.202360\n",
      "ep 1303: ep_len:500 episode reward: total was -63.870000. running mean: -40.439036\n",
      "ep 1303: ep_len:3 episode reward: total was -1.500000. running mean: -40.049646\n",
      "ep 1303: ep_len:568 episode reward: total was -18.320000. running mean: -39.832349\n",
      "ep 1303: ep_len:509 episode reward: total was -96.060000. running mean: -40.394626\n",
      "epsilon:0.142189 episode_count: 9128. steps_count: 3971319.000000\n",
      "Time elapsed:  11398.075500249863\n",
      "ep 1304: ep_len:179 episode reward: total was -8.450000. running mean: -40.075179\n",
      "ep 1304: ep_len:290 episode reward: total was -45.580000. running mean: -40.130228\n",
      "ep 1304: ep_len:644 episode reward: total was -114.930000. running mean: -40.878225\n",
      "ep 1304: ep_len:500 episode reward: total was 14.060000. running mean: -40.328843\n",
      "ep 1304: ep_len:115 episode reward: total was -16.230000. running mean: -40.087855\n",
      "ep 1304: ep_len:539 episode reward: total was -62.230000. running mean: -40.309276\n",
      "ep 1304: ep_len:546 episode reward: total was -50.800000. running mean: -40.414183\n",
      "epsilon:0.142145 episode_count: 9135. steps_count: 3974132.000000\n",
      "Time elapsed:  11406.808777570724\n",
      "ep 1305: ep_len:609 episode reward: total was 9.100000. running mean: -39.919042\n",
      "ep 1305: ep_len:548 episode reward: total was -103.160000. running mean: -40.551451\n",
      "ep 1305: ep_len:620 episode reward: total was -64.150000. running mean: -40.787437\n",
      "ep 1305: ep_len:539 episode reward: total was 8.410000. running mean: -40.295462\n",
      "ep 1305: ep_len:3 episode reward: total was 0.000000. running mean: -39.892508\n",
      "ep 1305: ep_len:165 episode reward: total was 4.530000. running mean: -39.448283\n",
      "ep 1305: ep_len:574 episode reward: total was -63.870000. running mean: -39.692500\n",
      "epsilon:0.142101 episode_count: 9142. steps_count: 3977190.000000\n",
      "Time elapsed:  11415.487859010696\n",
      "ep 1306: ep_len:512 episode reward: total was -10.030000. running mean: -39.395875\n",
      "ep 1306: ep_len:301 episode reward: total was -62.050000. running mean: -39.622416\n",
      "ep 1306: ep_len:552 episode reward: total was -55.950000. running mean: -39.785692\n",
      "ep 1306: ep_len:500 episode reward: total was -10.690000. running mean: -39.494735\n",
      "ep 1306: ep_len:87 episode reward: total was -1.740000. running mean: -39.117188\n",
      "ep 1306: ep_len:246 episode reward: total was 27.460000. running mean: -38.451416\n",
      "ep 1306: ep_len:356 episode reward: total was -63.700000. running mean: -38.703902\n",
      "epsilon:0.142056 episode_count: 9149. steps_count: 3979744.000000\n",
      "Time elapsed:  11423.523608207703\n",
      "ep 1307: ep_len:213 episode reward: total was -8.840000. running mean: -38.405263\n",
      "ep 1307: ep_len:503 episode reward: total was -12.210000. running mean: -38.143310\n",
      "ep 1307: ep_len:517 episode reward: total was -89.720000. running mean: -38.659077\n",
      "ep 1307: ep_len:361 episode reward: total was -13.380000. running mean: -38.406286\n",
      "ep 1307: ep_len:48 episode reward: total was 13.500000. running mean: -37.887223\n",
      "ep 1307: ep_len:501 episode reward: total was -30.430000. running mean: -37.812651\n",
      "ep 1307: ep_len:599 episode reward: total was -47.930000. running mean: -37.913824\n",
      "epsilon:0.142012 episode_count: 9156. steps_count: 3982486.000000\n",
      "Time elapsed:  11434.046669483185\n",
      "ep 1308: ep_len:134 episode reward: total was -8.640000. running mean: -37.621086\n",
      "ep 1308: ep_len:538 episode reward: total was -30.960000. running mean: -37.554475\n",
      "ep 1308: ep_len:396 episode reward: total was 10.480000. running mean: -37.074131\n",
      "ep 1308: ep_len:510 episode reward: total was -78.770000. running mean: -37.491089\n",
      "ep 1308: ep_len:3 episode reward: total was 0.000000. running mean: -37.116178\n",
      "ep 1308: ep_len:676 episode reward: total was -42.220000. running mean: -37.167217\n",
      "ep 1308: ep_len:286 episode reward: total was -37.850000. running mean: -37.174044\n",
      "epsilon:0.141968 episode_count: 9163. steps_count: 3985029.000000\n",
      "Time elapsed:  11441.981793642044\n",
      "ep 1309: ep_len:131 episode reward: total was -27.160000. running mean: -37.073904\n",
      "ep 1309: ep_len:524 episode reward: total was -52.460000. running mean: -37.227765\n",
      "ep 1309: ep_len:551 episode reward: total was -50.180000. running mean: -37.357287\n",
      "ep 1309: ep_len:500 episode reward: total was -44.610000. running mean: -37.429814\n",
      "ep 1309: ep_len:3 episode reward: total was 0.000000. running mean: -37.055516\n",
      "ep 1309: ep_len:517 episode reward: total was -69.010000. running mean: -37.375061\n",
      "ep 1309: ep_len:336 episode reward: total was -86.790000. running mean: -37.869211\n",
      "epsilon:0.141923 episode_count: 9170. steps_count: 3987591.000000\n",
      "Time elapsed:  11449.964022159576\n",
      "ep 1310: ep_len:242 episode reward: total was 1.690000. running mean: -37.473618\n",
      "ep 1310: ep_len:510 episode reward: total was -81.220000. running mean: -37.911082\n",
      "ep 1310: ep_len:500 episode reward: total was -11.410000. running mean: -37.646071\n",
      "ep 1310: ep_len:504 episode reward: total was -60.380000. running mean: -37.873411\n",
      "ep 1310: ep_len:3 episode reward: total was 0.000000. running mean: -37.494677\n",
      "ep 1310: ep_len:531 episode reward: total was -85.350000. running mean: -37.973230\n",
      "ep 1310: ep_len:609 episode reward: total was -41.090000. running mean: -38.004398\n",
      "epsilon:0.141879 episode_count: 9177. steps_count: 3990490.000000\n",
      "Time elapsed:  11458.354302406311\n",
      "ep 1311: ep_len:500 episode reward: total was 8.670000. running mean: -37.537654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1311: ep_len:286 episode reward: total was -95.200000. running mean: -38.114277\n",
      "ep 1311: ep_len:650 episode reward: total was -63.980000. running mean: -38.372934\n",
      "ep 1311: ep_len:553 episode reward: total was -40.100000. running mean: -38.390205\n",
      "ep 1311: ep_len:3 episode reward: total was -1.500000. running mean: -38.021303\n",
      "ep 1311: ep_len:613 episode reward: total was -115.120000. running mean: -38.792290\n",
      "ep 1311: ep_len:577 episode reward: total was -66.780000. running mean: -39.072167\n",
      "epsilon:0.141835 episode_count: 9184. steps_count: 3993672.000000\n",
      "Time elapsed:  11466.560751914978\n",
      "ep 1312: ep_len:602 episode reward: total was -5.940000. running mean: -38.740845\n",
      "ep 1312: ep_len:501 episode reward: total was -64.720000. running mean: -39.000637\n",
      "ep 1312: ep_len:644 episode reward: total was -19.710000. running mean: -38.807730\n",
      "ep 1312: ep_len:500 episode reward: total was -34.420000. running mean: -38.763853\n",
      "ep 1312: ep_len:3 episode reward: total was 0.000000. running mean: -38.376215\n",
      "ep 1312: ep_len:572 episode reward: total was -39.240000. running mean: -38.384852\n",
      "ep 1312: ep_len:500 episode reward: total was -57.610000. running mean: -38.577104\n",
      "epsilon:0.141790 episode_count: 9191. steps_count: 3996994.000000\n",
      "Time elapsed:  11475.30869102478\n",
      "ep 1313: ep_len:559 episode reward: total was -75.570000. running mean: -38.947033\n",
      "ep 1313: ep_len:524 episode reward: total was 6.600000. running mean: -38.491563\n",
      "ep 1313: ep_len:500 episode reward: total was -173.110000. running mean: -39.837747\n",
      "ep 1313: ep_len:115 episode reward: total was 11.440000. running mean: -39.324969\n",
      "ep 1313: ep_len:3 episode reward: total was 0.000000. running mean: -38.931720\n",
      "ep 1313: ep_len:500 episode reward: total was -43.510000. running mean: -38.977503\n",
      "ep 1313: ep_len:515 episode reward: total was -65.500000. running mean: -39.242728\n",
      "epsilon:0.141746 episode_count: 9198. steps_count: 3999710.000000\n",
      "Time elapsed:  11483.063454389572\n",
      "ep 1314: ep_len:540 episode reward: total was 5.330000. running mean: -38.797000\n",
      "ep 1314: ep_len:605 episode reward: total was -90.690000. running mean: -39.315930\n",
      "ep 1314: ep_len:519 episode reward: total was -57.990000. running mean: -39.502671\n",
      "ep 1314: ep_len:500 episode reward: total was -38.360000. running mean: -39.491244\n",
      "ep 1314: ep_len:83 episode reward: total was 15.670000. running mean: -38.939632\n",
      "ep 1314: ep_len:209 episode reward: total was 3.000000. running mean: -38.520235\n",
      "ep 1314: ep_len:316 episode reward: total was -24.740000. running mean: -38.382433\n",
      "epsilon:0.141702 episode_count: 9205. steps_count: 4002482.000000\n",
      "Time elapsed:  11490.521526098251\n",
      "ep 1315: ep_len:501 episode reward: total was -45.690000. running mean: -38.455509\n",
      "ep 1315: ep_len:500 episode reward: total was -25.120000. running mean: -38.322154\n",
      "ep 1315: ep_len:79 episode reward: total was -4.760000. running mean: -37.986532\n",
      "ep 1315: ep_len:161 episode reward: total was 6.650000. running mean: -37.540167\n",
      "ep 1315: ep_len:53 episode reward: total was 22.000000. running mean: -36.944765\n",
      "ep 1315: ep_len:615 episode reward: total was -44.320000. running mean: -37.018518\n",
      "ep 1315: ep_len:602 episode reward: total was -49.250000. running mean: -37.140832\n",
      "epsilon:0.141657 episode_count: 9212. steps_count: 4004993.000000\n",
      "Time elapsed:  11497.354418516159\n",
      "ep 1316: ep_len:570 episode reward: total was 6.400000. running mean: -36.705424\n",
      "ep 1316: ep_len:500 episode reward: total was -31.900000. running mean: -36.657370\n",
      "ep 1316: ep_len:576 episode reward: total was -65.330000. running mean: -36.944096\n",
      "ep 1316: ep_len:500 episode reward: total was -11.280000. running mean: -36.687455\n",
      "ep 1316: ep_len:3 episode reward: total was 0.000000. running mean: -36.320581\n",
      "ep 1316: ep_len:500 episode reward: total was -21.500000. running mean: -36.172375\n",
      "ep 1316: ep_len:563 episode reward: total was -43.780000. running mean: -36.248451\n",
      "epsilon:0.141613 episode_count: 9219. steps_count: 4008205.000000\n",
      "Time elapsed:  11505.911154031754\n",
      "ep 1317: ep_len:134 episode reward: total was -9.070000. running mean: -35.976667\n",
      "ep 1317: ep_len:503 episode reward: total was -56.080000. running mean: -36.177700\n",
      "ep 1317: ep_len:560 episode reward: total was -43.720000. running mean: -36.253123\n",
      "ep 1317: ep_len:520 episode reward: total was -3.830000. running mean: -35.928892\n",
      "ep 1317: ep_len:50 episode reward: total was 14.500000. running mean: -35.424603\n",
      "ep 1317: ep_len:522 episode reward: total was -76.940000. running mean: -35.839757\n",
      "ep 1317: ep_len:604 episode reward: total was -57.340000. running mean: -36.054759\n",
      "epsilon:0.141569 episode_count: 9226. steps_count: 4011098.000000\n",
      "Time elapsed:  11513.673109054565\n",
      "ep 1318: ep_len:643 episode reward: total was -92.080000. running mean: -36.615012\n",
      "ep 1318: ep_len:540 episode reward: total was -78.160000. running mean: -37.030461\n",
      "ep 1318: ep_len:531 episode reward: total was -87.040000. running mean: -37.530557\n",
      "ep 1318: ep_len:500 episode reward: total was -40.340000. running mean: -37.558651\n",
      "ep 1318: ep_len:50 episode reward: total was 13.000000. running mean: -37.053065\n",
      "ep 1318: ep_len:520 episode reward: total was -85.170000. running mean: -37.534234\n",
      "ep 1318: ep_len:617 episode reward: total was -35.280000. running mean: -37.511692\n",
      "epsilon:0.141524 episode_count: 9233. steps_count: 4014499.000000\n",
      "Time elapsed:  11522.701747894287\n",
      "ep 1319: ep_len:500 episode reward: total was -24.720000. running mean: -37.383775\n",
      "ep 1319: ep_len:515 episode reward: total was -57.590000. running mean: -37.585837\n",
      "ep 1319: ep_len:647 episode reward: total was -50.630000. running mean: -37.716279\n",
      "ep 1319: ep_len:500 episode reward: total was -21.840000. running mean: -37.557516\n",
      "ep 1319: ep_len:3 episode reward: total was 0.000000. running mean: -37.181941\n",
      "ep 1319: ep_len:500 episode reward: total was -75.590000. running mean: -37.566021\n",
      "ep 1319: ep_len:526 episode reward: total was -65.190000. running mean: -37.842261\n",
      "epsilon:0.141480 episode_count: 9240. steps_count: 4017690.000000\n",
      "Time elapsed:  11531.014593601227\n",
      "ep 1320: ep_len:531 episode reward: total was -112.890000. running mean: -38.592739\n",
      "ep 1320: ep_len:549 episode reward: total was -14.140000. running mean: -38.348211\n",
      "ep 1320: ep_len:662 episode reward: total was -117.380000. running mean: -39.138529\n",
      "ep 1320: ep_len:167 episode reward: total was 6.640000. running mean: -38.680744\n",
      "ep 1320: ep_len:3 episode reward: total was -1.500000. running mean: -38.308936\n",
      "ep 1320: ep_len:162 episode reward: total was 9.090000. running mean: -37.834947\n",
      "ep 1320: ep_len:501 episode reward: total was -27.960000. running mean: -37.736197\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.141436 episode_count: 9247. steps_count: 4020265.000000\n",
      "Time elapsed:  11543.981544971466\n",
      "ep 1321: ep_len:657 episode reward: total was -91.910000. running mean: -38.277935\n",
      "ep 1321: ep_len:624 episode reward: total was -36.520000. running mean: -38.260356\n",
      "ep 1321: ep_len:455 episode reward: total was -1.980000. running mean: -37.897553\n",
      "ep 1321: ep_len:43 episode reward: total was -2.830000. running mean: -37.546877\n",
      "ep 1321: ep_len:3 episode reward: total was 0.000000. running mean: -37.171408\n",
      "ep 1321: ep_len:314 episode reward: total was -10.440000. running mean: -36.904094\n",
      "ep 1321: ep_len:571 episode reward: total was -53.560000. running mean: -37.070653\n",
      "epsilon:0.141391 episode_count: 9254. steps_count: 4022932.000000\n",
      "Time elapsed:  11551.285690069199\n",
      "ep 1322: ep_len:632 episode reward: total was -114.880000. running mean: -37.848747\n",
      "ep 1322: ep_len:500 episode reward: total was -52.180000. running mean: -37.992059\n",
      "ep 1322: ep_len:603 episode reward: total was -54.600000. running mean: -38.158139\n",
      "ep 1322: ep_len:500 episode reward: total was -17.980000. running mean: -37.956357\n",
      "ep 1322: ep_len:3 episode reward: total was 0.000000. running mean: -37.576794\n",
      "ep 1322: ep_len:647 episode reward: total was -32.590000. running mean: -37.526926\n",
      "ep 1322: ep_len:524 episode reward: total was -66.000000. running mean: -37.811657\n",
      "epsilon:0.141347 episode_count: 9261. steps_count: 4026341.000000\n",
      "Time elapsed:  11560.07899594307\n",
      "ep 1323: ep_len:506 episode reward: total was -85.590000. running mean: -38.289440\n",
      "ep 1323: ep_len:506 episode reward: total was -5.430000. running mean: -37.960846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1323: ep_len:79 episode reward: total was -27.260000. running mean: -37.853837\n",
      "ep 1323: ep_len:566 episode reward: total was -10.990000. running mean: -37.585199\n",
      "ep 1323: ep_len:105 episode reward: total was 26.270000. running mean: -36.946647\n",
      "ep 1323: ep_len:562 episode reward: total was -45.650000. running mean: -37.033680\n",
      "ep 1323: ep_len:603 episode reward: total was -66.530000. running mean: -37.328643\n",
      "epsilon:0.141303 episode_count: 9268. steps_count: 4029268.000000\n",
      "Time elapsed:  11568.02814245224\n",
      "ep 1324: ep_len:613 episode reward: total was -0.530000. running mean: -36.960657\n",
      "ep 1324: ep_len:500 episode reward: total was -35.410000. running mean: -36.945150\n",
      "ep 1324: ep_len:679 episode reward: total was -83.820000. running mean: -37.413899\n",
      "ep 1324: ep_len:512 episode reward: total was 13.080000. running mean: -36.908960\n",
      "ep 1324: ep_len:3 episode reward: total was -1.500000. running mean: -36.554870\n",
      "ep 1324: ep_len:616 episode reward: total was -55.030000. running mean: -36.739622\n",
      "ep 1324: ep_len:518 episode reward: total was -72.540000. running mean: -37.097625\n",
      "epsilon:0.141258 episode_count: 9275. steps_count: 4032709.000000\n",
      "Time elapsed:  11577.288449525833\n",
      "ep 1325: ep_len:502 episode reward: total was -56.060000. running mean: -37.287249\n",
      "ep 1325: ep_len:500 episode reward: total was -76.830000. running mean: -37.682677\n",
      "ep 1325: ep_len:500 episode reward: total was -24.560000. running mean: -37.551450\n",
      "ep 1325: ep_len:550 episode reward: total was 5.500000. running mean: -37.120935\n",
      "ep 1325: ep_len:3 episode reward: total was 1.010000. running mean: -36.739626\n",
      "ep 1325: ep_len:523 episode reward: total was -16.770000. running mean: -36.539930\n",
      "ep 1325: ep_len:508 episode reward: total was -74.440000. running mean: -36.918931\n",
      "epsilon:0.141214 episode_count: 9282. steps_count: 4035795.000000\n",
      "Time elapsed:  11585.999675750732\n",
      "ep 1326: ep_len:536 episode reward: total was 18.300000. running mean: -36.366741\n",
      "ep 1326: ep_len:501 episode reward: total was -0.760000. running mean: -36.010674\n",
      "ep 1326: ep_len:564 episode reward: total was -29.050000. running mean: -35.941067\n",
      "ep 1326: ep_len:155 episode reward: total was 9.100000. running mean: -35.490656\n",
      "ep 1326: ep_len:3 episode reward: total was 1.010000. running mean: -35.125650\n",
      "ep 1326: ep_len:612 episode reward: total was -156.030000. running mean: -36.334693\n",
      "ep 1326: ep_len:500 episode reward: total was -135.600000. running mean: -37.327346\n",
      "epsilon:0.141170 episode_count: 9289. steps_count: 4038666.000000\n",
      "Time elapsed:  11593.800515651703\n",
      "ep 1327: ep_len:529 episode reward: total was -27.930000. running mean: -37.233373\n",
      "ep 1327: ep_len:500 episode reward: total was -11.340000. running mean: -36.974439\n",
      "ep 1327: ep_len:670 episode reward: total was -102.310000. running mean: -37.627795\n",
      "ep 1327: ep_len:584 episode reward: total was -6.480000. running mean: -37.316317\n",
      "ep 1327: ep_len:3 episode reward: total was 0.000000. running mean: -36.943154\n",
      "ep 1327: ep_len:180 episode reward: total was -39.660000. running mean: -36.970322\n",
      "ep 1327: ep_len:528 episode reward: total was -48.840000. running mean: -37.089019\n",
      "epsilon:0.141125 episode_count: 9296. steps_count: 4041660.000000\n",
      "Time elapsed:  11602.284835100174\n",
      "ep 1328: ep_len:662 episode reward: total was -168.100000. running mean: -38.399129\n",
      "ep 1328: ep_len:504 episode reward: total was -47.630000. running mean: -38.491437\n",
      "ep 1328: ep_len:571 episode reward: total was -97.690000. running mean: -39.083423\n",
      "ep 1328: ep_len:554 episode reward: total was -31.650000. running mean: -39.009089\n",
      "ep 1328: ep_len:94 episode reward: total was -8.740000. running mean: -38.706398\n",
      "ep 1328: ep_len:500 episode reward: total was -5.920000. running mean: -38.378534\n",
      "ep 1328: ep_len:614 episode reward: total was -64.940000. running mean: -38.644149\n",
      "epsilon:0.141081 episode_count: 9303. steps_count: 4045159.000000\n",
      "Time elapsed:  11611.405429124832\n",
      "ep 1329: ep_len:545 episode reward: total was -24.820000. running mean: -38.505907\n",
      "ep 1329: ep_len:501 episode reward: total was -49.410000. running mean: -38.614948\n",
      "ep 1329: ep_len:500 episode reward: total was -34.530000. running mean: -38.574099\n",
      "ep 1329: ep_len:527 episode reward: total was 6.620000. running mean: -38.122158\n",
      "ep 1329: ep_len:131 episode reward: total was 3.850000. running mean: -37.702436\n",
      "ep 1329: ep_len:510 episode reward: total was -14.020000. running mean: -37.465612\n",
      "ep 1329: ep_len:583 episode reward: total was -70.060000. running mean: -37.791556\n",
      "epsilon:0.141037 episode_count: 9310. steps_count: 4048456.000000\n",
      "Time elapsed:  11620.217555999756\n",
      "ep 1330: ep_len:595 episode reward: total was -16.930000. running mean: -37.582940\n",
      "ep 1330: ep_len:647 episode reward: total was -39.530000. running mean: -37.602411\n",
      "ep 1330: ep_len:583 episode reward: total was -56.330000. running mean: -37.789687\n",
      "ep 1330: ep_len:524 episode reward: total was -51.900000. running mean: -37.930790\n",
      "ep 1330: ep_len:3 episode reward: total was 0.000000. running mean: -37.551482\n",
      "ep 1330: ep_len:514 episode reward: total was -28.200000. running mean: -37.457967\n",
      "ep 1330: ep_len:500 episode reward: total was -43.990000. running mean: -37.523287\n",
      "epsilon:0.140992 episode_count: 9317. steps_count: 4051822.000000\n",
      "Time elapsed:  11629.085103273392\n",
      "ep 1331: ep_len:500 episode reward: total was -127.990000. running mean: -38.427954\n",
      "ep 1331: ep_len:500 episode reward: total was -68.370000. running mean: -38.727375\n",
      "ep 1331: ep_len:500 episode reward: total was -29.170000. running mean: -38.631801\n",
      "ep 1331: ep_len:567 episode reward: total was 16.310000. running mean: -38.082383\n",
      "ep 1331: ep_len:53 episode reward: total was 3.180000. running mean: -37.669759\n",
      "ep 1331: ep_len:500 episode reward: total was -57.010000. running mean: -37.863162\n",
      "ep 1331: ep_len:500 episode reward: total was -48.700000. running mean: -37.971530\n",
      "epsilon:0.140948 episode_count: 9324. steps_count: 4054942.000000\n",
      "Time elapsed:  11637.358515501022\n",
      "ep 1332: ep_len:628 episode reward: total was -60.490000. running mean: -38.196715\n",
      "ep 1332: ep_len:500 episode reward: total was -52.780000. running mean: -38.342548\n",
      "ep 1332: ep_len:655 episode reward: total was -89.080000. running mean: -38.849922\n",
      "ep 1332: ep_len:586 episode reward: total was 27.270000. running mean: -38.188723\n",
      "ep 1332: ep_len:3 episode reward: total was 1.010000. running mean: -37.796736\n",
      "ep 1332: ep_len:668 episode reward: total was -96.900000. running mean: -38.387768\n",
      "ep 1332: ep_len:259 episode reward: total was -10.630000. running mean: -38.110191\n",
      "epsilon:0.140904 episode_count: 9331. steps_count: 4058241.000000\n",
      "Time elapsed:  11646.054304122925\n",
      "ep 1333: ep_len:131 episode reward: total was -8.700000. running mean: -37.816089\n",
      "ep 1333: ep_len:501 episode reward: total was -55.530000. running mean: -37.993228\n",
      "ep 1333: ep_len:593 episode reward: total was -73.310000. running mean: -38.346396\n",
      "ep 1333: ep_len:603 episode reward: total was 25.270000. running mean: -37.710232\n",
      "ep 1333: ep_len:3 episode reward: total was 0.000000. running mean: -37.333129\n",
      "ep 1333: ep_len:510 episode reward: total was -44.490000. running mean: -37.404698\n",
      "ep 1333: ep_len:602 episode reward: total was -107.750000. running mean: -38.108151\n",
      "epsilon:0.140859 episode_count: 9338. steps_count: 4061184.000000\n",
      "Time elapsed:  11653.81127524376\n",
      "ep 1334: ep_len:509 episode reward: total was -59.820000. running mean: -38.325269\n",
      "ep 1334: ep_len:292 episode reward: total was -33.410000. running mean: -38.276117\n",
      "ep 1334: ep_len:570 episode reward: total was -37.270000. running mean: -38.266056\n",
      "ep 1334: ep_len:519 episode reward: total was -86.800000. running mean: -38.751395\n",
      "ep 1334: ep_len:3 episode reward: total was 0.000000. running mean: -38.363881\n",
      "ep 1334: ep_len:666 episode reward: total was -50.710000. running mean: -38.487342\n",
      "ep 1334: ep_len:592 episode reward: total was -61.710000. running mean: -38.719569\n",
      "epsilon:0.140815 episode_count: 9345. steps_count: 4064335.000000\n",
      "Time elapsed:  11665.180883169174\n",
      "ep 1335: ep_len:207 episode reward: total was -20.500000. running mean: -38.537373\n",
      "ep 1335: ep_len:500 episode reward: total was -52.400000. running mean: -38.675999\n",
      "ep 1335: ep_len:395 episode reward: total was -5.440000. running mean: -38.343639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1335: ep_len:500 episode reward: total was -96.450000. running mean: -38.924703\n",
      "ep 1335: ep_len:50 episode reward: total was 17.500000. running mean: -38.360456\n",
      "ep 1335: ep_len:505 episode reward: total was -35.730000. running mean: -38.334151\n",
      "ep 1335: ep_len:523 episode reward: total was -69.060000. running mean: -38.641410\n",
      "epsilon:0.140771 episode_count: 9352. steps_count: 4067015.000000\n",
      "Time elapsed:  11672.355031013489\n",
      "ep 1336: ep_len:659 episode reward: total was -116.340000. running mean: -39.418396\n",
      "ep 1336: ep_len:549 episode reward: total was -20.450000. running mean: -39.228712\n",
      "ep 1336: ep_len:530 episode reward: total was -54.360000. running mean: -39.380025\n",
      "ep 1336: ep_len:500 episode reward: total was -81.570000. running mean: -39.801925\n",
      "ep 1336: ep_len:48 episode reward: total was 16.500000. running mean: -39.238905\n",
      "ep 1336: ep_len:526 episode reward: total was -104.060000. running mean: -39.887116\n",
      "ep 1336: ep_len:331 episode reward: total was -24.260000. running mean: -39.730845\n",
      "epsilon:0.140726 episode_count: 9359. steps_count: 4070158.000000\n",
      "Time elapsed:  11680.726081132889\n",
      "ep 1337: ep_len:534 episode reward: total was -104.780000. running mean: -40.381337\n",
      "ep 1337: ep_len:563 episode reward: total was -24.570000. running mean: -40.223223\n",
      "ep 1337: ep_len:501 episode reward: total was -59.820000. running mean: -40.419191\n",
      "ep 1337: ep_len:522 episode reward: total was -11.660000. running mean: -40.131599\n",
      "ep 1337: ep_len:3 episode reward: total was 0.000000. running mean: -39.730283\n",
      "ep 1337: ep_len:541 episode reward: total was -85.860000. running mean: -40.191580\n",
      "ep 1337: ep_len:610 episode reward: total was -36.580000. running mean: -40.155465\n",
      "epsilon:0.140682 episode_count: 9366. steps_count: 4073432.000000\n",
      "Time elapsed:  11689.388293981552\n",
      "ep 1338: ep_len:524 episode reward: total was 4.790000. running mean: -39.706010\n",
      "ep 1338: ep_len:500 episode reward: total was -37.870000. running mean: -39.687650\n",
      "ep 1338: ep_len:630 episode reward: total was -40.460000. running mean: -39.695373\n",
      "ep 1338: ep_len:521 episode reward: total was -0.220000. running mean: -39.300620\n",
      "ep 1338: ep_len:3 episode reward: total was 0.000000. running mean: -38.907613\n",
      "ep 1338: ep_len:630 episode reward: total was -37.390000. running mean: -38.892437\n",
      "ep 1338: ep_len:505 episode reward: total was -73.830000. running mean: -39.241813\n",
      "epsilon:0.140638 episode_count: 9373. steps_count: 4076745.000000\n",
      "Time elapsed:  11698.099682569504\n",
      "ep 1339: ep_len:500 episode reward: total was 26.760000. running mean: -38.581795\n",
      "ep 1339: ep_len:501 episode reward: total was -73.330000. running mean: -38.929277\n",
      "ep 1339: ep_len:500 episode reward: total was 19.640000. running mean: -38.343584\n",
      "ep 1339: ep_len:170 episode reward: total was -12.830000. running mean: -38.088448\n",
      "ep 1339: ep_len:3 episode reward: total was -1.500000. running mean: -37.722564\n",
      "ep 1339: ep_len:245 episode reward: total was 24.820000. running mean: -37.097138\n",
      "ep 1339: ep_len:570 episode reward: total was -50.440000. running mean: -37.230567\n",
      "epsilon:0.140593 episode_count: 9380. steps_count: 4079234.000000\n",
      "Time elapsed:  11705.137368917465\n",
      "ep 1340: ep_len:640 episode reward: total was -80.790000. running mean: -37.666161\n",
      "ep 1340: ep_len:570 episode reward: total was -51.380000. running mean: -37.803299\n",
      "ep 1340: ep_len:666 episode reward: total was -72.200000. running mean: -38.147266\n",
      "ep 1340: ep_len:389 episode reward: total was -46.260000. running mean: -38.228394\n",
      "ep 1340: ep_len:3 episode reward: total was 0.000000. running mean: -37.846110\n",
      "ep 1340: ep_len:594 episode reward: total was -31.610000. running mean: -37.783749\n",
      "ep 1340: ep_len:500 episode reward: total was -22.480000. running mean: -37.630711\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.140549 episode_count: 9387. steps_count: 4082596.000000\n",
      "Time elapsed:  11719.017531871796\n",
      "ep 1341: ep_len:503 episode reward: total was -35.200000. running mean: -37.606404\n",
      "ep 1341: ep_len:621 episode reward: total was 10.480000. running mean: -37.125540\n",
      "ep 1341: ep_len:592 episode reward: total was -102.300000. running mean: -37.777285\n",
      "ep 1341: ep_len:516 episode reward: total was -28.980000. running mean: -37.689312\n",
      "ep 1341: ep_len:126 episode reward: total was -0.690000. running mean: -37.319319\n",
      "ep 1341: ep_len:213 episode reward: total was -3.880000. running mean: -36.984925\n",
      "ep 1341: ep_len:527 episode reward: total was -68.350000. running mean: -37.298576\n",
      "epsilon:0.140505 episode_count: 9394. steps_count: 4085694.000000\n",
      "Time elapsed:  11728.521344423294\n",
      "ep 1342: ep_len:609 episode reward: total was -25.460000. running mean: -37.180190\n",
      "ep 1342: ep_len:296 episode reward: total was -59.530000. running mean: -37.403689\n",
      "ep 1342: ep_len:500 episode reward: total was -76.510000. running mean: -37.794752\n",
      "ep 1342: ep_len:501 episode reward: total was -42.110000. running mean: -37.837904\n",
      "ep 1342: ep_len:101 episode reward: total was -66.260000. running mean: -38.122125\n",
      "ep 1342: ep_len:500 episode reward: total was -7.790000. running mean: -37.818804\n",
      "ep 1342: ep_len:501 episode reward: total was -59.700000. running mean: -38.037616\n",
      "epsilon:0.140460 episode_count: 9401. steps_count: 4088702.000000\n",
      "Time elapsed:  11736.54915523529\n",
      "ep 1343: ep_len:551 episode reward: total was 5.690000. running mean: -37.600340\n",
      "ep 1343: ep_len:500 episode reward: total was -49.680000. running mean: -37.721136\n",
      "ep 1343: ep_len:516 episode reward: total was -110.860000. running mean: -38.452525\n",
      "ep 1343: ep_len:518 episode reward: total was -6.780000. running mean: -38.135800\n",
      "ep 1343: ep_len:79 episode reward: total was -60.820000. running mean: -38.362642\n",
      "ep 1343: ep_len:648 episode reward: total was -50.160000. running mean: -38.480615\n",
      "ep 1343: ep_len:612 episode reward: total was -38.070000. running mean: -38.476509\n",
      "epsilon:0.140416 episode_count: 9408. steps_count: 4092126.000000\n",
      "Time elapsed:  11745.70052576065\n",
      "ep 1344: ep_len:500 episode reward: total was -23.550000. running mean: -38.327244\n",
      "ep 1344: ep_len:558 episode reward: total was -73.290000. running mean: -38.676872\n",
      "ep 1344: ep_len:550 episode reward: total was -141.370000. running mean: -39.703803\n",
      "ep 1344: ep_len:500 episode reward: total was -43.030000. running mean: -39.737065\n",
      "ep 1344: ep_len:85 episode reward: total was 8.220000. running mean: -39.257494\n",
      "ep 1344: ep_len:535 episode reward: total was -47.060000. running mean: -39.335519\n",
      "ep 1344: ep_len:528 episode reward: total was -31.740000. running mean: -39.259564\n",
      "epsilon:0.140372 episode_count: 9415. steps_count: 4095382.000000\n",
      "Time elapsed:  11754.257137060165\n",
      "ep 1345: ep_len:539 episode reward: total was -150.080000. running mean: -40.367768\n",
      "ep 1345: ep_len:507 episode reward: total was -29.340000. running mean: -40.257491\n",
      "ep 1345: ep_len:500 episode reward: total was -14.830000. running mean: -40.003216\n",
      "ep 1345: ep_len:169 episode reward: total was 5.170000. running mean: -39.551484\n",
      "ep 1345: ep_len:3 episode reward: total was -1.500000. running mean: -39.170969\n",
      "ep 1345: ep_len:500 episode reward: total was -26.560000. running mean: -39.044859\n",
      "ep 1345: ep_len:624 episode reward: total was -111.550000. running mean: -39.769911\n",
      "epsilon:0.140327 episode_count: 9422. steps_count: 4098224.000000\n",
      "Time elapsed:  11761.896226882935\n",
      "ep 1346: ep_len:506 episode reward: total was -9.420000. running mean: -39.466411\n",
      "ep 1346: ep_len:343 episode reward: total was -61.130000. running mean: -39.683047\n",
      "ep 1346: ep_len:500 episode reward: total was -63.270000. running mean: -39.918917\n",
      "ep 1346: ep_len:500 episode reward: total was -83.690000. running mean: -40.356628\n",
      "ep 1346: ep_len:3 episode reward: total was 0.000000. running mean: -39.953061\n",
      "ep 1346: ep_len:307 episode reward: total was 1.520000. running mean: -39.538331\n",
      "ep 1346: ep_len:313 episode reward: total was -26.180000. running mean: -39.404747\n",
      "epsilon:0.140283 episode_count: 9429. steps_count: 4100696.000000\n",
      "Time elapsed:  11768.576243638992\n",
      "ep 1347: ep_len:646 episode reward: total was -136.220000. running mean: -40.372900\n",
      "ep 1347: ep_len:500 episode reward: total was -10.950000. running mean: -40.078671\n",
      "ep 1347: ep_len:563 episode reward: total was -84.480000. running mean: -40.522684\n",
      "ep 1347: ep_len:512 episode reward: total was -82.560000. running mean: -40.943057\n",
      "ep 1347: ep_len:3 episode reward: total was -1.500000. running mean: -40.548627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1347: ep_len:513 episode reward: total was -86.330000. running mean: -41.006441\n",
      "ep 1347: ep_len:500 episode reward: total was -46.200000. running mean: -41.058376\n",
      "epsilon:0.140239 episode_count: 9436. steps_count: 4103933.000000\n",
      "Time elapsed:  11777.08351802826\n",
      "ep 1348: ep_len:246 episode reward: total was -2.290000. running mean: -40.670692\n",
      "ep 1348: ep_len:526 episode reward: total was -15.650000. running mean: -40.420486\n",
      "ep 1348: ep_len:79 episode reward: total was -5.770000. running mean: -40.073981\n",
      "ep 1348: ep_len:521 episode reward: total was 5.350000. running mean: -39.619741\n",
      "ep 1348: ep_len:90 episode reward: total was 21.740000. running mean: -39.006143\n",
      "ep 1348: ep_len:525 episode reward: total was -65.810000. running mean: -39.274182\n",
      "ep 1348: ep_len:534 episode reward: total was -31.870000. running mean: -39.200140\n",
      "epsilon:0.140194 episode_count: 9443. steps_count: 4106454.000000\n",
      "Time elapsed:  11785.195292711258\n",
      "ep 1349: ep_len:241 episode reward: total was -1.300000. running mean: -38.821139\n",
      "ep 1349: ep_len:501 episode reward: total was -46.350000. running mean: -38.896427\n",
      "ep 1349: ep_len:500 episode reward: total was -44.910000. running mean: -38.956563\n",
      "ep 1349: ep_len:557 episode reward: total was -38.450000. running mean: -38.951497\n",
      "ep 1349: ep_len:3 episode reward: total was -1.500000. running mean: -38.576983\n",
      "ep 1349: ep_len:500 episode reward: total was -96.080000. running mean: -39.152013\n",
      "ep 1349: ep_len:500 episode reward: total was -62.680000. running mean: -39.387293\n",
      "epsilon:0.140150 episode_count: 9450. steps_count: 4109256.000000\n",
      "Time elapsed:  11792.742470026016\n",
      "ep 1350: ep_len:134 episode reward: total was -6.070000. running mean: -39.054120\n",
      "ep 1350: ep_len:525 episode reward: total was -46.680000. running mean: -39.130378\n",
      "ep 1350: ep_len:544 episode reward: total was -48.530000. running mean: -39.224375\n",
      "ep 1350: ep_len:161 episode reward: total was 4.160000. running mean: -38.790531\n",
      "ep 1350: ep_len:108 episode reward: total was 7.780000. running mean: -38.324826\n",
      "ep 1350: ep_len:517 episode reward: total was -12.700000. running mean: -38.068577\n",
      "ep 1350: ep_len:534 episode reward: total was -41.390000. running mean: -38.101792\n",
      "epsilon:0.140106 episode_count: 9457. steps_count: 4111779.000000\n",
      "Time elapsed:  11799.792907476425\n",
      "ep 1351: ep_len:617 episode reward: total was -99.140000. running mean: -38.712174\n",
      "ep 1351: ep_len:500 episode reward: total was -37.960000. running mean: -38.704652\n",
      "ep 1351: ep_len:615 episode reward: total was -37.060000. running mean: -38.688205\n",
      "ep 1351: ep_len:500 episode reward: total was -40.770000. running mean: -38.709023\n",
      "ep 1351: ep_len:3 episode reward: total was -1.500000. running mean: -38.336933\n",
      "ep 1351: ep_len:564 episode reward: total was -66.220000. running mean: -38.615764\n",
      "ep 1351: ep_len:523 episode reward: total was -54.380000. running mean: -38.773406\n",
      "epsilon:0.140061 episode_count: 9464. steps_count: 4115101.000000\n",
      "Time elapsed:  11808.460184335709\n",
      "ep 1352: ep_len:600 episode reward: total was -58.820000. running mean: -38.973872\n",
      "ep 1352: ep_len:603 episode reward: total was -79.270000. running mean: -39.376833\n",
      "ep 1352: ep_len:500 episode reward: total was -64.020000. running mean: -39.623265\n",
      "ep 1352: ep_len:596 episode reward: total was -41.620000. running mean: -39.643232\n",
      "ep 1352: ep_len:3 episode reward: total was 1.010000. running mean: -39.236700\n",
      "ep 1352: ep_len:514 episode reward: total was -93.510000. running mean: -39.779433\n",
      "ep 1352: ep_len:530 episode reward: total was -46.520000. running mean: -39.846839\n",
      "epsilon:0.140017 episode_count: 9471. steps_count: 4118447.000000\n",
      "Time elapsed:  11817.512288331985\n",
      "ep 1353: ep_len:673 episode reward: total was -91.810000. running mean: -40.366470\n",
      "ep 1353: ep_len:637 episode reward: total was -14.520000. running mean: -40.108006\n",
      "ep 1353: ep_len:568 episode reward: total was -63.530000. running mean: -40.342226\n",
      "ep 1353: ep_len:501 episode reward: total was 12.440000. running mean: -39.814403\n",
      "ep 1353: ep_len:129 episode reward: total was -3.170000. running mean: -39.447959\n",
      "ep 1353: ep_len:226 episode reward: total was 11.830000. running mean: -38.935180\n",
      "ep 1353: ep_len:587 episode reward: total was -53.890000. running mean: -39.084728\n",
      "epsilon:0.139973 episode_count: 9478. steps_count: 4121768.000000\n",
      "Time elapsed:  11826.244088888168\n",
      "ep 1354: ep_len:580 episode reward: total was 16.570000. running mean: -38.528181\n",
      "ep 1354: ep_len:500 episode reward: total was -33.670000. running mean: -38.479599\n",
      "ep 1354: ep_len:530 episode reward: total was -56.660000. running mean: -38.661403\n",
      "ep 1354: ep_len:54 episode reward: total was -11.210000. running mean: -38.386889\n",
      "ep 1354: ep_len:1 episode reward: total was -1.000000. running mean: -38.013020\n",
      "ep 1354: ep_len:573 episode reward: total was -62.120000. running mean: -38.254090\n",
      "ep 1354: ep_len:323 episode reward: total was -41.320000. running mean: -38.284749\n",
      "epsilon:0.139928 episode_count: 9485. steps_count: 4124329.000000\n",
      "Time elapsed:  11833.11031126976\n",
      "ep 1355: ep_len:536 episode reward: total was -94.200000. running mean: -38.843901\n",
      "ep 1355: ep_len:569 episode reward: total was -44.450000. running mean: -38.899962\n",
      "ep 1355: ep_len:577 episode reward: total was -4.640000. running mean: -38.557363\n",
      "ep 1355: ep_len:554 episode reward: total was 14.070000. running mean: -38.031089\n",
      "ep 1355: ep_len:90 episode reward: total was -2.260000. running mean: -37.673378\n",
      "ep 1355: ep_len:244 episode reward: total was 19.820000. running mean: -37.098444\n",
      "ep 1355: ep_len:187 episode reward: total was -22.670000. running mean: -36.954160\n",
      "epsilon:0.139884 episode_count: 9492. steps_count: 4127086.000000\n",
      "Time elapsed:  11840.559544324875\n",
      "ep 1356: ep_len:575 episode reward: total was -19.290000. running mean: -36.777518\n",
      "ep 1356: ep_len:500 episode reward: total was -26.910000. running mean: -36.678843\n",
      "ep 1356: ep_len:608 episode reward: total was -73.680000. running mean: -37.048855\n",
      "ep 1356: ep_len:564 episode reward: total was -2.350000. running mean: -36.701866\n",
      "ep 1356: ep_len:106 episode reward: total was -2.280000. running mean: -36.357648\n",
      "ep 1356: ep_len:547 episode reward: total was -62.920000. running mean: -36.623271\n",
      "ep 1356: ep_len:564 episode reward: total was -23.260000. running mean: -36.489638\n",
      "epsilon:0.139840 episode_count: 9499. steps_count: 4130550.000000\n",
      "Time elapsed:  11849.762787103653\n",
      "ep 1357: ep_len:526 episode reward: total was 8.680000. running mean: -36.037942\n",
      "ep 1357: ep_len:624 episode reward: total was 16.070000. running mean: -35.516863\n",
      "ep 1357: ep_len:500 episode reward: total was -32.790000. running mean: -35.489594\n",
      "ep 1357: ep_len:500 episode reward: total was -0.710000. running mean: -35.141798\n",
      "ep 1357: ep_len:99 episode reward: total was 16.230000. running mean: -34.628080\n",
      "ep 1357: ep_len:530 episode reward: total was -82.500000. running mean: -35.106799\n",
      "ep 1357: ep_len:500 episode reward: total was -73.680000. running mean: -35.492531\n",
      "epsilon:0.139795 episode_count: 9506. steps_count: 4133829.000000\n",
      "Time elapsed:  11858.366039276123\n",
      "ep 1358: ep_len:611 episode reward: total was -76.490000. running mean: -35.902506\n",
      "ep 1358: ep_len:510 episode reward: total was -41.030000. running mean: -35.953781\n",
      "ep 1358: ep_len:548 episode reward: total was -61.290000. running mean: -36.207143\n",
      "ep 1358: ep_len:37 episode reward: total was -0.840000. running mean: -35.853472\n",
      "ep 1358: ep_len:102 episode reward: total was 8.760000. running mean: -35.407337\n",
      "ep 1358: ep_len:669 episode reward: total was -53.220000. running mean: -35.585464\n",
      "ep 1358: ep_len:608 episode reward: total was -45.290000. running mean: -35.682509\n",
      "epsilon:0.139751 episode_count: 9513. steps_count: 4136914.000000\n",
      "Time elapsed:  11868.36454296112\n",
      "ep 1359: ep_len:586 episode reward: total was 16.170000. running mean: -35.163984\n",
      "ep 1359: ep_len:550 episode reward: total was 19.900000. running mean: -34.613344\n",
      "ep 1359: ep_len:577 episode reward: total was -57.910000. running mean: -34.846311\n",
      "ep 1359: ep_len:563 episode reward: total was 32.910000. running mean: -34.168747\n",
      "ep 1359: ep_len:3 episode reward: total was -1.500000. running mean: -33.842060\n",
      "ep 1359: ep_len:240 episode reward: total was 29.510000. running mean: -33.208539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1359: ep_len:553 episode reward: total was -35.680000. running mean: -33.233254\n",
      "epsilon:0.139707 episode_count: 9520. steps_count: 4139986.000000\n",
      "Time elapsed:  11876.555287361145\n",
      "ep 1360: ep_len:238 episode reward: total was 3.780000. running mean: -32.863121\n",
      "ep 1360: ep_len:575 episode reward: total was -62.980000. running mean: -33.164290\n",
      "ep 1360: ep_len:582 episode reward: total was -39.760000. running mean: -33.230247\n",
      "ep 1360: ep_len:500 episode reward: total was -27.640000. running mean: -33.174345\n",
      "ep 1360: ep_len:3 episode reward: total was 0.000000. running mean: -32.842601\n",
      "ep 1360: ep_len:514 episode reward: total was -67.590000. running mean: -33.190075\n",
      "ep 1360: ep_len:500 episode reward: total was -53.040000. running mean: -33.388575\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.139662 episode_count: 9527. steps_count: 4142898.000000\n",
      "Time elapsed:  11889.033436775208\n",
      "ep 1361: ep_len:500 episode reward: total was 36.200000. running mean: -32.692689\n",
      "ep 1361: ep_len:500 episode reward: total was -13.690000. running mean: -32.502662\n",
      "ep 1361: ep_len:551 episode reward: total was -82.650000. running mean: -33.004135\n",
      "ep 1361: ep_len:56 episode reward: total was 2.350000. running mean: -32.650594\n",
      "ep 1361: ep_len:108 episode reward: total was -5.750000. running mean: -32.381588\n",
      "ep 1361: ep_len:575 episode reward: total was -48.460000. running mean: -32.542372\n",
      "ep 1361: ep_len:540 episode reward: total was -52.100000. running mean: -32.737948\n",
      "epsilon:0.139618 episode_count: 9534. steps_count: 4145728.000000\n",
      "Time elapsed:  11897.423859119415\n",
      "ep 1362: ep_len:518 episode reward: total was -83.810000. running mean: -33.248669\n",
      "ep 1362: ep_len:576 episode reward: total was 12.620000. running mean: -32.789982\n",
      "ep 1362: ep_len:599 episode reward: total was -93.730000. running mean: -33.399382\n",
      "ep 1362: ep_len:501 episode reward: total was 2.950000. running mean: -33.035889\n",
      "ep 1362: ep_len:3 episode reward: total was 0.000000. running mean: -32.705530\n",
      "ep 1362: ep_len:524 episode reward: total was -118.080000. running mean: -33.559274\n",
      "ep 1362: ep_len:535 episode reward: total was -67.890000. running mean: -33.902582\n",
      "epsilon:0.139574 episode_count: 9541. steps_count: 4148984.000000\n",
      "Time elapsed:  11907.251681566238\n",
      "ep 1363: ep_len:568 episode reward: total was 5.790000. running mean: -33.505656\n",
      "ep 1363: ep_len:546 episode reward: total was 27.950000. running mean: -32.891099\n",
      "ep 1363: ep_len:450 episode reward: total was 14.070000. running mean: -32.421488\n",
      "ep 1363: ep_len:518 episode reward: total was -17.500000. running mean: -32.272273\n",
      "ep 1363: ep_len:42 episode reward: total was 7.500000. running mean: -31.874551\n",
      "ep 1363: ep_len:615 episode reward: total was -147.630000. running mean: -33.032105\n",
      "ep 1363: ep_len:500 episode reward: total was -106.860000. running mean: -33.770384\n",
      "epsilon:0.139529 episode_count: 9548. steps_count: 4152223.000000\n",
      "Time elapsed:  11916.017898321152\n",
      "ep 1364: ep_len:500 episode reward: total was 17.730000. running mean: -33.255380\n",
      "ep 1364: ep_len:543 episode reward: total was -59.400000. running mean: -33.516827\n",
      "ep 1364: ep_len:561 episode reward: total was -69.110000. running mean: -33.872758\n",
      "ep 1364: ep_len:509 episode reward: total was -19.960000. running mean: -33.733631\n",
      "ep 1364: ep_len:130 episode reward: total was -79.710000. running mean: -34.193394\n",
      "ep 1364: ep_len:524 episode reward: total was -81.030000. running mean: -34.661760\n",
      "ep 1364: ep_len:500 episode reward: total was -40.790000. running mean: -34.723043\n",
      "epsilon:0.139485 episode_count: 9555. steps_count: 4155490.000000\n",
      "Time elapsed:  11925.771504163742\n",
      "ep 1365: ep_len:219 episode reward: total was 0.190000. running mean: -34.373912\n",
      "ep 1365: ep_len:501 episode reward: total was -44.370000. running mean: -34.473873\n",
      "ep 1365: ep_len:523 episode reward: total was -10.460000. running mean: -34.233735\n",
      "ep 1365: ep_len:521 episode reward: total was -66.520000. running mean: -34.556597\n",
      "ep 1365: ep_len:50 episode reward: total was 13.000000. running mean: -34.081031\n",
      "ep 1365: ep_len:323 episode reward: total was -14.110000. running mean: -33.881321\n",
      "ep 1365: ep_len:291 episode reward: total was -31.640000. running mean: -33.858908\n",
      "epsilon:0.139441 episode_count: 9562. steps_count: 4157918.000000\n",
      "Time elapsed:  11932.606832504272\n",
      "ep 1366: ep_len:637 episode reward: total was -26.520000. running mean: -33.785519\n",
      "ep 1366: ep_len:626 episode reward: total was -51.680000. running mean: -33.964463\n",
      "ep 1366: ep_len:565 episode reward: total was -80.290000. running mean: -34.427719\n",
      "ep 1366: ep_len:544 episode reward: total was -9.680000. running mean: -34.180242\n",
      "ep 1366: ep_len:3 episode reward: total was 1.010000. running mean: -33.828339\n",
      "ep 1366: ep_len:500 episode reward: total was -63.570000. running mean: -34.125756\n",
      "ep 1366: ep_len:521 episode reward: total was -30.910000. running mean: -34.093598\n",
      "epsilon:0.139396 episode_count: 9569. steps_count: 4161314.000000\n",
      "Time elapsed:  11941.43769288063\n",
      "ep 1367: ep_len:600 episode reward: total was -35.010000. running mean: -34.102762\n",
      "ep 1367: ep_len:587 episode reward: total was -78.100000. running mean: -34.542735\n",
      "ep 1367: ep_len:64 episode reward: total was -8.250000. running mean: -34.279807\n",
      "ep 1367: ep_len:56 episode reward: total was -1.690000. running mean: -33.953909\n",
      "ep 1367: ep_len:83 episode reward: total was 10.740000. running mean: -33.506970\n",
      "ep 1367: ep_len:524 episode reward: total was -39.790000. running mean: -33.569800\n",
      "ep 1367: ep_len:528 episode reward: total was -47.340000. running mean: -33.707502\n",
      "epsilon:0.139352 episode_count: 9576. steps_count: 4163756.000000\n",
      "Time elapsed:  11948.127225399017\n",
      "ep 1368: ep_len:119 episode reward: total was -4.080000. running mean: -33.411227\n",
      "ep 1368: ep_len:500 episode reward: total was -10.510000. running mean: -33.182215\n",
      "ep 1368: ep_len:351 episode reward: total was -8.280000. running mean: -32.933193\n",
      "ep 1368: ep_len:626 episode reward: total was -13.710000. running mean: -32.740961\n",
      "ep 1368: ep_len:3 episode reward: total was 1.010000. running mean: -32.403451\n",
      "ep 1368: ep_len:537 episode reward: total was -30.330000. running mean: -32.382717\n",
      "ep 1368: ep_len:599 episode reward: total was -81.620000. running mean: -32.875090\n",
      "epsilon:0.139308 episode_count: 9583. steps_count: 4166491.000000\n",
      "Time elapsed:  11955.576761484146\n",
      "ep 1369: ep_len:104 episode reward: total was 7.830000. running mean: -32.468039\n",
      "ep 1369: ep_len:500 episode reward: total was -6.150000. running mean: -32.204858\n",
      "ep 1369: ep_len:651 episode reward: total was -65.830000. running mean: -32.541110\n",
      "ep 1369: ep_len:555 episode reward: total was 5.550000. running mean: -32.160199\n",
      "ep 1369: ep_len:45 episode reward: total was -39.490000. running mean: -32.233497\n",
      "ep 1369: ep_len:532 episode reward: total was -56.610000. running mean: -32.477262\n",
      "ep 1369: ep_len:500 episode reward: total was -98.040000. running mean: -33.132889\n",
      "epsilon:0.139263 episode_count: 9590. steps_count: 4169378.000000\n",
      "Time elapsed:  11964.644731760025\n",
      "ep 1370: ep_len:572 episode reward: total was -5.210000. running mean: -32.853660\n",
      "ep 1370: ep_len:501 episode reward: total was -77.430000. running mean: -33.299424\n",
      "ep 1370: ep_len:553 episode reward: total was -42.460000. running mean: -33.391029\n",
      "ep 1370: ep_len:541 episode reward: total was -10.170000. running mean: -33.158819\n",
      "ep 1370: ep_len:3 episode reward: total was -1.500000. running mean: -32.842231\n",
      "ep 1370: ep_len:531 episode reward: total was -65.890000. running mean: -33.172709\n",
      "ep 1370: ep_len:565 episode reward: total was -60.340000. running mean: -33.444382\n",
      "epsilon:0.139219 episode_count: 9597. steps_count: 4172644.000000\n",
      "Time elapsed:  11971.809297800064\n",
      "ep 1371: ep_len:542 episode reward: total was -20.990000. running mean: -33.319838\n",
      "ep 1371: ep_len:612 episode reward: total was -28.050000. running mean: -33.267139\n",
      "ep 1371: ep_len:566 episode reward: total was -40.870000. running mean: -33.343168\n",
      "ep 1371: ep_len:512 episode reward: total was -170.140000. running mean: -34.711136\n",
      "ep 1371: ep_len:116 episode reward: total was 18.790000. running mean: -34.176125\n",
      "ep 1371: ep_len:227 episode reward: total was 6.670000. running mean: -33.767664\n",
      "ep 1371: ep_len:577 episode reward: total was -75.490000. running mean: -34.184887\n",
      "epsilon:0.139175 episode_count: 9604. steps_count: 4175796.000000\n",
      "Time elapsed:  11980.330377340317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1372: ep_len:249 episode reward: total was 6.280000. running mean: -33.780238\n",
      "ep 1372: ep_len:178 episode reward: total was -40.970000. running mean: -33.852136\n",
      "ep 1372: ep_len:393 episode reward: total was 10.160000. running mean: -33.412014\n",
      "ep 1372: ep_len:500 episode reward: total was -31.020000. running mean: -33.388094\n",
      "ep 1372: ep_len:129 episode reward: total was 2.330000. running mean: -33.030913\n",
      "ep 1372: ep_len:500 episode reward: total was -10.440000. running mean: -32.805004\n",
      "ep 1372: ep_len:300 episode reward: total was -47.070000. running mean: -32.947654\n",
      "epsilon:0.139130 episode_count: 9611. steps_count: 4178045.000000\n",
      "Time elapsed:  11986.586829423904\n",
      "ep 1373: ep_len:500 episode reward: total was 37.600000. running mean: -32.242178\n",
      "ep 1373: ep_len:502 episode reward: total was -59.880000. running mean: -32.518556\n",
      "ep 1373: ep_len:556 episode reward: total was -41.830000. running mean: -32.611670\n",
      "ep 1373: ep_len:507 episode reward: total was -24.290000. running mean: -32.528454\n",
      "ep 1373: ep_len:3 episode reward: total was 0.000000. running mean: -32.203169\n",
      "ep 1373: ep_len:508 episode reward: total was -34.650000. running mean: -32.227637\n",
      "ep 1373: ep_len:584 episode reward: total was -33.310000. running mean: -32.238461\n",
      "epsilon:0.139086 episode_count: 9618. steps_count: 4181205.000000\n",
      "Time elapsed:  11994.838506698608\n",
      "ep 1374: ep_len:659 episode reward: total was -126.910000. running mean: -33.185176\n",
      "ep 1374: ep_len:512 episode reward: total was -56.360000. running mean: -33.416925\n",
      "ep 1374: ep_len:79 episode reward: total was 0.260000. running mean: -33.080155\n",
      "ep 1374: ep_len:504 episode reward: total was 5.060000. running mean: -32.698754\n",
      "ep 1374: ep_len:127 episode reward: total was 25.360000. running mean: -32.118166\n",
      "ep 1374: ep_len:506 episode reward: total was -126.510000. running mean: -33.062085\n",
      "ep 1374: ep_len:306 episode reward: total was -39.210000. running mean: -33.123564\n",
      "epsilon:0.139042 episode_count: 9625. steps_count: 4183898.000000\n",
      "Time elapsed:  12002.453579187393\n",
      "ep 1375: ep_len:228 episode reward: total was -41.960000. running mean: -33.211928\n",
      "ep 1375: ep_len:612 episode reward: total was -73.030000. running mean: -33.610109\n",
      "ep 1375: ep_len:500 episode reward: total was -35.690000. running mean: -33.630908\n",
      "ep 1375: ep_len:500 episode reward: total was -46.400000. running mean: -33.758599\n",
      "ep 1375: ep_len:3 episode reward: total was 0.000000. running mean: -33.421013\n",
      "ep 1375: ep_len:500 episode reward: total was -39.050000. running mean: -33.477303\n",
      "ep 1375: ep_len:329 episode reward: total was -59.930000. running mean: -33.741830\n",
      "epsilon:0.138997 episode_count: 9632. steps_count: 4186570.000000\n",
      "Time elapsed:  12011.287237405777\n",
      "ep 1376: ep_len:230 episode reward: total was -17.820000. running mean: -33.582611\n",
      "ep 1376: ep_len:500 episode reward: total was -7.650000. running mean: -33.323285\n",
      "ep 1376: ep_len:616 episode reward: total was -43.770000. running mean: -33.427752\n",
      "ep 1376: ep_len:531 episode reward: total was -60.450000. running mean: -33.697975\n",
      "ep 1376: ep_len:3 episode reward: total was 0.000000. running mean: -33.360995\n",
      "ep 1376: ep_len:500 episode reward: total was -33.920000. running mean: -33.366585\n",
      "ep 1376: ep_len:552 episode reward: total was -64.180000. running mean: -33.674719\n",
      "epsilon:0.138953 episode_count: 9639. steps_count: 4189502.000000\n",
      "Time elapsed:  12018.30270242691\n",
      "ep 1377: ep_len:647 episode reward: total was -94.650000. running mean: -34.284472\n",
      "ep 1377: ep_len:636 episode reward: total was 4.840000. running mean: -33.893227\n",
      "ep 1377: ep_len:429 episode reward: total was -17.520000. running mean: -33.729495\n",
      "ep 1377: ep_len:170 episode reward: total was 11.200000. running mean: -33.280200\n",
      "ep 1377: ep_len:3 episode reward: total was 0.000000. running mean: -32.947398\n",
      "ep 1377: ep_len:167 episode reward: total was 2.500000. running mean: -32.592924\n",
      "ep 1377: ep_len:336 episode reward: total was -69.660000. running mean: -32.963595\n",
      "epsilon:0.138909 episode_count: 9646. steps_count: 4191890.000000\n",
      "Time elapsed:  12024.659472703934\n",
      "ep 1378: ep_len:626 episode reward: total was 12.450000. running mean: -32.509459\n",
      "ep 1378: ep_len:543 episode reward: total was 25.350000. running mean: -31.930864\n",
      "ep 1378: ep_len:606 episode reward: total was -27.760000. running mean: -31.889156\n",
      "ep 1378: ep_len:119 episode reward: total was -3.930000. running mean: -31.609564\n",
      "ep 1378: ep_len:95 episode reward: total was 15.240000. running mean: -31.141068\n",
      "ep 1378: ep_len:543 episode reward: total was -57.760000. running mean: -31.407258\n",
      "ep 1378: ep_len:538 episode reward: total was -57.920000. running mean: -31.672385\n",
      "epsilon:0.138864 episode_count: 9653. steps_count: 4194960.000000\n",
      "Time elapsed:  12032.99039721489\n",
      "ep 1379: ep_len:506 episode reward: total was -1.180000. running mean: -31.367461\n",
      "ep 1379: ep_len:507 episode reward: total was -29.140000. running mean: -31.345187\n",
      "ep 1379: ep_len:541 episode reward: total was -38.610000. running mean: -31.417835\n",
      "ep 1379: ep_len:500 episode reward: total was -61.150000. running mean: -31.715157\n",
      "ep 1379: ep_len:73 episode reward: total was -45.350000. running mean: -31.851505\n",
      "ep 1379: ep_len:500 episode reward: total was -14.770000. running mean: -31.680690\n",
      "ep 1379: ep_len:185 episode reward: total was -15.630000. running mean: -31.520183\n",
      "epsilon:0.138820 episode_count: 9660. steps_count: 4197772.000000\n",
      "Time elapsed:  12041.08892774582\n",
      "ep 1380: ep_len:586 episode reward: total was 19.000000. running mean: -31.014981\n",
      "ep 1380: ep_len:156 episode reward: total was -22.550000. running mean: -30.930331\n",
      "ep 1380: ep_len:375 episode reward: total was 2.070000. running mean: -30.600328\n",
      "ep 1380: ep_len:104 episode reward: total was 0.490000. running mean: -30.289425\n",
      "ep 1380: ep_len:3 episode reward: total was 0.000000. running mean: -29.986531\n",
      "ep 1380: ep_len:642 episode reward: total was -55.230000. running mean: -30.238965\n",
      "ep 1380: ep_len:579 episode reward: total was -87.360000. running mean: -30.810176\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.138776 episode_count: 9667. steps_count: 4200217.000000\n",
      "Time elapsed:  12050.469021558762\n",
      "ep 1381: ep_len:560 episode reward: total was -28.530000. running mean: -30.787374\n",
      "ep 1381: ep_len:510 episode reward: total was -40.660000. running mean: -30.886100\n",
      "ep 1381: ep_len:560 episode reward: total was -77.810000. running mean: -31.355339\n",
      "ep 1381: ep_len:501 episode reward: total was -24.060000. running mean: -31.282386\n",
      "ep 1381: ep_len:89 episode reward: total was -8.240000. running mean: -31.051962\n",
      "ep 1381: ep_len:583 episode reward: total was -77.310000. running mean: -31.514542\n",
      "ep 1381: ep_len:592 episode reward: total was -53.930000. running mean: -31.738697\n",
      "epsilon:0.138731 episode_count: 9674. steps_count: 4203612.000000\n",
      "Time elapsed:  12059.359128713608\n",
      "ep 1382: ep_len:112 episode reward: total was 6.750000. running mean: -31.353810\n",
      "ep 1382: ep_len:500 episode reward: total was -7.810000. running mean: -31.118372\n",
      "ep 1382: ep_len:398 episode reward: total was -6.120000. running mean: -30.868388\n",
      "ep 1382: ep_len:500 episode reward: total was 5.580000. running mean: -30.503904\n",
      "ep 1382: ep_len:23 episode reward: total was -9.990000. running mean: -30.298765\n",
      "ep 1382: ep_len:672 episode reward: total was -59.980000. running mean: -30.595577\n",
      "ep 1382: ep_len:547 episode reward: total was -84.740000. running mean: -31.137022\n",
      "epsilon:0.138687 episode_count: 9681. steps_count: 4206364.000000\n",
      "Time elapsed:  12066.129034996033\n",
      "ep 1383: ep_len:503 episode reward: total was -89.190000. running mean: -31.717551\n",
      "ep 1383: ep_len:500 episode reward: total was -168.450000. running mean: -33.084876\n",
      "ep 1383: ep_len:583 episode reward: total was -45.050000. running mean: -33.204527\n",
      "ep 1383: ep_len:516 episode reward: total was -47.900000. running mean: -33.351482\n",
      "ep 1383: ep_len:3 episode reward: total was 0.000000. running mean: -33.017967\n",
      "ep 1383: ep_len:500 episode reward: total was -21.330000. running mean: -32.901087\n",
      "ep 1383: ep_len:621 episode reward: total was -62.060000. running mean: -33.192677\n",
      "epsilon:0.138643 episode_count: 9688. steps_count: 4209590.000000\n",
      "Time elapsed:  12074.619212388992\n",
      "ep 1384: ep_len:121 episode reward: total was -7.700000. running mean: -32.937750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1384: ep_len:500 episode reward: total was -9.880000. running mean: -32.707172\n",
      "ep 1384: ep_len:633 episode reward: total was -138.930000. running mean: -33.769401\n",
      "ep 1384: ep_len:510 episode reward: total was -14.430000. running mean: -33.576007\n",
      "ep 1384: ep_len:48 episode reward: total was 16.500000. running mean: -33.075246\n",
      "ep 1384: ep_len:500 episode reward: total was -53.380000. running mean: -33.278294\n",
      "ep 1384: ep_len:607 episode reward: total was -19.230000. running mean: -33.137811\n",
      "epsilon:0.138598 episode_count: 9695. steps_count: 4212509.000000\n",
      "Time elapsed:  12082.462743282318\n",
      "ep 1385: ep_len:566 episode reward: total was -54.670000. running mean: -33.353133\n",
      "ep 1385: ep_len:500 episode reward: total was 8.980000. running mean: -32.929802\n",
      "ep 1385: ep_len:556 episode reward: total was -43.540000. running mean: -33.035904\n",
      "ep 1385: ep_len:46 episode reward: total was -4.820000. running mean: -32.753745\n",
      "ep 1385: ep_len:128 episode reward: total was 4.800000. running mean: -32.378207\n",
      "ep 1385: ep_len:500 episode reward: total was -19.980000. running mean: -32.254225\n",
      "ep 1385: ep_len:517 episode reward: total was -65.710000. running mean: -32.588783\n",
      "epsilon:0.138554 episode_count: 9702. steps_count: 4215322.000000\n",
      "Time elapsed:  12090.240007638931\n",
      "ep 1386: ep_len:523 episode reward: total was -116.940000. running mean: -33.432295\n",
      "ep 1386: ep_len:530 episode reward: total was -10.190000. running mean: -33.199872\n",
      "ep 1386: ep_len:500 episode reward: total was -48.870000. running mean: -33.356573\n",
      "ep 1386: ep_len:130 episode reward: total was -3.010000. running mean: -33.053108\n",
      "ep 1386: ep_len:112 episode reward: total was -21.220000. running mean: -32.934777\n",
      "ep 1386: ep_len:500 episode reward: total was -53.000000. running mean: -33.135429\n",
      "ep 1386: ep_len:500 episode reward: total was -42.680000. running mean: -33.230874\n",
      "epsilon:0.138510 episode_count: 9709. steps_count: 4218117.000000\n",
      "Time elapsed:  12097.767005443573\n",
      "ep 1387: ep_len:518 episode reward: total was -120.400000. running mean: -34.102566\n",
      "ep 1387: ep_len:346 episode reward: total was -42.580000. running mean: -34.187340\n",
      "ep 1387: ep_len:615 episode reward: total was -167.930000. running mean: -35.524767\n",
      "ep 1387: ep_len:617 episode reward: total was 5.670000. running mean: -35.112819\n",
      "ep 1387: ep_len:3 episode reward: total was 1.010000. running mean: -34.751591\n",
      "ep 1387: ep_len:526 episode reward: total was -158.360000. running mean: -35.987675\n",
      "ep 1387: ep_len:565 episode reward: total was -82.320000. running mean: -36.450998\n",
      "epsilon:0.138465 episode_count: 9716. steps_count: 4221307.000000\n",
      "Time elapsed:  12106.280548810959\n",
      "ep 1388: ep_len:563 episode reward: total was -52.010000. running mean: -36.606588\n",
      "ep 1388: ep_len:500 episode reward: total was -90.300000. running mean: -37.143522\n",
      "ep 1388: ep_len:500 episode reward: total was -19.040000. running mean: -36.962487\n",
      "ep 1388: ep_len:510 episode reward: total was -44.040000. running mean: -37.033262\n",
      "ep 1388: ep_len:3 episode reward: total was 0.000000. running mean: -36.662930\n",
      "ep 1388: ep_len:589 episode reward: total was -56.100000. running mean: -36.857300\n",
      "ep 1388: ep_len:623 episode reward: total was -40.250000. running mean: -36.891227\n",
      "epsilon:0.138421 episode_count: 9723. steps_count: 4224595.000000\n",
      "Time elapsed:  12114.880807876587\n",
      "ep 1389: ep_len:570 episode reward: total was -40.490000. running mean: -36.927215\n",
      "ep 1389: ep_len:500 episode reward: total was -15.590000. running mean: -36.713843\n",
      "ep 1389: ep_len:500 episode reward: total was -34.070000. running mean: -36.687404\n",
      "ep 1389: ep_len:625 episode reward: total was -18.210000. running mean: -36.502630\n",
      "ep 1389: ep_len:3 episode reward: total was 0.000000. running mean: -36.137604\n",
      "ep 1389: ep_len:519 episode reward: total was -59.460000. running mean: -36.370828\n",
      "ep 1389: ep_len:591 episode reward: total was -52.960000. running mean: -36.536720\n",
      "epsilon:0.138377 episode_count: 9730. steps_count: 4227903.000000\n",
      "Time elapsed:  12123.346255540848\n",
      "ep 1390: ep_len:134 episode reward: total was 6.050000. running mean: -36.110853\n",
      "ep 1390: ep_len:548 episode reward: total was -140.250000. running mean: -37.152244\n",
      "ep 1390: ep_len:623 episode reward: total was -181.340000. running mean: -38.594122\n",
      "ep 1390: ep_len:551 episode reward: total was 13.350000. running mean: -38.074680\n",
      "ep 1390: ep_len:108 episode reward: total was 22.230000. running mean: -37.471634\n",
      "ep 1390: ep_len:503 episode reward: total was -33.490000. running mean: -37.431817\n",
      "ep 1390: ep_len:620 episode reward: total was -122.470000. running mean: -38.282199\n",
      "epsilon:0.138332 episode_count: 9737. steps_count: 4230990.000000\n",
      "Time elapsed:  12131.592343091965\n",
      "ep 1391: ep_len:637 episode reward: total was -6.070000. running mean: -37.960077\n",
      "ep 1391: ep_len:371 episode reward: total was -26.540000. running mean: -37.845876\n",
      "ep 1391: ep_len:361 episode reward: total was -3.950000. running mean: -37.506918\n",
      "ep 1391: ep_len:500 episode reward: total was -9.380000. running mean: -37.225648\n",
      "ep 1391: ep_len:3 episode reward: total was 0.000000. running mean: -36.853392\n",
      "ep 1391: ep_len:530 episode reward: total was -13.490000. running mean: -36.619758\n",
      "ep 1391: ep_len:564 episode reward: total was -17.780000. running mean: -36.431360\n",
      "epsilon:0.138288 episode_count: 9744. steps_count: 4233956.000000\n",
      "Time elapsed:  12139.693348407745\n",
      "ep 1392: ep_len:917 episode reward: total was -423.050000. running mean: -40.297547\n",
      "ep 1392: ep_len:634 episode reward: total was -30.270000. running mean: -40.197271\n",
      "ep 1392: ep_len:397 episode reward: total was -3.710000. running mean: -39.832399\n",
      "ep 1392: ep_len:607 episode reward: total was -6.250000. running mean: -39.496575\n",
      "ep 1392: ep_len:107 episode reward: total was 18.210000. running mean: -38.919509\n",
      "ep 1392: ep_len:500 episode reward: total was -17.160000. running mean: -38.701914\n",
      "ep 1392: ep_len:346 episode reward: total was -36.560000. running mean: -38.680495\n",
      "epsilon:0.138244 episode_count: 9751. steps_count: 4237464.000000\n",
      "Time elapsed:  12149.212909936905\n",
      "ep 1393: ep_len:552 episode reward: total was -14.020000. running mean: -38.433890\n",
      "ep 1393: ep_len:535 episode reward: total was -3.210000. running mean: -38.081651\n",
      "ep 1393: ep_len:627 episode reward: total was -57.290000. running mean: -38.273734\n",
      "ep 1393: ep_len:518 episode reward: total was -38.190000. running mean: -38.272897\n",
      "ep 1393: ep_len:94 episode reward: total was 6.260000. running mean: -37.827568\n",
      "ep 1393: ep_len:609 episode reward: total was -18.090000. running mean: -37.630192\n",
      "ep 1393: ep_len:305 episode reward: total was -49.830000. running mean: -37.752190\n",
      "epsilon:0.138199 episode_count: 9758. steps_count: 4240704.000000\n",
      "Time elapsed:  12157.975983381271\n",
      "ep 1394: ep_len:501 episode reward: total was -102.340000. running mean: -38.398068\n",
      "ep 1394: ep_len:352 episode reward: total was -46.660000. running mean: -38.480688\n",
      "ep 1394: ep_len:539 episode reward: total was -68.250000. running mean: -38.778381\n",
      "ep 1394: ep_len:500 episode reward: total was -11.810000. running mean: -38.508697\n",
      "ep 1394: ep_len:3 episode reward: total was 0.000000. running mean: -38.123610\n",
      "ep 1394: ep_len:500 episode reward: total was -41.710000. running mean: -38.159474\n",
      "ep 1394: ep_len:577 episode reward: total was -50.310000. running mean: -38.280979\n",
      "epsilon:0.138155 episode_count: 9765. steps_count: 4243676.000000\n",
      "Time elapsed:  12167.348530292511\n",
      "ep 1395: ep_len:228 episode reward: total was -39.440000. running mean: -38.292569\n",
      "ep 1395: ep_len:172 episode reward: total was -19.240000. running mean: -38.102044\n",
      "ep 1395: ep_len:79 episode reward: total was -27.260000. running mean: -37.993623\n",
      "ep 1395: ep_len:500 episode reward: total was -96.670000. running mean: -38.580387\n",
      "ep 1395: ep_len:3 episode reward: total was 0.000000. running mean: -38.194583\n",
      "ep 1395: ep_len:530 episode reward: total was -45.580000. running mean: -38.268437\n",
      "ep 1395: ep_len:530 episode reward: total was -45.880000. running mean: -38.344553\n",
      "epsilon:0.138111 episode_count: 9772. steps_count: 4245718.000000\n",
      "Time elapsed:  12173.060688495636\n",
      "ep 1396: ep_len:232 episode reward: total was -12.710000. running mean: -38.088208\n",
      "ep 1396: ep_len:581 episode reward: total was -22.060000. running mean: -37.927925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1396: ep_len:530 episode reward: total was -16.900000. running mean: -37.717646\n",
      "ep 1396: ep_len:620 episode reward: total was 19.130000. running mean: -37.149170\n",
      "ep 1396: ep_len:3 episode reward: total was 0.000000. running mean: -36.777678\n",
      "ep 1396: ep_len:500 episode reward: total was -37.100000. running mean: -36.780901\n",
      "ep 1396: ep_len:182 episode reward: total was -42.710000. running mean: -36.840192\n",
      "epsilon:0.138066 episode_count: 9779. steps_count: 4248366.000000\n",
      "Time elapsed:  12180.84955573082\n",
      "ep 1397: ep_len:216 episode reward: total was -4.970000. running mean: -36.521490\n",
      "ep 1397: ep_len:500 episode reward: total was 2.180000. running mean: -36.134475\n",
      "ep 1397: ep_len:544 episode reward: total was -162.390000. running mean: -37.397031\n",
      "ep 1397: ep_len:500 episode reward: total was -3.250000. running mean: -37.055560\n",
      "ep 1397: ep_len:3 episode reward: total was 1.010000. running mean: -36.674905\n",
      "ep 1397: ep_len:539 episode reward: total was -51.700000. running mean: -36.825156\n",
      "ep 1397: ep_len:592 episode reward: total was -111.480000. running mean: -37.571704\n",
      "epsilon:0.138022 episode_count: 9786. steps_count: 4251260.000000\n",
      "Time elapsed:  12188.62604856491\n",
      "ep 1398: ep_len:513 episode reward: total was -74.590000. running mean: -37.941887\n",
      "ep 1398: ep_len:504 episode reward: total was 45.240000. running mean: -37.110068\n",
      "ep 1398: ep_len:562 episode reward: total was -69.130000. running mean: -37.430268\n",
      "ep 1398: ep_len:500 episode reward: total was 11.720000. running mean: -36.938765\n",
      "ep 1398: ep_len:111 episode reward: total was 12.740000. running mean: -36.441977\n",
      "ep 1398: ep_len:538 episode reward: total was -54.970000. running mean: -36.627257\n",
      "ep 1398: ep_len:507 episode reward: total was -28.660000. running mean: -36.547585\n",
      "epsilon:0.137978 episode_count: 9793. steps_count: 4254495.000000\n",
      "Time elapsed:  12198.3285176754\n",
      "ep 1399: ep_len:555 episode reward: total was -34.060000. running mean: -36.522709\n",
      "ep 1399: ep_len:534 episode reward: total was -88.520000. running mean: -37.042682\n",
      "ep 1399: ep_len:500 episode reward: total was -89.530000. running mean: -37.567555\n",
      "ep 1399: ep_len:527 episode reward: total was -61.040000. running mean: -37.802280\n",
      "ep 1399: ep_len:50 episode reward: total was 17.500000. running mean: -37.249257\n",
      "ep 1399: ep_len:506 episode reward: total was -21.340000. running mean: -37.090164\n",
      "ep 1399: ep_len:506 episode reward: total was -53.090000. running mean: -37.250163\n",
      "epsilon:0.137933 episode_count: 9800. steps_count: 4257673.000000\n",
      "Time elapsed:  12208.169329166412\n",
      "ep 1400: ep_len:667 episode reward: total was -91.350000. running mean: -37.791161\n",
      "ep 1400: ep_len:572 episode reward: total was -81.190000. running mean: -38.225149\n",
      "ep 1400: ep_len:500 episode reward: total was -61.300000. running mean: -38.455898\n",
      "ep 1400: ep_len:501 episode reward: total was -14.590000. running mean: -38.217239\n",
      "ep 1400: ep_len:52 episode reward: total was 20.000000. running mean: -37.635066\n",
      "ep 1400: ep_len:500 episode reward: total was -2.310000. running mean: -37.281816\n",
      "ep 1400: ep_len:566 episode reward: total was -49.560000. running mean: -37.404598\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.137889 episode_count: 9807. steps_count: 4261031.000000\n",
      "Time elapsed:  12223.644237041473\n",
      "ep 1401: ep_len:615 episode reward: total was -57.840000. running mean: -37.608952\n",
      "ep 1401: ep_len:641 episode reward: total was -0.810000. running mean: -37.240962\n",
      "ep 1401: ep_len:609 episode reward: total was -67.640000. running mean: -37.544953\n",
      "ep 1401: ep_len:566 episode reward: total was -23.110000. running mean: -37.400603\n",
      "ep 1401: ep_len:101 episode reward: total was -6.770000. running mean: -37.094297\n",
      "ep 1401: ep_len:500 episode reward: total was -67.080000. running mean: -37.394154\n",
      "ep 1401: ep_len:614 episode reward: total was -114.620000. running mean: -38.166412\n",
      "epsilon:0.137845 episode_count: 9814. steps_count: 4264677.000000\n",
      "Time elapsed:  12232.960662841797\n",
      "ep 1402: ep_len:605 episode reward: total was 21.110000. running mean: -37.573648\n",
      "ep 1402: ep_len:570 episode reward: total was 35.540000. running mean: -36.842512\n",
      "ep 1402: ep_len:636 episode reward: total was -98.000000. running mean: -37.454087\n",
      "ep 1402: ep_len:404 episode reward: total was -5.710000. running mean: -37.136646\n",
      "ep 1402: ep_len:3 episode reward: total was 0.000000. running mean: -36.765279\n",
      "ep 1402: ep_len:564 episode reward: total was -107.400000. running mean: -37.471627\n",
      "ep 1402: ep_len:321 episode reward: total was -33.260000. running mean: -37.429510\n",
      "epsilon:0.137800 episode_count: 9821. steps_count: 4267780.000000\n",
      "Time elapsed:  12241.224752664566\n",
      "ep 1403: ep_len:534 episode reward: total was -10.630000. running mean: -37.161515\n",
      "ep 1403: ep_len:532 episode reward: total was -76.220000. running mean: -37.552100\n",
      "ep 1403: ep_len:527 episode reward: total was -32.940000. running mean: -37.505979\n",
      "ep 1403: ep_len:536 episode reward: total was 17.870000. running mean: -36.952219\n",
      "ep 1403: ep_len:3 episode reward: total was -1.500000. running mean: -36.597697\n",
      "ep 1403: ep_len:527 episode reward: total was -44.050000. running mean: -36.672220\n",
      "ep 1403: ep_len:526 episode reward: total was -55.830000. running mean: -36.863798\n",
      "epsilon:0.137756 episode_count: 9828. steps_count: 4270965.000000\n",
      "Time elapsed:  12249.74806690216\n",
      "ep 1404: ep_len:532 episode reward: total was 3.200000. running mean: -36.463160\n",
      "ep 1404: ep_len:506 episode reward: total was -78.670000. running mean: -36.885228\n",
      "ep 1404: ep_len:436 episode reward: total was -5.390000. running mean: -36.570276\n",
      "ep 1404: ep_len:505 episode reward: total was -9.220000. running mean: -36.296773\n",
      "ep 1404: ep_len:109 episode reward: total was 6.840000. running mean: -35.865406\n",
      "ep 1404: ep_len:550 episode reward: total was 10.550000. running mean: -35.401252\n",
      "ep 1404: ep_len:600 episode reward: total was -40.780000. running mean: -35.455039\n",
      "epsilon:0.137712 episode_count: 9835. steps_count: 4274203.000000\n",
      "Time elapsed:  12258.307681560516\n",
      "ep 1405: ep_len:671 episode reward: total was -81.580000. running mean: -35.916289\n",
      "ep 1405: ep_len:336 episode reward: total was -20.980000. running mean: -35.766926\n",
      "ep 1405: ep_len:539 episode reward: total was -55.020000. running mean: -35.959456\n",
      "ep 1405: ep_len:578 episode reward: total was -49.030000. running mean: -36.090162\n",
      "ep 1405: ep_len:98 episode reward: total was 17.720000. running mean: -35.552060\n",
      "ep 1405: ep_len:518 episode reward: total was -50.540000. running mean: -35.701940\n",
      "ep 1405: ep_len:500 episode reward: total was -33.250000. running mean: -35.677420\n",
      "epsilon:0.137667 episode_count: 9842. steps_count: 4277443.000000\n",
      "Time elapsed:  12265.199456691742\n",
      "ep 1406: ep_len:607 episode reward: total was 15.560000. running mean: -35.165046\n",
      "ep 1406: ep_len:500 episode reward: total was -6.960000. running mean: -34.882996\n",
      "ep 1406: ep_len:556 episode reward: total was -48.020000. running mean: -35.014366\n",
      "ep 1406: ep_len:554 episode reward: total was -15.300000. running mean: -34.817222\n",
      "ep 1406: ep_len:3 episode reward: total was -1.500000. running mean: -34.484050\n",
      "ep 1406: ep_len:531 episode reward: total was -68.930000. running mean: -34.828509\n",
      "ep 1406: ep_len:500 episode reward: total was -3.290000. running mean: -34.513124\n",
      "epsilon:0.137623 episode_count: 9849. steps_count: 4280694.000000\n",
      "Time elapsed:  12273.913741111755\n",
      "ep 1407: ep_len:603 episode reward: total was -5.050000. running mean: -34.218493\n",
      "ep 1407: ep_len:500 episode reward: total was -43.320000. running mean: -34.309508\n",
      "ep 1407: ep_len:653 episode reward: total was -109.810000. running mean: -35.064513\n",
      "ep 1407: ep_len:500 episode reward: total was -67.650000. running mean: -35.390368\n",
      "ep 1407: ep_len:93 episode reward: total was 11.730000. running mean: -34.919164\n",
      "ep 1407: ep_len:621 episode reward: total was -54.400000. running mean: -35.113973\n",
      "ep 1407: ep_len:564 episode reward: total was -48.500000. running mean: -35.247833\n",
      "epsilon:0.137579 episode_count: 9856. steps_count: 4284228.000000\n",
      "Time elapsed:  12283.182760477066\n",
      "ep 1408: ep_len:134 episode reward: total was 2.440000. running mean: -34.870954\n",
      "ep 1408: ep_len:548 episode reward: total was -7.200000. running mean: -34.594245\n",
      "ep 1408: ep_len:681 episode reward: total was -59.060000. running mean: -34.838902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1408: ep_len:500 episode reward: total was -21.690000. running mean: -34.707413\n",
      "ep 1408: ep_len:54 episode reward: total was 18.000000. running mean: -34.180339\n",
      "ep 1408: ep_len:517 episode reward: total was -82.530000. running mean: -34.663836\n",
      "ep 1408: ep_len:543 episode reward: total was -51.470000. running mean: -34.831898\n",
      "epsilon:0.137534 episode_count: 9863. steps_count: 4287205.000000\n",
      "Time elapsed:  12291.185270786285\n",
      "ep 1409: ep_len:501 episode reward: total was 0.390000. running mean: -34.479679\n",
      "ep 1409: ep_len:500 episode reward: total was 35.050000. running mean: -33.784382\n",
      "ep 1409: ep_len:574 episode reward: total was -40.230000. running mean: -33.848838\n",
      "ep 1409: ep_len:546 episode reward: total was -64.760000. running mean: -34.157950\n",
      "ep 1409: ep_len:3 episode reward: total was 0.000000. running mean: -33.816370\n",
      "ep 1409: ep_len:500 episode reward: total was -55.120000. running mean: -34.029406\n",
      "ep 1409: ep_len:200 episode reward: total was -18.050000. running mean: -33.869612\n",
      "epsilon:0.137490 episode_count: 9870. steps_count: 4290029.000000\n",
      "Time elapsed:  12300.305089235306\n",
      "ep 1410: ep_len:500 episode reward: total was 43.630000. running mean: -33.094616\n",
      "ep 1410: ep_len:512 episode reward: total was -34.890000. running mean: -33.112570\n",
      "ep 1410: ep_len:500 episode reward: total was -54.950000. running mean: -33.330944\n",
      "ep 1410: ep_len:510 episode reward: total was -28.490000. running mean: -33.282535\n",
      "ep 1410: ep_len:3 episode reward: total was 0.000000. running mean: -32.949710\n",
      "ep 1410: ep_len:500 episode reward: total was -46.710000. running mean: -33.087312\n",
      "ep 1410: ep_len:593 episode reward: total was -73.450000. running mean: -33.490939\n",
      "epsilon:0.137446 episode_count: 9877. steps_count: 4293147.000000\n",
      "Time elapsed:  12308.533376455307\n",
      "ep 1411: ep_len:502 episode reward: total was -50.150000. running mean: -33.657530\n",
      "ep 1411: ep_len:609 episode reward: total was -2.200000. running mean: -33.342955\n",
      "ep 1411: ep_len:613 episode reward: total was -15.500000. running mean: -33.164525\n",
      "ep 1411: ep_len:56 episode reward: total was -2.180000. running mean: -32.854680\n",
      "ep 1411: ep_len:116 episode reward: total was 21.760000. running mean: -32.308533\n",
      "ep 1411: ep_len:554 episode reward: total was -34.760000. running mean: -32.333048\n",
      "ep 1411: ep_len:502 episode reward: total was -65.630000. running mean: -32.666017\n",
      "epsilon:0.137401 episode_count: 9884. steps_count: 4296099.000000\n",
      "Time elapsed:  12315.902607917786\n",
      "ep 1412: ep_len:545 episode reward: total was -65.790000. running mean: -32.997257\n",
      "ep 1412: ep_len:500 episode reward: total was -3.940000. running mean: -32.706684\n",
      "ep 1412: ep_len:649 episode reward: total was -72.900000. running mean: -33.108618\n",
      "ep 1412: ep_len:526 episode reward: total was 1.830000. running mean: -32.759231\n",
      "ep 1412: ep_len:3 episode reward: total was -1.500000. running mean: -32.446639\n",
      "ep 1412: ep_len:558 episode reward: total was -69.560000. running mean: -32.817773\n",
      "ep 1412: ep_len:500 episode reward: total was -43.580000. running mean: -32.925395\n",
      "epsilon:0.137357 episode_count: 9891. steps_count: 4299380.000000\n",
      "Time elapsed:  12322.10340809822\n",
      "ep 1413: ep_len:500 episode reward: total was -4.740000. running mean: -32.643541\n",
      "ep 1413: ep_len:580 episode reward: total was -54.770000. running mean: -32.864806\n",
      "ep 1413: ep_len:507 episode reward: total was -100.810000. running mean: -33.544258\n",
      "ep 1413: ep_len:588 episode reward: total was 3.330000. running mean: -33.175515\n",
      "ep 1413: ep_len:3 episode reward: total was 1.010000. running mean: -32.833660\n",
      "ep 1413: ep_len:500 episode reward: total was -18.540000. running mean: -32.690723\n",
      "ep 1413: ep_len:500 episode reward: total was -44.000000. running mean: -32.803816\n",
      "epsilon:0.137313 episode_count: 9898. steps_count: 4302558.000000\n",
      "Time elapsed:  12331.28628706932\n",
      "ep 1414: ep_len:548 episode reward: total was -49.380000. running mean: -32.969578\n",
      "ep 1414: ep_len:192 episode reward: total was -23.260000. running mean: -32.872482\n",
      "ep 1414: ep_len:517 episode reward: total was 9.100000. running mean: -32.452757\n",
      "ep 1414: ep_len:510 episode reward: total was -52.490000. running mean: -32.653130\n",
      "ep 1414: ep_len:3 episode reward: total was 0.000000. running mean: -32.326598\n",
      "ep 1414: ep_len:500 episode reward: total was -82.160000. running mean: -32.824932\n",
      "ep 1414: ep_len:578 episode reward: total was -203.640000. running mean: -34.533083\n",
      "epsilon:0.137268 episode_count: 9905. steps_count: 4305406.000000\n",
      "Time elapsed:  12340.581680059433\n",
      "ep 1415: ep_len:500 episode reward: total was 21.250000. running mean: -33.975252\n",
      "ep 1415: ep_len:500 episode reward: total was -161.740000. running mean: -35.252900\n",
      "ep 1415: ep_len:360 episode reward: total was -10.390000. running mean: -35.004271\n",
      "ep 1415: ep_len:500 episode reward: total was -39.590000. running mean: -35.050128\n",
      "ep 1415: ep_len:50 episode reward: total was 13.000000. running mean: -34.569627\n",
      "ep 1415: ep_len:596 episode reward: total was -8.090000. running mean: -34.304831\n",
      "ep 1415: ep_len:577 episode reward: total was -44.760000. running mean: -34.409382\n",
      "epsilon:0.137224 episode_count: 9912. steps_count: 4308489.000000\n",
      "Time elapsed:  12351.661774873734\n",
      "ep 1416: ep_len:500 episode reward: total was -19.480000. running mean: -34.260088\n",
      "ep 1416: ep_len:598 episode reward: total was -36.530000. running mean: -34.282787\n",
      "ep 1416: ep_len:570 episode reward: total was -139.760000. running mean: -35.337560\n",
      "ep 1416: ep_len:509 episode reward: total was -25.720000. running mean: -35.241384\n",
      "ep 1416: ep_len:3 episode reward: total was 0.000000. running mean: -34.888970\n",
      "ep 1416: ep_len:521 episode reward: total was -105.680000. running mean: -35.596880\n",
      "ep 1416: ep_len:526 episode reward: total was -51.300000. running mean: -35.753912\n",
      "epsilon:0.137180 episode_count: 9919. steps_count: 4311716.000000\n",
      "Time elapsed:  12360.136200904846\n",
      "ep 1417: ep_len:263 episode reward: total was 4.370000. running mean: -35.352673\n",
      "ep 1417: ep_len:500 episode reward: total was -95.070000. running mean: -35.949846\n",
      "ep 1417: ep_len:500 episode reward: total was -35.860000. running mean: -35.948947\n",
      "ep 1417: ep_len:113 episode reward: total was 6.860000. running mean: -35.520858\n",
      "ep 1417: ep_len:3 episode reward: total was 1.010000. running mean: -35.155549\n",
      "ep 1417: ep_len:533 episode reward: total was -125.410000. running mean: -36.058094\n",
      "ep 1417: ep_len:562 episode reward: total was -86.550000. running mean: -36.563013\n",
      "epsilon:0.137135 episode_count: 9926. steps_count: 4314190.000000\n",
      "Time elapsed:  12366.982242822647\n",
      "ep 1418: ep_len:588 episode reward: total was 6.160000. running mean: -36.135783\n",
      "ep 1418: ep_len:500 episode reward: total was -45.560000. running mean: -36.230025\n",
      "ep 1418: ep_len:389 episode reward: total was 34.440000. running mean: -35.523325\n",
      "ep 1418: ep_len:543 episode reward: total was -2.880000. running mean: -35.196891\n",
      "ep 1418: ep_len:3 episode reward: total was 1.010000. running mean: -34.834823\n",
      "ep 1418: ep_len:228 episode reward: total was 21.830000. running mean: -34.268174\n",
      "ep 1418: ep_len:194 episode reward: total was -31.700000. running mean: -34.242493\n",
      "epsilon:0.137091 episode_count: 9933. steps_count: 4316635.000000\n",
      "Time elapsed:  12373.69326043129\n",
      "ep 1419: ep_len:246 episode reward: total was 5.270000. running mean: -33.847368\n",
      "ep 1419: ep_len:500 episode reward: total was -15.600000. running mean: -33.664894\n",
      "ep 1419: ep_len:395 episode reward: total was 8.920000. running mean: -33.239045\n",
      "ep 1419: ep_len:500 episode reward: total was -15.410000. running mean: -33.060755\n",
      "ep 1419: ep_len:89 episode reward: total was 6.210000. running mean: -32.668047\n",
      "ep 1419: ep_len:316 episode reward: total was 6.110000. running mean: -32.280267\n",
      "ep 1419: ep_len:593 episode reward: total was -49.300000. running mean: -32.450464\n",
      "epsilon:0.137047 episode_count: 9940. steps_count: 4319274.000000\n",
      "Time elapsed:  12381.002111911774\n",
      "ep 1420: ep_len:572 episode reward: total was -88.700000. running mean: -33.012959\n",
      "ep 1420: ep_len:515 episode reward: total was -62.250000. running mean: -33.305330\n",
      "ep 1420: ep_len:349 episode reward: total was -3.210000. running mean: -33.004376\n",
      "ep 1420: ep_len:584 episode reward: total was -20.530000. running mean: -32.879633\n",
      "ep 1420: ep_len:79 episode reward: total was 6.750000. running mean: -32.483336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1420: ep_len:167 episode reward: total was 8.650000. running mean: -32.072003\n",
      "ep 1420: ep_len:598 episode reward: total was -35.940000. running mean: -32.110683\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.137002 episode_count: 9947. steps_count: 4322138.000000\n",
      "Time elapsed:  12392.901514530182\n",
      "ep 1421: ep_len:576 episode reward: total was 21.370000. running mean: -31.575876\n",
      "ep 1421: ep_len:528 episode reward: total was -20.620000. running mean: -31.466317\n",
      "ep 1421: ep_len:575 episode reward: total was -77.560000. running mean: -31.927254\n",
      "ep 1421: ep_len:157 episode reward: total was -5.430000. running mean: -31.662282\n",
      "ep 1421: ep_len:3 episode reward: total was 0.000000. running mean: -31.345659\n",
      "ep 1421: ep_len:602 episode reward: total was -47.320000. running mean: -31.505402\n",
      "ep 1421: ep_len:509 episode reward: total was -52.420000. running mean: -31.714548\n",
      "epsilon:0.136958 episode_count: 9954. steps_count: 4325088.000000\n",
      "Time elapsed:  12402.495551347733\n",
      "ep 1422: ep_len:255 episode reward: total was -4.300000. running mean: -31.440403\n",
      "ep 1422: ep_len:500 episode reward: total was -18.190000. running mean: -31.307899\n",
      "ep 1422: ep_len:68 episode reward: total was -6.370000. running mean: -31.058520\n",
      "ep 1422: ep_len:126 episode reward: total was 1.080000. running mean: -30.737134\n",
      "ep 1422: ep_len:104 episode reward: total was 19.250000. running mean: -30.237263\n",
      "ep 1422: ep_len:562 episode reward: total was -68.900000. running mean: -30.623890\n",
      "ep 1422: ep_len:530 episode reward: total was -29.900000. running mean: -30.616652\n",
      "epsilon:0.136914 episode_count: 9961. steps_count: 4327233.000000\n",
      "Time elapsed:  12409.052999734879\n",
      "ep 1423: ep_len:507 episode reward: total was 18.850000. running mean: -30.121985\n",
      "ep 1423: ep_len:550 episode reward: total was 54.430000. running mean: -29.276465\n",
      "ep 1423: ep_len:541 episode reward: total was -62.580000. running mean: -29.609501\n",
      "ep 1423: ep_len:536 episode reward: total was 6.420000. running mean: -29.249206\n",
      "ep 1423: ep_len:3 episode reward: total was -1.500000. running mean: -28.971713\n",
      "ep 1423: ep_len:546 episode reward: total was -44.470000. running mean: -29.126696\n",
      "ep 1423: ep_len:209 episode reward: total was -13.820000. running mean: -28.973629\n",
      "epsilon:0.136869 episode_count: 9968. steps_count: 4330125.000000\n",
      "Time elapsed:  12416.7047021389\n",
      "ep 1424: ep_len:249 episode reward: total was -17.190000. running mean: -28.855793\n",
      "ep 1424: ep_len:500 episode reward: total was -51.800000. running mean: -29.085235\n",
      "ep 1424: ep_len:74 episode reward: total was 2.290000. running mean: -28.771483\n",
      "ep 1424: ep_len:517 episode reward: total was -40.910000. running mean: -28.892868\n",
      "ep 1424: ep_len:3 episode reward: total was -1.500000. running mean: -28.618939\n",
      "ep 1424: ep_len:680 episode reward: total was -13.040000. running mean: -28.463150\n",
      "ep 1424: ep_len:500 episode reward: total was -35.400000. running mean: -28.532518\n",
      "epsilon:0.136825 episode_count: 9975. steps_count: 4332648.000000\n",
      "Time elapsed:  12423.526270389557\n",
      "ep 1425: ep_len:229 episode reward: total was -16.880000. running mean: -28.415993\n",
      "ep 1425: ep_len:500 episode reward: total was -140.440000. running mean: -29.536233\n",
      "ep 1425: ep_len:66 episode reward: total was -0.300000. running mean: -29.243871\n",
      "ep 1425: ep_len:506 episode reward: total was -133.630000. running mean: -30.287732\n",
      "ep 1425: ep_len:53 episode reward: total was 17.500000. running mean: -29.809855\n",
      "ep 1425: ep_len:500 episode reward: total was -154.840000. running mean: -31.060156\n",
      "ep 1425: ep_len:263 episode reward: total was -41.950000. running mean: -31.169055\n",
      "epsilon:0.136781 episode_count: 9982. steps_count: 4334765.000000\n",
      "Time elapsed:  12429.403114557266\n",
      "ep 1426: ep_len:644 episode reward: total was -156.140000. running mean: -32.418764\n",
      "ep 1426: ep_len:612 episode reward: total was -13.600000. running mean: -32.230577\n",
      "ep 1426: ep_len:363 episode reward: total was -16.480000. running mean: -32.073071\n",
      "ep 1426: ep_len:500 episode reward: total was -27.830000. running mean: -32.030640\n",
      "ep 1426: ep_len:3 episode reward: total was 0.000000. running mean: -31.710334\n",
      "ep 1426: ep_len:583 episode reward: total was -16.730000. running mean: -31.560530\n",
      "ep 1426: ep_len:586 episode reward: total was -38.290000. running mean: -31.627825\n",
      "epsilon:0.136736 episode_count: 9989. steps_count: 4338056.000000\n",
      "Time elapsed:  12437.994299411774\n",
      "ep 1427: ep_len:539 episode reward: total was 30.410000. running mean: -31.007447\n",
      "ep 1427: ep_len:533 episode reward: total was -36.210000. running mean: -31.059472\n",
      "ep 1427: ep_len:572 episode reward: total was -71.690000. running mean: -31.465778\n",
      "ep 1427: ep_len:124 episode reward: total was -5.470000. running mean: -31.205820\n",
      "ep 1427: ep_len:48 episode reward: total was 19.500000. running mean: -30.698762\n",
      "ep 1427: ep_len:539 episode reward: total was -13.130000. running mean: -30.523074\n",
      "ep 1427: ep_len:516 episode reward: total was -39.030000. running mean: -30.608143\n",
      "epsilon:0.136692 episode_count: 9996. steps_count: 4340927.000000\n",
      "Time elapsed:  12445.806774616241\n",
      "ep 1428: ep_len:686 episode reward: total was -95.200000. running mean: -31.254062\n",
      "ep 1428: ep_len:277 episode reward: total was -35.460000. running mean: -31.296121\n",
      "ep 1428: ep_len:623 episode reward: total was -47.080000. running mean: -31.453960\n",
      "ep 1428: ep_len:401 episode reward: total was -39.760000. running mean: -31.537020\n",
      "ep 1428: ep_len:93 episode reward: total was 7.230000. running mean: -31.149350\n",
      "ep 1428: ep_len:500 episode reward: total was -24.310000. running mean: -31.080957\n",
      "ep 1428: ep_len:513 episode reward: total was -56.550000. running mean: -31.335647\n",
      "epsilon:0.136648 episode_count: 10003. steps_count: 4344020.000000\n",
      "Time elapsed:  12454.104764223099\n",
      "ep 1429: ep_len:532 episode reward: total was -61.110000. running mean: -31.633391\n",
      "ep 1429: ep_len:580 episode reward: total was 42.830000. running mean: -30.888757\n",
      "ep 1429: ep_len:451 episode reward: total was -7.530000. running mean: -30.655169\n",
      "ep 1429: ep_len:544 episode reward: total was -26.710000. running mean: -30.615718\n",
      "ep 1429: ep_len:3 episode reward: total was 1.010000. running mean: -30.299460\n",
      "ep 1429: ep_len:508 episode reward: total was -163.320000. running mean: -31.629666\n",
      "ep 1429: ep_len:608 episode reward: total was -26.110000. running mean: -31.574469\n",
      "epsilon:0.136603 episode_count: 10010. steps_count: 4347246.000000\n",
      "Time elapsed:  12462.78666305542\n",
      "ep 1430: ep_len:619 episode reward: total was -32.390000. running mean: -31.582624\n",
      "ep 1430: ep_len:562 episode reward: total was -84.520000. running mean: -32.111998\n",
      "ep 1430: ep_len:649 episode reward: total was -132.660000. running mean: -33.117478\n",
      "ep 1430: ep_len:500 episode reward: total was -37.650000. running mean: -33.162803\n",
      "ep 1430: ep_len:3 episode reward: total was 0.000000. running mean: -32.831175\n",
      "ep 1430: ep_len:294 episode reward: total was 2.340000. running mean: -32.479464\n",
      "ep 1430: ep_len:522 episode reward: total was -13.900000. running mean: -32.293669\n",
      "epsilon:0.136559 episode_count: 10017. steps_count: 4350395.000000\n",
      "Time elapsed:  12471.264440774918\n",
      "ep 1431: ep_len:500 episode reward: total was 44.110000. running mean: -31.529632\n",
      "ep 1431: ep_len:645 episode reward: total was 16.170000. running mean: -31.052636\n",
      "ep 1431: ep_len:79 episode reward: total was -1.730000. running mean: -30.759410\n",
      "ep 1431: ep_len:153 episode reward: total was -20.860000. running mean: -30.660416\n",
      "ep 1431: ep_len:84 episode reward: total was 17.700000. running mean: -30.176811\n",
      "ep 1431: ep_len:525 episode reward: total was -37.910000. running mean: -30.254143\n",
      "ep 1431: ep_len:532 episode reward: total was -65.280000. running mean: -30.604402\n",
      "epsilon:0.136515 episode_count: 10024. steps_count: 4352913.000000\n",
      "Time elapsed:  12478.134658813477\n",
      "ep 1432: ep_len:500 episode reward: total was 21.660000. running mean: -30.081758\n",
      "ep 1432: ep_len:500 episode reward: total was -41.780000. running mean: -30.198740\n",
      "ep 1432: ep_len:559 episode reward: total was -104.340000. running mean: -30.940153\n",
      "ep 1432: ep_len:576 episode reward: total was -121.320000. running mean: -31.843951\n",
      "ep 1432: ep_len:42 episode reward: total was 10.500000. running mean: -31.420512\n",
      "ep 1432: ep_len:516 episode reward: total was -104.050000. running mean: -32.146807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1432: ep_len:204 episode reward: total was -23.540000. running mean: -32.060739\n",
      "epsilon:0.136470 episode_count: 10031. steps_count: 4355810.000000\n",
      "Time elapsed:  12486.950250864029\n",
      "ep 1433: ep_len:599 episode reward: total was -89.860000. running mean: -32.638731\n",
      "ep 1433: ep_len:500 episode reward: total was -10.490000. running mean: -32.417244\n",
      "ep 1433: ep_len:628 episode reward: total was -72.180000. running mean: -32.814871\n",
      "ep 1433: ep_len:514 episode reward: total was 7.790000. running mean: -32.408823\n",
      "ep 1433: ep_len:97 episode reward: total was 22.670000. running mean: -31.858035\n",
      "ep 1433: ep_len:500 episode reward: total was -47.730000. running mean: -32.016754\n",
      "ep 1433: ep_len:589 episode reward: total was -79.580000. running mean: -32.492387\n",
      "epsilon:0.136426 episode_count: 10038. steps_count: 4359237.000000\n",
      "Time elapsed:  12496.148452997208\n",
      "ep 1434: ep_len:753 episode reward: total was -401.460000. running mean: -36.182063\n",
      "ep 1434: ep_len:513 episode reward: total was 20.960000. running mean: -35.610642\n",
      "ep 1434: ep_len:500 episode reward: total was 2.710000. running mean: -35.227436\n",
      "ep 1434: ep_len:514 episode reward: total was -30.280000. running mean: -35.177961\n",
      "ep 1434: ep_len:49 episode reward: total was 14.000000. running mean: -34.686182\n",
      "ep 1434: ep_len:522 episode reward: total was -177.010000. running mean: -36.109420\n",
      "ep 1434: ep_len:640 episode reward: total was -60.970000. running mean: -36.358026\n",
      "epsilon:0.136382 episode_count: 10045. steps_count: 4362728.000000\n",
      "Time elapsed:  12506.756002187729\n",
      "ep 1435: ep_len:536 episode reward: total was -131.630000. running mean: -37.310745\n",
      "ep 1435: ep_len:500 episode reward: total was -17.330000. running mean: -37.110938\n",
      "ep 1435: ep_len:554 episode reward: total was -16.540000. running mean: -36.905229\n",
      "ep 1435: ep_len:395 episode reward: total was -53.730000. running mean: -37.073476\n",
      "ep 1435: ep_len:92 episode reward: total was 3.210000. running mean: -36.670642\n",
      "ep 1435: ep_len:500 episode reward: total was -17.620000. running mean: -36.480135\n",
      "ep 1435: ep_len:527 episode reward: total was -38.730000. running mean: -36.502634\n",
      "epsilon:0.136337 episode_count: 10052. steps_count: 4365832.000000\n",
      "Time elapsed:  12515.196716547012\n",
      "ep 1436: ep_len:620 episode reward: total was 12.830000. running mean: -36.009307\n",
      "ep 1436: ep_len:530 episode reward: total was -72.720000. running mean: -36.376414\n",
      "ep 1436: ep_len:647 episode reward: total was -66.210000. running mean: -36.674750\n",
      "ep 1436: ep_len:531 episode reward: total was -63.630000. running mean: -36.944303\n",
      "ep 1436: ep_len:3 episode reward: total was 0.000000. running mean: -36.574860\n",
      "ep 1436: ep_len:261 episode reward: total was 26.970000. running mean: -35.939411\n",
      "ep 1436: ep_len:500 episode reward: total was 5.240000. running mean: -35.527617\n",
      "epsilon:0.136293 episode_count: 10059. steps_count: 4368924.000000\n",
      "Time elapsed:  12523.242483854294\n",
      "ep 1437: ep_len:500 episode reward: total was 7.410000. running mean: -35.098241\n",
      "ep 1437: ep_len:530 episode reward: total was -14.750000. running mean: -34.894758\n",
      "ep 1437: ep_len:430 episode reward: total was -19.540000. running mean: -34.741211\n",
      "ep 1437: ep_len:528 episode reward: total was -9.580000. running mean: -34.489599\n",
      "ep 1437: ep_len:3 episode reward: total was -0.490000. running mean: -34.149603\n",
      "ep 1437: ep_len:526 episode reward: total was -29.210000. running mean: -34.100207\n",
      "ep 1437: ep_len:501 episode reward: total was -48.830000. running mean: -34.247505\n",
      "epsilon:0.136249 episode_count: 10066. steps_count: 4371942.000000\n",
      "Time elapsed:  12530.663441181183\n",
      "ep 1438: ep_len:500 episode reward: total was -66.120000. running mean: -34.566230\n",
      "ep 1438: ep_len:500 episode reward: total was -14.150000. running mean: -34.362067\n",
      "ep 1438: ep_len:596 episode reward: total was -71.870000. running mean: -34.737147\n",
      "ep 1438: ep_len:587 episode reward: total was 41.850000. running mean: -33.971275\n",
      "ep 1438: ep_len:3 episode reward: total was 1.010000. running mean: -33.621462\n",
      "ep 1438: ep_len:168 episode reward: total was 13.990000. running mean: -33.145348\n",
      "ep 1438: ep_len:346 episode reward: total was -29.550000. running mean: -33.109394\n",
      "epsilon:0.136204 episode_count: 10073. steps_count: 4374642.000000\n",
      "Time elapsed:  12539.18304514885\n",
      "ep 1439: ep_len:94 episode reward: total was -14.770000. running mean: -32.926000\n",
      "ep 1439: ep_len:664 episode reward: total was -85.250000. running mean: -33.449240\n",
      "ep 1439: ep_len:500 episode reward: total was -24.640000. running mean: -33.361148\n",
      "ep 1439: ep_len:584 episode reward: total was 26.050000. running mean: -32.767037\n",
      "ep 1439: ep_len:3 episode reward: total was -1.500000. running mean: -32.454366\n",
      "ep 1439: ep_len:554 episode reward: total was -67.500000. running mean: -32.804822\n",
      "ep 1439: ep_len:273 episode reward: total was -2.810000. running mean: -32.504874\n",
      "epsilon:0.136160 episode_count: 10080. steps_count: 4377314.000000\n",
      "Time elapsed:  12546.298911571503\n",
      "ep 1440: ep_len:500 episode reward: total was 42.680000. running mean: -31.753026\n",
      "ep 1440: ep_len:632 episode reward: total was -24.240000. running mean: -31.677895\n",
      "ep 1440: ep_len:406 episode reward: total was -33.080000. running mean: -31.691916\n",
      "ep 1440: ep_len:501 episode reward: total was -11.910000. running mean: -31.494097\n",
      "ep 1440: ep_len:97 episode reward: total was -27.720000. running mean: -31.456356\n",
      "ep 1440: ep_len:667 episode reward: total was -83.660000. running mean: -31.978393\n",
      "ep 1440: ep_len:615 episode reward: total was -30.680000. running mean: -31.965409\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.136116 episode_count: 10087. steps_count: 4380732.000000\n",
      "Time elapsed:  12561.196885824203\n",
      "ep 1441: ep_len:591 episode reward: total was -17.750000. running mean: -31.823255\n",
      "ep 1441: ep_len:155 episode reward: total was -18.620000. running mean: -31.691222\n",
      "ep 1441: ep_len:500 episode reward: total was -53.740000. running mean: -31.911710\n",
      "ep 1441: ep_len:501 episode reward: total was -91.040000. running mean: -32.502993\n",
      "ep 1441: ep_len:3 episode reward: total was 1.010000. running mean: -32.167863\n",
      "ep 1441: ep_len:544 episode reward: total was -101.540000. running mean: -32.861584\n",
      "ep 1441: ep_len:612 episode reward: total was -102.830000. running mean: -33.561268\n",
      "epsilon:0.136071 episode_count: 10094. steps_count: 4383638.000000\n",
      "Time elapsed:  12569.008511304855\n",
      "ep 1442: ep_len:500 episode reward: total was 27.820000. running mean: -32.947456\n",
      "ep 1442: ep_len:546 episode reward: total was -46.840000. running mean: -33.086381\n",
      "ep 1442: ep_len:621 episode reward: total was -52.890000. running mean: -33.284417\n",
      "ep 1442: ep_len:500 episode reward: total was -24.350000. running mean: -33.195073\n",
      "ep 1442: ep_len:86 episode reward: total was 17.110000. running mean: -32.692022\n",
      "ep 1442: ep_len:585 episode reward: total was -39.820000. running mean: -32.763302\n",
      "ep 1442: ep_len:546 episode reward: total was -43.420000. running mean: -32.869869\n",
      "epsilon:0.136027 episode_count: 10101. steps_count: 4387022.000000\n",
      "Time elapsed:  12577.873829364777\n",
      "ep 1443: ep_len:229 episode reward: total was 3.290000. running mean: -32.508270\n",
      "ep 1443: ep_len:500 episode reward: total was -39.950000. running mean: -32.582688\n",
      "ep 1443: ep_len:555 episode reward: total was -47.470000. running mean: -32.731561\n",
      "ep 1443: ep_len:507 episode reward: total was -31.270000. running mean: -32.716945\n",
      "ep 1443: ep_len:3 episode reward: total was -1.500000. running mean: -32.404776\n",
      "ep 1443: ep_len:318 episode reward: total was -23.470000. running mean: -32.315428\n",
      "ep 1443: ep_len:312 episode reward: total was -1.960000. running mean: -32.011874\n",
      "epsilon:0.135983 episode_count: 10108. steps_count: 4389446.000000\n",
      "Time elapsed:  12585.436922311783\n",
      "ep 1444: ep_len:604 episode reward: total was -33.600000. running mean: -32.027755\n",
      "ep 1444: ep_len:517 episode reward: total was -20.620000. running mean: -31.913677\n",
      "ep 1444: ep_len:375 episode reward: total was -36.360000. running mean: -31.958141\n",
      "ep 1444: ep_len:400 episode reward: total was -10.640000. running mean: -31.744959\n",
      "ep 1444: ep_len:3 episode reward: total was 0.000000. running mean: -31.427510\n",
      "ep 1444: ep_len:625 episode reward: total was -52.420000. running mean: -31.637435\n",
      "ep 1444: ep_len:309 episode reward: total was -26.230000. running mean: -31.583360\n",
      "epsilon:0.135938 episode_count: 10115. steps_count: 4392279.000000\n",
      "Time elapsed:  12592.883064746857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1445: ep_len:563 episode reward: total was -41.510000. running mean: -31.682627\n",
      "ep 1445: ep_len:565 episode reward: total was -25.470000. running mean: -31.620500\n",
      "ep 1445: ep_len:547 episode reward: total was -260.890000. running mean: -33.913195\n",
      "ep 1445: ep_len:590 episode reward: total was -10.530000. running mean: -33.679363\n",
      "ep 1445: ep_len:3 episode reward: total was 1.010000. running mean: -33.332470\n",
      "ep 1445: ep_len:512 episode reward: total was -78.910000. running mean: -33.788245\n",
      "ep 1445: ep_len:604 episode reward: total was -93.460000. running mean: -34.384963\n",
      "epsilon:0.135894 episode_count: 10122. steps_count: 4395663.000000\n",
      "Time elapsed:  12601.881519317627\n",
      "ep 1446: ep_len:533 episode reward: total was -15.570000. running mean: -34.196813\n",
      "ep 1446: ep_len:267 episode reward: total was -39.080000. running mean: -34.245645\n",
      "ep 1446: ep_len:500 episode reward: total was -86.150000. running mean: -34.764688\n",
      "ep 1446: ep_len:500 episode reward: total was 34.820000. running mean: -34.068842\n",
      "ep 1446: ep_len:82 episode reward: total was -9.810000. running mean: -33.826253\n",
      "ep 1446: ep_len:532 episode reward: total was -72.090000. running mean: -34.208891\n",
      "ep 1446: ep_len:292 episode reward: total was -13.120000. running mean: -33.998002\n",
      "epsilon:0.135850 episode_count: 10129. steps_count: 4398369.000000\n",
      "Time elapsed:  12609.289005279541\n",
      "ep 1447: ep_len:573 episode reward: total was 15.580000. running mean: -33.502222\n",
      "ep 1447: ep_len:526 episode reward: total was -17.360000. running mean: -33.340799\n",
      "ep 1447: ep_len:597 episode reward: total was -56.320000. running mean: -33.570591\n",
      "ep 1447: ep_len:500 episode reward: total was -8.290000. running mean: -33.317786\n",
      "ep 1447: ep_len:73 episode reward: total was 11.220000. running mean: -32.872408\n",
      "ep 1447: ep_len:634 episode reward: total was -66.160000. running mean: -33.205284\n",
      "ep 1447: ep_len:589 episode reward: total was -93.890000. running mean: -33.812131\n",
      "epsilon:0.135805 episode_count: 10136. steps_count: 4401861.000000\n",
      "Time elapsed:  12618.633985757828\n",
      "ep 1448: ep_len:596 episode reward: total was -22.200000. running mean: -33.696009\n",
      "ep 1448: ep_len:633 episode reward: total was -35.570000. running mean: -33.714749\n",
      "ep 1448: ep_len:392 episode reward: total was 6.860000. running mean: -33.309002\n",
      "ep 1448: ep_len:540 episode reward: total was -4.820000. running mean: -33.024112\n",
      "ep 1448: ep_len:113 episode reward: total was 12.330000. running mean: -32.570571\n",
      "ep 1448: ep_len:620 episode reward: total was -33.290000. running mean: -32.577765\n",
      "ep 1448: ep_len:590 episode reward: total was -56.410000. running mean: -32.816087\n",
      "epsilon:0.135761 episode_count: 10143. steps_count: 4405345.000000\n",
      "Time elapsed:  12629.168756723404\n",
      "ep 1449: ep_len:110 episode reward: total was 10.430000. running mean: -32.383627\n",
      "ep 1449: ep_len:197 episode reward: total was -35.910000. running mean: -32.418890\n",
      "ep 1449: ep_len:500 episode reward: total was -1.570000. running mean: -32.110401\n",
      "ep 1449: ep_len:516 episode reward: total was -10.190000. running mean: -31.891197\n",
      "ep 1449: ep_len:3 episode reward: total was -1.500000. running mean: -31.587285\n",
      "ep 1449: ep_len:541 episode reward: total was -0.990000. running mean: -31.281313\n",
      "ep 1449: ep_len:211 episode reward: total was -15.450000. running mean: -31.122999\n",
      "epsilon:0.135717 episode_count: 10150. steps_count: 4407423.000000\n",
      "Time elapsed:  12634.983983755112\n",
      "ep 1450: ep_len:500 episode reward: total was 16.170000. running mean: -30.650069\n",
      "ep 1450: ep_len:500 episode reward: total was -71.020000. running mean: -31.053769\n",
      "ep 1450: ep_len:609 episode reward: total was -32.410000. running mean: -31.067331\n",
      "ep 1450: ep_len:503 episode reward: total was 4.190000. running mean: -30.714758\n",
      "ep 1450: ep_len:103 episode reward: total was 17.250000. running mean: -30.235110\n",
      "ep 1450: ep_len:702 episode reward: total was -53.150000. running mean: -30.464259\n",
      "ep 1450: ep_len:198 episode reward: total was -20.170000. running mean: -30.361316\n",
      "epsilon:0.135672 episode_count: 10157. steps_count: 4410538.000000\n",
      "Time elapsed:  12643.582925319672\n",
      "ep 1451: ep_len:567 episode reward: total was 12.670000. running mean: -29.931003\n",
      "ep 1451: ep_len:511 episode reward: total was -73.550000. running mean: -30.367193\n",
      "ep 1451: ep_len:500 episode reward: total was -22.760000. running mean: -30.291121\n",
      "ep 1451: ep_len:593 episode reward: total was -40.550000. running mean: -30.393710\n",
      "ep 1451: ep_len:3 episode reward: total was 0.000000. running mean: -30.089773\n",
      "ep 1451: ep_len:500 episode reward: total was -20.550000. running mean: -29.994375\n",
      "ep 1451: ep_len:519 episode reward: total was -45.210000. running mean: -30.146532\n",
      "epsilon:0.135628 episode_count: 10164. steps_count: 4413731.000000\n",
      "Time elapsed:  12652.117936372757\n",
      "ep 1452: ep_len:500 episode reward: total was -128.170000. running mean: -31.126766\n",
      "ep 1452: ep_len:183 episode reward: total was -49.220000. running mean: -31.307699\n",
      "ep 1452: ep_len:641 episode reward: total was -25.380000. running mean: -31.248422\n",
      "ep 1452: ep_len:511 episode reward: total was -22.540000. running mean: -31.161337\n",
      "ep 1452: ep_len:3 episode reward: total was 0.000000. running mean: -30.849724\n",
      "ep 1452: ep_len:336 episode reward: total was -20.470000. running mean: -30.745927\n",
      "ep 1452: ep_len:500 episode reward: total was -46.770000. running mean: -30.906167\n",
      "epsilon:0.135584 episode_count: 10171. steps_count: 4416405.000000\n",
      "Time elapsed:  12659.970279932022\n",
      "ep 1453: ep_len:586 episode reward: total was -20.100000. running mean: -30.798106\n",
      "ep 1453: ep_len:282 episode reward: total was -115.850000. running mean: -31.648625\n",
      "ep 1453: ep_len:639 episode reward: total was -84.620000. running mean: -32.178338\n",
      "ep 1453: ep_len:543 episode reward: total was -8.410000. running mean: -31.940655\n",
      "ep 1453: ep_len:3 episode reward: total was 0.000000. running mean: -31.621249\n",
      "ep 1453: ep_len:519 episode reward: total was -52.510000. running mean: -31.830136\n",
      "ep 1453: ep_len:516 episode reward: total was -53.490000. running mean: -32.046735\n",
      "epsilon:0.135539 episode_count: 10178. steps_count: 4419493.000000\n",
      "Time elapsed:  12671.26182603836\n",
      "ep 1454: ep_len:500 episode reward: total was 26.890000. running mean: -31.457367\n",
      "ep 1454: ep_len:555 episode reward: total was -84.760000. running mean: -31.990394\n",
      "ep 1454: ep_len:340 episode reward: total was -36.840000. running mean: -32.038890\n",
      "ep 1454: ep_len:501 episode reward: total was -108.310000. running mean: -32.801601\n",
      "ep 1454: ep_len:99 episode reward: total was 12.740000. running mean: -32.346185\n",
      "ep 1454: ep_len:512 episode reward: total was -38.910000. running mean: -32.411823\n",
      "ep 1454: ep_len:171 episode reward: total was -30.760000. running mean: -32.395305\n",
      "epsilon:0.135495 episode_count: 10185. steps_count: 4422171.000000\n",
      "Time elapsed:  12678.031843185425\n",
      "ep 1455: ep_len:626 episode reward: total was -77.870000. running mean: -32.850052\n",
      "ep 1455: ep_len:598 episode reward: total was -46.660000. running mean: -32.988151\n",
      "ep 1455: ep_len:588 episode reward: total was -50.060000. running mean: -33.158870\n",
      "ep 1455: ep_len:582 episode reward: total was -10.760000. running mean: -32.934881\n",
      "ep 1455: ep_len:3 episode reward: total was 0.000000. running mean: -32.605532\n",
      "ep 1455: ep_len:500 episode reward: total was -17.400000. running mean: -32.453477\n",
      "ep 1455: ep_len:308 episode reward: total was -45.340000. running mean: -32.582342\n",
      "epsilon:0.135451 episode_count: 10192. steps_count: 4425376.000000\n",
      "Time elapsed:  12688.09004187584\n",
      "ep 1456: ep_len:122 episode reward: total was -7.110000. running mean: -32.327619\n",
      "ep 1456: ep_len:556 episode reward: total was -28.900000. running mean: -32.293342\n",
      "ep 1456: ep_len:622 episode reward: total was -70.460000. running mean: -32.675009\n",
      "ep 1456: ep_len:500 episode reward: total was -15.880000. running mean: -32.507059\n",
      "ep 1456: ep_len:3 episode reward: total was 1.010000. running mean: -32.171888\n",
      "ep 1456: ep_len:500 episode reward: total was -57.240000. running mean: -32.422569\n",
      "ep 1456: ep_len:211 episode reward: total was -33.110000. running mean: -32.429444\n",
      "epsilon:0.135406 episode_count: 10199. steps_count: 4427890.000000\n",
      "Time elapsed:  12694.978821516037\n",
      "ep 1457: ep_len:212 episode reward: total was -82.500000. running mean: -32.930149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1457: ep_len:605 episode reward: total was -9.300000. running mean: -32.693848\n",
      "ep 1457: ep_len:637 episode reward: total was -72.390000. running mean: -33.090809\n",
      "ep 1457: ep_len:148 episode reward: total was 3.010000. running mean: -32.729801\n",
      "ep 1457: ep_len:3 episode reward: total was 1.010000. running mean: -32.392403\n",
      "ep 1457: ep_len:500 episode reward: total was -21.280000. running mean: -32.281279\n",
      "ep 1457: ep_len:550 episode reward: total was -101.200000. running mean: -32.970466\n",
      "epsilon:0.135362 episode_count: 10206. steps_count: 4430545.000000\n",
      "Time elapsed:  12703.626450061798\n",
      "ep 1458: ep_len:500 episode reward: total was -39.060000. running mean: -33.031362\n",
      "ep 1458: ep_len:200 episode reward: total was 3.880000. running mean: -32.662248\n",
      "ep 1458: ep_len:619 episode reward: total was -146.510000. running mean: -33.800726\n",
      "ep 1458: ep_len:515 episode reward: total was -97.880000. running mean: -34.441518\n",
      "ep 1458: ep_len:3 episode reward: total was 0.000000. running mean: -34.097103\n",
      "ep 1458: ep_len:613 episode reward: total was -53.540000. running mean: -34.291532\n",
      "ep 1458: ep_len:500 episode reward: total was -42.200000. running mean: -34.370617\n",
      "epsilon:0.135318 episode_count: 10213. steps_count: 4433495.000000\n",
      "Time elapsed:  12713.263077259064\n",
      "ep 1459: ep_len:510 episode reward: total was -18.850000. running mean: -34.215411\n",
      "ep 1459: ep_len:568 episode reward: total was -51.660000. running mean: -34.389857\n",
      "ep 1459: ep_len:575 episode reward: total was -79.620000. running mean: -34.842158\n",
      "ep 1459: ep_len:533 episode reward: total was -52.560000. running mean: -35.019336\n",
      "ep 1459: ep_len:64 episode reward: total was 13.120000. running mean: -34.537943\n",
      "ep 1459: ep_len:677 episode reward: total was -22.530000. running mean: -34.417864\n",
      "ep 1459: ep_len:589 episode reward: total was -49.950000. running mean: -34.573185\n",
      "epsilon:0.135273 episode_count: 10220. steps_count: 4437011.000000\n",
      "Time elapsed:  12722.325033187866\n",
      "ep 1460: ep_len:500 episode reward: total was 12.160000. running mean: -34.105853\n",
      "ep 1460: ep_len:500 episode reward: total was -33.740000. running mean: -34.102195\n",
      "ep 1460: ep_len:614 episode reward: total was -33.400000. running mean: -34.095173\n",
      "ep 1460: ep_len:506 episode reward: total was 24.470000. running mean: -33.509521\n",
      "ep 1460: ep_len:98 episode reward: total was -51.840000. running mean: -33.692826\n",
      "ep 1460: ep_len:500 episode reward: total was -17.100000. running mean: -33.526898\n",
      "ep 1460: ep_len:565 episode reward: total was -81.910000. running mean: -34.010729\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.135229 episode_count: 10227. steps_count: 4440294.000000\n",
      "Time elapsed:  12735.468195438385\n",
      "ep 1461: ep_len:500 episode reward: total was -27.000000. running mean: -33.940621\n",
      "ep 1461: ep_len:530 episode reward: total was -36.360000. running mean: -33.964815\n",
      "ep 1461: ep_len:623 episode reward: total was -74.930000. running mean: -34.374467\n",
      "ep 1461: ep_len:581 episode reward: total was -24.480000. running mean: -34.275522\n",
      "ep 1461: ep_len:3 episode reward: total was -1.500000. running mean: -33.947767\n",
      "ep 1461: ep_len:634 episode reward: total was -43.940000. running mean: -34.047689\n",
      "ep 1461: ep_len:512 episode reward: total was -49.780000. running mean: -34.205012\n",
      "epsilon:0.135185 episode_count: 10234. steps_count: 4443677.000000\n",
      "Time elapsed:  12744.284456729889\n",
      "ep 1462: ep_len:604 episode reward: total was 2.710000. running mean: -33.835862\n",
      "ep 1462: ep_len:550 episode reward: total was -69.580000. running mean: -34.193304\n",
      "ep 1462: ep_len:658 episode reward: total was -84.150000. running mean: -34.692871\n",
      "ep 1462: ep_len:396 episode reward: total was 10.410000. running mean: -34.241842\n",
      "ep 1462: ep_len:98 episode reward: total was -62.250000. running mean: -34.521924\n",
      "ep 1462: ep_len:588 episode reward: total was -25.590000. running mean: -34.432604\n",
      "ep 1462: ep_len:317 episode reward: total was -30.880000. running mean: -34.397078\n",
      "epsilon:0.135140 episode_count: 10241. steps_count: 4446888.000000\n",
      "Time elapsed:  12754.405277252197\n",
      "ep 1463: ep_len:628 episode reward: total was -81.370000. running mean: -34.866807\n",
      "ep 1463: ep_len:532 episode reward: total was -38.300000. running mean: -34.901139\n",
      "ep 1463: ep_len:522 episode reward: total was -58.550000. running mean: -35.137628\n",
      "ep 1463: ep_len:418 episode reward: total was -7.030000. running mean: -34.856552\n",
      "ep 1463: ep_len:98 episode reward: total was -29.240000. running mean: -34.800386\n",
      "ep 1463: ep_len:501 episode reward: total was -5.300000. running mean: -34.505382\n",
      "ep 1463: ep_len:240 episode reward: total was -38.140000. running mean: -34.541729\n",
      "epsilon:0.135096 episode_count: 10248. steps_count: 4449827.000000\n",
      "Time elapsed:  12763.269619464874\n",
      "ep 1464: ep_len:527 episode reward: total was -75.670000. running mean: -34.953011\n",
      "ep 1464: ep_len:588 episode reward: total was -30.660000. running mean: -34.910081\n",
      "ep 1464: ep_len:649 episode reward: total was -90.030000. running mean: -35.461280\n",
      "ep 1464: ep_len:500 episode reward: total was 12.730000. running mean: -34.979367\n",
      "ep 1464: ep_len:3 episode reward: total was 0.000000. running mean: -34.629574\n",
      "ep 1464: ep_len:544 episode reward: total was -141.580000. running mean: -35.699078\n",
      "ep 1464: ep_len:282 episode reward: total was -46.490000. running mean: -35.806987\n",
      "epsilon:0.135052 episode_count: 10255. steps_count: 4452920.000000\n",
      "Time elapsed:  12772.699743747711\n",
      "ep 1465: ep_len:569 episode reward: total was -39.820000. running mean: -35.847117\n",
      "ep 1465: ep_len:500 episode reward: total was -68.460000. running mean: -36.173246\n",
      "ep 1465: ep_len:622 episode reward: total was -42.000000. running mean: -36.231514\n",
      "ep 1465: ep_len:500 episode reward: total was -10.900000. running mean: -35.978199\n",
      "ep 1465: ep_len:104 episode reward: total was -63.810000. running mean: -36.256517\n",
      "ep 1465: ep_len:557 episode reward: total was -34.710000. running mean: -36.241052\n",
      "ep 1465: ep_len:500 episode reward: total was -40.840000. running mean: -36.287041\n",
      "epsilon:0.135007 episode_count: 10262. steps_count: 4456272.000000\n",
      "Time elapsed:  12782.783549308777\n",
      "ep 1466: ep_len:687 episode reward: total was -101.950000. running mean: -36.943671\n",
      "ep 1466: ep_len:500 episode reward: total was 10.850000. running mean: -36.465734\n",
      "ep 1466: ep_len:538 episode reward: total was -68.510000. running mean: -36.786177\n",
      "ep 1466: ep_len:601 episode reward: total was 24.630000. running mean: -36.172015\n",
      "ep 1466: ep_len:41 episode reward: total was 7.000000. running mean: -35.740295\n",
      "ep 1466: ep_len:500 episode reward: total was -43.670000. running mean: -35.819592\n",
      "ep 1466: ep_len:346 episode reward: total was -33.710000. running mean: -35.798496\n",
      "epsilon:0.134963 episode_count: 10269. steps_count: 4459485.000000\n",
      "Time elapsed:  12792.565402507782\n",
      "ep 1467: ep_len:699 episode reward: total was -129.290000. running mean: -36.733411\n",
      "ep 1467: ep_len:641 episode reward: total was -159.850000. running mean: -37.964577\n",
      "ep 1467: ep_len:500 episode reward: total was -49.100000. running mean: -38.075931\n",
      "ep 1467: ep_len:599 episode reward: total was 13.080000. running mean: -37.564372\n",
      "ep 1467: ep_len:96 episode reward: total was -53.230000. running mean: -37.721028\n",
      "ep 1467: ep_len:501 episode reward: total was -70.070000. running mean: -38.044518\n",
      "ep 1467: ep_len:271 episode reward: total was -21.020000. running mean: -37.874272\n",
      "epsilon:0.134919 episode_count: 10276. steps_count: 4462792.000000\n",
      "Time elapsed:  12802.431534290314\n",
      "ep 1468: ep_len:500 episode reward: total was 22.230000. running mean: -37.273230\n",
      "ep 1468: ep_len:500 episode reward: total was 19.220000. running mean: -36.708297\n",
      "ep 1468: ep_len:422 episode reward: total was -0.030000. running mean: -36.341514\n",
      "ep 1468: ep_len:568 episode reward: total was -1.450000. running mean: -35.992599\n",
      "ep 1468: ep_len:54 episode reward: total was 12.000000. running mean: -35.512673\n",
      "ep 1468: ep_len:520 episode reward: total was -45.580000. running mean: -35.613347\n",
      "ep 1468: ep_len:176 episode reward: total was -35.910000. running mean: -35.616313\n",
      "epsilon:0.134874 episode_count: 10283. steps_count: 4465532.000000\n",
      "Time elapsed:  12810.68636250496\n",
      "ep 1469: ep_len:580 episode reward: total was -20.030000. running mean: -35.460450\n",
      "ep 1469: ep_len:185 episode reward: total was -27.460000. running mean: -35.380445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1469: ep_len:500 episode reward: total was -23.420000. running mean: -35.260841\n",
      "ep 1469: ep_len:130 episode reward: total was 5.010000. running mean: -34.858133\n",
      "ep 1469: ep_len:75 episode reward: total was 5.730000. running mean: -34.452251\n",
      "ep 1469: ep_len:543 episode reward: total was -44.310000. running mean: -34.550829\n",
      "ep 1469: ep_len:515 episode reward: total was -46.600000. running mean: -34.671320\n",
      "epsilon:0.134830 episode_count: 10290. steps_count: 4468060.000000\n",
      "Time elapsed:  12819.613686561584\n",
      "ep 1470: ep_len:559 episode reward: total was -93.620000. running mean: -35.260807\n",
      "ep 1470: ep_len:535 episode reward: total was -42.080000. running mean: -35.328999\n",
      "ep 1470: ep_len:697 episode reward: total was -81.520000. running mean: -35.790909\n",
      "ep 1470: ep_len:565 episode reward: total was 3.210000. running mean: -35.400900\n",
      "ep 1470: ep_len:126 episode reward: total was 9.340000. running mean: -34.953491\n",
      "ep 1470: ep_len:620 episode reward: total was -58.840000. running mean: -35.192356\n",
      "ep 1470: ep_len:547 episode reward: total was -73.110000. running mean: -35.571533\n",
      "epsilon:0.134786 episode_count: 10297. steps_count: 4471709.000000\n",
      "Time elapsed:  12827.314677238464\n",
      "ep 1471: ep_len:570 episode reward: total was -85.370000. running mean: -36.069517\n",
      "ep 1471: ep_len:528 episode reward: total was -80.950000. running mean: -36.518322\n",
      "ep 1471: ep_len:551 episode reward: total was -48.260000. running mean: -36.635739\n",
      "ep 1471: ep_len:503 episode reward: total was -3.860000. running mean: -36.307982\n",
      "ep 1471: ep_len:104 episode reward: total was 13.220000. running mean: -35.812702\n",
      "ep 1471: ep_len:626 episode reward: total was -57.090000. running mean: -36.025475\n",
      "ep 1471: ep_len:543 episode reward: total was -31.330000. running mean: -35.978520\n",
      "epsilon:0.134741 episode_count: 10304. steps_count: 4475134.000000\n",
      "Time elapsed:  12837.467971801758\n",
      "ep 1472: ep_len:589 episode reward: total was 1.340000. running mean: -35.605335\n",
      "ep 1472: ep_len:552 episode reward: total was -29.010000. running mean: -35.539381\n",
      "ep 1472: ep_len:64 episode reward: total was 4.150000. running mean: -35.142488\n",
      "ep 1472: ep_len:500 episode reward: total was 7.740000. running mean: -34.713663\n",
      "ep 1472: ep_len:114 episode reward: total was 15.710000. running mean: -34.209426\n",
      "ep 1472: ep_len:571 episode reward: total was -68.090000. running mean: -34.548232\n",
      "ep 1472: ep_len:603 episode reward: total was -25.510000. running mean: -34.457850\n",
      "epsilon:0.134697 episode_count: 10311. steps_count: 4478127.000000\n",
      "Time elapsed:  12846.363234758377\n",
      "ep 1473: ep_len:637 episode reward: total was -90.000000. running mean: -35.013271\n",
      "ep 1473: ep_len:500 episode reward: total was -7.580000. running mean: -34.738938\n",
      "ep 1473: ep_len:436 episode reward: total was -20.020000. running mean: -34.591749\n",
      "ep 1473: ep_len:533 episode reward: total was -12.950000. running mean: -34.375331\n",
      "ep 1473: ep_len:80 episode reward: total was 16.160000. running mean: -33.869978\n",
      "ep 1473: ep_len:510 episode reward: total was -215.230000. running mean: -35.683578\n",
      "ep 1473: ep_len:640 episode reward: total was -36.150000. running mean: -35.688243\n",
      "epsilon:0.134653 episode_count: 10318. steps_count: 4481463.000000\n",
      "Time elapsed:  12856.81553530693\n",
      "ep 1474: ep_len:602 episode reward: total was -91.300000. running mean: -36.244360\n",
      "ep 1474: ep_len:500 episode reward: total was -14.680000. running mean: -36.028717\n",
      "ep 1474: ep_len:592 episode reward: total was -37.630000. running mean: -36.044729\n",
      "ep 1474: ep_len:515 episode reward: total was -50.630000. running mean: -36.190582\n",
      "ep 1474: ep_len:87 episode reward: total was 4.720000. running mean: -35.781476\n",
      "ep 1474: ep_len:513 episode reward: total was -6.310000. running mean: -35.486761\n",
      "ep 1474: ep_len:504 episode reward: total was -30.070000. running mean: -35.432594\n",
      "epsilon:0.134608 episode_count: 10325. steps_count: 4484776.000000\n",
      "Time elapsed:  12866.823049783707\n",
      "ep 1475: ep_len:615 episode reward: total was -81.010000. running mean: -35.888368\n",
      "ep 1475: ep_len:629 episode reward: total was 8.050000. running mean: -35.448984\n",
      "ep 1475: ep_len:614 episode reward: total was -30.120000. running mean: -35.395694\n",
      "ep 1475: ep_len:521 episode reward: total was -37.300000. running mean: -35.414737\n",
      "ep 1475: ep_len:3 episode reward: total was 0.000000. running mean: -35.060590\n",
      "ep 1475: ep_len:704 episode reward: total was -41.020000. running mean: -35.120184\n",
      "ep 1475: ep_len:348 episode reward: total was -10.490000. running mean: -34.873882\n",
      "epsilon:0.134564 episode_count: 10332. steps_count: 4488210.000000\n",
      "Time elapsed:  12877.16603064537\n",
      "ep 1476: ep_len:542 episode reward: total was 17.420000. running mean: -34.350944\n",
      "ep 1476: ep_len:502 episode reward: total was -46.080000. running mean: -34.468234\n",
      "ep 1476: ep_len:79 episode reward: total was -2.770000. running mean: -34.151252\n",
      "ep 1476: ep_len:500 episode reward: total was -29.600000. running mean: -34.105739\n",
      "ep 1476: ep_len:86 episode reward: total was -9.280000. running mean: -33.857482\n",
      "ep 1476: ep_len:645 episode reward: total was -82.770000. running mean: -34.346607\n",
      "ep 1476: ep_len:585 episode reward: total was -42.740000. running mean: -34.430541\n",
      "epsilon:0.134520 episode_count: 10339. steps_count: 4491149.000000\n",
      "Time elapsed:  12885.737354278564\n",
      "ep 1477: ep_len:260 episode reward: total was -1.230000. running mean: -34.098536\n",
      "ep 1477: ep_len:520 episode reward: total was 11.600000. running mean: -33.641550\n",
      "ep 1477: ep_len:548 episode reward: total was -40.740000. running mean: -33.712535\n",
      "ep 1477: ep_len:500 episode reward: total was -65.090000. running mean: -34.026309\n",
      "ep 1477: ep_len:3 episode reward: total was -1.500000. running mean: -33.701046\n",
      "ep 1477: ep_len:567 episode reward: total was -109.870000. running mean: -34.462736\n",
      "ep 1477: ep_len:549 episode reward: total was -90.560000. running mean: -35.023708\n",
      "epsilon:0.134475 episode_count: 10346. steps_count: 4494096.000000\n",
      "Time elapsed:  12894.367476940155\n",
      "ep 1478: ep_len:599 episode reward: total was -23.090000. running mean: -34.904371\n",
      "ep 1478: ep_len:346 episode reward: total was -97.070000. running mean: -35.526028\n",
      "ep 1478: ep_len:676 episode reward: total was -95.190000. running mean: -36.122667\n",
      "ep 1478: ep_len:500 episode reward: total was -34.370000. running mean: -36.105141\n",
      "ep 1478: ep_len:3 episode reward: total was -1.500000. running mean: -35.759089\n",
      "ep 1478: ep_len:678 episode reward: total was -115.600000. running mean: -36.557498\n",
      "ep 1478: ep_len:500 episode reward: total was -34.130000. running mean: -36.533223\n",
      "epsilon:0.134431 episode_count: 10353. steps_count: 4497398.000000\n",
      "Time elapsed:  12906.298641443253\n",
      "ep 1479: ep_len:218 episode reward: total was -1.410000. running mean: -36.181991\n",
      "ep 1479: ep_len:673 episode reward: total was -103.140000. running mean: -36.851571\n",
      "ep 1479: ep_len:445 episode reward: total was -69.180000. running mean: -37.174856\n",
      "ep 1479: ep_len:518 episode reward: total was -18.700000. running mean: -36.990107\n",
      "ep 1479: ep_len:56 episode reward: total was 20.500000. running mean: -36.415206\n",
      "ep 1479: ep_len:598 episode reward: total was -27.840000. running mean: -36.329454\n",
      "ep 1479: ep_len:512 episode reward: total was -46.690000. running mean: -36.433059\n",
      "epsilon:0.134387 episode_count: 10360. steps_count: 4500418.000000\n",
      "Time elapsed:  12914.298903942108\n",
      "ep 1480: ep_len:607 episode reward: total was -21.000000. running mean: -36.278729\n",
      "ep 1480: ep_len:304 episode reward: total was -9.230000. running mean: -36.008241\n",
      "ep 1480: ep_len:642 episode reward: total was -64.450000. running mean: -36.292659\n",
      "ep 1480: ep_len:599 episode reward: total was 2.640000. running mean: -35.903332\n",
      "ep 1480: ep_len:56 episode reward: total was 16.000000. running mean: -35.384299\n",
      "ep 1480: ep_len:583 episode reward: total was -63.750000. running mean: -35.667956\n",
      "ep 1480: ep_len:504 episode reward: total was -126.350000. running mean: -36.574777\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.134342 episode_count: 10367. steps_count: 4503713.000000\n",
      "Time elapsed:  12927.533096790314\n",
      "ep 1481: ep_len:637 episode reward: total was -60.090000. running mean: -36.809929\n",
      "ep 1481: ep_len:560 episode reward: total was -47.180000. running mean: -36.913630\n",
      "ep 1481: ep_len:633 episode reward: total was -69.100000. running mean: -37.235493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1481: ep_len:501 episode reward: total was 16.630000. running mean: -36.696838\n",
      "ep 1481: ep_len:126 episode reward: total was -3.690000. running mean: -36.366770\n",
      "ep 1481: ep_len:315 episode reward: total was -8.130000. running mean: -36.084402\n",
      "ep 1481: ep_len:500 episode reward: total was -41.400000. running mean: -36.137558\n",
      "epsilon:0.134298 episode_count: 10374. steps_count: 4506985.000000\n",
      "Time elapsed:  12936.380039215088\n",
      "ep 1482: ep_len:643 episode reward: total was -14.920000. running mean: -35.925383\n",
      "ep 1482: ep_len:591 episode reward: total was -69.520000. running mean: -36.261329\n",
      "ep 1482: ep_len:564 episode reward: total was -52.550000. running mean: -36.424215\n",
      "ep 1482: ep_len:508 episode reward: total was 17.430000. running mean: -35.885673\n",
      "ep 1482: ep_len:3 episode reward: total was 0.000000. running mean: -35.526817\n",
      "ep 1482: ep_len:542 episode reward: total was -56.900000. running mean: -35.740548\n",
      "ep 1482: ep_len:500 episode reward: total was -48.400000. running mean: -35.867143\n",
      "epsilon:0.134254 episode_count: 10381. steps_count: 4510336.000000\n",
      "Time elapsed:  12946.371945381165\n",
      "ep 1483: ep_len:534 episode reward: total was -27.650000. running mean: -35.784972\n",
      "ep 1483: ep_len:536 episode reward: total was -49.930000. running mean: -35.926422\n",
      "ep 1483: ep_len:500 episode reward: total was -28.030000. running mean: -35.847458\n",
      "ep 1483: ep_len:568 episode reward: total was 3.330000. running mean: -35.455683\n",
      "ep 1483: ep_len:3 episode reward: total was -1.500000. running mean: -35.116126\n",
      "ep 1483: ep_len:323 episode reward: total was -32.020000. running mean: -35.085165\n",
      "ep 1483: ep_len:210 episode reward: total was -19.470000. running mean: -34.929013\n",
      "epsilon:0.134209 episode_count: 10388. steps_count: 4513010.000000\n",
      "Time elapsed:  12954.746170759201\n",
      "ep 1484: ep_len:213 episode reward: total was 13.630000. running mean: -34.443423\n",
      "ep 1484: ep_len:347 episode reward: total was -59.640000. running mean: -34.695389\n",
      "ep 1484: ep_len:371 episode reward: total was -14.410000. running mean: -34.492535\n",
      "ep 1484: ep_len:166 episode reward: total was -5.420000. running mean: -34.201810\n",
      "ep 1484: ep_len:3 episode reward: total was 0.000000. running mean: -33.859792\n",
      "ep 1484: ep_len:501 episode reward: total was -59.740000. running mean: -34.118594\n",
      "ep 1484: ep_len:312 episode reward: total was -23.630000. running mean: -34.013708\n",
      "epsilon:0.134165 episode_count: 10395. steps_count: 4514923.000000\n",
      "Time elapsed:  12962.64601945877\n",
      "ep 1485: ep_len:587 episode reward: total was -14.520000. running mean: -33.818771\n",
      "ep 1485: ep_len:628 episode reward: total was -54.470000. running mean: -34.025283\n",
      "ep 1485: ep_len:546 episode reward: total was -71.160000. running mean: -34.396630\n",
      "ep 1485: ep_len:543 episode reward: total was 12.660000. running mean: -33.926064\n",
      "ep 1485: ep_len:3 episode reward: total was 1.010000. running mean: -33.576703\n",
      "ep 1485: ep_len:500 episode reward: total was -106.060000. running mean: -34.301536\n",
      "ep 1485: ep_len:579 episode reward: total was -53.190000. running mean: -34.490421\n",
      "epsilon:0.134121 episode_count: 10402. steps_count: 4518309.000000\n",
      "Time elapsed:  12974.460865736008\n",
      "ep 1486: ep_len:501 episode reward: total was 16.850000. running mean: -33.977017\n",
      "ep 1486: ep_len:516 episode reward: total was -23.980000. running mean: -33.877046\n",
      "ep 1486: ep_len:514 episode reward: total was -41.740000. running mean: -33.955676\n",
      "ep 1486: ep_len:166 episode reward: total was -7.450000. running mean: -33.690619\n",
      "ep 1486: ep_len:96 episode reward: total was 20.730000. running mean: -33.146413\n",
      "ep 1486: ep_len:571 episode reward: total was -131.860000. running mean: -34.133549\n",
      "ep 1486: ep_len:593 episode reward: total was -29.150000. running mean: -34.083713\n",
      "epsilon:0.134076 episode_count: 10409. steps_count: 4521266.000000\n",
      "Time elapsed:  12983.504942655563\n",
      "ep 1487: ep_len:500 episode reward: total was 28.550000. running mean: -33.457376\n",
      "ep 1487: ep_len:500 episode reward: total was -40.800000. running mean: -33.530802\n",
      "ep 1487: ep_len:571 episode reward: total was -53.050000. running mean: -33.725994\n",
      "ep 1487: ep_len:489 episode reward: total was -16.990000. running mean: -33.558634\n",
      "ep 1487: ep_len:100 episode reward: total was 13.240000. running mean: -33.090648\n",
      "ep 1487: ep_len:500 episode reward: total was -39.940000. running mean: -33.159142\n",
      "ep 1487: ep_len:352 episode reward: total was -21.040000. running mean: -33.037950\n",
      "epsilon:0.134032 episode_count: 10416. steps_count: 4524278.000000\n",
      "Time elapsed:  12993.021196603775\n",
      "ep 1488: ep_len:560 episode reward: total was 29.260000. running mean: -32.414971\n",
      "ep 1488: ep_len:510 episode reward: total was 8.150000. running mean: -32.009321\n",
      "ep 1488: ep_len:642 episode reward: total was -73.650000. running mean: -32.425728\n",
      "ep 1488: ep_len:401 episode reward: total was -34.100000. running mean: -32.442471\n",
      "ep 1488: ep_len:48 episode reward: total was 14.510000. running mean: -31.972946\n",
      "ep 1488: ep_len:500 episode reward: total was -98.500000. running mean: -32.638216\n",
      "ep 1488: ep_len:503 episode reward: total was -45.700000. running mean: -32.768834\n",
      "epsilon:0.133988 episode_count: 10423. steps_count: 4527442.000000\n",
      "Time elapsed:  13004.988373041153\n",
      "ep 1489: ep_len:539 episode reward: total was -7.190000. running mean: -32.513046\n",
      "ep 1489: ep_len:600 episode reward: total was -9.180000. running mean: -32.279715\n",
      "ep 1489: ep_len:396 episode reward: total was -19.230000. running mean: -32.149218\n",
      "ep 1489: ep_len:56 episode reward: total was -6.680000. running mean: -31.894526\n",
      "ep 1489: ep_len:3 episode reward: total was 0.000000. running mean: -31.575581\n",
      "ep 1489: ep_len:546 episode reward: total was -73.060000. running mean: -31.990425\n",
      "ep 1489: ep_len:505 episode reward: total was -65.910000. running mean: -32.329621\n",
      "epsilon:0.133943 episode_count: 10430. steps_count: 4530087.000000\n",
      "Time elapsed:  13012.959970235825\n",
      "ep 1490: ep_len:261 episode reward: total was -24.730000. running mean: -32.253625\n",
      "ep 1490: ep_len:183 episode reward: total was -26.750000. running mean: -32.198588\n",
      "ep 1490: ep_len:577 episode reward: total was -139.640000. running mean: -33.273002\n",
      "ep 1490: ep_len:516 episode reward: total was -29.480000. running mean: -33.235072\n",
      "ep 1490: ep_len:106 episode reward: total was 4.700000. running mean: -32.855722\n",
      "ep 1490: ep_len:500 episode reward: total was -40.760000. running mean: -32.934764\n",
      "ep 1490: ep_len:546 episode reward: total was -34.190000. running mean: -32.947317\n",
      "epsilon:0.133899 episode_count: 10437. steps_count: 4532776.000000\n",
      "Time elapsed:  13020.605433225632\n",
      "ep 1491: ep_len:500 episode reward: total was 24.220000. running mean: -32.375644\n",
      "ep 1491: ep_len:500 episode reward: total was -30.300000. running mean: -32.354887\n",
      "ep 1491: ep_len:569 episode reward: total was -57.340000. running mean: -32.604738\n",
      "ep 1491: ep_len:41 episode reward: total was -11.250000. running mean: -32.391191\n",
      "ep 1491: ep_len:53 episode reward: total was 16.000000. running mean: -31.907279\n",
      "ep 1491: ep_len:600 episode reward: total was 3.530000. running mean: -31.552906\n",
      "ep 1491: ep_len:582 episode reward: total was -35.890000. running mean: -31.596277\n",
      "epsilon:0.133855 episode_count: 10444. steps_count: 4535621.000000\n",
      "Time elapsed:  13029.3578042984\n",
      "ep 1492: ep_len:581 episode reward: total was 20.560000. running mean: -31.074714\n",
      "ep 1492: ep_len:515 episode reward: total was -83.440000. running mean: -31.598367\n",
      "ep 1492: ep_len:560 episode reward: total was -61.040000. running mean: -31.892784\n",
      "ep 1492: ep_len:170 episode reward: total was 16.680000. running mean: -31.407056\n",
      "ep 1492: ep_len:3 episode reward: total was 1.010000. running mean: -31.082885\n",
      "ep 1492: ep_len:168 episode reward: total was 2.600000. running mean: -30.746056\n",
      "ep 1492: ep_len:500 episode reward: total was -49.550000. running mean: -30.934096\n",
      "epsilon:0.133810 episode_count: 10451. steps_count: 4538118.000000\n",
      "Time elapsed:  13038.829488754272\n",
      "ep 1493: ep_len:579 episode reward: total was -47.160000. running mean: -31.096355\n",
      "ep 1493: ep_len:500 episode reward: total was -29.650000. running mean: -31.081891\n",
      "ep 1493: ep_len:500 episode reward: total was -71.090000. running mean: -31.481972\n",
      "ep 1493: ep_len:500 episode reward: total was -35.690000. running mean: -31.524053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1493: ep_len:3 episode reward: total was 1.010000. running mean: -31.198712\n",
      "ep 1493: ep_len:523 episode reward: total was -81.090000. running mean: -31.697625\n",
      "ep 1493: ep_len:500 episode reward: total was -72.700000. running mean: -32.107649\n",
      "epsilon:0.133766 episode_count: 10458. steps_count: 4541223.000000\n",
      "Time elapsed:  13048.270711183548\n",
      "ep 1494: ep_len:508 episode reward: total was -87.510000. running mean: -32.661672\n",
      "ep 1494: ep_len:501 episode reward: total was -66.850000. running mean: -33.003556\n",
      "ep 1494: ep_len:540 episode reward: total was -140.950000. running mean: -34.083020\n",
      "ep 1494: ep_len:534 episode reward: total was -3.810000. running mean: -33.780290\n",
      "ep 1494: ep_len:3 episode reward: total was 0.000000. running mean: -33.442487\n",
      "ep 1494: ep_len:552 episode reward: total was -22.020000. running mean: -33.328262\n",
      "ep 1494: ep_len:298 episode reward: total was -24.840000. running mean: -33.243379\n",
      "epsilon:0.133722 episode_count: 10465. steps_count: 4544159.000000\n",
      "Time elapsed:  13057.365807294846\n",
      "ep 1495: ep_len:515 episode reward: total was -62.440000. running mean: -33.535346\n",
      "ep 1495: ep_len:538 episode reward: total was 20.520000. running mean: -32.994792\n",
      "ep 1495: ep_len:573 episode reward: total was -23.970000. running mean: -32.904544\n",
      "ep 1495: ep_len:528 episode reward: total was 18.900000. running mean: -32.386499\n",
      "ep 1495: ep_len:92 episode reward: total was 17.230000. running mean: -31.890334\n",
      "ep 1495: ep_len:621 episode reward: total was -25.410000. running mean: -31.825530\n",
      "ep 1495: ep_len:504 episode reward: total was -72.980000. running mean: -32.237075\n",
      "epsilon:0.133677 episode_count: 10472. steps_count: 4547530.000000\n",
      "Time elapsed:  13069.979679346085\n",
      "ep 1496: ep_len:638 episode reward: total was -34.250000. running mean: -32.257204\n",
      "ep 1496: ep_len:533 episode reward: total was -47.470000. running mean: -32.409332\n",
      "ep 1496: ep_len:507 episode reward: total was -55.580000. running mean: -32.641039\n",
      "ep 1496: ep_len:500 episode reward: total was -46.830000. running mean: -32.782929\n",
      "ep 1496: ep_len:108 episode reward: total was -42.240000. running mean: -32.877499\n",
      "ep 1496: ep_len:500 episode reward: total was -2.560000. running mean: -32.574324\n",
      "ep 1496: ep_len:503 episode reward: total was -57.480000. running mean: -32.823381\n",
      "epsilon:0.133633 episode_count: 10479. steps_count: 4550819.000000\n",
      "Time elapsed:  13082.0153901577\n",
      "ep 1497: ep_len:500 episode reward: total was 10.510000. running mean: -32.390047\n",
      "ep 1497: ep_len:541 episode reward: total was -34.450000. running mean: -32.410647\n",
      "ep 1497: ep_len:679 episode reward: total was -74.470000. running mean: -32.831240\n",
      "ep 1497: ep_len:56 episode reward: total was 1.340000. running mean: -32.489528\n",
      "ep 1497: ep_len:3 episode reward: total was 0.000000. running mean: -32.164633\n",
      "ep 1497: ep_len:500 episode reward: total was -14.100000. running mean: -31.983986\n",
      "ep 1497: ep_len:631 episode reward: total was -119.450000. running mean: -32.858646\n",
      "epsilon:0.133589 episode_count: 10486. steps_count: 4553729.000000\n",
      "Time elapsed:  13090.400456666946\n",
      "ep 1498: ep_len:520 episode reward: total was -6.560000. running mean: -32.595660\n",
      "ep 1498: ep_len:363 episode reward: total was -31.760000. running mean: -32.587303\n",
      "ep 1498: ep_len:387 episode reward: total was -10.720000. running mean: -32.368630\n",
      "ep 1498: ep_len:507 episode reward: total was -15.720000. running mean: -32.202144\n",
      "ep 1498: ep_len:3 episode reward: total was -1.500000. running mean: -31.895123\n",
      "ep 1498: ep_len:528 episode reward: total was -111.550000. running mean: -32.691671\n",
      "ep 1498: ep_len:500 episode reward: total was -39.120000. running mean: -32.755955\n",
      "epsilon:0.133544 episode_count: 10493. steps_count: 4556537.000000\n",
      "Time elapsed:  13099.068387031555\n",
      "ep 1499: ep_len:661 episode reward: total was -147.210000. running mean: -33.900495\n",
      "ep 1499: ep_len:501 episode reward: total was -30.910000. running mean: -33.870590\n",
      "ep 1499: ep_len:438 episode reward: total was -28.750000. running mean: -33.819384\n",
      "ep 1499: ep_len:562 episode reward: total was -28.260000. running mean: -33.763790\n",
      "ep 1499: ep_len:3 episode reward: total was 0.000000. running mean: -33.426153\n",
      "ep 1499: ep_len:564 episode reward: total was -29.190000. running mean: -33.383791\n",
      "ep 1499: ep_len:581 episode reward: total was -78.370000. running mean: -33.833653\n",
      "epsilon:0.133500 episode_count: 10500. steps_count: 4559847.000000\n",
      "Time elapsed:  13108.882316350937\n",
      "ep 1500: ep_len:560 episode reward: total was -83.510000. running mean: -34.330417\n",
      "ep 1500: ep_len:299 episode reward: total was -15.460000. running mean: -34.141712\n",
      "ep 1500: ep_len:665 episode reward: total was -68.350000. running mean: -34.483795\n",
      "ep 1500: ep_len:500 episode reward: total was 9.490000. running mean: -34.044057\n",
      "ep 1500: ep_len:93 episode reward: total was -6.760000. running mean: -33.771217\n",
      "ep 1500: ep_len:322 episode reward: total was 1.060000. running mean: -33.422905\n",
      "ep 1500: ep_len:505 episode reward: total was -61.990000. running mean: -33.708576\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.133456 episode_count: 10507. steps_count: 4562791.000000\n",
      "Time elapsed:  13123.440284729004\n",
      "ep 1501: ep_len:609 episode reward: total was -14.250000. running mean: -33.513990\n",
      "ep 1501: ep_len:500 episode reward: total was -33.400000. running mean: -33.512850\n",
      "ep 1501: ep_len:68 episode reward: total was 8.750000. running mean: -33.090221\n",
      "ep 1501: ep_len:563 episode reward: total was -62.020000. running mean: -33.379519\n",
      "ep 1501: ep_len:3 episode reward: total was 1.010000. running mean: -33.035624\n",
      "ep 1501: ep_len:231 episode reward: total was 20.390000. running mean: -32.501368\n",
      "ep 1501: ep_len:500 episode reward: total was -59.200000. running mean: -32.768354\n",
      "epsilon:0.133411 episode_count: 10514. steps_count: 4565265.000000\n",
      "Time elapsed:  13130.513855457306\n",
      "ep 1502: ep_len:500 episode reward: total was 20.760000. running mean: -32.233071\n",
      "ep 1502: ep_len:548 episode reward: total was -12.290000. running mean: -32.033640\n",
      "ep 1502: ep_len:589 episode reward: total was -41.300000. running mean: -32.126303\n",
      "ep 1502: ep_len:117 episode reward: total was 1.540000. running mean: -31.789640\n",
      "ep 1502: ep_len:88 episode reward: total was 9.690000. running mean: -31.374844\n",
      "ep 1502: ep_len:172 episode reward: total was 10.050000. running mean: -30.960596\n",
      "ep 1502: ep_len:517 episode reward: total was -46.660000. running mean: -31.117590\n",
      "epsilon:0.133367 episode_count: 10521. steps_count: 4567796.000000\n",
      "Time elapsed:  13137.45514845848\n",
      "ep 1503: ep_len:127 episode reward: total was -6.140000. running mean: -30.867814\n",
      "ep 1503: ep_len:589 episode reward: total was -89.770000. running mean: -31.456836\n",
      "ep 1503: ep_len:424 episode reward: total was 6.920000. running mean: -31.073067\n",
      "ep 1503: ep_len:54 episode reward: total was -1.710000. running mean: -30.779437\n",
      "ep 1503: ep_len:3 episode reward: total was -1.500000. running mean: -30.486642\n",
      "ep 1503: ep_len:500 episode reward: total was -24.310000. running mean: -30.424876\n",
      "ep 1503: ep_len:517 episode reward: total was -47.680000. running mean: -30.597427\n",
      "epsilon:0.133323 episode_count: 10528. steps_count: 4570010.000000\n",
      "Time elapsed:  13143.646964788437\n",
      "ep 1504: ep_len:209 episode reward: total was -4.900000. running mean: -30.340453\n",
      "ep 1504: ep_len:248 episode reward: total was -9.670000. running mean: -30.133748\n",
      "ep 1504: ep_len:583 episode reward: total was -108.100000. running mean: -30.913411\n",
      "ep 1504: ep_len:503 episode reward: total was -13.840000. running mean: -30.742677\n",
      "ep 1504: ep_len:3 episode reward: total was -3.000000. running mean: -30.465250\n",
      "ep 1504: ep_len:634 episode reward: total was -68.340000. running mean: -30.843997\n",
      "ep 1504: ep_len:500 episode reward: total was -106.880000. running mean: -31.604357\n",
      "epsilon:0.133278 episode_count: 10535. steps_count: 4572690.000000\n",
      "Time elapsed:  13150.08254480362\n",
      "ep 1505: ep_len:510 episode reward: total was -28.490000. running mean: -31.573214\n",
      "ep 1505: ep_len:592 episode reward: total was -15.350000. running mean: -31.410982\n",
      "ep 1505: ep_len:411 episode reward: total was -18.320000. running mean: -31.280072\n",
      "ep 1505: ep_len:523 episode reward: total was -32.050000. running mean: -31.287771\n",
      "ep 1505: ep_len:33 episode reward: total was 10.500000. running mean: -30.869893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1505: ep_len:205 episode reward: total was 5.830000. running mean: -30.502894\n",
      "ep 1505: ep_len:343 episode reward: total was -26.100000. running mean: -30.458866\n",
      "epsilon:0.133234 episode_count: 10542. steps_count: 4575307.000000\n",
      "Time elapsed:  13157.118838310242\n",
      "ep 1506: ep_len:229 episode reward: total was 3.260000. running mean: -30.121677\n",
      "ep 1506: ep_len:614 episode reward: total was -46.430000. running mean: -30.284760\n",
      "ep 1506: ep_len:696 episode reward: total was -84.930000. running mean: -30.831213\n",
      "ep 1506: ep_len:520 episode reward: total was -32.060000. running mean: -30.843500\n",
      "ep 1506: ep_len:93 episode reward: total was 11.210000. running mean: -30.422965\n",
      "ep 1506: ep_len:570 episode reward: total was -43.840000. running mean: -30.557136\n",
      "ep 1506: ep_len:272 episode reward: total was -31.410000. running mean: -30.565664\n",
      "epsilon:0.133190 episode_count: 10549. steps_count: 4578301.000000\n",
      "Time elapsed:  13165.147722959518\n",
      "ep 1507: ep_len:575 episode reward: total was -1.660000. running mean: -30.276608\n",
      "ep 1507: ep_len:184 episode reward: total was -38.380000. running mean: -30.357642\n",
      "ep 1507: ep_len:619 episode reward: total was -27.650000. running mean: -30.330565\n",
      "ep 1507: ep_len:511 episode reward: total was -77.760000. running mean: -30.804860\n",
      "ep 1507: ep_len:79 episode reward: total was -9.230000. running mean: -30.589111\n",
      "ep 1507: ep_len:598 episode reward: total was -56.120000. running mean: -30.844420\n",
      "ep 1507: ep_len:536 episode reward: total was -33.570000. running mean: -30.871676\n",
      "epsilon:0.133145 episode_count: 10556. steps_count: 4581403.000000\n",
      "Time elapsed:  13173.325562477112\n",
      "ep 1508: ep_len:510 episode reward: total was -19.250000. running mean: -30.755459\n",
      "ep 1508: ep_len:521 episode reward: total was -77.900000. running mean: -31.226904\n",
      "ep 1508: ep_len:656 episode reward: total was -166.580000. running mean: -32.580435\n",
      "ep 1508: ep_len:500 episode reward: total was -38.170000. running mean: -32.636331\n",
      "ep 1508: ep_len:3 episode reward: total was 0.000000. running mean: -32.309968\n",
      "ep 1508: ep_len:156 episode reward: total was 6.890000. running mean: -31.917968\n",
      "ep 1508: ep_len:500 episode reward: total was -48.380000. running mean: -32.082588\n",
      "epsilon:0.133101 episode_count: 10563. steps_count: 4584249.000000\n",
      "Time elapsed:  13182.318424463272\n",
      "ep 1509: ep_len:212 episode reward: total was -4.530000. running mean: -31.807062\n",
      "ep 1509: ep_len:500 episode reward: total was -5.670000. running mean: -31.545692\n",
      "ep 1509: ep_len:606 episode reward: total was -63.010000. running mean: -31.860335\n",
      "ep 1509: ep_len:621 episode reward: total was -66.680000. running mean: -32.208532\n",
      "ep 1509: ep_len:3 episode reward: total was 0.000000. running mean: -31.886446\n",
      "ep 1509: ep_len:186 episode reward: total was 7.100000. running mean: -31.496582\n",
      "ep 1509: ep_len:500 episode reward: total was -54.950000. running mean: -31.731116\n",
      "epsilon:0.133057 episode_count: 10570. steps_count: 4586877.000000\n",
      "Time elapsed:  13189.328228712082\n",
      "ep 1510: ep_len:120 episode reward: total was -15.150000. running mean: -31.565305\n",
      "ep 1510: ep_len:500 episode reward: total was -52.280000. running mean: -31.772452\n",
      "ep 1510: ep_len:680 episode reward: total was -83.800000. running mean: -32.292727\n",
      "ep 1510: ep_len:39 episode reward: total was 3.160000. running mean: -31.938200\n",
      "ep 1510: ep_len:3 episode reward: total was -1.500000. running mean: -31.633818\n",
      "ep 1510: ep_len:625 episode reward: total was -51.020000. running mean: -31.827680\n",
      "ep 1510: ep_len:595 episode reward: total was -49.370000. running mean: -32.003103\n",
      "epsilon:0.133012 episode_count: 10577. steps_count: 4589439.000000\n",
      "Time elapsed:  13198.38926911354\n",
      "ep 1511: ep_len:620 episode reward: total was -143.490000. running mean: -33.117972\n",
      "ep 1511: ep_len:531 episode reward: total was -29.340000. running mean: -33.080192\n",
      "ep 1511: ep_len:661 episode reward: total was -92.620000. running mean: -33.675590\n",
      "ep 1511: ep_len:500 episode reward: total was -26.700000. running mean: -33.605834\n",
      "ep 1511: ep_len:79 episode reward: total was -33.900000. running mean: -33.608776\n",
      "ep 1511: ep_len:511 episode reward: total was -87.420000. running mean: -34.146888\n",
      "ep 1511: ep_len:529 episode reward: total was -69.660000. running mean: -34.502019\n",
      "epsilon:0.132968 episode_count: 10584. steps_count: 4592870.000000\n",
      "Time elapsed:  13207.365413665771\n",
      "ep 1512: ep_len:500 episode reward: total was 0.500000. running mean: -34.151999\n",
      "ep 1512: ep_len:500 episode reward: total was -22.040000. running mean: -34.030879\n",
      "ep 1512: ep_len:543 episode reward: total was -83.880000. running mean: -34.529370\n",
      "ep 1512: ep_len:610 episode reward: total was 10.890000. running mean: -34.075177\n",
      "ep 1512: ep_len:112 episode reward: total was -9.720000. running mean: -33.831625\n",
      "ep 1512: ep_len:515 episode reward: total was -47.160000. running mean: -33.964909\n",
      "ep 1512: ep_len:546 episode reward: total was -53.800000. running mean: -34.163260\n",
      "epsilon:0.132924 episode_count: 10591. steps_count: 4596196.000000\n",
      "Time elapsed:  13216.073737859726\n",
      "ep 1513: ep_len:560 episode reward: total was 5.970000. running mean: -33.761927\n",
      "ep 1513: ep_len:503 episode reward: total was -77.580000. running mean: -34.200108\n",
      "ep 1513: ep_len:667 episode reward: total was -131.290000. running mean: -35.171007\n",
      "ep 1513: ep_len:500 episode reward: total was 22.500000. running mean: -34.594297\n",
      "ep 1513: ep_len:87 episode reward: total was 8.730000. running mean: -34.161054\n",
      "ep 1513: ep_len:560 episode reward: total was -7.010000. running mean: -33.889543\n",
      "ep 1513: ep_len:500 episode reward: total was -55.990000. running mean: -34.110548\n",
      "epsilon:0.132879 episode_count: 10598. steps_count: 4599573.000000\n",
      "Time elapsed:  13226.529285907745\n",
      "ep 1514: ep_len:121 episode reward: total was 0.990000. running mean: -33.759542\n",
      "ep 1514: ep_len:500 episode reward: total was -69.230000. running mean: -34.114247\n",
      "ep 1514: ep_len:373 episode reward: total was 2.260000. running mean: -33.750504\n",
      "ep 1514: ep_len:590 episode reward: total was 0.070000. running mean: -33.412299\n",
      "ep 1514: ep_len:37 episode reward: total was 14.000000. running mean: -32.938176\n",
      "ep 1514: ep_len:248 episode reward: total was 13.830000. running mean: -32.470494\n",
      "ep 1514: ep_len:508 episode reward: total was -64.860000. running mean: -32.794390\n",
      "epsilon:0.132835 episode_count: 10605. steps_count: 4601950.000000\n",
      "Time elapsed:  13233.149003505707\n",
      "ep 1515: ep_len:623 episode reward: total was -0.970000. running mean: -32.476146\n",
      "ep 1515: ep_len:176 episode reward: total was -7.720000. running mean: -32.228584\n",
      "ep 1515: ep_len:79 episode reward: total was -6.260000. running mean: -31.968898\n",
      "ep 1515: ep_len:56 episode reward: total was -14.220000. running mean: -31.791409\n",
      "ep 1515: ep_len:107 episode reward: total was 12.760000. running mean: -31.345895\n",
      "ep 1515: ep_len:500 episode reward: total was -26.030000. running mean: -31.292736\n",
      "ep 1515: ep_len:524 episode reward: total was -42.230000. running mean: -31.402109\n",
      "epsilon:0.132791 episode_count: 10612. steps_count: 4604015.000000\n",
      "Time elapsed:  13239.077649593353\n",
      "ep 1516: ep_len:197 episode reward: total was -10.500000. running mean: -31.193088\n",
      "ep 1516: ep_len:280 episode reward: total was -35.610000. running mean: -31.237257\n",
      "ep 1516: ep_len:500 episode reward: total was -21.610000. running mean: -31.140984\n",
      "ep 1516: ep_len:40 episode reward: total was -3.810000. running mean: -30.867675\n",
      "ep 1516: ep_len:3 episode reward: total was 0.000000. running mean: -30.558998\n",
      "ep 1516: ep_len:186 episode reward: total was 13.100000. running mean: -30.122408\n",
      "ep 1516: ep_len:342 episode reward: total was -32.860000. running mean: -30.149784\n",
      "epsilon:0.132746 episode_count: 10619. steps_count: 4605563.000000\n",
      "Time elapsed:  13243.668094158173\n",
      "ep 1517: ep_len:579 episode reward: total was -125.650000. running mean: -31.104786\n",
      "ep 1517: ep_len:558 episode reward: total was -114.680000. running mean: -31.940538\n",
      "ep 1517: ep_len:512 episode reward: total was -22.430000. running mean: -31.845433\n",
      "ep 1517: ep_len:532 episode reward: total was -27.600000. running mean: -31.802978\n",
      "ep 1517: ep_len:76 episode reward: total was 5.560000. running mean: -31.429349\n",
      "ep 1517: ep_len:500 episode reward: total was -40.890000. running mean: -31.523955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1517: ep_len:583 episode reward: total was -40.170000. running mean: -31.610416\n",
      "epsilon:0.132702 episode_count: 10626. steps_count: 4608903.000000\n",
      "Time elapsed:  13252.587610244751\n",
      "ep 1518: ep_len:252 episode reward: total was -9.320000. running mean: -31.387511\n",
      "ep 1518: ep_len:500 episode reward: total was 9.450000. running mean: -30.979136\n",
      "ep 1518: ep_len:500 episode reward: total was -6.160000. running mean: -30.730945\n",
      "ep 1518: ep_len:556 episode reward: total was -15.680000. running mean: -30.580435\n",
      "ep 1518: ep_len:130 episode reward: total was 13.330000. running mean: -30.141331\n",
      "ep 1518: ep_len:500 episode reward: total was 3.730000. running mean: -29.802618\n",
      "ep 1518: ep_len:615 episode reward: total was -31.130000. running mean: -29.815892\n",
      "epsilon:0.132658 episode_count: 10633. steps_count: 4611956.000000\n",
      "Time elapsed:  13260.868569135666\n",
      "ep 1519: ep_len:501 episode reward: total was -24.140000. running mean: -29.759133\n",
      "ep 1519: ep_len:556 episode reward: total was -190.090000. running mean: -31.362441\n",
      "ep 1519: ep_len:547 episode reward: total was -86.200000. running mean: -31.910817\n",
      "ep 1519: ep_len:510 episode reward: total was -45.200000. running mean: -32.043709\n",
      "ep 1519: ep_len:3 episode reward: total was -1.500000. running mean: -31.738272\n",
      "ep 1519: ep_len:603 episode reward: total was -35.140000. running mean: -31.772289\n",
      "ep 1519: ep_len:500 episode reward: total was -28.950000. running mean: -31.744066\n",
      "epsilon:0.132613 episode_count: 10640. steps_count: 4615176.000000\n",
      "Time elapsed:  13271.2022960186\n",
      "ep 1520: ep_len:641 episode reward: total was -174.010000. running mean: -33.166725\n",
      "ep 1520: ep_len:554 episode reward: total was -45.610000. running mean: -33.291158\n",
      "ep 1520: ep_len:556 episode reward: total was -52.280000. running mean: -33.481047\n",
      "ep 1520: ep_len:521 episode reward: total was -20.480000. running mean: -33.351036\n",
      "ep 1520: ep_len:3 episode reward: total was 0.000000. running mean: -33.017526\n",
      "ep 1520: ep_len:524 episode reward: total was -107.900000. running mean: -33.766351\n",
      "ep 1520: ep_len:603 episode reward: total was -22.320000. running mean: -33.651887\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.132569 episode_count: 10647. steps_count: 4618578.000000\n",
      "Time elapsed:  13285.609578847885\n",
      "ep 1521: ep_len:500 episode reward: total was -27.610000. running mean: -33.591468\n",
      "ep 1521: ep_len:569 episode reward: total was 58.420000. running mean: -32.671353\n",
      "ep 1521: ep_len:745 episode reward: total was -222.580000. running mean: -34.570440\n",
      "ep 1521: ep_len:518 episode reward: total was -41.820000. running mean: -34.642936\n",
      "ep 1521: ep_len:3 episode reward: total was -1.500000. running mean: -34.311506\n",
      "ep 1521: ep_len:687 episode reward: total was -42.350000. running mean: -34.391891\n",
      "ep 1521: ep_len:523 episode reward: total was -44.910000. running mean: -34.497072\n",
      "epsilon:0.132525 episode_count: 10654. steps_count: 4622123.000000\n",
      "Time elapsed:  13294.912077665329\n",
      "ep 1522: ep_len:134 episode reward: total was -0.560000. running mean: -34.157701\n",
      "ep 1522: ep_len:500 episode reward: total was -137.870000. running mean: -35.194824\n",
      "ep 1522: ep_len:500 episode reward: total was -38.640000. running mean: -35.229276\n",
      "ep 1522: ep_len:158 episode reward: total was -0.940000. running mean: -34.886383\n",
      "ep 1522: ep_len:3 episode reward: total was 0.000000. running mean: -34.537520\n",
      "ep 1522: ep_len:565 episode reward: total was -35.890000. running mean: -34.551044\n",
      "ep 1522: ep_len:518 episode reward: total was -38.960000. running mean: -34.595134\n",
      "epsilon:0.132480 episode_count: 10661. steps_count: 4624501.000000\n",
      "Time elapsed:  13301.415783405304\n",
      "ep 1523: ep_len:541 episode reward: total was -77.440000. running mean: -35.023583\n",
      "ep 1523: ep_len:592 episode reward: total was 30.170000. running mean: -34.371647\n",
      "ep 1523: ep_len:558 episode reward: total was -53.650000. running mean: -34.564430\n",
      "ep 1523: ep_len:152 episode reward: total was 10.050000. running mean: -34.118286\n",
      "ep 1523: ep_len:3 episode reward: total was 0.000000. running mean: -33.777103\n",
      "ep 1523: ep_len:500 episode reward: total was -38.450000. running mean: -33.823832\n",
      "ep 1523: ep_len:590 episode reward: total was -42.120000. running mean: -33.906794\n",
      "epsilon:0.132436 episode_count: 10668. steps_count: 4627437.000000\n",
      "Time elapsed:  13309.285858154297\n",
      "ep 1524: ep_len:500 episode reward: total was 49.690000. running mean: -33.070826\n",
      "ep 1524: ep_len:500 episode reward: total was -34.780000. running mean: -33.087918\n",
      "ep 1524: ep_len:560 episode reward: total was -43.960000. running mean: -33.196638\n",
      "ep 1524: ep_len:500 episode reward: total was -44.250000. running mean: -33.307172\n",
      "ep 1524: ep_len:3 episode reward: total was 1.010000. running mean: -32.964000\n",
      "ep 1524: ep_len:597 episode reward: total was -23.330000. running mean: -32.867660\n",
      "ep 1524: ep_len:500 episode reward: total was -56.790000. running mean: -33.106884\n",
      "epsilon:0.132392 episode_count: 10675. steps_count: 4630597.000000\n",
      "Time elapsed:  13317.713615655899\n",
      "ep 1525: ep_len:535 episode reward: total was -57.320000. running mean: -33.349015\n",
      "ep 1525: ep_len:500 episode reward: total was 10.540000. running mean: -32.910125\n",
      "ep 1525: ep_len:553 episode reward: total was -77.290000. running mean: -33.353924\n",
      "ep 1525: ep_len:500 episode reward: total was -49.830000. running mean: -33.518684\n",
      "ep 1525: ep_len:3 episode reward: total was 0.000000. running mean: -33.183497\n",
      "ep 1525: ep_len:513 episode reward: total was -47.650000. running mean: -33.328162\n",
      "ep 1525: ep_len:315 episode reward: total was -54.880000. running mean: -33.543681\n",
      "epsilon:0.132347 episode_count: 10682. steps_count: 4633516.000000\n",
      "Time elapsed:  13326.940408706665\n",
      "ep 1526: ep_len:500 episode reward: total was 2.550000. running mean: -33.182744\n",
      "ep 1526: ep_len:500 episode reward: total was -2.970000. running mean: -32.880617\n",
      "ep 1526: ep_len:500 episode reward: total was -27.330000. running mean: -32.825110\n",
      "ep 1526: ep_len:509 episode reward: total was -57.820000. running mean: -33.075059\n",
      "ep 1526: ep_len:93 episode reward: total was -1.250000. running mean: -32.756809\n",
      "ep 1526: ep_len:616 episode reward: total was -86.100000. running mean: -33.290241\n",
      "ep 1526: ep_len:500 episode reward: total was -93.670000. running mean: -33.894038\n",
      "epsilon:0.132303 episode_count: 10689. steps_count: 4636734.000000\n",
      "Time elapsed:  13335.332269191742\n",
      "ep 1527: ep_len:500 episode reward: total was 14.580000. running mean: -33.409298\n",
      "ep 1527: ep_len:548 episode reward: total was -32.890000. running mean: -33.404105\n",
      "ep 1527: ep_len:574 episode reward: total was -64.690000. running mean: -33.716964\n",
      "ep 1527: ep_len:500 episode reward: total was -9.350000. running mean: -33.473294\n",
      "ep 1527: ep_len:100 episode reward: total was 15.230000. running mean: -32.986261\n",
      "ep 1527: ep_len:522 episode reward: total was -69.720000. running mean: -33.353599\n",
      "ep 1527: ep_len:504 episode reward: total was -67.830000. running mean: -33.698363\n",
      "epsilon:0.132259 episode_count: 10696. steps_count: 4639982.000000\n",
      "Time elapsed:  13344.022067308426\n",
      "ep 1528: ep_len:506 episode reward: total was 37.850000. running mean: -32.982879\n",
      "ep 1528: ep_len:544 episode reward: total was 17.670000. running mean: -32.476350\n",
      "ep 1528: ep_len:453 episode reward: total was -5.030000. running mean: -32.201887\n",
      "ep 1528: ep_len:151 episode reward: total was 4.640000. running mean: -31.833468\n",
      "ep 1528: ep_len:3 episode reward: total was 1.010000. running mean: -31.505033\n",
      "ep 1528: ep_len:504 episode reward: total was -33.660000. running mean: -31.526583\n",
      "ep 1528: ep_len:187 episode reward: total was -18.260000. running mean: -31.393917\n",
      "epsilon:0.132214 episode_count: 10703. steps_count: 4642330.000000\n",
      "Time elapsed:  13350.642744541168\n",
      "ep 1529: ep_len:579 episode reward: total was -11.900000. running mean: -31.198978\n",
      "ep 1529: ep_len:516 episode reward: total was 14.170000. running mean: -30.745288\n",
      "ep 1529: ep_len:79 episode reward: total was -1.760000. running mean: -30.455435\n",
      "ep 1529: ep_len:129 episode reward: total was -9.000000. running mean: -30.240881\n",
      "ep 1529: ep_len:3 episode reward: total was -1.500000. running mean: -29.953472\n",
      "ep 1529: ep_len:566 episode reward: total was -24.550000. running mean: -29.899437\n",
      "ep 1529: ep_len:585 episode reward: total was -19.810000. running mean: -29.798543\n",
      "epsilon:0.132170 episode_count: 10710. steps_count: 4644787.000000\n",
      "Time elapsed:  13359.704259872437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1530: ep_len:534 episode reward: total was -25.680000. running mean: -29.757358\n",
      "ep 1530: ep_len:500 episode reward: total was -18.930000. running mean: -29.649084\n",
      "ep 1530: ep_len:79 episode reward: total was 1.760000. running mean: -29.334993\n",
      "ep 1530: ep_len:527 episode reward: total was -81.720000. running mean: -29.858843\n",
      "ep 1530: ep_len:3 episode reward: total was -0.490000. running mean: -29.565155\n",
      "ep 1530: ep_len:630 episode reward: total was -15.840000. running mean: -29.427903\n",
      "ep 1530: ep_len:556 episode reward: total was -39.780000. running mean: -29.531424\n",
      "epsilon:0.132126 episode_count: 10717. steps_count: 4647616.000000\n",
      "Time elapsed:  13366.03649187088\n",
      "ep 1531: ep_len:534 episode reward: total was -93.870000. running mean: -30.174810\n",
      "ep 1531: ep_len:642 episode reward: total was -124.210000. running mean: -31.115162\n",
      "ep 1531: ep_len:569 episode reward: total was -30.310000. running mean: -31.107110\n",
      "ep 1531: ep_len:514 episode reward: total was -21.340000. running mean: -31.009439\n",
      "ep 1531: ep_len:50 episode reward: total was -0.500000. running mean: -30.704345\n",
      "ep 1531: ep_len:637 episode reward: total was -18.970000. running mean: -30.587001\n",
      "ep 1531: ep_len:500 episode reward: total was -54.920000. running mean: -30.830331\n",
      "epsilon:0.132081 episode_count: 10724. steps_count: 4651062.000000\n",
      "Time elapsed:  13376.832181453705\n",
      "ep 1532: ep_len:196 episode reward: total was -0.810000. running mean: -30.530128\n",
      "ep 1532: ep_len:605 episode reward: total was -58.740000. running mean: -30.812227\n",
      "ep 1532: ep_len:598 episode reward: total was -54.220000. running mean: -31.046304\n",
      "ep 1532: ep_len:389 episode reward: total was -21.530000. running mean: -30.951141\n",
      "ep 1532: ep_len:3 episode reward: total was 0.000000. running mean: -30.641630\n",
      "ep 1532: ep_len:672 episode reward: total was -33.930000. running mean: -30.674514\n",
      "ep 1532: ep_len:500 episode reward: total was -13.550000. running mean: -30.503268\n",
      "epsilon:0.132037 episode_count: 10731. steps_count: 4654025.000000\n",
      "Time elapsed:  13384.696584701538\n",
      "ep 1533: ep_len:523 episode reward: total was 30.200000. running mean: -29.896236\n",
      "ep 1533: ep_len:563 episode reward: total was -74.150000. running mean: -30.338773\n",
      "ep 1533: ep_len:449 episode reward: total was -25.000000. running mean: -30.285386\n",
      "ep 1533: ep_len:526 episode reward: total was 11.050000. running mean: -29.872032\n",
      "ep 1533: ep_len:3 episode reward: total was 0.000000. running mean: -29.573312\n",
      "ep 1533: ep_len:551 episode reward: total was -40.740000. running mean: -29.684978\n",
      "ep 1533: ep_len:545 episode reward: total was -109.670000. running mean: -30.484829\n",
      "epsilon:0.131993 episode_count: 10738. steps_count: 4657185.000000\n",
      "Time elapsed:  13393.937636613846\n",
      "ep 1534: ep_len:528 episode reward: total was -105.920000. running mean: -31.239180\n",
      "ep 1534: ep_len:500 episode reward: total was -4.690000. running mean: -30.973689\n",
      "ep 1534: ep_len:500 episode reward: total was -13.490000. running mean: -30.798852\n",
      "ep 1534: ep_len:500 episode reward: total was -23.970000. running mean: -30.730563\n",
      "ep 1534: ep_len:3 episode reward: total was 1.010000. running mean: -30.413158\n",
      "ep 1534: ep_len:500 episode reward: total was -25.310000. running mean: -30.362126\n",
      "ep 1534: ep_len:589 episode reward: total was -29.440000. running mean: -30.352905\n",
      "epsilon:0.131948 episode_count: 10745. steps_count: 4660305.000000\n",
      "Time elapsed:  13403.266758680344\n",
      "ep 1535: ep_len:500 episode reward: total was 2.750000. running mean: -30.021876\n",
      "ep 1535: ep_len:500 episode reward: total was 36.120000. running mean: -29.360457\n",
      "ep 1535: ep_len:603 episode reward: total was -30.780000. running mean: -29.374652\n",
      "ep 1535: ep_len:517 episode reward: total was -30.340000. running mean: -29.384306\n",
      "ep 1535: ep_len:3 episode reward: total was -1.500000. running mean: -29.105463\n",
      "ep 1535: ep_len:522 episode reward: total was -27.030000. running mean: -29.084708\n",
      "ep 1535: ep_len:299 episode reward: total was -21.450000. running mean: -29.008361\n",
      "epsilon:0.131904 episode_count: 10752. steps_count: 4663249.000000\n",
      "Time elapsed:  13412.850500583649\n",
      "ep 1536: ep_len:529 episode reward: total was -100.900000. running mean: -29.727277\n",
      "ep 1536: ep_len:500 episode reward: total was -47.630000. running mean: -29.906305\n",
      "ep 1536: ep_len:564 episode reward: total was -46.580000. running mean: -30.073042\n",
      "ep 1536: ep_len:500 episode reward: total was -75.330000. running mean: -30.525611\n",
      "ep 1536: ep_len:99 episode reward: total was 17.760000. running mean: -30.042755\n",
      "ep 1536: ep_len:523 episode reward: total was -27.160000. running mean: -30.013928\n",
      "ep 1536: ep_len:501 episode reward: total was -67.840000. running mean: -30.392188\n",
      "epsilon:0.131860 episode_count: 10759. steps_count: 4666465.000000\n",
      "Time elapsed:  13421.358031749725\n",
      "ep 1537: ep_len:500 episode reward: total was -8.130000. running mean: -30.169566\n",
      "ep 1537: ep_len:608 episode reward: total was 33.910000. running mean: -29.528771\n",
      "ep 1537: ep_len:580 episode reward: total was -34.250000. running mean: -29.575983\n",
      "ep 1537: ep_len:507 episode reward: total was -67.330000. running mean: -29.953523\n",
      "ep 1537: ep_len:3 episode reward: total was 0.000000. running mean: -29.653988\n",
      "ep 1537: ep_len:602 episode reward: total was -36.500000. running mean: -29.722448\n",
      "ep 1537: ep_len:205 episode reward: total was -64.120000. running mean: -30.066424\n",
      "epsilon:0.131815 episode_count: 10766. steps_count: 4669470.000000\n",
      "Time elapsed:  13429.37529039383\n",
      "ep 1538: ep_len:618 episode reward: total was 10.600000. running mean: -29.659759\n",
      "ep 1538: ep_len:553 episode reward: total was 52.960000. running mean: -28.833562\n",
      "ep 1538: ep_len:604 episode reward: total was -38.640000. running mean: -28.931626\n",
      "ep 1538: ep_len:147 episode reward: total was 7.490000. running mean: -28.567410\n",
      "ep 1538: ep_len:3 episode reward: total was 1.010000. running mean: -28.271636\n",
      "ep 1538: ep_len:689 episode reward: total was -38.930000. running mean: -28.378219\n",
      "ep 1538: ep_len:333 episode reward: total was -32.220000. running mean: -28.416637\n",
      "epsilon:0.131771 episode_count: 10773. steps_count: 4672417.000000\n",
      "Time elapsed:  13437.684320688248\n",
      "ep 1539: ep_len:597 episode reward: total was -21.760000. running mean: -28.350071\n",
      "ep 1539: ep_len:500 episode reward: total was 7.260000. running mean: -27.993970\n",
      "ep 1539: ep_len:525 episode reward: total was -21.570000. running mean: -27.929730\n",
      "ep 1539: ep_len:594 episode reward: total was -4.360000. running mean: -27.694033\n",
      "ep 1539: ep_len:67 episode reward: total was 11.770000. running mean: -27.299393\n",
      "ep 1539: ep_len:590 episode reward: total was -29.090000. running mean: -27.317299\n",
      "ep 1539: ep_len:509 episode reward: total was -29.130000. running mean: -27.335426\n",
      "epsilon:0.131727 episode_count: 10780. steps_count: 4675799.000000\n",
      "Time elapsed:  13446.599248886108\n",
      "ep 1540: ep_len:545 episode reward: total was 3.640000. running mean: -27.025672\n",
      "ep 1540: ep_len:501 episode reward: total was -21.540000. running mean: -26.970815\n",
      "ep 1540: ep_len:657 episode reward: total was -80.430000. running mean: -27.505407\n",
      "ep 1540: ep_len:145 episode reward: total was 7.610000. running mean: -27.154253\n",
      "ep 1540: ep_len:3 episode reward: total was 0.000000. running mean: -26.882710\n",
      "ep 1540: ep_len:512 episode reward: total was -57.410000. running mean: -27.187983\n",
      "ep 1540: ep_len:619 episode reward: total was -83.890000. running mean: -27.755003\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.131682 episode_count: 10787. steps_count: 4678781.000000\n",
      "Time elapsed:  13459.370553255081\n",
      "ep 1541: ep_len:131 episode reward: total was -6.620000. running mean: -27.543653\n",
      "ep 1541: ep_len:500 episode reward: total was 24.120000. running mean: -27.027017\n",
      "ep 1541: ep_len:596 episode reward: total was -126.800000. running mean: -28.024746\n",
      "ep 1541: ep_len:502 episode reward: total was -26.210000. running mean: -28.006599\n",
      "ep 1541: ep_len:3 episode reward: total was -0.490000. running mean: -27.731433\n",
      "ep 1541: ep_len:511 episode reward: total was -57.900000. running mean: -28.033119\n",
      "ep 1541: ep_len:513 episode reward: total was -49.930000. running mean: -28.252088\n",
      "epsilon:0.131638 episode_count: 10794. steps_count: 4681537.000000\n",
      "Time elapsed:  13465.985228538513\n",
      "ep 1542: ep_len:606 episode reward: total was -79.820000. running mean: -28.767767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1542: ep_len:500 episode reward: total was 9.910000. running mean: -28.380989\n",
      "ep 1542: ep_len:610 episode reward: total was -67.100000. running mean: -28.768179\n",
      "ep 1542: ep_len:534 episode reward: total was -126.990000. running mean: -29.750397\n",
      "ep 1542: ep_len:3 episode reward: total was -1.500000. running mean: -29.467893\n",
      "ep 1542: ep_len:574 episode reward: total was -27.130000. running mean: -29.444514\n",
      "ep 1542: ep_len:195 episode reward: total was -37.890000. running mean: -29.528969\n",
      "epsilon:0.131594 episode_count: 10801. steps_count: 4684559.000000\n",
      "Time elapsed:  13474.189907550812\n",
      "ep 1543: ep_len:507 episode reward: total was -99.520000. running mean: -30.228880\n",
      "ep 1543: ep_len:500 episode reward: total was -37.170000. running mean: -30.298291\n",
      "ep 1543: ep_len:500 episode reward: total was -21.350000. running mean: -30.208808\n",
      "ep 1543: ep_len:536 episode reward: total was -46.820000. running mean: -30.374920\n",
      "ep 1543: ep_len:67 episode reward: total was -2.310000. running mean: -30.094271\n",
      "ep 1543: ep_len:506 episode reward: total was -76.360000. running mean: -30.556928\n",
      "ep 1543: ep_len:500 episode reward: total was -27.260000. running mean: -30.523959\n",
      "epsilon:0.131549 episode_count: 10808. steps_count: 4687675.000000\n",
      "Time elapsed:  13482.638359546661\n",
      "ep 1544: ep_len:601 episode reward: total was -152.100000. running mean: -31.739719\n",
      "ep 1544: ep_len:523 episode reward: total was -41.660000. running mean: -31.838922\n",
      "ep 1544: ep_len:500 episode reward: total was -117.380000. running mean: -32.694333\n",
      "ep 1544: ep_len:500 episode reward: total was -50.360000. running mean: -32.870989\n",
      "ep 1544: ep_len:48 episode reward: total was 15.000000. running mean: -32.392279\n",
      "ep 1544: ep_len:505 episode reward: total was -47.430000. running mean: -32.542657\n",
      "ep 1544: ep_len:632 episode reward: total was -16.980000. running mean: -32.387030\n",
      "epsilon:0.131505 episode_count: 10815. steps_count: 4690984.000000\n",
      "Time elapsed:  13491.392975091934\n",
      "ep 1545: ep_len:631 episode reward: total was -87.610000. running mean: -32.939260\n",
      "ep 1545: ep_len:584 episode reward: total was 40.330000. running mean: -32.206567\n",
      "ep 1545: ep_len:506 episode reward: total was -81.660000. running mean: -32.701101\n",
      "ep 1545: ep_len:503 episode reward: total was -76.770000. running mean: -33.141790\n",
      "ep 1545: ep_len:131 episode reward: total was -5.180000. running mean: -32.862173\n",
      "ep 1545: ep_len:248 episode reward: total was 22.980000. running mean: -32.303751\n",
      "ep 1545: ep_len:548 episode reward: total was -42.880000. running mean: -32.409513\n",
      "epsilon:0.131461 episode_count: 10822. steps_count: 4694135.000000\n",
      "Time elapsed:  13499.651944637299\n",
      "ep 1546: ep_len:500 episode reward: total was 16.360000. running mean: -31.921818\n",
      "ep 1546: ep_len:572 episode reward: total was -23.130000. running mean: -31.833900\n",
      "ep 1546: ep_len:500 episode reward: total was -37.850000. running mean: -31.894061\n",
      "ep 1546: ep_len:500 episode reward: total was -31.730000. running mean: -31.892420\n",
      "ep 1546: ep_len:3 episode reward: total was -1.500000. running mean: -31.588496\n",
      "ep 1546: ep_len:500 episode reward: total was -89.870000. running mean: -32.171311\n",
      "ep 1546: ep_len:297 episode reward: total was -23.140000. running mean: -32.080998\n",
      "epsilon:0.131416 episode_count: 10829. steps_count: 4697007.000000\n",
      "Time elapsed:  13506.725796461105\n",
      "ep 1547: ep_len:603 episode reward: total was -62.980000. running mean: -32.389988\n",
      "ep 1547: ep_len:500 episode reward: total was -53.930000. running mean: -32.605388\n",
      "ep 1547: ep_len:580 episode reward: total was -97.340000. running mean: -33.252734\n",
      "ep 1547: ep_len:500 episode reward: total was 13.220000. running mean: -32.788007\n",
      "ep 1547: ep_len:3 episode reward: total was 0.000000. running mean: -32.460127\n",
      "ep 1547: ep_len:623 episode reward: total was -79.530000. running mean: -32.930826\n",
      "ep 1547: ep_len:204 episode reward: total was -40.220000. running mean: -33.003717\n",
      "epsilon:0.131372 episode_count: 10836. steps_count: 4700020.000000\n",
      "Time elapsed:  13516.682014226913\n",
      "ep 1548: ep_len:582 episode reward: total was -3.510000. running mean: -32.708780\n",
      "ep 1548: ep_len:268 episode reward: total was -29.040000. running mean: -32.672092\n",
      "ep 1548: ep_len:581 episode reward: total was -63.150000. running mean: -32.976872\n",
      "ep 1548: ep_len:556 episode reward: total was -7.720000. running mean: -32.724303\n",
      "ep 1548: ep_len:109 episode reward: total was 15.720000. running mean: -32.239860\n",
      "ep 1548: ep_len:242 episode reward: total was 18.820000. running mean: -31.729261\n",
      "ep 1548: ep_len:565 episode reward: total was -42.190000. running mean: -31.833869\n",
      "epsilon:0.131328 episode_count: 10843. steps_count: 4702923.000000\n",
      "Time elapsed:  13524.541955471039\n",
      "ep 1549: ep_len:500 episode reward: total was 27.960000. running mean: -31.235930\n",
      "ep 1549: ep_len:547 episode reward: total was -13.130000. running mean: -31.054871\n",
      "ep 1549: ep_len:408 episode reward: total was -23.000000. running mean: -30.974322\n",
      "ep 1549: ep_len:56 episode reward: total was -7.170000. running mean: -30.736279\n",
      "ep 1549: ep_len:3 episode reward: total was -1.500000. running mean: -30.443916\n",
      "ep 1549: ep_len:573 episode reward: total was -125.890000. running mean: -31.398377\n",
      "ep 1549: ep_len:631 episode reward: total was -66.180000. running mean: -31.746193\n",
      "epsilon:0.131283 episode_count: 10850. steps_count: 4705641.000000\n",
      "Time elapsed:  13531.313910961151\n",
      "ep 1550: ep_len:643 episode reward: total was -161.990000. running mean: -33.048631\n",
      "ep 1550: ep_len:525 episode reward: total was -46.060000. running mean: -33.178745\n",
      "ep 1550: ep_len:597 episode reward: total was -122.470000. running mean: -34.071657\n",
      "ep 1550: ep_len:525 episode reward: total was -43.800000. running mean: -34.168941\n",
      "ep 1550: ep_len:3 episode reward: total was -1.500000. running mean: -33.842251\n",
      "ep 1550: ep_len:155 episode reward: total was 32.990000. running mean: -33.173929\n",
      "ep 1550: ep_len:559 episode reward: total was -24.750000. running mean: -33.089689\n",
      "epsilon:0.131239 episode_count: 10857. steps_count: 4708648.000000\n",
      "Time elapsed:  13539.458260059357\n",
      "ep 1551: ep_len:645 episode reward: total was -98.060000. running mean: -33.739393\n",
      "ep 1551: ep_len:578 episode reward: total was -65.310000. running mean: -34.055099\n",
      "ep 1551: ep_len:536 episode reward: total was -19.530000. running mean: -33.909848\n",
      "ep 1551: ep_len:530 episode reward: total was -11.230000. running mean: -33.683049\n",
      "ep 1551: ep_len:130 episode reward: total was -50.150000. running mean: -33.847719\n",
      "ep 1551: ep_len:635 episode reward: total was -81.410000. running mean: -34.323342\n",
      "ep 1551: ep_len:531 episode reward: total was -28.410000. running mean: -34.264208\n",
      "epsilon:0.131195 episode_count: 10864. steps_count: 4712233.000000\n",
      "Time elapsed:  13550.930568218231\n",
      "ep 1552: ep_len:246 episode reward: total was -10.820000. running mean: -34.029766\n",
      "ep 1552: ep_len:500 episode reward: total was -69.040000. running mean: -34.379868\n",
      "ep 1552: ep_len:670 episode reward: total was -87.980000. running mean: -34.915870\n",
      "ep 1552: ep_len:525 episode reward: total was -47.830000. running mean: -35.045011\n",
      "ep 1552: ep_len:2 episode reward: total was -0.500000. running mean: -34.699561\n",
      "ep 1552: ep_len:500 episode reward: total was -66.970000. running mean: -35.022265\n",
      "ep 1552: ep_len:587 episode reward: total was -73.960000. running mean: -35.411643\n",
      "epsilon:0.131150 episode_count: 10871. steps_count: 4715263.000000\n",
      "Time elapsed:  13558.886462211609\n",
      "ep 1553: ep_len:243 episode reward: total was -5.800000. running mean: -35.115526\n",
      "ep 1553: ep_len:500 episode reward: total was -103.980000. running mean: -35.804171\n",
      "ep 1553: ep_len:500 episode reward: total was 0.660000. running mean: -35.439529\n",
      "ep 1553: ep_len:516 episode reward: total was -21.550000. running mean: -35.300634\n",
      "ep 1553: ep_len:3 episode reward: total was 1.010000. running mean: -34.937528\n",
      "ep 1553: ep_len:500 episode reward: total was -51.970000. running mean: -35.107852\n",
      "ep 1553: ep_len:500 episode reward: total was -52.820000. running mean: -35.284974\n",
      "epsilon:0.131106 episode_count: 10878. steps_count: 4718025.000000\n",
      "Time elapsed:  13566.471019268036\n",
      "ep 1554: ep_len:581 episode reward: total was -17.930000. running mean: -35.111424\n",
      "ep 1554: ep_len:565 episode reward: total was -151.800000. running mean: -36.278310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1554: ep_len:592 episode reward: total was -85.630000. running mean: -36.771827\n",
      "ep 1554: ep_len:500 episode reward: total was 12.030000. running mean: -36.283808\n",
      "ep 1554: ep_len:3 episode reward: total was 0.000000. running mean: -35.920970\n",
      "ep 1554: ep_len:143 episode reward: total was 16.580000. running mean: -35.395961\n",
      "ep 1554: ep_len:501 episode reward: total was -71.700000. running mean: -35.759001\n",
      "epsilon:0.131062 episode_count: 10885. steps_count: 4720910.000000\n",
      "Time elapsed:  13574.29913854599\n",
      "ep 1555: ep_len:243 episode reward: total was 5.730000. running mean: -35.344111\n",
      "ep 1555: ep_len:500 episode reward: total was 34.130000. running mean: -34.649370\n",
      "ep 1555: ep_len:53 episode reward: total was 3.120000. running mean: -34.271676\n",
      "ep 1555: ep_len:501 episode reward: total was -44.600000. running mean: -34.374959\n",
      "ep 1555: ep_len:3 episode reward: total was 1.010000. running mean: -34.021110\n",
      "ep 1555: ep_len:564 episode reward: total was -3.600000. running mean: -33.716899\n",
      "ep 1555: ep_len:500 episode reward: total was -37.600000. running mean: -33.755730\n",
      "epsilon:0.131017 episode_count: 10892. steps_count: 4723274.000000\n",
      "Time elapsed:  13580.776875019073\n",
      "ep 1556: ep_len:511 episode reward: total was -66.520000. running mean: -34.083372\n",
      "ep 1556: ep_len:543 episode reward: total was -75.420000. running mean: -34.496739\n",
      "ep 1556: ep_len:527 episode reward: total was -62.610000. running mean: -34.777871\n",
      "ep 1556: ep_len:548 episode reward: total was -26.690000. running mean: -34.696993\n",
      "ep 1556: ep_len:103 episode reward: total was 23.770000. running mean: -34.112323\n",
      "ep 1556: ep_len:284 episode reward: total was 0.950000. running mean: -33.761699\n",
      "ep 1556: ep_len:525 episode reward: total was -33.690000. running mean: -33.760983\n",
      "epsilon:0.130973 episode_count: 10899. steps_count: 4726315.000000\n",
      "Time elapsed:  13588.92616391182\n",
      "ep 1557: ep_len:500 episode reward: total was 31.720000. running mean: -33.106173\n",
      "ep 1557: ep_len:500 episode reward: total was 17.090000. running mean: -32.604211\n",
      "ep 1557: ep_len:523 episode reward: total was -58.360000. running mean: -32.861769\n",
      "ep 1557: ep_len:512 episode reward: total was -22.600000. running mean: -32.759151\n",
      "ep 1557: ep_len:50 episode reward: total was 16.000000. running mean: -32.271560\n",
      "ep 1557: ep_len:681 episode reward: total was -160.170000. running mean: -33.550544\n",
      "ep 1557: ep_len:515 episode reward: total was -41.580000. running mean: -33.630839\n",
      "epsilon:0.130929 episode_count: 10906. steps_count: 4729596.000000\n",
      "Time elapsed:  13597.645288944244\n",
      "ep 1558: ep_len:532 episode reward: total was -3.290000. running mean: -33.327430\n",
      "ep 1558: ep_len:568 episode reward: total was -50.450000. running mean: -33.498656\n",
      "ep 1558: ep_len:584 episode reward: total was -56.500000. running mean: -33.728669\n",
      "ep 1558: ep_len:500 episode reward: total was 20.540000. running mean: -33.185983\n",
      "ep 1558: ep_len:3 episode reward: total was 0.000000. running mean: -32.854123\n",
      "ep 1558: ep_len:501 episode reward: total was -21.890000. running mean: -32.744482\n",
      "ep 1558: ep_len:599 episode reward: total was -46.890000. running mean: -32.885937\n",
      "epsilon:0.130884 episode_count: 10913. steps_count: 4732883.000000\n",
      "Time elapsed:  13606.354375123978\n",
      "ep 1559: ep_len:211 episode reward: total was 14.620000. running mean: -32.410877\n",
      "ep 1559: ep_len:500 episode reward: total was -28.430000. running mean: -32.371069\n",
      "ep 1559: ep_len:500 episode reward: total was -14.280000. running mean: -32.190158\n",
      "ep 1559: ep_len:502 episode reward: total was -8.650000. running mean: -31.954756\n",
      "ep 1559: ep_len:56 episode reward: total was -13.990000. running mean: -31.775109\n",
      "ep 1559: ep_len:509 episode reward: total was -47.410000. running mean: -31.931458\n",
      "ep 1559: ep_len:579 episode reward: total was -92.400000. running mean: -32.536143\n",
      "epsilon:0.130840 episode_count: 10920. steps_count: 4735740.000000\n",
      "Time elapsed:  13612.068723201752\n",
      "ep 1560: ep_len:561 episode reward: total was -77.990000. running mean: -32.990682\n",
      "ep 1560: ep_len:365 episode reward: total was -74.190000. running mean: -33.402675\n",
      "ep 1560: ep_len:517 episode reward: total was -60.710000. running mean: -33.675748\n",
      "ep 1560: ep_len:619 episode reward: total was -9.070000. running mean: -33.429691\n",
      "ep 1560: ep_len:3 episode reward: total was 1.010000. running mean: -33.085294\n",
      "ep 1560: ep_len:607 episode reward: total was -31.980000. running mean: -33.074241\n",
      "ep 1560: ep_len:566 episode reward: total was -63.710000. running mean: -33.380598\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.130796 episode_count: 10927. steps_count: 4738978.000000\n",
      "Time elapsed:  13623.657249212265\n",
      "ep 1561: ep_len:571 episode reward: total was 1.580000. running mean: -33.030992\n",
      "ep 1561: ep_len:500 episode reward: total was -96.590000. running mean: -33.666583\n",
      "ep 1561: ep_len:515 episode reward: total was -68.450000. running mean: -34.014417\n",
      "ep 1561: ep_len:518 episode reward: total was -33.520000. running mean: -34.009473\n",
      "ep 1561: ep_len:48 episode reward: total was 13.500000. running mean: -33.534378\n",
      "ep 1561: ep_len:325 episode reward: total was -14.580000. running mean: -33.344834\n",
      "ep 1561: ep_len:546 episode reward: total was -25.990000. running mean: -33.271286\n",
      "epsilon:0.130751 episode_count: 10934. steps_count: 4742001.000000\n",
      "Time elapsed:  13631.801073789597\n",
      "ep 1562: ep_len:542 episode reward: total was -22.780000. running mean: -33.166373\n",
      "ep 1562: ep_len:613 episode reward: total was -79.080000. running mean: -33.625509\n",
      "ep 1562: ep_len:565 episode reward: total was -71.980000. running mean: -34.009054\n",
      "ep 1562: ep_len:500 episode reward: total was -36.470000. running mean: -34.033663\n",
      "ep 1562: ep_len:109 episode reward: total was 23.740000. running mean: -33.455927\n",
      "ep 1562: ep_len:570 episode reward: total was -4.770000. running mean: -33.169068\n",
      "ep 1562: ep_len:584 episode reward: total was -57.860000. running mean: -33.415977\n",
      "epsilon:0.130707 episode_count: 10941. steps_count: 4745484.000000\n",
      "Time elapsed:  13640.977170705795\n",
      "ep 1563: ep_len:508 episode reward: total was -14.330000. running mean: -33.225117\n",
      "ep 1563: ep_len:501 episode reward: total was -74.420000. running mean: -33.637066\n",
      "ep 1563: ep_len:572 episode reward: total was -46.350000. running mean: -33.764195\n",
      "ep 1563: ep_len:360 episode reward: total was -27.320000. running mean: -33.699753\n",
      "ep 1563: ep_len:86 episode reward: total was -18.740000. running mean: -33.550156\n",
      "ep 1563: ep_len:515 episode reward: total was -61.090000. running mean: -33.825554\n",
      "ep 1563: ep_len:503 episode reward: total was -36.140000. running mean: -33.848699\n",
      "epsilon:0.130663 episode_count: 10948. steps_count: 4748529.000000\n",
      "Time elapsed:  13649.104565858841\n",
      "ep 1564: ep_len:232 episode reward: total was -16.290000. running mean: -33.673112\n",
      "ep 1564: ep_len:523 episode reward: total was -24.200000. running mean: -33.578381\n",
      "ep 1564: ep_len:559 episode reward: total was -74.420000. running mean: -33.986797\n",
      "ep 1564: ep_len:513 episode reward: total was -45.600000. running mean: -34.102929\n",
      "ep 1564: ep_len:121 episode reward: total was 21.750000. running mean: -33.544400\n",
      "ep 1564: ep_len:589 episode reward: total was -7.400000. running mean: -33.282956\n",
      "ep 1564: ep_len:313 episode reward: total was -52.320000. running mean: -33.473326\n",
      "epsilon:0.130618 episode_count: 10955. steps_count: 4751379.000000\n",
      "Time elapsed:  13656.900133371353\n",
      "ep 1565: ep_len:587 episode reward: total was -21.010000. running mean: -33.348693\n",
      "ep 1565: ep_len:500 episode reward: total was -42.100000. running mean: -33.436206\n",
      "ep 1565: ep_len:537 episode reward: total was -52.880000. running mean: -33.630644\n",
      "ep 1565: ep_len:513 episode reward: total was -25.220000. running mean: -33.546537\n",
      "ep 1565: ep_len:3 episode reward: total was 1.010000. running mean: -33.200972\n",
      "ep 1565: ep_len:500 episode reward: total was -50.320000. running mean: -33.372162\n",
      "ep 1565: ep_len:519 episode reward: total was -54.840000. running mean: -33.586841\n",
      "epsilon:0.130574 episode_count: 10962. steps_count: 4754538.000000\n",
      "Time elapsed:  13666.90361070633\n",
      "ep 1566: ep_len:547 episode reward: total was 11.340000. running mean: -33.137572\n",
      "ep 1566: ep_len:550 episode reward: total was -69.700000. running mean: -33.503196\n",
      "ep 1566: ep_len:617 episode reward: total was -45.690000. running mean: -33.625064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1566: ep_len:536 episode reward: total was 2.450000. running mean: -33.264314\n",
      "ep 1566: ep_len:105 episode reward: total was -3.760000. running mean: -32.969271\n",
      "ep 1566: ep_len:546 episode reward: total was -4.390000. running mean: -32.683478\n",
      "ep 1566: ep_len:592 episode reward: total was -22.000000. running mean: -32.576643\n",
      "epsilon:0.130530 episode_count: 10969. steps_count: 4758031.000000\n",
      "Time elapsed:  13676.155436754227\n",
      "ep 1567: ep_len:590 episode reward: total was -36.860000. running mean: -32.619477\n",
      "ep 1567: ep_len:570 episode reward: total was -36.610000. running mean: -32.659382\n",
      "ep 1567: ep_len:513 episode reward: total was -53.370000. running mean: -32.866488\n",
      "ep 1567: ep_len:407 episode reward: total was -35.630000. running mean: -32.894123\n",
      "ep 1567: ep_len:3 episode reward: total was 1.010000. running mean: -32.555082\n",
      "ep 1567: ep_len:516 episode reward: total was -53.270000. running mean: -32.762231\n",
      "ep 1567: ep_len:619 episode reward: total was -14.400000. running mean: -32.578609\n",
      "epsilon:0.130485 episode_count: 10976. steps_count: 4761249.000000\n",
      "Time elapsed:  13684.647816181183\n",
      "ep 1568: ep_len:500 episode reward: total was -105.130000. running mean: -33.304123\n",
      "ep 1568: ep_len:524 episode reward: total was -47.360000. running mean: -33.444682\n",
      "ep 1568: ep_len:544 episode reward: total was -53.150000. running mean: -33.641735\n",
      "ep 1568: ep_len:542 episode reward: total was -19.170000. running mean: -33.497017\n",
      "ep 1568: ep_len:3 episode reward: total was 0.000000. running mean: -33.162047\n",
      "ep 1568: ep_len:501 episode reward: total was -23.510000. running mean: -33.065527\n",
      "ep 1568: ep_len:524 episode reward: total was -50.780000. running mean: -33.242672\n",
      "epsilon:0.130441 episode_count: 10983. steps_count: 4764387.000000\n",
      "Time elapsed:  13694.710485935211\n",
      "ep 1569: ep_len:500 episode reward: total was 3.720000. running mean: -32.873045\n",
      "ep 1569: ep_len:604 episode reward: total was -35.010000. running mean: -32.894414\n",
      "ep 1569: ep_len:544 episode reward: total was -69.340000. running mean: -33.258870\n",
      "ep 1569: ep_len:500 episode reward: total was -25.680000. running mean: -33.183082\n",
      "ep 1569: ep_len:118 episode reward: total was -44.650000. running mean: -33.297751\n",
      "ep 1569: ep_len:500 episode reward: total was -80.050000. running mean: -33.765273\n",
      "ep 1569: ep_len:631 episode reward: total was -41.640000. running mean: -33.844020\n",
      "epsilon:0.130397 episode_count: 10990. steps_count: 4767784.000000\n",
      "Time elapsed:  13703.67568230629\n",
      "ep 1570: ep_len:542 episode reward: total was -0.980000. running mean: -33.515380\n",
      "ep 1570: ep_len:500 episode reward: total was 6.010000. running mean: -33.120126\n",
      "ep 1570: ep_len:592 episode reward: total was -42.990000. running mean: -33.218825\n",
      "ep 1570: ep_len:500 episode reward: total was -29.290000. running mean: -33.179537\n",
      "ep 1570: ep_len:85 episode reward: total was -61.280000. running mean: -33.460542\n",
      "ep 1570: ep_len:797 episode reward: total was -244.850000. running mean: -35.574436\n",
      "ep 1570: ep_len:541 episode reward: total was -57.370000. running mean: -35.792392\n",
      "epsilon:0.130352 episode_count: 10997. steps_count: 4771341.000000\n",
      "Time elapsed:  13714.470259666443\n",
      "ep 1571: ep_len:500 episode reward: total was 19.970000. running mean: -35.234768\n",
      "ep 1571: ep_len:592 episode reward: total was 64.820000. running mean: -34.234220\n",
      "ep 1571: ep_len:598 episode reward: total was -115.470000. running mean: -35.046578\n",
      "ep 1571: ep_len:95 episode reward: total was 8.820000. running mean: -34.607912\n",
      "ep 1571: ep_len:3 episode reward: total was 1.010000. running mean: -34.251733\n",
      "ep 1571: ep_len:501 episode reward: total was -18.010000. running mean: -34.089316\n",
      "ep 1571: ep_len:621 episode reward: total was -31.090000. running mean: -34.059323\n",
      "epsilon:0.130308 episode_count: 11004. steps_count: 4774251.000000\n",
      "Time elapsed:  13722.244530677795\n",
      "ep 1572: ep_len:500 episode reward: total was 26.090000. running mean: -33.457829\n",
      "ep 1572: ep_len:558 episode reward: total was -25.990000. running mean: -33.383151\n",
      "ep 1572: ep_len:518 episode reward: total was -20.340000. running mean: -33.252720\n",
      "ep 1572: ep_len:500 episode reward: total was -41.660000. running mean: -33.336792\n",
      "ep 1572: ep_len:3 episode reward: total was 1.010000. running mean: -32.993324\n",
      "ep 1572: ep_len:529 episode reward: total was -220.760000. running mean: -34.870991\n",
      "ep 1572: ep_len:603 episode reward: total was -50.290000. running mean: -35.025181\n",
      "epsilon:0.130264 episode_count: 11011. steps_count: 4777462.000000\n",
      "Time elapsed:  13730.570903301239\n",
      "ep 1573: ep_len:608 episode reward: total was 7.010000. running mean: -34.604830\n",
      "ep 1573: ep_len:593 episode reward: total was -56.090000. running mean: -34.819681\n",
      "ep 1573: ep_len:556 episode reward: total was -31.760000. running mean: -34.789084\n",
      "ep 1573: ep_len:500 episode reward: total was 0.960000. running mean: -34.431594\n",
      "ep 1573: ep_len:3 episode reward: total was 0.000000. running mean: -34.087278\n",
      "ep 1573: ep_len:514 episode reward: total was -60.180000. running mean: -34.348205\n",
      "ep 1573: ep_len:513 episode reward: total was -86.270000. running mean: -34.867423\n",
      "epsilon:0.130219 episode_count: 11018. steps_count: 4780749.000000\n",
      "Time elapsed:  13740.980100870132\n",
      "ep 1574: ep_len:500 episode reward: total was 14.370000. running mean: -34.375049\n",
      "ep 1574: ep_len:500 episode reward: total was -11.670000. running mean: -34.147998\n",
      "ep 1574: ep_len:561 episode reward: total was -76.580000. running mean: -34.572318\n",
      "ep 1574: ep_len:510 episode reward: total was 26.910000. running mean: -33.957495\n",
      "ep 1574: ep_len:3 episode reward: total was 1.010000. running mean: -33.607820\n",
      "ep 1574: ep_len:500 episode reward: total was -39.140000. running mean: -33.663142\n",
      "ep 1574: ep_len:500 episode reward: total was -28.180000. running mean: -33.608310\n",
      "epsilon:0.130175 episode_count: 11025. steps_count: 4783823.000000\n",
      "Time elapsed:  13751.09741449356\n",
      "ep 1575: ep_len:134 episode reward: total was -0.500000. running mean: -33.277227\n",
      "ep 1575: ep_len:598 episode reward: total was -24.300000. running mean: -33.187455\n",
      "ep 1575: ep_len:517 episode reward: total was -58.850000. running mean: -33.444080\n",
      "ep 1575: ep_len:622 episode reward: total was 23.040000. running mean: -32.879240\n",
      "ep 1575: ep_len:114 episode reward: total was 25.290000. running mean: -32.297547\n",
      "ep 1575: ep_len:501 episode reward: total was -33.950000. running mean: -32.314072\n",
      "ep 1575: ep_len:577 episode reward: total was -50.090000. running mean: -32.491831\n",
      "epsilon:0.130131 episode_count: 11032. steps_count: 4786886.000000\n",
      "Time elapsed:  13759.266839027405\n",
      "ep 1576: ep_len:547 episode reward: total was -88.630000. running mean: -33.053213\n",
      "ep 1576: ep_len:500 episode reward: total was 8.480000. running mean: -32.637881\n",
      "ep 1576: ep_len:630 episode reward: total was -37.280000. running mean: -32.684302\n",
      "ep 1576: ep_len:560 episode reward: total was -17.080000. running mean: -32.528259\n",
      "ep 1576: ep_len:3 episode reward: total was 0.000000. running mean: -32.202976\n",
      "ep 1576: ep_len:519 episode reward: total was -19.290000. running mean: -32.073846\n",
      "ep 1576: ep_len:194 episode reward: total was -21.220000. running mean: -31.965308\n",
      "epsilon:0.130086 episode_count: 11039. steps_count: 4789839.000000\n",
      "Time elapsed:  13767.274661540985\n",
      "ep 1577: ep_len:568 episode reward: total was -77.250000. running mean: -32.418155\n",
      "ep 1577: ep_len:505 episode reward: total was -53.350000. running mean: -32.627473\n",
      "ep 1577: ep_len:79 episode reward: total was 1.760000. running mean: -32.283599\n",
      "ep 1577: ep_len:500 episode reward: total was 9.030000. running mean: -31.870463\n",
      "ep 1577: ep_len:3 episode reward: total was 1.010000. running mean: -31.541658\n",
      "ep 1577: ep_len:568 episode reward: total was -49.080000. running mean: -31.717041\n",
      "ep 1577: ep_len:332 episode reward: total was -50.170000. running mean: -31.901571\n",
      "epsilon:0.130042 episode_count: 11046. steps_count: 4792394.000000\n",
      "Time elapsed:  13774.357625484467\n",
      "ep 1578: ep_len:501 episode reward: total was 22.890000. running mean: -31.353655\n",
      "ep 1578: ep_len:500 episode reward: total was -91.800000. running mean: -31.958119\n",
      "ep 1578: ep_len:413 episode reward: total was -172.890000. running mean: -33.367438\n",
      "ep 1578: ep_len:540 episode reward: total was -63.390000. running mean: -33.667663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1578: ep_len:52 episode reward: total was 15.500000. running mean: -33.175987\n",
      "ep 1578: ep_len:245 episode reward: total was 6.420000. running mean: -32.780027\n",
      "ep 1578: ep_len:573 episode reward: total was -22.000000. running mean: -32.672226\n",
      "epsilon:0.129998 episode_count: 11053. steps_count: 4795218.000000\n",
      "Time elapsed:  13781.841601610184\n",
      "ep 1579: ep_len:514 episode reward: total was -68.270000. running mean: -33.028204\n",
      "ep 1579: ep_len:500 episode reward: total was 7.130000. running mean: -32.626622\n",
      "ep 1579: ep_len:366 episode reward: total was 8.100000. running mean: -32.219356\n",
      "ep 1579: ep_len:500 episode reward: total was -17.660000. running mean: -32.073762\n",
      "ep 1579: ep_len:54 episode reward: total was 19.010000. running mean: -31.562925\n",
      "ep 1579: ep_len:629 episode reward: total was -69.120000. running mean: -31.938495\n",
      "ep 1579: ep_len:338 episode reward: total was -22.280000. running mean: -31.841910\n",
      "epsilon:0.129953 episode_count: 11060. steps_count: 4798119.000000\n",
      "Time elapsed:  13789.72820019722\n",
      "ep 1580: ep_len:500 episode reward: total was 36.280000. running mean: -31.160691\n",
      "ep 1580: ep_len:600 episode reward: total was -66.620000. running mean: -31.515284\n",
      "ep 1580: ep_len:681 episode reward: total was -82.900000. running mean: -32.029132\n",
      "ep 1580: ep_len:408 episode reward: total was -20.020000. running mean: -31.909040\n",
      "ep 1580: ep_len:80 episode reward: total was -33.230000. running mean: -31.922250\n",
      "ep 1580: ep_len:518 episode reward: total was -53.910000. running mean: -32.142127\n",
      "ep 1580: ep_len:500 episode reward: total was -55.270000. running mean: -32.373406\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.129909 episode_count: 11067. steps_count: 4801406.000000\n",
      "Time elapsed:  13804.461580753326\n",
      "ep 1581: ep_len:500 episode reward: total was 8.490000. running mean: -31.964772\n",
      "ep 1581: ep_len:365 episode reward: total was -38.290000. running mean: -32.028024\n",
      "ep 1581: ep_len:64 episode reward: total was -2.740000. running mean: -31.735144\n",
      "ep 1581: ep_len:575 episode reward: total was 6.890000. running mean: -31.348893\n",
      "ep 1581: ep_len:3 episode reward: total was -0.490000. running mean: -31.040304\n",
      "ep 1581: ep_len:672 episode reward: total was -76.380000. running mean: -31.493701\n",
      "ep 1581: ep_len:540 episode reward: total was -41.880000. running mean: -31.597564\n",
      "epsilon:0.129865 episode_count: 11074. steps_count: 4804125.000000\n",
      "Time elapsed:  13811.74737238884\n",
      "ep 1582: ep_len:539 episode reward: total was 18.900000. running mean: -31.092588\n",
      "ep 1582: ep_len:553 episode reward: total was -58.280000. running mean: -31.364462\n",
      "ep 1582: ep_len:500 episode reward: total was -12.900000. running mean: -31.179818\n",
      "ep 1582: ep_len:500 episode reward: total was -27.610000. running mean: -31.144119\n",
      "ep 1582: ep_len:3 episode reward: total was -1.500000. running mean: -30.847678\n",
      "ep 1582: ep_len:585 episode reward: total was -147.480000. running mean: -32.014001\n",
      "ep 1582: ep_len:500 episode reward: total was -171.260000. running mean: -33.406461\n",
      "epsilon:0.129820 episode_count: 11081. steps_count: 4807305.000000\n",
      "Time elapsed:  13819.471968650818\n",
      "ep 1583: ep_len:647 episode reward: total was -74.220000. running mean: -33.814597\n",
      "ep 1583: ep_len:500 episode reward: total was -46.890000. running mean: -33.945351\n",
      "ep 1583: ep_len:79 episode reward: total was 2.310000. running mean: -33.582797\n",
      "ep 1583: ep_len:500 episode reward: total was -5.090000. running mean: -33.297869\n",
      "ep 1583: ep_len:113 episode reward: total was 7.740000. running mean: -32.887491\n",
      "ep 1583: ep_len:544 episode reward: total was -56.620000. running mean: -33.124816\n",
      "ep 1583: ep_len:195 episode reward: total was -46.220000. running mean: -33.255768\n",
      "epsilon:0.129776 episode_count: 11088. steps_count: 4809883.000000\n",
      "Time elapsed:  13826.52163720131\n",
      "ep 1584: ep_len:500 episode reward: total was -100.920000. running mean: -33.932410\n",
      "ep 1584: ep_len:634 episode reward: total was -23.800000. running mean: -33.831086\n",
      "ep 1584: ep_len:504 episode reward: total was 4.160000. running mean: -33.451175\n",
      "ep 1584: ep_len:509 episode reward: total was -1.380000. running mean: -33.130463\n",
      "ep 1584: ep_len:89 episode reward: total was -45.800000. running mean: -33.257159\n",
      "ep 1584: ep_len:500 episode reward: total was -19.840000. running mean: -33.122987\n",
      "ep 1584: ep_len:602 episode reward: total was -40.420000. running mean: -33.195957\n",
      "epsilon:0.129732 episode_count: 11095. steps_count: 4813221.000000\n",
      "Time elapsed:  13835.277137517929\n",
      "ep 1585: ep_len:500 episode reward: total was 4.670000. running mean: -32.817298\n",
      "ep 1585: ep_len:500 episode reward: total was 43.150000. running mean: -32.057625\n",
      "ep 1585: ep_len:561 episode reward: total was -87.400000. running mean: -32.611048\n",
      "ep 1585: ep_len:508 episode reward: total was -33.130000. running mean: -32.616238\n",
      "ep 1585: ep_len:130 episode reward: total was 13.330000. running mean: -32.156775\n",
      "ep 1585: ep_len:652 episode reward: total was -49.690000. running mean: -32.332108\n",
      "ep 1585: ep_len:501 episode reward: total was -60.660000. running mean: -32.615387\n",
      "epsilon:0.129687 episode_count: 11102. steps_count: 4816573.000000\n",
      "Time elapsed:  13844.060342788696\n",
      "ep 1586: ep_len:526 episode reward: total was -55.370000. running mean: -32.842933\n",
      "ep 1586: ep_len:574 episode reward: total was 26.200000. running mean: -32.252503\n",
      "ep 1586: ep_len:396 episode reward: total was -60.590000. running mean: -32.535878\n",
      "ep 1586: ep_len:597 episode reward: total was -17.890000. running mean: -32.389420\n",
      "ep 1586: ep_len:47 episode reward: total was 20.500000. running mean: -31.860525\n",
      "ep 1586: ep_len:653 episode reward: total was -51.050000. running mean: -32.052420\n",
      "ep 1586: ep_len:519 episode reward: total was -85.660000. running mean: -32.588496\n",
      "epsilon:0.129643 episode_count: 11109. steps_count: 4819885.000000\n",
      "Time elapsed:  13852.718853473663\n",
      "ep 1587: ep_len:621 episode reward: total was -96.610000. running mean: -33.228711\n",
      "ep 1587: ep_len:572 episode reward: total was -29.010000. running mean: -33.186524\n",
      "ep 1587: ep_len:384 episode reward: total was 0.920000. running mean: -32.845459\n",
      "ep 1587: ep_len:502 episode reward: total was -41.060000. running mean: -32.927604\n",
      "ep 1587: ep_len:3 episode reward: total was 1.010000. running mean: -32.588228\n",
      "ep 1587: ep_len:550 episode reward: total was -52.600000. running mean: -32.788346\n",
      "ep 1587: ep_len:601 episode reward: total was -71.350000. running mean: -33.173962\n",
      "epsilon:0.129599 episode_count: 11116. steps_count: 4823118.000000\n",
      "Time elapsed:  13861.209518909454\n",
      "ep 1588: ep_len:226 episode reward: total was -27.960000. running mean: -33.121823\n",
      "ep 1588: ep_len:546 episode reward: total was 30.950000. running mean: -32.481104\n",
      "ep 1588: ep_len:500 episode reward: total was -66.540000. running mean: -32.821693\n",
      "ep 1588: ep_len:546 episode reward: total was -14.800000. running mean: -32.641476\n",
      "ep 1588: ep_len:56 episode reward: total was -30.000000. running mean: -32.615062\n",
      "ep 1588: ep_len:553 episode reward: total was -36.350000. running mean: -32.652411\n",
      "ep 1588: ep_len:614 episode reward: total was -30.290000. running mean: -32.628787\n",
      "epsilon:0.129554 episode_count: 11123. steps_count: 4826159.000000\n",
      "Time elapsed:  13871.410670042038\n",
      "ep 1589: ep_len:500 episode reward: total was 8.370000. running mean: -32.218799\n",
      "ep 1589: ep_len:502 episode reward: total was 9.680000. running mean: -31.799811\n",
      "ep 1589: ep_len:574 episode reward: total was -29.650000. running mean: -31.778313\n",
      "ep 1589: ep_len:500 episode reward: total was -49.190000. running mean: -31.952430\n",
      "ep 1589: ep_len:3 episode reward: total was 0.000000. running mean: -31.632906\n",
      "ep 1589: ep_len:643 episode reward: total was -50.390000. running mean: -31.820477\n",
      "ep 1589: ep_len:560 episode reward: total was -0.590000. running mean: -31.508172\n",
      "epsilon:0.129510 episode_count: 11130. steps_count: 4829441.000000\n",
      "Time elapsed:  13881.64392876625\n",
      "ep 1590: ep_len:191 episode reward: total was 6.730000. running mean: -31.125790\n",
      "ep 1590: ep_len:500 episode reward: total was 39.500000. running mean: -30.419532\n",
      "ep 1590: ep_len:361 episode reward: total was 18.760000. running mean: -29.927737\n",
      "ep 1590: ep_len:409 episode reward: total was -33.560000. running mean: -29.964059\n",
      "ep 1590: ep_len:3 episode reward: total was 0.000000. running mean: -29.664419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1590: ep_len:244 episode reward: total was 21.720000. running mean: -29.150575\n",
      "ep 1590: ep_len:173 episode reward: total was -14.520000. running mean: -29.004269\n",
      "epsilon:0.129466 episode_count: 11137. steps_count: 4831322.000000\n",
      "Time elapsed:  13887.118184566498\n",
      "ep 1591: ep_len:610 episode reward: total was -76.650000. running mean: -29.480726\n",
      "ep 1591: ep_len:542 episode reward: total was -40.120000. running mean: -29.587119\n",
      "ep 1591: ep_len:698 episode reward: total was -137.370000. running mean: -30.664948\n",
      "ep 1591: ep_len:500 episode reward: total was -19.220000. running mean: -30.550498\n",
      "ep 1591: ep_len:3 episode reward: total was 0.000000. running mean: -30.244993\n",
      "ep 1591: ep_len:608 episode reward: total was -62.610000. running mean: -30.568643\n",
      "ep 1591: ep_len:532 episode reward: total was -78.120000. running mean: -31.044157\n",
      "epsilon:0.129421 episode_count: 11144. steps_count: 4834815.000000\n",
      "Time elapsed:  13898.37628531456\n",
      "ep 1592: ep_len:568 episode reward: total was 1.060000. running mean: -30.723115\n",
      "ep 1592: ep_len:636 episode reward: total was -148.340000. running mean: -31.899284\n",
      "ep 1592: ep_len:549 episode reward: total was -67.430000. running mean: -32.254591\n",
      "ep 1592: ep_len:598 episode reward: total was -16.840000. running mean: -32.100445\n",
      "ep 1592: ep_len:133 episode reward: total was 16.330000. running mean: -31.616141\n",
      "ep 1592: ep_len:623 episode reward: total was -49.350000. running mean: -31.793480\n",
      "ep 1592: ep_len:517 episode reward: total was -48.170000. running mean: -31.957245\n",
      "epsilon:0.129377 episode_count: 11151. steps_count: 4838439.000000\n",
      "Time elapsed:  13907.812221288681\n",
      "ep 1593: ep_len:579 episode reward: total was -8.420000. running mean: -31.721872\n",
      "ep 1593: ep_len:595 episode reward: total was -29.190000. running mean: -31.696554\n",
      "ep 1593: ep_len:503 episode reward: total was -27.380000. running mean: -31.653388\n",
      "ep 1593: ep_len:366 episode reward: total was -37.210000. running mean: -31.708954\n",
      "ep 1593: ep_len:3 episode reward: total was 0.000000. running mean: -31.391865\n",
      "ep 1593: ep_len:500 episode reward: total was -22.030000. running mean: -31.298246\n",
      "ep 1593: ep_len:574 episode reward: total was -58.170000. running mean: -31.566964\n",
      "epsilon:0.129333 episode_count: 11158. steps_count: 4841559.000000\n",
      "Time elapsed:  13915.748048305511\n",
      "ep 1594: ep_len:691 episode reward: total was -68.430000. running mean: -31.935594\n",
      "ep 1594: ep_len:500 episode reward: total was -111.060000. running mean: -32.726838\n",
      "ep 1594: ep_len:641 episode reward: total was -55.430000. running mean: -32.953870\n",
      "ep 1594: ep_len:401 episode reward: total was -9.680000. running mean: -32.721131\n",
      "ep 1594: ep_len:3 episode reward: total was 0.000000. running mean: -32.393920\n",
      "ep 1594: ep_len:583 episode reward: total was -65.200000. running mean: -32.721980\n",
      "ep 1594: ep_len:553 episode reward: total was -52.700000. running mean: -32.921761\n",
      "epsilon:0.129288 episode_count: 11165. steps_count: 4844931.000000\n",
      "Time elapsed:  13924.783514976501\n",
      "ep 1595: ep_len:500 episode reward: total was -26.930000. running mean: -32.861843\n",
      "ep 1595: ep_len:513 episode reward: total was -49.720000. running mean: -33.030425\n",
      "ep 1595: ep_len:569 episode reward: total was -125.570000. running mean: -33.955820\n",
      "ep 1595: ep_len:501 episode reward: total was -42.350000. running mean: -34.039762\n",
      "ep 1595: ep_len:3 episode reward: total was 0.000000. running mean: -33.699364\n",
      "ep 1595: ep_len:522 episode reward: total was -75.660000. running mean: -34.118971\n",
      "ep 1595: ep_len:331 episode reward: total was -96.160000. running mean: -34.739381\n",
      "epsilon:0.129244 episode_count: 11172. steps_count: 4847870.000000\n",
      "Time elapsed:  13932.604923963547\n",
      "ep 1596: ep_len:204 episode reward: total was 5.580000. running mean: -34.336187\n",
      "ep 1596: ep_len:178 episode reward: total was -26.280000. running mean: -34.255625\n",
      "ep 1596: ep_len:599 episode reward: total was -57.730000. running mean: -34.490369\n",
      "ep 1596: ep_len:586 episode reward: total was -0.980000. running mean: -34.155266\n",
      "ep 1596: ep_len:3 episode reward: total was 1.010000. running mean: -33.803613\n",
      "ep 1596: ep_len:500 episode reward: total was -48.630000. running mean: -33.951877\n",
      "ep 1596: ep_len:572 episode reward: total was -67.500000. running mean: -34.287358\n",
      "epsilon:0.129200 episode_count: 11179. steps_count: 4850512.000000\n",
      "Time elapsed:  13939.678089380264\n",
      "ep 1597: ep_len:500 episode reward: total was -56.790000. running mean: -34.512384\n",
      "ep 1597: ep_len:546 episode reward: total was -32.170000. running mean: -34.488961\n",
      "ep 1597: ep_len:523 episode reward: total was -53.880000. running mean: -34.682871\n",
      "ep 1597: ep_len:580 episode reward: total was 11.500000. running mean: -34.221042\n",
      "ep 1597: ep_len:3 episode reward: total was 0.000000. running mean: -33.878832\n",
      "ep 1597: ep_len:500 episode reward: total was -10.790000. running mean: -33.647943\n",
      "ep 1597: ep_len:593 episode reward: total was -31.270000. running mean: -33.624164\n",
      "epsilon:0.129155 episode_count: 11186. steps_count: 4853757.000000\n",
      "Time elapsed:  13948.426883459091\n",
      "ep 1598: ep_len:194 episode reward: total was -5.970000. running mean: -33.347622\n",
      "ep 1598: ep_len:500 episode reward: total was -30.800000. running mean: -33.322146\n",
      "ep 1598: ep_len:447 episode reward: total was -16.790000. running mean: -33.156825\n",
      "ep 1598: ep_len:500 episode reward: total was -99.210000. running mean: -33.817356\n",
      "ep 1598: ep_len:3 episode reward: total was 0.000000. running mean: -33.479183\n",
      "ep 1598: ep_len:500 episode reward: total was -86.300000. running mean: -34.007391\n",
      "ep 1598: ep_len:179 episode reward: total was -30.190000. running mean: -33.969217\n",
      "epsilon:0.129111 episode_count: 11193. steps_count: 4856080.000000\n",
      "Time elapsed:  13954.78019785881\n",
      "ep 1599: ep_len:500 episode reward: total was 14.070000. running mean: -33.488825\n",
      "ep 1599: ep_len:511 episode reward: total was -132.660000. running mean: -34.480537\n",
      "ep 1599: ep_len:501 episode reward: total was -43.270000. running mean: -34.568431\n",
      "ep 1599: ep_len:513 episode reward: total was -74.310000. running mean: -34.965847\n",
      "ep 1599: ep_len:94 episode reward: total was 26.220000. running mean: -34.353989\n",
      "ep 1599: ep_len:553 episode reward: total was -52.350000. running mean: -34.533949\n",
      "ep 1599: ep_len:542 episode reward: total was -79.230000. running mean: -34.980909\n",
      "epsilon:0.129067 episode_count: 11200. steps_count: 4859294.000000\n",
      "Time elapsed:  13963.268260240555\n",
      "ep 1600: ep_len:574 episode reward: total was 3.840000. running mean: -34.592700\n",
      "ep 1600: ep_len:603 episode reward: total was -39.620000. running mean: -34.642973\n",
      "ep 1600: ep_len:573 episode reward: total was -65.470000. running mean: -34.951243\n",
      "ep 1600: ep_len:507 episode reward: total was -24.480000. running mean: -34.846531\n",
      "ep 1600: ep_len:3 episode reward: total was -1.500000. running mean: -34.513066\n",
      "ep 1600: ep_len:505 episode reward: total was -36.580000. running mean: -34.533735\n",
      "ep 1600: ep_len:344 episode reward: total was -0.980000. running mean: -34.198198\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.129022 episode_count: 11207. steps_count: 4862403.000000\n",
      "Time elapsed:  13976.352763175964\n",
      "ep 1601: ep_len:585 episode reward: total was 19.530000. running mean: -33.660916\n",
      "ep 1601: ep_len:500 episode reward: total was 19.500000. running mean: -33.129307\n",
      "ep 1601: ep_len:544 episode reward: total was -92.450000. running mean: -33.722513\n",
      "ep 1601: ep_len:501 episode reward: total was -37.980000. running mean: -33.765088\n",
      "ep 1601: ep_len:3 episode reward: total was 0.000000. running mean: -33.427437\n",
      "ep 1601: ep_len:664 episode reward: total was -158.110000. running mean: -34.674263\n",
      "ep 1601: ep_len:563 episode reward: total was -54.590000. running mean: -34.873420\n",
      "epsilon:0.128978 episode_count: 11214. steps_count: 4865763.000000\n",
      "Time elapsed:  13985.232418060303\n",
      "ep 1602: ep_len:500 episode reward: total was -166.590000. running mean: -36.190586\n",
      "ep 1602: ep_len:340 episode reward: total was -48.550000. running mean: -36.314180\n",
      "ep 1602: ep_len:611 episode reward: total was -134.680000. running mean: -37.297839\n",
      "ep 1602: ep_len:500 episode reward: total was -19.300000. running mean: -37.117860\n",
      "ep 1602: ep_len:48 episode reward: total was 18.000000. running mean: -36.566682\n",
      "ep 1602: ep_len:500 episode reward: total was -30.510000. running mean: -36.506115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1602: ep_len:616 episode reward: total was -21.460000. running mean: -36.355654\n",
      "epsilon:0.128934 episode_count: 11221. steps_count: 4868878.000000\n",
      "Time elapsed:  13993.795949935913\n",
      "ep 1603: ep_len:548 episode reward: total was -84.460000. running mean: -36.836697\n",
      "ep 1603: ep_len:527 episode reward: total was -10.430000. running mean: -36.572630\n",
      "ep 1603: ep_len:567 episode reward: total was -91.860000. running mean: -37.125504\n",
      "ep 1603: ep_len:153 episode reward: total was -4.460000. running mean: -36.798849\n",
      "ep 1603: ep_len:89 episode reward: total was 16.770000. running mean: -36.263160\n",
      "ep 1603: ep_len:510 episode reward: total was -97.170000. running mean: -36.872229\n",
      "ep 1603: ep_len:291 episode reward: total was -28.490000. running mean: -36.788406\n",
      "epsilon:0.128889 episode_count: 11228. steps_count: 4871563.000000\n",
      "Time elapsed:  14001.186680078506\n",
      "ep 1604: ep_len:505 episode reward: total was -35.980000. running mean: -36.780322\n",
      "ep 1604: ep_len:538 episode reward: total was -94.220000. running mean: -37.354719\n",
      "ep 1604: ep_len:610 episode reward: total was -56.460000. running mean: -37.545772\n",
      "ep 1604: ep_len:571 episode reward: total was -0.100000. running mean: -37.171314\n",
      "ep 1604: ep_len:3 episode reward: total was 0.000000. running mean: -36.799601\n",
      "ep 1604: ep_len:236 episode reward: total was 19.280000. running mean: -36.238805\n",
      "ep 1604: ep_len:506 episode reward: total was -65.010000. running mean: -36.526517\n",
      "epsilon:0.128845 episode_count: 11235. steps_count: 4874532.000000\n",
      "Time elapsed:  14009.12797832489\n",
      "ep 1605: ep_len:631 episode reward: total was -27.680000. running mean: -36.438052\n",
      "ep 1605: ep_len:296 episode reward: total was -49.910000. running mean: -36.572771\n",
      "ep 1605: ep_len:652 episode reward: total was -76.440000. running mean: -36.971444\n",
      "ep 1605: ep_len:519 episode reward: total was -19.740000. running mean: -36.799129\n",
      "ep 1605: ep_len:3 episode reward: total was 1.010000. running mean: -36.421038\n",
      "ep 1605: ep_len:571 episode reward: total was -18.540000. running mean: -36.242227\n",
      "ep 1605: ep_len:550 episode reward: total was -66.470000. running mean: -36.544505\n",
      "epsilon:0.128801 episode_count: 11242. steps_count: 4877754.000000\n",
      "Time elapsed:  14017.497885465622\n",
      "ep 1606: ep_len:500 episode reward: total was 43.200000. running mean: -35.747060\n",
      "ep 1606: ep_len:514 episode reward: total was -42.120000. running mean: -35.810790\n",
      "ep 1606: ep_len:600 episode reward: total was -51.700000. running mean: -35.969682\n",
      "ep 1606: ep_len:500 episode reward: total was -27.640000. running mean: -35.886385\n",
      "ep 1606: ep_len:3 episode reward: total was 1.010000. running mean: -35.517421\n",
      "ep 1606: ep_len:500 episode reward: total was -28.020000. running mean: -35.442447\n",
      "ep 1606: ep_len:299 episode reward: total was -50.060000. running mean: -35.588622\n",
      "epsilon:0.128756 episode_count: 11249. steps_count: 4880670.000000\n",
      "Time elapsed:  14024.443872213364\n",
      "ep 1607: ep_len:508 episode reward: total was -8.430000. running mean: -35.317036\n",
      "ep 1607: ep_len:500 episode reward: total was -6.270000. running mean: -35.026566\n",
      "ep 1607: ep_len:462 episode reward: total was 13.580000. running mean: -34.540500\n",
      "ep 1607: ep_len:531 episode reward: total was -112.260000. running mean: -35.317695\n",
      "ep 1607: ep_len:51 episode reward: total was 21.000000. running mean: -34.754518\n",
      "ep 1607: ep_len:508 episode reward: total was -27.240000. running mean: -34.679373\n",
      "ep 1607: ep_len:500 episode reward: total was -31.610000. running mean: -34.648679\n",
      "epsilon:0.128712 episode_count: 11256. steps_count: 4883730.000000\n",
      "Time elapsed:  14032.75380396843\n",
      "ep 1608: ep_len:128 episode reward: total was -7.110000. running mean: -34.373292\n",
      "ep 1608: ep_len:280 episode reward: total was -53.590000. running mean: -34.565459\n",
      "ep 1608: ep_len:611 episode reward: total was -31.830000. running mean: -34.538105\n",
      "ep 1608: ep_len:505 episode reward: total was -33.830000. running mean: -34.531024\n",
      "ep 1608: ep_len:3 episode reward: total was 1.010000. running mean: -34.175614\n",
      "ep 1608: ep_len:500 episode reward: total was -8.480000. running mean: -33.918657\n",
      "ep 1608: ep_len:501 episode reward: total was -60.200000. running mean: -34.181471\n",
      "epsilon:0.128668 episode_count: 11263. steps_count: 4886258.000000\n",
      "Time elapsed:  14039.83270573616\n",
      "ep 1609: ep_len:539 episode reward: total was -44.300000. running mean: -34.282656\n",
      "ep 1609: ep_len:500 episode reward: total was -41.860000. running mean: -34.358430\n",
      "ep 1609: ep_len:74 episode reward: total was -8.760000. running mean: -34.102445\n",
      "ep 1609: ep_len:500 episode reward: total was 21.640000. running mean: -33.545021\n",
      "ep 1609: ep_len:3 episode reward: total was 0.000000. running mean: -33.209571\n",
      "ep 1609: ep_len:563 episode reward: total was -87.690000. running mean: -33.754375\n",
      "ep 1609: ep_len:316 episode reward: total was -2.980000. running mean: -33.446631\n",
      "epsilon:0.128623 episode_count: 11270. steps_count: 4888753.000000\n",
      "Time elapsed:  14046.716315984726\n",
      "ep 1610: ep_len:500 episode reward: total was 25.750000. running mean: -32.854665\n",
      "ep 1610: ep_len:552 episode reward: total was -54.710000. running mean: -33.073218\n",
      "ep 1610: ep_len:585 episode reward: total was -46.870000. running mean: -33.211186\n",
      "ep 1610: ep_len:500 episode reward: total was -19.870000. running mean: -33.077774\n",
      "ep 1610: ep_len:96 episode reward: total was 14.240000. running mean: -32.604596\n",
      "ep 1610: ep_len:633 episode reward: total was -42.150000. running mean: -32.700051\n",
      "ep 1610: ep_len:336 episode reward: total was -19.700000. running mean: -32.570050\n",
      "epsilon:0.128579 episode_count: 11277. steps_count: 4891955.000000\n",
      "Time elapsed:  14057.131161928177\n",
      "ep 1611: ep_len:503 episode reward: total was -89.270000. running mean: -33.137049\n",
      "ep 1611: ep_len:500 episode reward: total was -18.900000. running mean: -32.994679\n",
      "ep 1611: ep_len:628 episode reward: total was -52.900000. running mean: -33.193732\n",
      "ep 1611: ep_len:373 episode reward: total was -16.700000. running mean: -33.028795\n",
      "ep 1611: ep_len:3 episode reward: total was 1.010000. running mean: -32.688407\n",
      "ep 1611: ep_len:602 episode reward: total was -66.280000. running mean: -33.024323\n",
      "ep 1611: ep_len:590 episode reward: total was -59.150000. running mean: -33.285580\n",
      "epsilon:0.128535 episode_count: 11284. steps_count: 4895154.000000\n",
      "Time elapsed:  14065.602138757706\n",
      "ep 1612: ep_len:531 episode reward: total was 35.470000. running mean: -32.598024\n",
      "ep 1612: ep_len:267 episode reward: total was -29.970000. running mean: -32.571744\n",
      "ep 1612: ep_len:70 episode reward: total was 2.250000. running mean: -32.223526\n",
      "ep 1612: ep_len:500 episode reward: total was 21.060000. running mean: -31.690691\n",
      "ep 1612: ep_len:101 episode reward: total was 18.270000. running mean: -31.191084\n",
      "ep 1612: ep_len:500 episode reward: total was -62.400000. running mean: -31.503173\n",
      "ep 1612: ep_len:544 episode reward: total was -44.950000. running mean: -31.637641\n",
      "epsilon:0.128490 episode_count: 11291. steps_count: 4897667.000000\n",
      "Time elapsed:  14072.467077970505\n",
      "ep 1613: ep_len:625 episode reward: total was 2.350000. running mean: -31.297765\n",
      "ep 1613: ep_len:514 episode reward: total was -52.800000. running mean: -31.512787\n",
      "ep 1613: ep_len:636 episode reward: total was -25.240000. running mean: -31.450059\n",
      "ep 1613: ep_len:401 episode reward: total was -24.590000. running mean: -31.381459\n",
      "ep 1613: ep_len:3 episode reward: total was 1.010000. running mean: -31.057544\n",
      "ep 1613: ep_len:524 episode reward: total was -74.540000. running mean: -31.492369\n",
      "ep 1613: ep_len:569 episode reward: total was -89.170000. running mean: -32.069145\n",
      "epsilon:0.128446 episode_count: 11298. steps_count: 4900939.000000\n",
      "Time elapsed:  14081.297365903854\n",
      "ep 1614: ep_len:500 episode reward: total was 48.490000. running mean: -31.263554\n",
      "ep 1614: ep_len:501 episode reward: total was -32.940000. running mean: -31.280318\n",
      "ep 1614: ep_len:585 episode reward: total was -41.720000. running mean: -31.384715\n",
      "ep 1614: ep_len:500 episode reward: total was -6.550000. running mean: -31.136368\n",
      "ep 1614: ep_len:3 episode reward: total was -1.500000. running mean: -30.840004\n",
      "ep 1614: ep_len:500 episode reward: total was -30.850000. running mean: -30.840104\n",
      "ep 1614: ep_len:632 episode reward: total was -27.290000. running mean: -30.804603\n",
      "epsilon:0.128402 episode_count: 11305. steps_count: 4904160.000000\n",
      "Time elapsed:  14093.0595870018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1615: ep_len:518 episode reward: total was -127.980000. running mean: -31.776357\n",
      "ep 1615: ep_len:545 episode reward: total was -25.390000. running mean: -31.712493\n",
      "ep 1615: ep_len:557 episode reward: total was -23.200000. running mean: -31.627369\n",
      "ep 1615: ep_len:500 episode reward: total was 28.200000. running mean: -31.029095\n",
      "ep 1615: ep_len:117 episode reward: total was -0.200000. running mean: -30.720804\n",
      "ep 1615: ep_len:672 episode reward: total was -26.950000. running mean: -30.683096\n",
      "ep 1615: ep_len:500 episode reward: total was -48.280000. running mean: -30.859065\n",
      "epsilon:0.128357 episode_count: 11312. steps_count: 4907569.000000\n",
      "Time elapsed:  14103.83174419403\n",
      "ep 1616: ep_len:241 episode reward: total was -128.800000. running mean: -31.838474\n",
      "ep 1616: ep_len:572 episode reward: total was -66.670000. running mean: -32.186790\n",
      "ep 1616: ep_len:500 episode reward: total was -26.480000. running mean: -32.129722\n",
      "ep 1616: ep_len:160 episode reward: total was 13.590000. running mean: -31.672524\n",
      "ep 1616: ep_len:3 episode reward: total was 1.010000. running mean: -31.345699\n",
      "ep 1616: ep_len:512 episode reward: total was -59.400000. running mean: -31.626242\n",
      "ep 1616: ep_len:550 episode reward: total was -33.420000. running mean: -31.644180\n",
      "epsilon:0.128313 episode_count: 11319. steps_count: 4910107.000000\n",
      "Time elapsed:  14110.950018167496\n",
      "ep 1617: ep_len:652 episode reward: total was -81.620000. running mean: -32.143938\n",
      "ep 1617: ep_len:542 episode reward: total was -41.230000. running mean: -32.234799\n",
      "ep 1617: ep_len:504 episode reward: total was -39.710000. running mean: -32.309551\n",
      "ep 1617: ep_len:515 episode reward: total was 17.230000. running mean: -31.814155\n",
      "ep 1617: ep_len:73 episode reward: total was 12.690000. running mean: -31.369114\n",
      "ep 1617: ep_len:537 episode reward: total was -127.650000. running mean: -32.331922\n",
      "ep 1617: ep_len:500 episode reward: total was -39.400000. running mean: -32.402603\n",
      "epsilon:0.128269 episode_count: 11326. steps_count: 4913430.000000\n",
      "Time elapsed:  14119.771890640259\n",
      "ep 1618: ep_len:240 episode reward: total was -24.380000. running mean: -32.322377\n",
      "ep 1618: ep_len:500 episode reward: total was -36.580000. running mean: -32.364953\n",
      "ep 1618: ep_len:573 episode reward: total was -77.440000. running mean: -32.815704\n",
      "ep 1618: ep_len:500 episode reward: total was -33.630000. running mean: -32.823847\n",
      "ep 1618: ep_len:88 episode reward: total was 17.250000. running mean: -32.323108\n",
      "ep 1618: ep_len:584 episode reward: total was -27.250000. running mean: -32.272377\n",
      "ep 1618: ep_len:612 episode reward: total was -42.400000. running mean: -32.373653\n",
      "epsilon:0.128224 episode_count: 11333. steps_count: 4916527.000000\n",
      "Time elapsed:  14127.876566648483\n",
      "ep 1619: ep_len:567 episode reward: total was -106.090000. running mean: -33.110817\n",
      "ep 1619: ep_len:607 episode reward: total was -80.370000. running mean: -33.583409\n",
      "ep 1619: ep_len:500 episode reward: total was -32.950000. running mean: -33.577075\n",
      "ep 1619: ep_len:163 episode reward: total was 22.670000. running mean: -33.014604\n",
      "ep 1619: ep_len:126 episode reward: total was -3.150000. running mean: -32.715958\n",
      "ep 1619: ep_len:216 episode reward: total was -5.230000. running mean: -32.441098\n",
      "ep 1619: ep_len:501 episode reward: total was -35.450000. running mean: -32.471187\n",
      "epsilon:0.128180 episode_count: 11340. steps_count: 4919207.000000\n",
      "Time elapsed:  14135.040236711502\n",
      "ep 1620: ep_len:552 episode reward: total was 8.060000. running mean: -32.065875\n",
      "ep 1620: ep_len:500 episode reward: total was -32.920000. running mean: -32.074417\n",
      "ep 1620: ep_len:79 episode reward: total was -40.290000. running mean: -32.156573\n",
      "ep 1620: ep_len:502 episode reward: total was -19.090000. running mean: -32.025907\n",
      "ep 1620: ep_len:3 episode reward: total was 0.000000. running mean: -31.705648\n",
      "ep 1620: ep_len:175 episode reward: total was 10.480000. running mean: -31.283791\n",
      "ep 1620: ep_len:580 episode reward: total was -37.350000. running mean: -31.344453\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.128136 episode_count: 11347. steps_count: 4921598.000000\n",
      "Time elapsed:  14146.856712579727\n",
      "ep 1621: ep_len:501 episode reward: total was 20.250000. running mean: -30.828509\n",
      "ep 1621: ep_len:201 episode reward: total was -19.650000. running mean: -30.716724\n",
      "ep 1621: ep_len:635 episode reward: total was -63.660000. running mean: -31.046157\n",
      "ep 1621: ep_len:567 episode reward: total was 9.570000. running mean: -30.639995\n",
      "ep 1621: ep_len:1 episode reward: total was -1.000000. running mean: -30.343595\n",
      "ep 1621: ep_len:594 episode reward: total was -116.870000. running mean: -31.208859\n",
      "ep 1621: ep_len:622 episode reward: total was -41.230000. running mean: -31.309070\n",
      "epsilon:0.128091 episode_count: 11354. steps_count: 4924719.000000\n",
      "Time elapsed:  14156.729167461395\n",
      "ep 1622: ep_len:557 episode reward: total was 5.960000. running mean: -30.936380\n",
      "ep 1622: ep_len:500 episode reward: total was -50.520000. running mean: -31.132216\n",
      "ep 1622: ep_len:623 episode reward: total was -47.630000. running mean: -31.297194\n",
      "ep 1622: ep_len:507 episode reward: total was 28.530000. running mean: -30.698922\n",
      "ep 1622: ep_len:3 episode reward: total was 0.000000. running mean: -30.391933\n",
      "ep 1622: ep_len:607 episode reward: total was -64.070000. running mean: -30.728713\n",
      "ep 1622: ep_len:299 episode reward: total was -31.310000. running mean: -30.734526\n",
      "epsilon:0.128047 episode_count: 11361. steps_count: 4927815.000000\n",
      "Time elapsed:  14166.644388198853\n",
      "ep 1623: ep_len:509 episode reward: total was -41.620000. running mean: -30.843381\n",
      "ep 1623: ep_len:547 episode reward: total was -23.320000. running mean: -30.768147\n",
      "ep 1623: ep_len:600 episode reward: total was -73.950000. running mean: -31.199966\n",
      "ep 1623: ep_len:545 episode reward: total was -18.460000. running mean: -31.072566\n",
      "ep 1623: ep_len:3 episode reward: total was 1.010000. running mean: -30.751740\n",
      "ep 1623: ep_len:626 episode reward: total was -56.280000. running mean: -31.007023\n",
      "ep 1623: ep_len:531 episode reward: total was -25.320000. running mean: -30.950153\n",
      "epsilon:0.128003 episode_count: 11368. steps_count: 4931176.000000\n",
      "Time elapsed:  14177.296870231628\n",
      "ep 1624: ep_len:634 episode reward: total was -28.410000. running mean: -30.924751\n",
      "ep 1624: ep_len:519 episode reward: total was -31.280000. running mean: -30.928304\n",
      "ep 1624: ep_len:540 episode reward: total was -23.260000. running mean: -30.851621\n",
      "ep 1624: ep_len:500 episode reward: total was 12.180000. running mean: -30.421304\n",
      "ep 1624: ep_len:3 episode reward: total was 1.010000. running mean: -30.106991\n",
      "ep 1624: ep_len:549 episode reward: total was 11.120000. running mean: -29.694721\n",
      "ep 1624: ep_len:613 episode reward: total was -16.710000. running mean: -29.564874\n",
      "epsilon:0.127958 episode_count: 11375. steps_count: 4934534.000000\n",
      "Time elapsed:  14186.179490089417\n",
      "ep 1625: ep_len:502 episode reward: total was -115.470000. running mean: -30.423925\n",
      "ep 1625: ep_len:575 episode reward: total was -118.730000. running mean: -31.306986\n",
      "ep 1625: ep_len:682 episode reward: total was -42.860000. running mean: -31.422516\n",
      "ep 1625: ep_len:500 episode reward: total was 22.960000. running mean: -30.878691\n",
      "ep 1625: ep_len:53 episode reward: total was 19.000000. running mean: -30.379904\n",
      "ep 1625: ep_len:232 episode reward: total was 16.450000. running mean: -29.911605\n",
      "ep 1625: ep_len:318 episode reward: total was -15.660000. running mean: -29.769089\n",
      "epsilon:0.127914 episode_count: 11382. steps_count: 4937396.000000\n",
      "Time elapsed:  14194.887462615967\n",
      "ep 1626: ep_len:636 episode reward: total was -103.530000. running mean: -30.506698\n",
      "ep 1626: ep_len:585 episode reward: total was -29.150000. running mean: -30.493131\n",
      "ep 1626: ep_len:393 episode reward: total was -3.170000. running mean: -30.219900\n",
      "ep 1626: ep_len:409 episode reward: total was 0.540000. running mean: -29.912301\n",
      "ep 1626: ep_len:108 episode reward: total was 21.740000. running mean: -29.395778\n",
      "ep 1626: ep_len:579 episode reward: total was -23.200000. running mean: -29.333820\n",
      "ep 1626: ep_len:567 episode reward: total was -29.500000. running mean: -29.335482\n",
      "epsilon:0.127870 episode_count: 11389. steps_count: 4940673.000000\n",
      "Time elapsed:  14204.982135772705\n",
      "ep 1627: ep_len:533 episode reward: total was -8.400000. running mean: -29.126127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1627: ep_len:381 episode reward: total was -70.550000. running mean: -29.540366\n",
      "ep 1627: ep_len:368 episode reward: total was 15.250000. running mean: -29.092462\n",
      "ep 1627: ep_len:500 episode reward: total was -13.500000. running mean: -28.936538\n",
      "ep 1627: ep_len:3 episode reward: total was 1.010000. running mean: -28.637072\n",
      "ep 1627: ep_len:500 episode reward: total was -36.950000. running mean: -28.720202\n",
      "ep 1627: ep_len:575 episode reward: total was -157.020000. running mean: -30.003200\n",
      "epsilon:0.127825 episode_count: 11396. steps_count: 4943533.000000\n",
      "Time elapsed:  14212.485131025314\n",
      "ep 1628: ep_len:500 episode reward: total was -53.300000. running mean: -30.236168\n",
      "ep 1628: ep_len:513 episode reward: total was -34.770000. running mean: -30.281506\n",
      "ep 1628: ep_len:500 episode reward: total was -25.130000. running mean: -30.229991\n",
      "ep 1628: ep_len:414 episode reward: total was -20.490000. running mean: -30.132591\n",
      "ep 1628: ep_len:123 episode reward: total was 12.830000. running mean: -29.702965\n",
      "ep 1628: ep_len:580 episode reward: total was -20.120000. running mean: -29.607135\n",
      "ep 1628: ep_len:500 episode reward: total was -29.740000. running mean: -29.608464\n",
      "epsilon:0.127781 episode_count: 11403. steps_count: 4946663.000000\n",
      "Time elapsed:  14220.781186819077\n",
      "ep 1629: ep_len:535 episode reward: total was -9.620000. running mean: -29.408579\n",
      "ep 1629: ep_len:511 episode reward: total was 15.030000. running mean: -28.964194\n",
      "ep 1629: ep_len:547 episode reward: total was -73.090000. running mean: -29.405452\n",
      "ep 1629: ep_len:523 episode reward: total was -24.490000. running mean: -29.356297\n",
      "ep 1629: ep_len:3 episode reward: total was 0.000000. running mean: -29.062734\n",
      "ep 1629: ep_len:531 episode reward: total was -51.370000. running mean: -29.285807\n",
      "ep 1629: ep_len:568 episode reward: total was -76.880000. running mean: -29.761749\n",
      "epsilon:0.127737 episode_count: 11410. steps_count: 4949881.000000\n",
      "Time elapsed:  14229.315294027328\n",
      "ep 1630: ep_len:116 episode reward: total was -3.130000. running mean: -29.495431\n",
      "ep 1630: ep_len:565 episode reward: total was -22.110000. running mean: -29.421577\n",
      "ep 1630: ep_len:544 episode reward: total was -55.080000. running mean: -29.678161\n",
      "ep 1630: ep_len:567 episode reward: total was -6.980000. running mean: -29.451180\n",
      "ep 1630: ep_len:3 episode reward: total was 0.000000. running mean: -29.156668\n",
      "ep 1630: ep_len:574 episode reward: total was -16.290000. running mean: -29.028001\n",
      "ep 1630: ep_len:315 episode reward: total was -37.950000. running mean: -29.117221\n",
      "epsilon:0.127692 episode_count: 11417. steps_count: 4952565.000000\n",
      "Time elapsed:  14236.169781208038\n",
      "ep 1631: ep_len:550 episode reward: total was -12.090000. running mean: -28.946949\n",
      "ep 1631: ep_len:539 episode reward: total was -43.700000. running mean: -29.094479\n",
      "ep 1631: ep_len:531 episode reward: total was -33.970000. running mean: -29.143235\n",
      "ep 1631: ep_len:503 episode reward: total was -22.620000. running mean: -29.078002\n",
      "ep 1631: ep_len:3 episode reward: total was 0.000000. running mean: -28.787222\n",
      "ep 1631: ep_len:626 episode reward: total was -77.500000. running mean: -29.274350\n",
      "ep 1631: ep_len:601 episode reward: total was -203.120000. running mean: -31.012806\n",
      "epsilon:0.127648 episode_count: 11424. steps_count: 4955918.000000\n",
      "Time elapsed:  14248.707519292831\n",
      "ep 1632: ep_len:189 episode reward: total was -1.120000. running mean: -30.713878\n",
      "ep 1632: ep_len:553 episode reward: total was -33.570000. running mean: -30.742440\n",
      "ep 1632: ep_len:707 episode reward: total was -48.210000. running mean: -30.917115\n",
      "ep 1632: ep_len:535 episode reward: total was -42.560000. running mean: -31.033544\n",
      "ep 1632: ep_len:128 episode reward: total was 17.290000. running mean: -30.550309\n",
      "ep 1632: ep_len:584 episode reward: total was -26.240000. running mean: -30.507206\n",
      "ep 1632: ep_len:579 episode reward: total was -42.940000. running mean: -30.631533\n",
      "epsilon:0.127604 episode_count: 11431. steps_count: 4959193.000000\n",
      "Time elapsed:  14257.27097773552\n",
      "ep 1633: ep_len:570 episode reward: total was 12.810000. running mean: -30.197118\n",
      "ep 1633: ep_len:305 episode reward: total was -44.830000. running mean: -30.343447\n",
      "ep 1633: ep_len:310 episode reward: total was 4.840000. running mean: -29.991613\n",
      "ep 1633: ep_len:500 episode reward: total was -31.940000. running mean: -30.011096\n",
      "ep 1633: ep_len:3 episode reward: total was -1.500000. running mean: -29.725985\n",
      "ep 1633: ep_len:575 episode reward: total was -32.450000. running mean: -29.753226\n",
      "ep 1633: ep_len:275 episode reward: total was -16.840000. running mean: -29.624093\n",
      "epsilon:0.127559 episode_count: 11438. steps_count: 4961731.000000\n",
      "Time elapsed:  14264.181045293808\n",
      "ep 1634: ep_len:502 episode reward: total was 20.700000. running mean: -29.120852\n",
      "ep 1634: ep_len:500 episode reward: total was -10.060000. running mean: -28.930244\n",
      "ep 1634: ep_len:500 episode reward: total was -50.460000. running mean: -29.145541\n",
      "ep 1634: ep_len:56 episode reward: total was -12.690000. running mean: -28.980986\n",
      "ep 1634: ep_len:3 episode reward: total was 0.000000. running mean: -28.691176\n",
      "ep 1634: ep_len:550 episode reward: total was -37.800000. running mean: -28.782264\n",
      "ep 1634: ep_len:586 episode reward: total was -50.710000. running mean: -29.001542\n",
      "epsilon:0.127515 episode_count: 11445. steps_count: 4964428.000000\n",
      "Time elapsed:  14271.549159526825\n",
      "ep 1635: ep_len:500 episode reward: total was 23.240000. running mean: -28.479126\n",
      "ep 1635: ep_len:545 episode reward: total was 21.410000. running mean: -27.980235\n",
      "ep 1635: ep_len:594 episode reward: total was -70.920000. running mean: -28.409633\n",
      "ep 1635: ep_len:503 episode reward: total was -25.700000. running mean: -28.382536\n",
      "ep 1635: ep_len:1 episode reward: total was -1.000000. running mean: -28.108711\n",
      "ep 1635: ep_len:536 episode reward: total was -77.510000. running mean: -28.602724\n",
      "ep 1635: ep_len:564 episode reward: total was -52.900000. running mean: -28.845697\n",
      "epsilon:0.127471 episode_count: 11452. steps_count: 4967671.000000\n",
      "Time elapsed:  14280.046310186386\n",
      "ep 1636: ep_len:573 episode reward: total was -75.910000. running mean: -29.316340\n",
      "ep 1636: ep_len:514 episode reward: total was -8.510000. running mean: -29.108276\n",
      "ep 1636: ep_len:500 episode reward: total was -21.750000. running mean: -29.034694\n",
      "ep 1636: ep_len:501 episode reward: total was 2.700000. running mean: -28.717347\n",
      "ep 1636: ep_len:91 episode reward: total was -8.770000. running mean: -28.517873\n",
      "ep 1636: ep_len:502 episode reward: total was -46.470000. running mean: -28.697394\n",
      "ep 1636: ep_len:506 episode reward: total was -39.790000. running mean: -28.808320\n",
      "epsilon:0.127426 episode_count: 11459. steps_count: 4970858.000000\n",
      "Time elapsed:  14288.450397968292\n",
      "ep 1637: ep_len:580 episode reward: total was -84.290000. running mean: -29.363137\n",
      "ep 1637: ep_len:624 episode reward: total was -51.140000. running mean: -29.580906\n",
      "ep 1637: ep_len:560 episode reward: total was -53.140000. running mean: -29.816497\n",
      "ep 1637: ep_len:623 episode reward: total was -4.340000. running mean: -29.561732\n",
      "ep 1637: ep_len:3 episode reward: total was 0.000000. running mean: -29.266115\n",
      "ep 1637: ep_len:500 episode reward: total was -27.840000. running mean: -29.251853\n",
      "ep 1637: ep_len:526 episode reward: total was -52.470000. running mean: -29.484035\n",
      "epsilon:0.127382 episode_count: 11466. steps_count: 4974274.000000\n",
      "Time elapsed:  14296.197456598282\n",
      "ep 1638: ep_len:134 episode reward: total was -38.760000. running mean: -29.576795\n",
      "ep 1638: ep_len:500 episode reward: total was 16.960000. running mean: -29.111427\n",
      "ep 1638: ep_len:500 episode reward: total was -61.630000. running mean: -29.436612\n",
      "ep 1638: ep_len:502 episode reward: total was -14.300000. running mean: -29.285246\n",
      "ep 1638: ep_len:3 episode reward: total was 1.010000. running mean: -28.982294\n",
      "ep 1638: ep_len:500 episode reward: total was -47.070000. running mean: -29.163171\n",
      "ep 1638: ep_len:500 episode reward: total was -62.830000. running mean: -29.499839\n",
      "epsilon:0.127338 episode_count: 11473. steps_count: 4976913.000000\n",
      "Time elapsed:  14303.415809392929\n",
      "ep 1639: ep_len:529 episode reward: total was -32.550000. running mean: -29.530341\n",
      "ep 1639: ep_len:556 episode reward: total was -40.010000. running mean: -29.635137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1639: ep_len:609 episode reward: total was -51.190000. running mean: -29.850686\n",
      "ep 1639: ep_len:521 episode reward: total was -41.570000. running mean: -29.967879\n",
      "ep 1639: ep_len:126 episode reward: total was 8.790000. running mean: -29.580300\n",
      "ep 1639: ep_len:595 episode reward: total was -58.010000. running mean: -29.864597\n",
      "ep 1639: ep_len:537 episode reward: total was -18.000000. running mean: -29.745951\n",
      "epsilon:0.127293 episode_count: 11480. steps_count: 4980386.000000\n",
      "Time elapsed:  14312.49517917633\n",
      "ep 1640: ep_len:500 episode reward: total was 25.700000. running mean: -29.191492\n",
      "ep 1640: ep_len:554 episode reward: total was -28.970000. running mean: -29.189277\n",
      "ep 1640: ep_len:554 episode reward: total was -63.270000. running mean: -29.530084\n",
      "ep 1640: ep_len:504 episode reward: total was -60.200000. running mean: -29.836783\n",
      "ep 1640: ep_len:3 episode reward: total was 1.010000. running mean: -29.528315\n",
      "ep 1640: ep_len:684 episode reward: total was -33.170000. running mean: -29.564732\n",
      "ep 1640: ep_len:575 episode reward: total was -46.610000. running mean: -29.735185\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.127249 episode_count: 11487. steps_count: 4983760.000000\n",
      "Time elapsed:  14328.536158323288\n",
      "ep 1641: ep_len:553 episode reward: total was 44.400000. running mean: -28.993833\n",
      "ep 1641: ep_len:173 episode reward: total was -26.730000. running mean: -28.971195\n",
      "ep 1641: ep_len:667 episode reward: total was -213.580000. running mean: -30.817283\n",
      "ep 1641: ep_len:508 episode reward: total was -42.070000. running mean: -30.929810\n",
      "ep 1641: ep_len:84 episode reward: total was 7.750000. running mean: -30.543012\n",
      "ep 1641: ep_len:501 episode reward: total was -13.290000. running mean: -30.370482\n",
      "ep 1641: ep_len:500 episode reward: total was -52.160000. running mean: -30.588377\n",
      "epsilon:0.127205 episode_count: 11494. steps_count: 4986746.000000\n",
      "Time elapsed:  14336.466500282288\n",
      "ep 1642: ep_len:500 episode reward: total was 23.980000. running mean: -30.042693\n",
      "ep 1642: ep_len:588 episode reward: total was -54.880000. running mean: -30.291066\n",
      "ep 1642: ep_len:500 episode reward: total was -8.920000. running mean: -30.077356\n",
      "ep 1642: ep_len:598 episode reward: total was 6.940000. running mean: -29.707182\n",
      "ep 1642: ep_len:96 episode reward: total was 12.220000. running mean: -29.287910\n",
      "ep 1642: ep_len:556 episode reward: total was -12.310000. running mean: -29.118131\n",
      "ep 1642: ep_len:585 episode reward: total was -50.810000. running mean: -29.335050\n",
      "epsilon:0.127160 episode_count: 11501. steps_count: 4990169.000000\n",
      "Time elapsed:  14345.32349729538\n",
      "ep 1643: ep_len:108 episode reward: total was 12.770000. running mean: -28.913999\n",
      "ep 1643: ep_len:522 episode reward: total was -25.830000. running mean: -28.883159\n",
      "ep 1643: ep_len:434 episode reward: total was -5.130000. running mean: -28.645628\n",
      "ep 1643: ep_len:533 episode reward: total was 23.250000. running mean: -28.126671\n",
      "ep 1643: ep_len:68 episode reward: total was 13.680000. running mean: -27.708605\n",
      "ep 1643: ep_len:596 episode reward: total was -12.000000. running mean: -27.551519\n",
      "ep 1643: ep_len:577 episode reward: total was -31.630000. running mean: -27.592303\n",
      "epsilon:0.127116 episode_count: 11508. steps_count: 4993007.000000\n",
      "Time elapsed:  14355.016023874283\n",
      "ep 1644: ep_len:500 episode reward: total was 24.280000. running mean: -27.073580\n",
      "ep 1644: ep_len:616 episode reward: total was -0.110000. running mean: -26.803945\n",
      "ep 1644: ep_len:626 episode reward: total was -50.040000. running mean: -27.036305\n",
      "ep 1644: ep_len:170 episode reward: total was 7.190000. running mean: -26.694042\n",
      "ep 1644: ep_len:3 episode reward: total was 0.000000. running mean: -26.427102\n",
      "ep 1644: ep_len:188 episode reward: total was 2.170000. running mean: -26.141131\n",
      "ep 1644: ep_len:558 episode reward: total was -36.210000. running mean: -26.241819\n",
      "epsilon:0.127072 episode_count: 11515. steps_count: 4995668.000000\n",
      "Time elapsed:  14362.226655006409\n",
      "ep 1645: ep_len:500 episode reward: total was -1.790000. running mean: -25.997301\n",
      "ep 1645: ep_len:593 episode reward: total was 20.590000. running mean: -25.531428\n",
      "ep 1645: ep_len:59 episode reward: total was -11.210000. running mean: -25.388214\n",
      "ep 1645: ep_len:510 episode reward: total was -5.900000. running mean: -25.193332\n",
      "ep 1645: ep_len:3 episode reward: total was 1.010000. running mean: -24.931298\n",
      "ep 1645: ep_len:500 episode reward: total was -37.150000. running mean: -25.053485\n",
      "ep 1645: ep_len:300 episode reward: total was -35.310000. running mean: -25.156051\n",
      "epsilon:0.127027 episode_count: 11522. steps_count: 4998133.000000\n",
      "Time elapsed:  14369.165020942688\n",
      "ep 1646: ep_len:500 episode reward: total was 30.080000. running mean: -24.603690\n",
      "ep 1646: ep_len:500 episode reward: total was -33.610000. running mean: -24.693753\n",
      "ep 1646: ep_len:556 episode reward: total was -43.450000. running mean: -24.881316\n",
      "ep 1646: ep_len:528 episode reward: total was 7.300000. running mean: -24.559503\n",
      "ep 1646: ep_len:3 episode reward: total was 1.010000. running mean: -24.303807\n",
      "ep 1646: ep_len:540 episode reward: total was -16.130000. running mean: -24.222069\n",
      "ep 1646: ep_len:318 episode reward: total was -27.150000. running mean: -24.251349\n",
      "epsilon:0.126983 episode_count: 11529. steps_count: 5001078.000000\n",
      "Time elapsed:  14377.086735725403\n",
      "ep 1647: ep_len:500 episode reward: total was -23.920000. running mean: -24.248035\n",
      "ep 1647: ep_len:500 episode reward: total was 26.890000. running mean: -23.736655\n",
      "ep 1647: ep_len:501 episode reward: total was -9.860000. running mean: -23.597888\n",
      "ep 1647: ep_len:511 episode reward: total was 2.280000. running mean: -23.339109\n",
      "ep 1647: ep_len:3 episode reward: total was -1.500000. running mean: -23.120718\n",
      "ep 1647: ep_len:707 episode reward: total was -18.610000. running mean: -23.075611\n",
      "ep 1647: ep_len:583 episode reward: total was -44.910000. running mean: -23.293955\n",
      "epsilon:0.126939 episode_count: 11536. steps_count: 5004383.000000\n",
      "Time elapsed:  14385.776961565018\n",
      "ep 1648: ep_len:551 episode reward: total was -6.460000. running mean: -23.125616\n",
      "ep 1648: ep_len:191 episode reward: total was 2.320000. running mean: -22.871159\n",
      "ep 1648: ep_len:569 episode reward: total was -63.290000. running mean: -23.275348\n",
      "ep 1648: ep_len:500 episode reward: total was -50.930000. running mean: -23.551894\n",
      "ep 1648: ep_len:3 episode reward: total was 1.010000. running mean: -23.306275\n",
      "ep 1648: ep_len:555 episode reward: total was -119.030000. running mean: -24.263513\n",
      "ep 1648: ep_len:610 episode reward: total was -52.560000. running mean: -24.546477\n",
      "epsilon:0.126894 episode_count: 11543. steps_count: 5007362.000000\n",
      "Time elapsed:  14393.737797260284\n",
      "ep 1649: ep_len:192 episode reward: total was -14.990000. running mean: -24.450913\n",
      "ep 1649: ep_len:599 episode reward: total was -13.190000. running mean: -24.338304\n",
      "ep 1649: ep_len:560 episode reward: total was -81.290000. running mean: -24.907821\n",
      "ep 1649: ep_len:595 episode reward: total was 11.680000. running mean: -24.541942\n",
      "ep 1649: ep_len:88 episode reward: total was 21.170000. running mean: -24.084823\n",
      "ep 1649: ep_len:592 episode reward: total was 6.020000. running mean: -23.783775\n",
      "ep 1649: ep_len:534 episode reward: total was -68.860000. running mean: -24.234537\n",
      "epsilon:0.126850 episode_count: 11550. steps_count: 5010522.000000\n",
      "Time elapsed:  14402.069782018661\n",
      "ep 1650: ep_len:520 episode reward: total was 21.740000. running mean: -23.774792\n",
      "ep 1650: ep_len:501 episode reward: total was 24.460000. running mean: -23.292444\n",
      "ep 1650: ep_len:558 episode reward: total was -126.320000. running mean: -24.322719\n",
      "ep 1650: ep_len:500 episode reward: total was -60.170000. running mean: -24.681192\n",
      "ep 1650: ep_len:3 episode reward: total was 0.000000. running mean: -24.434380\n",
      "ep 1650: ep_len:634 episode reward: total was -55.130000. running mean: -24.741336\n",
      "ep 1650: ep_len:585 episode reward: total was -37.010000. running mean: -24.864023\n",
      "epsilon:0.126806 episode_count: 11557. steps_count: 5013823.000000\n",
      "Time elapsed:  14412.96294093132\n",
      "ep 1651: ep_len:597 episode reward: total was -65.550000. running mean: -25.270883\n",
      "ep 1651: ep_len:522 episode reward: total was -9.630000. running mean: -25.114474\n",
      "ep 1651: ep_len:510 episode reward: total was -81.980000. running mean: -25.683129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1651: ep_len:527 episode reward: total was -20.090000. running mean: -25.627198\n",
      "ep 1651: ep_len:124 episode reward: total was 9.850000. running mean: -25.272426\n",
      "ep 1651: ep_len:500 episode reward: total was -29.060000. running mean: -25.310302\n",
      "ep 1651: ep_len:511 episode reward: total was -36.690000. running mean: -25.424099\n",
      "epsilon:0.126761 episode_count: 11564. steps_count: 5017114.000000\n",
      "Time elapsed:  14423.639533281326\n",
      "ep 1652: ep_len:565 episode reward: total was 12.570000. running mean: -25.044158\n",
      "ep 1652: ep_len:628 episode reward: total was -33.470000. running mean: -25.128416\n",
      "ep 1652: ep_len:609 episode reward: total was -10.190000. running mean: -24.979032\n",
      "ep 1652: ep_len:502 episode reward: total was 28.210000. running mean: -24.447142\n",
      "ep 1652: ep_len:3 episode reward: total was -3.000000. running mean: -24.232670\n",
      "ep 1652: ep_len:592 episode reward: total was -36.810000. running mean: -24.358443\n",
      "ep 1652: ep_len:568 episode reward: total was -37.450000. running mean: -24.489359\n",
      "epsilon:0.126717 episode_count: 11571. steps_count: 5020581.000000\n",
      "Time elapsed:  14434.84380364418\n",
      "ep 1653: ep_len:500 episode reward: total was 16.630000. running mean: -24.078165\n",
      "ep 1653: ep_len:630 episode reward: total was -0.030000. running mean: -23.837684\n",
      "ep 1653: ep_len:547 episode reward: total was -22.730000. running mean: -23.826607\n",
      "ep 1653: ep_len:157 episode reward: total was 5.170000. running mean: -23.536641\n",
      "ep 1653: ep_len:3 episode reward: total was 1.010000. running mean: -23.291174\n",
      "ep 1653: ep_len:500 episode reward: total was -72.430000. running mean: -23.782563\n",
      "ep 1653: ep_len:500 episode reward: total was -79.750000. running mean: -24.342237\n",
      "epsilon:0.126673 episode_count: 11578. steps_count: 5023418.000000\n",
      "Time elapsed:  14442.410349845886\n",
      "ep 1654: ep_len:617 episode reward: total was -32.900000. running mean: -24.427815\n",
      "ep 1654: ep_len:527 episode reward: total was -33.980000. running mean: -24.523337\n",
      "ep 1654: ep_len:526 episode reward: total was -30.010000. running mean: -24.578203\n",
      "ep 1654: ep_len:500 episode reward: total was -39.520000. running mean: -24.727621\n",
      "ep 1654: ep_len:3 episode reward: total was 1.010000. running mean: -24.470245\n",
      "ep 1654: ep_len:624 episode reward: total was -61.420000. running mean: -24.839742\n",
      "ep 1654: ep_len:500 episode reward: total was -40.840000. running mean: -24.999745\n",
      "epsilon:0.126628 episode_count: 11585. steps_count: 5026715.000000\n",
      "Time elapsed:  14451.065445423126\n",
      "ep 1655: ep_len:247 episode reward: total was 0.670000. running mean: -24.743048\n",
      "ep 1655: ep_len:571 episode reward: total was -91.610000. running mean: -25.411717\n",
      "ep 1655: ep_len:500 episode reward: total was -26.880000. running mean: -25.426400\n",
      "ep 1655: ep_len:132 episode reward: total was 2.050000. running mean: -25.151636\n",
      "ep 1655: ep_len:56 episode reward: total was 20.010000. running mean: -24.700020\n",
      "ep 1655: ep_len:285 episode reward: total was 23.430000. running mean: -24.218719\n",
      "ep 1655: ep_len:338 episode reward: total was -26.170000. running mean: -24.238232\n",
      "epsilon:0.126584 episode_count: 11592. steps_count: 5028844.000000\n",
      "Time elapsed:  14457.004341840744\n",
      "ep 1656: ep_len:226 episode reward: total was -4.870000. running mean: -24.044550\n",
      "ep 1656: ep_len:519 episode reward: total was 14.800000. running mean: -23.656104\n",
      "ep 1656: ep_len:642 episode reward: total was -60.720000. running mean: -24.026743\n",
      "ep 1656: ep_len:538 episode reward: total was 3.910000. running mean: -23.747376\n",
      "ep 1656: ep_len:98 episode reward: total was 20.230000. running mean: -23.307602\n",
      "ep 1656: ep_len:526 episode reward: total was -109.370000. running mean: -24.168226\n",
      "ep 1656: ep_len:566 episode reward: total was -23.740000. running mean: -24.163944\n",
      "epsilon:0.126540 episode_count: 11599. steps_count: 5031959.000000\n",
      "Time elapsed:  14467.129130125046\n",
      "ep 1657: ep_len:535 episode reward: total was -95.190000. running mean: -24.874204\n",
      "ep 1657: ep_len:184 episode reward: total was -23.620000. running mean: -24.861662\n",
      "ep 1657: ep_len:622 episode reward: total was -151.500000. running mean: -26.128046\n",
      "ep 1657: ep_len:501 episode reward: total was -67.520000. running mean: -26.541965\n",
      "ep 1657: ep_len:116 episode reward: total was 11.330000. running mean: -26.163246\n",
      "ep 1657: ep_len:500 episode reward: total was -31.590000. running mean: -26.217513\n",
      "ep 1657: ep_len:167 episode reward: total was 4.770000. running mean: -25.907638\n",
      "epsilon:0.126495 episode_count: 11606. steps_count: 5034584.000000\n",
      "Time elapsed:  14474.33854842186\n",
      "ep 1658: ep_len:500 episode reward: total was -33.280000. running mean: -25.981362\n",
      "ep 1658: ep_len:522 episode reward: total was 0.230000. running mean: -25.719248\n",
      "ep 1658: ep_len:553 episode reward: total was -45.380000. running mean: -25.915856\n",
      "ep 1658: ep_len:610 episode reward: total was 31.210000. running mean: -25.344597\n",
      "ep 1658: ep_len:3 episode reward: total was 1.010000. running mean: -25.081051\n",
      "ep 1658: ep_len:570 episode reward: total was -73.100000. running mean: -25.561241\n",
      "ep 1658: ep_len:581 episode reward: total was -66.710000. running mean: -25.972728\n",
      "epsilon:0.126451 episode_count: 11613. steps_count: 5037923.000000\n",
      "Time elapsed:  14483.138895988464\n",
      "ep 1659: ep_len:548 episode reward: total was -14.750000. running mean: -25.860501\n",
      "ep 1659: ep_len:619 episode reward: total was -11.540000. running mean: -25.717296\n",
      "ep 1659: ep_len:583 episode reward: total was -71.080000. running mean: -26.170923\n",
      "ep 1659: ep_len:540 episode reward: total was -1.410000. running mean: -25.923314\n",
      "ep 1659: ep_len:111 episode reward: total was 20.730000. running mean: -25.456781\n",
      "ep 1659: ep_len:258 episode reward: total was 20.420000. running mean: -24.998013\n",
      "ep 1659: ep_len:597 episode reward: total was -57.250000. running mean: -25.320533\n",
      "epsilon:0.126407 episode_count: 11620. steps_count: 5041179.000000\n",
      "Time elapsed:  14491.837414741516\n",
      "ep 1660: ep_len:543 episode reward: total was 16.320000. running mean: -24.904127\n",
      "ep 1660: ep_len:538 episode reward: total was -43.350000. running mean: -25.088586\n",
      "ep 1660: ep_len:553 episode reward: total was -48.340000. running mean: -25.321100\n",
      "ep 1660: ep_len:170 episode reward: total was 1.700000. running mean: -25.050889\n",
      "ep 1660: ep_len:3 episode reward: total was 1.010000. running mean: -24.790280\n",
      "ep 1660: ep_len:500 episode reward: total was -45.630000. running mean: -24.998677\n",
      "ep 1660: ep_len:333 episode reward: total was -33.100000. running mean: -25.079691\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.126362 episode_count: 11627. steps_count: 5043819.000000\n",
      "Time elapsed:  14503.921129465103\n",
      "ep 1661: ep_len:632 episode reward: total was -11.110000. running mean: -24.939994\n",
      "ep 1661: ep_len:611 episode reward: total was 22.030000. running mean: -24.470294\n",
      "ep 1661: ep_len:562 episode reward: total was -78.240000. running mean: -25.007991\n",
      "ep 1661: ep_len:500 episode reward: total was -14.080000. running mean: -24.898711\n",
      "ep 1661: ep_len:3 episode reward: total was 1.010000. running mean: -24.639624\n",
      "ep 1661: ep_len:587 episode reward: total was -157.290000. running mean: -25.966128\n",
      "ep 1661: ep_len:211 episode reward: total was -34.030000. running mean: -26.046766\n",
      "epsilon:0.126318 episode_count: 11634. steps_count: 5046925.000000\n",
      "Time elapsed:  14511.52136015892\n",
      "ep 1662: ep_len:129 episode reward: total was 0.950000. running mean: -25.776799\n",
      "ep 1662: ep_len:515 episode reward: total was -67.730000. running mean: -26.196331\n",
      "ep 1662: ep_len:648 episode reward: total was -41.490000. running mean: -26.349267\n",
      "ep 1662: ep_len:512 episode reward: total was -34.990000. running mean: -26.435675\n",
      "ep 1662: ep_len:77 episode reward: total was 17.660000. running mean: -25.994718\n",
      "ep 1662: ep_len:533 episode reward: total was -23.360000. running mean: -25.968371\n",
      "ep 1662: ep_len:540 episode reward: total was -54.290000. running mean: -26.251587\n",
      "epsilon:0.126274 episode_count: 11641. steps_count: 5049879.000000\n",
      "Time elapsed:  14519.768054246902\n",
      "ep 1663: ep_len:118 episode reward: total was -10.240000. running mean: -26.091471\n",
      "ep 1663: ep_len:596 episode reward: total was -33.710000. running mean: -26.167657\n",
      "ep 1663: ep_len:575 episode reward: total was -50.620000. running mean: -26.412180\n",
      "ep 1663: ep_len:56 episode reward: total was -3.220000. running mean: -26.180258\n",
      "ep 1663: ep_len:3 episode reward: total was 0.000000. running mean: -25.918456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1663: ep_len:649 episode reward: total was -36.710000. running mean: -26.026371\n",
      "ep 1663: ep_len:334 episode reward: total was -30.400000. running mean: -26.070107\n",
      "epsilon:0.126229 episode_count: 11648. steps_count: 5052210.000000\n",
      "Time elapsed:  14525.622448205948\n",
      "ep 1664: ep_len:581 episode reward: total was -90.920000. running mean: -26.718606\n",
      "ep 1664: ep_len:503 episode reward: total was -30.290000. running mean: -26.754320\n",
      "ep 1664: ep_len:79 episode reward: total was -2.740000. running mean: -26.514177\n",
      "ep 1664: ep_len:501 episode reward: total was -44.230000. running mean: -26.691335\n",
      "ep 1664: ep_len:106 episode reward: total was 10.760000. running mean: -26.316822\n",
      "ep 1664: ep_len:521 episode reward: total was -78.330000. running mean: -26.836954\n",
      "ep 1664: ep_len:584 episode reward: total was -38.050000. running mean: -26.949084\n",
      "epsilon:0.126185 episode_count: 11655. steps_count: 5055085.000000\n",
      "Time elapsed:  14533.24541759491\n",
      "ep 1665: ep_len:500 episode reward: total was -70.610000. running mean: -27.385693\n",
      "ep 1665: ep_len:331 episode reward: total was -175.490000. running mean: -28.866736\n",
      "ep 1665: ep_len:625 episode reward: total was -138.750000. running mean: -29.965569\n",
      "ep 1665: ep_len:500 episode reward: total was -27.840000. running mean: -29.944313\n",
      "ep 1665: ep_len:3 episode reward: total was 1.010000. running mean: -29.634770\n",
      "ep 1665: ep_len:508 episode reward: total was -34.590000. running mean: -29.684322\n",
      "ep 1665: ep_len:526 episode reward: total was -97.130000. running mean: -30.358779\n",
      "epsilon:0.126141 episode_count: 11662. steps_count: 5058078.000000\n",
      "Time elapsed:  14543.427302598953\n",
      "ep 1666: ep_len:243 episode reward: total was 0.770000. running mean: -30.047491\n",
      "ep 1666: ep_len:527 episode reward: total was -68.640000. running mean: -30.433416\n",
      "ep 1666: ep_len:381 episode reward: total was 13.120000. running mean: -29.997882\n",
      "ep 1666: ep_len:500 episode reward: total was -1.920000. running mean: -29.717104\n",
      "ep 1666: ep_len:3 episode reward: total was 0.000000. running mean: -29.419932\n",
      "ep 1666: ep_len:528 episode reward: total was 8.350000. running mean: -29.042233\n",
      "ep 1666: ep_len:500 episode reward: total was -21.050000. running mean: -28.962311\n",
      "epsilon:0.126096 episode_count: 11669. steps_count: 5060760.000000\n",
      "Time elapsed:  14551.657846927643\n",
      "ep 1667: ep_len:500 episode reward: total was 16.180000. running mean: -28.510888\n",
      "ep 1667: ep_len:604 episode reward: total was 38.740000. running mean: -27.838379\n",
      "ep 1667: ep_len:634 episode reward: total was -52.780000. running mean: -28.087795\n",
      "ep 1667: ep_len:122 episode reward: total was -11.050000. running mean: -27.917417\n",
      "ep 1667: ep_len:83 episode reward: total was -36.260000. running mean: -28.000843\n",
      "ep 1667: ep_len:660 episode reward: total was -31.730000. running mean: -28.038134\n",
      "ep 1667: ep_len:530 episode reward: total was -32.560000. running mean: -28.083353\n",
      "epsilon:0.126052 episode_count: 11676. steps_count: 5063893.000000\n",
      "Time elapsed:  14559.87632894516\n",
      "ep 1668: ep_len:602 episode reward: total was -82.540000. running mean: -28.627920\n",
      "ep 1668: ep_len:518 episode reward: total was -2.830000. running mean: -28.369940\n",
      "ep 1668: ep_len:356 episode reward: total was -25.810000. running mean: -28.344341\n",
      "ep 1668: ep_len:500 episode reward: total was -52.470000. running mean: -28.585598\n",
      "ep 1668: ep_len:3 episode reward: total was 0.000000. running mean: -28.299742\n",
      "ep 1668: ep_len:534 episode reward: total was -70.660000. running mean: -28.723344\n",
      "ep 1668: ep_len:512 episode reward: total was -61.810000. running mean: -29.054211\n",
      "epsilon:0.126008 episode_count: 11683. steps_count: 5066918.000000\n",
      "Time elapsed:  14567.34446978569\n",
      "ep 1669: ep_len:500 episode reward: total was -13.490000. running mean: -28.898569\n",
      "ep 1669: ep_len:627 episode reward: total was -48.490000. running mean: -29.094483\n",
      "ep 1669: ep_len:549 episode reward: total was -66.460000. running mean: -29.468138\n",
      "ep 1669: ep_len:118 episode reward: total was 6.500000. running mean: -29.108457\n",
      "ep 1669: ep_len:3 episode reward: total was -1.500000. running mean: -28.832372\n",
      "ep 1669: ep_len:606 episode reward: total was -65.530000. running mean: -29.199348\n",
      "ep 1669: ep_len:612 episode reward: total was -17.650000. running mean: -29.083855\n",
      "epsilon:0.125963 episode_count: 11690. steps_count: 5069933.000000\n",
      "Time elapsed:  14577.393070459366\n",
      "ep 1670: ep_len:501 episode reward: total was 39.810000. running mean: -28.394916\n",
      "ep 1670: ep_len:581 episode reward: total was -4.340000. running mean: -28.154367\n",
      "ep 1670: ep_len:383 episode reward: total was 7.110000. running mean: -27.801724\n",
      "ep 1670: ep_len:519 episode reward: total was -91.540000. running mean: -28.439106\n",
      "ep 1670: ep_len:115 episode reward: total was 0.790000. running mean: -28.146815\n",
      "ep 1670: ep_len:500 episode reward: total was -78.190000. running mean: -28.647247\n",
      "ep 1670: ep_len:513 episode reward: total was -42.800000. running mean: -28.788775\n",
      "epsilon:0.125919 episode_count: 11697. steps_count: 5073045.000000\n",
      "Time elapsed:  14585.79968881607\n",
      "ep 1671: ep_len:597 episode reward: total was 29.000000. running mean: -28.210887\n",
      "ep 1671: ep_len:271 episode reward: total was -69.920000. running mean: -28.627978\n",
      "ep 1671: ep_len:79 episode reward: total was -0.230000. running mean: -28.343998\n",
      "ep 1671: ep_len:500 episode reward: total was -11.280000. running mean: -28.173358\n",
      "ep 1671: ep_len:3 episode reward: total was 0.000000. running mean: -27.891625\n",
      "ep 1671: ep_len:628 episode reward: total was -71.430000. running mean: -28.327008\n",
      "ep 1671: ep_len:505 episode reward: total was -62.760000. running mean: -28.671338\n",
      "epsilon:0.125875 episode_count: 11704. steps_count: 5075628.000000\n",
      "Time elapsed:  14593.043994665146\n",
      "ep 1672: ep_len:229 episode reward: total was -3.810000. running mean: -28.422725\n",
      "ep 1672: ep_len:581 episode reward: total was -36.590000. running mean: -28.504398\n",
      "ep 1672: ep_len:594 episode reward: total was -110.940000. running mean: -29.328754\n",
      "ep 1672: ep_len:511 episode reward: total was -61.440000. running mean: -29.649866\n",
      "ep 1672: ep_len:3 episode reward: total was 1.010000. running mean: -29.343268\n",
      "ep 1672: ep_len:636 episode reward: total was -14.970000. running mean: -29.199535\n",
      "ep 1672: ep_len:500 episode reward: total was -50.770000. running mean: -29.415240\n",
      "epsilon:0.125830 episode_count: 11711. steps_count: 5078682.000000\n",
      "Time elapsed:  14603.313657045364\n",
      "ep 1673: ep_len:501 episode reward: total was -54.740000. running mean: -29.668487\n",
      "ep 1673: ep_len:500 episode reward: total was -95.690000. running mean: -30.328702\n",
      "ep 1673: ep_len:500 episode reward: total was -5.600000. running mean: -30.081415\n",
      "ep 1673: ep_len:608 episode reward: total was 13.690000. running mean: -29.643701\n",
      "ep 1673: ep_len:3 episode reward: total was 1.010000. running mean: -29.337164\n",
      "ep 1673: ep_len:500 episode reward: total was -49.930000. running mean: -29.543092\n",
      "ep 1673: ep_len:326 episode reward: total was -40.780000. running mean: -29.655462\n",
      "epsilon:0.125786 episode_count: 11718. steps_count: 5081620.000000\n",
      "Time elapsed:  14615.079371452332\n",
      "ep 1674: ep_len:607 episode reward: total was 26.130000. running mean: -29.097607\n",
      "ep 1674: ep_len:297 episode reward: total was -166.500000. running mean: -30.471631\n",
      "ep 1674: ep_len:500 episode reward: total was -60.960000. running mean: -30.776515\n",
      "ep 1674: ep_len:618 episode reward: total was -35.850000. running mean: -30.827249\n",
      "ep 1674: ep_len:3 episode reward: total was -1.500000. running mean: -30.533977\n",
      "ep 1674: ep_len:504 episode reward: total was -20.730000. running mean: -30.435937\n",
      "ep 1674: ep_len:506 episode reward: total was -44.070000. running mean: -30.572278\n",
      "epsilon:0.125742 episode_count: 11725. steps_count: 5084655.000000\n",
      "Time elapsed:  14623.40359210968\n",
      "ep 1675: ep_len:106 episode reward: total was -5.590000. running mean: -30.322455\n",
      "ep 1675: ep_len:500 episode reward: total was 11.040000. running mean: -29.908830\n",
      "ep 1675: ep_len:79 episode reward: total was -2.770000. running mean: -29.637442\n",
      "ep 1675: ep_len:132 episode reward: total was 12.560000. running mean: -29.215468\n",
      "ep 1675: ep_len:3 episode reward: total was 1.010000. running mean: -28.913213\n",
      "ep 1675: ep_len:249 episode reward: total was 10.840000. running mean: -28.515681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1675: ep_len:618 episode reward: total was -43.360000. running mean: -28.664124\n",
      "epsilon:0.125697 episode_count: 11732. steps_count: 5086342.000000\n",
      "Time elapsed:  14628.340980291367\n",
      "ep 1676: ep_len:580 episode reward: total was -102.040000. running mean: -29.397883\n",
      "ep 1676: ep_len:274 episode reward: total was -32.000000. running mean: -29.423904\n",
      "ep 1676: ep_len:614 episode reward: total was -53.190000. running mean: -29.661565\n",
      "ep 1676: ep_len:501 episode reward: total was -85.450000. running mean: -30.219449\n",
      "ep 1676: ep_len:3 episode reward: total was 1.010000. running mean: -29.907155\n",
      "ep 1676: ep_len:649 episode reward: total was -25.290000. running mean: -29.860983\n",
      "ep 1676: ep_len:514 episode reward: total was -91.300000. running mean: -30.475373\n",
      "epsilon:0.125653 episode_count: 11739. steps_count: 5089477.000000\n",
      "Time elapsed:  14636.05342054367\n",
      "ep 1677: ep_len:615 episode reward: total was -198.920000. running mean: -32.159820\n",
      "ep 1677: ep_len:274 episode reward: total was -49.460000. running mean: -32.332822\n",
      "ep 1677: ep_len:834 episode reward: total was -354.810000. running mean: -35.557593\n",
      "ep 1677: ep_len:418 episode reward: total was -22.060000. running mean: -35.422617\n",
      "ep 1677: ep_len:3 episode reward: total was -1.500000. running mean: -35.083391\n",
      "ep 1677: ep_len:576 episode reward: total was -20.040000. running mean: -34.932957\n",
      "ep 1677: ep_len:355 episode reward: total was -29.520000. running mean: -34.878828\n",
      "epsilon:0.125609 episode_count: 11746. steps_count: 5092552.000000\n",
      "Time elapsed:  14646.336383581161\n",
      "ep 1678: ep_len:228 episode reward: total was -40.460000. running mean: -34.934639\n",
      "ep 1678: ep_len:500 episode reward: total was -56.550000. running mean: -35.150793\n",
      "ep 1678: ep_len:500 episode reward: total was -76.000000. running mean: -35.559285\n",
      "ep 1678: ep_len:530 episode reward: total was -32.360000. running mean: -35.527292\n",
      "ep 1678: ep_len:3 episode reward: total was 1.010000. running mean: -35.161919\n",
      "ep 1678: ep_len:525 episode reward: total was -30.480000. running mean: -35.115100\n",
      "ep 1678: ep_len:558 episode reward: total was -14.580000. running mean: -34.909749\n",
      "epsilon:0.125564 episode_count: 11753. steps_count: 5095396.000000\n",
      "Time elapsed:  14653.809042930603\n",
      "ep 1679: ep_len:550 episode reward: total was -42.560000. running mean: -34.986252\n",
      "ep 1679: ep_len:174 episode reward: total was -25.780000. running mean: -34.894189\n",
      "ep 1679: ep_len:553 episode reward: total was -18.690000. running mean: -34.732147\n",
      "ep 1679: ep_len:500 episode reward: total was 10.770000. running mean: -34.277126\n",
      "ep 1679: ep_len:3 episode reward: total was 1.010000. running mean: -33.924255\n",
      "ep 1679: ep_len:500 episode reward: total was -15.040000. running mean: -33.735412\n",
      "ep 1679: ep_len:529 episode reward: total was -93.650000. running mean: -34.334558\n",
      "epsilon:0.125520 episode_count: 11760. steps_count: 5098205.000000\n",
      "Time elapsed:  14663.193345546722\n",
      "ep 1680: ep_len:624 episode reward: total was -71.150000. running mean: -34.702712\n",
      "ep 1680: ep_len:658 episode reward: total was -9.400000. running mean: -34.449685\n",
      "ep 1680: ep_len:627 episode reward: total was -51.820000. running mean: -34.623388\n",
      "ep 1680: ep_len:114 episode reward: total was 3.530000. running mean: -34.241854\n",
      "ep 1680: ep_len:3 episode reward: total was 0.000000. running mean: -33.899436\n",
      "ep 1680: ep_len:500 episode reward: total was 19.710000. running mean: -33.363342\n",
      "ep 1680: ep_len:291 episode reward: total was -31.750000. running mean: -33.347208\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.125476 episode_count: 11767. steps_count: 5101022.000000\n",
      "Time elapsed:  14678.510460138321\n",
      "ep 1681: ep_len:500 episode reward: total was -21.050000. running mean: -33.224236\n",
      "ep 1681: ep_len:500 episode reward: total was 0.550000. running mean: -32.886494\n",
      "ep 1681: ep_len:385 episode reward: total was 16.740000. running mean: -32.390229\n",
      "ep 1681: ep_len:587 episode reward: total was 29.630000. running mean: -31.770026\n",
      "ep 1681: ep_len:3 episode reward: total was 1.010000. running mean: -31.442226\n",
      "ep 1681: ep_len:592 episode reward: total was -24.810000. running mean: -31.375904\n",
      "ep 1681: ep_len:330 episode reward: total was -5.040000. running mean: -31.112545\n",
      "epsilon:0.125431 episode_count: 11774. steps_count: 5103919.000000\n",
      "Time elapsed:  14688.053725719452\n",
      "ep 1682: ep_len:588 episode reward: total was -26.200000. running mean: -31.063419\n",
      "ep 1682: ep_len:565 episode reward: total was -71.630000. running mean: -31.469085\n",
      "ep 1682: ep_len:500 episode reward: total was -19.300000. running mean: -31.347394\n",
      "ep 1682: ep_len:535 episode reward: total was -13.470000. running mean: -31.168620\n",
      "ep 1682: ep_len:78 episode reward: total was -11.720000. running mean: -30.974134\n",
      "ep 1682: ep_len:500 episode reward: total was 17.350000. running mean: -30.490893\n",
      "ep 1682: ep_len:569 episode reward: total was -49.850000. running mean: -30.684484\n",
      "epsilon:0.125387 episode_count: 11781. steps_count: 5107254.000000\n",
      "Time elapsed:  14696.86136007309\n",
      "ep 1683: ep_len:500 episode reward: total was 10.110000. running mean: -30.276539\n",
      "ep 1683: ep_len:609 episode reward: total was -17.730000. running mean: -30.151074\n",
      "ep 1683: ep_len:616 episode reward: total was -64.780000. running mean: -30.497363\n",
      "ep 1683: ep_len:527 episode reward: total was -43.690000. running mean: -30.629289\n",
      "ep 1683: ep_len:3 episode reward: total was 1.010000. running mean: -30.312896\n",
      "ep 1683: ep_len:500 episode reward: total was -13.180000. running mean: -30.141567\n",
      "ep 1683: ep_len:500 episode reward: total was -30.280000. running mean: -30.142952\n",
      "epsilon:0.125343 episode_count: 11788. steps_count: 5110509.000000\n",
      "Time elapsed:  14705.403616189957\n",
      "ep 1684: ep_len:571 episode reward: total was -8.430000. running mean: -29.925822\n",
      "ep 1684: ep_len:500 episode reward: total was -33.480000. running mean: -29.961364\n",
      "ep 1684: ep_len:553 episode reward: total was -2.470000. running mean: -29.686450\n",
      "ep 1684: ep_len:550 episode reward: total was 23.820000. running mean: -29.151386\n",
      "ep 1684: ep_len:88 episode reward: total was 8.220000. running mean: -28.777672\n",
      "ep 1684: ep_len:656 episode reward: total was -50.100000. running mean: -28.990895\n",
      "ep 1684: ep_len:532 episode reward: total was -42.820000. running mean: -29.129186\n",
      "epsilon:0.125298 episode_count: 11795. steps_count: 5113959.000000\n",
      "Time elapsed:  14716.479169607162\n",
      "ep 1685: ep_len:501 episode reward: total was 21.350000. running mean: -28.624395\n",
      "ep 1685: ep_len:272 episode reward: total was -15.900000. running mean: -28.497151\n",
      "ep 1685: ep_len:454 episode reward: total was 14.110000. running mean: -28.071079\n",
      "ep 1685: ep_len:516 episode reward: total was 4.150000. running mean: -27.748868\n",
      "ep 1685: ep_len:3 episode reward: total was 0.000000. running mean: -27.471380\n",
      "ep 1685: ep_len:306 episode reward: total was -3.660000. running mean: -27.233266\n",
      "ep 1685: ep_len:521 episode reward: total was -29.260000. running mean: -27.253533\n",
      "epsilon:0.125254 episode_count: 11802. steps_count: 5116532.000000\n",
      "Time elapsed:  14722.978375196457\n",
      "ep 1686: ep_len:545 episode reward: total was -124.830000. running mean: -28.229298\n",
      "ep 1686: ep_len:500 episode reward: total was 25.930000. running mean: -27.687705\n",
      "ep 1686: ep_len:525 episode reward: total was -66.890000. running mean: -28.079728\n",
      "ep 1686: ep_len:144 episode reward: total was -1.450000. running mean: -27.813431\n",
      "ep 1686: ep_len:56 episode reward: total was 16.160000. running mean: -27.373696\n",
      "ep 1686: ep_len:601 episode reward: total was -26.370000. running mean: -27.363659\n",
      "ep 1686: ep_len:178 episode reward: total was -12.780000. running mean: -27.217823\n",
      "epsilon:0.125210 episode_count: 11809. steps_count: 5119081.000000\n",
      "Time elapsed:  14729.955001354218\n",
      "ep 1687: ep_len:642 episode reward: total was -9.050000. running mean: -27.036144\n",
      "ep 1687: ep_len:501 episode reward: total was -30.270000. running mean: -27.068483\n",
      "ep 1687: ep_len:582 episode reward: total was -35.750000. running mean: -27.155298\n",
      "ep 1687: ep_len:132 episode reward: total was 8.550000. running mean: -26.798245\n",
      "ep 1687: ep_len:3 episode reward: total was 1.010000. running mean: -26.520163\n",
      "ep 1687: ep_len:628 episode reward: total was -81.410000. running mean: -27.069061\n",
      "ep 1687: ep_len:617 episode reward: total was -80.620000. running mean: -27.604570\n",
      "epsilon:0.125165 episode_count: 11816. steps_count: 5122186.000000\n",
      "Time elapsed:  14740.34008860588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1688: ep_len:240 episode reward: total was -21.750000. running mean: -27.546025\n",
      "ep 1688: ep_len:537 episode reward: total was -24.010000. running mean: -27.510665\n",
      "ep 1688: ep_len:579 episode reward: total was -47.690000. running mean: -27.712458\n",
      "ep 1688: ep_len:617 episode reward: total was -29.460000. running mean: -27.729933\n",
      "ep 1688: ep_len:3 episode reward: total was 1.010000. running mean: -27.442534\n",
      "ep 1688: ep_len:336 episode reward: total was -73.310000. running mean: -27.901209\n",
      "ep 1688: ep_len:509 episode reward: total was -93.100000. running mean: -28.553197\n",
      "epsilon:0.125121 episode_count: 11823. steps_count: 5125007.000000\n",
      "Time elapsed:  14747.989957809448\n",
      "ep 1689: ep_len:638 episode reward: total was -8.850000. running mean: -28.356165\n",
      "ep 1689: ep_len:501 episode reward: total was -35.410000. running mean: -28.426703\n",
      "ep 1689: ep_len:511 episode reward: total was -109.380000. running mean: -29.236236\n",
      "ep 1689: ep_len:153 episode reward: total was 14.060000. running mean: -28.803274\n",
      "ep 1689: ep_len:3 episode reward: total was 0.000000. running mean: -28.515241\n",
      "ep 1689: ep_len:505 episode reward: total was -79.490000. running mean: -29.024988\n",
      "ep 1689: ep_len:625 episode reward: total was -19.650000. running mean: -28.931239\n",
      "epsilon:0.125077 episode_count: 11830. steps_count: 5127943.000000\n",
      "Time elapsed:  14758.13157916069\n",
      "ep 1690: ep_len:511 episode reward: total was -17.770000. running mean: -28.819626\n",
      "ep 1690: ep_len:500 episode reward: total was -60.340000. running mean: -29.134830\n",
      "ep 1690: ep_len:599 episode reward: total was -35.540000. running mean: -29.198882\n",
      "ep 1690: ep_len:521 episode reward: total was -16.600000. running mean: -29.072893\n",
      "ep 1690: ep_len:56 episode reward: total was 8.500000. running mean: -28.697164\n",
      "ep 1690: ep_len:500 episode reward: total was -106.050000. running mean: -29.470692\n",
      "ep 1690: ep_len:586 episode reward: total was -87.740000. running mean: -30.053385\n",
      "epsilon:0.125032 episode_count: 11837. steps_count: 5131216.000000\n",
      "Time elapsed:  14767.058489084244\n",
      "ep 1691: ep_len:608 episode reward: total was -6.700000. running mean: -29.819851\n",
      "ep 1691: ep_len:500 episode reward: total was -20.720000. running mean: -29.728853\n",
      "ep 1691: ep_len:430 episode reward: total was 9.490000. running mean: -29.336664\n",
      "ep 1691: ep_len:500 episode reward: total was -34.680000. running mean: -29.390098\n",
      "ep 1691: ep_len:3 episode reward: total was 1.010000. running mean: -29.086097\n",
      "ep 1691: ep_len:539 episode reward: total was -33.950000. running mean: -29.134736\n",
      "ep 1691: ep_len:592 episode reward: total was -1.050000. running mean: -28.853888\n",
      "epsilon:0.124988 episode_count: 11844. steps_count: 5134388.000000\n",
      "Time elapsed:  14775.32093334198\n",
      "ep 1692: ep_len:589 episode reward: total was -68.100000. running mean: -29.246350\n",
      "ep 1692: ep_len:536 episode reward: total was 22.430000. running mean: -28.729586\n",
      "ep 1692: ep_len:545 episode reward: total was -22.660000. running mean: -28.668890\n",
      "ep 1692: ep_len:56 episode reward: total was 0.790000. running mean: -28.374301\n",
      "ep 1692: ep_len:131 episode reward: total was 12.820000. running mean: -27.962358\n",
      "ep 1692: ep_len:517 episode reward: total was -45.340000. running mean: -28.136135\n",
      "ep 1692: ep_len:575 episode reward: total was -41.750000. running mean: -28.272273\n",
      "epsilon:0.124944 episode_count: 11851. steps_count: 5137337.000000\n",
      "Time elapsed:  14783.13704419136\n",
      "ep 1693: ep_len:654 episode reward: total was -4.430000. running mean: -28.033851\n",
      "ep 1693: ep_len:500 episode reward: total was -179.790000. running mean: -29.551412\n",
      "ep 1693: ep_len:500 episode reward: total was -39.290000. running mean: -29.648798\n",
      "ep 1693: ep_len:569 episode reward: total was -26.840000. running mean: -29.620710\n",
      "ep 1693: ep_len:3 episode reward: total was 1.010000. running mean: -29.314403\n",
      "ep 1693: ep_len:158 episode reward: total was -5.580000. running mean: -29.077059\n",
      "ep 1693: ep_len:334 episode reward: total was 5.560000. running mean: -28.730688\n",
      "epsilon:0.124899 episode_count: 11858. steps_count: 5140055.000000\n",
      "Time elapsed:  14790.208844423294\n",
      "ep 1694: ep_len:501 episode reward: total was 31.240000. running mean: -28.130981\n",
      "ep 1694: ep_len:571 episode reward: total was 12.950000. running mean: -27.720172\n",
      "ep 1694: ep_len:500 episode reward: total was -62.700000. running mean: -28.069970\n",
      "ep 1694: ep_len:621 episode reward: total was 3.040000. running mean: -27.758870\n",
      "ep 1694: ep_len:3 episode reward: total was 0.000000. running mean: -27.481281\n",
      "ep 1694: ep_len:612 episode reward: total was -46.660000. running mean: -27.673069\n",
      "ep 1694: ep_len:500 episode reward: total was -9.690000. running mean: -27.493238\n",
      "epsilon:0.124855 episode_count: 11865. steps_count: 5143363.000000\n",
      "Time elapsed:  14796.534790992737\n",
      "ep 1695: ep_len:504 episode reward: total was 28.670000. running mean: -26.931606\n",
      "ep 1695: ep_len:501 episode reward: total was -72.990000. running mean: -27.392190\n",
      "ep 1695: ep_len:576 episode reward: total was -69.530000. running mean: -27.813568\n",
      "ep 1695: ep_len:500 episode reward: total was -22.090000. running mean: -27.756332\n",
      "ep 1695: ep_len:55 episode reward: total was 19.510000. running mean: -27.283669\n",
      "ep 1695: ep_len:503 episode reward: total was -15.950000. running mean: -27.170332\n",
      "ep 1695: ep_len:255 episode reward: total was -20.410000. running mean: -27.102729\n",
      "epsilon:0.124811 episode_count: 11872. steps_count: 5146257.000000\n",
      "Time elapsed:  14803.94465970993\n",
      "ep 1696: ep_len:607 episode reward: total was -12.120000. running mean: -26.952901\n",
      "ep 1696: ep_len:503 episode reward: total was -7.870000. running mean: -26.762072\n",
      "ep 1696: ep_len:500 episode reward: total was -2.590000. running mean: -26.520352\n",
      "ep 1696: ep_len:520 episode reward: total was -2.390000. running mean: -26.279048\n",
      "ep 1696: ep_len:89 episode reward: total was -48.290000. running mean: -26.499158\n",
      "ep 1696: ep_len:544 episode reward: total was 18.270000. running mean: -26.051466\n",
      "ep 1696: ep_len:585 episode reward: total was -56.340000. running mean: -26.354351\n",
      "epsilon:0.124766 episode_count: 11879. steps_count: 5149605.000000\n",
      "Time elapsed:  14814.694973230362\n",
      "ep 1697: ep_len:501 episode reward: total was -5.060000. running mean: -26.141408\n",
      "ep 1697: ep_len:510 episode reward: total was -30.990000. running mean: -26.189894\n",
      "ep 1697: ep_len:531 episode reward: total was -63.510000. running mean: -26.563095\n",
      "ep 1697: ep_len:574 episode reward: total was 7.690000. running mean: -26.220564\n",
      "ep 1697: ep_len:3 episode reward: total was 1.010000. running mean: -25.948258\n",
      "ep 1697: ep_len:221 episode reward: total was 23.780000. running mean: -25.450976\n",
      "ep 1697: ep_len:533 episode reward: total was -63.190000. running mean: -25.828366\n",
      "epsilon:0.124722 episode_count: 11886. steps_count: 5152478.000000\n",
      "Time elapsed:  14821.402315616608\n",
      "ep 1698: ep_len:118 episode reward: total was 1.390000. running mean: -25.556182\n",
      "ep 1698: ep_len:500 episode reward: total was -50.890000. running mean: -25.809520\n",
      "ep 1698: ep_len:569 episode reward: total was -101.010000. running mean: -26.561525\n",
      "ep 1698: ep_len:598 episode reward: total was -52.070000. running mean: -26.816610\n",
      "ep 1698: ep_len:89 episode reward: total was 16.220000. running mean: -26.386244\n",
      "ep 1698: ep_len:621 episode reward: total was -45.590000. running mean: -26.578281\n",
      "ep 1698: ep_len:515 episode reward: total was -53.500000. running mean: -26.847499\n",
      "epsilon:0.124678 episode_count: 11893. steps_count: 5155488.000000\n",
      "Time elapsed:  14831.118088960648\n",
      "ep 1699: ep_len:521 episode reward: total was 38.490000. running mean: -26.194124\n",
      "ep 1699: ep_len:186 episode reward: total was -28.280000. running mean: -26.214982\n",
      "ep 1699: ep_len:572 episode reward: total was -55.440000. running mean: -26.507233\n",
      "ep 1699: ep_len:597 episode reward: total was 1.580000. running mean: -26.226360\n",
      "ep 1699: ep_len:105 episode reward: total was -32.260000. running mean: -26.286697\n",
      "ep 1699: ep_len:504 episode reward: total was -59.370000. running mean: -26.617530\n",
      "ep 1699: ep_len:533 episode reward: total was -46.540000. running mean: -26.816754\n",
      "epsilon:0.124633 episode_count: 11900. steps_count: 5158506.000000\n",
      "Time elapsed:  14841.306656360626\n",
      "ep 1700: ep_len:575 episode reward: total was 49.110000. running mean: -26.057487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1700: ep_len:501 episode reward: total was -12.060000. running mean: -25.917512\n",
      "ep 1700: ep_len:500 episode reward: total was -18.450000. running mean: -25.842837\n",
      "ep 1700: ep_len:508 episode reward: total was 11.100000. running mean: -25.473408\n",
      "ep 1700: ep_len:3 episode reward: total was 1.010000. running mean: -25.208574\n",
      "ep 1700: ep_len:683 episode reward: total was -37.470000. running mean: -25.331189\n",
      "ep 1700: ep_len:567 episode reward: total was -36.750000. running mean: -25.445377\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.124589 episode_count: 11907. steps_count: 5161843.000000\n",
      "Time elapsed:  14854.908212900162\n",
      "ep 1701: ep_len:642 episode reward: total was -85.360000. running mean: -26.044523\n",
      "ep 1701: ep_len:500 episode reward: total was -26.810000. running mean: -26.052178\n",
      "ep 1701: ep_len:364 episode reward: total was 32.350000. running mean: -25.468156\n",
      "ep 1701: ep_len:511 episode reward: total was 23.740000. running mean: -24.976074\n",
      "ep 1701: ep_len:3 episode reward: total was 1.010000. running mean: -24.716214\n",
      "ep 1701: ep_len:500 episode reward: total was -64.370000. running mean: -25.112752\n",
      "ep 1701: ep_len:531 episode reward: total was -6.360000. running mean: -24.925224\n",
      "epsilon:0.124545 episode_count: 11914. steps_count: 5164894.000000\n",
      "Time elapsed:  14864.820810079575\n",
      "ep 1702: ep_len:675 episode reward: total was -56.750000. running mean: -25.243472\n",
      "ep 1702: ep_len:500 episode reward: total was -27.520000. running mean: -25.266237\n",
      "ep 1702: ep_len:500 episode reward: total was -21.010000. running mean: -25.223675\n",
      "ep 1702: ep_len:132 episode reward: total was 11.060000. running mean: -24.860838\n",
      "ep 1702: ep_len:102 episode reward: total was 21.650000. running mean: -24.395730\n",
      "ep 1702: ep_len:522 episode reward: total was -63.770000. running mean: -24.789472\n",
      "ep 1702: ep_len:500 episode reward: total was -88.800000. running mean: -25.429578\n",
      "epsilon:0.124500 episode_count: 11921. steps_count: 5167825.000000\n",
      "Time elapsed:  14872.6620926857\n",
      "ep 1703: ep_len:575 episode reward: total was -83.420000. running mean: -26.009482\n",
      "ep 1703: ep_len:628 episode reward: total was -32.090000. running mean: -26.070287\n",
      "ep 1703: ep_len:616 episode reward: total was -17.070000. running mean: -25.980284\n",
      "ep 1703: ep_len:592 episode reward: total was 17.750000. running mean: -25.542981\n",
      "ep 1703: ep_len:103 episode reward: total was 16.250000. running mean: -25.125051\n",
      "ep 1703: ep_len:501 episode reward: total was -41.940000. running mean: -25.293201\n",
      "ep 1703: ep_len:516 episode reward: total was -43.010000. running mean: -25.470369\n",
      "epsilon:0.124456 episode_count: 11928. steps_count: 5171356.000000\n",
      "Time elapsed:  14881.819934129715\n",
      "ep 1704: ep_len:265 episode reward: total was 2.230000. running mean: -25.193365\n",
      "ep 1704: ep_len:529 episode reward: total was 12.560000. running mean: -24.815832\n",
      "ep 1704: ep_len:500 episode reward: total was -37.310000. running mean: -24.940773\n",
      "ep 1704: ep_len:577 episode reward: total was -17.600000. running mean: -24.867366\n",
      "ep 1704: ep_len:90 episode reward: total was -53.230000. running mean: -25.150992\n",
      "ep 1704: ep_len:630 episode reward: total was -102.990000. running mean: -25.929382\n",
      "ep 1704: ep_len:210 episode reward: total was -14.050000. running mean: -25.810588\n",
      "epsilon:0.124412 episode_count: 11935. steps_count: 5174157.000000\n",
      "Time elapsed:  14889.637478351593\n",
      "ep 1705: ep_len:601 episode reward: total was -23.680000. running mean: -25.789282\n",
      "ep 1705: ep_len:605 episode reward: total was -44.320000. running mean: -25.974589\n",
      "ep 1705: ep_len:643 episode reward: total was -21.280000. running mean: -25.927644\n",
      "ep 1705: ep_len:540 episode reward: total was -49.050000. running mean: -26.158867\n",
      "ep 1705: ep_len:3 episode reward: total was 0.000000. running mean: -25.897278\n",
      "ep 1705: ep_len:179 episode reward: total was 20.100000. running mean: -25.437306\n",
      "ep 1705: ep_len:604 episode reward: total was -26.080000. running mean: -25.443733\n",
      "epsilon:0.124367 episode_count: 11942. steps_count: 5177332.000000\n",
      "Time elapsed:  14897.810274600983\n",
      "ep 1706: ep_len:541 episode reward: total was -15.320000. running mean: -25.342495\n",
      "ep 1706: ep_len:502 episode reward: total was -59.240000. running mean: -25.681470\n",
      "ep 1706: ep_len:577 episode reward: total was -64.660000. running mean: -26.071256\n",
      "ep 1706: ep_len:500 episode reward: total was -1.930000. running mean: -25.829843\n",
      "ep 1706: ep_len:3 episode reward: total was 0.000000. running mean: -25.571545\n",
      "ep 1706: ep_len:170 episode reward: total was 11.070000. running mean: -25.205129\n",
      "ep 1706: ep_len:181 episode reward: total was -22.820000. running mean: -25.181278\n",
      "epsilon:0.124323 episode_count: 11949. steps_count: 5179806.000000\n",
      "Time elapsed:  14904.654672145844\n",
      "ep 1707: ep_len:647 episode reward: total was -22.100000. running mean: -25.150465\n",
      "ep 1707: ep_len:356 episode reward: total was -66.700000. running mean: -25.565960\n",
      "ep 1707: ep_len:500 episode reward: total was -22.820000. running mean: -25.538501\n",
      "ep 1707: ep_len:154 episode reward: total was 2.070000. running mean: -25.262416\n",
      "ep 1707: ep_len:108 episode reward: total was 19.750000. running mean: -24.812292\n",
      "ep 1707: ep_len:630 episode reward: total was 9.070000. running mean: -24.473469\n",
      "ep 1707: ep_len:561 episode reward: total was -65.220000. running mean: -24.880934\n",
      "epsilon:0.124279 episode_count: 11956. steps_count: 5182762.000000\n",
      "Time elapsed:  14914.859582424164\n",
      "ep 1708: ep_len:500 episode reward: total was 54.460000. running mean: -24.087525\n",
      "ep 1708: ep_len:631 episode reward: total was -11.350000. running mean: -23.960149\n",
      "ep 1708: ep_len:563 episode reward: total was -46.660000. running mean: -24.187148\n",
      "ep 1708: ep_len:535 episode reward: total was -111.300000. running mean: -25.058277\n",
      "ep 1708: ep_len:3 episode reward: total was 0.000000. running mean: -24.807694\n",
      "ep 1708: ep_len:679 episode reward: total was -68.390000. running mean: -25.243517\n",
      "ep 1708: ep_len:561 episode reward: total was -65.910000. running mean: -25.650182\n",
      "epsilon:0.124234 episode_count: 11963. steps_count: 5186234.000000\n",
      "Time elapsed:  14924.148219108582\n",
      "ep 1709: ep_len:132 episode reward: total was -6.090000. running mean: -25.454580\n",
      "ep 1709: ep_len:532 episode reward: total was -40.810000. running mean: -25.608134\n",
      "ep 1709: ep_len:382 episode reward: total was 6.390000. running mean: -25.288153\n",
      "ep 1709: ep_len:411 episode reward: total was -14.080000. running mean: -25.176071\n",
      "ep 1709: ep_len:101 episode reward: total was 15.730000. running mean: -24.767010\n",
      "ep 1709: ep_len:502 episode reward: total was -57.910000. running mean: -25.098440\n",
      "ep 1709: ep_len:533 episode reward: total was -196.220000. running mean: -26.809656\n",
      "epsilon:0.124190 episode_count: 11970. steps_count: 5188827.000000\n",
      "Time elapsed:  14931.258824110031\n",
      "ep 1710: ep_len:636 episode reward: total was -57.230000. running mean: -27.113859\n",
      "ep 1710: ep_len:618 episode reward: total was -14.030000. running mean: -26.983021\n",
      "ep 1710: ep_len:500 episode reward: total was -21.590000. running mean: -26.929091\n",
      "ep 1710: ep_len:56 episode reward: total was 1.340000. running mean: -26.646400\n",
      "ep 1710: ep_len:30 episode reward: total was 10.500000. running mean: -26.274936\n",
      "ep 1710: ep_len:600 episode reward: total was -45.460000. running mean: -26.466786\n",
      "ep 1710: ep_len:503 episode reward: total was -26.090000. running mean: -26.463018\n",
      "epsilon:0.124146 episode_count: 11977. steps_count: 5191770.000000\n",
      "Time elapsed:  14939.11700463295\n",
      "ep 1711: ep_len:661 episode reward: total was -93.220000. running mean: -27.130588\n",
      "ep 1711: ep_len:500 episode reward: total was 12.960000. running mean: -26.729682\n",
      "ep 1711: ep_len:359 episode reward: total was 1.320000. running mean: -26.449186\n",
      "ep 1711: ep_len:500 episode reward: total was 17.050000. running mean: -26.014194\n",
      "ep 1711: ep_len:3 episode reward: total was 1.010000. running mean: -25.743952\n",
      "ep 1711: ep_len:616 episode reward: total was -114.430000. running mean: -26.630812\n",
      "ep 1711: ep_len:529 episode reward: total was -70.380000. running mean: -27.068304\n",
      "epsilon:0.124101 episode_count: 11984. steps_count: 5194938.000000\n",
      "Time elapsed:  14947.317299127579\n",
      "ep 1712: ep_len:601 episode reward: total was 41.150000. running mean: -26.386121\n",
      "ep 1712: ep_len:500 episode reward: total was 39.670000. running mean: -25.725560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1712: ep_len:79 episode reward: total was 0.780000. running mean: -25.460504\n",
      "ep 1712: ep_len:504 episode reward: total was -78.170000. running mean: -25.987599\n",
      "ep 1712: ep_len:3 episode reward: total was 1.010000. running mean: -25.717623\n",
      "ep 1712: ep_len:536 episode reward: total was -28.810000. running mean: -25.748547\n",
      "ep 1712: ep_len:595 episode reward: total was -31.290000. running mean: -25.803962\n",
      "epsilon:0.124057 episode_count: 11991. steps_count: 5197756.000000\n",
      "Time elapsed:  14954.95667886734\n",
      "ep 1713: ep_len:516 episode reward: total was -46.920000. running mean: -26.015122\n",
      "ep 1713: ep_len:173 episode reward: total was 13.250000. running mean: -25.622471\n",
      "ep 1713: ep_len:508 episode reward: total was -103.500000. running mean: -26.401246\n",
      "ep 1713: ep_len:500 episode reward: total was 18.230000. running mean: -25.954934\n",
      "ep 1713: ep_len:79 episode reward: total was -2.770000. running mean: -25.723084\n",
      "ep 1713: ep_len:598 episode reward: total was -20.100000. running mean: -25.666853\n",
      "ep 1713: ep_len:205 episode reward: total was -22.120000. running mean: -25.631385\n",
      "epsilon:0.124013 episode_count: 11998. steps_count: 5200335.000000\n",
      "Time elapsed:  14961.908739566803\n",
      "ep 1714: ep_len:501 episode reward: total was -86.960000. running mean: -26.244671\n",
      "ep 1714: ep_len:553 episode reward: total was 51.100000. running mean: -25.471224\n",
      "ep 1714: ep_len:630 episode reward: total was -57.380000. running mean: -25.790312\n",
      "ep 1714: ep_len:516 episode reward: total was -62.960000. running mean: -26.162009\n",
      "ep 1714: ep_len:98 episode reward: total was -15.250000. running mean: -26.052889\n",
      "ep 1714: ep_len:501 episode reward: total was -71.220000. running mean: -26.504560\n",
      "ep 1714: ep_len:548 episode reward: total was -48.770000. running mean: -26.727214\n",
      "epsilon:0.123968 episode_count: 12005. steps_count: 5203682.000000\n",
      "Time elapsed:  14970.719656944275\n",
      "ep 1715: ep_len:526 episode reward: total was -72.180000. running mean: -27.181742\n",
      "ep 1715: ep_len:501 episode reward: total was -39.200000. running mean: -27.301925\n",
      "ep 1715: ep_len:500 episode reward: total was -1.070000. running mean: -27.039606\n",
      "ep 1715: ep_len:56 episode reward: total was -0.680000. running mean: -26.776009\n",
      "ep 1715: ep_len:3 episode reward: total was 1.010000. running mean: -26.498149\n",
      "ep 1715: ep_len:503 episode reward: total was -19.210000. running mean: -26.425268\n",
      "ep 1715: ep_len:619 episode reward: total was -48.640000. running mean: -26.647415\n",
      "epsilon:0.123924 episode_count: 12012. steps_count: 5206390.000000\n",
      "Time elapsed:  14977.924786806107\n",
      "ep 1716: ep_len:500 episode reward: total was 7.050000. running mean: -26.310441\n",
      "ep 1716: ep_len:517 episode reward: total was -67.280000. running mean: -26.720137\n",
      "ep 1716: ep_len:596 episode reward: total was -65.590000. running mean: -27.108835\n",
      "ep 1716: ep_len:504 episode reward: total was -103.840000. running mean: -27.876147\n",
      "ep 1716: ep_len:3 episode reward: total was 0.000000. running mean: -27.597385\n",
      "ep 1716: ep_len:564 episode reward: total was -89.980000. running mean: -28.221212\n",
      "ep 1716: ep_len:542 episode reward: total was 3.330000. running mean: -27.905699\n",
      "epsilon:0.123880 episode_count: 12019. steps_count: 5209616.000000\n",
      "Time elapsed:  14989.911820173264\n",
      "ep 1717: ep_len:500 episode reward: total was -20.940000. running mean: -27.836042\n",
      "ep 1717: ep_len:567 episode reward: total was -132.130000. running mean: -28.878982\n",
      "ep 1717: ep_len:592 episode reward: total was -41.980000. running mean: -29.009992\n",
      "ep 1717: ep_len:510 episode reward: total was -160.990000. running mean: -30.329792\n",
      "ep 1717: ep_len:3 episode reward: total was 1.010000. running mean: -30.016394\n",
      "ep 1717: ep_len:506 episode reward: total was -16.480000. running mean: -29.881030\n",
      "ep 1717: ep_len:350 episode reward: total was -4.010000. running mean: -29.622320\n",
      "epsilon:0.123835 episode_count: 12026. steps_count: 5212644.000000\n",
      "Time elapsed:  14997.95397400856\n",
      "ep 1718: ep_len:539 episode reward: total was -77.000000. running mean: -30.096097\n",
      "ep 1718: ep_len:115 episode reward: total was -8.550000. running mean: -29.880636\n",
      "ep 1718: ep_len:559 episode reward: total was -102.310000. running mean: -30.604930\n",
      "ep 1718: ep_len:501 episode reward: total was -44.560000. running mean: -30.744480\n",
      "ep 1718: ep_len:3 episode reward: total was 0.000000. running mean: -30.437036\n",
      "ep 1718: ep_len:553 episode reward: total was -11.940000. running mean: -30.252065\n",
      "ep 1718: ep_len:500 episode reward: total was -28.760000. running mean: -30.237145\n",
      "epsilon:0.123791 episode_count: 12033. steps_count: 5215414.000000\n",
      "Time elapsed:  15004.803530693054\n",
      "ep 1719: ep_len:227 episode reward: total was -0.850000. running mean: -29.943273\n",
      "ep 1719: ep_len:565 episode reward: total was -64.410000. running mean: -30.287940\n",
      "ep 1719: ep_len:530 episode reward: total was -70.020000. running mean: -30.685261\n",
      "ep 1719: ep_len:517 episode reward: total was 28.950000. running mean: -30.088908\n",
      "ep 1719: ep_len:3 episode reward: total was 0.000000. running mean: -29.788019\n",
      "ep 1719: ep_len:628 episode reward: total was -99.420000. running mean: -30.484339\n",
      "ep 1719: ep_len:500 episode reward: total was -69.930000. running mean: -30.878796\n",
      "epsilon:0.123747 episode_count: 12040. steps_count: 5218384.000000\n",
      "Time elapsed:  15014.623171329498\n",
      "ep 1720: ep_len:500 episode reward: total was 2.490000. running mean: -30.545108\n",
      "ep 1720: ep_len:665 episode reward: total was -2.290000. running mean: -30.262557\n",
      "ep 1720: ep_len:528 episode reward: total was -65.770000. running mean: -30.617631\n",
      "ep 1720: ep_len:507 episode reward: total was 13.270000. running mean: -30.178755\n",
      "ep 1720: ep_len:3 episode reward: total was 1.010000. running mean: -29.866867\n",
      "ep 1720: ep_len:542 episode reward: total was -59.190000. running mean: -30.160099\n",
      "ep 1720: ep_len:563 episode reward: total was -99.020000. running mean: -30.848698\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.123702 episode_count: 12047. steps_count: 5221692.000000\n",
      "Time elapsed:  15028.424613952637\n",
      "ep 1721: ep_len:590 episode reward: total was -0.410000. running mean: -30.544311\n",
      "ep 1721: ep_len:639 episode reward: total was 1.650000. running mean: -30.222367\n",
      "ep 1721: ep_len:596 episode reward: total was -52.660000. running mean: -30.446744\n",
      "ep 1721: ep_len:507 episode reward: total was 3.430000. running mean: -30.107976\n",
      "ep 1721: ep_len:3 episode reward: total was 1.010000. running mean: -29.796797\n",
      "ep 1721: ep_len:500 episode reward: total was -101.890000. running mean: -30.517729\n",
      "ep 1721: ep_len:518 episode reward: total was -52.890000. running mean: -30.741451\n",
      "epsilon:0.123658 episode_count: 12054. steps_count: 5225045.000000\n",
      "Time elapsed:  15037.197661399841\n",
      "ep 1722: ep_len:598 episode reward: total was -251.150000. running mean: -32.945537\n",
      "ep 1722: ep_len:500 episode reward: total was -48.430000. running mean: -33.100381\n",
      "ep 1722: ep_len:500 episode reward: total was -61.440000. running mean: -33.383778\n",
      "ep 1722: ep_len:544 episode reward: total was -52.780000. running mean: -33.577740\n",
      "ep 1722: ep_len:3 episode reward: total was 0.000000. running mean: -33.241962\n",
      "ep 1722: ep_len:663 episode reward: total was -66.810000. running mean: -33.577643\n",
      "ep 1722: ep_len:202 episode reward: total was -31.210000. running mean: -33.553966\n",
      "epsilon:0.123614 episode_count: 12061. steps_count: 5228055.000000\n",
      "Time elapsed:  15045.10584115982\n",
      "ep 1723: ep_len:557 episode reward: total was -90.030000. running mean: -34.118727\n",
      "ep 1723: ep_len:500 episode reward: total was -37.880000. running mean: -34.156339\n",
      "ep 1723: ep_len:516 episode reward: total was -0.850000. running mean: -33.823276\n",
      "ep 1723: ep_len:500 episode reward: total was -35.780000. running mean: -33.842843\n",
      "ep 1723: ep_len:3 episode reward: total was 1.010000. running mean: -33.494315\n",
      "ep 1723: ep_len:518 episode reward: total was -61.430000. running mean: -33.773672\n",
      "ep 1723: ep_len:578 episode reward: total was -16.080000. running mean: -33.596735\n",
      "epsilon:0.123569 episode_count: 12068. steps_count: 5231227.000000\n",
      "Time elapsed:  15053.565557718277\n",
      "ep 1724: ep_len:574 episode reward: total was 25.080000. running mean: -33.009968\n",
      "ep 1724: ep_len:528 episode reward: total was -44.670000. running mean: -33.126568\n",
      "ep 1724: ep_len:656 episode reward: total was -61.650000. running mean: -33.411802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1724: ep_len:500 episode reward: total was -52.100000. running mean: -33.598684\n",
      "ep 1724: ep_len:37 episode reward: total was -24.500000. running mean: -33.507697\n",
      "ep 1724: ep_len:549 episode reward: total was -17.040000. running mean: -33.343020\n",
      "ep 1724: ep_len:553 episode reward: total was -37.110000. running mean: -33.380690\n",
      "epsilon:0.123525 episode_count: 12075. steps_count: 5234624.000000\n",
      "Time elapsed:  15064.538331270218\n",
      "ep 1725: ep_len:567 episode reward: total was 13.040000. running mean: -32.916483\n",
      "ep 1725: ep_len:545 episode reward: total was -78.950000. running mean: -33.376819\n",
      "ep 1725: ep_len:529 episode reward: total was -97.290000. running mean: -34.015950\n",
      "ep 1725: ep_len:120 episode reward: total was -6.540000. running mean: -33.741191\n",
      "ep 1725: ep_len:103 episode reward: total was 14.710000. running mean: -33.256679\n",
      "ep 1725: ep_len:560 episode reward: total was -80.860000. running mean: -33.732712\n",
      "ep 1725: ep_len:284 episode reward: total was -47.880000. running mean: -33.874185\n",
      "epsilon:0.123481 episode_count: 12082. steps_count: 5237332.000000\n",
      "Time elapsed:  15071.825215101242\n",
      "ep 1726: ep_len:516 episode reward: total was -15.670000. running mean: -33.692143\n",
      "ep 1726: ep_len:525 episode reward: total was -48.650000. running mean: -33.841722\n",
      "ep 1726: ep_len:619 episode reward: total was -8.250000. running mean: -33.585805\n",
      "ep 1726: ep_len:524 episode reward: total was -5.870000. running mean: -33.308646\n",
      "ep 1726: ep_len:44 episode reward: total was 13.000000. running mean: -32.845560\n",
      "ep 1726: ep_len:507 episode reward: total was 2.740000. running mean: -32.489704\n",
      "ep 1726: ep_len:515 episode reward: total was -43.370000. running mean: -32.598507\n",
      "epsilon:0.123436 episode_count: 12089. steps_count: 5240582.000000\n",
      "Time elapsed:  15077.151834964752\n",
      "ep 1727: ep_len:510 episode reward: total was -30.050000. running mean: -32.573022\n",
      "ep 1727: ep_len:604 episode reward: total was -53.270000. running mean: -32.779992\n",
      "ep 1727: ep_len:637 episode reward: total was -80.050000. running mean: -33.252692\n",
      "ep 1727: ep_len:511 episode reward: total was -29.980000. running mean: -33.219965\n",
      "ep 1727: ep_len:80 episode reward: total was 19.190000. running mean: -32.695866\n",
      "ep 1727: ep_len:806 episode reward: total was -232.820000. running mean: -34.697107\n",
      "ep 1727: ep_len:500 episode reward: total was -172.680000. running mean: -36.076936\n",
      "epsilon:0.123392 episode_count: 12096. steps_count: 5244230.000000\n",
      "Time elapsed:  15086.763412714005\n",
      "ep 1728: ep_len:579 episode reward: total was -28.480000. running mean: -36.000966\n",
      "ep 1728: ep_len:584 episode reward: total was 20.920000. running mean: -35.431757\n",
      "ep 1728: ep_len:531 episode reward: total was -58.730000. running mean: -35.664739\n",
      "ep 1728: ep_len:621 episode reward: total was -63.820000. running mean: -35.946292\n",
      "ep 1728: ep_len:101 episode reward: total was 6.270000. running mean: -35.524129\n",
      "ep 1728: ep_len:602 episode reward: total was -37.050000. running mean: -35.539388\n",
      "ep 1728: ep_len:585 episode reward: total was -19.920000. running mean: -35.383194\n",
      "epsilon:0.123348 episode_count: 12103. steps_count: 5247833.000000\n",
      "Time elapsed:  15096.180215120316\n",
      "ep 1729: ep_len:533 episode reward: total was -73.900000. running mean: -35.768362\n",
      "ep 1729: ep_len:500 episode reward: total was -104.660000. running mean: -36.457278\n",
      "ep 1729: ep_len:79 episode reward: total was 4.790000. running mean: -36.044805\n",
      "ep 1729: ep_len:516 episode reward: total was -41.210000. running mean: -36.096457\n",
      "ep 1729: ep_len:106 episode reward: total was 20.740000. running mean: -35.528093\n",
      "ep 1729: ep_len:643 episode reward: total was -122.510000. running mean: -36.397912\n",
      "ep 1729: ep_len:591 episode reward: total was -55.930000. running mean: -36.593233\n",
      "epsilon:0.123303 episode_count: 12110. steps_count: 5250801.000000\n",
      "Time elapsed:  15104.046646118164\n",
      "ep 1730: ep_len:520 episode reward: total was 7.910000. running mean: -36.148200\n",
      "ep 1730: ep_len:606 episode reward: total was -6.700000. running mean: -35.853718\n",
      "ep 1730: ep_len:500 episode reward: total was -15.990000. running mean: -35.655081\n",
      "ep 1730: ep_len:540 episode reward: total was 6.810000. running mean: -35.230430\n",
      "ep 1730: ep_len:54 episode reward: total was 18.000000. running mean: -34.698126\n",
      "ep 1730: ep_len:544 episode reward: total was -34.690000. running mean: -34.698045\n",
      "ep 1730: ep_len:510 episode reward: total was -39.010000. running mean: -34.741164\n",
      "epsilon:0.123259 episode_count: 12117. steps_count: 5254075.000000\n",
      "Time elapsed:  15111.849518060684\n",
      "ep 1731: ep_len:621 episode reward: total was -58.480000. running mean: -34.978553\n",
      "ep 1731: ep_len:570 episode reward: total was -66.990000. running mean: -35.298667\n",
      "ep 1731: ep_len:529 episode reward: total was -60.590000. running mean: -35.551581\n",
      "ep 1731: ep_len:551 episode reward: total was 18.800000. running mean: -35.008065\n",
      "ep 1731: ep_len:93 episode reward: total was 17.240000. running mean: -34.485584\n",
      "ep 1731: ep_len:500 episode reward: total was -47.220000. running mean: -34.612928\n",
      "ep 1731: ep_len:535 episode reward: total was -30.070000. running mean: -34.567499\n",
      "epsilon:0.123215 episode_count: 12124. steps_count: 5257474.000000\n",
      "Time elapsed:  15124.759652853012\n",
      "ep 1732: ep_len:500 episode reward: total was -10.610000. running mean: -34.327924\n",
      "ep 1732: ep_len:201 episode reward: total was -30.210000. running mean: -34.286745\n",
      "ep 1732: ep_len:552 episode reward: total was -0.790000. running mean: -33.951777\n",
      "ep 1732: ep_len:500 episode reward: total was -28.480000. running mean: -33.897060\n",
      "ep 1732: ep_len:69 episode reward: total was 6.130000. running mean: -33.496789\n",
      "ep 1732: ep_len:553 episode reward: total was -73.470000. running mean: -33.896521\n",
      "ep 1732: ep_len:579 episode reward: total was -129.600000. running mean: -34.853556\n",
      "epsilon:0.123170 episode_count: 12131. steps_count: 5260428.000000\n",
      "Time elapsed:  15132.82951760292\n",
      "ep 1733: ep_len:521 episode reward: total was -80.190000. running mean: -35.306920\n",
      "ep 1733: ep_len:288 episode reward: total was -10.370000. running mean: -35.057551\n",
      "ep 1733: ep_len:534 episode reward: total was -28.870000. running mean: -34.995676\n",
      "ep 1733: ep_len:500 episode reward: total was -24.410000. running mean: -34.889819\n",
      "ep 1733: ep_len:3 episode reward: total was -1.500000. running mean: -34.555921\n",
      "ep 1733: ep_len:578 episode reward: total was -21.460000. running mean: -34.424961\n",
      "ep 1733: ep_len:500 episode reward: total was -58.350000. running mean: -34.664212\n",
      "epsilon:0.123126 episode_count: 12138. steps_count: 5263352.000000\n",
      "Time elapsed:  15140.665661096573\n",
      "ep 1734: ep_len:563 episode reward: total was 18.730000. running mean: -34.130270\n",
      "ep 1734: ep_len:516 episode reward: total was 38.610000. running mean: -33.402867\n",
      "ep 1734: ep_len:698 episode reward: total was -60.850000. running mean: -33.677338\n",
      "ep 1734: ep_len:50 episode reward: total was 0.270000. running mean: -33.337865\n",
      "ep 1734: ep_len:3 episode reward: total was 1.010000. running mean: -32.994386\n",
      "ep 1734: ep_len:221 episode reward: total was 9.270000. running mean: -32.571742\n",
      "ep 1734: ep_len:582 episode reward: total was -23.930000. running mean: -32.485325\n",
      "epsilon:0.123082 episode_count: 12145. steps_count: 5265985.000000\n",
      "Time elapsed:  15148.10631108284\n",
      "ep 1735: ep_len:625 episode reward: total was -65.360000. running mean: -32.814072\n",
      "ep 1735: ep_len:500 episode reward: total was -19.400000. running mean: -32.679931\n",
      "ep 1735: ep_len:575 episode reward: total was -55.780000. running mean: -32.910932\n",
      "ep 1735: ep_len:56 episode reward: total was -1.170000. running mean: -32.593522\n",
      "ep 1735: ep_len:126 episode reward: total was 20.850000. running mean: -32.059087\n",
      "ep 1735: ep_len:575 episode reward: total was -35.730000. running mean: -32.095796\n",
      "ep 1735: ep_len:211 episode reward: total was -24.970000. running mean: -32.024538\n",
      "epsilon:0.123037 episode_count: 12152. steps_count: 5268653.000000\n",
      "Time elapsed:  15155.283150672913\n",
      "ep 1736: ep_len:610 episode reward: total was -72.950000. running mean: -32.433793\n",
      "ep 1736: ep_len:516 episode reward: total was -30.270000. running mean: -32.412155\n",
      "ep 1736: ep_len:521 episode reward: total was -91.920000. running mean: -33.007233\n",
      "ep 1736: ep_len:508 episode reward: total was -1.780000. running mean: -32.694961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1736: ep_len:76 episode reward: total was -4.840000. running mean: -32.416412\n",
      "ep 1736: ep_len:500 episode reward: total was -20.860000. running mean: -32.300847\n",
      "ep 1736: ep_len:500 episode reward: total was -23.590000. running mean: -32.213739\n",
      "epsilon:0.122993 episode_count: 12159. steps_count: 5271884.000000\n",
      "Time elapsed:  15165.691526174545\n",
      "ep 1737: ep_len:508 episode reward: total was 14.200000. running mean: -31.749602\n",
      "ep 1737: ep_len:500 episode reward: total was 23.910000. running mean: -31.193006\n",
      "ep 1737: ep_len:500 episode reward: total was -5.150000. running mean: -30.932575\n",
      "ep 1737: ep_len:501 episode reward: total was -11.420000. running mean: -30.737450\n",
      "ep 1737: ep_len:111 episode reward: total was 26.760000. running mean: -30.162475\n",
      "ep 1737: ep_len:655 episode reward: total was -14.960000. running mean: -30.010450\n",
      "ep 1737: ep_len:318 episode reward: total was -9.980000. running mean: -29.810146\n",
      "epsilon:0.122949 episode_count: 12166. steps_count: 5274977.000000\n",
      "Time elapsed:  15173.722774982452\n",
      "ep 1738: ep_len:514 episode reward: total was 20.460000. running mean: -29.307445\n",
      "ep 1738: ep_len:615 episode reward: total was 4.140000. running mean: -28.972970\n",
      "ep 1738: ep_len:519 episode reward: total was -22.310000. running mean: -28.906340\n",
      "ep 1738: ep_len:514 episode reward: total was -35.400000. running mean: -28.971277\n",
      "ep 1738: ep_len:3 episode reward: total was -1.500000. running mean: -28.696564\n",
      "ep 1738: ep_len:584 episode reward: total was -30.990000. running mean: -28.719499\n",
      "ep 1738: ep_len:561 episode reward: total was -47.070000. running mean: -28.903004\n",
      "epsilon:0.122904 episode_count: 12173. steps_count: 5278287.000000\n",
      "Time elapsed:  15182.473225593567\n",
      "ep 1739: ep_len:501 episode reward: total was -68.260000. running mean: -29.296574\n",
      "ep 1739: ep_len:500 episode reward: total was 5.340000. running mean: -28.950208\n",
      "ep 1739: ep_len:433 episode reward: total was -7.930000. running mean: -28.740006\n",
      "ep 1739: ep_len:500 episode reward: total was -40.650000. running mean: -28.859106\n",
      "ep 1739: ep_len:3 episode reward: total was 1.010000. running mean: -28.560415\n",
      "ep 1739: ep_len:500 episode reward: total was -56.600000. running mean: -28.840810\n",
      "ep 1739: ep_len:572 episode reward: total was -38.460000. running mean: -28.937002\n",
      "epsilon:0.122860 episode_count: 12180. steps_count: 5281296.000000\n",
      "Time elapsed:  15190.51651096344\n",
      "ep 1740: ep_len:573 episode reward: total was -55.620000. running mean: -29.203832\n",
      "ep 1740: ep_len:500 episode reward: total was 54.680000. running mean: -28.364994\n",
      "ep 1740: ep_len:402 episode reward: total was 12.530000. running mean: -27.956044\n",
      "ep 1740: ep_len:500 episode reward: total was -31.370000. running mean: -27.990184\n",
      "ep 1740: ep_len:3 episode reward: total was 1.010000. running mean: -27.700182\n",
      "ep 1740: ep_len:149 episode reward: total was -16.530000. running mean: -27.588480\n",
      "ep 1740: ep_len:595 episode reward: total was -67.750000. running mean: -27.990095\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.122816 episode_count: 12187. steps_count: 5284018.000000\n",
      "Time elapsed:  15202.466054439545\n",
      "ep 1741: ep_len:607 episode reward: total was -96.270000. running mean: -28.672894\n",
      "ep 1741: ep_len:520 episode reward: total was -53.540000. running mean: -28.921565\n",
      "ep 1741: ep_len:440 episode reward: total was 17.030000. running mean: -28.462050\n",
      "ep 1741: ep_len:512 episode reward: total was -44.940000. running mean: -28.626829\n",
      "ep 1741: ep_len:3 episode reward: total was -1.500000. running mean: -28.355561\n",
      "ep 1741: ep_len:538 episode reward: total was -103.090000. running mean: -29.102905\n",
      "ep 1741: ep_len:524 episode reward: total was -26.810000. running mean: -29.079976\n",
      "epsilon:0.122771 episode_count: 12194. steps_count: 5287162.000000\n",
      "Time elapsed:  15210.70487356186\n",
      "ep 1742: ep_len:625 episode reward: total was -98.940000. running mean: -29.778576\n",
      "ep 1742: ep_len:605 episode reward: total was -64.930000. running mean: -30.130091\n",
      "ep 1742: ep_len:567 episode reward: total was -43.960000. running mean: -30.268390\n",
      "ep 1742: ep_len:542 episode reward: total was -18.790000. running mean: -30.153606\n",
      "ep 1742: ep_len:87 episode reward: total was 22.630000. running mean: -29.625770\n",
      "ep 1742: ep_len:512 episode reward: total was -20.190000. running mean: -29.531412\n",
      "ep 1742: ep_len:305 episode reward: total was -27.130000. running mean: -29.507398\n",
      "epsilon:0.122727 episode_count: 12201. steps_count: 5290405.000000\n",
      "Time elapsed:  15221.176531791687\n",
      "ep 1743: ep_len:520 episode reward: total was 25.790000. running mean: -28.954424\n",
      "ep 1743: ep_len:506 episode reward: total was 5.940000. running mean: -28.605480\n",
      "ep 1743: ep_len:550 episode reward: total was -63.530000. running mean: -28.954725\n",
      "ep 1743: ep_len:601 episode reward: total was 39.110000. running mean: -28.274078\n",
      "ep 1743: ep_len:3 episode reward: total was 1.010000. running mean: -27.981237\n",
      "ep 1743: ep_len:500 episode reward: total was -30.440000. running mean: -28.005825\n",
      "ep 1743: ep_len:517 episode reward: total was -103.620000. running mean: -28.761966\n",
      "epsilon:0.122683 episode_count: 12208. steps_count: 5293602.000000\n",
      "Time elapsed:  15229.687751293182\n",
      "ep 1744: ep_len:500 episode reward: total was 43.200000. running mean: -28.042347\n",
      "ep 1744: ep_len:638 episode reward: total was -24.180000. running mean: -28.003723\n",
      "ep 1744: ep_len:545 episode reward: total was -32.110000. running mean: -28.044786\n",
      "ep 1744: ep_len:500 episode reward: total was -43.940000. running mean: -28.203738\n",
      "ep 1744: ep_len:3 episode reward: total was 1.010000. running mean: -27.911601\n",
      "ep 1744: ep_len:544 episode reward: total was -50.380000. running mean: -28.136285\n",
      "ep 1744: ep_len:632 episode reward: total was -65.750000. running mean: -28.512422\n",
      "epsilon:0.122638 episode_count: 12215. steps_count: 5296964.000000\n",
      "Time elapsed:  15237.821741819382\n",
      "ep 1745: ep_len:131 episode reward: total was -1.110000. running mean: -28.238398\n",
      "ep 1745: ep_len:500 episode reward: total was -36.610000. running mean: -28.322114\n",
      "ep 1745: ep_len:79 episode reward: total was -0.260000. running mean: -28.041493\n",
      "ep 1745: ep_len:500 episode reward: total was -45.580000. running mean: -28.216878\n",
      "ep 1745: ep_len:45 episode reward: total was -1.500000. running mean: -27.949709\n",
      "ep 1745: ep_len:545 episode reward: total was -42.400000. running mean: -28.094212\n",
      "ep 1745: ep_len:530 episode reward: total was -44.960000. running mean: -28.262870\n",
      "epsilon:0.122594 episode_count: 12222. steps_count: 5299294.000000\n",
      "Time elapsed:  15244.321212291718\n",
      "ep 1746: ep_len:571 episode reward: total was -68.130000. running mean: -28.661541\n",
      "ep 1746: ep_len:500 episode reward: total was -19.200000. running mean: -28.566926\n",
      "ep 1746: ep_len:573 episode reward: total was -73.260000. running mean: -29.013856\n",
      "ep 1746: ep_len:624 episode reward: total was -13.310000. running mean: -28.856818\n",
      "ep 1746: ep_len:3 episode reward: total was 1.010000. running mean: -28.558150\n",
      "ep 1746: ep_len:522 episode reward: total was -76.690000. running mean: -29.039468\n",
      "ep 1746: ep_len:506 episode reward: total was -19.730000. running mean: -28.946373\n",
      "epsilon:0.122550 episode_count: 12229. steps_count: 5302593.000000\n",
      "Time elapsed:  15254.775227308273\n",
      "ep 1747: ep_len:613 episode reward: total was 5.220000. running mean: -28.604710\n",
      "ep 1747: ep_len:635 episode reward: total was 12.750000. running mean: -28.191163\n",
      "ep 1747: ep_len:452 episode reward: total was 8.210000. running mean: -27.827151\n",
      "ep 1747: ep_len:562 episode reward: total was 15.790000. running mean: -27.390979\n",
      "ep 1747: ep_len:109 episode reward: total was 17.710000. running mean: -26.939970\n",
      "ep 1747: ep_len:169 episode reward: total was 13.450000. running mean: -26.536070\n",
      "ep 1747: ep_len:261 episode reward: total was -19.460000. running mean: -26.465309\n",
      "epsilon:0.122505 episode_count: 12236. steps_count: 5305394.000000\n",
      "Time elapsed:  15261.705731153488\n",
      "ep 1748: ep_len:529 episode reward: total was -11.060000. running mean: -26.311256\n",
      "ep 1748: ep_len:500 episode reward: total was -85.960000. running mean: -26.907744\n",
      "ep 1748: ep_len:500 episode reward: total was -31.000000. running mean: -26.948666\n",
      "ep 1748: ep_len:541 episode reward: total was 22.890000. running mean: -26.450279\n",
      "ep 1748: ep_len:99 episode reward: total was -55.260000. running mean: -26.738377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1748: ep_len:500 episode reward: total was -34.860000. running mean: -26.819593\n",
      "ep 1748: ep_len:298 episode reward: total was -36.750000. running mean: -26.918897\n",
      "epsilon:0.122461 episode_count: 12243. steps_count: 5308361.000000\n",
      "Time elapsed:  15269.443862199783\n",
      "ep 1749: ep_len:176 episode reward: total was 18.460000. running mean: -26.465108\n",
      "ep 1749: ep_len:282 episode reward: total was -63.960000. running mean: -26.840057\n",
      "ep 1749: ep_len:551 episode reward: total was -60.180000. running mean: -27.173456\n",
      "ep 1749: ep_len:502 episode reward: total was -16.040000. running mean: -27.062122\n",
      "ep 1749: ep_len:3 episode reward: total was 1.010000. running mean: -26.781401\n",
      "ep 1749: ep_len:500 episode reward: total was -16.880000. running mean: -26.682387\n",
      "ep 1749: ep_len:587 episode reward: total was -25.610000. running mean: -26.671663\n",
      "epsilon:0.122417 episode_count: 12250. steps_count: 5310962.000000\n",
      "Time elapsed:  15278.950252771378\n",
      "ep 1750: ep_len:501 episode reward: total was -2.350000. running mean: -26.428446\n",
      "ep 1750: ep_len:501 episode reward: total was -7.070000. running mean: -26.234862\n",
      "ep 1750: ep_len:525 episode reward: total was -76.080000. running mean: -26.733313\n",
      "ep 1750: ep_len:535 episode reward: total was -38.200000. running mean: -26.847980\n",
      "ep 1750: ep_len:3 episode reward: total was 1.010000. running mean: -26.569400\n",
      "ep 1750: ep_len:502 episode reward: total was -36.550000. running mean: -26.669206\n",
      "ep 1750: ep_len:573 episode reward: total was -61.130000. running mean: -27.013814\n",
      "epsilon:0.122372 episode_count: 12257. steps_count: 5314102.000000\n",
      "Time elapsed:  15285.491941928864\n",
      "ep 1751: ep_len:500 episode reward: total was -5.470000. running mean: -26.798376\n",
      "ep 1751: ep_len:626 episode reward: total was -6.320000. running mean: -26.593592\n",
      "ep 1751: ep_len:500 episode reward: total was -29.170000. running mean: -26.619356\n",
      "ep 1751: ep_len:501 episode reward: total was -18.540000. running mean: -26.538563\n",
      "ep 1751: ep_len:79 episode reward: total was -5.340000. running mean: -26.326577\n",
      "ep 1751: ep_len:500 episode reward: total was -69.120000. running mean: -26.754511\n",
      "ep 1751: ep_len:534 episode reward: total was -40.610000. running mean: -26.893066\n",
      "epsilon:0.122328 episode_count: 12264. steps_count: 5317342.000000\n",
      "Time elapsed:  15296.112477064133\n",
      "ep 1752: ep_len:500 episode reward: total was 33.110000. running mean: -26.293035\n",
      "ep 1752: ep_len:500 episode reward: total was -3.380000. running mean: -26.063905\n",
      "ep 1752: ep_len:502 episode reward: total was -13.970000. running mean: -25.942966\n",
      "ep 1752: ep_len:589 episode reward: total was 9.000000. running mean: -25.593536\n",
      "ep 1752: ep_len:125 episode reward: total was -12.190000. running mean: -25.459501\n",
      "ep 1752: ep_len:501 episode reward: total was -27.260000. running mean: -25.477506\n",
      "ep 1752: ep_len:627 episode reward: total was -32.460000. running mean: -25.547331\n",
      "epsilon:0.122284 episode_count: 12271. steps_count: 5320686.000000\n",
      "Time elapsed:  15307.266310930252\n",
      "ep 1753: ep_len:619 episode reward: total was -53.090000. running mean: -25.822758\n",
      "ep 1753: ep_len:272 episode reward: total was -6.310000. running mean: -25.627630\n",
      "ep 1753: ep_len:632 episode reward: total was -36.460000. running mean: -25.735954\n",
      "ep 1753: ep_len:500 episode reward: total was 14.940000. running mean: -25.329194\n",
      "ep 1753: ep_len:3 episode reward: total was 1.010000. running mean: -25.065802\n",
      "ep 1753: ep_len:586 episode reward: total was -75.520000. running mean: -25.570344\n",
      "ep 1753: ep_len:500 episode reward: total was -36.330000. running mean: -25.677941\n",
      "epsilon:0.122239 episode_count: 12278. steps_count: 5323798.000000\n",
      "Time elapsed:  15315.631693601608\n",
      "ep 1754: ep_len:118 episode reward: total was -11.130000. running mean: -25.532461\n",
      "ep 1754: ep_len:338 episode reward: total was -32.140000. running mean: -25.598537\n",
      "ep 1754: ep_len:564 episode reward: total was -52.440000. running mean: -25.866951\n",
      "ep 1754: ep_len:500 episode reward: total was 10.380000. running mean: -25.504482\n",
      "ep 1754: ep_len:50 episode reward: total was 13.000000. running mean: -25.119437\n",
      "ep 1754: ep_len:500 episode reward: total was -70.000000. running mean: -25.568243\n",
      "ep 1754: ep_len:565 episode reward: total was -76.510000. running mean: -26.077660\n",
      "epsilon:0.122195 episode_count: 12285. steps_count: 5326433.000000\n",
      "Time elapsed:  15321.116247653961\n",
      "ep 1755: ep_len:521 episode reward: total was -35.630000. running mean: -26.173184\n",
      "ep 1755: ep_len:543 episode reward: total was -93.720000. running mean: -26.848652\n",
      "ep 1755: ep_len:682 episode reward: total was -71.450000. running mean: -27.294665\n",
      "ep 1755: ep_len:513 episode reward: total was -23.690000. running mean: -27.258619\n",
      "ep 1755: ep_len:3 episode reward: total was 1.010000. running mean: -26.975932\n",
      "ep 1755: ep_len:501 episode reward: total was -15.530000. running mean: -26.861473\n",
      "ep 1755: ep_len:501 episode reward: total was -27.180000. running mean: -26.864658\n",
      "epsilon:0.122151 episode_count: 12292. steps_count: 5329697.000000\n",
      "Time elapsed:  15328.863007307053\n",
      "ep 1756: ep_len:226 episode reward: total was -0.840000. running mean: -26.604412\n",
      "ep 1756: ep_len:570 episode reward: total was -23.500000. running mean: -26.573368\n",
      "ep 1756: ep_len:398 episode reward: total was 16.350000. running mean: -26.144134\n",
      "ep 1756: ep_len:500 episode reward: total was -13.430000. running mean: -26.016993\n",
      "ep 1756: ep_len:3 episode reward: total was 1.010000. running mean: -25.746723\n",
      "ep 1756: ep_len:637 episode reward: total was -13.080000. running mean: -25.620056\n",
      "ep 1756: ep_len:551 episode reward: total was -58.960000. running mean: -25.953455\n",
      "epsilon:0.122106 episode_count: 12299. steps_count: 5332582.000000\n",
      "Time elapsed:  15336.480412006378\n",
      "ep 1757: ep_len:551 episode reward: total was 34.640000. running mean: -25.347520\n",
      "ep 1757: ep_len:348 episode reward: total was -24.140000. running mean: -25.335445\n",
      "ep 1757: ep_len:500 episode reward: total was 5.180000. running mean: -25.030291\n",
      "ep 1757: ep_len:500 episode reward: total was 36.180000. running mean: -24.418188\n",
      "ep 1757: ep_len:77 episode reward: total was 16.740000. running mean: -24.006606\n",
      "ep 1757: ep_len:534 episode reward: total was -48.910000. running mean: -24.255640\n",
      "ep 1757: ep_len:211 episode reward: total was -25.580000. running mean: -24.268884\n",
      "epsilon:0.122062 episode_count: 12306. steps_count: 5335303.000000\n",
      "Time elapsed:  15343.785151720047\n",
      "ep 1758: ep_len:604 episode reward: total was -102.700000. running mean: -25.053195\n",
      "ep 1758: ep_len:500 episode reward: total was -15.410000. running mean: -24.956763\n",
      "ep 1758: ep_len:500 episode reward: total was -19.490000. running mean: -24.902095\n",
      "ep 1758: ep_len:500 episode reward: total was -22.190000. running mean: -24.874974\n",
      "ep 1758: ep_len:3 episode reward: total was -1.500000. running mean: -24.641224\n",
      "ep 1758: ep_len:592 episode reward: total was -85.060000. running mean: -25.245412\n",
      "ep 1758: ep_len:328 episode reward: total was -48.030000. running mean: -25.473258\n",
      "epsilon:0.122018 episode_count: 12313. steps_count: 5338330.000000\n",
      "Time elapsed:  15351.897143602371\n",
      "ep 1759: ep_len:688 episode reward: total was -87.830000. running mean: -26.096826\n",
      "ep 1759: ep_len:501 episode reward: total was -24.140000. running mean: -26.077257\n",
      "ep 1759: ep_len:416 episode reward: total was 5.700000. running mean: -25.759485\n",
      "ep 1759: ep_len:50 episode reward: total was 0.330000. running mean: -25.498590\n",
      "ep 1759: ep_len:3 episode reward: total was 0.000000. running mean: -25.243604\n",
      "ep 1759: ep_len:642 episode reward: total was -43.910000. running mean: -25.430268\n",
      "ep 1759: ep_len:540 episode reward: total was -49.190000. running mean: -25.667865\n",
      "epsilon:0.121973 episode_count: 12320. steps_count: 5341170.000000\n",
      "Time elapsed:  15360.13158917427\n",
      "ep 1760: ep_len:882 episode reward: total was -417.250000. running mean: -29.583687\n",
      "ep 1760: ep_len:500 episode reward: total was -4.760000. running mean: -29.335450\n",
      "ep 1760: ep_len:658 episode reward: total was -122.230000. running mean: -30.264395\n",
      "ep 1760: ep_len:500 episode reward: total was -18.990000. running mean: -30.151651\n",
      "ep 1760: ep_len:3 episode reward: total was 0.000000. running mean: -29.850135\n",
      "ep 1760: ep_len:500 episode reward: total was -67.670000. running mean: -30.228333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1760: ep_len:569 episode reward: total was -44.370000. running mean: -30.369750\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.121929 episode_count: 12327. steps_count: 5344782.000000\n",
      "Time elapsed:  15375.984858512878\n",
      "ep 1761: ep_len:605 episode reward: total was -65.250000. running mean: -30.718553\n",
      "ep 1761: ep_len:528 episode reward: total was -35.950000. running mean: -30.770867\n",
      "ep 1761: ep_len:452 episode reward: total was 10.780000. running mean: -30.355358\n",
      "ep 1761: ep_len:500 episode reward: total was -22.810000. running mean: -30.279905\n",
      "ep 1761: ep_len:58 episode reward: total was 11.610000. running mean: -29.861006\n",
      "ep 1761: ep_len:557 episode reward: total was -43.990000. running mean: -30.002296\n",
      "ep 1761: ep_len:500 episode reward: total was -24.340000. running mean: -29.945673\n",
      "epsilon:0.121885 episode_count: 12334. steps_count: 5347982.000000\n",
      "Time elapsed:  15385.77661728859\n",
      "ep 1762: ep_len:216 episode reward: total was -1.340000. running mean: -29.659616\n",
      "ep 1762: ep_len:500 episode reward: total was -28.990000. running mean: -29.652920\n",
      "ep 1762: ep_len:627 episode reward: total was -42.850000. running mean: -29.784891\n",
      "ep 1762: ep_len:552 episode reward: total was -47.270000. running mean: -29.959742\n",
      "ep 1762: ep_len:3 episode reward: total was 0.000000. running mean: -29.660144\n",
      "ep 1762: ep_len:504 episode reward: total was 15.640000. running mean: -29.207143\n",
      "ep 1762: ep_len:602 episode reward: total was -16.430000. running mean: -29.079371\n",
      "epsilon:0.121840 episode_count: 12341. steps_count: 5350986.000000\n",
      "Time elapsed:  15397.979014158249\n",
      "ep 1763: ep_len:121 episode reward: total was -11.070000. running mean: -28.899278\n",
      "ep 1763: ep_len:562 episode reward: total was 29.450000. running mean: -28.315785\n",
      "ep 1763: ep_len:464 episode reward: total was 11.700000. running mean: -27.915627\n",
      "ep 1763: ep_len:528 episode reward: total was -55.960000. running mean: -28.196071\n",
      "ep 1763: ep_len:3 episode reward: total was 1.010000. running mean: -27.904010\n",
      "ep 1763: ep_len:500 episode reward: total was -45.530000. running mean: -28.080270\n",
      "ep 1763: ep_len:500 episode reward: total was -51.340000. running mean: -28.312867\n",
      "epsilon:0.121796 episode_count: 12348. steps_count: 5353664.000000\n",
      "Time elapsed:  15411.163829565048\n",
      "ep 1764: ep_len:508 episode reward: total was 5.240000. running mean: -27.977339\n",
      "ep 1764: ep_len:529 episode reward: total was 6.190000. running mean: -27.635665\n",
      "ep 1764: ep_len:79 episode reward: total was 1.760000. running mean: -27.341709\n",
      "ep 1764: ep_len:560 episode reward: total was 17.940000. running mean: -26.888892\n",
      "ep 1764: ep_len:3 episode reward: total was -1.500000. running mean: -26.635003\n",
      "ep 1764: ep_len:654 episode reward: total was -55.050000. running mean: -26.919153\n",
      "ep 1764: ep_len:617 episode reward: total was -55.680000. running mean: -27.206761\n",
      "epsilon:0.121752 episode_count: 12355. steps_count: 5356614.000000\n",
      "Time elapsed:  15419.393401622772\n",
      "ep 1765: ep_len:134 episode reward: total was -14.640000. running mean: -27.081093\n",
      "ep 1765: ep_len:514 episode reward: total was -62.870000. running mean: -27.438982\n",
      "ep 1765: ep_len:580 episode reward: total was -123.540000. running mean: -28.399993\n",
      "ep 1765: ep_len:558 episode reward: total was -107.630000. running mean: -29.192293\n",
      "ep 1765: ep_len:3 episode reward: total was 1.010000. running mean: -28.890270\n",
      "ep 1765: ep_len:558 episode reward: total was -29.470000. running mean: -28.896067\n",
      "ep 1765: ep_len:600 episode reward: total was -78.400000. running mean: -29.391106\n",
      "epsilon:0.121707 episode_count: 12362. steps_count: 5359561.000000\n",
      "Time elapsed:  15427.180851221085\n",
      "ep 1766: ep_len:501 episode reward: total was 31.340000. running mean: -28.783795\n",
      "ep 1766: ep_len:585 episode reward: total was -37.730000. running mean: -28.873257\n",
      "ep 1766: ep_len:661 episode reward: total was -75.970000. running mean: -29.344225\n",
      "ep 1766: ep_len:500 episode reward: total was -50.510000. running mean: -29.555883\n",
      "ep 1766: ep_len:93 episode reward: total was 3.710000. running mean: -29.223224\n",
      "ep 1766: ep_len:500 episode reward: total was -52.000000. running mean: -29.450992\n",
      "ep 1766: ep_len:501 episode reward: total was -33.660000. running mean: -29.493082\n",
      "epsilon:0.121663 episode_count: 12369. steps_count: 5362902.000000\n",
      "Time elapsed:  15436.488157749176\n",
      "ep 1767: ep_len:569 episode reward: total was 30.880000. running mean: -28.889351\n",
      "ep 1767: ep_len:549 episode reward: total was -25.790000. running mean: -28.858357\n",
      "ep 1767: ep_len:564 episode reward: total was -39.820000. running mean: -28.967974\n",
      "ep 1767: ep_len:595 episode reward: total was 17.440000. running mean: -28.503894\n",
      "ep 1767: ep_len:3 episode reward: total was 0.000000. running mean: -28.218855\n",
      "ep 1767: ep_len:500 episode reward: total was -9.300000. running mean: -28.029667\n",
      "ep 1767: ep_len:325 episode reward: total was -28.690000. running mean: -28.036270\n",
      "epsilon:0.121619 episode_count: 12376. steps_count: 5366007.000000\n",
      "Time elapsed:  15444.749661922455\n",
      "ep 1768: ep_len:504 episode reward: total was -14.950000. running mean: -27.905407\n",
      "ep 1768: ep_len:500 episode reward: total was -15.310000. running mean: -27.779453\n",
      "ep 1768: ep_len:500 episode reward: total was -20.890000. running mean: -27.710559\n",
      "ep 1768: ep_len:515 episode reward: total was -37.810000. running mean: -27.811553\n",
      "ep 1768: ep_len:3 episode reward: total was 0.000000. running mean: -27.533437\n",
      "ep 1768: ep_len:500 episode reward: total was -9.880000. running mean: -27.356903\n",
      "ep 1768: ep_len:548 episode reward: total was -90.410000. running mean: -27.987434\n",
      "epsilon:0.121574 episode_count: 12383. steps_count: 5369077.000000\n",
      "Time elapsed:  15452.82761311531\n",
      "ep 1769: ep_len:507 episode reward: total was -72.300000. running mean: -28.430560\n",
      "ep 1769: ep_len:592 episode reward: total was -24.250000. running mean: -28.388754\n",
      "ep 1769: ep_len:521 episode reward: total was -6.280000. running mean: -28.167667\n",
      "ep 1769: ep_len:500 episode reward: total was -38.180000. running mean: -28.267790\n",
      "ep 1769: ep_len:92 episode reward: total was -45.320000. running mean: -28.438312\n",
      "ep 1769: ep_len:505 episode reward: total was -103.740000. running mean: -29.191329\n",
      "ep 1769: ep_len:578 episode reward: total was -12.680000. running mean: -29.026216\n",
      "epsilon:0.121530 episode_count: 12390. steps_count: 5372372.000000\n",
      "Time elapsed:  15463.426672458649\n",
      "ep 1770: ep_len:129 episode reward: total was -2.570000. running mean: -28.761653\n",
      "ep 1770: ep_len:500 episode reward: total was -34.970000. running mean: -28.823737\n",
      "ep 1770: ep_len:67 episode reward: total was 1.150000. running mean: -28.524000\n",
      "ep 1770: ep_len:503 episode reward: total was -34.210000. running mean: -28.580860\n",
      "ep 1770: ep_len:101 episode reward: total was 2.260000. running mean: -28.272451\n",
      "ep 1770: ep_len:500 episode reward: total was -25.120000. running mean: -28.240926\n",
      "ep 1770: ep_len:346 episode reward: total was -31.140000. running mean: -28.269917\n",
      "epsilon:0.121486 episode_count: 12397. steps_count: 5374518.000000\n",
      "Time elapsed:  15469.506876945496\n",
      "ep 1771: ep_len:536 episode reward: total was -72.550000. running mean: -28.712718\n",
      "ep 1771: ep_len:500 episode reward: total was 1.650000. running mean: -28.409091\n",
      "ep 1771: ep_len:587 episode reward: total was -52.200000. running mean: -28.647000\n",
      "ep 1771: ep_len:511 episode reward: total was 33.170000. running mean: -28.028830\n",
      "ep 1771: ep_len:73 episode reward: total was -18.810000. running mean: -27.936642\n",
      "ep 1771: ep_len:500 episode reward: total was -9.420000. running mean: -27.751475\n",
      "ep 1771: ep_len:335 episode reward: total was -19.190000. running mean: -27.665860\n",
      "epsilon:0.121441 episode_count: 12404. steps_count: 5377560.000000\n",
      "Time elapsed:  15477.620217561722\n",
      "ep 1772: ep_len:562 episode reward: total was 33.160000. running mean: -27.057602\n",
      "ep 1772: ep_len:500 episode reward: total was 25.080000. running mean: -26.536226\n",
      "ep 1772: ep_len:581 episode reward: total was -76.370000. running mean: -27.034564\n",
      "ep 1772: ep_len:521 episode reward: total was 30.650000. running mean: -26.457718\n",
      "ep 1772: ep_len:112 episode reward: total was -8.300000. running mean: -26.276141\n",
      "ep 1772: ep_len:219 episode reward: total was 22.140000. running mean: -25.791979\n",
      "ep 1772: ep_len:523 episode reward: total was -36.110000. running mean: -25.895160\n",
      "epsilon:0.121397 episode_count: 12411. steps_count: 5380578.000000\n",
      "Time elapsed:  15485.52638411522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1773: ep_len:599 episode reward: total was 22.010000. running mean: -25.416108\n",
      "ep 1773: ep_len:500 episode reward: total was -96.700000. running mean: -26.128947\n",
      "ep 1773: ep_len:435 episode reward: total was -11.490000. running mean: -25.982557\n",
      "ep 1773: ep_len:500 episode reward: total was -30.120000. running mean: -26.023932\n",
      "ep 1773: ep_len:3 episode reward: total was 1.010000. running mean: -25.753592\n",
      "ep 1773: ep_len:500 episode reward: total was -10.350000. running mean: -25.599557\n",
      "ep 1773: ep_len:500 episode reward: total was -37.470000. running mean: -25.718261\n",
      "epsilon:0.121353 episode_count: 12418. steps_count: 5383615.000000\n",
      "Time elapsed:  15495.622647047043\n",
      "ep 1774: ep_len:614 episode reward: total was -62.280000. running mean: -26.083878\n",
      "ep 1774: ep_len:201 episode reward: total was -5.570000. running mean: -25.878740\n",
      "ep 1774: ep_len:655 episode reward: total was -90.060000. running mean: -26.520552\n",
      "ep 1774: ep_len:613 episode reward: total was 17.560000. running mean: -26.079747\n",
      "ep 1774: ep_len:111 episode reward: total was 4.720000. running mean: -25.771749\n",
      "ep 1774: ep_len:585 episode reward: total was -3.640000. running mean: -25.550432\n",
      "ep 1774: ep_len:606 episode reward: total was -34.190000. running mean: -25.636827\n",
      "epsilon:0.121308 episode_count: 12425. steps_count: 5387000.000000\n",
      "Time elapsed:  15504.508203029633\n",
      "ep 1775: ep_len:545 episode reward: total was -4.930000. running mean: -25.429759\n",
      "ep 1775: ep_len:591 episode reward: total was -60.170000. running mean: -25.777162\n",
      "ep 1775: ep_len:650 episode reward: total was -14.080000. running mean: -25.660190\n",
      "ep 1775: ep_len:519 episode reward: total was 19.750000. running mean: -25.206088\n",
      "ep 1775: ep_len:3 episode reward: total was 1.010000. running mean: -24.943927\n",
      "ep 1775: ep_len:503 episode reward: total was -31.370000. running mean: -25.008188\n",
      "ep 1775: ep_len:588 episode reward: total was -51.070000. running mean: -25.268806\n",
      "epsilon:0.121264 episode_count: 12432. steps_count: 5390399.000000\n",
      "Time elapsed:  15513.472663402557\n",
      "ep 1776: ep_len:106 episode reward: total was -9.780000. running mean: -25.113918\n",
      "ep 1776: ep_len:500 episode reward: total was -22.080000. running mean: -25.083579\n",
      "ep 1776: ep_len:648 episode reward: total was -78.930000. running mean: -25.622043\n",
      "ep 1776: ep_len:500 episode reward: total was -21.090000. running mean: -25.576723\n",
      "ep 1776: ep_len:87 episode reward: total was 15.220000. running mean: -25.168755\n",
      "ep 1776: ep_len:500 episode reward: total was -30.590000. running mean: -25.222968\n",
      "ep 1776: ep_len:500 episode reward: total was -28.690000. running mean: -25.257638\n",
      "epsilon:0.121220 episode_count: 12439. steps_count: 5393240.000000\n",
      "Time elapsed:  15521.033256053925\n",
      "ep 1777: ep_len:500 episode reward: total was 3.230000. running mean: -24.972762\n",
      "ep 1777: ep_len:274 episode reward: total was -60.350000. running mean: -25.326534\n",
      "ep 1777: ep_len:603 episode reward: total was -26.010000. running mean: -25.333369\n",
      "ep 1777: ep_len:513 episode reward: total was -31.370000. running mean: -25.393735\n",
      "ep 1777: ep_len:84 episode reward: total was 8.150000. running mean: -25.058298\n",
      "ep 1777: ep_len:605 episode reward: total was -42.430000. running mean: -25.232015\n",
      "ep 1777: ep_len:507 episode reward: total was -23.880000. running mean: -25.218495\n",
      "epsilon:0.121175 episode_count: 12446. steps_count: 5396326.000000\n",
      "Time elapsed:  15529.167227983475\n",
      "ep 1778: ep_len:611 episode reward: total was 7.200000. running mean: -24.894310\n",
      "ep 1778: ep_len:599 episode reward: total was -65.230000. running mean: -25.297667\n",
      "ep 1778: ep_len:500 episode reward: total was -38.040000. running mean: -25.425090\n",
      "ep 1778: ep_len:523 episode reward: total was -29.150000. running mean: -25.462339\n",
      "ep 1778: ep_len:55 episode reward: total was 18.010000. running mean: -25.027616\n",
      "ep 1778: ep_len:522 episode reward: total was -34.590000. running mean: -25.123239\n",
      "ep 1778: ep_len:205 episode reward: total was -27.660000. running mean: -25.148607\n",
      "epsilon:0.121131 episode_count: 12453. steps_count: 5399341.000000\n",
      "Time elapsed:  15537.161281347275\n",
      "ep 1779: ep_len:635 episode reward: total was -11.480000. running mean: -25.011921\n",
      "ep 1779: ep_len:540 episode reward: total was -42.690000. running mean: -25.188702\n",
      "ep 1779: ep_len:652 episode reward: total was -87.640000. running mean: -25.813215\n",
      "ep 1779: ep_len:122 episode reward: total was -6.460000. running mean: -25.619683\n",
      "ep 1779: ep_len:3 episode reward: total was 1.010000. running mean: -25.353386\n",
      "ep 1779: ep_len:504 episode reward: total was -0.240000. running mean: -25.102252\n",
      "ep 1779: ep_len:536 episode reward: total was -3.810000. running mean: -24.889329\n",
      "epsilon:0.121087 episode_count: 12460. steps_count: 5402333.000000\n",
      "Time elapsed:  15547.06016588211\n",
      "ep 1780: ep_len:515 episode reward: total was -71.380000. running mean: -25.354236\n",
      "ep 1780: ep_len:500 episode reward: total was -14.680000. running mean: -25.247494\n",
      "ep 1780: ep_len:445 episode reward: total was 26.720000. running mean: -24.727819\n",
      "ep 1780: ep_len:501 episode reward: total was -16.120000. running mean: -24.641741\n",
      "ep 1780: ep_len:69 episode reward: total was 14.670000. running mean: -24.248623\n",
      "ep 1780: ep_len:601 episode reward: total was -166.090000. running mean: -25.667037\n",
      "ep 1780: ep_len:278 episode reward: total was -19.600000. running mean: -25.606367\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.121042 episode_count: 12467. steps_count: 5405242.000000\n",
      "Time elapsed:  15561.926480770111\n",
      "ep 1781: ep_len:591 episode reward: total was -7.280000. running mean: -25.423103\n",
      "ep 1781: ep_len:500 episode reward: total was 35.660000. running mean: -24.812272\n",
      "ep 1781: ep_len:500 episode reward: total was -59.850000. running mean: -25.162649\n",
      "ep 1781: ep_len:541 episode reward: total was -46.750000. running mean: -25.378523\n",
      "ep 1781: ep_len:52 episode reward: total was 21.500000. running mean: -24.909737\n",
      "ep 1781: ep_len:521 episode reward: total was -46.950000. running mean: -25.130140\n",
      "ep 1781: ep_len:511 episode reward: total was -44.040000. running mean: -25.319239\n",
      "epsilon:0.120998 episode_count: 12474. steps_count: 5408458.000000\n",
      "Time elapsed:  15570.278700113297\n",
      "ep 1782: ep_len:500 episode reward: total was 26.150000. running mean: -24.804546\n",
      "ep 1782: ep_len:610 episode reward: total was -66.490000. running mean: -25.221401\n",
      "ep 1782: ep_len:523 episode reward: total was -106.330000. running mean: -26.032487\n",
      "ep 1782: ep_len:500 episode reward: total was 37.370000. running mean: -25.398462\n",
      "ep 1782: ep_len:3 episode reward: total was -1.500000. running mean: -25.159477\n",
      "ep 1782: ep_len:560 episode reward: total was -65.690000. running mean: -25.564783\n",
      "ep 1782: ep_len:566 episode reward: total was -60.600000. running mean: -25.915135\n",
      "epsilon:0.120954 episode_count: 12481. steps_count: 5411720.000000\n",
      "Time elapsed:  15578.872788906097\n",
      "ep 1783: ep_len:539 episode reward: total was -111.510000. running mean: -26.771083\n",
      "ep 1783: ep_len:686 episode reward: total was 15.800000. running mean: -26.345373\n",
      "ep 1783: ep_len:644 episode reward: total was -89.530000. running mean: -26.977219\n",
      "ep 1783: ep_len:120 episode reward: total was -8.620000. running mean: -26.793647\n",
      "ep 1783: ep_len:3 episode reward: total was 1.010000. running mean: -26.515610\n",
      "ep 1783: ep_len:500 episode reward: total was -23.990000. running mean: -26.490354\n",
      "ep 1783: ep_len:268 episode reward: total was -40.450000. running mean: -26.629951\n",
      "epsilon:0.120909 episode_count: 12488. steps_count: 5414480.000000\n",
      "Time elapsed:  15586.35370516777\n",
      "ep 1784: ep_len:604 episode reward: total was -39.770000. running mean: -26.761351\n",
      "ep 1784: ep_len:574 episode reward: total was -47.860000. running mean: -26.972338\n",
      "ep 1784: ep_len:358 episode reward: total was 6.180000. running mean: -26.640814\n",
      "ep 1784: ep_len:500 episode reward: total was 13.410000. running mean: -26.240306\n",
      "ep 1784: ep_len:3 episode reward: total was 1.010000. running mean: -25.967803\n",
      "ep 1784: ep_len:569 episode reward: total was -15.350000. running mean: -25.861625\n",
      "ep 1784: ep_len:526 episode reward: total was -9.880000. running mean: -25.701809\n",
      "epsilon:0.120865 episode_count: 12495. steps_count: 5417614.000000\n",
      "Time elapsed:  15595.54786324501\n",
      "ep 1785: ep_len:668 episode reward: total was -78.610000. running mean: -26.230891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1785: ep_len:502 episode reward: total was -99.430000. running mean: -26.962882\n",
      "ep 1785: ep_len:566 episode reward: total was -109.460000. running mean: -27.787853\n",
      "ep 1785: ep_len:500 episode reward: total was 15.350000. running mean: -27.356474\n",
      "ep 1785: ep_len:58 episode reward: total was -20.400000. running mean: -27.286910\n",
      "ep 1785: ep_len:500 episode reward: total was -9.790000. running mean: -27.111940\n",
      "ep 1785: ep_len:501 episode reward: total was -144.680000. running mean: -28.287621\n",
      "epsilon:0.120821 episode_count: 12502. steps_count: 5420909.000000\n",
      "Time elapsed:  15604.05107665062\n",
      "ep 1786: ep_len:551 episode reward: total was 30.140000. running mean: -27.703345\n",
      "ep 1786: ep_len:582 episode reward: total was -58.910000. running mean: -28.015411\n",
      "ep 1786: ep_len:558 episode reward: total was -53.070000. running mean: -28.265957\n",
      "ep 1786: ep_len:500 episode reward: total was 42.790000. running mean: -27.555398\n",
      "ep 1786: ep_len:3 episode reward: total was 1.010000. running mean: -27.269744\n",
      "ep 1786: ep_len:500 episode reward: total was -66.820000. running mean: -27.665246\n",
      "ep 1786: ep_len:500 episode reward: total was -7.700000. running mean: -27.465594\n",
      "epsilon:0.120776 episode_count: 12509. steps_count: 5424103.000000\n",
      "Time elapsed:  15612.250345230103\n",
      "ep 1787: ep_len:219 episode reward: total was -13.370000. running mean: -27.324638\n",
      "ep 1787: ep_len:276 episode reward: total was -4.980000. running mean: -27.101192\n",
      "ep 1787: ep_len:500 episode reward: total was -21.700000. running mean: -27.047180\n",
      "ep 1787: ep_len:604 episode reward: total was -13.900000. running mean: -26.915708\n",
      "ep 1787: ep_len:3 episode reward: total was 1.010000. running mean: -26.636451\n",
      "ep 1787: ep_len:514 episode reward: total was -41.420000. running mean: -26.784286\n",
      "ep 1787: ep_len:334 episode reward: total was -58.680000. running mean: -27.103243\n",
      "epsilon:0.120732 episode_count: 12516. steps_count: 5426553.000000\n",
      "Time elapsed:  15618.942080497742\n",
      "ep 1788: ep_len:109 episode reward: total was -1.060000. running mean: -26.842811\n",
      "ep 1788: ep_len:604 episode reward: total was -20.630000. running mean: -26.780683\n",
      "ep 1788: ep_len:608 episode reward: total was -130.030000. running mean: -27.813176\n",
      "ep 1788: ep_len:500 episode reward: total was -11.980000. running mean: -27.654844\n",
      "ep 1788: ep_len:3 episode reward: total was -1.500000. running mean: -27.393296\n",
      "ep 1788: ep_len:564 episode reward: total was -72.400000. running mean: -27.843363\n",
      "ep 1788: ep_len:554 episode reward: total was -50.800000. running mean: -28.072929\n",
      "epsilon:0.120688 episode_count: 12523. steps_count: 5429495.000000\n",
      "Time elapsed:  15627.067301750183\n",
      "ep 1789: ep_len:547 episode reward: total was -106.870000. running mean: -28.860900\n",
      "ep 1789: ep_len:548 episode reward: total was -37.010000. running mean: -28.942391\n",
      "ep 1789: ep_len:64 episode reward: total was 3.810000. running mean: -28.614867\n",
      "ep 1789: ep_len:556 episode reward: total was -10.170000. running mean: -28.430418\n",
      "ep 1789: ep_len:3 episode reward: total was 1.010000. running mean: -28.136014\n",
      "ep 1789: ep_len:521 episode reward: total was -39.700000. running mean: -28.251654\n",
      "ep 1789: ep_len:189 episode reward: total was -23.050000. running mean: -28.199637\n",
      "epsilon:0.120643 episode_count: 12530. steps_count: 5431923.000000\n",
      "Time elapsed:  15633.845057964325\n",
      "ep 1790: ep_len:500 episode reward: total was 17.400000. running mean: -27.743641\n",
      "ep 1790: ep_len:200 episode reward: total was -8.580000. running mean: -27.552005\n",
      "ep 1790: ep_len:593 episode reward: total was -59.970000. running mean: -27.876185\n",
      "ep 1790: ep_len:559 episode reward: total was 1.930000. running mean: -27.578123\n",
      "ep 1790: ep_len:3 episode reward: total was -1.500000. running mean: -27.317342\n",
      "ep 1790: ep_len:567 episode reward: total was -28.460000. running mean: -27.328768\n",
      "ep 1790: ep_len:595 episode reward: total was -30.390000. running mean: -27.359380\n",
      "epsilon:0.120599 episode_count: 12537. steps_count: 5434940.000000\n",
      "Time elapsed:  15641.745208501816\n",
      "ep 1791: ep_len:649 episode reward: total was 4.910000. running mean: -27.036687\n",
      "ep 1791: ep_len:590 episode reward: total was -33.510000. running mean: -27.101420\n",
      "ep 1791: ep_len:500 episode reward: total was -41.470000. running mean: -27.245106\n",
      "ep 1791: ep_len:505 episode reward: total was -62.300000. running mean: -27.595655\n",
      "ep 1791: ep_len:73 episode reward: total was 13.150000. running mean: -27.188198\n",
      "ep 1791: ep_len:529 episode reward: total was -2.700000. running mean: -26.943316\n",
      "ep 1791: ep_len:515 episode reward: total was -26.810000. running mean: -26.941983\n",
      "epsilon:0.120555 episode_count: 12544. steps_count: 5438301.000000\n",
      "Time elapsed:  15649.63006401062\n",
      "ep 1792: ep_len:681 episode reward: total was -97.480000. running mean: -27.647363\n",
      "ep 1792: ep_len:556 episode reward: total was -41.910000. running mean: -27.789989\n",
      "ep 1792: ep_len:583 episode reward: total was -39.810000. running mean: -27.910190\n",
      "ep 1792: ep_len:500 episode reward: total was 20.110000. running mean: -27.429988\n",
      "ep 1792: ep_len:3 episode reward: total was 0.000000. running mean: -27.155688\n",
      "ep 1792: ep_len:500 episode reward: total was -9.600000. running mean: -26.980131\n",
      "ep 1792: ep_len:531 episode reward: total was -32.490000. running mean: -27.035230\n",
      "epsilon:0.120510 episode_count: 12551. steps_count: 5441655.000000\n",
      "Time elapsed:  15664.931138277054\n",
      "ep 1793: ep_len:677 episode reward: total was -69.120000. running mean: -27.456077\n",
      "ep 1793: ep_len:500 episode reward: total was -36.530000. running mean: -27.546816\n",
      "ep 1793: ep_len:538 episode reward: total was -65.490000. running mean: -27.926248\n",
      "ep 1793: ep_len:529 episode reward: total was -73.650000. running mean: -28.383486\n",
      "ep 1793: ep_len:3 episode reward: total was 1.010000. running mean: -28.089551\n",
      "ep 1793: ep_len:562 episode reward: total was -18.530000. running mean: -27.993955\n",
      "ep 1793: ep_len:347 episode reward: total was -31.650000. running mean: -28.030516\n",
      "epsilon:0.120466 episode_count: 12558. steps_count: 5444811.000000\n",
      "Time elapsed:  15675.359444856644\n",
      "ep 1794: ep_len:214 episode reward: total was 12.140000. running mean: -27.628811\n",
      "ep 1794: ep_len:290 episode reward: total was -35.510000. running mean: -27.707623\n",
      "ep 1794: ep_len:500 episode reward: total was -5.770000. running mean: -27.488246\n",
      "ep 1794: ep_len:500 episode reward: total was -13.500000. running mean: -27.348364\n",
      "ep 1794: ep_len:3 episode reward: total was 0.000000. running mean: -27.074880\n",
      "ep 1794: ep_len:500 episode reward: total was -87.900000. running mean: -27.683132\n",
      "ep 1794: ep_len:284 episode reward: total was -62.810000. running mean: -28.034400\n",
      "epsilon:0.120422 episode_count: 12565. steps_count: 5447102.000000\n",
      "Time elapsed:  15683.660172462463\n",
      "ep 1795: ep_len:528 episode reward: total was -77.770000. running mean: -28.531756\n",
      "ep 1795: ep_len:501 episode reward: total was 17.800000. running mean: -28.068439\n",
      "ep 1795: ep_len:451 episode reward: total was 24.610000. running mean: -27.541654\n",
      "ep 1795: ep_len:574 episode reward: total was 13.980000. running mean: -27.126438\n",
      "ep 1795: ep_len:3 episode reward: total was 1.010000. running mean: -26.845073\n",
      "ep 1795: ep_len:589 episode reward: total was -9.450000. running mean: -26.671123\n",
      "ep 1795: ep_len:524 episode reward: total was -60.790000. running mean: -27.012311\n",
      "epsilon:0.120377 episode_count: 12572. steps_count: 5450272.000000\n",
      "Time elapsed:  15692.239067077637\n",
      "ep 1796: ep_len:659 episode reward: total was -90.270000. running mean: -27.644888\n",
      "ep 1796: ep_len:500 episode reward: total was -37.850000. running mean: -27.746939\n",
      "ep 1796: ep_len:668 episode reward: total was -37.340000. running mean: -27.842870\n",
      "ep 1796: ep_len:563 episode reward: total was 4.750000. running mean: -27.516941\n",
      "ep 1796: ep_len:3 episode reward: total was 1.010000. running mean: -27.231672\n",
      "ep 1796: ep_len:179 episode reward: total was 19.010000. running mean: -26.769255\n",
      "ep 1796: ep_len:545 episode reward: total was -73.530000. running mean: -27.236863\n",
      "epsilon:0.120333 episode_count: 12579. steps_count: 5453389.000000\n",
      "Time elapsed:  15700.40152144432\n",
      "ep 1797: ep_len:500 episode reward: total was -72.780000. running mean: -27.692294\n",
      "ep 1797: ep_len:512 episode reward: total was 37.030000. running mean: -27.045071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1797: ep_len:509 episode reward: total was -17.300000. running mean: -26.947620\n",
      "ep 1797: ep_len:509 episode reward: total was 17.700000. running mean: -26.501144\n",
      "ep 1797: ep_len:51 episode reward: total was -14.500000. running mean: -26.381133\n",
      "ep 1797: ep_len:500 episode reward: total was 5.320000. running mean: -26.064121\n",
      "ep 1797: ep_len:500 episode reward: total was -12.570000. running mean: -25.929180\n",
      "epsilon:0.120289 episode_count: 12586. steps_count: 5456470.000000\n",
      "Time elapsed:  15708.63999414444\n",
      "ep 1798: ep_len:501 episode reward: total was 10.300000. running mean: -25.566888\n",
      "ep 1798: ep_len:611 episode reward: total was -182.110000. running mean: -27.132319\n",
      "ep 1798: ep_len:613 episode reward: total was -40.940000. running mean: -27.270396\n",
      "ep 1798: ep_len:587 episode reward: total was 31.810000. running mean: -26.679592\n",
      "ep 1798: ep_len:37 episode reward: total was 12.010000. running mean: -26.292696\n",
      "ep 1798: ep_len:310 episode reward: total was 13.580000. running mean: -25.893969\n",
      "ep 1798: ep_len:189 episode reward: total was -21.150000. running mean: -25.846530\n",
      "epsilon:0.120244 episode_count: 12593. steps_count: 5459318.000000\n",
      "Time elapsed:  15718.70333147049\n",
      "ep 1799: ep_len:524 episode reward: total was -145.700000. running mean: -27.045064\n",
      "ep 1799: ep_len:500 episode reward: total was -0.360000. running mean: -26.778214\n",
      "ep 1799: ep_len:500 episode reward: total was -55.090000. running mean: -27.061332\n",
      "ep 1799: ep_len:170 episode reward: total was 4.620000. running mean: -26.744518\n",
      "ep 1799: ep_len:3 episode reward: total was 1.010000. running mean: -26.466973\n",
      "ep 1799: ep_len:500 episode reward: total was -0.990000. running mean: -26.212203\n",
      "ep 1799: ep_len:500 episode reward: total was -86.870000. running mean: -26.818781\n",
      "epsilon:0.120200 episode_count: 12600. steps_count: 5462015.000000\n",
      "Time elapsed:  15726.269994735718\n",
      "ep 1800: ep_len:647 episode reward: total was -155.520000. running mean: -28.105794\n",
      "ep 1800: ep_len:500 episode reward: total was 3.740000. running mean: -27.787336\n",
      "ep 1800: ep_len:551 episode reward: total was -93.140000. running mean: -28.440862\n",
      "ep 1800: ep_len:500 episode reward: total was 1.320000. running mean: -28.143254\n",
      "ep 1800: ep_len:3 episode reward: total was 1.010000. running mean: -27.851721\n",
      "ep 1800: ep_len:646 episode reward: total was 0.300000. running mean: -27.570204\n",
      "ep 1800: ep_len:531 episode reward: total was -103.320000. running mean: -28.327702\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.120156 episode_count: 12607. steps_count: 5465393.000000\n",
      "Time elapsed:  15740.724213838577\n",
      "ep 1801: ep_len:612 episode reward: total was -101.270000. running mean: -29.057125\n",
      "ep 1801: ep_len:591 episode reward: total was -38.970000. running mean: -29.156254\n",
      "ep 1801: ep_len:658 episode reward: total was -129.210000. running mean: -30.156791\n",
      "ep 1801: ep_len:500 episode reward: total was -42.890000. running mean: -30.284123\n",
      "ep 1801: ep_len:94 episode reward: total was -10.730000. running mean: -30.088582\n",
      "ep 1801: ep_len:500 episode reward: total was -75.910000. running mean: -30.546796\n",
      "ep 1801: ep_len:211 episode reward: total was -19.060000. running mean: -30.431928\n",
      "epsilon:0.120111 episode_count: 12614. steps_count: 5468559.000000\n",
      "Time elapsed:  15749.168218374252\n",
      "ep 1802: ep_len:502 episode reward: total was 23.730000. running mean: -29.890309\n",
      "ep 1802: ep_len:606 episode reward: total was -0.270000. running mean: -29.594106\n",
      "ep 1802: ep_len:566 episode reward: total was -58.590000. running mean: -29.884065\n",
      "ep 1802: ep_len:591 episode reward: total was 8.470000. running mean: -29.500524\n",
      "ep 1802: ep_len:127 episode reward: total was 13.330000. running mean: -29.072219\n",
      "ep 1802: ep_len:312 episode reward: total was -10.950000. running mean: -28.890997\n",
      "ep 1802: ep_len:573 episode reward: total was -33.490000. running mean: -28.936987\n",
      "epsilon:0.120067 episode_count: 12621. steps_count: 5471836.000000\n",
      "Time elapsed:  15758.105617284775\n",
      "ep 1803: ep_len:619 episode reward: total was -96.150000. running mean: -29.609117\n",
      "ep 1803: ep_len:500 episode reward: total was 1.820000. running mean: -29.294826\n",
      "ep 1803: ep_len:541 episode reward: total was -3.380000. running mean: -29.035677\n",
      "ep 1803: ep_len:547 episode reward: total was 0.300000. running mean: -28.742321\n",
      "ep 1803: ep_len:51 episode reward: total was 15.000000. running mean: -28.304897\n",
      "ep 1803: ep_len:500 episode reward: total was -53.810000. running mean: -28.559948\n",
      "ep 1803: ep_len:559 episode reward: total was -32.680000. running mean: -28.601149\n",
      "epsilon:0.120023 episode_count: 12628. steps_count: 5475153.000000\n",
      "Time elapsed:  15766.850920438766\n",
      "ep 1804: ep_len:500 episode reward: total was 21.860000. running mean: -28.096537\n",
      "ep 1804: ep_len:528 episode reward: total was 26.760000. running mean: -27.547972\n",
      "ep 1804: ep_len:607 episode reward: total was -24.090000. running mean: -27.513392\n",
      "ep 1804: ep_len:155 episode reward: total was 0.070000. running mean: -27.237558\n",
      "ep 1804: ep_len:3 episode reward: total was 1.010000. running mean: -26.955083\n",
      "ep 1804: ep_len:500 episode reward: total was 0.950000. running mean: -26.676032\n",
      "ep 1804: ep_len:592 episode reward: total was -219.700000. running mean: -28.606272\n",
      "epsilon:0.119978 episode_count: 12635. steps_count: 5478038.000000\n",
      "Time elapsed:  15777.276564598083\n",
      "ep 1805: ep_len:513 episode reward: total was -15.250000. running mean: -28.472709\n",
      "ep 1805: ep_len:521 episode reward: total was 38.320000. running mean: -27.804782\n",
      "ep 1805: ep_len:500 episode reward: total was 5.090000. running mean: -27.475834\n",
      "ep 1805: ep_len:504 episode reward: total was -21.020000. running mean: -27.411276\n",
      "ep 1805: ep_len:3 episode reward: total was 1.010000. running mean: -27.127063\n",
      "ep 1805: ep_len:500 episode reward: total was -38.490000. running mean: -27.240692\n",
      "ep 1805: ep_len:612 episode reward: total was -16.360000. running mean: -27.131885\n",
      "epsilon:0.119934 episode_count: 12642. steps_count: 5481191.000000\n",
      "Time elapsed:  15785.585861682892\n",
      "ep 1806: ep_len:582 episode reward: total was -37.310000. running mean: -27.233667\n",
      "ep 1806: ep_len:562 episode reward: total was -41.250000. running mean: -27.373830\n",
      "ep 1806: ep_len:500 episode reward: total was -47.040000. running mean: -27.570492\n",
      "ep 1806: ep_len:500 episode reward: total was 9.210000. running mean: -27.202687\n",
      "ep 1806: ep_len:108 episode reward: total was -8.230000. running mean: -27.012960\n",
      "ep 1806: ep_len:545 episode reward: total was -18.940000. running mean: -26.932230\n",
      "ep 1806: ep_len:527 episode reward: total was -31.360000. running mean: -26.976508\n",
      "epsilon:0.119890 episode_count: 12649. steps_count: 5484515.000000\n",
      "Time elapsed:  15794.217099189758\n",
      "ep 1807: ep_len:582 episode reward: total was 16.190000. running mean: -26.544843\n",
      "ep 1807: ep_len:673 episode reward: total was 34.580000. running mean: -25.933594\n",
      "ep 1807: ep_len:569 episode reward: total was -24.070000. running mean: -25.914958\n",
      "ep 1807: ep_len:500 episode reward: total was -7.900000. running mean: -25.734809\n",
      "ep 1807: ep_len:85 episode reward: total was 4.240000. running mean: -25.435061\n",
      "ep 1807: ep_len:536 episode reward: total was -46.610000. running mean: -25.646810\n",
      "ep 1807: ep_len:520 episode reward: total was -41.560000. running mean: -25.805942\n",
      "epsilon:0.119845 episode_count: 12656. steps_count: 5487980.000000\n",
      "Time elapsed:  15802.530873537064\n",
      "ep 1808: ep_len:211 episode reward: total was -1.480000. running mean: -25.562683\n",
      "ep 1808: ep_len:501 episode reward: total was -42.410000. running mean: -25.731156\n",
      "ep 1808: ep_len:515 episode reward: total was -36.580000. running mean: -25.839644\n",
      "ep 1808: ep_len:526 episode reward: total was -1.260000. running mean: -25.593848\n",
      "ep 1808: ep_len:1 episode reward: total was -1.000000. running mean: -25.347909\n",
      "ep 1808: ep_len:230 episode reward: total was 11.780000. running mean: -24.976630\n",
      "ep 1808: ep_len:501 episode reward: total was -50.490000. running mean: -25.231764\n",
      "epsilon:0.119801 episode_count: 12663. steps_count: 5490465.000000\n",
      "Time elapsed:  15807.418738365173\n",
      "ep 1809: ep_len:248 episode reward: total was -7.350000. running mean: -25.052946\n",
      "ep 1809: ep_len:510 episode reward: total was -14.340000. running mean: -24.945817\n",
      "ep 1809: ep_len:560 episode reward: total was -20.920000. running mean: -24.905559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1809: ep_len:500 episode reward: total was 10.050000. running mean: -24.556003\n",
      "ep 1809: ep_len:3 episode reward: total was 1.010000. running mean: -24.300343\n",
      "ep 1809: ep_len:561 episode reward: total was -55.990000. running mean: -24.617240\n",
      "ep 1809: ep_len:291 episode reward: total was -16.190000. running mean: -24.532967\n",
      "epsilon:0.119757 episode_count: 12670. steps_count: 5493138.000000\n",
      "Time elapsed:  15814.721989154816\n",
      "ep 1810: ep_len:226 episode reward: total was 1.670000. running mean: -24.270938\n",
      "ep 1810: ep_len:630 episode reward: total was -11.380000. running mean: -24.142028\n",
      "ep 1810: ep_len:612 episode reward: total was -55.980000. running mean: -24.460408\n",
      "ep 1810: ep_len:500 episode reward: total was -29.510000. running mean: -24.510904\n",
      "ep 1810: ep_len:90 episode reward: total was 1.230000. running mean: -24.253495\n",
      "ep 1810: ep_len:584 episode reward: total was -105.100000. running mean: -25.061960\n",
      "ep 1810: ep_len:534 episode reward: total was -12.380000. running mean: -24.935140\n",
      "epsilon:0.119712 episode_count: 12677. steps_count: 5496314.000000\n",
      "Time elapsed:  15823.27082657814\n",
      "ep 1811: ep_len:500 episode reward: total was 27.160000. running mean: -24.414189\n",
      "ep 1811: ep_len:500 episode reward: total was 26.520000. running mean: -23.904847\n",
      "ep 1811: ep_len:500 episode reward: total was -64.370000. running mean: -24.309498\n",
      "ep 1811: ep_len:500 episode reward: total was 42.700000. running mean: -23.639404\n",
      "ep 1811: ep_len:3 episode reward: total was 1.010000. running mean: -23.392909\n",
      "ep 1811: ep_len:226 episode reward: total was 13.880000. running mean: -23.020180\n",
      "ep 1811: ep_len:588 episode reward: total was -33.030000. running mean: -23.120279\n",
      "epsilon:0.119668 episode_count: 12684. steps_count: 5499131.000000\n",
      "Time elapsed:  15830.816625833511\n",
      "ep 1812: ep_len:500 episode reward: total was 13.290000. running mean: -22.756176\n",
      "ep 1812: ep_len:611 episode reward: total was -64.420000. running mean: -23.172814\n",
      "ep 1812: ep_len:518 episode reward: total was -26.920000. running mean: -23.210286\n",
      "ep 1812: ep_len:159 episode reward: total was 2.190000. running mean: -22.956283\n",
      "ep 1812: ep_len:93 episode reward: total was 18.740000. running mean: -22.539320\n",
      "ep 1812: ep_len:506 episode reward: total was -66.790000. running mean: -22.981827\n",
      "ep 1812: ep_len:597 episode reward: total was 3.620000. running mean: -22.715809\n",
      "epsilon:0.119624 episode_count: 12691. steps_count: 5502115.000000\n",
      "Time elapsed:  15838.814289808273\n",
      "ep 1813: ep_len:120 episode reward: total was 8.940000. running mean: -22.399251\n",
      "ep 1813: ep_len:524 episode reward: total was -57.850000. running mean: -22.753758\n",
      "ep 1813: ep_len:582 episode reward: total was -68.230000. running mean: -23.208521\n",
      "ep 1813: ep_len:500 episode reward: total was 48.700000. running mean: -22.489435\n",
      "ep 1813: ep_len:3 episode reward: total was 1.010000. running mean: -22.254441\n",
      "ep 1813: ep_len:544 episode reward: total was -50.590000. running mean: -22.537797\n",
      "ep 1813: ep_len:568 episode reward: total was -158.920000. running mean: -23.901619\n",
      "epsilon:0.119579 episode_count: 12698. steps_count: 5504956.000000\n",
      "Time elapsed:  15846.328478336334\n",
      "ep 1814: ep_len:570 episode reward: total was -172.570000. running mean: -25.388302\n",
      "ep 1814: ep_len:611 episode reward: total was -79.480000. running mean: -25.929219\n",
      "ep 1814: ep_len:649 episode reward: total was -64.860000. running mean: -26.318527\n",
      "ep 1814: ep_len:537 episode reward: total was 40.720000. running mean: -25.648142\n",
      "ep 1814: ep_len:3 episode reward: total was 1.010000. running mean: -25.381561\n",
      "ep 1814: ep_len:536 episode reward: total was -109.740000. running mean: -26.225145\n",
      "ep 1814: ep_len:184 episode reward: total was -46.940000. running mean: -26.432293\n",
      "epsilon:0.119535 episode_count: 12705. steps_count: 5508046.000000\n",
      "Time elapsed:  15852.843163728714\n",
      "ep 1815: ep_len:500 episode reward: total was 23.400000. running mean: -25.933971\n",
      "ep 1815: ep_len:500 episode reward: total was 36.460000. running mean: -25.310031\n",
      "ep 1815: ep_len:567 episode reward: total was -68.300000. running mean: -25.739931\n",
      "ep 1815: ep_len:500 episode reward: total was -11.070000. running mean: -25.593231\n",
      "ep 1815: ep_len:3 episode reward: total was 1.010000. running mean: -25.327199\n",
      "ep 1815: ep_len:501 episode reward: total was -94.450000. running mean: -26.018427\n",
      "ep 1815: ep_len:279 episode reward: total was -5.570000. running mean: -25.813943\n",
      "epsilon:0.119491 episode_count: 12712. steps_count: 5510896.000000\n",
      "Time elapsed:  15862.473550081253\n",
      "ep 1816: ep_len:500 episode reward: total was 21.550000. running mean: -25.340303\n",
      "ep 1816: ep_len:501 episode reward: total was -11.330000. running mean: -25.200200\n",
      "ep 1816: ep_len:517 episode reward: total was -18.690000. running mean: -25.135098\n",
      "ep 1816: ep_len:521 episode reward: total was -8.640000. running mean: -24.970147\n",
      "ep 1816: ep_len:3 episode reward: total was -0.490000. running mean: -24.725346\n",
      "ep 1816: ep_len:589 episode reward: total was -41.800000. running mean: -24.896092\n",
      "ep 1816: ep_len:352 episode reward: total was -5.920000. running mean: -24.706331\n",
      "epsilon:0.119446 episode_count: 12719. steps_count: 5513879.000000\n",
      "Time elapsed:  15870.496504068375\n",
      "ep 1817: ep_len:567 episode reward: total was -56.500000. running mean: -25.024268\n",
      "ep 1817: ep_len:585 episode reward: total was 12.300000. running mean: -24.651025\n",
      "ep 1817: ep_len:619 episode reward: total was -64.870000. running mean: -25.053215\n",
      "ep 1817: ep_len:562 episode reward: total was 27.400000. running mean: -24.528683\n",
      "ep 1817: ep_len:97 episode reward: total was 17.250000. running mean: -24.110896\n",
      "ep 1817: ep_len:500 episode reward: total was -20.660000. running mean: -24.076387\n",
      "ep 1817: ep_len:558 episode reward: total was -26.260000. running mean: -24.098223\n",
      "epsilon:0.119402 episode_count: 12726. steps_count: 5517367.000000\n",
      "Time elapsed:  15882.113107919693\n",
      "ep 1818: ep_len:616 episode reward: total was -91.040000. running mean: -24.767641\n",
      "ep 1818: ep_len:501 episode reward: total was -32.700000. running mean: -24.846965\n",
      "ep 1818: ep_len:565 episode reward: total was -31.730000. running mean: -24.915795\n",
      "ep 1818: ep_len:54 episode reward: total was -14.150000. running mean: -24.808137\n",
      "ep 1818: ep_len:75 episode reward: total was 18.100000. running mean: -24.379056\n",
      "ep 1818: ep_len:563 episode reward: total was -13.830000. running mean: -24.273565\n",
      "ep 1818: ep_len:630 episode reward: total was -37.080000. running mean: -24.401629\n",
      "epsilon:0.119358 episode_count: 12733. steps_count: 5520371.000000\n",
      "Time elapsed:  15892.546243190765\n",
      "ep 1819: ep_len:514 episode reward: total was -57.590000. running mean: -24.733513\n",
      "ep 1819: ep_len:500 episode reward: total was 18.500000. running mean: -24.301178\n",
      "ep 1819: ep_len:524 episode reward: total was -49.900000. running mean: -24.557166\n",
      "ep 1819: ep_len:501 episode reward: total was -58.660000. running mean: -24.898195\n",
      "ep 1819: ep_len:128 episode reward: total was -12.190000. running mean: -24.771113\n",
      "ep 1819: ep_len:523 episode reward: total was -50.710000. running mean: -25.030502\n",
      "ep 1819: ep_len:509 episode reward: total was -42.480000. running mean: -25.204997\n",
      "epsilon:0.119313 episode_count: 12740. steps_count: 5523570.000000\n",
      "Time elapsed:  15900.920098543167\n",
      "ep 1820: ep_len:558 episode reward: total was -2.790000. running mean: -24.980847\n",
      "ep 1820: ep_len:539 episode reward: total was -10.450000. running mean: -24.835538\n",
      "ep 1820: ep_len:79 episode reward: total was 5.340000. running mean: -24.533783\n",
      "ep 1820: ep_len:500 episode reward: total was 0.780000. running mean: -24.280645\n",
      "ep 1820: ep_len:3 episode reward: total was -1.500000. running mean: -24.052838\n",
      "ep 1820: ep_len:629 episode reward: total was -55.620000. running mean: -24.368510\n",
      "ep 1820: ep_len:332 episode reward: total was -3.180000. running mean: -24.156625\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.119269 episode_count: 12747. steps_count: 5526210.000000\n",
      "Time elapsed:  15915.574806213379\n",
      "ep 1821: ep_len:228 episode reward: total was 1.200000. running mean: -23.903059\n",
      "ep 1821: ep_len:500 episode reward: total was 6.910000. running mean: -23.594928\n",
      "ep 1821: ep_len:607 episode reward: total was -66.930000. running mean: -24.028279\n",
      "ep 1821: ep_len:617 episode reward: total was 39.030000. running mean: -23.397696\n",
      "ep 1821: ep_len:3 episode reward: total was 1.010000. running mean: -23.153619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1821: ep_len:500 episode reward: total was -9.680000. running mean: -23.018883\n",
      "ep 1821: ep_len:560 episode reward: total was -5.820000. running mean: -22.846894\n",
      "epsilon:0.119225 episode_count: 12754. steps_count: 5529225.000000\n",
      "Time elapsed:  15925.606463432312\n",
      "ep 1822: ep_len:643 episode reward: total was -55.560000. running mean: -23.174025\n",
      "ep 1822: ep_len:527 episode reward: total was -32.940000. running mean: -23.271685\n",
      "ep 1822: ep_len:554 episode reward: total was -30.940000. running mean: -23.348368\n",
      "ep 1822: ep_len:500 episode reward: total was -2.700000. running mean: -23.141884\n",
      "ep 1822: ep_len:3 episode reward: total was 1.010000. running mean: -22.900365\n",
      "ep 1822: ep_len:553 episode reward: total was -12.040000. running mean: -22.791762\n",
      "ep 1822: ep_len:612 episode reward: total was -30.630000. running mean: -22.870144\n",
      "epsilon:0.119180 episode_count: 12761. steps_count: 5532617.000000\n",
      "Time elapsed:  15936.487334728241\n",
      "ep 1823: ep_len:519 episode reward: total was -49.880000. running mean: -23.140243\n",
      "ep 1823: ep_len:602 episode reward: total was 5.500000. running mean: -22.853840\n",
      "ep 1823: ep_len:500 episode reward: total was -0.130000. running mean: -22.626602\n",
      "ep 1823: ep_len:500 episode reward: total was -4.550000. running mean: -22.445836\n",
      "ep 1823: ep_len:105 episode reward: total was 24.250000. running mean: -21.978878\n",
      "ep 1823: ep_len:500 episode reward: total was -35.410000. running mean: -22.113189\n",
      "ep 1823: ep_len:589 episode reward: total was -29.040000. running mean: -22.182457\n",
      "epsilon:0.119136 episode_count: 12768. steps_count: 5535932.000000\n",
      "Time elapsed:  15945.1739590168\n",
      "ep 1824: ep_len:500 episode reward: total was -12.950000. running mean: -22.090132\n",
      "ep 1824: ep_len:501 episode reward: total was -41.420000. running mean: -22.283431\n",
      "ep 1824: ep_len:580 episode reward: total was -32.550000. running mean: -22.386097\n",
      "ep 1824: ep_len:523 episode reward: total was -59.430000. running mean: -22.756536\n",
      "ep 1824: ep_len:3 episode reward: total was 1.010000. running mean: -22.518870\n",
      "ep 1824: ep_len:571 episode reward: total was -127.430000. running mean: -23.567982\n",
      "ep 1824: ep_len:558 episode reward: total was -36.210000. running mean: -23.694402\n",
      "epsilon:0.119092 episode_count: 12775. steps_count: 5539168.000000\n",
      "Time elapsed:  15953.571649312973\n",
      "ep 1825: ep_len:643 episode reward: total was -13.420000. running mean: -23.591658\n",
      "ep 1825: ep_len:567 episode reward: total was -4.060000. running mean: -23.396341\n",
      "ep 1825: ep_len:655 episode reward: total was -21.130000. running mean: -23.373678\n",
      "ep 1825: ep_len:626 episode reward: total was -83.870000. running mean: -23.978641\n",
      "ep 1825: ep_len:119 episode reward: total was 14.700000. running mean: -23.591855\n",
      "ep 1825: ep_len:563 episode reward: total was -13.480000. running mean: -23.490736\n",
      "ep 1825: ep_len:603 episode reward: total was -84.510000. running mean: -24.100929\n",
      "epsilon:0.119047 episode_count: 12782. steps_count: 5542944.000000\n",
      "Time elapsed:  15963.36988902092\n",
      "ep 1826: ep_len:547 episode reward: total was -66.810000. running mean: -24.528019\n",
      "ep 1826: ep_len:532 episode reward: total was -58.700000. running mean: -24.869739\n",
      "ep 1826: ep_len:562 episode reward: total was 5.220000. running mean: -24.568842\n",
      "ep 1826: ep_len:523 episode reward: total was 34.560000. running mean: -23.977553\n",
      "ep 1826: ep_len:102 episode reward: total was 8.270000. running mean: -23.655078\n",
      "ep 1826: ep_len:601 episode reward: total was -26.810000. running mean: -23.686627\n",
      "ep 1826: ep_len:569 episode reward: total was -42.010000. running mean: -23.869861\n",
      "epsilon:0.119003 episode_count: 12789. steps_count: 5546380.000000\n",
      "Time elapsed:  15972.767839670181\n",
      "ep 1827: ep_len:501 episode reward: total was -7.800000. running mean: -23.709162\n",
      "ep 1827: ep_len:581 episode reward: total was -22.360000. running mean: -23.695671\n",
      "ep 1827: ep_len:412 episode reward: total was 17.960000. running mean: -23.279114\n",
      "ep 1827: ep_len:500 episode reward: total was -28.180000. running mean: -23.328123\n",
      "ep 1827: ep_len:74 episode reward: total was -5.270000. running mean: -23.147542\n",
      "ep 1827: ep_len:622 episode reward: total was -63.680000. running mean: -23.552866\n",
      "ep 1827: ep_len:500 episode reward: total was -22.810000. running mean: -23.545437\n",
      "epsilon:0.118959 episode_count: 12796. steps_count: 5549570.000000\n",
      "Time elapsed:  15981.303646326065\n",
      "ep 1828: ep_len:500 episode reward: total was 14.480000. running mean: -23.165183\n",
      "ep 1828: ep_len:628 episode reward: total was 29.680000. running mean: -22.636731\n",
      "ep 1828: ep_len:533 episode reward: total was -42.630000. running mean: -22.836664\n",
      "ep 1828: ep_len:516 episode reward: total was -34.770000. running mean: -22.955997\n",
      "ep 1828: ep_len:56 episode reward: total was 19.000000. running mean: -22.536437\n",
      "ep 1828: ep_len:504 episode reward: total was -84.100000. running mean: -23.152073\n",
      "ep 1828: ep_len:306 episode reward: total was -41.920000. running mean: -23.339752\n",
      "epsilon:0.118914 episode_count: 12803. steps_count: 5552613.000000\n",
      "Time elapsed:  15989.246253967285\n",
      "ep 1829: ep_len:537 episode reward: total was -0.270000. running mean: -23.109055\n",
      "ep 1829: ep_len:662 episode reward: total was -156.030000. running mean: -24.438264\n",
      "ep 1829: ep_len:584 episode reward: total was -86.680000. running mean: -25.060682\n",
      "ep 1829: ep_len:525 episode reward: total was 17.940000. running mean: -24.630675\n",
      "ep 1829: ep_len:3 episode reward: total was -1.500000. running mean: -24.399368\n",
      "ep 1829: ep_len:582 episode reward: total was -38.910000. running mean: -24.544474\n",
      "ep 1829: ep_len:313 episode reward: total was -51.540000. running mean: -24.814430\n",
      "epsilon:0.118870 episode_count: 12810. steps_count: 5555819.000000\n",
      "Time elapsed:  15997.83482670784\n",
      "ep 1830: ep_len:644 episode reward: total was -50.140000. running mean: -25.067685\n",
      "ep 1830: ep_len:592 episode reward: total was -31.180000. running mean: -25.128808\n",
      "ep 1830: ep_len:500 episode reward: total was -38.750000. running mean: -25.265020\n",
      "ep 1830: ep_len:507 episode reward: total was -58.430000. running mean: -25.596670\n",
      "ep 1830: ep_len:122 episode reward: total was -13.200000. running mean: -25.472703\n",
      "ep 1830: ep_len:569 episode reward: total was -32.970000. running mean: -25.547676\n",
      "ep 1830: ep_len:524 episode reward: total was -39.070000. running mean: -25.682900\n",
      "epsilon:0.118826 episode_count: 12817. steps_count: 5559277.000000\n",
      "Time elapsed:  16006.982867240906\n",
      "ep 1831: ep_len:662 episode reward: total was -100.210000. running mean: -26.428171\n",
      "ep 1831: ep_len:504 episode reward: total was 2.550000. running mean: -26.138389\n",
      "ep 1831: ep_len:500 episode reward: total was -50.320000. running mean: -26.380205\n",
      "ep 1831: ep_len:576 episode reward: total was 6.160000. running mean: -26.054803\n",
      "ep 1831: ep_len:3 episode reward: total was 1.010000. running mean: -25.784155\n",
      "ep 1831: ep_len:538 episode reward: total was -30.220000. running mean: -25.828513\n",
      "ep 1831: ep_len:500 episode reward: total was -25.180000. running mean: -25.822028\n",
      "epsilon:0.118781 episode_count: 12824. steps_count: 5562560.000000\n",
      "Time elapsed:  16017.67172384262\n",
      "ep 1832: ep_len:115 episode reward: total was 1.450000. running mean: -25.549308\n",
      "ep 1832: ep_len:500 episode reward: total was -36.000000. running mean: -25.653815\n",
      "ep 1832: ep_len:546 episode reward: total was 2.540000. running mean: -25.371877\n",
      "ep 1832: ep_len:56 episode reward: total was -8.670000. running mean: -25.204858\n",
      "ep 1832: ep_len:99 episode reward: total was -52.350000. running mean: -25.476309\n",
      "ep 1832: ep_len:308 episode reward: total was -2.020000. running mean: -25.241746\n",
      "ep 1832: ep_len:500 episode reward: total was -19.240000. running mean: -25.181729\n",
      "epsilon:0.118737 episode_count: 12831. steps_count: 5564684.000000\n",
      "Time elapsed:  16023.691641569138\n",
      "ep 1833: ep_len:538 episode reward: total was -28.320000. running mean: -25.213112\n",
      "ep 1833: ep_len:520 episode reward: total was -56.570000. running mean: -25.526680\n",
      "ep 1833: ep_len:624 episode reward: total was -39.100000. running mean: -25.662414\n",
      "ep 1833: ep_len:500 episode reward: total was -21.030000. running mean: -25.616090\n",
      "ep 1833: ep_len:3 episode reward: total was 1.010000. running mean: -25.349829\n",
      "ep 1833: ep_len:634 episode reward: total was -14.620000. running mean: -25.242530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1833: ep_len:207 episode reward: total was -9.980000. running mean: -25.089905\n",
      "epsilon:0.118693 episode_count: 12838. steps_count: 5567710.000000\n",
      "Time elapsed:  16031.791118621826\n",
      "ep 1834: ep_len:616 episode reward: total was -45.460000. running mean: -25.293606\n",
      "ep 1834: ep_len:500 episode reward: total was 40.600000. running mean: -24.634670\n",
      "ep 1834: ep_len:586 episode reward: total was -74.700000. running mean: -25.135323\n",
      "ep 1834: ep_len:558 episode reward: total was 1.420000. running mean: -24.869770\n",
      "ep 1834: ep_len:3 episode reward: total was -1.500000. running mean: -24.636072\n",
      "ep 1834: ep_len:551 episode reward: total was -4.690000. running mean: -24.436612\n",
      "ep 1834: ep_len:586 episode reward: total was -19.060000. running mean: -24.382845\n",
      "epsilon:0.118648 episode_count: 12845. steps_count: 5571110.000000\n",
      "Time elapsed:  16043.238531827927\n",
      "ep 1835: ep_len:174 episode reward: total was 16.970000. running mean: -23.969317\n",
      "ep 1835: ep_len:524 episode reward: total was -34.840000. running mean: -24.078024\n",
      "ep 1835: ep_len:576 episode reward: total was -41.990000. running mean: -24.257144\n",
      "ep 1835: ep_len:509 episode reward: total was 4.660000. running mean: -23.967972\n",
      "ep 1835: ep_len:126 episode reward: total was 15.340000. running mean: -23.574892\n",
      "ep 1835: ep_len:613 episode reward: total was -29.430000. running mean: -23.633443\n",
      "ep 1835: ep_len:545 episode reward: total was -61.950000. running mean: -24.016609\n",
      "epsilon:0.118604 episode_count: 12852. steps_count: 5574177.000000\n",
      "Time elapsed:  16051.487007856369\n",
      "ep 1836: ep_len:743 episode reward: total was -117.940000. running mean: -24.955843\n",
      "ep 1836: ep_len:593 episode reward: total was -38.970000. running mean: -25.095985\n",
      "ep 1836: ep_len:555 episode reward: total was -59.040000. running mean: -25.435425\n",
      "ep 1836: ep_len:157 episode reward: total was -6.440000. running mean: -25.245470\n",
      "ep 1836: ep_len:3 episode reward: total was 1.010000. running mean: -24.982916\n",
      "ep 1836: ep_len:517 episode reward: total was -71.460000. running mean: -25.447687\n",
      "ep 1836: ep_len:513 episode reward: total was -16.780000. running mean: -25.361010\n",
      "epsilon:0.118560 episode_count: 12859. steps_count: 5577258.000000\n",
      "Time elapsed:  16061.989421129227\n",
      "ep 1837: ep_len:514 episode reward: total was 17.940000. running mean: -24.928000\n",
      "ep 1837: ep_len:544 episode reward: total was 25.070000. running mean: -24.428020\n",
      "ep 1837: ep_len:557 episode reward: total was -48.430000. running mean: -24.668039\n",
      "ep 1837: ep_len:571 episode reward: total was 26.440000. running mean: -24.156959\n",
      "ep 1837: ep_len:3 episode reward: total was 1.010000. running mean: -23.905289\n",
      "ep 1837: ep_len:543 episode reward: total was -2.290000. running mean: -23.689137\n",
      "ep 1837: ep_len:613 episode reward: total was -37.770000. running mean: -23.829945\n",
      "epsilon:0.118515 episode_count: 12866. steps_count: 5580603.000000\n",
      "Time elapsed:  16070.55937552452\n",
      "ep 1838: ep_len:637 episode reward: total was -86.880000. running mean: -24.460446\n",
      "ep 1838: ep_len:500 episode reward: total was -59.990000. running mean: -24.815741\n",
      "ep 1838: ep_len:500 episode reward: total was -15.260000. running mean: -24.720184\n",
      "ep 1838: ep_len:594 episode reward: total was 48.810000. running mean: -23.984882\n",
      "ep 1838: ep_len:3 episode reward: total was 0.000000. running mean: -23.745033\n",
      "ep 1838: ep_len:507 episode reward: total was -21.630000. running mean: -23.723883\n",
      "ep 1838: ep_len:573 episode reward: total was -85.070000. running mean: -24.337344\n",
      "epsilon:0.118471 episode_count: 12873. steps_count: 5583917.000000\n",
      "Time elapsed:  16080.312350273132\n",
      "ep 1839: ep_len:540 episode reward: total was 16.260000. running mean: -23.931371\n",
      "ep 1839: ep_len:500 episode reward: total was 1.130000. running mean: -23.680757\n",
      "ep 1839: ep_len:398 episode reward: total was 0.780000. running mean: -23.436149\n",
      "ep 1839: ep_len:504 episode reward: total was -20.110000. running mean: -23.402888\n",
      "ep 1839: ep_len:3 episode reward: total was 1.010000. running mean: -23.158759\n",
      "ep 1839: ep_len:538 episode reward: total was -62.950000. running mean: -23.556671\n",
      "ep 1839: ep_len:184 episode reward: total was -23.590000. running mean: -23.557005\n",
      "epsilon:0.118427 episode_count: 12880. steps_count: 5586584.000000\n",
      "Time elapsed:  16086.32016301155\n",
      "ep 1840: ep_len:119 episode reward: total was -13.080000. running mean: -23.452235\n",
      "ep 1840: ep_len:500 episode reward: total was 11.030000. running mean: -23.107412\n",
      "ep 1840: ep_len:666 episode reward: total was -56.830000. running mean: -23.444638\n",
      "ep 1840: ep_len:132 episode reward: total was 0.010000. running mean: -23.210092\n",
      "ep 1840: ep_len:86 episode reward: total was 10.250000. running mean: -22.875491\n",
      "ep 1840: ep_len:688 episode reward: total was -31.390000. running mean: -22.960636\n",
      "ep 1840: ep_len:192 episode reward: total was -15.150000. running mean: -22.882530\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.118382 episode_count: 12887. steps_count: 5588967.000000\n",
      "Time elapsed:  16097.413354873657\n",
      "ep 1841: ep_len:660 episode reward: total was -63.720000. running mean: -23.290904\n",
      "ep 1841: ep_len:500 episode reward: total was -14.060000. running mean: -23.198595\n",
      "ep 1841: ep_len:583 episode reward: total was -54.400000. running mean: -23.510609\n",
      "ep 1841: ep_len:550 episode reward: total was 34.890000. running mean: -22.926603\n",
      "ep 1841: ep_len:94 episode reward: total was 18.260000. running mean: -22.514737\n",
      "ep 1841: ep_len:169 episode reward: total was 28.020000. running mean: -22.009390\n",
      "ep 1841: ep_len:666 episode reward: total was -54.040000. running mean: -22.329696\n",
      "epsilon:0.118338 episode_count: 12894. steps_count: 5592189.000000\n",
      "Time elapsed:  16106.230635404587\n",
      "ep 1842: ep_len:227 episode reward: total was -7.810000. running mean: -22.184499\n",
      "ep 1842: ep_len:184 episode reward: total was -25.910000. running mean: -22.221754\n",
      "ep 1842: ep_len:445 episode reward: total was -6.520000. running mean: -22.064736\n",
      "ep 1842: ep_len:516 episode reward: total was -6.720000. running mean: -21.911289\n",
      "ep 1842: ep_len:3 episode reward: total was 0.000000. running mean: -21.692176\n",
      "ep 1842: ep_len:631 episode reward: total was -35.060000. running mean: -21.825854\n",
      "ep 1842: ep_len:624 episode reward: total was -45.840000. running mean: -22.065996\n",
      "epsilon:0.118294 episode_count: 12901. steps_count: 5594819.000000\n",
      "Time elapsed:  16113.221842288971\n",
      "ep 1843: ep_len:595 episode reward: total was 24.310000. running mean: -21.602236\n",
      "ep 1843: ep_len:526 episode reward: total was 38.910000. running mean: -20.997114\n",
      "ep 1843: ep_len:608 episode reward: total was -50.480000. running mean: -21.291942\n",
      "ep 1843: ep_len:160 episode reward: total was 0.140000. running mean: -21.077623\n",
      "ep 1843: ep_len:3 episode reward: total was 1.010000. running mean: -20.856747\n",
      "ep 1843: ep_len:624 episode reward: total was -46.110000. running mean: -21.109279\n",
      "ep 1843: ep_len:500 episode reward: total was -36.470000. running mean: -21.262886\n",
      "epsilon:0.118249 episode_count: 12908. steps_count: 5597835.000000\n",
      "Time elapsed:  16121.0409283638\n",
      "ep 1844: ep_len:265 episode reward: total was -23.250000. running mean: -21.282758\n",
      "ep 1844: ep_len:606 episode reward: total was 11.280000. running mean: -20.957130\n",
      "ep 1844: ep_len:866 episode reward: total was -297.020000. running mean: -23.717759\n",
      "ep 1844: ep_len:530 episode reward: total was -339.170000. running mean: -26.872281\n",
      "ep 1844: ep_len:3 episode reward: total was 1.010000. running mean: -26.593458\n",
      "ep 1844: ep_len:610 episode reward: total was -24.270000. running mean: -26.570224\n",
      "ep 1844: ep_len:500 episode reward: total was -172.480000. running mean: -28.029322\n",
      "epsilon:0.118205 episode_count: 12915. steps_count: 5601215.000000\n",
      "Time elapsed:  16129.966154813766\n",
      "ep 1845: ep_len:646 episode reward: total was 26.070000. running mean: -27.488328\n",
      "ep 1845: ep_len:501 episode reward: total was 23.280000. running mean: -26.980645\n",
      "ep 1845: ep_len:79 episode reward: total was -8.250000. running mean: -26.793339\n",
      "ep 1845: ep_len:500 episode reward: total was -8.140000. running mean: -26.606805\n",
      "ep 1845: ep_len:3 episode reward: total was 1.010000. running mean: -26.330637\n",
      "ep 1845: ep_len:174 episode reward: total was 28.220000. running mean: -25.785131\n",
      "ep 1845: ep_len:211 episode reward: total was -48.690000. running mean: -26.014179\n",
      "epsilon:0.118161 episode_count: 12922. steps_count: 5603329.000000\n",
      "Time elapsed:  16140.649696350098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1846: ep_len:610 episode reward: total was 21.010000. running mean: -25.543938\n",
      "ep 1846: ep_len:534 episode reward: total was -61.300000. running mean: -25.901498\n",
      "ep 1846: ep_len:378 episode reward: total was 11.680000. running mean: -25.525683\n",
      "ep 1846: ep_len:503 episode reward: total was -32.940000. running mean: -25.599826\n",
      "ep 1846: ep_len:94 episode reward: total was 14.740000. running mean: -25.196428\n",
      "ep 1846: ep_len:500 episode reward: total was -34.950000. running mean: -25.293964\n",
      "ep 1846: ep_len:536 episode reward: total was -137.320000. running mean: -26.414224\n",
      "epsilon:0.118116 episode_count: 12929. steps_count: 5606484.000000\n",
      "Time elapsed:  16151.09959936142\n",
      "ep 1847: ep_len:532 episode reward: total was -63.800000. running mean: -26.788082\n",
      "ep 1847: ep_len:582 episode reward: total was 3.330000. running mean: -26.486901\n",
      "ep 1847: ep_len:59 episode reward: total was 4.160000. running mean: -26.180432\n",
      "ep 1847: ep_len:537 episode reward: total was 17.550000. running mean: -25.743128\n",
      "ep 1847: ep_len:3 episode reward: total was 0.000000. running mean: -25.485697\n",
      "ep 1847: ep_len:501 episode reward: total was -16.950000. running mean: -25.400340\n",
      "ep 1847: ep_len:556 episode reward: total was -10.060000. running mean: -25.246936\n",
      "epsilon:0.118072 episode_count: 12936. steps_count: 5609254.000000\n",
      "Time elapsed:  16160.635455846786\n",
      "ep 1848: ep_len:566 episode reward: total was 8.230000. running mean: -24.912167\n",
      "ep 1848: ep_len:584 episode reward: total was -54.670000. running mean: -25.209745\n",
      "ep 1848: ep_len:641 episode reward: total was -68.250000. running mean: -25.640148\n",
      "ep 1848: ep_len:501 episode reward: total was -1.190000. running mean: -25.395646\n",
      "ep 1848: ep_len:49 episode reward: total was 20.000000. running mean: -24.941690\n",
      "ep 1848: ep_len:543 episode reward: total was -34.520000. running mean: -25.037473\n",
      "ep 1848: ep_len:513 episode reward: total was -36.900000. running mean: -25.156098\n",
      "epsilon:0.118028 episode_count: 12943. steps_count: 5612651.000000\n",
      "Time elapsed:  16171.567070484161\n",
      "ep 1849: ep_len:523 episode reward: total was -66.360000. running mean: -25.568137\n",
      "ep 1849: ep_len:641 episode reward: total was 34.710000. running mean: -24.965356\n",
      "ep 1849: ep_len:568 episode reward: total was -45.490000. running mean: -25.170602\n",
      "ep 1849: ep_len:500 episode reward: total was -54.850000. running mean: -25.467396\n",
      "ep 1849: ep_len:110 episode reward: total was 22.250000. running mean: -24.990222\n",
      "ep 1849: ep_len:500 episode reward: total was -26.200000. running mean: -25.002320\n",
      "ep 1849: ep_len:500 episode reward: total was -58.580000. running mean: -25.338097\n",
      "epsilon:0.117983 episode_count: 12950. steps_count: 5615993.000000\n",
      "Time elapsed:  16182.328522205353\n",
      "ep 1850: ep_len:505 episode reward: total was 8.230000. running mean: -25.002416\n",
      "ep 1850: ep_len:595 episode reward: total was 56.660000. running mean: -24.185792\n",
      "ep 1850: ep_len:543 episode reward: total was -61.950000. running mean: -24.563434\n",
      "ep 1850: ep_len:500 episode reward: total was -61.310000. running mean: -24.930899\n",
      "ep 1850: ep_len:3 episode reward: total was 1.010000. running mean: -24.671490\n",
      "ep 1850: ep_len:643 episode reward: total was -41.540000. running mean: -24.840176\n",
      "ep 1850: ep_len:565 episode reward: total was -36.670000. running mean: -24.958474\n",
      "epsilon:0.117939 episode_count: 12957. steps_count: 5619347.000000\n",
      "Time elapsed:  16191.227575302124\n",
      "ep 1851: ep_len:656 episode reward: total was -84.030000. running mean: -25.549189\n",
      "ep 1851: ep_len:604 episode reward: total was -4.660000. running mean: -25.340297\n",
      "ep 1851: ep_len:358 episode reward: total was -16.290000. running mean: -25.249794\n",
      "ep 1851: ep_len:595 episode reward: total was 29.470000. running mean: -24.702596\n",
      "ep 1851: ep_len:3 episode reward: total was 1.010000. running mean: -24.445470\n",
      "ep 1851: ep_len:513 episode reward: total was -24.860000. running mean: -24.449616\n",
      "ep 1851: ep_len:581 episode reward: total was -32.690000. running mean: -24.532019\n",
      "epsilon:0.117895 episode_count: 12964. steps_count: 5622657.000000\n",
      "Time elapsed:  16200.113060712814\n",
      "ep 1852: ep_len:549 episode reward: total was -96.820000. running mean: -25.254899\n",
      "ep 1852: ep_len:630 episode reward: total was -1.100000. running mean: -25.013350\n",
      "ep 1852: ep_len:430 episode reward: total was -4.870000. running mean: -24.811917\n",
      "ep 1852: ep_len:511 episode reward: total was -23.920000. running mean: -24.802998\n",
      "ep 1852: ep_len:3 episode reward: total was 0.000000. running mean: -24.554968\n",
      "ep 1852: ep_len:626 episode reward: total was -58.640000. running mean: -24.895818\n",
      "ep 1852: ep_len:500 episode reward: total was -39.770000. running mean: -25.044560\n",
      "epsilon:0.117850 episode_count: 12971. steps_count: 5625906.000000\n",
      "Time elapsed:  16208.83353972435\n",
      "ep 1853: ep_len:500 episode reward: total was -85.370000. running mean: -25.647814\n",
      "ep 1853: ep_len:500 episode reward: total was 37.220000. running mean: -25.019136\n",
      "ep 1853: ep_len:548 episode reward: total was -36.580000. running mean: -25.134745\n",
      "ep 1853: ep_len:578 episode reward: total was -8.010000. running mean: -24.963497\n",
      "ep 1853: ep_len:3 episode reward: total was 0.000000. running mean: -24.713862\n",
      "ep 1853: ep_len:534 episode reward: total was -40.070000. running mean: -24.867424\n",
      "ep 1853: ep_len:500 episode reward: total was -94.900000. running mean: -25.567749\n",
      "epsilon:0.117806 episode_count: 12978. steps_count: 5629069.000000\n",
      "Time elapsed:  16219.439014196396\n",
      "ep 1854: ep_len:206 episode reward: total was 20.840000. running mean: -25.103672\n",
      "ep 1854: ep_len:521 episode reward: total was 7.420000. running mean: -24.778435\n",
      "ep 1854: ep_len:617 episode reward: total was -78.170000. running mean: -25.312351\n",
      "ep 1854: ep_len:118 episode reward: total was -3.480000. running mean: -25.094027\n",
      "ep 1854: ep_len:2 episode reward: total was -0.500000. running mean: -24.848087\n",
      "ep 1854: ep_len:512 episode reward: total was -145.700000. running mean: -26.056606\n",
      "ep 1854: ep_len:526 episode reward: total was -42.280000. running mean: -26.218840\n",
      "epsilon:0.117762 episode_count: 12985. steps_count: 5631571.000000\n",
      "Time elapsed:  16226.250692129135\n",
      "ep 1855: ep_len:632 episode reward: total was 0.640000. running mean: -25.950252\n",
      "ep 1855: ep_len:500 episode reward: total was -94.110000. running mean: -26.631849\n",
      "ep 1855: ep_len:512 episode reward: total was -63.890000. running mean: -27.004431\n",
      "ep 1855: ep_len:394 episode reward: total was -45.360000. running mean: -27.187986\n",
      "ep 1855: ep_len:3 episode reward: total was 1.010000. running mean: -26.906007\n",
      "ep 1855: ep_len:500 episode reward: total was 0.560000. running mean: -26.631346\n",
      "ep 1855: ep_len:292 episode reward: total was -17.310000. running mean: -26.538133\n",
      "epsilon:0.117717 episode_count: 12992. steps_count: 5634404.000000\n",
      "Time elapsed:  16233.75068116188\n",
      "ep 1856: ep_len:619 episode reward: total was -27.830000. running mean: -26.551052\n",
      "ep 1856: ep_len:524 episode reward: total was 23.920000. running mean: -26.046341\n",
      "ep 1856: ep_len:576 episode reward: total was -64.360000. running mean: -26.429478\n",
      "ep 1856: ep_len:170 episode reward: total was -6.840000. running mean: -26.233583\n",
      "ep 1856: ep_len:3 episode reward: total was 1.010000. running mean: -25.961147\n",
      "ep 1856: ep_len:653 episode reward: total was -29.960000. running mean: -26.001136\n",
      "ep 1856: ep_len:500 episode reward: total was -2.800000. running mean: -25.769124\n",
      "epsilon:0.117673 episode_count: 12999. steps_count: 5637449.000000\n",
      "Time elapsed:  16244.01715540886\n",
      "ep 1857: ep_len:619 episode reward: total was -54.220000. running mean: -26.053633\n",
      "ep 1857: ep_len:500 episode reward: total was -28.280000. running mean: -26.075897\n",
      "ep 1857: ep_len:538 episode reward: total was -2.680000. running mean: -25.841938\n",
      "ep 1857: ep_len:563 episode reward: total was -0.720000. running mean: -25.590718\n",
      "ep 1857: ep_len:3 episode reward: total was 1.010000. running mean: -25.324711\n",
      "ep 1857: ep_len:500 episode reward: total was -53.930000. running mean: -25.610764\n",
      "ep 1857: ep_len:500 episode reward: total was -97.990000. running mean: -26.334556\n",
      "epsilon:0.117629 episode_count: 13006. steps_count: 5640672.000000\n",
      "Time elapsed:  16252.414681196213\n",
      "ep 1858: ep_len:611 episode reward: total was 25.610000. running mean: -25.815111\n",
      "ep 1858: ep_len:500 episode reward: total was -32.670000. running mean: -25.883660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1858: ep_len:569 episode reward: total was -117.670000. running mean: -26.801523\n",
      "ep 1858: ep_len:153 episode reward: total was 15.570000. running mean: -26.377808\n",
      "ep 1858: ep_len:96 episode reward: total was 14.240000. running mean: -25.971630\n",
      "ep 1858: ep_len:537 episode reward: total was -25.670000. running mean: -25.968614\n",
      "ep 1858: ep_len:545 episode reward: total was -29.900000. running mean: -26.007927\n",
      "epsilon:0.117584 episode_count: 13013. steps_count: 5643683.000000\n",
      "Time elapsed:  16260.128093957901\n",
      "ep 1859: ep_len:239 episode reward: total was 7.750000. running mean: -25.670348\n",
      "ep 1859: ep_len:589 episode reward: total was -9.020000. running mean: -25.503845\n",
      "ep 1859: ep_len:71 episode reward: total was 3.700000. running mean: -25.211806\n",
      "ep 1859: ep_len:577 episode reward: total was 12.460000. running mean: -24.835088\n",
      "ep 1859: ep_len:93 episode reward: total was 13.750000. running mean: -24.449237\n",
      "ep 1859: ep_len:586 episode reward: total was -76.970000. running mean: -24.974445\n",
      "ep 1859: ep_len:589 episode reward: total was -35.450000. running mean: -25.079200\n",
      "epsilon:0.117540 episode_count: 13020. steps_count: 5646427.000000\n",
      "Time elapsed:  16266.95063662529\n",
      "ep 1860: ep_len:535 episode reward: total was -11.860000. running mean: -24.947008\n",
      "ep 1860: ep_len:337 episode reward: total was -62.510000. running mean: -25.322638\n",
      "ep 1860: ep_len:543 episode reward: total was -57.770000. running mean: -25.647112\n",
      "ep 1860: ep_len:525 episode reward: total was -42.010000. running mean: -25.810741\n",
      "ep 1860: ep_len:3 episode reward: total was 0.000000. running mean: -25.552633\n",
      "ep 1860: ep_len:513 episode reward: total was -34.820000. running mean: -25.645307\n",
      "ep 1860: ep_len:500 episode reward: total was -78.860000. running mean: -26.177454\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.117496 episode_count: 13027. steps_count: 5649383.000000\n",
      "Time elapsed:  16277.566667556763\n",
      "ep 1861: ep_len:632 episode reward: total was -61.390000. running mean: -26.529580\n",
      "ep 1861: ep_len:549 episode reward: total was -84.380000. running mean: -27.108084\n",
      "ep 1861: ep_len:556 episode reward: total was -53.840000. running mean: -27.375403\n",
      "ep 1861: ep_len:532 episode reward: total was -134.450000. running mean: -28.446149\n",
      "ep 1861: ep_len:112 episode reward: total was 7.850000. running mean: -28.083187\n",
      "ep 1861: ep_len:531 episode reward: total was -29.530000. running mean: -28.097655\n",
      "ep 1861: ep_len:577 episode reward: total was -19.260000. running mean: -28.009279\n",
      "epsilon:0.117451 episode_count: 13034. steps_count: 5652872.000000\n",
      "Time elapsed:  16291.436868667603\n",
      "ep 1862: ep_len:570 episode reward: total was 26.110000. running mean: -27.468086\n",
      "ep 1862: ep_len:500 episode reward: total was 0.400000. running mean: -27.189405\n",
      "ep 1862: ep_len:500 episode reward: total was -17.720000. running mean: -27.094711\n",
      "ep 1862: ep_len:501 episode reward: total was -30.460000. running mean: -27.128364\n",
      "ep 1862: ep_len:3 episode reward: total was 1.010000. running mean: -26.846980\n",
      "ep 1862: ep_len:500 episode reward: total was -44.190000. running mean: -27.020411\n",
      "ep 1862: ep_len:500 episode reward: total was -38.970000. running mean: -27.139907\n",
      "epsilon:0.117407 episode_count: 13041. steps_count: 5655946.000000\n",
      "Time elapsed:  16299.320865154266\n",
      "ep 1863: ep_len:500 episode reward: total was 29.460000. running mean: -26.573908\n",
      "ep 1863: ep_len:619 episode reward: total was -110.600000. running mean: -27.414168\n",
      "ep 1863: ep_len:600 episode reward: total was -40.680000. running mean: -27.546827\n",
      "ep 1863: ep_len:516 episode reward: total was -28.000000. running mean: -27.551358\n",
      "ep 1863: ep_len:106 episode reward: total was 11.280000. running mean: -27.163045\n",
      "ep 1863: ep_len:568 episode reward: total was 22.000000. running mean: -26.671414\n",
      "ep 1863: ep_len:560 episode reward: total was -12.750000. running mean: -26.532200\n",
      "epsilon:0.117363 episode_count: 13048. steps_count: 5659415.000000\n",
      "Time elapsed:  16307.094173431396\n",
      "ep 1864: ep_len:527 episode reward: total was -77.070000. running mean: -27.037578\n",
      "ep 1864: ep_len:500 episode reward: total was -171.130000. running mean: -28.478503\n",
      "ep 1864: ep_len:635 episode reward: total was -46.490000. running mean: -28.658617\n",
      "ep 1864: ep_len:552 episode reward: total was 1.330000. running mean: -28.358731\n",
      "ep 1864: ep_len:3 episode reward: total was 1.010000. running mean: -28.065044\n",
      "ep 1864: ep_len:582 episode reward: total was 3.380000. running mean: -27.750594\n",
      "ep 1864: ep_len:630 episode reward: total was -7.920000. running mean: -27.552288\n",
      "epsilon:0.117318 episode_count: 13055. steps_count: 5662844.000000\n",
      "Time elapsed:  16318.172836065292\n",
      "ep 1865: ep_len:602 episode reward: total was -100.210000. running mean: -28.278865\n",
      "ep 1865: ep_len:600 episode reward: total was -44.500000. running mean: -28.441076\n",
      "ep 1865: ep_len:549 episode reward: total was -65.010000. running mean: -28.806765\n",
      "ep 1865: ep_len:711 episode reward: total was -163.190000. running mean: -30.150598\n",
      "ep 1865: ep_len:94 episode reward: total was 5.250000. running mean: -29.796592\n",
      "ep 1865: ep_len:158 episode reward: total was 19.490000. running mean: -29.303726\n",
      "ep 1865: ep_len:283 episode reward: total was -25.940000. running mean: -29.270089\n",
      "epsilon:0.117274 episode_count: 13062. steps_count: 5665841.000000\n",
      "Time elapsed:  16326.894014120102\n",
      "ep 1866: ep_len:516 episode reward: total was -35.600000. running mean: -29.333388\n",
      "ep 1866: ep_len:516 episode reward: total was -18.350000. running mean: -29.223554\n",
      "ep 1866: ep_len:561 episode reward: total was -42.360000. running mean: -29.354918\n",
      "ep 1866: ep_len:502 episode reward: total was -7.750000. running mean: -29.138869\n",
      "ep 1866: ep_len:3 episode reward: total was 1.010000. running mean: -28.837380\n",
      "ep 1866: ep_len:500 episode reward: total was 0.260000. running mean: -28.546407\n",
      "ep 1866: ep_len:308 episode reward: total was -17.730000. running mean: -28.438242\n",
      "epsilon:0.117230 episode_count: 13069. steps_count: 5668747.000000\n",
      "Time elapsed:  16334.677850723267\n",
      "ep 1867: ep_len:500 episode reward: total was -68.690000. running mean: -28.840760\n",
      "ep 1867: ep_len:500 episode reward: total was -43.650000. running mean: -28.988852\n",
      "ep 1867: ep_len:649 episode reward: total was -45.740000. running mean: -29.156364\n",
      "ep 1867: ep_len:518 episode reward: total was 17.210000. running mean: -28.692700\n",
      "ep 1867: ep_len:107 episode reward: total was -0.280000. running mean: -28.408573\n",
      "ep 1867: ep_len:500 episode reward: total was -39.110000. running mean: -28.515588\n",
      "ep 1867: ep_len:557 episode reward: total was -55.000000. running mean: -28.780432\n",
      "epsilon:0.117185 episode_count: 13076. steps_count: 5672078.000000\n",
      "Time elapsed:  16345.60367488861\n",
      "ep 1868: ep_len:500 episode reward: total was 20.240000. running mean: -28.290227\n",
      "ep 1868: ep_len:501 episode reward: total was -4.440000. running mean: -28.051725\n",
      "ep 1868: ep_len:382 episode reward: total was 4.870000. running mean: -27.722508\n",
      "ep 1868: ep_len:545 episode reward: total was -37.340000. running mean: -27.818683\n",
      "ep 1868: ep_len:99 episode reward: total was 15.740000. running mean: -27.383096\n",
      "ep 1868: ep_len:583 episode reward: total was -173.310000. running mean: -28.842365\n",
      "ep 1868: ep_len:615 episode reward: total was -16.810000. running mean: -28.722041\n",
      "epsilon:0.117141 episode_count: 13083. steps_count: 5675303.000000\n",
      "Time elapsed:  16356.12853717804\n",
      "ep 1869: ep_len:519 episode reward: total was 4.250000. running mean: -28.392321\n",
      "ep 1869: ep_len:500 episode reward: total was -98.860000. running mean: -29.096998\n",
      "ep 1869: ep_len:503 episode reward: total was -1.320000. running mean: -28.819228\n",
      "ep 1869: ep_len:116 episode reward: total was 8.970000. running mean: -28.441335\n",
      "ep 1869: ep_len:73 episode reward: total was -25.730000. running mean: -28.414222\n",
      "ep 1869: ep_len:517 episode reward: total was -35.180000. running mean: -28.481880\n",
      "ep 1869: ep_len:270 episode reward: total was -42.010000. running mean: -28.617161\n",
      "epsilon:0.117097 episode_count: 13090. steps_count: 5677801.000000\n",
      "Time elapsed:  16363.077322244644\n",
      "ep 1870: ep_len:205 episode reward: total was -0.810000. running mean: -28.339089\n",
      "ep 1870: ep_len:500 episode reward: total was -18.360000. running mean: -28.239299\n",
      "ep 1870: ep_len:542 episode reward: total was -47.110000. running mean: -28.428006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1870: ep_len:121 episode reward: total was 17.620000. running mean: -27.967526\n",
      "ep 1870: ep_len:90 episode reward: total was 1.600000. running mean: -27.671850\n",
      "ep 1870: ep_len:513 episode reward: total was -48.050000. running mean: -27.875632\n",
      "ep 1870: ep_len:505 episode reward: total was -63.440000. running mean: -28.231275\n",
      "epsilon:0.117052 episode_count: 13097. steps_count: 5680277.000000\n",
      "Time elapsed:  16370.65725684166\n",
      "ep 1871: ep_len:579 episode reward: total was -64.250000. running mean: -28.591463\n",
      "ep 1871: ep_len:506 episode reward: total was -100.120000. running mean: -29.306748\n",
      "ep 1871: ep_len:612 episode reward: total was -10.490000. running mean: -29.118581\n",
      "ep 1871: ep_len:508 episode reward: total was 5.200000. running mean: -28.775395\n",
      "ep 1871: ep_len:3 episode reward: total was 1.010000. running mean: -28.477541\n",
      "ep 1871: ep_len:527 episode reward: total was -23.310000. running mean: -28.425865\n",
      "ep 1871: ep_len:500 episode reward: total was -40.840000. running mean: -28.550007\n",
      "epsilon:0.117008 episode_count: 13104. steps_count: 5683512.000000\n",
      "Time elapsed:  16380.984738111496\n",
      "ep 1872: ep_len:219 episode reward: total was 2.730000. running mean: -28.237207\n",
      "ep 1872: ep_len:558 episode reward: total was 3.820000. running mean: -27.916635\n",
      "ep 1872: ep_len:534 episode reward: total was -27.300000. running mean: -27.910468\n",
      "ep 1872: ep_len:628 episode reward: total was 21.660000. running mean: -27.414764\n",
      "ep 1872: ep_len:42 episode reward: total was 19.500000. running mean: -26.945616\n",
      "ep 1872: ep_len:554 episode reward: total was 25.040000. running mean: -26.425760\n",
      "ep 1872: ep_len:503 episode reward: total was -43.950000. running mean: -26.601002\n",
      "epsilon:0.116964 episode_count: 13111. steps_count: 5686550.000000\n",
      "Time elapsed:  16388.888954639435\n",
      "ep 1873: ep_len:500 episode reward: total was 34.260000. running mean: -25.992392\n",
      "ep 1873: ep_len:505 episode reward: total was -67.890000. running mean: -26.411368\n",
      "ep 1873: ep_len:607 episode reward: total was -59.020000. running mean: -26.737455\n",
      "ep 1873: ep_len:516 episode reward: total was -5.810000. running mean: -26.528180\n",
      "ep 1873: ep_len:3 episode reward: total was -3.000000. running mean: -26.292898\n",
      "ep 1873: ep_len:589 episode reward: total was -35.190000. running mean: -26.381869\n",
      "ep 1873: ep_len:607 episode reward: total was -48.790000. running mean: -26.605951\n",
      "epsilon:0.116919 episode_count: 13118. steps_count: 5689877.000000\n",
      "Time elapsed:  16397.87175679207\n",
      "ep 1874: ep_len:569 episode reward: total was -26.180000. running mean: -26.601691\n",
      "ep 1874: ep_len:526 episode reward: total was 17.120000. running mean: -26.164474\n",
      "ep 1874: ep_len:500 episode reward: total was -7.270000. running mean: -25.975529\n",
      "ep 1874: ep_len:556 episode reward: total was -58.900000. running mean: -26.304774\n",
      "ep 1874: ep_len:3 episode reward: total was 0.000000. running mean: -26.041726\n",
      "ep 1874: ep_len:297 episode reward: total was -11.590000. running mean: -25.897209\n",
      "ep 1874: ep_len:263 episode reward: total was -32.870000. running mean: -25.966937\n",
      "epsilon:0.116875 episode_count: 13125. steps_count: 5692591.000000\n",
      "Time elapsed:  16407.55751800537\n",
      "ep 1875: ep_len:500 episode reward: total was 30.370000. running mean: -25.403568\n",
      "ep 1875: ep_len:500 episode reward: total was -31.240000. running mean: -25.461932\n",
      "ep 1875: ep_len:571 episode reward: total was -9.080000. running mean: -25.298113\n",
      "ep 1875: ep_len:574 episode reward: total was 12.910000. running mean: -24.916032\n",
      "ep 1875: ep_len:3 episode reward: total was -1.500000. running mean: -24.681871\n",
      "ep 1875: ep_len:588 episode reward: total was -27.940000. running mean: -24.714453\n",
      "ep 1875: ep_len:557 episode reward: total was -69.780000. running mean: -25.165108\n",
      "epsilon:0.116831 episode_count: 13132. steps_count: 5695884.000000\n",
      "Time elapsed:  16416.172417402267\n",
      "ep 1876: ep_len:561 episode reward: total was -4.520000. running mean: -24.958657\n",
      "ep 1876: ep_len:271 episode reward: total was -5.950000. running mean: -24.768570\n",
      "ep 1876: ep_len:537 episode reward: total was -36.310000. running mean: -24.883985\n",
      "ep 1876: ep_len:384 episode reward: total was -23.110000. running mean: -24.866245\n",
      "ep 1876: ep_len:3 episode reward: total was -0.490000. running mean: -24.622482\n",
      "ep 1876: ep_len:523 episode reward: total was -61.300000. running mean: -24.989258\n",
      "ep 1876: ep_len:211 episode reward: total was -29.650000. running mean: -25.035865\n",
      "epsilon:0.116786 episode_count: 13139. steps_count: 5698374.000000\n",
      "Time elapsed:  16425.48712515831\n",
      "ep 1877: ep_len:123 episode reward: total was -6.120000. running mean: -24.846706\n",
      "ep 1877: ep_len:507 episode reward: total was -35.280000. running mean: -24.951039\n",
      "ep 1877: ep_len:500 episode reward: total was -23.540000. running mean: -24.936929\n",
      "ep 1877: ep_len:118 episode reward: total was 9.390000. running mean: -24.593660\n",
      "ep 1877: ep_len:3 episode reward: total was 1.010000. running mean: -24.337623\n",
      "ep 1877: ep_len:500 episode reward: total was -9.130000. running mean: -24.185547\n",
      "ep 1877: ep_len:621 episode reward: total was -38.160000. running mean: -24.325291\n",
      "epsilon:0.116742 episode_count: 13146. steps_count: 5700746.000000\n",
      "Time elapsed:  16434.7727599144\n",
      "ep 1878: ep_len:581 episode reward: total was -77.730000. running mean: -24.859338\n",
      "ep 1878: ep_len:662 episode reward: total was -154.880000. running mean: -26.159545\n",
      "ep 1878: ep_len:79 episode reward: total was 4.790000. running mean: -25.850050\n",
      "ep 1878: ep_len:406 episode reward: total was -14.340000. running mean: -25.734949\n",
      "ep 1878: ep_len:105 episode reward: total was 27.250000. running mean: -25.205100\n",
      "ep 1878: ep_len:612 episode reward: total was -145.880000. running mean: -26.411849\n",
      "ep 1878: ep_len:500 episode reward: total was -0.600000. running mean: -26.153730\n",
      "epsilon:0.116698 episode_count: 13153. steps_count: 5703691.000000\n",
      "Time elapsed:  16442.685607671738\n",
      "ep 1879: ep_len:681 episode reward: total was -46.150000. running mean: -26.353693\n",
      "ep 1879: ep_len:505 episode reward: total was -5.030000. running mean: -26.140456\n",
      "ep 1879: ep_len:500 episode reward: total was -3.230000. running mean: -25.911351\n",
      "ep 1879: ep_len:513 episode reward: total was -44.990000. running mean: -26.102138\n",
      "ep 1879: ep_len:3 episode reward: total was 1.010000. running mean: -25.831016\n",
      "ep 1879: ep_len:505 episode reward: total was -38.260000. running mean: -25.955306\n",
      "ep 1879: ep_len:515 episode reward: total was -33.950000. running mean: -26.035253\n",
      "epsilon:0.116653 episode_count: 13160. steps_count: 5706913.000000\n",
      "Time elapsed:  16453.480585336685\n",
      "ep 1880: ep_len:603 episode reward: total was -42.720000. running mean: -26.202101\n",
      "ep 1880: ep_len:274 episode reward: total was -17.090000. running mean: -26.110980\n",
      "ep 1880: ep_len:654 episode reward: total was -67.360000. running mean: -26.523470\n",
      "ep 1880: ep_len:500 episode reward: total was -0.050000. running mean: -26.258735\n",
      "ep 1880: ep_len:105 episode reward: total was 26.270000. running mean: -25.733448\n",
      "ep 1880: ep_len:523 episode reward: total was -79.490000. running mean: -26.271013\n",
      "ep 1880: ep_len:327 episode reward: total was -25.120000. running mean: -26.259503\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.116609 episode_count: 13167. steps_count: 5709899.000000\n",
      "Time elapsed:  16466.19866657257\n",
      "ep 1881: ep_len:749 episode reward: total was -107.890000. running mean: -27.075808\n",
      "ep 1881: ep_len:587 episode reward: total was -100.460000. running mean: -27.809650\n",
      "ep 1881: ep_len:76 episode reward: total was 2.800000. running mean: -27.503554\n",
      "ep 1881: ep_len:525 episode reward: total was -8.770000. running mean: -27.316218\n",
      "ep 1881: ep_len:3 episode reward: total was -1.500000. running mean: -27.058056\n",
      "ep 1881: ep_len:605 episode reward: total was -20.730000. running mean: -26.994775\n",
      "ep 1881: ep_len:510 episode reward: total was -50.630000. running mean: -27.231128\n",
      "epsilon:0.116565 episode_count: 13174. steps_count: 5712954.000000\n",
      "Time elapsed:  16473.980885505676\n",
      "ep 1882: ep_len:619 episode reward: total was -47.820000. running mean: -27.437016\n",
      "ep 1882: ep_len:505 episode reward: total was 12.830000. running mean: -27.034346\n",
      "ep 1882: ep_len:50 episode reward: total was -6.310000. running mean: -26.827103\n",
      "ep 1882: ep_len:545 episode reward: total was -101.930000. running mean: -27.578132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1882: ep_len:74 episode reward: total was 5.110000. running mean: -27.251250\n",
      "ep 1882: ep_len:535 episode reward: total was -35.120000. running mean: -27.329938\n",
      "ep 1882: ep_len:624 episode reward: total was -26.490000. running mean: -27.321538\n",
      "epsilon:0.116520 episode_count: 13181. steps_count: 5715906.000000\n",
      "Time elapsed:  16481.675939559937\n",
      "ep 1883: ep_len:170 episode reward: total was -4.470000. running mean: -27.093023\n",
      "ep 1883: ep_len:509 episode reward: total was -40.640000. running mean: -27.228493\n",
      "ep 1883: ep_len:535 episode reward: total was -34.030000. running mean: -27.296508\n",
      "ep 1883: ep_len:526 episode reward: total was -41.730000. running mean: -27.440843\n",
      "ep 1883: ep_len:50 episode reward: total was 11.500000. running mean: -27.051434\n",
      "ep 1883: ep_len:500 episode reward: total was -2.850000. running mean: -26.809420\n",
      "ep 1883: ep_len:531 episode reward: total was -67.200000. running mean: -27.213326\n",
      "epsilon:0.116476 episode_count: 13188. steps_count: 5718727.000000\n",
      "Time elapsed:  16489.07939529419\n",
      "ep 1884: ep_len:500 episode reward: total was 7.690000. running mean: -26.864293\n",
      "ep 1884: ep_len:500 episode reward: total was 2.070000. running mean: -26.574950\n",
      "ep 1884: ep_len:643 episode reward: total was -57.760000. running mean: -26.886800\n",
      "ep 1884: ep_len:500 episode reward: total was 11.970000. running mean: -26.498232\n",
      "ep 1884: ep_len:3 episode reward: total was -1.500000. running mean: -26.248250\n",
      "ep 1884: ep_len:664 episode reward: total was -11.460000. running mean: -26.100367\n",
      "ep 1884: ep_len:583 episode reward: total was -48.900000. running mean: -26.328364\n",
      "epsilon:0.116432 episode_count: 13195. steps_count: 5722120.000000\n",
      "Time elapsed:  16500.20819067955\n",
      "ep 1885: ep_len:604 episode reward: total was -87.670000. running mean: -26.941780\n",
      "ep 1885: ep_len:500 episode reward: total was -5.140000. running mean: -26.723762\n",
      "ep 1885: ep_len:555 episode reward: total was -33.640000. running mean: -26.792925\n",
      "ep 1885: ep_len:562 episode reward: total was 7.800000. running mean: -26.446995\n",
      "ep 1885: ep_len:3 episode reward: total was 1.010000. running mean: -26.172425\n",
      "ep 1885: ep_len:310 episode reward: total was 9.570000. running mean: -25.815001\n",
      "ep 1885: ep_len:303 episode reward: total was -14.140000. running mean: -25.698251\n",
      "epsilon:0.116387 episode_count: 13202. steps_count: 5724957.000000\n",
      "Time elapsed:  16507.93179178238\n",
      "ep 1886: ep_len:624 episode reward: total was -42.290000. running mean: -25.864169\n",
      "ep 1886: ep_len:547 episode reward: total was -39.200000. running mean: -25.997527\n",
      "ep 1886: ep_len:624 episode reward: total was -38.290000. running mean: -26.120452\n",
      "ep 1886: ep_len:531 episode reward: total was -15.920000. running mean: -26.018447\n",
      "ep 1886: ep_len:104 episode reward: total was 27.270000. running mean: -25.485563\n",
      "ep 1886: ep_len:584 episode reward: total was -28.410000. running mean: -25.514807\n",
      "ep 1886: ep_len:500 episode reward: total was -1.090000. running mean: -25.270559\n",
      "epsilon:0.116343 episode_count: 13209. steps_count: 5728471.000000\n",
      "Time elapsed:  16517.677345752716\n",
      "ep 1887: ep_len:528 episode reward: total was -8.410000. running mean: -25.101953\n",
      "ep 1887: ep_len:571 episode reward: total was -20.060000. running mean: -25.051534\n",
      "ep 1887: ep_len:79 episode reward: total was 5.830000. running mean: -24.742718\n",
      "ep 1887: ep_len:406 episode reward: total was -12.040000. running mean: -24.615691\n",
      "ep 1887: ep_len:3 episode reward: total was -0.490000. running mean: -24.374434\n",
      "ep 1887: ep_len:526 episode reward: total was -76.590000. running mean: -24.896590\n",
      "ep 1887: ep_len:500 episode reward: total was -29.790000. running mean: -24.945524\n",
      "epsilon:0.116299 episode_count: 13216. steps_count: 5731084.000000\n",
      "Time elapsed:  16526.89315056801\n",
      "ep 1888: ep_len:554 episode reward: total was -15.610000. running mean: -24.852169\n",
      "ep 1888: ep_len:500 episode reward: total was 0.830000. running mean: -24.595347\n",
      "ep 1888: ep_len:566 episode reward: total was -38.200000. running mean: -24.731394\n",
      "ep 1888: ep_len:543 episode reward: total was 40.330000. running mean: -24.080780\n",
      "ep 1888: ep_len:88 episode reward: total was -15.780000. running mean: -23.997772\n",
      "ep 1888: ep_len:705 episode reward: total was -22.640000. running mean: -23.984194\n",
      "ep 1888: ep_len:572 episode reward: total was -33.790000. running mean: -24.082252\n",
      "epsilon:0.116254 episode_count: 13223. steps_count: 5734612.000000\n",
      "Time elapsed:  16538.678340673447\n",
      "ep 1889: ep_len:535 episode reward: total was -88.760000. running mean: -24.729030\n",
      "ep 1889: ep_len:603 episode reward: total was -27.870000. running mean: -24.760440\n",
      "ep 1889: ep_len:560 episode reward: total was -87.850000. running mean: -25.391335\n",
      "ep 1889: ep_len:505 episode reward: total was -50.500000. running mean: -25.642422\n",
      "ep 1889: ep_len:112 episode reward: total was 28.760000. running mean: -25.098398\n",
      "ep 1889: ep_len:623 episode reward: total was -42.620000. running mean: -25.273614\n",
      "ep 1889: ep_len:587 episode reward: total was -57.790000. running mean: -25.598777\n",
      "epsilon:0.116210 episode_count: 13230. steps_count: 5738137.000000\n",
      "Time elapsed:  16548.44108223915\n",
      "ep 1890: ep_len:563 episode reward: total was -66.800000. running mean: -26.010790\n",
      "ep 1890: ep_len:612 episode reward: total was -60.780000. running mean: -26.358482\n",
      "ep 1890: ep_len:581 episode reward: total was -62.080000. running mean: -26.715697\n",
      "ep 1890: ep_len:521 episode reward: total was -28.490000. running mean: -26.733440\n",
      "ep 1890: ep_len:3 episode reward: total was 1.010000. running mean: -26.456006\n",
      "ep 1890: ep_len:180 episode reward: total was 13.190000. running mean: -26.059546\n",
      "ep 1890: ep_len:606 episode reward: total was -26.980000. running mean: -26.068750\n",
      "epsilon:0.116166 episode_count: 13237. steps_count: 5741203.000000\n",
      "Time elapsed:  16559.0431535244\n",
      "ep 1891: ep_len:215 episode reward: total was -6.920000. running mean: -25.877263\n",
      "ep 1891: ep_len:522 episode reward: total was -92.330000. running mean: -26.541790\n",
      "ep 1891: ep_len:572 episode reward: total was -16.570000. running mean: -26.442072\n",
      "ep 1891: ep_len:598 episode reward: total was -3.550000. running mean: -26.213151\n",
      "ep 1891: ep_len:3 episode reward: total was 1.010000. running mean: -25.940920\n",
      "ep 1891: ep_len:500 episode reward: total was -37.990000. running mean: -26.061411\n",
      "ep 1891: ep_len:500 episode reward: total was -28.400000. running mean: -26.084797\n",
      "epsilon:0.116121 episode_count: 13244. steps_count: 5744113.000000\n",
      "Time elapsed:  16569.207434892654\n",
      "ep 1892: ep_len:593 episode reward: total was -25.430000. running mean: -26.078249\n",
      "ep 1892: ep_len:500 episode reward: total was -34.550000. running mean: -26.162966\n",
      "ep 1892: ep_len:536 episode reward: total was -77.060000. running mean: -26.671936\n",
      "ep 1892: ep_len:534 episode reward: total was -22.680000. running mean: -26.632017\n",
      "ep 1892: ep_len:3 episode reward: total was 0.000000. running mean: -26.365697\n",
      "ep 1892: ep_len:186 episode reward: total was 11.110000. running mean: -25.990940\n",
      "ep 1892: ep_len:500 episode reward: total was -43.120000. running mean: -26.162231\n",
      "epsilon:0.116077 episode_count: 13251. steps_count: 5746965.000000\n",
      "Time elapsed:  16576.724421977997\n",
      "ep 1893: ep_len:122 episode reward: total was -4.140000. running mean: -25.942008\n",
      "ep 1893: ep_len:284 episode reward: total was -11.240000. running mean: -25.794988\n",
      "ep 1893: ep_len:360 episode reward: total was -45.290000. running mean: -25.989938\n",
      "ep 1893: ep_len:567 episode reward: total was -0.770000. running mean: -25.737739\n",
      "ep 1893: ep_len:90 episode reward: total was 24.250000. running mean: -25.237861\n",
      "ep 1893: ep_len:169 episode reward: total was 8.090000. running mean: -24.904583\n",
      "ep 1893: ep_len:501 episode reward: total was -50.610000. running mean: -25.161637\n",
      "epsilon:0.116033 episode_count: 13258. steps_count: 5749058.000000\n",
      "Time elapsed:  16582.978709459305\n",
      "ep 1894: ep_len:530 episode reward: total was -49.650000. running mean: -25.406521\n",
      "ep 1894: ep_len:500 episode reward: total was 30.040000. running mean: -24.852055\n",
      "ep 1894: ep_len:500 episode reward: total was -30.150000. running mean: -24.905035\n",
      "ep 1894: ep_len:113 episode reward: total was 12.860000. running mean: -24.527385\n",
      "ep 1894: ep_len:95 episode reward: total was 16.740000. running mean: -24.114711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1894: ep_len:616 episode reward: total was -72.710000. running mean: -24.600664\n",
      "ep 1894: ep_len:584 episode reward: total was -64.530000. running mean: -24.999957\n",
      "epsilon:0.115988 episode_count: 13265. steps_count: 5751996.000000\n",
      "Time elapsed:  16590.884642124176\n",
      "ep 1895: ep_len:562 episode reward: total was 13.260000. running mean: -24.617357\n",
      "ep 1895: ep_len:555 episode reward: total was -71.600000. running mean: -25.087184\n",
      "ep 1895: ep_len:611 episode reward: total was -58.440000. running mean: -25.420712\n",
      "ep 1895: ep_len:404 episode reward: total was -13.050000. running mean: -25.297005\n",
      "ep 1895: ep_len:2 episode reward: total was -0.500000. running mean: -25.049035\n",
      "ep 1895: ep_len:534 episode reward: total was -29.740000. running mean: -25.095944\n",
      "ep 1895: ep_len:598 episode reward: total was -22.160000. running mean: -25.066585\n",
      "epsilon:0.115944 episode_count: 13272. steps_count: 5755262.000000\n",
      "Time elapsed:  16599.62543773651\n",
      "ep 1896: ep_len:569 episode reward: total was 34.500000. running mean: -24.470919\n",
      "ep 1896: ep_len:599 episode reward: total was -89.210000. running mean: -25.118310\n",
      "ep 1896: ep_len:601 episode reward: total was -24.860000. running mean: -25.115727\n",
      "ep 1896: ep_len:132 episode reward: total was 14.550000. running mean: -24.719070\n",
      "ep 1896: ep_len:128 episode reward: total was 18.820000. running mean: -24.283679\n",
      "ep 1896: ep_len:501 episode reward: total was -33.720000. running mean: -24.378042\n",
      "ep 1896: ep_len:274 episode reward: total was -11.890000. running mean: -24.253162\n",
      "epsilon:0.115900 episode_count: 13279. steps_count: 5758066.000000\n",
      "Time elapsed:  16607.14729142189\n",
      "ep 1897: ep_len:594 episode reward: total was -23.010000. running mean: -24.240730\n",
      "ep 1897: ep_len:597 episode reward: total was -53.370000. running mean: -24.532023\n",
      "ep 1897: ep_len:548 episode reward: total was -42.640000. running mean: -24.713103\n",
      "ep 1897: ep_len:575 episode reward: total was 17.420000. running mean: -24.291772\n",
      "ep 1897: ep_len:3 episode reward: total was 1.010000. running mean: -24.038754\n",
      "ep 1897: ep_len:501 episode reward: total was -48.290000. running mean: -24.281266\n",
      "ep 1897: ep_len:541 episode reward: total was -26.150000. running mean: -24.299954\n",
      "epsilon:0.115855 episode_count: 13286. steps_count: 5761425.000000\n",
      "Time elapsed:  16616.09822678566\n",
      "ep 1898: ep_len:619 episode reward: total was -67.930000. running mean: -24.736254\n",
      "ep 1898: ep_len:573 episode reward: total was 55.990000. running mean: -23.928992\n",
      "ep 1898: ep_len:618 episode reward: total was -51.680000. running mean: -24.206502\n",
      "ep 1898: ep_len:500 episode reward: total was -31.160000. running mean: -24.276037\n",
      "ep 1898: ep_len:3 episode reward: total was 1.010000. running mean: -24.023176\n",
      "ep 1898: ep_len:536 episode reward: total was -59.210000. running mean: -24.375044\n",
      "ep 1898: ep_len:569 episode reward: total was -14.560000. running mean: -24.276894\n",
      "epsilon:0.115811 episode_count: 13293. steps_count: 5764843.000000\n",
      "Time elapsed:  16627.24774336815\n",
      "ep 1899: ep_len:247 episode reward: total was -8.950000. running mean: -24.123625\n",
      "ep 1899: ep_len:542 episode reward: total was -21.890000. running mean: -24.101289\n",
      "ep 1899: ep_len:559 episode reward: total was -51.760000. running mean: -24.377876\n",
      "ep 1899: ep_len:506 episode reward: total was 10.780000. running mean: -24.026297\n",
      "ep 1899: ep_len:42 episode reward: total was 15.000000. running mean: -23.636034\n",
      "ep 1899: ep_len:605 episode reward: total was -0.750000. running mean: -23.407174\n",
      "ep 1899: ep_len:505 episode reward: total was -28.250000. running mean: -23.455602\n",
      "epsilon:0.115767 episode_count: 13300. steps_count: 5767849.000000\n",
      "Time elapsed:  16635.26731300354\n",
      "ep 1900: ep_len:542 episode reward: total was -20.390000. running mean: -23.424946\n",
      "ep 1900: ep_len:500 episode reward: total was 26.720000. running mean: -22.923497\n",
      "ep 1900: ep_len:659 episode reward: total was -65.590000. running mean: -23.350162\n",
      "ep 1900: ep_len:500 episode reward: total was -8.380000. running mean: -23.200460\n",
      "ep 1900: ep_len:95 episode reward: total was 18.730000. running mean: -22.781155\n",
      "ep 1900: ep_len:508 episode reward: total was -68.080000. running mean: -23.234144\n",
      "ep 1900: ep_len:571 episode reward: total was -25.720000. running mean: -23.259002\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.115722 episode_count: 13307. steps_count: 5771224.000000\n",
      "Time elapsed:  16651.159998893738\n",
      "ep 1901: ep_len:500 episode reward: total was 29.670000. running mean: -22.729712\n",
      "ep 1901: ep_len:628 episode reward: total was -59.090000. running mean: -23.093315\n",
      "ep 1901: ep_len:594 episode reward: total was -59.680000. running mean: -23.459182\n",
      "ep 1901: ep_len:48 episode reward: total was -4.710000. running mean: -23.271690\n",
      "ep 1901: ep_len:98 episode reward: total was 27.730000. running mean: -22.761673\n",
      "ep 1901: ep_len:500 episode reward: total was -2.070000. running mean: -22.554757\n",
      "ep 1901: ep_len:236 episode reward: total was -109.550000. running mean: -23.424709\n",
      "epsilon:0.115678 episode_count: 13314. steps_count: 5773828.000000\n",
      "Time elapsed:  16658.27228450775\n",
      "ep 1902: ep_len:500 episode reward: total was 21.500000. running mean: -22.975462\n",
      "ep 1902: ep_len:550 episode reward: total was 20.120000. running mean: -22.544507\n",
      "ep 1902: ep_len:560 episode reward: total was -36.640000. running mean: -22.685462\n",
      "ep 1902: ep_len:511 episode reward: total was -54.530000. running mean: -23.003908\n",
      "ep 1902: ep_len:3 episode reward: total was 1.010000. running mean: -22.763769\n",
      "ep 1902: ep_len:586 episode reward: total was -41.580000. running mean: -22.951931\n",
      "ep 1902: ep_len:614 episode reward: total was -30.080000. running mean: -23.023212\n",
      "epsilon:0.115634 episode_count: 13321. steps_count: 5777152.000000\n",
      "Time elapsed:  16667.016574144363\n",
      "ep 1903: ep_len:549 episode reward: total was -26.690000. running mean: -23.059880\n",
      "ep 1903: ep_len:539 episode reward: total was 6.020000. running mean: -22.769081\n",
      "ep 1903: ep_len:563 episode reward: total was -68.870000. running mean: -23.230090\n",
      "ep 1903: ep_len:500 episode reward: total was -32.510000. running mean: -23.322889\n",
      "ep 1903: ep_len:3 episode reward: total was -1.500000. running mean: -23.104660\n",
      "ep 1903: ep_len:224 episode reward: total was 0.030000. running mean: -22.873314\n",
      "ep 1903: ep_len:573 episode reward: total was -28.970000. running mean: -22.934280\n",
      "epsilon:0.115589 episode_count: 13328. steps_count: 5780103.000000\n",
      "Time elapsed:  16675.96898317337\n",
      "ep 1904: ep_len:500 episode reward: total was 26.580000. running mean: -22.439138\n",
      "ep 1904: ep_len:586 episode reward: total was -26.660000. running mean: -22.481346\n",
      "ep 1904: ep_len:500 episode reward: total was -27.760000. running mean: -22.534133\n",
      "ep 1904: ep_len:500 episode reward: total was 5.060000. running mean: -22.258191\n",
      "ep 1904: ep_len:87 episode reward: total was 16.230000. running mean: -21.873310\n",
      "ep 1904: ep_len:530 episode reward: total was -2.790000. running mean: -21.682476\n",
      "ep 1904: ep_len:597 episode reward: total was -5.760000. running mean: -21.523252\n",
      "epsilon:0.115545 episode_count: 13335. steps_count: 5783403.000000\n",
      "Time elapsed:  16684.98185634613\n",
      "ep 1905: ep_len:514 episode reward: total was 18.820000. running mean: -21.119819\n",
      "ep 1905: ep_len:562 episode reward: total was -40.850000. running mean: -21.317121\n",
      "ep 1905: ep_len:591 episode reward: total was -24.630000. running mean: -21.350250\n",
      "ep 1905: ep_len:511 episode reward: total was -0.310000. running mean: -21.139847\n",
      "ep 1905: ep_len:3 episode reward: total was -3.000000. running mean: -20.958449\n",
      "ep 1905: ep_len:602 episode reward: total was 5.620000. running mean: -20.692664\n",
      "ep 1905: ep_len:572 episode reward: total was -31.270000. running mean: -20.798438\n",
      "epsilon:0.115501 episode_count: 13342. steps_count: 5786758.000000\n",
      "Time elapsed:  16693.95516204834\n",
      "ep 1906: ep_len:637 episode reward: total was -45.560000. running mean: -21.046053\n",
      "ep 1906: ep_len:516 episode reward: total was -21.040000. running mean: -21.045993\n",
      "ep 1906: ep_len:538 episode reward: total was -57.560000. running mean: -21.411133\n",
      "ep 1906: ep_len:581 episode reward: total was 18.410000. running mean: -21.012921\n",
      "ep 1906: ep_len:3 episode reward: total was 0.000000. running mean: -20.802792\n",
      "ep 1906: ep_len:505 episode reward: total was -115.080000. running mean: -21.745564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1906: ep_len:310 episode reward: total was -34.850000. running mean: -21.876609\n",
      "epsilon:0.115456 episode_count: 13349. steps_count: 5789848.000000\n",
      "Time elapsed:  16702.25172829628\n",
      "ep 1907: ep_len:500 episode reward: total was 21.590000. running mean: -21.441943\n",
      "ep 1907: ep_len:550 episode reward: total was 15.850000. running mean: -21.069023\n",
      "ep 1907: ep_len:500 episode reward: total was 5.000000. running mean: -20.808333\n",
      "ep 1907: ep_len:526 episode reward: total was 26.690000. running mean: -20.333350\n",
      "ep 1907: ep_len:3 episode reward: total was -1.500000. running mean: -20.145016\n",
      "ep 1907: ep_len:517 episode reward: total was -36.410000. running mean: -20.307666\n",
      "ep 1907: ep_len:299 episode reward: total was -39.640000. running mean: -20.500989\n",
      "epsilon:0.115412 episode_count: 13356. steps_count: 5792743.000000\n",
      "Time elapsed:  16710.151731967926\n",
      "ep 1908: ep_len:258 episode reward: total was -3.850000. running mean: -20.334479\n",
      "ep 1908: ep_len:527 episode reward: total was 0.560000. running mean: -20.125535\n",
      "ep 1908: ep_len:451 episode reward: total was -33.690000. running mean: -20.261179\n",
      "ep 1908: ep_len:170 episode reward: total was -2.940000. running mean: -20.087967\n",
      "ep 1908: ep_len:3 episode reward: total was 1.010000. running mean: -19.876988\n",
      "ep 1908: ep_len:529 episode reward: total was -24.220000. running mean: -19.920418\n",
      "ep 1908: ep_len:290 episode reward: total was -10.260000. running mean: -19.823814\n",
      "epsilon:0.115368 episode_count: 13363. steps_count: 5794971.000000\n",
      "Time elapsed:  16717.64068222046\n",
      "ep 1909: ep_len:116 episode reward: total was -2.060000. running mean: -19.646176\n",
      "ep 1909: ep_len:500 episode reward: total was -51.540000. running mean: -19.965114\n",
      "ep 1909: ep_len:500 episode reward: total was -32.500000. running mean: -20.090463\n",
      "ep 1909: ep_len:525 episode reward: total was 23.820000. running mean: -19.651358\n",
      "ep 1909: ep_len:100 episode reward: total was -24.870000. running mean: -19.703545\n",
      "ep 1909: ep_len:216 episode reward: total was -10.010000. running mean: -19.606609\n",
      "ep 1909: ep_len:532 episode reward: total was -22.680000. running mean: -19.637343\n",
      "epsilon:0.115323 episode_count: 13370. steps_count: 5797460.000000\n",
      "Time elapsed:  16724.44110226631\n",
      "ep 1910: ep_len:129 episode reward: total was -2.140000. running mean: -19.462370\n",
      "ep 1910: ep_len:501 episode reward: total was 47.090000. running mean: -18.796846\n",
      "ep 1910: ep_len:500 episode reward: total was -48.260000. running mean: -19.091477\n",
      "ep 1910: ep_len:540 episode reward: total was 24.320000. running mean: -18.657363\n",
      "ep 1910: ep_len:3 episode reward: total was 1.010000. running mean: -18.460689\n",
      "ep 1910: ep_len:521 episode reward: total was -34.560000. running mean: -18.621682\n",
      "ep 1910: ep_len:184 episode reward: total was -8.590000. running mean: -18.521365\n",
      "epsilon:0.115279 episode_count: 13377. steps_count: 5799838.000000\n",
      "Time elapsed:  16733.189007997513\n",
      "ep 1911: ep_len:553 episode reward: total was -93.040000. running mean: -19.266552\n",
      "ep 1911: ep_len:529 episode reward: total was -63.420000. running mean: -19.708086\n",
      "ep 1911: ep_len:568 episode reward: total was -53.680000. running mean: -20.047805\n",
      "ep 1911: ep_len:507 episode reward: total was 5.760000. running mean: -19.789727\n",
      "ep 1911: ep_len:3 episode reward: total was -1.500000. running mean: -19.606830\n",
      "ep 1911: ep_len:535 episode reward: total was -38.390000. running mean: -19.794662\n",
      "ep 1911: ep_len:608 episode reward: total was -31.160000. running mean: -19.908315\n",
      "epsilon:0.115235 episode_count: 13384. steps_count: 5803141.000000\n",
      "Time elapsed:  16741.880724668503\n",
      "ep 1912: ep_len:649 episode reward: total was -90.800000. running mean: -20.617232\n",
      "ep 1912: ep_len:522 episode reward: total was 10.860000. running mean: -20.302460\n",
      "ep 1912: ep_len:560 episode reward: total was 2.320000. running mean: -20.076235\n",
      "ep 1912: ep_len:623 episode reward: total was 7.360000. running mean: -19.801873\n",
      "ep 1912: ep_len:3 episode reward: total was 1.010000. running mean: -19.593754\n",
      "ep 1912: ep_len:609 episode reward: total was -42.110000. running mean: -19.818916\n",
      "ep 1912: ep_len:575 episode reward: total was -33.510000. running mean: -19.955827\n",
      "epsilon:0.115190 episode_count: 13391. steps_count: 5806682.000000\n",
      "Time elapsed:  16751.03243470192\n",
      "ep 1913: ep_len:519 episode reward: total was -25.620000. running mean: -20.012469\n",
      "ep 1913: ep_len:537 episode reward: total was 69.480000. running mean: -19.117544\n",
      "ep 1913: ep_len:690 episode reward: total was -54.540000. running mean: -19.471769\n",
      "ep 1913: ep_len:500 episode reward: total was 36.550000. running mean: -18.911551\n",
      "ep 1913: ep_len:3 episode reward: total was 0.000000. running mean: -18.722436\n",
      "ep 1913: ep_len:508 episode reward: total was -25.560000. running mean: -18.790811\n",
      "ep 1913: ep_len:506 episode reward: total was -161.170000. running mean: -20.214603\n",
      "epsilon:0.115146 episode_count: 13398. steps_count: 5809945.000000\n",
      "Time elapsed:  16759.693808317184\n",
      "ep 1914: ep_len:552 episode reward: total was -57.740000. running mean: -20.589857\n",
      "ep 1914: ep_len:542 episode reward: total was -35.990000. running mean: -20.743858\n",
      "ep 1914: ep_len:500 episode reward: total was -167.390000. running mean: -22.210320\n",
      "ep 1914: ep_len:56 episode reward: total was -1.690000. running mean: -22.005117\n",
      "ep 1914: ep_len:3 episode reward: total was 1.010000. running mean: -21.774966\n",
      "ep 1914: ep_len:500 episode reward: total was -20.430000. running mean: -21.761516\n",
      "ep 1914: ep_len:532 episode reward: total was -39.740000. running mean: -21.941301\n",
      "epsilon:0.115102 episode_count: 13405. steps_count: 5812630.000000\n",
      "Time elapsed:  16767.09894132614\n",
      "ep 1915: ep_len:527 episode reward: total was -21.000000. running mean: -21.931888\n",
      "ep 1915: ep_len:501 episode reward: total was 2.210000. running mean: -21.690469\n",
      "ep 1915: ep_len:527 episode reward: total was -49.890000. running mean: -21.972464\n",
      "ep 1915: ep_len:612 episode reward: total was -1.820000. running mean: -21.770940\n",
      "ep 1915: ep_len:3 episode reward: total was 1.010000. running mean: -21.543130\n",
      "ep 1915: ep_len:623 episode reward: total was -38.920000. running mean: -21.716899\n",
      "ep 1915: ep_len:526 episode reward: total was -30.900000. running mean: -21.808730\n",
      "epsilon:0.115057 episode_count: 13412. steps_count: 5815949.000000\n",
      "Time elapsed:  16775.71246767044\n",
      "ep 1916: ep_len:570 episode reward: total was 24.680000. running mean: -21.343843\n",
      "ep 1916: ep_len:504 episode reward: total was -142.630000. running mean: -22.556704\n",
      "ep 1916: ep_len:500 episode reward: total was -13.090000. running mean: -22.462037\n",
      "ep 1916: ep_len:519 episode reward: total was 16.120000. running mean: -22.076217\n",
      "ep 1916: ep_len:3 episode reward: total was -0.490000. running mean: -21.860355\n",
      "ep 1916: ep_len:539 episode reward: total was -0.430000. running mean: -21.646051\n",
      "ep 1916: ep_len:337 episode reward: total was -16.200000. running mean: -21.591590\n",
      "epsilon:0.115013 episode_count: 13419. steps_count: 5818921.000000\n",
      "Time elapsed:  16788.01486158371\n",
      "ep 1917: ep_len:619 episode reward: total was -59.180000. running mean: -21.967475\n",
      "ep 1917: ep_len:367 episode reward: total was -90.530000. running mean: -22.653100\n",
      "ep 1917: ep_len:356 episode reward: total was 13.230000. running mean: -22.294269\n",
      "ep 1917: ep_len:546 episode reward: total was 18.900000. running mean: -21.882326\n",
      "ep 1917: ep_len:3 episode reward: total was 1.010000. running mean: -21.653403\n",
      "ep 1917: ep_len:511 episode reward: total was -130.060000. running mean: -22.737469\n",
      "ep 1917: ep_len:589 episode reward: total was -8.150000. running mean: -22.591594\n",
      "epsilon:0.114969 episode_count: 13426. steps_count: 5821912.000000\n",
      "Time elapsed:  16795.957762002945\n",
      "ep 1918: ep_len:594 episode reward: total was 37.540000. running mean: -21.990278\n",
      "ep 1918: ep_len:520 episode reward: total was 12.790000. running mean: -21.642475\n",
      "ep 1918: ep_len:629 episode reward: total was -63.700000. running mean: -22.063051\n",
      "ep 1918: ep_len:502 episode reward: total was -38.080000. running mean: -22.223220\n",
      "ep 1918: ep_len:119 episode reward: total was 7.830000. running mean: -21.922688\n",
      "ep 1918: ep_len:575 episode reward: total was -41.240000. running mean: -22.115861\n",
      "ep 1918: ep_len:280 episode reward: total was -5.890000. running mean: -21.953602\n",
      "epsilon:0.114924 episode_count: 13433. steps_count: 5825131.000000\n",
      "Time elapsed:  16804.488969802856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1919: ep_len:191 episode reward: total was -4.960000. running mean: -21.783666\n",
      "ep 1919: ep_len:527 episode reward: total was -25.040000. running mean: -21.816230\n",
      "ep 1919: ep_len:546 episode reward: total was -49.640000. running mean: -22.094467\n",
      "ep 1919: ep_len:500 episode reward: total was -2.110000. running mean: -21.894623\n",
      "ep 1919: ep_len:131 episode reward: total was 14.870000. running mean: -21.526977\n",
      "ep 1919: ep_len:186 episode reward: total was 12.180000. running mean: -21.189907\n",
      "ep 1919: ep_len:542 episode reward: total was -18.820000. running mean: -21.166208\n",
      "epsilon:0.114880 episode_count: 13440. steps_count: 5827754.000000\n",
      "Time elapsed:  16811.520325660706\n",
      "ep 1920: ep_len:563 episode reward: total was -39.830000. running mean: -21.352846\n",
      "ep 1920: ep_len:618 episode reward: total was -2.940000. running mean: -21.168717\n",
      "ep 1920: ep_len:619 episode reward: total was -36.210000. running mean: -21.319130\n",
      "ep 1920: ep_len:501 episode reward: total was 5.150000. running mean: -21.054439\n",
      "ep 1920: ep_len:3 episode reward: total was 1.010000. running mean: -20.833794\n",
      "ep 1920: ep_len:692 episode reward: total was 0.030000. running mean: -20.625156\n",
      "ep 1920: ep_len:561 episode reward: total was -33.770000. running mean: -20.756605\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.114836 episode_count: 13447. steps_count: 5831311.000000\n",
      "Time elapsed:  16826.080533981323\n",
      "ep 1921: ep_len:603 episode reward: total was -12.520000. running mean: -20.674239\n",
      "ep 1921: ep_len:556 episode reward: total was -10.790000. running mean: -20.575396\n",
      "ep 1921: ep_len:531 episode reward: total was -50.210000. running mean: -20.871742\n",
      "ep 1921: ep_len:56 episode reward: total was -8.160000. running mean: -20.744625\n",
      "ep 1921: ep_len:86 episode reward: total was 18.730000. running mean: -20.349879\n",
      "ep 1921: ep_len:622 episode reward: total was 28.660000. running mean: -19.859780\n",
      "ep 1921: ep_len:264 episode reward: total was -18.470000. running mean: -19.845882\n",
      "epsilon:0.114791 episode_count: 13454. steps_count: 5834029.000000\n",
      "Time elapsed:  16833.415817260742\n",
      "ep 1922: ep_len:500 episode reward: total was 21.160000. running mean: -19.435823\n",
      "ep 1922: ep_len:613 episode reward: total was -135.170000. running mean: -20.593165\n",
      "ep 1922: ep_len:642 episode reward: total was -39.380000. running mean: -20.781033\n",
      "ep 1922: ep_len:534 episode reward: total was 12.630000. running mean: -20.446923\n",
      "ep 1922: ep_len:33 episode reward: total was 9.000000. running mean: -20.152454\n",
      "ep 1922: ep_len:635 episode reward: total was -50.530000. running mean: -20.456229\n",
      "ep 1922: ep_len:500 episode reward: total was -10.300000. running mean: -20.354667\n",
      "epsilon:0.114747 episode_count: 13461. steps_count: 5837486.000000\n",
      "Time elapsed:  16842.50623869896\n",
      "ep 1923: ep_len:643 episode reward: total was 9.940000. running mean: -20.051720\n",
      "ep 1923: ep_len:505 episode reward: total was -24.100000. running mean: -20.092203\n",
      "ep 1923: ep_len:544 episode reward: total was -65.550000. running mean: -20.546781\n",
      "ep 1923: ep_len:163 episode reward: total was -3.820000. running mean: -20.379513\n",
      "ep 1923: ep_len:82 episode reward: total was -7.750000. running mean: -20.253218\n",
      "ep 1923: ep_len:524 episode reward: total was -43.800000. running mean: -20.488686\n",
      "ep 1923: ep_len:283 episode reward: total was -0.690000. running mean: -20.290699\n",
      "epsilon:0.114703 episode_count: 13468. steps_count: 5840230.000000\n",
      "Time elapsed:  16850.641971588135\n",
      "ep 1924: ep_len:529 episode reward: total was -96.310000. running mean: -21.050892\n",
      "ep 1924: ep_len:500 episode reward: total was -25.590000. running mean: -21.096283\n",
      "ep 1924: ep_len:500 episode reward: total was -6.840000. running mean: -20.953720\n",
      "ep 1924: ep_len:501 episode reward: total was -8.790000. running mean: -20.832083\n",
      "ep 1924: ep_len:3 episode reward: total was 1.010000. running mean: -20.613662\n",
      "ep 1924: ep_len:525 episode reward: total was -52.700000. running mean: -20.934526\n",
      "ep 1924: ep_len:327 episode reward: total was -10.120000. running mean: -20.826381\n",
      "epsilon:0.114658 episode_count: 13475. steps_count: 5843115.000000\n",
      "Time elapsed:  16859.633329629898\n",
      "ep 1925: ep_len:134 episode reward: total was -1.050000. running mean: -20.628617\n",
      "ep 1925: ep_len:510 episode reward: total was -9.120000. running mean: -20.513531\n",
      "ep 1925: ep_len:613 episode reward: total was -63.210000. running mean: -20.940495\n",
      "ep 1925: ep_len:514 episode reward: total was -31.300000. running mean: -21.044090\n",
      "ep 1925: ep_len:49 episode reward: total was 18.500000. running mean: -20.648649\n",
      "ep 1925: ep_len:500 episode reward: total was 11.530000. running mean: -20.326863\n",
      "ep 1925: ep_len:550 episode reward: total was -49.080000. running mean: -20.614394\n",
      "epsilon:0.114614 episode_count: 13482. steps_count: 5845985.000000\n",
      "Time elapsed:  16867.07643532753\n",
      "ep 1926: ep_len:581 episode reward: total was -14.180000. running mean: -20.550050\n",
      "ep 1926: ep_len:500 episode reward: total was -38.120000. running mean: -20.725750\n",
      "ep 1926: ep_len:71 episode reward: total was 0.180000. running mean: -20.516692\n",
      "ep 1926: ep_len:524 episode reward: total was -45.760000. running mean: -20.769125\n",
      "ep 1926: ep_len:3 episode reward: total was 1.010000. running mean: -20.551334\n",
      "ep 1926: ep_len:500 episode reward: total was 3.570000. running mean: -20.310121\n",
      "ep 1926: ep_len:500 episode reward: total was -23.740000. running mean: -20.344420\n",
      "epsilon:0.114570 episode_count: 13489. steps_count: 5848664.000000\n",
      "Time elapsed:  16871.54681444168\n",
      "ep 1927: ep_len:604 episode reward: total was 16.770000. running mean: -19.973275\n",
      "ep 1927: ep_len:642 episode reward: total was -14.940000. running mean: -19.922943\n",
      "ep 1927: ep_len:500 episode reward: total was -51.430000. running mean: -20.238013\n",
      "ep 1927: ep_len:500 episode reward: total was 1.190000. running mean: -20.023733\n",
      "ep 1927: ep_len:3 episode reward: total was 0.000000. running mean: -19.823496\n",
      "ep 1927: ep_len:526 episode reward: total was -46.600000. running mean: -20.091261\n",
      "ep 1927: ep_len:639 episode reward: total was -15.130000. running mean: -20.041648\n",
      "epsilon:0.114525 episode_count: 13496. steps_count: 5852078.000000\n",
      "Time elapsed:  16880.57184958458\n",
      "ep 1928: ep_len:579 episode reward: total was 44.110000. running mean: -19.400132\n",
      "ep 1928: ep_len:521 episode reward: total was -18.490000. running mean: -19.391030\n",
      "ep 1928: ep_len:500 episode reward: total was 8.610000. running mean: -19.111020\n",
      "ep 1928: ep_len:519 episode reward: total was -23.740000. running mean: -19.157310\n",
      "ep 1928: ep_len:104 episode reward: total was 5.750000. running mean: -18.908237\n",
      "ep 1928: ep_len:524 episode reward: total was -101.250000. running mean: -19.731654\n",
      "ep 1928: ep_len:523 episode reward: total was -34.720000. running mean: -19.881538\n",
      "epsilon:0.114481 episode_count: 13503. steps_count: 5855348.000000\n",
      "Time elapsed:  16889.052872896194\n",
      "ep 1929: ep_len:550 episode reward: total was -54.220000. running mean: -20.224922\n",
      "ep 1929: ep_len:500 episode reward: total was 36.610000. running mean: -19.656573\n",
      "ep 1929: ep_len:649 episode reward: total was -166.000000. running mean: -21.120008\n",
      "ep 1929: ep_len:503 episode reward: total was -93.340000. running mean: -21.842207\n",
      "ep 1929: ep_len:86 episode reward: total was 15.610000. running mean: -21.467685\n",
      "ep 1929: ep_len:500 episode reward: total was 23.990000. running mean: -21.013109\n",
      "ep 1929: ep_len:500 episode reward: total was -14.380000. running mean: -20.946777\n",
      "epsilon:0.114437 episode_count: 13510. steps_count: 5858636.000000\n",
      "Time elapsed:  16897.458423614502\n",
      "ep 1930: ep_len:122 episode reward: total was 2.350000. running mean: -20.713810\n",
      "ep 1930: ep_len:500 episode reward: total was 29.200000. running mean: -20.214672\n",
      "ep 1930: ep_len:501 episode reward: total was -7.440000. running mean: -20.086925\n",
      "ep 1930: ep_len:500 episode reward: total was 18.610000. running mean: -19.699956\n",
      "ep 1930: ep_len:3 episode reward: total was -1.500000. running mean: -19.517956\n",
      "ep 1930: ep_len:500 episode reward: total was -14.170000. running mean: -19.464476\n",
      "ep 1930: ep_len:539 episode reward: total was -32.220000. running mean: -19.592032\n",
      "epsilon:0.114392 episode_count: 13517. steps_count: 5861301.000000\n",
      "Time elapsed:  16904.78444623947\n",
      "ep 1931: ep_len:535 episode reward: total was -9.260000. running mean: -19.488711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1931: ep_len:504 episode reward: total was -26.440000. running mean: -19.558224\n",
      "ep 1931: ep_len:528 episode reward: total was -12.660000. running mean: -19.489242\n",
      "ep 1931: ep_len:516 episode reward: total was -46.180000. running mean: -19.756150\n",
      "ep 1931: ep_len:86 episode reward: total was 19.250000. running mean: -19.366088\n",
      "ep 1931: ep_len:578 episode reward: total was -14.480000. running mean: -19.317227\n",
      "ep 1931: ep_len:171 episode reward: total was -1.100000. running mean: -19.135055\n",
      "epsilon:0.114348 episode_count: 13524. steps_count: 5864219.000000\n",
      "Time elapsed:  16914.900455474854\n",
      "ep 1932: ep_len:516 episode reward: total was -24.350000. running mean: -19.187204\n",
      "ep 1932: ep_len:293 episode reward: total was -8.470000. running mean: -19.080032\n",
      "ep 1932: ep_len:596 episode reward: total was -31.280000. running mean: -19.202032\n",
      "ep 1932: ep_len:629 episode reward: total was -427.840000. running mean: -23.288412\n",
      "ep 1932: ep_len:3 episode reward: total was 0.000000. running mean: -23.055528\n",
      "ep 1932: ep_len:564 episode reward: total was 6.500000. running mean: -22.759972\n",
      "ep 1932: ep_len:521 episode reward: total was -63.050000. running mean: -23.162873\n",
      "epsilon:0.114304 episode_count: 13531. steps_count: 5867341.000000\n",
      "Time elapsed:  16923.078616142273\n",
      "ep 1933: ep_len:553 episode reward: total was -47.830000. running mean: -23.409544\n",
      "ep 1933: ep_len:575 episode reward: total was 24.880000. running mean: -22.926648\n",
      "ep 1933: ep_len:519 episode reward: total was -45.350000. running mean: -23.150882\n",
      "ep 1933: ep_len:598 episode reward: total was -20.170000. running mean: -23.121073\n",
      "ep 1933: ep_len:3 episode reward: total was 0.000000. running mean: -22.889862\n",
      "ep 1933: ep_len:629 episode reward: total was -147.070000. running mean: -24.131664\n",
      "ep 1933: ep_len:559 episode reward: total was -76.450000. running mean: -24.654847\n",
      "epsilon:0.114259 episode_count: 13538. steps_count: 5870777.000000\n",
      "Time elapsed:  16932.175818681717\n",
      "ep 1934: ep_len:593 episode reward: total was -94.230000. running mean: -25.350599\n",
      "ep 1934: ep_len:628 episode reward: total was 11.670000. running mean: -24.980393\n",
      "ep 1934: ep_len:558 episode reward: total was -76.430000. running mean: -25.494889\n",
      "ep 1934: ep_len:501 episode reward: total was -1.100000. running mean: -25.250940\n",
      "ep 1934: ep_len:50 episode reward: total was 14.500000. running mean: -24.853430\n",
      "ep 1934: ep_len:511 episode reward: total was -68.670000. running mean: -25.291596\n",
      "ep 1934: ep_len:573 episode reward: total was -4.500000. running mean: -25.083680\n",
      "epsilon:0.114215 episode_count: 13545. steps_count: 5874191.000000\n",
      "Time elapsed:  16941.17943239212\n",
      "ep 1935: ep_len:615 episode reward: total was -109.570000. running mean: -25.928543\n",
      "ep 1935: ep_len:623 episode reward: total was -107.930000. running mean: -26.748558\n",
      "ep 1935: ep_len:536 episode reward: total was -51.980000. running mean: -27.000872\n",
      "ep 1935: ep_len:504 episode reward: total was -128.070000. running mean: -28.011564\n",
      "ep 1935: ep_len:3 episode reward: total was 1.010000. running mean: -27.721348\n",
      "ep 1935: ep_len:542 episode reward: total was -56.260000. running mean: -28.006735\n",
      "ep 1935: ep_len:577 episode reward: total was -133.760000. running mean: -29.064267\n",
      "epsilon:0.114171 episode_count: 13552. steps_count: 5877591.000000\n",
      "Time elapsed:  16952.43290734291\n",
      "ep 1936: ep_len:205 episode reward: total was -0.200000. running mean: -28.775625\n",
      "ep 1936: ep_len:504 episode reward: total was -39.360000. running mean: -28.881468\n",
      "ep 1936: ep_len:621 episode reward: total was -114.950000. running mean: -29.742154\n",
      "ep 1936: ep_len:56 episode reward: total was -9.190000. running mean: -29.536632\n",
      "ep 1936: ep_len:3 episode reward: total was -1.500000. running mean: -29.256266\n",
      "ep 1936: ep_len:306 episode reward: total was -27.290000. running mean: -29.236603\n",
      "ep 1936: ep_len:547 episode reward: total was -64.820000. running mean: -29.592437\n",
      "epsilon:0.114126 episode_count: 13559. steps_count: 5879833.000000\n",
      "Time elapsed:  16958.668696403503\n",
      "ep 1937: ep_len:114 episode reward: total was 14.880000. running mean: -29.147713\n",
      "ep 1937: ep_len:500 episode reward: total was 17.420000. running mean: -28.682036\n",
      "ep 1937: ep_len:595 episode reward: total was -32.890000. running mean: -28.724115\n",
      "ep 1937: ep_len:531 episode reward: total was -3.660000. running mean: -28.473474\n",
      "ep 1937: ep_len:106 episode reward: total was 22.730000. running mean: -27.961439\n",
      "ep 1937: ep_len:500 episode reward: total was -2.860000. running mean: -27.710425\n",
      "ep 1937: ep_len:610 episode reward: total was 4.150000. running mean: -27.391821\n",
      "epsilon:0.114082 episode_count: 13566. steps_count: 5882789.000000\n",
      "Time elapsed:  16966.952359437943\n",
      "ep 1938: ep_len:534 episode reward: total was 16.970000. running mean: -26.948202\n",
      "ep 1938: ep_len:520 episode reward: total was -13.340000. running mean: -26.812120\n",
      "ep 1938: ep_len:544 episode reward: total was -61.180000. running mean: -27.155799\n",
      "ep 1938: ep_len:504 episode reward: total was -8.190000. running mean: -26.966141\n",
      "ep 1938: ep_len:3 episode reward: total was 0.000000. running mean: -26.696480\n",
      "ep 1938: ep_len:524 episode reward: total was -91.030000. running mean: -27.339815\n",
      "ep 1938: ep_len:532 episode reward: total was -47.900000. running mean: -27.545417\n",
      "epsilon:0.114038 episode_count: 13573. steps_count: 5885950.000000\n",
      "Time elapsed:  16976.77419257164\n",
      "ep 1939: ep_len:500 episode reward: total was 45.250000. running mean: -26.817463\n",
      "ep 1939: ep_len:567 episode reward: total was -9.690000. running mean: -26.646188\n",
      "ep 1939: ep_len:576 episode reward: total was -94.760000. running mean: -27.327326\n",
      "ep 1939: ep_len:500 episode reward: total was -58.710000. running mean: -27.641153\n",
      "ep 1939: ep_len:49 episode reward: total was 15.500000. running mean: -27.209741\n",
      "ep 1939: ep_len:521 episode reward: total was -35.810000. running mean: -27.295744\n",
      "ep 1939: ep_len:535 episode reward: total was -18.410000. running mean: -27.206887\n",
      "epsilon:0.113993 episode_count: 13580. steps_count: 5889198.000000\n",
      "Time elapsed:  16985.467587709427\n",
      "ep 1940: ep_len:616 episode reward: total was 39.570000. running mean: -26.539118\n",
      "ep 1940: ep_len:500 episode reward: total was -42.890000. running mean: -26.702627\n",
      "ep 1940: ep_len:500 episode reward: total was -25.320000. running mean: -26.688800\n",
      "ep 1940: ep_len:516 episode reward: total was -11.870000. running mean: -26.540612\n",
      "ep 1940: ep_len:121 episode reward: total was 6.840000. running mean: -26.206806\n",
      "ep 1940: ep_len:531 episode reward: total was -24.640000. running mean: -26.191138\n",
      "ep 1940: ep_len:626 episode reward: total was -34.070000. running mean: -26.269927\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.113949 episode_count: 13587. steps_count: 5892608.000000\n",
      "Time elapsed:  16999.37673854828\n",
      "ep 1941: ep_len:580 episode reward: total was 36.620000. running mean: -25.641027\n",
      "ep 1941: ep_len:507 episode reward: total was -19.590000. running mean: -25.580517\n",
      "ep 1941: ep_len:392 episode reward: total was 16.840000. running mean: -25.156312\n",
      "ep 1941: ep_len:590 episode reward: total was 41.420000. running mean: -24.490549\n",
      "ep 1941: ep_len:3 episode reward: total was -1.500000. running mean: -24.260643\n",
      "ep 1941: ep_len:526 episode reward: total was -37.800000. running mean: -24.396037\n",
      "ep 1941: ep_len:503 episode reward: total was -26.120000. running mean: -24.413277\n",
      "epsilon:0.113905 episode_count: 13594. steps_count: 5895709.000000\n",
      "Time elapsed:  17007.64525103569\n",
      "ep 1942: ep_len:535 episode reward: total was 26.650000. running mean: -23.902644\n",
      "ep 1942: ep_len:577 episode reward: total was -49.380000. running mean: -24.157417\n",
      "ep 1942: ep_len:672 episode reward: total was -36.500000. running mean: -24.280843\n",
      "ep 1942: ep_len:520 episode reward: total was 2.930000. running mean: -24.008735\n",
      "ep 1942: ep_len:3 episode reward: total was 0.000000. running mean: -23.768647\n",
      "ep 1942: ep_len:529 episode reward: total was -38.510000. running mean: -23.916061\n",
      "ep 1942: ep_len:285 episode reward: total was -49.970000. running mean: -24.176600\n",
      "epsilon:0.113860 episode_count: 13601. steps_count: 5898830.000000\n",
      "Time elapsed:  17018.988711595535\n",
      "ep 1943: ep_len:568 episode reward: total was -75.410000. running mean: -24.688934\n",
      "ep 1943: ep_len:500 episode reward: total was -2.460000. running mean: -24.466645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1943: ep_len:500 episode reward: total was 5.220000. running mean: -24.169779\n",
      "ep 1943: ep_len:56 episode reward: total was 1.830000. running mean: -23.909781\n",
      "ep 1943: ep_len:127 episode reward: total was 18.840000. running mean: -23.482283\n",
      "ep 1943: ep_len:530 episode reward: total was -40.320000. running mean: -23.650660\n",
      "ep 1943: ep_len:562 episode reward: total was -21.770000. running mean: -23.631854\n",
      "epsilon:0.113816 episode_count: 13608. steps_count: 5901673.000000\n",
      "Time elapsed:  17027.108869552612\n",
      "ep 1944: ep_len:644 episode reward: total was -100.550000. running mean: -24.401035\n",
      "ep 1944: ep_len:532 episode reward: total was -129.800000. running mean: -25.455025\n",
      "ep 1944: ep_len:500 episode reward: total was -50.890000. running mean: -25.709374\n",
      "ep 1944: ep_len:129 episode reward: total was -12.540000. running mean: -25.577681\n",
      "ep 1944: ep_len:89 episode reward: total was 5.230000. running mean: -25.269604\n",
      "ep 1944: ep_len:225 episode reward: total was 11.730000. running mean: -24.899608\n",
      "ep 1944: ep_len:580 episode reward: total was -59.840000. running mean: -25.249012\n",
      "epsilon:0.113772 episode_count: 13615. steps_count: 5904372.000000\n",
      "Time elapsed:  17035.26492500305\n",
      "ep 1945: ep_len:502 episode reward: total was -50.880000. running mean: -25.505322\n",
      "ep 1945: ep_len:501 episode reward: total was -118.440000. running mean: -26.434668\n",
      "ep 1945: ep_len:502 episode reward: total was -58.040000. running mean: -26.750722\n",
      "ep 1945: ep_len:500 episode reward: total was -8.540000. running mean: -26.568614\n",
      "ep 1945: ep_len:3 episode reward: total was 0.000000. running mean: -26.302928\n",
      "ep 1945: ep_len:549 episode reward: total was -26.890000. running mean: -26.308799\n",
      "ep 1945: ep_len:529 episode reward: total was -32.180000. running mean: -26.367511\n",
      "epsilon:0.113727 episode_count: 13622. steps_count: 5907458.000000\n",
      "Time elapsed:  17048.12401533127\n",
      "ep 1946: ep_len:500 episode reward: total was 36.590000. running mean: -25.737936\n",
      "ep 1946: ep_len:605 episode reward: total was -9.290000. running mean: -25.573457\n",
      "ep 1946: ep_len:500 episode reward: total was -3.930000. running mean: -25.357022\n",
      "ep 1946: ep_len:500 episode reward: total was -5.390000. running mean: -25.157352\n",
      "ep 1946: ep_len:101 episode reward: total was 17.720000. running mean: -24.728578\n",
      "ep 1946: ep_len:592 episode reward: total was -14.870000. running mean: -24.629993\n",
      "ep 1946: ep_len:504 episode reward: total was -50.560000. running mean: -24.889293\n",
      "epsilon:0.113683 episode_count: 13629. steps_count: 5910760.000000\n",
      "Time elapsed:  17058.157531499863\n",
      "ep 1947: ep_len:569 episode reward: total was -20.970000. running mean: -24.850100\n",
      "ep 1947: ep_len:500 episode reward: total was -6.830000. running mean: -24.669899\n",
      "ep 1947: ep_len:583 episode reward: total was -22.240000. running mean: -24.645600\n",
      "ep 1947: ep_len:379 episode reward: total was -27.110000. running mean: -24.670244\n",
      "ep 1947: ep_len:3 episode reward: total was 1.010000. running mean: -24.413441\n",
      "ep 1947: ep_len:558 episode reward: total was -161.390000. running mean: -25.783207\n",
      "ep 1947: ep_len:608 episode reward: total was -38.260000. running mean: -25.907975\n",
      "epsilon:0.113639 episode_count: 13636. steps_count: 5913960.000000\n",
      "Time elapsed:  17075.552772045135\n",
      "ep 1948: ep_len:500 episode reward: total was 2.370000. running mean: -25.625195\n",
      "ep 1948: ep_len:582 episode reward: total was -75.740000. running mean: -26.126343\n",
      "ep 1948: ep_len:575 episode reward: total was -87.980000. running mean: -26.744880\n",
      "ep 1948: ep_len:523 episode reward: total was 4.680000. running mean: -26.430631\n",
      "ep 1948: ep_len:3 episode reward: total was 1.010000. running mean: -26.156225\n",
      "ep 1948: ep_len:530 episode reward: total was -43.780000. running mean: -26.332462\n",
      "ep 1948: ep_len:500 episode reward: total was -27.080000. running mean: -26.339938\n",
      "epsilon:0.113594 episode_count: 13643. steps_count: 5917173.000000\n",
      "Time elapsed:  17089.100975990295\n",
      "ep 1949: ep_len:583 episode reward: total was -0.550000. running mean: -26.082038\n",
      "ep 1949: ep_len:567 episode reward: total was -26.540000. running mean: -26.086618\n",
      "ep 1949: ep_len:636 episode reward: total was -25.820000. running mean: -26.083952\n",
      "ep 1949: ep_len:565 episode reward: total was 3.920000. running mean: -25.783912\n",
      "ep 1949: ep_len:106 episode reward: total was 20.220000. running mean: -25.323873\n",
      "ep 1949: ep_len:512 episode reward: total was 5.610000. running mean: -25.014534\n",
      "ep 1949: ep_len:589 episode reward: total was -33.700000. running mean: -25.101389\n",
      "epsilon:0.113550 episode_count: 13650. steps_count: 5920731.000000\n",
      "Time elapsed:  17099.64733362198\n",
      "ep 1950: ep_len:565 episode reward: total was -17.530000. running mean: -25.025675\n",
      "ep 1950: ep_len:297 episode reward: total was -5.470000. running mean: -24.830118\n",
      "ep 1950: ep_len:535 episode reward: total was -33.440000. running mean: -24.916217\n",
      "ep 1950: ep_len:508 episode reward: total was -29.060000. running mean: -24.957655\n",
      "ep 1950: ep_len:3 episode reward: total was 1.010000. running mean: -24.697978\n",
      "ep 1950: ep_len:512 episode reward: total was -62.280000. running mean: -25.073799\n",
      "ep 1950: ep_len:591 episode reward: total was -14.110000. running mean: -24.964161\n",
      "epsilon:0.113506 episode_count: 13657. steps_count: 5923742.000000\n",
      "Time elapsed:  17109.073496103287\n",
      "ep 1951: ep_len:684 episode reward: total was -162.710000. running mean: -26.341619\n",
      "ep 1951: ep_len:500 episode reward: total was -30.380000. running mean: -26.382003\n",
      "ep 1951: ep_len:500 episode reward: total was -25.220000. running mean: -26.370383\n",
      "ep 1951: ep_len:626 episode reward: total was 17.120000. running mean: -25.935479\n",
      "ep 1951: ep_len:3 episode reward: total was 0.000000. running mean: -25.676124\n",
      "ep 1951: ep_len:500 episode reward: total was -31.550000. running mean: -25.734863\n",
      "ep 1951: ep_len:547 episode reward: total was -45.200000. running mean: -25.929514\n",
      "epsilon:0.113461 episode_count: 13664. steps_count: 5927102.000000\n",
      "Time elapsed:  17119.21374464035\n",
      "ep 1952: ep_len:655 episode reward: total was -71.360000. running mean: -26.383819\n",
      "ep 1952: ep_len:549 episode reward: total was 3.480000. running mean: -26.085181\n",
      "ep 1952: ep_len:500 episode reward: total was -1.330000. running mean: -25.837629\n",
      "ep 1952: ep_len:117 episode reward: total was 8.460000. running mean: -25.494653\n",
      "ep 1952: ep_len:3 episode reward: total was 1.010000. running mean: -25.229606\n",
      "ep 1952: ep_len:249 episode reward: total was 24.460000. running mean: -24.732710\n",
      "ep 1952: ep_len:541 episode reward: total was -7.540000. running mean: -24.560783\n",
      "epsilon:0.113417 episode_count: 13671. steps_count: 5929716.000000\n",
      "Time elapsed:  17126.408900022507\n",
      "ep 1953: ep_len:554 episode reward: total was -25.410000. running mean: -24.569275\n",
      "ep 1953: ep_len:201 episode reward: total was 0.400000. running mean: -24.319583\n",
      "ep 1953: ep_len:602 episode reward: total was -86.510000. running mean: -24.941487\n",
      "ep 1953: ep_len:500 episode reward: total was 16.540000. running mean: -24.526672\n",
      "ep 1953: ep_len:3 episode reward: total was 1.010000. running mean: -24.271305\n",
      "ep 1953: ep_len:500 episode reward: total was -13.730000. running mean: -24.165892\n",
      "ep 1953: ep_len:509 episode reward: total was -12.750000. running mean: -24.051733\n",
      "epsilon:0.113373 episode_count: 13678. steps_count: 5932585.000000\n",
      "Time elapsed:  17135.670511960983\n",
      "ep 1954: ep_len:519 episode reward: total was -56.710000. running mean: -24.378316\n",
      "ep 1954: ep_len:611 episode reward: total was 10.220000. running mean: -24.032333\n",
      "ep 1954: ep_len:559 episode reward: total was -67.980000. running mean: -24.471809\n",
      "ep 1954: ep_len:49 episode reward: total was -1.270000. running mean: -24.239791\n",
      "ep 1954: ep_len:3 episode reward: total was 1.010000. running mean: -23.987293\n",
      "ep 1954: ep_len:514 episode reward: total was -48.400000. running mean: -24.231420\n",
      "ep 1954: ep_len:500 episode reward: total was -66.550000. running mean: -24.654606\n",
      "epsilon:0.113328 episode_count: 13685. steps_count: 5935340.000000\n",
      "Time elapsed:  17144.22182416916\n",
      "ep 1955: ep_len:539 episode reward: total was 33.970000. running mean: -24.068360\n",
      "ep 1955: ep_len:501 episode reward: total was -9.390000. running mean: -23.921577\n",
      "ep 1955: ep_len:351 episode reward: total was 7.330000. running mean: -23.609061\n",
      "ep 1955: ep_len:56 episode reward: total was -2.700000. running mean: -23.399970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1955: ep_len:3 episode reward: total was 1.010000. running mean: -23.155871\n",
      "ep 1955: ep_len:500 episode reward: total was -85.620000. running mean: -23.780512\n",
      "ep 1955: ep_len:301 episode reward: total was -42.000000. running mean: -23.962707\n",
      "epsilon:0.113284 episode_count: 13692. steps_count: 5937591.000000\n",
      "Time elapsed:  17151.48291707039\n",
      "ep 1956: ep_len:574 episode reward: total was 24.560000. running mean: -23.477480\n",
      "ep 1956: ep_len:501 episode reward: total was -18.390000. running mean: -23.426605\n",
      "ep 1956: ep_len:380 episode reward: total was 21.220000. running mean: -22.980139\n",
      "ep 1956: ep_len:500 episode reward: total was -31.500000. running mean: -23.065337\n",
      "ep 1956: ep_len:93 episode reward: total was -62.240000. running mean: -23.457084\n",
      "ep 1956: ep_len:588 episode reward: total was -37.160000. running mean: -23.594113\n",
      "ep 1956: ep_len:516 episode reward: total was -91.590000. running mean: -24.274072\n",
      "epsilon:0.113240 episode_count: 13699. steps_count: 5940743.000000\n",
      "Time elapsed:  17161.019260644913\n",
      "ep 1957: ep_len:500 episode reward: total was -61.520000. running mean: -24.646531\n",
      "ep 1957: ep_len:551 episode reward: total was 31.640000. running mean: -24.083666\n",
      "ep 1957: ep_len:428 episode reward: total was -10.330000. running mean: -23.946129\n",
      "ep 1957: ep_len:502 episode reward: total was -42.600000. running mean: -24.132668\n",
      "ep 1957: ep_len:3 episode reward: total was 0.000000. running mean: -23.891341\n",
      "ep 1957: ep_len:555 episode reward: total was -130.690000. running mean: -24.959328\n",
      "ep 1957: ep_len:560 episode reward: total was -98.180000. running mean: -25.691535\n",
      "epsilon:0.113195 episode_count: 13706. steps_count: 5943842.000000\n",
      "Time elapsed:  17170.153638362885\n",
      "ep 1958: ep_len:531 episode reward: total was -5.730000. running mean: -25.491919\n",
      "ep 1958: ep_len:563 episode reward: total was -60.680000. running mean: -25.843800\n",
      "ep 1958: ep_len:79 episode reward: total was 6.810000. running mean: -25.517262\n",
      "ep 1958: ep_len:366 episode reward: total was -16.700000. running mean: -25.429090\n",
      "ep 1958: ep_len:2 episode reward: total was -0.500000. running mean: -25.179799\n",
      "ep 1958: ep_len:665 episode reward: total was -32.900000. running mean: -25.257001\n",
      "ep 1958: ep_len:500 episode reward: total was -26.930000. running mean: -25.273731\n",
      "epsilon:0.113151 episode_count: 13713. steps_count: 5946548.000000\n",
      "Time elapsed:  17178.660432577133\n",
      "ep 1959: ep_len:222 episode reward: total was -21.520000. running mean: -25.236193\n",
      "ep 1959: ep_len:588 episode reward: total was -2.970000. running mean: -25.013531\n",
      "ep 1959: ep_len:643 episode reward: total was -5.540000. running mean: -24.818796\n",
      "ep 1959: ep_len:500 episode reward: total was -28.450000. running mean: -24.855108\n",
      "ep 1959: ep_len:3 episode reward: total was 1.010000. running mean: -24.596457\n",
      "ep 1959: ep_len:501 episode reward: total was -27.750000. running mean: -24.627992\n",
      "ep 1959: ep_len:504 episode reward: total was -24.530000. running mean: -24.627013\n",
      "epsilon:0.113107 episode_count: 13720. steps_count: 5949509.000000\n",
      "Time elapsed:  17187.715574502945\n",
      "ep 1960: ep_len:129 episode reward: total was -2.630000. running mean: -24.407042\n",
      "ep 1960: ep_len:614 episode reward: total was -240.040000. running mean: -26.563372\n",
      "ep 1960: ep_len:561 episode reward: total was -32.740000. running mean: -26.625138\n",
      "ep 1960: ep_len:500 episode reward: total was -36.720000. running mean: -26.726087\n",
      "ep 1960: ep_len:3 episode reward: total was 1.010000. running mean: -26.448726\n",
      "ep 1960: ep_len:673 episode reward: total was -39.650000. running mean: -26.580739\n",
      "ep 1960: ep_len:535 episode reward: total was -13.000000. running mean: -26.444931\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.113062 episode_count: 13727. steps_count: 5952524.000000\n",
      "Time elapsed:  17202.349875211716\n",
      "ep 1961: ep_len:248 episode reward: total was -4.870000. running mean: -26.229182\n",
      "ep 1961: ep_len:574 episode reward: total was 41.860000. running mean: -25.548290\n",
      "ep 1961: ep_len:525 episode reward: total was -77.480000. running mean: -26.067607\n",
      "ep 1961: ep_len:544 episode reward: total was -4.100000. running mean: -25.847931\n",
      "ep 1961: ep_len:3 episode reward: total was 0.000000. running mean: -25.589452\n",
      "ep 1961: ep_len:294 episode reward: total was -102.270000. running mean: -26.356257\n",
      "ep 1961: ep_len:296 episode reward: total was -15.250000. running mean: -26.245195\n",
      "epsilon:0.113018 episode_count: 13734. steps_count: 5955008.000000\n",
      "Time elapsed:  17212.30597257614\n",
      "ep 1962: ep_len:593 episode reward: total was -56.950000. running mean: -26.552243\n",
      "ep 1962: ep_len:528 episode reward: total was 12.550000. running mean: -26.161221\n",
      "ep 1962: ep_len:501 episode reward: total was -6.080000. running mean: -25.960408\n",
      "ep 1962: ep_len:500 episode reward: total was -78.030000. running mean: -26.481104\n",
      "ep 1962: ep_len:94 episode reward: total was -37.740000. running mean: -26.593693\n",
      "ep 1962: ep_len:550 episode reward: total was -5.860000. running mean: -26.386356\n",
      "ep 1962: ep_len:519 episode reward: total was -116.970000. running mean: -27.292193\n",
      "epsilon:0.112974 episode_count: 13741. steps_count: 5958293.000000\n",
      "Time elapsed:  17222.168588638306\n",
      "ep 1963: ep_len:617 episode reward: total was -43.770000. running mean: -27.456971\n",
      "ep 1963: ep_len:527 episode reward: total was -81.870000. running mean: -28.001101\n",
      "ep 1963: ep_len:633 episode reward: total was -91.660000. running mean: -28.637690\n",
      "ep 1963: ep_len:501 episode reward: total was -45.510000. running mean: -28.806413\n",
      "ep 1963: ep_len:101 episode reward: total was 1.750000. running mean: -28.500849\n",
      "ep 1963: ep_len:615 episode reward: total was -14.820000. running mean: -28.364041\n",
      "ep 1963: ep_len:520 episode reward: total was -32.380000. running mean: -28.404200\n",
      "epsilon:0.112929 episode_count: 13748. steps_count: 5961807.000000\n",
      "Time elapsed:  17232.680816173553\n",
      "ep 1964: ep_len:557 episode reward: total was -46.130000. running mean: -28.581458\n",
      "ep 1964: ep_len:261 episode reward: total was -85.860000. running mean: -29.154244\n",
      "ep 1964: ep_len:636 episode reward: total was -31.750000. running mean: -29.180201\n",
      "ep 1964: ep_len:526 episode reward: total was -15.200000. running mean: -29.040399\n",
      "ep 1964: ep_len:3 episode reward: total was 0.000000. running mean: -28.749995\n",
      "ep 1964: ep_len:534 episode reward: total was -78.220000. running mean: -29.244695\n",
      "ep 1964: ep_len:601 episode reward: total was -25.260000. running mean: -29.204848\n",
      "epsilon:0.112885 episode_count: 13755. steps_count: 5964925.000000\n",
      "Time elapsed:  17242.295392274857\n",
      "ep 1965: ep_len:662 episode reward: total was -52.990000. running mean: -29.442700\n",
      "ep 1965: ep_len:601 episode reward: total was -52.020000. running mean: -29.668473\n",
      "ep 1965: ep_len:601 episode reward: total was -70.240000. running mean: -30.074188\n",
      "ep 1965: ep_len:505 episode reward: total was -68.260000. running mean: -30.456046\n",
      "ep 1965: ep_len:128 episode reward: total was 10.350000. running mean: -30.047986\n",
      "ep 1965: ep_len:661 episode reward: total was -27.100000. running mean: -30.018506\n",
      "ep 1965: ep_len:272 episode reward: total was -60.060000. running mean: -30.318921\n",
      "epsilon:0.112841 episode_count: 13762. steps_count: 5968355.000000\n",
      "Time elapsed:  17252.528444767\n",
      "ep 1966: ep_len:500 episode reward: total was -21.350000. running mean: -30.229232\n",
      "ep 1966: ep_len:500 episode reward: total was -1.810000. running mean: -29.945039\n",
      "ep 1966: ep_len:320 episode reward: total was 17.610000. running mean: -29.469489\n",
      "ep 1966: ep_len:500 episode reward: total was -61.010000. running mean: -29.784894\n",
      "ep 1966: ep_len:87 episode reward: total was 14.760000. running mean: -29.339445\n",
      "ep 1966: ep_len:505 episode reward: total was 13.870000. running mean: -28.907351\n",
      "ep 1966: ep_len:520 episode reward: total was -37.990000. running mean: -28.998177\n",
      "epsilon:0.112796 episode_count: 13769. steps_count: 5971287.000000\n",
      "Time elapsed:  17260.943790197372\n",
      "ep 1967: ep_len:132 episode reward: total was 3.980000. running mean: -28.668395\n",
      "ep 1967: ep_len:500 episode reward: total was 5.780000. running mean: -28.323911\n",
      "ep 1967: ep_len:623 episode reward: total was -4.090000. running mean: -28.081572\n",
      "ep 1967: ep_len:520 episode reward: total was -40.220000. running mean: -28.202956\n",
      "ep 1967: ep_len:129 episode reward: total was 8.270000. running mean: -27.838227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1967: ep_len:503 episode reward: total was 15.870000. running mean: -27.401145\n",
      "ep 1967: ep_len:587 episode reward: total was -30.610000. running mean: -27.433233\n",
      "epsilon:0.112752 episode_count: 13776. steps_count: 5974281.000000\n",
      "Time elapsed:  17270.07764148712\n",
      "ep 1968: ep_len:225 episode reward: total was 4.110000. running mean: -27.117801\n",
      "ep 1968: ep_len:505 episode reward: total was -95.740000. running mean: -27.804023\n",
      "ep 1968: ep_len:590 episode reward: total was -73.270000. running mean: -28.258683\n",
      "ep 1968: ep_len:418 episode reward: total was -45.560000. running mean: -28.431696\n",
      "ep 1968: ep_len:98 episode reward: total was 19.220000. running mean: -27.955179\n",
      "ep 1968: ep_len:647 episode reward: total was -33.060000. running mean: -28.006227\n",
      "ep 1968: ep_len:528 episode reward: total was -31.820000. running mean: -28.044365\n",
      "epsilon:0.112708 episode_count: 13783. steps_count: 5977292.000000\n",
      "Time elapsed:  17284.588121414185\n",
      "ep 1969: ep_len:665 episode reward: total was -67.400000. running mean: -28.437921\n",
      "ep 1969: ep_len:514 episode reward: total was -33.960000. running mean: -28.493142\n",
      "ep 1969: ep_len:57 episode reward: total was -5.380000. running mean: -28.262010\n",
      "ep 1969: ep_len:625 episode reward: total was 7.250000. running mean: -27.906890\n",
      "ep 1969: ep_len:56 episode reward: total was 17.500000. running mean: -27.452821\n",
      "ep 1969: ep_len:500 episode reward: total was -12.230000. running mean: -27.300593\n",
      "ep 1969: ep_len:521 episode reward: total was -39.020000. running mean: -27.417787\n",
      "epsilon:0.112663 episode_count: 13790. steps_count: 5980230.000000\n",
      "Time elapsed:  17293.995676279068\n",
      "ep 1970: ep_len:549 episode reward: total was -52.240000. running mean: -27.666009\n",
      "ep 1970: ep_len:503 episode reward: total was -2.040000. running mean: -27.409749\n",
      "ep 1970: ep_len:566 episode reward: total was -85.420000. running mean: -27.989852\n",
      "ep 1970: ep_len:529 episode reward: total was 9.740000. running mean: -27.612553\n",
      "ep 1970: ep_len:3 episode reward: total was 0.000000. running mean: -27.336428\n",
      "ep 1970: ep_len:500 episode reward: total was -111.870000. running mean: -28.181764\n",
      "ep 1970: ep_len:575 episode reward: total was -16.910000. running mean: -28.069046\n",
      "epsilon:0.112619 episode_count: 13797. steps_count: 5983455.000000\n",
      "Time elapsed:  17307.03088235855\n",
      "ep 1971: ep_len:576 episode reward: total was 19.380000. running mean: -27.594555\n",
      "ep 1971: ep_len:500 episode reward: total was -23.150000. running mean: -27.550110\n",
      "ep 1971: ep_len:500 episode reward: total was -15.350000. running mean: -27.428109\n",
      "ep 1971: ep_len:500 episode reward: total was 6.600000. running mean: -27.087828\n",
      "ep 1971: ep_len:3 episode reward: total was 0.000000. running mean: -26.816949\n",
      "ep 1971: ep_len:524 episode reward: total was -68.920000. running mean: -27.237980\n",
      "ep 1971: ep_len:561 episode reward: total was -37.150000. running mean: -27.337100\n",
      "epsilon:0.112575 episode_count: 13804. steps_count: 5986619.000000\n",
      "Time elapsed:  17316.688523054123\n",
      "ep 1972: ep_len:531 episode reward: total was -26.350000. running mean: -27.327229\n",
      "ep 1972: ep_len:558 episode reward: total was 37.040000. running mean: -26.683557\n",
      "ep 1972: ep_len:533 episode reward: total was -34.530000. running mean: -26.762021\n",
      "ep 1972: ep_len:566 episode reward: total was -30.410000. running mean: -26.798501\n",
      "ep 1972: ep_len:56 episode reward: total was 22.000000. running mean: -26.310516\n",
      "ep 1972: ep_len:500 episode reward: total was -84.400000. running mean: -26.891411\n",
      "ep 1972: ep_len:579 episode reward: total was -64.610000. running mean: -27.268597\n",
      "epsilon:0.112530 episode_count: 13811. steps_count: 5989942.000000\n",
      "Time elapsed:  17326.619697332382\n",
      "ep 1973: ep_len:586 episode reward: total was -18.290000. running mean: -27.178811\n",
      "ep 1973: ep_len:577 episode reward: total was 3.930000. running mean: -26.867723\n",
      "ep 1973: ep_len:528 episode reward: total was -45.380000. running mean: -27.052845\n",
      "ep 1973: ep_len:163 episode reward: total was 9.670000. running mean: -26.685617\n",
      "ep 1973: ep_len:3 episode reward: total was 0.000000. running mean: -26.418761\n",
      "ep 1973: ep_len:571 episode reward: total was -29.980000. running mean: -26.454373\n",
      "ep 1973: ep_len:516 episode reward: total was -23.310000. running mean: -26.422930\n",
      "epsilon:0.112486 episode_count: 13818. steps_count: 5992886.000000\n",
      "Time elapsed:  17335.626080513\n",
      "ep 1974: ep_len:526 episode reward: total was 10.510000. running mean: -26.053600\n",
      "ep 1974: ep_len:600 episode reward: total was 17.040000. running mean: -25.622664\n",
      "ep 1974: ep_len:500 episode reward: total was 2.250000. running mean: -25.343938\n",
      "ep 1974: ep_len:500 episode reward: total was 38.140000. running mean: -24.709098\n",
      "ep 1974: ep_len:3 episode reward: total was 0.000000. running mean: -24.462007\n",
      "ep 1974: ep_len:549 episode reward: total was 13.970000. running mean: -24.077687\n",
      "ep 1974: ep_len:320 episode reward: total was -76.860000. running mean: -24.605510\n",
      "epsilon:0.112442 episode_count: 13825. steps_count: 5995884.000000\n",
      "Time elapsed:  17344.820341825485\n",
      "ep 1975: ep_len:661 episode reward: total was -41.370000. running mean: -24.773155\n",
      "ep 1975: ep_len:332 episode reward: total was -100.810000. running mean: -25.533524\n",
      "ep 1975: ep_len:519 episode reward: total was -9.080000. running mean: -25.368988\n",
      "ep 1975: ep_len:615 episode reward: total was 17.800000. running mean: -24.937299\n",
      "ep 1975: ep_len:124 episode reward: total was 20.870000. running mean: -24.479226\n",
      "ep 1975: ep_len:616 episode reward: total was -0.390000. running mean: -24.238333\n",
      "ep 1975: ep_len:500 episode reward: total was -64.050000. running mean: -24.636450\n",
      "epsilon:0.112397 episode_count: 13832. steps_count: 5999251.000000\n",
      "Time elapsed:  17355.105697631836\n",
      "ep 1976: ep_len:509 episode reward: total was -15.370000. running mean: -24.543785\n",
      "ep 1976: ep_len:593 episode reward: total was 33.390000. running mean: -23.964448\n",
      "ep 1976: ep_len:667 episode reward: total was -82.230000. running mean: -24.547103\n",
      "ep 1976: ep_len:601 episode reward: total was -25.250000. running mean: -24.554132\n",
      "ep 1976: ep_len:103 episode reward: total was -40.160000. running mean: -24.710191\n",
      "ep 1976: ep_len:500 episode reward: total was -36.470000. running mean: -24.827789\n",
      "ep 1976: ep_len:614 episode reward: total was -79.420000. running mean: -25.373711\n",
      "epsilon:0.112353 episode_count: 13839. steps_count: 6002838.000000\n",
      "Time elapsed:  17365.804919719696\n",
      "ep 1977: ep_len:243 episode reward: total was 4.690000. running mean: -25.073074\n",
      "ep 1977: ep_len:583 episode reward: total was -28.230000. running mean: -25.104643\n",
      "ep 1977: ep_len:558 episode reward: total was -66.620000. running mean: -25.519797\n",
      "ep 1977: ep_len:544 episode reward: total was 35.810000. running mean: -24.906499\n",
      "ep 1977: ep_len:3 episode reward: total was 0.000000. running mean: -24.657434\n",
      "ep 1977: ep_len:573 episode reward: total was -61.460000. running mean: -25.025459\n",
      "ep 1977: ep_len:520 episode reward: total was -25.680000. running mean: -25.032005\n",
      "epsilon:0.112309 episode_count: 13846. steps_count: 6005862.000000\n",
      "Time elapsed:  17377.012835025787\n",
      "ep 1978: ep_len:574 episode reward: total was 4.270000. running mean: -24.738985\n",
      "ep 1978: ep_len:350 episode reward: total was -32.870000. running mean: -24.820295\n",
      "ep 1978: ep_len:587 episode reward: total was -49.330000. running mean: -25.065392\n",
      "ep 1978: ep_len:56 episode reward: total was -15.740000. running mean: -24.972138\n",
      "ep 1978: ep_len:3 episode reward: total was 1.010000. running mean: -24.712317\n",
      "ep 1978: ep_len:500 episode reward: total was 2.780000. running mean: -24.437393\n",
      "ep 1978: ep_len:583 episode reward: total was -5.700000. running mean: -24.250020\n",
      "epsilon:0.112264 episode_count: 13853. steps_count: 6008515.000000\n",
      "Time elapsed:  17389.578815221786\n",
      "ep 1979: ep_len:134 episode reward: total was 2.960000. running mean: -23.977919\n",
      "ep 1979: ep_len:607 episode reward: total was -4.380000. running mean: -23.781940\n",
      "ep 1979: ep_len:378 episode reward: total was -30.120000. running mean: -23.845321\n",
      "ep 1979: ep_len:515 episode reward: total was -6.430000. running mean: -23.671168\n",
      "ep 1979: ep_len:90 episode reward: total was 17.760000. running mean: -23.256856\n",
      "ep 1979: ep_len:587 episode reward: total was -175.290000. running mean: -24.777187\n",
      "ep 1979: ep_len:587 episode reward: total was -29.460000. running mean: -24.824015\n",
      "epsilon:0.112220 episode_count: 13860. steps_count: 6011413.000000\n",
      "Time elapsed:  17398.441885471344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1980: ep_len:532 episode reward: total was -72.630000. running mean: -25.302075\n",
      "ep 1980: ep_len:525 episode reward: total was 13.200000. running mean: -24.917055\n",
      "ep 1980: ep_len:426 episode reward: total was 28.430000. running mean: -24.383584\n",
      "ep 1980: ep_len:500 episode reward: total was -4.990000. running mean: -24.189648\n",
      "ep 1980: ep_len:3 episode reward: total was 0.000000. running mean: -23.947752\n",
      "ep 1980: ep_len:551 episode reward: total was -59.310000. running mean: -24.301374\n",
      "ep 1980: ep_len:287 episode reward: total was -15.830000. running mean: -24.216660\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.112176 episode_count: 13867. steps_count: 6014237.000000\n",
      "Time elapsed:  17410.161328554153\n",
      "ep 1981: ep_len:217 episode reward: total was -3.810000. running mean: -24.012594\n",
      "ep 1981: ep_len:582 episode reward: total was 6.430000. running mean: -23.708168\n",
      "ep 1981: ep_len:555 episode reward: total was -43.520000. running mean: -23.906286\n",
      "ep 1981: ep_len:506 episode reward: total was -2.260000. running mean: -23.689823\n",
      "ep 1981: ep_len:3 episode reward: total was 1.010000. running mean: -23.442825\n",
      "ep 1981: ep_len:536 episode reward: total was -115.930000. running mean: -24.367697\n",
      "ep 1981: ep_len:500 episode reward: total was -81.240000. running mean: -24.936420\n",
      "epsilon:0.112131 episode_count: 13874. steps_count: 6017136.000000\n",
      "Time elapsed:  17418.00135231018\n",
      "ep 1982: ep_len:513 episode reward: total was -110.310000. running mean: -25.790156\n",
      "ep 1982: ep_len:665 episode reward: total was -167.200000. running mean: -27.204254\n",
      "ep 1982: ep_len:559 episode reward: total was -30.280000. running mean: -27.235012\n",
      "ep 1982: ep_len:500 episode reward: total was 1.740000. running mean: -26.945261\n",
      "ep 1982: ep_len:3 episode reward: total was 0.000000. running mean: -26.675809\n",
      "ep 1982: ep_len:500 episode reward: total was -224.600000. running mean: -28.655051\n",
      "ep 1982: ep_len:500 episode reward: total was -64.490000. running mean: -29.013400\n",
      "epsilon:0.112087 episode_count: 13881. steps_count: 6020376.000000\n",
      "Time elapsed:  17426.551073551178\n",
      "ep 1983: ep_len:240 episode reward: total was 10.110000. running mean: -28.622166\n",
      "ep 1983: ep_len:500 episode reward: total was -15.250000. running mean: -28.488445\n",
      "ep 1983: ep_len:68 episode reward: total was -0.220000. running mean: -28.205760\n",
      "ep 1983: ep_len:577 episode reward: total was 7.800000. running mean: -27.845703\n",
      "ep 1983: ep_len:3 episode reward: total was 0.000000. running mean: -27.567246\n",
      "ep 1983: ep_len:619 episode reward: total was -35.730000. running mean: -27.648873\n",
      "ep 1983: ep_len:190 episode reward: total was -21.720000. running mean: -27.589584\n",
      "epsilon:0.112043 episode_count: 13888. steps_count: 6022573.000000\n",
      "Time elapsed:  17435.472781419754\n",
      "ep 1984: ep_len:134 episode reward: total was -9.560000. running mean: -27.409288\n",
      "ep 1984: ep_len:500 episode reward: total was -44.520000. running mean: -27.580396\n",
      "ep 1984: ep_len:658 episode reward: total was -57.430000. running mean: -27.878892\n",
      "ep 1984: ep_len:125 episode reward: total was 4.500000. running mean: -27.555103\n",
      "ep 1984: ep_len:3 episode reward: total was 1.010000. running mean: -27.269452\n",
      "ep 1984: ep_len:500 episode reward: total was -46.610000. running mean: -27.462857\n",
      "ep 1984: ep_len:583 episode reward: total was 21.630000. running mean: -26.971929\n",
      "epsilon:0.111998 episode_count: 13895. steps_count: 6025076.000000\n",
      "Time elapsed:  17445.657689094543\n",
      "ep 1985: ep_len:646 episode reward: total was -97.380000. running mean: -27.676009\n",
      "ep 1985: ep_len:516 episode reward: total was -71.180000. running mean: -28.111049\n",
      "ep 1985: ep_len:547 episode reward: total was -40.210000. running mean: -28.232039\n",
      "ep 1985: ep_len:170 episode reward: total was 3.140000. running mean: -27.918318\n",
      "ep 1985: ep_len:3 episode reward: total was 0.000000. running mean: -27.639135\n",
      "ep 1985: ep_len:316 episode reward: total was -0.010000. running mean: -27.362844\n",
      "ep 1985: ep_len:628 episode reward: total was -2.110000. running mean: -27.110315\n",
      "epsilon:0.111954 episode_count: 13902. steps_count: 6027902.000000\n",
      "Time elapsed:  17453.21139216423\n",
      "ep 1986: ep_len:631 episode reward: total was -19.930000. running mean: -27.038512\n",
      "ep 1986: ep_len:554 episode reward: total was -52.800000. running mean: -27.296127\n",
      "ep 1986: ep_len:500 episode reward: total was -44.220000. running mean: -27.465366\n",
      "ep 1986: ep_len:606 episode reward: total was -10.330000. running mean: -27.294012\n",
      "ep 1986: ep_len:3 episode reward: total was 0.000000. running mean: -27.021072\n",
      "ep 1986: ep_len:653 episode reward: total was -32.140000. running mean: -27.072261\n",
      "ep 1986: ep_len:501 episode reward: total was -70.120000. running mean: -27.502739\n",
      "epsilon:0.111910 episode_count: 13909. steps_count: 6031350.000000\n",
      "Time elapsed:  17462.619843244553\n",
      "ep 1987: ep_len:566 episode reward: total was -82.350000. running mean: -28.051211\n",
      "ep 1987: ep_len:560 episode reward: total was -45.540000. running mean: -28.226099\n",
      "ep 1987: ep_len:500 episode reward: total was -27.990000. running mean: -28.223738\n",
      "ep 1987: ep_len:500 episode reward: total was -62.420000. running mean: -28.565701\n",
      "ep 1987: ep_len:2 episode reward: total was -0.500000. running mean: -28.285044\n",
      "ep 1987: ep_len:508 episode reward: total was -1.600000. running mean: -28.018193\n",
      "ep 1987: ep_len:343 episode reward: total was -24.710000. running mean: -27.985111\n",
      "epsilon:0.111865 episode_count: 13916. steps_count: 6034329.000000\n",
      "Time elapsed:  17470.627853631973\n",
      "ep 1988: ep_len:563 episode reward: total was 2.510000. running mean: -27.680160\n",
      "ep 1988: ep_len:621 episode reward: total was -2.170000. running mean: -27.425059\n",
      "ep 1988: ep_len:622 episode reward: total was -57.550000. running mean: -27.726308\n",
      "ep 1988: ep_len:500 episode reward: total was -36.110000. running mean: -27.810145\n",
      "ep 1988: ep_len:3 episode reward: total was 1.010000. running mean: -27.521944\n",
      "ep 1988: ep_len:500 episode reward: total was -32.070000. running mean: -27.567424\n",
      "ep 1988: ep_len:579 episode reward: total was -37.490000. running mean: -27.666650\n",
      "epsilon:0.111821 episode_count: 13923. steps_count: 6037717.000000\n",
      "Time elapsed:  17479.510147333145\n",
      "ep 1989: ep_len:131 episode reward: total was 8.530000. running mean: -27.304683\n",
      "ep 1989: ep_len:501 episode reward: total was -77.060000. running mean: -27.802237\n",
      "ep 1989: ep_len:559 episode reward: total was -62.200000. running mean: -28.146214\n",
      "ep 1989: ep_len:594 episode reward: total was 23.370000. running mean: -27.631052\n",
      "ep 1989: ep_len:79 episode reward: total was 1.760000. running mean: -27.337142\n",
      "ep 1989: ep_len:701 episode reward: total was -51.150000. running mean: -27.575270\n",
      "ep 1989: ep_len:500 episode reward: total was -20.390000. running mean: -27.503417\n",
      "epsilon:0.111777 episode_count: 13930. steps_count: 6040782.000000\n",
      "Time elapsed:  17487.64498400688\n",
      "ep 1990: ep_len:604 episode reward: total was -50.910000. running mean: -27.737483\n",
      "ep 1990: ep_len:507 episode reward: total was -6.790000. running mean: -27.528008\n",
      "ep 1990: ep_len:606 episode reward: total was -72.240000. running mean: -27.975128\n",
      "ep 1990: ep_len:500 episode reward: total was 35.240000. running mean: -27.342977\n",
      "ep 1990: ep_len:3 episode reward: total was 0.000000. running mean: -27.069547\n",
      "ep 1990: ep_len:609 episode reward: total was -72.000000. running mean: -27.518852\n",
      "ep 1990: ep_len:500 episode reward: total was -38.230000. running mean: -27.625963\n",
      "epsilon:0.111732 episode_count: 13937. steps_count: 6044111.000000\n",
      "Time elapsed:  17496.5831823349\n",
      "ep 1991: ep_len:544 episode reward: total was -14.040000. running mean: -27.490104\n",
      "ep 1991: ep_len:649 episode reward: total was 2.290000. running mean: -27.192303\n",
      "ep 1991: ep_len:360 episode reward: total was 6.500000. running mean: -26.855380\n",
      "ep 1991: ep_len:592 episode reward: total was -17.670000. running mean: -26.763526\n",
      "ep 1991: ep_len:80 episode reward: total was 11.720000. running mean: -26.378691\n",
      "ep 1991: ep_len:500 episode reward: total was -11.120000. running mean: -26.226104\n",
      "ep 1991: ep_len:547 episode reward: total was -32.490000. running mean: -26.288743\n",
      "epsilon:0.111688 episode_count: 13944. steps_count: 6047383.000000\n",
      "Time elapsed:  17502.9035320282\n",
      "ep 1992: ep_len:563 episode reward: total was -88.470000. running mean: -26.910555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1992: ep_len:520 episode reward: total was -7.540000. running mean: -26.716850\n",
      "ep 1992: ep_len:515 episode reward: total was -58.370000. running mean: -27.033381\n",
      "ep 1992: ep_len:500 episode reward: total was -16.650000. running mean: -26.929547\n",
      "ep 1992: ep_len:108 episode reward: total was 27.220000. running mean: -26.388052\n",
      "ep 1992: ep_len:290 episode reward: total was 20.970000. running mean: -25.914471\n",
      "ep 1992: ep_len:509 episode reward: total was -56.780000. running mean: -26.223127\n",
      "epsilon:0.111644 episode_count: 13951. steps_count: 6050388.000000\n",
      "Time elapsed:  17509.825320482254\n",
      "ep 1993: ep_len:645 episode reward: total was -70.730000. running mean: -26.668195\n",
      "ep 1993: ep_len:540 episode reward: total was -81.470000. running mean: -27.216213\n",
      "ep 1993: ep_len:584 episode reward: total was -57.090000. running mean: -27.514951\n",
      "ep 1993: ep_len:540 episode reward: total was -1.150000. running mean: -27.251302\n",
      "ep 1993: ep_len:3 episode reward: total was 1.010000. running mean: -26.968689\n",
      "ep 1993: ep_len:288 episode reward: total was 0.930000. running mean: -26.689702\n",
      "ep 1993: ep_len:500 episode reward: total was -52.520000. running mean: -26.948005\n",
      "epsilon:0.111599 episode_count: 13958. steps_count: 6053488.000000\n",
      "Time elapsed:  17518.158262968063\n",
      "ep 1994: ep_len:501 episode reward: total was -92.780000. running mean: -27.606325\n",
      "ep 1994: ep_len:622 episode reward: total was -162.520000. running mean: -28.955462\n",
      "ep 1994: ep_len:438 episode reward: total was 14.130000. running mean: -28.524607\n",
      "ep 1994: ep_len:603 episode reward: total was 6.040000. running mean: -28.178961\n",
      "ep 1994: ep_len:3 episode reward: total was 1.010000. running mean: -27.887071\n",
      "ep 1994: ep_len:500 episode reward: total was 12.260000. running mean: -27.485601\n",
      "ep 1994: ep_len:551 episode reward: total was -44.750000. running mean: -27.658245\n",
      "epsilon:0.111555 episode_count: 13965. steps_count: 6056706.000000\n",
      "Time elapsed:  17526.789045095444\n",
      "ep 1995: ep_len:570 episode reward: total was -77.900000. running mean: -28.160662\n",
      "ep 1995: ep_len:354 episode reward: total was -11.960000. running mean: -27.998655\n",
      "ep 1995: ep_len:576 episode reward: total was -0.920000. running mean: -27.727869\n",
      "ep 1995: ep_len:511 episode reward: total was -36.190000. running mean: -27.812490\n",
      "ep 1995: ep_len:128 episode reward: total was 14.840000. running mean: -27.385965\n",
      "ep 1995: ep_len:544 episode reward: total was -110.080000. running mean: -28.212906\n",
      "ep 1995: ep_len:551 episode reward: total was -22.170000. running mean: -28.152477\n",
      "epsilon:0.111511 episode_count: 13972. steps_count: 6059940.000000\n",
      "Time elapsed:  17535.298005104065\n",
      "ep 1996: ep_len:500 episode reward: total was 45.860000. running mean: -27.412352\n",
      "ep 1996: ep_len:525 episode reward: total was 3.900000. running mean: -27.099228\n",
      "ep 1996: ep_len:610 episode reward: total was -48.870000. running mean: -27.316936\n",
      "ep 1996: ep_len:546 episode reward: total was -90.370000. running mean: -27.947467\n",
      "ep 1996: ep_len:98 episode reward: total was -43.780000. running mean: -28.105792\n",
      "ep 1996: ep_len:531 episode reward: total was -25.000000. running mean: -28.074734\n",
      "ep 1996: ep_len:500 episode reward: total was -36.110000. running mean: -28.155087\n",
      "epsilon:0.111466 episode_count: 13979. steps_count: 6063250.000000\n",
      "Time elapsed:  17546.728066444397\n",
      "ep 1997: ep_len:628 episode reward: total was -89.910000. running mean: -28.772636\n",
      "ep 1997: ep_len:363 episode reward: total was -197.410000. running mean: -30.459010\n",
      "ep 1997: ep_len:502 episode reward: total was -48.050000. running mean: -30.634919\n",
      "ep 1997: ep_len:501 episode reward: total was -12.840000. running mean: -30.456970\n",
      "ep 1997: ep_len:3 episode reward: total was 1.010000. running mean: -30.142301\n",
      "ep 1997: ep_len:540 episode reward: total was -6.000000. running mean: -29.900878\n",
      "ep 1997: ep_len:314 episode reward: total was -28.210000. running mean: -29.883969\n",
      "epsilon:0.111422 episode_count: 13986. steps_count: 6066101.000000\n",
      "Time elapsed:  17556.673451900482\n",
      "ep 1998: ep_len:500 episode reward: total was -107.420000. running mean: -30.659329\n",
      "ep 1998: ep_len:500 episode reward: total was 10.130000. running mean: -30.251436\n",
      "ep 1998: ep_len:500 episode reward: total was 7.460000. running mean: -29.874321\n",
      "ep 1998: ep_len:593 episode reward: total was 50.360000. running mean: -29.071978\n",
      "ep 1998: ep_len:3 episode reward: total was 1.010000. running mean: -28.771158\n",
      "ep 1998: ep_len:590 episode reward: total was -21.900000. running mean: -28.702447\n",
      "ep 1998: ep_len:559 episode reward: total was -94.950000. running mean: -29.364922\n",
      "epsilon:0.111378 episode_count: 13993. steps_count: 6069346.000000\n",
      "Time elapsed:  17564.92117333412\n",
      "ep 1999: ep_len:227 episode reward: total was 12.330000. running mean: -28.947973\n",
      "ep 1999: ep_len:551 episode reward: total was 40.460000. running mean: -28.253893\n",
      "ep 1999: ep_len:545 episode reward: total was 3.300000. running mean: -27.938354\n",
      "ep 1999: ep_len:500 episode reward: total was 43.220000. running mean: -27.226771\n",
      "ep 1999: ep_len:3 episode reward: total was 1.010000. running mean: -26.944403\n",
      "ep 1999: ep_len:506 episode reward: total was -16.420000. running mean: -26.839159\n",
      "ep 1999: ep_len:206 episode reward: total was -37.660000. running mean: -26.947368\n",
      "epsilon:0.111333 episode_count: 14000. steps_count: 6071884.000000\n",
      "Time elapsed:  17571.972867012024\n",
      "ep 2000: ep_len:605 episode reward: total was -21.120000. running mean: -26.889094\n",
      "ep 2000: ep_len:500 episode reward: total was -11.720000. running mean: -26.737403\n",
      "ep 2000: ep_len:633 episode reward: total was -57.650000. running mean: -27.046529\n",
      "ep 2000: ep_len:518 episode reward: total was -14.130000. running mean: -26.917364\n",
      "ep 2000: ep_len:80 episode reward: total was 14.230000. running mean: -26.505890\n",
      "ep 2000: ep_len:226 episode reward: total was 23.800000. running mean: -26.002831\n",
      "ep 2000: ep_len:512 episode reward: total was -27.390000. running mean: -26.016703\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.111289 episode_count: 14007. steps_count: 6074958.000000\n",
      "Time elapsed:  17585.087306022644\n",
      "ep 2001: ep_len:576 episode reward: total was 20.020000. running mean: -25.556336\n",
      "ep 2001: ep_len:568 episode reward: total was -116.940000. running mean: -26.470172\n",
      "ep 2001: ep_len:406 episode reward: total was 10.960000. running mean: -26.095871\n",
      "ep 2001: ep_len:507 episode reward: total was -41.130000. running mean: -26.246212\n",
      "ep 2001: ep_len:3 episode reward: total was 1.010000. running mean: -25.973650\n",
      "ep 2001: ep_len:655 episode reward: total was -39.140000. running mean: -26.105313\n",
      "ep 2001: ep_len:298 episode reward: total was -28.410000. running mean: -26.128360\n",
      "epsilon:0.111245 episode_count: 14014. steps_count: 6077971.000000\n",
      "Time elapsed:  17592.950954198837\n",
      "ep 2002: ep_len:530 episode reward: total was -64.910000. running mean: -26.516177\n",
      "ep 2002: ep_len:588 episode reward: total was -23.070000. running mean: -26.481715\n",
      "ep 2002: ep_len:540 episode reward: total was -128.340000. running mean: -27.500298\n",
      "ep 2002: ep_len:510 episode reward: total was -41.170000. running mean: -27.636995\n",
      "ep 2002: ep_len:3 episode reward: total was 1.010000. running mean: -27.350525\n",
      "ep 2002: ep_len:522 episode reward: total was -57.770000. running mean: -27.654720\n",
      "ep 2002: ep_len:542 episode reward: total was -50.070000. running mean: -27.878872\n",
      "epsilon:0.111200 episode_count: 14021. steps_count: 6081206.000000\n",
      "Time elapsed:  17600.638674020767\n",
      "ep 2003: ep_len:504 episode reward: total was 36.870000. running mean: -27.231384\n",
      "ep 2003: ep_len:500 episode reward: total was 3.660000. running mean: -26.922470\n",
      "ep 2003: ep_len:583 episode reward: total was -64.130000. running mean: -27.294545\n",
      "ep 2003: ep_len:536 episode reward: total was 1.110000. running mean: -27.010500\n",
      "ep 2003: ep_len:49 episode reward: total was 20.000000. running mean: -26.540395\n",
      "ep 2003: ep_len:600 episode reward: total was -66.370000. running mean: -26.938691\n",
      "ep 2003: ep_len:626 episode reward: total was -33.140000. running mean: -27.000704\n",
      "epsilon:0.111156 episode_count: 14028. steps_count: 6084604.000000\n",
      "Time elapsed:  17609.346725463867\n",
      "ep 2004: ep_len:575 episode reward: total was -132.650000. running mean: -28.057197\n",
      "ep 2004: ep_len:568 episode reward: total was 46.650000. running mean: -27.310125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2004: ep_len:646 episode reward: total was -84.430000. running mean: -27.881324\n",
      "ep 2004: ep_len:160 episode reward: total was 7.670000. running mean: -27.525810\n",
      "ep 2004: ep_len:3 episode reward: total was 1.010000. running mean: -27.240452\n",
      "ep 2004: ep_len:710 episode reward: total was -25.760000. running mean: -27.225648\n",
      "ep 2004: ep_len:500 episode reward: total was -35.570000. running mean: -27.309091\n",
      "epsilon:0.111112 episode_count: 14035. steps_count: 6087766.000000\n",
      "Time elapsed:  17620.295128822327\n",
      "ep 2005: ep_len:132 episode reward: total was 11.020000. running mean: -26.925800\n",
      "ep 2005: ep_len:501 episode reward: total was 0.470000. running mean: -26.651842\n",
      "ep 2005: ep_len:537 episode reward: total was -56.490000. running mean: -26.950224\n",
      "ep 2005: ep_len:580 episode reward: total was -1.190000. running mean: -26.692622\n",
      "ep 2005: ep_len:3 episode reward: total was 0.000000. running mean: -26.425695\n",
      "ep 2005: ep_len:537 episode reward: total was -36.660000. running mean: -26.528038\n",
      "ep 2005: ep_len:532 episode reward: total was -40.010000. running mean: -26.662858\n",
      "epsilon:0.111067 episode_count: 14042. steps_count: 6090588.000000\n",
      "Time elapsed:  17627.14328289032\n",
      "ep 2006: ep_len:575 episode reward: total was -92.370000. running mean: -27.319930\n",
      "ep 2006: ep_len:500 episode reward: total was -30.870000. running mean: -27.355430\n",
      "ep 2006: ep_len:619 episode reward: total was -57.970000. running mean: -27.661576\n",
      "ep 2006: ep_len:558 episode reward: total was 20.830000. running mean: -27.176660\n",
      "ep 2006: ep_len:3 episode reward: total was 1.010000. running mean: -26.894794\n",
      "ep 2006: ep_len:500 episode reward: total was 15.680000. running mean: -26.469046\n",
      "ep 2006: ep_len:566 episode reward: total was -70.300000. running mean: -26.907355\n",
      "epsilon:0.111023 episode_count: 14049. steps_count: 6093909.000000\n",
      "Time elapsed:  17638.915937423706\n",
      "ep 2007: ep_len:637 episode reward: total was -58.630000. running mean: -27.224582\n",
      "ep 2007: ep_len:524 episode reward: total was -43.460000. running mean: -27.386936\n",
      "ep 2007: ep_len:513 episode reward: total was -46.850000. running mean: -27.581566\n",
      "ep 2007: ep_len:500 episode reward: total was -30.330000. running mean: -27.609051\n",
      "ep 2007: ep_len:3 episode reward: total was 1.010000. running mean: -27.322860\n",
      "ep 2007: ep_len:544 episode reward: total was -9.680000. running mean: -27.146432\n",
      "ep 2007: ep_len:500 episode reward: total was -44.250000. running mean: -27.317467\n",
      "epsilon:0.110979 episode_count: 14056. steps_count: 6097130.000000\n",
      "Time elapsed:  17649.781933784485\n",
      "ep 2008: ep_len:543 episode reward: total was -47.830000. running mean: -27.522593\n",
      "ep 2008: ep_len:500 episode reward: total was 19.030000. running mean: -27.057067\n",
      "ep 2008: ep_len:516 episode reward: total was -64.340000. running mean: -27.429896\n",
      "ep 2008: ep_len:528 episode reward: total was -4.590000. running mean: -27.201497\n",
      "ep 2008: ep_len:3 episode reward: total was 1.010000. running mean: -26.919382\n",
      "ep 2008: ep_len:600 episode reward: total was -11.330000. running mean: -26.763488\n",
      "ep 2008: ep_len:523 episode reward: total was -28.350000. running mean: -26.779353\n",
      "epsilon:0.110934 episode_count: 14063. steps_count: 6100343.000000\n",
      "Time elapsed:  17656.32839035988\n",
      "ep 2009: ep_len:574 episode reward: total was 8.370000. running mean: -26.427860\n",
      "ep 2009: ep_len:501 episode reward: total was -40.460000. running mean: -26.568181\n",
      "ep 2009: ep_len:638 episode reward: total was -26.840000. running mean: -26.570899\n",
      "ep 2009: ep_len:512 episode reward: total was 22.740000. running mean: -26.077790\n",
      "ep 2009: ep_len:3 episode reward: total was 1.010000. running mean: -25.806913\n",
      "ep 2009: ep_len:601 episode reward: total was -2.400000. running mean: -25.572843\n",
      "ep 2009: ep_len:512 episode reward: total was -27.040000. running mean: -25.587515\n",
      "epsilon:0.110890 episode_count: 14070. steps_count: 6103684.000000\n",
      "Time elapsed:  17664.089853286743\n",
      "ep 2010: ep_len:191 episode reward: total was 5.630000. running mean: -25.275340\n",
      "ep 2010: ep_len:536 episode reward: total was 46.420000. running mean: -24.558386\n",
      "ep 2010: ep_len:501 episode reward: total was -28.610000. running mean: -24.598903\n",
      "ep 2010: ep_len:502 episode reward: total was -36.100000. running mean: -24.713914\n",
      "ep 2010: ep_len:3 episode reward: total was -1.500000. running mean: -24.481774\n",
      "ep 2010: ep_len:502 episode reward: total was -98.900000. running mean: -25.225957\n",
      "ep 2010: ep_len:604 episode reward: total was -38.640000. running mean: -25.360097\n",
      "epsilon:0.110846 episode_count: 14077. steps_count: 6106523.000000\n",
      "Time elapsed:  17671.81609249115\n",
      "ep 2011: ep_len:500 episode reward: total was 36.710000. running mean: -24.739396\n",
      "ep 2011: ep_len:500 episode reward: total was 24.150000. running mean: -24.250502\n",
      "ep 2011: ep_len:524 episode reward: total was -44.080000. running mean: -24.448797\n",
      "ep 2011: ep_len:103 episode reward: total was 8.960000. running mean: -24.114709\n",
      "ep 2011: ep_len:3 episode reward: total was 1.010000. running mean: -23.863462\n",
      "ep 2011: ep_len:613 episode reward: total was -119.930000. running mean: -24.824127\n",
      "ep 2011: ep_len:544 episode reward: total was -21.890000. running mean: -24.794786\n",
      "epsilon:0.110801 episode_count: 14084. steps_count: 6109310.000000\n",
      "Time elapsed:  17679.34105682373\n",
      "ep 2012: ep_len:220 episode reward: total was 12.720000. running mean: -24.419638\n",
      "ep 2012: ep_len:177 episode reward: total was -43.900000. running mean: -24.614442\n",
      "ep 2012: ep_len:79 episode reward: total was -2.250000. running mean: -24.390798\n",
      "ep 2012: ep_len:537 episode reward: total was -35.750000. running mean: -24.504390\n",
      "ep 2012: ep_len:3 episode reward: total was 1.010000. running mean: -24.249246\n",
      "ep 2012: ep_len:512 episode reward: total was -99.980000. running mean: -25.006553\n",
      "ep 2012: ep_len:328 episode reward: total was -18.130000. running mean: -24.937788\n",
      "epsilon:0.110757 episode_count: 14091. steps_count: 6111166.000000\n",
      "Time elapsed:  17685.79827284813\n",
      "ep 2013: ep_len:543 episode reward: total was 23.380000. running mean: -24.454610\n",
      "ep 2013: ep_len:540 episode reward: total was 7.820000. running mean: -24.131864\n",
      "ep 2013: ep_len:556 episode reward: total was -38.760000. running mean: -24.278145\n",
      "ep 2013: ep_len:500 episode reward: total was -21.170000. running mean: -24.247064\n",
      "ep 2013: ep_len:3 episode reward: total was 1.010000. running mean: -23.994493\n",
      "ep 2013: ep_len:539 episode reward: total was -10.890000. running mean: -23.863448\n",
      "ep 2013: ep_len:562 episode reward: total was -18.890000. running mean: -23.813714\n",
      "epsilon:0.110713 episode_count: 14098. steps_count: 6114409.000000\n",
      "Time elapsed:  17693.42460989952\n",
      "ep 2014: ep_len:671 episode reward: total was -70.680000. running mean: -24.282376\n",
      "ep 2014: ep_len:562 episode reward: total was 70.320000. running mean: -23.336353\n",
      "ep 2014: ep_len:567 episode reward: total was -65.150000. running mean: -23.754489\n",
      "ep 2014: ep_len:565 episode reward: total was 46.040000. running mean: -23.056544\n",
      "ep 2014: ep_len:130 episode reward: total was -52.650000. running mean: -23.352479\n",
      "ep 2014: ep_len:500 episode reward: total was -68.900000. running mean: -23.807954\n",
      "ep 2014: ep_len:500 episode reward: total was -34.610000. running mean: -23.915974\n",
      "epsilon:0.110668 episode_count: 14105. steps_count: 6117904.000000\n",
      "Time elapsed:  17704.94125676155\n",
      "ep 2015: ep_len:500 episode reward: total was 33.050000. running mean: -23.346315\n",
      "ep 2015: ep_len:532 episode reward: total was 34.970000. running mean: -22.763152\n",
      "ep 2015: ep_len:636 episode reward: total was -16.090000. running mean: -22.696420\n",
      "ep 2015: ep_len:598 episode reward: total was 32.450000. running mean: -22.144956\n",
      "ep 2015: ep_len:3 episode reward: total was 1.010000. running mean: -21.913406\n",
      "ep 2015: ep_len:500 episode reward: total was 18.770000. running mean: -21.506572\n",
      "ep 2015: ep_len:592 episode reward: total was -23.930000. running mean: -21.530807\n",
      "epsilon:0.110624 episode_count: 14112. steps_count: 6121265.000000\n",
      "Time elapsed:  17712.439692735672\n",
      "ep 2016: ep_len:518 episode reward: total was -204.120000. running mean: -23.356698\n",
      "ep 2016: ep_len:537 episode reward: total was -20.950000. running mean: -23.332631\n",
      "ep 2016: ep_len:79 episode reward: total was -1.760000. running mean: -23.116905\n",
      "ep 2016: ep_len:502 episode reward: total was -58.650000. running mean: -23.472236\n",
      "ep 2016: ep_len:3 episode reward: total was -1.500000. running mean: -23.252514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2016: ep_len:507 episode reward: total was -79.780000. running mean: -23.817789\n",
      "ep 2016: ep_len:500 episode reward: total was -14.950000. running mean: -23.729111\n",
      "epsilon:0.110580 episode_count: 14119. steps_count: 6123911.000000\n",
      "Time elapsed:  17719.739342212677\n",
      "ep 2017: ep_len:558 episode reward: total was 19.290000. running mean: -23.298920\n",
      "ep 2017: ep_len:292 episode reward: total was -71.690000. running mean: -23.782830\n",
      "ep 2017: ep_len:500 episode reward: total was -100.470000. running mean: -24.549702\n",
      "ep 2017: ep_len:502 episode reward: total was -92.810000. running mean: -25.232305\n",
      "ep 2017: ep_len:3 episode reward: total was 1.010000. running mean: -24.969882\n",
      "ep 2017: ep_len:500 episode reward: total was -32.040000. running mean: -25.040583\n",
      "ep 2017: ep_len:500 episode reward: total was -8.390000. running mean: -24.874077\n",
      "epsilon:0.110535 episode_count: 14126. steps_count: 6126766.000000\n",
      "Time elapsed:  17727.605083942413\n",
      "ep 2018: ep_len:500 episode reward: total was 36.860000. running mean: -24.256737\n",
      "ep 2018: ep_len:367 episode reward: total was 1.090000. running mean: -24.003269\n",
      "ep 2018: ep_len:612 episode reward: total was -24.480000. running mean: -24.008037\n",
      "ep 2018: ep_len:54 episode reward: total was -0.210000. running mean: -23.770056\n",
      "ep 2018: ep_len:72 episode reward: total was -49.840000. running mean: -24.030756\n",
      "ep 2018: ep_len:691 episode reward: total was -23.300000. running mean: -24.023448\n",
      "ep 2018: ep_len:541 episode reward: total was -83.300000. running mean: -24.616214\n",
      "epsilon:0.110491 episode_count: 14133. steps_count: 6129603.000000\n",
      "Time elapsed:  17732.818087100983\n",
      "ep 2019: ep_len:204 episode reward: total was -11.960000. running mean: -24.489651\n",
      "ep 2019: ep_len:500 episode reward: total was -12.250000. running mean: -24.367255\n",
      "ep 2019: ep_len:552 episode reward: total was -63.600000. running mean: -24.759582\n",
      "ep 2019: ep_len:500 episode reward: total was -17.910000. running mean: -24.691087\n",
      "ep 2019: ep_len:119 episode reward: total was -60.250000. running mean: -25.046676\n",
      "ep 2019: ep_len:606 episode reward: total was -19.310000. running mean: -24.989309\n",
      "ep 2019: ep_len:500 episode reward: total was 7.080000. running mean: -24.668616\n",
      "epsilon:0.110447 episode_count: 14140. steps_count: 6132584.000000\n",
      "Time elapsed:  17740.137224197388\n",
      "ep 2020: ep_len:539 episode reward: total was 31.160000. running mean: -24.110330\n",
      "ep 2020: ep_len:500 episode reward: total was 2.130000. running mean: -23.847926\n",
      "ep 2020: ep_len:500 episode reward: total was -26.690000. running mean: -23.876347\n",
      "ep 2020: ep_len:55 episode reward: total was 2.310000. running mean: -23.614484\n",
      "ep 2020: ep_len:131 episode reward: total was -20.180000. running mean: -23.580139\n",
      "ep 2020: ep_len:659 episode reward: total was -37.760000. running mean: -23.721937\n",
      "ep 2020: ep_len:519 episode reward: total was -24.070000. running mean: -23.725418\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.110402 episode_count: 14147. steps_count: 6135487.000000\n",
      "Time elapsed:  17752.756322145462\n",
      "ep 2021: ep_len:565 episode reward: total was 27.300000. running mean: -23.215164\n",
      "ep 2021: ep_len:500 episode reward: total was -44.590000. running mean: -23.428912\n",
      "ep 2021: ep_len:500 episode reward: total was -55.160000. running mean: -23.746223\n",
      "ep 2021: ep_len:500 episode reward: total was 19.070000. running mean: -23.318061\n",
      "ep 2021: ep_len:3 episode reward: total was 1.010000. running mean: -23.074780\n",
      "ep 2021: ep_len:153 episode reward: total was 5.480000. running mean: -22.789232\n",
      "ep 2021: ep_len:194 episode reward: total was -13.480000. running mean: -22.696140\n",
      "epsilon:0.110358 episode_count: 14154. steps_count: 6137902.000000\n",
      "Time elapsed:  17759.539061546326\n",
      "ep 2022: ep_len:500 episode reward: total was 1.560000. running mean: -22.453579\n",
      "ep 2022: ep_len:501 episode reward: total was -32.790000. running mean: -22.556943\n",
      "ep 2022: ep_len:538 episode reward: total was -33.140000. running mean: -22.662774\n",
      "ep 2022: ep_len:500 episode reward: total was -35.330000. running mean: -22.789446\n",
      "ep 2022: ep_len:3 episode reward: total was 1.010000. running mean: -22.551451\n",
      "ep 2022: ep_len:500 episode reward: total was -42.640000. running mean: -22.752337\n",
      "ep 2022: ep_len:557 episode reward: total was 5.290000. running mean: -22.471913\n",
      "epsilon:0.110314 episode_count: 14161. steps_count: 6141001.000000\n",
      "Time elapsed:  17767.769020080566\n",
      "ep 2023: ep_len:100 episode reward: total was -19.120000. running mean: -22.438394\n",
      "ep 2023: ep_len:500 episode reward: total was 3.020000. running mean: -22.183810\n",
      "ep 2023: ep_len:79 episode reward: total was 3.260000. running mean: -21.929372\n",
      "ep 2023: ep_len:532 episode reward: total was -14.060000. running mean: -21.850679\n",
      "ep 2023: ep_len:3 episode reward: total was 1.010000. running mean: -21.622072\n",
      "ep 2023: ep_len:544 episode reward: total was -45.680000. running mean: -21.862651\n",
      "ep 2023: ep_len:507 episode reward: total was -28.320000. running mean: -21.927225\n",
      "epsilon:0.110269 episode_count: 14168. steps_count: 6143266.000000\n",
      "Time elapsed:  17774.123494386673\n",
      "ep 2024: ep_len:632 episode reward: total was -69.820000. running mean: -22.406152\n",
      "ep 2024: ep_len:507 episode reward: total was -36.430000. running mean: -22.546391\n",
      "ep 2024: ep_len:404 episode reward: total was -12.000000. running mean: -22.440927\n",
      "ep 2024: ep_len:548 episode reward: total was 22.560000. running mean: -21.990918\n",
      "ep 2024: ep_len:75 episode reward: total was -43.720000. running mean: -22.208208\n",
      "ep 2024: ep_len:506 episode reward: total was -11.320000. running mean: -22.099326\n",
      "ep 2024: ep_len:605 episode reward: total was -3.240000. running mean: -21.910733\n",
      "epsilon:0.110225 episode_count: 14175. steps_count: 6146543.000000\n",
      "Time elapsed:  17782.86394238472\n",
      "ep 2025: ep_len:501 episode reward: total was 9.170000. running mean: -21.599926\n",
      "ep 2025: ep_len:544 episode reward: total was 21.260000. running mean: -21.171326\n",
      "ep 2025: ep_len:75 episode reward: total was 5.240000. running mean: -20.907213\n",
      "ep 2025: ep_len:521 episode reward: total was -50.260000. running mean: -21.200741\n",
      "ep 2025: ep_len:50 episode reward: total was 20.500000. running mean: -20.783734\n",
      "ep 2025: ep_len:566 episode reward: total was -31.350000. running mean: -20.889396\n",
      "ep 2025: ep_len:333 episode reward: total was -20.590000. running mean: -20.886402\n",
      "epsilon:0.110181 episode_count: 14182. steps_count: 6149133.000000\n",
      "Time elapsed:  17789.92495751381\n",
      "ep 2026: ep_len:566 episode reward: total was -31.750000. running mean: -20.995038\n",
      "ep 2026: ep_len:535 episode reward: total was -31.910000. running mean: -21.104188\n",
      "ep 2026: ep_len:594 episode reward: total was -108.100000. running mean: -21.974146\n",
      "ep 2026: ep_len:509 episode reward: total was -3.700000. running mean: -21.791405\n",
      "ep 2026: ep_len:3 episode reward: total was 0.000000. running mean: -21.573491\n",
      "ep 2026: ep_len:500 episode reward: total was -68.620000. running mean: -22.043956\n",
      "ep 2026: ep_len:500 episode reward: total was -17.190000. running mean: -21.995416\n",
      "epsilon:0.110136 episode_count: 14189. steps_count: 6152340.000000\n",
      "Time elapsed:  17798.31153011322\n",
      "ep 2027: ep_len:500 episode reward: total was 26.190000. running mean: -21.513562\n",
      "ep 2027: ep_len:530 episode reward: total was -28.770000. running mean: -21.586126\n",
      "ep 2027: ep_len:554 episode reward: total was -73.950000. running mean: -22.109765\n",
      "ep 2027: ep_len:500 episode reward: total was -15.430000. running mean: -22.042967\n",
      "ep 2027: ep_len:86 episode reward: total was -60.750000. running mean: -22.430038\n",
      "ep 2027: ep_len:634 episode reward: total was -52.340000. running mean: -22.729137\n",
      "ep 2027: ep_len:516 episode reward: total was -26.470000. running mean: -22.766546\n",
      "epsilon:0.110092 episode_count: 14196. steps_count: 6155660.000000\n",
      "Time elapsed:  17807.246955633163\n",
      "ep 2028: ep_len:121 episode reward: total was -3.570000. running mean: -22.574581\n",
      "ep 2028: ep_len:515 episode reward: total was -9.060000. running mean: -22.439435\n",
      "ep 2028: ep_len:79 episode reward: total was -0.750000. running mean: -22.222540\n",
      "ep 2028: ep_len:537 episode reward: total was 15.560000. running mean: -21.844715\n",
      "ep 2028: ep_len:3 episode reward: total was 1.010000. running mean: -21.616168\n",
      "ep 2028: ep_len:553 episode reward: total was -40.020000. running mean: -21.800206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2028: ep_len:302 episode reward: total was -39.090000. running mean: -21.973104\n",
      "epsilon:0.110048 episode_count: 14203. steps_count: 6157770.000000\n",
      "Time elapsed:  17814.554625034332\n",
      "ep 2029: ep_len:592 episode reward: total was 20.240000. running mean: -21.550973\n",
      "ep 2029: ep_len:507 episode reward: total was 20.690000. running mean: -21.128563\n",
      "ep 2029: ep_len:606 episode reward: total was -20.060000. running mean: -21.117878\n",
      "ep 2029: ep_len:532 episode reward: total was -33.910000. running mean: -21.245799\n",
      "ep 2029: ep_len:94 episode reward: total was 8.220000. running mean: -20.951141\n",
      "ep 2029: ep_len:501 episode reward: total was 1.860000. running mean: -20.723030\n",
      "ep 2029: ep_len:503 episode reward: total was -35.730000. running mean: -20.873099\n",
      "epsilon:0.110003 episode_count: 14210. steps_count: 6161105.000000\n",
      "Time elapsed:  17825.49607872963\n",
      "ep 2030: ep_len:551 episode reward: total was 20.340000. running mean: -20.460968\n",
      "ep 2030: ep_len:527 episode reward: total was -42.200000. running mean: -20.678359\n",
      "ep 2030: ep_len:432 episode reward: total was 17.900000. running mean: -20.292575\n",
      "ep 2030: ep_len:362 episode reward: total was -75.880000. running mean: -20.848449\n",
      "ep 2030: ep_len:3 episode reward: total was 1.010000. running mean: -20.629865\n",
      "ep 2030: ep_len:657 episode reward: total was -38.130000. running mean: -20.804866\n",
      "ep 2030: ep_len:345 episode reward: total was -2.590000. running mean: -20.622717\n",
      "epsilon:0.109959 episode_count: 14217. steps_count: 6163982.000000\n",
      "Time elapsed:  17833.053253173828\n",
      "ep 2031: ep_len:601 episode reward: total was -95.020000. running mean: -21.366690\n",
      "ep 2031: ep_len:509 episode reward: total was -46.520000. running mean: -21.618223\n",
      "ep 2031: ep_len:500 episode reward: total was 3.290000. running mean: -21.369141\n",
      "ep 2031: ep_len:534 episode reward: total was 44.220000. running mean: -20.713250\n",
      "ep 2031: ep_len:69 episode reward: total was 9.680000. running mean: -20.409317\n",
      "ep 2031: ep_len:305 episode reward: total was -11.450000. running mean: -20.319724\n",
      "ep 2031: ep_len:273 episode reward: total was -8.630000. running mean: -20.202827\n",
      "epsilon:0.109915 episode_count: 14224. steps_count: 6166773.000000\n",
      "Time elapsed:  17839.75173330307\n",
      "ep 2032: ep_len:505 episode reward: total was -51.950000. running mean: -20.520299\n",
      "ep 2032: ep_len:500 episode reward: total was -12.390000. running mean: -20.438996\n",
      "ep 2032: ep_len:500 episode reward: total was 38.310000. running mean: -19.851506\n",
      "ep 2032: ep_len:168 episode reward: total was 16.670000. running mean: -19.486291\n",
      "ep 2032: ep_len:43 episode reward: total was 12.500000. running mean: -19.166428\n",
      "ep 2032: ep_len:644 episode reward: total was -9.850000. running mean: -19.073263\n",
      "ep 2032: ep_len:587 episode reward: total was -39.210000. running mean: -19.274631\n",
      "epsilon:0.109870 episode_count: 14231. steps_count: 6169720.000000\n",
      "Time elapsed:  17851.067878723145\n",
      "ep 2033: ep_len:568 episode reward: total was 42.420000. running mean: -18.657684\n",
      "ep 2033: ep_len:539 episode reward: total was 13.600000. running mean: -18.335108\n",
      "ep 2033: ep_len:373 episode reward: total was -3.530000. running mean: -18.187056\n",
      "ep 2033: ep_len:56 episode reward: total was 4.830000. running mean: -17.956886\n",
      "ep 2033: ep_len:3 episode reward: total was 1.010000. running mean: -17.767217\n",
      "ep 2033: ep_len:631 episode reward: total was -28.810000. running mean: -17.877645\n",
      "ep 2033: ep_len:527 episode reward: total was -33.700000. running mean: -18.035868\n",
      "epsilon:0.109826 episode_count: 14238. steps_count: 6172417.000000\n",
      "Time elapsed:  17860.83294081688\n",
      "ep 2034: ep_len:251 episode reward: total was -14.420000. running mean: -17.999710\n",
      "ep 2034: ep_len:544 episode reward: total was -36.260000. running mean: -18.182313\n",
      "ep 2034: ep_len:613 episode reward: total was -59.150000. running mean: -18.591990\n",
      "ep 2034: ep_len:167 episode reward: total was 14.210000. running mean: -18.263970\n",
      "ep 2034: ep_len:3 episode reward: total was -1.500000. running mean: -18.096330\n",
      "ep 2034: ep_len:549 episode reward: total was -27.360000. running mean: -18.188967\n",
      "ep 2034: ep_len:500 episode reward: total was 17.880000. running mean: -17.828277\n",
      "epsilon:0.109782 episode_count: 14245. steps_count: 6175044.000000\n",
      "Time elapsed:  17868.783061504364\n",
      "ep 2035: ep_len:668 episode reward: total was -106.160000. running mean: -18.711594\n",
      "ep 2035: ep_len:535 episode reward: total was 36.130000. running mean: -18.163178\n",
      "ep 2035: ep_len:577 episode reward: total was -43.600000. running mean: -18.417546\n",
      "ep 2035: ep_len:559 episode reward: total was -25.290000. running mean: -18.486271\n",
      "ep 2035: ep_len:3 episode reward: total was -1.500000. running mean: -18.316408\n",
      "ep 2035: ep_len:544 episode reward: total was -82.880000. running mean: -18.962044\n",
      "ep 2035: ep_len:617 episode reward: total was -41.620000. running mean: -19.188624\n",
      "epsilon:0.109737 episode_count: 14252. steps_count: 6178547.000000\n",
      "Time elapsed:  17880.59318614006\n",
      "ep 2036: ep_len:503 episode reward: total was 24.900000. running mean: -18.747738\n",
      "ep 2036: ep_len:601 episode reward: total was 49.600000. running mean: -18.064260\n",
      "ep 2036: ep_len:565 episode reward: total was -45.510000. running mean: -18.338718\n",
      "ep 2036: ep_len:414 episode reward: total was -16.930000. running mean: -18.324630\n",
      "ep 2036: ep_len:3 episode reward: total was -1.500000. running mean: -18.156384\n",
      "ep 2036: ep_len:500 episode reward: total was -16.450000. running mean: -18.139320\n",
      "ep 2036: ep_len:592 episode reward: total was 9.920000. running mean: -17.858727\n",
      "epsilon:0.109693 episode_count: 14259. steps_count: 6181725.000000\n",
      "Time elapsed:  17890.30433154106\n",
      "ep 2037: ep_len:500 episode reward: total was 21.680000. running mean: -17.463340\n",
      "ep 2037: ep_len:375 episode reward: total was -58.130000. running mean: -17.870006\n",
      "ep 2037: ep_len:587 episode reward: total was -32.390000. running mean: -18.015206\n",
      "ep 2037: ep_len:615 episode reward: total was -10.360000. running mean: -17.938654\n",
      "ep 2037: ep_len:3 episode reward: total was 1.010000. running mean: -17.749168\n",
      "ep 2037: ep_len:500 episode reward: total was -21.810000. running mean: -17.789776\n",
      "ep 2037: ep_len:580 episode reward: total was -1.870000. running mean: -17.630578\n",
      "epsilon:0.109649 episode_count: 14266. steps_count: 6184885.000000\n",
      "Time elapsed:  17896.530566453934\n",
      "ep 2038: ep_len:215 episode reward: total was 6.150000. running mean: -17.392772\n",
      "ep 2038: ep_len:601 episode reward: total was 3.710000. running mean: -17.181745\n",
      "ep 2038: ep_len:508 episode reward: total was -11.070000. running mean: -17.120627\n",
      "ep 2038: ep_len:508 episode reward: total was -31.940000. running mean: -17.268821\n",
      "ep 2038: ep_len:38 episode reward: total was 13.000000. running mean: -16.966133\n",
      "ep 2038: ep_len:532 episode reward: total was -30.090000. running mean: -17.097371\n",
      "ep 2038: ep_len:505 episode reward: total was -28.160000. running mean: -17.207998\n",
      "epsilon:0.109604 episode_count: 14273. steps_count: 6187792.000000\n",
      "Time elapsed:  17907.129191875458\n",
      "ep 2039: ep_len:218 episode reward: total was 7.160000. running mean: -16.964318\n",
      "ep 2039: ep_len:533 episode reward: total was 43.890000. running mean: -16.355775\n",
      "ep 2039: ep_len:601 episode reward: total was -24.560000. running mean: -16.437817\n",
      "ep 2039: ep_len:500 episode reward: total was -165.280000. running mean: -17.926239\n",
      "ep 2039: ep_len:3 episode reward: total was 0.000000. running mean: -17.746976\n",
      "ep 2039: ep_len:554 episode reward: total was -51.910000. running mean: -18.088607\n",
      "ep 2039: ep_len:588 episode reward: total was -40.480000. running mean: -18.312520\n",
      "epsilon:0.109560 episode_count: 14280. steps_count: 6190789.000000\n",
      "Time elapsed:  17917.793496847153\n",
      "ep 2040: ep_len:243 episode reward: total was 17.220000. running mean: -17.957195\n",
      "ep 2040: ep_len:510 episode reward: total was 52.770000. running mean: -17.249923\n",
      "ep 2040: ep_len:419 episode reward: total was -115.710000. running mean: -18.234524\n",
      "ep 2040: ep_len:487 episode reward: total was -20.580000. running mean: -18.257979\n",
      "ep 2040: ep_len:83 episode reward: total was 13.740000. running mean: -17.937999\n",
      "ep 2040: ep_len:500 episode reward: total was 11.490000. running mean: -17.643719\n",
      "ep 2040: ep_len:501 episode reward: total was -41.380000. running mean: -17.881082\n",
      "Initial position:  [  2   0  17  87  63 149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:0.109516 episode_count: 14287. steps_count: 6193532.000000\n",
      "Time elapsed:  17930.99300980568\n",
      "ep 2041: ep_len:229 episode reward: total was 15.840000. running mean: -17.543871\n",
      "ep 2041: ep_len:293 episode reward: total was -5.300000. running mean: -17.421432\n",
      "ep 2041: ep_len:655 episode reward: total was -102.060000. running mean: -18.267818\n",
      "ep 2041: ep_len:500 episode reward: total was -32.200000. running mean: -18.407140\n",
      "ep 2041: ep_len:3 episode reward: total was 1.010000. running mean: -18.212968\n",
      "ep 2041: ep_len:500 episode reward: total was -18.540000. running mean: -18.216239\n",
      "ep 2041: ep_len:553 episode reward: total was -17.680000. running mean: -18.210876\n",
      "epsilon:0.109471 episode_count: 14294. steps_count: 6196265.000000\n",
      "Time elapsed:  17942.28493833542\n",
      "ep 2042: ep_len:607 episode reward: total was 15.030000. running mean: -17.878468\n",
      "ep 2042: ep_len:501 episode reward: total was 36.600000. running mean: -17.333683\n",
      "ep 2042: ep_len:576 episode reward: total was -81.130000. running mean: -17.971646\n",
      "ep 2042: ep_len:506 episode reward: total was -24.830000. running mean: -18.040230\n",
      "ep 2042: ep_len:3 episode reward: total was 1.010000. running mean: -17.849727\n",
      "ep 2042: ep_len:508 episode reward: total was -42.250000. running mean: -18.093730\n",
      "ep 2042: ep_len:523 episode reward: total was -16.370000. running mean: -18.076493\n",
      "epsilon:0.109427 episode_count: 14301. steps_count: 6199489.000000\n",
      "Time elapsed:  17950.738923072815\n",
      "ep 2043: ep_len:514 episode reward: total was 12.220000. running mean: -17.773528\n",
      "ep 2043: ep_len:500 episode reward: total was 10.070000. running mean: -17.495093\n",
      "ep 2043: ep_len:505 episode reward: total was -42.380000. running mean: -17.743942\n",
      "ep 2043: ep_len:582 episode reward: total was -1.860000. running mean: -17.585102\n",
      "ep 2043: ep_len:3 episode reward: total was 1.010000. running mean: -17.399151\n",
      "ep 2043: ep_len:500 episode reward: total was -21.430000. running mean: -17.439460\n",
      "ep 2043: ep_len:168 episode reward: total was -21.760000. running mean: -17.482665\n",
      "epsilon:0.109383 episode_count: 14308. steps_count: 6202261.000000\n",
      "Time elapsed:  17960.6660695076\n",
      "ep 2044: ep_len:500 episode reward: total was -64.400000. running mean: -17.951838\n",
      "ep 2044: ep_len:500 episode reward: total was 35.150000. running mean: -17.420820\n",
      "ep 2044: ep_len:501 episode reward: total was -3.500000. running mean: -17.281612\n",
      "ep 2044: ep_len:56 episode reward: total was -8.680000. running mean: -17.195596\n",
      "ep 2044: ep_len:3 episode reward: total was -1.500000. running mean: -17.038640\n",
      "ep 2044: ep_len:514 episode reward: total was -45.250000. running mean: -17.320753\n",
      "ep 2044: ep_len:208 episode reward: total was -24.630000. running mean: -17.393846\n",
      "epsilon:0.109338 episode_count: 14315. steps_count: 6204543.000000\n",
      "Time elapsed:  17967.158083677292\n",
      "ep 2045: ep_len:570 episode reward: total was 55.280000. running mean: -16.667107\n",
      "ep 2045: ep_len:522 episode reward: total was -14.150000. running mean: -16.641936\n",
      "ep 2045: ep_len:527 episode reward: total was -35.760000. running mean: -16.833117\n",
      "ep 2045: ep_len:158 episode reward: total was 14.090000. running mean: -16.523886\n",
      "ep 2045: ep_len:3 episode reward: total was 1.010000. running mean: -16.348547\n",
      "ep 2045: ep_len:553 episode reward: total was -43.800000. running mean: -16.623061\n",
      "ep 2045: ep_len:557 episode reward: total was -23.240000. running mean: -16.689231\n",
      "epsilon:0.109294 episode_count: 14322. steps_count: 6207433.000000\n",
      "Time elapsed:  17977.660798311234\n",
      "ep 2046: ep_len:609 episode reward: total was -53.280000. running mean: -17.055139\n",
      "ep 2046: ep_len:541 episode reward: total was 38.940000. running mean: -16.495187\n",
      "ep 2046: ep_len:657 episode reward: total was -39.110000. running mean: -16.721335\n",
      "ep 2046: ep_len:500 episode reward: total was -25.700000. running mean: -16.811122\n",
      "ep 2046: ep_len:126 episode reward: total was 14.280000. running mean: -16.500211\n",
      "ep 2046: ep_len:580 episode reward: total was -60.010000. running mean: -16.935309\n",
      "ep 2046: ep_len:578 episode reward: total was -77.020000. running mean: -17.536156\n",
      "epsilon:0.109250 episode_count: 14329. steps_count: 6211024.000000\n",
      "Time elapsed:  17989.58250451088\n",
      "ep 2047: ep_len:206 episode reward: total was -17.910000. running mean: -17.539894\n",
      "ep 2047: ep_len:501 episode reward: total was -25.410000. running mean: -17.618595\n",
      "ep 2047: ep_len:650 episode reward: total was -147.430000. running mean: -18.916709\n",
      "ep 2047: ep_len:603 episode reward: total was 5.800000. running mean: -18.669542\n",
      "ep 2047: ep_len:3 episode reward: total was -1.500000. running mean: -18.497847\n",
      "ep 2047: ep_len:500 episode reward: total was -20.850000. running mean: -18.521368\n",
      "ep 2047: ep_len:621 episode reward: total was -6.870000. running mean: -18.404854\n",
      "epsilon:0.109205 episode_count: 14336. steps_count: 6214108.000000\n",
      "Time elapsed:  17997.788846731186\n",
      "ep 2048: ep_len:134 episode reward: total was -12.680000. running mean: -18.347606\n",
      "ep 2048: ep_len:605 episode reward: total was 30.210000. running mean: -17.862030\n",
      "ep 2048: ep_len:79 episode reward: total was -9.780000. running mean: -17.781210\n",
      "ep 2048: ep_len:132 episode reward: total was 10.540000. running mean: -17.497997\n",
      "ep 2048: ep_len:104 episode reward: total was 16.770000. running mean: -17.155317\n",
      "ep 2048: ep_len:170 episode reward: total was 27.050000. running mean: -16.713264\n",
      "ep 2048: ep_len:625 episode reward: total was 3.570000. running mean: -16.510432\n",
      "epsilon:0.109161 episode_count: 14343. steps_count: 6215957.000000\n",
      "Time elapsed:  18003.09283399582\n",
      "ep 2049: ep_len:725 episode reward: total was -95.440000. running mean: -17.299727\n",
      "ep 2049: ep_len:520 episode reward: total was 32.860000. running mean: -16.798130\n",
      "ep 2049: ep_len:53 episode reward: total was -6.310000. running mean: -16.693249\n",
      "ep 2049: ep_len:500 episode reward: total was 2.070000. running mean: -16.505616\n",
      "ep 2049: ep_len:3 episode reward: total was 0.000000. running mean: -16.340560\n",
      "ep 2049: ep_len:594 episode reward: total was -16.040000. running mean: -16.337554\n",
      "ep 2049: ep_len:179 episode reward: total was -26.450000. running mean: -16.438679\n",
      "epsilon:0.109117 episode_count: 14350. steps_count: 6218531.000000\n",
      "Time elapsed:  18009.809012413025\n",
      "ep 2050: ep_len:216 episode reward: total was 3.990000. running mean: -16.234392\n",
      "ep 2050: ep_len:522 episode reward: total was 48.980000. running mean: -15.582248\n",
      "ep 2050: ep_len:551 episode reward: total was -33.290000. running mean: -15.759326\n",
      "ep 2050: ep_len:500 episode reward: total was -18.550000. running mean: -15.787232\n",
      "ep 2050: ep_len:3 episode reward: total was 1.010000. running mean: -15.619260\n",
      "ep 2050: ep_len:500 episode reward: total was -11.220000. running mean: -15.575268\n",
      "ep 2050: ep_len:504 episode reward: total was -24.890000. running mean: -15.668415\n",
      "epsilon:0.109072 episode_count: 14357. steps_count: 6221327.000000\n",
      "Time elapsed:  18017.602956056595\n",
      "ep 2051: ep_len:500 episode reward: total was 19.410000. running mean: -15.317631\n",
      "ep 2051: ep_len:197 episode reward: total was -6.100000. running mean: -15.225454\n",
      "ep 2051: ep_len:592 episode reward: total was -112.350000. running mean: -16.196700\n",
      "ep 2051: ep_len:132 episode reward: total was 3.010000. running mean: -16.004633\n",
      "ep 2051: ep_len:106 episode reward: total was 0.750000. running mean: -15.837087\n",
      "ep 2051: ep_len:217 episode reward: total was 37.060000. running mean: -15.308116\n",
      "ep 2051: ep_len:500 episode reward: total was -46.460000. running mean: -15.619635\n",
      "epsilon:0.109028 episode_count: 14364. steps_count: 6223571.000000\n",
      "Time elapsed:  18023.057423830032\n",
      "ep 2052: ep_len:505 episode reward: total was 4.560000. running mean: -15.417838\n",
      "ep 2052: ep_len:553 episode reward: total was 64.630000. running mean: -14.617360\n",
      "ep 2052: ep_len:501 episode reward: total was -40.170000. running mean: -14.872886\n",
      "ep 2052: ep_len:45 episode reward: total was 4.230000. running mean: -14.681857\n",
      "ep 2052: ep_len:3 episode reward: total was 1.010000. running mean: -14.524939\n",
      "ep 2052: ep_len:592 episode reward: total was 1.440000. running mean: -14.365289\n",
      "ep 2052: ep_len:500 episode reward: total was -50.860000. running mean: -14.730236\n",
      "epsilon:0.108984 episode_count: 14371. steps_count: 6226270.000000\n",
      "Time elapsed:  18030.50970506668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2053: ep_len:500 episode reward: total was 48.070000. running mean: -14.102234\n",
      "ep 2053: ep_len:617 episode reward: total was 88.390000. running mean: -13.077312\n",
      "ep 2053: ep_len:550 episode reward: total was -64.160000. running mean: -13.588139\n",
      "ep 2053: ep_len:91 episode reward: total was -0.130000. running mean: -13.453557\n",
      "ep 2053: ep_len:54 episode reward: total was 15.000000. running mean: -13.169022\n",
      "ep 2053: ep_len:162 episode reward: total was 18.700000. running mean: -12.850331\n",
      "ep 2053: ep_len:317 episode reward: total was -52.670000. running mean: -13.248528\n",
      "epsilon:0.108939 episode_count: 14378. steps_count: 6228561.000000\n",
      "Time elapsed:  18036.93531680107\n",
      "ep 2054: ep_len:628 episode reward: total was -61.900000. running mean: -13.735043\n",
      "ep 2054: ep_len:569 episode reward: total was -51.090000. running mean: -14.108592\n",
      "ep 2054: ep_len:69 episode reward: total was 3.370000. running mean: -13.933807\n",
      "ep 2054: ep_len:500 episode reward: total was -65.350000. running mean: -14.447968\n",
      "ep 2054: ep_len:109 episode reward: total was 27.260000. running mean: -14.030889\n",
      "ep 2054: ep_len:501 episode reward: total was -24.020000. running mean: -14.130780\n",
      "ep 2054: ep_len:572 episode reward: total was 3.840000. running mean: -13.951072\n",
      "epsilon:0.108895 episode_count: 14385. steps_count: 6231509.000000\n",
      "Time elapsed:  18044.801980495453\n",
      "ep 2055: ep_len:500 episode reward: total was 20.850000. running mean: -13.603061\n",
      "ep 2055: ep_len:510 episode reward: total was 0.660000. running mean: -13.460431\n",
      "ep 2055: ep_len:523 episode reward: total was -54.090000. running mean: -13.866726\n",
      "ep 2055: ep_len:516 episode reward: total was -24.080000. running mean: -13.968859\n",
      "ep 2055: ep_len:61 episode reward: total was 21.110000. running mean: -13.618071\n",
      "ep 2055: ep_len:230 episode reward: total was 17.320000. running mean: -13.308690\n",
      "ep 2055: ep_len:590 episode reward: total was -17.460000. running mean: -13.350203\n",
      "epsilon:0.108851 episode_count: 14392. steps_count: 6234439.000000\n",
      "Time elapsed:  18058.094061613083\n",
      "ep 2056: ep_len:643 episode reward: total was -55.350000. running mean: -13.770201\n",
      "ep 2056: ep_len:580 episode reward: total was -18.420000. running mean: -13.816699\n",
      "ep 2056: ep_len:578 episode reward: total was 6.390000. running mean: -13.614632\n",
      "ep 2056: ep_len:511 episode reward: total was -22.790000. running mean: -13.706386\n",
      "ep 2056: ep_len:3 episode reward: total was 1.010000. running mean: -13.559222\n",
      "ep 2056: ep_len:563 episode reward: total was -26.390000. running mean: -13.687530\n",
      "ep 2056: ep_len:500 episode reward: total was -20.010000. running mean: -13.750754\n",
      "epsilon:0.108806 episode_count: 14399. steps_count: 6237817.000000\n",
      "Time elapsed:  18067.01183438301\n",
      "ep 2057: ep_len:626 episode reward: total was -18.730000. running mean: -13.800547\n",
      "ep 2057: ep_len:506 episode reward: total was -27.610000. running mean: -13.938641\n",
      "ep 2057: ep_len:570 episode reward: total was -45.700000. running mean: -14.256255\n",
      "ep 2057: ep_len:109 episode reward: total was 4.880000. running mean: -14.064892\n",
      "ep 2057: ep_len:98 episode reward: total was 5.640000. running mean: -13.867843\n",
      "ep 2057: ep_len:587 episode reward: total was -96.760000. running mean: -14.696765\n",
      "ep 2057: ep_len:591 episode reward: total was -43.460000. running mean: -14.984397\n",
      "epsilon:0.108762 episode_count: 14406. steps_count: 6240904.000000\n",
      "Time elapsed:  18075.226183652878\n",
      "ep 2058: ep_len:614 episode reward: total was -24.090000. running mean: -15.075453\n",
      "ep 2058: ep_len:501 episode reward: total was 23.890000. running mean: -14.685799\n",
      "ep 2058: ep_len:370 episode reward: total was 23.810000. running mean: -14.300841\n",
      "ep 2058: ep_len:500 episode reward: total was -51.110000. running mean: -14.668932\n",
      "ep 2058: ep_len:3 episode reward: total was 1.010000. running mean: -14.512143\n",
      "ep 2058: ep_len:531 episode reward: total was -40.800000. running mean: -14.775022\n",
      "ep 2058: ep_len:577 episode reward: total was -11.270000. running mean: -14.739971\n",
      "epsilon:0.108718 episode_count: 14413. steps_count: 6244000.000000\n",
      "Time elapsed:  18083.469165802002\n",
      "ep 2059: ep_len:536 episode reward: total was 8.070000. running mean: -14.511872\n",
      "ep 2059: ep_len:500 episode reward: total was -23.320000. running mean: -14.599953\n",
      "ep 2059: ep_len:546 episode reward: total was -40.870000. running mean: -14.862653\n",
      "ep 2059: ep_len:505 episode reward: total was -30.380000. running mean: -15.017827\n",
      "ep 2059: ep_len:101 episode reward: total was 13.650000. running mean: -14.731149\n",
      "ep 2059: ep_len:500 episode reward: total was -11.390000. running mean: -14.697737\n",
      "ep 2059: ep_len:576 episode reward: total was -4.960000. running mean: -14.600360\n",
      "epsilon:0.108673 episode_count: 14420. steps_count: 6247264.000000\n",
      "Time elapsed:  18088.851115942\n",
      "ep 2060: ep_len:545 episode reward: total was 24.510000. running mean: -14.209256\n",
      "ep 2060: ep_len:500 episode reward: total was 32.860000. running mean: -13.738564\n",
      "ep 2060: ep_len:591 episode reward: total was -40.310000. running mean: -14.004278\n",
      "ep 2060: ep_len:507 episode reward: total was -50.000000. running mean: -14.364235\n",
      "ep 2060: ep_len:3 episode reward: total was 1.010000. running mean: -14.210493\n",
      "ep 2060: ep_len:500 episode reward: total was -42.300000. running mean: -14.491388\n",
      "ep 2060: ep_len:319 episode reward: total was -22.080000. running mean: -14.567274\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.108629 episode_count: 14427. steps_count: 6250229.000000\n",
      "Time elapsed:  18103.873973608017\n",
      "ep 2061: ep_len:250 episode reward: total was -3.170000. running mean: -14.453301\n",
      "ep 2061: ep_len:298 episode reward: total was -11.400000. running mean: -14.422768\n",
      "ep 2061: ep_len:428 episode reward: total was -4.400000. running mean: -14.322541\n",
      "ep 2061: ep_len:500 episode reward: total was -42.380000. running mean: -14.603115\n",
      "ep 2061: ep_len:3 episode reward: total was 1.010000. running mean: -14.446984\n",
      "ep 2061: ep_len:636 episode reward: total was -21.670000. running mean: -14.519214\n",
      "ep 2061: ep_len:606 episode reward: total was -42.530000. running mean: -14.799322\n",
      "epsilon:0.108585 episode_count: 14434. steps_count: 6252950.000000\n",
      "Time elapsed:  18111.245387792587\n",
      "ep 2062: ep_len:572 episode reward: total was -10.110000. running mean: -14.752429\n",
      "ep 2062: ep_len:282 episode reward: total was -89.490000. running mean: -15.499805\n",
      "ep 2062: ep_len:500 episode reward: total was -2.740000. running mean: -15.372207\n",
      "ep 2062: ep_len:500 episode reward: total was 30.130000. running mean: -14.917184\n",
      "ep 2062: ep_len:3 episode reward: total was 1.010000. running mean: -14.757913\n",
      "ep 2062: ep_len:519 episode reward: total was -150.980000. running mean: -16.120134\n",
      "ep 2062: ep_len:563 episode reward: total was -10.730000. running mean: -16.066232\n",
      "epsilon:0.108540 episode_count: 14441. steps_count: 6255889.000000\n",
      "Time elapsed:  18121.59112906456\n",
      "ep 2063: ep_len:646 episode reward: total was -7.790000. running mean: -15.983470\n",
      "ep 2063: ep_len:500 episode reward: total was -69.220000. running mean: -16.515835\n",
      "ep 2063: ep_len:567 episode reward: total was -63.140000. running mean: -16.982077\n",
      "ep 2063: ep_len:500 episode reward: total was -35.170000. running mean: -17.163956\n",
      "ep 2063: ep_len:91 episode reward: total was 25.760000. running mean: -16.734716\n",
      "ep 2063: ep_len:629 episode reward: total was -29.070000. running mean: -16.858069\n",
      "ep 2063: ep_len:315 episode reward: total was -29.160000. running mean: -16.981089\n",
      "epsilon:0.108496 episode_count: 14448. steps_count: 6259137.000000\n",
      "Time elapsed:  18130.184056043625\n",
      "ep 2064: ep_len:501 episode reward: total was -47.990000. running mean: -17.291178\n",
      "ep 2064: ep_len:621 episode reward: total was 49.590000. running mean: -16.622366\n",
      "ep 2064: ep_len:407 episode reward: total was 10.630000. running mean: -16.349842\n",
      "ep 2064: ep_len:557 episode reward: total was -7.280000. running mean: -16.259144\n",
      "ep 2064: ep_len:3 episode reward: total was -1.500000. running mean: -16.111552\n",
      "ep 2064: ep_len:526 episode reward: total was -15.740000. running mean: -16.107837\n",
      "ep 2064: ep_len:500 episode reward: total was -30.830000. running mean: -16.255059\n",
      "epsilon:0.108452 episode_count: 14455. steps_count: 6262252.000000\n",
      "Time elapsed:  18138.408690929413\n",
      "ep 2065: ep_len:638 episode reward: total was -451.790000. running mean: -20.610408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2065: ep_len:525 episode reward: total was -10.030000. running mean: -20.504604\n",
      "ep 2065: ep_len:521 episode reward: total was -56.220000. running mean: -20.861758\n",
      "ep 2065: ep_len:500 episode reward: total was -33.090000. running mean: -20.984040\n",
      "ep 2065: ep_len:3 episode reward: total was 1.010000. running mean: -20.764100\n",
      "ep 2065: ep_len:503 episode reward: total was -100.840000. running mean: -21.564859\n",
      "ep 2065: ep_len:583 episode reward: total was -80.390000. running mean: -22.153110\n",
      "epsilon:0.108407 episode_count: 14462. steps_count: 6265525.000000\n",
      "Time elapsed:  18149.58187508583\n",
      "ep 2066: ep_len:527 episode reward: total was -20.630000. running mean: -22.137879\n",
      "ep 2066: ep_len:575 episode reward: total was -118.550000. running mean: -23.102000\n",
      "ep 2066: ep_len:551 episode reward: total was -41.680000. running mean: -23.287780\n",
      "ep 2066: ep_len:500 episode reward: total was -21.920000. running mean: -23.274103\n",
      "ep 2066: ep_len:50 episode reward: total was 19.000000. running mean: -22.851362\n",
      "ep 2066: ep_len:302 episode reward: total was 6.490000. running mean: -22.557948\n",
      "ep 2066: ep_len:503 episode reward: total was -25.700000. running mean: -22.589368\n",
      "epsilon:0.108363 episode_count: 14469. steps_count: 6268533.000000\n",
      "Time elapsed:  18157.90013408661\n",
      "ep 2067: ep_len:501 episode reward: total was 59.030000. running mean: -21.773175\n",
      "ep 2067: ep_len:560 episode reward: total was -35.250000. running mean: -21.907943\n",
      "ep 2067: ep_len:568 episode reward: total was -55.020000. running mean: -22.239064\n",
      "ep 2067: ep_len:581 episode reward: total was 18.070000. running mean: -21.835973\n",
      "ep 2067: ep_len:3 episode reward: total was 1.010000. running mean: -21.607513\n",
      "ep 2067: ep_len:500 episode reward: total was -19.230000. running mean: -21.583738\n",
      "ep 2067: ep_len:540 episode reward: total was -14.920000. running mean: -21.517101\n",
      "epsilon:0.108319 episode_count: 14476. steps_count: 6271786.000000\n",
      "Time elapsed:  18166.688257932663\n",
      "ep 2068: ep_len:544 episode reward: total was -82.600000. running mean: -22.127930\n",
      "ep 2068: ep_len:535 episode reward: total was -45.030000. running mean: -22.356950\n",
      "ep 2068: ep_len:632 episode reward: total was -38.360000. running mean: -22.516981\n",
      "ep 2068: ep_len:608 episode reward: total was 49.560000. running mean: -21.796211\n",
      "ep 2068: ep_len:3 episode reward: total was -1.500000. running mean: -21.593249\n",
      "ep 2068: ep_len:556 episode reward: total was -35.490000. running mean: -21.732216\n",
      "ep 2068: ep_len:598 episode reward: total was -79.060000. running mean: -22.305494\n",
      "epsilon:0.108274 episode_count: 14483. steps_count: 6275262.000000\n",
      "Time elapsed:  18181.753713607788\n",
      "ep 2069: ep_len:539 episode reward: total was -23.330000. running mean: -22.315739\n",
      "ep 2069: ep_len:511 episode reward: total was 9.270000. running mean: -21.999882\n",
      "ep 2069: ep_len:548 episode reward: total was -139.120000. running mean: -23.171083\n",
      "ep 2069: ep_len:144 episode reward: total was 7.000000. running mean: -22.869372\n",
      "ep 2069: ep_len:3 episode reward: total was 1.010000. running mean: -22.630579\n",
      "ep 2069: ep_len:688 episode reward: total was -34.050000. running mean: -22.744773\n",
      "ep 2069: ep_len:598 episode reward: total was -6.060000. running mean: -22.577925\n",
      "epsilon:0.108230 episode_count: 14490. steps_count: 6278293.000000\n",
      "Time elapsed:  18189.669857263565\n",
      "ep 2070: ep_len:619 episode reward: total was -54.820000. running mean: -22.900346\n",
      "ep 2070: ep_len:584 episode reward: total was -88.100000. running mean: -23.552342\n",
      "ep 2070: ep_len:592 episode reward: total was -41.170000. running mean: -23.728519\n",
      "ep 2070: ep_len:517 episode reward: total was -5.160000. running mean: -23.542834\n",
      "ep 2070: ep_len:3 episode reward: total was 1.010000. running mean: -23.297305\n",
      "ep 2070: ep_len:585 episode reward: total was 22.780000. running mean: -22.836532\n",
      "ep 2070: ep_len:544 episode reward: total was -117.360000. running mean: -23.781767\n",
      "epsilon:0.108186 episode_count: 14497. steps_count: 6281737.000000\n",
      "Time elapsed:  18198.65306711197\n",
      "ep 2071: ep_len:119 episode reward: total was -10.140000. running mean: -23.645349\n",
      "ep 2071: ep_len:500 episode reward: total was -17.600000. running mean: -23.584896\n",
      "ep 2071: ep_len:564 episode reward: total was 20.480000. running mean: -23.144247\n",
      "ep 2071: ep_len:589 episode reward: total was -20.300000. running mean: -23.115804\n",
      "ep 2071: ep_len:3 episode reward: total was 1.010000. running mean: -22.874546\n",
      "ep 2071: ep_len:559 episode reward: total was -61.530000. running mean: -23.261101\n",
      "ep 2071: ep_len:597 episode reward: total was -27.220000. running mean: -23.300690\n",
      "epsilon:0.108141 episode_count: 14504. steps_count: 6284668.000000\n",
      "Time elapsed:  18206.77281498909\n",
      "ep 2072: ep_len:569 episode reward: total was -71.820000. running mean: -23.785883\n",
      "ep 2072: ep_len:533 episode reward: total was 64.140000. running mean: -22.906624\n",
      "ep 2072: ep_len:422 episode reward: total was 0.680000. running mean: -22.670758\n",
      "ep 2072: ep_len:500 episode reward: total was -56.080000. running mean: -23.004850\n",
      "ep 2072: ep_len:3 episode reward: total was -1.500000. running mean: -22.789802\n",
      "ep 2072: ep_len:585 episode reward: total was -8.680000. running mean: -22.648704\n",
      "ep 2072: ep_len:311 episode reward: total was -10.110000. running mean: -22.523317\n",
      "epsilon:0.108097 episode_count: 14511. steps_count: 6287591.000000\n",
      "Time elapsed:  18219.56837081909\n",
      "ep 2073: ep_len:208 episode reward: total was 4.150000. running mean: -22.256584\n",
      "ep 2073: ep_len:548 episode reward: total was -27.970000. running mean: -22.313718\n",
      "ep 2073: ep_len:595 episode reward: total was -29.120000. running mean: -22.381781\n",
      "ep 2073: ep_len:132 episode reward: total was -5.420000. running mean: -22.212163\n",
      "ep 2073: ep_len:102 episode reward: total was -8.260000. running mean: -22.072641\n",
      "ep 2073: ep_len:500 episode reward: total was -42.430000. running mean: -22.276215\n",
      "ep 2073: ep_len:500 episode reward: total was -25.500000. running mean: -22.308453\n",
      "epsilon:0.108053 episode_count: 14518. steps_count: 6290176.000000\n",
      "Time elapsed:  18226.535093307495\n",
      "ep 2074: ep_len:208 episode reward: total was -8.340000. running mean: -22.168768\n",
      "ep 2074: ep_len:589 episode reward: total was -48.860000. running mean: -22.435680\n",
      "ep 2074: ep_len:539 episode reward: total was -13.750000. running mean: -22.348824\n",
      "ep 2074: ep_len:500 episode reward: total was -11.030000. running mean: -22.235635\n",
      "ep 2074: ep_len:3 episode reward: total was -1.500000. running mean: -22.028279\n",
      "ep 2074: ep_len:510 episode reward: total was -26.980000. running mean: -22.077796\n",
      "ep 2074: ep_len:307 episode reward: total was -21.630000. running mean: -22.073318\n",
      "epsilon:0.108008 episode_count: 14525. steps_count: 6292832.000000\n",
      "Time elapsed:  18233.621549606323\n",
      "ep 2075: ep_len:500 episode reward: total was -58.430000. running mean: -22.436885\n",
      "ep 2075: ep_len:191 episode reward: total was -5.090000. running mean: -22.263416\n",
      "ep 2075: ep_len:654 episode reward: total was -81.530000. running mean: -22.856082\n",
      "ep 2075: ep_len:529 episode reward: total was 17.290000. running mean: -22.454621\n",
      "ep 2075: ep_len:95 episode reward: total was 16.250000. running mean: -22.067575\n",
      "ep 2075: ep_len:670 episode reward: total was -13.600000. running mean: -21.982899\n",
      "ep 2075: ep_len:612 episode reward: total was -15.070000. running mean: -21.913770\n",
      "epsilon:0.107964 episode_count: 14532. steps_count: 6296083.000000\n",
      "Time elapsed:  18243.18626999855\n",
      "ep 2076: ep_len:500 episode reward: total was 32.270000. running mean: -21.371933\n",
      "ep 2076: ep_len:188 episode reward: total was -12.340000. running mean: -21.281613\n",
      "ep 2076: ep_len:565 episode reward: total was -46.740000. running mean: -21.536197\n",
      "ep 2076: ep_len:419 episode reward: total was 7.680000. running mean: -21.244035\n",
      "ep 2076: ep_len:3 episode reward: total was -1.500000. running mean: -21.046595\n",
      "ep 2076: ep_len:541 episode reward: total was -54.250000. running mean: -21.378629\n",
      "ep 2076: ep_len:625 episode reward: total was -11.910000. running mean: -21.283943\n",
      "epsilon:0.107920 episode_count: 14539. steps_count: 6298924.000000\n",
      "Time elapsed:  18250.9621322155\n",
      "ep 2077: ep_len:611 episode reward: total was -65.260000. running mean: -21.723703\n",
      "ep 2077: ep_len:192 episode reward: total was -18.700000. running mean: -21.693466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2077: ep_len:405 episode reward: total was 31.940000. running mean: -21.157131\n",
      "ep 2077: ep_len:485 episode reward: total was -25.610000. running mean: -21.201660\n",
      "ep 2077: ep_len:3 episode reward: total was 0.000000. running mean: -20.989644\n",
      "ep 2077: ep_len:516 episode reward: total was -59.970000. running mean: -21.379447\n",
      "ep 2077: ep_len:553 episode reward: total was -7.880000. running mean: -21.244453\n",
      "epsilon:0.107875 episode_count: 14546. steps_count: 6301689.000000\n",
      "Time elapsed:  18258.419271707535\n",
      "ep 2078: ep_len:237 episode reward: total was -10.880000. running mean: -21.140808\n",
      "ep 2078: ep_len:598 episode reward: total was -2.280000. running mean: -20.952200\n",
      "ep 2078: ep_len:641 episode reward: total was -62.900000. running mean: -21.371678\n",
      "ep 2078: ep_len:504 episode reward: total was 28.810000. running mean: -20.869861\n",
      "ep 2078: ep_len:3 episode reward: total was -3.000000. running mean: -20.691163\n",
      "ep 2078: ep_len:534 episode reward: total was -47.090000. running mean: -20.955151\n",
      "ep 2078: ep_len:561 episode reward: total was -22.310000. running mean: -20.968700\n",
      "epsilon:0.107831 episode_count: 14553. steps_count: 6304767.000000\n",
      "Time elapsed:  18269.130957603455\n",
      "ep 2079: ep_len:562 episode reward: total was -56.120000. running mean: -21.320213\n",
      "ep 2079: ep_len:542 episode reward: total was -51.970000. running mean: -21.626710\n",
      "ep 2079: ep_len:336 episode reward: total was 4.460000. running mean: -21.365843\n",
      "ep 2079: ep_len:583 episode reward: total was 8.290000. running mean: -21.069285\n",
      "ep 2079: ep_len:43 episode reward: total was 17.000000. running mean: -20.688592\n",
      "ep 2079: ep_len:511 episode reward: total was -47.710000. running mean: -20.958806\n",
      "ep 2079: ep_len:590 episode reward: total was -36.680000. running mean: -21.116018\n",
      "epsilon:0.107787 episode_count: 14560. steps_count: 6307934.000000\n",
      "Time elapsed:  18277.6618309021\n",
      "ep 2080: ep_len:237 episode reward: total was 5.130000. running mean: -20.853558\n",
      "ep 2080: ep_len:500 episode reward: total was 10.830000. running mean: -20.536722\n",
      "ep 2080: ep_len:500 episode reward: total was -3.940000. running mean: -20.370755\n",
      "ep 2080: ep_len:500 episode reward: total was 12.000000. running mean: -20.047047\n",
      "ep 2080: ep_len:3 episode reward: total was 1.010000. running mean: -19.836477\n",
      "ep 2080: ep_len:565 episode reward: total was -34.510000. running mean: -19.983212\n",
      "ep 2080: ep_len:501 episode reward: total was -25.350000. running mean: -20.036880\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.107742 episode_count: 14567. steps_count: 6310740.000000\n",
      "Time elapsed:  18289.349615335464\n",
      "ep 2081: ep_len:195 episode reward: total was -3.050000. running mean: -19.867011\n",
      "ep 2081: ep_len:540 episode reward: total was -1.230000. running mean: -19.680641\n",
      "ep 2081: ep_len:581 episode reward: total was -69.430000. running mean: -20.178135\n",
      "ep 2081: ep_len:119 episode reward: total was -4.170000. running mean: -20.018053\n",
      "ep 2081: ep_len:3 episode reward: total was 1.010000. running mean: -19.807773\n",
      "ep 2081: ep_len:518 episode reward: total was -18.100000. running mean: -19.790695\n",
      "ep 2081: ep_len:623 episode reward: total was -20.380000. running mean: -19.796588\n",
      "epsilon:0.107698 episode_count: 14574. steps_count: 6313319.000000\n",
      "Time elapsed:  18296.555480718613\n",
      "ep 2082: ep_len:571 episode reward: total was -50.190000. running mean: -20.100522\n",
      "ep 2082: ep_len:288 episode reward: total was -72.760000. running mean: -20.627117\n",
      "ep 2082: ep_len:606 episode reward: total was -39.790000. running mean: -20.818746\n",
      "ep 2082: ep_len:569 episode reward: total was -20.880000. running mean: -20.819359\n",
      "ep 2082: ep_len:3 episode reward: total was 0.000000. running mean: -20.611165\n",
      "ep 2082: ep_len:597 episode reward: total was -30.060000. running mean: -20.705653\n",
      "ep 2082: ep_len:515 episode reward: total was -37.740000. running mean: -20.875997\n",
      "epsilon:0.107654 episode_count: 14581. steps_count: 6316468.000000\n",
      "Time elapsed:  18304.888907194138\n",
      "ep 2083: ep_len:128 episode reward: total was 4.370000. running mean: -20.623537\n",
      "ep 2083: ep_len:500 episode reward: total was -76.330000. running mean: -21.180601\n",
      "ep 2083: ep_len:626 episode reward: total was -22.150000. running mean: -21.190295\n",
      "ep 2083: ep_len:522 episode reward: total was 39.550000. running mean: -20.582892\n",
      "ep 2083: ep_len:3 episode reward: total was 1.010000. running mean: -20.366964\n",
      "ep 2083: ep_len:500 episode reward: total was -55.740000. running mean: -20.720694\n",
      "ep 2083: ep_len:352 episode reward: total was -3.530000. running mean: -20.548787\n",
      "epsilon:0.107609 episode_count: 14588. steps_count: 6319099.000000\n",
      "Time elapsed:  18313.756094694138\n",
      "ep 2084: ep_len:225 episode reward: total was 4.230000. running mean: -20.300999\n",
      "ep 2084: ep_len:566 episode reward: total was -73.490000. running mean: -20.832889\n",
      "ep 2084: ep_len:628 episode reward: total was -76.650000. running mean: -21.391060\n",
      "ep 2084: ep_len:605 episode reward: total was 35.710000. running mean: -20.820050\n",
      "ep 2084: ep_len:3 episode reward: total was 1.010000. running mean: -20.601749\n",
      "ep 2084: ep_len:660 episode reward: total was -36.690000. running mean: -20.762632\n",
      "ep 2084: ep_len:550 episode reward: total was -0.190000. running mean: -20.556905\n",
      "epsilon:0.107565 episode_count: 14595. steps_count: 6322336.000000\n",
      "Time elapsed:  18322.148066997528\n",
      "ep 2085: ep_len:127 episode reward: total was -24.750000. running mean: -20.598836\n",
      "ep 2085: ep_len:500 episode reward: total was -14.630000. running mean: -20.539148\n",
      "ep 2085: ep_len:359 episode reward: total was 10.140000. running mean: -20.232356\n",
      "ep 2085: ep_len:151 episode reward: total was -0.820000. running mean: -20.038233\n",
      "ep 2085: ep_len:99 episode reward: total was 24.770000. running mean: -19.590150\n",
      "ep 2085: ep_len:501 episode reward: total was -24.410000. running mean: -19.638349\n",
      "ep 2085: ep_len:579 episode reward: total was -5.050000. running mean: -19.492465\n",
      "epsilon:0.107521 episode_count: 14602. steps_count: 6324652.000000\n",
      "Time elapsed:  18328.51350736618\n",
      "ep 2086: ep_len:670 episode reward: total was -28.880000. running mean: -19.586341\n",
      "ep 2086: ep_len:529 episode reward: total was -1.980000. running mean: -19.410277\n",
      "ep 2086: ep_len:611 episode reward: total was -13.990000. running mean: -19.356075\n",
      "ep 2086: ep_len:500 episode reward: total was 9.870000. running mean: -19.063814\n",
      "ep 2086: ep_len:50 episode reward: total was 21.510000. running mean: -18.658076\n",
      "ep 2086: ep_len:504 episode reward: total was -31.780000. running mean: -18.789295\n",
      "ep 2086: ep_len:553 episode reward: total was -21.650000. running mean: -18.817902\n",
      "epsilon:0.107476 episode_count: 14609. steps_count: 6328069.000000\n",
      "Time elapsed:  18337.417635917664\n",
      "ep 2087: ep_len:134 episode reward: total was -0.500000. running mean: -18.634723\n",
      "ep 2087: ep_len:603 episode reward: total was -40.060000. running mean: -18.848976\n",
      "ep 2087: ep_len:581 episode reward: total was -45.480000. running mean: -19.115286\n",
      "ep 2087: ep_len:599 episode reward: total was 35.150000. running mean: -18.572633\n",
      "ep 2087: ep_len:1 episode reward: total was -1.000000. running mean: -18.396907\n",
      "ep 2087: ep_len:501 episode reward: total was -30.130000. running mean: -18.514238\n",
      "ep 2087: ep_len:593 episode reward: total was -38.290000. running mean: -18.711995\n",
      "epsilon:0.107432 episode_count: 14616. steps_count: 6331081.000000\n",
      "Time elapsed:  18349.088039159775\n",
      "ep 2088: ep_len:643 episode reward: total was -132.760000. running mean: -19.852475\n",
      "ep 2088: ep_len:315 episode reward: total was -38.800000. running mean: -20.041951\n",
      "ep 2088: ep_len:526 episode reward: total was -51.880000. running mean: -20.360331\n",
      "ep 2088: ep_len:93 episode reward: total was -3.690000. running mean: -20.193628\n",
      "ep 2088: ep_len:90 episode reward: total was -45.220000. running mean: -20.443892\n",
      "ep 2088: ep_len:661 episode reward: total was -32.880000. running mean: -20.568253\n",
      "ep 2088: ep_len:525 episode reward: total was -6.550000. running mean: -20.428070\n",
      "epsilon:0.107388 episode_count: 14623. steps_count: 6333934.000000\n",
      "Time elapsed:  18354.58779001236\n",
      "ep 2089: ep_len:239 episode reward: total was -16.800000. running mean: -20.391789\n",
      "ep 2089: ep_len:631 episode reward: total was 25.300000. running mean: -19.934872\n",
      "ep 2089: ep_len:595 episode reward: total was -55.220000. running mean: -20.287723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2089: ep_len:536 episode reward: total was 38.370000. running mean: -19.701146\n",
      "ep 2089: ep_len:3 episode reward: total was -0.490000. running mean: -19.509034\n",
      "ep 2089: ep_len:245 episode reward: total was 3.850000. running mean: -19.275444\n",
      "ep 2089: ep_len:583 episode reward: total was -8.760000. running mean: -19.170289\n",
      "epsilon:0.107343 episode_count: 14630. steps_count: 6336766.000000\n",
      "Time elapsed:  18364.244522094727\n",
      "ep 2090: ep_len:568 episode reward: total was -52.630000. running mean: -19.504886\n",
      "ep 2090: ep_len:328 episode reward: total was -12.190000. running mean: -19.431738\n",
      "ep 2090: ep_len:533 episode reward: total was -60.360000. running mean: -19.841020\n",
      "ep 2090: ep_len:500 episode reward: total was 37.740000. running mean: -19.265210\n",
      "ep 2090: ep_len:3 episode reward: total was 1.010000. running mean: -19.062458\n",
      "ep 2090: ep_len:557 episode reward: total was -37.600000. running mean: -19.247833\n",
      "ep 2090: ep_len:509 episode reward: total was -55.000000. running mean: -19.605355\n",
      "epsilon:0.107299 episode_count: 14637. steps_count: 6339764.000000\n",
      "Time elapsed:  18372.146145105362\n",
      "ep 2091: ep_len:574 episode reward: total was -149.330000. running mean: -20.902601\n",
      "ep 2091: ep_len:521 episode reward: total was -69.990000. running mean: -21.393475\n",
      "ep 2091: ep_len:540 episode reward: total was -44.070000. running mean: -21.620241\n",
      "ep 2091: ep_len:515 episode reward: total was -14.090000. running mean: -21.544938\n",
      "ep 2091: ep_len:3 episode reward: total was 1.010000. running mean: -21.319389\n",
      "ep 2091: ep_len:535 episode reward: total was -26.420000. running mean: -21.370395\n",
      "ep 2091: ep_len:333 episode reward: total was -21.170000. running mean: -21.368391\n",
      "epsilon:0.107255 episode_count: 14644. steps_count: 6342785.000000\n",
      "Time elapsed:  18383.35797739029\n",
      "ep 2092: ep_len:562 episode reward: total was -33.200000. running mean: -21.486707\n",
      "ep 2092: ep_len:553 episode reward: total was -42.150000. running mean: -21.693340\n",
      "ep 2092: ep_len:556 episode reward: total was 7.980000. running mean: -21.396607\n",
      "ep 2092: ep_len:557 episode reward: total was -22.130000. running mean: -21.403941\n",
      "ep 2092: ep_len:102 episode reward: total was 28.750000. running mean: -20.902401\n",
      "ep 2092: ep_len:182 episode reward: total was 13.120000. running mean: -20.562177\n",
      "ep 2092: ep_len:546 episode reward: total was -24.450000. running mean: -20.601055\n",
      "epsilon:0.107210 episode_count: 14651. steps_count: 6345843.000000\n",
      "Time elapsed:  18390.695280075073\n",
      "ep 2093: ep_len:643 episode reward: total was -0.100000. running mean: -20.396045\n",
      "ep 2093: ep_len:500 episode reward: total was -37.480000. running mean: -20.566884\n",
      "ep 2093: ep_len:682 episode reward: total was -77.120000. running mean: -21.132416\n",
      "ep 2093: ep_len:501 episode reward: total was -6.790000. running mean: -20.988991\n",
      "ep 2093: ep_len:3 episode reward: total was 1.010000. running mean: -20.769002\n",
      "ep 2093: ep_len:592 episode reward: total was 22.270000. running mean: -20.338612\n",
      "ep 2093: ep_len:500 episode reward: total was -22.340000. running mean: -20.358625\n",
      "epsilon:0.107166 episode_count: 14658. steps_count: 6349264.000000\n",
      "Time elapsed:  18398.335895061493\n",
      "ep 2094: ep_len:594 episode reward: total was 24.760000. running mean: -19.907439\n",
      "ep 2094: ep_len:501 episode reward: total was 8.100000. running mean: -19.627365\n",
      "ep 2094: ep_len:623 episode reward: total was -51.140000. running mean: -19.942491\n",
      "ep 2094: ep_len:501 episode reward: total was 29.280000. running mean: -19.450266\n",
      "ep 2094: ep_len:3 episode reward: total was 1.010000. running mean: -19.245664\n",
      "ep 2094: ep_len:500 episode reward: total was -39.880000. running mean: -19.452007\n",
      "ep 2094: ep_len:574 episode reward: total was -12.480000. running mean: -19.382287\n",
      "epsilon:0.107122 episode_count: 14665. steps_count: 6352560.000000\n",
      "Time elapsed:  18404.85188627243\n",
      "ep 2095: ep_len:500 episode reward: total was 10.430000. running mean: -19.084164\n",
      "ep 2095: ep_len:500 episode reward: total was -14.610000. running mean: -19.039422\n",
      "ep 2095: ep_len:556 episode reward: total was 15.040000. running mean: -18.698628\n",
      "ep 2095: ep_len:145 episode reward: total was 18.460000. running mean: -18.327042\n",
      "ep 2095: ep_len:56 episode reward: total was 19.000000. running mean: -17.953771\n",
      "ep 2095: ep_len:500 episode reward: total was -17.440000. running mean: -17.948634\n",
      "ep 2095: ep_len:546 episode reward: total was -27.960000. running mean: -18.048747\n",
      "epsilon:0.107077 episode_count: 14672. steps_count: 6355363.000000\n",
      "Time elapsed:  18412.45449614525\n",
      "ep 2096: ep_len:575 episode reward: total was -15.800000. running mean: -18.026260\n",
      "ep 2096: ep_len:500 episode reward: total was -26.370000. running mean: -18.109697\n",
      "ep 2096: ep_len:556 episode reward: total was -22.730000. running mean: -18.155900\n",
      "ep 2096: ep_len:529 episode reward: total was 20.460000. running mean: -17.769741\n",
      "ep 2096: ep_len:3 episode reward: total was 1.010000. running mean: -17.581944\n",
      "ep 2096: ep_len:526 episode reward: total was -40.500000. running mean: -17.811124\n",
      "ep 2096: ep_len:500 episode reward: total was -7.050000. running mean: -17.703513\n",
      "epsilon:0.107033 episode_count: 14679. steps_count: 6358552.000000\n",
      "Time elapsed:  18420.913219451904\n",
      "ep 2097: ep_len:216 episode reward: total was -24.320000. running mean: -17.769678\n",
      "ep 2097: ep_len:504 episode reward: total was -81.370000. running mean: -18.405681\n",
      "ep 2097: ep_len:574 episode reward: total was -50.890000. running mean: -18.730524\n",
      "ep 2097: ep_len:501 episode reward: total was -38.280000. running mean: -18.926019\n",
      "ep 2097: ep_len:84 episode reward: total was 8.210000. running mean: -18.654659\n",
      "ep 2097: ep_len:594 episode reward: total was -55.100000. running mean: -19.019112\n",
      "ep 2097: ep_len:307 episode reward: total was -63.190000. running mean: -19.460821\n",
      "epsilon:0.106989 episode_count: 14686. steps_count: 6361332.000000\n",
      "Time elapsed:  18428.40155339241\n",
      "ep 2098: ep_len:623 episode reward: total was -104.770000. running mean: -20.313913\n",
      "ep 2098: ep_len:500 episode reward: total was -63.820000. running mean: -20.748974\n",
      "ep 2098: ep_len:500 episode reward: total was -32.510000. running mean: -20.866584\n",
      "ep 2098: ep_len:530 episode reward: total was 26.440000. running mean: -20.393518\n",
      "ep 2098: ep_len:3 episode reward: total was -1.500000. running mean: -20.204583\n",
      "ep 2098: ep_len:500 episode reward: total was -52.280000. running mean: -20.525337\n",
      "ep 2098: ep_len:296 episode reward: total was -22.520000. running mean: -20.545284\n",
      "epsilon:0.106944 episode_count: 14693. steps_count: 6364284.000000\n",
      "Time elapsed:  18438.34642791748\n",
      "ep 2099: ep_len:505 episode reward: total was 10.820000. running mean: -20.231631\n",
      "ep 2099: ep_len:566 episode reward: total was -50.010000. running mean: -20.529415\n",
      "ep 2099: ep_len:587 episode reward: total was -62.770000. running mean: -20.951821\n",
      "ep 2099: ep_len:574 episode reward: total was 32.440000. running mean: -20.417903\n",
      "ep 2099: ep_len:53 episode reward: total was 18.510000. running mean: -20.028623\n",
      "ep 2099: ep_len:654 episode reward: total was -25.920000. running mean: -20.087537\n",
      "ep 2099: ep_len:512 episode reward: total was -20.260000. running mean: -20.089262\n",
      "epsilon:0.106900 episode_count: 14700. steps_count: 6367735.000000\n",
      "Time elapsed:  18446.994570970535\n",
      "ep 2100: ep_len:636 episode reward: total was -70.570000. running mean: -20.594069\n",
      "ep 2100: ep_len:324 episode reward: total was -11.300000. running mean: -20.501129\n",
      "ep 2100: ep_len:440 episode reward: total was 6.620000. running mean: -20.229917\n",
      "ep 2100: ep_len:513 episode reward: total was 14.360000. running mean: -19.884018\n",
      "ep 2100: ep_len:85 episode reward: total was -13.820000. running mean: -19.823378\n",
      "ep 2100: ep_len:515 episode reward: total was 13.590000. running mean: -19.489244\n",
      "ep 2100: ep_len:511 episode reward: total was -54.490000. running mean: -19.839252\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.106856 episode_count: 14707. steps_count: 6370759.000000\n",
      "Time elapsed:  18459.05911707878\n",
      "ep 2101: ep_len:564 episode reward: total was -98.530000. running mean: -20.626159\n",
      "ep 2101: ep_len:646 episode reward: total was 31.700000. running mean: -20.102898\n",
      "ep 2101: ep_len:578 episode reward: total was -0.520000. running mean: -19.907069\n",
      "ep 2101: ep_len:566 episode reward: total was 21.060000. running mean: -19.497398\n",
      "ep 2101: ep_len:3 episode reward: total was 1.010000. running mean: -19.292324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2101: ep_len:565 episode reward: total was -35.890000. running mean: -19.458301\n",
      "ep 2101: ep_len:624 episode reward: total was -14.460000. running mean: -19.408318\n",
      "epsilon:0.106811 episode_count: 14714. steps_count: 6374305.000000\n",
      "Time elapsed:  18468.013328552246\n",
      "ep 2102: ep_len:520 episode reward: total was -54.800000. running mean: -19.762235\n",
      "ep 2102: ep_len:500 episode reward: total was 16.490000. running mean: -19.399712\n",
      "ep 2102: ep_len:390 episode reward: total was 32.460000. running mean: -18.881115\n",
      "ep 2102: ep_len:378 episode reward: total was -19.810000. running mean: -18.890404\n",
      "ep 2102: ep_len:3 episode reward: total was 1.010000. running mean: -18.691400\n",
      "ep 2102: ep_len:513 episode reward: total was -15.600000. running mean: -18.660486\n",
      "ep 2102: ep_len:517 episode reward: total was -23.140000. running mean: -18.705281\n",
      "epsilon:0.106767 episode_count: 14721. steps_count: 6377126.000000\n",
      "Time elapsed:  18475.66782617569\n",
      "ep 2103: ep_len:500 episode reward: total was 31.050000. running mean: -18.207728\n",
      "ep 2103: ep_len:662 episode reward: total was -120.170000. running mean: -19.227351\n",
      "ep 2103: ep_len:676 episode reward: total was -32.640000. running mean: -19.361477\n",
      "ep 2103: ep_len:517 episode reward: total was -25.940000. running mean: -19.427263\n",
      "ep 2103: ep_len:2 episode reward: total was -0.500000. running mean: -19.237990\n",
      "ep 2103: ep_len:501 episode reward: total was -16.670000. running mean: -19.212310\n",
      "ep 2103: ep_len:293 episode reward: total was -29.730000. running mean: -19.317487\n",
      "epsilon:0.106723 episode_count: 14728. steps_count: 6380277.000000\n",
      "Time elapsed:  18487.40506339073\n",
      "ep 2104: ep_len:646 episode reward: total was -104.330000. running mean: -20.167612\n",
      "ep 2104: ep_len:500 episode reward: total was -10.020000. running mean: -20.066136\n",
      "ep 2104: ep_len:533 episode reward: total was -74.870000. running mean: -20.614175\n",
      "ep 2104: ep_len:515 episode reward: total was 36.420000. running mean: -20.043833\n",
      "ep 2104: ep_len:92 episode reward: total was 10.130000. running mean: -19.742095\n",
      "ep 2104: ep_len:631 episode reward: total was -0.010000. running mean: -19.544774\n",
      "ep 2104: ep_len:500 episode reward: total was -19.090000. running mean: -19.540226\n",
      "epsilon:0.106678 episode_count: 14735. steps_count: 6383694.000000\n",
      "Time elapsed:  18499.664301872253\n",
      "ep 2105: ep_len:624 episode reward: total was -41.740000. running mean: -19.762224\n",
      "ep 2105: ep_len:500 episode reward: total was -128.590000. running mean: -20.850501\n",
      "ep 2105: ep_len:604 episode reward: total was -8.710000. running mean: -20.729096\n",
      "ep 2105: ep_len:515 episode reward: total was -26.760000. running mean: -20.789405\n",
      "ep 2105: ep_len:92 episode reward: total was 22.710000. running mean: -20.354411\n",
      "ep 2105: ep_len:559 episode reward: total was -23.530000. running mean: -20.386167\n",
      "ep 2105: ep_len:506 episode reward: total was -31.490000. running mean: -20.497206\n",
      "epsilon:0.106634 episode_count: 14742. steps_count: 6387094.000000\n",
      "Time elapsed:  18512.17206311226\n",
      "ep 2106: ep_len:500 episode reward: total was 62.390000. running mean: -19.668334\n",
      "ep 2106: ep_len:517 episode reward: total was 2.900000. running mean: -19.442650\n",
      "ep 2106: ep_len:79 episode reward: total was 4.760000. running mean: -19.200624\n",
      "ep 2106: ep_len:571 episode reward: total was 17.960000. running mean: -18.829017\n",
      "ep 2106: ep_len:105 episode reward: total was 12.710000. running mean: -18.513627\n",
      "ep 2106: ep_len:535 episode reward: total was -48.830000. running mean: -18.816791\n",
      "ep 2106: ep_len:596 episode reward: total was -28.600000. running mean: -18.914623\n",
      "epsilon:0.106590 episode_count: 14749. steps_count: 6389997.000000\n",
      "Time elapsed:  18519.907245874405\n",
      "ep 2107: ep_len:86 episode reward: total was 5.810000. running mean: -18.667377\n",
      "ep 2107: ep_len:540 episode reward: total was -12.280000. running mean: -18.603503\n",
      "ep 2107: ep_len:522 episode reward: total was -40.700000. running mean: -18.824468\n",
      "ep 2107: ep_len:500 episode reward: total was -32.600000. running mean: -18.962223\n",
      "ep 2107: ep_len:3 episode reward: total was 1.010000. running mean: -18.762501\n",
      "ep 2107: ep_len:641 episode reward: total was -0.320000. running mean: -18.578076\n",
      "ep 2107: ep_len:509 episode reward: total was -27.820000. running mean: -18.670495\n",
      "epsilon:0.106545 episode_count: 14756. steps_count: 6392798.000000\n",
      "Time elapsed:  18527.252088069916\n",
      "ep 2108: ep_len:555 episode reward: total was 2.030000. running mean: -18.463490\n",
      "ep 2108: ep_len:543 episode reward: total was -8.780000. running mean: -18.366656\n",
      "ep 2108: ep_len:660 episode reward: total was -42.200000. running mean: -18.604989\n",
      "ep 2108: ep_len:590 episode reward: total was -0.540000. running mean: -18.424339\n",
      "ep 2108: ep_len:2 episode reward: total was -2.000000. running mean: -18.260096\n",
      "ep 2108: ep_len:571 episode reward: total was -8.560000. running mean: -18.163095\n",
      "ep 2108: ep_len:292 episode reward: total was -24.750000. running mean: -18.228964\n",
      "epsilon:0.106501 episode_count: 14763. steps_count: 6396011.000000\n",
      "Time elapsed:  18535.827978610992\n",
      "ep 2109: ep_len:126 episode reward: total was -8.020000. running mean: -18.126874\n",
      "ep 2109: ep_len:343 episode reward: total was -9.440000. running mean: -18.040005\n",
      "ep 2109: ep_len:630 episode reward: total was -84.070000. running mean: -18.700305\n",
      "ep 2109: ep_len:500 episode reward: total was -19.930000. running mean: -18.712602\n",
      "ep 2109: ep_len:104 episode reward: total was 26.750000. running mean: -18.257976\n",
      "ep 2109: ep_len:605 episode reward: total was -11.360000. running mean: -18.188997\n",
      "ep 2109: ep_len:544 episode reward: total was -71.000000. running mean: -18.717107\n",
      "epsilon:0.106457 episode_count: 14770. steps_count: 6398863.000000\n",
      "Time elapsed:  18543.476004362106\n",
      "ep 2110: ep_len:245 episode reward: total was -1.280000. running mean: -18.542735\n",
      "ep 2110: ep_len:583 episode reward: total was -52.940000. running mean: -18.886708\n",
      "ep 2110: ep_len:79 episode reward: total was 6.840000. running mean: -18.629441\n",
      "ep 2110: ep_len:170 episode reward: total was 3.160000. running mean: -18.411547\n",
      "ep 2110: ep_len:3 episode reward: total was 1.010000. running mean: -18.217331\n",
      "ep 2110: ep_len:519 episode reward: total was -42.970000. running mean: -18.464858\n",
      "ep 2110: ep_len:611 episode reward: total was -34.460000. running mean: -18.624809\n",
      "epsilon:0.106412 episode_count: 14777. steps_count: 6401073.000000\n",
      "Time elapsed:  18549.75572371483\n",
      "ep 2111: ep_len:500 episode reward: total was 54.710000. running mean: -17.891461\n",
      "ep 2111: ep_len:543 episode reward: total was 53.540000. running mean: -17.177147\n",
      "ep 2111: ep_len:349 episode reward: total was 30.820000. running mean: -16.697175\n",
      "ep 2111: ep_len:510 episode reward: total was -27.510000. running mean: -16.805303\n",
      "ep 2111: ep_len:53 episode reward: total was 17.010000. running mean: -16.467150\n",
      "ep 2111: ep_len:171 episode reward: total was 23.140000. running mean: -16.071079\n",
      "ep 2111: ep_len:500 episode reward: total was -47.330000. running mean: -16.383668\n",
      "epsilon:0.106368 episode_count: 14784. steps_count: 6403699.000000\n",
      "Time elapsed:  18557.793751001358\n",
      "ep 2112: ep_len:531 episode reward: total was -101.180000. running mean: -17.231631\n",
      "ep 2112: ep_len:544 episode reward: total was -23.950000. running mean: -17.298815\n",
      "ep 2112: ep_len:544 episode reward: total was -31.940000. running mean: -17.445227\n",
      "ep 2112: ep_len:42 episode reward: total was -7.290000. running mean: -17.343675\n",
      "ep 2112: ep_len:3 episode reward: total was 1.010000. running mean: -17.160138\n",
      "ep 2112: ep_len:330 episode reward: total was 13.140000. running mean: -16.857136\n",
      "ep 2112: ep_len:563 episode reward: total was -2.540000. running mean: -16.713965\n",
      "epsilon:0.106324 episode_count: 14791. steps_count: 6406256.000000\n",
      "Time elapsed:  18564.85511636734\n",
      "ep 2113: ep_len:529 episode reward: total was -18.510000. running mean: -16.731925\n",
      "ep 2113: ep_len:505 episode reward: total was -6.280000. running mean: -16.627406\n",
      "ep 2113: ep_len:644 episode reward: total was -46.090000. running mean: -16.922032\n",
      "ep 2113: ep_len:500 episode reward: total was -45.080000. running mean: -17.203612\n",
      "ep 2113: ep_len:3 episode reward: total was 1.010000. running mean: -17.021476\n",
      "ep 2113: ep_len:501 episode reward: total was -148.520000. running mean: -18.336461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2113: ep_len:541 episode reward: total was -7.620000. running mean: -18.229296\n",
      "epsilon:0.106279 episode_count: 14798. steps_count: 6409479.000000\n",
      "Time elapsed:  18573.254725933075\n",
      "ep 2114: ep_len:595 episode reward: total was -177.600000. running mean: -19.823003\n",
      "ep 2114: ep_len:500 episode reward: total was 41.300000. running mean: -19.211773\n",
      "ep 2114: ep_len:500 episode reward: total was -47.240000. running mean: -19.492056\n",
      "ep 2114: ep_len:500 episode reward: total was -6.620000. running mean: -19.363335\n",
      "ep 2114: ep_len:3 episode reward: total was 1.010000. running mean: -19.159602\n",
      "ep 2114: ep_len:502 episode reward: total was 1.920000. running mean: -18.948806\n",
      "ep 2114: ep_len:501 episode reward: total was 1.050000. running mean: -18.748818\n",
      "epsilon:0.106235 episode_count: 14805. steps_count: 6412580.000000\n",
      "Time elapsed:  18581.46298980713\n",
      "ep 2115: ep_len:658 episode reward: total was 4.300000. running mean: -18.518329\n",
      "ep 2115: ep_len:500 episode reward: total was 15.430000. running mean: -18.178846\n",
      "ep 2115: ep_len:543 episode reward: total was -34.930000. running mean: -18.346358\n",
      "ep 2115: ep_len:547 episode reward: total was 19.070000. running mean: -17.972194\n",
      "ep 2115: ep_len:3 episode reward: total was 1.010000. running mean: -17.782372\n",
      "ep 2115: ep_len:554 episode reward: total was -27.950000. running mean: -17.884048\n",
      "ep 2115: ep_len:549 episode reward: total was -129.010000. running mean: -18.995308\n",
      "epsilon:0.106191 episode_count: 14812. steps_count: 6415934.000000\n",
      "Time elapsed:  18590.217046499252\n",
      "ep 2116: ep_len:253 episode reward: total was 14.810000. running mean: -18.657255\n",
      "ep 2116: ep_len:367 episode reward: total was -136.540000. running mean: -19.836082\n",
      "ep 2116: ep_len:575 episode reward: total was -44.020000. running mean: -20.077922\n",
      "ep 2116: ep_len:613 episode reward: total was 19.720000. running mean: -19.679942\n",
      "ep 2116: ep_len:126 episode reward: total was 1.300000. running mean: -19.470143\n",
      "ep 2116: ep_len:500 episode reward: total was -16.920000. running mean: -19.444641\n",
      "ep 2116: ep_len:501 episode reward: total was -36.750000. running mean: -19.617695\n",
      "epsilon:0.106146 episode_count: 14819. steps_count: 6418869.000000\n",
      "Time elapsed:  18597.775272130966\n",
      "ep 2117: ep_len:500 episode reward: total was 41.180000. running mean: -19.009718\n",
      "ep 2117: ep_len:500 episode reward: total was 44.150000. running mean: -18.378121\n",
      "ep 2117: ep_len:500 episode reward: total was -22.830000. running mean: -18.422640\n",
      "ep 2117: ep_len:536 episode reward: total was 11.550000. running mean: -18.122913\n",
      "ep 2117: ep_len:3 episode reward: total was 0.000000. running mean: -17.941684\n",
      "ep 2117: ep_len:598 episode reward: total was -9.360000. running mean: -17.855867\n",
      "ep 2117: ep_len:560 episode reward: total was -3.350000. running mean: -17.710809\n",
      "epsilon:0.106102 episode_count: 14826. steps_count: 6422066.000000\n",
      "Time elapsed:  18611.873251914978\n",
      "ep 2118: ep_len:500 episode reward: total was 41.300000. running mean: -17.120701\n",
      "ep 2118: ep_len:504 episode reward: total was -73.170000. running mean: -17.681194\n",
      "ep 2118: ep_len:590 episode reward: total was -49.950000. running mean: -18.003882\n",
      "ep 2118: ep_len:576 episode reward: total was -24.960000. running mean: -18.073443\n",
      "ep 2118: ep_len:94 episode reward: total was -9.160000. running mean: -17.984308\n",
      "ep 2118: ep_len:518 episode reward: total was -13.720000. running mean: -17.941665\n",
      "ep 2118: ep_len:544 episode reward: total was 13.300000. running mean: -17.629249\n",
      "epsilon:0.106058 episode_count: 14833. steps_count: 6425392.000000\n",
      "Time elapsed:  18620.730726718903\n",
      "ep 2119: ep_len:501 episode reward: total was 33.940000. running mean: -17.113556\n",
      "ep 2119: ep_len:512 episode reward: total was -39.430000. running mean: -17.336721\n",
      "ep 2119: ep_len:79 episode reward: total was 8.310000. running mean: -17.080253\n",
      "ep 2119: ep_len:517 episode reward: total was -29.690000. running mean: -17.206351\n",
      "ep 2119: ep_len:3 episode reward: total was 1.010000. running mean: -17.024187\n",
      "ep 2119: ep_len:546 episode reward: total was -91.810000. running mean: -17.772045\n",
      "ep 2119: ep_len:583 episode reward: total was -23.400000. running mean: -17.828325\n",
      "epsilon:0.106013 episode_count: 14840. steps_count: 6428133.000000\n",
      "Time elapsed:  18628.31518125534\n",
      "ep 2120: ep_len:520 episode reward: total was -117.480000. running mean: -18.824842\n",
      "ep 2120: ep_len:262 episode reward: total was -31.140000. running mean: -18.947993\n",
      "ep 2120: ep_len:685 episode reward: total was -64.600000. running mean: -19.404513\n",
      "ep 2120: ep_len:579 episode reward: total was 11.610000. running mean: -19.094368\n",
      "ep 2120: ep_len:3 episode reward: total was 1.010000. running mean: -18.893325\n",
      "ep 2120: ep_len:513 episode reward: total was -19.400000. running mean: -18.898391\n",
      "ep 2120: ep_len:324 episode reward: total was -15.910000. running mean: -18.868507\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.105969 episode_count: 14847. steps_count: 6431019.000000\n",
      "Time elapsed:  18640.893798589706\n",
      "ep 2121: ep_len:500 episode reward: total was 21.620000. running mean: -18.463622\n",
      "ep 2121: ep_len:500 episode reward: total was 58.860000. running mean: -17.690386\n",
      "ep 2121: ep_len:571 episode reward: total was -32.250000. running mean: -17.835982\n",
      "ep 2121: ep_len:626 episode reward: total was 2.240000. running mean: -17.635222\n",
      "ep 2121: ep_len:3 episode reward: total was -1.500000. running mean: -17.473870\n",
      "ep 2121: ep_len:243 episode reward: total was 8.240000. running mean: -17.216732\n",
      "ep 2121: ep_len:617 episode reward: total was -0.330000. running mean: -17.047864\n",
      "epsilon:0.105925 episode_count: 14854. steps_count: 6434079.000000\n",
      "Time elapsed:  18648.567754030228\n",
      "ep 2122: ep_len:580 episode reward: total was -72.170000. running mean: -17.599086\n",
      "ep 2122: ep_len:500 episode reward: total was 4.400000. running mean: -17.379095\n",
      "ep 2122: ep_len:63 episode reward: total was -4.710000. running mean: -17.252404\n",
      "ep 2122: ep_len:528 episode reward: total was 26.510000. running mean: -16.814780\n",
      "ep 2122: ep_len:3 episode reward: total was 1.010000. running mean: -16.636532\n",
      "ep 2122: ep_len:500 episode reward: total was 21.610000. running mean: -16.254067\n",
      "ep 2122: ep_len:608 episode reward: total was -36.760000. running mean: -16.459126\n",
      "epsilon:0.105880 episode_count: 14861. steps_count: 6436861.000000\n",
      "Time elapsed:  18654.43216609955\n",
      "ep 2123: ep_len:500 episode reward: total was 27.280000. running mean: -16.021735\n",
      "ep 2123: ep_len:500 episode reward: total was -23.470000. running mean: -16.096217\n",
      "ep 2123: ep_len:635 episode reward: total was -66.060000. running mean: -16.595855\n",
      "ep 2123: ep_len:500 episode reward: total was 7.800000. running mean: -16.351897\n",
      "ep 2123: ep_len:3 episode reward: total was 1.010000. running mean: -16.178278\n",
      "ep 2123: ep_len:509 episode reward: total was -20.880000. running mean: -16.225295\n",
      "ep 2123: ep_len:548 episode reward: total was -29.330000. running mean: -16.356342\n",
      "epsilon:0.105836 episode_count: 14868. steps_count: 6440056.000000\n",
      "Time elapsed:  18666.386768102646\n",
      "ep 2124: ep_len:562 episode reward: total was 36.100000. running mean: -15.831779\n",
      "ep 2124: ep_len:606 episode reward: total was 16.530000. running mean: -15.508161\n",
      "ep 2124: ep_len:79 episode reward: total was 9.350000. running mean: -15.259579\n",
      "ep 2124: ep_len:528 episode reward: total was 15.860000. running mean: -14.948383\n",
      "ep 2124: ep_len:78 episode reward: total was -31.290000. running mean: -15.111799\n",
      "ep 2124: ep_len:500 episode reward: total was 5.820000. running mean: -14.902482\n",
      "ep 2124: ep_len:557 episode reward: total was -6.740000. running mean: -14.820857\n",
      "epsilon:0.105792 episode_count: 14875. steps_count: 6442966.000000\n",
      "Time elapsed:  18674.164444446564\n",
      "ep 2125: ep_len:603 episode reward: total was 24.610000. running mean: -14.426548\n",
      "ep 2125: ep_len:624 episode reward: total was 12.500000. running mean: -14.157283\n",
      "ep 2125: ep_len:501 episode reward: total was -19.060000. running mean: -14.206310\n",
      "ep 2125: ep_len:621 episode reward: total was 31.220000. running mean: -13.752047\n",
      "ep 2125: ep_len:118 episode reward: total was -55.650000. running mean: -14.171026\n",
      "ep 2125: ep_len:620 episode reward: total was 24.200000. running mean: -13.787316\n",
      "ep 2125: ep_len:590 episode reward: total was -34.130000. running mean: -13.990743\n",
      "epsilon:0.105747 episode_count: 14882. steps_count: 6446643.000000\n",
      "Time elapsed:  18686.9729681015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2126: ep_len:500 episode reward: total was -40.900000. running mean: -14.259835\n",
      "ep 2126: ep_len:514 episode reward: total was 46.570000. running mean: -13.651537\n",
      "ep 2126: ep_len:68 episode reward: total was 3.180000. running mean: -13.483222\n",
      "ep 2126: ep_len:552 episode reward: total was 8.040000. running mean: -13.267989\n",
      "ep 2126: ep_len:3 episode reward: total was 1.010000. running mean: -13.125210\n",
      "ep 2126: ep_len:569 episode reward: total was 10.030000. running mean: -12.893657\n",
      "ep 2126: ep_len:558 episode reward: total was -44.740000. running mean: -13.212121\n",
      "epsilon:0.105703 episode_count: 14889. steps_count: 6449407.000000\n",
      "Time elapsed:  18696.077179670334\n",
      "ep 2127: ep_len:500 episode reward: total was 11.700000. running mean: -12.963000\n",
      "ep 2127: ep_len:500 episode reward: total was 47.690000. running mean: -12.356470\n",
      "ep 2127: ep_len:534 episode reward: total was -38.040000. running mean: -12.613305\n",
      "ep 2127: ep_len:595 episode reward: total was -10.470000. running mean: -12.591872\n",
      "ep 2127: ep_len:3 episode reward: total was 1.010000. running mean: -12.455853\n",
      "ep 2127: ep_len:703 episode reward: total was -20.800000. running mean: -12.539295\n",
      "ep 2127: ep_len:503 episode reward: total was -23.780000. running mean: -12.651702\n",
      "epsilon:0.105659 episode_count: 14896. steps_count: 6452745.000000\n",
      "Time elapsed:  18704.893855810165\n",
      "ep 2128: ep_len:628 episode reward: total was 10.550000. running mean: -12.419685\n",
      "ep 2128: ep_len:500 episode reward: total was 40.570000. running mean: -11.889788\n",
      "ep 2128: ep_len:570 episode reward: total was -50.320000. running mean: -12.274090\n",
      "ep 2128: ep_len:132 episode reward: total was 2.580000. running mean: -12.125549\n",
      "ep 2128: ep_len:98 episode reward: total was 18.670000. running mean: -11.817594\n",
      "ep 2128: ep_len:246 episode reward: total was 32.480000. running mean: -11.374618\n",
      "ep 2128: ep_len:601 episode reward: total was -19.090000. running mean: -11.451771\n",
      "epsilon:0.105614 episode_count: 14903. steps_count: 6455520.000000\n",
      "Time elapsed:  18712.310989379883\n",
      "ep 2129: ep_len:603 episode reward: total was 41.640000. running mean: -10.920854\n",
      "ep 2129: ep_len:500 episode reward: total was -29.620000. running mean: -11.107845\n",
      "ep 2129: ep_len:642 episode reward: total was -31.610000. running mean: -11.312867\n",
      "ep 2129: ep_len:145 episode reward: total was -7.340000. running mean: -11.273138\n",
      "ep 2129: ep_len:130 episode reward: total was 14.860000. running mean: -11.011807\n",
      "ep 2129: ep_len:613 episode reward: total was 5.550000. running mean: -10.846189\n",
      "ep 2129: ep_len:500 episode reward: total was -40.210000. running mean: -11.139827\n",
      "epsilon:0.105570 episode_count: 14910. steps_count: 6458653.000000\n",
      "Time elapsed:  18720.64027786255\n",
      "ep 2130: ep_len:579 episode reward: total was 37.160000. running mean: -10.656829\n",
      "ep 2130: ep_len:500 episode reward: total was 37.650000. running mean: -10.173760\n",
      "ep 2130: ep_len:558 episode reward: total was -48.780000. running mean: -10.559823\n",
      "ep 2130: ep_len:40 episode reward: total was 0.230000. running mean: -10.451924\n",
      "ep 2130: ep_len:112 episode reward: total was 3.720000. running mean: -10.310205\n",
      "ep 2130: ep_len:544 episode reward: total was -123.170000. running mean: -11.438803\n",
      "ep 2130: ep_len:621 episode reward: total was -3.910000. running mean: -11.363515\n",
      "epsilon:0.105526 episode_count: 14917. steps_count: 6461607.000000\n",
      "Time elapsed:  18728.464852809906\n",
      "ep 2131: ep_len:526 episode reward: total was 54.050000. running mean: -10.709380\n",
      "ep 2131: ep_len:171 episode reward: total was -54.020000. running mean: -11.142486\n",
      "ep 2131: ep_len:531 episode reward: total was -76.520000. running mean: -11.796261\n",
      "ep 2131: ep_len:21 episode reward: total was -3.920000. running mean: -11.717499\n",
      "ep 2131: ep_len:38 episode reward: total was 11.500000. running mean: -11.485324\n",
      "ep 2131: ep_len:592 episode reward: total was -74.030000. running mean: -12.110770\n",
      "ep 2131: ep_len:612 episode reward: total was -25.480000. running mean: -12.244463\n",
      "epsilon:0.105481 episode_count: 14924. steps_count: 6464098.000000\n",
      "Time elapsed:  18735.378700256348\n",
      "ep 2132: ep_len:554 episode reward: total was -49.780000. running mean: -12.619818\n",
      "ep 2132: ep_len:500 episode reward: total was 23.840000. running mean: -12.255220\n",
      "ep 2132: ep_len:533 episode reward: total was 6.730000. running mean: -12.065368\n",
      "ep 2132: ep_len:505 episode reward: total was 19.330000. running mean: -11.751414\n",
      "ep 2132: ep_len:77 episode reward: total was 17.660000. running mean: -11.457300\n",
      "ep 2132: ep_len:645 episode reward: total was -18.330000. running mean: -11.526027\n",
      "ep 2132: ep_len:301 episode reward: total was -10.640000. running mean: -11.517167\n",
      "epsilon:0.105437 episode_count: 14931. steps_count: 6467213.000000\n",
      "Time elapsed:  18743.68200278282\n",
      "ep 2133: ep_len:509 episode reward: total was -66.490000. running mean: -12.066895\n",
      "ep 2133: ep_len:500 episode reward: total was -29.860000. running mean: -12.244826\n",
      "ep 2133: ep_len:579 episode reward: total was -30.210000. running mean: -12.424478\n",
      "ep 2133: ep_len:513 episode reward: total was -43.400000. running mean: -12.734233\n",
      "ep 2133: ep_len:3 episode reward: total was 1.010000. running mean: -12.596791\n",
      "ep 2133: ep_len:500 episode reward: total was 1.960000. running mean: -12.451223\n",
      "ep 2133: ep_len:621 episode reward: total was -21.440000. running mean: -12.541111\n",
      "epsilon:0.105393 episode_count: 14938. steps_count: 6470438.000000\n",
      "Time elapsed:  18752.268431663513\n",
      "ep 2134: ep_len:188 episode reward: total was -28.070000. running mean: -12.696399\n",
      "ep 2134: ep_len:544 episode reward: total was -15.620000. running mean: -12.725635\n",
      "ep 2134: ep_len:444 episode reward: total was 4.430000. running mean: -12.554079\n",
      "ep 2134: ep_len:529 episode reward: total was 14.260000. running mean: -12.285938\n",
      "ep 2134: ep_len:3 episode reward: total was 1.010000. running mean: -12.152979\n",
      "ep 2134: ep_len:548 episode reward: total was 16.590000. running mean: -11.865549\n",
      "ep 2134: ep_len:324 episode reward: total was 7.070000. running mean: -11.676194\n",
      "epsilon:0.105348 episode_count: 14945. steps_count: 6473018.000000\n",
      "Time elapsed:  18759.274551153183\n",
      "ep 2135: ep_len:173 episode reward: total was 6.240000. running mean: -11.497032\n",
      "ep 2135: ep_len:662 episode reward: total was -27.240000. running mean: -11.654461\n",
      "ep 2135: ep_len:500 episode reward: total was -38.290000. running mean: -11.920817\n",
      "ep 2135: ep_len:500 episode reward: total was -19.650000. running mean: -11.998109\n",
      "ep 2135: ep_len:3 episode reward: total was 1.010000. running mean: -11.868027\n",
      "ep 2135: ep_len:557 episode reward: total was -6.060000. running mean: -11.809947\n",
      "ep 2135: ep_len:195 episode reward: total was -11.570000. running mean: -11.807548\n",
      "epsilon:0.105304 episode_count: 14952. steps_count: 6475608.000000\n",
      "Time elapsed:  18769.76882982254\n",
      "ep 2136: ep_len:620 episode reward: total was -91.920000. running mean: -12.608672\n",
      "ep 2136: ep_len:255 episode reward: total was -19.000000. running mean: -12.672586\n",
      "ep 2136: ep_len:638 episode reward: total was -34.800000. running mean: -12.893860\n",
      "ep 2136: ep_len:500 episode reward: total was -1.390000. running mean: -12.778821\n",
      "ep 2136: ep_len:3 episode reward: total was 1.010000. running mean: -12.640933\n",
      "ep 2136: ep_len:546 episode reward: total was -68.230000. running mean: -13.196824\n",
      "ep 2136: ep_len:273 episode reward: total was -17.260000. running mean: -13.237455\n",
      "epsilon:0.105260 episode_count: 14959. steps_count: 6478443.000000\n",
      "Time elapsed:  18776.564838647842\n",
      "ep 2137: ep_len:616 episode reward: total was -75.340000. running mean: -13.858481\n",
      "ep 2137: ep_len:500 episode reward: total was 54.120000. running mean: -13.178696\n",
      "ep 2137: ep_len:632 episode reward: total was -8.260000. running mean: -13.129509\n",
      "ep 2137: ep_len:571 episode reward: total was -106.500000. running mean: -14.063214\n",
      "ep 2137: ep_len:109 episode reward: total was 0.750000. running mean: -13.915082\n",
      "ep 2137: ep_len:500 episode reward: total was -15.510000. running mean: -13.931031\n",
      "ep 2137: ep_len:567 episode reward: total was -9.160000. running mean: -13.883321\n",
      "epsilon:0.105215 episode_count: 14966. steps_count: 6481938.000000\n",
      "Time elapsed:  18786.147162675858\n",
      "ep 2138: ep_len:591 episode reward: total was -9.690000. running mean: -13.841387\n",
      "ep 2138: ep_len:633 episode reward: total was 0.320000. running mean: -13.699774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2138: ep_len:622 episode reward: total was -96.240000. running mean: -14.525176\n",
      "ep 2138: ep_len:598 episode reward: total was 23.530000. running mean: -14.144624\n",
      "ep 2138: ep_len:90 episode reward: total was 21.680000. running mean: -13.786378\n",
      "ep 2138: ep_len:640 episode reward: total was -11.690000. running mean: -13.765414\n",
      "ep 2138: ep_len:601 episode reward: total was 15.640000. running mean: -13.471360\n",
      "epsilon:0.105171 episode_count: 14973. steps_count: 6485713.000000\n",
      "Time elapsed:  18795.861505270004\n",
      "ep 2139: ep_len:545 episode reward: total was -61.320000. running mean: -13.949846\n",
      "ep 2139: ep_len:520 episode reward: total was -45.550000. running mean: -14.265848\n",
      "ep 2139: ep_len:662 episode reward: total was -96.910000. running mean: -15.092289\n",
      "ep 2139: ep_len:162 episode reward: total was -2.400000. running mean: -14.965366\n",
      "ep 2139: ep_len:3 episode reward: total was 1.010000. running mean: -14.805613\n",
      "ep 2139: ep_len:500 episode reward: total was -12.410000. running mean: -14.781657\n",
      "ep 2139: ep_len:523 episode reward: total was -55.530000. running mean: -15.189140\n",
      "epsilon:0.105127 episode_count: 14980. steps_count: 6488628.000000\n",
      "Time elapsed:  18806.37301659584\n",
      "ep 2140: ep_len:521 episode reward: total was -86.550000. running mean: -15.902749\n",
      "ep 2140: ep_len:500 episode reward: total was 21.180000. running mean: -15.531921\n",
      "ep 2140: ep_len:531 episode reward: total was -46.470000. running mean: -15.841302\n",
      "ep 2140: ep_len:573 episode reward: total was 28.230000. running mean: -15.400589\n",
      "ep 2140: ep_len:2 episode reward: total was -0.500000. running mean: -15.251583\n",
      "ep 2140: ep_len:552 episode reward: total was -66.480000. running mean: -15.763867\n",
      "ep 2140: ep_len:588 episode reward: total was -36.190000. running mean: -15.968129\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.105082 episode_count: 14987. steps_count: 6491895.000000\n",
      "Time elapsed:  18819.604090690613\n",
      "ep 2141: ep_len:214 episode reward: total was -2.280000. running mean: -15.831247\n",
      "ep 2141: ep_len:583 episode reward: total was 17.920000. running mean: -15.493735\n",
      "ep 2141: ep_len:580 episode reward: total was -12.800000. running mean: -15.466797\n",
      "ep 2141: ep_len:625 episode reward: total was 21.700000. running mean: -15.095130\n",
      "ep 2141: ep_len:3 episode reward: total was 1.010000. running mean: -14.934078\n",
      "ep 2141: ep_len:246 episode reward: total was 32.970000. running mean: -14.455037\n",
      "ep 2141: ep_len:504 episode reward: total was -61.910000. running mean: -14.929587\n",
      "epsilon:0.105038 episode_count: 14994. steps_count: 6494650.000000\n",
      "Time elapsed:  18828.755088567734\n",
      "ep 2142: ep_len:590 episode reward: total was 23.140000. running mean: -14.548891\n",
      "ep 2142: ep_len:575 episode reward: total was -93.330000. running mean: -15.336702\n",
      "ep 2142: ep_len:399 episode reward: total was 6.780000. running mean: -15.115535\n",
      "ep 2142: ep_len:500 episode reward: total was 27.770000. running mean: -14.686680\n",
      "ep 2142: ep_len:53 episode reward: total was 17.500000. running mean: -14.364813\n",
      "ep 2142: ep_len:637 episode reward: total was -10.170000. running mean: -14.322865\n",
      "ep 2142: ep_len:584 episode reward: total was -32.890000. running mean: -14.508536\n",
      "epsilon:0.104994 episode_count: 15001. steps_count: 6497988.000000\n",
      "Time elapsed:  18837.63627934456\n",
      "ep 2143: ep_len:543 episode reward: total was 32.390000. running mean: -14.039551\n",
      "ep 2143: ep_len:594 episode reward: total was -32.070000. running mean: -14.219855\n",
      "ep 2143: ep_len:500 episode reward: total was -36.000000. running mean: -14.437657\n",
      "ep 2143: ep_len:56 episode reward: total was -3.680000. running mean: -14.330080\n",
      "ep 2143: ep_len:3 episode reward: total was 1.010000. running mean: -14.176680\n",
      "ep 2143: ep_len:581 episode reward: total was 4.860000. running mean: -13.986313\n",
      "ep 2143: ep_len:342 episode reward: total was -74.070000. running mean: -14.587150\n",
      "epsilon:0.104949 episode_count: 15008. steps_count: 6500607.000000\n",
      "Time elapsed:  18847.862101078033\n",
      "ep 2144: ep_len:583 episode reward: total was 12.790000. running mean: -14.313378\n",
      "ep 2144: ep_len:536 episode reward: total was 19.370000. running mean: -13.976544\n",
      "ep 2144: ep_len:500 episode reward: total was -15.220000. running mean: -13.988979\n",
      "ep 2144: ep_len:615 episode reward: total was 60.990000. running mean: -13.239189\n",
      "ep 2144: ep_len:3 episode reward: total was 1.010000. running mean: -13.096697\n",
      "ep 2144: ep_len:606 episode reward: total was -15.770000. running mean: -13.123430\n",
      "ep 2144: ep_len:500 episode reward: total was -65.700000. running mean: -13.649196\n",
      "epsilon:0.104905 episode_count: 15015. steps_count: 6503950.000000\n",
      "Time elapsed:  18856.718394994736\n",
      "ep 2145: ep_len:511 episode reward: total was -115.920000. running mean: -14.671904\n",
      "ep 2145: ep_len:515 episode reward: total was -40.320000. running mean: -14.928385\n",
      "ep 2145: ep_len:431 episode reward: total was 17.920000. running mean: -14.599901\n",
      "ep 2145: ep_len:523 episode reward: total was -118.860000. running mean: -15.642502\n",
      "ep 2145: ep_len:3 episode reward: total was 1.010000. running mean: -15.475977\n",
      "ep 2145: ep_len:253 episode reward: total was 20.280000. running mean: -15.118417\n",
      "ep 2145: ep_len:609 episode reward: total was -62.540000. running mean: -15.592633\n",
      "epsilon:0.104861 episode_count: 15022. steps_count: 6506795.000000\n",
      "Time elapsed:  18864.26477074623\n",
      "ep 2146: ep_len:227 episode reward: total was -2.360000. running mean: -15.460307\n",
      "ep 2146: ep_len:500 episode reward: total was -27.930000. running mean: -15.585004\n",
      "ep 2146: ep_len:571 episode reward: total was -35.860000. running mean: -15.787754\n",
      "ep 2146: ep_len:56 episode reward: total was 3.820000. running mean: -15.591676\n",
      "ep 2146: ep_len:112 episode reward: total was -0.690000. running mean: -15.442659\n",
      "ep 2146: ep_len:646 episode reward: total was -15.590000. running mean: -15.444133\n",
      "ep 2146: ep_len:626 episode reward: total was 10.210000. running mean: -15.187591\n",
      "epsilon:0.104816 episode_count: 15029. steps_count: 6509533.000000\n",
      "Time elapsed:  18872.76873779297\n",
      "ep 2147: ep_len:500 episode reward: total was 25.290000. running mean: -14.782816\n",
      "ep 2147: ep_len:500 episode reward: total was -61.600000. running mean: -15.250987\n",
      "ep 2147: ep_len:614 episode reward: total was -40.190000. running mean: -15.500378\n",
      "ep 2147: ep_len:538 episode reward: total was -48.440000. running mean: -15.829774\n",
      "ep 2147: ep_len:2 episode reward: total was -0.500000. running mean: -15.676476\n",
      "ep 2147: ep_len:682 episode reward: total was -42.590000. running mean: -15.945611\n",
      "ep 2147: ep_len:518 episode reward: total was -155.450000. running mean: -17.340655\n",
      "epsilon:0.104772 episode_count: 15036. steps_count: 6512887.000000\n",
      "Time elapsed:  18881.62367939949\n",
      "ep 2148: ep_len:540 episode reward: total was 29.970000. running mean: -16.867549\n",
      "ep 2148: ep_len:500 episode reward: total was -27.790000. running mean: -16.976773\n",
      "ep 2148: ep_len:540 episode reward: total was -39.010000. running mean: -17.197105\n",
      "ep 2148: ep_len:609 episode reward: total was 3.260000. running mean: -16.992534\n",
      "ep 2148: ep_len:3 episode reward: total was -0.490000. running mean: -16.827509\n",
      "ep 2148: ep_len:286 episode reward: total was 21.540000. running mean: -16.443834\n",
      "ep 2148: ep_len:561 episode reward: total was -29.840000. running mean: -16.577796\n",
      "epsilon:0.104728 episode_count: 15043. steps_count: 6515926.000000\n",
      "Time elapsed:  18894.06082057953\n",
      "ep 2149: ep_len:555 episode reward: total was -24.570000. running mean: -16.657718\n",
      "ep 2149: ep_len:500 episode reward: total was 20.540000. running mean: -16.285740\n",
      "ep 2149: ep_len:556 episode reward: total was -94.680000. running mean: -17.069683\n",
      "ep 2149: ep_len:582 episode reward: total was -34.970000. running mean: -17.248686\n",
      "ep 2149: ep_len:81 episode reward: total was 22.630000. running mean: -16.849899\n",
      "ep 2149: ep_len:588 episode reward: total was -32.840000. running mean: -17.009800\n",
      "ep 2149: ep_len:597 episode reward: total was -29.680000. running mean: -17.136502\n",
      "epsilon:0.104683 episode_count: 15050. steps_count: 6519385.000000\n",
      "Time elapsed:  18903.150704860687\n",
      "ep 2150: ep_len:609 episode reward: total was -102.130000. running mean: -17.986437\n",
      "ep 2150: ep_len:565 episode reward: total was 39.490000. running mean: -17.411673\n",
      "ep 2150: ep_len:66 episode reward: total was 6.190000. running mean: -17.175656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2150: ep_len:170 episode reward: total was 11.640000. running mean: -16.887500\n",
      "ep 2150: ep_len:128 episode reward: total was 14.880000. running mean: -16.569825\n",
      "ep 2150: ep_len:702 episode reward: total was -164.140000. running mean: -18.045526\n",
      "ep 2150: ep_len:572 episode reward: total was -11.040000. running mean: -17.975471\n",
      "epsilon:0.104639 episode_count: 15057. steps_count: 6522197.000000\n",
      "Time elapsed:  18911.565904140472\n",
      "ep 2151: ep_len:559 episode reward: total was 44.120000. running mean: -17.354516\n",
      "ep 2151: ep_len:526 episode reward: total was -18.260000. running mean: -17.363571\n",
      "ep 2151: ep_len:500 episode reward: total was -34.720000. running mean: -17.537136\n",
      "ep 2151: ep_len:505 episode reward: total was 16.250000. running mean: -17.199264\n",
      "ep 2151: ep_len:115 episode reward: total was 1.760000. running mean: -17.009672\n",
      "ep 2151: ep_len:507 episode reward: total was -89.550000. running mean: -17.735075\n",
      "ep 2151: ep_len:512 episode reward: total was -18.140000. running mean: -17.739124\n",
      "epsilon:0.104595 episode_count: 15064. steps_count: 6525421.000000\n",
      "Time elapsed:  18921.286903858185\n",
      "ep 2152: ep_len:569 episode reward: total was -118.140000. running mean: -18.743133\n",
      "ep 2152: ep_len:614 episode reward: total was -40.930000. running mean: -18.965001\n",
      "ep 2152: ep_len:683 episode reward: total was -63.120000. running mean: -19.406551\n",
      "ep 2152: ep_len:565 episode reward: total was 59.290000. running mean: -18.619586\n",
      "ep 2152: ep_len:55 episode reward: total was 21.010000. running mean: -18.223290\n",
      "ep 2152: ep_len:519 episode reward: total was -48.350000. running mean: -18.524557\n",
      "ep 2152: ep_len:553 episode reward: total was -31.280000. running mean: -18.652112\n",
      "epsilon:0.104550 episode_count: 15071. steps_count: 6528979.000000\n",
      "Time elapsed:  18931.91605567932\n",
      "ep 2153: ep_len:500 episode reward: total was 35.820000. running mean: -18.107391\n",
      "ep 2153: ep_len:524 episode reward: total was -59.010000. running mean: -18.516417\n",
      "ep 2153: ep_len:515 episode reward: total was -45.450000. running mean: -18.785752\n",
      "ep 2153: ep_len:500 episode reward: total was 34.680000. running mean: -18.251095\n",
      "ep 2153: ep_len:3 episode reward: total was 1.010000. running mean: -18.058484\n",
      "ep 2153: ep_len:533 episode reward: total was -22.660000. running mean: -18.104499\n",
      "ep 2153: ep_len:534 episode reward: total was -73.300000. running mean: -18.656454\n",
      "epsilon:0.104506 episode_count: 15078. steps_count: 6532088.000000\n",
      "Time elapsed:  18941.41095995903\n",
      "ep 2154: ep_len:608 episode reward: total was -136.780000. running mean: -19.837690\n",
      "ep 2154: ep_len:590 episode reward: total was -39.110000. running mean: -20.030413\n",
      "ep 2154: ep_len:65 episode reward: total was 0.150000. running mean: -19.828609\n",
      "ep 2154: ep_len:500 episode reward: total was -6.980000. running mean: -19.700122\n",
      "ep 2154: ep_len:130 episode reward: total was 23.380000. running mean: -19.269321\n",
      "ep 2154: ep_len:527 episode reward: total was -94.410000. running mean: -20.020728\n",
      "ep 2154: ep_len:552 episode reward: total was -43.270000. running mean: -20.253221\n",
      "epsilon:0.104462 episode_count: 15085. steps_count: 6535060.000000\n",
      "Time elapsed:  18950.49820613861\n",
      "ep 2155: ep_len:500 episode reward: total was 11.850000. running mean: -19.932189\n",
      "ep 2155: ep_len:503 episode reward: total was -22.780000. running mean: -19.960667\n",
      "ep 2155: ep_len:500 episode reward: total was -47.770000. running mean: -20.238760\n",
      "ep 2155: ep_len:519 episode reward: total was 6.710000. running mean: -19.969272\n",
      "ep 2155: ep_len:3 episode reward: total was 1.010000. running mean: -19.759480\n",
      "ep 2155: ep_len:571 episode reward: total was -46.560000. running mean: -20.027485\n",
      "ep 2155: ep_len:523 episode reward: total was -18.850000. running mean: -20.015710\n",
      "epsilon:0.104417 episode_count: 15092. steps_count: 6538179.000000\n",
      "Time elapsed:  18959.097584962845\n",
      "ep 2156: ep_len:265 episode reward: total was -2.770000. running mean: -19.843253\n",
      "ep 2156: ep_len:585 episode reward: total was 22.400000. running mean: -19.420820\n",
      "ep 2156: ep_len:649 episode reward: total was -28.970000. running mean: -19.516312\n",
      "ep 2156: ep_len:532 episode reward: total was -22.430000. running mean: -19.545449\n",
      "ep 2156: ep_len:3 episode reward: total was -1.500000. running mean: -19.364995\n",
      "ep 2156: ep_len:525 episode reward: total was -43.670000. running mean: -19.608045\n",
      "ep 2156: ep_len:587 episode reward: total was -86.160000. running mean: -20.273564\n",
      "epsilon:0.104373 episode_count: 15099. steps_count: 6541325.000000\n",
      "Time elapsed:  18968.55915260315\n",
      "ep 2157: ep_len:198 episode reward: total was 2.640000. running mean: -20.044429\n",
      "ep 2157: ep_len:500 episode reward: total was -40.360000. running mean: -20.247584\n",
      "ep 2157: ep_len:625 episode reward: total was -45.870000. running mean: -20.503808\n",
      "ep 2157: ep_len:132 episode reward: total was -7.470000. running mean: -20.373470\n",
      "ep 2157: ep_len:3 episode reward: total was 1.010000. running mean: -20.159636\n",
      "ep 2157: ep_len:500 episode reward: total was 36.480000. running mean: -19.593239\n",
      "ep 2157: ep_len:534 episode reward: total was -31.570000. running mean: -19.713007\n",
      "epsilon:0.104329 episode_count: 15106. steps_count: 6543817.000000\n",
      "Time elapsed:  18981.428832530975\n",
      "ep 2158: ep_len:519 episode reward: total was 37.930000. running mean: -19.136577\n",
      "ep 2158: ep_len:590 episode reward: total was -63.490000. running mean: -19.580111\n",
      "ep 2158: ep_len:542 episode reward: total was -61.680000. running mean: -20.001110\n",
      "ep 2158: ep_len:157 episode reward: total was -3.540000. running mean: -19.836499\n",
      "ep 2158: ep_len:3 episode reward: total was 1.010000. running mean: -19.628034\n",
      "ep 2158: ep_len:706 episode reward: total was -207.570000. running mean: -21.507454\n",
      "ep 2158: ep_len:582 episode reward: total was -21.740000. running mean: -21.509779\n",
      "epsilon:0.104284 episode_count: 15113. steps_count: 6546916.000000\n",
      "Time elapsed:  18991.29244875908\n",
      "ep 2159: ep_len:210 episode reward: total was 14.180000. running mean: -21.152881\n",
      "ep 2159: ep_len:344 episode reward: total was -35.510000. running mean: -21.296452\n",
      "ep 2159: ep_len:397 episode reward: total was 16.950000. running mean: -20.913988\n",
      "ep 2159: ep_len:406 episode reward: total was -15.900000. running mean: -20.863848\n",
      "ep 2159: ep_len:3 episode reward: total was 1.010000. running mean: -20.645110\n",
      "ep 2159: ep_len:240 episode reward: total was 2.270000. running mean: -20.415958\n",
      "ep 2159: ep_len:553 episode reward: total was -19.230000. running mean: -20.404099\n",
      "epsilon:0.104240 episode_count: 15120. steps_count: 6549069.000000\n",
      "Time elapsed:  18998.741846323013\n",
      "ep 2160: ep_len:586 episode reward: total was -93.930000. running mean: -21.139358\n",
      "ep 2160: ep_len:588 episode reward: total was -21.410000. running mean: -21.142064\n",
      "ep 2160: ep_len:543 episode reward: total was -9.030000. running mean: -21.020944\n",
      "ep 2160: ep_len:523 episode reward: total was -105.950000. running mean: -21.870234\n",
      "ep 2160: ep_len:3 episode reward: total was -1.500000. running mean: -21.666532\n",
      "ep 2160: ep_len:186 episode reward: total was 15.670000. running mean: -21.293167\n",
      "ep 2160: ep_len:505 episode reward: total was -44.430000. running mean: -21.524535\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.104196 episode_count: 15127. steps_count: 6552003.000000\n",
      "Time elapsed:  19017.164216518402\n",
      "ep 2161: ep_len:511 episode reward: total was -9.190000. running mean: -21.401190\n",
      "ep 2161: ep_len:500 episode reward: total was -15.350000. running mean: -21.340678\n",
      "ep 2161: ep_len:546 episode reward: total was -40.610000. running mean: -21.533371\n",
      "ep 2161: ep_len:547 episode reward: total was 36.180000. running mean: -20.956237\n",
      "ep 2161: ep_len:95 episode reward: total was 31.160000. running mean: -20.435075\n",
      "ep 2161: ep_len:555 episode reward: total was -40.550000. running mean: -20.636224\n",
      "ep 2161: ep_len:559 episode reward: total was 20.890000. running mean: -20.220962\n",
      "epsilon:0.104151 episode_count: 15134. steps_count: 6555316.000000\n",
      "Time elapsed:  19031.39479470253\n",
      "ep 2162: ep_len:500 episode reward: total was 55.840000. running mean: -19.460352\n",
      "ep 2162: ep_len:604 episode reward: total was -42.890000. running mean: -19.694649\n",
      "ep 2162: ep_len:509 episode reward: total was -39.340000. running mean: -19.891102\n",
      "ep 2162: ep_len:500 episode reward: total was -5.030000. running mean: -19.742491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2162: ep_len:50 episode reward: total was 16.000000. running mean: -19.385066\n",
      "ep 2162: ep_len:278 episode reward: total was 25.010000. running mean: -18.941116\n",
      "ep 2162: ep_len:500 episode reward: total was -29.360000. running mean: -19.045304\n",
      "epsilon:0.104107 episode_count: 15141. steps_count: 6558257.000000\n",
      "Time elapsed:  19044.744256734848\n",
      "ep 2163: ep_len:563 episode reward: total was -40.960000. running mean: -19.264451\n",
      "ep 2163: ep_len:502 episode reward: total was 18.730000. running mean: -18.884507\n",
      "ep 2163: ep_len:506 episode reward: total was -57.330000. running mean: -19.268962\n",
      "ep 2163: ep_len:581 episode reward: total was 14.270000. running mean: -18.933572\n",
      "ep 2163: ep_len:3 episode reward: total was 1.010000. running mean: -18.734136\n",
      "ep 2163: ep_len:625 episode reward: total was -13.590000. running mean: -18.682695\n",
      "ep 2163: ep_len:563 episode reward: total was -67.130000. running mean: -19.167168\n",
      "epsilon:0.104063 episode_count: 15148. steps_count: 6561600.000000\n",
      "Time elapsed:  19058.095834970474\n",
      "ep 2164: ep_len:508 episode reward: total was -71.710000. running mean: -19.692596\n",
      "ep 2164: ep_len:501 episode reward: total was 10.540000. running mean: -19.390270\n",
      "ep 2164: ep_len:634 episode reward: total was -83.080000. running mean: -20.027168\n",
      "ep 2164: ep_len:146 episode reward: total was 1.560000. running mean: -19.811296\n",
      "ep 2164: ep_len:112 episode reward: total was 12.840000. running mean: -19.484783\n",
      "ep 2164: ep_len:500 episode reward: total was -207.660000. running mean: -21.366535\n",
      "ep 2164: ep_len:181 episode reward: total was -5.500000. running mean: -21.207870\n",
      "epsilon:0.104018 episode_count: 15155. steps_count: 6564182.000000\n",
      "Time elapsed:  19067.651106357574\n",
      "ep 2165: ep_len:676 episode reward: total was -67.720000. running mean: -21.672991\n",
      "ep 2165: ep_len:305 episode reward: total was -35.780000. running mean: -21.814061\n",
      "ep 2165: ep_len:630 episode reward: total was -65.180000. running mean: -22.247721\n",
      "ep 2165: ep_len:587 episode reward: total was 11.210000. running mean: -21.913144\n",
      "ep 2165: ep_len:3 episode reward: total was 1.010000. running mean: -21.683912\n",
      "ep 2165: ep_len:573 episode reward: total was -29.020000. running mean: -21.757273\n",
      "ep 2165: ep_len:611 episode reward: total was -32.220000. running mean: -21.861900\n",
      "epsilon:0.103974 episode_count: 15162. steps_count: 6567567.000000\n",
      "Time elapsed:  19077.821708202362\n",
      "ep 2166: ep_len:602 episode reward: total was 43.210000. running mean: -21.211181\n",
      "ep 2166: ep_len:640 episode reward: total was -17.030000. running mean: -21.169369\n",
      "ep 2166: ep_len:590 episode reward: total was -43.680000. running mean: -21.394476\n",
      "ep 2166: ep_len:501 episode reward: total was -50.720000. running mean: -21.687731\n",
      "ep 2166: ep_len:3 episode reward: total was 1.010000. running mean: -21.460754\n",
      "ep 2166: ep_len:500 episode reward: total was -25.550000. running mean: -21.501646\n",
      "ep 2166: ep_len:500 episode reward: total was -4.060000. running mean: -21.327230\n",
      "epsilon:0.103930 episode_count: 15169. steps_count: 6570903.000000\n",
      "Time elapsed:  19092.22275018692\n",
      "ep 2167: ep_len:500 episode reward: total was 3.290000. running mean: -21.081057\n",
      "ep 2167: ep_len:500 episode reward: total was 26.640000. running mean: -20.603847\n",
      "ep 2167: ep_len:551 episode reward: total was -53.850000. running mean: -20.936308\n",
      "ep 2167: ep_len:500 episode reward: total was 25.260000. running mean: -20.474345\n",
      "ep 2167: ep_len:3 episode reward: total was 1.010000. running mean: -20.259502\n",
      "ep 2167: ep_len:573 episode reward: total was -0.940000. running mean: -20.066307\n",
      "ep 2167: ep_len:346 episode reward: total was -2.000000. running mean: -19.885644\n",
      "epsilon:0.103885 episode_count: 15176. steps_count: 6573876.000000\n",
      "Time elapsed:  19101.27504634857\n",
      "ep 2168: ep_len:751 episode reward: total was -100.930000. running mean: -20.696087\n",
      "ep 2168: ep_len:500 episode reward: total was -24.680000. running mean: -20.735926\n",
      "ep 2168: ep_len:500 episode reward: total was -35.020000. running mean: -20.878767\n",
      "ep 2168: ep_len:557 episode reward: total was 53.940000. running mean: -20.130579\n",
      "ep 2168: ep_len:3 episode reward: total was 1.010000. running mean: -19.919174\n",
      "ep 2168: ep_len:525 episode reward: total was -12.120000. running mean: -19.841182\n",
      "ep 2168: ep_len:627 episode reward: total was -9.930000. running mean: -19.742070\n",
      "epsilon:0.103841 episode_count: 15183. steps_count: 6577339.000000\n",
      "Time elapsed:  19111.516466379166\n",
      "ep 2169: ep_len:197 episode reward: total was 21.210000. running mean: -19.332549\n",
      "ep 2169: ep_len:194 episode reward: total was 10.920000. running mean: -19.030024\n",
      "ep 2169: ep_len:500 episode reward: total was -32.670000. running mean: -19.166424\n",
      "ep 2169: ep_len:580 episode reward: total was 28.670000. running mean: -18.688059\n",
      "ep 2169: ep_len:3 episode reward: total was 1.010000. running mean: -18.491079\n",
      "ep 2169: ep_len:541 episode reward: total was -24.840000. running mean: -18.554568\n",
      "ep 2169: ep_len:565 episode reward: total was -23.410000. running mean: -18.603122\n",
      "epsilon:0.103797 episode_count: 15190. steps_count: 6579919.000000\n",
      "Time elapsed:  19119.664111852646\n",
      "ep 2170: ep_len:509 episode reward: total was 9.660000. running mean: -18.320491\n",
      "ep 2170: ep_len:570 episode reward: total was 7.020000. running mean: -18.067086\n",
      "ep 2170: ep_len:500 episode reward: total was -56.810000. running mean: -18.454515\n",
      "ep 2170: ep_len:513 episode reward: total was -4.690000. running mean: -18.316870\n",
      "ep 2170: ep_len:2 episode reward: total was -0.500000. running mean: -18.138702\n",
      "ep 2170: ep_len:523 episode reward: total was -54.230000. running mean: -18.499614\n",
      "ep 2170: ep_len:584 episode reward: total was -63.150000. running mean: -18.946118\n",
      "epsilon:0.103752 episode_count: 15197. steps_count: 6583120.000000\n",
      "Time elapsed:  19128.82289338112\n",
      "ep 2171: ep_len:506 episode reward: total was 10.750000. running mean: -18.649157\n",
      "ep 2171: ep_len:526 episode reward: total was 13.440000. running mean: -18.328266\n",
      "ep 2171: ep_len:659 episode reward: total was -6.180000. running mean: -18.206783\n",
      "ep 2171: ep_len:148 episode reward: total was 12.500000. running mean: -17.899715\n",
      "ep 2171: ep_len:3 episode reward: total was 1.010000. running mean: -17.710618\n",
      "ep 2171: ep_len:572 episode reward: total was -33.460000. running mean: -17.868112\n",
      "ep 2171: ep_len:654 episode reward: total was -195.480000. running mean: -19.644231\n",
      "epsilon:0.103708 episode_count: 15204. steps_count: 6586188.000000\n",
      "Time elapsed:  19136.851504087448\n",
      "ep 2172: ep_len:123 episode reward: total was -3.550000. running mean: -19.483288\n",
      "ep 2172: ep_len:500 episode reward: total was -8.120000. running mean: -19.369655\n",
      "ep 2172: ep_len:611 episode reward: total was -29.270000. running mean: -19.468659\n",
      "ep 2172: ep_len:423 episode reward: total was -29.860000. running mean: -19.572572\n",
      "ep 2172: ep_len:3 episode reward: total was 1.010000. running mean: -19.366747\n",
      "ep 2172: ep_len:625 episode reward: total was 8.770000. running mean: -19.085379\n",
      "ep 2172: ep_len:500 episode reward: total was 10.420000. running mean: -18.790325\n",
      "epsilon:0.103664 episode_count: 15211. steps_count: 6588973.000000\n",
      "Time elapsed:  19147.262739419937\n",
      "ep 2173: ep_len:590 episode reward: total was 9.150000. running mean: -18.510922\n",
      "ep 2173: ep_len:635 episode reward: total was 1.350000. running mean: -18.312313\n",
      "ep 2173: ep_len:625 episode reward: total was -63.550000. running mean: -18.764690\n",
      "ep 2173: ep_len:56 episode reward: total was -7.690000. running mean: -18.653943\n",
      "ep 2173: ep_len:3 episode reward: total was 1.010000. running mean: -18.457303\n",
      "ep 2173: ep_len:571 episode reward: total was -0.530000. running mean: -18.278030\n",
      "ep 2173: ep_len:511 episode reward: total was -16.190000. running mean: -18.257150\n",
      "epsilon:0.103619 episode_count: 15218. steps_count: 6591964.000000\n",
      "Time elapsed:  19157.92571759224\n",
      "ep 2174: ep_len:500 episode reward: total was -17.850000. running mean: -18.253079\n",
      "ep 2174: ep_len:500 episode reward: total was 39.700000. running mean: -17.673548\n",
      "ep 2174: ep_len:569 episode reward: total was -49.270000. running mean: -17.989512\n",
      "ep 2174: ep_len:500 episode reward: total was 29.530000. running mean: -17.514317\n",
      "ep 2174: ep_len:3 episode reward: total was 1.010000. running mean: -17.329074\n",
      "ep 2174: ep_len:508 episode reward: total was -40.260000. running mean: -17.558383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2174: ep_len:507 episode reward: total was -24.030000. running mean: -17.623099\n",
      "epsilon:0.103575 episode_count: 15225. steps_count: 6595051.000000\n",
      "Time elapsed:  19165.950971364975\n",
      "ep 2175: ep_len:700 episode reward: total was -70.600000. running mean: -18.152868\n",
      "ep 2175: ep_len:191 episode reward: total was -10.540000. running mean: -18.076740\n",
      "ep 2175: ep_len:542 episode reward: total was -65.330000. running mean: -18.549272\n",
      "ep 2175: ep_len:507 episode reward: total was -29.440000. running mean: -18.658180\n",
      "ep 2175: ep_len:93 episode reward: total was -19.710000. running mean: -18.668698\n",
      "ep 2175: ep_len:582 episode reward: total was -70.040000. running mean: -19.182411\n",
      "ep 2175: ep_len:553 episode reward: total was 15.600000. running mean: -18.834587\n",
      "epsilon:0.103531 episode_count: 15232. steps_count: 6598219.000000\n",
      "Time elapsed:  19174.432075738907\n",
      "ep 2176: ep_len:644 episode reward: total was -15.340000. running mean: -18.799641\n",
      "ep 2176: ep_len:572 episode reward: total was -4.010000. running mean: -18.651744\n",
      "ep 2176: ep_len:596 episode reward: total was -28.250000. running mean: -18.747727\n",
      "ep 2176: ep_len:537 episode reward: total was -5.170000. running mean: -18.611950\n",
      "ep 2176: ep_len:112 episode reward: total was 1.800000. running mean: -18.407830\n",
      "ep 2176: ep_len:567 episode reward: total was -51.870000. running mean: -18.742452\n",
      "ep 2176: ep_len:551 episode reward: total was -37.800000. running mean: -18.933027\n",
      "epsilon:0.103486 episode_count: 15239. steps_count: 6601798.000000\n",
      "Time elapsed:  19184.12140727043\n",
      "ep 2177: ep_len:240 episode reward: total was 1.840000. running mean: -18.725297\n",
      "ep 2177: ep_len:531 episode reward: total was -12.030000. running mean: -18.658344\n",
      "ep 2177: ep_len:556 episode reward: total was -47.050000. running mean: -18.942261\n",
      "ep 2177: ep_len:500 episode reward: total was 41.080000. running mean: -18.342038\n",
      "ep 2177: ep_len:3 episode reward: total was -0.490000. running mean: -18.163518\n",
      "ep 2177: ep_len:502 episode reward: total was -1.950000. running mean: -18.001383\n",
      "ep 2177: ep_len:569 episode reward: total was -36.240000. running mean: -18.183769\n",
      "epsilon:0.103442 episode_count: 15246. steps_count: 6604699.000000\n",
      "Time elapsed:  19191.841906547546\n",
      "ep 2178: ep_len:593 episode reward: total was -6.280000. running mean: -18.064731\n",
      "ep 2178: ep_len:514 episode reward: total was 19.260000. running mean: -17.691484\n",
      "ep 2178: ep_len:500 episode reward: total was -12.510000. running mean: -17.639669\n",
      "ep 2178: ep_len:500 episode reward: total was -16.480000. running mean: -17.628072\n",
      "ep 2178: ep_len:3 episode reward: total was 1.010000. running mean: -17.441692\n",
      "ep 2178: ep_len:325 episode reward: total was 12.170000. running mean: -17.145575\n",
      "ep 2178: ep_len:532 episode reward: total was -14.040000. running mean: -17.114519\n",
      "epsilon:0.103398 episode_count: 15253. steps_count: 6607666.000000\n",
      "Time elapsed:  19199.79795384407\n",
      "ep 2179: ep_len:526 episode reward: total was -40.970000. running mean: -17.353074\n",
      "ep 2179: ep_len:357 episode reward: total was 0.900000. running mean: -17.170543\n",
      "ep 2179: ep_len:546 episode reward: total was -9.310000. running mean: -17.091938\n",
      "ep 2179: ep_len:542 episode reward: total was -6.770000. running mean: -16.988718\n",
      "ep 2179: ep_len:3 episode reward: total was 1.010000. running mean: -16.808731\n",
      "ep 2179: ep_len:500 episode reward: total was -98.440000. running mean: -17.625044\n",
      "ep 2179: ep_len:500 episode reward: total was -37.610000. running mean: -17.824893\n",
      "epsilon:0.103353 episode_count: 15260. steps_count: 6610640.000000\n",
      "Time elapsed:  19207.583515167236\n",
      "ep 2180: ep_len:201 episode reward: total was 4.630000. running mean: -17.600344\n",
      "ep 2180: ep_len:575 episode reward: total was -133.390000. running mean: -18.758241\n",
      "ep 2180: ep_len:500 episode reward: total was 4.230000. running mean: -18.528358\n",
      "ep 2180: ep_len:533 episode reward: total was -11.640000. running mean: -18.459475\n",
      "ep 2180: ep_len:3 episode reward: total was 1.010000. running mean: -18.264780\n",
      "ep 2180: ep_len:512 episode reward: total was -94.780000. running mean: -19.029932\n",
      "ep 2180: ep_len:500 episode reward: total was -9.880000. running mean: -18.938433\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.103309 episode_count: 15267. steps_count: 6613464.000000\n",
      "Time elapsed:  19219.856781482697\n",
      "ep 2181: ep_len:595 episode reward: total was 17.660000. running mean: -18.572449\n",
      "ep 2181: ep_len:500 episode reward: total was -24.060000. running mean: -18.627324\n",
      "ep 2181: ep_len:548 episode reward: total was -105.980000. running mean: -19.500851\n",
      "ep 2181: ep_len:508 episode reward: total was -17.450000. running mean: -19.480342\n",
      "ep 2181: ep_len:3 episode reward: total was 1.010000. running mean: -19.275439\n",
      "ep 2181: ep_len:574 episode reward: total was -47.090000. running mean: -19.553585\n",
      "ep 2181: ep_len:537 episode reward: total was 4.950000. running mean: -19.308549\n",
      "epsilon:0.103265 episode_count: 15274. steps_count: 6616729.000000\n",
      "Time elapsed:  19228.596616268158\n",
      "ep 2182: ep_len:260 episode reward: total was -17.630000. running mean: -19.291763\n",
      "ep 2182: ep_len:500 episode reward: total was 81.290000. running mean: -18.285946\n",
      "ep 2182: ep_len:538 episode reward: total was -9.030000. running mean: -18.193386\n",
      "ep 2182: ep_len:500 episode reward: total was -6.190000. running mean: -18.073352\n",
      "ep 2182: ep_len:3 episode reward: total was 1.010000. running mean: -17.882519\n",
      "ep 2182: ep_len:500 episode reward: total was 14.870000. running mean: -17.554994\n",
      "ep 2182: ep_len:565 episode reward: total was 6.530000. running mean: -17.314144\n",
      "epsilon:0.103220 episode_count: 15281. steps_count: 6619595.000000\n",
      "Time elapsed:  19236.276796340942\n",
      "ep 2183: ep_len:637 episode reward: total was -149.010000. running mean: -18.631102\n",
      "ep 2183: ep_len:523 episode reward: total was 48.650000. running mean: -17.958291\n",
      "ep 2183: ep_len:647 episode reward: total was -63.410000. running mean: -18.412808\n",
      "ep 2183: ep_len:538 episode reward: total was 21.330000. running mean: -18.015380\n",
      "ep 2183: ep_len:86 episode reward: total was 18.210000. running mean: -17.653126\n",
      "ep 2183: ep_len:543 episode reward: total was -20.530000. running mean: -17.681895\n",
      "ep 2183: ep_len:515 episode reward: total was -0.080000. running mean: -17.505876\n",
      "epsilon:0.103176 episode_count: 15288. steps_count: 6623084.000000\n",
      "Time elapsed:  19245.414978981018\n",
      "ep 2184: ep_len:123 episode reward: total was 5.450000. running mean: -17.276317\n",
      "ep 2184: ep_len:593 episode reward: total was -149.040000. running mean: -18.593954\n",
      "ep 2184: ep_len:500 episode reward: total was -27.640000. running mean: -18.684415\n",
      "ep 2184: ep_len:497 episode reward: total was -10.190000. running mean: -18.599471\n",
      "ep 2184: ep_len:45 episode reward: total was 16.500000. running mean: -18.248476\n",
      "ep 2184: ep_len:631 episode reward: total was -4.440000. running mean: -18.110391\n",
      "ep 2184: ep_len:500 episode reward: total was -28.120000. running mean: -18.210487\n",
      "epsilon:0.103132 episode_count: 15295. steps_count: 6625973.000000\n",
      "Time elapsed:  19253.105246067047\n",
      "ep 2185: ep_len:527 episode reward: total was -38.730000. running mean: -18.415682\n",
      "ep 2185: ep_len:515 episode reward: total was 44.130000. running mean: -17.790225\n",
      "ep 2185: ep_len:686 episode reward: total was -7.290000. running mean: -17.685223\n",
      "ep 2185: ep_len:519 episode reward: total was -19.370000. running mean: -17.702071\n",
      "ep 2185: ep_len:119 episode reward: total was 16.340000. running mean: -17.361650\n",
      "ep 2185: ep_len:500 episode reward: total was -27.890000. running mean: -17.466934\n",
      "ep 2185: ep_len:181 episode reward: total was -32.950000. running mean: -17.621764\n",
      "epsilon:0.103087 episode_count: 15302. steps_count: 6629020.000000\n",
      "Time elapsed:  19265.02387714386\n",
      "ep 2186: ep_len:583 episode reward: total was -41.400000. running mean: -17.859547\n",
      "ep 2186: ep_len:514 episode reward: total was 23.160000. running mean: -17.449351\n",
      "ep 2186: ep_len:620 episode reward: total was -34.590000. running mean: -17.620758\n",
      "ep 2186: ep_len:170 episode reward: total was 11.170000. running mean: -17.332850\n",
      "ep 2186: ep_len:3 episode reward: total was 1.010000. running mean: -17.149422\n",
      "ep 2186: ep_len:575 episode reward: total was 12.970000. running mean: -16.848228\n",
      "ep 2186: ep_len:326 episode reward: total was -14.630000. running mean: -16.826045\n",
      "epsilon:0.103043 episode_count: 15309. steps_count: 6631811.000000\n",
      "Time elapsed:  19272.422836780548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2187: ep_len:524 episode reward: total was -89.860000. running mean: -17.556385\n",
      "ep 2187: ep_len:552 episode reward: total was 38.990000. running mean: -16.990921\n",
      "ep 2187: ep_len:653 episode reward: total was -32.940000. running mean: -17.150412\n",
      "ep 2187: ep_len:125 episode reward: total was -12.440000. running mean: -17.103308\n",
      "ep 2187: ep_len:3 episode reward: total was -0.490000. running mean: -16.937175\n",
      "ep 2187: ep_len:520 episode reward: total was -71.420000. running mean: -17.482003\n",
      "ep 2187: ep_len:562 episode reward: total was -0.050000. running mean: -17.307683\n",
      "epsilon:0.102999 episode_count: 15316. steps_count: 6634750.000000\n",
      "Time elapsed:  19280.337959051132\n",
      "ep 2188: ep_len:601 episode reward: total was -121.740000. running mean: -18.352006\n",
      "ep 2188: ep_len:500 episode reward: total was 24.680000. running mean: -17.921686\n",
      "ep 2188: ep_len:593 episode reward: total was -44.040000. running mean: -18.182869\n",
      "ep 2188: ep_len:570 episode reward: total was 46.850000. running mean: -17.532540\n",
      "ep 2188: ep_len:80 episode reward: total was 8.750000. running mean: -17.269715\n",
      "ep 2188: ep_len:607 episode reward: total was 22.360000. running mean: -16.873418\n",
      "ep 2188: ep_len:509 episode reward: total was -52.340000. running mean: -17.228084\n",
      "epsilon:0.102954 episode_count: 15323. steps_count: 6638210.000000\n",
      "Time elapsed:  19289.29525923729\n",
      "ep 2189: ep_len:551 episode reward: total was 29.710000. running mean: -16.758703\n",
      "ep 2189: ep_len:529 episode reward: total was -76.680000. running mean: -17.357916\n",
      "ep 2189: ep_len:542 episode reward: total was -38.210000. running mean: -17.566437\n",
      "ep 2189: ep_len:132 episode reward: total was 10.570000. running mean: -17.285072\n",
      "ep 2189: ep_len:3 episode reward: total was 1.010000. running mean: -17.102121\n",
      "ep 2189: ep_len:305 episode reward: total was 12.520000. running mean: -16.805900\n",
      "ep 2189: ep_len:500 episode reward: total was -86.870000. running mean: -17.506541\n",
      "epsilon:0.102910 episode_count: 15330. steps_count: 6640772.000000\n",
      "Time elapsed:  19296.463879346848\n",
      "ep 2190: ep_len:500 episode reward: total was 23.550000. running mean: -17.095976\n",
      "ep 2190: ep_len:502 episode reward: total was -91.710000. running mean: -17.842116\n",
      "ep 2190: ep_len:591 episode reward: total was -58.230000. running mean: -18.245995\n",
      "ep 2190: ep_len:500 episode reward: total was 33.360000. running mean: -17.729935\n",
      "ep 2190: ep_len:3 episode reward: total was 1.010000. running mean: -17.542536\n",
      "ep 2190: ep_len:581 episode reward: total was 12.120000. running mean: -17.245910\n",
      "ep 2190: ep_len:288 episode reward: total was -47.750000. running mean: -17.550951\n",
      "epsilon:0.102866 episode_count: 15337. steps_count: 6643737.000000\n",
      "Time elapsed:  19306.020050764084\n",
      "ep 2191: ep_len:560 episode reward: total was -13.380000. running mean: -17.509242\n",
      "ep 2191: ep_len:500 episode reward: total was -1.170000. running mean: -17.345849\n",
      "ep 2191: ep_len:501 episode reward: total was -2.130000. running mean: -17.193691\n",
      "ep 2191: ep_len:513 episode reward: total was -66.380000. running mean: -17.685554\n",
      "ep 2191: ep_len:47 episode reward: total was 14.500000. running mean: -17.363698\n",
      "ep 2191: ep_len:500 episode reward: total was -27.330000. running mean: -17.463361\n",
      "ep 2191: ep_len:645 episode reward: total was -19.450000. running mean: -17.483228\n",
      "epsilon:0.102821 episode_count: 15344. steps_count: 6647003.000000\n",
      "Time elapsed:  19314.62965321541\n",
      "ep 2192: ep_len:265 episode reward: total was 14.860000. running mean: -17.159795\n",
      "ep 2192: ep_len:526 episode reward: total was -8.590000. running mean: -17.074097\n",
      "ep 2192: ep_len:530 episode reward: total was -83.880000. running mean: -17.742157\n",
      "ep 2192: ep_len:594 episode reward: total was 29.520000. running mean: -17.269535\n",
      "ep 2192: ep_len:3 episode reward: total was 1.010000. running mean: -17.086740\n",
      "ep 2192: ep_len:511 episode reward: total was -6.680000. running mean: -16.982672\n",
      "ep 2192: ep_len:507 episode reward: total was -39.840000. running mean: -17.211245\n",
      "epsilon:0.102777 episode_count: 15351. steps_count: 6649939.000000\n",
      "Time elapsed:  19322.564514160156\n",
      "ep 2193: ep_len:205 episode reward: total was -20.480000. running mean: -17.243933\n",
      "ep 2193: ep_len:500 episode reward: total was -10.690000. running mean: -17.178394\n",
      "ep 2193: ep_len:531 episode reward: total was -4.960000. running mean: -17.056210\n",
      "ep 2193: ep_len:501 episode reward: total was -31.400000. running mean: -17.199648\n",
      "ep 2193: ep_len:90 episode reward: total was 6.680000. running mean: -16.960851\n",
      "ep 2193: ep_len:602 episode reward: total was 1.880000. running mean: -16.772443\n",
      "ep 2193: ep_len:500 episode reward: total was -21.020000. running mean: -16.814918\n",
      "epsilon:0.102733 episode_count: 15358. steps_count: 6652868.000000\n",
      "Time elapsed:  19330.42320084572\n",
      "ep 2194: ep_len:583 episode reward: total was 50.270000. running mean: -16.144069\n",
      "ep 2194: ep_len:506 episode reward: total was -29.130000. running mean: -16.273928\n",
      "ep 2194: ep_len:569 episode reward: total was -73.560000. running mean: -16.846789\n",
      "ep 2194: ep_len:521 episode reward: total was -48.120000. running mean: -17.159521\n",
      "ep 2194: ep_len:3 episode reward: total was 1.010000. running mean: -16.977826\n",
      "ep 2194: ep_len:540 episode reward: total was 27.920000. running mean: -16.528848\n",
      "ep 2194: ep_len:588 episode reward: total was -9.160000. running mean: -16.455159\n",
      "epsilon:0.102688 episode_count: 15365. steps_count: 6656178.000000\n",
      "Time elapsed:  19339.47652554512\n",
      "ep 2195: ep_len:104 episode reward: total was 5.750000. running mean: -16.233108\n",
      "ep 2195: ep_len:500 episode reward: total was 52.810000. running mean: -15.542677\n",
      "ep 2195: ep_len:500 episode reward: total was -18.090000. running mean: -15.568150\n",
      "ep 2195: ep_len:516 episode reward: total was -30.220000. running mean: -15.714668\n",
      "ep 2195: ep_len:56 episode reward: total was -10.500000. running mean: -15.662522\n",
      "ep 2195: ep_len:556 episode reward: total was 10.090000. running mean: -15.404996\n",
      "ep 2195: ep_len:606 episode reward: total was 8.520000. running mean: -15.165746\n",
      "epsilon:0.102644 episode_count: 15372. steps_count: 6659016.000000\n",
      "Time elapsed:  19349.208796977997\n",
      "ep 2196: ep_len:581 episode reward: total was 40.620000. running mean: -14.607889\n",
      "ep 2196: ep_len:193 episode reward: total was -17.220000. running mean: -14.634010\n",
      "ep 2196: ep_len:575 episode reward: total was -2.030000. running mean: -14.507970\n",
      "ep 2196: ep_len:535 episode reward: total was 20.340000. running mean: -14.159490\n",
      "ep 2196: ep_len:3 episode reward: total was 1.010000. running mean: -14.007795\n",
      "ep 2196: ep_len:189 episode reward: total was 17.700000. running mean: -13.690717\n",
      "ep 2196: ep_len:611 episode reward: total was 33.790000. running mean: -13.215910\n",
      "epsilon:0.102600 episode_count: 15379. steps_count: 6661703.000000\n",
      "Time elapsed:  19356.006808280945\n",
      "ep 2197: ep_len:503 episode reward: total was 26.230000. running mean: -12.821451\n",
      "ep 2197: ep_len:193 episode reward: total was -8.560000. running mean: -12.778837\n",
      "ep 2197: ep_len:444 episode reward: total was 15.420000. running mean: -12.496848\n",
      "ep 2197: ep_len:158 episode reward: total was -13.000000. running mean: -12.501880\n",
      "ep 2197: ep_len:3 episode reward: total was 0.000000. running mean: -12.376861\n",
      "ep 2197: ep_len:500 episode reward: total was -15.960000. running mean: -12.412692\n",
      "ep 2197: ep_len:500 episode reward: total was -7.670000. running mean: -12.365265\n",
      "epsilon:0.102555 episode_count: 15386. steps_count: 6664004.000000\n",
      "Time elapsed:  19362.490988731384\n",
      "ep 2198: ep_len:511 episode reward: total was -66.120000. running mean: -12.902813\n",
      "ep 2198: ep_len:500 episode reward: total was 13.370000. running mean: -12.640085\n",
      "ep 2198: ep_len:637 episode reward: total was -59.420000. running mean: -13.107884\n",
      "ep 2198: ep_len:509 episode reward: total was -4.710000. running mean: -13.023905\n",
      "ep 2198: ep_len:3 episode reward: total was 1.010000. running mean: -12.883566\n",
      "ep 2198: ep_len:155 episode reward: total was 15.450000. running mean: -12.600230\n",
      "ep 2198: ep_len:528 episode reward: total was -5.730000. running mean: -12.531528\n",
      "epsilon:0.102511 episode_count: 15393. steps_count: 6666847.000000\n",
      "Time elapsed:  19378.514659166336\n",
      "ep 2199: ep_len:500 episode reward: total was -12.810000. running mean: -12.534313\n",
      "ep 2199: ep_len:517 episode reward: total was -5.390000. running mean: -12.462870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2199: ep_len:540 episode reward: total was -88.420000. running mean: -13.222441\n",
      "ep 2199: ep_len:500 episode reward: total was -19.480000. running mean: -13.285016\n",
      "ep 2199: ep_len:3 episode reward: total was 1.010000. running mean: -13.142066\n",
      "ep 2199: ep_len:500 episode reward: total was -27.950000. running mean: -13.290146\n",
      "ep 2199: ep_len:500 episode reward: total was 10.720000. running mean: -13.050044\n",
      "epsilon:0.102467 episode_count: 15400. steps_count: 6669907.000000\n",
      "Time elapsed:  19386.76677107811\n",
      "ep 2200: ep_len:205 episode reward: total was 10.700000. running mean: -12.812544\n",
      "ep 2200: ep_len:166 episode reward: total was -20.860000. running mean: -12.893018\n",
      "ep 2200: ep_len:595 episode reward: total was -34.560000. running mean: -13.109688\n",
      "ep 2200: ep_len:510 episode reward: total was 20.100000. running mean: -12.777591\n",
      "ep 2200: ep_len:3 episode reward: total was 1.010000. running mean: -12.639715\n",
      "ep 2200: ep_len:625 episode reward: total was -128.570000. running mean: -13.799018\n",
      "ep 2200: ep_len:176 episode reward: total was -34.010000. running mean: -14.001128\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.102422 episode_count: 15407. steps_count: 6672187.000000\n",
      "Time elapsed:  19397.628616333008\n",
      "ep 2201: ep_len:520 episode reward: total was 34.440000. running mean: -13.516717\n",
      "ep 2201: ep_len:330 episode reward: total was -16.640000. running mean: -13.547950\n",
      "ep 2201: ep_len:542 episode reward: total was -40.130000. running mean: -13.813770\n",
      "ep 2201: ep_len:500 episode reward: total was 14.640000. running mean: -13.529232\n",
      "ep 2201: ep_len:3 episode reward: total was -0.490000. running mean: -13.398840\n",
      "ep 2201: ep_len:291 episode reward: total was -11.040000. running mean: -13.375252\n",
      "ep 2201: ep_len:590 episode reward: total was -4.540000. running mean: -13.286899\n",
      "epsilon:0.102378 episode_count: 15414. steps_count: 6674963.000000\n",
      "Time elapsed:  19401.996465682983\n",
      "ep 2202: ep_len:109 episode reward: total was 1.390000. running mean: -13.140130\n",
      "ep 2202: ep_len:595 episode reward: total was 14.920000. running mean: -12.859529\n",
      "ep 2202: ep_len:367 episode reward: total was -0.690000. running mean: -12.737834\n",
      "ep 2202: ep_len:593 episode reward: total was 15.320000. running mean: -12.457255\n",
      "ep 2202: ep_len:3 episode reward: total was 1.010000. running mean: -12.322583\n",
      "ep 2202: ep_len:612 episode reward: total was -19.530000. running mean: -12.394657\n",
      "ep 2202: ep_len:500 episode reward: total was -32.880000. running mean: -12.599510\n",
      "epsilon:0.102334 episode_count: 15421. steps_count: 6677742.000000\n",
      "Time elapsed:  19408.54695701599\n",
      "ep 2203: ep_len:506 episode reward: total was -64.490000. running mean: -13.118415\n",
      "ep 2203: ep_len:500 episode reward: total was -38.360000. running mean: -13.370831\n",
      "ep 2203: ep_len:663 episode reward: total was -39.600000. running mean: -13.633123\n",
      "ep 2203: ep_len:526 episode reward: total was 1.500000. running mean: -13.481791\n",
      "ep 2203: ep_len:3 episode reward: total was 1.010000. running mean: -13.336874\n",
      "ep 2203: ep_len:500 episode reward: total was -46.000000. running mean: -13.663505\n",
      "ep 2203: ep_len:583 episode reward: total was 4.490000. running mean: -13.481970\n",
      "epsilon:0.102289 episode_count: 15428. steps_count: 6681023.000000\n",
      "Time elapsed:  19420.07420182228\n",
      "ep 2204: ep_len:658 episode reward: total was -123.180000. running mean: -14.578950\n",
      "ep 2204: ep_len:549 episode reward: total was 55.210000. running mean: -13.881061\n",
      "ep 2204: ep_len:613 episode reward: total was -18.260000. running mean: -13.924850\n",
      "ep 2204: ep_len:500 episode reward: total was 15.920000. running mean: -13.626401\n",
      "ep 2204: ep_len:89 episode reward: total was 6.240000. running mean: -13.427737\n",
      "ep 2204: ep_len:586 episode reward: total was 13.330000. running mean: -13.160160\n",
      "ep 2204: ep_len:528 episode reward: total was -56.370000. running mean: -13.592258\n",
      "epsilon:0.102245 episode_count: 15435. steps_count: 6684546.000000\n",
      "Time elapsed:  19429.46425795555\n",
      "ep 2205: ep_len:501 episode reward: total was -42.120000. running mean: -13.877536\n",
      "ep 2205: ep_len:500 episode reward: total was 55.500000. running mean: -13.183761\n",
      "ep 2205: ep_len:552 episode reward: total was -20.690000. running mean: -13.258823\n",
      "ep 2205: ep_len:573 episode reward: total was 67.730000. running mean: -12.448935\n",
      "ep 2205: ep_len:3 episode reward: total was 1.010000. running mean: -12.314345\n",
      "ep 2205: ep_len:574 episode reward: total was -117.490000. running mean: -13.366102\n",
      "ep 2205: ep_len:194 episode reward: total was -51.400000. running mean: -13.746441\n",
      "epsilon:0.102201 episode_count: 15442. steps_count: 6687443.000000\n",
      "Time elapsed:  19439.30281829834\n",
      "ep 2206: ep_len:238 episode reward: total was 12.440000. running mean: -13.484576\n",
      "ep 2206: ep_len:527 episode reward: total was -25.920000. running mean: -13.608931\n",
      "ep 2206: ep_len:501 episode reward: total was 6.310000. running mean: -13.409741\n",
      "ep 2206: ep_len:500 episode reward: total was -24.150000. running mean: -13.517144\n",
      "ep 2206: ep_len:3 episode reward: total was 1.010000. running mean: -13.371873\n",
      "ep 2206: ep_len:529 episode reward: total was 0.510000. running mean: -13.233054\n",
      "ep 2206: ep_len:556 episode reward: total was -1.630000. running mean: -13.117023\n",
      "epsilon:0.102156 episode_count: 15449. steps_count: 6690297.000000\n",
      "Time elapsed:  19443.896990776062\n",
      "ep 2207: ep_len:563 episode reward: total was -69.710000. running mean: -13.682953\n",
      "ep 2207: ep_len:500 episode reward: total was -65.000000. running mean: -14.196124\n",
      "ep 2207: ep_len:562 episode reward: total was -54.930000. running mean: -14.603462\n",
      "ep 2207: ep_len:576 episode reward: total was 35.060000. running mean: -14.106828\n",
      "ep 2207: ep_len:3 episode reward: total was 0.000000. running mean: -13.965759\n",
      "ep 2207: ep_len:504 episode reward: total was 2.980000. running mean: -13.796302\n",
      "ep 2207: ep_len:187 episode reward: total was -22.160000. running mean: -13.879939\n",
      "epsilon:0.102112 episode_count: 15456. steps_count: 6693192.000000\n",
      "Time elapsed:  19451.014354228973\n",
      "ep 2208: ep_len:218 episode reward: total was 12.270000. running mean: -13.618439\n",
      "ep 2208: ep_len:500 episode reward: total was 28.030000. running mean: -13.201955\n",
      "ep 2208: ep_len:613 episode reward: total was -57.480000. running mean: -13.644735\n",
      "ep 2208: ep_len:503 episode reward: total was 43.550000. running mean: -13.072788\n",
      "ep 2208: ep_len:3 episode reward: total was 1.010000. running mean: -12.931960\n",
      "ep 2208: ep_len:524 episode reward: total was -105.540000. running mean: -13.858041\n",
      "ep 2208: ep_len:586 episode reward: total was -93.860000. running mean: -14.658060\n",
      "epsilon:0.102068 episode_count: 15463. steps_count: 6696139.000000\n",
      "Time elapsed:  19458.902203321457\n",
      "ep 2209: ep_len:550 episode reward: total was -78.990000. running mean: -15.301380\n",
      "ep 2209: ep_len:505 episode reward: total was 32.940000. running mean: -14.818966\n",
      "ep 2209: ep_len:79 episode reward: total was 0.780000. running mean: -14.662976\n",
      "ep 2209: ep_len:574 episode reward: total was 14.650000. running mean: -14.369846\n",
      "ep 2209: ep_len:85 episode reward: total was -60.760000. running mean: -14.833748\n",
      "ep 2209: ep_len:591 episode reward: total was -18.970000. running mean: -14.875110\n",
      "ep 2209: ep_len:508 episode reward: total was -6.180000. running mean: -14.788159\n",
      "epsilon:0.102023 episode_count: 15470. steps_count: 6699031.000000\n",
      "Time elapsed:  19466.537553071976\n",
      "ep 2210: ep_len:500 episode reward: total was 8.730000. running mean: -14.552978\n",
      "ep 2210: ep_len:500 episode reward: total was 11.510000. running mean: -14.292348\n",
      "ep 2210: ep_len:500 episode reward: total was -17.160000. running mean: -14.321024\n",
      "ep 2210: ep_len:516 episode reward: total was 23.360000. running mean: -13.944214\n",
      "ep 2210: ep_len:54 episode reward: total was 22.500000. running mean: -13.579772\n",
      "ep 2210: ep_len:552 episode reward: total was 7.690000. running mean: -13.367074\n",
      "ep 2210: ep_len:597 episode reward: total was 12.490000. running mean: -13.108504\n",
      "epsilon:0.101979 episode_count: 15477. steps_count: 6702250.000000\n",
      "Time elapsed:  19475.146629571915\n",
      "ep 2211: ep_len:657 episode reward: total was -28.670000. running mean: -13.264119\n",
      "ep 2211: ep_len:507 episode reward: total was -25.740000. running mean: -13.388877\n",
      "ep 2211: ep_len:568 episode reward: total was -65.580000. running mean: -13.910789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2211: ep_len:514 episode reward: total was -129.030000. running mean: -15.061981\n",
      "ep 2211: ep_len:3 episode reward: total was 1.010000. running mean: -14.901261\n",
      "ep 2211: ep_len:601 episode reward: total was 4.820000. running mean: -14.704048\n",
      "ep 2211: ep_len:515 episode reward: total was -2.740000. running mean: -14.584408\n",
      "epsilon:0.101935 episode_count: 15484. steps_count: 6705615.000000\n",
      "Time elapsed:  19484.084535121918\n",
      "ep 2212: ep_len:587 episode reward: total was -50.400000. running mean: -14.942564\n",
      "ep 2212: ep_len:578 episode reward: total was -7.760000. running mean: -14.870738\n",
      "ep 2212: ep_len:675 episode reward: total was -59.500000. running mean: -15.317031\n",
      "ep 2212: ep_len:162 episode reward: total was 3.670000. running mean: -15.127160\n",
      "ep 2212: ep_len:87 episode reward: total was 14.700000. running mean: -14.828889\n",
      "ep 2212: ep_len:531 episode reward: total was -23.720000. running mean: -14.917800\n",
      "ep 2212: ep_len:530 episode reward: total was -64.950000. running mean: -15.418122\n",
      "epsilon:0.101890 episode_count: 15491. steps_count: 6708765.000000\n",
      "Time elapsed:  19495.971044778824\n",
      "ep 2213: ep_len:125 episode reward: total was 2.500000. running mean: -15.238941\n",
      "ep 2213: ep_len:506 episode reward: total was 0.480000. running mean: -15.081751\n",
      "ep 2213: ep_len:521 episode reward: total was -11.300000. running mean: -15.043934\n",
      "ep 2213: ep_len:537 episode reward: total was 37.420000. running mean: -14.519294\n",
      "ep 2213: ep_len:96 episode reward: total was 8.270000. running mean: -14.291402\n",
      "ep 2213: ep_len:617 episode reward: total was -79.450000. running mean: -14.942988\n",
      "ep 2213: ep_len:623 episode reward: total was -34.060000. running mean: -15.134158\n",
      "epsilon:0.101846 episode_count: 15498. steps_count: 6711790.000000\n",
      "Time elapsed:  19504.140181303024\n",
      "ep 2214: ep_len:249 episode reward: total was 11.330000. running mean: -14.869516\n",
      "ep 2214: ep_len:266 episode reward: total was -25.040000. running mean: -14.971221\n",
      "ep 2214: ep_len:79 episode reward: total was -3.720000. running mean: -14.858709\n",
      "ep 2214: ep_len:500 episode reward: total was 1.490000. running mean: -14.695222\n",
      "ep 2214: ep_len:3 episode reward: total was 1.010000. running mean: -14.538169\n",
      "ep 2214: ep_len:519 episode reward: total was -114.020000. running mean: -15.532988\n",
      "ep 2214: ep_len:551 episode reward: total was -3.210000. running mean: -15.409758\n",
      "epsilon:0.101802 episode_count: 15505. steps_count: 6713957.000000\n",
      "Time elapsed:  19510.252548456192\n",
      "ep 2215: ep_len:526 episode reward: total was -5.220000. running mean: -15.307860\n",
      "ep 2215: ep_len:549 episode reward: total was 27.580000. running mean: -14.878982\n",
      "ep 2215: ep_len:589 episode reward: total was -77.470000. running mean: -15.504892\n",
      "ep 2215: ep_len:37 episode reward: total was 4.150000. running mean: -15.308343\n",
      "ep 2215: ep_len:53 episode reward: total was 21.510000. running mean: -14.940159\n",
      "ep 2215: ep_len:533 episode reward: total was -50.110000. running mean: -15.291858\n",
      "ep 2215: ep_len:542 episode reward: total was -45.280000. running mean: -15.591739\n",
      "epsilon:0.101757 episode_count: 15512. steps_count: 6716786.000000\n",
      "Time elapsed:  19517.907732009888\n",
      "ep 2216: ep_len:247 episode reward: total was 8.650000. running mean: -15.349322\n",
      "ep 2216: ep_len:500 episode reward: total was -17.490000. running mean: -15.370729\n",
      "ep 2216: ep_len:500 episode reward: total was -0.970000. running mean: -15.226721\n",
      "ep 2216: ep_len:588 episode reward: total was 27.830000. running mean: -14.796154\n",
      "ep 2216: ep_len:119 episode reward: total was -24.620000. running mean: -14.894393\n",
      "ep 2216: ep_len:569 episode reward: total was -20.500000. running mean: -14.950449\n",
      "ep 2216: ep_len:500 episode reward: total was -6.810000. running mean: -14.869044\n",
      "epsilon:0.101713 episode_count: 15519. steps_count: 6719809.000000\n",
      "Time elapsed:  19526.66548848152\n",
      "ep 2217: ep_len:503 episode reward: total was -58.410000. running mean: -15.304454\n",
      "ep 2217: ep_len:500 episode reward: total was -14.350000. running mean: -15.294909\n",
      "ep 2217: ep_len:647 episode reward: total was -28.000000. running mean: -15.421960\n",
      "ep 2217: ep_len:129 episode reward: total was 3.550000. running mean: -15.232241\n",
      "ep 2217: ep_len:3 episode reward: total was 1.010000. running mean: -15.069818\n",
      "ep 2217: ep_len:558 episode reward: total was -33.510000. running mean: -15.254220\n",
      "ep 2217: ep_len:541 episode reward: total was 7.190000. running mean: -15.029778\n",
      "epsilon:0.101669 episode_count: 15526. steps_count: 6722690.000000\n",
      "Time elapsed:  19537.406053304672\n",
      "ep 2218: ep_len:601 episode reward: total was 0.880000. running mean: -14.870680\n",
      "ep 2218: ep_len:586 episode reward: total was -4.890000. running mean: -14.770873\n",
      "ep 2218: ep_len:578 episode reward: total was -4.940000. running mean: -14.672564\n",
      "ep 2218: ep_len:500 episode reward: total was -15.120000. running mean: -14.677039\n",
      "ep 2218: ep_len:3 episode reward: total was -1.500000. running mean: -14.545268\n",
      "ep 2218: ep_len:509 episode reward: total was -19.200000. running mean: -14.591816\n",
      "ep 2218: ep_len:560 episode reward: total was -73.310000. running mean: -15.178998\n",
      "epsilon:0.101624 episode_count: 15533. steps_count: 6726027.000000\n",
      "Time elapsed:  19546.241300344467\n",
      "ep 2219: ep_len:224 episode reward: total was 2.720000. running mean: -15.000008\n",
      "ep 2219: ep_len:500 episode reward: total was -16.100000. running mean: -15.011008\n",
      "ep 2219: ep_len:336 episode reward: total was -43.780000. running mean: -15.298697\n",
      "ep 2219: ep_len:544 episode reward: total was 34.420000. running mean: -14.801510\n",
      "ep 2219: ep_len:3 episode reward: total was -0.490000. running mean: -14.658395\n",
      "ep 2219: ep_len:550 episode reward: total was -107.580000. running mean: -15.587611\n",
      "ep 2219: ep_len:521 episode reward: total was -3.920000. running mean: -15.470935\n",
      "epsilon:0.101580 episode_count: 15540. steps_count: 6728705.000000\n",
      "Time elapsed:  19553.46276283264\n",
      "ep 2220: ep_len:500 episode reward: total was 29.490000. running mean: -15.021326\n",
      "ep 2220: ep_len:500 episode reward: total was 61.830000. running mean: -14.252813\n",
      "ep 2220: ep_len:50 episode reward: total was -5.850000. running mean: -14.168785\n",
      "ep 2220: ep_len:514 episode reward: total was -19.360000. running mean: -14.220697\n",
      "ep 2220: ep_len:3 episode reward: total was 0.000000. running mean: -14.078490\n",
      "ep 2220: ep_len:585 episode reward: total was -48.740000. running mean: -14.425105\n",
      "ep 2220: ep_len:500 episode reward: total was -8.730000. running mean: -14.368154\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.101536 episode_count: 15547. steps_count: 6731357.000000\n",
      "Time elapsed:  19565.67425918579\n",
      "ep 2221: ep_len:570 episode reward: total was 12.950000. running mean: -14.094972\n",
      "ep 2221: ep_len:280 episode reward: total was 9.750000. running mean: -13.856523\n",
      "ep 2221: ep_len:445 episode reward: total was 19.190000. running mean: -13.526057\n",
      "ep 2221: ep_len:500 episode reward: total was 34.860000. running mean: -13.042197\n",
      "ep 2221: ep_len:109 episode reward: total was 22.760000. running mean: -12.684175\n",
      "ep 2221: ep_len:664 episode reward: total was -36.740000. running mean: -12.924733\n",
      "ep 2221: ep_len:508 episode reward: total was -11.870000. running mean: -12.914186\n",
      "epsilon:0.101491 episode_count: 15554. steps_count: 6734433.000000\n",
      "Time elapsed:  19578.341352939606\n",
      "ep 2222: ep_len:557 episode reward: total was -56.580000. running mean: -13.350844\n",
      "ep 2222: ep_len:500 episode reward: total was -47.060000. running mean: -13.687935\n",
      "ep 2222: ep_len:598 episode reward: total was -201.330000. running mean: -15.564356\n",
      "ep 2222: ep_len:504 episode reward: total was -1.430000. running mean: -15.423012\n",
      "ep 2222: ep_len:3 episode reward: total was 1.010000. running mean: -15.258682\n",
      "ep 2222: ep_len:619 episode reward: total was -48.430000. running mean: -15.590396\n",
      "ep 2222: ep_len:342 episode reward: total was -26.620000. running mean: -15.700692\n",
      "epsilon:0.101447 episode_count: 15561. steps_count: 6737556.000000\n",
      "Time elapsed:  19586.492130041122\n",
      "ep 2223: ep_len:614 episode reward: total was -63.290000. running mean: -16.176585\n",
      "ep 2223: ep_len:504 episode reward: total was -23.850000. running mean: -16.253319\n",
      "ep 2223: ep_len:500 episode reward: total was 5.780000. running mean: -16.032986\n",
      "ep 2223: ep_len:585 episode reward: total was -22.420000. running mean: -16.096856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2223: ep_len:77 episode reward: total was -5.270000. running mean: -15.988587\n",
      "ep 2223: ep_len:500 episode reward: total was -25.360000. running mean: -16.082301\n",
      "ep 2223: ep_len:278 episode reward: total was -16.350000. running mean: -16.084978\n",
      "epsilon:0.101403 episode_count: 15568. steps_count: 6740614.000000\n",
      "Time elapsed:  19597.43798804283\n",
      "ep 2224: ep_len:500 episode reward: total was -52.840000. running mean: -16.452529\n",
      "ep 2224: ep_len:660 episode reward: total was -0.570000. running mean: -16.293703\n",
      "ep 2224: ep_len:558 episode reward: total was -63.640000. running mean: -16.767166\n",
      "ep 2224: ep_len:56 episode reward: total was 0.300000. running mean: -16.596495\n",
      "ep 2224: ep_len:3 episode reward: total was 1.010000. running mean: -16.420430\n",
      "ep 2224: ep_len:512 episode reward: total was -66.470000. running mean: -16.920925\n",
      "ep 2224: ep_len:568 episode reward: total was 3.010000. running mean: -16.721616\n",
      "epsilon:0.101358 episode_count: 15575. steps_count: 6743471.000000\n",
      "Time elapsed:  19605.264421463013\n",
      "ep 2225: ep_len:500 episode reward: total was -73.850000. running mean: -17.292900\n",
      "ep 2225: ep_len:605 episode reward: total was 5.500000. running mean: -17.064971\n",
      "ep 2225: ep_len:448 episode reward: total was -0.240000. running mean: -16.896721\n",
      "ep 2225: ep_len:518 episode reward: total was -20.890000. running mean: -16.936654\n",
      "ep 2225: ep_len:3 episode reward: total was 1.010000. running mean: -16.757187\n",
      "ep 2225: ep_len:500 episode reward: total was -14.430000. running mean: -16.733916\n",
      "ep 2225: ep_len:518 episode reward: total was -6.380000. running mean: -16.630376\n",
      "epsilon:0.101314 episode_count: 15582. steps_count: 6746563.000000\n",
      "Time elapsed:  19616.36580348015\n",
      "ep 2226: ep_len:605 episode reward: total was -79.120000. running mean: -17.255273\n",
      "ep 2226: ep_len:510 episode reward: total was 40.350000. running mean: -16.679220\n",
      "ep 2226: ep_len:529 episode reward: total was -41.960000. running mean: -16.932028\n",
      "ep 2226: ep_len:500 episode reward: total was 1.260000. running mean: -16.750107\n",
      "ep 2226: ep_len:3 episode reward: total was 1.010000. running mean: -16.572506\n",
      "ep 2226: ep_len:500 episode reward: total was -23.770000. running mean: -16.644481\n",
      "ep 2226: ep_len:184 episode reward: total was -26.860000. running mean: -16.746637\n",
      "epsilon:0.101270 episode_count: 15589. steps_count: 6749394.000000\n",
      "Time elapsed:  19626.82001066208\n",
      "ep 2227: ep_len:652 episode reward: total was -60.030000. running mean: -17.179470\n",
      "ep 2227: ep_len:195 episode reward: total was 4.870000. running mean: -16.958975\n",
      "ep 2227: ep_len:538 episode reward: total was -18.260000. running mean: -16.971986\n",
      "ep 2227: ep_len:132 episode reward: total was 3.550000. running mean: -16.766766\n",
      "ep 2227: ep_len:49 episode reward: total was 20.000000. running mean: -16.399098\n",
      "ep 2227: ep_len:556 episode reward: total was -5.800000. running mean: -16.293107\n",
      "ep 2227: ep_len:500 episode reward: total was -88.030000. running mean: -17.010476\n",
      "epsilon:0.101225 episode_count: 15596. steps_count: 6752016.000000\n",
      "Time elapsed:  19633.854094982147\n",
      "ep 2228: ep_len:502 episode reward: total was -11.920000. running mean: -16.959571\n",
      "ep 2228: ep_len:531 episode reward: total was 23.920000. running mean: -16.550776\n",
      "ep 2228: ep_len:658 episode reward: total was -39.090000. running mean: -16.776168\n",
      "ep 2228: ep_len:170 episode reward: total was 8.660000. running mean: -16.521806\n",
      "ep 2228: ep_len:3 episode reward: total was 1.010000. running mean: -16.346488\n",
      "ep 2228: ep_len:500 episode reward: total was -33.880000. running mean: -16.521823\n",
      "ep 2228: ep_len:574 episode reward: total was -8.490000. running mean: -16.441505\n",
      "epsilon:0.101181 episode_count: 15603. steps_count: 6754954.000000\n",
      "Time elapsed:  19644.436818122864\n",
      "ep 2229: ep_len:572 episode reward: total was 17.680000. running mean: -16.100290\n",
      "ep 2229: ep_len:518 episode reward: total was -65.930000. running mean: -16.598587\n",
      "ep 2229: ep_len:506 episode reward: total was -19.840000. running mean: -16.631001\n",
      "ep 2229: ep_len:165 episode reward: total was 14.670000. running mean: -16.317991\n",
      "ep 2229: ep_len:90 episode reward: total was 26.270000. running mean: -15.892111\n",
      "ep 2229: ep_len:500 episode reward: total was -25.890000. running mean: -15.992090\n",
      "ep 2229: ep_len:501 episode reward: total was -13.910000. running mean: -15.971269\n",
      "epsilon:0.101137 episode_count: 15610. steps_count: 6757806.000000\n",
      "Time elapsed:  19651.990472316742\n",
      "ep 2230: ep_len:692 episode reward: total was -38.300000. running mean: -16.194557\n",
      "ep 2230: ep_len:588 episode reward: total was -11.510000. running mean: -16.147711\n",
      "ep 2230: ep_len:501 episode reward: total was -33.780000. running mean: -16.324034\n",
      "ep 2230: ep_len:607 episode reward: total was 24.420000. running mean: -15.916594\n",
      "ep 2230: ep_len:3 episode reward: total was 1.010000. running mean: -15.747328\n",
      "ep 2230: ep_len:152 episode reward: total was 26.010000. running mean: -15.329754\n",
      "ep 2230: ep_len:308 episode reward: total was -1.110000. running mean: -15.187557\n",
      "epsilon:0.101092 episode_count: 15617. steps_count: 6760657.000000\n",
      "Time elapsed:  19659.970219135284\n",
      "ep 2231: ep_len:644 episode reward: total was -67.180000. running mean: -15.707481\n",
      "ep 2231: ep_len:607 episode reward: total was -20.750000. running mean: -15.757906\n",
      "ep 2231: ep_len:613 episode reward: total was -37.590000. running mean: -15.976227\n",
      "ep 2231: ep_len:589 episode reward: total was 23.110000. running mean: -15.585365\n",
      "ep 2231: ep_len:3 episode reward: total was 0.000000. running mean: -15.429511\n",
      "ep 2231: ep_len:529 episode reward: total was -54.540000. running mean: -15.820616\n",
      "ep 2231: ep_len:513 episode reward: total was -31.570000. running mean: -15.978110\n",
      "epsilon:0.101048 episode_count: 15624. steps_count: 6764155.000000\n",
      "Time elapsed:  19669.1708176136\n",
      "ep 2232: ep_len:527 episode reward: total was -7.230000. running mean: -15.890629\n",
      "ep 2232: ep_len:543 episode reward: total was -69.790000. running mean: -16.429623\n",
      "ep 2232: ep_len:404 episode reward: total was 9.400000. running mean: -16.171327\n",
      "ep 2232: ep_len:528 episode reward: total was -53.960000. running mean: -16.549213\n",
      "ep 2232: ep_len:3 episode reward: total was -0.490000. running mean: -16.388621\n",
      "ep 2232: ep_len:569 episode reward: total was 18.150000. running mean: -16.043235\n",
      "ep 2232: ep_len:579 episode reward: total was -19.700000. running mean: -16.079803\n",
      "epsilon:0.101004 episode_count: 15631. steps_count: 6767308.000000\n",
      "Time elapsed:  19677.50535964966\n",
      "ep 2233: ep_len:214 episode reward: total was 17.190000. running mean: -15.747105\n",
      "ep 2233: ep_len:351 episode reward: total was -2.740000. running mean: -15.617034\n",
      "ep 2233: ep_len:500 episode reward: total was 16.670000. running mean: -15.294163\n",
      "ep 2233: ep_len:600 episode reward: total was -3.720000. running mean: -15.178422\n",
      "ep 2233: ep_len:3 episode reward: total was 1.010000. running mean: -15.016537\n",
      "ep 2233: ep_len:612 episode reward: total was -24.530000. running mean: -15.111672\n",
      "ep 2233: ep_len:608 episode reward: total was -17.340000. running mean: -15.133955\n",
      "epsilon:0.100959 episode_count: 15638. steps_count: 6770196.000000\n",
      "Time elapsed:  19685.263855695724\n",
      "ep 2234: ep_len:502 episode reward: total was -96.420000. running mean: -15.946816\n",
      "ep 2234: ep_len:618 episode reward: total was -117.260000. running mean: -16.959948\n",
      "ep 2234: ep_len:628 episode reward: total was -38.210000. running mean: -17.172448\n",
      "ep 2234: ep_len:528 episode reward: total was 27.600000. running mean: -16.724724\n",
      "ep 2234: ep_len:3 episode reward: total was 1.010000. running mean: -16.547376\n",
      "ep 2234: ep_len:548 episode reward: total was -21.550000. running mean: -16.597403\n",
      "ep 2234: ep_len:500 episode reward: total was -2.680000. running mean: -16.458229\n",
      "epsilon:0.100915 episode_count: 15645. steps_count: 6773523.000000\n",
      "Time elapsed:  19694.17233467102\n",
      "ep 2235: ep_len:500 episode reward: total was -27.780000. running mean: -16.571446\n",
      "ep 2235: ep_len:500 episode reward: total was 18.900000. running mean: -16.216732\n",
      "ep 2235: ep_len:562 episode reward: total was -30.220000. running mean: -16.356764\n",
      "ep 2235: ep_len:501 episode reward: total was 59.520000. running mean: -15.597997\n",
      "ep 2235: ep_len:3 episode reward: total was 1.010000. running mean: -15.431917\n",
      "ep 2235: ep_len:522 episode reward: total was -38.830000. running mean: -15.665898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2235: ep_len:508 episode reward: total was -5.470000. running mean: -15.563939\n",
      "epsilon:0.100871 episode_count: 15652. steps_count: 6776619.000000\n",
      "Time elapsed:  19702.088792085648\n",
      "ep 2236: ep_len:229 episode reward: total was 0.720000. running mean: -15.401099\n",
      "ep 2236: ep_len:653 episode reward: total was -130.790000. running mean: -16.554988\n",
      "ep 2236: ep_len:565 episode reward: total was -53.430000. running mean: -16.923738\n",
      "ep 2236: ep_len:509 episode reward: total was -49.810000. running mean: -17.252601\n",
      "ep 2236: ep_len:3 episode reward: total was 0.000000. running mean: -17.080075\n",
      "ep 2236: ep_len:500 episode reward: total was -21.660000. running mean: -17.125874\n",
      "ep 2236: ep_len:502 episode reward: total was -39.750000. running mean: -17.352116\n",
      "epsilon:0.100826 episode_count: 15659. steps_count: 6779580.000000\n",
      "Time elapsed:  19710.208696365356\n",
      "ep 2237: ep_len:602 episode reward: total was -19.900000. running mean: -17.377594\n",
      "ep 2237: ep_len:520 episode reward: total was -48.030000. running mean: -17.684118\n",
      "ep 2237: ep_len:605 episode reward: total was -20.450000. running mean: -17.711777\n",
      "ep 2237: ep_len:500 episode reward: total was 27.520000. running mean: -17.259460\n",
      "ep 2237: ep_len:3 episode reward: total was 1.010000. running mean: -17.076765\n",
      "ep 2237: ep_len:500 episode reward: total was -15.240000. running mean: -17.058397\n",
      "ep 2237: ep_len:568 episode reward: total was -1.490000. running mean: -16.902713\n",
      "epsilon:0.100782 episode_count: 15666. steps_count: 6782878.000000\n",
      "Time elapsed:  19717.77259516716\n",
      "ep 2238: ep_len:500 episode reward: total was 6.780000. running mean: -16.665886\n",
      "ep 2238: ep_len:565 episode reward: total was -4.950000. running mean: -16.548727\n",
      "ep 2238: ep_len:612 episode reward: total was -146.400000. running mean: -17.847240\n",
      "ep 2238: ep_len:500 episode reward: total was -92.610000. running mean: -18.594868\n",
      "ep 2238: ep_len:105 episode reward: total was 22.690000. running mean: -18.182019\n",
      "ep 2238: ep_len:679 episode reward: total was -33.620000. running mean: -18.336399\n",
      "ep 2238: ep_len:531 episode reward: total was -43.940000. running mean: -18.592435\n",
      "epsilon:0.100738 episode_count: 15673. steps_count: 6786370.000000\n",
      "Time elapsed:  19726.924243450165\n",
      "ep 2239: ep_len:107 episode reward: total was -12.040000. running mean: -18.526910\n",
      "ep 2239: ep_len:500 episode reward: total was 11.140000. running mean: -18.230241\n",
      "ep 2239: ep_len:640 episode reward: total was -45.470000. running mean: -18.502639\n",
      "ep 2239: ep_len:541 episode reward: total was 22.570000. running mean: -18.091913\n",
      "ep 2239: ep_len:128 episode reward: total was 25.370000. running mean: -17.657293\n",
      "ep 2239: ep_len:329 episode reward: total was -13.930000. running mean: -17.620020\n",
      "ep 2239: ep_len:500 episode reward: total was -40.090000. running mean: -17.844720\n",
      "epsilon:0.100693 episode_count: 15680. steps_count: 6789115.000000\n",
      "Time elapsed:  19734.274258613586\n",
      "ep 2240: ep_len:554 episode reward: total was -6.580000. running mean: -17.732073\n",
      "ep 2240: ep_len:500 episode reward: total was -55.850000. running mean: -18.113252\n",
      "ep 2240: ep_len:539 episode reward: total was -15.150000. running mean: -18.083620\n",
      "ep 2240: ep_len:504 episode reward: total was 0.650000. running mean: -17.896284\n",
      "ep 2240: ep_len:3 episode reward: total was 1.010000. running mean: -17.707221\n",
      "ep 2240: ep_len:518 episode reward: total was -38.710000. running mean: -17.917249\n",
      "ep 2240: ep_len:521 episode reward: total was -29.300000. running mean: -18.031076\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.100649 episode_count: 15687. steps_count: 6792254.000000\n",
      "Time elapsed:  19747.31133747101\n",
      "ep 2241: ep_len:592 episode reward: total was -4.490000. running mean: -17.895665\n",
      "ep 2241: ep_len:500 episode reward: total was 4.190000. running mean: -17.674809\n",
      "ep 2241: ep_len:624 episode reward: total was -89.070000. running mean: -18.388761\n",
      "ep 2241: ep_len:421 episode reward: total was 1.490000. running mean: -18.189973\n",
      "ep 2241: ep_len:108 episode reward: total was -4.270000. running mean: -18.050773\n",
      "ep 2241: ep_len:230 episode reward: total was 27.790000. running mean: -17.592366\n",
      "ep 2241: ep_len:523 episode reward: total was -34.440000. running mean: -17.760842\n",
      "epsilon:0.100605 episode_count: 15694. steps_count: 6795252.000000\n",
      "Time elapsed:  19755.974816560745\n",
      "ep 2242: ep_len:500 episode reward: total was -21.580000. running mean: -17.799033\n",
      "ep 2242: ep_len:558 episode reward: total was -13.390000. running mean: -17.754943\n",
      "ep 2242: ep_len:562 episode reward: total was -41.980000. running mean: -17.997194\n",
      "ep 2242: ep_len:503 episode reward: total was -1.900000. running mean: -17.836222\n",
      "ep 2242: ep_len:3 episode reward: total was 1.010000. running mean: -17.647760\n",
      "ep 2242: ep_len:582 episode reward: total was -50.600000. running mean: -17.977282\n",
      "ep 2242: ep_len:599 episode reward: total was -19.870000. running mean: -17.996209\n",
      "epsilon:0.100560 episode_count: 15701. steps_count: 6798559.000000\n",
      "Time elapsed:  19764.688881874084\n",
      "ep 2243: ep_len:571 episode reward: total was 29.150000. running mean: -17.524747\n",
      "ep 2243: ep_len:500 episode reward: total was -33.890000. running mean: -17.688400\n",
      "ep 2243: ep_len:500 episode reward: total was -19.370000. running mean: -17.705216\n",
      "ep 2243: ep_len:500 episode reward: total was 12.510000. running mean: -17.403063\n",
      "ep 2243: ep_len:105 episode reward: total was 24.220000. running mean: -16.986833\n",
      "ep 2243: ep_len:513 episode reward: total was -29.960000. running mean: -17.116564\n",
      "ep 2243: ep_len:211 episode reward: total was -30.200000. running mean: -17.247399\n",
      "epsilon:0.100516 episode_count: 15708. steps_count: 6801459.000000\n",
      "Time elapsed:  19774.408444166183\n",
      "ep 2244: ep_len:500 episode reward: total was -7.330000. running mean: -17.148225\n",
      "ep 2244: ep_len:540 episode reward: total was 58.520000. running mean: -16.391543\n",
      "ep 2244: ep_len:367 episode reward: total was -51.220000. running mean: -16.739827\n",
      "ep 2244: ep_len:90 episode reward: total was 2.980000. running mean: -16.542629\n",
      "ep 2244: ep_len:50 episode reward: total was 20.500000. running mean: -16.172203\n",
      "ep 2244: ep_len:671 episode reward: total was 4.690000. running mean: -15.963581\n",
      "ep 2244: ep_len:565 episode reward: total was -2.620000. running mean: -15.830145\n",
      "epsilon:0.100472 episode_count: 15715. steps_count: 6804242.000000\n",
      "Time elapsed:  19781.99558019638\n",
      "ep 2245: ep_len:580 episode reward: total was 4.780000. running mean: -15.624043\n",
      "ep 2245: ep_len:378 episode reward: total was -28.580000. running mean: -15.753603\n",
      "ep 2245: ep_len:500 episode reward: total was -38.930000. running mean: -15.985367\n",
      "ep 2245: ep_len:582 episode reward: total was 16.390000. running mean: -15.661613\n",
      "ep 2245: ep_len:74 episode reward: total was 10.190000. running mean: -15.403097\n",
      "ep 2245: ep_len:500 episode reward: total was -0.290000. running mean: -15.251966\n",
      "ep 2245: ep_len:500 episode reward: total was -38.220000. running mean: -15.481646\n",
      "epsilon:0.100427 episode_count: 15722. steps_count: 6807356.000000\n",
      "Time elapsed:  19790.756034612656\n",
      "ep 2246: ep_len:569 episode reward: total was 18.600000. running mean: -15.140830\n",
      "ep 2246: ep_len:528 episode reward: total was 28.890000. running mean: -14.700522\n",
      "ep 2246: ep_len:631 episode reward: total was -62.850000. running mean: -15.182016\n",
      "ep 2246: ep_len:500 episode reward: total was -7.590000. running mean: -15.106096\n",
      "ep 2246: ep_len:3 episode reward: total was 1.010000. running mean: -14.944935\n",
      "ep 2246: ep_len:501 episode reward: total was -7.380000. running mean: -14.869286\n",
      "ep 2246: ep_len:589 episode reward: total was -36.820000. running mean: -15.088793\n",
      "epsilon:0.100383 episode_count: 15729. steps_count: 6810677.000000\n",
      "Time elapsed:  19802.420521736145\n",
      "ep 2247: ep_len:633 episode reward: total was 18.560000. running mean: -14.752305\n",
      "ep 2247: ep_len:558 episode reward: total was 39.210000. running mean: -14.212682\n",
      "ep 2247: ep_len:536 episode reward: total was -38.690000. running mean: -14.457455\n",
      "ep 2247: ep_len:524 episode reward: total was -35.180000. running mean: -14.664681\n",
      "ep 2247: ep_len:54 episode reward: total was 24.000000. running mean: -14.278034\n",
      "ep 2247: ep_len:507 episode reward: total was -23.670000. running mean: -14.371954\n",
      "ep 2247: ep_len:629 episode reward: total was -41.020000. running mean: -14.638434\n",
      "epsilon:0.100339 episode_count: 15736. steps_count: 6814118.000000\n",
      "Time elapsed:  19811.401218652725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2248: ep_len:509 episode reward: total was -14.420000. running mean: -14.636250\n",
      "ep 2248: ep_len:535 episode reward: total was -34.190000. running mean: -14.831787\n",
      "ep 2248: ep_len:564 episode reward: total was 3.520000. running mean: -14.648269\n",
      "ep 2248: ep_len:500 episode reward: total was -7.100000. running mean: -14.572787\n",
      "ep 2248: ep_len:93 episode reward: total was -14.280000. running mean: -14.569859\n",
      "ep 2248: ep_len:541 episode reward: total was -19.150000. running mean: -14.615660\n",
      "ep 2248: ep_len:620 episode reward: total was -35.100000. running mean: -14.820504\n",
      "epsilon:0.100294 episode_count: 15743. steps_count: 6817480.000000\n",
      "Time elapsed:  19820.480558156967\n",
      "ep 2249: ep_len:237 episode reward: total was 5.340000. running mean: -14.618899\n",
      "ep 2249: ep_len:647 episode reward: total was 26.130000. running mean: -14.211410\n",
      "ep 2249: ep_len:500 episode reward: total was -29.540000. running mean: -14.364695\n",
      "ep 2249: ep_len:56 episode reward: total was 3.330000. running mean: -14.187749\n",
      "ep 2249: ep_len:3 episode reward: total was 1.010000. running mean: -14.035771\n",
      "ep 2249: ep_len:539 episode reward: total was -78.600000. running mean: -14.681413\n",
      "ep 2249: ep_len:526 episode reward: total was 6.460000. running mean: -14.469999\n",
      "epsilon:0.100250 episode_count: 15750. steps_count: 6819988.000000\n",
      "Time elapsed:  19827.388956546783\n",
      "ep 2250: ep_len:524 episode reward: total was 27.820000. running mean: -14.047099\n",
      "ep 2250: ep_len:500 episode reward: total was -22.710000. running mean: -14.133728\n",
      "ep 2250: ep_len:532 episode reward: total was -97.730000. running mean: -14.969691\n",
      "ep 2250: ep_len:500 episode reward: total was -27.150000. running mean: -15.091494\n",
      "ep 2250: ep_len:121 episode reward: total was 20.250000. running mean: -14.738079\n",
      "ep 2250: ep_len:500 episode reward: total was 29.740000. running mean: -14.293298\n",
      "ep 2250: ep_len:649 episode reward: total was -6.340000. running mean: -14.213765\n",
      "epsilon:0.100206 episode_count: 15757. steps_count: 6823314.000000\n",
      "Time elapsed:  19839.096638202667\n",
      "ep 2251: ep_len:500 episode reward: total was 11.930000. running mean: -13.952328\n",
      "ep 2251: ep_len:517 episode reward: total was 15.330000. running mean: -13.659504\n",
      "ep 2251: ep_len:500 episode reward: total was -8.590000. running mean: -13.608809\n",
      "ep 2251: ep_len:519 episode reward: total was -33.350000. running mean: -13.806221\n",
      "ep 2251: ep_len:3 episode reward: total was 0.000000. running mean: -13.668159\n",
      "ep 2251: ep_len:504 episode reward: total was -25.940000. running mean: -13.790877\n",
      "ep 2251: ep_len:510 episode reward: total was -66.620000. running mean: -14.319169\n",
      "epsilon:0.100161 episode_count: 15764. steps_count: 6826367.000000\n",
      "Time elapsed:  19846.98828983307\n",
      "ep 2252: ep_len:573 episode reward: total was 30.810000. running mean: -13.867877\n",
      "ep 2252: ep_len:610 episode reward: total was -4.830000. running mean: -13.777498\n",
      "ep 2252: ep_len:381 episode reward: total was 15.810000. running mean: -13.481623\n",
      "ep 2252: ep_len:500 episode reward: total was -31.680000. running mean: -13.663607\n",
      "ep 2252: ep_len:3 episode reward: total was 1.010000. running mean: -13.516871\n",
      "ep 2252: ep_len:652 episode reward: total was -13.420000. running mean: -13.515902\n",
      "ep 2252: ep_len:501 episode reward: total was -41.810000. running mean: -13.798843\n",
      "epsilon:0.100117 episode_count: 15771. steps_count: 6829587.000000\n",
      "Time elapsed:  19855.462153196335\n",
      "ep 2253: ep_len:208 episode reward: total was 2.660000. running mean: -13.634255\n",
      "ep 2253: ep_len:365 episode reward: total was -17.570000. running mean: -13.673612\n",
      "ep 2253: ep_len:500 episode reward: total was 4.460000. running mean: -13.492276\n",
      "ep 2253: ep_len:508 episode reward: total was -14.210000. running mean: -13.499453\n",
      "ep 2253: ep_len:3 episode reward: total was 1.010000. running mean: -13.354359\n",
      "ep 2253: ep_len:566 episode reward: total was 19.900000. running mean: -13.021815\n",
      "ep 2253: ep_len:554 episode reward: total was 9.510000. running mean: -12.796497\n",
      "epsilon:0.100073 episode_count: 15778. steps_count: 6832291.000000\n",
      "Time elapsed:  19862.79468035698\n",
      "ep 2254: ep_len:606 episode reward: total was -2.040000. running mean: -12.688932\n",
      "ep 2254: ep_len:591 episode reward: total was 17.950000. running mean: -12.382543\n",
      "ep 2254: ep_len:500 episode reward: total was -28.500000. running mean: -12.543717\n",
      "ep 2254: ep_len:410 episode reward: total was -18.520000. running mean: -12.603480\n",
      "ep 2254: ep_len:3 episode reward: total was 1.010000. running mean: -12.467345\n",
      "ep 2254: ep_len:633 episode reward: total was -28.070000. running mean: -12.623372\n",
      "ep 2254: ep_len:501 episode reward: total was -41.120000. running mean: -12.908338\n",
      "epsilon:0.100028 episode_count: 15785. steps_count: 6835535.000000\n",
      "Time elapsed:  19870.09112882614\n",
      "ep 2255: ep_len:546 episode reward: total was 39.220000. running mean: -12.387055\n",
      "ep 2255: ep_len:500 episode reward: total was 31.120000. running mean: -11.951984\n",
      "ep 2255: ep_len:500 episode reward: total was 5.540000. running mean: -11.777064\n",
      "ep 2255: ep_len:121 episode reward: total was 16.460000. running mean: -11.494694\n",
      "ep 2255: ep_len:3 episode reward: total was -1.500000. running mean: -11.394747\n",
      "ep 2255: ep_len:599 episode reward: total was -19.000000. running mean: -11.470799\n",
      "ep 2255: ep_len:282 episode reward: total was -11.880000. running mean: -11.474891\n",
      "epsilon:0.099984 episode_count: 15792. steps_count: 6838086.000000\n",
      "Time elapsed:  19876.780746221542\n",
      "ep 2256: ep_len:500 episode reward: total was 23.310000. running mean: -11.127042\n",
      "ep 2256: ep_len:585 episode reward: total was 22.990000. running mean: -10.785872\n",
      "ep 2256: ep_len:613 episode reward: total was -32.820000. running mean: -11.006213\n",
      "ep 2256: ep_len:588 episode reward: total was 13.240000. running mean: -10.763751\n",
      "ep 2256: ep_len:81 episode reward: total was 15.740000. running mean: -10.498714\n",
      "ep 2256: ep_len:500 episode reward: total was 9.880000. running mean: -10.294927\n",
      "ep 2256: ep_len:345 episode reward: total was -3.970000. running mean: -10.231677\n",
      "epsilon:0.099940 episode_count: 15799. steps_count: 6841298.000000\n",
      "Time elapsed:  19887.675455331802\n",
      "ep 2257: ep_len:541 episode reward: total was 39.350000. running mean: -9.735860\n",
      "ep 2257: ep_len:338 episode reward: total was -82.590000. running mean: -10.464402\n",
      "ep 2257: ep_len:504 episode reward: total was -33.370000. running mean: -10.693458\n",
      "ep 2257: ep_len:500 episode reward: total was 15.300000. running mean: -10.433523\n",
      "ep 2257: ep_len:47 episode reward: total was 16.000000. running mean: -10.169188\n",
      "ep 2257: ep_len:517 episode reward: total was -55.950000. running mean: -10.626996\n",
      "ep 2257: ep_len:544 episode reward: total was -4.960000. running mean: -10.570326\n",
      "epsilon:0.099895 episode_count: 15806. steps_count: 6844289.000000\n",
      "Time elapsed:  19895.596071004868\n",
      "ep 2258: ep_len:216 episode reward: total was -3.850000. running mean: -10.503123\n",
      "ep 2258: ep_len:515 episode reward: total was -15.540000. running mean: -10.553492\n",
      "ep 2258: ep_len:548 episode reward: total was -34.440000. running mean: -10.792357\n",
      "ep 2258: ep_len:481 episode reward: total was -31.710000. running mean: -11.001533\n",
      "ep 2258: ep_len:82 episode reward: total was -30.230000. running mean: -11.193818\n",
      "ep 2258: ep_len:560 episode reward: total was -28.960000. running mean: -11.371480\n",
      "ep 2258: ep_len:500 episode reward: total was -14.180000. running mean: -11.399565\n",
      "epsilon:0.099851 episode_count: 15813. steps_count: 6847191.000000\n",
      "Time elapsed:  19906.13751220703\n",
      "ep 2259: ep_len:651 episode reward: total was -51.450000. running mean: -11.800069\n",
      "ep 2259: ep_len:625 episode reward: total was -19.430000. running mean: -11.876369\n",
      "ep 2259: ep_len:401 episode reward: total was 4.730000. running mean: -11.710305\n",
      "ep 2259: ep_len:500 episode reward: total was -8.970000. running mean: -11.682902\n",
      "ep 2259: ep_len:52 episode reward: total was 14.000000. running mean: -11.426073\n",
      "ep 2259: ep_len:500 episode reward: total was -60.540000. running mean: -11.917212\n",
      "ep 2259: ep_len:591 episode reward: total was -18.190000. running mean: -11.979940\n",
      "epsilon:0.099807 episode_count: 15820. steps_count: 6850511.000000\n",
      "Time elapsed:  19914.91481781006\n",
      "ep 2260: ep_len:501 episode reward: total was -51.020000. running mean: -12.370341\n",
      "ep 2260: ep_len:547 episode reward: total was -57.400000. running mean: -12.820637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2260: ep_len:500 episode reward: total was 4.620000. running mean: -12.646231\n",
      "ep 2260: ep_len:500 episode reward: total was -24.640000. running mean: -12.766168\n",
      "ep 2260: ep_len:3 episode reward: total was 1.010000. running mean: -12.628407\n",
      "ep 2260: ep_len:634 episode reward: total was -14.260000. running mean: -12.644723\n",
      "ep 2260: ep_len:500 episode reward: total was -39.710000. running mean: -12.915376\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.099762 episode_count: 15827. steps_count: 6853696.000000\n",
      "Time elapsed:  19927.968373298645\n",
      "ep 2261: ep_len:122 episode reward: total was -0.100000. running mean: -12.787222\n",
      "ep 2261: ep_len:500 episode reward: total was 56.020000. running mean: -12.099150\n",
      "ep 2261: ep_len:364 episode reward: total was 30.760000. running mean: -11.670558\n",
      "ep 2261: ep_len:507 episode reward: total was 15.860000. running mean: -11.395252\n",
      "ep 2261: ep_len:75 episode reward: total was 13.170000. running mean: -11.149600\n",
      "ep 2261: ep_len:285 episode reward: total was 16.050000. running mean: -10.877604\n",
      "ep 2261: ep_len:275 episode reward: total was -7.050000. running mean: -10.839328\n",
      "epsilon:0.099718 episode_count: 15834. steps_count: 6855824.000000\n",
      "Time elapsed:  19933.92557311058\n",
      "ep 2262: ep_len:562 episode reward: total was 26.210000. running mean: -10.468835\n",
      "ep 2262: ep_len:175 episode reward: total was 1.730000. running mean: -10.346846\n",
      "ep 2262: ep_len:500 episode reward: total was 24.260000. running mean: -10.000778\n",
      "ep 2262: ep_len:153 episode reward: total was 6.150000. running mean: -9.839270\n",
      "ep 2262: ep_len:37 episode reward: total was 14.000000. running mean: -9.600877\n",
      "ep 2262: ep_len:598 episode reward: total was -41.010000. running mean: -9.914969\n",
      "ep 2262: ep_len:559 episode reward: total was -144.770000. running mean: -11.263519\n",
      "epsilon:0.099674 episode_count: 15841. steps_count: 6858408.000000\n",
      "Time elapsed:  19944.218499422073\n",
      "ep 2263: ep_len:501 episode reward: total was 3.280000. running mean: -11.118084\n",
      "ep 2263: ep_len:500 episode reward: total was -10.750000. running mean: -11.114403\n",
      "ep 2263: ep_len:519 episode reward: total was -19.500000. running mean: -11.198259\n",
      "ep 2263: ep_len:500 episode reward: total was -34.220000. running mean: -11.428476\n",
      "ep 2263: ep_len:96 episode reward: total was 17.730000. running mean: -11.136891\n",
      "ep 2263: ep_len:240 episode reward: total was 36.860000. running mean: -10.656923\n",
      "ep 2263: ep_len:509 episode reward: total was -24.660000. running mean: -10.796953\n",
      "epsilon:0.099629 episode_count: 15848. steps_count: 6861273.000000\n",
      "Time elapsed:  19951.90829348564\n",
      "ep 2264: ep_len:596 episode reward: total was 11.330000. running mean: -10.575684\n",
      "ep 2264: ep_len:529 episode reward: total was -10.630000. running mean: -10.576227\n",
      "ep 2264: ep_len:570 episode reward: total was -27.700000. running mean: -10.747465\n",
      "ep 2264: ep_len:510 episode reward: total was -4.970000. running mean: -10.689690\n",
      "ep 2264: ep_len:3 episode reward: total was 1.010000. running mean: -10.572693\n",
      "ep 2264: ep_len:547 episode reward: total was 9.240000. running mean: -10.374566\n",
      "ep 2264: ep_len:594 episode reward: total was -28.200000. running mean: -10.552821\n",
      "epsilon:0.099585 episode_count: 15855. steps_count: 6864622.000000\n",
      "Time elapsed:  19960.95710492134\n",
      "ep 2265: ep_len:500 episode reward: total was -11.030000. running mean: -10.557592\n",
      "ep 2265: ep_len:558 episode reward: total was -12.510000. running mean: -10.577116\n",
      "ep 2265: ep_len:605 episode reward: total was 17.810000. running mean: -10.293245\n",
      "ep 2265: ep_len:544 episode reward: total was -5.180000. running mean: -10.242113\n",
      "ep 2265: ep_len:116 episode reward: total was 10.860000. running mean: -10.031092\n",
      "ep 2265: ep_len:625 episode reward: total was 11.110000. running mean: -9.819681\n",
      "ep 2265: ep_len:313 episode reward: total was -0.460000. running mean: -9.726084\n",
      "epsilon:0.099541 episode_count: 15862. steps_count: 6867883.000000\n",
      "Time elapsed:  19969.438840150833\n",
      "ep 2266: ep_len:229 episode reward: total was -57.830000. running mean: -10.207123\n",
      "ep 2266: ep_len:522 episode reward: total was -65.550000. running mean: -10.760552\n",
      "ep 2266: ep_len:566 episode reward: total was -4.440000. running mean: -10.697346\n",
      "ep 2266: ep_len:513 episode reward: total was 50.300000. running mean: -10.087373\n",
      "ep 2266: ep_len:3 episode reward: total was 1.010000. running mean: -9.976399\n",
      "ep 2266: ep_len:616 episode reward: total was 3.680000. running mean: -9.839835\n",
      "ep 2266: ep_len:584 episode reward: total was -8.800000. running mean: -9.829437\n",
      "epsilon:0.099496 episode_count: 15869. steps_count: 6870916.000000\n",
      "Time elapsed:  19977.77870631218\n",
      "ep 2267: ep_len:568 episode reward: total was -89.980000. running mean: -10.630942\n",
      "ep 2267: ep_len:578 episode reward: total was -15.440000. running mean: -10.679033\n",
      "ep 2267: ep_len:444 episode reward: total was 33.170000. running mean: -10.240543\n",
      "ep 2267: ep_len:500 episode reward: total was 27.480000. running mean: -9.863337\n",
      "ep 2267: ep_len:3 episode reward: total was 1.010000. running mean: -9.754604\n",
      "ep 2267: ep_len:500 episode reward: total was -1.170000. running mean: -9.668758\n",
      "ep 2267: ep_len:560 episode reward: total was -72.390000. running mean: -10.295970\n",
      "epsilon:0.099452 episode_count: 15876. steps_count: 6874069.000000\n",
      "Time elapsed:  19985.853253364563\n",
      "ep 2268: ep_len:128 episode reward: total was 0.850000. running mean: -10.184511\n",
      "ep 2268: ep_len:532 episode reward: total was 42.440000. running mean: -9.658265\n",
      "ep 2268: ep_len:79 episode reward: total was -1.240000. running mean: -9.574083\n",
      "ep 2268: ep_len:500 episode reward: total was 12.060000. running mean: -9.357742\n",
      "ep 2268: ep_len:3 episode reward: total was 1.010000. running mean: -9.254065\n",
      "ep 2268: ep_len:656 episode reward: total was -13.470000. running mean: -9.296224\n",
      "ep 2268: ep_len:621 episode reward: total was -27.870000. running mean: -9.481962\n",
      "epsilon:0.099408 episode_count: 15883. steps_count: 6876588.000000\n",
      "Time elapsed:  19992.78102183342\n",
      "ep 2269: ep_len:134 episode reward: total was -9.100000. running mean: -9.478142\n",
      "ep 2269: ep_len:512 episode reward: total was -11.960000. running mean: -9.502961\n",
      "ep 2269: ep_len:372 episode reward: total was 1.940000. running mean: -9.388531\n",
      "ep 2269: ep_len:500 episode reward: total was -5.940000. running mean: -9.354046\n",
      "ep 2269: ep_len:90 episode reward: total was 7.200000. running mean: -9.188505\n",
      "ep 2269: ep_len:538 episode reward: total was -43.700000. running mean: -9.533620\n",
      "ep 2269: ep_len:607 episode reward: total was -34.800000. running mean: -9.786284\n",
      "epsilon:0.099363 episode_count: 15890. steps_count: 6879341.000000\n",
      "Time elapsed:  20003.194073200226\n",
      "ep 2270: ep_len:500 episode reward: total was 72.740000. running mean: -8.961021\n",
      "ep 2270: ep_len:632 episode reward: total was -38.090000. running mean: -9.252311\n",
      "ep 2270: ep_len:623 episode reward: total was -40.100000. running mean: -9.560788\n",
      "ep 2270: ep_len:132 episode reward: total was 7.600000. running mean: -9.389180\n",
      "ep 2270: ep_len:73 episode reward: total was -39.730000. running mean: -9.692588\n",
      "ep 2270: ep_len:500 episode reward: total was -68.040000. running mean: -10.276062\n",
      "ep 2270: ep_len:590 episode reward: total was -28.190000. running mean: -10.455202\n",
      "epsilon:0.099319 episode_count: 15897. steps_count: 6882391.000000\n",
      "Time elapsed:  20011.31880235672\n",
      "ep 2271: ep_len:522 episode reward: total was -19.610000. running mean: -10.546750\n",
      "ep 2271: ep_len:329 episode reward: total was -10.340000. running mean: -10.544682\n",
      "ep 2271: ep_len:405 episode reward: total was 10.080000. running mean: -10.338435\n",
      "ep 2271: ep_len:500 episode reward: total was -36.240000. running mean: -10.597451\n",
      "ep 2271: ep_len:3 episode reward: total was 1.010000. running mean: -10.481376\n",
      "ep 2271: ep_len:503 episode reward: total was -35.570000. running mean: -10.732263\n",
      "ep 2271: ep_len:527 episode reward: total was -166.210000. running mean: -12.287040\n",
      "epsilon:0.099275 episode_count: 15904. steps_count: 6885180.000000\n",
      "Time elapsed:  20018.76356124878\n",
      "ep 2272: ep_len:506 episode reward: total was -71.750000. running mean: -12.881670\n",
      "ep 2272: ep_len:500 episode reward: total was -6.970000. running mean: -12.822553\n",
      "ep 2272: ep_len:505 episode reward: total was -34.140000. running mean: -13.035727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2272: ep_len:500 episode reward: total was 50.750000. running mean: -12.397870\n",
      "ep 2272: ep_len:3 episode reward: total was 1.010000. running mean: -12.263791\n",
      "ep 2272: ep_len:647 episode reward: total was -24.070000. running mean: -12.381854\n",
      "ep 2272: ep_len:240 episode reward: total was -53.170000. running mean: -12.789735\n",
      "epsilon:0.099230 episode_count: 15911. steps_count: 6888081.000000\n",
      "Time elapsed:  20026.63387131691\n",
      "ep 2273: ep_len:503 episode reward: total was 14.830000. running mean: -12.513538\n",
      "ep 2273: ep_len:582 episode reward: total was 3.280000. running mean: -12.355602\n",
      "ep 2273: ep_len:646 episode reward: total was -158.190000. running mean: -13.813946\n",
      "ep 2273: ep_len:500 episode reward: total was 44.200000. running mean: -13.233807\n",
      "ep 2273: ep_len:3 episode reward: total was 1.010000. running mean: -13.091369\n",
      "ep 2273: ep_len:643 episode reward: total was 27.850000. running mean: -12.681955\n",
      "ep 2273: ep_len:605 episode reward: total was -59.800000. running mean: -13.153136\n",
      "epsilon:0.099186 episode_count: 15918. steps_count: 6891563.000000\n",
      "Time elapsed:  20046.067711353302\n",
      "ep 2274: ep_len:510 episode reward: total was -13.980000. running mean: -13.161404\n",
      "ep 2274: ep_len:580 episode reward: total was -7.390000. running mean: -13.103690\n",
      "ep 2274: ep_len:521 episode reward: total was 18.270000. running mean: -12.789953\n",
      "ep 2274: ep_len:525 episode reward: total was 38.390000. running mean: -12.278154\n",
      "ep 2274: ep_len:80 episode reward: total was -17.820000. running mean: -12.333572\n",
      "ep 2274: ep_len:241 episode reward: total was 35.060000. running mean: -11.859636\n",
      "ep 2274: ep_len:281 episode reward: total was 14.780000. running mean: -11.593240\n",
      "epsilon:0.099142 episode_count: 15925. steps_count: 6894301.000000\n",
      "Time elapsed:  20058.171798229218\n",
      "ep 2275: ep_len:500 episode reward: total was 20.210000. running mean: -11.275208\n",
      "ep 2275: ep_len:569 episode reward: total was 66.300000. running mean: -10.499456\n",
      "ep 2275: ep_len:79 episode reward: total was -0.230000. running mean: -10.396761\n",
      "ep 2275: ep_len:515 episode reward: total was -28.440000. running mean: -10.577193\n",
      "ep 2275: ep_len:3 episode reward: total was 1.010000. running mean: -10.461321\n",
      "ep 2275: ep_len:515 episode reward: total was -121.850000. running mean: -11.575208\n",
      "ep 2275: ep_len:641 episode reward: total was -55.370000. running mean: -12.013156\n",
      "epsilon:0.099097 episode_count: 15932. steps_count: 6897123.000000\n",
      "Time elapsed:  20065.797243118286\n",
      "ep 2276: ep_len:522 episode reward: total was -1.160000. running mean: -11.904625\n",
      "ep 2276: ep_len:518 episode reward: total was -5.260000. running mean: -11.838178\n",
      "ep 2276: ep_len:656 episode reward: total was -89.350000. running mean: -12.613297\n",
      "ep 2276: ep_len:587 episode reward: total was 27.990000. running mean: -12.207264\n",
      "ep 2276: ep_len:3 episode reward: total was 1.010000. running mean: -12.075091\n",
      "ep 2276: ep_len:534 episode reward: total was -85.810000. running mean: -12.812440\n",
      "ep 2276: ep_len:193 episode reward: total was -37.640000. running mean: -13.060716\n",
      "epsilon:0.099053 episode_count: 15939. steps_count: 6900136.000000\n",
      "Time elapsed:  20072.336144685745\n",
      "ep 2277: ep_len:553 episode reward: total was -41.070000. running mean: -13.340809\n",
      "ep 2277: ep_len:500 episode reward: total was 24.560000. running mean: -12.961800\n",
      "ep 2277: ep_len:417 episode reward: total was -18.140000. running mean: -13.013582\n",
      "ep 2277: ep_len:579 episode reward: total was -80.940000. running mean: -13.692847\n",
      "ep 2277: ep_len:3 episode reward: total was 1.010000. running mean: -13.545818\n",
      "ep 2277: ep_len:500 episode reward: total was -28.530000. running mean: -13.695660\n",
      "ep 2277: ep_len:500 episode reward: total was -44.510000. running mean: -14.003803\n",
      "epsilon:0.099009 episode_count: 15946. steps_count: 6903188.000000\n",
      "Time elapsed:  20083.745856523514\n",
      "ep 2278: ep_len:265 episode reward: total was 0.320000. running mean: -13.860565\n",
      "ep 2278: ep_len:530 episode reward: total was -17.170000. running mean: -13.893660\n",
      "ep 2278: ep_len:646 episode reward: total was -31.020000. running mean: -14.064923\n",
      "ep 2278: ep_len:510 episode reward: total was -41.890000. running mean: -14.343174\n",
      "ep 2278: ep_len:102 episode reward: total was 12.250000. running mean: -14.077242\n",
      "ep 2278: ep_len:500 episode reward: total was -78.610000. running mean: -14.722570\n",
      "ep 2278: ep_len:583 episode reward: total was -8.610000. running mean: -14.661444\n",
      "epsilon:0.098964 episode_count: 15953. steps_count: 6906324.000000\n",
      "Time elapsed:  20092.081081151962\n",
      "ep 2279: ep_len:501 episode reward: total was 36.720000. running mean: -14.147630\n",
      "ep 2279: ep_len:500 episode reward: total was 10.260000. running mean: -13.903553\n",
      "ep 2279: ep_len:508 episode reward: total was 12.990000. running mean: -13.634618\n",
      "ep 2279: ep_len:402 episode reward: total was -19.410000. running mean: -13.692372\n",
      "ep 2279: ep_len:3 episode reward: total was 1.010000. running mean: -13.545348\n",
      "ep 2279: ep_len:568 episode reward: total was 20.220000. running mean: -13.207694\n",
      "ep 2279: ep_len:283 episode reward: total was -10.610000. running mean: -13.181717\n",
      "epsilon:0.098920 episode_count: 15960. steps_count: 6909089.000000\n",
      "Time elapsed:  20099.500270605087\n",
      "ep 2280: ep_len:652 episode reward: total was 13.060000. running mean: -12.919300\n",
      "ep 2280: ep_len:533 episode reward: total was 22.860000. running mean: -12.561507\n",
      "ep 2280: ep_len:395 episode reward: total was 34.960000. running mean: -12.086292\n",
      "ep 2280: ep_len:509 episode reward: total was 15.670000. running mean: -11.808729\n",
      "ep 2280: ep_len:3 episode reward: total was 1.010000. running mean: -11.680542\n",
      "ep 2280: ep_len:289 episode reward: total was 9.110000. running mean: -11.472637\n",
      "ep 2280: ep_len:605 episode reward: total was -30.640000. running mean: -11.664310\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.098876 episode_count: 15967. steps_count: 6912075.000000\n",
      "Time elapsed:  20112.368099689484\n",
      "ep 2281: ep_len:573 episode reward: total was -57.230000. running mean: -12.119967\n",
      "ep 2281: ep_len:516 episode reward: total was 17.400000. running mean: -11.824767\n",
      "ep 2281: ep_len:626 episode reward: total was -34.870000. running mean: -12.055220\n",
      "ep 2281: ep_len:537 episode reward: total was -17.330000. running mean: -12.107968\n",
      "ep 2281: ep_len:3 episode reward: total was 0.000000. running mean: -11.986888\n",
      "ep 2281: ep_len:500 episode reward: total was 20.460000. running mean: -11.662419\n",
      "ep 2281: ep_len:593 episode reward: total was 8.620000. running mean: -11.459595\n",
      "epsilon:0.098831 episode_count: 15974. steps_count: 6915423.000000\n",
      "Time elapsed:  20121.12010908127\n",
      "ep 2282: ep_len:582 episode reward: total was -2.630000. running mean: -11.371299\n",
      "ep 2282: ep_len:507 episode reward: total was -12.720000. running mean: -11.384786\n",
      "ep 2282: ep_len:585 episode reward: total was -17.040000. running mean: -11.441338\n",
      "ep 2282: ep_len:504 episode reward: total was -7.950000. running mean: -11.406425\n",
      "ep 2282: ep_len:125 episode reward: total was -68.650000. running mean: -11.978860\n",
      "ep 2282: ep_len:551 episode reward: total was -34.590000. running mean: -12.204972\n",
      "ep 2282: ep_len:586 episode reward: total was -5.410000. running mean: -12.137022\n",
      "epsilon:0.098787 episode_count: 15981. steps_count: 6918863.000000\n",
      "Time elapsed:  20133.256239414215\n",
      "ep 2283: ep_len:500 episode reward: total was 9.180000. running mean: -11.923852\n",
      "ep 2283: ep_len:500 episode reward: total was 17.410000. running mean: -11.630513\n",
      "ep 2283: ep_len:611 episode reward: total was -22.290000. running mean: -11.737108\n",
      "ep 2283: ep_len:590 episode reward: total was 17.370000. running mean: -11.446037\n",
      "ep 2283: ep_len:3 episode reward: total was 1.010000. running mean: -11.321477\n",
      "ep 2283: ep_len:157 episode reward: total was 16.580000. running mean: -11.042462\n",
      "ep 2283: ep_len:556 episode reward: total was -62.180000. running mean: -11.553837\n",
      "epsilon:0.098743 episode_count: 15988. steps_count: 6921780.000000\n",
      "Time elapsed:  20141.08664250374\n",
      "ep 2284: ep_len:211 episode reward: total was 3.570000. running mean: -11.402599\n",
      "ep 2284: ep_len:539 episode reward: total was -100.480000. running mean: -12.293373\n",
      "ep 2284: ep_len:500 episode reward: total was -10.490000. running mean: -12.275339\n",
      "ep 2284: ep_len:500 episode reward: total was 22.980000. running mean: -11.922786\n",
      "ep 2284: ep_len:3 episode reward: total was 1.010000. running mean: -11.793458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2284: ep_len:509 episode reward: total was -0.280000. running mean: -11.678323\n",
      "ep 2284: ep_len:508 episode reward: total was -25.160000. running mean: -11.813140\n",
      "epsilon:0.098698 episode_count: 15995. steps_count: 6924550.000000\n",
      "Time elapsed:  20149.055967330933\n",
      "ep 2285: ep_len:245 episode reward: total was 0.240000. running mean: -11.692609\n",
      "ep 2285: ep_len:550 episode reward: total was -35.340000. running mean: -11.929083\n",
      "ep 2285: ep_len:539 episode reward: total was -41.570000. running mean: -12.225492\n",
      "ep 2285: ep_len:500 episode reward: total was 8.180000. running mean: -12.021437\n",
      "ep 2285: ep_len:111 episode reward: total was 24.250000. running mean: -11.658723\n",
      "ep 2285: ep_len:595 episode reward: total was -9.860000. running mean: -11.640735\n",
      "ep 2285: ep_len:540 episode reward: total was -181.980000. running mean: -13.344128\n",
      "epsilon:0.098654 episode_count: 16002. steps_count: 6927630.000000\n",
      "Time elapsed:  20156.251028060913\n",
      "ep 2286: ep_len:558 episode reward: total was -44.140000. running mean: -13.652087\n",
      "ep 2286: ep_len:201 episode reward: total was -32.690000. running mean: -13.842466\n",
      "ep 2286: ep_len:500 episode reward: total was -1.000000. running mean: -13.714041\n",
      "ep 2286: ep_len:504 episode reward: total was -44.790000. running mean: -14.024801\n",
      "ep 2286: ep_len:3 episode reward: total was -1.500000. running mean: -13.899553\n",
      "ep 2286: ep_len:552 episode reward: total was -35.860000. running mean: -14.119157\n",
      "ep 2286: ep_len:500 episode reward: total was -26.410000. running mean: -14.242066\n",
      "epsilon:0.098610 episode_count: 16009. steps_count: 6930448.000000\n",
      "Time elapsed:  20163.806553840637\n",
      "ep 2287: ep_len:502 episode reward: total was -122.530000. running mean: -15.324945\n",
      "ep 2287: ep_len:534 episode reward: total was 60.140000. running mean: -14.570296\n",
      "ep 2287: ep_len:506 episode reward: total was -14.040000. running mean: -14.564993\n",
      "ep 2287: ep_len:500 episode reward: total was -17.460000. running mean: -14.593943\n",
      "ep 2287: ep_len:93 episode reward: total was -41.730000. running mean: -14.865303\n",
      "ep 2287: ep_len:625 episode reward: total was -10.110000. running mean: -14.817750\n",
      "ep 2287: ep_len:561 episode reward: total was -194.990000. running mean: -16.619473\n",
      "epsilon:0.098565 episode_count: 16016. steps_count: 6933769.000000\n",
      "Time elapsed:  20172.71350979805\n",
      "ep 2288: ep_len:673 episode reward: total was -53.670000. running mean: -16.989978\n",
      "ep 2288: ep_len:258 episode reward: total was -75.250000. running mean: -17.572578\n",
      "ep 2288: ep_len:615 episode reward: total was -12.800000. running mean: -17.524852\n",
      "ep 2288: ep_len:520 episode reward: total was 31.330000. running mean: -17.036304\n",
      "ep 2288: ep_len:101 episode reward: total was -50.740000. running mean: -17.373341\n",
      "ep 2288: ep_len:512 episode reward: total was -11.150000. running mean: -17.311107\n",
      "ep 2288: ep_len:500 episode reward: total was -17.680000. running mean: -17.314796\n",
      "epsilon:0.098521 episode_count: 16023. steps_count: 6936948.000000\n",
      "Time elapsed:  20181.721680402756\n",
      "ep 2289: ep_len:645 episode reward: total was -28.090000. running mean: -17.422548\n",
      "ep 2289: ep_len:500 episode reward: total was 38.690000. running mean: -16.861423\n",
      "ep 2289: ep_len:416 episode reward: total was 12.840000. running mean: -16.564409\n",
      "ep 2289: ep_len:574 episode reward: total was 39.940000. running mean: -15.999365\n",
      "ep 2289: ep_len:3 episode reward: total was 1.010000. running mean: -15.829271\n",
      "ep 2289: ep_len:518 episode reward: total was 9.320000. running mean: -15.577778\n",
      "ep 2289: ep_len:500 episode reward: total was -1.400000. running mean: -15.436000\n",
      "epsilon:0.098477 episode_count: 16030. steps_count: 6940104.000000\n",
      "Time elapsed:  20192.979533433914\n",
      "ep 2290: ep_len:508 episode reward: total was 22.940000. running mean: -15.052240\n",
      "ep 2290: ep_len:500 episode reward: total was -3.160000. running mean: -14.933318\n",
      "ep 2290: ep_len:500 episode reward: total was -23.520000. running mean: -15.019185\n",
      "ep 2290: ep_len:582 episode reward: total was 7.550000. running mean: -14.793493\n",
      "ep 2290: ep_len:107 episode reward: total was 26.230000. running mean: -14.383258\n",
      "ep 2290: ep_len:565 episode reward: total was -35.430000. running mean: -14.593726\n",
      "ep 2290: ep_len:500 episode reward: total was -50.750000. running mean: -14.955288\n",
      "epsilon:0.098432 episode_count: 16037. steps_count: 6943366.000000\n",
      "Time elapsed:  20201.568674325943\n",
      "ep 2291: ep_len:214 episode reward: total was -10.390000. running mean: -14.909635\n",
      "ep 2291: ep_len:596 episode reward: total was -42.670000. running mean: -15.187239\n",
      "ep 2291: ep_len:667 episode reward: total was -83.780000. running mean: -15.873167\n",
      "ep 2291: ep_len:500 episode reward: total was 8.400000. running mean: -15.630435\n",
      "ep 2291: ep_len:113 episode reward: total was 15.700000. running mean: -15.317131\n",
      "ep 2291: ep_len:529 episode reward: total was -35.200000. running mean: -15.515959\n",
      "ep 2291: ep_len:526 episode reward: total was -26.380000. running mean: -15.624600\n",
      "epsilon:0.098388 episode_count: 16044. steps_count: 6946511.000000\n",
      "Time elapsed:  20209.758720874786\n",
      "ep 2292: ep_len:602 episode reward: total was 49.530000. running mean: -14.973054\n",
      "ep 2292: ep_len:588 episode reward: total was -15.590000. running mean: -14.979223\n",
      "ep 2292: ep_len:431 episode reward: total was -3.480000. running mean: -14.864231\n",
      "ep 2292: ep_len:622 episode reward: total was 34.170000. running mean: -14.373889\n",
      "ep 2292: ep_len:133 episode reward: total was 18.350000. running mean: -14.046650\n",
      "ep 2292: ep_len:502 episode reward: total was -13.680000. running mean: -14.042983\n",
      "ep 2292: ep_len:500 episode reward: total was -47.140000. running mean: -14.373953\n",
      "epsilon:0.098344 episode_count: 16051. steps_count: 6949889.000000\n",
      "Time elapsed:  20218.757482528687\n",
      "ep 2293: ep_len:500 episode reward: total was -20.850000. running mean: -14.438714\n",
      "ep 2293: ep_len:500 episode reward: total was 19.990000. running mean: -14.094427\n",
      "ep 2293: ep_len:603 episode reward: total was -18.790000. running mean: -14.141382\n",
      "ep 2293: ep_len:500 episode reward: total was -6.350000. running mean: -14.063469\n",
      "ep 2293: ep_len:86 episode reward: total was 17.780000. running mean: -13.745034\n",
      "ep 2293: ep_len:500 episode reward: total was -21.660000. running mean: -13.824184\n",
      "ep 2293: ep_len:500 episode reward: total was -25.690000. running mean: -13.942842\n",
      "epsilon:0.098299 episode_count: 16058. steps_count: 6953078.000000\n",
      "Time elapsed:  20227.18432354927\n",
      "ep 2294: ep_len:215 episode reward: total was 4.620000. running mean: -13.757213\n",
      "ep 2294: ep_len:589 episode reward: total was -0.400000. running mean: -13.623641\n",
      "ep 2294: ep_len:523 episode reward: total was -55.780000. running mean: -14.045205\n",
      "ep 2294: ep_len:503 episode reward: total was -3.000000. running mean: -13.934753\n",
      "ep 2294: ep_len:95 episode reward: total was 19.280000. running mean: -13.602605\n",
      "ep 2294: ep_len:532 episode reward: total was 8.450000. running mean: -13.382079\n",
      "ep 2294: ep_len:500 episode reward: total was -12.180000. running mean: -13.370058\n",
      "epsilon:0.098255 episode_count: 16065. steps_count: 6956035.000000\n",
      "Time elapsed:  20234.972411632538\n",
      "ep 2295: ep_len:184 episode reward: total was -5.370000. running mean: -13.290058\n",
      "ep 2295: ep_len:500 episode reward: total was -0.680000. running mean: -13.163957\n",
      "ep 2295: ep_len:632 episode reward: total was -46.710000. running mean: -13.499418\n",
      "ep 2295: ep_len:500 episode reward: total was -5.490000. running mean: -13.419324\n",
      "ep 2295: ep_len:84 episode reward: total was 21.250000. running mean: -13.072630\n",
      "ep 2295: ep_len:500 episode reward: total was 25.580000. running mean: -12.686104\n",
      "ep 2295: ep_len:510 episode reward: total was -43.970000. running mean: -12.998943\n",
      "epsilon:0.098211 episode_count: 16072. steps_count: 6958945.000000\n",
      "Time elapsed:  20246.436553001404\n",
      "ep 2296: ep_len:188 episode reward: total was -8.270000. running mean: -12.951653\n",
      "ep 2296: ep_len:638 episode reward: total was -2.370000. running mean: -12.845837\n",
      "ep 2296: ep_len:602 episode reward: total was -41.940000. running mean: -13.136779\n",
      "ep 2296: ep_len:579 episode reward: total was -1.240000. running mean: -13.017811\n",
      "ep 2296: ep_len:82 episode reward: total was 7.640000. running mean: -12.811233\n",
      "ep 2296: ep_len:525 episode reward: total was 11.090000. running mean: -12.572220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2296: ep_len:529 episode reward: total was 15.460000. running mean: -12.291898\n",
      "epsilon:0.098166 episode_count: 16079. steps_count: 6962088.000000\n",
      "Time elapsed:  20254.78795194626\n",
      "ep 2297: ep_len:501 episode reward: total was -52.050000. running mean: -12.689479\n",
      "ep 2297: ep_len:500 episode reward: total was -90.640000. running mean: -13.468984\n",
      "ep 2297: ep_len:433 episode reward: total was 5.060000. running mean: -13.283695\n",
      "ep 2297: ep_len:570 episode reward: total was 9.600000. running mean: -13.054858\n",
      "ep 2297: ep_len:92 episode reward: total was 14.330000. running mean: -12.781009\n",
      "ep 2297: ep_len:300 episode reward: total was 6.530000. running mean: -12.587899\n",
      "ep 2297: ep_len:500 episode reward: total was -56.420000. running mean: -13.026220\n",
      "epsilon:0.098122 episode_count: 16086. steps_count: 6964984.000000\n",
      "Time elapsed:  20259.65407705307\n",
      "ep 2298: ep_len:512 episode reward: total was -30.610000. running mean: -13.202058\n",
      "ep 2298: ep_len:500 episode reward: total was 3.220000. running mean: -13.037837\n",
      "ep 2298: ep_len:405 episode reward: total was -13.090000. running mean: -13.038359\n",
      "ep 2298: ep_len:500 episode reward: total was -36.370000. running mean: -13.271675\n",
      "ep 2298: ep_len:79 episode reward: total was 4.180000. running mean: -13.097158\n",
      "ep 2298: ep_len:610 episode reward: total was -12.420000. running mean: -13.090387\n",
      "ep 2298: ep_len:500 episode reward: total was 14.710000. running mean: -12.812383\n",
      "epsilon:0.098078 episode_count: 16093. steps_count: 6968090.000000\n",
      "Time elapsed:  20266.81595659256\n",
      "ep 2299: ep_len:638 episode reward: total was -15.490000. running mean: -12.839159\n",
      "ep 2299: ep_len:591 episode reward: total was 15.830000. running mean: -12.552468\n",
      "ep 2299: ep_len:525 episode reward: total was -38.400000. running mean: -12.810943\n",
      "ep 2299: ep_len:507 episode reward: total was 5.120000. running mean: -12.631633\n",
      "ep 2299: ep_len:3 episode reward: total was 1.010000. running mean: -12.495217\n",
      "ep 2299: ep_len:614 episode reward: total was -30.390000. running mean: -12.674165\n",
      "ep 2299: ep_len:548 episode reward: total was 1.460000. running mean: -12.532823\n",
      "epsilon:0.098033 episode_count: 16100. steps_count: 6971516.000000\n",
      "Time elapsed:  20275.82933139801\n",
      "ep 2300: ep_len:647 episode reward: total was -16.070000. running mean: -12.568195\n",
      "ep 2300: ep_len:579 episode reward: total was -106.600000. running mean: -13.508513\n",
      "ep 2300: ep_len:612 episode reward: total was -23.030000. running mean: -13.603728\n",
      "ep 2300: ep_len:382 episode reward: total was -62.740000. running mean: -14.095091\n",
      "ep 2300: ep_len:45 episode reward: total was 16.500000. running mean: -13.789140\n",
      "ep 2300: ep_len:500 episode reward: total was 21.660000. running mean: -13.434648\n",
      "ep 2300: ep_len:579 episode reward: total was -7.870000. running mean: -13.379002\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.097989 episode_count: 16107. steps_count: 6974860.000000\n",
      "Time elapsed:  20289.540368556976\n",
      "ep 2301: ep_len:500 episode reward: total was 37.510000. running mean: -12.870112\n",
      "ep 2301: ep_len:355 episode reward: total was -75.440000. running mean: -13.495811\n",
      "ep 2301: ep_len:500 episode reward: total was -1.740000. running mean: -13.378253\n",
      "ep 2301: ep_len:500 episode reward: total was 20.760000. running mean: -13.036870\n",
      "ep 2301: ep_len:102 episode reward: total was 22.140000. running mean: -12.685101\n",
      "ep 2301: ep_len:631 episode reward: total was -21.680000. running mean: -12.775050\n",
      "ep 2301: ep_len:181 episode reward: total was -18.660000. running mean: -12.833900\n",
      "epsilon:0.097945 episode_count: 16114. steps_count: 6977629.000000\n",
      "Time elapsed:  20297.316551446915\n",
      "ep 2302: ep_len:600 episode reward: total was -3.640000. running mean: -12.741961\n",
      "ep 2302: ep_len:500 episode reward: total was 45.710000. running mean: -12.157441\n",
      "ep 2302: ep_len:639 episode reward: total was -27.080000. running mean: -12.306667\n",
      "ep 2302: ep_len:500 episode reward: total was 60.180000. running mean: -11.581800\n",
      "ep 2302: ep_len:3 episode reward: total was -3.000000. running mean: -11.495982\n",
      "ep 2302: ep_len:621 episode reward: total was -25.850000. running mean: -11.639522\n",
      "ep 2302: ep_len:519 episode reward: total was -37.300000. running mean: -11.896127\n",
      "epsilon:0.097900 episode_count: 16121. steps_count: 6981011.000000\n",
      "Time elapsed:  20309.13734483719\n",
      "ep 2303: ep_len:572 episode reward: total was -2.150000. running mean: -11.798666\n",
      "ep 2303: ep_len:632 episode reward: total was 48.110000. running mean: -11.199579\n",
      "ep 2303: ep_len:511 episode reward: total was -38.960000. running mean: -11.477183\n",
      "ep 2303: ep_len:507 episode reward: total was 38.880000. running mean: -10.973612\n",
      "ep 2303: ep_len:54 episode reward: total was -20.500000. running mean: -11.068876\n",
      "ep 2303: ep_len:500 episode reward: total was 0.160000. running mean: -10.956587\n",
      "ep 2303: ep_len:536 episode reward: total was -13.890000. running mean: -10.985921\n",
      "epsilon:0.097856 episode_count: 16128. steps_count: 6984323.000000\n",
      "Time elapsed:  20317.99194574356\n",
      "ep 2304: ep_len:642 episode reward: total was -69.600000. running mean: -11.572062\n",
      "ep 2304: ep_len:583 episode reward: total was 46.930000. running mean: -10.987041\n",
      "ep 2304: ep_len:532 episode reward: total was -46.840000. running mean: -11.345571\n",
      "ep 2304: ep_len:500 episode reward: total was -9.580000. running mean: -11.327915\n",
      "ep 2304: ep_len:3 episode reward: total was 0.000000. running mean: -11.214636\n",
      "ep 2304: ep_len:519 episode reward: total was 30.190000. running mean: -10.800589\n",
      "ep 2304: ep_len:542 episode reward: total was -23.970000. running mean: -10.932284\n",
      "epsilon:0.097812 episode_count: 16135. steps_count: 6987644.000000\n",
      "Time elapsed:  20329.78746032715\n",
      "ep 2305: ep_len:500 episode reward: total was -1.640000. running mean: -10.839361\n",
      "ep 2305: ep_len:604 episode reward: total was 11.380000. running mean: -10.617167\n",
      "ep 2305: ep_len:536 episode reward: total was -7.810000. running mean: -10.589095\n",
      "ep 2305: ep_len:610 episode reward: total was -49.670000. running mean: -10.979904\n",
      "ep 2305: ep_len:99 episode reward: total was 23.760000. running mean: -10.632505\n",
      "ep 2305: ep_len:565 episode reward: total was 12.380000. running mean: -10.402380\n",
      "ep 2305: ep_len:541 episode reward: total was -16.350000. running mean: -10.461857\n",
      "epsilon:0.097767 episode_count: 16142. steps_count: 6991099.000000\n",
      "Time elapsed:  20338.88996052742\n",
      "ep 2306: ep_len:121 episode reward: total was 5.370000. running mean: -10.303538\n",
      "ep 2306: ep_len:630 episode reward: total was 9.890000. running mean: -10.101603\n",
      "ep 2306: ep_len:69 episode reward: total was -4.740000. running mean: -10.047987\n",
      "ep 2306: ep_len:512 episode reward: total was 33.360000. running mean: -9.613907\n",
      "ep 2306: ep_len:3 episode reward: total was -0.490000. running mean: -9.522668\n",
      "ep 2306: ep_len:612 episode reward: total was -9.630000. running mean: -9.523741\n",
      "ep 2306: ep_len:576 episode reward: total was -36.600000. running mean: -9.794504\n",
      "epsilon:0.097723 episode_count: 16149. steps_count: 6993622.000000\n",
      "Time elapsed:  20345.820085048676\n",
      "ep 2307: ep_len:500 episode reward: total was 29.430000. running mean: -9.402259\n",
      "ep 2307: ep_len:512 episode reward: total was -11.780000. running mean: -9.426036\n",
      "ep 2307: ep_len:500 episode reward: total was -35.220000. running mean: -9.683976\n",
      "ep 2307: ep_len:56 episode reward: total was -7.640000. running mean: -9.663536\n",
      "ep 2307: ep_len:3 episode reward: total was 1.010000. running mean: -9.556800\n",
      "ep 2307: ep_len:297 episode reward: total was 26.370000. running mean: -9.197532\n",
      "ep 2307: ep_len:623 episode reward: total was -29.120000. running mean: -9.396757\n",
      "epsilon:0.097679 episode_count: 16156. steps_count: 6996113.000000\n",
      "Time elapsed:  20355.668805122375\n",
      "ep 2308: ep_len:511 episode reward: total was 8.260000. running mean: -9.220190\n",
      "ep 2308: ep_len:500 episode reward: total was -1.680000. running mean: -9.144788\n",
      "ep 2308: ep_len:403 episode reward: total was 42.630000. running mean: -8.627040\n",
      "ep 2308: ep_len:105 episode reward: total was -2.020000. running mean: -8.560969\n",
      "ep 2308: ep_len:3 episode reward: total was 1.010000. running mean: -8.465260\n",
      "ep 2308: ep_len:322 episode reward: total was 6.050000. running mean: -8.320107\n",
      "ep 2308: ep_len:504 episode reward: total was -28.170000. running mean: -8.518606\n",
      "epsilon:0.097634 episode_count: 16163. steps_count: 6998461.000000\n",
      "Time elapsed:  20362.032490730286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2309: ep_len:544 episode reward: total was 36.010000. running mean: -8.073320\n",
      "ep 2309: ep_len:500 episode reward: total was 12.940000. running mean: -7.863187\n",
      "ep 2309: ep_len:79 episode reward: total was -0.230000. running mean: -7.786855\n",
      "ep 2309: ep_len:508 episode reward: total was -20.060000. running mean: -7.909586\n",
      "ep 2309: ep_len:3 episode reward: total was 0.000000. running mean: -7.830491\n",
      "ep 2309: ep_len:559 episode reward: total was -2.280000. running mean: -7.774986\n",
      "ep 2309: ep_len:591 episode reward: total was -17.540000. running mean: -7.872636\n",
      "epsilon:0.097590 episode_count: 16170. steps_count: 7001245.000000\n",
      "Time elapsed:  20369.648324728012\n",
      "ep 2310: ep_len:636 episode reward: total was -81.870000. running mean: -8.612609\n",
      "ep 2310: ep_len:511 episode reward: total was -5.590000. running mean: -8.582383\n",
      "ep 2310: ep_len:429 episode reward: total was 47.770000. running mean: -8.018859\n",
      "ep 2310: ep_len:500 episode reward: total was -5.170000. running mean: -7.990371\n",
      "ep 2310: ep_len:3 episode reward: total was 0.000000. running mean: -7.910467\n",
      "ep 2310: ep_len:578 episode reward: total was -16.460000. running mean: -7.995963\n",
      "ep 2310: ep_len:592 episode reward: total was -27.850000. running mean: -8.194503\n",
      "epsilon:0.097546 episode_count: 16177. steps_count: 7004494.000000\n",
      "Time elapsed:  20378.216372013092\n",
      "ep 2311: ep_len:613 episode reward: total was 11.070000. running mean: -8.001858\n",
      "ep 2311: ep_len:568 episode reward: total was -30.130000. running mean: -8.223139\n",
      "ep 2311: ep_len:501 episode reward: total was -63.750000. running mean: -8.778408\n",
      "ep 2311: ep_len:500 episode reward: total was 11.170000. running mean: -8.578924\n",
      "ep 2311: ep_len:3 episode reward: total was 1.010000. running mean: -8.483035\n",
      "ep 2311: ep_len:500 episode reward: total was -11.740000. running mean: -8.515604\n",
      "ep 2311: ep_len:573 episode reward: total was 10.780000. running mean: -8.322648\n",
      "epsilon:0.097501 episode_count: 16184. steps_count: 7007752.000000\n",
      "Time elapsed:  20386.845952033997\n",
      "ep 2312: ep_len:522 episode reward: total was 22.540000. running mean: -8.014022\n",
      "ep 2312: ep_len:500 episode reward: total was 26.470000. running mean: -7.669181\n",
      "ep 2312: ep_len:600 episode reward: total was -56.080000. running mean: -8.153290\n",
      "ep 2312: ep_len:149 episode reward: total was 3.510000. running mean: -8.036657\n",
      "ep 2312: ep_len:3 episode reward: total was 1.010000. running mean: -7.946190\n",
      "ep 2312: ep_len:500 episode reward: total was 8.650000. running mean: -7.780228\n",
      "ep 2312: ep_len:564 episode reward: total was -43.120000. running mean: -8.133626\n",
      "epsilon:0.097457 episode_count: 16191. steps_count: 7010590.000000\n",
      "Time elapsed:  20394.26510834694\n",
      "ep 2313: ep_len:500 episode reward: total was 3.780000. running mean: -8.014490\n",
      "ep 2313: ep_len:500 episode reward: total was 54.670000. running mean: -7.387645\n",
      "ep 2313: ep_len:633 episode reward: total was -28.910000. running mean: -7.602868\n",
      "ep 2313: ep_len:514 episode reward: total was 21.780000. running mean: -7.309040\n",
      "ep 2313: ep_len:3 episode reward: total was 1.010000. running mean: -7.225849\n",
      "ep 2313: ep_len:616 episode reward: total was -16.360000. running mean: -7.317191\n",
      "ep 2313: ep_len:582 episode reward: total was -20.620000. running mean: -7.450219\n",
      "epsilon:0.097413 episode_count: 16198. steps_count: 7013938.000000\n",
      "Time elapsed:  20403.113040208817\n",
      "ep 2314: ep_len:569 episode reward: total was 16.220000. running mean: -7.213517\n",
      "ep 2314: ep_len:515 episode reward: total was -11.260000. running mean: -7.253982\n",
      "ep 2314: ep_len:538 episode reward: total was -42.530000. running mean: -7.606742\n",
      "ep 2314: ep_len:613 episode reward: total was 63.450000. running mean: -6.896174\n",
      "ep 2314: ep_len:3 episode reward: total was 0.000000. running mean: -6.827213\n",
      "ep 2314: ep_len:559 episode reward: total was 19.300000. running mean: -6.565940\n",
      "ep 2314: ep_len:579 episode reward: total was 7.010000. running mean: -6.430181\n",
      "epsilon:0.097368 episode_count: 16205. steps_count: 7017314.000000\n",
      "Time elapsed:  20414.89277267456\n",
      "ep 2315: ep_len:213 episode reward: total was -5.470000. running mean: -6.420579\n",
      "ep 2315: ep_len:500 episode reward: total was 9.910000. running mean: -6.257273\n",
      "ep 2315: ep_len:604 episode reward: total was -33.240000. running mean: -6.527101\n",
      "ep 2315: ep_len:500 episode reward: total was 29.810000. running mean: -6.163730\n",
      "ep 2315: ep_len:101 episode reward: total was 21.240000. running mean: -5.889692\n",
      "ep 2315: ep_len:569 episode reward: total was -8.700000. running mean: -5.917795\n",
      "ep 2315: ep_len:640 episode reward: total was -7.380000. running mean: -5.932418\n",
      "epsilon:0.097324 episode_count: 16212. steps_count: 7020441.000000\n",
      "Time elapsed:  20423.70000243187\n",
      "ep 2316: ep_len:563 episode reward: total was 42.050000. running mean: -5.452593\n",
      "ep 2316: ep_len:500 episode reward: total was 35.470000. running mean: -5.043367\n",
      "ep 2316: ep_len:650 episode reward: total was -97.340000. running mean: -5.966334\n",
      "ep 2316: ep_len:525 episode reward: total was -18.050000. running mean: -6.087170\n",
      "ep 2316: ep_len:92 episode reward: total was 15.240000. running mean: -5.873899\n",
      "ep 2316: ep_len:180 episode reward: total was 30.180000. running mean: -5.513360\n",
      "ep 2316: ep_len:574 episode reward: total was -43.890000. running mean: -5.897126\n",
      "epsilon:0.097280 episode_count: 16219. steps_count: 7023525.000000\n",
      "Time elapsed:  20428.931552648544\n",
      "ep 2317: ep_len:519 episode reward: total was -66.080000. running mean: -6.498955\n",
      "ep 2317: ep_len:636 episode reward: total was 0.910000. running mean: -6.424865\n",
      "ep 2317: ep_len:500 episode reward: total was 34.440000. running mean: -6.016217\n",
      "ep 2317: ep_len:500 episode reward: total was 10.260000. running mean: -5.853454\n",
      "ep 2317: ep_len:3 episode reward: total was 1.010000. running mean: -5.784820\n",
      "ep 2317: ep_len:500 episode reward: total was -22.980000. running mean: -5.956772\n",
      "ep 2317: ep_len:573 episode reward: total was -35.250000. running mean: -6.249704\n",
      "epsilon:0.097235 episode_count: 16226. steps_count: 7026756.000000\n",
      "Time elapsed:  20439.541005134583\n",
      "ep 2318: ep_len:575 episode reward: total was 39.600000. running mean: -5.791207\n",
      "ep 2318: ep_len:500 episode reward: total was 9.850000. running mean: -5.634795\n",
      "ep 2318: ep_len:451 episode reward: total was -10.960000. running mean: -5.688047\n",
      "ep 2318: ep_len:500 episode reward: total was 0.450000. running mean: -5.626667\n",
      "ep 2318: ep_len:3 episode reward: total was -0.490000. running mean: -5.575300\n",
      "ep 2318: ep_len:722 episode reward: total was -251.500000. running mean: -8.034547\n",
      "ep 2318: ep_len:513 episode reward: total was 13.310000. running mean: -7.821101\n",
      "epsilon:0.097191 episode_count: 16233. steps_count: 7030020.000000\n",
      "Time elapsed:  20448.041903972626\n",
      "ep 2319: ep_len:546 episode reward: total was 42.940000. running mean: -7.313490\n",
      "ep 2319: ep_len:334 episode reward: total was 17.620000. running mean: -7.064155\n",
      "ep 2319: ep_len:526 episode reward: total was -80.280000. running mean: -7.796314\n",
      "ep 2319: ep_len:526 episode reward: total was -28.760000. running mean: -8.005951\n",
      "ep 2319: ep_len:3 episode reward: total was 0.000000. running mean: -7.925891\n",
      "ep 2319: ep_len:500 episode reward: total was -37.960000. running mean: -8.226232\n",
      "ep 2319: ep_len:292 episode reward: total was -36.980000. running mean: -8.513770\n",
      "epsilon:0.097147 episode_count: 16240. steps_count: 7032747.000000\n",
      "Time elapsed:  20455.570937156677\n",
      "ep 2320: ep_len:201 episode reward: total was 5.610000. running mean: -8.372532\n",
      "ep 2320: ep_len:520 episode reward: total was -30.820000. running mean: -8.597007\n",
      "ep 2320: ep_len:379 episode reward: total was 22.460000. running mean: -8.286437\n",
      "ep 2320: ep_len:500 episode reward: total was -10.440000. running mean: -8.307973\n",
      "ep 2320: ep_len:85 episode reward: total was -48.730000. running mean: -8.712193\n",
      "ep 2320: ep_len:548 episode reward: total was -30.910000. running mean: -8.934171\n",
      "ep 2320: ep_len:500 episode reward: total was 1.050000. running mean: -8.834329\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.097102 episode_count: 16247. steps_count: 7035480.000000\n",
      "Time elapsed:  20467.931648254395\n",
      "ep 2321: ep_len:558 episode reward: total was 47.630000. running mean: -8.269686\n",
      "ep 2321: ep_len:500 episode reward: total was 21.160000. running mean: -7.975389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2321: ep_len:564 episode reward: total was -53.720000. running mean: -8.432835\n",
      "ep 2321: ep_len:606 episode reward: total was 20.870000. running mean: -8.139807\n",
      "ep 2321: ep_len:3 episode reward: total was 0.000000. running mean: -8.058409\n",
      "ep 2321: ep_len:500 episode reward: total was -36.300000. running mean: -8.340825\n",
      "ep 2321: ep_len:623 episode reward: total was -19.460000. running mean: -8.452016\n",
      "epsilon:0.097058 episode_count: 16254. steps_count: 7038834.000000\n",
      "Time elapsed:  20473.13562464714\n",
      "ep 2322: ep_len:581 episode reward: total was 13.520000. running mean: -8.232296\n",
      "ep 2322: ep_len:519 episode reward: total was 15.810000. running mean: -7.991873\n",
      "ep 2322: ep_len:348 episode reward: total was 17.280000. running mean: -7.739155\n",
      "ep 2322: ep_len:517 episode reward: total was 40.830000. running mean: -7.253463\n",
      "ep 2322: ep_len:42 episode reward: total was 19.500000. running mean: -6.985928\n",
      "ep 2322: ep_len:500 episode reward: total was -9.260000. running mean: -7.008669\n",
      "ep 2322: ep_len:347 episode reward: total was -20.050000. running mean: -7.139082\n",
      "epsilon:0.097014 episode_count: 16261. steps_count: 7041688.000000\n",
      "Time elapsed:  20480.363949775696\n",
      "ep 2323: ep_len:613 episode reward: total was 8.070000. running mean: -6.986992\n",
      "ep 2323: ep_len:199 episode reward: total was -14.160000. running mean: -7.058722\n",
      "ep 2323: ep_len:619 episode reward: total was -51.730000. running mean: -7.505434\n",
      "ep 2323: ep_len:500 episode reward: total was 23.970000. running mean: -7.190680\n",
      "ep 2323: ep_len:3 episode reward: total was -1.500000. running mean: -7.133773\n",
      "ep 2323: ep_len:612 episode reward: total was -86.650000. running mean: -7.928936\n",
      "ep 2323: ep_len:500 episode reward: total was -11.380000. running mean: -7.963446\n",
      "epsilon:0.096969 episode_count: 16268. steps_count: 7044734.000000\n",
      "Time elapsed:  20488.196751117706\n",
      "ep 2324: ep_len:585 episode reward: total was -76.310000. running mean: -8.646912\n",
      "ep 2324: ep_len:520 episode reward: total was 44.050000. running mean: -8.119943\n",
      "ep 2324: ep_len:500 episode reward: total was -14.080000. running mean: -8.179543\n",
      "ep 2324: ep_len:532 episode reward: total was 26.340000. running mean: -7.834348\n",
      "ep 2324: ep_len:3 episode reward: total was 0.000000. running mean: -7.756004\n",
      "ep 2324: ep_len:302 episode reward: total was -9.890000. running mean: -7.777344\n",
      "ep 2324: ep_len:564 episode reward: total was 4.040000. running mean: -7.659171\n",
      "epsilon:0.096925 episode_count: 16275. steps_count: 7047740.000000\n",
      "Time elapsed:  20498.80106329918\n",
      "ep 2325: ep_len:187 episode reward: total was -61.390000. running mean: -8.196479\n",
      "ep 2325: ep_len:505 episode reward: total was 48.520000. running mean: -7.629314\n",
      "ep 2325: ep_len:500 episode reward: total was 26.300000. running mean: -7.290021\n",
      "ep 2325: ep_len:414 episode reward: total was -11.390000. running mean: -7.331021\n",
      "ep 2325: ep_len:3 episode reward: total was 1.010000. running mean: -7.247611\n",
      "ep 2325: ep_len:500 episode reward: total was -47.840000. running mean: -7.653535\n",
      "ep 2325: ep_len:562 episode reward: total was -4.270000. running mean: -7.619699\n",
      "epsilon:0.096881 episode_count: 16282. steps_count: 7050411.000000\n",
      "Time elapsed:  20506.142195940018\n",
      "ep 2326: ep_len:500 episode reward: total was 49.320000. running mean: -7.050302\n",
      "ep 2326: ep_len:620 episode reward: total was -32.620000. running mean: -7.305999\n",
      "ep 2326: ep_len:562 episode reward: total was -24.770000. running mean: -7.480639\n",
      "ep 2326: ep_len:132 episode reward: total was 13.020000. running mean: -7.275633\n",
      "ep 2326: ep_len:3 episode reward: total was 1.010000. running mean: -7.192777\n",
      "ep 2326: ep_len:500 episode reward: total was -14.750000. running mean: -7.268349\n",
      "ep 2326: ep_len:594 episode reward: total was -9.610000. running mean: -7.291765\n",
      "epsilon:0.096836 episode_count: 16289. steps_count: 7053322.000000\n",
      "Time elapsed:  20513.937630414963\n",
      "ep 2327: ep_len:549 episode reward: total was -39.730000. running mean: -7.616148\n",
      "ep 2327: ep_len:597 episode reward: total was 26.630000. running mean: -7.273686\n",
      "ep 2327: ep_len:700 episode reward: total was -43.080000. running mean: -7.631749\n",
      "ep 2327: ep_len:139 episode reward: total was 2.960000. running mean: -7.525832\n",
      "ep 2327: ep_len:113 episode reward: total was -49.240000. running mean: -7.942973\n",
      "ep 2327: ep_len:520 episode reward: total was -74.670000. running mean: -8.610244\n",
      "ep 2327: ep_len:275 episode reward: total was 6.210000. running mean: -8.462041\n",
      "epsilon:0.096792 episode_count: 16296. steps_count: 7056215.000000\n",
      "Time elapsed:  20521.795387744904\n",
      "ep 2328: ep_len:523 episode reward: total was -3.530000. running mean: -8.412721\n",
      "ep 2328: ep_len:500 episode reward: total was -14.320000. running mean: -8.471794\n",
      "ep 2328: ep_len:500 episode reward: total was 9.610000. running mean: -8.290976\n",
      "ep 2328: ep_len:46 episode reward: total was 1.320000. running mean: -8.194866\n",
      "ep 2328: ep_len:114 episode reward: total was -59.260000. running mean: -8.705517\n",
      "ep 2328: ep_len:500 episode reward: total was 6.890000. running mean: -8.549562\n",
      "ep 2328: ep_len:501 episode reward: total was -0.420000. running mean: -8.468267\n",
      "epsilon:0.096748 episode_count: 16303. steps_count: 7058899.000000\n",
      "Time elapsed:  20532.0254881382\n",
      "ep 2329: ep_len:651 episode reward: total was 18.250000. running mean: -8.201084\n",
      "ep 2329: ep_len:607 episode reward: total was -24.200000. running mean: -8.361073\n",
      "ep 2329: ep_len:681 episode reward: total was -20.650000. running mean: -8.483962\n",
      "ep 2329: ep_len:414 episode reward: total was 8.580000. running mean: -8.313323\n",
      "ep 2329: ep_len:3 episode reward: total was 1.010000. running mean: -8.220089\n",
      "ep 2329: ep_len:181 episode reward: total was 24.190000. running mean: -7.895989\n",
      "ep 2329: ep_len:556 episode reward: total was -133.050000. running mean: -9.147529\n",
      "epsilon:0.096703 episode_count: 16310. steps_count: 7061992.000000\n",
      "Time elapsed:  20538.9489235878\n",
      "ep 2330: ep_len:605 episode reward: total was 26.410000. running mean: -8.791953\n",
      "ep 2330: ep_len:501 episode reward: total was -5.180000. running mean: -8.755834\n",
      "ep 2330: ep_len:535 episode reward: total was -11.530000. running mean: -8.783576\n",
      "ep 2330: ep_len:620 episode reward: total was 10.840000. running mean: -8.587340\n",
      "ep 2330: ep_len:3 episode reward: total was 1.010000. running mean: -8.491366\n",
      "ep 2330: ep_len:252 episode reward: total was 18.890000. running mean: -8.217553\n",
      "ep 2330: ep_len:531 episode reward: total was -17.550000. running mean: -8.310877\n",
      "epsilon:0.096659 episode_count: 16317. steps_count: 7065039.000000\n",
      "Time elapsed:  20547.097388982773\n",
      "ep 2331: ep_len:500 episode reward: total was 39.680000. running mean: -7.830968\n",
      "ep 2331: ep_len:577 episode reward: total was -8.190000. running mean: -7.834559\n",
      "ep 2331: ep_len:583 episode reward: total was 0.010000. running mean: -7.756113\n",
      "ep 2331: ep_len:505 episode reward: total was -13.220000. running mean: -7.810752\n",
      "ep 2331: ep_len:95 episode reward: total was -15.250000. running mean: -7.885144\n",
      "ep 2331: ep_len:537 episode reward: total was -44.060000. running mean: -8.246893\n",
      "ep 2331: ep_len:576 episode reward: total was -11.650000. running mean: -8.280924\n",
      "epsilon:0.096615 episode_count: 16324. steps_count: 7068412.000000\n",
      "Time elapsed:  20555.791126728058\n",
      "ep 2332: ep_len:630 episode reward: total was 19.540000. running mean: -8.002715\n",
      "ep 2332: ep_len:639 episode reward: total was -65.890000. running mean: -8.581588\n",
      "ep 2332: ep_len:556 episode reward: total was -38.310000. running mean: -8.878872\n",
      "ep 2332: ep_len:569 episode reward: total was 20.550000. running mean: -8.584583\n",
      "ep 2332: ep_len:3 episode reward: total was -1.500000. running mean: -8.513737\n",
      "ep 2332: ep_len:500 episode reward: total was -85.760000. running mean: -9.286200\n",
      "ep 2332: ep_len:312 episode reward: total was 12.380000. running mean: -9.069538\n",
      "epsilon:0.096570 episode_count: 16331. steps_count: 7071621.000000\n",
      "Time elapsed:  20564.33412694931\n",
      "ep 2333: ep_len:521 episode reward: total was -37.390000. running mean: -9.352743\n",
      "ep 2333: ep_len:649 episode reward: total was 26.410000. running mean: -8.995115\n",
      "ep 2333: ep_len:553 episode reward: total was -39.530000. running mean: -9.300464\n",
      "ep 2333: ep_len:132 episode reward: total was 19.570000. running mean: -9.011759\n",
      "ep 2333: ep_len:3 episode reward: total was 0.000000. running mean: -8.921642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2333: ep_len:567 episode reward: total was -12.300000. running mean: -8.955425\n",
      "ep 2333: ep_len:612 episode reward: total was -10.300000. running mean: -8.968871\n",
      "epsilon:0.096526 episode_count: 16338. steps_count: 7074658.000000\n",
      "Time elapsed:  20575.73593902588\n",
      "ep 2334: ep_len:698 episode reward: total was -127.090000. running mean: -10.150082\n",
      "ep 2334: ep_len:624 episode reward: total was 9.580000. running mean: -9.952782\n",
      "ep 2334: ep_len:63 episode reward: total was -10.310000. running mean: -9.956354\n",
      "ep 2334: ep_len:585 episode reward: total was 31.610000. running mean: -9.540690\n",
      "ep 2334: ep_len:105 episode reward: total was 5.700000. running mean: -9.388283\n",
      "ep 2334: ep_len:519 episode reward: total was -37.240000. running mean: -9.666800\n",
      "ep 2334: ep_len:634 episode reward: total was -24.950000. running mean: -9.819632\n",
      "epsilon:0.096482 episode_count: 16345. steps_count: 7077886.000000\n",
      "Time elapsed:  20584.15356373787\n",
      "ep 2335: ep_len:500 episode reward: total was 30.250000. running mean: -9.418936\n",
      "ep 2335: ep_len:333 episode reward: total was -18.320000. running mean: -9.507947\n",
      "ep 2335: ep_len:525 episode reward: total was -14.120000. running mean: -9.554067\n",
      "ep 2335: ep_len:532 episode reward: total was 0.790000. running mean: -9.450627\n",
      "ep 2335: ep_len:44 episode reward: total was 16.000000. running mean: -9.196120\n",
      "ep 2335: ep_len:614 episode reward: total was -12.870000. running mean: -9.232859\n",
      "ep 2335: ep_len:560 episode reward: total was -10.630000. running mean: -9.246831\n",
      "epsilon:0.096437 episode_count: 16352. steps_count: 7080994.000000\n",
      "Time elapsed:  20592.436005353928\n",
      "ep 2336: ep_len:256 episode reward: total was -8.300000. running mean: -9.237362\n",
      "ep 2336: ep_len:593 episode reward: total was -54.180000. running mean: -9.686789\n",
      "ep 2336: ep_len:619 episode reward: total was -39.220000. running mean: -9.982121\n",
      "ep 2336: ep_len:576 episode reward: total was 14.490000. running mean: -9.737400\n",
      "ep 2336: ep_len:3 episode reward: total was 1.010000. running mean: -9.629926\n",
      "ep 2336: ep_len:500 episode reward: total was -4.710000. running mean: -9.580726\n",
      "ep 2336: ep_len:565 episode reward: total was -11.560000. running mean: -9.600519\n",
      "epsilon:0.096393 episode_count: 16359. steps_count: 7084106.000000\n",
      "Time elapsed:  20604.260503053665\n",
      "ep 2337: ep_len:636 episode reward: total was -10.420000. running mean: -9.608714\n",
      "ep 2337: ep_len:548 episode reward: total was 3.910000. running mean: -9.473527\n",
      "ep 2337: ep_len:551 episode reward: total was -54.360000. running mean: -9.922391\n",
      "ep 2337: ep_len:517 episode reward: total was -30.590000. running mean: -10.129068\n",
      "ep 2337: ep_len:3 episode reward: total was 0.000000. running mean: -10.027777\n",
      "ep 2337: ep_len:510 episode reward: total was -36.970000. running mean: -10.297199\n",
      "ep 2337: ep_len:515 episode reward: total was -62.590000. running mean: -10.820127\n",
      "epsilon:0.096349 episode_count: 16366. steps_count: 7087386.000000\n",
      "Time elapsed:  20615.984199762344\n",
      "ep 2338: ep_len:500 episode reward: total was 6.670000. running mean: -10.645226\n",
      "ep 2338: ep_len:516 episode reward: total was 43.120000. running mean: -10.107574\n",
      "ep 2338: ep_len:454 episode reward: total was 19.100000. running mean: -9.815498\n",
      "ep 2338: ep_len:42 episode reward: total was -1.770000. running mean: -9.735043\n",
      "ep 2338: ep_len:3 episode reward: total was 1.010000. running mean: -9.627592\n",
      "ep 2338: ep_len:602 episode reward: total was -25.400000. running mean: -9.785316\n",
      "ep 2338: ep_len:584 episode reward: total was 4.550000. running mean: -9.641963\n",
      "epsilon:0.096304 episode_count: 16373. steps_count: 7090087.000000\n",
      "Time elapsed:  20626.447885751724\n",
      "ep 2339: ep_len:592 episode reward: total was 49.880000. running mean: -9.046744\n",
      "ep 2339: ep_len:510 episode reward: total was 37.150000. running mean: -8.584776\n",
      "ep 2339: ep_len:614 episode reward: total was -12.120000. running mean: -8.620128\n",
      "ep 2339: ep_len:500 episode reward: total was -6.120000. running mean: -8.595127\n",
      "ep 2339: ep_len:3 episode reward: total was 1.010000. running mean: -8.499076\n",
      "ep 2339: ep_len:500 episode reward: total was -80.750000. running mean: -9.221585\n",
      "ep 2339: ep_len:304 episode reward: total was -24.260000. running mean: -9.371969\n",
      "epsilon:0.096260 episode_count: 16380. steps_count: 7093110.000000\n",
      "Time elapsed:  20638.353805303574\n",
      "ep 2340: ep_len:248 episode reward: total was -10.720000. running mean: -9.385450\n",
      "ep 2340: ep_len:500 episode reward: total was -51.990000. running mean: -9.811495\n",
      "ep 2340: ep_len:550 episode reward: total was -42.870000. running mean: -10.142080\n",
      "ep 2340: ep_len:509 episode reward: total was -12.680000. running mean: -10.167459\n",
      "ep 2340: ep_len:85 episode reward: total was 11.680000. running mean: -9.948985\n",
      "ep 2340: ep_len:186 episode reward: total was 13.100000. running mean: -9.718495\n",
      "ep 2340: ep_len:304 episode reward: total was -14.790000. running mean: -9.769210\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.096216 episode_count: 16387. steps_count: 7095492.000000\n",
      "Time elapsed:  20650.145795106888\n",
      "ep 2341: ep_len:564 episode reward: total was 24.340000. running mean: -9.428118\n",
      "ep 2341: ep_len:501 episode reward: total was 45.690000. running mean: -8.876937\n",
      "ep 2341: ep_len:355 episode reward: total was 28.800000. running mean: -8.500167\n",
      "ep 2341: ep_len:167 episode reward: total was 4.170000. running mean: -8.373466\n",
      "ep 2341: ep_len:3 episode reward: total was 1.010000. running mean: -8.279631\n",
      "ep 2341: ep_len:657 episode reward: total was -117.310000. running mean: -9.369935\n",
      "ep 2341: ep_len:500 episode reward: total was -55.110000. running mean: -9.827335\n",
      "epsilon:0.096171 episode_count: 16394. steps_count: 7098239.000000\n",
      "Time elapsed:  20660.519911527634\n",
      "ep 2342: ep_len:556 episode reward: total was -11.280000. running mean: -9.841862\n",
      "ep 2342: ep_len:574 episode reward: total was -40.020000. running mean: -10.143643\n",
      "ep 2342: ep_len:424 episode reward: total was 1.960000. running mean: -10.022607\n",
      "ep 2342: ep_len:500 episode reward: total was 20.790000. running mean: -9.714481\n",
      "ep 2342: ep_len:73 episode reward: total was 4.730000. running mean: -9.570036\n",
      "ep 2342: ep_len:628 episode reward: total was -54.420000. running mean: -10.018536\n",
      "ep 2342: ep_len:577 episode reward: total was -27.930000. running mean: -10.197650\n",
      "epsilon:0.096127 episode_count: 16401. steps_count: 7101571.000000\n",
      "Time elapsed:  20669.573570489883\n",
      "ep 2343: ep_len:500 episode reward: total was 47.600000. running mean: -9.619674\n",
      "ep 2343: ep_len:539 episode reward: total was 69.650000. running mean: -8.826977\n",
      "ep 2343: ep_len:500 episode reward: total was -11.830000. running mean: -8.857007\n",
      "ep 2343: ep_len:553 episode reward: total was 11.870000. running mean: -8.649737\n",
      "ep 2343: ep_len:3 episode reward: total was 1.010000. running mean: -8.553140\n",
      "ep 2343: ep_len:286 episode reward: total was 3.150000. running mean: -8.436108\n",
      "ep 2343: ep_len:621 episode reward: total was -70.100000. running mean: -9.052747\n",
      "epsilon:0.096083 episode_count: 16408. steps_count: 7104573.000000\n",
      "Time elapsed:  20677.487907648087\n",
      "ep 2344: ep_len:560 episode reward: total was 29.810000. running mean: -8.664120\n",
      "ep 2344: ep_len:500 episode reward: total was -22.870000. running mean: -8.806179\n",
      "ep 2344: ep_len:563 episode reward: total was -27.610000. running mean: -8.994217\n",
      "ep 2344: ep_len:500 episode reward: total was 11.840000. running mean: -8.785875\n",
      "ep 2344: ep_len:130 episode reward: total was 13.330000. running mean: -8.564716\n",
      "ep 2344: ep_len:513 episode reward: total was -53.800000. running mean: -9.017069\n",
      "ep 2344: ep_len:211 episode reward: total was -4.890000. running mean: -8.975798\n",
      "epsilon:0.096038 episode_count: 16415. steps_count: 7107550.000000\n",
      "Time elapsed:  20685.577125549316\n",
      "ep 2345: ep_len:500 episode reward: total was 35.920000. running mean: -8.526840\n",
      "ep 2345: ep_len:557 episode reward: total was -7.840000. running mean: -8.519972\n",
      "ep 2345: ep_len:644 episode reward: total was -22.670000. running mean: -8.661472\n",
      "ep 2345: ep_len:512 episode reward: total was 3.310000. running mean: -8.541757\n",
      "ep 2345: ep_len:3 episode reward: total was 1.010000. running mean: -8.446240\n",
      "ep 2345: ep_len:507 episode reward: total was -95.060000. running mean: -9.312377\n",
      "ep 2345: ep_len:166 episode reward: total was -4.710000. running mean: -9.266354\n",
      "epsilon:0.095994 episode_count: 16422. steps_count: 7110439.000000\n",
      "Time elapsed:  20693.326551675797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2346: ep_len:500 episode reward: total was 24.230000. running mean: -8.931390\n",
      "ep 2346: ep_len:568 episode reward: total was -109.160000. running mean: -9.933676\n",
      "ep 2346: ep_len:536 episode reward: total was -39.720000. running mean: -10.231539\n",
      "ep 2346: ep_len:530 episode reward: total was -11.360000. running mean: -10.242824\n",
      "ep 2346: ep_len:3 episode reward: total was 1.010000. running mean: -10.130296\n",
      "ep 2346: ep_len:518 episode reward: total was -32.690000. running mean: -10.355893\n",
      "ep 2346: ep_len:500 episode reward: total was -17.950000. running mean: -10.431834\n",
      "epsilon:0.095950 episode_count: 16429. steps_count: 7113594.000000\n",
      "Time elapsed:  20701.726260900497\n",
      "ep 2347: ep_len:265 episode reward: total was 1.390000. running mean: -10.313616\n",
      "ep 2347: ep_len:519 episode reward: total was -29.040000. running mean: -10.500879\n",
      "ep 2347: ep_len:607 episode reward: total was -61.420000. running mean: -11.010071\n",
      "ep 2347: ep_len:541 episode reward: total was -9.010000. running mean: -10.990070\n",
      "ep 2347: ep_len:3 episode reward: total was 1.010000. running mean: -10.870069\n",
      "ep 2347: ep_len:639 episode reward: total was -13.220000. running mean: -10.893569\n",
      "ep 2347: ep_len:313 episode reward: total was -66.400000. running mean: -11.448633\n",
      "epsilon:0.095905 episode_count: 16436. steps_count: 7116481.000000\n",
      "Time elapsed:  20709.52037000656\n",
      "ep 2348: ep_len:229 episode reward: total was 1.180000. running mean: -11.322346\n",
      "ep 2348: ep_len:510 episode reward: total was -37.870000. running mean: -11.587823\n",
      "ep 2348: ep_len:380 episode reward: total was 36.310000. running mean: -11.108845\n",
      "ep 2348: ep_len:500 episode reward: total was -5.120000. running mean: -11.048956\n",
      "ep 2348: ep_len:128 episode reward: total was 19.380000. running mean: -10.744667\n",
      "ep 2348: ep_len:518 episode reward: total was -28.300000. running mean: -10.920220\n",
      "ep 2348: ep_len:634 episode reward: total was -57.760000. running mean: -11.388618\n",
      "epsilon:0.095861 episode_count: 16443. steps_count: 7119380.000000\n",
      "Time elapsed:  20717.341179847717\n",
      "ep 2349: ep_len:501 episode reward: total was -112.190000. running mean: -12.396632\n",
      "ep 2349: ep_len:526 episode reward: total was -43.500000. running mean: -12.707665\n",
      "ep 2349: ep_len:500 episode reward: total was 18.930000. running mean: -12.391289\n",
      "ep 2349: ep_len:502 episode reward: total was 15.530000. running mean: -12.112076\n",
      "ep 2349: ep_len:49 episode reward: total was 17.000000. running mean: -11.820955\n",
      "ep 2349: ep_len:589 episode reward: total was -3.940000. running mean: -11.742146\n",
      "ep 2349: ep_len:622 episode reward: total was -28.430000. running mean: -11.909024\n",
      "epsilon:0.095817 episode_count: 16450. steps_count: 7122669.000000\n",
      "Time elapsed:  20725.94221639633\n",
      "ep 2350: ep_len:621 episode reward: total was -44.160000. running mean: -12.231534\n",
      "ep 2350: ep_len:621 episode reward: total was -51.550000. running mean: -12.624719\n",
      "ep 2350: ep_len:399 episode reward: total was 30.940000. running mean: -12.189071\n",
      "ep 2350: ep_len:542 episode reward: total was 0.920000. running mean: -12.057981\n",
      "ep 2350: ep_len:3 episode reward: total was 1.010000. running mean: -11.927301\n",
      "ep 2350: ep_len:521 episode reward: total was 3.250000. running mean: -11.775528\n",
      "ep 2350: ep_len:544 episode reward: total was -37.470000. running mean: -12.032473\n",
      "epsilon:0.095772 episode_count: 16457. steps_count: 7125920.000000\n",
      "Time elapsed:  20734.66123366356\n",
      "ep 2351: ep_len:500 episode reward: total was -81.350000. running mean: -12.725648\n",
      "ep 2351: ep_len:527 episode reward: total was -39.430000. running mean: -12.992691\n",
      "ep 2351: ep_len:450 episode reward: total was 17.190000. running mean: -12.690864\n",
      "ep 2351: ep_len:56 episode reward: total was 0.820000. running mean: -12.555756\n",
      "ep 2351: ep_len:55 episode reward: total was 15.500000. running mean: -12.275198\n",
      "ep 2351: ep_len:500 episode reward: total was -12.170000. running mean: -12.274146\n",
      "ep 2351: ep_len:592 episode reward: total was -131.910000. running mean: -13.470505\n",
      "epsilon:0.095728 episode_count: 16464. steps_count: 7128600.000000\n",
      "Time elapsed:  20741.900423526764\n",
      "ep 2352: ep_len:134 episode reward: total was -2.610000. running mean: -13.361900\n",
      "ep 2352: ep_len:563 episode reward: total was -5.660000. running mean: -13.284881\n",
      "ep 2352: ep_len:500 episode reward: total was -1.930000. running mean: -13.171332\n",
      "ep 2352: ep_len:529 episode reward: total was 31.120000. running mean: -12.728419\n",
      "ep 2352: ep_len:87 episode reward: total was 24.190000. running mean: -12.359234\n",
      "ep 2352: ep_len:557 episode reward: total was -59.450000. running mean: -12.830142\n",
      "ep 2352: ep_len:500 episode reward: total was -36.160000. running mean: -13.063441\n",
      "epsilon:0.095684 episode_count: 16471. steps_count: 7131470.000000\n",
      "Time elapsed:  20749.803642749786\n",
      "ep 2353: ep_len:221 episode reward: total was 14.260000. running mean: -12.790206\n",
      "ep 2353: ep_len:544 episode reward: total was -127.150000. running mean: -13.933804\n",
      "ep 2353: ep_len:649 episode reward: total was -31.170000. running mean: -14.106166\n",
      "ep 2353: ep_len:513 episode reward: total was 24.520000. running mean: -13.719904\n",
      "ep 2353: ep_len:94 episode reward: total was 24.260000. running mean: -13.340105\n",
      "ep 2353: ep_len:194 episode reward: total was 41.450000. running mean: -12.792204\n",
      "ep 2353: ep_len:594 episode reward: total was -19.180000. running mean: -12.856082\n",
      "epsilon:0.095639 episode_count: 16478. steps_count: 7134279.000000\n",
      "Time elapsed:  20757.373924970627\n",
      "ep 2354: ep_len:568 episode reward: total was 38.270000. running mean: -12.344822\n",
      "ep 2354: ep_len:500 episode reward: total was -13.980000. running mean: -12.361173\n",
      "ep 2354: ep_len:574 episode reward: total was -25.320000. running mean: -12.490762\n",
      "ep 2354: ep_len:546 episode reward: total was 24.870000. running mean: -12.117154\n",
      "ep 2354: ep_len:3 episode reward: total was 0.000000. running mean: -11.995982\n",
      "ep 2354: ep_len:606 episode reward: total was -12.360000. running mean: -11.999623\n",
      "ep 2354: ep_len:503 episode reward: total was -54.390000. running mean: -12.423526\n",
      "epsilon:0.095595 episode_count: 16485. steps_count: 7137579.000000\n",
      "Time elapsed:  20765.960717201233\n",
      "ep 2355: ep_len:595 episode reward: total was 5.250000. running mean: -12.246791\n",
      "ep 2355: ep_len:611 episode reward: total was -0.730000. running mean: -12.131623\n",
      "ep 2355: ep_len:500 episode reward: total was 5.940000. running mean: -11.950907\n",
      "ep 2355: ep_len:390 episode reward: total was -0.490000. running mean: -11.836298\n",
      "ep 2355: ep_len:95 episode reward: total was 27.210000. running mean: -11.445835\n",
      "ep 2355: ep_len:575 episode reward: total was -76.890000. running mean: -12.100277\n",
      "ep 2355: ep_len:500 episode reward: total was -34.010000. running mean: -12.319374\n",
      "epsilon:0.095551 episode_count: 16492. steps_count: 7140845.000000\n",
      "Time elapsed:  20774.68488907814\n",
      "ep 2356: ep_len:229 episode reward: total was 6.230000. running mean: -12.133880\n",
      "ep 2356: ep_len:576 episode reward: total was -44.480000. running mean: -12.457341\n",
      "ep 2356: ep_len:540 episode reward: total was -58.740000. running mean: -12.920168\n",
      "ep 2356: ep_len:500 episode reward: total was 0.720000. running mean: -12.783766\n",
      "ep 2356: ep_len:3 episode reward: total was -1.500000. running mean: -12.670929\n",
      "ep 2356: ep_len:648 episode reward: total was 24.470000. running mean: -12.299519\n",
      "ep 2356: ep_len:504 episode reward: total was -41.360000. running mean: -12.590124\n",
      "epsilon:0.095506 episode_count: 16499. steps_count: 7143845.000000\n",
      "Time elapsed:  20782.747772932053\n",
      "ep 2357: ep_len:244 episode reward: total was -9.370000. running mean: -12.557923\n",
      "ep 2357: ep_len:517 episode reward: total was -33.650000. running mean: -12.768844\n",
      "ep 2357: ep_len:681 episode reward: total was -30.540000. running mean: -12.946555\n",
      "ep 2357: ep_len:500 episode reward: total was 36.640000. running mean: -12.450690\n",
      "ep 2357: ep_len:3 episode reward: total was -0.490000. running mean: -12.331083\n",
      "ep 2357: ep_len:551 episode reward: total was -21.720000. running mean: -12.424972\n",
      "ep 2357: ep_len:553 episode reward: total was -46.970000. running mean: -12.770422\n",
      "epsilon:0.095462 episode_count: 16506. steps_count: 7146894.000000\n",
      "Time elapsed:  20791.099044561386\n",
      "ep 2358: ep_len:699 episode reward: total was -36.360000. running mean: -13.006318\n",
      "ep 2358: ep_len:514 episode reward: total was -65.110000. running mean: -13.527355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2358: ep_len:429 episode reward: total was 25.580000. running mean: -13.136281\n",
      "ep 2358: ep_len:508 episode reward: total was 8.950000. running mean: -12.915418\n",
      "ep 2358: ep_len:96 episode reward: total was 18.740000. running mean: -12.598864\n",
      "ep 2358: ep_len:537 episode reward: total was -11.350000. running mean: -12.586376\n",
      "ep 2358: ep_len:500 episode reward: total was -57.280000. running mean: -13.033312\n",
      "epsilon:0.095418 episode_count: 16513. steps_count: 7150177.000000\n",
      "Time elapsed:  20801.827481985092\n",
      "ep 2359: ep_len:211 episode reward: total was 0.600000. running mean: -12.896979\n",
      "ep 2359: ep_len:567 episode reward: total was -12.200000. running mean: -12.890009\n",
      "ep 2359: ep_len:606 episode reward: total was -20.350000. running mean: -12.964609\n",
      "ep 2359: ep_len:161 episode reward: total was -8.910000. running mean: -12.924063\n",
      "ep 2359: ep_len:3 episode reward: total was 1.010000. running mean: -12.784722\n",
      "ep 2359: ep_len:579 episode reward: total was -18.080000. running mean: -12.837675\n",
      "ep 2359: ep_len:529 episode reward: total was -6.430000. running mean: -12.773598\n",
      "epsilon:0.095373 episode_count: 16520. steps_count: 7152833.000000\n",
      "Time elapsed:  20809.11974310875\n",
      "ep 2360: ep_len:546 episode reward: total was -36.610000. running mean: -13.011962\n",
      "ep 2360: ep_len:585 episode reward: total was 35.690000. running mean: -12.524943\n",
      "ep 2360: ep_len:638 episode reward: total was -73.700000. running mean: -13.136693\n",
      "ep 2360: ep_len:525 episode reward: total was 1.600000. running mean: -12.989326\n",
      "ep 2360: ep_len:36 episode reward: total was 13.500000. running mean: -12.724433\n",
      "ep 2360: ep_len:547 episode reward: total was 3.450000. running mean: -12.562689\n",
      "ep 2360: ep_len:540 episode reward: total was -11.010000. running mean: -12.547162\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.095329 episode_count: 16527. steps_count: 7156250.000000\n",
      "Time elapsed:  20822.452210903168\n",
      "ep 2361: ep_len:582 episode reward: total was -41.880000. running mean: -12.840490\n",
      "ep 2361: ep_len:500 episode reward: total was 16.230000. running mean: -12.549785\n",
      "ep 2361: ep_len:549 episode reward: total was -80.950000. running mean: -13.233787\n",
      "ep 2361: ep_len:370 episode reward: total was 18.670000. running mean: -12.914749\n",
      "ep 2361: ep_len:129 episode reward: total was 20.300000. running mean: -12.582602\n",
      "ep 2361: ep_len:566 episode reward: total was -33.700000. running mean: -12.793776\n",
      "ep 2361: ep_len:500 episode reward: total was -6.380000. running mean: -12.729638\n",
      "epsilon:0.095285 episode_count: 16534. steps_count: 7159446.000000\n",
      "Time elapsed:  20831.006552934647\n",
      "ep 2362: ep_len:676 episode reward: total was -61.910000. running mean: -13.221442\n",
      "ep 2362: ep_len:512 episode reward: total was 19.110000. running mean: -12.898127\n",
      "ep 2362: ep_len:384 episode reward: total was -12.500000. running mean: -12.894146\n",
      "ep 2362: ep_len:536 episode reward: total was 10.070000. running mean: -12.664505\n",
      "ep 2362: ep_len:3 episode reward: total was -1.500000. running mean: -12.552860\n",
      "ep 2362: ep_len:500 episode reward: total was 3.930000. running mean: -12.388031\n",
      "ep 2362: ep_len:587 episode reward: total was -116.740000. running mean: -13.431551\n",
      "epsilon:0.095240 episode_count: 16541. steps_count: 7162644.000000\n",
      "Time elapsed:  20839.34916329384\n",
      "ep 2363: ep_len:500 episode reward: total was 28.500000. running mean: -13.012235\n",
      "ep 2363: ep_len:501 episode reward: total was -27.150000. running mean: -13.153613\n",
      "ep 2363: ep_len:566 episode reward: total was -33.090000. running mean: -13.352977\n",
      "ep 2363: ep_len:56 episode reward: total was 3.820000. running mean: -13.181247\n",
      "ep 2363: ep_len:96 episode reward: total was 14.670000. running mean: -12.902734\n",
      "ep 2363: ep_len:508 episode reward: total was -32.730000. running mean: -13.101007\n",
      "ep 2363: ep_len:514 episode reward: total was -39.890000. running mean: -13.368897\n",
      "epsilon:0.095196 episode_count: 16548. steps_count: 7165385.000000\n",
      "Time elapsed:  20850.480680704117\n",
      "ep 2364: ep_len:500 episode reward: total was 48.710000. running mean: -12.748108\n",
      "ep 2364: ep_len:500 episode reward: total was 6.170000. running mean: -12.558927\n",
      "ep 2364: ep_len:583 episode reward: total was -55.840000. running mean: -12.991738\n",
      "ep 2364: ep_len:615 episode reward: total was 28.210000. running mean: -12.579720\n",
      "ep 2364: ep_len:3 episode reward: total was 1.010000. running mean: -12.443823\n",
      "ep 2364: ep_len:177 episode reward: total was 8.050000. running mean: -12.238885\n",
      "ep 2364: ep_len:567 episode reward: total was -32.960000. running mean: -12.446096\n",
      "epsilon:0.095152 episode_count: 16555. steps_count: 7168330.000000\n",
      "Time elapsed:  20858.403637886047\n",
      "ep 2365: ep_len:567 episode reward: total was -124.440000. running mean: -13.566035\n",
      "ep 2365: ep_len:609 episode reward: total was 37.960000. running mean: -13.050775\n",
      "ep 2365: ep_len:635 episode reward: total was -24.760000. running mean: -13.167867\n",
      "ep 2365: ep_len:500 episode reward: total was -9.000000. running mean: -13.126188\n",
      "ep 2365: ep_len:2 episode reward: total was -0.500000. running mean: -12.999926\n",
      "ep 2365: ep_len:616 episode reward: total was -11.690000. running mean: -12.986827\n",
      "ep 2365: ep_len:287 episode reward: total was -14.790000. running mean: -13.004859\n",
      "epsilon:0.095107 episode_count: 16562. steps_count: 7171546.000000\n",
      "Time elapsed:  20870.64698123932\n",
      "ep 2366: ep_len:134 episode reward: total was 8.990000. running mean: -12.784910\n",
      "ep 2366: ep_len:510 episode reward: total was -0.180000. running mean: -12.658861\n",
      "ep 2366: ep_len:506 episode reward: total was -32.260000. running mean: -12.854873\n",
      "ep 2366: ep_len:500 episode reward: total was -8.400000. running mean: -12.810324\n",
      "ep 2366: ep_len:3 episode reward: total was 1.010000. running mean: -12.672121\n",
      "ep 2366: ep_len:501 episode reward: total was -17.910000. running mean: -12.724499\n",
      "ep 2366: ep_len:211 episode reward: total was -22.520000. running mean: -12.822454\n",
      "epsilon:0.095063 episode_count: 16569. steps_count: 7173911.000000\n",
      "Time elapsed:  20877.194563388824\n",
      "ep 2367: ep_len:517 episode reward: total was -65.720000. running mean: -13.351430\n",
      "ep 2367: ep_len:594 episode reward: total was 10.520000. running mean: -13.112716\n",
      "ep 2367: ep_len:412 episode reward: total was 1.800000. running mean: -12.963588\n",
      "ep 2367: ep_len:500 episode reward: total was 20.630000. running mean: -12.627653\n",
      "ep 2367: ep_len:3 episode reward: total was 1.010000. running mean: -12.491276\n",
      "ep 2367: ep_len:513 episode reward: total was -19.550000. running mean: -12.561863\n",
      "ep 2367: ep_len:602 episode reward: total was -30.520000. running mean: -12.741445\n",
      "epsilon:0.095019 episode_count: 16576. steps_count: 7177052.000000\n",
      "Time elapsed:  20885.613293409348\n",
      "ep 2368: ep_len:510 episode reward: total was 38.410000. running mean: -12.229930\n",
      "ep 2368: ep_len:500 episode reward: total was -5.980000. running mean: -12.167431\n",
      "ep 2368: ep_len:500 episode reward: total was -18.980000. running mean: -12.235557\n",
      "ep 2368: ep_len:53 episode reward: total was 0.820000. running mean: -12.105001\n",
      "ep 2368: ep_len:3 episode reward: total was 1.010000. running mean: -11.973851\n",
      "ep 2368: ep_len:500 episode reward: total was -29.450000. running mean: -12.148612\n",
      "ep 2368: ep_len:500 episode reward: total was -39.700000. running mean: -12.424126\n",
      "epsilon:0.094974 episode_count: 16583. steps_count: 7179618.000000\n",
      "Time elapsed:  20892.56300497055\n",
      "ep 2369: ep_len:500 episode reward: total was -11.010000. running mean: -12.409985\n",
      "ep 2369: ep_len:500 episode reward: total was 10.950000. running mean: -12.176385\n",
      "ep 2369: ep_len:500 episode reward: total was -3.090000. running mean: -12.085521\n",
      "ep 2369: ep_len:507 episode reward: total was 18.690000. running mean: -11.777766\n",
      "ep 2369: ep_len:92 episode reward: total was 15.730000. running mean: -11.502689\n",
      "ep 2369: ep_len:173 episode reward: total was 20.680000. running mean: -11.180862\n",
      "ep 2369: ep_len:525 episode reward: total was -71.550000. running mean: -11.784553\n",
      "epsilon:0.094930 episode_count: 16590. steps_count: 7182415.000000\n",
      "Time elapsed:  20899.98078918457\n",
      "ep 2370: ep_len:516 episode reward: total was 9.420000. running mean: -11.572507\n",
      "ep 2370: ep_len:501 episode reward: total was -16.820000. running mean: -11.624982\n",
      "ep 2370: ep_len:893 episode reward: total was -310.290000. running mean: -14.611633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2370: ep_len:107 episode reward: total was 10.560000. running mean: -14.359916\n",
      "ep 2370: ep_len:83 episode reward: total was 16.160000. running mean: -14.054717\n",
      "ep 2370: ep_len:501 episode reward: total was -5.910000. running mean: -13.973270\n",
      "ep 2370: ep_len:358 episode reward: total was -34.170000. running mean: -14.175237\n",
      "epsilon:0.094886 episode_count: 16597. steps_count: 7185374.000000\n",
      "Time elapsed:  20907.93093252182\n",
      "ep 2371: ep_len:564 episode reward: total was -69.270000. running mean: -14.726185\n",
      "ep 2371: ep_len:559 episode reward: total was -108.870000. running mean: -15.667623\n",
      "ep 2371: ep_len:647 episode reward: total was -150.110000. running mean: -17.012047\n",
      "ep 2371: ep_len:500 episode reward: total was -19.340000. running mean: -17.035326\n",
      "ep 2371: ep_len:94 episode reward: total was 26.740000. running mean: -16.597573\n",
      "ep 2371: ep_len:680 episode reward: total was -157.730000. running mean: -18.008897\n",
      "ep 2371: ep_len:526 episode reward: total was -38.880000. running mean: -18.217608\n",
      "epsilon:0.094841 episode_count: 16604. steps_count: 7188944.000000\n",
      "Time elapsed:  20921.25896716118\n",
      "ep 2372: ep_len:500 episode reward: total was -3.660000. running mean: -18.072032\n",
      "ep 2372: ep_len:518 episode reward: total was -26.040000. running mean: -18.151712\n",
      "ep 2372: ep_len:555 episode reward: total was -28.340000. running mean: -18.253595\n",
      "ep 2372: ep_len:54 episode reward: total was 1.260000. running mean: -18.058459\n",
      "ep 2372: ep_len:3 episode reward: total was 1.010000. running mean: -17.867774\n",
      "ep 2372: ep_len:563 episode reward: total was -13.950000. running mean: -17.828597\n",
      "ep 2372: ep_len:560 episode reward: total was -47.960000. running mean: -18.129911\n",
      "epsilon:0.094797 episode_count: 16611. steps_count: 7191697.000000\n",
      "Time elapsed:  20928.652684688568\n",
      "ep 2373: ep_len:650 episode reward: total was -138.200000. running mean: -19.330611\n",
      "ep 2373: ep_len:292 episode reward: total was -10.300000. running mean: -19.240305\n",
      "ep 2373: ep_len:606 episode reward: total was -88.540000. running mean: -19.933302\n",
      "ep 2373: ep_len:524 episode reward: total was -30.920000. running mean: -20.043169\n",
      "ep 2373: ep_len:77 episode reward: total was 18.640000. running mean: -19.656338\n",
      "ep 2373: ep_len:500 episode reward: total was -34.770000. running mean: -19.807474\n",
      "ep 2373: ep_len:500 episode reward: total was -18.560000. running mean: -19.794999\n",
      "epsilon:0.094753 episode_count: 16618. steps_count: 7194846.000000\n",
      "Time elapsed:  20940.65177631378\n",
      "ep 2374: ep_len:555 episode reward: total was -12.050000. running mean: -19.717549\n",
      "ep 2374: ep_len:500 episode reward: total was 29.450000. running mean: -19.225874\n",
      "ep 2374: ep_len:500 episode reward: total was 6.130000. running mean: -18.972315\n",
      "ep 2374: ep_len:573 episode reward: total was -13.630000. running mean: -18.918892\n",
      "ep 2374: ep_len:96 episode reward: total was 21.250000. running mean: -18.517203\n",
      "ep 2374: ep_len:528 episode reward: total was -95.400000. running mean: -19.286031\n",
      "ep 2374: ep_len:575 episode reward: total was 4.640000. running mean: -19.046771\n",
      "epsilon:0.094708 episode_count: 16625. steps_count: 7198173.000000\n",
      "Time elapsed:  20949.132436037064\n",
      "ep 2375: ep_len:515 episode reward: total was -92.650000. running mean: -19.782803\n",
      "ep 2375: ep_len:500 episode reward: total was 65.750000. running mean: -18.927475\n",
      "ep 2375: ep_len:660 episode reward: total was -32.430000. running mean: -19.062500\n",
      "ep 2375: ep_len:540 episode reward: total was 32.400000. running mean: -18.547875\n",
      "ep 2375: ep_len:110 episode reward: total was 24.310000. running mean: -18.119297\n",
      "ep 2375: ep_len:525 episode reward: total was -49.910000. running mean: -18.437204\n",
      "ep 2375: ep_len:584 episode reward: total was 3.350000. running mean: -18.219332\n",
      "epsilon:0.094664 episode_count: 16632. steps_count: 7201607.000000\n",
      "Time elapsed:  20961.31814146042\n",
      "ep 2376: ep_len:500 episode reward: total was -50.880000. running mean: -18.545938\n",
      "ep 2376: ep_len:500 episode reward: total was 43.280000. running mean: -17.927679\n",
      "ep 2376: ep_len:528 episode reward: total was -50.570000. running mean: -18.254102\n",
      "ep 2376: ep_len:504 episode reward: total was 0.680000. running mean: -18.064761\n",
      "ep 2376: ep_len:3 episode reward: total was 0.000000. running mean: -17.884113\n",
      "ep 2376: ep_len:525 episode reward: total was -25.520000. running mean: -17.960472\n",
      "ep 2376: ep_len:324 episode reward: total was -5.160000. running mean: -17.832468\n",
      "epsilon:0.094620 episode_count: 16639. steps_count: 7204491.000000\n",
      "Time elapsed:  20969.4206199646\n",
      "ep 2377: ep_len:500 episode reward: total was 21.410000. running mean: -17.440043\n",
      "ep 2377: ep_len:515 episode reward: total was -30.800000. running mean: -17.573642\n",
      "ep 2377: ep_len:500 episode reward: total was -20.010000. running mean: -17.598006\n",
      "ep 2377: ep_len:500 episode reward: total was 4.350000. running mean: -17.378526\n",
      "ep 2377: ep_len:88 episode reward: total was 22.760000. running mean: -16.977141\n",
      "ep 2377: ep_len:603 episode reward: total was -10.780000. running mean: -16.915169\n",
      "ep 2377: ep_len:521 episode reward: total was -22.800000. running mean: -16.974018\n",
      "epsilon:0.094575 episode_count: 16646. steps_count: 7207718.000000\n",
      "Time elapsed:  20978.033568143845\n",
      "ep 2378: ep_len:607 episode reward: total was 32.590000. running mean: -16.478377\n",
      "ep 2378: ep_len:550 episode reward: total was -42.640000. running mean: -16.739994\n",
      "ep 2378: ep_len:363 episode reward: total was -0.500000. running mean: -16.577594\n",
      "ep 2378: ep_len:509 episode reward: total was 16.710000. running mean: -16.244718\n",
      "ep 2378: ep_len:3 episode reward: total was 1.010000. running mean: -16.072171\n",
      "ep 2378: ep_len:162 episode reward: total was 16.560000. running mean: -15.745849\n",
      "ep 2378: ep_len:500 episode reward: total was 29.290000. running mean: -15.295490\n",
      "epsilon:0.094531 episode_count: 16653. steps_count: 7210412.000000\n",
      "Time elapsed:  20985.297442913055\n",
      "ep 2379: ep_len:518 episode reward: total was -45.350000. running mean: -15.596036\n",
      "ep 2379: ep_len:550 episode reward: total was 58.530000. running mean: -14.854775\n",
      "ep 2379: ep_len:368 episode reward: total was 33.710000. running mean: -14.369127\n",
      "ep 2379: ep_len:584 episode reward: total was 1.110000. running mean: -14.214336\n",
      "ep 2379: ep_len:3 episode reward: total was 0.000000. running mean: -14.072193\n",
      "ep 2379: ep_len:624 episode reward: total was 7.730000. running mean: -13.854171\n",
      "ep 2379: ep_len:567 episode reward: total was -45.780000. running mean: -14.173429\n",
      "epsilon:0.094487 episode_count: 16660. steps_count: 7213626.000000\n",
      "Time elapsed:  20993.905848026276\n",
      "ep 2380: ep_len:571 episode reward: total was -43.180000. running mean: -14.463495\n",
      "ep 2380: ep_len:594 episode reward: total was 9.080000. running mean: -14.228060\n",
      "ep 2380: ep_len:574 episode reward: total was -16.480000. running mean: -14.250579\n",
      "ep 2380: ep_len:526 episode reward: total was -15.590000. running mean: -14.263974\n",
      "ep 2380: ep_len:3 episode reward: total was 1.010000. running mean: -14.111234\n",
      "ep 2380: ep_len:500 episode reward: total was -5.130000. running mean: -14.021421\n",
      "ep 2380: ep_len:599 episode reward: total was -6.640000. running mean: -13.947607\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.094442 episode_count: 16667. steps_count: 7216993.000000\n",
      "Time elapsed:  21007.64479970932\n",
      "ep 2381: ep_len:644 episode reward: total was -68.130000. running mean: -14.489431\n",
      "ep 2381: ep_len:593 episode reward: total was -33.380000. running mean: -14.678337\n",
      "ep 2381: ep_len:563 episode reward: total was -36.680000. running mean: -14.898353\n",
      "ep 2381: ep_len:510 episode reward: total was -88.420000. running mean: -15.633570\n",
      "ep 2381: ep_len:106 episode reward: total was 23.250000. running mean: -15.244734\n",
      "ep 2381: ep_len:228 episode reward: total was 19.960000. running mean: -14.892687\n",
      "ep 2381: ep_len:500 episode reward: total was -87.470000. running mean: -15.618460\n",
      "epsilon:0.094398 episode_count: 16674. steps_count: 7220137.000000\n",
      "Time elapsed:  21015.990583896637\n",
      "ep 2382: ep_len:619 episode reward: total was -46.690000. running mean: -15.929175\n",
      "ep 2382: ep_len:604 episode reward: total was -11.630000. running mean: -15.886184\n",
      "ep 2382: ep_len:585 episode reward: total was -44.350000. running mean: -16.170822\n",
      "ep 2382: ep_len:558 episode reward: total was 4.150000. running mean: -15.967614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2382: ep_len:112 episode reward: total was 9.270000. running mean: -15.715237\n",
      "ep 2382: ep_len:695 episode reward: total was -18.670000. running mean: -15.744785\n",
      "ep 2382: ep_len:500 episode reward: total was -41.180000. running mean: -15.999137\n",
      "epsilon:0.094354 episode_count: 16681. steps_count: 7223810.000000\n",
      "Time elapsed:  21026.250193834305\n",
      "ep 2383: ep_len:638 episode reward: total was 12.030000. running mean: -15.718846\n",
      "ep 2383: ep_len:582 episode reward: total was 40.780000. running mean: -15.153857\n",
      "ep 2383: ep_len:500 episode reward: total was -9.810000. running mean: -15.100419\n",
      "ep 2383: ep_len:500 episode reward: total was -97.010000. running mean: -15.919515\n",
      "ep 2383: ep_len:44 episode reward: total was -22.500000. running mean: -15.985320\n",
      "ep 2383: ep_len:500 episode reward: total was 11.740000. running mean: -15.708066\n",
      "ep 2383: ep_len:500 episode reward: total was -24.660000. running mean: -15.797586\n",
      "epsilon:0.094309 episode_count: 16688. steps_count: 7227074.000000\n",
      "Time elapsed:  21034.99396300316\n",
      "ep 2384: ep_len:663 episode reward: total was -22.460000. running mean: -15.864210\n",
      "ep 2384: ep_len:500 episode reward: total was 1.400000. running mean: -15.691568\n",
      "ep 2384: ep_len:500 episode reward: total was -6.750000. running mean: -15.602152\n",
      "ep 2384: ep_len:587 episode reward: total was 4.660000. running mean: -15.399531\n",
      "ep 2384: ep_len:3 episode reward: total was 1.010000. running mean: -15.235435\n",
      "ep 2384: ep_len:525 episode reward: total was -31.850000. running mean: -15.401581\n",
      "ep 2384: ep_len:500 episode reward: total was -53.520000. running mean: -15.782765\n",
      "epsilon:0.094265 episode_count: 16695. steps_count: 7230352.000000\n",
      "Time elapsed:  21043.59161376953\n",
      "ep 2385: ep_len:506 episode reward: total was -2.510000. running mean: -15.650037\n",
      "ep 2385: ep_len:503 episode reward: total was -5.280000. running mean: -15.546337\n",
      "ep 2385: ep_len:570 episode reward: total was -54.420000. running mean: -15.935074\n",
      "ep 2385: ep_len:520 episode reward: total was -57.370000. running mean: -16.349423\n",
      "ep 2385: ep_len:3 episode reward: total was 1.010000. running mean: -16.175829\n",
      "ep 2385: ep_len:628 episode reward: total was -23.850000. running mean: -16.252570\n",
      "ep 2385: ep_len:598 episode reward: total was 10.670000. running mean: -15.983345\n",
      "epsilon:0.094221 episode_count: 16702. steps_count: 7233680.000000\n",
      "Time elapsed:  21052.303394317627\n",
      "ep 2386: ep_len:629 episode reward: total was -32.910000. running mean: -16.152611\n",
      "ep 2386: ep_len:377 episode reward: total was -42.300000. running mean: -16.414085\n",
      "ep 2386: ep_len:561 episode reward: total was -48.780000. running mean: -16.737744\n",
      "ep 2386: ep_len:579 episode reward: total was 40.300000. running mean: -16.167367\n",
      "ep 2386: ep_len:3 episode reward: total was 1.010000. running mean: -15.995593\n",
      "ep 2386: ep_len:516 episode reward: total was -37.730000. running mean: -16.212937\n",
      "ep 2386: ep_len:586 episode reward: total was -48.650000. running mean: -16.537308\n",
      "epsilon:0.094176 episode_count: 16709. steps_count: 7236931.000000\n",
      "Time elapsed:  21060.80871605873\n",
      "ep 2387: ep_len:573 episode reward: total was 43.280000. running mean: -15.939135\n",
      "ep 2387: ep_len:509 episode reward: total was -27.240000. running mean: -16.052143\n",
      "ep 2387: ep_len:506 episode reward: total was 17.720000. running mean: -15.714422\n",
      "ep 2387: ep_len:500 episode reward: total was 21.320000. running mean: -15.344078\n",
      "ep 2387: ep_len:3 episode reward: total was -1.990000. running mean: -15.210537\n",
      "ep 2387: ep_len:522 episode reward: total was -12.470000. running mean: -15.183132\n",
      "ep 2387: ep_len:592 episode reward: total was -14.660000. running mean: -15.177900\n",
      "epsilon:0.094132 episode_count: 16716. steps_count: 7240136.000000\n",
      "Time elapsed:  21069.21179986\n",
      "ep 2388: ep_len:227 episode reward: total was 7.340000. running mean: -14.952721\n",
      "ep 2388: ep_len:500 episode reward: total was -46.060000. running mean: -15.263794\n",
      "ep 2388: ep_len:369 episode reward: total was -6.080000. running mean: -15.171956\n",
      "ep 2388: ep_len:500 episode reward: total was 2.890000. running mean: -14.991337\n",
      "ep 2388: ep_len:126 episode reward: total was 23.760000. running mean: -14.603823\n",
      "ep 2388: ep_len:182 episode reward: total was 22.670000. running mean: -14.231085\n",
      "ep 2388: ep_len:528 episode reward: total was -11.580000. running mean: -14.204574\n",
      "epsilon:0.094088 episode_count: 16723. steps_count: 7242568.000000\n",
      "Time elapsed:  21075.61677670479\n",
      "ep 2389: ep_len:599 episode reward: total was -81.230000. running mean: -14.874828\n",
      "ep 2389: ep_len:500 episode reward: total was -7.350000. running mean: -14.799580\n",
      "ep 2389: ep_len:680 episode reward: total was -31.040000. running mean: -14.961984\n",
      "ep 2389: ep_len:41 episode reward: total was 4.190000. running mean: -14.770464\n",
      "ep 2389: ep_len:77 episode reward: total was -46.760000. running mean: -15.090360\n",
      "ep 2389: ep_len:522 episode reward: total was -17.440000. running mean: -15.113856\n",
      "ep 2389: ep_len:342 episode reward: total was -21.540000. running mean: -15.178118\n",
      "epsilon:0.094043 episode_count: 16730. steps_count: 7245329.000000\n",
      "Time elapsed:  21080.04195547104\n",
      "ep 2390: ep_len:568 episode reward: total was -105.650000. running mean: -16.082837\n",
      "ep 2390: ep_len:522 episode reward: total was 60.720000. running mean: -15.314808\n",
      "ep 2390: ep_len:668 episode reward: total was -79.100000. running mean: -15.952660\n",
      "ep 2390: ep_len:592 episode reward: total was 43.560000. running mean: -15.357533\n",
      "ep 2390: ep_len:3 episode reward: total was 1.010000. running mean: -15.193858\n",
      "ep 2390: ep_len:227 episode reward: total was 30.420000. running mean: -14.737720\n",
      "ep 2390: ep_len:303 episode reward: total was 11.910000. running mean: -14.471242\n",
      "epsilon:0.093999 episode_count: 16737. steps_count: 7248212.000000\n",
      "Time elapsed:  21085.73053908348\n",
      "ep 2391: ep_len:546 episode reward: total was -38.780000. running mean: -14.714330\n",
      "ep 2391: ep_len:500 episode reward: total was -17.010000. running mean: -14.737287\n",
      "ep 2391: ep_len:530 episode reward: total was -44.470000. running mean: -15.034614\n",
      "ep 2391: ep_len:501 episode reward: total was -50.260000. running mean: -15.386868\n",
      "ep 2391: ep_len:3 episode reward: total was 1.010000. running mean: -15.222899\n",
      "ep 2391: ep_len:647 episode reward: total was -3.490000. running mean: -15.105570\n",
      "ep 2391: ep_len:277 episode reward: total was 4.390000. running mean: -14.910614\n",
      "epsilon:0.093955 episode_count: 16744. steps_count: 7251216.000000\n",
      "Time elapsed:  21093.580675840378\n",
      "ep 2392: ep_len:564 episode reward: total was 8.420000. running mean: -14.677308\n",
      "ep 2392: ep_len:626 episode reward: total was 14.380000. running mean: -14.386735\n",
      "ep 2392: ep_len:500 episode reward: total was -14.880000. running mean: -14.391668\n",
      "ep 2392: ep_len:414 episode reward: total was -67.470000. running mean: -14.922451\n",
      "ep 2392: ep_len:100 episode reward: total was 24.230000. running mean: -14.530926\n",
      "ep 2392: ep_len:244 episode reward: total was 6.960000. running mean: -14.316017\n",
      "ep 2392: ep_len:504 episode reward: total was -11.170000. running mean: -14.284557\n",
      "epsilon:0.093910 episode_count: 16751. steps_count: 7254168.000000\n",
      "Time elapsed:  21101.457250118256\n",
      "ep 2393: ep_len:531 episode reward: total was -13.400000. running mean: -14.275711\n",
      "ep 2393: ep_len:500 episode reward: total was -13.850000. running mean: -14.271454\n",
      "ep 2393: ep_len:500 episode reward: total was -50.870000. running mean: -14.637440\n",
      "ep 2393: ep_len:500 episode reward: total was -25.470000. running mean: -14.745765\n",
      "ep 2393: ep_len:3 episode reward: total was 1.010000. running mean: -14.588208\n",
      "ep 2393: ep_len:538 episode reward: total was -31.340000. running mean: -14.755726\n",
      "ep 2393: ep_len:528 episode reward: total was -9.440000. running mean: -14.702568\n",
      "epsilon:0.093866 episode_count: 16758. steps_count: 7257268.000000\n",
      "Time elapsed:  21108.577793121338\n",
      "ep 2394: ep_len:607 episode reward: total was -92.170000. running mean: -15.477243\n",
      "ep 2394: ep_len:500 episode reward: total was 2.480000. running mean: -15.297670\n",
      "ep 2394: ep_len:411 episode reward: total was -19.940000. running mean: -15.344094\n",
      "ep 2394: ep_len:500 episode reward: total was -31.780000. running mean: -15.508453\n",
      "ep 2394: ep_len:3 episode reward: total was 1.010000. running mean: -15.343268\n",
      "ep 2394: ep_len:500 episode reward: total was -30.540000. running mean: -15.495235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2394: ep_len:637 episode reward: total was -36.430000. running mean: -15.704583\n",
      "epsilon:0.093822 episode_count: 16765. steps_count: 7260426.000000\n",
      "Time elapsed:  21116.88734936714\n",
      "ep 2395: ep_len:528 episode reward: total was 19.400000. running mean: -15.353537\n",
      "ep 2395: ep_len:500 episode reward: total was 35.790000. running mean: -14.842102\n",
      "ep 2395: ep_len:366 episode reward: total was -9.160000. running mean: -14.785281\n",
      "ep 2395: ep_len:591 episode reward: total was 25.180000. running mean: -14.385628\n",
      "ep 2395: ep_len:92 episode reward: total was 19.740000. running mean: -14.044372\n",
      "ep 2395: ep_len:692 episode reward: total was -46.250000. running mean: -14.366428\n",
      "ep 2395: ep_len:519 episode reward: total was -32.250000. running mean: -14.545264\n",
      "epsilon:0.093777 episode_count: 16772. steps_count: 7263714.000000\n",
      "Time elapsed:  21125.730543613434\n",
      "ep 2396: ep_len:531 episode reward: total was 10.780000. running mean: -14.292011\n",
      "ep 2396: ep_len:500 episode reward: total was -0.390000. running mean: -14.152991\n",
      "ep 2396: ep_len:659 episode reward: total was -31.590000. running mean: -14.327361\n",
      "ep 2396: ep_len:500 episode reward: total was -17.790000. running mean: -14.361988\n",
      "ep 2396: ep_len:3 episode reward: total was 1.010000. running mean: -14.208268\n",
      "ep 2396: ep_len:181 episode reward: total was 16.170000. running mean: -13.904485\n",
      "ep 2396: ep_len:599 episode reward: total was -1.580000. running mean: -13.781240\n",
      "epsilon:0.093733 episode_count: 16779. steps_count: 7266687.000000\n",
      "Time elapsed:  21133.659135580063\n",
      "ep 2397: ep_len:540 episode reward: total was -53.720000. running mean: -14.180628\n",
      "ep 2397: ep_len:539 episode reward: total was 46.660000. running mean: -13.572221\n",
      "ep 2397: ep_len:570 episode reward: total was -30.300000. running mean: -13.739499\n",
      "ep 2397: ep_len:170 episode reward: total was -0.400000. running mean: -13.606104\n",
      "ep 2397: ep_len:3 episode reward: total was 1.010000. running mean: -13.459943\n",
      "ep 2397: ep_len:500 episode reward: total was -9.260000. running mean: -13.417944\n",
      "ep 2397: ep_len:598 episode reward: total was 0.690000. running mean: -13.276864\n",
      "epsilon:0.093689 episode_count: 16786. steps_count: 7269607.000000\n",
      "Time elapsed:  21141.46484065056\n",
      "ep 2398: ep_len:112 episode reward: total was -12.020000. running mean: -13.264296\n",
      "ep 2398: ep_len:546 episode reward: total was 16.710000. running mean: -12.964553\n",
      "ep 2398: ep_len:373 episode reward: total was 10.950000. running mean: -12.725407\n",
      "ep 2398: ep_len:506 episode reward: total was -84.390000. running mean: -13.442053\n",
      "ep 2398: ep_len:3 episode reward: total was 1.010000. running mean: -13.297533\n",
      "ep 2398: ep_len:523 episode reward: total was -11.920000. running mean: -13.283757\n",
      "ep 2398: ep_len:549 episode reward: total was -32.010000. running mean: -13.471020\n",
      "epsilon:0.093644 episode_count: 16793. steps_count: 7272219.000000\n",
      "Time elapsed:  21148.645392656326\n",
      "ep 2399: ep_len:554 episode reward: total was 36.480000. running mean: -12.971510\n",
      "ep 2399: ep_len:500 episode reward: total was -7.470000. running mean: -12.916494\n",
      "ep 2399: ep_len:693 episode reward: total was -19.640000. running mean: -12.983729\n",
      "ep 2399: ep_len:392 episode reward: total was -9.490000. running mean: -12.948792\n",
      "ep 2399: ep_len:3 episode reward: total was 1.010000. running mean: -12.809204\n",
      "ep 2399: ep_len:583 episode reward: total was -19.410000. running mean: -12.875212\n",
      "ep 2399: ep_len:625 episode reward: total was -23.940000. running mean: -12.985860\n",
      "epsilon:0.093600 episode_count: 16800. steps_count: 7275569.000000\n",
      "Time elapsed:  21156.353444099426\n",
      "ep 2400: ep_len:550 episode reward: total was 32.400000. running mean: -12.532001\n",
      "ep 2400: ep_len:553 episode reward: total was 9.000000. running mean: -12.316681\n",
      "ep 2400: ep_len:555 episode reward: total was -25.740000. running mean: -12.450915\n",
      "ep 2400: ep_len:410 episode reward: total was 22.060000. running mean: -12.105806\n",
      "ep 2400: ep_len:75 episode reward: total was 1.110000. running mean: -11.973647\n",
      "ep 2400: ep_len:500 episode reward: total was -16.550000. running mean: -12.019411\n",
      "ep 2400: ep_len:264 episode reward: total was -97.390000. running mean: -12.873117\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.093556 episode_count: 16807. steps_count: 7278476.000000\n",
      "Time elapsed:  21169.163300275803\n",
      "ep 2401: ep_len:554 episode reward: total was -59.910000. running mean: -13.343486\n",
      "ep 2401: ep_len:511 episode reward: total was -19.390000. running mean: -13.403951\n",
      "ep 2401: ep_len:548 episode reward: total was -44.240000. running mean: -13.712311\n",
      "ep 2401: ep_len:56 episode reward: total was 0.330000. running mean: -13.571888\n",
      "ep 2401: ep_len:41 episode reward: total was 20.010000. running mean: -13.236069\n",
      "ep 2401: ep_len:318 episode reward: total was 15.770000. running mean: -12.946009\n",
      "ep 2401: ep_len:500 episode reward: total was -7.860000. running mean: -12.895149\n",
      "epsilon:0.093511 episode_count: 16814. steps_count: 7281004.000000\n",
      "Time elapsed:  21176.03826236725\n",
      "ep 2402: ep_len:224 episode reward: total was 3.300000. running mean: -12.733197\n",
      "ep 2402: ep_len:283 episode reward: total was -82.540000. running mean: -13.431265\n",
      "ep 2402: ep_len:596 episode reward: total was -60.700000. running mean: -13.903952\n",
      "ep 2402: ep_len:544 episode reward: total was 14.350000. running mean: -13.621413\n",
      "ep 2402: ep_len:93 episode reward: total was 17.760000. running mean: -13.307599\n",
      "ep 2402: ep_len:576 episode reward: total was -27.290000. running mean: -13.447423\n",
      "ep 2402: ep_len:500 episode reward: total was -113.860000. running mean: -14.451549\n",
      "epsilon:0.093467 episode_count: 16821. steps_count: 7283820.000000\n",
      "Time elapsed:  21183.603269815445\n",
      "ep 2403: ep_len:225 episode reward: total was 15.770000. running mean: -14.149333\n",
      "ep 2403: ep_len:500 episode reward: total was 8.760000. running mean: -13.920240\n",
      "ep 2403: ep_len:570 episode reward: total was -56.490000. running mean: -14.345937\n",
      "ep 2403: ep_len:500 episode reward: total was 25.360000. running mean: -13.948878\n",
      "ep 2403: ep_len:108 episode reward: total was 28.230000. running mean: -13.527089\n",
      "ep 2403: ep_len:534 episode reward: total was -26.860000. running mean: -13.660418\n",
      "ep 2403: ep_len:506 episode reward: total was -18.870000. running mean: -13.712514\n",
      "epsilon:0.093423 episode_count: 16828. steps_count: 7286763.000000\n",
      "Time elapsed:  21191.47614789009\n",
      "ep 2404: ep_len:518 episode reward: total was -90.330000. running mean: -14.478689\n",
      "ep 2404: ep_len:500 episode reward: total was -18.250000. running mean: -14.516402\n",
      "ep 2404: ep_len:548 episode reward: total was -133.830000. running mean: -15.709538\n",
      "ep 2404: ep_len:500 episode reward: total was 44.600000. running mean: -15.106443\n",
      "ep 2404: ep_len:33 episode reward: total was 10.500000. running mean: -14.850378\n",
      "ep 2404: ep_len:551 episode reward: total was 4.210000. running mean: -14.659775\n",
      "ep 2404: ep_len:188 episode reward: total was -43.380000. running mean: -14.946977\n",
      "epsilon:0.093378 episode_count: 16835. steps_count: 7289601.000000\n",
      "Time elapsed:  21199.043122768402\n",
      "ep 2405: ep_len:629 episode reward: total was -21.060000. running mean: -15.008107\n",
      "ep 2405: ep_len:350 episode reward: total was -51.880000. running mean: -15.376826\n",
      "ep 2405: ep_len:554 episode reward: total was -48.240000. running mean: -15.705458\n",
      "ep 2405: ep_len:513 episode reward: total was -16.800000. running mean: -15.716403\n",
      "ep 2405: ep_len:74 episode reward: total was 17.630000. running mean: -15.382939\n",
      "ep 2405: ep_len:535 episode reward: total was -24.260000. running mean: -15.471710\n",
      "ep 2405: ep_len:591 episode reward: total was -37.090000. running mean: -15.687893\n",
      "epsilon:0.093334 episode_count: 16842. steps_count: 7292847.000000\n",
      "Time elapsed:  21207.39889240265\n",
      "ep 2406: ep_len:550 episode reward: total was -13.630000. running mean: -15.667314\n",
      "ep 2406: ep_len:559 episode reward: total was -17.360000. running mean: -15.684241\n",
      "ep 2406: ep_len:376 episode reward: total was 21.390000. running mean: -15.313498\n",
      "ep 2406: ep_len:500 episode reward: total was -31.320000. running mean: -15.473563\n",
      "ep 2406: ep_len:113 episode reward: total was 31.190000. running mean: -15.006927\n",
      "ep 2406: ep_len:600 episode reward: total was -23.570000. running mean: -15.092558\n",
      "ep 2406: ep_len:590 episode reward: total was -59.350000. running mean: -15.535133\n",
      "epsilon:0.093290 episode_count: 16849. steps_count: 7296135.000000\n",
      "Time elapsed:  21216.09047460556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2407: ep_len:242 episode reward: total was 7.840000. running mean: -15.301381\n",
      "ep 2407: ep_len:579 episode reward: total was 44.910000. running mean: -14.699267\n",
      "ep 2407: ep_len:500 episode reward: total was -6.070000. running mean: -14.612975\n",
      "ep 2407: ep_len:516 episode reward: total was -48.140000. running mean: -14.948245\n",
      "ep 2407: ep_len:89 episode reward: total was 14.230000. running mean: -14.656463\n",
      "ep 2407: ep_len:501 episode reward: total was 4.590000. running mean: -14.463998\n",
      "ep 2407: ep_len:514 episode reward: total was -17.540000. running mean: -14.494758\n",
      "epsilon:0.093245 episode_count: 16856. steps_count: 7299076.000000\n",
      "Time elapsed:  21223.929730415344\n",
      "ep 2408: ep_len:500 episode reward: total was -2.710000. running mean: -14.376910\n",
      "ep 2408: ep_len:500 episode reward: total was -61.450000. running mean: -14.847641\n",
      "ep 2408: ep_len:500 episode reward: total was 3.900000. running mean: -14.660165\n",
      "ep 2408: ep_len:503 episode reward: total was -10.960000. running mean: -14.623163\n",
      "ep 2408: ep_len:89 episode reward: total was 18.730000. running mean: -14.289632\n",
      "ep 2408: ep_len:500 episode reward: total was -10.690000. running mean: -14.253635\n",
      "ep 2408: ep_len:500 episode reward: total was -35.770000. running mean: -14.468799\n",
      "epsilon:0.093201 episode_count: 16863. steps_count: 7302168.000000\n",
      "Time elapsed:  21231.01016521454\n",
      "ep 2409: ep_len:241 episode reward: total was 18.870000. running mean: -14.135411\n",
      "ep 2409: ep_len:555 episode reward: total was -15.120000. running mean: -14.145257\n",
      "ep 2409: ep_len:637 episode reward: total was -34.870000. running mean: -14.352504\n",
      "ep 2409: ep_len:170 episode reward: total was 14.690000. running mean: -14.062079\n",
      "ep 2409: ep_len:3 episode reward: total was 1.010000. running mean: -13.911358\n",
      "ep 2409: ep_len:518 episode reward: total was -6.210000. running mean: -13.834345\n",
      "ep 2409: ep_len:501 episode reward: total was 4.770000. running mean: -13.648301\n",
      "epsilon:0.093157 episode_count: 16870. steps_count: 7304793.000000\n",
      "Time elapsed:  21238.209467172623\n",
      "ep 2410: ep_len:134 episode reward: total was 15.540000. running mean: -13.356418\n",
      "ep 2410: ep_len:625 episode reward: total was -138.660000. running mean: -14.609454\n",
      "ep 2410: ep_len:453 episode reward: total was 41.100000. running mean: -14.052360\n",
      "ep 2410: ep_len:500 episode reward: total was -15.100000. running mean: -14.062836\n",
      "ep 2410: ep_len:91 episode reward: total was 19.240000. running mean: -13.729808\n",
      "ep 2410: ep_len:657 episode reward: total was -68.070000. running mean: -14.273210\n",
      "ep 2410: ep_len:501 episode reward: total was -32.260000. running mean: -14.453078\n",
      "epsilon:0.093112 episode_count: 16877. steps_count: 7307754.000000\n",
      "Time elapsed:  21245.78902888298\n",
      "ep 2411: ep_len:637 episode reward: total was 3.570000. running mean: -14.272847\n",
      "ep 2411: ep_len:601 episode reward: total was -147.960000. running mean: -15.609718\n",
      "ep 2411: ep_len:79 episode reward: total was -2.770000. running mean: -15.481321\n",
      "ep 2411: ep_len:500 episode reward: total was 1.660000. running mean: -15.309908\n",
      "ep 2411: ep_len:117 episode reward: total was -40.630000. running mean: -15.563109\n",
      "ep 2411: ep_len:500 episode reward: total was -7.560000. running mean: -15.483078\n",
      "ep 2411: ep_len:500 episode reward: total was -15.520000. running mean: -15.483447\n",
      "epsilon:0.093068 episode_count: 16884. steps_count: 7310688.000000\n",
      "Time elapsed:  21253.594982147217\n",
      "ep 2412: ep_len:540 episode reward: total was 22.810000. running mean: -15.100513\n",
      "ep 2412: ep_len:500 episode reward: total was -1.590000. running mean: -14.965407\n",
      "ep 2412: ep_len:500 episode reward: total was -14.820000. running mean: -14.963953\n",
      "ep 2412: ep_len:500 episode reward: total was -17.280000. running mean: -14.987114\n",
      "ep 2412: ep_len:3 episode reward: total was 1.010000. running mean: -14.827143\n",
      "ep 2412: ep_len:500 episode reward: total was 7.770000. running mean: -14.601171\n",
      "ep 2412: ep_len:350 episode reward: total was -22.770000. running mean: -14.682859\n",
      "epsilon:0.093024 episode_count: 16891. steps_count: 7313581.000000\n",
      "Time elapsed:  21261.48277783394\n",
      "ep 2413: ep_len:530 episode reward: total was 29.560000. running mean: -14.240431\n",
      "ep 2413: ep_len:500 episode reward: total was 8.350000. running mean: -14.014527\n",
      "ep 2413: ep_len:500 episode reward: total was -26.200000. running mean: -14.136381\n",
      "ep 2413: ep_len:533 episode reward: total was -52.200000. running mean: -14.517018\n",
      "ep 2413: ep_len:105 episode reward: total was 16.170000. running mean: -14.210147\n",
      "ep 2413: ep_len:653 episode reward: total was -23.320000. running mean: -14.301246\n",
      "ep 2413: ep_len:501 episode reward: total was -49.110000. running mean: -14.649333\n",
      "epsilon:0.092979 episode_count: 16898. steps_count: 7316903.000000\n",
      "Time elapsed:  21270.655875205994\n",
      "ep 2414: ep_len:628 episode reward: total was -5.270000. running mean: -14.555540\n",
      "ep 2414: ep_len:500 episode reward: total was 32.010000. running mean: -14.089885\n",
      "ep 2414: ep_len:79 episode reward: total was 3.260000. running mean: -13.916386\n",
      "ep 2414: ep_len:101 episode reward: total was -4.070000. running mean: -13.817922\n",
      "ep 2414: ep_len:131 episode reward: total was 30.360000. running mean: -13.376143\n",
      "ep 2414: ep_len:171 episode reward: total was 20.570000. running mean: -13.036681\n",
      "ep 2414: ep_len:573 episode reward: total was -29.650000. running mean: -13.202815\n",
      "epsilon:0.092935 episode_count: 16905. steps_count: 7319086.000000\n",
      "Time elapsed:  21276.846897125244\n",
      "ep 2415: ep_len:587 episode reward: total was 17.190000. running mean: -12.898886\n",
      "ep 2415: ep_len:560 episode reward: total was 65.090000. running mean: -12.118997\n",
      "ep 2415: ep_len:442 episode reward: total was 25.220000. running mean: -11.745608\n",
      "ep 2415: ep_len:500 episode reward: total was -7.870000. running mean: -11.706851\n",
      "ep 2415: ep_len:56 episode reward: total was -8.960000. running mean: -11.679383\n",
      "ep 2415: ep_len:642 episode reward: total was -105.220000. running mean: -12.614789\n",
      "ep 2415: ep_len:500 episode reward: total was -32.650000. running mean: -12.815141\n",
      "epsilon:0.092891 episode_count: 16912. steps_count: 7322373.000000\n",
      "Time elapsed:  21289.107517957687\n",
      "ep 2416: ep_len:248 episode reward: total was 3.710000. running mean: -12.649890\n",
      "ep 2416: ep_len:290 episode reward: total was -2.300000. running mean: -12.546391\n",
      "ep 2416: ep_len:525 episode reward: total was -36.860000. running mean: -12.789527\n",
      "ep 2416: ep_len:630 episode reward: total was 1.580000. running mean: -12.645832\n",
      "ep 2416: ep_len:3 episode reward: total was 1.010000. running mean: -12.509273\n",
      "ep 2416: ep_len:504 episode reward: total was 17.330000. running mean: -12.210881\n",
      "ep 2416: ep_len:576 episode reward: total was 15.100000. running mean: -11.937772\n",
      "epsilon:0.092846 episode_count: 16919. steps_count: 7325149.000000\n",
      "Time elapsed:  21296.545618534088\n",
      "ep 2417: ep_len:607 episode reward: total was 7.480000. running mean: -11.743594\n",
      "ep 2417: ep_len:500 episode reward: total was -11.590000. running mean: -11.742058\n",
      "ep 2417: ep_len:620 episode reward: total was -22.930000. running mean: -11.853938\n",
      "ep 2417: ep_len:378 episode reward: total was 0.030000. running mean: -11.735098\n",
      "ep 2417: ep_len:3 episode reward: total was -1.500000. running mean: -11.632747\n",
      "ep 2417: ep_len:605 episode reward: total was -68.970000. running mean: -12.206120\n",
      "ep 2417: ep_len:500 episode reward: total was -33.120000. running mean: -12.415259\n",
      "epsilon:0.092802 episode_count: 16926. steps_count: 7328362.000000\n",
      "Time elapsed:  21304.962729215622\n",
      "ep 2418: ep_len:622 episode reward: total was -3.160000. running mean: -12.322706\n",
      "ep 2418: ep_len:512 episode reward: total was -13.360000. running mean: -12.333079\n",
      "ep 2418: ep_len:536 episode reward: total was 3.300000. running mean: -12.176748\n",
      "ep 2418: ep_len:500 episode reward: total was 31.500000. running mean: -11.739981\n",
      "ep 2418: ep_len:3 episode reward: total was -1.990000. running mean: -11.642481\n",
      "ep 2418: ep_len:500 episode reward: total was 21.030000. running mean: -11.315756\n",
      "ep 2418: ep_len:500 episode reward: total was -28.000000. running mean: -11.482598\n",
      "epsilon:0.092758 episode_count: 16933. steps_count: 7331535.000000\n",
      "Time elapsed:  21311.298260211945\n",
      "ep 2419: ep_len:510 episode reward: total was -37.920000. running mean: -11.746973\n",
      "ep 2419: ep_len:528 episode reward: total was -25.880000. running mean: -11.888303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2419: ep_len:387 episode reward: total was 12.380000. running mean: -11.645620\n",
      "ep 2419: ep_len:603 episode reward: total was 64.850000. running mean: -10.880664\n",
      "ep 2419: ep_len:3 episode reward: total was 1.010000. running mean: -10.761757\n",
      "ep 2419: ep_len:312 episode reward: total was 4.230000. running mean: -10.611839\n",
      "ep 2419: ep_len:530 episode reward: total was 0.490000. running mean: -10.500821\n",
      "epsilon:0.092713 episode_count: 16940. steps_count: 7334408.000000\n",
      "Time elapsed:  21318.95941734314\n",
      "ep 2420: ep_len:558 episode reward: total was 12.700000. running mean: -10.268813\n",
      "ep 2420: ep_len:592 episode reward: total was 0.150000. running mean: -10.164625\n",
      "ep 2420: ep_len:79 episode reward: total was -1.760000. running mean: -10.080578\n",
      "ep 2420: ep_len:506 episode reward: total was -7.170000. running mean: -10.051473\n",
      "ep 2420: ep_len:3 episode reward: total was -1.500000. running mean: -9.965958\n",
      "ep 2420: ep_len:155 episode reward: total was 8.470000. running mean: -9.781598\n",
      "ep 2420: ep_len:309 episode reward: total was -6.150000. running mean: -9.745282\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.092669 episode_count: 16947. steps_count: 7336610.000000\n",
      "Time elapsed:  21330.0234644413\n",
      "ep 2421: ep_len:586 episode reward: total was -146.640000. running mean: -11.114229\n",
      "ep 2421: ep_len:595 episode reward: total was -17.470000. running mean: -11.177787\n",
      "ep 2421: ep_len:548 episode reward: total was -42.980000. running mean: -11.495809\n",
      "ep 2421: ep_len:500 episode reward: total was -0.000000. running mean: -11.380851\n",
      "ep 2421: ep_len:109 episode reward: total was 11.680000. running mean: -11.150243\n",
      "ep 2421: ep_len:168 episode reward: total was 13.710000. running mean: -10.901640\n",
      "ep 2421: ep_len:195 episode reward: total was -13.190000. running mean: -10.924524\n",
      "epsilon:0.092625 episode_count: 16954. steps_count: 7339311.000000\n",
      "Time elapsed:  21337.29540157318\n",
      "ep 2422: ep_len:613 episode reward: total was -68.660000. running mean: -11.501879\n",
      "ep 2422: ep_len:531 episode reward: total was -28.360000. running mean: -11.670460\n",
      "ep 2422: ep_len:435 episode reward: total was 31.060000. running mean: -11.243155\n",
      "ep 2422: ep_len:509 episode reward: total was -25.230000. running mean: -11.383024\n",
      "ep 2422: ep_len:89 episode reward: total was 20.230000. running mean: -11.066893\n",
      "ep 2422: ep_len:598 episode reward: total was 23.620000. running mean: -10.720025\n",
      "ep 2422: ep_len:501 episode reward: total was -10.800000. running mean: -10.720824\n",
      "epsilon:0.092580 episode_count: 16961. steps_count: 7342587.000000\n",
      "Time elapsed:  21344.938954114914\n",
      "ep 2423: ep_len:110 episode reward: total was 0.790000. running mean: -10.605716\n",
      "ep 2423: ep_len:500 episode reward: total was 3.430000. running mean: -10.465359\n",
      "ep 2423: ep_len:584 episode reward: total was -8.570000. running mean: -10.446405\n",
      "ep 2423: ep_len:503 episode reward: total was -22.340000. running mean: -10.565341\n",
      "ep 2423: ep_len:3 episode reward: total was 0.000000. running mean: -10.459688\n",
      "ep 2423: ep_len:572 episode reward: total was 23.140000. running mean: -10.123691\n",
      "ep 2423: ep_len:570 episode reward: total was -51.090000. running mean: -10.533354\n",
      "epsilon:0.092536 episode_count: 16968. steps_count: 7345429.000000\n",
      "Time elapsed:  21354.65219593048\n",
      "ep 2424: ep_len:652 episode reward: total was -92.230000. running mean: -11.350321\n",
      "ep 2424: ep_len:500 episode reward: total was 2.710000. running mean: -11.209717\n",
      "ep 2424: ep_len:582 episode reward: total was -53.090000. running mean: -11.628520\n",
      "ep 2424: ep_len:500 episode reward: total was 1.500000. running mean: -11.497235\n",
      "ep 2424: ep_len:3 episode reward: total was 1.010000. running mean: -11.372163\n",
      "ep 2424: ep_len:863 episode reward: total was -307.060000. running mean: -14.329041\n",
      "ep 2424: ep_len:500 episode reward: total was -45.110000. running mean: -14.636851\n",
      "epsilon:0.092492 episode_count: 16975. steps_count: 7349029.000000\n",
      "Time elapsed:  21361.91004538536\n",
      "ep 2425: ep_len:571 episode reward: total was 27.470000. running mean: -14.215782\n",
      "ep 2425: ep_len:500 episode reward: total was 12.180000. running mean: -13.951824\n",
      "ep 2425: ep_len:629 episode reward: total was -40.530000. running mean: -14.217606\n",
      "ep 2425: ep_len:582 episode reward: total was -21.930000. running mean: -14.294730\n",
      "ep 2425: ep_len:42 episode reward: total was 17.510000. running mean: -13.976683\n",
      "ep 2425: ep_len:668 episode reward: total was -19.900000. running mean: -14.035916\n",
      "ep 2425: ep_len:573 episode reward: total was -6.500000. running mean: -13.960557\n",
      "epsilon:0.092447 episode_count: 16982. steps_count: 7352594.000000\n",
      "Time elapsed:  21374.90812063217\n",
      "ep 2426: ep_len:637 episode reward: total was -70.130000. running mean: -14.522251\n",
      "ep 2426: ep_len:659 episode reward: total was -98.190000. running mean: -15.358929\n",
      "ep 2426: ep_len:539 episode reward: total was -34.830000. running mean: -15.553639\n",
      "ep 2426: ep_len:560 episode reward: total was 11.210000. running mean: -15.286003\n",
      "ep 2426: ep_len:3 episode reward: total was 0.000000. running mean: -15.133143\n",
      "ep 2426: ep_len:501 episode reward: total was -30.630000. running mean: -15.288111\n",
      "ep 2426: ep_len:522 episode reward: total was 10.400000. running mean: -15.031230\n",
      "epsilon:0.092403 episode_count: 16989. steps_count: 7356015.000000\n",
      "Time elapsed:  21383.832725048065\n",
      "ep 2427: ep_len:255 episode reward: total was -0.180000. running mean: -14.882718\n",
      "ep 2427: ep_len:167 episode reward: total was -5.820000. running mean: -14.792091\n",
      "ep 2427: ep_len:630 episode reward: total was -83.160000. running mean: -15.475770\n",
      "ep 2427: ep_len:838 episode reward: total was -554.050000. running mean: -20.861512\n",
      "ep 2427: ep_len:3 episode reward: total was 1.010000. running mean: -20.642797\n",
      "ep 2427: ep_len:544 episode reward: total was -10.650000. running mean: -20.542869\n",
      "ep 2427: ep_len:572 episode reward: total was -13.120000. running mean: -20.468640\n",
      "epsilon:0.092359 episode_count: 16996. steps_count: 7359024.000000\n",
      "Time elapsed:  21391.290417909622\n",
      "ep 2428: ep_len:557 episode reward: total was 55.120000. running mean: -19.712754\n",
      "ep 2428: ep_len:501 episode reward: total was 18.040000. running mean: -19.335226\n",
      "ep 2428: ep_len:561 episode reward: total was -34.040000. running mean: -19.482274\n",
      "ep 2428: ep_len:500 episode reward: total was 7.870000. running mean: -19.208751\n",
      "ep 2428: ep_len:129 episode reward: total was 9.860000. running mean: -18.918064\n",
      "ep 2428: ep_len:610 episode reward: total was 15.900000. running mean: -18.569883\n",
      "ep 2428: ep_len:211 episode reward: total was -17.990000. running mean: -18.564084\n",
      "epsilon:0.092314 episode_count: 17003. steps_count: 7362093.000000\n",
      "Time elapsed:  21399.218129634857\n",
      "ep 2429: ep_len:575 episode reward: total was 39.140000. running mean: -17.987044\n",
      "ep 2429: ep_len:590 episode reward: total was -88.020000. running mean: -18.687373\n",
      "ep 2429: ep_len:358 episode reward: total was 33.790000. running mean: -18.162599\n",
      "ep 2429: ep_len:576 episode reward: total was 7.700000. running mean: -17.903973\n",
      "ep 2429: ep_len:3 episode reward: total was -1.500000. running mean: -17.739934\n",
      "ep 2429: ep_len:516 episode reward: total was -8.760000. running mean: -17.650134\n",
      "ep 2429: ep_len:525 episode reward: total was -41.740000. running mean: -17.891033\n",
      "epsilon:0.092270 episode_count: 17010. steps_count: 7365236.000000\n",
      "Time elapsed:  21407.461100578308\n",
      "ep 2430: ep_len:539 episode reward: total was -46.760000. running mean: -18.179723\n",
      "ep 2430: ep_len:622 episode reward: total was 5.430000. running mean: -17.943626\n",
      "ep 2430: ep_len:376 episode reward: total was 36.420000. running mean: -17.399989\n",
      "ep 2430: ep_len:504 episode reward: total was -24.420000. running mean: -17.470189\n",
      "ep 2430: ep_len:91 episode reward: total was 14.740000. running mean: -17.148087\n",
      "ep 2430: ep_len:503 episode reward: total was -14.650000. running mean: -17.123107\n",
      "ep 2430: ep_len:590 episode reward: total was -44.400000. running mean: -17.395876\n",
      "epsilon:0.092226 episode_count: 17017. steps_count: 7368461.000000\n",
      "Time elapsed:  21415.934319496155\n",
      "ep 2431: ep_len:640 episode reward: total was -81.950000. running mean: -18.041417\n",
      "ep 2431: ep_len:572 episode reward: total was 46.690000. running mean: -17.394103\n",
      "ep 2431: ep_len:610 episode reward: total was -25.990000. running mean: -17.480062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2431: ep_len:521 episode reward: total was -19.840000. running mean: -17.503661\n",
      "ep 2431: ep_len:3 episode reward: total was 1.010000. running mean: -17.318524\n",
      "ep 2431: ep_len:518 episode reward: total was 33.490000. running mean: -16.810439\n",
      "ep 2431: ep_len:292 episode reward: total was -26.200000. running mean: -16.904335\n",
      "epsilon:0.092181 episode_count: 17024. steps_count: 7371617.000000\n",
      "Time elapsed:  21424.371612787247\n",
      "ep 2432: ep_len:500 episode reward: total was 61.380000. running mean: -16.121491\n",
      "ep 2432: ep_len:583 episode reward: total was 10.220000. running mean: -15.858076\n",
      "ep 2432: ep_len:559 episode reward: total was -58.770000. running mean: -16.287196\n",
      "ep 2432: ep_len:509 episode reward: total was -30.620000. running mean: -16.430524\n",
      "ep 2432: ep_len:74 episode reward: total was 19.130000. running mean: -16.074918\n",
      "ep 2432: ep_len:612 episode reward: total was -26.300000. running mean: -16.177169\n",
      "ep 2432: ep_len:574 episode reward: total was -164.640000. running mean: -17.661798\n",
      "epsilon:0.092137 episode_count: 17031. steps_count: 7375028.000000\n",
      "Time elapsed:  21437.47176861763\n",
      "ep 2433: ep_len:500 episode reward: total was 52.170000. running mean: -16.963480\n",
      "ep 2433: ep_len:506 episode reward: total was -66.870000. running mean: -17.462545\n",
      "ep 2433: ep_len:651 episode reward: total was -61.820000. running mean: -17.906119\n",
      "ep 2433: ep_len:528 episode reward: total was 16.120000. running mean: -17.565858\n",
      "ep 2433: ep_len:3 episode reward: total was 1.010000. running mean: -17.380100\n",
      "ep 2433: ep_len:500 episode reward: total was -25.240000. running mean: -17.458699\n",
      "ep 2433: ep_len:637 episode reward: total was -9.400000. running mean: -17.378112\n",
      "epsilon:0.092093 episode_count: 17038. steps_count: 7378353.000000\n",
      "Time elapsed:  21445.930994272232\n",
      "ep 2434: ep_len:634 episode reward: total was -67.560000. running mean: -17.879931\n",
      "ep 2434: ep_len:303 episode reward: total was -5.350000. running mean: -17.754631\n",
      "ep 2434: ep_len:538 episode reward: total was -59.930000. running mean: -18.176385\n",
      "ep 2434: ep_len:500 episode reward: total was -29.230000. running mean: -18.286921\n",
      "ep 2434: ep_len:3 episode reward: total was 1.010000. running mean: -18.093952\n",
      "ep 2434: ep_len:526 episode reward: total was -49.090000. running mean: -18.403912\n",
      "ep 2434: ep_len:572 episode reward: total was -10.190000. running mean: -18.321773\n",
      "epsilon:0.092048 episode_count: 17045. steps_count: 7381429.000000\n",
      "Time elapsed:  21453.473806381226\n",
      "ep 2435: ep_len:636 episode reward: total was -62.100000. running mean: -18.759555\n",
      "ep 2435: ep_len:500 episode reward: total was 25.290000. running mean: -18.319060\n",
      "ep 2435: ep_len:625 episode reward: total was 5.370000. running mean: -18.082169\n",
      "ep 2435: ep_len:500 episode reward: total was 13.970000. running mean: -17.761648\n",
      "ep 2435: ep_len:3 episode reward: total was 1.010000. running mean: -17.573931\n",
      "ep 2435: ep_len:574 episode reward: total was -4.420000. running mean: -17.442392\n",
      "ep 2435: ep_len:545 episode reward: total was -1.820000. running mean: -17.286168\n",
      "epsilon:0.092004 episode_count: 17052. steps_count: 7384812.000000\n",
      "Time elapsed:  21460.820078849792\n",
      "ep 2436: ep_len:549 episode reward: total was -61.400000. running mean: -17.727306\n",
      "ep 2436: ep_len:521 episode reward: total was 18.430000. running mean: -17.365733\n",
      "ep 2436: ep_len:633 episode reward: total was -31.820000. running mean: -17.510276\n",
      "ep 2436: ep_len:170 episode reward: total was 8.690000. running mean: -17.248273\n",
      "ep 2436: ep_len:3 episode reward: total was 1.010000. running mean: -17.065690\n",
      "ep 2436: ep_len:500 episode reward: total was 14.230000. running mean: -16.752733\n",
      "ep 2436: ep_len:500 episode reward: total was -18.740000. running mean: -16.772606\n",
      "epsilon:0.091960 episode_count: 17059. steps_count: 7387688.000000\n",
      "Time elapsed:  21468.42406630516\n",
      "ep 2437: ep_len:555 episode reward: total was -8.690000. running mean: -16.691780\n",
      "ep 2437: ep_len:500 episode reward: total was -114.220000. running mean: -17.667062\n",
      "ep 2437: ep_len:593 episode reward: total was -66.110000. running mean: -18.151492\n",
      "ep 2437: ep_len:500 episode reward: total was 1.560000. running mean: -17.954377\n",
      "ep 2437: ep_len:2 episode reward: total was -2.000000. running mean: -17.794833\n",
      "ep 2437: ep_len:536 episode reward: total was -35.550000. running mean: -17.972385\n",
      "ep 2437: ep_len:556 episode reward: total was 8.550000. running mean: -17.707161\n",
      "epsilon:0.091915 episode_count: 17066. steps_count: 7390930.000000\n",
      "Time elapsed:  21476.95175409317\n",
      "ep 2438: ep_len:502 episode reward: total was 40.900000. running mean: -17.121089\n",
      "ep 2438: ep_len:500 episode reward: total was -4.710000. running mean: -16.996978\n",
      "ep 2438: ep_len:386 episode reward: total was 43.590000. running mean: -16.391108\n",
      "ep 2438: ep_len:545 episode reward: total was 27.560000. running mean: -15.951597\n",
      "ep 2438: ep_len:3 episode reward: total was 1.010000. running mean: -15.781981\n",
      "ep 2438: ep_len:552 episode reward: total was -21.330000. running mean: -15.837462\n",
      "ep 2438: ep_len:589 episode reward: total was -95.350000. running mean: -16.632587\n",
      "epsilon:0.091871 episode_count: 17073. steps_count: 7394007.000000\n",
      "Time elapsed:  21485.0821082592\n",
      "ep 2439: ep_len:568 episode reward: total was 24.830000. running mean: -16.217961\n",
      "ep 2439: ep_len:544 episode reward: total was -0.290000. running mean: -16.058682\n",
      "ep 2439: ep_len:679 episode reward: total was -148.130000. running mean: -17.379395\n",
      "ep 2439: ep_len:500 episode reward: total was 2.420000. running mean: -17.181401\n",
      "ep 2439: ep_len:84 episode reward: total was 10.230000. running mean: -16.907287\n",
      "ep 2439: ep_len:582 episode reward: total was 5.320000. running mean: -16.685014\n",
      "ep 2439: ep_len:579 episode reward: total was -101.680000. running mean: -17.534964\n",
      "epsilon:0.091827 episode_count: 17080. steps_count: 7397543.000000\n",
      "Time elapsed:  21494.377296447754\n",
      "ep 2440: ep_len:264 episode reward: total was 11.330000. running mean: -17.246314\n",
      "ep 2440: ep_len:500 episode reward: total was 71.030000. running mean: -16.363551\n",
      "ep 2440: ep_len:532 episode reward: total was -37.600000. running mean: -16.575915\n",
      "ep 2440: ep_len:518 episode reward: total was -28.600000. running mean: -16.696156\n",
      "ep 2440: ep_len:3 episode reward: total was 1.010000. running mean: -16.519095\n",
      "ep 2440: ep_len:500 episode reward: total was -42.360000. running mean: -16.777504\n",
      "ep 2440: ep_len:566 episode reward: total was -130.220000. running mean: -17.911929\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.091782 episode_count: 17087. steps_count: 7400426.000000\n",
      "Time elapsed:  21504.631192684174\n",
      "ep 2441: ep_len:194 episode reward: total was -9.890000. running mean: -17.831709\n",
      "ep 2441: ep_len:508 episode reward: total was -33.120000. running mean: -17.984592\n",
      "ep 2441: ep_len:79 episode reward: total was 2.770000. running mean: -17.777046\n",
      "ep 2441: ep_len:405 episode reward: total was 0.460000. running mean: -17.594676\n",
      "ep 2441: ep_len:90 episode reward: total was 25.260000. running mean: -17.166129\n",
      "ep 2441: ep_len:573 episode reward: total was 30.250000. running mean: -16.691968\n",
      "ep 2441: ep_len:579 episode reward: total was 6.100000. running mean: -16.464048\n",
      "epsilon:0.091738 episode_count: 17094. steps_count: 7402854.000000\n",
      "Time elapsed:  21511.897532701492\n",
      "ep 2442: ep_len:500 episode reward: total was -31.920000. running mean: -16.618608\n",
      "ep 2442: ep_len:500 episode reward: total was 18.830000. running mean: -16.264122\n",
      "ep 2442: ep_len:431 episode reward: total was 11.520000. running mean: -15.986280\n",
      "ep 2442: ep_len:553 episode reward: total was 10.500000. running mean: -15.721418\n",
      "ep 2442: ep_len:3 episode reward: total was 0.000000. running mean: -15.564203\n",
      "ep 2442: ep_len:500 episode reward: total was 30.780000. running mean: -15.100761\n",
      "ep 2442: ep_len:541 episode reward: total was -97.360000. running mean: -15.923354\n",
      "epsilon:0.091694 episode_count: 17101. steps_count: 7405882.000000\n",
      "Time elapsed:  21519.88853573799\n",
      "ep 2443: ep_len:565 episode reward: total was -2.710000. running mean: -15.791220\n",
      "ep 2443: ep_len:544 episode reward: total was -25.440000. running mean: -15.887708\n",
      "ep 2443: ep_len:564 episode reward: total was 2.870000. running mean: -15.700131\n",
      "ep 2443: ep_len:503 episode reward: total was 27.610000. running mean: -15.267030\n",
      "ep 2443: ep_len:3 episode reward: total was 1.010000. running mean: -15.104259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2443: ep_len:500 episode reward: total was -86.400000. running mean: -15.817217\n",
      "ep 2443: ep_len:538 episode reward: total was -48.150000. running mean: -16.140545\n",
      "epsilon:0.091649 episode_count: 17108. steps_count: 7409099.000000\n",
      "Time elapsed:  21531.60653448105\n",
      "ep 2444: ep_len:574 episode reward: total was -73.700000. running mean: -16.716139\n",
      "ep 2444: ep_len:591 episode reward: total was -11.420000. running mean: -16.663178\n",
      "ep 2444: ep_len:500 episode reward: total was -165.770000. running mean: -18.154246\n",
      "ep 2444: ep_len:56 episode reward: total was -1.170000. running mean: -17.984404\n",
      "ep 2444: ep_len:3 episode reward: total was 1.010000. running mean: -17.794460\n",
      "ep 2444: ep_len:525 episode reward: total was 16.410000. running mean: -17.452415\n",
      "ep 2444: ep_len:500 episode reward: total was 2.030000. running mean: -17.257591\n",
      "epsilon:0.091605 episode_count: 17115. steps_count: 7411848.000000\n",
      "Time elapsed:  21538.884106636047\n",
      "ep 2445: ep_len:530 episode reward: total was 21.350000. running mean: -16.871515\n",
      "ep 2445: ep_len:545 episode reward: total was 36.250000. running mean: -16.340300\n",
      "ep 2445: ep_len:569 episode reward: total was -26.210000. running mean: -16.438997\n",
      "ep 2445: ep_len:575 episode reward: total was 14.480000. running mean: -16.129807\n",
      "ep 2445: ep_len:85 episode reward: total was 18.230000. running mean: -15.786209\n",
      "ep 2445: ep_len:500 episode reward: total was -34.310000. running mean: -15.971447\n",
      "ep 2445: ep_len:555 episode reward: total was -81.140000. running mean: -16.623132\n",
      "epsilon:0.091561 episode_count: 17122. steps_count: 7415207.000000\n",
      "Time elapsed:  21550.796978473663\n",
      "ep 2446: ep_len:571 episode reward: total was -128.610000. running mean: -17.743001\n",
      "ep 2446: ep_len:500 episode reward: total was -8.030000. running mean: -17.645871\n",
      "ep 2446: ep_len:580 episode reward: total was -5.050000. running mean: -17.519912\n",
      "ep 2446: ep_len:118 episode reward: total was 4.580000. running mean: -17.298913\n",
      "ep 2446: ep_len:92 episode reward: total was 23.260000. running mean: -16.893324\n",
      "ep 2446: ep_len:587 episode reward: total was -75.100000. running mean: -17.475391\n",
      "ep 2446: ep_len:552 episode reward: total was -39.110000. running mean: -17.691737\n",
      "epsilon:0.091516 episode_count: 17129. steps_count: 7418207.000000\n",
      "Time elapsed:  21558.821485042572\n",
      "ep 2447: ep_len:538 episode reward: total was -48.780000. running mean: -18.002619\n",
      "ep 2447: ep_len:557 episode reward: total was -24.190000. running mean: -18.064493\n",
      "ep 2447: ep_len:587 episode reward: total was -0.220000. running mean: -17.886048\n",
      "ep 2447: ep_len:500 episode reward: total was -13.040000. running mean: -17.837588\n",
      "ep 2447: ep_len:89 episode reward: total was 13.250000. running mean: -17.526712\n",
      "ep 2447: ep_len:588 episode reward: total was -41.170000. running mean: -17.763145\n",
      "ep 2447: ep_len:569 episode reward: total was -21.740000. running mean: -17.802913\n",
      "epsilon:0.091472 episode_count: 17136. steps_count: 7421635.000000\n",
      "Time elapsed:  21567.842977285385\n",
      "ep 2448: ep_len:119 episode reward: total was -6.710000. running mean: -17.691984\n",
      "ep 2448: ep_len:601 episode reward: total was 18.040000. running mean: -17.334664\n",
      "ep 2448: ep_len:403 episode reward: total was 37.090000. running mean: -16.790418\n",
      "ep 2448: ep_len:525 episode reward: total was -13.140000. running mean: -16.753914\n",
      "ep 2448: ep_len:55 episode reward: total was 23.000000. running mean: -16.356374\n",
      "ep 2448: ep_len:533 episode reward: total was -43.370000. running mean: -16.626511\n",
      "ep 2448: ep_len:312 episode reward: total was -24.930000. running mean: -16.709546\n",
      "epsilon:0.091428 episode_count: 17143. steps_count: 7424183.000000\n",
      "Time elapsed:  21574.778799295425\n",
      "ep 2449: ep_len:253 episode reward: total was 9.290000. running mean: -16.449550\n",
      "ep 2449: ep_len:500 episode reward: total was 3.360000. running mean: -16.251455\n",
      "ep 2449: ep_len:599 episode reward: total was -194.580000. running mean: -18.034740\n",
      "ep 2449: ep_len:581 episode reward: total was 29.360000. running mean: -17.560793\n",
      "ep 2449: ep_len:87 episode reward: total was 17.180000. running mean: -17.213385\n",
      "ep 2449: ep_len:500 episode reward: total was -18.230000. running mean: -17.223551\n",
      "ep 2449: ep_len:513 episode reward: total was -8.920000. running mean: -17.140515\n",
      "epsilon:0.091383 episode_count: 17150. steps_count: 7427216.000000\n",
      "Time elapsed:  21586.789397478104\n",
      "ep 2450: ep_len:500 episode reward: total was -29.990000. running mean: -17.269010\n",
      "ep 2450: ep_len:501 episode reward: total was 59.430000. running mean: -16.502020\n",
      "ep 2450: ep_len:555 episode reward: total was -34.800000. running mean: -16.685000\n",
      "ep 2450: ep_len:532 episode reward: total was -30.370000. running mean: -16.821850\n",
      "ep 2450: ep_len:3 episode reward: total was -1.500000. running mean: -16.668631\n",
      "ep 2450: ep_len:508 episode reward: total was -53.050000. running mean: -17.032445\n",
      "ep 2450: ep_len:511 episode reward: total was -21.590000. running mean: -17.078021\n",
      "epsilon:0.091339 episode_count: 17157. steps_count: 7430326.000000\n",
      "Time elapsed:  21599.331713199615\n",
      "ep 2451: ep_len:500 episode reward: total was 39.410000. running mean: -16.513140\n",
      "ep 2451: ep_len:500 episode reward: total was 27.660000. running mean: -16.071409\n",
      "ep 2451: ep_len:573 episode reward: total was -35.470000. running mean: -16.265395\n",
      "ep 2451: ep_len:585 episode reward: total was 10.690000. running mean: -15.995841\n",
      "ep 2451: ep_len:90 episode reward: total was 15.220000. running mean: -15.683683\n",
      "ep 2451: ep_len:696 episode reward: total was -198.580000. running mean: -17.512646\n",
      "ep 2451: ep_len:192 episode reward: total was -33.670000. running mean: -17.674219\n",
      "epsilon:0.091295 episode_count: 17164. steps_count: 7433462.000000\n",
      "Time elapsed:  21608.73604273796\n",
      "ep 2452: ep_len:122 episode reward: total was 2.380000. running mean: -17.473677\n",
      "ep 2452: ep_len:500 episode reward: total was -19.650000. running mean: -17.495440\n",
      "ep 2452: ep_len:500 episode reward: total was -28.010000. running mean: -17.600586\n",
      "ep 2452: ep_len:500 episode reward: total was -16.320000. running mean: -17.587780\n",
      "ep 2452: ep_len:3 episode reward: total was -1.500000. running mean: -17.426902\n",
      "ep 2452: ep_len:312 episode reward: total was 27.160000. running mean: -16.981033\n",
      "ep 2452: ep_len:567 episode reward: total was -12.130000. running mean: -16.932523\n",
      "epsilon:0.091250 episode_count: 17171. steps_count: 7435966.000000\n",
      "Time elapsed:  21621.436406612396\n",
      "ep 2453: ep_len:500 episode reward: total was 5.480000. running mean: -16.708398\n",
      "ep 2453: ep_len:500 episode reward: total was -31.320000. running mean: -16.854514\n",
      "ep 2453: ep_len:711 episode reward: total was -43.890000. running mean: -17.124869\n",
      "ep 2453: ep_len:546 episode reward: total was 22.360000. running mean: -16.730020\n",
      "ep 2453: ep_len:3 episode reward: total was 1.010000. running mean: -16.552620\n",
      "ep 2453: ep_len:589 episode reward: total was 19.910000. running mean: -16.187993\n",
      "ep 2453: ep_len:508 episode reward: total was -2.890000. running mean: -16.055014\n",
      "epsilon:0.091206 episode_count: 17178. steps_count: 7439323.000000\n",
      "Time elapsed:  21631.36267018318\n",
      "ep 2454: ep_len:500 episode reward: total was 37.960000. running mean: -15.514863\n",
      "ep 2454: ep_len:528 episode reward: total was 63.110000. running mean: -14.728615\n",
      "ep 2454: ep_len:574 episode reward: total was -24.620000. running mean: -14.827529\n",
      "ep 2454: ep_len:597 episode reward: total was 58.080000. running mean: -14.098453\n",
      "ep 2454: ep_len:101 episode reward: total was 23.750000. running mean: -13.719969\n",
      "ep 2454: ep_len:549 episode reward: total was 20.270000. running mean: -13.380069\n",
      "ep 2454: ep_len:505 episode reward: total was -48.340000. running mean: -13.729668\n",
      "epsilon:0.091162 episode_count: 17185. steps_count: 7442677.000000\n",
      "Time elapsed:  21647.40795493126\n",
      "ep 2455: ep_len:537 episode reward: total was 15.280000. running mean: -13.439572\n",
      "ep 2455: ep_len:500 episode reward: total was -9.200000. running mean: -13.397176\n",
      "ep 2455: ep_len:595 episode reward: total was 9.570000. running mean: -13.167504\n",
      "ep 2455: ep_len:515 episode reward: total was -34.890000. running mean: -13.384729\n",
      "ep 2455: ep_len:3 episode reward: total was 1.010000. running mean: -13.240782\n",
      "ep 2455: ep_len:645 episode reward: total was -9.270000. running mean: -13.201074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2455: ep_len:564 episode reward: total was -36.690000. running mean: -13.435963\n",
      "epsilon:0.091117 episode_count: 17192. steps_count: 7446036.000000\n",
      "Time elapsed:  21657.451502084732\n",
      "ep 2456: ep_len:510 episode reward: total was 14.480000. running mean: -13.156804\n",
      "ep 2456: ep_len:176 episode reward: total was -14.670000. running mean: -13.171936\n",
      "ep 2456: ep_len:574 episode reward: total was -26.690000. running mean: -13.307116\n",
      "ep 2456: ep_len:514 episode reward: total was -31.970000. running mean: -13.493745\n",
      "ep 2456: ep_len:3 episode reward: total was -1.500000. running mean: -13.373808\n",
      "ep 2456: ep_len:500 episode reward: total was -34.920000. running mean: -13.589270\n",
      "ep 2456: ep_len:500 episode reward: total was 1.420000. running mean: -13.439177\n",
      "epsilon:0.091073 episode_count: 17199. steps_count: 7448813.000000\n",
      "Time elapsed:  21666.020475149155\n",
      "ep 2457: ep_len:638 episode reward: total was 7.500000. running mean: -13.229785\n",
      "ep 2457: ep_len:547 episode reward: total was -13.980000. running mean: -13.237287\n",
      "ep 2457: ep_len:514 episode reward: total was -8.340000. running mean: -13.188314\n",
      "ep 2457: ep_len:633 episode reward: total was 64.080000. running mean: -12.415631\n",
      "ep 2457: ep_len:3 episode reward: total was 1.010000. running mean: -12.281375\n",
      "ep 2457: ep_len:575 episode reward: total was -6.670000. running mean: -12.225261\n",
      "ep 2457: ep_len:546 episode reward: total was -34.910000. running mean: -12.452109\n",
      "epsilon:0.091029 episode_count: 17206. steps_count: 7452269.000000\n",
      "Time elapsed:  21676.41289997101\n",
      "ep 2458: ep_len:601 episode reward: total was 48.800000. running mean: -11.839588\n",
      "ep 2458: ep_len:565 episode reward: total was -5.640000. running mean: -11.777592\n",
      "ep 2458: ep_len:434 episode reward: total was 20.700000. running mean: -11.452816\n",
      "ep 2458: ep_len:500 episode reward: total was 38.290000. running mean: -10.955388\n",
      "ep 2458: ep_len:3 episode reward: total was 1.010000. running mean: -10.835734\n",
      "ep 2458: ep_len:512 episode reward: total was -87.800000. running mean: -11.605376\n",
      "ep 2458: ep_len:500 episode reward: total was -26.680000. running mean: -11.756123\n",
      "epsilon:0.090984 episode_count: 17213. steps_count: 7455384.000000\n",
      "Time elapsed:  21685.96550679207\n",
      "ep 2459: ep_len:504 episode reward: total was -47.400000. running mean: -12.112561\n",
      "ep 2459: ep_len:500 episode reward: total was -55.550000. running mean: -12.546936\n",
      "ep 2459: ep_len:623 episode reward: total was -9.820000. running mean: -12.519666\n",
      "ep 2459: ep_len:501 episode reward: total was -4.230000. running mean: -12.436770\n",
      "ep 2459: ep_len:128 episode reward: total was 15.310000. running mean: -12.159302\n",
      "ep 2459: ep_len:583 episode reward: total was 45.250000. running mean: -11.585209\n",
      "ep 2459: ep_len:608 episode reward: total was -28.550000. running mean: -11.754857\n",
      "epsilon:0.090940 episode_count: 17220. steps_count: 7458831.000000\n",
      "Time elapsed:  21696.190095424652\n",
      "ep 2460: ep_len:605 episode reward: total was 36.890000. running mean: -11.268408\n",
      "ep 2460: ep_len:502 episode reward: total was 7.570000. running mean: -11.080024\n",
      "ep 2460: ep_len:583 episode reward: total was -26.380000. running mean: -11.233024\n",
      "ep 2460: ep_len:592 episode reward: total was -23.330000. running mean: -11.353994\n",
      "ep 2460: ep_len:3 episode reward: total was 1.010000. running mean: -11.230354\n",
      "ep 2460: ep_len:500 episode reward: total was -29.970000. running mean: -11.417750\n",
      "ep 2460: ep_len:500 episode reward: total was 24.470000. running mean: -11.058873\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.090896 episode_count: 17227. steps_count: 7462116.000000\n",
      "Time elapsed:  21710.68255853653\n",
      "ep 2461: ep_len:500 episode reward: total was -2.500000. running mean: -10.973284\n",
      "ep 2461: ep_len:545 episode reward: total was 64.140000. running mean: -10.222151\n",
      "ep 2461: ep_len:540 episode reward: total was -48.690000. running mean: -10.606830\n",
      "ep 2461: ep_len:506 episode reward: total was -1.840000. running mean: -10.519161\n",
      "ep 2461: ep_len:3 episode reward: total was 1.010000. running mean: -10.403870\n",
      "ep 2461: ep_len:571 episode reward: total was -68.410000. running mean: -10.983931\n",
      "ep 2461: ep_len:545 episode reward: total was -46.220000. running mean: -11.336292\n",
      "epsilon:0.090851 episode_count: 17234. steps_count: 7465326.000000\n",
      "Time elapsed:  21720.33096385002\n",
      "ep 2462: ep_len:205 episode reward: total was -3.320000. running mean: -11.256129\n",
      "ep 2462: ep_len:639 episode reward: total was 28.140000. running mean: -10.862168\n",
      "ep 2462: ep_len:730 episode reward: total was -148.290000. running mean: -12.236446\n",
      "ep 2462: ep_len:520 episode reward: total was -54.430000. running mean: -12.658381\n",
      "ep 2462: ep_len:46 episode reward: total was 15.010000. running mean: -12.381698\n",
      "ep 2462: ep_len:558 episode reward: total was -32.870000. running mean: -12.586581\n",
      "ep 2462: ep_len:337 episode reward: total was -13.080000. running mean: -12.591515\n",
      "epsilon:0.090807 episode_count: 17241. steps_count: 7468361.000000\n",
      "Time elapsed:  21729.788328886032\n",
      "ep 2463: ep_len:511 episode reward: total was -11.590000. running mean: -12.581500\n",
      "ep 2463: ep_len:500 episode reward: total was 27.140000. running mean: -12.184285\n",
      "ep 2463: ep_len:501 episode reward: total was -18.600000. running mean: -12.248442\n",
      "ep 2463: ep_len:500 episode reward: total was 35.380000. running mean: -11.772157\n",
      "ep 2463: ep_len:3 episode reward: total was 1.010000. running mean: -11.644336\n",
      "ep 2463: ep_len:682 episode reward: total was 1.800000. running mean: -11.509893\n",
      "ep 2463: ep_len:573 episode reward: total was -15.660000. running mean: -11.551394\n",
      "epsilon:0.090763 episode_count: 17248. steps_count: 7471631.000000\n",
      "Time elapsed:  21739.683547258377\n",
      "ep 2464: ep_len:500 episode reward: total was -30.480000. running mean: -11.740680\n",
      "ep 2464: ep_len:500 episode reward: total was 25.530000. running mean: -11.367973\n",
      "ep 2464: ep_len:566 episode reward: total was -33.770000. running mean: -11.591993\n",
      "ep 2464: ep_len:505 episode reward: total was -1.060000. running mean: -11.486673\n",
      "ep 2464: ep_len:3 episode reward: total was 0.000000. running mean: -11.371806\n",
      "ep 2464: ep_len:533 episode reward: total was -55.580000. running mean: -11.813888\n",
      "ep 2464: ep_len:587 episode reward: total was 4.210000. running mean: -11.653650\n",
      "epsilon:0.090718 episode_count: 17255. steps_count: 7474825.000000\n",
      "Time elapsed:  21754.366031885147\n",
      "ep 2465: ep_len:582 episode reward: total was 29.480000. running mean: -11.242313\n",
      "ep 2465: ep_len:374 episode reward: total was -66.710000. running mean: -11.796990\n",
      "ep 2465: ep_len:567 episode reward: total was -47.500000. running mean: -12.154020\n",
      "ep 2465: ep_len:519 episode reward: total was -21.320000. running mean: -12.245680\n",
      "ep 2465: ep_len:129 episode reward: total was 9.340000. running mean: -12.029823\n",
      "ep 2465: ep_len:160 episode reward: total was 17.090000. running mean: -11.738625\n",
      "ep 2465: ep_len:550 episode reward: total was 27.990000. running mean: -11.341339\n",
      "epsilon:0.090674 episode_count: 17262. steps_count: 7477706.000000\n",
      "Time elapsed:  21763.068202733994\n",
      "ep 2466: ep_len:639 episode reward: total was -10.830000. running mean: -11.336225\n",
      "ep 2466: ep_len:338 episode reward: total was -62.470000. running mean: -11.847563\n",
      "ep 2466: ep_len:423 episode reward: total was -38.650000. running mean: -12.115587\n",
      "ep 2466: ep_len:573 episode reward: total was 36.940000. running mean: -11.625031\n",
      "ep 2466: ep_len:3 episode reward: total was 1.010000. running mean: -11.498681\n",
      "ep 2466: ep_len:224 episode reward: total was 11.470000. running mean: -11.268994\n",
      "ep 2466: ep_len:202 episode reward: total was -11.470000. running mean: -11.271004\n",
      "epsilon:0.090630 episode_count: 17269. steps_count: 7480108.000000\n",
      "Time elapsed:  21770.613522529602\n",
      "ep 2467: ep_len:500 episode reward: total was 61.750000. running mean: -10.540794\n",
      "ep 2467: ep_len:563 episode reward: total was -28.490000. running mean: -10.720286\n",
      "ep 2467: ep_len:526 episode reward: total was -10.640000. running mean: -10.719483\n",
      "ep 2467: ep_len:515 episode reward: total was -10.200000. running mean: -10.714289\n",
      "ep 2467: ep_len:110 episode reward: total was 33.270000. running mean: -10.274446\n",
      "ep 2467: ep_len:226 episode reward: total was 31.700000. running mean: -9.854701\n",
      "ep 2467: ep_len:352 episode reward: total was -9.530000. running mean: -9.851454\n",
      "epsilon:0.090585 episode_count: 17276. steps_count: 7482900.000000\n",
      "Time elapsed:  21779.226253032684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2468: ep_len:650 episode reward: total was -449.190000. running mean: -14.244840\n",
      "ep 2468: ep_len:594 episode reward: total was -15.200000. running mean: -14.254391\n",
      "ep 2468: ep_len:409 episode reward: total was 42.970000. running mean: -13.682147\n",
      "ep 2468: ep_len:577 episode reward: total was -4.900000. running mean: -13.594326\n",
      "ep 2468: ep_len:3 episode reward: total was 1.010000. running mean: -13.448283\n",
      "ep 2468: ep_len:500 episode reward: total was 29.470000. running mean: -13.019100\n",
      "ep 2468: ep_len:500 episode reward: total was -50.880000. running mean: -13.397709\n",
      "epsilon:0.090541 episode_count: 17283. steps_count: 7486133.000000\n",
      "Time elapsed:  21799.57218813896\n",
      "ep 2469: ep_len:600 episode reward: total was -34.170000. running mean: -13.605432\n",
      "ep 2469: ep_len:582 episode reward: total was 0.820000. running mean: -13.461177\n",
      "ep 2469: ep_len:79 episode reward: total was 6.290000. running mean: -13.263666\n",
      "ep 2469: ep_len:500 episode reward: total was -12.890000. running mean: -13.259929\n",
      "ep 2469: ep_len:48 episode reward: total was 15.000000. running mean: -12.977330\n",
      "ep 2469: ep_len:541 episode reward: total was -35.650000. running mean: -13.204056\n",
      "ep 2469: ep_len:500 episode reward: total was 36.130000. running mean: -12.710716\n",
      "epsilon:0.090497 episode_count: 17290. steps_count: 7488983.000000\n",
      "Time elapsed:  21809.621917009354\n",
      "ep 2470: ep_len:640 episode reward: total was -14.940000. running mean: -12.733009\n",
      "ep 2470: ep_len:599 episode reward: total was 101.980000. running mean: -11.585879\n",
      "ep 2470: ep_len:565 episode reward: total was -143.680000. running mean: -12.906820\n",
      "ep 2470: ep_len:500 episode reward: total was -1.990000. running mean: -12.797652\n",
      "ep 2470: ep_len:3 episode reward: total was 0.000000. running mean: -12.669675\n",
      "ep 2470: ep_len:581 episode reward: total was -97.090000. running mean: -13.513878\n",
      "ep 2470: ep_len:507 episode reward: total was -6.250000. running mean: -13.441240\n",
      "epsilon:0.090452 episode_count: 17297. steps_count: 7492378.000000\n",
      "Time elapsed:  21819.771163225174\n",
      "ep 2471: ep_len:521 episode reward: total was 33.620000. running mean: -12.970627\n",
      "ep 2471: ep_len:500 episode reward: total was 3.800000. running mean: -12.802921\n",
      "ep 2471: ep_len:578 episode reward: total was -28.470000. running mean: -12.959592\n",
      "ep 2471: ep_len:500 episode reward: total was -0.450000. running mean: -12.834496\n",
      "ep 2471: ep_len:3 episode reward: total was 1.010000. running mean: -12.696051\n",
      "ep 2471: ep_len:530 episode reward: total was 19.130000. running mean: -12.377790\n",
      "ep 2471: ep_len:500 episode reward: total was -103.240000. running mean: -13.286412\n",
      "epsilon:0.090408 episode_count: 17304. steps_count: 7495510.000000\n",
      "Time elapsed:  21829.237238407135\n",
      "ep 2472: ep_len:203 episode reward: total was -18.340000. running mean: -13.336948\n",
      "ep 2472: ep_len:567 episode reward: total was 35.600000. running mean: -12.847579\n",
      "ep 2472: ep_len:575 episode reward: total was -38.070000. running mean: -13.099803\n",
      "ep 2472: ep_len:538 episode reward: total was -4.290000. running mean: -13.011705\n",
      "ep 2472: ep_len:83 episode reward: total was 5.230000. running mean: -12.829288\n",
      "ep 2472: ep_len:555 episode reward: total was -50.840000. running mean: -13.209395\n",
      "ep 2472: ep_len:203 episode reward: total was -9.500000. running mean: -13.172301\n",
      "epsilon:0.090364 episode_count: 17311. steps_count: 7498234.000000\n",
      "Time elapsed:  21839.70454955101\n",
      "ep 2473: ep_len:565 episode reward: total was 20.550000. running mean: -12.835078\n",
      "ep 2473: ep_len:601 episode reward: total was 25.400000. running mean: -12.452727\n",
      "ep 2473: ep_len:536 episode reward: total was -38.190000. running mean: -12.710100\n",
      "ep 2473: ep_len:500 episode reward: total was 44.720000. running mean: -12.135799\n",
      "ep 2473: ep_len:3 episode reward: total was -0.490000. running mean: -12.019341\n",
      "ep 2473: ep_len:500 episode reward: total was -54.090000. running mean: -12.440048\n",
      "ep 2473: ep_len:569 episode reward: total was -85.020000. running mean: -13.165847\n",
      "epsilon:0.090319 episode_count: 17318. steps_count: 7501508.000000\n",
      "Time elapsed:  21849.343367099762\n",
      "ep 2474: ep_len:520 episode reward: total was -9.290000. running mean: -13.127089\n",
      "ep 2474: ep_len:201 episode reward: total was -4.620000. running mean: -13.042018\n",
      "ep 2474: ep_len:614 episode reward: total was -29.810000. running mean: -13.209698\n",
      "ep 2474: ep_len:56 episode reward: total was 3.330000. running mean: -13.044301\n",
      "ep 2474: ep_len:3 episode reward: total was 1.010000. running mean: -12.903758\n",
      "ep 2474: ep_len:501 episode reward: total was -0.650000. running mean: -12.781220\n",
      "ep 2474: ep_len:510 episode reward: total was -22.240000. running mean: -12.875808\n",
      "epsilon:0.090275 episode_count: 17325. steps_count: 7503913.000000\n",
      "Time elapsed:  21856.980568170547\n",
      "ep 2475: ep_len:217 episode reward: total was -4.240000. running mean: -12.789450\n",
      "ep 2475: ep_len:578 episode reward: total was 54.190000. running mean: -12.119655\n",
      "ep 2475: ep_len:612 episode reward: total was -57.050000. running mean: -12.568959\n",
      "ep 2475: ep_len:526 episode reward: total was 17.000000. running mean: -12.273269\n",
      "ep 2475: ep_len:130 episode reward: total was 12.840000. running mean: -12.022136\n",
      "ep 2475: ep_len:602 episode reward: total was -13.410000. running mean: -12.036015\n",
      "ep 2475: ep_len:546 episode reward: total was -20.770000. running mean: -12.123355\n",
      "epsilon:0.090231 episode_count: 17332. steps_count: 7507124.000000\n",
      "Time elapsed:  21872.31795310974\n",
      "ep 2476: ep_len:663 episode reward: total was -72.420000. running mean: -12.726321\n",
      "ep 2476: ep_len:568 episode reward: total was 80.420000. running mean: -11.794858\n",
      "ep 2476: ep_len:367 episode reward: total was 18.760000. running mean: -11.489310\n",
      "ep 2476: ep_len:593 episode reward: total was 15.990000. running mean: -11.214517\n",
      "ep 2476: ep_len:39 episode reward: total was 14.510000. running mean: -10.957271\n",
      "ep 2476: ep_len:592 episode reward: total was -20.300000. running mean: -11.050699\n",
      "ep 2476: ep_len:524 episode reward: total was 17.000000. running mean: -10.770192\n",
      "epsilon:0.090186 episode_count: 17339. steps_count: 7510470.000000\n",
      "Time elapsed:  21882.532736301422\n",
      "ep 2477: ep_len:220 episode reward: total was 11.620000. running mean: -10.546290\n",
      "ep 2477: ep_len:500 episode reward: total was 56.290000. running mean: -9.877927\n",
      "ep 2477: ep_len:385 episode reward: total was 35.440000. running mean: -9.424748\n",
      "ep 2477: ep_len:589 episode reward: total was 45.350000. running mean: -8.877000\n",
      "ep 2477: ep_len:3 episode reward: total was 1.010000. running mean: -8.778130\n",
      "ep 2477: ep_len:521 episode reward: total was -24.920000. running mean: -8.939549\n",
      "ep 2477: ep_len:503 episode reward: total was 6.350000. running mean: -8.786653\n",
      "epsilon:0.090142 episode_count: 17346. steps_count: 7513191.000000\n",
      "Time elapsed:  21890.915603637695\n",
      "ep 2478: ep_len:504 episode reward: total was -13.400000. running mean: -8.832787\n",
      "ep 2478: ep_len:279 episode reward: total was -27.510000. running mean: -9.019559\n",
      "ep 2478: ep_len:560 episode reward: total was -24.800000. running mean: -9.177363\n",
      "ep 2478: ep_len:500 episode reward: total was 42.820000. running mean: -8.657390\n",
      "ep 2478: ep_len:88 episode reward: total was -3.840000. running mean: -8.609216\n",
      "ep 2478: ep_len:558 episode reward: total was 22.870000. running mean: -8.294424\n",
      "ep 2478: ep_len:357 episode reward: total was -56.960000. running mean: -8.781079\n",
      "epsilon:0.090098 episode_count: 17353. steps_count: 7516037.000000\n",
      "Time elapsed:  21899.720001220703\n",
      "ep 2479: ep_len:521 episode reward: total was -25.720000. running mean: -8.950469\n",
      "ep 2479: ep_len:500 episode reward: total was -6.460000. running mean: -8.925564\n",
      "ep 2479: ep_len:540 episode reward: total was 4.970000. running mean: -8.786608\n",
      "ep 2479: ep_len:574 episode reward: total was 27.550000. running mean: -8.423242\n",
      "ep 2479: ep_len:3 episode reward: total was 1.010000. running mean: -8.328910\n",
      "ep 2479: ep_len:548 episode reward: total was -7.400000. running mean: -8.319621\n",
      "ep 2479: ep_len:308 episode reward: total was -34.360000. running mean: -8.580024\n",
      "epsilon:0.090053 episode_count: 17360. steps_count: 7519031.000000\n",
      "Time elapsed:  21908.945283412933\n",
      "ep 2480: ep_len:229 episode reward: total was -9.470000. running mean: -8.588924\n",
      "ep 2480: ep_len:201 episode reward: total was 3.890000. running mean: -8.464135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2480: ep_len:503 episode reward: total was -36.240000. running mean: -8.741894\n",
      "ep 2480: ep_len:118 episode reward: total was 0.050000. running mean: -8.653975\n",
      "ep 2480: ep_len:3 episode reward: total was 1.010000. running mean: -8.557335\n",
      "ep 2480: ep_len:513 episode reward: total was -33.180000. running mean: -8.803562\n",
      "ep 2480: ep_len:518 episode reward: total was -22.030000. running mean: -8.935826\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.090009 episode_count: 17367. steps_count: 7521116.000000\n",
      "Time elapsed:  21920.84713435173\n",
      "ep 2481: ep_len:527 episode reward: total was 8.810000. running mean: -8.758368\n",
      "ep 2481: ep_len:613 episode reward: total was 18.610000. running mean: -8.484684\n",
      "ep 2481: ep_len:565 episode reward: total was -46.270000. running mean: -8.862537\n",
      "ep 2481: ep_len:406 episode reward: total was -38.590000. running mean: -9.159812\n",
      "ep 2481: ep_len:3 episode reward: total was 1.010000. running mean: -9.058114\n",
      "ep 2481: ep_len:634 episode reward: total was 18.910000. running mean: -8.778433\n",
      "ep 2481: ep_len:512 episode reward: total was -20.140000. running mean: -8.892048\n",
      "epsilon:0.089965 episode_count: 17374. steps_count: 7524376.000000\n",
      "Time elapsed:  21930.701990127563\n",
      "ep 2482: ep_len:210 episode reward: total was 15.830000. running mean: -8.644828\n",
      "ep 2482: ep_len:500 episode reward: total was -69.840000. running mean: -9.256779\n",
      "ep 2482: ep_len:380 episode reward: total was 19.350000. running mean: -8.970712\n",
      "ep 2482: ep_len:40 episode reward: total was 1.700000. running mean: -8.864005\n",
      "ep 2482: ep_len:3 episode reward: total was 0.000000. running mean: -8.775365\n",
      "ep 2482: ep_len:500 episode reward: total was -3.350000. running mean: -8.721111\n",
      "ep 2482: ep_len:531 episode reward: total was -25.510000. running mean: -8.889000\n",
      "epsilon:0.089920 episode_count: 17381. steps_count: 7526540.000000\n",
      "Time elapsed:  21937.706298351288\n",
      "ep 2483: ep_len:577 episode reward: total was 4.300000. running mean: -8.757110\n",
      "ep 2483: ep_len:517 episode reward: total was -5.820000. running mean: -8.727739\n",
      "ep 2483: ep_len:502 episode reward: total was -25.750000. running mean: -8.897961\n",
      "ep 2483: ep_len:104 episode reward: total was 8.010000. running mean: -8.728882\n",
      "ep 2483: ep_len:3 episode reward: total was 1.010000. running mean: -8.631493\n",
      "ep 2483: ep_len:590 episode reward: total was 13.760000. running mean: -8.407578\n",
      "ep 2483: ep_len:612 episode reward: total was -9.880000. running mean: -8.422302\n",
      "epsilon:0.089876 episode_count: 17388. steps_count: 7529445.000000\n",
      "Time elapsed:  21946.77767086029\n",
      "ep 2484: ep_len:634 episode reward: total was 26.650000. running mean: -8.071579\n",
      "ep 2484: ep_len:691 episode reward: total was -188.120000. running mean: -9.872063\n",
      "ep 2484: ep_len:638 episode reward: total was -65.840000. running mean: -10.431743\n",
      "ep 2484: ep_len:519 episode reward: total was 57.950000. running mean: -9.747925\n",
      "ep 2484: ep_len:3 episode reward: total was 1.010000. running mean: -9.640346\n",
      "ep 2484: ep_len:624 episode reward: total was -105.650000. running mean: -10.600443\n",
      "ep 2484: ep_len:500 episode reward: total was -7.540000. running mean: -10.569838\n",
      "epsilon:0.089832 episode_count: 17395. steps_count: 7533054.000000\n",
      "Time elapsed:  21957.54973435402\n",
      "ep 2485: ep_len:264 episode reward: total was 19.410000. running mean: -10.270040\n",
      "ep 2485: ep_len:256 episode reward: total was 8.220000. running mean: -10.085139\n",
      "ep 2485: ep_len:600 episode reward: total was -22.630000. running mean: -10.210588\n",
      "ep 2485: ep_len:500 episode reward: total was 28.800000. running mean: -9.820482\n",
      "ep 2485: ep_len:71 episode reward: total was -15.770000. running mean: -9.879977\n",
      "ep 2485: ep_len:539 episode reward: total was -15.270000. running mean: -9.933877\n",
      "ep 2485: ep_len:205 episode reward: total was -30.020000. running mean: -10.134739\n",
      "epsilon:0.089787 episode_count: 17402. steps_count: 7535489.000000\n",
      "Time elapsed:  21965.09643292427\n",
      "ep 2486: ep_len:554 episode reward: total was 8.260000. running mean: -9.950791\n",
      "ep 2486: ep_len:500 episode reward: total was 29.020000. running mean: -9.561083\n",
      "ep 2486: ep_len:470 episode reward: total was 19.200000. running mean: -9.273473\n",
      "ep 2486: ep_len:47 episode reward: total was -0.770000. running mean: -9.188438\n",
      "ep 2486: ep_len:103 episode reward: total was 19.730000. running mean: -8.899253\n",
      "ep 2486: ep_len:500 episode reward: total was 1.150000. running mean: -8.798761\n",
      "ep 2486: ep_len:606 episode reward: total was -15.560000. running mean: -8.866373\n",
      "epsilon:0.089743 episode_count: 17409. steps_count: 7538269.000000\n",
      "Time elapsed:  21973.440083265305\n",
      "ep 2487: ep_len:554 episode reward: total was 2.900000. running mean: -8.748710\n",
      "ep 2487: ep_len:606 episode reward: total was -29.290000. running mean: -8.954123\n",
      "ep 2487: ep_len:574 episode reward: total was -119.210000. running mean: -10.056681\n",
      "ep 2487: ep_len:515 episode reward: total was 19.110000. running mean: -9.765014\n",
      "ep 2487: ep_len:50 episode reward: total was 21.510000. running mean: -9.452264\n",
      "ep 2487: ep_len:610 episode reward: total was -80.860000. running mean: -10.166342\n",
      "ep 2487: ep_len:547 episode reward: total was -19.660000. running mean: -10.261278\n",
      "epsilon:0.089699 episode_count: 17416. steps_count: 7541725.000000\n",
      "Time elapsed:  21981.21688246727\n",
      "ep 2488: ep_len:500 episode reward: total was 33.690000. running mean: -9.821765\n",
      "ep 2488: ep_len:500 episode reward: total was -25.550000. running mean: -9.979048\n",
      "ep 2488: ep_len:500 episode reward: total was 39.750000. running mean: -9.481757\n",
      "ep 2488: ep_len:600 episode reward: total was 46.110000. running mean: -8.925840\n",
      "ep 2488: ep_len:3 episode reward: total was 0.000000. running mean: -8.836581\n",
      "ep 2488: ep_len:500 episode reward: total was 32.220000. running mean: -8.426016\n",
      "ep 2488: ep_len:195 episode reward: total was 0.110000. running mean: -8.340655\n",
      "epsilon:0.089654 episode_count: 17423. steps_count: 7544523.000000\n",
      "Time elapsed:  21989.051048755646\n",
      "ep 2489: ep_len:601 episode reward: total was -53.470000. running mean: -8.791949\n",
      "ep 2489: ep_len:627 episode reward: total was -19.440000. running mean: -8.898429\n",
      "ep 2489: ep_len:370 episode reward: total was 31.890000. running mean: -8.490545\n",
      "ep 2489: ep_len:578 episode reward: total was 47.290000. running mean: -7.932740\n",
      "ep 2489: ep_len:3 episode reward: total was 0.000000. running mean: -7.853412\n",
      "ep 2489: ep_len:500 episode reward: total was 2.610000. running mean: -7.748778\n",
      "ep 2489: ep_len:510 episode reward: total was -32.080000. running mean: -7.992090\n",
      "epsilon:0.089610 episode_count: 17430. steps_count: 7547712.000000\n",
      "Time elapsed:  21997.500487565994\n",
      "ep 2490: ep_len:117 episode reward: total was -11.750000. running mean: -8.029669\n",
      "ep 2490: ep_len:526 episode reward: total was -6.130000. running mean: -8.010673\n",
      "ep 2490: ep_len:542 episode reward: total was -89.270000. running mean: -8.823266\n",
      "ep 2490: ep_len:505 episode reward: total was -36.190000. running mean: -9.096933\n",
      "ep 2490: ep_len:129 episode reward: total was 21.800000. running mean: -8.787964\n",
      "ep 2490: ep_len:522 episode reward: total was -93.490000. running mean: -9.634984\n",
      "ep 2490: ep_len:640 episode reward: total was -15.310000. running mean: -9.691735\n",
      "epsilon:0.089566 episode_count: 17437. steps_count: 7550693.000000\n",
      "Time elapsed:  22009.13995218277\n",
      "ep 2491: ep_len:500 episode reward: total was 40.790000. running mean: -9.186917\n",
      "ep 2491: ep_len:527 episode reward: total was -22.890000. running mean: -9.323948\n",
      "ep 2491: ep_len:594 episode reward: total was -43.640000. running mean: -9.667109\n",
      "ep 2491: ep_len:502 episode reward: total was -33.680000. running mean: -9.907237\n",
      "ep 2491: ep_len:3 episode reward: total was 1.010000. running mean: -9.798065\n",
      "ep 2491: ep_len:619 episode reward: total was -3.710000. running mean: -9.737184\n",
      "ep 2491: ep_len:568 episode reward: total was -38.010000. running mean: -10.019913\n",
      "epsilon:0.089521 episode_count: 17444. steps_count: 7554006.000000\n",
      "Time elapsed:  22017.994368314743\n",
      "ep 2492: ep_len:551 episode reward: total was -25.930000. running mean: -10.179013\n",
      "ep 2492: ep_len:602 episode reward: total was -57.430000. running mean: -10.651523\n",
      "ep 2492: ep_len:500 episode reward: total was 8.340000. running mean: -10.461608\n",
      "ep 2492: ep_len:500 episode reward: total was -1.510000. running mean: -10.372092\n",
      "ep 2492: ep_len:3 episode reward: total was 1.010000. running mean: -10.258271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2492: ep_len:516 episode reward: total was -13.340000. running mean: -10.289088\n",
      "ep 2492: ep_len:581 episode reward: total was -47.740000. running mean: -10.663597\n",
      "epsilon:0.089477 episode_count: 17451. steps_count: 7557259.000000\n",
      "Time elapsed:  22030.536329984665\n",
      "ep 2493: ep_len:509 episode reward: total was -99.930000. running mean: -11.556262\n",
      "ep 2493: ep_len:536 episode reward: total was 58.050000. running mean: -10.860199\n",
      "ep 2493: ep_len:555 episode reward: total was -32.350000. running mean: -11.075097\n",
      "ep 2493: ep_len:525 episode reward: total was 22.870000. running mean: -10.735646\n",
      "ep 2493: ep_len:3 episode reward: total was 1.010000. running mean: -10.618189\n",
      "ep 2493: ep_len:658 episode reward: total was 26.100000. running mean: -10.251008\n",
      "ep 2493: ep_len:294 episode reward: total was 10.010000. running mean: -10.048398\n",
      "epsilon:0.089433 episode_count: 17458. steps_count: 7560339.000000\n",
      "Time elapsed:  22037.21480202675\n",
      "ep 2494: ep_len:530 episode reward: total was -43.360000. running mean: -10.381514\n",
      "ep 2494: ep_len:500 episode reward: total was -31.900000. running mean: -10.596698\n",
      "ep 2494: ep_len:500 episode reward: total was 9.400000. running mean: -10.396731\n",
      "ep 2494: ep_len:170 episode reward: total was 14.200000. running mean: -10.150764\n",
      "ep 2494: ep_len:3 episode reward: total was 1.010000. running mean: -10.039156\n",
      "ep 2494: ep_len:544 episode reward: total was -26.460000. running mean: -10.203365\n",
      "ep 2494: ep_len:617 episode reward: total was 9.970000. running mean: -10.001631\n",
      "epsilon:0.089388 episode_count: 17465. steps_count: 7563203.000000\n",
      "Time elapsed:  22041.989287614822\n",
      "ep 2495: ep_len:573 episode reward: total was 19.630000. running mean: -9.705315\n",
      "ep 2495: ep_len:524 episode reward: total was 17.980000. running mean: -9.428462\n",
      "ep 2495: ep_len:500 episode reward: total was -31.870000. running mean: -9.652877\n",
      "ep 2495: ep_len:500 episode reward: total was 1.900000. running mean: -9.537348\n",
      "ep 2495: ep_len:3 episode reward: total was -0.490000. running mean: -9.446875\n",
      "ep 2495: ep_len:687 episode reward: total was -35.500000. running mean: -9.707406\n",
      "ep 2495: ep_len:500 episode reward: total was 13.110000. running mean: -9.479232\n",
      "epsilon:0.089344 episode_count: 17472. steps_count: 7566490.000000\n",
      "Time elapsed:  22050.735576868057\n",
      "ep 2496: ep_len:229 episode reward: total was 16.760000. running mean: -9.216840\n",
      "ep 2496: ep_len:580 episode reward: total was -7.100000. running mean: -9.195671\n",
      "ep 2496: ep_len:564 episode reward: total was -17.110000. running mean: -9.274815\n",
      "ep 2496: ep_len:130 episode reward: total was -0.010000. running mean: -9.182167\n",
      "ep 2496: ep_len:3 episode reward: total was 1.010000. running mean: -9.080245\n",
      "ep 2496: ep_len:546 episode reward: total was -12.020000. running mean: -9.109642\n",
      "ep 2496: ep_len:326 episode reward: total was 12.460000. running mean: -8.893946\n",
      "epsilon:0.089300 episode_count: 17479. steps_count: 7568868.000000\n",
      "Time elapsed:  22057.268565416336\n",
      "ep 2497: ep_len:534 episode reward: total was -81.620000. running mean: -9.621207\n",
      "ep 2497: ep_len:608 episode reward: total was 3.010000. running mean: -9.494894\n",
      "ep 2497: ep_len:554 episode reward: total was 8.800000. running mean: -9.311946\n",
      "ep 2497: ep_len:506 episode reward: total was 23.520000. running mean: -8.983626\n",
      "ep 2497: ep_len:3 episode reward: total was -0.490000. running mean: -8.898690\n",
      "ep 2497: ep_len:690 episode reward: total was -147.060000. running mean: -10.280303\n",
      "ep 2497: ep_len:627 episode reward: total was -4.000000. running mean: -10.217500\n",
      "epsilon:0.089255 episode_count: 17486. steps_count: 7572390.000000\n",
      "Time elapsed:  22066.601234674454\n",
      "ep 2498: ep_len:577 episode reward: total was -5.030000. running mean: -10.165625\n",
      "ep 2498: ep_len:505 episode reward: total was -97.670000. running mean: -11.040669\n",
      "ep 2498: ep_len:673 episode reward: total was -49.570000. running mean: -11.425962\n",
      "ep 2498: ep_len:521 episode reward: total was 7.690000. running mean: -11.234802\n",
      "ep 2498: ep_len:112 episode reward: total was 17.220000. running mean: -10.950254\n",
      "ep 2498: ep_len:508 episode reward: total was -25.310000. running mean: -11.093852\n",
      "ep 2498: ep_len:513 episode reward: total was -17.970000. running mean: -11.162613\n",
      "epsilon:0.089211 episode_count: 17493. steps_count: 7575799.000000\n",
      "Time elapsed:  22080.099811315536\n",
      "ep 2499: ep_len:665 episode reward: total was -40.620000. running mean: -11.457187\n",
      "ep 2499: ep_len:353 episode reward: total was -0.120000. running mean: -11.343815\n",
      "ep 2499: ep_len:537 episode reward: total was -24.510000. running mean: -11.475477\n",
      "ep 2499: ep_len:522 episode reward: total was -13.860000. running mean: -11.499322\n",
      "ep 2499: ep_len:3 episode reward: total was 1.010000. running mean: -11.374229\n",
      "ep 2499: ep_len:610 episode reward: total was -1.980000. running mean: -11.280287\n",
      "ep 2499: ep_len:550 episode reward: total was -45.260000. running mean: -11.620084\n",
      "epsilon:0.089167 episode_count: 17500. steps_count: 7579039.000000\n",
      "Time elapsed:  22088.54124903679\n",
      "ep 2500: ep_len:500 episode reward: total was 5.830000. running mean: -11.445583\n",
      "ep 2500: ep_len:500 episode reward: total was -10.060000. running mean: -11.431727\n",
      "ep 2500: ep_len:556 episode reward: total was -41.150000. running mean: -11.728910\n",
      "ep 2500: ep_len:500 episode reward: total was -187.600000. running mean: -13.487621\n",
      "ep 2500: ep_len:130 episode reward: total was 28.880000. running mean: -13.063945\n",
      "ep 2500: ep_len:634 episode reward: total was 10.080000. running mean: -12.832505\n",
      "ep 2500: ep_len:515 episode reward: total was -38.990000. running mean: -13.094080\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.089122 episode_count: 17507. steps_count: 7582374.000000\n",
      "Time elapsed:  22102.245518922806\n",
      "ep 2501: ep_len:637 episode reward: total was -116.110000. running mean: -14.124239\n",
      "ep 2501: ep_len:500 episode reward: total was 63.710000. running mean: -13.345897\n",
      "ep 2501: ep_len:500 episode reward: total was 4.780000. running mean: -13.164638\n",
      "ep 2501: ep_len:604 episode reward: total was 24.580000. running mean: -12.787192\n",
      "ep 2501: ep_len:3 episode reward: total was 1.010000. running mean: -12.649220\n",
      "ep 2501: ep_len:500 episode reward: total was -15.050000. running mean: -12.673228\n",
      "ep 2501: ep_len:570 episode reward: total was -27.820000. running mean: -12.824695\n",
      "epsilon:0.089078 episode_count: 17514. steps_count: 7585688.000000\n",
      "Time elapsed:  22111.013246774673\n",
      "ep 2502: ep_len:567 episode reward: total was -38.570000. running mean: -13.082148\n",
      "ep 2502: ep_len:599 episode reward: total was -19.300000. running mean: -13.144327\n",
      "ep 2502: ep_len:598 episode reward: total was -44.420000. running mean: -13.457084\n",
      "ep 2502: ep_len:500 episode reward: total was -21.210000. running mean: -13.534613\n",
      "ep 2502: ep_len:3 episode reward: total was 1.010000. running mean: -13.389167\n",
      "ep 2502: ep_len:610 episode reward: total was 17.390000. running mean: -13.081375\n",
      "ep 2502: ep_len:550 episode reward: total was -0.070000. running mean: -12.951261\n",
      "epsilon:0.089034 episode_count: 17521. steps_count: 7589115.000000\n",
      "Time elapsed:  22122.352323532104\n",
      "ep 2503: ep_len:80 episode reward: total was 9.420000. running mean: -12.727549\n",
      "ep 2503: ep_len:588 episode reward: total was -21.170000. running mean: -12.811973\n",
      "ep 2503: ep_len:379 episode reward: total was 46.060000. running mean: -12.223253\n",
      "ep 2503: ep_len:500 episode reward: total was 32.080000. running mean: -11.780221\n",
      "ep 2503: ep_len:3 episode reward: total was 1.010000. running mean: -11.652319\n",
      "ep 2503: ep_len:580 episode reward: total was -1.570000. running mean: -11.551495\n",
      "ep 2503: ep_len:503 episode reward: total was 8.210000. running mean: -11.353880\n",
      "epsilon:0.088989 episode_count: 17528. steps_count: 7591748.000000\n",
      "Time elapsed:  22132.9284760952\n",
      "ep 2504: ep_len:533 episode reward: total was -6.430000. running mean: -11.304642\n",
      "ep 2504: ep_len:500 episode reward: total was -5.310000. running mean: -11.244695\n",
      "ep 2504: ep_len:535 episode reward: total was -32.570000. running mean: -11.457948\n",
      "ep 2504: ep_len:562 episode reward: total was 17.290000. running mean: -11.170469\n",
      "ep 2504: ep_len:3 episode reward: total was 1.010000. running mean: -11.048664\n",
      "ep 2504: ep_len:301 episode reward: total was 9.630000. running mean: -10.841877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2504: ep_len:567 episode reward: total was -4.050000. running mean: -10.773959\n",
      "epsilon:0.088945 episode_count: 17535. steps_count: 7594749.000000\n",
      "Time elapsed:  22140.92549920082\n",
      "ep 2505: ep_len:134 episode reward: total was 3.420000. running mean: -10.632019\n",
      "ep 2505: ep_len:500 episode reward: total was 29.680000. running mean: -10.228899\n",
      "ep 2505: ep_len:69 episode reward: total was -3.240000. running mean: -10.159010\n",
      "ep 2505: ep_len:607 episode reward: total was 51.080000. running mean: -9.546620\n",
      "ep 2505: ep_len:133 episode reward: total was 36.870000. running mean: -9.082454\n",
      "ep 2505: ep_len:231 episode reward: total was 47.390000. running mean: -8.517729\n",
      "ep 2505: ep_len:552 episode reward: total was 22.900000. running mean: -8.203552\n",
      "epsilon:0.088901 episode_count: 17542. steps_count: 7596975.000000\n",
      "Time elapsed:  22147.096753835678\n",
      "ep 2506: ep_len:501 episode reward: total was -39.570000. running mean: -8.517216\n",
      "ep 2506: ep_len:580 episode reward: total was 22.400000. running mean: -8.208044\n",
      "ep 2506: ep_len:532 episode reward: total was -39.650000. running mean: -8.522464\n",
      "ep 2506: ep_len:531 episode reward: total was -75.570000. running mean: -9.192939\n",
      "ep 2506: ep_len:3 episode reward: total was 1.010000. running mean: -9.090910\n",
      "ep 2506: ep_len:253 episode reward: total was 24.930000. running mean: -8.750701\n",
      "ep 2506: ep_len:504 episode reward: total was -7.870000. running mean: -8.741894\n",
      "epsilon:0.088856 episode_count: 17549. steps_count: 7599879.000000\n",
      "Time elapsed:  22158.30602788925\n",
      "ep 2507: ep_len:500 episode reward: total was 11.580000. running mean: -8.538675\n",
      "ep 2507: ep_len:607 episode reward: total was 35.180000. running mean: -8.101488\n",
      "ep 2507: ep_len:537 episode reward: total was -38.620000. running mean: -8.406673\n",
      "ep 2507: ep_len:170 episode reward: total was 9.230000. running mean: -8.230306\n",
      "ep 2507: ep_len:125 episode reward: total was 19.370000. running mean: -7.954303\n",
      "ep 2507: ep_len:503 episode reward: total was -49.400000. running mean: -8.368760\n",
      "ep 2507: ep_len:528 episode reward: total was 11.440000. running mean: -8.170673\n",
      "epsilon:0.088812 episode_count: 17556. steps_count: 7602849.000000\n",
      "Time elapsed:  22166.229561328888\n",
      "ep 2508: ep_len:638 episode reward: total was -7.620000. running mean: -8.165166\n",
      "ep 2508: ep_len:630 episode reward: total was 72.680000. running mean: -7.356714\n",
      "ep 2508: ep_len:79 episode reward: total was 0.260000. running mean: -7.280547\n",
      "ep 2508: ep_len:495 episode reward: total was -43.210000. running mean: -7.639842\n",
      "ep 2508: ep_len:3 episode reward: total was 1.010000. running mean: -7.553343\n",
      "ep 2508: ep_len:174 episode reward: total was 20.690000. running mean: -7.270910\n",
      "ep 2508: ep_len:597 episode reward: total was -30.710000. running mean: -7.505301\n",
      "epsilon:0.088768 episode_count: 17563. steps_count: 7605465.000000\n",
      "Time elapsed:  22173.299287319183\n",
      "ep 2509: ep_len:541 episode reward: total was -6.440000. running mean: -7.494648\n",
      "ep 2509: ep_len:500 episode reward: total was -10.510000. running mean: -7.524801\n",
      "ep 2509: ep_len:501 episode reward: total was -24.640000. running mean: -7.695953\n",
      "ep 2509: ep_len:517 episode reward: total was 28.590000. running mean: -7.333094\n",
      "ep 2509: ep_len:91 episode reward: total was -55.280000. running mean: -7.812563\n",
      "ep 2509: ep_len:505 episode reward: total was -162.010000. running mean: -9.354537\n",
      "ep 2509: ep_len:211 episode reward: total was -15.540000. running mean: -9.416392\n",
      "epsilon:0.088723 episode_count: 17570. steps_count: 7608331.000000\n",
      "Time elapsed:  22181.00138783455\n",
      "ep 2510: ep_len:500 episode reward: total was 11.340000. running mean: -9.208828\n",
      "ep 2510: ep_len:506 episode reward: total was 42.680000. running mean: -8.689939\n",
      "ep 2510: ep_len:500 episode reward: total was -20.030000. running mean: -8.803340\n",
      "ep 2510: ep_len:401 episode reward: total was -12.930000. running mean: -8.844607\n",
      "ep 2510: ep_len:80 episode reward: total was 16.250000. running mean: -8.593661\n",
      "ep 2510: ep_len:571 episode reward: total was -61.000000. running mean: -9.117724\n",
      "ep 2510: ep_len:616 episode reward: total was -50.740000. running mean: -9.533947\n",
      "epsilon:0.088679 episode_count: 17577. steps_count: 7611505.000000\n",
      "Time elapsed:  22189.26867556572\n",
      "ep 2511: ep_len:595 episode reward: total was -31.910000. running mean: -9.757707\n",
      "ep 2511: ep_len:500 episode reward: total was 7.230000. running mean: -9.587830\n",
      "ep 2511: ep_len:552 episode reward: total was -41.010000. running mean: -9.902052\n",
      "ep 2511: ep_len:500 episode reward: total was -15.610000. running mean: -9.959131\n",
      "ep 2511: ep_len:113 episode reward: total was 28.740000. running mean: -9.572140\n",
      "ep 2511: ep_len:606 episode reward: total was -45.190000. running mean: -9.928319\n",
      "ep 2511: ep_len:199 episode reward: total was -14.990000. running mean: -9.978935\n",
      "epsilon:0.088635 episode_count: 17584. steps_count: 7614570.000000\n",
      "Time elapsed:  22197.36523914337\n",
      "ep 2512: ep_len:500 episode reward: total was 54.950000. running mean: -9.329646\n",
      "ep 2512: ep_len:564 episode reward: total was -9.150000. running mean: -9.327850\n",
      "ep 2512: ep_len:422 episode reward: total was 22.780000. running mean: -9.006771\n",
      "ep 2512: ep_len:579 episode reward: total was 23.090000. running mean: -8.685803\n",
      "ep 2512: ep_len:98 episode reward: total was 21.270000. running mean: -8.386245\n",
      "ep 2512: ep_len:500 episode reward: total was -22.860000. running mean: -8.530983\n",
      "ep 2512: ep_len:522 episode reward: total was -11.350000. running mean: -8.559173\n",
      "epsilon:0.088590 episode_count: 17591. steps_count: 7617755.000000\n",
      "Time elapsed:  22205.915835380554\n",
      "ep 2513: ep_len:250 episode reward: total was 10.390000. running mean: -8.369681\n",
      "ep 2513: ep_len:568 episode reward: total was -14.760000. running mean: -8.433585\n",
      "ep 2513: ep_len:588 episode reward: total was -38.400000. running mean: -8.733249\n",
      "ep 2513: ep_len:552 episode reward: total was 34.360000. running mean: -8.302316\n",
      "ep 2513: ep_len:90 episode reward: total was -8.740000. running mean: -8.306693\n",
      "ep 2513: ep_len:500 episode reward: total was -12.120000. running mean: -8.344826\n",
      "ep 2513: ep_len:530 episode reward: total was 6.410000. running mean: -8.197278\n",
      "epsilon:0.088546 episode_count: 17598. steps_count: 7620833.000000\n",
      "Time elapsed:  22214.322558403015\n",
      "ep 2514: ep_len:504 episode reward: total was 49.090000. running mean: -7.624405\n",
      "ep 2514: ep_len:546 episode reward: total was 20.990000. running mean: -7.338261\n",
      "ep 2514: ep_len:611 episode reward: total was -20.880000. running mean: -7.473678\n",
      "ep 2514: ep_len:588 episode reward: total was -148.720000. running mean: -8.886142\n",
      "ep 2514: ep_len:3 episode reward: total was 1.010000. running mean: -8.787180\n",
      "ep 2514: ep_len:601 episode reward: total was -23.960000. running mean: -8.938908\n",
      "ep 2514: ep_len:358 episode reward: total was -51.160000. running mean: -9.361119\n",
      "epsilon:0.088502 episode_count: 17605. steps_count: 7624044.000000\n",
      "Time elapsed:  22222.979729175568\n",
      "ep 2515: ep_len:225 episode reward: total was 8.250000. running mean: -9.185008\n",
      "ep 2515: ep_len:290 episode reward: total was -0.370000. running mean: -9.096858\n",
      "ep 2515: ep_len:568 episode reward: total was -34.300000. running mean: -9.348890\n",
      "ep 2515: ep_len:527 episode reward: total was 13.230000. running mean: -9.123101\n",
      "ep 2515: ep_len:3 episode reward: total was 1.010000. running mean: -9.021770\n",
      "ep 2515: ep_len:661 episode reward: total was -18.070000. running mean: -9.112252\n",
      "ep 2515: ep_len:500 episode reward: total was -17.950000. running mean: -9.200629\n",
      "epsilon:0.088457 episode_count: 17612. steps_count: 7626818.000000\n",
      "Time elapsed:  22236.603577375412\n",
      "ep 2516: ep_len:211 episode reward: total was -5.550000. running mean: -9.164123\n",
      "ep 2516: ep_len:538 episode reward: total was -0.970000. running mean: -9.082182\n",
      "ep 2516: ep_len:500 episode reward: total was -4.210000. running mean: -9.033460\n",
      "ep 2516: ep_len:612 episode reward: total was -29.560000. running mean: -9.238725\n",
      "ep 2516: ep_len:3 episode reward: total was -1.500000. running mean: -9.161338\n",
      "ep 2516: ep_len:170 episode reward: total was 32.070000. running mean: -8.749025\n",
      "ep 2516: ep_len:524 episode reward: total was -5.730000. running mean: -8.718835\n",
      "epsilon:0.088413 episode_count: 17619. steps_count: 7629376.000000\n",
      "Time elapsed:  22240.74805521965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2517: ep_len:615 episode reward: total was -206.070000. running mean: -10.692346\n",
      "ep 2517: ep_len:252 episode reward: total was -46.600000. running mean: -11.051423\n",
      "ep 2517: ep_len:595 episode reward: total was -80.640000. running mean: -11.747309\n",
      "ep 2517: ep_len:541 episode reward: total was 27.250000. running mean: -11.357335\n",
      "ep 2517: ep_len:90 episode reward: total was 16.750000. running mean: -11.076262\n",
      "ep 2517: ep_len:612 episode reward: total was 13.950000. running mean: -10.825999\n",
      "ep 2517: ep_len:331 episode reward: total was -14.700000. running mean: -10.864739\n",
      "epsilon:0.088369 episode_count: 17626. steps_count: 7632412.000000\n",
      "Time elapsed:  22246.859456777573\n",
      "ep 2518: ep_len:599 episode reward: total was -71.280000. running mean: -11.468892\n",
      "ep 2518: ep_len:603 episode reward: total was -11.180000. running mean: -11.466003\n",
      "ep 2518: ep_len:569 episode reward: total was -30.370000. running mean: -11.655043\n",
      "ep 2518: ep_len:127 episode reward: total was -11.500000. running mean: -11.653493\n",
      "ep 2518: ep_len:3 episode reward: total was 1.010000. running mean: -11.526858\n",
      "ep 2518: ep_len:500 episode reward: total was -24.760000. running mean: -11.659189\n",
      "ep 2518: ep_len:290 episode reward: total was -3.270000. running mean: -11.575297\n",
      "epsilon:0.088324 episode_count: 17633. steps_count: 7635103.000000\n",
      "Time elapsed:  22254.040403842926\n",
      "ep 2519: ep_len:605 episode reward: total was 19.520000. running mean: -11.264344\n",
      "ep 2519: ep_len:550 episode reward: total was -2.630000. running mean: -11.178001\n",
      "ep 2519: ep_len:574 episode reward: total was -150.380000. running mean: -12.570021\n",
      "ep 2519: ep_len:56 episode reward: total was 2.320000. running mean: -12.421121\n",
      "ep 2519: ep_len:94 episode reward: total was 18.200000. running mean: -12.114909\n",
      "ep 2519: ep_len:314 episode reward: total was 0.490000. running mean: -11.988860\n",
      "ep 2519: ep_len:618 episode reward: total was -9.570000. running mean: -11.964672\n",
      "epsilon:0.088280 episode_count: 17640. steps_count: 7637914.000000\n",
      "Time elapsed:  22265.504505634308\n",
      "ep 2520: ep_len:589 episode reward: total was 69.920000. running mean: -11.145825\n",
      "ep 2520: ep_len:608 episode reward: total was -1.480000. running mean: -11.049167\n",
      "ep 2520: ep_len:554 episode reward: total was -55.940000. running mean: -11.498075\n",
      "ep 2520: ep_len:539 episode reward: total was 32.970000. running mean: -11.053394\n",
      "ep 2520: ep_len:3 episode reward: total was 1.010000. running mean: -10.932760\n",
      "ep 2520: ep_len:570 episode reward: total was -33.480000. running mean: -11.158233\n",
      "ep 2520: ep_len:505 episode reward: total was -2.350000. running mean: -11.070151\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.088236 episode_count: 17647. steps_count: 7641282.000000\n",
      "Time elapsed:  22279.31197333336\n",
      "ep 2521: ep_len:546 episode reward: total was -4.500000. running mean: -11.004449\n",
      "ep 2521: ep_len:524 episode reward: total was 12.800000. running mean: -10.766405\n",
      "ep 2521: ep_len:438 episode reward: total was 39.630000. running mean: -10.262440\n",
      "ep 2521: ep_len:500 episode reward: total was -22.990000. running mean: -10.389716\n",
      "ep 2521: ep_len:92 episode reward: total was 16.190000. running mean: -10.123919\n",
      "ep 2521: ep_len:500 episode reward: total was -30.760000. running mean: -10.330280\n",
      "ep 2521: ep_len:621 episode reward: total was -57.620000. running mean: -10.803177\n",
      "epsilon:0.088191 episode_count: 17654. steps_count: 7644503.000000\n",
      "Time elapsed:  22291.734382629395\n",
      "ep 2522: ep_len:134 episode reward: total was 1.430000. running mean: -10.680845\n",
      "ep 2522: ep_len:197 episode reward: total was -21.100000. running mean: -10.785037\n",
      "ep 2522: ep_len:562 episode reward: total was -47.230000. running mean: -11.149486\n",
      "ep 2522: ep_len:56 episode reward: total was -10.700000. running mean: -11.144991\n",
      "ep 2522: ep_len:94 episode reward: total was 26.250000. running mean: -10.771042\n",
      "ep 2522: ep_len:702 episode reward: total was -23.830000. running mean: -10.901631\n",
      "ep 2522: ep_len:537 episode reward: total was 12.770000. running mean: -10.664915\n",
      "epsilon:0.088147 episode_count: 17661. steps_count: 7646785.000000\n",
      "Time elapsed:  22298.236668109894\n",
      "ep 2523: ep_len:567 episode reward: total was 43.080000. running mean: -10.127466\n",
      "ep 2523: ep_len:292 episode reward: total was 17.340000. running mean: -9.852791\n",
      "ep 2523: ep_len:536 episode reward: total was -52.480000. running mean: -10.279063\n",
      "ep 2523: ep_len:162 episode reward: total was 6.680000. running mean: -10.109472\n",
      "ep 2523: ep_len:3 episode reward: total was 1.010000. running mean: -9.998278\n",
      "ep 2523: ep_len:541 episode reward: total was 18.290000. running mean: -9.715395\n",
      "ep 2523: ep_len:589 episode reward: total was -14.690000. running mean: -9.765141\n",
      "epsilon:0.088103 episode_count: 17668. steps_count: 7649475.000000\n",
      "Time elapsed:  22309.655272245407\n",
      "ep 2524: ep_len:510 episode reward: total was -67.520000. running mean: -10.342690\n",
      "ep 2524: ep_len:500 episode reward: total was -16.020000. running mean: -10.399463\n",
      "ep 2524: ep_len:355 episode reward: total was 42.360000. running mean: -9.871868\n",
      "ep 2524: ep_len:500 episode reward: total was -46.410000. running mean: -10.237249\n",
      "ep 2524: ep_len:83 episode reward: total was -45.810000. running mean: -10.592977\n",
      "ep 2524: ep_len:538 episode reward: total was -100.980000. running mean: -11.496847\n",
      "ep 2524: ep_len:538 episode reward: total was -44.480000. running mean: -11.826679\n",
      "epsilon:0.088058 episode_count: 17675. steps_count: 7652499.000000\n",
      "Time elapsed:  22317.670322179794\n",
      "ep 2525: ep_len:246 episode reward: total was 2.890000. running mean: -11.679512\n",
      "ep 2525: ep_len:500 episode reward: total was 33.060000. running mean: -11.232117\n",
      "ep 2525: ep_len:558 episode reward: total was -24.850000. running mean: -11.368296\n",
      "ep 2525: ep_len:500 episode reward: total was 7.910000. running mean: -11.175513\n",
      "ep 2525: ep_len:3 episode reward: total was 1.010000. running mean: -11.053658\n",
      "ep 2525: ep_len:522 episode reward: total was -4.670000. running mean: -10.989821\n",
      "ep 2525: ep_len:554 episode reward: total was 10.060000. running mean: -10.779323\n",
      "epsilon:0.088014 episode_count: 17682. steps_count: 7655382.000000\n",
      "Time elapsed:  22325.362578868866\n",
      "ep 2526: ep_len:547 episode reward: total was -104.180000. running mean: -11.713330\n",
      "ep 2526: ep_len:549 episode reward: total was 6.690000. running mean: -11.529296\n",
      "ep 2526: ep_len:500 episode reward: total was 9.390000. running mean: -11.320103\n",
      "ep 2526: ep_len:500 episode reward: total was 51.850000. running mean: -10.688402\n",
      "ep 2526: ep_len:3 episode reward: total was 1.010000. running mean: -10.571418\n",
      "ep 2526: ep_len:587 episode reward: total was -15.650000. running mean: -10.622204\n",
      "ep 2526: ep_len:526 episode reward: total was 11.560000. running mean: -10.400382\n",
      "epsilon:0.087970 episode_count: 17689. steps_count: 7658594.000000\n",
      "Time elapsed:  22337.88263821602\n",
      "ep 2527: ep_len:500 episode reward: total was 63.340000. running mean: -9.662978\n",
      "ep 2527: ep_len:501 episode reward: total was -27.890000. running mean: -9.845248\n",
      "ep 2527: ep_len:631 episode reward: total was -24.250000. running mean: -9.989296\n",
      "ep 2527: ep_len:509 episode reward: total was -40.130000. running mean: -10.290703\n",
      "ep 2527: ep_len:132 episode reward: total was 34.840000. running mean: -9.839396\n",
      "ep 2527: ep_len:500 episode reward: total was -6.940000. running mean: -9.810402\n",
      "ep 2527: ep_len:573 episode reward: total was 10.310000. running mean: -9.609198\n",
      "epsilon:0.087925 episode_count: 17696. steps_count: 7661940.000000\n",
      "Time elapsed:  22346.494256734848\n",
      "ep 2528: ep_len:566 episode reward: total was 8.780000. running mean: -9.425306\n",
      "ep 2528: ep_len:543 episode reward: total was 8.050000. running mean: -9.250553\n",
      "ep 2528: ep_len:603 episode reward: total was -17.720000. running mean: -9.335247\n",
      "ep 2528: ep_len:500 episode reward: total was -42.490000. running mean: -9.666795\n",
      "ep 2528: ep_len:3 episode reward: total was 1.010000. running mean: -9.560027\n",
      "ep 2528: ep_len:651 episode reward: total was 18.290000. running mean: -9.281527\n",
      "ep 2528: ep_len:265 episode reward: total was -5.920000. running mean: -9.247911\n",
      "epsilon:0.087881 episode_count: 17703. steps_count: 7665071.000000\n",
      "Time elapsed:  22358.569620609283\n",
      "ep 2529: ep_len:568 episode reward: total was 13.190000. running mean: -9.023532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2529: ep_len:500 episode reward: total was 50.780000. running mean: -8.425497\n",
      "ep 2529: ep_len:558 episode reward: total was -27.450000. running mean: -8.615742\n",
      "ep 2529: ep_len:132 episode reward: total was 23.610000. running mean: -8.293485\n",
      "ep 2529: ep_len:3 episode reward: total was 1.010000. running mean: -8.200450\n",
      "ep 2529: ep_len:546 episode reward: total was -64.610000. running mean: -8.764545\n",
      "ep 2529: ep_len:556 episode reward: total was 13.600000. running mean: -8.540900\n",
      "epsilon:0.087837 episode_count: 17710. steps_count: 7667934.000000\n",
      "Time elapsed:  22364.708193063736\n",
      "ep 2530: ep_len:585 episode reward: total was 42.960000. running mean: -8.025891\n",
      "ep 2530: ep_len:500 episode reward: total was 17.530000. running mean: -7.770332\n",
      "ep 2530: ep_len:528 episode reward: total was -30.530000. running mean: -7.997929\n",
      "ep 2530: ep_len:521 episode reward: total was -6.110000. running mean: -7.979049\n",
      "ep 2530: ep_len:93 episode reward: total was -43.770000. running mean: -8.336959\n",
      "ep 2530: ep_len:648 episode reward: total was -6.900000. running mean: -8.322589\n",
      "ep 2530: ep_len:602 episode reward: total was -20.620000. running mean: -8.445563\n",
      "epsilon:0.087792 episode_count: 17717. steps_count: 7671411.000000\n",
      "Time elapsed:  22377.113220214844\n",
      "ep 2531: ep_len:538 episode reward: total was -61.320000. running mean: -8.974308\n",
      "ep 2531: ep_len:501 episode reward: total was 1.730000. running mean: -8.867265\n",
      "ep 2531: ep_len:548 episode reward: total was -13.920000. running mean: -8.917792\n",
      "ep 2531: ep_len:506 episode reward: total was 4.590000. running mean: -8.782714\n",
      "ep 2531: ep_len:50 episode reward: total was 20.010000. running mean: -8.494787\n",
      "ep 2531: ep_len:500 episode reward: total was -8.370000. running mean: -8.493539\n",
      "ep 2531: ep_len:584 episode reward: total was -19.490000. running mean: -8.603504\n",
      "epsilon:0.087748 episode_count: 17724. steps_count: 7674638.000000\n",
      "Time elapsed:  22384.48935008049\n",
      "ep 2532: ep_len:265 episode reward: total was -1.150000. running mean: -8.528969\n",
      "ep 2532: ep_len:500 episode reward: total was 12.200000. running mean: -8.321679\n",
      "ep 2532: ep_len:694 episode reward: total was -79.780000. running mean: -9.036262\n",
      "ep 2532: ep_len:124 episode reward: total was 5.490000. running mean: -8.890999\n",
      "ep 2532: ep_len:3 episode reward: total was 1.010000. running mean: -8.791989\n",
      "ep 2532: ep_len:183 episode reward: total was -43.440000. running mean: -9.138470\n",
      "ep 2532: ep_len:556 episode reward: total was -3.300000. running mean: -9.080085\n",
      "epsilon:0.087704 episode_count: 17731. steps_count: 7676963.000000\n",
      "Time elapsed:  22390.972080230713\n",
      "ep 2533: ep_len:656 episode reward: total was -14.910000. running mean: -9.138384\n",
      "ep 2533: ep_len:500 episode reward: total was -6.480000. running mean: -9.111800\n",
      "ep 2533: ep_len:568 episode reward: total was -71.650000. running mean: -9.737182\n",
      "ep 2533: ep_len:624 episode reward: total was 55.170000. running mean: -9.088110\n",
      "ep 2533: ep_len:2 episode reward: total was -0.500000. running mean: -9.002229\n",
      "ep 2533: ep_len:173 episode reward: total was 36.110000. running mean: -8.551107\n",
      "ep 2533: ep_len:583 episode reward: total was -12.880000. running mean: -8.594396\n",
      "epsilon:0.087659 episode_count: 17738. steps_count: 7680069.000000\n",
      "Time elapsed:  22399.3406188488\n",
      "ep 2534: ep_len:593 episode reward: total was 21.790000. running mean: -8.290552\n",
      "ep 2534: ep_len:500 episode reward: total was 1.520000. running mean: -8.192446\n",
      "ep 2534: ep_len:527 episode reward: total was -66.810000. running mean: -8.778622\n",
      "ep 2534: ep_len:579 episode reward: total was 46.750000. running mean: -8.223336\n",
      "ep 2534: ep_len:3 episode reward: total was 1.010000. running mean: -8.131002\n",
      "ep 2534: ep_len:635 episode reward: total was 32.660000. running mean: -7.723092\n",
      "ep 2534: ep_len:531 episode reward: total was 6.300000. running mean: -7.582861\n",
      "epsilon:0.087615 episode_count: 17745. steps_count: 7683437.000000\n",
      "Time elapsed:  22411.638547182083\n",
      "ep 2535: ep_len:586 episode reward: total was 25.870000. running mean: -7.248333\n",
      "ep 2535: ep_len:541 episode reward: total was 26.440000. running mean: -6.911450\n",
      "ep 2535: ep_len:555 episode reward: total was -51.230000. running mean: -7.354635\n",
      "ep 2535: ep_len:524 episode reward: total was 7.720000. running mean: -7.203889\n",
      "ep 2535: ep_len:3 episode reward: total was 1.010000. running mean: -7.121750\n",
      "ep 2535: ep_len:293 episode reward: total was -5.330000. running mean: -7.103832\n",
      "ep 2535: ep_len:500 episode reward: total was 1.570000. running mean: -7.017094\n",
      "epsilon:0.087571 episode_count: 17752. steps_count: 7686439.000000\n",
      "Time elapsed:  22419.67764544487\n",
      "ep 2536: ep_len:642 episode reward: total was 13.570000. running mean: -6.811223\n",
      "ep 2536: ep_len:500 episode reward: total was 42.980000. running mean: -6.313311\n",
      "ep 2536: ep_len:578 episode reward: total was -2.390000. running mean: -6.274078\n",
      "ep 2536: ep_len:544 episode reward: total was -3.370000. running mean: -6.245037\n",
      "ep 2536: ep_len:54 episode reward: total was 21.000000. running mean: -5.972587\n",
      "ep 2536: ep_len:559 episode reward: total was -2.180000. running mean: -5.934661\n",
      "ep 2536: ep_len:574 episode reward: total was 3.250000. running mean: -5.842814\n",
      "epsilon:0.087526 episode_count: 17759. steps_count: 7689890.000000\n",
      "Time elapsed:  22428.63041806221\n",
      "ep 2537: ep_len:555 episode reward: total was 18.860000. running mean: -5.595786\n",
      "ep 2537: ep_len:500 episode reward: total was 31.030000. running mean: -5.229528\n",
      "ep 2537: ep_len:569 episode reward: total was -51.460000. running mean: -5.691833\n",
      "ep 2537: ep_len:514 episode reward: total was -58.920000. running mean: -6.224114\n",
      "ep 2537: ep_len:3 episode reward: total was 1.010000. running mean: -6.151773\n",
      "ep 2537: ep_len:626 episode reward: total was -130.150000. running mean: -7.391756\n",
      "ep 2537: ep_len:615 episode reward: total was 8.630000. running mean: -7.231538\n",
      "epsilon:0.087482 episode_count: 17766. steps_count: 7693272.000000\n",
      "Time elapsed:  22437.486270666122\n",
      "ep 2538: ep_len:500 episode reward: total was 26.670000. running mean: -6.892523\n",
      "ep 2538: ep_len:500 episode reward: total was 82.440000. running mean: -5.999197\n",
      "ep 2538: ep_len:567 episode reward: total was -27.750000. running mean: -6.216705\n",
      "ep 2538: ep_len:613 episode reward: total was 17.690000. running mean: -5.977638\n",
      "ep 2538: ep_len:79 episode reward: total was 23.130000. running mean: -5.686562\n",
      "ep 2538: ep_len:501 episode reward: total was -14.300000. running mean: -5.772696\n",
      "ep 2538: ep_len:596 episode reward: total was -28.340000. running mean: -5.998369\n",
      "epsilon:0.087438 episode_count: 17773. steps_count: 7696628.000000\n",
      "Time elapsed:  22446.23686170578\n",
      "ep 2539: ep_len:573 episode reward: total was 39.830000. running mean: -5.540086\n",
      "ep 2539: ep_len:500 episode reward: total was 2.420000. running mean: -5.460485\n",
      "ep 2539: ep_len:615 episode reward: total was -7.730000. running mean: -5.483180\n",
      "ep 2539: ep_len:551 episode reward: total was 29.300000. running mean: -5.135348\n",
      "ep 2539: ep_len:102 episode reward: total was 7.780000. running mean: -5.006195\n",
      "ep 2539: ep_len:609 episode reward: total was 15.760000. running mean: -4.798533\n",
      "ep 2539: ep_len:254 episode reward: total was -54.250000. running mean: -5.293047\n",
      "epsilon:0.087393 episode_count: 17780. steps_count: 7699832.000000\n",
      "Time elapsed:  22454.436979293823\n",
      "ep 2540: ep_len:502 episode reward: total was -4.300000. running mean: -5.283117\n",
      "ep 2540: ep_len:587 episode reward: total was 0.670000. running mean: -5.223586\n",
      "ep 2540: ep_len:530 episode reward: total was -16.170000. running mean: -5.333050\n",
      "ep 2540: ep_len:515 episode reward: total was 10.990000. running mean: -5.169819\n",
      "ep 2540: ep_len:3 episode reward: total was 1.010000. running mean: -5.108021\n",
      "ep 2540: ep_len:690 episode reward: total was -12.720000. running mean: -5.184141\n",
      "ep 2540: ep_len:500 episode reward: total was -11.990000. running mean: -5.252200\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.087349 episode_count: 17787. steps_count: 7703159.000000\n",
      "Time elapsed:  22467.839784383774\n",
      "ep 2541: ep_len:500 episode reward: total was 30.410000. running mean: -4.895578\n",
      "ep 2541: ep_len:515 episode reward: total was -22.620000. running mean: -5.072822\n",
      "ep 2541: ep_len:571 episode reward: total was -34.260000. running mean: -5.364694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2541: ep_len:500 episode reward: total was 48.930000. running mean: -4.821747\n",
      "ep 2541: ep_len:91 episode reward: total was 12.230000. running mean: -4.651229\n",
      "ep 2541: ep_len:507 episode reward: total was -22.940000. running mean: -4.834117\n",
      "ep 2541: ep_len:250 episode reward: total was -86.190000. running mean: -5.647676\n",
      "epsilon:0.087305 episode_count: 17794. steps_count: 7706093.000000\n",
      "Time elapsed:  22475.65420269966\n",
      "ep 2542: ep_len:504 episode reward: total was 17.040000. running mean: -5.420799\n",
      "ep 2542: ep_len:500 episode reward: total was 53.410000. running mean: -4.832491\n",
      "ep 2542: ep_len:555 episode reward: total was -38.990000. running mean: -5.174066\n",
      "ep 2542: ep_len:500 episode reward: total was 39.740000. running mean: -4.724925\n",
      "ep 2542: ep_len:128 episode reward: total was 16.340000. running mean: -4.514276\n",
      "ep 2542: ep_len:500 episode reward: total was -22.020000. running mean: -4.689333\n",
      "ep 2542: ep_len:521 episode reward: total was -15.510000. running mean: -4.797540\n",
      "epsilon:0.087260 episode_count: 17801. steps_count: 7709301.000000\n",
      "Time elapsed:  22487.507979869843\n",
      "ep 2543: ep_len:500 episode reward: total was 37.330000. running mean: -4.376265\n",
      "ep 2543: ep_len:500 episode reward: total was 17.350000. running mean: -4.159002\n",
      "ep 2543: ep_len:500 episode reward: total was -58.790000. running mean: -4.705312\n",
      "ep 2543: ep_len:591 episode reward: total was 54.370000. running mean: -4.114559\n",
      "ep 2543: ep_len:3 episode reward: total was 0.000000. running mean: -4.073413\n",
      "ep 2543: ep_len:692 episode reward: total was -32.470000. running mean: -4.357379\n",
      "ep 2543: ep_len:308 episode reward: total was -26.700000. running mean: -4.580805\n",
      "epsilon:0.087216 episode_count: 17808. steps_count: 7712395.000000\n",
      "Time elapsed:  22502.704471111298\n",
      "ep 2544: ep_len:553 episode reward: total was -40.320000. running mean: -4.938197\n",
      "ep 2544: ep_len:500 episode reward: total was 30.040000. running mean: -4.588415\n",
      "ep 2544: ep_len:61 episode reward: total was -4.700000. running mean: -4.589531\n",
      "ep 2544: ep_len:527 episode reward: total was -17.760000. running mean: -4.721236\n",
      "ep 2544: ep_len:98 episode reward: total was 18.150000. running mean: -4.492524\n",
      "ep 2544: ep_len:501 episode reward: total was 6.750000. running mean: -4.380098\n",
      "ep 2544: ep_len:560 episode reward: total was 2.880000. running mean: -4.307497\n",
      "epsilon:0.087172 episode_count: 17815. steps_count: 7715195.000000\n",
      "Time elapsed:  22510.212040901184\n",
      "ep 2545: ep_len:500 episode reward: total was 27.990000. running mean: -3.984522\n",
      "ep 2545: ep_len:517 episode reward: total was -0.590000. running mean: -3.950577\n",
      "ep 2545: ep_len:60 episode reward: total was 2.670000. running mean: -3.884371\n",
      "ep 2545: ep_len:500 episode reward: total was 7.040000. running mean: -3.775128\n",
      "ep 2545: ep_len:3 episode reward: total was 1.010000. running mean: -3.727276\n",
      "ep 2545: ep_len:664 episode reward: total was -7.140000. running mean: -3.761404\n",
      "ep 2545: ep_len:573 episode reward: total was 17.970000. running mean: -3.544090\n",
      "epsilon:0.087127 episode_count: 17822. steps_count: 7718012.000000\n",
      "Time elapsed:  22517.846835374832\n",
      "ep 2546: ep_len:565 episode reward: total was 19.480000. running mean: -3.313849\n",
      "ep 2546: ep_len:500 episode reward: total was 4.890000. running mean: -3.231810\n",
      "ep 2546: ep_len:525 episode reward: total was 13.810000. running mean: -3.061392\n",
      "ep 2546: ep_len:500 episode reward: total was 12.990000. running mean: -2.900878\n",
      "ep 2546: ep_len:112 episode reward: total was 32.770000. running mean: -2.544169\n",
      "ep 2546: ep_len:563 episode reward: total was -13.320000. running mean: -2.651928\n",
      "ep 2546: ep_len:500 episode reward: total was -189.460000. running mean: -4.520008\n",
      "epsilon:0.087083 episode_count: 17829. steps_count: 7721277.000000\n",
      "Time elapsed:  22530.751272201538\n",
      "ep 2547: ep_len:227 episode reward: total was -2.820000. running mean: -4.503008\n",
      "ep 2547: ep_len:586 episode reward: total was 22.170000. running mean: -4.236278\n",
      "ep 2547: ep_len:73 episode reward: total was -3.780000. running mean: -4.231715\n",
      "ep 2547: ep_len:519 episode reward: total was 26.280000. running mean: -3.926598\n",
      "ep 2547: ep_len:3 episode reward: total was 1.010000. running mean: -3.877232\n",
      "ep 2547: ep_len:526 episode reward: total was -67.410000. running mean: -4.512560\n",
      "ep 2547: ep_len:593 episode reward: total was -27.930000. running mean: -4.746734\n",
      "epsilon:0.087039 episode_count: 17836. steps_count: 7723804.000000\n",
      "Time elapsed:  22537.508070230484\n",
      "ep 2548: ep_len:509 episode reward: total was 53.940000. running mean: -4.159867\n",
      "ep 2548: ep_len:587 episode reward: total was -31.620000. running mean: -4.434468\n",
      "ep 2548: ep_len:502 episode reward: total was 17.800000. running mean: -4.212124\n",
      "ep 2548: ep_len:500 episode reward: total was 31.370000. running mean: -3.856302\n",
      "ep 2548: ep_len:32 episode reward: total was 14.010000. running mean: -3.677639\n",
      "ep 2548: ep_len:594 episode reward: total was -19.870000. running mean: -3.839563\n",
      "ep 2548: ep_len:600 episode reward: total was -40.760000. running mean: -4.208767\n",
      "epsilon:0.086994 episode_count: 17843. steps_count: 7727128.000000\n",
      "Time elapsed:  22549.714675188065\n",
      "ep 2549: ep_len:543 episode reward: total was -1.440000. running mean: -4.181080\n",
      "ep 2549: ep_len:500 episode reward: total was 4.030000. running mean: -4.098969\n",
      "ep 2549: ep_len:621 episode reward: total was -128.550000. running mean: -5.343479\n",
      "ep 2549: ep_len:153 episode reward: total was 4.610000. running mean: -5.243944\n",
      "ep 2549: ep_len:67 episode reward: total was 8.250000. running mean: -5.109005\n",
      "ep 2549: ep_len:500 episode reward: total was -21.310000. running mean: -5.271015\n",
      "ep 2549: ep_len:541 episode reward: total was 22.650000. running mean: -4.991805\n",
      "epsilon:0.086950 episode_count: 17850. steps_count: 7730053.000000\n",
      "Time elapsed:  22560.979897260666\n",
      "ep 2550: ep_len:500 episode reward: total was 54.810000. running mean: -4.393787\n",
      "ep 2550: ep_len:651 episode reward: total was 70.470000. running mean: -3.645149\n",
      "ep 2550: ep_len:55 episode reward: total was 0.720000. running mean: -3.601497\n",
      "ep 2550: ep_len:500 episode reward: total was 20.660000. running mean: -3.358882\n",
      "ep 2550: ep_len:3 episode reward: total was 1.010000. running mean: -3.315194\n",
      "ep 2550: ep_len:665 episode reward: total was 13.500000. running mean: -3.147042\n",
      "ep 2550: ep_len:584 episode reward: total was -83.310000. running mean: -3.948671\n",
      "epsilon:0.086906 episode_count: 17857. steps_count: 7733011.000000\n",
      "Time elapsed:  22573.58657670021\n",
      "ep 2551: ep_len:252 episode reward: total was -0.700000. running mean: -3.916185\n",
      "ep 2551: ep_len:500 episode reward: total was -19.000000. running mean: -4.067023\n",
      "ep 2551: ep_len:500 episode reward: total was -42.280000. running mean: -4.449152\n",
      "ep 2551: ep_len:502 episode reward: total was 5.250000. running mean: -4.352161\n",
      "ep 2551: ep_len:3 episode reward: total was 1.010000. running mean: -4.298539\n",
      "ep 2551: ep_len:500 episode reward: total was 17.900000. running mean: -4.076554\n",
      "ep 2551: ep_len:585 episode reward: total was 20.320000. running mean: -3.832588\n",
      "epsilon:0.086861 episode_count: 17864. steps_count: 7735853.000000\n",
      "Time elapsed:  22582.235765457153\n",
      "ep 2552: ep_len:637 episode reward: total was -76.620000. running mean: -4.560463\n",
      "ep 2552: ep_len:501 episode reward: total was 21.610000. running mean: -4.298758\n",
      "ep 2552: ep_len:561 episode reward: total was -10.430000. running mean: -4.360070\n",
      "ep 2552: ep_len:513 episode reward: total was 9.660000. running mean: -4.219870\n",
      "ep 2552: ep_len:3 episode reward: total was 1.010000. running mean: -4.167571\n",
      "ep 2552: ep_len:632 episode reward: total was 3.740000. running mean: -4.088495\n",
      "ep 2552: ep_len:203 episode reward: total was -9.070000. running mean: -4.138310\n",
      "epsilon:0.086817 episode_count: 17871. steps_count: 7738903.000000\n",
      "Time elapsed:  22590.173077344894\n",
      "ep 2553: ep_len:500 episode reward: total was -50.760000. running mean: -4.604527\n",
      "ep 2553: ep_len:524 episode reward: total was 36.250000. running mean: -4.195982\n",
      "ep 2553: ep_len:552 episode reward: total was -46.060000. running mean: -4.614622\n",
      "ep 2553: ep_len:520 episode reward: total was 15.000000. running mean: -4.418476\n",
      "ep 2553: ep_len:35 episode reward: total was 15.510000. running mean: -4.219191\n",
      "ep 2553: ep_len:575 episode reward: total was -31.770000. running mean: -4.494699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2553: ep_len:546 episode reward: total was 11.080000. running mean: -4.338952\n",
      "epsilon:0.086773 episode_count: 17878. steps_count: 7742155.000000\n",
      "Time elapsed:  22599.74240231514\n",
      "ep 2554: ep_len:500 episode reward: total was -15.560000. running mean: -4.451163\n",
      "ep 2554: ep_len:602 episode reward: total was 35.160000. running mean: -4.055051\n",
      "ep 2554: ep_len:392 episode reward: total was 40.010000. running mean: -3.614401\n",
      "ep 2554: ep_len:577 episode reward: total was 11.800000. running mean: -3.460257\n",
      "ep 2554: ep_len:3 episode reward: total was 1.010000. running mean: -3.415554\n",
      "ep 2554: ep_len:500 episode reward: total was -14.100000. running mean: -3.522398\n",
      "ep 2554: ep_len:500 episode reward: total was -5.270000. running mean: -3.539874\n",
      "epsilon:0.086728 episode_count: 17885. steps_count: 7745229.000000\n",
      "Time elapsed:  22613.15671658516\n",
      "ep 2555: ep_len:603 episode reward: total was -51.320000. running mean: -4.017676\n",
      "ep 2555: ep_len:628 episode reward: total was 30.680000. running mean: -3.670699\n",
      "ep 2555: ep_len:356 episode reward: total was 22.260000. running mean: -3.411392\n",
      "ep 2555: ep_len:500 episode reward: total was -25.520000. running mean: -3.632478\n",
      "ep 2555: ep_len:3 episode reward: total was 1.010000. running mean: -3.586053\n",
      "ep 2555: ep_len:569 episode reward: total was -9.130000. running mean: -3.641493\n",
      "ep 2555: ep_len:327 episode reward: total was 0.890000. running mean: -3.596178\n",
      "epsilon:0.086684 episode_count: 17892. steps_count: 7748215.000000\n",
      "Time elapsed:  22621.950026988983\n",
      "ep 2556: ep_len:553 episode reward: total was -44.620000. running mean: -4.006416\n",
      "ep 2556: ep_len:296 episode reward: total was -12.920000. running mean: -4.095552\n",
      "ep 2556: ep_len:500 episode reward: total was -9.200000. running mean: -4.146596\n",
      "ep 2556: ep_len:500 episode reward: total was 16.530000. running mean: -3.939830\n",
      "ep 2556: ep_len:49 episode reward: total was 21.500000. running mean: -3.685432\n",
      "ep 2556: ep_len:253 episode reward: total was 39.500000. running mean: -3.253578\n",
      "ep 2556: ep_len:500 episode reward: total was 8.400000. running mean: -3.137042\n",
      "epsilon:0.086640 episode_count: 17899. steps_count: 7750866.000000\n",
      "Time elapsed:  22627.724075317383\n",
      "ep 2557: ep_len:264 episode reward: total was -38.130000. running mean: -3.486972\n",
      "ep 2557: ep_len:501 episode reward: total was -33.620000. running mean: -3.788302\n",
      "ep 2557: ep_len:607 episode reward: total was -45.230000. running mean: -4.202719\n",
      "ep 2557: ep_len:529 episode reward: total was -18.040000. running mean: -4.341092\n",
      "ep 2557: ep_len:3 episode reward: total was -0.490000. running mean: -4.302581\n",
      "ep 2557: ep_len:561 episode reward: total was -5.890000. running mean: -4.318455\n",
      "ep 2557: ep_len:623 episode reward: total was -42.970000. running mean: -4.704970\n",
      "epsilon:0.086595 episode_count: 17906. steps_count: 7753954.000000\n",
      "Time elapsed:  22636.86904644966\n",
      "ep 2558: ep_len:512 episode reward: total was -33.890000. running mean: -4.996821\n",
      "ep 2558: ep_len:516 episode reward: total was 5.670000. running mean: -4.890152\n",
      "ep 2558: ep_len:594 episode reward: total was -24.820000. running mean: -5.089451\n",
      "ep 2558: ep_len:387 episode reward: total was 21.070000. running mean: -4.827856\n",
      "ep 2558: ep_len:3 episode reward: total was 1.010000. running mean: -4.769478\n",
      "ep 2558: ep_len:615 episode reward: total was -13.250000. running mean: -4.854283\n",
      "ep 2558: ep_len:503 episode reward: total was -33.940000. running mean: -5.145140\n",
      "epsilon:0.086551 episode_count: 17913. steps_count: 7757084.000000\n",
      "Time elapsed:  22652.71907567978\n",
      "ep 2559: ep_len:134 episode reward: total was -2.060000. running mean: -5.114289\n",
      "ep 2559: ep_len:184 episode reward: total was -17.220000. running mean: -5.235346\n",
      "ep 2559: ep_len:537 episode reward: total was -23.380000. running mean: -5.416792\n",
      "ep 2559: ep_len:500 episode reward: total was -5.320000. running mean: -5.415825\n",
      "ep 2559: ep_len:3 episode reward: total was 1.010000. running mean: -5.351566\n",
      "ep 2559: ep_len:533 episode reward: total was 10.640000. running mean: -5.191651\n",
      "ep 2559: ep_len:503 episode reward: total was 8.740000. running mean: -5.052334\n",
      "epsilon:0.086507 episode_count: 17920. steps_count: 7759478.000000\n",
      "Time elapsed:  22660.153513669968\n",
      "ep 2560: ep_len:505 episode reward: total was 36.100000. running mean: -4.640811\n",
      "ep 2560: ep_len:553 episode reward: total was 46.410000. running mean: -4.130303\n",
      "ep 2560: ep_len:79 episode reward: total was 0.780000. running mean: -4.081200\n",
      "ep 2560: ep_len:421 episode reward: total was 25.670000. running mean: -3.783688\n",
      "ep 2560: ep_len:3 episode reward: total was 1.010000. running mean: -3.735751\n",
      "ep 2560: ep_len:312 episode reward: total was 9.220000. running mean: -3.606193\n",
      "ep 2560: ep_len:304 episode reward: total was -17.250000. running mean: -3.742631\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.086462 episode_count: 17927. steps_count: 7761655.000000\n",
      "Time elapsed:  22670.570774793625\n",
      "ep 2561: ep_len:500 episode reward: total was 54.780000. running mean: -3.157405\n",
      "ep 2561: ep_len:542 episode reward: total was 22.000000. running mean: -2.905831\n",
      "ep 2561: ep_len:500 episode reward: total was 5.150000. running mean: -2.825273\n",
      "ep 2561: ep_len:500 episode reward: total was 17.050000. running mean: -2.626520\n",
      "ep 2561: ep_len:3 episode reward: total was 1.010000. running mean: -2.590155\n",
      "ep 2561: ep_len:321 episode reward: total was -4.860000. running mean: -2.612853\n",
      "ep 2561: ep_len:620 episode reward: total was -0.280000. running mean: -2.589525\n",
      "epsilon:0.086418 episode_count: 17934. steps_count: 7764641.000000\n",
      "Time elapsed:  22680.680589437485\n",
      "ep 2562: ep_len:500 episode reward: total was 35.400000. running mean: -2.209629\n",
      "ep 2562: ep_len:507 episode reward: total was -29.310000. running mean: -2.480633\n",
      "ep 2562: ep_len:674 episode reward: total was -25.560000. running mean: -2.711427\n",
      "ep 2562: ep_len:500 episode reward: total was -6.130000. running mean: -2.745613\n",
      "ep 2562: ep_len:3 episode reward: total was -3.000000. running mean: -2.748156\n",
      "ep 2562: ep_len:592 episode reward: total was 19.820000. running mean: -2.522475\n",
      "ep 2562: ep_len:545 episode reward: total was -27.460000. running mean: -2.771850\n",
      "epsilon:0.086374 episode_count: 17941. steps_count: 7767962.000000\n",
      "Time elapsed:  22689.551986932755\n",
      "ep 2563: ep_len:503 episode reward: total was -50.080000. running mean: -3.244932\n",
      "ep 2563: ep_len:502 episode reward: total was -1.610000. running mean: -3.228582\n",
      "ep 2563: ep_len:541 episode reward: total was -40.800000. running mean: -3.604296\n",
      "ep 2563: ep_len:507 episode reward: total was 33.280000. running mean: -3.235453\n",
      "ep 2563: ep_len:3 episode reward: total was 1.010000. running mean: -3.192999\n",
      "ep 2563: ep_len:178 episode reward: total was 35.760000. running mean: -2.803469\n",
      "ep 2563: ep_len:580 episode reward: total was -31.450000. running mean: -3.089934\n",
      "epsilon:0.086329 episode_count: 17948. steps_count: 7770776.000000\n",
      "Time elapsed:  22702.954904317856\n",
      "ep 2564: ep_len:632 episode reward: total was 43.040000. running mean: -2.628635\n",
      "ep 2564: ep_len:696 episode reward: total was -106.720000. running mean: -3.669549\n",
      "ep 2564: ep_len:606 episode reward: total was -43.400000. running mean: -4.066853\n",
      "ep 2564: ep_len:518 episode reward: total was 16.540000. running mean: -3.860785\n",
      "ep 2564: ep_len:3 episode reward: total was 1.010000. running mean: -3.812077\n",
      "ep 2564: ep_len:586 episode reward: total was 4.260000. running mean: -3.731356\n",
      "ep 2564: ep_len:510 episode reward: total was -23.540000. running mean: -3.929442\n",
      "epsilon:0.086285 episode_count: 17955. steps_count: 7774327.000000\n",
      "Time elapsed:  22710.016764640808\n",
      "ep 2565: ep_len:576 episode reward: total was 34.000000. running mean: -3.550148\n",
      "ep 2565: ep_len:191 episode reward: total was -27.250000. running mean: -3.787146\n",
      "ep 2565: ep_len:397 episode reward: total was 39.540000. running mean: -3.353875\n",
      "ep 2565: ep_len:47 episode reward: total was -0.250000. running mean: -3.322836\n",
      "ep 2565: ep_len:54 episode reward: total was 18.000000. running mean: -3.109608\n",
      "ep 2565: ep_len:505 episode reward: total was 6.320000. running mean: -3.015312\n",
      "ep 2565: ep_len:528 episode reward: total was 22.060000. running mean: -2.764559\n",
      "epsilon:0.086241 episode_count: 17962. steps_count: 7776625.000000\n",
      "Time elapsed:  22714.899607896805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2566: ep_len:560 episode reward: total was -8.480000. running mean: -2.821713\n",
      "ep 2566: ep_len:521 episode reward: total was 14.950000. running mean: -2.643996\n",
      "ep 2566: ep_len:387 episode reward: total was -16.520000. running mean: -2.782756\n",
      "ep 2566: ep_len:55 episode reward: total was -1.670000. running mean: -2.771628\n",
      "ep 2566: ep_len:110 episode reward: total was 21.270000. running mean: -2.531212\n",
      "ep 2566: ep_len:533 episode reward: total was 8.110000. running mean: -2.424800\n",
      "ep 2566: ep_len:733 episode reward: total was -85.380000. running mean: -3.254352\n",
      "epsilon:0.086196 episode_count: 17969. steps_count: 7779524.000000\n",
      "Time elapsed:  22733.551753520966\n",
      "ep 2567: ep_len:622 episode reward: total was -78.740000. running mean: -4.009209\n",
      "ep 2567: ep_len:551 episode reward: total was -66.260000. running mean: -4.631716\n",
      "ep 2567: ep_len:407 episode reward: total was 20.080000. running mean: -4.384599\n",
      "ep 2567: ep_len:152 episode reward: total was 11.050000. running mean: -4.230253\n",
      "ep 2567: ep_len:50 episode reward: total was 17.500000. running mean: -4.012951\n",
      "ep 2567: ep_len:561 episode reward: total was 0.370000. running mean: -3.969121\n",
      "ep 2567: ep_len:530 episode reward: total was 4.850000. running mean: -3.880930\n",
      "epsilon:0.086152 episode_count: 17976. steps_count: 7782397.000000\n",
      "Time elapsed:  22744.016561746597\n",
      "ep 2568: ep_len:545 episode reward: total was 8.350000. running mean: -3.758621\n",
      "ep 2568: ep_len:509 episode reward: total was -24.810000. running mean: -3.969135\n",
      "ep 2568: ep_len:443 episode reward: total was 29.270000. running mean: -3.636743\n",
      "ep 2568: ep_len:514 episode reward: total was -20.140000. running mean: -3.801776\n",
      "ep 2568: ep_len:35 episode reward: total was 14.500000. running mean: -3.618758\n",
      "ep 2568: ep_len:531 episode reward: total was -51.400000. running mean: -4.096570\n",
      "ep 2568: ep_len:559 episode reward: total was -15.470000. running mean: -4.210305\n",
      "epsilon:0.086108 episode_count: 17983. steps_count: 7785533.000000\n",
      "Time elapsed:  22752.20380306244\n",
      "ep 2569: ep_len:551 episode reward: total was 24.270000. running mean: -3.925502\n",
      "ep 2569: ep_len:584 episode reward: total was 9.710000. running mean: -3.789147\n",
      "ep 2569: ep_len:664 episode reward: total was -26.230000. running mean: -4.013555\n",
      "ep 2569: ep_len:587 episode reward: total was 31.870000. running mean: -3.654720\n",
      "ep 2569: ep_len:55 episode reward: total was -46.490000. running mean: -4.083072\n",
      "ep 2569: ep_len:569 episode reward: total was -31.440000. running mean: -4.356642\n",
      "ep 2569: ep_len:560 episode reward: total was -63.450000. running mean: -4.947575\n",
      "epsilon:0.086063 episode_count: 17990. steps_count: 7789103.000000\n",
      "Time elapsed:  22768.282650470734\n",
      "ep 2570: ep_len:613 episode reward: total was -53.570000. running mean: -5.433800\n",
      "ep 2570: ep_len:284 episode reward: total was -75.830000. running mean: -6.137762\n",
      "ep 2570: ep_len:375 episode reward: total was 29.880000. running mean: -5.777584\n",
      "ep 2570: ep_len:500 episode reward: total was 43.650000. running mean: -5.283308\n",
      "ep 2570: ep_len:3 episode reward: total was 1.010000. running mean: -5.220375\n",
      "ep 2570: ep_len:500 episode reward: total was 14.650000. running mean: -5.021671\n",
      "ep 2570: ep_len:540 episode reward: total was 3.840000. running mean: -4.933055\n",
      "epsilon:0.086019 episode_count: 17997. steps_count: 7791918.000000\n",
      "Time elapsed:  22775.473383665085\n",
      "ep 2571: ep_len:616 episode reward: total was 19.340000. running mean: -4.690324\n",
      "ep 2571: ep_len:500 episode reward: total was -77.720000. running mean: -5.420621\n",
      "ep 2571: ep_len:539 episode reward: total was -5.060000. running mean: -5.417015\n",
      "ep 2571: ep_len:132 episode reward: total was 21.070000. running mean: -5.152144\n",
      "ep 2571: ep_len:3 episode reward: total was 1.010000. running mean: -5.090523\n",
      "ep 2571: ep_len:503 episode reward: total was -37.000000. running mean: -5.409618\n",
      "ep 2571: ep_len:500 episode reward: total was -19.620000. running mean: -5.551722\n",
      "epsilon:0.085975 episode_count: 18004. steps_count: 7794711.000000\n",
      "Time elapsed:  22784.15023946762\n",
      "ep 2572: ep_len:517 episode reward: total was -65.270000. running mean: -6.148904\n",
      "ep 2572: ep_len:551 episode reward: total was -15.910000. running mean: -6.246515\n",
      "ep 2572: ep_len:625 episode reward: total was -12.890000. running mean: -6.312950\n",
      "ep 2572: ep_len:56 episode reward: total was 0.330000. running mean: -6.246521\n",
      "ep 2572: ep_len:3 episode reward: total was 1.010000. running mean: -6.173955\n",
      "ep 2572: ep_len:536 episode reward: total was -14.850000. running mean: -6.260716\n",
      "ep 2572: ep_len:511 episode reward: total was -16.750000. running mean: -6.365609\n",
      "epsilon:0.085930 episode_count: 18011. steps_count: 7797510.000000\n",
      "Time elapsed:  22792.702080965042\n",
      "ep 2573: ep_len:545 episode reward: total was -16.630000. running mean: -6.468253\n",
      "ep 2573: ep_len:571 episode reward: total was -5.640000. running mean: -6.459970\n",
      "ep 2573: ep_len:67 episode reward: total was -4.760000. running mean: -6.442970\n",
      "ep 2573: ep_len:532 episode reward: total was 6.210000. running mean: -6.316441\n",
      "ep 2573: ep_len:3 episode reward: total was 1.010000. running mean: -6.243176\n",
      "ep 2573: ep_len:548 episode reward: total was -2.810000. running mean: -6.208845\n",
      "ep 2573: ep_len:295 episode reward: total was -59.560000. running mean: -6.742356\n",
      "epsilon:0.085886 episode_count: 18018. steps_count: 7800071.000000\n",
      "Time elapsed:  22812.355758666992\n",
      "ep 2574: ep_len:544 episode reward: total was -21.560000. running mean: -6.890533\n",
      "ep 2574: ep_len:542 episode reward: total was -45.440000. running mean: -7.276027\n",
      "ep 2574: ep_len:500 episode reward: total was -10.340000. running mean: -7.306667\n",
      "ep 2574: ep_len:525 episode reward: total was -35.400000. running mean: -7.587600\n",
      "ep 2574: ep_len:67 episode reward: total was 10.730000. running mean: -7.404424\n",
      "ep 2574: ep_len:596 episode reward: total was -24.900000. running mean: -7.579380\n",
      "ep 2574: ep_len:513 episode reward: total was -16.240000. running mean: -7.665986\n",
      "epsilon:0.085842 episode_count: 18025. steps_count: 7803358.000000\n",
      "Time elapsed:  22822.10442495346\n",
      "ep 2575: ep_len:554 episode reward: total was 36.200000. running mean: -7.227326\n",
      "ep 2575: ep_len:501 episode reward: total was 40.020000. running mean: -6.754853\n",
      "ep 2575: ep_len:72 episode reward: total was -7.250000. running mean: -6.759805\n",
      "ep 2575: ep_len:500 episode reward: total was 16.830000. running mean: -6.523907\n",
      "ep 2575: ep_len:127 episode reward: total was 8.830000. running mean: -6.370367\n",
      "ep 2575: ep_len:607 episode reward: total was -4.220000. running mean: -6.348864\n",
      "ep 2575: ep_len:561 episode reward: total was -37.950000. running mean: -6.664875\n",
      "epsilon:0.085797 episode_count: 18032. steps_count: 7806280.000000\n",
      "Time elapsed:  22829.885248184204\n",
      "ep 2576: ep_len:583 episode reward: total was -70.670000. running mean: -7.304926\n",
      "ep 2576: ep_len:279 episode reward: total was -20.410000. running mean: -7.435977\n",
      "ep 2576: ep_len:460 episode reward: total was 42.760000. running mean: -6.934017\n",
      "ep 2576: ep_len:502 episode reward: total was 39.110000. running mean: -6.473577\n",
      "ep 2576: ep_len:3 episode reward: total was -0.490000. running mean: -6.413741\n",
      "ep 2576: ep_len:562 episode reward: total was 1.240000. running mean: -6.337204\n",
      "ep 2576: ep_len:524 episode reward: total was -19.650000. running mean: -6.470332\n",
      "epsilon:0.085753 episode_count: 18039. steps_count: 7809193.000000\n",
      "Time elapsed:  22837.82156085968\n",
      "ep 2577: ep_len:507 episode reward: total was -8.560000. running mean: -6.491229\n",
      "ep 2577: ep_len:668 episode reward: total was 59.130000. running mean: -5.835016\n",
      "ep 2577: ep_len:536 episode reward: total was -6.040000. running mean: -5.837066\n",
      "ep 2577: ep_len:610 episode reward: total was 17.320000. running mean: -5.605496\n",
      "ep 2577: ep_len:3 episode reward: total was 1.010000. running mean: -5.539341\n",
      "ep 2577: ep_len:602 episode reward: total was 36.880000. running mean: -5.115147\n",
      "ep 2577: ep_len:296 episode reward: total was -32.300000. running mean: -5.386996\n",
      "epsilon:0.085709 episode_count: 18046. steps_count: 7812415.000000\n",
      "Time elapsed:  22846.434213876724\n",
      "ep 2578: ep_len:657 episode reward: total was -29.440000. running mean: -5.627526\n",
      "ep 2578: ep_len:500 episode reward: total was -10.210000. running mean: -5.673350\n",
      "ep 2578: ep_len:500 episode reward: total was -4.220000. running mean: -5.658817\n",
      "ep 2578: ep_len:56 episode reward: total was 1.340000. running mean: -5.588829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2578: ep_len:3 episode reward: total was 1.010000. running mean: -5.522841\n",
      "ep 2578: ep_len:524 episode reward: total was -25.270000. running mean: -5.720312\n",
      "ep 2578: ep_len:553 episode reward: total was -21.220000. running mean: -5.875309\n",
      "epsilon:0.085664 episode_count: 18053. steps_count: 7815208.000000\n",
      "Time elapsed:  22852.49435544014\n",
      "ep 2579: ep_len:545 episode reward: total was -14.480000. running mean: -5.961356\n",
      "ep 2579: ep_len:365 episode reward: total was -9.670000. running mean: -5.998442\n",
      "ep 2579: ep_len:562 episode reward: total was -30.290000. running mean: -6.241358\n",
      "ep 2579: ep_len:500 episode reward: total was 9.090000. running mean: -6.088044\n",
      "ep 2579: ep_len:3 episode reward: total was 1.010000. running mean: -6.017064\n",
      "ep 2579: ep_len:578 episode reward: total was -31.140000. running mean: -6.268293\n",
      "ep 2579: ep_len:517 episode reward: total was -13.990000. running mean: -6.345510\n",
      "epsilon:0.085620 episode_count: 18060. steps_count: 7818278.000000\n",
      "Time elapsed:  22859.43937420845\n",
      "ep 2580: ep_len:215 episode reward: total was 13.650000. running mean: -6.145555\n",
      "ep 2580: ep_len:511 episode reward: total was 6.180000. running mean: -6.022300\n",
      "ep 2580: ep_len:622 episode reward: total was -11.970000. running mean: -6.081777\n",
      "ep 2580: ep_len:547 episode reward: total was -35.620000. running mean: -6.377159\n",
      "ep 2580: ep_len:125 episode reward: total was 21.850000. running mean: -6.094887\n",
      "ep 2580: ep_len:579 episode reward: total was -5.380000. running mean: -6.087738\n",
      "ep 2580: ep_len:500 episode reward: total was -0.820000. running mean: -6.035061\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.085576 episode_count: 18067. steps_count: 7821377.000000\n",
      "Time elapsed:  22870.20450425148\n",
      "ep 2581: ep_len:535 episode reward: total was -4.390000. running mean: -6.018610\n",
      "ep 2581: ep_len:610 episode reward: total was -183.680000. running mean: -7.795224\n",
      "ep 2581: ep_len:595 episode reward: total was 1.530000. running mean: -7.701972\n",
      "ep 2581: ep_len:500 episode reward: total was -25.420000. running mean: -7.879152\n",
      "ep 2581: ep_len:69 episode reward: total was 18.620000. running mean: -7.614161\n",
      "ep 2581: ep_len:662 episode reward: total was -209.320000. running mean: -9.631219\n",
      "ep 2581: ep_len:535 episode reward: total was -28.080000. running mean: -9.815707\n",
      "epsilon:0.085531 episode_count: 18074. steps_count: 7824883.000000\n",
      "Time elapsed:  22879.34824180603\n",
      "ep 2582: ep_len:636 episode reward: total was -44.870000. running mean: -10.166250\n",
      "ep 2582: ep_len:606 episode reward: total was -1.210000. running mean: -10.076687\n",
      "ep 2582: ep_len:500 episode reward: total was -16.610000. running mean: -10.142021\n",
      "ep 2582: ep_len:501 episode reward: total was 53.590000. running mean: -9.504700\n",
      "ep 2582: ep_len:98 episode reward: total was 23.260000. running mean: -9.177053\n",
      "ep 2582: ep_len:301 episode reward: total was 19.000000. running mean: -8.895283\n",
      "ep 2582: ep_len:561 episode reward: total was -4.730000. running mean: -8.853630\n",
      "epsilon:0.085487 episode_count: 18081. steps_count: 7828086.000000\n",
      "Time elapsed:  22888.65252161026\n",
      "ep 2583: ep_len:134 episode reward: total was 3.940000. running mean: -8.725694\n",
      "ep 2583: ep_len:500 episode reward: total was 7.440000. running mean: -8.564037\n",
      "ep 2583: ep_len:565 episode reward: total was -10.050000. running mean: -8.578896\n",
      "ep 2583: ep_len:500 episode reward: total was 12.090000. running mean: -8.372207\n",
      "ep 2583: ep_len:99 episode reward: total was 19.750000. running mean: -8.090985\n",
      "ep 2583: ep_len:171 episode reward: total was 25.500000. running mean: -7.755076\n",
      "ep 2583: ep_len:517 episode reward: total was -20.230000. running mean: -7.879825\n",
      "epsilon:0.085443 episode_count: 18088. steps_count: 7830572.000000\n",
      "Time elapsed:  22896.49377989769\n",
      "ep 2584: ep_len:548 episode reward: total was 23.250000. running mean: -7.568527\n",
      "ep 2584: ep_len:592 episode reward: total was 1.880000. running mean: -7.474041\n",
      "ep 2584: ep_len:500 episode reward: total was 12.930000. running mean: -7.270001\n",
      "ep 2584: ep_len:615 episode reward: total was 7.770000. running mean: -7.119601\n",
      "ep 2584: ep_len:86 episode reward: total was 17.260000. running mean: -6.875805\n",
      "ep 2584: ep_len:501 episode reward: total was 2.280000. running mean: -6.784247\n",
      "ep 2584: ep_len:598 episode reward: total was -2.540000. running mean: -6.741804\n",
      "epsilon:0.085398 episode_count: 18095. steps_count: 7834012.000000\n",
      "Time elapsed:  22906.094423532486\n",
      "ep 2585: ep_len:241 episode reward: total was -8.150000. running mean: -6.755886\n",
      "ep 2585: ep_len:500 episode reward: total was -2.730000. running mean: -6.715627\n",
      "ep 2585: ep_len:582 episode reward: total was -27.940000. running mean: -6.927871\n",
      "ep 2585: ep_len:361 episode reward: total was -115.700000. running mean: -8.015592\n",
      "ep 2585: ep_len:3 episode reward: total was 1.010000. running mean: -7.925337\n",
      "ep 2585: ep_len:568 episode reward: total was -30.470000. running mean: -8.150783\n",
      "ep 2585: ep_len:525 episode reward: total was -8.150000. running mean: -8.150775\n",
      "epsilon:0.085354 episode_count: 18102. steps_count: 7836792.000000\n",
      "Time elapsed:  22913.9414870739\n",
      "ep 2586: ep_len:623 episode reward: total was -37.410000. running mean: -8.443368\n",
      "ep 2586: ep_len:524 episode reward: total was -13.220000. running mean: -8.491134\n",
      "ep 2586: ep_len:570 episode reward: total was -13.620000. running mean: -8.542423\n",
      "ep 2586: ep_len:500 episode reward: total was 55.710000. running mean: -7.899898\n",
      "ep 2586: ep_len:82 episode reward: total was 15.200000. running mean: -7.668899\n",
      "ep 2586: ep_len:589 episode reward: total was -34.400000. running mean: -7.936210\n",
      "ep 2586: ep_len:597 episode reward: total was 9.640000. running mean: -7.760448\n",
      "epsilon:0.085310 episode_count: 18109. steps_count: 7840277.000000\n",
      "Time elapsed:  22930.39555811882\n",
      "ep 2587: ep_len:500 episode reward: total was 27.530000. running mean: -7.407544\n",
      "ep 2587: ep_len:579 episode reward: total was -8.600000. running mean: -7.419468\n",
      "ep 2587: ep_len:588 episode reward: total was -29.400000. running mean: -7.639274\n",
      "ep 2587: ep_len:500 episode reward: total was -9.740000. running mean: -7.660281\n",
      "ep 2587: ep_len:3 episode reward: total was 1.010000. running mean: -7.573578\n",
      "ep 2587: ep_len:533 episode reward: total was -17.150000. running mean: -7.669342\n",
      "ep 2587: ep_len:627 episode reward: total was -9.130000. running mean: -7.683949\n",
      "epsilon:0.085265 episode_count: 18116. steps_count: 7843607.000000\n",
      "Time elapsed:  22940.54117488861\n",
      "ep 2588: ep_len:203 episode reward: total was 3.090000. running mean: -7.576209\n",
      "ep 2588: ep_len:500 episode reward: total was 47.290000. running mean: -7.027547\n",
      "ep 2588: ep_len:607 episode reward: total was -44.480000. running mean: -7.402072\n",
      "ep 2588: ep_len:625 episode reward: total was 13.370000. running mean: -7.194351\n",
      "ep 2588: ep_len:3 episode reward: total was 1.010000. running mean: -7.112308\n",
      "ep 2588: ep_len:500 episode reward: total was 22.300000. running mean: -6.818185\n",
      "ep 2588: ep_len:500 episode reward: total was 7.700000. running mean: -6.673003\n",
      "epsilon:0.085221 episode_count: 18123. steps_count: 7846545.000000\n",
      "Time elapsed:  22956.076879024506\n",
      "ep 2589: ep_len:134 episode reward: total was 8.500000. running mean: -6.521273\n",
      "ep 2589: ep_len:626 episode reward: total was 26.850000. running mean: -6.187560\n",
      "ep 2589: ep_len:610 episode reward: total was -73.590000. running mean: -6.861584\n",
      "ep 2589: ep_len:132 episode reward: total was 11.580000. running mean: -6.677168\n",
      "ep 2589: ep_len:3 episode reward: total was 1.010000. running mean: -6.600297\n",
      "ep 2589: ep_len:227 episode reward: total was 33.270000. running mean: -6.201594\n",
      "ep 2589: ep_len:529 episode reward: total was -45.280000. running mean: -6.592378\n",
      "epsilon:0.085177 episode_count: 18130. steps_count: 7848806.000000\n",
      "Time elapsed:  22963.382774829865\n",
      "ep 2590: ep_len:534 episode reward: total was 27.580000. running mean: -6.250654\n",
      "ep 2590: ep_len:500 episode reward: total was 23.530000. running mean: -5.952848\n",
      "ep 2590: ep_len:500 episode reward: total was 4.990000. running mean: -5.843419\n",
      "ep 2590: ep_len:553 episode reward: total was 54.050000. running mean: -5.244485\n",
      "ep 2590: ep_len:3 episode reward: total was 1.010000. running mean: -5.181940\n",
      "ep 2590: ep_len:500 episode reward: total was -18.850000. running mean: -5.318621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2590: ep_len:621 episode reward: total was -130.020000. running mean: -6.565634\n",
      "epsilon:0.085132 episode_count: 18137. steps_count: 7852017.000000\n",
      "Time elapsed:  22972.83217072487\n",
      "ep 2591: ep_len:531 episode reward: total was 33.210000. running mean: -6.167878\n",
      "ep 2591: ep_len:528 episode reward: total was 1.260000. running mean: -6.093599\n",
      "ep 2591: ep_len:530 episode reward: total was -36.890000. running mean: -6.401563\n",
      "ep 2591: ep_len:56 episode reward: total was -10.150000. running mean: -6.439048\n",
      "ep 2591: ep_len:94 episode reward: total was 30.630000. running mean: -6.068357\n",
      "ep 2591: ep_len:522 episode reward: total was -5.610000. running mean: -6.063774\n",
      "ep 2591: ep_len:585 episode reward: total was -15.430000. running mean: -6.157436\n",
      "epsilon:0.085088 episode_count: 18144. steps_count: 7854863.000000\n",
      "Time elapsed:  22987.11318588257\n",
      "ep 2592: ep_len:661 episode reward: total was -77.640000. running mean: -6.872262\n",
      "ep 2592: ep_len:352 episode reward: total was -154.680000. running mean: -8.350339\n",
      "ep 2592: ep_len:561 episode reward: total was -56.070000. running mean: -8.827536\n",
      "ep 2592: ep_len:508 episode reward: total was -21.410000. running mean: -8.953360\n",
      "ep 2592: ep_len:3 episode reward: total was -0.490000. running mean: -8.868727\n",
      "ep 2592: ep_len:584 episode reward: total was 10.370000. running mean: -8.676339\n",
      "ep 2592: ep_len:500 episode reward: total was -8.870000. running mean: -8.678276\n",
      "epsilon:0.085044 episode_count: 18151. steps_count: 7858032.000000\n",
      "Time elapsed:  22995.493685245514\n",
      "ep 2593: ep_len:621 episode reward: total was 0.200000. running mean: -8.589493\n",
      "ep 2593: ep_len:500 episode reward: total was -162.540000. running mean: -10.128998\n",
      "ep 2593: ep_len:583 episode reward: total was -7.490000. running mean: -10.102608\n",
      "ep 2593: ep_len:530 episode reward: total was 27.790000. running mean: -9.723682\n",
      "ep 2593: ep_len:3 episode reward: total was 1.010000. running mean: -9.616345\n",
      "ep 2593: ep_len:516 episode reward: total was -58.790000. running mean: -10.108082\n",
      "ep 2593: ep_len:324 episode reward: total was 9.070000. running mean: -9.916301\n",
      "epsilon:0.084999 episode_count: 18158. steps_count: 7861109.000000\n",
      "Time elapsed:  23004.828005075455\n",
      "ep 2594: ep_len:640 episode reward: total was -13.350000. running mean: -9.950638\n",
      "ep 2594: ep_len:500 episode reward: total was -1.690000. running mean: -9.868032\n",
      "ep 2594: ep_len:443 episode reward: total was 19.050000. running mean: -9.578851\n",
      "ep 2594: ep_len:578 episode reward: total was 27.000000. running mean: -9.213063\n",
      "ep 2594: ep_len:112 episode reward: total was 32.250000. running mean: -8.798432\n",
      "ep 2594: ep_len:548 episode reward: total was -87.490000. running mean: -9.585348\n",
      "ep 2594: ep_len:581 episode reward: total was 0.540000. running mean: -9.484094\n",
      "epsilon:0.084955 episode_count: 18165. steps_count: 7864511.000000\n",
      "Time elapsed:  23019.362167596817\n",
      "ep 2595: ep_len:229 episode reward: total was 6.200000. running mean: -9.327253\n",
      "ep 2595: ep_len:169 episode reward: total was -2.770000. running mean: -9.261681\n",
      "ep 2595: ep_len:633 episode reward: total was -30.440000. running mean: -9.473464\n",
      "ep 2595: ep_len:500 episode reward: total was 10.870000. running mean: -9.270029\n",
      "ep 2595: ep_len:3 episode reward: total was 1.010000. running mean: -9.167229\n",
      "ep 2595: ep_len:677 episode reward: total was -8.880000. running mean: -9.164357\n",
      "ep 2595: ep_len:500 episode reward: total was -29.450000. running mean: -9.367213\n",
      "epsilon:0.084911 episode_count: 18172. steps_count: 7867222.000000\n",
      "Time elapsed:  23027.010827302933\n",
      "ep 2596: ep_len:567 episode reward: total was -11.810000. running mean: -9.391641\n",
      "ep 2596: ep_len:500 episode reward: total was -94.070000. running mean: -10.238425\n",
      "ep 2596: ep_len:625 episode reward: total was -15.650000. running mean: -10.292541\n",
      "ep 2596: ep_len:500 episode reward: total was -29.820000. running mean: -10.487815\n",
      "ep 2596: ep_len:3 episode reward: total was 1.010000. running mean: -10.372837\n",
      "ep 2596: ep_len:566 episode reward: total was -6.680000. running mean: -10.335909\n",
      "ep 2596: ep_len:595 episode reward: total was 2.540000. running mean: -10.207150\n",
      "epsilon:0.084866 episode_count: 18179. steps_count: 7870578.000000\n",
      "Time elapsed:  23035.929478168488\n",
      "ep 2597: ep_len:573 episode reward: total was 63.640000. running mean: -9.468678\n",
      "ep 2597: ep_len:639 episode reward: total was -199.550000. running mean: -11.369491\n",
      "ep 2597: ep_len:519 episode reward: total was -61.310000. running mean: -11.868896\n",
      "ep 2597: ep_len:521 episode reward: total was -2.060000. running mean: -11.770807\n",
      "ep 2597: ep_len:104 episode reward: total was 25.220000. running mean: -11.400899\n",
      "ep 2597: ep_len:513 episode reward: total was -15.080000. running mean: -11.437690\n",
      "ep 2597: ep_len:500 episode reward: total was -12.650000. running mean: -11.449813\n",
      "epsilon:0.084822 episode_count: 18186. steps_count: 7873947.000000\n",
      "Time elapsed:  23046.002940893173\n",
      "ep 2598: ep_len:543 episode reward: total was -148.140000. running mean: -12.816715\n",
      "ep 2598: ep_len:538 episode reward: total was 70.360000. running mean: -11.984948\n",
      "ep 2598: ep_len:528 episode reward: total was -34.970000. running mean: -12.214799\n",
      "ep 2598: ep_len:500 episode reward: total was 49.930000. running mean: -11.593351\n",
      "ep 2598: ep_len:3 episode reward: total was 1.010000. running mean: -11.467317\n",
      "ep 2598: ep_len:596 episode reward: total was -12.990000. running mean: -11.482544\n",
      "ep 2598: ep_len:589 episode reward: total was 0.260000. running mean: -11.365119\n",
      "epsilon:0.084778 episode_count: 18193. steps_count: 7877244.000000\n",
      "Time elapsed:  23061.34507751465\n",
      "ep 2599: ep_len:500 episode reward: total was 18.130000. running mean: -11.070167\n",
      "ep 2599: ep_len:569 episode reward: total was -46.850000. running mean: -11.427966\n",
      "ep 2599: ep_len:619 episode reward: total was -18.340000. running mean: -11.497086\n",
      "ep 2599: ep_len:526 episode reward: total was -63.240000. running mean: -12.014515\n",
      "ep 2599: ep_len:3 episode reward: total was -0.490000. running mean: -11.899270\n",
      "ep 2599: ep_len:573 episode reward: total was -15.610000. running mean: -11.936377\n",
      "ep 2599: ep_len:202 episode reward: total was -27.050000. running mean: -12.087514\n",
      "epsilon:0.084733 episode_count: 18200. steps_count: 7880236.000000\n",
      "Time elapsed:  23070.4905230999\n",
      "ep 2600: ep_len:593 episode reward: total was 5.200000. running mean: -11.914638\n",
      "ep 2600: ep_len:500 episode reward: total was -3.840000. running mean: -11.833892\n",
      "ep 2600: ep_len:613 episode reward: total was -40.590000. running mean: -12.121453\n",
      "ep 2600: ep_len:500 episode reward: total was -109.850000. running mean: -13.098739\n",
      "ep 2600: ep_len:3 episode reward: total was -0.490000. running mean: -12.972651\n",
      "ep 2600: ep_len:500 episode reward: total was 13.050000. running mean: -12.712425\n",
      "ep 2600: ep_len:581 episode reward: total was 12.870000. running mean: -12.456600\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.084689 episode_count: 18207. steps_count: 7883526.000000\n",
      "Time elapsed:  23084.163024902344\n",
      "ep 2601: ep_len:593 episode reward: total was -3.830000. running mean: -12.370334\n",
      "ep 2601: ep_len:536 episode reward: total was -31.330000. running mean: -12.559931\n",
      "ep 2601: ep_len:421 episode reward: total was 37.010000. running mean: -12.064232\n",
      "ep 2601: ep_len:500 episode reward: total was 11.420000. running mean: -11.829389\n",
      "ep 2601: ep_len:2 episode reward: total was -0.500000. running mean: -11.716096\n",
      "ep 2601: ep_len:246 episode reward: total was 15.920000. running mean: -11.439735\n",
      "ep 2601: ep_len:339 episode reward: total was -9.480000. running mean: -11.420137\n",
      "epsilon:0.084645 episode_count: 18214. steps_count: 7886163.000000\n",
      "Time elapsed:  23097.087513685226\n",
      "ep 2602: ep_len:561 episode reward: total was 20.170000. running mean: -11.104236\n",
      "ep 2602: ep_len:500 episode reward: total was 2.500000. running mean: -10.968194\n",
      "ep 2602: ep_len:385 episode reward: total was 21.300000. running mean: -10.645512\n",
      "ep 2602: ep_len:500 episode reward: total was 33.180000. running mean: -10.207256\n",
      "ep 2602: ep_len:3 episode reward: total was 1.010000. running mean: -10.095084\n",
      "ep 2602: ep_len:584 episode reward: total was -68.620000. running mean: -10.680333\n",
      "ep 2602: ep_len:322 episode reward: total was -6.960000. running mean: -10.643130\n",
      "epsilon:0.084600 episode_count: 18221. steps_count: 7889018.000000\n",
      "Time elapsed:  23108.88255572319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2603: ep_len:610 episode reward: total was 26.440000. running mean: -10.272298\n",
      "ep 2603: ep_len:580 episode reward: total was -37.530000. running mean: -10.544875\n",
      "ep 2603: ep_len:500 episode reward: total was -23.650000. running mean: -10.675927\n",
      "ep 2603: ep_len:98 episode reward: total was 1.930000. running mean: -10.549867\n",
      "ep 2603: ep_len:88 episode reward: total was 23.250000. running mean: -10.211869\n",
      "ep 2603: ep_len:501 episode reward: total was -8.770000. running mean: -10.197450\n",
      "ep 2603: ep_len:523 episode reward: total was -62.840000. running mean: -10.723876\n",
      "epsilon:0.084556 episode_count: 18228. steps_count: 7891918.000000\n",
      "Time elapsed:  23117.716921567917\n",
      "ep 2604: ep_len:517 episode reward: total was -23.250000. running mean: -10.849137\n",
      "ep 2604: ep_len:554 episode reward: total was -13.890000. running mean: -10.879545\n",
      "ep 2604: ep_len:500 episode reward: total was 12.840000. running mean: -10.642350\n",
      "ep 2604: ep_len:543 episode reward: total was 24.490000. running mean: -10.291026\n",
      "ep 2604: ep_len:128 episode reward: total was 15.340000. running mean: -10.034716\n",
      "ep 2604: ep_len:500 episode reward: total was -15.930000. running mean: -10.093669\n",
      "ep 2604: ep_len:500 episode reward: total was -47.100000. running mean: -10.463732\n",
      "epsilon:0.084512 episode_count: 18235. steps_count: 7895160.000000\n",
      "Time elapsed:  23127.642895698547\n",
      "ep 2605: ep_len:562 episode reward: total was -136.060000. running mean: -11.719695\n",
      "ep 2605: ep_len:500 episode reward: total was 45.120000. running mean: -11.151298\n",
      "ep 2605: ep_len:562 episode reward: total was -53.920000. running mean: -11.578985\n",
      "ep 2605: ep_len:500 episode reward: total was 8.010000. running mean: -11.383095\n",
      "ep 2605: ep_len:77 episode reward: total was 15.270000. running mean: -11.116564\n",
      "ep 2605: ep_len:506 episode reward: total was -7.280000. running mean: -11.078199\n",
      "ep 2605: ep_len:555 episode reward: total was -14.650000. running mean: -11.113917\n",
      "epsilon:0.084467 episode_count: 18242. steps_count: 7898422.000000\n",
      "Time elapsed:  23138.32569360733\n",
      "ep 2606: ep_len:246 episode reward: total was 13.660000. running mean: -10.866178\n",
      "ep 2606: ep_len:594 episode reward: total was -0.100000. running mean: -10.758516\n",
      "ep 2606: ep_len:636 episode reward: total was -45.500000. running mean: -11.105931\n",
      "ep 2606: ep_len:566 episode reward: total was 32.270000. running mean: -10.672171\n",
      "ep 2606: ep_len:3 episode reward: total was 1.010000. running mean: -10.555350\n",
      "ep 2606: ep_len:507 episode reward: total was -39.520000. running mean: -10.844996\n",
      "ep 2606: ep_len:546 episode reward: total was -0.000000. running mean: -10.736546\n",
      "epsilon:0.084423 episode_count: 18249. steps_count: 7901520.000000\n",
      "Time elapsed:  23148.201990365982\n",
      "ep 2607: ep_len:502 episode reward: total was 13.590000. running mean: -10.493281\n",
      "ep 2607: ep_len:500 episode reward: total was -7.090000. running mean: -10.459248\n",
      "ep 2607: ep_len:624 episode reward: total was -23.680000. running mean: -10.591455\n",
      "ep 2607: ep_len:500 episode reward: total was -42.250000. running mean: -10.908041\n",
      "ep 2607: ep_len:3 episode reward: total was -0.490000. running mean: -10.803860\n",
      "ep 2607: ep_len:295 episode reward: total was -0.560000. running mean: -10.701422\n",
      "ep 2607: ep_len:501 episode reward: total was -31.530000. running mean: -10.909708\n",
      "epsilon:0.084379 episode_count: 18256. steps_count: 7904445.000000\n",
      "Time elapsed:  23157.0843436718\n",
      "ep 2608: ep_len:586 episode reward: total was 62.210000. running mean: -10.178511\n",
      "ep 2608: ep_len:604 episode reward: total was 82.970000. running mean: -9.247025\n",
      "ep 2608: ep_len:613 episode reward: total was -35.290000. running mean: -9.507455\n",
      "ep 2608: ep_len:500 episode reward: total was 31.990000. running mean: -9.092481\n",
      "ep 2608: ep_len:110 episode reward: total was 18.850000. running mean: -8.813056\n",
      "ep 2608: ep_len:567 episode reward: total was -1.840000. running mean: -8.743325\n",
      "ep 2608: ep_len:513 episode reward: total was -23.800000. running mean: -8.893892\n",
      "epsilon:0.084334 episode_count: 18263. steps_count: 7907938.000000\n",
      "Time elapsed:  23185.443923473358\n",
      "ep 2609: ep_len:134 episode reward: total was -17.120000. running mean: -8.976153\n",
      "ep 2609: ep_len:500 episode reward: total was -0.310000. running mean: -8.889492\n",
      "ep 2609: ep_len:79 episode reward: total was 2.740000. running mean: -8.773197\n",
      "ep 2609: ep_len:500 episode reward: total was -34.100000. running mean: -9.026465\n",
      "ep 2609: ep_len:50 episode reward: total was 20.010000. running mean: -8.736100\n",
      "ep 2609: ep_len:500 episode reward: total was 16.590000. running mean: -8.482839\n",
      "ep 2609: ep_len:626 episode reward: total was -39.190000. running mean: -8.789911\n",
      "epsilon:0.084290 episode_count: 18270. steps_count: 7910327.000000\n",
      "Time elapsed:  23198.3170940876\n",
      "ep 2610: ep_len:500 episode reward: total was 54.770000. running mean: -8.154312\n",
      "ep 2610: ep_len:529 episode reward: total was -20.330000. running mean: -8.276068\n",
      "ep 2610: ep_len:560 episode reward: total was -40.060000. running mean: -8.593908\n",
      "ep 2610: ep_len:567 episode reward: total was 34.340000. running mean: -8.164569\n",
      "ep 2610: ep_len:3 episode reward: total was 1.010000. running mean: -8.072823\n",
      "ep 2610: ep_len:230 episode reward: total was 34.160000. running mean: -7.650495\n",
      "ep 2610: ep_len:579 episode reward: total was -12.590000. running mean: -7.699890\n",
      "epsilon:0.084246 episode_count: 18277. steps_count: 7913295.000000\n",
      "Time elapsed:  23209.62758898735\n",
      "ep 2611: ep_len:568 episode reward: total was 35.090000. running mean: -7.271991\n",
      "ep 2611: ep_len:531 episode reward: total was -34.020000. running mean: -7.539471\n",
      "ep 2611: ep_len:346 episode reward: total was 22.000000. running mean: -7.244076\n",
      "ep 2611: ep_len:530 episode reward: total was 27.360000. running mean: -6.898035\n",
      "ep 2611: ep_len:3 episode reward: total was 1.010000. running mean: -6.818955\n",
      "ep 2611: ep_len:316 episode reward: total was 9.600000. running mean: -6.654766\n",
      "ep 2611: ep_len:516 episode reward: total was -22.200000. running mean: -6.810218\n",
      "epsilon:0.084201 episode_count: 18284. steps_count: 7916105.000000\n",
      "Time elapsed:  23223.1216609478\n",
      "ep 2612: ep_len:134 episode reward: total was 10.000000. running mean: -6.642116\n",
      "ep 2612: ep_len:515 episode reward: total was -48.170000. running mean: -7.057395\n",
      "ep 2612: ep_len:433 episode reward: total was -1.780000. running mean: -7.004621\n",
      "ep 2612: ep_len:500 episode reward: total was -15.490000. running mean: -7.089474\n",
      "ep 2612: ep_len:3 episode reward: total was 1.010000. running mean: -7.008480\n",
      "ep 2612: ep_len:555 episode reward: total was 2.930000. running mean: -6.909095\n",
      "ep 2612: ep_len:500 episode reward: total was -44.410000. running mean: -7.284104\n",
      "epsilon:0.084157 episode_count: 18291. steps_count: 7918745.000000\n",
      "Time elapsed:  23231.365614652634\n",
      "ep 2613: ep_len:500 episode reward: total was 83.850000. running mean: -6.372763\n",
      "ep 2613: ep_len:500 episode reward: total was -10.410000. running mean: -6.413135\n",
      "ep 2613: ep_len:544 episode reward: total was 39.040000. running mean: -5.958604\n",
      "ep 2613: ep_len:500 episode reward: total was 40.680000. running mean: -5.492218\n",
      "ep 2613: ep_len:3 episode reward: total was -1.500000. running mean: -5.452296\n",
      "ep 2613: ep_len:529 episode reward: total was -40.930000. running mean: -5.807073\n",
      "ep 2613: ep_len:500 episode reward: total was -44.300000. running mean: -6.192002\n",
      "epsilon:0.084113 episode_count: 18298. steps_count: 7921821.000000\n",
      "Time elapsed:  23240.728140115738\n",
      "ep 2614: ep_len:592 episode reward: total was 13.910000. running mean: -5.990982\n",
      "ep 2614: ep_len:540 episode reward: total was 58.980000. running mean: -5.341272\n",
      "ep 2614: ep_len:571 episode reward: total was -47.890000. running mean: -5.766759\n",
      "ep 2614: ep_len:46 episode reward: total was 4.210000. running mean: -5.666992\n",
      "ep 2614: ep_len:3 episode reward: total was -1.500000. running mean: -5.625322\n",
      "ep 2614: ep_len:648 episode reward: total was 6.140000. running mean: -5.507669\n",
      "ep 2614: ep_len:625 episode reward: total was 3.870000. running mean: -5.413892\n",
      "epsilon:0.084068 episode_count: 18305. steps_count: 7924846.000000\n",
      "Time elapsed:  23255.774097681046\n",
      "ep 2615: ep_len:638 episode reward: total was -59.590000. running mean: -5.955653\n",
      "ep 2615: ep_len:590 episode reward: total was 9.480000. running mean: -5.801297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2615: ep_len:69 episode reward: total was -3.240000. running mean: -5.775684\n",
      "ep 2615: ep_len:502 episode reward: total was -4.970000. running mean: -5.767627\n",
      "ep 2615: ep_len:87 episode reward: total was 5.210000. running mean: -5.657851\n",
      "ep 2615: ep_len:529 episode reward: total was -14.480000. running mean: -5.746072\n",
      "ep 2615: ep_len:500 episode reward: total was -9.850000. running mean: -5.787111\n",
      "epsilon:0.084024 episode_count: 18312. steps_count: 7927761.000000\n",
      "Time elapsed:  23264.74894452095\n",
      "ep 2616: ep_len:584 episode reward: total was 63.750000. running mean: -5.091740\n",
      "ep 2616: ep_len:539 episode reward: total was -32.350000. running mean: -5.364323\n",
      "ep 2616: ep_len:560 episode reward: total was -12.070000. running mean: -5.431380\n",
      "ep 2616: ep_len:128 episode reward: total was 21.550000. running mean: -5.161566\n",
      "ep 2616: ep_len:3 episode reward: total was 1.010000. running mean: -5.099850\n",
      "ep 2616: ep_len:546 episode reward: total was -23.160000. running mean: -5.280452\n",
      "ep 2616: ep_len:534 episode reward: total was -3.440000. running mean: -5.262047\n",
      "epsilon:0.083980 episode_count: 18319. steps_count: 7930655.000000\n",
      "Time elapsed:  23273.517211437225\n",
      "ep 2617: ep_len:500 episode reward: total was 48.310000. running mean: -4.726327\n",
      "ep 2617: ep_len:500 episode reward: total was 19.750000. running mean: -4.481563\n",
      "ep 2617: ep_len:501 episode reward: total was -9.170000. running mean: -4.528448\n",
      "ep 2617: ep_len:421 episode reward: total was 38.240000. running mean: -4.100763\n",
      "ep 2617: ep_len:94 episode reward: total was 21.720000. running mean: -3.842556\n",
      "ep 2617: ep_len:500 episode reward: total was -117.750000. running mean: -4.981630\n",
      "ep 2617: ep_len:341 episode reward: total was -7.960000. running mean: -5.011414\n",
      "epsilon:0.083935 episode_count: 18326. steps_count: 7933512.000000\n",
      "Time elapsed:  23287.87059521675\n",
      "ep 2618: ep_len:599 episode reward: total was 49.450000. running mean: -4.466800\n",
      "ep 2618: ep_len:182 episode reward: total was -0.190000. running mean: -4.424032\n",
      "ep 2618: ep_len:500 episode reward: total was -28.620000. running mean: -4.665991\n",
      "ep 2618: ep_len:552 episode reward: total was 49.640000. running mean: -4.122931\n",
      "ep 2618: ep_len:99 episode reward: total was 25.750000. running mean: -3.824202\n",
      "ep 2618: ep_len:555 episode reward: total was -35.530000. running mean: -4.141260\n",
      "ep 2618: ep_len:599 episode reward: total was 8.510000. running mean: -4.014747\n",
      "epsilon:0.083891 episode_count: 18333. steps_count: 7936598.000000\n",
      "Time elapsed:  23297.233455896378\n",
      "ep 2619: ep_len:257 episode reward: total was 0.270000. running mean: -3.971900\n",
      "ep 2619: ep_len:508 episode reward: total was 5.640000. running mean: -3.875781\n",
      "ep 2619: ep_len:500 episode reward: total was -12.580000. running mean: -3.962823\n",
      "ep 2619: ep_len:553 episode reward: total was 38.530000. running mean: -3.537895\n",
      "ep 2619: ep_len:115 episode reward: total was 18.780000. running mean: -3.314716\n",
      "ep 2619: ep_len:639 episode reward: total was -1.860000. running mean: -3.300169\n",
      "ep 2619: ep_len:532 episode reward: total was -15.650000. running mean: -3.423667\n",
      "epsilon:0.083847 episode_count: 18340. steps_count: 7939702.000000\n",
      "Time elapsed:  23306.746687173843\n",
      "ep 2620: ep_len:577 episode reward: total was 59.310000. running mean: -2.796330\n",
      "ep 2620: ep_len:284 episode reward: total was -7.410000. running mean: -2.842467\n",
      "ep 2620: ep_len:681 episode reward: total was -143.890000. running mean: -4.252942\n",
      "ep 2620: ep_len:506 episode reward: total was -33.000000. running mean: -4.540413\n",
      "ep 2620: ep_len:3 episode reward: total was 1.010000. running mean: -4.484909\n",
      "ep 2620: ep_len:597 episode reward: total was -31.590000. running mean: -4.755960\n",
      "ep 2620: ep_len:314 episode reward: total was 9.540000. running mean: -4.613000\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.083802 episode_count: 18347. steps_count: 7942664.000000\n",
      "Time elapsed:  23320.8754427433\n",
      "ep 2621: ep_len:548 episode reward: total was 45.510000. running mean: -4.111770\n",
      "ep 2621: ep_len:549 episode reward: total was 27.520000. running mean: -3.795453\n",
      "ep 2621: ep_len:577 episode reward: total was -23.400000. running mean: -3.991498\n",
      "ep 2621: ep_len:40 episode reward: total was 5.680000. running mean: -3.894783\n",
      "ep 2621: ep_len:3 episode reward: total was 1.010000. running mean: -3.845735\n",
      "ep 2621: ep_len:165 episode reward: total was 18.670000. running mean: -3.620578\n",
      "ep 2621: ep_len:602 episode reward: total was 13.440000. running mean: -3.449972\n",
      "epsilon:0.083758 episode_count: 18354. steps_count: 7945148.000000\n",
      "Time elapsed:  23334.34762096405\n",
      "ep 2622: ep_len:526 episode reward: total was 70.010000. running mean: -2.715372\n",
      "ep 2622: ep_len:358 episode reward: total was -23.210000. running mean: -2.920319\n",
      "ep 2622: ep_len:500 episode reward: total was -38.900000. running mean: -3.280115\n",
      "ep 2622: ep_len:500 episode reward: total was 27.000000. running mean: -2.977314\n",
      "ep 2622: ep_len:81 episode reward: total was -29.350000. running mean: -3.241041\n",
      "ep 2622: ep_len:610 episode reward: total was -9.830000. running mean: -3.306931\n",
      "ep 2622: ep_len:614 episode reward: total was -31.780000. running mean: -3.591661\n",
      "epsilon:0.083714 episode_count: 18361. steps_count: 7948337.000000\n",
      "Time elapsed:  23348.235926151276\n",
      "ep 2623: ep_len:649 episode reward: total was -80.700000. running mean: -4.362745\n",
      "ep 2623: ep_len:581 episode reward: total was -77.160000. running mean: -5.090717\n",
      "ep 2623: ep_len:340 episode reward: total was 54.180000. running mean: -4.498010\n",
      "ep 2623: ep_len:500 episode reward: total was 34.220000. running mean: -4.110830\n",
      "ep 2623: ep_len:3 episode reward: total was 1.010000. running mean: -4.059622\n",
      "ep 2623: ep_len:500 episode reward: total was -63.910000. running mean: -4.658126\n",
      "ep 2623: ep_len:521 episode reward: total was 8.980000. running mean: -4.521744\n",
      "epsilon:0.083669 episode_count: 18368. steps_count: 7951431.000000\n",
      "Time elapsed:  23354.51188325882\n",
      "ep 2624: ep_len:641 episode reward: total was -30.000000. running mean: -4.776527\n",
      "ep 2624: ep_len:515 episode reward: total was 61.180000. running mean: -4.116962\n",
      "ep 2624: ep_len:604 episode reward: total was -45.830000. running mean: -4.534092\n",
      "ep 2624: ep_len:529 episode reward: total was -47.920000. running mean: -4.967951\n",
      "ep 2624: ep_len:3 episode reward: total was 1.010000. running mean: -4.908172\n",
      "ep 2624: ep_len:506 episode reward: total was -17.180000. running mean: -5.030890\n",
      "ep 2624: ep_len:500 episode reward: total was 3.900000. running mean: -4.941581\n",
      "epsilon:0.083625 episode_count: 18375. steps_count: 7954729.000000\n",
      "Time elapsed:  23363.142335653305\n",
      "ep 2625: ep_len:553 episode reward: total was 58.750000. running mean: -4.304665\n",
      "ep 2625: ep_len:526 episode reward: total was 5.270000. running mean: -4.208918\n",
      "ep 2625: ep_len:70 episode reward: total was 5.710000. running mean: -4.109729\n",
      "ep 2625: ep_len:567 episode reward: total was 20.950000. running mean: -3.859132\n",
      "ep 2625: ep_len:101 episode reward: total was 26.230000. running mean: -3.558241\n",
      "ep 2625: ep_len:621 episode reward: total was -61.380000. running mean: -4.136458\n",
      "ep 2625: ep_len:513 episode reward: total was 20.530000. running mean: -3.889794\n",
      "epsilon:0.083581 episode_count: 18382. steps_count: 7957680.000000\n",
      "Time elapsed:  23375.117216348648\n",
      "ep 2626: ep_len:596 episode reward: total was -45.900000. running mean: -4.309896\n",
      "ep 2626: ep_len:500 episode reward: total was 31.820000. running mean: -3.948597\n",
      "ep 2626: ep_len:572 episode reward: total was -35.270000. running mean: -4.261811\n",
      "ep 2626: ep_len:106 episode reward: total was -0.450000. running mean: -4.223693\n",
      "ep 2626: ep_len:3 episode reward: total was 1.010000. running mean: -4.171356\n",
      "ep 2626: ep_len:543 episode reward: total was -28.700000. running mean: -4.416642\n",
      "ep 2626: ep_len:605 episode reward: total was -11.620000. running mean: -4.488676\n",
      "epsilon:0.083536 episode_count: 18389. steps_count: 7960605.000000\n",
      "Time elapsed:  23384.64825940132\n",
      "ep 2627: ep_len:545 episode reward: total was 29.930000. running mean: -4.144489\n",
      "ep 2627: ep_len:252 episode reward: total was -133.480000. running mean: -5.437844\n",
      "ep 2627: ep_len:500 episode reward: total was -24.210000. running mean: -5.625566\n",
      "ep 2627: ep_len:528 episode reward: total was 10.100000. running mean: -5.468310\n",
      "ep 2627: ep_len:3 episode reward: total was 1.010000. running mean: -5.403527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2627: ep_len:500 episode reward: total was 4.870000. running mean: -5.300792\n",
      "ep 2627: ep_len:522 episode reward: total was -32.280000. running mean: -5.570584\n",
      "epsilon:0.083492 episode_count: 18396. steps_count: 7963455.000000\n",
      "Time elapsed:  23393.411890745163\n",
      "ep 2628: ep_len:223 episode reward: total was 9.640000. running mean: -5.418478\n",
      "ep 2628: ep_len:500 episode reward: total was -28.340000. running mean: -5.647693\n",
      "ep 2628: ep_len:434 episode reward: total was 31.170000. running mean: -5.279516\n",
      "ep 2628: ep_len:43 episode reward: total was -10.220000. running mean: -5.328921\n",
      "ep 2628: ep_len:3 episode reward: total was 1.010000. running mean: -5.265532\n",
      "ep 2628: ep_len:500 episode reward: total was 0.570000. running mean: -5.207177\n",
      "ep 2628: ep_len:304 episode reward: total was -9.950000. running mean: -5.254605\n",
      "epsilon:0.083448 episode_count: 18403. steps_count: 7965462.000000\n",
      "Time elapsed:  23405.62650156021\n",
      "ep 2629: ep_len:116 episode reward: total was 7.000000. running mean: -5.132059\n",
      "ep 2629: ep_len:594 episode reward: total was -73.390000. running mean: -5.814638\n",
      "ep 2629: ep_len:564 episode reward: total was -39.360000. running mean: -6.150092\n",
      "ep 2629: ep_len:383 episode reward: total was 27.900000. running mean: -5.809591\n",
      "ep 2629: ep_len:3 episode reward: total was 1.010000. running mean: -5.741395\n",
      "ep 2629: ep_len:527 episode reward: total was -27.360000. running mean: -5.957581\n",
      "ep 2629: ep_len:609 episode reward: total was -285.290000. running mean: -8.750905\n",
      "epsilon:0.083403 episode_count: 18410. steps_count: 7968258.000000\n",
      "Time elapsed:  23420.863017082214\n",
      "ep 2630: ep_len:625 episode reward: total was -115.070000. running mean: -9.814096\n",
      "ep 2630: ep_len:521 episode reward: total was 16.970000. running mean: -9.546255\n",
      "ep 2630: ep_len:589 episode reward: total was -22.360000. running mean: -9.674393\n",
      "ep 2630: ep_len:506 episode reward: total was -17.020000. running mean: -9.747849\n",
      "ep 2630: ep_len:120 episode reward: total was 12.340000. running mean: -9.526970\n",
      "ep 2630: ep_len:510 episode reward: total was 1.190000. running mean: -9.419800\n",
      "ep 2630: ep_len:295 episode reward: total was -10.700000. running mean: -9.432602\n",
      "epsilon:0.083359 episode_count: 18417. steps_count: 7971424.000000\n",
      "Time elapsed:  23436.763043880463\n",
      "ep 2631: ep_len:691 episode reward: total was -53.890000. running mean: -9.877176\n",
      "ep 2631: ep_len:551 episode reward: total was -29.180000. running mean: -10.070205\n",
      "ep 2631: ep_len:546 episode reward: total was -29.380000. running mean: -10.263303\n",
      "ep 2631: ep_len:551 episode reward: total was 51.120000. running mean: -9.649470\n",
      "ep 2631: ep_len:53 episode reward: total was 20.500000. running mean: -9.347975\n",
      "ep 2631: ep_len:521 episode reward: total was -33.490000. running mean: -9.589395\n",
      "ep 2631: ep_len:332 episode reward: total was -22.290000. running mean: -9.716401\n",
      "epsilon:0.083315 episode_count: 18424. steps_count: 7974669.000000\n",
      "Time elapsed:  23452.552121400833\n",
      "ep 2632: ep_len:516 episode reward: total was -2.480000. running mean: -9.644037\n",
      "ep 2632: ep_len:188 episode reward: total was -7.630000. running mean: -9.623897\n",
      "ep 2632: ep_len:563 episode reward: total was -46.620000. running mean: -9.993858\n",
      "ep 2632: ep_len:537 episode reward: total was 31.080000. running mean: -9.583119\n",
      "ep 2632: ep_len:123 episode reward: total was 22.350000. running mean: -9.263788\n",
      "ep 2632: ep_len:500 episode reward: total was -25.310000. running mean: -9.424250\n",
      "ep 2632: ep_len:504 episode reward: total was -17.360000. running mean: -9.503608\n",
      "epsilon:0.083270 episode_count: 18431. steps_count: 7977600.000000\n",
      "Time elapsed:  23467.68875169754\n",
      "ep 2633: ep_len:517 episode reward: total was -41.490000. running mean: -9.823472\n",
      "ep 2633: ep_len:512 episode reward: total was 20.530000. running mean: -9.519937\n",
      "ep 2633: ep_len:532 episode reward: total was 9.290000. running mean: -9.331838\n",
      "ep 2633: ep_len:519 episode reward: total was 29.940000. running mean: -8.939119\n",
      "ep 2633: ep_len:3 episode reward: total was 1.010000. running mean: -8.839628\n",
      "ep 2633: ep_len:500 episode reward: total was -0.800000. running mean: -8.759232\n",
      "ep 2633: ep_len:520 episode reward: total was -8.820000. running mean: -8.759839\n",
      "epsilon:0.083226 episode_count: 18438. steps_count: 7980703.000000\n",
      "Time elapsed:  23477.061591863632\n",
      "ep 2634: ep_len:610 episode reward: total was 29.870000. running mean: -8.373541\n",
      "ep 2634: ep_len:505 episode reward: total was -20.420000. running mean: -8.494006\n",
      "ep 2634: ep_len:599 episode reward: total was -25.490000. running mean: -8.663966\n",
      "ep 2634: ep_len:56 episode reward: total was -9.220000. running mean: -8.669526\n",
      "ep 2634: ep_len:3 episode reward: total was 1.010000. running mean: -8.572731\n",
      "ep 2634: ep_len:508 episode reward: total was -34.140000. running mean: -8.828403\n",
      "ep 2634: ep_len:602 episode reward: total was 45.870000. running mean: -8.281419\n",
      "epsilon:0.083182 episode_count: 18445. steps_count: 7983586.000000\n",
      "Time elapsed:  23485.930795431137\n",
      "ep 2635: ep_len:563 episode reward: total was -61.690000. running mean: -8.815505\n",
      "ep 2635: ep_len:501 episode reward: total was 21.430000. running mean: -8.513050\n",
      "ep 2635: ep_len:547 episode reward: total was -26.160000. running mean: -8.689520\n",
      "ep 2635: ep_len:132 episode reward: total was 8.060000. running mean: -8.522024\n",
      "ep 2635: ep_len:3 episode reward: total was 1.010000. running mean: -8.426704\n",
      "ep 2635: ep_len:635 episode reward: total was -21.840000. running mean: -8.560837\n",
      "ep 2635: ep_len:324 episode reward: total was 8.540000. running mean: -8.389829\n",
      "epsilon:0.083137 episode_count: 18452. steps_count: 7986291.000000\n",
      "Time elapsed:  23494.286322832108\n",
      "ep 2636: ep_len:500 episode reward: total was 28.910000. running mean: -8.016830\n",
      "ep 2636: ep_len:511 episode reward: total was -154.800000. running mean: -9.484662\n",
      "ep 2636: ep_len:500 episode reward: total was -1.720000. running mean: -9.407015\n",
      "ep 2636: ep_len:500 episode reward: total was -20.140000. running mean: -9.514345\n",
      "ep 2636: ep_len:3 episode reward: total was 1.010000. running mean: -9.409102\n",
      "ep 2636: ep_len:568 episode reward: total was -13.910000. running mean: -9.454111\n",
      "ep 2636: ep_len:501 episode reward: total was -3.750000. running mean: -9.397070\n",
      "epsilon:0.083093 episode_count: 18459. steps_count: 7989374.000000\n",
      "Time elapsed:  23502.515902519226\n",
      "ep 2637: ep_len:225 episode reward: total was 8.150000. running mean: -9.221599\n",
      "ep 2637: ep_len:500 episode reward: total was 29.440000. running mean: -8.834983\n",
      "ep 2637: ep_len:535 episode reward: total was 9.560000. running mean: -8.651033\n",
      "ep 2637: ep_len:360 episode reward: total was 10.150000. running mean: -8.463023\n",
      "ep 2637: ep_len:3 episode reward: total was 1.010000. running mean: -8.368293\n",
      "ep 2637: ep_len:615 episode reward: total was -15.710000. running mean: -8.441710\n",
      "ep 2637: ep_len:288 episode reward: total was -20.200000. running mean: -8.559293\n",
      "epsilon:0.083049 episode_count: 18466. steps_count: 7991900.000000\n",
      "Time elapsed:  23509.28439974785\n",
      "ep 2638: ep_len:562 episode reward: total was 41.640000. running mean: -8.057300\n",
      "ep 2638: ep_len:593 episode reward: total was -36.780000. running mean: -8.344527\n",
      "ep 2638: ep_len:574 episode reward: total was 19.830000. running mean: -8.062781\n",
      "ep 2638: ep_len:540 episode reward: total was -141.290000. running mean: -9.395054\n",
      "ep 2638: ep_len:3 episode reward: total was 1.010000. running mean: -9.291003\n",
      "ep 2638: ep_len:168 episode reward: total was -2.470000. running mean: -9.222793\n",
      "ep 2638: ep_len:545 episode reward: total was -42.600000. running mean: -9.556565\n",
      "epsilon:0.083004 episode_count: 18473. steps_count: 7994885.000000\n",
      "Time elapsed:  23517.142925024033\n",
      "ep 2639: ep_len:506 episode reward: total was 7.850000. running mean: -9.382499\n",
      "ep 2639: ep_len:500 episode reward: total was 50.230000. running mean: -8.786374\n",
      "ep 2639: ep_len:527 episode reward: total was -54.070000. running mean: -9.239211\n",
      "ep 2639: ep_len:500 episode reward: total was 35.500000. running mean: -8.791819\n",
      "ep 2639: ep_len:98 episode reward: total was 27.240000. running mean: -8.431500\n",
      "ep 2639: ep_len:500 episode reward: total was 7.500000. running mean: -8.272185\n",
      "ep 2639: ep_len:319 episode reward: total was -46.510000. running mean: -8.654564\n",
      "epsilon:0.082960 episode_count: 18480. steps_count: 7997835.000000\n",
      "Time elapsed:  23525.094583034515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2640: ep_len:513 episode reward: total was -75.150000. running mean: -9.319518\n",
      "ep 2640: ep_len:541 episode reward: total was 73.590000. running mean: -8.490423\n",
      "ep 2640: ep_len:538 episode reward: total was 1.340000. running mean: -8.392119\n",
      "ep 2640: ep_len:561 episode reward: total was 23.430000. running mean: -8.073897\n",
      "ep 2640: ep_len:3 episode reward: total was 1.010000. running mean: -7.983058\n",
      "ep 2640: ep_len:514 episode reward: total was 27.340000. running mean: -7.629828\n",
      "ep 2640: ep_len:545 episode reward: total was 34.490000. running mean: -7.208630\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.082916 episode_count: 18487. steps_count: 8001050.000000\n",
      "Time elapsed:  23538.434795618057\n",
      "ep 2641: ep_len:134 episode reward: total was -3.560000. running mean: -7.172143\n",
      "ep 2641: ep_len:500 episode reward: total was 53.870000. running mean: -6.561722\n",
      "ep 2641: ep_len:557 episode reward: total was -36.430000. running mean: -6.860405\n",
      "ep 2641: ep_len:504 episode reward: total was -27.600000. running mean: -7.067801\n",
      "ep 2641: ep_len:3 episode reward: total was 1.010000. running mean: -6.987023\n",
      "ep 2641: ep_len:504 episode reward: total was -47.250000. running mean: -7.389652\n",
      "ep 2641: ep_len:593 episode reward: total was 4.850000. running mean: -7.267256\n",
      "epsilon:0.082871 episode_count: 18494. steps_count: 8003845.000000\n",
      "Time elapsed:  23545.84653520584\n",
      "ep 2642: ep_len:546 episode reward: total was 40.960000. running mean: -6.784983\n",
      "ep 2642: ep_len:590 episode reward: total was 40.490000. running mean: -6.312233\n",
      "ep 2642: ep_len:500 episode reward: total was -16.170000. running mean: -6.410811\n",
      "ep 2642: ep_len:40 episode reward: total was -0.290000. running mean: -6.349603\n",
      "ep 2642: ep_len:131 episode reward: total was 24.340000. running mean: -6.042707\n",
      "ep 2642: ep_len:500 episode reward: total was -72.190000. running mean: -6.704180\n",
      "ep 2642: ep_len:500 episode reward: total was 20.270000. running mean: -6.434438\n",
      "epsilon:0.082827 episode_count: 18501. steps_count: 8006652.000000\n",
      "Time elapsed:  23553.41403889656\n",
      "ep 2643: ep_len:518 episode reward: total was 36.810000. running mean: -6.001994\n",
      "ep 2643: ep_len:300 episode reward: total was -5.230000. running mean: -5.994274\n",
      "ep 2643: ep_len:602 episode reward: total was -63.850000. running mean: -6.572831\n",
      "ep 2643: ep_len:535 episode reward: total was 42.260000. running mean: -6.084503\n",
      "ep 2643: ep_len:109 episode reward: total was 30.750000. running mean: -5.716158\n",
      "ep 2643: ep_len:525 episode reward: total was -17.010000. running mean: -5.829096\n",
      "ep 2643: ep_len:205 episode reward: total was -3.420000. running mean: -5.805005\n",
      "epsilon:0.082783 episode_count: 18508. steps_count: 8009446.000000\n",
      "Time elapsed:  23564.6971282959\n",
      "ep 2644: ep_len:613 episode reward: total was 64.420000. running mean: -5.102755\n",
      "ep 2644: ep_len:500 episode reward: total was -110.800000. running mean: -6.159728\n",
      "ep 2644: ep_len:667 episode reward: total was -185.530000. running mean: -7.953430\n",
      "ep 2644: ep_len:611 episode reward: total was 19.840000. running mean: -7.675496\n",
      "ep 2644: ep_len:50 episode reward: total was 16.000000. running mean: -7.438741\n",
      "ep 2644: ep_len:214 episode reward: total was 28.730000. running mean: -7.077054\n",
      "ep 2644: ep_len:580 episode reward: total was -196.890000. running mean: -8.975183\n",
      "epsilon:0.082738 episode_count: 18515. steps_count: 8012681.000000\n",
      "Time elapsed:  23577.887408971786\n",
      "ep 2645: ep_len:134 episode reward: total was 3.510000. running mean: -8.850331\n",
      "ep 2645: ep_len:500 episode reward: total was 20.990000. running mean: -8.551928\n",
      "ep 2645: ep_len:617 episode reward: total was -6.210000. running mean: -8.528509\n",
      "ep 2645: ep_len:524 episode reward: total was -3.800000. running mean: -8.481224\n",
      "ep 2645: ep_len:3 episode reward: total was 1.010000. running mean: -8.386311\n",
      "ep 2645: ep_len:500 episode reward: total was 31.510000. running mean: -7.987348\n",
      "ep 2645: ep_len:573 episode reward: total was -112.420000. running mean: -9.031675\n",
      "epsilon:0.082694 episode_count: 18522. steps_count: 8015532.000000\n",
      "Time elapsed:  23589.325988054276\n",
      "ep 2646: ep_len:561 episode reward: total was -83.990000. running mean: -9.781258\n",
      "ep 2646: ep_len:500 episode reward: total was -99.270000. running mean: -10.676145\n",
      "ep 2646: ep_len:623 episode reward: total was -19.640000. running mean: -10.765784\n",
      "ep 2646: ep_len:520 episode reward: total was -16.790000. running mean: -10.826026\n",
      "ep 2646: ep_len:108 episode reward: total was -43.740000. running mean: -11.155166\n",
      "ep 2646: ep_len:587 episode reward: total was 49.610000. running mean: -10.547514\n",
      "ep 2646: ep_len:597 episode reward: total was 14.880000. running mean: -10.293239\n",
      "epsilon:0.082650 episode_count: 18529. steps_count: 8019028.000000\n",
      "Time elapsed:  23598.748309612274\n",
      "ep 2647: ep_len:573 episode reward: total was -46.040000. running mean: -10.650707\n",
      "ep 2647: ep_len:524 episode reward: total was 11.420000. running mean: -10.430000\n",
      "ep 2647: ep_len:674 episode reward: total was -19.320000. running mean: -10.518900\n",
      "ep 2647: ep_len:500 episode reward: total was 14.200000. running mean: -10.271711\n",
      "ep 2647: ep_len:3 episode reward: total was 1.010000. running mean: -10.158893\n",
      "ep 2647: ep_len:500 episode reward: total was 5.310000. running mean: -10.004205\n",
      "ep 2647: ep_len:610 episode reward: total was -7.590000. running mean: -9.980062\n",
      "epsilon:0.082605 episode_count: 18536. steps_count: 8022412.000000\n",
      "Time elapsed:  23607.562839746475\n",
      "ep 2648: ep_len:104 episode reward: total was 14.990000. running mean: -9.730362\n",
      "ep 2648: ep_len:500 episode reward: total was 29.440000. running mean: -9.338658\n",
      "ep 2648: ep_len:507 episode reward: total was 8.370000. running mean: -9.161572\n",
      "ep 2648: ep_len:502 episode reward: total was 37.250000. running mean: -8.697456\n",
      "ep 2648: ep_len:3 episode reward: total was 0.000000. running mean: -8.610481\n",
      "ep 2648: ep_len:519 episode reward: total was 0.400000. running mean: -8.520377\n",
      "ep 2648: ep_len:500 episode reward: total was -24.420000. running mean: -8.679373\n",
      "epsilon:0.082561 episode_count: 18543. steps_count: 8025047.000000\n",
      "Time elapsed:  23618.43754005432\n",
      "ep 2649: ep_len:583 episode reward: total was 27.530000. running mean: -8.317279\n",
      "ep 2649: ep_len:581 episode reward: total was 25.100000. running mean: -7.983106\n",
      "ep 2649: ep_len:627 episode reward: total was -1.520000. running mean: -7.918475\n",
      "ep 2649: ep_len:56 episode reward: total was -7.150000. running mean: -7.910790\n",
      "ep 2649: ep_len:3 episode reward: total was 1.010000. running mean: -7.821583\n",
      "ep 2649: ep_len:306 episode reward: total was -4.740000. running mean: -7.790767\n",
      "ep 2649: ep_len:615 episode reward: total was -14.520000. running mean: -7.858059\n",
      "epsilon:0.082517 episode_count: 18550. steps_count: 8027818.000000\n",
      "Time elapsed:  23625.130766630173\n",
      "ep 2650: ep_len:521 episode reward: total was -47.020000. running mean: -8.249678\n",
      "ep 2650: ep_len:620 episode reward: total was 5.630000. running mean: -8.110882\n",
      "ep 2650: ep_len:500 episode reward: total was 35.950000. running mean: -7.670273\n",
      "ep 2650: ep_len:549 episode reward: total was 22.430000. running mean: -7.369270\n",
      "ep 2650: ep_len:3 episode reward: total was -1.500000. running mean: -7.310577\n",
      "ep 2650: ep_len:500 episode reward: total was -261.060000. running mean: -9.848072\n",
      "ep 2650: ep_len:211 episode reward: total was 0.160000. running mean: -9.747991\n",
      "epsilon:0.082472 episode_count: 18557. steps_count: 8030722.000000\n",
      "Time elapsed:  23633.063128232956\n",
      "ep 2651: ep_len:134 episode reward: total was 19.060000. running mean: -9.459911\n",
      "ep 2651: ep_len:181 episode reward: total was -6.690000. running mean: -9.432212\n",
      "ep 2651: ep_len:394 episode reward: total was 43.890000. running mean: -8.898990\n",
      "ep 2651: ep_len:122 episode reward: total was 5.010000. running mean: -8.759900\n",
      "ep 2651: ep_len:3 episode reward: total was 1.010000. running mean: -8.662201\n",
      "ep 2651: ep_len:509 episode reward: total was 3.740000. running mean: -8.538179\n",
      "ep 2651: ep_len:608 episode reward: total was 29.600000. running mean: -8.156797\n",
      "epsilon:0.082428 episode_count: 18564. steps_count: 8032673.000000\n",
      "Time elapsed:  23638.65195584297\n",
      "ep 2652: ep_len:527 episode reward: total was -34.760000. running mean: -8.422829\n",
      "ep 2652: ep_len:592 episode reward: total was -95.040000. running mean: -9.289001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2652: ep_len:592 episode reward: total was -151.500000. running mean: -10.711111\n",
      "ep 2652: ep_len:154 episode reward: total was 10.650000. running mean: -10.497500\n",
      "ep 2652: ep_len:3 episode reward: total was 1.010000. running mean: -10.382425\n",
      "ep 2652: ep_len:519 episode reward: total was -57.120000. running mean: -10.849800\n",
      "ep 2652: ep_len:500 episode reward: total was -40.690000. running mean: -11.148202\n",
      "epsilon:0.082384 episode_count: 18571. steps_count: 8035560.000000\n",
      "Time elapsed:  23646.355775117874\n",
      "ep 2653: ep_len:500 episode reward: total was -52.570000. running mean: -11.562420\n",
      "ep 2653: ep_len:581 episode reward: total was 7.640000. running mean: -11.370396\n",
      "ep 2653: ep_len:500 episode reward: total was 13.700000. running mean: -11.119692\n",
      "ep 2653: ep_len:500 episode reward: total was -22.680000. running mean: -11.235295\n",
      "ep 2653: ep_len:85 episode reward: total was 19.730000. running mean: -10.925642\n",
      "ep 2653: ep_len:214 episode reward: total was 29.100000. running mean: -10.525386\n",
      "ep 2653: ep_len:536 episode reward: total was -19.150000. running mean: -10.611632\n",
      "epsilon:0.082339 episode_count: 18578. steps_count: 8038476.000000\n",
      "Time elapsed:  23654.17652273178\n",
      "ep 2654: ep_len:227 episode reward: total was -13.460000. running mean: -10.640116\n",
      "ep 2654: ep_len:586 episode reward: total was -70.190000. running mean: -11.235615\n",
      "ep 2654: ep_len:384 episode reward: total was 10.300000. running mean: -11.020259\n",
      "ep 2654: ep_len:45 episode reward: total was -7.780000. running mean: -10.987856\n",
      "ep 2654: ep_len:49 episode reward: total was 17.000000. running mean: -10.707977\n",
      "ep 2654: ep_len:555 episode reward: total was 7.690000. running mean: -10.523998\n",
      "ep 2654: ep_len:544 episode reward: total was 4.700000. running mean: -10.371758\n",
      "epsilon:0.082295 episode_count: 18585. steps_count: 8040866.000000\n",
      "Time elapsed:  23660.817871809006\n",
      "ep 2655: ep_len:500 episode reward: total was 16.540000. running mean: -10.102640\n",
      "ep 2655: ep_len:554 episode reward: total was -71.580000. running mean: -10.717414\n",
      "ep 2655: ep_len:500 episode reward: total was 24.000000. running mean: -10.370240\n",
      "ep 2655: ep_len:517 episode reward: total was 17.160000. running mean: -10.094937\n",
      "ep 2655: ep_len:3 episode reward: total was 1.010000. running mean: -9.983888\n",
      "ep 2655: ep_len:595 episode reward: total was 48.990000. running mean: -9.394149\n",
      "ep 2655: ep_len:525 episode reward: total was -62.570000. running mean: -9.925907\n",
      "epsilon:0.082251 episode_count: 18592. steps_count: 8044060.000000\n",
      "Time elapsed:  23668.94446873665\n",
      "ep 2656: ep_len:583 episode reward: total was 40.700000. running mean: -9.419648\n",
      "ep 2656: ep_len:547 episode reward: total was 61.560000. running mean: -8.709852\n",
      "ep 2656: ep_len:613 episode reward: total was -26.710000. running mean: -8.889853\n",
      "ep 2656: ep_len:539 episode reward: total was 16.870000. running mean: -8.632255\n",
      "ep 2656: ep_len:88 episode reward: total was 23.250000. running mean: -8.313432\n",
      "ep 2656: ep_len:514 episode reward: total was -6.810000. running mean: -8.298398\n",
      "ep 2656: ep_len:501 episode reward: total was -1.960000. running mean: -8.235014\n",
      "epsilon:0.082206 episode_count: 18599. steps_count: 8047445.000000\n",
      "Time elapsed:  23681.60706782341\n",
      "ep 2657: ep_len:616 episode reward: total was -61.110000. running mean: -8.763764\n",
      "ep 2657: ep_len:563 episode reward: total was -3.390000. running mean: -8.710026\n",
      "ep 2657: ep_len:585 episode reward: total was -17.570000. running mean: -8.798626\n",
      "ep 2657: ep_len:500 episode reward: total was 51.530000. running mean: -8.195340\n",
      "ep 2657: ep_len:85 episode reward: total was 18.750000. running mean: -7.925886\n",
      "ep 2657: ep_len:602 episode reward: total was 27.430000. running mean: -7.572327\n",
      "ep 2657: ep_len:559 episode reward: total was -7.950000. running mean: -7.576104\n",
      "epsilon:0.082162 episode_count: 18606. steps_count: 8050955.000000\n",
      "Time elapsed:  23690.759420394897\n",
      "ep 2658: ep_len:130 episode reward: total was 3.440000. running mean: -7.465943\n",
      "ep 2658: ep_len:500 episode reward: total was -3.460000. running mean: -7.425884\n",
      "ep 2658: ep_len:665 episode reward: total was -14.690000. running mean: -7.498525\n",
      "ep 2658: ep_len:505 episode reward: total was -35.730000. running mean: -7.780840\n",
      "ep 2658: ep_len:3 episode reward: total was 1.010000. running mean: -7.692931\n",
      "ep 2658: ep_len:226 episode reward: total was 26.860000. running mean: -7.347402\n",
      "ep 2658: ep_len:503 episode reward: total was -11.550000. running mean: -7.389428\n",
      "epsilon:0.082118 episode_count: 18613. steps_count: 8053487.000000\n",
      "Time elapsed:  23697.581481695175\n",
      "ep 2659: ep_len:595 episode reward: total was 53.250000. running mean: -6.783034\n",
      "ep 2659: ep_len:294 episode reward: total was 1.200000. running mean: -6.703203\n",
      "ep 2659: ep_len:516 episode reward: total was -49.210000. running mean: -7.128271\n",
      "ep 2659: ep_len:583 episode reward: total was 35.840000. running mean: -6.698588\n",
      "ep 2659: ep_len:37 episode reward: total was 10.510000. running mean: -6.526503\n",
      "ep 2659: ep_len:521 episode reward: total was -34.420000. running mean: -6.805438\n",
      "ep 2659: ep_len:500 episode reward: total was -17.210000. running mean: -6.909483\n",
      "epsilon:0.082073 episode_count: 18620. steps_count: 8056533.000000\n",
      "Time elapsed:  23705.67498230934\n",
      "ep 2660: ep_len:606 episode reward: total was 30.130000. running mean: -6.539088\n",
      "ep 2660: ep_len:500 episode reward: total was 27.870000. running mean: -6.194997\n",
      "ep 2660: ep_len:513 episode reward: total was 1.300000. running mean: -6.120047\n",
      "ep 2660: ep_len:168 episode reward: total was 7.200000. running mean: -5.986847\n",
      "ep 2660: ep_len:3 episode reward: total was 1.010000. running mean: -5.916879\n",
      "ep 2660: ep_len:500 episode reward: total was 7.420000. running mean: -5.783510\n",
      "ep 2660: ep_len:557 episode reward: total was 12.600000. running mean: -5.599675\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.082029 episode_count: 18627. steps_count: 8059380.000000\n",
      "Time elapsed:  23718.08419585228\n",
      "ep 2661: ep_len:500 episode reward: total was -5.540000. running mean: -5.599078\n",
      "ep 2661: ep_len:544 episode reward: total was 15.990000. running mean: -5.383187\n",
      "ep 2661: ep_len:500 episode reward: total was -6.270000. running mean: -5.392055\n",
      "ep 2661: ep_len:500 episode reward: total was 6.520000. running mean: -5.272935\n",
      "ep 2661: ep_len:3 episode reward: total was 1.010000. running mean: -5.210105\n",
      "ep 2661: ep_len:168 episode reward: total was 9.430000. running mean: -5.063704\n",
      "ep 2661: ep_len:354 episode reward: total was -4.380000. running mean: -5.056867\n",
      "epsilon:0.081985 episode_count: 18634. steps_count: 8061949.000000\n",
      "Time elapsed:  23727.53982782364\n",
      "ep 2662: ep_len:264 episode reward: total was -3.230000. running mean: -5.038599\n",
      "ep 2662: ep_len:621 episode reward: total was 12.070000. running mean: -4.867513\n",
      "ep 2662: ep_len:66 episode reward: total was -0.180000. running mean: -4.820637\n",
      "ep 2662: ep_len:500 episode reward: total was -18.520000. running mean: -4.957631\n",
      "ep 2662: ep_len:3 episode reward: total was 1.010000. running mean: -4.897955\n",
      "ep 2662: ep_len:575 episode reward: total was -2.850000. running mean: -4.877475\n",
      "ep 2662: ep_len:314 episode reward: total was 0.420000. running mean: -4.824500\n",
      "epsilon:0.081940 episode_count: 18641. steps_count: 8064292.000000\n",
      "Time elapsed:  23733.948128461838\n",
      "ep 2663: ep_len:106 episode reward: total was -11.470000. running mean: -4.890955\n",
      "ep 2663: ep_len:557 episode reward: total was -8.580000. running mean: -4.927846\n",
      "ep 2663: ep_len:500 episode reward: total was -49.800000. running mean: -5.376567\n",
      "ep 2663: ep_len:500 episode reward: total was 19.440000. running mean: -5.128402\n",
      "ep 2663: ep_len:3 episode reward: total was -1.500000. running mean: -5.092118\n",
      "ep 2663: ep_len:531 episode reward: total was -23.260000. running mean: -5.273797\n",
      "ep 2663: ep_len:604 episode reward: total was -1.200000. running mean: -5.233059\n",
      "epsilon:0.081896 episode_count: 18648. steps_count: 8067093.000000\n",
      "Time elapsed:  23741.519619703293\n",
      "ep 2664: ep_len:128 episode reward: total was 12.480000. running mean: -5.055928\n",
      "ep 2664: ep_len:543 episode reward: total was 18.370000. running mean: -4.821669\n",
      "ep 2664: ep_len:597 episode reward: total was -10.040000. running mean: -4.873852\n",
      "ep 2664: ep_len:521 episode reward: total was -114.810000. running mean: -5.973214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2664: ep_len:53 episode reward: total was 24.510000. running mean: -5.668381\n",
      "ep 2664: ep_len:601 episode reward: total was -22.820000. running mean: -5.839898\n",
      "ep 2664: ep_len:588 episode reward: total was 12.700000. running mean: -5.654499\n",
      "epsilon:0.081852 episode_count: 18655. steps_count: 8070124.000000\n",
      "Time elapsed:  23749.675936937332\n",
      "ep 2665: ep_len:210 episode reward: total was 14.700000. running mean: -5.450954\n",
      "ep 2665: ep_len:500 episode reward: total was -27.500000. running mean: -5.671444\n",
      "ep 2665: ep_len:590 episode reward: total was -24.500000. running mean: -5.859730\n",
      "ep 2665: ep_len:510 episode reward: total was -9.970000. running mean: -5.900832\n",
      "ep 2665: ep_len:3 episode reward: total was 1.010000. running mean: -5.831724\n",
      "ep 2665: ep_len:500 episode reward: total was 13.350000. running mean: -5.639907\n",
      "ep 2665: ep_len:346 episode reward: total was -65.830000. running mean: -6.241808\n",
      "epsilon:0.081807 episode_count: 18662. steps_count: 8072783.000000\n",
      "Time elapsed:  23756.912578105927\n",
      "ep 2666: ep_len:236 episode reward: total was 28.280000. running mean: -5.896590\n",
      "ep 2666: ep_len:297 episode reward: total was -14.930000. running mean: -5.986924\n",
      "ep 2666: ep_len:540 episode reward: total was -29.960000. running mean: -6.226655\n",
      "ep 2666: ep_len:500 episode reward: total was 0.740000. running mean: -6.156988\n",
      "ep 2666: ep_len:52 episode reward: total was 18.500000. running mean: -5.910418\n",
      "ep 2666: ep_len:513 episode reward: total was -10.340000. running mean: -5.954714\n",
      "ep 2666: ep_len:574 episode reward: total was 1.030000. running mean: -5.884867\n",
      "epsilon:0.081763 episode_count: 18669. steps_count: 8075495.000000\n",
      "Time elapsed:  23764.35021018982\n",
      "ep 2667: ep_len:560 episode reward: total was -232.720000. running mean: -8.153218\n",
      "ep 2667: ep_len:504 episode reward: total was 5.350000. running mean: -8.018186\n",
      "ep 2667: ep_len:531 episode reward: total was -59.500000. running mean: -8.533004\n",
      "ep 2667: ep_len:392 episode reward: total was 10.090000. running mean: -8.346774\n",
      "ep 2667: ep_len:3 episode reward: total was 1.010000. running mean: -8.253206\n",
      "ep 2667: ep_len:584 episode reward: total was -3.650000. running mean: -8.207174\n",
      "ep 2667: ep_len:204 episode reward: total was -14.540000. running mean: -8.270502\n",
      "epsilon:0.081719 episode_count: 18676. steps_count: 8078273.000000\n",
      "Time elapsed:  23771.78844809532\n",
      "ep 2668: ep_len:503 episode reward: total was -35.870000. running mean: -8.546497\n",
      "ep 2668: ep_len:538 episode reward: total was -4.610000. running mean: -8.507132\n",
      "ep 2668: ep_len:639 episode reward: total was -67.940000. running mean: -9.101461\n",
      "ep 2668: ep_len:505 episode reward: total was 8.560000. running mean: -8.924847\n",
      "ep 2668: ep_len:3 episode reward: total was 1.010000. running mean: -8.825498\n",
      "ep 2668: ep_len:519 episode reward: total was -18.150000. running mean: -8.918743\n",
      "ep 2668: ep_len:559 episode reward: total was -8.250000. running mean: -8.912056\n",
      "epsilon:0.081674 episode_count: 18683. steps_count: 8081539.000000\n",
      "Time elapsed:  23780.715105056763\n",
      "ep 2669: ep_len:125 episode reward: total was -4.600000. running mean: -8.868935\n",
      "ep 2669: ep_len:584 episode reward: total was -38.550000. running mean: -9.165746\n",
      "ep 2669: ep_len:550 episode reward: total was -28.330000. running mean: -9.357388\n",
      "ep 2669: ep_len:567 episode reward: total was 63.620000. running mean: -8.627614\n",
      "ep 2669: ep_len:104 episode reward: total was 24.760000. running mean: -8.293738\n",
      "ep 2669: ep_len:500 episode reward: total was -25.750000. running mean: -8.468301\n",
      "ep 2669: ep_len:186 episode reward: total was -9.700000. running mean: -8.480618\n",
      "epsilon:0.081630 episode_count: 18690. steps_count: 8084155.000000\n",
      "Time elapsed:  23791.69708132744\n",
      "ep 2670: ep_len:638 episode reward: total was 3.920000. running mean: -8.356612\n",
      "ep 2670: ep_len:501 episode reward: total was 9.330000. running mean: -8.179746\n",
      "ep 2670: ep_len:579 episode reward: total was -40.110000. running mean: -8.499048\n",
      "ep 2670: ep_len:514 episode reward: total was 35.170000. running mean: -8.062358\n",
      "ep 2670: ep_len:3 episode reward: total was 1.010000. running mean: -7.971634\n",
      "ep 2670: ep_len:500 episode reward: total was -13.380000. running mean: -8.025718\n",
      "ep 2670: ep_len:529 episode reward: total was -16.360000. running mean: -8.109061\n",
      "epsilon:0.081586 episode_count: 18697. steps_count: 8087419.000000\n",
      "Time elapsed:  23803.984223365784\n",
      "ep 2671: ep_len:500 episode reward: total was 11.430000. running mean: -7.913670\n",
      "ep 2671: ep_len:500 episode reward: total was 5.040000. running mean: -7.784133\n",
      "ep 2671: ep_len:628 episode reward: total was -13.240000. running mean: -7.838692\n",
      "ep 2671: ep_len:553 episode reward: total was 3.850000. running mean: -7.721805\n",
      "ep 2671: ep_len:77 episode reward: total was 24.180000. running mean: -7.402787\n",
      "ep 2671: ep_len:597 episode reward: total was -45.030000. running mean: -7.779059\n",
      "ep 2671: ep_len:554 episode reward: total was -0.140000. running mean: -7.702668\n",
      "epsilon:0.081541 episode_count: 18704. steps_count: 8090828.000000\n",
      "Time elapsed:  23812.68066930771\n",
      "ep 2672: ep_len:557 episode reward: total was -56.580000. running mean: -8.191442\n",
      "ep 2672: ep_len:628 episode reward: total was 52.420000. running mean: -7.585327\n",
      "ep 2672: ep_len:572 episode reward: total was -75.150000. running mean: -8.260974\n",
      "ep 2672: ep_len:53 episode reward: total was -12.720000. running mean: -8.305564\n",
      "ep 2672: ep_len:88 episode reward: total was -59.750000. running mean: -8.820009\n",
      "ep 2672: ep_len:323 episode reward: total was 23.540000. running mean: -8.496409\n",
      "ep 2672: ep_len:288 episode reward: total was -6.350000. running mean: -8.474945\n",
      "epsilon:0.081497 episode_count: 18711. steps_count: 8093337.000000\n",
      "Time elapsed:  23819.704286575317\n",
      "ep 2673: ep_len:500 episode reward: total was 44.910000. running mean: -7.941095\n",
      "ep 2673: ep_len:518 episode reward: total was -1.310000. running mean: -7.874784\n",
      "ep 2673: ep_len:569 episode reward: total was -14.780000. running mean: -7.943836\n",
      "ep 2673: ep_len:510 episode reward: total was 17.480000. running mean: -7.689598\n",
      "ep 2673: ep_len:3 episode reward: total was -1.500000. running mean: -7.627702\n",
      "ep 2673: ep_len:500 episode reward: total was 41.250000. running mean: -7.138925\n",
      "ep 2673: ep_len:597 episode reward: total was -16.600000. running mean: -7.233536\n",
      "epsilon:0.081453 episode_count: 18718. steps_count: 8096534.000000\n",
      "Time elapsed:  23828.162354707718\n",
      "ep 2674: ep_len:596 episode reward: total was 29.990000. running mean: -6.861300\n",
      "ep 2674: ep_len:516 episode reward: total was -97.470000. running mean: -7.767387\n",
      "ep 2674: ep_len:503 episode reward: total was 6.290000. running mean: -7.626813\n",
      "ep 2674: ep_len:151 episode reward: total was 8.610000. running mean: -7.464445\n",
      "ep 2674: ep_len:106 episode reward: total was 31.240000. running mean: -7.077401\n",
      "ep 2674: ep_len:500 episode reward: total was -41.380000. running mean: -7.420427\n",
      "ep 2674: ep_len:283 episode reward: total was -17.000000. running mean: -7.516223\n",
      "epsilon:0.081408 episode_count: 18725. steps_count: 8099189.000000\n",
      "Time elapsed:  23835.524509191513\n",
      "ep 2675: ep_len:95 episode reward: total was 11.410000. running mean: -7.326960\n",
      "ep 2675: ep_len:625 episode reward: total was 27.590000. running mean: -6.977791\n",
      "ep 2675: ep_len:510 episode reward: total was -31.360000. running mean: -7.221613\n",
      "ep 2675: ep_len:518 episode reward: total was 3.460000. running mean: -7.114797\n",
      "ep 2675: ep_len:50 episode reward: total was 22.000000. running mean: -6.823649\n",
      "ep 2675: ep_len:629 episode reward: total was 11.520000. running mean: -6.640212\n",
      "ep 2675: ep_len:548 episode reward: total was 9.330000. running mean: -6.480510\n",
      "epsilon:0.081364 episode_count: 18732. steps_count: 8102164.000000\n",
      "Time elapsed:  23848.08087706566\n",
      "ep 2676: ep_len:630 episode reward: total was 25.440000. running mean: -6.161305\n",
      "ep 2676: ep_len:500 episode reward: total was 13.000000. running mean: -5.969692\n",
      "ep 2676: ep_len:356 episode reward: total was 4.840000. running mean: -5.861595\n",
      "ep 2676: ep_len:43 episode reward: total was -7.270000. running mean: -5.875679\n",
      "ep 2676: ep_len:110 episode reward: total was 23.720000. running mean: -5.579722\n",
      "ep 2676: ep_len:577 episode reward: total was 7.780000. running mean: -5.446125\n",
      "ep 2676: ep_len:500 episode reward: total was -2.620000. running mean: -5.417864\n",
      "epsilon:0.081320 episode_count: 18739. steps_count: 8104880.000000\n",
      "Time elapsed:  23855.47741675377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2677: ep_len:531 episode reward: total was -35.140000. running mean: -5.715085\n",
      "ep 2677: ep_len:500 episode reward: total was -59.240000. running mean: -6.250334\n",
      "ep 2677: ep_len:553 episode reward: total was -76.880000. running mean: -6.956631\n",
      "ep 2677: ep_len:513 episode reward: total was -19.190000. running mean: -7.078965\n",
      "ep 2677: ep_len:3 episode reward: total was -0.490000. running mean: -7.013075\n",
      "ep 2677: ep_len:587 episode reward: total was -25.780000. running mean: -7.200744\n",
      "ep 2677: ep_len:555 episode reward: total was 3.340000. running mean: -7.095337\n",
      "epsilon:0.081275 episode_count: 18746. steps_count: 8108122.000000\n",
      "Time elapsed:  23864.03650689125\n",
      "ep 2678: ep_len:555 episode reward: total was -58.790000. running mean: -7.612284\n",
      "ep 2678: ep_len:501 episode reward: total was 74.950000. running mean: -6.786661\n",
      "ep 2678: ep_len:500 episode reward: total was -15.060000. running mean: -6.869394\n",
      "ep 2678: ep_len:536 episode reward: total was 28.500000. running mean: -6.515700\n",
      "ep 2678: ep_len:100 episode reward: total was -52.280000. running mean: -6.973343\n",
      "ep 2678: ep_len:312 episode reward: total was 6.530000. running mean: -6.838310\n",
      "ep 2678: ep_len:609 episode reward: total was -21.390000. running mean: -6.983827\n",
      "epsilon:0.081231 episode_count: 18753. steps_count: 8111235.000000\n",
      "Time elapsed:  23871.19509744644\n",
      "ep 2679: ep_len:587 episode reward: total was 48.030000. running mean: -6.433688\n",
      "ep 2679: ep_len:528 episode reward: total was 36.810000. running mean: -6.001251\n",
      "ep 2679: ep_len:579 episode reward: total was -20.860000. running mean: -6.149839\n",
      "ep 2679: ep_len:500 episode reward: total was 46.170000. running mean: -5.626641\n",
      "ep 2679: ep_len:87 episode reward: total was 20.670000. running mean: -5.363674\n",
      "ep 2679: ep_len:500 episode reward: total was -62.290000. running mean: -5.932937\n",
      "ep 2679: ep_len:599 episode reward: total was 1.560000. running mean: -5.858008\n",
      "epsilon:0.081187 episode_count: 18760. steps_count: 8114615.000000\n",
      "Time elapsed:  23879.96746659279\n",
      "ep 2680: ep_len:681 episode reward: total was -48.540000. running mean: -6.284828\n",
      "ep 2680: ep_len:500 episode reward: total was 20.390000. running mean: -6.018080\n",
      "ep 2680: ep_len:567 episode reward: total was -44.930000. running mean: -6.407199\n",
      "ep 2680: ep_len:366 episode reward: total was -9.740000. running mean: -6.440527\n",
      "ep 2680: ep_len:3 episode reward: total was 1.010000. running mean: -6.366022\n",
      "ep 2680: ep_len:604 episode reward: total was 22.820000. running mean: -6.074161\n",
      "ep 2680: ep_len:571 episode reward: total was 15.040000. running mean: -5.863020\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.081142 episode_count: 18767. steps_count: 8117907.000000\n",
      "Time elapsed:  23895.346057653427\n",
      "ep 2681: ep_len:534 episode reward: total was -53.420000. running mean: -6.338590\n",
      "ep 2681: ep_len:580 episode reward: total was -2.320000. running mean: -6.298404\n",
      "ep 2681: ep_len:553 episode reward: total was 8.860000. running mean: -6.146820\n",
      "ep 2681: ep_len:509 episode reward: total was 12.200000. running mean: -5.963351\n",
      "ep 2681: ep_len:3 episode reward: total was 1.010000. running mean: -5.893618\n",
      "ep 2681: ep_len:178 episode reward: total was 15.220000. running mean: -5.682482\n",
      "ep 2681: ep_len:317 episode reward: total was 4.840000. running mean: -5.577257\n",
      "epsilon:0.081098 episode_count: 18774. steps_count: 8120581.000000\n",
      "Time elapsed:  23902.141995429993\n",
      "ep 2682: ep_len:500 episode reward: total was 14.400000. running mean: -5.377484\n",
      "ep 2682: ep_len:632 episode reward: total was -36.300000. running mean: -5.686710\n",
      "ep 2682: ep_len:578 episode reward: total was -56.510000. running mean: -6.194942\n",
      "ep 2682: ep_len:56 episode reward: total was -0.220000. running mean: -6.135193\n",
      "ep 2682: ep_len:91 episode reward: total was 18.720000. running mean: -5.886641\n",
      "ep 2682: ep_len:592 episode reward: total was 22.970000. running mean: -5.598075\n",
      "ep 2682: ep_len:281 episode reward: total was -33.120000. running mean: -5.873294\n",
      "epsilon:0.081054 episode_count: 18781. steps_count: 8123311.000000\n",
      "Time elapsed:  23909.522533893585\n",
      "ep 2683: ep_len:551 episode reward: total was 1.460000. running mean: -5.799961\n",
      "ep 2683: ep_len:552 episode reward: total was -9.640000. running mean: -5.838361\n",
      "ep 2683: ep_len:60 episode reward: total was 3.250000. running mean: -5.747478\n",
      "ep 2683: ep_len:391 episode reward: total was -36.050000. running mean: -6.050503\n",
      "ep 2683: ep_len:3 episode reward: total was -0.490000. running mean: -5.994898\n",
      "ep 2683: ep_len:299 episode reward: total was -36.270000. running mean: -6.297649\n",
      "ep 2683: ep_len:608 episode reward: total was 9.810000. running mean: -6.136572\n",
      "epsilon:0.081009 episode_count: 18788. steps_count: 8125775.000000\n",
      "Time elapsed:  23916.376634836197\n",
      "ep 2684: ep_len:551 episode reward: total was 45.720000. running mean: -5.618007\n",
      "ep 2684: ep_len:536 episode reward: total was 4.130000. running mean: -5.520527\n",
      "ep 2684: ep_len:569 episode reward: total was -41.940000. running mean: -5.884721\n",
      "ep 2684: ep_len:518 episode reward: total was 16.290000. running mean: -5.662974\n",
      "ep 2684: ep_len:89 episode reward: total was 20.750000. running mean: -5.398844\n",
      "ep 2684: ep_len:500 episode reward: total was -35.070000. running mean: -5.695556\n",
      "ep 2684: ep_len:205 episode reward: total was -9.970000. running mean: -5.738300\n",
      "epsilon:0.080965 episode_count: 18795. steps_count: 8128743.000000\n",
      "Time elapsed:  23925.06316447258\n",
      "ep 2685: ep_len:634 episode reward: total was 13.190000. running mean: -5.549017\n",
      "ep 2685: ep_len:583 episode reward: total was 39.640000. running mean: -5.097127\n",
      "ep 2685: ep_len:532 episode reward: total was -4.150000. running mean: -5.087656\n",
      "ep 2685: ep_len:507 episode reward: total was -22.950000. running mean: -5.266279\n",
      "ep 2685: ep_len:49 episode reward: total was 21.010000. running mean: -5.003517\n",
      "ep 2685: ep_len:500 episode reward: total was 40.850000. running mean: -4.544981\n",
      "ep 2685: ep_len:601 episode reward: total was 21.630000. running mean: -4.283232\n",
      "epsilon:0.080921 episode_count: 18802. steps_count: 8132149.000000\n",
      "Time elapsed:  23935.499408721924\n",
      "ep 2686: ep_len:614 episode reward: total was 22.560000. running mean: -4.014799\n",
      "ep 2686: ep_len:608 episode reward: total was -11.450000. running mean: -4.089151\n",
      "ep 2686: ep_len:649 episode reward: total was -8.830000. running mean: -4.136560\n",
      "ep 2686: ep_len:414 episode reward: total was -9.510000. running mean: -4.190294\n",
      "ep 2686: ep_len:3 episode reward: total was 1.010000. running mean: -4.138291\n",
      "ep 2686: ep_len:516 episode reward: total was 6.820000. running mean: -4.028708\n",
      "ep 2686: ep_len:503 episode reward: total was -37.360000. running mean: -4.362021\n",
      "epsilon:0.080876 episode_count: 18809. steps_count: 8135456.000000\n",
      "Time elapsed:  23943.614737272263\n",
      "ep 2687: ep_len:526 episode reward: total was 67.590000. running mean: -3.642501\n",
      "ep 2687: ep_len:569 episode reward: total was 37.050000. running mean: -3.235576\n",
      "ep 2687: ep_len:618 episode reward: total was -36.590000. running mean: -3.569120\n",
      "ep 2687: ep_len:510 episode reward: total was -11.780000. running mean: -3.651229\n",
      "ep 2687: ep_len:3 episode reward: total was 1.010000. running mean: -3.604617\n",
      "ep 2687: ep_len:580 episode reward: total was -22.270000. running mean: -3.791271\n",
      "ep 2687: ep_len:500 episode reward: total was -36.030000. running mean: -4.113658\n",
      "epsilon:0.080832 episode_count: 18816. steps_count: 8138762.000000\n",
      "Time elapsed:  23948.72787117958\n",
      "ep 2688: ep_len:561 episode reward: total was 52.810000. running mean: -3.544421\n",
      "ep 2688: ep_len:158 episode reward: total was -25.350000. running mean: -3.762477\n",
      "ep 2688: ep_len:609 episode reward: total was 0.770000. running mean: -3.717152\n",
      "ep 2688: ep_len:604 episode reward: total was -39.770000. running mean: -4.077681\n",
      "ep 2688: ep_len:3 episode reward: total was 1.010000. running mean: -4.026804\n",
      "ep 2688: ep_len:500 episode reward: total was -23.010000. running mean: -4.216636\n",
      "ep 2688: ep_len:524 episode reward: total was 10.570000. running mean: -4.068770\n",
      "epsilon:0.080788 episode_count: 18823. steps_count: 8141721.000000\n",
      "Time elapsed:  23954.653381109238\n",
      "ep 2689: ep_len:545 episode reward: total was 47.680000. running mean: -3.551282\n",
      "ep 2689: ep_len:500 episode reward: total was 20.350000. running mean: -3.312269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2689: ep_len:543 episode reward: total was -35.690000. running mean: -3.636046\n",
      "ep 2689: ep_len:130 episode reward: total was 16.580000. running mean: -3.433886\n",
      "ep 2689: ep_len:3 episode reward: total was 1.010000. running mean: -3.389447\n",
      "ep 2689: ep_len:617 episode reward: total was 11.430000. running mean: -3.241253\n",
      "ep 2689: ep_len:198 episode reward: total was -13.620000. running mean: -3.345040\n",
      "epsilon:0.080743 episode_count: 18830. steps_count: 8144257.000000\n",
      "Time elapsed:  23960.729047060013\n",
      "ep 2690: ep_len:577 episode reward: total was -30.730000. running mean: -3.618890\n",
      "ep 2690: ep_len:514 episode reward: total was -35.510000. running mean: -3.937801\n",
      "ep 2690: ep_len:79 episode reward: total was 8.800000. running mean: -3.810423\n",
      "ep 2690: ep_len:519 episode reward: total was 5.850000. running mean: -3.713819\n",
      "ep 2690: ep_len:3 episode reward: total was 1.010000. running mean: -3.666580\n",
      "ep 2690: ep_len:614 episode reward: total was -27.260000. running mean: -3.902515\n",
      "ep 2690: ep_len:530 episode reward: total was -3.970000. running mean: -3.903189\n",
      "epsilon:0.080699 episode_count: 18837. steps_count: 8147093.000000\n",
      "Time elapsed:  23968.332234621048\n",
      "ep 2691: ep_len:217 episode reward: total was -4.420000. running mean: -3.908358\n",
      "ep 2691: ep_len:514 episode reward: total was -12.820000. running mean: -3.997474\n",
      "ep 2691: ep_len:525 episode reward: total was -2.530000. running mean: -3.982799\n",
      "ep 2691: ep_len:517 episode reward: total was 47.980000. running mean: -3.463171\n",
      "ep 2691: ep_len:3 episode reward: total was 0.000000. running mean: -3.428540\n",
      "ep 2691: ep_len:670 episode reward: total was 3.570000. running mean: -3.358554\n",
      "ep 2691: ep_len:569 episode reward: total was 23.680000. running mean: -3.088169\n",
      "epsilon:0.080655 episode_count: 18844. steps_count: 8150108.000000\n",
      "Time elapsed:  23980.29641342163\n",
      "ep 2692: ep_len:602 episode reward: total was 21.550000. running mean: -2.841787\n",
      "ep 2692: ep_len:547 episode reward: total was 11.700000. running mean: -2.696369\n",
      "ep 2692: ep_len:500 episode reward: total was -1.760000. running mean: -2.687005\n",
      "ep 2692: ep_len:513 episode reward: total was 62.400000. running mean: -2.036135\n",
      "ep 2692: ep_len:128 episode reward: total was 25.370000. running mean: -1.762074\n",
      "ep 2692: ep_len:212 episode reward: total was 30.760000. running mean: -1.436853\n",
      "ep 2692: ep_len:500 episode reward: total was -16.540000. running mean: -1.587885\n",
      "epsilon:0.080610 episode_count: 18851. steps_count: 8153110.000000\n",
      "Time elapsed:  23988.261375427246\n",
      "ep 2693: ep_len:524 episode reward: total was -40.370000. running mean: -1.975706\n",
      "ep 2693: ep_len:500 episode reward: total was -4.080000. running mean: -1.996749\n",
      "ep 2693: ep_len:385 episode reward: total was 16.950000. running mean: -1.807281\n",
      "ep 2693: ep_len:626 episode reward: total was -30.380000. running mean: -2.093008\n",
      "ep 2693: ep_len:83 episode reward: total was 19.190000. running mean: -1.880178\n",
      "ep 2693: ep_len:500 episode reward: total was -9.120000. running mean: -1.952577\n",
      "ep 2693: ep_len:283 episode reward: total was -32.670000. running mean: -2.259751\n",
      "epsilon:0.080566 episode_count: 18858. steps_count: 8156011.000000\n",
      "Time elapsed:  23996.37483882904\n",
      "ep 2694: ep_len:500 episode reward: total was 26.680000. running mean: -1.970353\n",
      "ep 2694: ep_len:576 episode reward: total was -35.770000. running mean: -2.308350\n",
      "ep 2694: ep_len:534 episode reward: total was 23.420000. running mean: -2.051066\n",
      "ep 2694: ep_len:131 episode reward: total was 14.570000. running mean: -1.884856\n",
      "ep 2694: ep_len:48 episode reward: total was 22.010000. running mean: -1.645907\n",
      "ep 2694: ep_len:500 episode reward: total was 0.190000. running mean: -1.627548\n",
      "ep 2694: ep_len:509 episode reward: total was 14.180000. running mean: -1.469473\n",
      "epsilon:0.080522 episode_count: 18865. steps_count: 8158809.000000\n",
      "Time elapsed:  24003.93443107605\n",
      "ep 2695: ep_len:584 episode reward: total was -44.540000. running mean: -1.900178\n",
      "ep 2695: ep_len:608 episode reward: total was 2.100000. running mean: -1.860176\n",
      "ep 2695: ep_len:540 episode reward: total was 27.370000. running mean: -1.567874\n",
      "ep 2695: ep_len:508 episode reward: total was 20.860000. running mean: -1.343596\n",
      "ep 2695: ep_len:96 episode reward: total was -51.740000. running mean: -1.847560\n",
      "ep 2695: ep_len:611 episode reward: total was 17.510000. running mean: -1.653984\n",
      "ep 2695: ep_len:594 episode reward: total was 13.100000. running mean: -1.506444\n",
      "epsilon:0.080477 episode_count: 18872. steps_count: 8162350.000000\n",
      "Time elapsed:  24015.433495759964\n",
      "ep 2696: ep_len:217 episode reward: total was 17.190000. running mean: -1.319480\n",
      "ep 2696: ep_len:500 episode reward: total was 79.620000. running mean: -0.510085\n",
      "ep 2696: ep_len:531 episode reward: total was -59.240000. running mean: -1.097384\n",
      "ep 2696: ep_len:500 episode reward: total was 25.160000. running mean: -0.834810\n",
      "ep 2696: ep_len:88 episode reward: total was -47.900000. running mean: -1.305462\n",
      "ep 2696: ep_len:316 episode reward: total was 17.710000. running mean: -1.115307\n",
      "ep 2696: ep_len:507 episode reward: total was 28.400000. running mean: -0.820154\n",
      "epsilon:0.080433 episode_count: 18879. steps_count: 8165009.000000\n",
      "Time elapsed:  24022.19979786873\n",
      "ep 2697: ep_len:119 episode reward: total was -5.060000. running mean: -0.862553\n",
      "ep 2697: ep_len:575 episode reward: total was -18.300000. running mean: -1.036927\n",
      "ep 2697: ep_len:531 episode reward: total was -142.420000. running mean: -2.450758\n",
      "ep 2697: ep_len:604 episode reward: total was -3.980000. running mean: -2.466050\n",
      "ep 2697: ep_len:106 episode reward: total was 9.350000. running mean: -2.347890\n",
      "ep 2697: ep_len:522 episode reward: total was -49.410000. running mean: -2.818511\n",
      "ep 2697: ep_len:208 episode reward: total was -18.080000. running mean: -2.971126\n",
      "epsilon:0.080389 episode_count: 18886. steps_count: 8167674.000000\n",
      "Time elapsed:  24028.60650897026\n",
      "ep 2698: ep_len:605 episode reward: total was 34.680000. running mean: -2.594615\n",
      "ep 2698: ep_len:533 episode reward: total was -0.880000. running mean: -2.577469\n",
      "ep 2698: ep_len:381 episode reward: total was 23.090000. running mean: -2.320794\n",
      "ep 2698: ep_len:522 episode reward: total was 26.190000. running mean: -2.035686\n",
      "ep 2698: ep_len:3 episode reward: total was 1.010000. running mean: -2.005229\n",
      "ep 2698: ep_len:511 episode reward: total was 0.490000. running mean: -1.980277\n",
      "ep 2698: ep_len:591 episode reward: total was 7.490000. running mean: -1.885574\n",
      "epsilon:0.080344 episode_count: 18893. steps_count: 8170820.000000\n",
      "Time elapsed:  24041.366014003754\n",
      "ep 2699: ep_len:591 episode reward: total was -67.690000. running mean: -2.543618\n",
      "ep 2699: ep_len:512 episode reward: total was 39.680000. running mean: -2.121382\n",
      "ep 2699: ep_len:500 episode reward: total was -36.750000. running mean: -2.467668\n",
      "ep 2699: ep_len:502 episode reward: total was 49.910000. running mean: -1.943892\n",
      "ep 2699: ep_len:3 episode reward: total was 0.000000. running mean: -1.924453\n",
      "ep 2699: ep_len:321 episode reward: total was 19.170000. running mean: -1.713508\n",
      "ep 2699: ep_len:541 episode reward: total was -34.110000. running mean: -2.037473\n",
      "epsilon:0.080300 episode_count: 18900. steps_count: 8173790.000000\n",
      "Time elapsed:  24049.374587535858\n",
      "ep 2700: ep_len:664 episode reward: total was -76.350000. running mean: -2.780598\n",
      "ep 2700: ep_len:295 episode reward: total was -12.410000. running mean: -2.876892\n",
      "ep 2700: ep_len:558 episode reward: total was -0.580000. running mean: -2.853923\n",
      "ep 2700: ep_len:565 episode reward: total was 60.630000. running mean: -2.219084\n",
      "ep 2700: ep_len:3 episode reward: total was 1.010000. running mean: -2.186793\n",
      "ep 2700: ep_len:500 episode reward: total was -19.640000. running mean: -2.361325\n",
      "ep 2700: ep_len:609 episode reward: total was -25.230000. running mean: -2.590012\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.080256 episode_count: 18907. steps_count: 8176984.000000\n",
      "Time elapsed:  24062.722744464874\n",
      "ep 2701: ep_len:531 episode reward: total was -106.320000. running mean: -3.627312\n",
      "ep 2701: ep_len:575 episode reward: total was 34.170000. running mean: -3.249339\n",
      "ep 2701: ep_len:634 episode reward: total was 4.950000. running mean: -3.167346\n",
      "ep 2701: ep_len:516 episode reward: total was -54.230000. running mean: -3.677972\n",
      "ep 2701: ep_len:3 episode reward: total was 1.010000. running mean: -3.631092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2701: ep_len:310 episode reward: total was 18.140000. running mean: -3.413381\n",
      "ep 2701: ep_len:588 episode reward: total was 2.620000. running mean: -3.353048\n",
      "epsilon:0.080211 episode_count: 18914. steps_count: 8180141.000000\n",
      "Time elapsed:  24079.533474445343\n",
      "ep 2702: ep_len:594 episode reward: total was 19.500000. running mean: -3.124517\n",
      "ep 2702: ep_len:501 episode reward: total was 12.880000. running mean: -2.964472\n",
      "ep 2702: ep_len:595 episode reward: total was -18.340000. running mean: -3.118227\n",
      "ep 2702: ep_len:415 episode reward: total was 10.210000. running mean: -2.984945\n",
      "ep 2702: ep_len:110 episode reward: total was -29.700000. running mean: -3.252096\n",
      "ep 2702: ep_len:575 episode reward: total was -21.680000. running mean: -3.436375\n",
      "ep 2702: ep_len:528 episode reward: total was -73.300000. running mean: -4.135011\n",
      "epsilon:0.080167 episode_count: 18921. steps_count: 8183459.000000\n",
      "Time elapsed:  24088.491646766663\n",
      "ep 2703: ep_len:630 episode reward: total was -0.840000. running mean: -4.102061\n",
      "ep 2703: ep_len:646 episode reward: total was -106.740000. running mean: -5.128440\n",
      "ep 2703: ep_len:501 episode reward: total was -20.330000. running mean: -5.280456\n",
      "ep 2703: ep_len:56 episode reward: total was 1.830000. running mean: -5.209351\n",
      "ep 2703: ep_len:127 episode reward: total was -12.620000. running mean: -5.283458\n",
      "ep 2703: ep_len:500 episode reward: total was -8.530000. running mean: -5.315923\n",
      "ep 2703: ep_len:598 episode reward: total was -45.960000. running mean: -5.722364\n",
      "epsilon:0.080123 episode_count: 18928. steps_count: 8186517.000000\n",
      "Time elapsed:  24096.6329703331\n",
      "ep 2704: ep_len:500 episode reward: total was -47.750000. running mean: -6.142640\n",
      "ep 2704: ep_len:500 episode reward: total was 15.680000. running mean: -5.924414\n",
      "ep 2704: ep_len:588 episode reward: total was -4.920000. running mean: -5.914370\n",
      "ep 2704: ep_len:618 episode reward: total was 5.850000. running mean: -5.796726\n",
      "ep 2704: ep_len:49 episode reward: total was 19.510000. running mean: -5.543659\n",
      "ep 2704: ep_len:669 episode reward: total was 15.130000. running mean: -5.336922\n",
      "ep 2704: ep_len:503 episode reward: total was 23.580000. running mean: -5.047753\n",
      "epsilon:0.080078 episode_count: 18935. steps_count: 8189944.000000\n",
      "Time elapsed:  24105.75279021263\n",
      "ep 2705: ep_len:598 episode reward: total was -78.950000. running mean: -5.786775\n",
      "ep 2705: ep_len:503 episode reward: total was 68.630000. running mean: -5.042608\n",
      "ep 2705: ep_len:680 episode reward: total was -39.730000. running mean: -5.389482\n",
      "ep 2705: ep_len:577 episode reward: total was 14.110000. running mean: -5.194487\n",
      "ep 2705: ep_len:97 episode reward: total was 15.730000. running mean: -4.985242\n",
      "ep 2705: ep_len:524 episode reward: total was 3.970000. running mean: -4.895689\n",
      "ep 2705: ep_len:553 episode reward: total was -26.040000. running mean: -5.107133\n",
      "epsilon:0.080034 episode_count: 18942. steps_count: 8193476.000000\n",
      "Time elapsed:  24115.096645116806\n",
      "ep 2706: ep_len:500 episode reward: total was 37.730000. running mean: -4.678761\n",
      "ep 2706: ep_len:500 episode reward: total was 11.650000. running mean: -4.515474\n",
      "ep 2706: ep_len:630 episode reward: total was -2.690000. running mean: -4.497219\n",
      "ep 2706: ep_len:500 episode reward: total was 13.990000. running mean: -4.312347\n",
      "ep 2706: ep_len:3 episode reward: total was 1.010000. running mean: -4.259123\n",
      "ep 2706: ep_len:607 episode reward: total was 6.130000. running mean: -4.155232\n",
      "ep 2706: ep_len:500 episode reward: total was -4.550000. running mean: -4.159180\n",
      "epsilon:0.079990 episode_count: 18949. steps_count: 8196716.000000\n",
      "Time elapsed:  24123.74899148941\n",
      "ep 2707: ep_len:500 episode reward: total was 50.360000. running mean: -3.613988\n",
      "ep 2707: ep_len:614 episode reward: total was -63.500000. running mean: -4.212848\n",
      "ep 2707: ep_len:500 episode reward: total was 2.590000. running mean: -4.144820\n",
      "ep 2707: ep_len:51 episode reward: total was -1.680000. running mean: -4.120171\n",
      "ep 2707: ep_len:3 episode reward: total was 1.010000. running mean: -4.068870\n",
      "ep 2707: ep_len:500 episode reward: total was 2.400000. running mean: -4.004181\n",
      "ep 2707: ep_len:504 episode reward: total was 1.500000. running mean: -3.949139\n",
      "epsilon:0.079945 episode_count: 18956. steps_count: 8199388.000000\n",
      "Time elapsed:  24130.987095594406\n",
      "ep 2708: ep_len:500 episode reward: total was 62.080000. running mean: -3.288848\n",
      "ep 2708: ep_len:527 episode reward: total was 4.190000. running mean: -3.214059\n",
      "ep 2708: ep_len:654 episode reward: total was -82.220000. running mean: -4.004119\n",
      "ep 2708: ep_len:53 episode reward: total was -17.240000. running mean: -4.136477\n",
      "ep 2708: ep_len:3 episode reward: total was 1.010000. running mean: -4.085013\n",
      "ep 2708: ep_len:682 episode reward: total was 11.960000. running mean: -3.924563\n",
      "ep 2708: ep_len:630 episode reward: total was 13.200000. running mean: -3.753317\n",
      "epsilon:0.079901 episode_count: 18963. steps_count: 8202437.000000\n",
      "Time elapsed:  24139.13383579254\n",
      "ep 2709: ep_len:574 episode reward: total was 24.110000. running mean: -3.474684\n",
      "ep 2709: ep_len:500 episode reward: total was 0.980000. running mean: -3.430137\n",
      "ep 2709: ep_len:560 episode reward: total was -21.340000. running mean: -3.609236\n",
      "ep 2709: ep_len:500 episode reward: total was 20.870000. running mean: -3.364443\n",
      "ep 2709: ep_len:3 episode reward: total was 1.010000. running mean: -3.320699\n",
      "ep 2709: ep_len:557 episode reward: total was 1.380000. running mean: -3.273692\n",
      "ep 2709: ep_len:572 episode reward: total was -59.830000. running mean: -3.839255\n",
      "epsilon:0.079857 episode_count: 18970. steps_count: 8205703.000000\n",
      "Time elapsed:  24148.14760875702\n",
      "ep 2710: ep_len:598 episode reward: total was 52.180000. running mean: -3.279062\n",
      "ep 2710: ep_len:523 episode reward: total was -276.470000. running mean: -6.010972\n",
      "ep 2710: ep_len:434 episode reward: total was 34.260000. running mean: -5.608262\n",
      "ep 2710: ep_len:359 episode reward: total was -21.560000. running mean: -5.767779\n",
      "ep 2710: ep_len:3 episode reward: total was 1.010000. running mean: -5.700002\n",
      "ep 2710: ep_len:501 episode reward: total was 18.050000. running mean: -5.462502\n",
      "ep 2710: ep_len:569 episode reward: total was 17.130000. running mean: -5.236577\n",
      "epsilon:0.079812 episode_count: 18977. steps_count: 8208690.000000\n",
      "Time elapsed:  24155.81301665306\n",
      "ep 2711: ep_len:526 episode reward: total was 3.660000. running mean: -5.147611\n",
      "ep 2711: ep_len:590 episode reward: total was -2.080000. running mean: -5.116935\n",
      "ep 2711: ep_len:625 episode reward: total was 0.970000. running mean: -5.056065\n",
      "ep 2711: ep_len:547 episode reward: total was 34.960000. running mean: -4.655905\n",
      "ep 2711: ep_len:54 episode reward: total was 19.500000. running mean: -4.414346\n",
      "ep 2711: ep_len:603 episode reward: total was -37.350000. running mean: -4.743702\n",
      "ep 2711: ep_len:615 episode reward: total was -8.700000. running mean: -4.783265\n",
      "epsilon:0.079768 episode_count: 18984. steps_count: 8212250.000000\n",
      "Time elapsed:  24169.833651542664\n",
      "ep 2712: ep_len:526 episode reward: total was -41.410000. running mean: -5.149532\n",
      "ep 2712: ep_len:569 episode reward: total was 23.530000. running mean: -4.862737\n",
      "ep 2712: ep_len:542 episode reward: total was -19.260000. running mean: -5.006710\n",
      "ep 2712: ep_len:569 episode reward: total was 55.320000. running mean: -4.403443\n",
      "ep 2712: ep_len:80 episode reward: total was 23.660000. running mean: -4.122808\n",
      "ep 2712: ep_len:512 episode reward: total was -19.270000. running mean: -4.274280\n",
      "ep 2712: ep_len:168 episode reward: total was -14.640000. running mean: -4.377937\n",
      "epsilon:0.079724 episode_count: 18991. steps_count: 8215216.000000\n",
      "Time elapsed:  24177.643062353134\n",
      "ep 2713: ep_len:227 episode reward: total was 26.760000. running mean: -4.066558\n",
      "ep 2713: ep_len:500 episode reward: total was -0.060000. running mean: -4.026492\n",
      "ep 2713: ep_len:629 episode reward: total was -25.340000. running mean: -4.239627\n",
      "ep 2713: ep_len:605 episode reward: total was 67.740000. running mean: -3.519831\n",
      "ep 2713: ep_len:110 episode reward: total was 23.750000. running mean: -3.247133\n",
      "ep 2713: ep_len:150 episode reward: total was 24.000000. running mean: -2.974662\n",
      "ep 2713: ep_len:697 episode reward: total was -192.580000. running mean: -4.870715\n",
      "epsilon:0.079679 episode_count: 18998. steps_count: 8218134.000000\n",
      "Time elapsed:  24185.39428782463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2714: ep_len:559 episode reward: total was 53.090000. running mean: -4.291108\n",
      "ep 2714: ep_len:500 episode reward: total was 26.540000. running mean: -3.982797\n",
      "ep 2714: ep_len:632 episode reward: total was -33.360000. running mean: -4.276569\n",
      "ep 2714: ep_len:518 episode reward: total was -42.160000. running mean: -4.655403\n",
      "ep 2714: ep_len:51 episode reward: total was 19.010000. running mean: -4.418749\n",
      "ep 2714: ep_len:500 episode reward: total was 18.470000. running mean: -4.189862\n",
      "ep 2714: ep_len:624 episode reward: total was 5.040000. running mean: -4.097563\n",
      "epsilon:0.079635 episode_count: 19005. steps_count: 8221518.000000\n",
      "Time elapsed:  24194.30415058136\n",
      "ep 2715: ep_len:558 episode reward: total was -29.110000. running mean: -4.347687\n",
      "ep 2715: ep_len:500 episode reward: total was 32.510000. running mean: -3.979110\n",
      "ep 2715: ep_len:57 episode reward: total was 5.180000. running mean: -3.887519\n",
      "ep 2715: ep_len:160 episode reward: total was 17.630000. running mean: -3.672344\n",
      "ep 2715: ep_len:3 episode reward: total was 1.010000. running mean: -3.625521\n",
      "ep 2715: ep_len:328 episode reward: total was 5.770000. running mean: -3.531565\n",
      "ep 2715: ep_len:500 episode reward: total was 23.550000. running mean: -3.260750\n",
      "epsilon:0.079591 episode_count: 19012. steps_count: 8223624.000000\n",
      "Time elapsed:  24206.912350177765\n",
      "ep 2716: ep_len:500 episode reward: total was 77.110000. running mean: -2.457042\n",
      "ep 2716: ep_len:640 episode reward: total was 28.170000. running mean: -2.150772\n",
      "ep 2716: ep_len:500 episode reward: total was -35.170000. running mean: -2.480964\n",
      "ep 2716: ep_len:165 episode reward: total was 14.680000. running mean: -2.309355\n",
      "ep 2716: ep_len:56 episode reward: total was 26.010000. running mean: -2.026161\n",
      "ep 2716: ep_len:690 episode reward: total was -178.070000. running mean: -3.786599\n",
      "ep 2716: ep_len:286 episode reward: total was -15.690000. running mean: -3.905633\n",
      "epsilon:0.079546 episode_count: 19019. steps_count: 8226461.000000\n",
      "Time elapsed:  24219.371030807495\n",
      "ep 2717: ep_len:586 episode reward: total was 25.210000. running mean: -3.614477\n",
      "ep 2717: ep_len:511 episode reward: total was 18.480000. running mean: -3.393532\n",
      "ep 2717: ep_len:636 episode reward: total was -44.370000. running mean: -3.803297\n",
      "ep 2717: ep_len:559 episode reward: total was 1.240000. running mean: -3.752864\n",
      "ep 2717: ep_len:3 episode reward: total was 1.010000. running mean: -3.705235\n",
      "ep 2717: ep_len:501 episode reward: total was 0.920000. running mean: -3.658983\n",
      "ep 2717: ep_len:509 episode reward: total was 3.230000. running mean: -3.590093\n",
      "epsilon:0.079502 episode_count: 19026. steps_count: 8229766.000000\n",
      "Time elapsed:  24231.96830558777\n",
      "ep 2718: ep_len:220 episode reward: total was -21.290000. running mean: -3.767092\n",
      "ep 2718: ep_len:500 episode reward: total was 28.880000. running mean: -3.440621\n",
      "ep 2718: ep_len:606 episode reward: total was -9.120000. running mean: -3.497415\n",
      "ep 2718: ep_len:422 episode reward: total was 2.810000. running mean: -3.434341\n",
      "ep 2718: ep_len:55 episode reward: total was 21.010000. running mean: -3.189898\n",
      "ep 2718: ep_len:577 episode reward: total was -9.230000. running mean: -3.250299\n",
      "ep 2718: ep_len:500 episode reward: total was 3.590000. running mean: -3.181896\n",
      "epsilon:0.079458 episode_count: 19033. steps_count: 8232646.000000\n",
      "Time elapsed:  24239.749581098557\n",
      "ep 2719: ep_len:590 episode reward: total was -77.070000. running mean: -3.920777\n",
      "ep 2719: ep_len:500 episode reward: total was 21.770000. running mean: -3.663869\n",
      "ep 2719: ep_len:632 episode reward: total was -0.850000. running mean: -3.635730\n",
      "ep 2719: ep_len:148 episode reward: total was 4.620000. running mean: -3.553173\n",
      "ep 2719: ep_len:55 episode reward: total was 18.010000. running mean: -3.337541\n",
      "ep 2719: ep_len:523 episode reward: total was 0.550000. running mean: -3.298666\n",
      "ep 2719: ep_len:559 episode reward: total was -5.100000. running mean: -3.316679\n",
      "epsilon:0.079413 episode_count: 19040. steps_count: 8235653.000000\n",
      "Time elapsed:  24247.249976158142\n",
      "ep 2720: ep_len:514 episode reward: total was -232.170000. running mean: -5.605212\n",
      "ep 2720: ep_len:514 episode reward: total was -24.670000. running mean: -5.795860\n",
      "ep 2720: ep_len:500 episode reward: total was -167.170000. running mean: -7.409602\n",
      "ep 2720: ep_len:502 episode reward: total was 32.770000. running mean: -7.007806\n",
      "ep 2720: ep_len:3 episode reward: total was 1.010000. running mean: -6.927627\n",
      "ep 2720: ep_len:663 episode reward: total was -144.890000. running mean: -8.307251\n",
      "ep 2720: ep_len:543 episode reward: total was 2.040000. running mean: -8.203779\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.079369 episode_count: 19047. steps_count: 8238892.000000\n",
      "Time elapsed:  24258.727605819702\n",
      "ep 2721: ep_len:555 episode reward: total was 47.350000. running mean: -7.648241\n",
      "ep 2721: ep_len:514 episode reward: total was -3.380000. running mean: -7.605559\n",
      "ep 2721: ep_len:518 episode reward: total was -18.310000. running mean: -7.712603\n",
      "ep 2721: ep_len:162 episode reward: total was 11.070000. running mean: -7.524777\n",
      "ep 2721: ep_len:3 episode reward: total was 1.010000. running mean: -7.439429\n",
      "ep 2721: ep_len:504 episode reward: total was 11.550000. running mean: -7.249535\n",
      "ep 2721: ep_len:526 episode reward: total was 20.680000. running mean: -6.970239\n",
      "epsilon:0.079325 episode_count: 19054. steps_count: 8241674.000000\n",
      "Time elapsed:  24266.334821224213\n",
      "ep 2722: ep_len:500 episode reward: total was 57.430000. running mean: -6.326237\n",
      "ep 2722: ep_len:626 episode reward: total was -24.220000. running mean: -6.505175\n",
      "ep 2722: ep_len:427 episode reward: total was 45.240000. running mean: -5.987723\n",
      "ep 2722: ep_len:500 episode reward: total was 45.700000. running mean: -5.470846\n",
      "ep 2722: ep_len:3 episode reward: total was 1.010000. running mean: -5.406037\n",
      "ep 2722: ep_len:583 episode reward: total was 7.700000. running mean: -5.274977\n",
      "ep 2722: ep_len:603 episode reward: total was -13.060000. running mean: -5.352827\n",
      "epsilon:0.079280 episode_count: 19061. steps_count: 8244916.000000\n",
      "Time elapsed:  24274.900321245193\n",
      "ep 2723: ep_len:524 episode reward: total was 60.490000. running mean: -4.694399\n",
      "ep 2723: ep_len:168 episode reward: total was 9.280000. running mean: -4.554655\n",
      "ep 2723: ep_len:561 episode reward: total was -26.520000. running mean: -4.774308\n",
      "ep 2723: ep_len:500 episode reward: total was 7.220000. running mean: -4.654365\n",
      "ep 2723: ep_len:101 episode reward: total was 28.250000. running mean: -4.325322\n",
      "ep 2723: ep_len:557 episode reward: total was -13.250000. running mean: -4.414568\n",
      "ep 2723: ep_len:526 episode reward: total was -13.320000. running mean: -4.503623\n",
      "epsilon:0.079236 episode_count: 19068. steps_count: 8247853.000000\n",
      "Time elapsed:  24282.84033060074\n",
      "ep 2724: ep_len:123 episode reward: total was 0.460000. running mean: -4.453986\n",
      "ep 2724: ep_len:518 episode reward: total was -0.050000. running mean: -4.409947\n",
      "ep 2724: ep_len:561 episode reward: total was -18.480000. running mean: -4.550647\n",
      "ep 2724: ep_len:536 episode reward: total was 6.200000. running mean: -4.443141\n",
      "ep 2724: ep_len:92 episode reward: total was 22.250000. running mean: -4.176209\n",
      "ep 2724: ep_len:500 episode reward: total was -53.000000. running mean: -4.664447\n",
      "ep 2724: ep_len:204 episode reward: total was -23.080000. running mean: -4.848603\n",
      "epsilon:0.079192 episode_count: 19075. steps_count: 8250387.000000\n",
      "Time elapsed:  24289.832271575928\n",
      "ep 2725: ep_len:598 episode reward: total was 22.840000. running mean: -4.571717\n",
      "ep 2725: ep_len:500 episode reward: total was 5.130000. running mean: -4.474700\n",
      "ep 2725: ep_len:613 episode reward: total was -38.020000. running mean: -4.810153\n",
      "ep 2725: ep_len:49 episode reward: total was 1.270000. running mean: -4.749351\n",
      "ep 2725: ep_len:3 episode reward: total was -1.500000. running mean: -4.716857\n",
      "ep 2725: ep_len:500 episode reward: total was -0.230000. running mean: -4.671989\n",
      "ep 2725: ep_len:508 episode reward: total was -60.170000. running mean: -5.226969\n",
      "epsilon:0.079147 episode_count: 19082. steps_count: 8253158.000000\n",
      "Time elapsed:  24301.013296365738\n",
      "ep 2726: ep_len:221 episode reward: total was 5.170000. running mean: -5.122999\n",
      "ep 2726: ep_len:569 episode reward: total was -128.620000. running mean: -6.357969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2726: ep_len:69 episode reward: total was -1.740000. running mean: -6.311790\n",
      "ep 2726: ep_len:56 episode reward: total was 2.840000. running mean: -6.220272\n",
      "ep 2726: ep_len:1 episode reward: total was -1.000000. running mean: -6.168069\n",
      "ep 2726: ep_len:171 episode reward: total was 26.140000. running mean: -5.844988\n",
      "ep 2726: ep_len:623 episode reward: total was 23.410000. running mean: -5.552438\n",
      "epsilon:0.079103 episode_count: 19089. steps_count: 8254868.000000\n",
      "Time elapsed:  24305.927885770798\n",
      "ep 2727: ep_len:591 episode reward: total was 27.550000. running mean: -5.221414\n",
      "ep 2727: ep_len:500 episode reward: total was 15.100000. running mean: -5.018200\n",
      "ep 2727: ep_len:72 episode reward: total was -1.740000. running mean: -4.985418\n",
      "ep 2727: ep_len:506 episode reward: total was 20.350000. running mean: -4.732064\n",
      "ep 2727: ep_len:107 episode reward: total was 23.230000. running mean: -4.452443\n",
      "ep 2727: ep_len:587 episode reward: total was 7.030000. running mean: -4.337619\n",
      "ep 2727: ep_len:316 episode reward: total was 7.770000. running mean: -4.216542\n",
      "epsilon:0.079059 episode_count: 19096. steps_count: 8257547.000000\n",
      "Time elapsed:  24316.694838762283\n",
      "ep 2728: ep_len:120 episode reward: total was 11.450000. running mean: -4.059877\n",
      "ep 2728: ep_len:500 episode reward: total was -6.560000. running mean: -4.084878\n",
      "ep 2728: ep_len:688 episode reward: total was -62.000000. running mean: -4.664030\n",
      "ep 2728: ep_len:550 episode reward: total was 26.410000. running mean: -4.353289\n",
      "ep 2728: ep_len:3 episode reward: total was 1.010000. running mean: -4.299656\n",
      "ep 2728: ep_len:529 episode reward: total was -5.610000. running mean: -4.312760\n",
      "ep 2728: ep_len:564 episode reward: total was -23.650000. running mean: -4.506132\n",
      "epsilon:0.079014 episode_count: 19103. steps_count: 8260501.000000\n",
      "Time elapsed:  24333.82608127594\n",
      "ep 2729: ep_len:571 episode reward: total was 21.650000. running mean: -4.244571\n",
      "ep 2729: ep_len:542 episode reward: total was -6.390000. running mean: -4.266025\n",
      "ep 2729: ep_len:577 episode reward: total was -0.250000. running mean: -4.225865\n",
      "ep 2729: ep_len:383 episode reward: total was 29.970000. running mean: -3.883906\n",
      "ep 2729: ep_len:112 episode reward: total was 17.830000. running mean: -3.666767\n",
      "ep 2729: ep_len:334 episode reward: total was 7.700000. running mean: -3.553099\n",
      "ep 2729: ep_len:509 episode reward: total was 22.940000. running mean: -3.288169\n",
      "epsilon:0.078970 episode_count: 19110. steps_count: 8263529.000000\n",
      "Time elapsed:  24342.449795007706\n",
      "ep 2730: ep_len:505 episode reward: total was -0.170000. running mean: -3.256987\n",
      "ep 2730: ep_len:500 episode reward: total was 16.450000. running mean: -3.059917\n",
      "ep 2730: ep_len:530 episode reward: total was -57.820000. running mean: -3.607518\n",
      "ep 2730: ep_len:611 episode reward: total was 44.820000. running mean: -3.123243\n",
      "ep 2730: ep_len:70 episode reward: total was -31.390000. running mean: -3.405910\n",
      "ep 2730: ep_len:634 episode reward: total was 12.900000. running mean: -3.242851\n",
      "ep 2730: ep_len:588 episode reward: total was 31.250000. running mean: -2.897923\n",
      "epsilon:0.078926 episode_count: 19117. steps_count: 8266967.000000\n",
      "Time elapsed:  24351.360614538193\n",
      "ep 2731: ep_len:645 episode reward: total was 27.040000. running mean: -2.598543\n",
      "ep 2731: ep_len:613 episode reward: total was 9.690000. running mean: -2.475658\n",
      "ep 2731: ep_len:572 episode reward: total was -52.580000. running mean: -2.976701\n",
      "ep 2731: ep_len:578 episode reward: total was 36.580000. running mean: -2.581134\n",
      "ep 2731: ep_len:3 episode reward: total was 1.010000. running mean: -2.545223\n",
      "ep 2731: ep_len:661 episode reward: total was -125.390000. running mean: -3.773671\n",
      "ep 2731: ep_len:518 episode reward: total was -10.510000. running mean: -3.841034\n",
      "epsilon:0.078881 episode_count: 19124. steps_count: 8270557.000000\n",
      "Time elapsed:  24359.99097776413\n",
      "ep 2732: ep_len:213 episode reward: total was 0.620000. running mean: -3.796424\n",
      "ep 2732: ep_len:305 episode reward: total was -13.840000. running mean: -3.896859\n",
      "ep 2732: ep_len:565 episode reward: total was -43.720000. running mean: -4.295091\n",
      "ep 2732: ep_len:552 episode reward: total was 33.780000. running mean: -3.914340\n",
      "ep 2732: ep_len:130 episode reward: total was 15.290000. running mean: -3.722297\n",
      "ep 2732: ep_len:186 episode reward: total was 25.620000. running mean: -3.428874\n",
      "ep 2732: ep_len:605 episode reward: total was 5.000000. running mean: -3.344585\n",
      "epsilon:0.078837 episode_count: 19131. steps_count: 8273113.000000\n",
      "Time elapsed:  24365.543885231018\n",
      "ep 2733: ep_len:500 episode reward: total was 55.440000. running mean: -2.756739\n",
      "ep 2733: ep_len:500 episode reward: total was 86.510000. running mean: -1.864072\n",
      "ep 2733: ep_len:655 episode reward: total was -32.610000. running mean: -2.171531\n",
      "ep 2733: ep_len:565 episode reward: total was 43.090000. running mean: -1.718916\n",
      "ep 2733: ep_len:99 episode reward: total was -59.760000. running mean: -2.299326\n",
      "ep 2733: ep_len:500 episode reward: total was 21.740000. running mean: -2.058933\n",
      "ep 2733: ep_len:514 episode reward: total was 29.020000. running mean: -1.748144\n",
      "epsilon:0.078793 episode_count: 19138. steps_count: 8276446.000000\n",
      "Time elapsed:  24374.296417474747\n",
      "ep 2734: ep_len:500 episode reward: total was -70.810000. running mean: -2.438762\n",
      "ep 2734: ep_len:500 episode reward: total was 6.220000. running mean: -2.352175\n",
      "ep 2734: ep_len:577 episode reward: total was -8.980000. running mean: -2.418453\n",
      "ep 2734: ep_len:532 episode reward: total was 17.170000. running mean: -2.222568\n",
      "ep 2734: ep_len:3 episode reward: total was 1.010000. running mean: -2.190243\n",
      "ep 2734: ep_len:500 episode reward: total was -89.670000. running mean: -3.065040\n",
      "ep 2734: ep_len:500 episode reward: total was -58.090000. running mean: -3.615290\n",
      "epsilon:0.078748 episode_count: 19145. steps_count: 8279558.000000\n",
      "Time elapsed:  24382.630348205566\n",
      "ep 2735: ep_len:572 episode reward: total was 67.090000. running mean: -2.908237\n",
      "ep 2735: ep_len:518 episode reward: total was 5.570000. running mean: -2.823455\n",
      "ep 2735: ep_len:546 episode reward: total was 11.940000. running mean: -2.675820\n",
      "ep 2735: ep_len:595 episode reward: total was 32.170000. running mean: -2.327362\n",
      "ep 2735: ep_len:114 episode reward: total was 5.830000. running mean: -2.245788\n",
      "ep 2735: ep_len:537 episode reward: total was -29.560000. running mean: -2.518930\n",
      "ep 2735: ep_len:211 episode reward: total was -12.400000. running mean: -2.617741\n",
      "epsilon:0.078704 episode_count: 19152. steps_count: 8282651.000000\n",
      "Time elapsed:  24390.88187122345\n",
      "ep 2736: ep_len:656 episode reward: total was -28.500000. running mean: -2.876564\n",
      "ep 2736: ep_len:500 episode reward: total was 27.330000. running mean: -2.574498\n",
      "ep 2736: ep_len:387 episode reward: total was 7.030000. running mean: -2.478453\n",
      "ep 2736: ep_len:573 episode reward: total was 20.070000. running mean: -2.252969\n",
      "ep 2736: ep_len:98 episode reward: total was 17.260000. running mean: -2.057839\n",
      "ep 2736: ep_len:525 episode reward: total was -72.870000. running mean: -2.765961\n",
      "ep 2736: ep_len:500 episode reward: total was -28.620000. running mean: -3.024501\n",
      "epsilon:0.078660 episode_count: 19159. steps_count: 8285890.000000\n",
      "Time elapsed:  24399.326102256775\n",
      "ep 2737: ep_len:500 episode reward: total was 41.880000. running mean: -2.575456\n",
      "ep 2737: ep_len:524 episode reward: total was 33.060000. running mean: -2.219101\n",
      "ep 2737: ep_len:500 episode reward: total was -19.450000. running mean: -2.391410\n",
      "ep 2737: ep_len:500 episode reward: total was 28.280000. running mean: -2.084696\n",
      "ep 2737: ep_len:3 episode reward: total was 1.010000. running mean: -2.053749\n",
      "ep 2737: ep_len:500 episode reward: total was -12.440000. running mean: -2.157612\n",
      "ep 2737: ep_len:607 episode reward: total was -7.420000. running mean: -2.210236\n",
      "epsilon:0.078615 episode_count: 19166. steps_count: 8289024.000000\n",
      "Time elapsed:  24407.660044908524\n",
      "ep 2738: ep_len:500 episode reward: total was 70.220000. running mean: -1.485933\n",
      "ep 2738: ep_len:282 episode reward: total was -18.630000. running mean: -1.657374\n",
      "ep 2738: ep_len:500 episode reward: total was -4.100000. running mean: -1.681800\n",
      "ep 2738: ep_len:500 episode reward: total was 47.720000. running mean: -1.187782\n",
      "ep 2738: ep_len:89 episode reward: total was 22.770000. running mean: -0.948204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2738: ep_len:301 episode reward: total was 8.190000. running mean: -0.856822\n",
      "ep 2738: ep_len:561 episode reward: total was -76.510000. running mean: -1.613354\n",
      "epsilon:0.078571 episode_count: 19173. steps_count: 8291757.000000\n",
      "Time elapsed:  24415.10705113411\n",
      "ep 2739: ep_len:532 episode reward: total was 15.780000. running mean: -1.439421\n",
      "ep 2739: ep_len:594 episode reward: total was -20.690000. running mean: -1.631926\n",
      "ep 2739: ep_len:565 episode reward: total was -62.120000. running mean: -2.236807\n",
      "ep 2739: ep_len:500 episode reward: total was 8.390000. running mean: -2.130539\n",
      "ep 2739: ep_len:3 episode reward: total was -1.500000. running mean: -2.124234\n",
      "ep 2739: ep_len:540 episode reward: total was -27.010000. running mean: -2.373091\n",
      "ep 2739: ep_len:500 episode reward: total was 10.120000. running mean: -2.248160\n",
      "epsilon:0.078527 episode_count: 19180. steps_count: 8294991.000000\n",
      "Time elapsed:  24420.53311562538\n",
      "ep 2740: ep_len:648 episode reward: total was -33.660000. running mean: -2.562279\n",
      "ep 2740: ep_len:548 episode reward: total was -13.690000. running mean: -2.673556\n",
      "ep 2740: ep_len:501 episode reward: total was -32.150000. running mean: -2.968320\n",
      "ep 2740: ep_len:538 episode reward: total was 28.110000. running mean: -2.657537\n",
      "ep 2740: ep_len:3 episode reward: total was 1.010000. running mean: -2.620862\n",
      "ep 2740: ep_len:568 episode reward: total was -11.060000. running mean: -2.705253\n",
      "ep 2740: ep_len:581 episode reward: total was -91.410000. running mean: -3.592301\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.078482 episode_count: 19187. steps_count: 8298378.000000\n",
      "Time elapsed:  24430.512379169464\n",
      "ep 2741: ep_len:556 episode reward: total was -18.480000. running mean: -3.741178\n",
      "ep 2741: ep_len:619 episode reward: total was 18.250000. running mean: -3.521266\n",
      "ep 2741: ep_len:681 episode reward: total was -43.140000. running mean: -3.917453\n",
      "ep 2741: ep_len:500 episode reward: total was -5.640000. running mean: -3.934679\n",
      "ep 2741: ep_len:3 episode reward: total was 1.010000. running mean: -3.885232\n",
      "ep 2741: ep_len:160 episode reward: total was 18.130000. running mean: -3.665080\n",
      "ep 2741: ep_len:581 episode reward: total was -38.240000. running mean: -4.010829\n",
      "epsilon:0.078438 episode_count: 19194. steps_count: 8301478.000000\n",
      "Time elapsed:  24438.933703422546\n",
      "ep 2742: ep_len:594 episode reward: total was 59.370000. running mean: -3.377021\n",
      "ep 2742: ep_len:529 episode reward: total was 41.570000. running mean: -2.927550\n",
      "ep 2742: ep_len:613 episode reward: total was -12.410000. running mean: -3.022375\n",
      "ep 2742: ep_len:600 episode reward: total was 41.100000. running mean: -2.581151\n",
      "ep 2742: ep_len:3 episode reward: total was 1.010000. running mean: -2.545240\n",
      "ep 2742: ep_len:511 episode reward: total was -33.690000. running mean: -2.856687\n",
      "ep 2742: ep_len:500 episode reward: total was -37.380000. running mean: -3.201920\n",
      "epsilon:0.078394 episode_count: 19201. steps_count: 8304828.000000\n",
      "Time elapsed:  24451.604410648346\n",
      "ep 2743: ep_len:508 episode reward: total was -12.900000. running mean: -3.298901\n",
      "ep 2743: ep_len:187 episode reward: total was -13.270000. running mean: -3.398612\n",
      "ep 2743: ep_len:598 episode reward: total was 5.350000. running mean: -3.311126\n",
      "ep 2743: ep_len:500 episode reward: total was 28.160000. running mean: -2.996415\n",
      "ep 2743: ep_len:75 episode reward: total was 9.620000. running mean: -2.870251\n",
      "ep 2743: ep_len:620 episode reward: total was -0.740000. running mean: -2.848948\n",
      "ep 2743: ep_len:500 episode reward: total was -5.310000. running mean: -2.873559\n",
      "epsilon:0.078349 episode_count: 19208. steps_count: 8307816.000000\n",
      "Time elapsed:  24457.578644275665\n",
      "ep 2744: ep_len:553 episode reward: total was 39.500000. running mean: -2.449823\n",
      "ep 2744: ep_len:550 episode reward: total was 78.190000. running mean: -1.643425\n",
      "ep 2744: ep_len:500 episode reward: total was -5.790000. running mean: -1.684891\n",
      "ep 2744: ep_len:517 episode reward: total was 16.240000. running mean: -1.505642\n",
      "ep 2744: ep_len:3 episode reward: total was 1.010000. running mean: -1.480485\n",
      "ep 2744: ep_len:500 episode reward: total was -12.210000. running mean: -1.587780\n",
      "ep 2744: ep_len:566 episode reward: total was -28.770000. running mean: -1.859603\n",
      "epsilon:0.078305 episode_count: 19215. steps_count: 8311005.000000\n",
      "Time elapsed:  24462.619099855423\n",
      "ep 2745: ep_len:576 episode reward: total was -49.450000. running mean: -2.335507\n",
      "ep 2745: ep_len:500 episode reward: total was 4.610000. running mean: -2.266051\n",
      "ep 2745: ep_len:392 episode reward: total was -9.490000. running mean: -2.338291\n",
      "ep 2745: ep_len:500 episode reward: total was 2.350000. running mean: -2.291408\n",
      "ep 2745: ep_len:78 episode reward: total was -16.390000. running mean: -2.432394\n",
      "ep 2745: ep_len:533 episode reward: total was -52.160000. running mean: -2.929670\n",
      "ep 2745: ep_len:534 episode reward: total was -36.650000. running mean: -3.266873\n",
      "epsilon:0.078261 episode_count: 19222. steps_count: 8314118.000000\n",
      "Time elapsed:  24470.907135486603\n",
      "ep 2746: ep_len:500 episode reward: total was 53.940000. running mean: -2.694805\n",
      "ep 2746: ep_len:383 episode reward: total was 1.500000. running mean: -2.652857\n",
      "ep 2746: ep_len:500 episode reward: total was 11.010000. running mean: -2.516228\n",
      "ep 2746: ep_len:544 episode reward: total was 28.550000. running mean: -2.205566\n",
      "ep 2746: ep_len:3 episode reward: total was 1.010000. running mean: -2.173410\n",
      "ep 2746: ep_len:545 episode reward: total was -110.530000. running mean: -3.256976\n",
      "ep 2746: ep_len:564 episode reward: total was -0.990000. running mean: -3.234306\n",
      "epsilon:0.078216 episode_count: 19229. steps_count: 8317157.000000\n",
      "Time elapsed:  24476.77997279167\n",
      "ep 2747: ep_len:634 episode reward: total was 17.900000. running mean: -3.022963\n",
      "ep 2747: ep_len:591 episode reward: total was -18.690000. running mean: -3.179633\n",
      "ep 2747: ep_len:511 episode reward: total was -30.280000. running mean: -3.450637\n",
      "ep 2747: ep_len:559 episode reward: total was -1.540000. running mean: -3.431531\n",
      "ep 2747: ep_len:3 episode reward: total was 1.010000. running mean: -3.387115\n",
      "ep 2747: ep_len:705 episode reward: total was 37.810000. running mean: -2.975144\n",
      "ep 2747: ep_len:500 episode reward: total was 47.610000. running mean: -2.469293\n",
      "epsilon:0.078172 episode_count: 19236. steps_count: 8320660.000000\n",
      "Time elapsed:  24482.394787549973\n",
      "ep 2748: ep_len:742 episode reward: total was -54.690000. running mean: -2.991500\n",
      "ep 2748: ep_len:594 episode reward: total was -2.910000. running mean: -2.990685\n",
      "ep 2748: ep_len:524 episode reward: total was 7.830000. running mean: -2.882478\n",
      "ep 2748: ep_len:611 episode reward: total was 28.580000. running mean: -2.567853\n",
      "ep 2748: ep_len:84 episode reward: total was 13.720000. running mean: -2.404975\n",
      "ep 2748: ep_len:614 episode reward: total was -39.830000. running mean: -2.779225\n",
      "ep 2748: ep_len:516 episode reward: total was 0.730000. running mean: -2.744133\n",
      "epsilon:0.078128 episode_count: 19243. steps_count: 8324345.000000\n",
      "Time elapsed:  24492.212589502335\n",
      "ep 2749: ep_len:643 episode reward: total was 38.560000. running mean: -2.331091\n",
      "ep 2749: ep_len:500 episode reward: total was 12.830000. running mean: -2.179481\n",
      "ep 2749: ep_len:500 episode reward: total was 5.710000. running mean: -2.100586\n",
      "ep 2749: ep_len:378 episode reward: total was -88.160000. running mean: -2.961180\n",
      "ep 2749: ep_len:87 episode reward: total was 18.220000. running mean: -2.749368\n",
      "ep 2749: ep_len:516 episode reward: total was -27.630000. running mean: -2.998174\n",
      "ep 2749: ep_len:534 episode reward: total was -25.210000. running mean: -3.220293\n",
      "epsilon:0.078083 episode_count: 19250. steps_count: 8327503.000000\n",
      "Time elapsed:  24500.48052382469\n",
      "ep 2750: ep_len:623 episode reward: total was -69.560000. running mean: -3.883690\n",
      "ep 2750: ep_len:500 episode reward: total was -14.990000. running mean: -3.994753\n",
      "ep 2750: ep_len:653 episode reward: total was -68.130000. running mean: -4.636105\n",
      "ep 2750: ep_len:548 episode reward: total was 22.420000. running mean: -4.365544\n",
      "ep 2750: ep_len:110 episode reward: total was 20.200000. running mean: -4.119889\n",
      "ep 2750: ep_len:586 episode reward: total was -9.570000. running mean: -4.174390\n",
      "ep 2750: ep_len:503 episode reward: total was -0.420000. running mean: -4.136846\n",
      "epsilon:0.078039 episode_count: 19257. steps_count: 8331026.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  24509.8681640625\n",
      "ep 2751: ep_len:553 episode reward: total was -64.060000. running mean: -4.736078\n",
      "ep 2751: ep_len:581 episode reward: total was -4.340000. running mean: -4.732117\n",
      "ep 2751: ep_len:384 episode reward: total was 15.940000. running mean: -4.525396\n",
      "ep 2751: ep_len:530 episode reward: total was -41.800000. running mean: -4.898142\n",
      "ep 2751: ep_len:3 episode reward: total was 1.010000. running mean: -4.839060\n",
      "ep 2751: ep_len:534 episode reward: total was 19.200000. running mean: -4.598670\n",
      "ep 2751: ep_len:517 episode reward: total was -63.120000. running mean: -5.183883\n",
      "epsilon:0.077995 episode_count: 19264. steps_count: 8334128.000000\n",
      "Time elapsed:  24518.241249084473\n",
      "ep 2752: ep_len:645 episode reward: total was -146.390000. running mean: -6.595944\n",
      "ep 2752: ep_len:500 episode reward: total was -26.240000. running mean: -6.792385\n",
      "ep 2752: ep_len:584 episode reward: total was -13.800000. running mean: -6.862461\n",
      "ep 2752: ep_len:500 episode reward: total was 46.500000. running mean: -6.328836\n",
      "ep 2752: ep_len:55 episode reward: total was 21.500000. running mean: -6.050548\n",
      "ep 2752: ep_len:610 episode reward: total was -92.160000. running mean: -6.911642\n",
      "ep 2752: ep_len:575 episode reward: total was 16.800000. running mean: -6.674526\n",
      "epsilon:0.077950 episode_count: 19271. steps_count: 8337597.000000\n",
      "Time elapsed:  24527.395795583725\n",
      "ep 2753: ep_len:524 episode reward: total was -64.670000. running mean: -7.254481\n",
      "ep 2753: ep_len:500 episode reward: total was 8.470000. running mean: -7.097236\n",
      "ep 2753: ep_len:623 episode reward: total was -40.280000. running mean: -7.429064\n",
      "ep 2753: ep_len:611 episode reward: total was 53.640000. running mean: -6.818373\n",
      "ep 2753: ep_len:105 episode reward: total was 30.770000. running mean: -6.442489\n",
      "ep 2753: ep_len:500 episode reward: total was 12.190000. running mean: -6.256164\n",
      "ep 2753: ep_len:502 episode reward: total was -25.790000. running mean: -6.451503\n",
      "epsilon:0.077906 episode_count: 19278. steps_count: 8340962.000000\n",
      "Time elapsed:  24536.27755880356\n",
      "ep 2754: ep_len:555 episode reward: total was 9.340000. running mean: -6.293588\n",
      "ep 2754: ep_len:500 episode reward: total was 58.100000. running mean: -5.649652\n",
      "ep 2754: ep_len:618 episode reward: total was -15.810000. running mean: -5.751255\n",
      "ep 2754: ep_len:500 episode reward: total was 37.310000. running mean: -5.320643\n",
      "ep 2754: ep_len:3 episode reward: total was 1.010000. running mean: -5.257336\n",
      "ep 2754: ep_len:581 episode reward: total was -8.360000. running mean: -5.288363\n",
      "ep 2754: ep_len:549 episode reward: total was -24.370000. running mean: -5.479179\n",
      "epsilon:0.077862 episode_count: 19285. steps_count: 8344268.000000\n",
      "Time elapsed:  24545.110050439835\n",
      "ep 2755: ep_len:119 episode reward: total was 3.820000. running mean: -5.386187\n",
      "ep 2755: ep_len:500 episode reward: total was -28.660000. running mean: -5.618926\n",
      "ep 2755: ep_len:79 episode reward: total was 5.800000. running mean: -5.504736\n",
      "ep 2755: ep_len:500 episode reward: total was 20.720000. running mean: -5.242489\n",
      "ep 2755: ep_len:3 episode reward: total was 1.010000. running mean: -5.179964\n",
      "ep 2755: ep_len:500 episode reward: total was -99.500000. running mean: -6.123164\n",
      "ep 2755: ep_len:170 episode reward: total was -18.920000. running mean: -6.251133\n",
      "epsilon:0.077817 episode_count: 19292. steps_count: 8346139.000000\n",
      "Time elapsed:  24548.406368017197\n",
      "ep 2756: ep_len:247 episode reward: total was 15.730000. running mean: -6.031321\n",
      "ep 2756: ep_len:502 episode reward: total was 5.470000. running mean: -5.916308\n",
      "ep 2756: ep_len:599 episode reward: total was -15.890000. running mean: -6.016045\n",
      "ep 2756: ep_len:561 episode reward: total was 63.010000. running mean: -5.325785\n",
      "ep 2756: ep_len:3 episode reward: total was 1.010000. running mean: -5.262427\n",
      "ep 2756: ep_len:567 episode reward: total was -5.170000. running mean: -5.261503\n",
      "ep 2756: ep_len:589 episode reward: total was -3.970000. running mean: -5.248588\n",
      "epsilon:0.077773 episode_count: 19299. steps_count: 8349207.000000\n",
      "Time elapsed:  24553.3222322464\n",
      "ep 2757: ep_len:221 episode reward: total was 11.730000. running mean: -5.078802\n",
      "ep 2757: ep_len:500 episode reward: total was 13.080000. running mean: -4.897214\n",
      "ep 2757: ep_len:642 episode reward: total was -12.010000. running mean: -4.968342\n",
      "ep 2757: ep_len:381 episode reward: total was -133.300000. running mean: -6.251658\n",
      "ep 2757: ep_len:3 episode reward: total was 1.010000. running mean: -6.179042\n",
      "ep 2757: ep_len:611 episode reward: total was -85.860000. running mean: -6.975851\n",
      "ep 2757: ep_len:612 episode reward: total was 2.250000. running mean: -6.883593\n",
      "epsilon:0.077729 episode_count: 19306. steps_count: 8352177.000000\n",
      "Time elapsed:  24561.1854326725\n",
      "ep 2758: ep_len:614 episode reward: total was -6.850000. running mean: -6.883257\n",
      "ep 2758: ep_len:500 episode reward: total was 15.600000. running mean: -6.658424\n",
      "ep 2758: ep_len:552 episode reward: total was -34.260000. running mean: -6.934440\n",
      "ep 2758: ep_len:500 episode reward: total was 15.270000. running mean: -6.712395\n",
      "ep 2758: ep_len:3 episode reward: total was 1.010000. running mean: -6.635172\n",
      "ep 2758: ep_len:568 episode reward: total was 26.430000. running mean: -6.304520\n",
      "ep 2758: ep_len:588 episode reward: total was -1.200000. running mean: -6.253475\n",
      "epsilon:0.077684 episode_count: 19313. steps_count: 8355502.000000\n",
      "Time elapsed:  24570.124086856842\n",
      "ep 2759: ep_len:500 episode reward: total was 23.700000. running mean: -5.953940\n",
      "ep 2759: ep_len:502 episode reward: total was 70.300000. running mean: -5.191400\n",
      "ep 2759: ep_len:546 episode reward: total was -23.250000. running mean: -5.371986\n",
      "ep 2759: ep_len:513 episode reward: total was -5.810000. running mean: -5.376367\n",
      "ep 2759: ep_len:105 episode reward: total was 21.250000. running mean: -5.110103\n",
      "ep 2759: ep_len:500 episode reward: total was -310.840000. running mean: -8.167402\n",
      "ep 2759: ep_len:500 episode reward: total was -7.280000. running mean: -8.158528\n",
      "epsilon:0.077640 episode_count: 19320. steps_count: 8358668.000000\n",
      "Time elapsed:  24579.702242851257\n",
      "ep 2760: ep_len:259 episode reward: total was 18.960000. running mean: -7.887343\n",
      "ep 2760: ep_len:556 episode reward: total was 75.700000. running mean: -7.051469\n",
      "ep 2760: ep_len:526 episode reward: total was -7.000000. running mean: -7.050954\n",
      "ep 2760: ep_len:526 episode reward: total was -31.540000. running mean: -7.295845\n",
      "ep 2760: ep_len:3 episode reward: total was 1.010000. running mean: -7.212786\n",
      "ep 2760: ep_len:500 episode reward: total was -6.910000. running mean: -7.209759\n",
      "ep 2760: ep_len:529 episode reward: total was -32.240000. running mean: -7.460061\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.077596 episode_count: 19327. steps_count: 8361567.000000\n",
      "Time elapsed:  24595.562933683395\n",
      "ep 2761: ep_len:520 episode reward: total was -31.660000. running mean: -7.702060\n",
      "ep 2761: ep_len:184 episode reward: total was -31.570000. running mean: -7.940740\n",
      "ep 2761: ep_len:500 episode reward: total was -28.080000. running mean: -8.142132\n",
      "ep 2761: ep_len:501 episode reward: total was 42.350000. running mean: -7.637211\n",
      "ep 2761: ep_len:103 episode reward: total was 28.760000. running mean: -7.273239\n",
      "ep 2761: ep_len:566 episode reward: total was 37.220000. running mean: -6.828307\n",
      "ep 2761: ep_len:195 episode reward: total was -31.250000. running mean: -7.072524\n",
      "epsilon:0.077551 episode_count: 19334. steps_count: 8364136.000000\n",
      "Time elapsed:  24602.440390348434\n",
      "ep 2762: ep_len:501 episode reward: total was 27.760000. running mean: -6.724198\n",
      "ep 2762: ep_len:521 episode reward: total was -10.910000. running mean: -6.766056\n",
      "ep 2762: ep_len:500 episode reward: total was -16.600000. running mean: -6.864396\n",
      "ep 2762: ep_len:527 episode reward: total was -15.040000. running mean: -6.946152\n",
      "ep 2762: ep_len:77 episode reward: total was 13.680000. running mean: -6.739890\n",
      "ep 2762: ep_len:672 episode reward: total was -4.860000. running mean: -6.721091\n",
      "ep 2762: ep_len:317 episode reward: total was -43.980000. running mean: -7.093680\n",
      "epsilon:0.077507 episode_count: 19341. steps_count: 8367251.000000\n",
      "Time elapsed:  24613.300972223282\n",
      "ep 2763: ep_len:500 episode reward: total was -16.210000. running mean: -7.184844\n",
      "ep 2763: ep_len:500 episode reward: total was -111.890000. running mean: -8.231895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2763: ep_len:500 episode reward: total was -22.870000. running mean: -8.378276\n",
      "ep 2763: ep_len:556 episode reward: total was 25.740000. running mean: -8.037094\n",
      "ep 2763: ep_len:3 episode reward: total was 1.010000. running mean: -7.946623\n",
      "ep 2763: ep_len:506 episode reward: total was -0.430000. running mean: -7.871456\n",
      "ep 2763: ep_len:500 episode reward: total was -45.770000. running mean: -8.250442\n",
      "epsilon:0.077463 episode_count: 19348. steps_count: 8370316.000000\n",
      "Time elapsed:  24621.500097990036\n",
      "ep 2764: ep_len:128 episode reward: total was -0.100000. running mean: -8.168937\n",
      "ep 2764: ep_len:586 episode reward: total was 0.810000. running mean: -8.079148\n",
      "ep 2764: ep_len:351 episode reward: total was -18.190000. running mean: -8.180257\n",
      "ep 2764: ep_len:570 episode reward: total was 31.630000. running mean: -7.782154\n",
      "ep 2764: ep_len:3 episode reward: total was 1.010000. running mean: -7.694232\n",
      "ep 2764: ep_len:155 episode reward: total was 0.170000. running mean: -7.615590\n",
      "ep 2764: ep_len:609 episode reward: total was -13.800000. running mean: -7.677434\n",
      "epsilon:0.077418 episode_count: 19355. steps_count: 8372718.000000\n",
      "Time elapsed:  24628.170846939087\n",
      "ep 2765: ep_len:545 episode reward: total was -46.870000. running mean: -8.069360\n",
      "ep 2765: ep_len:576 episode reward: total was 25.090000. running mean: -7.737766\n",
      "ep 2765: ep_len:76 episode reward: total was 10.820000. running mean: -7.552189\n",
      "ep 2765: ep_len:580 episode reward: total was 36.240000. running mean: -7.114267\n",
      "ep 2765: ep_len:50 episode reward: total was 20.010000. running mean: -6.843024\n",
      "ep 2765: ep_len:555 episode reward: total was -44.620000. running mean: -7.220794\n",
      "ep 2765: ep_len:604 episode reward: total was -9.090000. running mean: -7.239486\n",
      "epsilon:0.077374 episode_count: 19362. steps_count: 8375704.000000\n",
      "Time elapsed:  24636.06349515915\n",
      "ep 2766: ep_len:500 episode reward: total was 3.470000. running mean: -7.132391\n",
      "ep 2766: ep_len:562 episode reward: total was 30.930000. running mean: -6.751767\n",
      "ep 2766: ep_len:530 episode reward: total was -46.830000. running mean: -7.152549\n",
      "ep 2766: ep_len:500 episode reward: total was -1.700000. running mean: -7.098024\n",
      "ep 2766: ep_len:81 episode reward: total was 12.100000. running mean: -6.906044\n",
      "ep 2766: ep_len:613 episode reward: total was 25.320000. running mean: -6.583783\n",
      "ep 2766: ep_len:287 episode reward: total was -13.870000. running mean: -6.656645\n",
      "epsilon:0.077330 episode_count: 19369. steps_count: 8378777.000000\n",
      "Time elapsed:  24644.137979507446\n",
      "ep 2767: ep_len:265 episode reward: total was -3.220000. running mean: -6.622279\n",
      "ep 2767: ep_len:663 episode reward: total was 47.230000. running mean: -6.083756\n",
      "ep 2767: ep_len:627 episode reward: total was -10.090000. running mean: -6.123819\n",
      "ep 2767: ep_len:523 episode reward: total was 28.550000. running mean: -5.777080\n",
      "ep 2767: ep_len:3 episode reward: total was 1.010000. running mean: -5.709210\n",
      "ep 2767: ep_len:633 episode reward: total was -140.540000. running mean: -7.057518\n",
      "ep 2767: ep_len:500 episode reward: total was -6.450000. running mean: -7.051442\n",
      "epsilon:0.077285 episode_count: 19376. steps_count: 8381991.000000\n",
      "Time elapsed:  24652.57219195366\n",
      "ep 2768: ep_len:500 episode reward: total was -52.590000. running mean: -7.506828\n",
      "ep 2768: ep_len:500 episode reward: total was 9.440000. running mean: -7.337360\n",
      "ep 2768: ep_len:409 episode reward: total was -23.490000. running mean: -7.498886\n",
      "ep 2768: ep_len:503 episode reward: total was 53.470000. running mean: -6.889197\n",
      "ep 2768: ep_len:92 episode reward: total was -48.920000. running mean: -7.309505\n",
      "ep 2768: ep_len:674 episode reward: total was -8.170000. running mean: -7.318110\n",
      "ep 2768: ep_len:635 episode reward: total was -6.540000. running mean: -7.310329\n",
      "epsilon:0.077241 episode_count: 19383. steps_count: 8385304.000000\n",
      "Time elapsed:  24661.19631576538\n",
      "ep 2769: ep_len:511 episode reward: total was 43.370000. running mean: -6.803526\n",
      "ep 2769: ep_len:538 episode reward: total was 2.310000. running mean: -6.712391\n",
      "ep 2769: ep_len:516 episode reward: total was -0.580000. running mean: -6.651067\n",
      "ep 2769: ep_len:415 episode reward: total was -12.910000. running mean: -6.713656\n",
      "ep 2769: ep_len:3 episode reward: total was 1.010000. running mean: -6.636419\n",
      "ep 2769: ep_len:506 episode reward: total was -75.230000. running mean: -7.322355\n",
      "ep 2769: ep_len:275 episode reward: total was -15.340000. running mean: -7.402532\n",
      "epsilon:0.077197 episode_count: 19390. steps_count: 8388068.000000\n",
      "Time elapsed:  24668.752951860428\n",
      "ep 2770: ep_len:592 episode reward: total was 3.020000. running mean: -7.298306\n",
      "ep 2770: ep_len:500 episode reward: total was 29.850000. running mean: -6.926823\n",
      "ep 2770: ep_len:355 episode reward: total was 1.370000. running mean: -6.843855\n",
      "ep 2770: ep_len:432 episode reward: total was -19.300000. running mean: -6.968416\n",
      "ep 2770: ep_len:70 episode reward: total was 14.670000. running mean: -6.752032\n",
      "ep 2770: ep_len:544 episode reward: total was 8.900000. running mean: -6.595512\n",
      "ep 2770: ep_len:527 episode reward: total was -9.360000. running mean: -6.623157\n",
      "epsilon:0.077152 episode_count: 19397. steps_count: 8391088.000000\n",
      "Time elapsed:  24676.788560152054\n",
      "ep 2771: ep_len:585 episode reward: total was 64.590000. running mean: -5.911025\n",
      "ep 2771: ep_len:500 episode reward: total was 94.620000. running mean: -4.905715\n",
      "ep 2771: ep_len:657 episode reward: total was -12.200000. running mean: -4.978658\n",
      "ep 2771: ep_len:586 episode reward: total was 18.390000. running mean: -4.744971\n",
      "ep 2771: ep_len:3 episode reward: total was 1.010000. running mean: -4.687422\n",
      "ep 2771: ep_len:615 episode reward: total was 14.650000. running mean: -4.494047\n",
      "ep 2771: ep_len:541 episode reward: total was -64.600000. running mean: -5.095107\n",
      "epsilon:0.077108 episode_count: 19404. steps_count: 8394575.000000\n",
      "Time elapsed:  24685.949457883835\n",
      "ep 2772: ep_len:586 episode reward: total was 1.590000. running mean: -5.028256\n",
      "ep 2772: ep_len:201 episode reward: total was -20.540000. running mean: -5.183373\n",
      "ep 2772: ep_len:422 episode reward: total was 2.140000. running mean: -5.110140\n",
      "ep 2772: ep_len:523 episode reward: total was -5.230000. running mean: -5.111338\n",
      "ep 2772: ep_len:3 episode reward: total was 1.010000. running mean: -5.050125\n",
      "ep 2772: ep_len:590 episode reward: total was -33.090000. running mean: -5.330524\n",
      "ep 2772: ep_len:293 episode reward: total was -6.180000. running mean: -5.339018\n",
      "epsilon:0.077064 episode_count: 19411. steps_count: 8397193.000000\n",
      "Time elapsed:  24697.776782035828\n",
      "ep 2773: ep_len:643 episode reward: total was -57.470000. running mean: -5.860328\n",
      "ep 2773: ep_len:500 episode reward: total was -15.130000. running mean: -5.953025\n",
      "ep 2773: ep_len:522 episode reward: total was -55.150000. running mean: -6.444995\n",
      "ep 2773: ep_len:518 episode reward: total was -2.480000. running mean: -6.405345\n",
      "ep 2773: ep_len:132 episode reward: total was 4.380000. running mean: -6.297491\n",
      "ep 2773: ep_len:310 episode reward: total was 21.420000. running mean: -6.020316\n",
      "ep 2773: ep_len:207 episode reward: total was 6.120000. running mean: -5.898913\n",
      "epsilon:0.077019 episode_count: 19418. steps_count: 8400025.000000\n",
      "Time elapsed:  24709.368052721024\n",
      "ep 2774: ep_len:659 episode reward: total was -50.260000. running mean: -6.342524\n",
      "ep 2774: ep_len:505 episode reward: total was -11.520000. running mean: -6.394299\n",
      "ep 2774: ep_len:637 episode reward: total was -36.800000. running mean: -6.698356\n",
      "ep 2774: ep_len:502 episode reward: total was 1.980000. running mean: -6.611572\n",
      "ep 2774: ep_len:3 episode reward: total was 1.010000. running mean: -6.535356\n",
      "ep 2774: ep_len:638 episode reward: total was -83.310000. running mean: -7.303103\n",
      "ep 2774: ep_len:531 episode reward: total was -5.150000. running mean: -7.281572\n",
      "epsilon:0.076975 episode_count: 19425. steps_count: 8403500.000000\n",
      "Time elapsed:  24718.351860284805\n",
      "ep 2775: ep_len:519 episode reward: total was 13.280000. running mean: -7.075956\n",
      "ep 2775: ep_len:580 episode reward: total was 32.200000. running mean: -6.683197\n",
      "ep 2775: ep_len:657 episode reward: total was -23.070000. running mean: -6.847065\n",
      "ep 2775: ep_len:500 episode reward: total was 10.210000. running mean: -6.676494\n",
      "ep 2775: ep_len:3 episode reward: total was 1.010000. running mean: -6.599629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2775: ep_len:517 episode reward: total was -8.170000. running mean: -6.615333\n",
      "ep 2775: ep_len:595 episode reward: total was -42.750000. running mean: -6.976679\n",
      "epsilon:0.076931 episode_count: 19432. steps_count: 8406871.000000\n",
      "Time elapsed:  24727.32498049736\n",
      "ep 2776: ep_len:517 episode reward: total was 39.370000. running mean: -6.513213\n",
      "ep 2776: ep_len:527 episode reward: total was 1.540000. running mean: -6.432681\n",
      "ep 2776: ep_len:366 episode reward: total was 17.280000. running mean: -6.195554\n",
      "ep 2776: ep_len:514 episode reward: total was 48.440000. running mean: -5.649198\n",
      "ep 2776: ep_len:3 episode reward: total was 0.000000. running mean: -5.592706\n",
      "ep 2776: ep_len:227 episode reward: total was 26.870000. running mean: -5.268079\n",
      "ep 2776: ep_len:593 episode reward: total was -31.110000. running mean: -5.526498\n",
      "epsilon:0.076886 episode_count: 19439. steps_count: 8409618.000000\n",
      "Time elapsed:  24734.757441997528\n",
      "ep 2777: ep_len:579 episode reward: total was 38.170000. running mean: -5.089533\n",
      "ep 2777: ep_len:514 episode reward: total was -30.800000. running mean: -5.346638\n",
      "ep 2777: ep_len:573 episode reward: total was 0.270000. running mean: -5.290472\n",
      "ep 2777: ep_len:56 episode reward: total was 0.330000. running mean: -5.234267\n",
      "ep 2777: ep_len:3 episode reward: total was 1.010000. running mean: -5.171824\n",
      "ep 2777: ep_len:500 episode reward: total was -61.530000. running mean: -5.735406\n",
      "ep 2777: ep_len:568 episode reward: total was 7.210000. running mean: -5.605952\n",
      "epsilon:0.076842 episode_count: 19446. steps_count: 8412411.000000\n",
      "Time elapsed:  24745.739431142807\n",
      "ep 2778: ep_len:515 episode reward: total was 2.510000. running mean: -5.524792\n",
      "ep 2778: ep_len:500 episode reward: total was 23.250000. running mean: -5.237045\n",
      "ep 2778: ep_len:548 episode reward: total was -1.960000. running mean: -5.204274\n",
      "ep 2778: ep_len:106 episode reward: total was -3.050000. running mean: -5.182731\n",
      "ep 2778: ep_len:90 episode reward: total was 19.720000. running mean: -4.933704\n",
      "ep 2778: ep_len:646 episode reward: total was -155.820000. running mean: -6.442567\n",
      "ep 2778: ep_len:519 episode reward: total was 23.590000. running mean: -6.142241\n",
      "epsilon:0.076798 episode_count: 19453. steps_count: 8415335.000000\n",
      "Time elapsed:  24751.703831911087\n",
      "ep 2779: ep_len:629 episode reward: total was -71.750000. running mean: -6.798319\n",
      "ep 2779: ep_len:500 episode reward: total was 30.740000. running mean: -6.422936\n",
      "ep 2779: ep_len:452 episode reward: total was -4.820000. running mean: -6.406906\n",
      "ep 2779: ep_len:500 episode reward: total was -52.130000. running mean: -6.864137\n",
      "ep 2779: ep_len:113 episode reward: total was 30.240000. running mean: -6.493096\n",
      "ep 2779: ep_len:635 episode reward: total was -59.460000. running mean: -7.022765\n",
      "ep 2779: ep_len:328 episode reward: total was -6.690000. running mean: -7.019437\n",
      "epsilon:0.076753 episode_count: 19460. steps_count: 8418492.000000\n",
      "Time elapsed:  24760.10398697853\n",
      "ep 2780: ep_len:543 episode reward: total was -3.520000. running mean: -6.984443\n",
      "ep 2780: ep_len:554 episode reward: total was -46.180000. running mean: -7.376398\n",
      "ep 2780: ep_len:638 episode reward: total was -14.110000. running mean: -7.443735\n",
      "ep 2780: ep_len:363 episode reward: total was -3.100000. running mean: -7.400297\n",
      "ep 2780: ep_len:3 episode reward: total was 1.010000. running mean: -7.316194\n",
      "ep 2780: ep_len:508 episode reward: total was 53.980000. running mean: -6.703232\n",
      "ep 2780: ep_len:623 episode reward: total was 17.330000. running mean: -6.462900\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.076709 episode_count: 19467. steps_count: 8421724.000000\n",
      "Time elapsed:  24772.135516643524\n",
      "ep 2781: ep_len:609 episode reward: total was 14.740000. running mean: -6.250871\n",
      "ep 2781: ep_len:577 episode reward: total was -18.050000. running mean: -6.368862\n",
      "ep 2781: ep_len:668 episode reward: total was -33.050000. running mean: -6.635674\n",
      "ep 2781: ep_len:118 episode reward: total was 11.440000. running mean: -6.454917\n",
      "ep 2781: ep_len:3 episode reward: total was 1.010000. running mean: -6.380268\n",
      "ep 2781: ep_len:569 episode reward: total was 14.230000. running mean: -6.174165\n",
      "ep 2781: ep_len:500 episode reward: total was -29.880000. running mean: -6.411223\n",
      "epsilon:0.076665 episode_count: 19474. steps_count: 8424768.000000\n",
      "Time elapsed:  24780.182293891907\n",
      "ep 2782: ep_len:528 episode reward: total was -44.820000. running mean: -6.795311\n",
      "ep 2782: ep_len:263 episode reward: total was -4.410000. running mean: -6.771458\n",
      "ep 2782: ep_len:500 episode reward: total was 2.240000. running mean: -6.681343\n",
      "ep 2782: ep_len:514 episode reward: total was -8.620000. running mean: -6.700730\n",
      "ep 2782: ep_len:3 episode reward: total was 1.010000. running mean: -6.623623\n",
      "ep 2782: ep_len:601 episode reward: total was -49.740000. running mean: -7.054786\n",
      "ep 2782: ep_len:207 episode reward: total was -1.960000. running mean: -7.003839\n",
      "epsilon:0.076620 episode_count: 19481. steps_count: 8427384.000000\n",
      "Time elapsed:  24791.190289974213\n",
      "ep 2783: ep_len:629 episode reward: total was -45.240000. running mean: -7.386200\n",
      "ep 2783: ep_len:524 episode reward: total was 12.760000. running mean: -7.184738\n",
      "ep 2783: ep_len:720 episode reward: total was -126.320000. running mean: -8.376091\n",
      "ep 2783: ep_len:527 episode reward: total was 10.340000. running mean: -8.188930\n",
      "ep 2783: ep_len:3 episode reward: total was -0.490000. running mean: -8.111941\n",
      "ep 2783: ep_len:500 episode reward: total was 7.850000. running mean: -7.952321\n",
      "ep 2783: ep_len:629 episode reward: total was -35.080000. running mean: -8.223598\n",
      "epsilon:0.076576 episode_count: 19488. steps_count: 8430916.000000\n",
      "Time elapsed:  24804.321788549423\n",
      "ep 2784: ep_len:500 episode reward: total was 22.260000. running mean: -7.918762\n",
      "ep 2784: ep_len:500 episode reward: total was 25.370000. running mean: -7.585874\n",
      "ep 2784: ep_len:500 episode reward: total was -25.210000. running mean: -7.762116\n",
      "ep 2784: ep_len:516 episode reward: total was 27.950000. running mean: -7.404995\n",
      "ep 2784: ep_len:47 episode reward: total was 19.000000. running mean: -7.140945\n",
      "ep 2784: ep_len:320 episode reward: total was 19.710000. running mean: -6.872435\n",
      "ep 2784: ep_len:540 episode reward: total was 7.210000. running mean: -6.731611\n",
      "epsilon:0.076532 episode_count: 19495. steps_count: 8433839.000000\n",
      "Time elapsed:  24812.090636491776\n",
      "ep 2785: ep_len:500 episode reward: total was -24.420000. running mean: -6.908495\n",
      "ep 2785: ep_len:502 episode reward: total was -4.370000. running mean: -6.883110\n",
      "ep 2785: ep_len:500 episode reward: total was -28.450000. running mean: -7.098779\n",
      "ep 2785: ep_len:506 episode reward: total was 18.790000. running mean: -6.839891\n",
      "ep 2785: ep_len:3 episode reward: total was 1.010000. running mean: -6.761392\n",
      "ep 2785: ep_len:603 episode reward: total was -14.510000. running mean: -6.838878\n",
      "ep 2785: ep_len:500 episode reward: total was -13.780000. running mean: -6.908289\n",
      "epsilon:0.076487 episode_count: 19502. steps_count: 8436953.000000\n",
      "Time elapsed:  24820.394973039627\n",
      "ep 2786: ep_len:669 episode reward: total was -16.860000. running mean: -7.007806\n",
      "ep 2786: ep_len:537 episode reward: total was 0.310000. running mean: -6.934628\n",
      "ep 2786: ep_len:615 episode reward: total was -11.190000. running mean: -6.977182\n",
      "ep 2786: ep_len:500 episode reward: total was 2.280000. running mean: -6.884610\n",
      "ep 2786: ep_len:81 episode reward: total was 23.240000. running mean: -6.583364\n",
      "ep 2786: ep_len:289 episode reward: total was -7.600000. running mean: -6.593530\n",
      "ep 2786: ep_len:500 episode reward: total was -9.420000. running mean: -6.621795\n",
      "epsilon:0.076443 episode_count: 19509. steps_count: 8440144.000000\n",
      "Time elapsed:  24827.786682844162\n",
      "ep 2787: ep_len:207 episode reward: total was 13.140000. running mean: -6.424177\n",
      "ep 2787: ep_len:631 episode reward: total was -10.730000. running mean: -6.467235\n",
      "ep 2787: ep_len:554 episode reward: total was -16.810000. running mean: -6.570663\n",
      "ep 2787: ep_len:618 episode reward: total was 60.190000. running mean: -5.903056\n",
      "ep 2787: ep_len:3 episode reward: total was 1.010000. running mean: -5.833926\n",
      "ep 2787: ep_len:599 episode reward: total was 4.890000. running mean: -5.726687\n",
      "ep 2787: ep_len:558 episode reward: total was -19.760000. running mean: -5.867020\n",
      "epsilon:0.076399 episode_count: 19516. steps_count: 8443314.000000\n",
      "Time elapsed:  24844.839849233627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2788: ep_len:663 episode reward: total was -58.520000. running mean: -6.393550\n",
      "ep 2788: ep_len:579 episode reward: total was -6.000000. running mean: -6.389614\n",
      "ep 2788: ep_len:539 episode reward: total was 34.530000. running mean: -5.980418\n",
      "ep 2788: ep_len:160 episode reward: total was 16.640000. running mean: -5.754214\n",
      "ep 2788: ep_len:3 episode reward: total was 1.010000. running mean: -5.686572\n",
      "ep 2788: ep_len:591 episode reward: total was 2.620000. running mean: -5.603506\n",
      "ep 2788: ep_len:500 episode reward: total was -27.160000. running mean: -5.819071\n",
      "epsilon:0.076354 episode_count: 19523. steps_count: 8446349.000000\n",
      "Time elapsed:  24852.978709697723\n",
      "ep 2789: ep_len:563 episode reward: total was -0.030000. running mean: -5.761180\n",
      "ep 2789: ep_len:500 episode reward: total was -0.950000. running mean: -5.713068\n",
      "ep 2789: ep_len:571 episode reward: total was 9.840000. running mean: -5.557538\n",
      "ep 2789: ep_len:500 episode reward: total was 51.940000. running mean: -4.982562\n",
      "ep 2789: ep_len:3 episode reward: total was 1.010000. running mean: -4.922637\n",
      "ep 2789: ep_len:563 episode reward: total was -28.810000. running mean: -5.161510\n",
      "ep 2789: ep_len:521 episode reward: total was 20.430000. running mean: -4.905595\n",
      "epsilon:0.076310 episode_count: 19530. steps_count: 8449570.000000\n",
      "Time elapsed:  24866.196935653687\n",
      "ep 2790: ep_len:647 episode reward: total was -6.490000. running mean: -4.921439\n",
      "ep 2790: ep_len:505 episode reward: total was -59.350000. running mean: -5.465725\n",
      "ep 2790: ep_len:382 episode reward: total was 30.210000. running mean: -5.108968\n",
      "ep 2790: ep_len:613 episode reward: total was 29.350000. running mean: -4.764378\n",
      "ep 2790: ep_len:3 episode reward: total was 1.010000. running mean: -4.706634\n",
      "ep 2790: ep_len:557 episode reward: total was -28.560000. running mean: -4.945168\n",
      "ep 2790: ep_len:507 episode reward: total was 6.020000. running mean: -4.835516\n",
      "epsilon:0.076266 episode_count: 19537. steps_count: 8452784.000000\n",
      "Time elapsed:  24874.739933013916\n",
      "ep 2791: ep_len:500 episode reward: total was 78.980000. running mean: -3.997361\n",
      "ep 2791: ep_len:500 episode reward: total was 42.520000. running mean: -3.532187\n",
      "ep 2791: ep_len:632 episode reward: total was 10.590000. running mean: -3.390965\n",
      "ep 2791: ep_len:533 episode reward: total was 20.680000. running mean: -3.150256\n",
      "ep 2791: ep_len:3 episode reward: total was 1.010000. running mean: -3.108653\n",
      "ep 2791: ep_len:522 episode reward: total was -59.950000. running mean: -3.677067\n",
      "ep 2791: ep_len:500 episode reward: total was -28.560000. running mean: -3.925896\n",
      "epsilon:0.076221 episode_count: 19544. steps_count: 8455974.000000\n",
      "Time elapsed:  24883.27734851837\n",
      "ep 2792: ep_len:514 episode reward: total was -11.470000. running mean: -4.001337\n",
      "ep 2792: ep_len:255 episode reward: total was -68.470000. running mean: -4.646024\n",
      "ep 2792: ep_len:500 episode reward: total was -26.480000. running mean: -4.864363\n",
      "ep 2792: ep_len:500 episode reward: total was 17.310000. running mean: -4.642620\n",
      "ep 2792: ep_len:106 episode reward: total was 32.220000. running mean: -4.273994\n",
      "ep 2792: ep_len:500 episode reward: total was 52.640000. running mean: -3.704854\n",
      "ep 2792: ep_len:532 episode reward: total was -54.610000. running mean: -4.213905\n",
      "epsilon:0.076177 episode_count: 19551. steps_count: 8458881.000000\n",
      "Time elapsed:  24894.82109451294\n",
      "ep 2793: ep_len:214 episode reward: total was 6.170000. running mean: -4.110066\n",
      "ep 2793: ep_len:567 episode reward: total was -22.940000. running mean: -4.298365\n",
      "ep 2793: ep_len:500 episode reward: total was -11.630000. running mean: -4.371682\n",
      "ep 2793: ep_len:500 episode reward: total was -12.310000. running mean: -4.451065\n",
      "ep 2793: ep_len:99 episode reward: total was 27.280000. running mean: -4.133754\n",
      "ep 2793: ep_len:513 episode reward: total was -23.670000. running mean: -4.329117\n",
      "ep 2793: ep_len:302 episode reward: total was -3.160000. running mean: -4.317426\n",
      "epsilon:0.076133 episode_count: 19558. steps_count: 8461576.000000\n",
      "Time elapsed:  24902.188928365707\n",
      "ep 2794: ep_len:554 episode reward: total was 74.620000. running mean: -3.528051\n",
      "ep 2794: ep_len:525 episode reward: total was -7.870000. running mean: -3.571471\n",
      "ep 2794: ep_len:587 episode reward: total was -33.070000. running mean: -3.866456\n",
      "ep 2794: ep_len:551 episode reward: total was 3.980000. running mean: -3.787992\n",
      "ep 2794: ep_len:3 episode reward: total was 1.010000. running mean: -3.740012\n",
      "ep 2794: ep_len:556 episode reward: total was 11.560000. running mean: -3.587012\n",
      "ep 2794: ep_len:569 episode reward: total was 1.590000. running mean: -3.535241\n",
      "epsilon:0.076088 episode_count: 19565. steps_count: 8464921.000000\n",
      "Time elapsed:  24915.05451464653\n",
      "ep 2795: ep_len:246 episode reward: total was 4.330000. running mean: -3.456589\n",
      "ep 2795: ep_len:545 episode reward: total was -31.430000. running mean: -3.736323\n",
      "ep 2795: ep_len:502 episode reward: total was 4.780000. running mean: -3.651160\n",
      "ep 2795: ep_len:500 episode reward: total was 24.270000. running mean: -3.371948\n",
      "ep 2795: ep_len:74 episode reward: total was 14.750000. running mean: -3.190729\n",
      "ep 2795: ep_len:500 episode reward: total was -59.390000. running mean: -3.752722\n",
      "ep 2795: ep_len:524 episode reward: total was 27.330000. running mean: -3.441894\n",
      "epsilon:0.076044 episode_count: 19572. steps_count: 8467812.000000\n",
      "Time elapsed:  24922.70935869217\n",
      "ep 2796: ep_len:547 episode reward: total was -16.960000. running mean: -3.577075\n",
      "ep 2796: ep_len:360 episode reward: total was -1.370000. running mean: -3.555005\n",
      "ep 2796: ep_len:566 episode reward: total was 14.650000. running mean: -3.372955\n",
      "ep 2796: ep_len:501 episode reward: total was 23.930000. running mean: -3.099925\n",
      "ep 2796: ep_len:3 episode reward: total was -0.490000. running mean: -3.073826\n",
      "ep 2796: ep_len:500 episode reward: total was 20.290000. running mean: -2.840188\n",
      "ep 2796: ep_len:520 episode reward: total was -182.310000. running mean: -4.634886\n",
      "epsilon:0.076000 episode_count: 19579. steps_count: 8470809.000000\n",
      "Time elapsed:  24930.74676179886\n",
      "ep 2797: ep_len:570 episode reward: total was 28.260000. running mean: -4.305937\n",
      "ep 2797: ep_len:500 episode reward: total was 8.440000. running mean: -4.178477\n",
      "ep 2797: ep_len:362 episode reward: total was 43.350000. running mean: -3.703193\n",
      "ep 2797: ep_len:56 episode reward: total was -1.170000. running mean: -3.677861\n",
      "ep 2797: ep_len:3 episode reward: total was 1.010000. running mean: -3.630982\n",
      "ep 2797: ep_len:618 episode reward: total was -17.700000. running mean: -3.771672\n",
      "ep 2797: ep_len:597 episode reward: total was 29.250000. running mean: -3.441456\n",
      "epsilon:0.075955 episode_count: 19586. steps_count: 8473515.000000\n",
      "Time elapsed:  24942.525653123856\n",
      "ep 2798: ep_len:644 episode reward: total was -30.030000. running mean: -3.707341\n",
      "ep 2798: ep_len:503 episode reward: total was -12.590000. running mean: -3.796168\n",
      "ep 2798: ep_len:555 episode reward: total was -53.070000. running mean: -4.288906\n",
      "ep 2798: ep_len:613 episode reward: total was 69.540000. running mean: -3.550617\n",
      "ep 2798: ep_len:3 episode reward: total was 1.010000. running mean: -3.505011\n",
      "ep 2798: ep_len:582 episode reward: total was -28.330000. running mean: -3.753261\n",
      "ep 2798: ep_len:500 episode reward: total was -20.380000. running mean: -3.919528\n",
      "epsilon:0.075911 episode_count: 19593. steps_count: 8476915.000000\n",
      "Time elapsed:  24950.628417253494\n",
      "ep 2799: ep_len:535 episode reward: total was -22.360000. running mean: -4.103933\n",
      "ep 2799: ep_len:500 episode reward: total was 37.010000. running mean: -3.692793\n",
      "ep 2799: ep_len:447 episode reward: total was 36.230000. running mean: -3.293565\n",
      "ep 2799: ep_len:507 episode reward: total was -3.090000. running mean: -3.291530\n",
      "ep 2799: ep_len:3 episode reward: total was 1.010000. running mean: -3.248514\n",
      "ep 2799: ep_len:500 episode reward: total was -54.400000. running mean: -3.760029\n",
      "ep 2799: ep_len:566 episode reward: total was -132.660000. running mean: -5.049029\n",
      "epsilon:0.075867 episode_count: 19600. steps_count: 8479973.000000\n",
      "Time elapsed:  24958.261947631836\n",
      "ep 2800: ep_len:253 episode reward: total was -10.660000. running mean: -5.105139\n",
      "ep 2800: ep_len:500 episode reward: total was -40.670000. running mean: -5.460787\n",
      "ep 2800: ep_len:443 episode reward: total was 50.360000. running mean: -4.902579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2800: ep_len:563 episode reward: total was 31.660000. running mean: -4.536954\n",
      "ep 2800: ep_len:107 episode reward: total was -60.200000. running mean: -5.093584\n",
      "ep 2800: ep_len:508 episode reward: total was -4.740000. running mean: -5.090048\n",
      "ep 2800: ep_len:590 episode reward: total was -18.440000. running mean: -5.223548\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.075822 episode_count: 19607. steps_count: 8482937.000000\n",
      "Time elapsed:  24972.16206932068\n",
      "ep 2801: ep_len:541 episode reward: total was 45.980000. running mean: -4.711512\n",
      "ep 2801: ep_len:548 episode reward: total was -76.660000. running mean: -5.430997\n",
      "ep 2801: ep_len:79 episode reward: total was 10.820000. running mean: -5.268487\n",
      "ep 2801: ep_len:56 episode reward: total was 1.310000. running mean: -5.202702\n",
      "ep 2801: ep_len:98 episode reward: total was 23.260000. running mean: -4.918075\n",
      "ep 2801: ep_len:583 episode reward: total was 41.540000. running mean: -4.453495\n",
      "ep 2801: ep_len:603 episode reward: total was 14.290000. running mean: -4.266060\n",
      "epsilon:0.075778 episode_count: 19614. steps_count: 8485445.000000\n",
      "Time elapsed:  24980.016573667526\n",
      "ep 2802: ep_len:500 episode reward: total was 10.200000. running mean: -4.121399\n",
      "ep 2802: ep_len:503 episode reward: total was 65.250000. running mean: -3.427685\n",
      "ep 2802: ep_len:414 episode reward: total was 49.480000. running mean: -2.898608\n",
      "ep 2802: ep_len:539 episode reward: total was 28.290000. running mean: -2.586722\n",
      "ep 2802: ep_len:3 episode reward: total was 1.010000. running mean: -2.550755\n",
      "ep 2802: ep_len:503 episode reward: total was 7.770000. running mean: -2.447547\n",
      "ep 2802: ep_len:661 episode reward: total was -98.500000. running mean: -3.408072\n",
      "epsilon:0.075734 episode_count: 19621. steps_count: 8488568.000000\n",
      "Time elapsed:  24997.29812312126\n",
      "ep 2803: ep_len:537 episode reward: total was 51.460000. running mean: -2.859391\n",
      "ep 2803: ep_len:570 episode reward: total was -37.790000. running mean: -3.208697\n",
      "ep 2803: ep_len:510 episode reward: total was -1.190000. running mean: -3.188510\n",
      "ep 2803: ep_len:520 episode reward: total was -3.870000. running mean: -3.195325\n",
      "ep 2803: ep_len:3 episode reward: total was 1.010000. running mean: -3.153272\n",
      "ep 2803: ep_len:500 episode reward: total was -15.130000. running mean: -3.273039\n",
      "ep 2803: ep_len:535 episode reward: total was 1.090000. running mean: -3.229409\n",
      "epsilon:0.075689 episode_count: 19628. steps_count: 8491743.000000\n",
      "Time elapsed:  25006.583925247192\n",
      "ep 2804: ep_len:536 episode reward: total was 24.640000. running mean: -2.950715\n",
      "ep 2804: ep_len:643 episode reward: total was 38.370000. running mean: -2.537508\n",
      "ep 2804: ep_len:348 episode reward: total was 29.880000. running mean: -2.213333\n",
      "ep 2804: ep_len:500 episode reward: total was 36.360000. running mean: -1.827599\n",
      "ep 2804: ep_len:3 episode reward: total was 1.010000. running mean: -1.799223\n",
      "ep 2804: ep_len:126 episode reward: total was 17.450000. running mean: -1.606731\n",
      "ep 2804: ep_len:354 episode reward: total was 3.530000. running mean: -1.555364\n",
      "epsilon:0.075645 episode_count: 19635. steps_count: 8494253.000000\n",
      "Time elapsed:  25013.590671300888\n",
      "ep 2805: ep_len:527 episode reward: total was -45.800000. running mean: -1.997810\n",
      "ep 2805: ep_len:604 episode reward: total was 2.680000. running mean: -1.951032\n",
      "ep 2805: ep_len:62 episode reward: total was 5.260000. running mean: -1.878922\n",
      "ep 2805: ep_len:504 episode reward: total was 4.610000. running mean: -1.814032\n",
      "ep 2805: ep_len:3 episode reward: total was 1.010000. running mean: -1.785792\n",
      "ep 2805: ep_len:500 episode reward: total was 3.760000. running mean: -1.730334\n",
      "ep 2805: ep_len:302 episode reward: total was -1.120000. running mean: -1.724231\n",
      "epsilon:0.075601 episode_count: 19642. steps_count: 8496755.000000\n",
      "Time elapsed:  25027.187610387802\n",
      "ep 2806: ep_len:134 episode reward: total was -5.550000. running mean: -1.762488\n",
      "ep 2806: ep_len:562 episode reward: total was -466.670000. running mean: -6.411564\n",
      "ep 2806: ep_len:587 episode reward: total was -40.690000. running mean: -6.754348\n",
      "ep 2806: ep_len:500 episode reward: total was 8.540000. running mean: -6.601404\n",
      "ep 2806: ep_len:3 episode reward: total was -0.490000. running mean: -6.540290\n",
      "ep 2806: ep_len:500 episode reward: total was 13.810000. running mean: -6.336788\n",
      "ep 2806: ep_len:595 episode reward: total was 18.830000. running mean: -6.085120\n",
      "epsilon:0.075556 episode_count: 19649. steps_count: 8499636.000000\n",
      "Time elapsed:  25036.0051009655\n",
      "ep 2807: ep_len:508 episode reward: total was -30.380000. running mean: -6.328068\n",
      "ep 2807: ep_len:511 episode reward: total was 74.890000. running mean: -5.515888\n",
      "ep 2807: ep_len:500 episode reward: total was 22.660000. running mean: -5.234129\n",
      "ep 2807: ep_len:574 episode reward: total was 57.540000. running mean: -4.606388\n",
      "ep 2807: ep_len:3 episode reward: total was 1.010000. running mean: -4.550224\n",
      "ep 2807: ep_len:539 episode reward: total was -15.180000. running mean: -4.656522\n",
      "ep 2807: ep_len:513 episode reward: total was 13.710000. running mean: -4.472856\n",
      "epsilon:0.075512 episode_count: 19656. steps_count: 8502784.000000\n",
      "Time elapsed:  25051.43305015564\n",
      "ep 2808: ep_len:550 episode reward: total was -74.920000. running mean: -5.177328\n",
      "ep 2808: ep_len:500 episode reward: total was 12.610000. running mean: -4.999454\n",
      "ep 2808: ep_len:439 episode reward: total was 46.130000. running mean: -4.488160\n",
      "ep 2808: ep_len:132 episode reward: total was 11.140000. running mean: -4.331878\n",
      "ep 2808: ep_len:3 episode reward: total was 1.010000. running mean: -4.278460\n",
      "ep 2808: ep_len:535 episode reward: total was -50.590000. running mean: -4.741575\n",
      "ep 2808: ep_len:513 episode reward: total was -33.350000. running mean: -5.027659\n",
      "epsilon:0.075468 episode_count: 19663. steps_count: 8505456.000000\n",
      "Time elapsed:  25057.10506439209\n",
      "ep 2809: ep_len:558 episode reward: total was 29.310000. running mean: -4.684283\n",
      "ep 2809: ep_len:546 episode reward: total was -73.690000. running mean: -5.374340\n",
      "ep 2809: ep_len:619 episode reward: total was -14.540000. running mean: -5.465996\n",
      "ep 2809: ep_len:500 episode reward: total was -36.800000. running mean: -5.779336\n",
      "ep 2809: ep_len:103 episode reward: total was 25.760000. running mean: -5.463943\n",
      "ep 2809: ep_len:638 episode reward: total was 26.000000. running mean: -5.149304\n",
      "ep 2809: ep_len:180 episode reward: total was -13.560000. running mean: -5.233411\n",
      "epsilon:0.075423 episode_count: 19670. steps_count: 8508600.000000\n",
      "Time elapsed:  25063.373844861984\n",
      "ep 2810: ep_len:555 episode reward: total was -64.120000. running mean: -5.822276\n",
      "ep 2810: ep_len:522 episode reward: total was 51.990000. running mean: -5.244154\n",
      "ep 2810: ep_len:500 episode reward: total was -12.520000. running mean: -5.316912\n",
      "ep 2810: ep_len:395 episode reward: total was 6.430000. running mean: -5.199443\n",
      "ep 2810: ep_len:3 episode reward: total was 1.010000. running mean: -5.137349\n",
      "ep 2810: ep_len:501 episode reward: total was 35.770000. running mean: -4.728275\n",
      "ep 2810: ep_len:522 episode reward: total was -42.760000. running mean: -5.108592\n",
      "epsilon:0.075379 episode_count: 19677. steps_count: 8511598.000000\n",
      "Time elapsed:  25071.20630145073\n",
      "ep 2811: ep_len:568 episode reward: total was 22.690000. running mean: -4.830606\n",
      "ep 2811: ep_len:500 episode reward: total was 39.430000. running mean: -4.388000\n",
      "ep 2811: ep_len:76 episode reward: total was 0.780000. running mean: -4.336320\n",
      "ep 2811: ep_len:569 episode reward: total was 30.470000. running mean: -3.988257\n",
      "ep 2811: ep_len:3 episode reward: total was 1.010000. running mean: -3.938275\n",
      "ep 2811: ep_len:164 episode reward: total was 21.540000. running mean: -3.683492\n",
      "ep 2811: ep_len:599 episode reward: total was 14.210000. running mean: -3.504557\n",
      "epsilon:0.075335 episode_count: 19684. steps_count: 8514077.000000\n",
      "Time elapsed:  25079.051040887833\n",
      "ep 2812: ep_len:566 episode reward: total was 21.600000. running mean: -3.253511\n",
      "ep 2812: ep_len:637 episode reward: total was 75.490000. running mean: -2.466076\n",
      "ep 2812: ep_len:650 episode reward: total was -3.460000. running mean: -2.476015\n",
      "ep 2812: ep_len:500 episode reward: total was -38.490000. running mean: -2.836155\n",
      "ep 2812: ep_len:3 episode reward: total was 1.010000. running mean: -2.797694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2812: ep_len:527 episode reward: total was -26.450000. running mean: -3.034217\n",
      "ep 2812: ep_len:503 episode reward: total was -16.740000. running mean: -3.171275\n",
      "epsilon:0.075290 episode_count: 19691. steps_count: 8517463.000000\n",
      "Time elapsed:  25089.149924993515\n",
      "ep 2813: ep_len:574 episode reward: total was -29.900000. running mean: -3.438562\n",
      "ep 2813: ep_len:500 episode reward: total was -10.060000. running mean: -3.504776\n",
      "ep 2813: ep_len:379 episode reward: total was 44.930000. running mean: -3.020429\n",
      "ep 2813: ep_len:500 episode reward: total was 20.770000. running mean: -2.782524\n",
      "ep 2813: ep_len:128 episode reward: total was 22.860000. running mean: -2.526099\n",
      "ep 2813: ep_len:501 episode reward: total was -38.690000. running mean: -2.887738\n",
      "ep 2813: ep_len:548 episode reward: total was -42.140000. running mean: -3.280261\n",
      "epsilon:0.075246 episode_count: 19698. steps_count: 8520593.000000\n",
      "Time elapsed:  25100.015137910843\n",
      "ep 2814: ep_len:608 episode reward: total was -44.160000. running mean: -3.689058\n",
      "ep 2814: ep_len:500 episode reward: total was 20.800000. running mean: -3.444167\n",
      "ep 2814: ep_len:655 episode reward: total was -14.820000. running mean: -3.557926\n",
      "ep 2814: ep_len:500 episode reward: total was 81.090000. running mean: -2.711447\n",
      "ep 2814: ep_len:85 episode reward: total was 20.250000. running mean: -2.481832\n",
      "ep 2814: ep_len:619 episode reward: total was 8.400000. running mean: -2.373014\n",
      "ep 2814: ep_len:588 episode reward: total was 4.980000. running mean: -2.299484\n",
      "epsilon:0.075202 episode_count: 19705. steps_count: 8524148.000000\n",
      "Time elapsed:  25110.624465465546\n",
      "ep 2815: ep_len:500 episode reward: total was 43.200000. running mean: -1.844489\n",
      "ep 2815: ep_len:500 episode reward: total was 69.700000. running mean: -1.129044\n",
      "ep 2815: ep_len:561 episode reward: total was -17.230000. running mean: -1.290053\n",
      "ep 2815: ep_len:590 episode reward: total was 37.260000. running mean: -0.904553\n",
      "ep 2815: ep_len:41 episode reward: total was 16.000000. running mean: -0.735507\n",
      "ep 2815: ep_len:558 episode reward: total was -62.100000. running mean: -1.349152\n",
      "ep 2815: ep_len:500 episode reward: total was 16.840000. running mean: -1.167261\n",
      "epsilon:0.075157 episode_count: 19712. steps_count: 8527398.000000\n",
      "Time elapsed:  25126.343554735184\n",
      "ep 2816: ep_len:663 episode reward: total was -52.820000. running mean: -1.683788\n",
      "ep 2816: ep_len:500 episode reward: total was 13.810000. running mean: -1.528850\n",
      "ep 2816: ep_len:411 episode reward: total was 46.610000. running mean: -1.047462\n",
      "ep 2816: ep_len:501 episode reward: total was 50.820000. running mean: -0.528787\n",
      "ep 2816: ep_len:3 episode reward: total was 1.010000. running mean: -0.513399\n",
      "ep 2816: ep_len:510 episode reward: total was -16.830000. running mean: -0.676565\n",
      "ep 2816: ep_len:556 episode reward: total was -7.570000. running mean: -0.745500\n",
      "epsilon:0.075113 episode_count: 19719. steps_count: 8530542.000000\n",
      "Time elapsed:  25143.886303424835\n",
      "ep 2817: ep_len:585 episode reward: total was -17.180000. running mean: -0.909845\n",
      "ep 2817: ep_len:602 episode reward: total was -94.290000. running mean: -1.843646\n",
      "ep 2817: ep_len:587 episode reward: total was 8.850000. running mean: -1.736710\n",
      "ep 2817: ep_len:508 episode reward: total was 19.060000. running mean: -1.528743\n",
      "ep 2817: ep_len:96 episode reward: total was 22.720000. running mean: -1.286255\n",
      "ep 2817: ep_len:538 episode reward: total was -20.930000. running mean: -1.482693\n",
      "ep 2817: ep_len:211 episode reward: total was -8.470000. running mean: -1.552566\n",
      "epsilon:0.075069 episode_count: 19726. steps_count: 8533669.000000\n",
      "Time elapsed:  25153.278460025787\n",
      "ep 2818: ep_len:216 episode reward: total was 8.670000. running mean: -1.450340\n",
      "ep 2818: ep_len:331 episode reward: total was -50.720000. running mean: -1.943037\n",
      "ep 2818: ep_len:451 episode reward: total was 38.690000. running mean: -1.536706\n",
      "ep 2818: ep_len:132 episode reward: total was 16.110000. running mean: -1.360239\n",
      "ep 2818: ep_len:3 episode reward: total was 1.010000. running mean: -1.336537\n",
      "ep 2818: ep_len:500 episode reward: total was 16.250000. running mean: -1.160671\n",
      "ep 2818: ep_len:501 episode reward: total was 23.130000. running mean: -0.917765\n",
      "epsilon:0.075024 episode_count: 19733. steps_count: 8535803.000000\n",
      "Time elapsed:  25160.019264936447\n",
      "ep 2819: ep_len:611 episode reward: total was -41.720000. running mean: -1.325787\n",
      "ep 2819: ep_len:500 episode reward: total was -22.570000. running mean: -1.538229\n",
      "ep 2819: ep_len:373 episode reward: total was 34.950000. running mean: -1.173347\n",
      "ep 2819: ep_len:513 episode reward: total was -17.720000. running mean: -1.338814\n",
      "ep 2819: ep_len:3 episode reward: total was 1.010000. running mean: -1.315325\n",
      "ep 2819: ep_len:500 episode reward: total was -26.790000. running mean: -1.570072\n",
      "ep 2819: ep_len:581 episode reward: total was 15.130000. running mean: -1.403071\n",
      "epsilon:0.074980 episode_count: 19740. steps_count: 8538884.000000\n",
      "Time elapsed:  25167.74083161354\n",
      "ep 2820: ep_len:255 episode reward: total was -14.160000. running mean: -1.530641\n",
      "ep 2820: ep_len:343 episode reward: total was -36.250000. running mean: -1.877834\n",
      "ep 2820: ep_len:53 episode reward: total was -10.750000. running mean: -1.966556\n",
      "ep 2820: ep_len:500 episode reward: total was -50.260000. running mean: -2.449490\n",
      "ep 2820: ep_len:3 episode reward: total was 1.010000. running mean: -2.414895\n",
      "ep 2820: ep_len:605 episode reward: total was 42.040000. running mean: -1.970347\n",
      "ep 2820: ep_len:576 episode reward: total was 8.080000. running mean: -1.869843\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.074936 episode_count: 19747. steps_count: 8541219.000000\n",
      "Time elapsed:  25183.36460995674\n",
      "ep 2821: ep_len:236 episode reward: total was 14.740000. running mean: -1.703745\n",
      "ep 2821: ep_len:500 episode reward: total was -16.120000. running mean: -1.847907\n",
      "ep 2821: ep_len:435 episode reward: total was 31.610000. running mean: -1.513328\n",
      "ep 2821: ep_len:508 episode reward: total was 29.030000. running mean: -1.207895\n",
      "ep 2821: ep_len:88 episode reward: total was 20.220000. running mean: -0.993616\n",
      "ep 2821: ep_len:690 episode reward: total was -33.330000. running mean: -1.316980\n",
      "ep 2821: ep_len:595 episode reward: total was 20.790000. running mean: -1.095910\n",
      "epsilon:0.074891 episode_count: 19754. steps_count: 8544271.000000\n",
      "Time elapsed:  25197.729617357254\n",
      "ep 2822: ep_len:212 episode reward: total was 22.340000. running mean: -0.861551\n",
      "ep 2822: ep_len:548 episode reward: total was -3.820000. running mean: -0.891135\n",
      "ep 2822: ep_len:500 episode reward: total was -2.210000. running mean: -0.904324\n",
      "ep 2822: ep_len:500 episode reward: total was 0.060000. running mean: -0.894681\n",
      "ep 2822: ep_len:2 episode reward: total was -0.500000. running mean: -0.890734\n",
      "ep 2822: ep_len:500 episode reward: total was 10.300000. running mean: -0.778827\n",
      "ep 2822: ep_len:705 episode reward: total was -251.180000. running mean: -3.282838\n",
      "epsilon:0.074847 episode_count: 19761. steps_count: 8547238.000000\n",
      "Time elapsed:  25206.759706497192\n",
      "ep 2823: ep_len:500 episode reward: total was 49.090000. running mean: -2.759110\n",
      "ep 2823: ep_len:592 episode reward: total was 51.070000. running mean: -2.220819\n",
      "ep 2823: ep_len:558 episode reward: total was -92.300000. running mean: -3.121611\n",
      "ep 2823: ep_len:521 episode reward: total was -0.250000. running mean: -3.092895\n",
      "ep 2823: ep_len:89 episode reward: total was 23.200000. running mean: -2.829966\n",
      "ep 2823: ep_len:248 episode reward: total was 29.440000. running mean: -2.507266\n",
      "ep 2823: ep_len:523 episode reward: total was -83.000000. running mean: -3.312193\n",
      "epsilon:0.074803 episode_count: 19768. steps_count: 8550269.000000\n",
      "Time elapsed:  25222.00361442566\n",
      "ep 2824: ep_len:519 episode reward: total was -51.470000. running mean: -3.793771\n",
      "ep 2824: ep_len:171 episode reward: total was -34.800000. running mean: -4.103834\n",
      "ep 2824: ep_len:500 episode reward: total was -19.710000. running mean: -4.259895\n",
      "ep 2824: ep_len:541 episode reward: total was 21.350000. running mean: -4.003796\n",
      "ep 2824: ep_len:78 episode reward: total was 15.130000. running mean: -3.812458\n",
      "ep 2824: ep_len:500 episode reward: total was -17.970000. running mean: -3.954034\n",
      "ep 2824: ep_len:555 episode reward: total was -23.200000. running mean: -4.146493\n",
      "epsilon:0.074758 episode_count: 19775. steps_count: 8553133.000000\n",
      "Time elapsed:  25230.825218439102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2825: ep_len:227 episode reward: total was 13.750000. running mean: -3.967529\n",
      "ep 2825: ep_len:578 episode reward: total was 25.610000. running mean: -3.671753\n",
      "ep 2825: ep_len:573 episode reward: total was -50.920000. running mean: -4.144236\n",
      "ep 2825: ep_len:500 episode reward: total was -5.310000. running mean: -4.155893\n",
      "ep 2825: ep_len:1 episode reward: total was -1.000000. running mean: -4.124334\n",
      "ep 2825: ep_len:529 episode reward: total was -30.690000. running mean: -4.389991\n",
      "ep 2825: ep_len:179 episode reward: total was -17.520000. running mean: -4.521291\n",
      "epsilon:0.074714 episode_count: 19782. steps_count: 8555720.000000\n",
      "Time elapsed:  25238.97991323471\n",
      "ep 2826: ep_len:522 episode reward: total was -93.630000. running mean: -5.412378\n",
      "ep 2826: ep_len:339 episode reward: total was 0.100000. running mean: -5.357254\n",
      "ep 2826: ep_len:368 episode reward: total was 6.950000. running mean: -5.234182\n",
      "ep 2826: ep_len:549 episode reward: total was 36.160000. running mean: -4.820240\n",
      "ep 2826: ep_len:3 episode reward: total was 1.010000. running mean: -4.761938\n",
      "ep 2826: ep_len:513 episode reward: total was 14.440000. running mean: -4.569918\n",
      "ep 2826: ep_len:606 episode reward: total was -7.610000. running mean: -4.600319\n",
      "epsilon:0.074670 episode_count: 19789. steps_count: 8558620.000000\n",
      "Time elapsed:  25247.763724327087\n",
      "ep 2827: ep_len:500 episode reward: total was 45.320000. running mean: -4.101116\n",
      "ep 2827: ep_len:502 episode reward: total was 4.220000. running mean: -4.017905\n",
      "ep 2827: ep_len:559 episode reward: total was -39.750000. running mean: -4.375226\n",
      "ep 2827: ep_len:551 episode reward: total was 51.640000. running mean: -3.815073\n",
      "ep 2827: ep_len:3 episode reward: total was 1.010000. running mean: -3.766823\n",
      "ep 2827: ep_len:552 episode reward: total was 9.500000. running mean: -3.634155\n",
      "ep 2827: ep_len:590 episode reward: total was 33.010000. running mean: -3.267713\n",
      "epsilon:0.074625 episode_count: 19796. steps_count: 8561877.000000\n",
      "Time elapsed:  25257.493753910065\n",
      "ep 2828: ep_len:256 episode reward: total was 17.890000. running mean: -3.056136\n",
      "ep 2828: ep_len:547 episode reward: total was -7.720000. running mean: -3.102774\n",
      "ep 2828: ep_len:572 episode reward: total was -9.000000. running mean: -3.161747\n",
      "ep 2828: ep_len:500 episode reward: total was 30.200000. running mean: -2.828129\n",
      "ep 2828: ep_len:3 episode reward: total was 1.010000. running mean: -2.789748\n",
      "ep 2828: ep_len:662 episode reward: total was 16.130000. running mean: -2.600550\n",
      "ep 2828: ep_len:500 episode reward: total was -80.530000. running mean: -3.379845\n",
      "epsilon:0.074581 episode_count: 19803. steps_count: 8564917.000000\n",
      "Time elapsed:  25273.231142520905\n",
      "ep 2829: ep_len:540 episode reward: total was 27.030000. running mean: -3.075747\n",
      "ep 2829: ep_len:600 episode reward: total was 34.360000. running mean: -2.701389\n",
      "ep 2829: ep_len:624 episode reward: total was -18.810000. running mean: -2.862475\n",
      "ep 2829: ep_len:514 episode reward: total was 7.090000. running mean: -2.762950\n",
      "ep 2829: ep_len:98 episode reward: total was 25.740000. running mean: -2.477921\n",
      "ep 2829: ep_len:515 episode reward: total was 34.160000. running mean: -2.111542\n",
      "ep 2829: ep_len:503 episode reward: total was 8.810000. running mean: -2.002326\n",
      "epsilon:0.074537 episode_count: 19810. steps_count: 8568311.000000\n",
      "Time elapsed:  25284.61926651001\n",
      "ep 2830: ep_len:520 episode reward: total was -26.780000. running mean: -2.250103\n",
      "ep 2830: ep_len:522 episode reward: total was 31.600000. running mean: -1.911602\n",
      "ep 2830: ep_len:70 episode reward: total was -29.770000. running mean: -2.190186\n",
      "ep 2830: ep_len:771 episode reward: total was -152.070000. running mean: -3.688984\n",
      "ep 2830: ep_len:70 episode reward: total was 9.720000. running mean: -3.554894\n",
      "ep 2830: ep_len:174 episode reward: total was 6.180000. running mean: -3.457545\n",
      "ep 2830: ep_len:576 episode reward: total was -23.280000. running mean: -3.655770\n",
      "epsilon:0.074492 episode_count: 19817. steps_count: 8571014.000000\n",
      "Time elapsed:  25292.942469358444\n",
      "ep 2831: ep_len:204 episode reward: total was -20.010000. running mean: -3.819312\n",
      "ep 2831: ep_len:561 episode reward: total was 31.500000. running mean: -3.466119\n",
      "ep 2831: ep_len:624 episode reward: total was -29.630000. running mean: -3.727758\n",
      "ep 2831: ep_len:132 episode reward: total was 7.490000. running mean: -3.615580\n",
      "ep 2831: ep_len:127 episode reward: total was 14.810000. running mean: -3.431324\n",
      "ep 2831: ep_len:652 episode reward: total was -0.530000. running mean: -3.402311\n",
      "ep 2831: ep_len:500 episode reward: total was -17.440000. running mean: -3.542688\n",
      "epsilon:0.074448 episode_count: 19824. steps_count: 8573814.000000\n",
      "Time elapsed:  25302.04057264328\n",
      "ep 2832: ep_len:507 episode reward: total was 57.910000. running mean: -2.928161\n",
      "ep 2832: ep_len:578 episode reward: total was -6.920000. running mean: -2.968080\n",
      "ep 2832: ep_len:632 episode reward: total was -31.260000. running mean: -3.250999\n",
      "ep 2832: ep_len:500 episode reward: total was 6.040000. running mean: -3.158089\n",
      "ep 2832: ep_len:3 episode reward: total was -1.500000. running mean: -3.141508\n",
      "ep 2832: ep_len:500 episode reward: total was -95.730000. running mean: -4.067393\n",
      "ep 2832: ep_len:344 episode reward: total was -24.120000. running mean: -4.267919\n",
      "epsilon:0.074404 episode_count: 19831. steps_count: 8576878.000000\n",
      "Time elapsed:  25311.244453907013\n",
      "ep 2833: ep_len:264 episode reward: total was -3.200000. running mean: -4.257240\n",
      "ep 2833: ep_len:525 episode reward: total was -1.150000. running mean: -4.226167\n",
      "ep 2833: ep_len:379 episode reward: total was 13.430000. running mean: -4.049606\n",
      "ep 2833: ep_len:634 episode reward: total was -499.520000. running mean: -9.004310\n",
      "ep 2833: ep_len:3 episode reward: total was 1.010000. running mean: -8.904167\n",
      "ep 2833: ep_len:556 episode reward: total was 36.870000. running mean: -8.446425\n",
      "ep 2833: ep_len:610 episode reward: total was -26.340000. running mean: -8.625361\n",
      "epsilon:0.074359 episode_count: 19838. steps_count: 8579849.000000\n",
      "Time elapsed:  25320.349106550217\n",
      "ep 2834: ep_len:518 episode reward: total was 54.190000. running mean: -7.997207\n",
      "ep 2834: ep_len:500 episode reward: total was 27.940000. running mean: -7.637835\n",
      "ep 2834: ep_len:441 episode reward: total was -14.810000. running mean: -7.709557\n",
      "ep 2834: ep_len:521 episode reward: total was 11.600000. running mean: -7.516461\n",
      "ep 2834: ep_len:3 episode reward: total was 1.010000. running mean: -7.431196\n",
      "ep 2834: ep_len:500 episode reward: total was -9.510000. running mean: -7.451984\n",
      "ep 2834: ep_len:198 episode reward: total was -21.640000. running mean: -7.593865\n",
      "epsilon:0.074315 episode_count: 19845. steps_count: 8582530.000000\n",
      "Time elapsed:  25330.971591949463\n",
      "ep 2835: ep_len:657 episode reward: total was -80.280000. running mean: -8.320726\n",
      "ep 2835: ep_len:353 episode reward: total was -64.250000. running mean: -8.880019\n",
      "ep 2835: ep_len:569 episode reward: total was -19.520000. running mean: -8.986419\n",
      "ep 2835: ep_len:525 episode reward: total was -17.110000. running mean: -9.067654\n",
      "ep 2835: ep_len:3 episode reward: total was 1.010000. running mean: -8.966878\n",
      "ep 2835: ep_len:500 episode reward: total was -35.390000. running mean: -9.231109\n",
      "ep 2835: ep_len:186 episode reward: total was -22.540000. running mean: -9.364198\n",
      "epsilon:0.074271 episode_count: 19852. steps_count: 8585323.000000\n",
      "Time elapsed:  25339.550001859665\n",
      "ep 2836: ep_len:606 episode reward: total was 19.420000. running mean: -9.076356\n",
      "ep 2836: ep_len:535 episode reward: total was 42.770000. running mean: -8.557892\n",
      "ep 2836: ep_len:558 episode reward: total was 16.080000. running mean: -8.311513\n",
      "ep 2836: ep_len:505 episode reward: total was -1.500000. running mean: -8.243398\n",
      "ep 2836: ep_len:3 episode reward: total was 1.010000. running mean: -8.150864\n",
      "ep 2836: ep_len:500 episode reward: total was -2.980000. running mean: -8.099156\n",
      "ep 2836: ep_len:500 episode reward: total was -18.830000. running mean: -8.206464\n",
      "epsilon:0.074226 episode_count: 19859. steps_count: 8588530.000000\n",
      "Time elapsed:  25348.322962999344\n",
      "ep 2837: ep_len:595 episode reward: total was 56.400000. running mean: -7.560400\n",
      "ep 2837: ep_len:507 episode reward: total was -18.380000. running mean: -7.668596\n",
      "ep 2837: ep_len:501 episode reward: total was -46.100000. running mean: -8.052910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2837: ep_len:500 episode reward: total was 14.310000. running mean: -7.829280\n",
      "ep 2837: ep_len:56 episode reward: total was 10.680000. running mean: -7.644188\n",
      "ep 2837: ep_len:542 episode reward: total was 10.890000. running mean: -7.458846\n",
      "ep 2837: ep_len:601 episode reward: total was 15.020000. running mean: -7.234057\n",
      "epsilon:0.074182 episode_count: 19866. steps_count: 8591832.000000\n",
      "Time elapsed:  25363.542054891586\n",
      "ep 2838: ep_len:577 episode reward: total was 20.790000. running mean: -6.953817\n",
      "ep 2838: ep_len:500 episode reward: total was 8.190000. running mean: -6.802379\n",
      "ep 2838: ep_len:527 episode reward: total was 19.060000. running mean: -6.543755\n",
      "ep 2838: ep_len:500 episode reward: total was -9.820000. running mean: -6.576517\n",
      "ep 2838: ep_len:3 episode reward: total was 1.010000. running mean: -6.500652\n",
      "ep 2838: ep_len:705 episode reward: total was -40.310000. running mean: -6.838746\n",
      "ep 2838: ep_len:211 episode reward: total was -51.260000. running mean: -7.282958\n",
      "epsilon:0.074138 episode_count: 19873. steps_count: 8594855.000000\n",
      "Time elapsed:  25372.33670115471\n",
      "ep 2839: ep_len:639 episode reward: total was -34.520000. running mean: -7.555329\n",
      "ep 2839: ep_len:501 episode reward: total was 23.700000. running mean: -7.242775\n",
      "ep 2839: ep_len:603 episode reward: total was 12.550000. running mean: -7.044847\n",
      "ep 2839: ep_len:586 episode reward: total was 32.160000. running mean: -6.652799\n",
      "ep 2839: ep_len:76 episode reward: total was 14.220000. running mean: -6.444071\n",
      "ep 2839: ep_len:655 episode reward: total was 26.560000. running mean: -6.114030\n",
      "ep 2839: ep_len:615 episode reward: total was 13.520000. running mean: -5.917690\n",
      "epsilon:0.074093 episode_count: 19880. steps_count: 8598530.000000\n",
      "Time elapsed:  25379.565984249115\n",
      "ep 2840: ep_len:566 episode reward: total was 18.330000. running mean: -5.675213\n",
      "ep 2840: ep_len:500 episode reward: total was 2.650000. running mean: -5.591961\n",
      "ep 2840: ep_len:606 episode reward: total was -26.860000. running mean: -5.804641\n",
      "ep 2840: ep_len:511 episode reward: total was 29.710000. running mean: -5.449495\n",
      "ep 2840: ep_len:3 episode reward: total was 1.010000. running mean: -5.384900\n",
      "ep 2840: ep_len:186 episode reward: total was 31.190000. running mean: -5.019151\n",
      "ep 2840: ep_len:543 episode reward: total was -54.380000. running mean: -5.512759\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.074049 episode_count: 19887. steps_count: 8601445.000000\n",
      "Time elapsed:  25389.79424881935\n",
      "ep 2841: ep_len:516 episode reward: total was -3.520000. running mean: -5.492832\n",
      "ep 2841: ep_len:500 episode reward: total was 110.010000. running mean: -4.337804\n",
      "ep 2841: ep_len:515 episode reward: total was -1.310000. running mean: -4.307526\n",
      "ep 2841: ep_len:500 episode reward: total was 15.430000. running mean: -4.110150\n",
      "ep 2841: ep_len:86 episode reward: total was 8.200000. running mean: -3.987049\n",
      "ep 2841: ep_len:242 episode reward: total was 32.530000. running mean: -3.621878\n",
      "ep 2841: ep_len:170 episode reward: total was -77.170000. running mean: -4.357360\n",
      "epsilon:0.074005 episode_count: 19894. steps_count: 8603974.000000\n",
      "Time elapsed:  25406.063873291016\n",
      "ep 2842: ep_len:549 episode reward: total was 25.230000. running mean: -4.061486\n",
      "ep 2842: ep_len:500 episode reward: total was 26.050000. running mean: -3.760371\n",
      "ep 2842: ep_len:398 episode reward: total was 12.000000. running mean: -3.602767\n",
      "ep 2842: ep_len:528 episode reward: total was 43.540000. running mean: -3.131340\n",
      "ep 2842: ep_len:93 episode reward: total was -50.820000. running mean: -3.608226\n",
      "ep 2842: ep_len:500 episode reward: total was -27.040000. running mean: -3.842544\n",
      "ep 2842: ep_len:537 episode reward: total was -1.000000. running mean: -3.814119\n",
      "epsilon:0.073960 episode_count: 19901. steps_count: 8607079.000000\n",
      "Time elapsed:  25413.929306983948\n",
      "ep 2843: ep_len:622 episode reward: total was -27.960000. running mean: -4.055577\n",
      "ep 2843: ep_len:500 episode reward: total was 10.280000. running mean: -3.912222\n",
      "ep 2843: ep_len:507 episode reward: total was 10.800000. running mean: -3.765099\n",
      "ep 2843: ep_len:512 episode reward: total was 45.180000. running mean: -3.275648\n",
      "ep 2843: ep_len:3 episode reward: total was 1.010000. running mean: -3.232792\n",
      "ep 2843: ep_len:500 episode reward: total was -11.720000. running mean: -3.317664\n",
      "ep 2843: ep_len:500 episode reward: total was 1.020000. running mean: -3.274287\n",
      "epsilon:0.073916 episode_count: 19908. steps_count: 8610223.000000\n",
      "Time elapsed:  25423.332968711853\n",
      "ep 2844: ep_len:591 episode reward: total was 37.190000. running mean: -2.869644\n",
      "ep 2844: ep_len:564 episode reward: total was 6.110000. running mean: -2.779848\n",
      "ep 2844: ep_len:79 episode reward: total was 9.320000. running mean: -2.658850\n",
      "ep 2844: ep_len:583 episode reward: total was 52.180000. running mean: -2.110461\n",
      "ep 2844: ep_len:3 episode reward: total was -1.500000. running mean: -2.104356\n",
      "ep 2844: ep_len:500 episode reward: total was 5.220000. running mean: -2.031113\n",
      "ep 2844: ep_len:302 episode reward: total was 1.230000. running mean: -1.998502\n",
      "epsilon:0.073872 episode_count: 19915. steps_count: 8612845.000000\n",
      "Time elapsed:  25431.547964811325\n",
      "ep 2845: ep_len:112 episode reward: total was 4.300000. running mean: -1.935517\n",
      "ep 2845: ep_len:551 episode reward: total was 78.800000. running mean: -1.128162\n",
      "ep 2845: ep_len:500 episode reward: total was 8.460000. running mean: -1.032280\n",
      "ep 2845: ep_len:571 episode reward: total was 29.530000. running mean: -0.726657\n",
      "ep 2845: ep_len:77 episode reward: total was 15.670000. running mean: -0.562691\n",
      "ep 2845: ep_len:581 episode reward: total was 19.730000. running mean: -0.359764\n",
      "ep 2845: ep_len:503 episode reward: total was -130.310000. running mean: -1.659266\n",
      "epsilon:0.073827 episode_count: 19922. steps_count: 8615740.000000\n",
      "Time elapsed:  25440.203843832016\n",
      "ep 2846: ep_len:510 episode reward: total was -44.770000. running mean: -2.090373\n",
      "ep 2846: ep_len:500 episode reward: total was 28.170000. running mean: -1.787770\n",
      "ep 2846: ep_len:528 episode reward: total was 32.050000. running mean: -1.449392\n",
      "ep 2846: ep_len:500 episode reward: total was 45.540000. running mean: -0.979498\n",
      "ep 2846: ep_len:90 episode reward: total was 15.710000. running mean: -0.812603\n",
      "ep 2846: ep_len:500 episode reward: total was 1.710000. running mean: -0.787377\n",
      "ep 2846: ep_len:301 episode reward: total was -3.940000. running mean: -0.818903\n",
      "epsilon:0.073783 episode_count: 19929. steps_count: 8618669.000000\n",
      "Time elapsed:  25461.230068922043\n",
      "ep 2847: ep_len:579 episode reward: total was -45.040000. running mean: -1.261114\n",
      "ep 2847: ep_len:502 episode reward: total was -40.330000. running mean: -1.651803\n",
      "ep 2847: ep_len:558 episode reward: total was -46.140000. running mean: -2.096685\n",
      "ep 2847: ep_len:500 episode reward: total was 30.550000. running mean: -1.770218\n",
      "ep 2847: ep_len:119 episode reward: total was -58.700000. running mean: -2.339516\n",
      "ep 2847: ep_len:521 episode reward: total was -8.110000. running mean: -2.397221\n",
      "ep 2847: ep_len:562 episode reward: total was 5.740000. running mean: -2.315849\n",
      "epsilon:0.073739 episode_count: 19936. steps_count: 8622010.000000\n",
      "Time elapsed:  25471.168679475784\n",
      "ep 2848: ep_len:237 episode reward: total was 7.820000. running mean: -2.214490\n",
      "ep 2848: ep_len:548 episode reward: total was -11.340000. running mean: -2.305745\n",
      "ep 2848: ep_len:455 episode reward: total was 38.330000. running mean: -1.899388\n",
      "ep 2848: ep_len:511 episode reward: total was -1.330000. running mean: -1.893694\n",
      "ep 2848: ep_len:3 episode reward: total was 1.010000. running mean: -1.864657\n",
      "ep 2848: ep_len:579 episode reward: total was -6.770000. running mean: -1.913710\n",
      "ep 2848: ep_len:562 episode reward: total was 0.970000. running mean: -1.884873\n",
      "epsilon:0.073694 episode_count: 19943. steps_count: 8624905.000000\n",
      "Time elapsed:  25486.779759168625\n",
      "ep 2849: ep_len:261 episode reward: total was 17.890000. running mean: -1.687125\n",
      "ep 2849: ep_len:500 episode reward: total was -1.100000. running mean: -1.681253\n",
      "ep 2849: ep_len:500 episode reward: total was -1.250000. running mean: -1.676941\n",
      "ep 2849: ep_len:500 episode reward: total was -60.420000. running mean: -2.264371\n",
      "ep 2849: ep_len:119 episode reward: total was 24.300000. running mean: -1.998728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2849: ep_len:553 episode reward: total was -6.800000. running mean: -2.046740\n",
      "ep 2849: ep_len:531 episode reward: total was -24.280000. running mean: -2.269073\n",
      "epsilon:0.073650 episode_count: 19950. steps_count: 8627869.000000\n",
      "Time elapsed:  25502.11825323105\n",
      "ep 2850: ep_len:500 episode reward: total was 21.840000. running mean: -2.027982\n",
      "ep 2850: ep_len:596 episode reward: total was -2.990000. running mean: -2.037602\n",
      "ep 2850: ep_len:558 episode reward: total was -41.930000. running mean: -2.436526\n",
      "ep 2850: ep_len:552 episode reward: total was 18.500000. running mean: -2.227161\n",
      "ep 2850: ep_len:3 episode reward: total was 1.010000. running mean: -2.194790\n",
      "ep 2850: ep_len:697 episode reward: total was 23.130000. running mean: -1.941542\n",
      "ep 2850: ep_len:574 episode reward: total was -1.740000. running mean: -1.939526\n",
      "epsilon:0.073606 episode_count: 19957. steps_count: 8631349.000000\n",
      "Time elapsed:  25510.78866481781\n",
      "ep 2851: ep_len:500 episode reward: total was -37.220000. running mean: -2.292331\n",
      "ep 2851: ep_len:517 episode reward: total was -19.560000. running mean: -2.465008\n",
      "ep 2851: ep_len:500 episode reward: total was 23.670000. running mean: -2.203658\n",
      "ep 2851: ep_len:512 episode reward: total was -8.360000. running mean: -2.265221\n",
      "ep 2851: ep_len:29 episode reward: total was 12.510000. running mean: -2.117469\n",
      "ep 2851: ep_len:611 episode reward: total was 1.060000. running mean: -2.085694\n",
      "ep 2851: ep_len:552 episode reward: total was -2.710000. running mean: -2.091937\n",
      "epsilon:0.073561 episode_count: 19964. steps_count: 8634570.000000\n",
      "Time elapsed:  25520.41812634468\n",
      "ep 2852: ep_len:524 episode reward: total was -59.640000. running mean: -2.667418\n",
      "ep 2852: ep_len:558 episode reward: total was -18.120000. running mean: -2.821944\n",
      "ep 2852: ep_len:500 episode reward: total was -4.100000. running mean: -2.834724\n",
      "ep 2852: ep_len:500 episode reward: total was 51.480000. running mean: -2.291577\n",
      "ep 2852: ep_len:3 episode reward: total was 1.010000. running mean: -2.258561\n",
      "ep 2852: ep_len:260 episode reward: total was 30.540000. running mean: -1.930576\n",
      "ep 2852: ep_len:560 episode reward: total was -8.840000. running mean: -1.999670\n",
      "epsilon:0.073517 episode_count: 19971. steps_count: 8637475.000000\n",
      "Time elapsed:  25529.90999174118\n",
      "ep 2853: ep_len:500 episode reward: total was 45.410000. running mean: -1.525573\n",
      "ep 2853: ep_len:651 episode reward: total was 30.650000. running mean: -1.203817\n",
      "ep 2853: ep_len:500 episode reward: total was -72.270000. running mean: -1.914479\n",
      "ep 2853: ep_len:500 episode reward: total was 28.500000. running mean: -1.610334\n",
      "ep 2853: ep_len:126 episode reward: total was 32.360000. running mean: -1.270631\n",
      "ep 2853: ep_len:523 episode reward: total was 28.870000. running mean: -0.969225\n",
      "ep 2853: ep_len:339 episode reward: total was -13.700000. running mean: -1.096533\n",
      "epsilon:0.073473 episode_count: 19978. steps_count: 8640614.000000\n",
      "Time elapsed:  25539.266513586044\n",
      "ep 2854: ep_len:623 episode reward: total was -18.460000. running mean: -1.270167\n",
      "ep 2854: ep_len:584 episode reward: total was -33.920000. running mean: -1.596666\n",
      "ep 2854: ep_len:454 episode reward: total was 43.250000. running mean: -1.148199\n",
      "ep 2854: ep_len:102 episode reward: total was 3.460000. running mean: -1.102117\n",
      "ep 2854: ep_len:109 episode reward: total was 27.750000. running mean: -0.813596\n",
      "ep 2854: ep_len:500 episode reward: total was 7.630000. running mean: -0.729160\n",
      "ep 2854: ep_len:342 episode reward: total was 4.110000. running mean: -0.680768\n",
      "epsilon:0.073428 episode_count: 19985. steps_count: 8643328.000000\n",
      "Time elapsed:  25551.84582877159\n",
      "ep 2855: ep_len:502 episode reward: total was 42.450000. running mean: -0.249460\n",
      "ep 2855: ep_len:505 episode reward: total was 10.680000. running mean: -0.140166\n",
      "ep 2855: ep_len:527 episode reward: total was -5.420000. running mean: -0.192964\n",
      "ep 2855: ep_len:613 episode reward: total was 27.910000. running mean: 0.088065\n",
      "ep 2855: ep_len:3 episode reward: total was 1.010000. running mean: 0.097285\n",
      "ep 2855: ep_len:509 episode reward: total was -21.730000. running mean: -0.120988\n",
      "ep 2855: ep_len:525 episode reward: total was -12.810000. running mean: -0.247878\n",
      "epsilon:0.073384 episode_count: 19992. steps_count: 8646512.000000\n",
      "Time elapsed:  25561.182980060577\n",
      "ep 2856: ep_len:500 episode reward: total was 47.920000. running mean: 0.233801\n",
      "ep 2856: ep_len:302 episode reward: total was -5.790000. running mean: 0.173563\n",
      "ep 2856: ep_len:425 episode reward: total was 28.350000. running mean: 0.455327\n",
      "ep 2856: ep_len:383 episode reward: total was -0.960000. running mean: 0.441174\n",
      "ep 2856: ep_len:53 episode reward: total was 22.000000. running mean: 0.656762\n",
      "ep 2856: ep_len:168 episode reward: total was 34.100000. running mean: 0.991194\n",
      "ep 2856: ep_len:549 episode reward: total was 8.940000. running mean: 1.070682\n",
      "epsilon:0.073340 episode_count: 19999. steps_count: 8648892.000000\n",
      "Time elapsed:  25568.5224776268\n",
      "ep 2857: ep_len:574 episode reward: total was -37.430000. running mean: 0.685676\n",
      "ep 2857: ep_len:517 episode reward: total was -10.350000. running mean: 0.575319\n",
      "ep 2857: ep_len:500 episode reward: total was -15.300000. running mean: 0.416566\n",
      "ep 2857: ep_len:500 episode reward: total was -46.890000. running mean: -0.056500\n",
      "ep 2857: ep_len:100 episode reward: total was 20.250000. running mean: 0.146565\n",
      "ep 2857: ep_len:543 episode reward: total was -202.840000. running mean: -1.883301\n",
      "ep 2857: ep_len:500 episode reward: total was -28.340000. running mean: -2.147868\n",
      "epsilon:0.073295 episode_count: 20006. steps_count: 8652126.000000\n",
      "Time elapsed:  25578.069298028946\n",
      "ep 2858: ep_len:607 episode reward: total was 17.310000. running mean: -1.953289\n",
      "ep 2858: ep_len:291 episode reward: total was -11.630000. running mean: -2.050056\n",
      "ep 2858: ep_len:512 episode reward: total was -21.440000. running mean: -2.243956\n",
      "ep 2858: ep_len:48 episode reward: total was -0.700000. running mean: -2.228516\n",
      "ep 2858: ep_len:109 episode reward: total was 28.240000. running mean: -1.923831\n",
      "ep 2858: ep_len:500 episode reward: total was 1.480000. running mean: -1.889793\n",
      "ep 2858: ep_len:534 episode reward: total was 17.590000. running mean: -1.694995\n",
      "epsilon:0.073251 episode_count: 20013. steps_count: 8654727.000000\n",
      "Time elapsed:  25585.372341632843\n",
      "ep 2859: ep_len:654 episode reward: total was -66.590000. running mean: -2.343945\n",
      "ep 2859: ep_len:500 episode reward: total was -47.640000. running mean: -2.796905\n",
      "ep 2859: ep_len:73 episode reward: total was 1.700000. running mean: -2.751936\n",
      "ep 2859: ep_len:46 episode reward: total was -6.210000. running mean: -2.786517\n",
      "ep 2859: ep_len:36 episode reward: total was 10.500000. running mean: -2.653652\n",
      "ep 2859: ep_len:307 episode reward: total was -31.330000. running mean: -2.940415\n",
      "ep 2859: ep_len:541 episode reward: total was 7.530000. running mean: -2.835711\n",
      "epsilon:0.073207 episode_count: 20020. steps_count: 8656884.000000\n",
      "Time elapsed:  25589.95370697975\n",
      "ep 2860: ep_len:500 episode reward: total was 22.140000. running mean: -2.585954\n",
      "ep 2860: ep_len:501 episode reward: total was -25.300000. running mean: -2.813094\n",
      "ep 2860: ep_len:683 episode reward: total was -26.820000. running mean: -3.053163\n",
      "ep 2860: ep_len:500 episode reward: total was 26.870000. running mean: -2.753932\n",
      "ep 2860: ep_len:3 episode reward: total was -1.500000. running mean: -2.741392\n",
      "ep 2860: ep_len:500 episode reward: total was 11.080000. running mean: -2.603178\n",
      "ep 2860: ep_len:583 episode reward: total was -11.780000. running mean: -2.694947\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.073162 episode_count: 20027. steps_count: 8660154.000000\n",
      "Time elapsed:  25601.8712310791\n",
      "ep 2861: ep_len:543 episode reward: total was 39.560000. running mean: -2.272397\n",
      "ep 2861: ep_len:280 episode reward: total was -59.820000. running mean: -2.847873\n",
      "ep 2861: ep_len:533 episode reward: total was -7.730000. running mean: -2.896695\n",
      "ep 2861: ep_len:417 episode reward: total was 28.280000. running mean: -2.584928\n",
      "ep 2861: ep_len:107 episode reward: total was 26.750000. running mean: -2.291578\n",
      "ep 2861: ep_len:500 episode reward: total was 1.720000. running mean: -2.251463\n",
      "ep 2861: ep_len:192 episode reward: total was -4.620000. running mean: -2.275148\n",
      "epsilon:0.073118 episode_count: 20034. steps_count: 8662726.000000\n",
      "Time elapsed:  25609.999708652496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2862: ep_len:509 episode reward: total was 47.250000. running mean: -1.779896\n",
      "ep 2862: ep_len:514 episode reward: total was -34.450000. running mean: -2.106597\n",
      "ep 2862: ep_len:628 episode reward: total was -14.840000. running mean: -2.233931\n",
      "ep 2862: ep_len:56 episode reward: total was -4.640000. running mean: -2.257992\n",
      "ep 2862: ep_len:3 episode reward: total was 1.010000. running mean: -2.225312\n",
      "ep 2862: ep_len:563 episode reward: total was -17.610000. running mean: -2.379159\n",
      "ep 2862: ep_len:585 episode reward: total was -2.600000. running mean: -2.381368\n",
      "epsilon:0.073074 episode_count: 20041. steps_count: 8665584.000000\n",
      "Time elapsed:  25617.742095708847\n",
      "ep 2863: ep_len:211 episode reward: total was -29.030000. running mean: -2.647854\n",
      "ep 2863: ep_len:533 episode reward: total was 18.670000. running mean: -2.434675\n",
      "ep 2863: ep_len:507 episode reward: total was -21.330000. running mean: -2.623629\n",
      "ep 2863: ep_len:627 episode reward: total was -116.490000. running mean: -3.762292\n",
      "ep 2863: ep_len:90 episode reward: total was 22.260000. running mean: -3.502069\n",
      "ep 2863: ep_len:500 episode reward: total was 40.470000. running mean: -3.062349\n",
      "ep 2863: ep_len:503 episode reward: total was -27.240000. running mean: -3.304125\n",
      "epsilon:0.073029 episode_count: 20048. steps_count: 8668555.000000\n",
      "Time elapsed:  25626.395755290985\n",
      "ep 2864: ep_len:526 episode reward: total was -15.420000. running mean: -3.425284\n",
      "ep 2864: ep_len:500 episode reward: total was -135.680000. running mean: -4.747831\n",
      "ep 2864: ep_len:594 episode reward: total was 3.080000. running mean: -4.669553\n",
      "ep 2864: ep_len:500 episode reward: total was 41.820000. running mean: -4.204657\n",
      "ep 2864: ep_len:3 episode reward: total was -0.490000. running mean: -4.167511\n",
      "ep 2864: ep_len:502 episode reward: total was 6.370000. running mean: -4.062136\n",
      "ep 2864: ep_len:295 episode reward: total was 2.190000. running mean: -3.999614\n",
      "epsilon:0.072985 episode_count: 20055. steps_count: 8671475.000000\n",
      "Time elapsed:  25642.331236839294\n",
      "ep 2865: ep_len:508 episode reward: total was -2.220000. running mean: -3.981818\n",
      "ep 2865: ep_len:176 episode reward: total was -6.800000. running mean: -4.010000\n",
      "ep 2865: ep_len:649 episode reward: total was -67.680000. running mean: -4.646700\n",
      "ep 2865: ep_len:535 episode reward: total was -36.470000. running mean: -4.964933\n",
      "ep 2865: ep_len:3 episode reward: total was 1.010000. running mean: -4.905184\n",
      "ep 2865: ep_len:529 episode reward: total was 10.120000. running mean: -4.754932\n",
      "ep 2865: ep_len:307 episode reward: total was -21.320000. running mean: -4.920582\n",
      "epsilon:0.072941 episode_count: 20062. steps_count: 8674182.000000\n",
      "Time elapsed:  25650.6307554245\n",
      "ep 2866: ep_len:540 episode reward: total was -23.600000. running mean: -5.107377\n",
      "ep 2866: ep_len:627 episode reward: total was 44.290000. running mean: -4.613403\n",
      "ep 2866: ep_len:568 episode reward: total was 13.480000. running mean: -4.432469\n",
      "ep 2866: ep_len:500 episode reward: total was 22.510000. running mean: -4.163044\n",
      "ep 2866: ep_len:3 episode reward: total was -1.500000. running mean: -4.136414\n",
      "ep 2866: ep_len:157 episode reward: total was 31.050000. running mean: -3.784550\n",
      "ep 2866: ep_len:572 episode reward: total was -62.490000. running mean: -4.371604\n",
      "epsilon:0.072896 episode_count: 20069. steps_count: 8677149.000000\n",
      "Time elapsed:  25659.639797449112\n",
      "ep 2867: ep_len:500 episode reward: total was 73.440000. running mean: -3.593488\n",
      "ep 2867: ep_len:500 episode reward: total was -0.650000. running mean: -3.564053\n",
      "ep 2867: ep_len:500 episode reward: total was -24.090000. running mean: -3.769313\n",
      "ep 2867: ep_len:500 episode reward: total was 21.580000. running mean: -3.515819\n",
      "ep 2867: ep_len:3 episode reward: total was 1.010000. running mean: -3.470561\n",
      "ep 2867: ep_len:522 episode reward: total was -28.920000. running mean: -3.725056\n",
      "ep 2867: ep_len:292 episode reward: total was -55.670000. running mean: -4.244505\n",
      "epsilon:0.072852 episode_count: 20076. steps_count: 8679966.000000\n",
      "Time elapsed:  25668.37368631363\n",
      "ep 2868: ep_len:612 episode reward: total was -46.980000. running mean: -4.671860\n",
      "ep 2868: ep_len:500 episode reward: total was 20.040000. running mean: -4.424741\n",
      "ep 2868: ep_len:543 episode reward: total was -41.470000. running mean: -4.795194\n",
      "ep 2868: ep_len:569 episode reward: total was 36.130000. running mean: -4.385942\n",
      "ep 2868: ep_len:39 episode reward: total was 12.000000. running mean: -4.222083\n",
      "ep 2868: ep_len:500 episode reward: total was -22.290000. running mean: -4.402762\n",
      "ep 2868: ep_len:555 episode reward: total was -54.640000. running mean: -4.905134\n",
      "epsilon:0.072808 episode_count: 20083. steps_count: 8683284.000000\n",
      "Time elapsed:  25678.61023926735\n",
      "ep 2869: ep_len:568 episode reward: total was 2.550000. running mean: -4.830583\n",
      "ep 2869: ep_len:513 episode reward: total was -12.350000. running mean: -4.905777\n",
      "ep 2869: ep_len:586 episode reward: total was -38.860000. running mean: -5.245319\n",
      "ep 2869: ep_len:56 episode reward: total was -10.180000. running mean: -5.294666\n",
      "ep 2869: ep_len:3 episode reward: total was 1.010000. running mean: -5.231619\n",
      "ep 2869: ep_len:308 episode reward: total was 10.620000. running mean: -5.073103\n",
      "ep 2869: ep_len:500 episode reward: total was 24.500000. running mean: -4.777372\n",
      "epsilon:0.072763 episode_count: 20090. steps_count: 8685818.000000\n",
      "Time elapsed:  25686.895811080933\n",
      "ep 2870: ep_len:510 episode reward: total was 10.860000. running mean: -4.620998\n",
      "ep 2870: ep_len:559 episode reward: total was 86.640000. running mean: -3.708388\n",
      "ep 2870: ep_len:677 episode reward: total was -17.940000. running mean: -3.850705\n",
      "ep 2870: ep_len:526 episode reward: total was -1.300000. running mean: -3.825198\n",
      "ep 2870: ep_len:3 episode reward: total was 1.010000. running mean: -3.776846\n",
      "ep 2870: ep_len:500 episode reward: total was -28.400000. running mean: -4.023077\n",
      "ep 2870: ep_len:500 episode reward: total was -6.860000. running mean: -4.051446\n",
      "epsilon:0.072719 episode_count: 20097. steps_count: 8689093.000000\n",
      "Time elapsed:  25697.00908589363\n",
      "ep 2871: ep_len:532 episode reward: total was -26.900000. running mean: -4.279932\n",
      "ep 2871: ep_len:540 episode reward: total was 24.860000. running mean: -3.988533\n",
      "ep 2871: ep_len:500 episode reward: total was 21.220000. running mean: -3.736447\n",
      "ep 2871: ep_len:509 episode reward: total was 56.200000. running mean: -3.137083\n",
      "ep 2871: ep_len:109 episode reward: total was 21.840000. running mean: -2.887312\n",
      "ep 2871: ep_len:576 episode reward: total was 56.580000. running mean: -2.292639\n",
      "ep 2871: ep_len:500 episode reward: total was -19.870000. running mean: -2.468412\n",
      "epsilon:0.072675 episode_count: 20104. steps_count: 8692359.000000\n",
      "Time elapsed:  25706.800344944\n",
      "ep 2872: ep_len:524 episode reward: total was 44.990000. running mean: -1.993828\n",
      "ep 2872: ep_len:596 episode reward: total was 0.720000. running mean: -1.966690\n",
      "ep 2872: ep_len:631 episode reward: total was -9.820000. running mean: -2.045223\n",
      "ep 2872: ep_len:562 episode reward: total was 55.950000. running mean: -1.465271\n",
      "ep 2872: ep_len:3 episode reward: total was 1.010000. running mean: -1.440518\n",
      "ep 2872: ep_len:515 episode reward: total was -31.860000. running mean: -1.744713\n",
      "ep 2872: ep_len:509 episode reward: total was 22.640000. running mean: -1.500866\n",
      "epsilon:0.072630 episode_count: 20111. steps_count: 8695699.000000\n",
      "Time elapsed:  25716.777851581573\n",
      "ep 2873: ep_len:242 episode reward: total was 6.860000. running mean: -1.417257\n",
      "ep 2873: ep_len:523 episode reward: total was 14.990000. running mean: -1.253185\n",
      "ep 2873: ep_len:526 episode reward: total was -14.030000. running mean: -1.380953\n",
      "ep 2873: ep_len:505 episode reward: total was -21.290000. running mean: -1.580043\n",
      "ep 2873: ep_len:120 episode reward: total was 8.830000. running mean: -1.475943\n",
      "ep 2873: ep_len:500 episode reward: total was -19.860000. running mean: -1.659783\n",
      "ep 2873: ep_len:505 episode reward: total was -20.190000. running mean: -1.845086\n",
      "epsilon:0.072586 episode_count: 20118. steps_count: 8698620.000000\n",
      "Time elapsed:  25731.776257276535\n",
      "ep 2874: ep_len:557 episode reward: total was -35.150000. running mean: -2.178135\n",
      "ep 2874: ep_len:500 episode reward: total was -26.230000. running mean: -2.418653\n",
      "ep 2874: ep_len:435 episode reward: total was 48.080000. running mean: -1.913667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2874: ep_len:502 episode reward: total was -7.600000. running mean: -1.970530\n",
      "ep 2874: ep_len:3 episode reward: total was 1.010000. running mean: -1.940725\n",
      "ep 2874: ep_len:532 episode reward: total was -40.340000. running mean: -2.324718\n",
      "ep 2874: ep_len:521 episode reward: total was -14.010000. running mean: -2.441570\n",
      "epsilon:0.072542 episode_count: 20125. steps_count: 8701670.000000\n",
      "Time elapsed:  25740.34145784378\n",
      "ep 2875: ep_len:548 episode reward: total was -2.360000. running mean: -2.440755\n",
      "ep 2875: ep_len:526 episode reward: total was 73.100000. running mean: -1.685347\n",
      "ep 2875: ep_len:575 episode reward: total was -34.590000. running mean: -2.014394\n",
      "ep 2875: ep_len:636 episode reward: total was 63.860000. running mean: -1.355650\n",
      "ep 2875: ep_len:3 episode reward: total was 1.010000. running mean: -1.331993\n",
      "ep 2875: ep_len:595 episode reward: total was 29.240000. running mean: -1.026273\n",
      "ep 2875: ep_len:563 episode reward: total was 21.690000. running mean: -0.799111\n",
      "epsilon:0.072497 episode_count: 20132. steps_count: 8705116.000000\n",
      "Time elapsed:  25750.56906080246\n",
      "ep 2876: ep_len:500 episode reward: total was 56.700000. running mean: -0.224120\n",
      "ep 2876: ep_len:550 episode reward: total was 99.640000. running mean: 0.774522\n",
      "ep 2876: ep_len:374 episode reward: total was 41.780000. running mean: 1.184576\n",
      "ep 2876: ep_len:403 episode reward: total was -2.980000. running mean: 1.142931\n",
      "ep 2876: ep_len:99 episode reward: total was 19.200000. running mean: 1.323501\n",
      "ep 2876: ep_len:231 episode reward: total was 27.280000. running mean: 1.583066\n",
      "ep 2876: ep_len:594 episode reward: total was 19.910000. running mean: 1.766336\n",
      "epsilon:0.072453 episode_count: 20139. steps_count: 8707867.000000\n",
      "Time elapsed:  25759.226259708405\n",
      "ep 2877: ep_len:542 episode reward: total was 33.000000. running mean: 2.078672\n",
      "ep 2877: ep_len:500 episode reward: total was 48.310000. running mean: 2.540986\n",
      "ep 2877: ep_len:555 episode reward: total was -15.420000. running mean: 2.361376\n",
      "ep 2877: ep_len:500 episode reward: total was 41.320000. running mean: 2.750962\n",
      "ep 2877: ep_len:50 episode reward: total was 17.010000. running mean: 2.893552\n",
      "ep 2877: ep_len:603 episode reward: total was 33.790000. running mean: 3.202517\n",
      "ep 2877: ep_len:192 episode reward: total was -4.160000. running mean: 3.128892\n",
      "epsilon:0.072409 episode_count: 20146. steps_count: 8710809.000000\n",
      "Time elapsed:  25772.46242761612\n",
      "ep 2878: ep_len:622 episode reward: total was -0.280000. running mean: 3.094803\n",
      "ep 2878: ep_len:570 episode reward: total was -44.340000. running mean: 2.620455\n",
      "ep 2878: ep_len:569 episode reward: total was -24.250000. running mean: 2.351750\n",
      "ep 2878: ep_len:503 episode reward: total was 0.440000. running mean: 2.332633\n",
      "ep 2878: ep_len:116 episode reward: total was 31.280000. running mean: 2.622106\n",
      "ep 2878: ep_len:500 episode reward: total was 28.720000. running mean: 2.883085\n",
      "ep 2878: ep_len:520 episode reward: total was -13.400000. running mean: 2.720254\n",
      "epsilon:0.072364 episode_count: 20153. steps_count: 8714209.000000\n",
      "Time elapsed:  25781.320202827454\n",
      "ep 2879: ep_len:515 episode reward: total was -29.650000. running mean: 2.396552\n",
      "ep 2879: ep_len:508 episode reward: total was 26.790000. running mean: 2.640486\n",
      "ep 2879: ep_len:582 episode reward: total was -20.200000. running mean: 2.412082\n",
      "ep 2879: ep_len:538 episode reward: total was 53.560000. running mean: 2.923561\n",
      "ep 2879: ep_len:3 episode reward: total was 1.010000. running mean: 2.904425\n",
      "ep 2879: ep_len:609 episode reward: total was -12.990000. running mean: 2.745481\n",
      "ep 2879: ep_len:302 episode reward: total was 4.980000. running mean: 2.767826\n",
      "epsilon:0.072320 episode_count: 20160. steps_count: 8717266.000000\n",
      "Time elapsed:  25790.83087491989\n",
      "ep 2880: ep_len:209 episode reward: total was 24.730000. running mean: 2.987448\n",
      "ep 2880: ep_len:527 episode reward: total was -20.340000. running mean: 2.754173\n",
      "ep 2880: ep_len:611 episode reward: total was -62.070000. running mean: 2.105932\n",
      "ep 2880: ep_len:528 episode reward: total was 56.090000. running mean: 2.645772\n",
      "ep 2880: ep_len:83 episode reward: total was 12.730000. running mean: 2.746615\n",
      "ep 2880: ep_len:527 episode reward: total was -43.420000. running mean: 2.284948\n",
      "ep 2880: ep_len:603 episode reward: total was -8.220000. running mean: 2.179899\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.072276 episode_count: 20167. steps_count: 8720354.000000\n",
      "Time elapsed:  25805.38104557991\n",
      "ep 2881: ep_len:500 episode reward: total was 33.110000. running mean: 2.489200\n",
      "ep 2881: ep_len:505 episode reward: total was -27.950000. running mean: 2.184808\n",
      "ep 2881: ep_len:597 episode reward: total was -35.220000. running mean: 1.810760\n",
      "ep 2881: ep_len:573 episode reward: total was -44.700000. running mean: 1.345652\n",
      "ep 2881: ep_len:1 episode reward: total was -1.000000. running mean: 1.322196\n",
      "ep 2881: ep_len:521 episode reward: total was 13.060000. running mean: 1.439574\n",
      "ep 2881: ep_len:509 episode reward: total was -43.430000. running mean: 0.990878\n",
      "epsilon:0.072231 episode_count: 20174. steps_count: 8723560.000000\n",
      "Time elapsed:  25815.065883874893\n",
      "ep 2882: ep_len:537 episode reward: total was 23.110000. running mean: 1.212069\n",
      "ep 2882: ep_len:536 episode reward: total was -84.480000. running mean: 0.355149\n",
      "ep 2882: ep_len:531 episode reward: total was -34.290000. running mean: 0.008697\n",
      "ep 2882: ep_len:503 episode reward: total was -5.800000. running mean: -0.049390\n",
      "ep 2882: ep_len:3 episode reward: total was -1.500000. running mean: -0.063896\n",
      "ep 2882: ep_len:527 episode reward: total was -55.180000. running mean: -0.615057\n",
      "ep 2882: ep_len:501 episode reward: total was -34.660000. running mean: -0.955506\n",
      "epsilon:0.072187 episode_count: 20181. steps_count: 8726698.000000\n",
      "Time elapsed:  25824.52349305153\n",
      "ep 2883: ep_len:500 episode reward: total was 70.780000. running mean: -0.238151\n",
      "ep 2883: ep_len:585 episode reward: total was -7.290000. running mean: -0.308670\n",
      "ep 2883: ep_len:582 episode reward: total was 15.820000. running mean: -0.147383\n",
      "ep 2883: ep_len:56 episode reward: total was -7.150000. running mean: -0.217409\n",
      "ep 2883: ep_len:3 episode reward: total was -1.500000. running mean: -0.230235\n",
      "ep 2883: ep_len:578 episode reward: total was -37.430000. running mean: -0.602233\n",
      "ep 2883: ep_len:523 episode reward: total was -21.510000. running mean: -0.811311\n",
      "epsilon:0.072143 episode_count: 20188. steps_count: 8729525.000000\n",
      "Time elapsed:  25840.346631526947\n",
      "ep 2884: ep_len:234 episode reward: total was 28.240000. running mean: -0.520797\n",
      "ep 2884: ep_len:602 episode reward: total was -4.730000. running mean: -0.562890\n",
      "ep 2884: ep_len:371 episode reward: total was 33.580000. running mean: -0.221461\n",
      "ep 2884: ep_len:532 episode reward: total was 5.320000. running mean: -0.166046\n",
      "ep 2884: ep_len:3 episode reward: total was 1.010000. running mean: -0.154286\n",
      "ep 2884: ep_len:671 episode reward: total was 14.200000. running mean: -0.010743\n",
      "ep 2884: ep_len:203 episode reward: total was -5.950000. running mean: -0.070135\n",
      "epsilon:0.072098 episode_count: 20195. steps_count: 8732141.000000\n",
      "Time elapsed:  25848.39073753357\n",
      "ep 2885: ep_len:568 episode reward: total was 45.410000. running mean: 0.384666\n",
      "ep 2885: ep_len:500 episode reward: total was 12.050000. running mean: 0.501319\n",
      "ep 2885: ep_len:632 episode reward: total was -13.740000. running mean: 0.358906\n",
      "ep 2885: ep_len:520 episode reward: total was 17.860000. running mean: 0.533917\n",
      "ep 2885: ep_len:3 episode reward: total was 1.010000. running mean: 0.538678\n",
      "ep 2885: ep_len:500 episode reward: total was -46.470000. running mean: 0.068591\n",
      "ep 2885: ep_len:509 episode reward: total was -24.050000. running mean: -0.172595\n",
      "epsilon:0.072054 episode_count: 20202. steps_count: 8735373.000000\n",
      "Time elapsed:  25858.20618247986\n",
      "ep 2886: ep_len:134 episode reward: total was -18.520000. running mean: -0.356069\n",
      "ep 2886: ep_len:579 episode reward: total was -32.270000. running mean: -0.675208\n",
      "ep 2886: ep_len:430 episode reward: total was 49.310000. running mean: -0.175356\n",
      "ep 2886: ep_len:416 episode reward: total was -0.870000. running mean: -0.182302\n",
      "ep 2886: ep_len:3 episode reward: total was 1.010000. running mean: -0.170379\n",
      "ep 2886: ep_len:320 episode reward: total was 14.540000. running mean: -0.023276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2886: ep_len:513 episode reward: total was -22.820000. running mean: -0.251243\n",
      "epsilon:0.072010 episode_count: 20209. steps_count: 8737768.000000\n",
      "Time elapsed:  25865.697746753693\n",
      "ep 2887: ep_len:642 episode reward: total was 35.120000. running mean: 0.102470\n",
      "ep 2887: ep_len:537 episode reward: total was -7.980000. running mean: 0.021645\n",
      "ep 2887: ep_len:556 episode reward: total was 32.460000. running mean: 0.346028\n",
      "ep 2887: ep_len:500 episode reward: total was 46.610000. running mean: 0.808668\n",
      "ep 2887: ep_len:106 episode reward: total was 32.250000. running mean: 1.123081\n",
      "ep 2887: ep_len:500 episode reward: total was -8.530000. running mean: 1.026551\n",
      "ep 2887: ep_len:276 episode reward: total was -28.650000. running mean: 0.729785\n",
      "epsilon:0.071965 episode_count: 20216. steps_count: 8740885.000000\n",
      "Time elapsed:  25875.14662528038\n",
      "ep 2888: ep_len:220 episode reward: total was 1.240000. running mean: 0.734887\n",
      "ep 2888: ep_len:156 episode reward: total was -15.700000. running mean: 0.570538\n",
      "ep 2888: ep_len:500 episode reward: total was 11.860000. running mean: 0.683433\n",
      "ep 2888: ep_len:546 episode reward: total was 51.280000. running mean: 1.189399\n",
      "ep 2888: ep_len:3 episode reward: total was 1.010000. running mean: 1.187605\n",
      "ep 2888: ep_len:500 episode reward: total was -88.730000. running mean: 0.288429\n",
      "ep 2888: ep_len:300 episode reward: total was 7.900000. running mean: 0.364544\n",
      "epsilon:0.071921 episode_count: 20223. steps_count: 8743110.000000\n",
      "Time elapsed:  25882.251166582108\n",
      "ep 2889: ep_len:96 episode reward: total was 5.910000. running mean: 0.419999\n",
      "ep 2889: ep_len:603 episode reward: total was -13.870000. running mean: 0.277099\n",
      "ep 2889: ep_len:398 episode reward: total was 45.430000. running mean: 0.728628\n",
      "ep 2889: ep_len:500 episode reward: total was -21.970000. running mean: 0.501642\n",
      "ep 2889: ep_len:78 episode reward: total was 23.180000. running mean: 0.728425\n",
      "ep 2889: ep_len:529 episode reward: total was -17.540000. running mean: 0.545741\n",
      "ep 2889: ep_len:315 episode reward: total was -3.090000. running mean: 0.509384\n",
      "epsilon:0.071877 episode_count: 20230. steps_count: 8745629.000000\n",
      "Time elapsed:  25896.8047811985\n",
      "ep 2890: ep_len:556 episode reward: total was 57.830000. running mean: 1.082590\n",
      "ep 2890: ep_len:595 episode reward: total was 9.210000. running mean: 1.163864\n",
      "ep 2890: ep_len:500 episode reward: total was -3.490000. running mean: 1.117325\n",
      "ep 2890: ep_len:506 episode reward: total was 15.700000. running mean: 1.263152\n",
      "ep 2890: ep_len:3 episode reward: total was 1.010000. running mean: 1.260620\n",
      "ep 2890: ep_len:674 episode reward: total was -5.210000. running mean: 1.195914\n",
      "ep 2890: ep_len:517 episode reward: total was -39.250000. running mean: 0.791455\n",
      "epsilon:0.071832 episode_count: 20237. steps_count: 8748980.000000\n",
      "Time elapsed:  25919.594601154327\n",
      "ep 2891: ep_len:500 episode reward: total was 37.470000. running mean: 1.158241\n",
      "ep 2891: ep_len:178 episode reward: total was 1.300000. running mean: 1.159658\n",
      "ep 2891: ep_len:500 episode reward: total was -102.100000. running mean: 0.127062\n",
      "ep 2891: ep_len:402 episode reward: total was -2.820000. running mean: 0.097591\n",
      "ep 2891: ep_len:79 episode reward: total was 14.740000. running mean: 0.244015\n",
      "ep 2891: ep_len:590 episode reward: total was -9.110000. running mean: 0.150475\n",
      "ep 2891: ep_len:552 episode reward: total was -32.340000. running mean: -0.174430\n",
      "epsilon:0.071788 episode_count: 20244. steps_count: 8751781.000000\n",
      "Time elapsed:  25928.193233013153\n",
      "ep 2892: ep_len:617 episode reward: total was -41.650000. running mean: -0.589186\n",
      "ep 2892: ep_len:527 episode reward: total was 57.770000. running mean: -0.005594\n",
      "ep 2892: ep_len:503 episode reward: total was 11.960000. running mean: 0.114062\n",
      "ep 2892: ep_len:500 episode reward: total was 16.410000. running mean: 0.277022\n",
      "ep 2892: ep_len:3 episode reward: total was 1.010000. running mean: 0.284351\n",
      "ep 2892: ep_len:500 episode reward: total was 10.690000. running mean: 0.388408\n",
      "ep 2892: ep_len:507 episode reward: total was -29.230000. running mean: 0.092224\n",
      "epsilon:0.071744 episode_count: 20251. steps_count: 8754938.000000\n",
      "Time elapsed:  25944.0449924469\n",
      "ep 2893: ep_len:502 episode reward: total was 15.760000. running mean: 0.248902\n",
      "ep 2893: ep_len:578 episode reward: total was -32.510000. running mean: -0.078687\n",
      "ep 2893: ep_len:580 episode reward: total was -35.590000. running mean: -0.433801\n",
      "ep 2893: ep_len:521 episode reward: total was -8.920000. running mean: -0.518663\n",
      "ep 2893: ep_len:3 episode reward: total was 1.010000. running mean: -0.503376\n",
      "ep 2893: ep_len:500 episode reward: total was 4.090000. running mean: -0.457442\n",
      "ep 2893: ep_len:528 episode reward: total was 14.320000. running mean: -0.309668\n",
      "epsilon:0.071699 episode_count: 20258. steps_count: 8758150.000000\n",
      "Time elapsed:  25953.749991178513\n",
      "ep 2894: ep_len:522 episode reward: total was -64.130000. running mean: -0.947871\n",
      "ep 2894: ep_len:568 episode reward: total was 38.190000. running mean: -0.556492\n",
      "ep 2894: ep_len:557 episode reward: total was -14.510000. running mean: -0.696027\n",
      "ep 2894: ep_len:500 episode reward: total was 27.910000. running mean: -0.409967\n",
      "ep 2894: ep_len:3 episode reward: total was 1.010000. running mean: -0.395767\n",
      "ep 2894: ep_len:500 episode reward: total was 21.730000. running mean: -0.174510\n",
      "ep 2894: ep_len:348 episode reward: total was 22.170000. running mean: 0.048935\n",
      "epsilon:0.071655 episode_count: 20265. steps_count: 8761148.000000\n",
      "Time elapsed:  25962.857266664505\n",
      "ep 2895: ep_len:582 episode reward: total was 38.790000. running mean: 0.436346\n",
      "ep 2895: ep_len:532 episode reward: total was 12.430000. running mean: 0.556282\n",
      "ep 2895: ep_len:464 episode reward: total was 63.980000. running mean: 1.190520\n",
      "ep 2895: ep_len:570 episode reward: total was 53.730000. running mean: 1.715914\n",
      "ep 2895: ep_len:3 episode reward: total was 1.010000. running mean: 1.708855\n",
      "ep 2895: ep_len:680 episode reward: total was 25.650000. running mean: 1.948267\n",
      "ep 2895: ep_len:512 episode reward: total was -13.860000. running mean: 1.790184\n",
      "epsilon:0.071611 episode_count: 20272. steps_count: 8764491.000000\n",
      "Time elapsed:  25978.890084028244\n",
      "ep 2896: ep_len:229 episode reward: total was 6.260000. running mean: 1.834882\n",
      "ep 2896: ep_len:511 episode reward: total was -5.460000. running mean: 1.761933\n",
      "ep 2896: ep_len:548 episode reward: total was -36.580000. running mean: 1.378514\n",
      "ep 2896: ep_len:502 episode reward: total was -3.820000. running mean: 1.326529\n",
      "ep 2896: ep_len:72 episode reward: total was 16.080000. running mean: 1.474064\n",
      "ep 2896: ep_len:500 episode reward: total was 30.370000. running mean: 1.763023\n",
      "ep 2896: ep_len:592 episode reward: total was -28.290000. running mean: 1.462493\n",
      "epsilon:0.071566 episode_count: 20279. steps_count: 8767445.000000\n",
      "Time elapsed:  25993.977207899094\n",
      "ep 2897: ep_len:511 episode reward: total was -43.420000. running mean: 1.013668\n",
      "ep 2897: ep_len:522 episode reward: total was -57.400000. running mean: 0.429531\n",
      "ep 2897: ep_len:500 episode reward: total was 24.780000. running mean: 0.673036\n",
      "ep 2897: ep_len:577 episode reward: total was 28.010000. running mean: 0.946406\n",
      "ep 2897: ep_len:3 episode reward: total was 1.010000. running mean: 0.947041\n",
      "ep 2897: ep_len:531 episode reward: total was -43.950000. running mean: 0.498071\n",
      "ep 2897: ep_len:545 episode reward: total was 15.500000. running mean: 0.648090\n",
      "epsilon:0.071522 episode_count: 20286. steps_count: 8770634.000000\n",
      "Time elapsed:  26003.477286338806\n",
      "ep 2898: ep_len:616 episode reward: total was 65.520000. running mean: 1.296809\n",
      "ep 2898: ep_len:513 episode reward: total was -4.210000. running mean: 1.241741\n",
      "ep 2898: ep_len:643 episode reward: total was -30.470000. running mean: 0.924624\n",
      "ep 2898: ep_len:504 episode reward: total was 38.340000. running mean: 1.298778\n",
      "ep 2898: ep_len:3 episode reward: total was 1.010000. running mean: 1.295890\n",
      "ep 2898: ep_len:527 episode reward: total was 43.700000. running mean: 1.719931\n",
      "ep 2898: ep_len:500 episode reward: total was 20.120000. running mean: 1.903932\n",
      "epsilon:0.071478 episode_count: 20293. steps_count: 8773940.000000\n",
      "Time elapsed:  26019.573573350906\n",
      "ep 2899: ep_len:113 episode reward: total was 4.950000. running mean: 1.934392\n",
      "ep 2899: ep_len:593 episode reward: total was 31.320000. running mean: 2.228248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2899: ep_len:344 episode reward: total was 25.350000. running mean: 2.459466\n",
      "ep 2899: ep_len:500 episode reward: total was 68.980000. running mean: 3.124671\n",
      "ep 2899: ep_len:96 episode reward: total was 26.240000. running mean: 3.355825\n",
      "ep 2899: ep_len:852 episode reward: total was -589.320000. running mean: -2.570934\n",
      "ep 2899: ep_len:505 episode reward: total was -10.230000. running mean: -2.647524\n",
      "epsilon:0.071433 episode_count: 20300. steps_count: 8776943.000000\n",
      "Time elapsed:  26028.78892660141\n",
      "ep 2900: ep_len:546 episode reward: total was 66.470000. running mean: -1.956349\n",
      "ep 2900: ep_len:189 episode reward: total was 4.780000. running mean: -1.888986\n",
      "ep 2900: ep_len:500 episode reward: total was 24.930000. running mean: -1.620796\n",
      "ep 2900: ep_len:411 episode reward: total was -6.330000. running mean: -1.667888\n",
      "ep 2900: ep_len:3 episode reward: total was -1.500000. running mean: -1.666209\n",
      "ep 2900: ep_len:555 episode reward: total was -41.160000. running mean: -2.061147\n",
      "ep 2900: ep_len:548 episode reward: total was 15.830000. running mean: -1.882235\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.071389 episode_count: 20307. steps_count: 8779695.000000\n",
      "Time elapsed:  26042.847319364548\n",
      "ep 2901: ep_len:639 episode reward: total was 23.700000. running mean: -1.626413\n",
      "ep 2901: ep_len:500 episode reward: total was 50.220000. running mean: -1.107949\n",
      "ep 2901: ep_len:589 episode reward: total was -4.380000. running mean: -1.140669\n",
      "ep 2901: ep_len:570 episode reward: total was 56.730000. running mean: -0.561963\n",
      "ep 2901: ep_len:3 episode reward: total was 1.010000. running mean: -0.546243\n",
      "ep 2901: ep_len:544 episode reward: total was -25.080000. running mean: -0.791581\n",
      "ep 2901: ep_len:554 episode reward: total was -82.020000. running mean: -1.603865\n",
      "epsilon:0.071345 episode_count: 20314. steps_count: 8783094.000000\n",
      "Time elapsed:  26052.989911794662\n",
      "ep 2902: ep_len:631 episode reward: total was -55.690000. running mean: -2.144726\n",
      "ep 2902: ep_len:500 episode reward: total was 75.970000. running mean: -1.363579\n",
      "ep 2902: ep_len:64 episode reward: total was 3.780000. running mean: -1.312143\n",
      "ep 2902: ep_len:536 episode reward: total was -0.450000. running mean: -1.303522\n",
      "ep 2902: ep_len:3 episode reward: total was 1.010000. running mean: -1.280386\n",
      "ep 2902: ep_len:508 episode reward: total was -16.080000. running mean: -1.428383\n",
      "ep 2902: ep_len:198 episode reward: total was 3.000000. running mean: -1.384099\n",
      "epsilon:0.071300 episode_count: 20321. steps_count: 8785534.000000\n",
      "Time elapsed:  26060.58692741394\n",
      "ep 2903: ep_len:652 episode reward: total was -77.600000. running mean: -2.146258\n",
      "ep 2903: ep_len:510 episode reward: total was 41.170000. running mean: -1.713095\n",
      "ep 2903: ep_len:500 episode reward: total was 9.700000. running mean: -1.598964\n",
      "ep 2903: ep_len:500 episode reward: total was 49.470000. running mean: -1.088275\n",
      "ep 2903: ep_len:102 episode reward: total was 0.280000. running mean: -1.074592\n",
      "ep 2903: ep_len:500 episode reward: total was 18.730000. running mean: -0.876546\n",
      "ep 2903: ep_len:561 episode reward: total was 0.620000. running mean: -0.861580\n",
      "epsilon:0.071256 episode_count: 20328. steps_count: 8788859.000000\n",
      "Time elapsed:  26070.666632175446\n",
      "ep 2904: ep_len:584 episode reward: total was 76.590000. running mean: -0.087065\n",
      "ep 2904: ep_len:518 episode reward: total was -22.610000. running mean: -0.312294\n",
      "ep 2904: ep_len:596 episode reward: total was -27.240000. running mean: -0.581571\n",
      "ep 2904: ep_len:500 episode reward: total was 11.350000. running mean: -0.462255\n",
      "ep 2904: ep_len:3 episode reward: total was 1.010000. running mean: -0.447533\n",
      "ep 2904: ep_len:500 episode reward: total was -3.230000. running mean: -0.475357\n",
      "ep 2904: ep_len:500 episode reward: total was -40.960000. running mean: -0.880204\n",
      "epsilon:0.071212 episode_count: 20335. steps_count: 8792060.000000\n",
      "Time elapsed:  26079.028263568878\n",
      "ep 2905: ep_len:538 episode reward: total was -84.630000. running mean: -1.717702\n",
      "ep 2905: ep_len:529 episode reward: total was 36.150000. running mean: -1.339025\n",
      "ep 2905: ep_len:502 episode reward: total was -9.820000. running mean: -1.423835\n",
      "ep 2905: ep_len:635 episode reward: total was 55.680000. running mean: -0.852796\n",
      "ep 2905: ep_len:3 episode reward: total was 1.010000. running mean: -0.834168\n",
      "ep 2905: ep_len:319 episode reward: total was 26.680000. running mean: -0.559027\n",
      "ep 2905: ep_len:298 episode reward: total was -8.040000. running mean: -0.633836\n",
      "epsilon:0.071167 episode_count: 20342. steps_count: 8794884.000000\n",
      "Time elapsed:  26089.220710992813\n",
      "ep 2906: ep_len:594 episode reward: total was -68.460000. running mean: -1.312098\n",
      "ep 2906: ep_len:342 episode reward: total was 3.900000. running mean: -1.259977\n",
      "ep 2906: ep_len:521 episode reward: total was -52.990000. running mean: -1.777277\n",
      "ep 2906: ep_len:500 episode reward: total was -58.750000. running mean: -2.347004\n",
      "ep 2906: ep_len:3 episode reward: total was 1.010000. running mean: -2.313434\n",
      "ep 2906: ep_len:605 episode reward: total was -20.850000. running mean: -2.498800\n",
      "ep 2906: ep_len:524 episode reward: total was 4.900000. running mean: -2.424812\n",
      "epsilon:0.071123 episode_count: 20349. steps_count: 8797973.000000\n",
      "Time elapsed:  26106.564318180084\n",
      "ep 2907: ep_len:672 episode reward: total was -71.030000. running mean: -3.110864\n",
      "ep 2907: ep_len:653 episode reward: total was 57.840000. running mean: -2.501355\n",
      "ep 2907: ep_len:659 episode reward: total was -43.990000. running mean: -2.916242\n",
      "ep 2907: ep_len:532 episode reward: total was -58.870000. running mean: -3.475779\n",
      "ep 2907: ep_len:3 episode reward: total was 1.010000. running mean: -3.430922\n",
      "ep 2907: ep_len:549 episode reward: total was 10.260000. running mean: -3.294012\n",
      "ep 2907: ep_len:627 episode reward: total was 21.950000. running mean: -3.041572\n",
      "epsilon:0.071079 episode_count: 20356. steps_count: 8801668.000000\n",
      "Time elapsed:  26116.17144727707\n",
      "ep 2908: ep_len:119 episode reward: total was 20.410000. running mean: -2.807056\n",
      "ep 2908: ep_len:500 episode reward: total was 20.810000. running mean: -2.570886\n",
      "ep 2908: ep_len:566 episode reward: total was -18.650000. running mean: -2.731677\n",
      "ep 2908: ep_len:527 episode reward: total was -0.810000. running mean: -2.712460\n",
      "ep 2908: ep_len:3 episode reward: total was 1.010000. running mean: -2.675236\n",
      "ep 2908: ep_len:226 episode reward: total was 21.810000. running mean: -2.430383\n",
      "ep 2908: ep_len:500 episode reward: total was -2.900000. running mean: -2.435080\n",
      "epsilon:0.071034 episode_count: 20363. steps_count: 8804109.000000\n",
      "Time elapsed:  26124.428908109665\n",
      "ep 2909: ep_len:555 episode reward: total was -29.140000. running mean: -2.702129\n",
      "ep 2909: ep_len:531 episode reward: total was 49.330000. running mean: -2.181807\n",
      "ep 2909: ep_len:552 episode reward: total was -58.930000. running mean: -2.749289\n",
      "ep 2909: ep_len:597 episode reward: total was 44.180000. running mean: -2.279996\n",
      "ep 2909: ep_len:3 episode reward: total was 1.010000. running mean: -2.247096\n",
      "ep 2909: ep_len:628 episode reward: total was -68.420000. running mean: -2.908826\n",
      "ep 2909: ep_len:529 episode reward: total was -5.520000. running mean: -2.934937\n",
      "epsilon:0.070990 episode_count: 20370. steps_count: 8807504.000000\n",
      "Time elapsed:  26133.32972717285\n",
      "ep 2910: ep_len:537 episode reward: total was -29.870000. running mean: -3.204288\n",
      "ep 2910: ep_len:500 episode reward: total was 64.990000. running mean: -2.522345\n",
      "ep 2910: ep_len:568 episode reward: total was -26.950000. running mean: -2.766622\n",
      "ep 2910: ep_len:56 episode reward: total was -1.140000. running mean: -2.750355\n",
      "ep 2910: ep_len:3 episode reward: total was 1.010000. running mean: -2.712752\n",
      "ep 2910: ep_len:645 episode reward: total was -194.350000. running mean: -4.629124\n",
      "ep 2910: ep_len:556 episode reward: total was 19.230000. running mean: -4.390533\n",
      "epsilon:0.070946 episode_count: 20377. steps_count: 8810369.000000\n",
      "Time elapsed:  26140.972407579422\n",
      "ep 2911: ep_len:541 episode reward: total was -18.830000. running mean: -4.534928\n",
      "ep 2911: ep_len:185 episode reward: total was -11.240000. running mean: -4.601978\n",
      "ep 2911: ep_len:565 episode reward: total was -22.790000. running mean: -4.783859\n",
      "ep 2911: ep_len:500 episode reward: total was -1.070000. running mean: -4.746720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2911: ep_len:103 episode reward: total was 23.740000. running mean: -4.461853\n",
      "ep 2911: ep_len:610 episode reward: total was -65.770000. running mean: -5.074934\n",
      "ep 2911: ep_len:570 episode reward: total was 4.220000. running mean: -4.981985\n",
      "epsilon:0.070901 episode_count: 20384. steps_count: 8813443.000000\n",
      "Time elapsed:  26158.794902801514\n",
      "ep 2912: ep_len:500 episode reward: total was 67.290000. running mean: -4.259265\n",
      "ep 2912: ep_len:305 episode reward: total was -20.510000. running mean: -4.421772\n",
      "ep 2912: ep_len:500 episode reward: total was 3.220000. running mean: -4.345355\n",
      "ep 2912: ep_len:511 episode reward: total was 50.290000. running mean: -3.799001\n",
      "ep 2912: ep_len:75 episode reward: total was -9.300000. running mean: -3.854011\n",
      "ep 2912: ep_len:500 episode reward: total was 17.420000. running mean: -3.641271\n",
      "ep 2912: ep_len:533 episode reward: total was -11.310000. running mean: -3.717958\n",
      "epsilon:0.070857 episode_count: 20391. steps_count: 8816367.000000\n",
      "Time elapsed:  26166.565452814102\n",
      "ep 2913: ep_len:547 episode reward: total was 18.690000. running mean: -3.493879\n",
      "ep 2913: ep_len:369 episode reward: total was -0.080000. running mean: -3.459740\n",
      "ep 2913: ep_len:623 episode reward: total was -69.530000. running mean: -4.120443\n",
      "ep 2913: ep_len:500 episode reward: total was 35.080000. running mean: -3.728438\n",
      "ep 2913: ep_len:49 episode reward: total was 16.510000. running mean: -3.526054\n",
      "ep 2913: ep_len:500 episode reward: total was 14.670000. running mean: -3.344093\n",
      "ep 2913: ep_len:500 episode reward: total was 12.230000. running mean: -3.188352\n",
      "epsilon:0.070813 episode_count: 20398. steps_count: 8819455.000000\n",
      "Time elapsed:  26174.844249010086\n",
      "ep 2914: ep_len:506 episode reward: total was 40.440000. running mean: -2.752069\n",
      "ep 2914: ep_len:500 episode reward: total was 75.420000. running mean: -1.970348\n",
      "ep 2914: ep_len:500 episode reward: total was 24.370000. running mean: -1.706945\n",
      "ep 2914: ep_len:528 episode reward: total was 32.740000. running mean: -1.362475\n",
      "ep 2914: ep_len:118 episode reward: total was 22.850000. running mean: -1.120350\n",
      "ep 2914: ep_len:676 episode reward: total was -3.260000. running mean: -1.141747\n",
      "ep 2914: ep_len:526 episode reward: total was 6.430000. running mean: -1.066029\n",
      "epsilon:0.070768 episode_count: 20405. steps_count: 8822809.000000\n",
      "Time elapsed:  26187.677392959595\n",
      "ep 2915: ep_len:500 episode reward: total was 62.360000. running mean: -0.431769\n",
      "ep 2915: ep_len:505 episode reward: total was -13.590000. running mean: -0.563351\n",
      "ep 2915: ep_len:557 episode reward: total was -45.080000. running mean: -1.008518\n",
      "ep 2915: ep_len:576 episode reward: total was 42.600000. running mean: -0.572433\n",
      "ep 2915: ep_len:3 episode reward: total was 1.010000. running mean: -0.556608\n",
      "ep 2915: ep_len:261 episode reward: total was 39.520000. running mean: -0.155842\n",
      "ep 2915: ep_len:263 episode reward: total was -9.590000. running mean: -0.250184\n",
      "epsilon:0.070724 episode_count: 20412. steps_count: 8825474.000000\n",
      "Time elapsed:  26194.952788591385\n",
      "ep 2916: ep_len:502 episode reward: total was -4.490000. running mean: -0.292582\n",
      "ep 2916: ep_len:511 episode reward: total was 22.860000. running mean: -0.061056\n",
      "ep 2916: ep_len:500 episode reward: total was -6.180000. running mean: -0.122246\n",
      "ep 2916: ep_len:56 episode reward: total was -0.160000. running mean: -0.122623\n",
      "ep 2916: ep_len:3 episode reward: total was 1.010000. running mean: -0.111297\n",
      "ep 2916: ep_len:636 episode reward: total was -5.530000. running mean: -0.165484\n",
      "ep 2916: ep_len:314 episode reward: total was -0.730000. running mean: -0.171129\n",
      "epsilon:0.070680 episode_count: 20419. steps_count: 8827996.000000\n",
      "Time elapsed:  26201.87372136116\n",
      "ep 2917: ep_len:623 episode reward: total was -85.090000. running mean: -1.020318\n",
      "ep 2917: ep_len:502 episode reward: total was -48.810000. running mean: -1.498215\n",
      "ep 2917: ep_len:500 episode reward: total was -21.460000. running mean: -1.697833\n",
      "ep 2917: ep_len:518 episode reward: total was -34.350000. running mean: -2.024354\n",
      "ep 2917: ep_len:87 episode reward: total was 29.120000. running mean: -1.712911\n",
      "ep 2917: ep_len:500 episode reward: total was 33.190000. running mean: -1.363882\n",
      "ep 2917: ep_len:601 episode reward: total was 22.220000. running mean: -1.128043\n",
      "epsilon:0.070635 episode_count: 20426. steps_count: 8831327.000000\n",
      "Time elapsed:  26211.876530647278\n",
      "ep 2918: ep_len:528 episode reward: total was -92.200000. running mean: -2.038762\n",
      "ep 2918: ep_len:586 episode reward: total was -30.930000. running mean: -2.327675\n",
      "ep 2918: ep_len:549 episode reward: total was -16.370000. running mean: -2.468098\n",
      "ep 2918: ep_len:500 episode reward: total was 18.280000. running mean: -2.260617\n",
      "ep 2918: ep_len:3 episode reward: total was -0.490000. running mean: -2.242911\n",
      "ep 2918: ep_len:500 episode reward: total was 2.270000. running mean: -2.197782\n",
      "ep 2918: ep_len:553 episode reward: total was -41.530000. running mean: -2.591104\n",
      "epsilon:0.070591 episode_count: 20433. steps_count: 8834546.000000\n",
      "Time elapsed:  26216.8514688015\n",
      "ep 2919: ep_len:646 episode reward: total was -63.190000. running mean: -3.197093\n",
      "ep 2919: ep_len:544 episode reward: total was 5.570000. running mean: -3.109422\n",
      "ep 2919: ep_len:518 episode reward: total was 40.310000. running mean: -2.675228\n",
      "ep 2919: ep_len:501 episode reward: total was 20.910000. running mean: -2.439375\n",
      "ep 2919: ep_len:3 episode reward: total was 1.010000. running mean: -2.404882\n",
      "ep 2919: ep_len:594 episode reward: total was -33.800000. running mean: -2.718833\n",
      "ep 2919: ep_len:284 episode reward: total was 0.760000. running mean: -2.684045\n",
      "epsilon:0.070547 episode_count: 20440. steps_count: 8837636.000000\n",
      "Time elapsed:  26221.6252245903\n",
      "ep 2920: ep_len:112 episode reward: total was -4.180000. running mean: -2.699004\n",
      "ep 2920: ep_len:533 episode reward: total was 1.340000. running mean: -2.658614\n",
      "ep 2920: ep_len:626 episode reward: total was -14.410000. running mean: -2.776128\n",
      "ep 2920: ep_len:502 episode reward: total was 39.140000. running mean: -2.356967\n",
      "ep 2920: ep_len:129 episode reward: total was 31.870000. running mean: -2.014697\n",
      "ep 2920: ep_len:588 episode reward: total was -40.740000. running mean: -2.401950\n",
      "ep 2920: ep_len:526 episode reward: total was -12.040000. running mean: -2.498331\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.070502 episode_count: 20447. steps_count: 8840652.000000\n",
      "Time elapsed:  26234.11177420616\n",
      "ep 2921: ep_len:264 episode reward: total was 18.410000. running mean: -2.289247\n",
      "ep 2921: ep_len:853 episode reward: total was -236.950000. running mean: -4.635855\n",
      "ep 2921: ep_len:611 episode reward: total was -53.010000. running mean: -5.119596\n",
      "ep 2921: ep_len:519 episode reward: total was -22.130000. running mean: -5.289700\n",
      "ep 2921: ep_len:3 episode reward: total was -0.490000. running mean: -5.241703\n",
      "ep 2921: ep_len:550 episode reward: total was -16.320000. running mean: -5.352486\n",
      "ep 2921: ep_len:557 episode reward: total was -19.490000. running mean: -5.493861\n",
      "epsilon:0.070458 episode_count: 20454. steps_count: 8844009.000000\n",
      "Time elapsed:  26242.992671489716\n",
      "ep 2922: ep_len:545 episode reward: total was -21.940000. running mean: -5.658323\n",
      "ep 2922: ep_len:261 episode reward: total was -17.840000. running mean: -5.780140\n",
      "ep 2922: ep_len:338 episode reward: total was -18.200000. running mean: -5.904338\n",
      "ep 2922: ep_len:146 episode reward: total was 11.670000. running mean: -5.728595\n",
      "ep 2922: ep_len:3 episode reward: total was 1.010000. running mean: -5.661209\n",
      "ep 2922: ep_len:560 episode reward: total was -27.360000. running mean: -5.878197\n",
      "ep 2922: ep_len:289 episode reward: total was -1.730000. running mean: -5.836715\n",
      "epsilon:0.070414 episode_count: 20461. steps_count: 8846151.000000\n",
      "Time elapsed:  26258.199029684067\n",
      "ep 2923: ep_len:222 episode reward: total was -5.290000. running mean: -5.831248\n",
      "ep 2923: ep_len:500 episode reward: total was 29.760000. running mean: -5.475335\n",
      "ep 2923: ep_len:586 episode reward: total was 6.050000. running mean: -5.360082\n",
      "ep 2923: ep_len:525 episode reward: total was -27.790000. running mean: -5.584381\n",
      "ep 2923: ep_len:102 episode reward: total was -39.730000. running mean: -5.925837\n",
      "ep 2923: ep_len:500 episode reward: total was 16.330000. running mean: -5.703279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2923: ep_len:510 episode reward: total was -28.060000. running mean: -5.926846\n",
      "epsilon:0.070369 episode_count: 20468. steps_count: 8849096.000000\n",
      "Time elapsed:  26264.20342183113\n",
      "ep 2924: ep_len:524 episode reward: total was -38.230000. running mean: -6.249878\n",
      "ep 2924: ep_len:603 episode reward: total was 17.000000. running mean: -6.017379\n",
      "ep 2924: ep_len:545 episode reward: total was -27.430000. running mean: -6.231505\n",
      "ep 2924: ep_len:500 episode reward: total was 62.930000. running mean: -5.539890\n",
      "ep 2924: ep_len:110 episode reward: total was 23.810000. running mean: -5.246391\n",
      "ep 2924: ep_len:563 episode reward: total was -38.240000. running mean: -5.576327\n",
      "ep 2924: ep_len:500 episode reward: total was -22.550000. running mean: -5.746064\n",
      "epsilon:0.070325 episode_count: 20475. steps_count: 8852441.000000\n",
      "Time elapsed:  26277.9366004467\n",
      "ep 2925: ep_len:257 episode reward: total was 17.420000. running mean: -5.514403\n",
      "ep 2925: ep_len:500 episode reward: total was -17.020000. running mean: -5.629459\n",
      "ep 2925: ep_len:576 episode reward: total was -24.620000. running mean: -5.819365\n",
      "ep 2925: ep_len:551 episode reward: total was 27.580000. running mean: -5.485371\n",
      "ep 2925: ep_len:3 episode reward: total was -0.490000. running mean: -5.435417\n",
      "ep 2925: ep_len:567 episode reward: total was -21.480000. running mean: -5.595863\n",
      "ep 2925: ep_len:550 episode reward: total was 6.510000. running mean: -5.474804\n",
      "epsilon:0.070281 episode_count: 20482. steps_count: 8855445.000000\n",
      "Time elapsed:  26290.477429628372\n",
      "ep 2926: ep_len:134 episode reward: total was -46.070000. running mean: -5.880756\n",
      "ep 2926: ep_len:500 episode reward: total was -85.290000. running mean: -6.674849\n",
      "ep 2926: ep_len:687 episode reward: total was -18.790000. running mean: -6.796000\n",
      "ep 2926: ep_len:500 episode reward: total was 61.890000. running mean: -6.109140\n",
      "ep 2926: ep_len:124 episode reward: total was 4.830000. running mean: -5.999749\n",
      "ep 2926: ep_len:541 episode reward: total was -3.130000. running mean: -5.971051\n",
      "ep 2926: ep_len:557 episode reward: total was -2.320000. running mean: -5.934541\n",
      "epsilon:0.070236 episode_count: 20489. steps_count: 8858488.000000\n",
      "Time elapsed:  26297.47703051567\n",
      "ep 2927: ep_len:536 episode reward: total was 49.390000. running mean: -5.381295\n",
      "ep 2927: ep_len:593 episode reward: total was 6.360000. running mean: -5.263883\n",
      "ep 2927: ep_len:500 episode reward: total was 18.500000. running mean: -5.026244\n",
      "ep 2927: ep_len:531 episode reward: total was 30.780000. running mean: -4.668181\n",
      "ep 2927: ep_len:3 episode reward: total was 1.010000. running mean: -4.611399\n",
      "ep 2927: ep_len:500 episode reward: total was 1.490000. running mean: -4.550385\n",
      "ep 2927: ep_len:549 episode reward: total was 16.990000. running mean: -4.334982\n",
      "epsilon:0.070192 episode_count: 20496. steps_count: 8861700.000000\n",
      "Time elapsed:  26310.167454004288\n",
      "ep 2928: ep_len:658 episode reward: total was -42.870000. running mean: -4.720332\n",
      "ep 2928: ep_len:536 episode reward: total was 16.610000. running mean: -4.507028\n",
      "ep 2928: ep_len:669 episode reward: total was -0.050000. running mean: -4.462458\n",
      "ep 2928: ep_len:501 episode reward: total was 0.110000. running mean: -4.416734\n",
      "ep 2928: ep_len:3 episode reward: total was 0.000000. running mean: -4.372566\n",
      "ep 2928: ep_len:500 episode reward: total was -10.090000. running mean: -4.429741\n",
      "ep 2928: ep_len:507 episode reward: total was 3.330000. running mean: -4.352143\n",
      "epsilon:0.070148 episode_count: 20503. steps_count: 8865074.000000\n",
      "Time elapsed:  26319.02161502838\n",
      "ep 2929: ep_len:643 episode reward: total was -69.800000. running mean: -5.006622\n",
      "ep 2929: ep_len:608 episode reward: total was 23.140000. running mean: -4.725156\n",
      "ep 2929: ep_len:527 episode reward: total was -1.250000. running mean: -4.690404\n",
      "ep 2929: ep_len:44 episode reward: total was -6.780000. running mean: -4.711300\n",
      "ep 2929: ep_len:3 episode reward: total was 1.010000. running mean: -4.654087\n",
      "ep 2929: ep_len:500 episode reward: total was -10.570000. running mean: -4.713246\n",
      "ep 2929: ep_len:535 episode reward: total was -36.540000. running mean: -5.031514\n",
      "epsilon:0.070103 episode_count: 20510. steps_count: 8867934.000000\n",
      "Time elapsed:  26331.221354484558\n",
      "ep 2930: ep_len:598 episode reward: total was 50.190000. running mean: -4.479298\n",
      "ep 2930: ep_len:535 episode reward: total was -67.320000. running mean: -5.107706\n",
      "ep 2930: ep_len:517 episode reward: total was -29.410000. running mean: -5.350728\n",
      "ep 2930: ep_len:500 episode reward: total was 71.490000. running mean: -4.582321\n",
      "ep 2930: ep_len:53 episode reward: total was 22.000000. running mean: -4.316498\n",
      "ep 2930: ep_len:618 episode reward: total was -10.360000. running mean: -4.376933\n",
      "ep 2930: ep_len:576 episode reward: total was -40.890000. running mean: -4.742064\n",
      "epsilon:0.070059 episode_count: 20517. steps_count: 8871331.000000\n",
      "Time elapsed:  26340.167474269867\n",
      "ep 2931: ep_len:528 episode reward: total was -34.420000. running mean: -5.038843\n",
      "ep 2931: ep_len:661 episode reward: total was -169.290000. running mean: -6.681355\n",
      "ep 2931: ep_len:553 episode reward: total was 17.080000. running mean: -6.443741\n",
      "ep 2931: ep_len:528 episode reward: total was -16.180000. running mean: -6.541104\n",
      "ep 2931: ep_len:51 episode reward: total was 21.000000. running mean: -6.265693\n",
      "ep 2931: ep_len:263 episode reward: total was 16.320000. running mean: -6.039836\n",
      "ep 2931: ep_len:286 episode reward: total was -87.150000. running mean: -6.850937\n",
      "epsilon:0.070015 episode_count: 20524. steps_count: 8874201.000000\n",
      "Time elapsed:  26347.869904756546\n",
      "ep 2932: ep_len:134 episode reward: total was 2.930000. running mean: -6.753128\n",
      "ep 2932: ep_len:500 episode reward: total was 68.510000. running mean: -6.000497\n",
      "ep 2932: ep_len:590 episode reward: total was -18.030000. running mean: -6.120792\n",
      "ep 2932: ep_len:500 episode reward: total was 87.680000. running mean: -5.182784\n",
      "ep 2932: ep_len:81 episode reward: total was 15.740000. running mean: -4.973556\n",
      "ep 2932: ep_len:554 episode reward: total was -119.910000. running mean: -6.122920\n",
      "ep 2932: ep_len:503 episode reward: total was -6.470000. running mean: -6.126391\n",
      "epsilon:0.069970 episode_count: 20531. steps_count: 8877063.000000\n",
      "Time elapsed:  26368.439176797867\n",
      "ep 2933: ep_len:573 episode reward: total was -42.980000. running mean: -6.494927\n",
      "ep 2933: ep_len:540 episode reward: total was 40.340000. running mean: -6.026578\n",
      "ep 2933: ep_len:574 episode reward: total was -22.940000. running mean: -6.195712\n",
      "ep 2933: ep_len:528 episode reward: total was 48.950000. running mean: -5.644255\n",
      "ep 2933: ep_len:3 episode reward: total was 1.010000. running mean: -5.577713\n",
      "ep 2933: ep_len:524 episode reward: total was -76.170000. running mean: -6.283635\n",
      "ep 2933: ep_len:347 episode reward: total was -8.140000. running mean: -6.302199\n",
      "epsilon:0.069926 episode_count: 20538. steps_count: 8880152.000000\n",
      "Time elapsed:  26376.67502140999\n",
      "ep 2934: ep_len:134 episode reward: total was 7.000000. running mean: -6.169177\n",
      "ep 2934: ep_len:500 episode reward: total was -93.600000. running mean: -7.043485\n",
      "ep 2934: ep_len:559 episode reward: total was 9.930000. running mean: -6.873750\n",
      "ep 2934: ep_len:526 episode reward: total was -2.590000. running mean: -6.830913\n",
      "ep 2934: ep_len:112 episode reward: total was 26.740000. running mean: -6.495204\n",
      "ep 2934: ep_len:576 episode reward: total was 9.920000. running mean: -6.331052\n",
      "ep 2934: ep_len:500 episode reward: total was 19.940000. running mean: -6.068341\n",
      "epsilon:0.069882 episode_count: 20545. steps_count: 8883059.000000\n",
      "Time elapsed:  26384.21580004692\n",
      "ep 2935: ep_len:529 episode reward: total was 42.250000. running mean: -5.585158\n",
      "ep 2935: ep_len:514 episode reward: total was -9.360000. running mean: -5.622906\n",
      "ep 2935: ep_len:528 episode reward: total was -25.290000. running mean: -5.819577\n",
      "ep 2935: ep_len:511 episode reward: total was -20.310000. running mean: -5.964481\n",
      "ep 2935: ep_len:3 episode reward: total was 1.010000. running mean: -5.894737\n",
      "ep 2935: ep_len:336 episode reward: total was 11.120000. running mean: -5.724589\n",
      "ep 2935: ep_len:563 episode reward: total was -26.940000. running mean: -5.936743\n",
      "epsilon:0.069837 episode_count: 20552. steps_count: 8886043.000000\n",
      "Time elapsed:  26395.42107605934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2936: ep_len:193 episode reward: total was 16.610000. running mean: -5.711276\n",
      "ep 2936: ep_len:500 episode reward: total was -9.780000. running mean: -5.751963\n",
      "ep 2936: ep_len:500 episode reward: total was 4.600000. running mean: -5.648444\n",
      "ep 2936: ep_len:500 episode reward: total was -12.270000. running mean: -5.714659\n",
      "ep 2936: ep_len:85 episode reward: total was 17.250000. running mean: -5.485012\n",
      "ep 2936: ep_len:631 episode reward: total was -10.940000. running mean: -5.539562\n",
      "ep 2936: ep_len:500 episode reward: total was 18.440000. running mean: -5.299767\n",
      "epsilon:0.069793 episode_count: 20559. steps_count: 8888952.000000\n",
      "Time elapsed:  26403.213859796524\n",
      "ep 2937: ep_len:500 episode reward: total was 42.560000. running mean: -4.821169\n",
      "ep 2937: ep_len:547 episode reward: total was 26.580000. running mean: -4.507157\n",
      "ep 2937: ep_len:533 episode reward: total was -9.340000. running mean: -4.555486\n",
      "ep 2937: ep_len:513 episode reward: total was 30.860000. running mean: -4.201331\n",
      "ep 2937: ep_len:131 episode reward: total was 29.840000. running mean: -3.860918\n",
      "ep 2937: ep_len:609 episode reward: total was 23.800000. running mean: -3.584308\n",
      "ep 2937: ep_len:500 episode reward: total was 24.320000. running mean: -3.305265\n",
      "epsilon:0.069749 episode_count: 20566. steps_count: 8892285.000000\n",
      "Time elapsed:  26411.955840587616\n",
      "ep 2938: ep_len:633 episode reward: total was -162.380000. running mean: -4.896013\n",
      "ep 2938: ep_len:542 episode reward: total was 35.090000. running mean: -4.496153\n",
      "ep 2938: ep_len:500 episode reward: total was -5.500000. running mean: -4.506191\n",
      "ep 2938: ep_len:511 episode reward: total was 24.760000. running mean: -4.213529\n",
      "ep 2938: ep_len:3 episode reward: total was 1.010000. running mean: -4.161294\n",
      "ep 2938: ep_len:568 episode reward: total was -14.740000. running mean: -4.267081\n",
      "ep 2938: ep_len:577 episode reward: total was 18.290000. running mean: -4.041510\n",
      "epsilon:0.069704 episode_count: 20573. steps_count: 8895619.000000\n",
      "Time elapsed:  26421.022850990295\n",
      "ep 2939: ep_len:500 episode reward: total was 72.830000. running mean: -3.272795\n",
      "ep 2939: ep_len:350 episode reward: total was -6.340000. running mean: -3.303467\n",
      "ep 2939: ep_len:624 episode reward: total was 8.830000. running mean: -3.182132\n",
      "ep 2939: ep_len:129 episode reward: total was 10.590000. running mean: -3.044411\n",
      "ep 2939: ep_len:3 episode reward: total was 1.010000. running mean: -3.003867\n",
      "ep 2939: ep_len:161 episode reward: total was 24.630000. running mean: -2.727528\n",
      "ep 2939: ep_len:574 episode reward: total was 20.950000. running mean: -2.490753\n",
      "epsilon:0.069660 episode_count: 20580. steps_count: 8897960.000000\n",
      "Time elapsed:  26427.256179332733\n",
      "ep 2940: ep_len:248 episode reward: total was 0.130000. running mean: -2.464545\n",
      "ep 2940: ep_len:667 episode reward: total was 31.600000. running mean: -2.123900\n",
      "ep 2940: ep_len:543 episode reward: total was -56.040000. running mean: -2.663061\n",
      "ep 2940: ep_len:576 episode reward: total was 52.600000. running mean: -2.110430\n",
      "ep 2940: ep_len:109 episode reward: total was 26.740000. running mean: -1.821926\n",
      "ep 2940: ep_len:647 episode reward: total was -0.730000. running mean: -1.811007\n",
      "ep 2940: ep_len:615 episode reward: total was -0.270000. running mean: -1.795597\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.069616 episode_count: 20587. steps_count: 8901365.000000\n",
      "Time elapsed:  26441.113919973373\n",
      "ep 2941: ep_len:637 episode reward: total was -45.320000. running mean: -2.230841\n",
      "ep 2941: ep_len:500 episode reward: total was -30.840000. running mean: -2.516932\n",
      "ep 2941: ep_len:500 episode reward: total was 13.420000. running mean: -2.357563\n",
      "ep 2941: ep_len:525 episode reward: total was 43.730000. running mean: -1.896687\n",
      "ep 2941: ep_len:2 episode reward: total was -0.500000. running mean: -1.882721\n",
      "ep 2941: ep_len:552 episode reward: total was 6.920000. running mean: -1.794693\n",
      "ep 2941: ep_len:561 episode reward: total was -14.720000. running mean: -1.923946\n",
      "epsilon:0.069571 episode_count: 20594. steps_count: 8904642.000000\n",
      "Time elapsed:  26449.797928333282\n",
      "ep 2942: ep_len:251 episode reward: total was 13.870000. running mean: -1.766007\n",
      "ep 2942: ep_len:613 episode reward: total was 15.650000. running mean: -1.591847\n",
      "ep 2942: ep_len:375 episode reward: total was 33.990000. running mean: -1.236028\n",
      "ep 2942: ep_len:547 episode reward: total was 55.090000. running mean: -0.672768\n",
      "ep 2942: ep_len:1 episode reward: total was -1.000000. running mean: -0.676040\n",
      "ep 2942: ep_len:500 episode reward: total was -15.180000. running mean: -0.821080\n",
      "ep 2942: ep_len:640 episode reward: total was -105.480000. running mean: -1.867669\n",
      "epsilon:0.069527 episode_count: 20601. steps_count: 8907569.000000\n",
      "Time elapsed:  26461.934194087982\n",
      "ep 2943: ep_len:219 episode reward: total was -7.300000. running mean: -1.921993\n",
      "ep 2943: ep_len:535 episode reward: total was 38.930000. running mean: -1.513473\n",
      "ep 2943: ep_len:408 episode reward: total was 51.510000. running mean: -0.983238\n",
      "ep 2943: ep_len:592 episode reward: total was 18.210000. running mean: -0.791306\n",
      "ep 2943: ep_len:3 episode reward: total was 1.010000. running mean: -0.773292\n",
      "ep 2943: ep_len:610 episode reward: total was 18.890000. running mean: -0.576660\n",
      "ep 2943: ep_len:207 episode reward: total was 5.600000. running mean: -0.514893\n",
      "epsilon:0.069483 episode_count: 20608. steps_count: 8910143.000000\n",
      "Time elapsed:  26466.710610866547\n",
      "ep 2944: ep_len:622 episode reward: total was -37.720000. running mean: -0.886944\n",
      "ep 2944: ep_len:503 episode reward: total was 19.610000. running mean: -0.681975\n",
      "ep 2944: ep_len:557 episode reward: total was 30.690000. running mean: -0.368255\n",
      "ep 2944: ep_len:512 episode reward: total was 19.310000. running mean: -0.171472\n",
      "ep 2944: ep_len:101 episode reward: total was 20.720000. running mean: 0.037442\n",
      "ep 2944: ep_len:500 episode reward: total was 42.360000. running mean: 0.460668\n",
      "ep 2944: ep_len:517 episode reward: total was -193.090000. running mean: -1.474839\n",
      "epsilon:0.069438 episode_count: 20615. steps_count: 8913455.000000\n",
      "Time elapsed:  26473.90522623062\n",
      "ep 2945: ep_len:629 episode reward: total was 43.200000. running mean: -1.028090\n",
      "ep 2945: ep_len:500 episode reward: total was 78.970000. running mean: -0.228109\n",
      "ep 2945: ep_len:656 episode reward: total was -13.900000. running mean: -0.364828\n",
      "ep 2945: ep_len:500 episode reward: total was 55.310000. running mean: 0.191920\n",
      "ep 2945: ep_len:3 episode reward: total was 1.010000. running mean: 0.200101\n",
      "ep 2945: ep_len:510 episode reward: total was -13.240000. running mean: 0.065700\n",
      "ep 2945: ep_len:173 episode reward: total was 16.830000. running mean: 0.233343\n",
      "epsilon:0.069394 episode_count: 20622. steps_count: 8916426.000000\n",
      "Time elapsed:  26481.965571641922\n",
      "ep 2946: ep_len:513 episode reward: total was -39.110000. running mean: -0.160091\n",
      "ep 2946: ep_len:337 episode reward: total was -3.380000. running mean: -0.192290\n",
      "ep 2946: ep_len:500 episode reward: total was 12.640000. running mean: -0.063967\n",
      "ep 2946: ep_len:586 episode reward: total was 29.230000. running mean: 0.228973\n",
      "ep 2946: ep_len:3 episode reward: total was 1.010000. running mean: 0.236783\n",
      "ep 2946: ep_len:600 episode reward: total was 44.270000. running mean: 0.677115\n",
      "ep 2946: ep_len:589 episode reward: total was -116.800000. running mean: -0.497656\n",
      "epsilon:0.069350 episode_count: 20629. steps_count: 8919554.000000\n",
      "Time elapsed:  26490.248785734177\n",
      "ep 2947: ep_len:500 episode reward: total was 47.330000. running mean: -0.019379\n",
      "ep 2947: ep_len:508 episode reward: total was -115.690000. running mean: -1.176086\n",
      "ep 2947: ep_len:611 episode reward: total was -57.140000. running mean: -1.735725\n",
      "ep 2947: ep_len:571 episode reward: total was 77.530000. running mean: -0.943067\n",
      "ep 2947: ep_len:3 episode reward: total was 1.010000. running mean: -0.923537\n",
      "ep 2947: ep_len:544 episode reward: total was 17.550000. running mean: -0.738801\n",
      "ep 2947: ep_len:561 episode reward: total was 6.120000. running mean: -0.670213\n",
      "epsilon:0.069305 episode_count: 20636. steps_count: 8922852.000000\n",
      "Time elapsed:  26498.864755630493\n",
      "ep 2948: ep_len:500 episode reward: total was 52.820000. running mean: -0.135311\n",
      "ep 2948: ep_len:500 episode reward: total was 8.920000. running mean: -0.044758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2948: ep_len:580 episode reward: total was 13.090000. running mean: 0.086589\n",
      "ep 2948: ep_len:355 episode reward: total was -27.120000. running mean: -0.185476\n",
      "ep 2948: ep_len:3 episode reward: total was 1.010000. running mean: -0.173522\n",
      "ep 2948: ep_len:527 episode reward: total was 23.190000. running mean: 0.060114\n",
      "ep 2948: ep_len:565 episode reward: total was 23.580000. running mean: 0.295312\n",
      "epsilon:0.069261 episode_count: 20643. steps_count: 8925882.000000\n",
      "Time elapsed:  26509.897861480713\n",
      "ep 2949: ep_len:596 episode reward: total was 59.000000. running mean: 0.882359\n",
      "ep 2949: ep_len:565 episode reward: total was 92.700000. running mean: 1.800536\n",
      "ep 2949: ep_len:545 episode reward: total was -3.160000. running mean: 1.750930\n",
      "ep 2949: ep_len:509 episode reward: total was -1.290000. running mean: 1.720521\n",
      "ep 2949: ep_len:3 episode reward: total was -0.490000. running mean: 1.698416\n",
      "ep 2949: ep_len:500 episode reward: total was 16.790000. running mean: 1.849332\n",
      "ep 2949: ep_len:535 episode reward: total was -117.120000. running mean: 0.659638\n",
      "epsilon:0.069217 episode_count: 20650. steps_count: 8929135.000000\n",
      "Time elapsed:  26522.81676673889\n",
      "ep 2950: ep_len:544 episode reward: total was 46.510000. running mean: 1.118142\n",
      "ep 2950: ep_len:544 episode reward: total was -13.790000. running mean: 0.969061\n",
      "ep 2950: ep_len:447 episode reward: total was 26.590000. running mean: 1.225270\n",
      "ep 2950: ep_len:550 episode reward: total was 17.140000. running mean: 1.384417\n",
      "ep 2950: ep_len:3 episode reward: total was 1.010000. running mean: 1.380673\n",
      "ep 2950: ep_len:613 episode reward: total was -49.230000. running mean: 0.874566\n",
      "ep 2950: ep_len:592 episode reward: total was -4.990000. running mean: 0.815921\n",
      "epsilon:0.069172 episode_count: 20657. steps_count: 8932428.000000\n",
      "Time elapsed:  26531.915748357773\n",
      "ep 2951: ep_len:541 episode reward: total was -96.990000. running mean: -0.162139\n",
      "ep 2951: ep_len:528 episode reward: total was -11.600000. running mean: -0.276517\n",
      "ep 2951: ep_len:521 episode reward: total was -23.730000. running mean: -0.511052\n",
      "ep 2951: ep_len:616 episode reward: total was 25.830000. running mean: -0.247641\n",
      "ep 2951: ep_len:3 episode reward: total was -0.490000. running mean: -0.250065\n",
      "ep 2951: ep_len:632 episode reward: total was -25.520000. running mean: -0.502764\n",
      "ep 2951: ep_len:603 episode reward: total was -7.020000. running mean: -0.567937\n",
      "epsilon:0.069128 episode_count: 20664. steps_count: 8935872.000000\n",
      "Time elapsed:  26540.86508655548\n",
      "ep 2952: ep_len:519 episode reward: total was 68.650000. running mean: 0.124243\n",
      "ep 2952: ep_len:622 episode reward: total was -184.040000. running mean: -1.717400\n",
      "ep 2952: ep_len:500 episode reward: total was -20.070000. running mean: -1.900926\n",
      "ep 2952: ep_len:542 episode reward: total was 54.410000. running mean: -1.337817\n",
      "ep 2952: ep_len:3 episode reward: total was -0.490000. running mean: -1.329338\n",
      "ep 2952: ep_len:500 episode reward: total was 34.040000. running mean: -0.975645\n",
      "ep 2952: ep_len:540 episode reward: total was 22.670000. running mean: -0.739189\n",
      "epsilon:0.069084 episode_count: 20671. steps_count: 8939098.000000\n",
      "Time elapsed:  26553.26591038704\n",
      "ep 2953: ep_len:134 episode reward: total was 4.490000. running mean: -0.686897\n",
      "ep 2953: ep_len:512 episode reward: total was -39.560000. running mean: -1.075628\n",
      "ep 2953: ep_len:568 episode reward: total was -6.100000. running mean: -1.125871\n",
      "ep 2953: ep_len:633 episode reward: total was 38.370000. running mean: -0.730913\n",
      "ep 2953: ep_len:98 episode reward: total was 37.680000. running mean: -0.346804\n",
      "ep 2953: ep_len:531 episode reward: total was -57.660000. running mean: -0.919936\n",
      "ep 2953: ep_len:589 episode reward: total was 19.570000. running mean: -0.715036\n",
      "epsilon:0.069039 episode_count: 20678. steps_count: 8942163.000000\n",
      "Time elapsed:  26564.788217306137\n",
      "ep 2954: ep_len:503 episode reward: total was -55.200000. running mean: -1.259886\n",
      "ep 2954: ep_len:518 episode reward: total was -22.410000. running mean: -1.471387\n",
      "ep 2954: ep_len:555 episode reward: total was -9.700000. running mean: -1.553673\n",
      "ep 2954: ep_len:55 episode reward: total was -7.160000. running mean: -1.609736\n",
      "ep 2954: ep_len:3 episode reward: total was 1.010000. running mean: -1.583539\n",
      "ep 2954: ep_len:305 episode reward: total was 19.160000. running mean: -1.376104\n",
      "ep 2954: ep_len:593 episode reward: total was -5.680000. running mean: -1.419143\n",
      "epsilon:0.068995 episode_count: 20685. steps_count: 8944695.000000\n",
      "Time elapsed:  26576.215826511383\n",
      "ep 2955: ep_len:536 episode reward: total was -44.420000. running mean: -1.849151\n",
      "ep 2955: ep_len:597 episode reward: total was 4.470000. running mean: -1.785960\n",
      "ep 2955: ep_len:557 episode reward: total was -42.800000. running mean: -2.196100\n",
      "ep 2955: ep_len:566 episode reward: total was 29.790000. running mean: -1.876239\n",
      "ep 2955: ep_len:89 episode reward: total was 23.200000. running mean: -1.625477\n",
      "ep 2955: ep_len:500 episode reward: total was -13.980000. running mean: -1.749022\n",
      "ep 2955: ep_len:598 episode reward: total was 20.680000. running mean: -1.524732\n",
      "epsilon:0.068951 episode_count: 20692. steps_count: 8948138.000000\n",
      "Time elapsed:  26590.445781469345\n",
      "ep 2956: ep_len:554 episode reward: total was 49.000000. running mean: -1.019484\n",
      "ep 2956: ep_len:559 episode reward: total was 53.470000. running mean: -0.474590\n",
      "ep 2956: ep_len:528 episode reward: total was -126.830000. running mean: -1.738144\n",
      "ep 2956: ep_len:572 episode reward: total was -7.310000. running mean: -1.793862\n",
      "ep 2956: ep_len:3 episode reward: total was 1.010000. running mean: -1.765824\n",
      "ep 2956: ep_len:501 episode reward: total was 9.590000. running mean: -1.652265\n",
      "ep 2956: ep_len:211 episode reward: total was 5.580000. running mean: -1.579943\n",
      "epsilon:0.068906 episode_count: 20699. steps_count: 8951066.000000\n",
      "Time elapsed:  26598.26957798004\n",
      "ep 2957: ep_len:554 episode reward: total was 23.500000. running mean: -1.329143\n",
      "ep 2957: ep_len:642 episode reward: total was -4.410000. running mean: -1.359952\n",
      "ep 2957: ep_len:549 episode reward: total was -47.720000. running mean: -1.823552\n",
      "ep 2957: ep_len:500 episode reward: total was 34.190000. running mean: -1.463417\n",
      "ep 2957: ep_len:91 episode reward: total was 17.710000. running mean: -1.271683\n",
      "ep 2957: ep_len:570 episode reward: total was -7.740000. running mean: -1.336366\n",
      "ep 2957: ep_len:538 episode reward: total was -2.120000. running mean: -1.344202\n",
      "epsilon:0.068862 episode_count: 20706. steps_count: 8954510.000000\n",
      "Time elapsed:  26606.8625164032\n",
      "ep 2958: ep_len:566 episode reward: total was 6.820000. running mean: -1.262560\n",
      "ep 2958: ep_len:500 episode reward: total was 34.770000. running mean: -0.902234\n",
      "ep 2958: ep_len:563 episode reward: total was -34.350000. running mean: -1.236712\n",
      "ep 2958: ep_len:500 episode reward: total was -24.310000. running mean: -1.467445\n",
      "ep 2958: ep_len:3 episode reward: total was -0.490000. running mean: -1.457671\n",
      "ep 2958: ep_len:541 episode reward: total was -33.150000. running mean: -1.774594\n",
      "ep 2958: ep_len:204 episode reward: total was -6.460000. running mean: -1.821448\n",
      "epsilon:0.068818 episode_count: 20713. steps_count: 8957387.000000\n",
      "Time elapsed:  26614.6693046093\n",
      "ep 2959: ep_len:566 episode reward: total was 45.450000. running mean: -1.348733\n",
      "ep 2959: ep_len:506 episode reward: total was -21.200000. running mean: -1.547246\n",
      "ep 2959: ep_len:588 episode reward: total was -9.110000. running mean: -1.622874\n",
      "ep 2959: ep_len:500 episode reward: total was -6.080000. running mean: -1.667445\n",
      "ep 2959: ep_len:3 episode reward: total was 1.010000. running mean: -1.640670\n",
      "ep 2959: ep_len:563 episode reward: total was -22.490000. running mean: -1.849164\n",
      "ep 2959: ep_len:613 episode reward: total was 5.350000. running mean: -1.777172\n",
      "epsilon:0.068773 episode_count: 20720. steps_count: 8960726.000000\n",
      "Time elapsed:  26628.74317407608\n",
      "ep 2960: ep_len:575 episode reward: total was 25.830000. running mean: -1.501100\n",
      "ep 2960: ep_len:523 episode reward: total was -11.210000. running mean: -1.598189\n",
      "ep 2960: ep_len:463 episode reward: total was 27.270000. running mean: -1.309508\n",
      "ep 2960: ep_len:536 episode reward: total was 20.790000. running mean: -1.088512\n",
      "ep 2960: ep_len:3 episode reward: total was 1.010000. running mean: -1.067527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2960: ep_len:538 episode reward: total was -3.090000. running mean: -1.087752\n",
      "ep 2960: ep_len:573 episode reward: total was -74.040000. running mean: -1.817275\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.068729 episode_count: 20727. steps_count: 8963937.000000\n",
      "Time elapsed:  26645.942946910858\n",
      "ep 2961: ep_len:665 episode reward: total was -59.260000. running mean: -2.391702\n",
      "ep 2961: ep_len:541 episode reward: total was 20.310000. running mean: -2.164685\n",
      "ep 2961: ep_len:556 episode reward: total was 40.910000. running mean: -1.733938\n",
      "ep 2961: ep_len:500 episode reward: total was 46.240000. running mean: -1.254199\n",
      "ep 2961: ep_len:3 episode reward: total was 1.010000. running mean: -1.231557\n",
      "ep 2961: ep_len:556 episode reward: total was -26.390000. running mean: -1.483141\n",
      "ep 2961: ep_len:594 episode reward: total was 19.860000. running mean: -1.269710\n",
      "epsilon:0.068685 episode_count: 20734. steps_count: 8967352.000000\n",
      "Time elapsed:  26659.761248350143\n",
      "ep 2962: ep_len:610 episode reward: total was -55.380000. running mean: -1.810812\n",
      "ep 2962: ep_len:500 episode reward: total was 64.130000. running mean: -1.151404\n",
      "ep 2962: ep_len:566 episode reward: total was -51.660000. running mean: -1.656490\n",
      "ep 2962: ep_len:396 episode reward: total was 7.380000. running mean: -1.566125\n",
      "ep 2962: ep_len:3 episode reward: total was 1.010000. running mean: -1.540364\n",
      "ep 2962: ep_len:556 episode reward: total was -14.190000. running mean: -1.666861\n",
      "ep 2962: ep_len:601 episode reward: total was -5.740000. running mean: -1.707592\n",
      "epsilon:0.068640 episode_count: 20741. steps_count: 8970584.000000\n",
      "Time elapsed:  26668.323729515076\n",
      "ep 2963: ep_len:229 episode reward: total was -109.180000. running mean: -2.782316\n",
      "ep 2963: ep_len:569 episode reward: total was 4.180000. running mean: -2.712693\n",
      "ep 2963: ep_len:74 episode reward: total was 9.270000. running mean: -2.592866\n",
      "ep 2963: ep_len:501 episode reward: total was -16.470000. running mean: -2.731637\n",
      "ep 2963: ep_len:52 episode reward: total was -50.990000. running mean: -3.214221\n",
      "ep 2963: ep_len:612 episode reward: total was -41.050000. running mean: -3.592579\n",
      "ep 2963: ep_len:339 episode reward: total was -65.650000. running mean: -4.213153\n",
      "epsilon:0.068596 episode_count: 20748. steps_count: 8972960.000000\n",
      "Time elapsed:  26674.878052711487\n",
      "ep 2964: ep_len:601 episode reward: total was 39.110000. running mean: -3.779921\n",
      "ep 2964: ep_len:500 episode reward: total was 30.630000. running mean: -3.435822\n",
      "ep 2964: ep_len:560 episode reward: total was -4.850000. running mean: -3.449964\n",
      "ep 2964: ep_len:602 episode reward: total was 20.710000. running mean: -3.208364\n",
      "ep 2964: ep_len:3 episode reward: total was 1.010000. running mean: -3.166181\n",
      "ep 2964: ep_len:588 episode reward: total was 2.780000. running mean: -3.106719\n",
      "ep 2964: ep_len:502 episode reward: total was -18.150000. running mean: -3.257152\n",
      "epsilon:0.068552 episode_count: 20755. steps_count: 8976316.000000\n",
      "Time elapsed:  26688.325566768646\n",
      "ep 2965: ep_len:572 episode reward: total was 13.890000. running mean: -3.085680\n",
      "ep 2965: ep_len:500 episode reward: total was -23.850000. running mean: -3.293323\n",
      "ep 2965: ep_len:65 episode reward: total was -0.740000. running mean: -3.267790\n",
      "ep 2965: ep_len:501 episode reward: total was 21.560000. running mean: -3.019512\n",
      "ep 2965: ep_len:3 episode reward: total was 1.010000. running mean: -2.979217\n",
      "ep 2965: ep_len:522 episode reward: total was -9.260000. running mean: -3.042025\n",
      "ep 2965: ep_len:625 episode reward: total was 22.880000. running mean: -2.782805\n",
      "epsilon:0.068507 episode_count: 20762. steps_count: 8979104.000000\n",
      "Time elapsed:  26695.968338489532\n",
      "ep 2966: ep_len:116 episode reward: total was 5.010000. running mean: -2.704877\n",
      "ep 2966: ep_len:601 episode reward: total was -54.180000. running mean: -3.219628\n",
      "ep 2966: ep_len:79 episode reward: total was 8.800000. running mean: -3.099432\n",
      "ep 2966: ep_len:527 episode reward: total was 38.110000. running mean: -2.687337\n",
      "ep 2966: ep_len:93 episode reward: total was 29.790000. running mean: -2.362564\n",
      "ep 2966: ep_len:528 episode reward: total was -51.780000. running mean: -2.856738\n",
      "ep 2966: ep_len:602 episode reward: total was -35.640000. running mean: -3.184571\n",
      "epsilon:0.068463 episode_count: 20769. steps_count: 8981650.000000\n",
      "Time elapsed:  26702.998729228973\n",
      "ep 2967: ep_len:134 episode reward: total was 4.980000. running mean: -3.102925\n",
      "ep 2967: ep_len:524 episode reward: total was 56.430000. running mean: -2.507596\n",
      "ep 2967: ep_len:656 episode reward: total was -35.020000. running mean: -2.832720\n",
      "ep 2967: ep_len:585 episode reward: total was 71.360000. running mean: -2.090793\n",
      "ep 2967: ep_len:3 episode reward: total was 1.010000. running mean: -2.059785\n",
      "ep 2967: ep_len:500 episode reward: total was 7.240000. running mean: -1.966787\n",
      "ep 2967: ep_len:344 episode reward: total was 4.990000. running mean: -1.897219\n",
      "epsilon:0.068419 episode_count: 20776. steps_count: 8984396.000000\n",
      "Time elapsed:  26713.28457379341\n",
      "ep 2968: ep_len:197 episode reward: total was 4.280000. running mean: -1.835447\n",
      "ep 2968: ep_len:338 episode reward: total was -139.800000. running mean: -3.215092\n",
      "ep 2968: ep_len:79 episode reward: total was 6.290000. running mean: -3.120041\n",
      "ep 2968: ep_len:500 episode reward: total was 17.680000. running mean: -2.912041\n",
      "ep 2968: ep_len:122 episode reward: total was 11.870000. running mean: -2.764221\n",
      "ep 2968: ep_len:261 episode reward: total was 33.520000. running mean: -2.401378\n",
      "ep 2968: ep_len:185 episode reward: total was -27.890000. running mean: -2.656265\n",
      "epsilon:0.068374 episode_count: 20783. steps_count: 8986078.000000\n",
      "Time elapsed:  26718.176094293594\n",
      "ep 2969: ep_len:528 episode reward: total was 25.920000. running mean: -2.370502\n",
      "ep 2969: ep_len:522 episode reward: total was 28.300000. running mean: -2.063797\n",
      "ep 2969: ep_len:548 episode reward: total was 0.180000. running mean: -2.041359\n",
      "ep 2969: ep_len:520 episode reward: total was 10.680000. running mean: -1.914145\n",
      "ep 2969: ep_len:109 episode reward: total was 8.350000. running mean: -1.811504\n",
      "ep 2969: ep_len:523 episode reward: total was -43.050000. running mean: -2.223889\n",
      "ep 2969: ep_len:512 episode reward: total was 8.280000. running mean: -2.118850\n",
      "epsilon:0.068330 episode_count: 20790. steps_count: 8989340.000000\n",
      "Time elapsed:  26724.112508773804\n",
      "ep 2970: ep_len:506 episode reward: total was 40.570000. running mean: -1.691962\n",
      "ep 2970: ep_len:358 episode reward: total was 16.030000. running mean: -1.514742\n",
      "ep 2970: ep_len:501 episode reward: total was -29.870000. running mean: -1.798295\n",
      "ep 2970: ep_len:612 episode reward: total was 32.920000. running mean: -1.451112\n",
      "ep 2970: ep_len:3 episode reward: total was 1.010000. running mean: -1.426500\n",
      "ep 2970: ep_len:514 episode reward: total was 17.480000. running mean: -1.237435\n",
      "ep 2970: ep_len:501 episode reward: total was -41.870000. running mean: -1.643761\n",
      "epsilon:0.068286 episode_count: 20797. steps_count: 8992335.000000\n",
      "Time elapsed:  26732.2117831707\n",
      "ep 2971: ep_len:207 episode reward: total was -1.890000. running mean: -1.646223\n",
      "ep 2971: ep_len:513 episode reward: total was -32.870000. running mean: -1.958461\n",
      "ep 2971: ep_len:504 episode reward: total was -4.150000. running mean: -1.980377\n",
      "ep 2971: ep_len:146 episode reward: total was 14.030000. running mean: -1.820273\n",
      "ep 2971: ep_len:2 episode reward: total was -0.500000. running mean: -1.807070\n",
      "ep 2971: ep_len:186 episode reward: total was 41.260000. running mean: -1.376399\n",
      "ep 2971: ep_len:565 episode reward: total was 19.720000. running mean: -1.165435\n",
      "epsilon:0.068241 episode_count: 20804. steps_count: 8994458.000000\n",
      "Time elapsed:  26745.199634552002\n",
      "ep 2972: ep_len:576 episode reward: total was 17.900000. running mean: -0.974781\n",
      "ep 2972: ep_len:565 episode reward: total was -85.040000. running mean: -1.815433\n",
      "ep 2972: ep_len:594 episode reward: total was -31.280000. running mean: -2.110079\n",
      "ep 2972: ep_len:525 episode reward: total was 50.770000. running mean: -1.581278\n",
      "ep 2972: ep_len:3 episode reward: total was 1.010000. running mean: -1.555365\n",
      "ep 2972: ep_len:589 episode reward: total was -6.600000. running mean: -1.605812\n",
      "ep 2972: ep_len:571 episode reward: total was -31.900000. running mean: -1.908754\n",
      "epsilon:0.068197 episode_count: 20811. steps_count: 8997881.000000\n",
      "Time elapsed:  26754.136713266373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2973: ep_len:596 episode reward: total was -65.990000. running mean: -2.549566\n",
      "ep 2973: ep_len:165 episode reward: total was 0.860000. running mean: -2.515470\n",
      "ep 2973: ep_len:642 episode reward: total was 0.680000. running mean: -2.483516\n",
      "ep 2973: ep_len:508 episode reward: total was 4.990000. running mean: -2.408781\n",
      "ep 2973: ep_len:3 episode reward: total was 1.010000. running mean: -2.374593\n",
      "ep 2973: ep_len:545 episode reward: total was -9.130000. running mean: -2.442147\n",
      "ep 2973: ep_len:538 episode reward: total was -36.430000. running mean: -2.782025\n",
      "epsilon:0.068153 episode_count: 20818. steps_count: 9000878.000000\n",
      "Time elapsed:  26763.246203660965\n",
      "ep 2974: ep_len:596 episode reward: total was 47.320000. running mean: -2.281005\n",
      "ep 2974: ep_len:529 episode reward: total was 5.280000. running mean: -2.205395\n",
      "ep 2974: ep_len:500 episode reward: total was -123.730000. running mean: -3.420641\n",
      "ep 2974: ep_len:520 episode reward: total was 44.510000. running mean: -2.941335\n",
      "ep 2974: ep_len:3 episode reward: total was -0.490000. running mean: -2.916821\n",
      "ep 2974: ep_len:529 episode reward: total was -6.750000. running mean: -2.955153\n",
      "ep 2974: ep_len:356 episode reward: total was 11.140000. running mean: -2.814202\n",
      "epsilon:0.068108 episode_count: 20825. steps_count: 9003911.000000\n",
      "Time elapsed:  26771.288141965866\n",
      "ep 2975: ep_len:642 episode reward: total was -60.530000. running mean: -3.391360\n",
      "ep 2975: ep_len:573 episode reward: total was 32.530000. running mean: -3.032146\n",
      "ep 2975: ep_len:74 episode reward: total was -0.740000. running mean: -3.009225\n",
      "ep 2975: ep_len:145 episode reward: total was -16.820000. running mean: -3.147332\n",
      "ep 2975: ep_len:3 episode reward: total was 1.010000. running mean: -3.105759\n",
      "ep 2975: ep_len:700 episode reward: total was 17.980000. running mean: -2.894901\n",
      "ep 2975: ep_len:614 episode reward: total was 15.270000. running mean: -2.713252\n",
      "epsilon:0.068064 episode_count: 20832. steps_count: 9006662.000000\n",
      "Time elapsed:  26778.849035024643\n",
      "ep 2976: ep_len:530 episode reward: total was 35.820000. running mean: -2.327920\n",
      "ep 2976: ep_len:598 episode reward: total was 25.400000. running mean: -2.050641\n",
      "ep 2976: ep_len:317 episode reward: total was 25.110000. running mean: -1.779034\n",
      "ep 2976: ep_len:502 episode reward: total was 31.410000. running mean: -1.447144\n",
      "ep 2976: ep_len:3 episode reward: total was 1.010000. running mean: -1.422572\n",
      "ep 2976: ep_len:632 episode reward: total was -115.970000. running mean: -2.568047\n",
      "ep 2976: ep_len:518 episode reward: total was 23.980000. running mean: -2.302566\n",
      "epsilon:0.068020 episode_count: 20839. steps_count: 9009762.000000\n",
      "Time elapsed:  26787.287984132767\n",
      "ep 2977: ep_len:220 episode reward: total was 19.820000. running mean: -2.081341\n",
      "ep 2977: ep_len:612 episode reward: total was 31.680000. running mean: -1.743727\n",
      "ep 2977: ep_len:538 episode reward: total was 12.140000. running mean: -1.604890\n",
      "ep 2977: ep_len:524 episode reward: total was 19.860000. running mean: -1.390241\n",
      "ep 2977: ep_len:88 episode reward: total was 21.750000. running mean: -1.158839\n",
      "ep 2977: ep_len:521 episode reward: total was -30.340000. running mean: -1.450650\n",
      "ep 2977: ep_len:513 episode reward: total was -68.130000. running mean: -2.117444\n",
      "epsilon:0.067975 episode_count: 20846. steps_count: 9012778.000000\n",
      "Time elapsed:  26795.41254734993\n",
      "ep 2978: ep_len:553 episode reward: total was 29.310000. running mean: -1.803169\n",
      "ep 2978: ep_len:574 episode reward: total was 4.560000. running mean: -1.739538\n",
      "ep 2978: ep_len:388 episode reward: total was 35.960000. running mean: -1.362542\n",
      "ep 2978: ep_len:511 episode reward: total was -77.190000. running mean: -2.120817\n",
      "ep 2978: ep_len:3 episode reward: total was 1.010000. running mean: -2.089509\n",
      "ep 2978: ep_len:286 episode reward: total was 15.080000. running mean: -1.917814\n",
      "ep 2978: ep_len:164 episode reward: total was 2.350000. running mean: -1.875135\n",
      "epsilon:0.067931 episode_count: 20853. steps_count: 9015257.000000\n",
      "Time elapsed:  26807.049422502518\n",
      "ep 2979: ep_len:501 episode reward: total was -26.330000. running mean: -2.119684\n",
      "ep 2979: ep_len:500 episode reward: total was 42.080000. running mean: -1.677687\n",
      "ep 2979: ep_len:79 episode reward: total was 2.770000. running mean: -1.633210\n",
      "ep 2979: ep_len:124 episode reward: total was 10.550000. running mean: -1.511378\n",
      "ep 2979: ep_len:3 episode reward: total was 1.010000. running mean: -1.486164\n",
      "ep 2979: ep_len:500 episode reward: total was 8.960000. running mean: -1.381703\n",
      "ep 2979: ep_len:500 episode reward: total was 16.900000. running mean: -1.198886\n",
      "epsilon:0.067887 episode_count: 20860. steps_count: 9017464.000000\n",
      "Time elapsed:  26817.58858513832\n",
      "ep 2980: ep_len:195 episode reward: total was -1.480000. running mean: -1.201697\n",
      "ep 2980: ep_len:592 episode reward: total was -67.230000. running mean: -1.861980\n",
      "ep 2980: ep_len:500 episode reward: total was 19.860000. running mean: -1.644760\n",
      "ep 2980: ep_len:509 episode reward: total was 23.890000. running mean: -1.389413\n",
      "ep 2980: ep_len:3 episode reward: total was -1.500000. running mean: -1.390518\n",
      "ep 2980: ep_len:587 episode reward: total was 4.640000. running mean: -1.330213\n",
      "ep 2980: ep_len:587 episode reward: total was 24.150000. running mean: -1.075411\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.067842 episode_count: 20867. steps_count: 9020437.000000\n",
      "Time elapsed:  26831.89541363716\n",
      "ep 2981: ep_len:575 episode reward: total was 63.270000. running mean: -0.431957\n",
      "ep 2981: ep_len:518 episode reward: total was 22.930000. running mean: -0.198337\n",
      "ep 2981: ep_len:500 episode reward: total was -15.140000. running mean: -0.347754\n",
      "ep 2981: ep_len:500 episode reward: total was -24.550000. running mean: -0.589777\n",
      "ep 2981: ep_len:97 episode reward: total was 20.710000. running mean: -0.376779\n",
      "ep 2981: ep_len:570 episode reward: total was -7.990000. running mean: -0.452911\n",
      "ep 2981: ep_len:512 episode reward: total was -22.190000. running mean: -0.670282\n",
      "epsilon:0.067798 episode_count: 20874. steps_count: 9023709.000000\n",
      "Time elapsed:  26840.592002868652\n",
      "ep 2982: ep_len:124 episode reward: total was 13.450000. running mean: -0.529079\n",
      "ep 2982: ep_len:575 episode reward: total was 1.260000. running mean: -0.511188\n",
      "ep 2982: ep_len:500 episode reward: total was -71.150000. running mean: -1.217576\n",
      "ep 2982: ep_len:506 episode reward: total was 25.610000. running mean: -0.949301\n",
      "ep 2982: ep_len:107 episode reward: total was -18.250000. running mean: -1.122308\n",
      "ep 2982: ep_len:597 episode reward: total was 13.310000. running mean: -0.977985\n",
      "ep 2982: ep_len:532 episode reward: total was 28.420000. running mean: -0.684005\n",
      "epsilon:0.067754 episode_count: 20881. steps_count: 9026650.000000\n",
      "Time elapsed:  26848.90134716034\n",
      "ep 2983: ep_len:557 episode reward: total was -215.520000. running mean: -2.832365\n",
      "ep 2983: ep_len:635 episode reward: total was 69.790000. running mean: -2.106141\n",
      "ep 2983: ep_len:70 episode reward: total was 2.190000. running mean: -2.063180\n",
      "ep 2983: ep_len:500 episode reward: total was 13.070000. running mean: -1.911848\n",
      "ep 2983: ep_len:2 episode reward: total was -0.500000. running mean: -1.897729\n",
      "ep 2983: ep_len:661 episode reward: total was -18.430000. running mean: -2.063052\n",
      "ep 2983: ep_len:589 episode reward: total was 36.180000. running mean: -1.680621\n",
      "epsilon:0.067709 episode_count: 20888. steps_count: 9029664.000000\n",
      "Time elapsed:  26857.034990787506\n",
      "ep 2984: ep_len:570 episode reward: total was 3.400000. running mean: -1.629815\n",
      "ep 2984: ep_len:621 episode reward: total was 22.500000. running mean: -1.388517\n",
      "ep 2984: ep_len:679 episode reward: total was -47.120000. running mean: -1.845832\n",
      "ep 2984: ep_len:535 episode reward: total was 57.020000. running mean: -1.257174\n",
      "ep 2984: ep_len:3 episode reward: total was 1.010000. running mean: -1.234502\n",
      "ep 2984: ep_len:538 episode reward: total was -80.880000. running mean: -2.030957\n",
      "ep 2984: ep_len:597 episode reward: total was -56.760000. running mean: -2.578247\n",
      "epsilon:0.067665 episode_count: 20895. steps_count: 9033207.000000\n",
      "Time elapsed:  26864.5023522377\n",
      "ep 2985: ep_len:630 episode reward: total was -47.030000. running mean: -3.022765\n",
      "ep 2985: ep_len:500 episode reward: total was -90.030000. running mean: -3.892837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2985: ep_len:531 episode reward: total was -23.150000. running mean: -4.085409\n",
      "ep 2985: ep_len:56 episode reward: total was 5.320000. running mean: -3.991355\n",
      "ep 2985: ep_len:99 episode reward: total was 30.250000. running mean: -3.648941\n",
      "ep 2985: ep_len:514 episode reward: total was 6.060000. running mean: -3.551852\n",
      "ep 2985: ep_len:336 episode reward: total was -6.970000. running mean: -3.586033\n",
      "epsilon:0.067621 episode_count: 20902. steps_count: 9035873.000000\n",
      "Time elapsed:  26872.290032863617\n",
      "ep 2986: ep_len:598 episode reward: total was 60.860000. running mean: -2.941573\n",
      "ep 2986: ep_len:575 episode reward: total was -9.600000. running mean: -3.008157\n",
      "ep 2986: ep_len:534 episode reward: total was -19.460000. running mean: -3.172676\n",
      "ep 2986: ep_len:544 episode reward: total was 28.630000. running mean: -2.854649\n",
      "ep 2986: ep_len:3 episode reward: total was 1.010000. running mean: -2.816002\n",
      "ep 2986: ep_len:607 episode reward: total was -36.170000. running mean: -3.149542\n",
      "ep 2986: ep_len:597 episode reward: total was 33.370000. running mean: -2.784347\n",
      "epsilon:0.067576 episode_count: 20909. steps_count: 9039331.000000\n",
      "Time elapsed:  26881.33573102951\n",
      "ep 2987: ep_len:177 episode reward: total was 6.930000. running mean: -2.687203\n",
      "ep 2987: ep_len:342 episode reward: total was -5.680000. running mean: -2.717131\n",
      "ep 2987: ep_len:467 episode reward: total was 25.750000. running mean: -2.432460\n",
      "ep 2987: ep_len:593 episode reward: total was 42.360000. running mean: -1.984535\n",
      "ep 2987: ep_len:3 episode reward: total was 1.010000. running mean: -1.954590\n",
      "ep 2987: ep_len:537 episode reward: total was -10.610000. running mean: -2.041144\n",
      "ep 2987: ep_len:584 episode reward: total was 4.360000. running mean: -1.977133\n",
      "epsilon:0.067532 episode_count: 20916. steps_count: 9042034.000000\n",
      "Time elapsed:  26888.651884555817\n",
      "ep 2988: ep_len:512 episode reward: total was 48.550000. running mean: -1.471861\n",
      "ep 2988: ep_len:542 episode reward: total was 1.740000. running mean: -1.439743\n",
      "ep 2988: ep_len:79 episode reward: total was 5.800000. running mean: -1.367345\n",
      "ep 2988: ep_len:132 episode reward: total was 6.120000. running mean: -1.292472\n",
      "ep 2988: ep_len:3 episode reward: total was 1.010000. running mean: -1.269447\n",
      "ep 2988: ep_len:527 episode reward: total was -10.550000. running mean: -1.362253\n",
      "ep 2988: ep_len:546 episode reward: total was 18.600000. running mean: -1.162630\n",
      "epsilon:0.067488 episode_count: 20923. steps_count: 9044375.000000\n",
      "Time elapsed:  26895.07429075241\n",
      "ep 2989: ep_len:606 episode reward: total was 63.890000. running mean: -0.512104\n",
      "ep 2989: ep_len:595 episode reward: total was 37.220000. running mean: -0.134783\n",
      "ep 2989: ep_len:462 episode reward: total was 53.400000. running mean: 0.400565\n",
      "ep 2989: ep_len:129 episode reward: total was 8.090000. running mean: 0.477459\n",
      "ep 2989: ep_len:3 episode reward: total was 1.010000. running mean: 0.482785\n",
      "ep 2989: ep_len:500 episode reward: total was -2.610000. running mean: 0.451857\n",
      "ep 2989: ep_len:628 episode reward: total was -7.730000. running mean: 0.370038\n",
      "epsilon:0.067443 episode_count: 20930. steps_count: 9047298.000000\n",
      "Time elapsed:  26907.002172231674\n",
      "ep 2990: ep_len:601 episode reward: total was -264.250000. running mean: -2.276162\n",
      "ep 2990: ep_len:537 episode reward: total was -18.600000. running mean: -2.439400\n",
      "ep 2990: ep_len:500 episode reward: total was -5.820000. running mean: -2.473206\n",
      "ep 2990: ep_len:505 episode reward: total was -39.960000. running mean: -2.848074\n",
      "ep 2990: ep_len:3 episode reward: total was 1.010000. running mean: -2.809494\n",
      "ep 2990: ep_len:500 episode reward: total was 11.920000. running mean: -2.662199\n",
      "ep 2990: ep_len:650 episode reward: total was -76.180000. running mean: -3.397377\n",
      "epsilon:0.067399 episode_count: 20937. steps_count: 9050594.000000\n",
      "Time elapsed:  26915.794502019882\n",
      "ep 2991: ep_len:527 episode reward: total was 27.170000. running mean: -3.091703\n",
      "ep 2991: ep_len:500 episode reward: total was -116.140000. running mean: -4.222186\n",
      "ep 2991: ep_len:665 episode reward: total was -71.170000. running mean: -4.891664\n",
      "ep 2991: ep_len:36 episode reward: total was -0.820000. running mean: -4.850947\n",
      "ep 2991: ep_len:75 episode reward: total was 8.270000. running mean: -4.719738\n",
      "ep 2991: ep_len:275 episode reward: total was 0.340000. running mean: -4.669141\n",
      "ep 2991: ep_len:500 episode reward: total was 9.860000. running mean: -4.523849\n",
      "epsilon:0.067355 episode_count: 20944. steps_count: 9053172.000000\n",
      "Time elapsed:  26922.6309466362\n",
      "ep 2992: ep_len:610 episode reward: total was -38.880000. running mean: -4.867411\n",
      "ep 2992: ep_len:646 episode reward: total was 12.660000. running mean: -4.692137\n",
      "ep 2992: ep_len:545 episode reward: total was -68.610000. running mean: -5.331315\n",
      "ep 2992: ep_len:559 episode reward: total was 9.970000. running mean: -5.178302\n",
      "ep 2992: ep_len:3 episode reward: total was 1.010000. running mean: -5.116419\n",
      "ep 2992: ep_len:180 episode reward: total was 29.630000. running mean: -4.768955\n",
      "ep 2992: ep_len:589 episode reward: total was 14.020000. running mean: -4.581065\n",
      "epsilon:0.067310 episode_count: 20951. steps_count: 9056304.000000\n",
      "Time elapsed:  26930.627140045166\n",
      "ep 2993: ep_len:630 episode reward: total was 29.760000. running mean: -4.237655\n",
      "ep 2993: ep_len:500 episode reward: total was 54.060000. running mean: -3.654678\n",
      "ep 2993: ep_len:524 episode reward: total was -27.870000. running mean: -3.896831\n",
      "ep 2993: ep_len:524 episode reward: total was 11.630000. running mean: -3.741563\n",
      "ep 2993: ep_len:3 episode reward: total was 1.010000. running mean: -3.694047\n",
      "ep 2993: ep_len:500 episode reward: total was 38.060000. running mean: -3.276507\n",
      "ep 2993: ep_len:290 episode reward: total was -26.590000. running mean: -3.509642\n",
      "epsilon:0.067266 episode_count: 20958. steps_count: 9059275.000000\n",
      "Time elapsed:  26943.07315993309\n",
      "ep 2994: ep_len:620 episode reward: total was -40.070000. running mean: -3.875245\n",
      "ep 2994: ep_len:516 episode reward: total was -0.050000. running mean: -3.836993\n",
      "ep 2994: ep_len:581 episode reward: total was -1.170000. running mean: -3.810323\n",
      "ep 2994: ep_len:101 episode reward: total was 5.440000. running mean: -3.717820\n",
      "ep 2994: ep_len:3 episode reward: total was 1.010000. running mean: -3.670542\n",
      "ep 2994: ep_len:650 episode reward: total was 29.460000. running mean: -3.339236\n",
      "ep 2994: ep_len:604 episode reward: total was 9.490000. running mean: -3.210944\n",
      "epsilon:0.067222 episode_count: 20965. steps_count: 9062350.000000\n",
      "Time elapsed:  26949.29070854187\n",
      "ep 2995: ep_len:500 episode reward: total was 53.970000. running mean: -2.639134\n",
      "ep 2995: ep_len:331 episode reward: total was 3.850000. running mean: -2.574243\n",
      "ep 2995: ep_len:500 episode reward: total was 20.190000. running mean: -2.346601\n",
      "ep 2995: ep_len:530 episode reward: total was 31.430000. running mean: -2.008835\n",
      "ep 2995: ep_len:3 episode reward: total was 1.010000. running mean: -1.978646\n",
      "ep 2995: ep_len:516 episode reward: total was -3.810000. running mean: -1.996960\n",
      "ep 2995: ep_len:522 episode reward: total was -79.570000. running mean: -2.772690\n",
      "epsilon:0.067177 episode_count: 20972. steps_count: 9065252.000000\n",
      "Time elapsed:  26956.957760810852\n",
      "ep 2996: ep_len:588 episode reward: total was 5.550000. running mean: -2.689463\n",
      "ep 2996: ep_len:554 episode reward: total was 15.680000. running mean: -2.505769\n",
      "ep 2996: ep_len:589 episode reward: total was 0.170000. running mean: -2.479011\n",
      "ep 2996: ep_len:596 episode reward: total was -0.020000. running mean: -2.454421\n",
      "ep 2996: ep_len:105 episode reward: total was -18.240000. running mean: -2.612277\n",
      "ep 2996: ep_len:510 episode reward: total was 2.800000. running mean: -2.558154\n",
      "ep 2996: ep_len:531 episode reward: total was -13.200000. running mean: -2.664572\n",
      "epsilon:0.067133 episode_count: 20979. steps_count: 9068725.000000\n",
      "Time elapsed:  26966.422295331955\n",
      "ep 2997: ep_len:556 episode reward: total was -12.070000. running mean: -2.758627\n",
      "ep 2997: ep_len:537 episode reward: total was -12.720000. running mean: -2.858240\n",
      "ep 2997: ep_len:506 episode reward: total was 7.380000. running mean: -2.755858\n",
      "ep 2997: ep_len:500 episode reward: total was -37.090000. running mean: -3.099199\n",
      "ep 2997: ep_len:3 episode reward: total was 1.010000. running mean: -3.058107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2997: ep_len:592 episode reward: total was 0.080000. running mean: -3.026726\n",
      "ep 2997: ep_len:591 episode reward: total was 3.070000. running mean: -2.965759\n",
      "epsilon:0.067089 episode_count: 20986. steps_count: 9072010.000000\n",
      "Time elapsed:  26976.27825665474\n",
      "ep 2998: ep_len:116 episode reward: total was -0.560000. running mean: -2.941701\n",
      "ep 2998: ep_len:500 episode reward: total was -6.870000. running mean: -2.980984\n",
      "ep 2998: ep_len:594 episode reward: total was 28.720000. running mean: -2.663975\n",
      "ep 2998: ep_len:500 episode reward: total was 52.890000. running mean: -2.108435\n",
      "ep 2998: ep_len:3 episode reward: total was 1.010000. running mean: -2.077251\n",
      "ep 2998: ep_len:598 episode reward: total was -6.680000. running mean: -2.123278\n",
      "ep 2998: ep_len:515 episode reward: total was -6.400000. running mean: -2.166045\n",
      "epsilon:0.067044 episode_count: 20993. steps_count: 9074836.000000\n",
      "Time elapsed:  26991.38182592392\n",
      "ep 2999: ep_len:556 episode reward: total was 27.110000. running mean: -1.873285\n",
      "ep 2999: ep_len:281 episode reward: total was -15.750000. running mean: -2.012052\n",
      "ep 2999: ep_len:545 episode reward: total was -34.840000. running mean: -2.340331\n",
      "ep 2999: ep_len:501 episode reward: total was 52.950000. running mean: -1.787428\n",
      "ep 2999: ep_len:128 episode reward: total was -10.680000. running mean: -1.876354\n",
      "ep 2999: ep_len:514 episode reward: total was -64.470000. running mean: -2.502290\n",
      "ep 2999: ep_len:537 episode reward: total was -38.300000. running mean: -2.860267\n",
      "epsilon:0.067000 episode_count: 21000. steps_count: 9077898.000000\n",
      "Time elapsed:  27000.715442419052\n",
      "ep 3000: ep_len:625 episode reward: total was -132.880000. running mean: -4.160465\n",
      "ep 3000: ep_len:534 episode reward: total was -21.520000. running mean: -4.334060\n",
      "ep 3000: ep_len:530 episode reward: total was -15.430000. running mean: -4.445019\n",
      "ep 3000: ep_len:500 episode reward: total was 7.190000. running mean: -4.328669\n",
      "ep 3000: ep_len:3 episode reward: total was 1.010000. running mean: -4.275283\n",
      "ep 3000: ep_len:500 episode reward: total was 35.910000. running mean: -3.873430\n",
      "ep 3000: ep_len:573 episode reward: total was 38.750000. running mean: -3.447195\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.066956 episode_count: 21007. steps_count: 9081163.000000\n",
      "Time elapsed:  27029.684163570404\n",
      "ep 3001: ep_len:517 episode reward: total was 47.360000. running mean: -2.939123\n",
      "ep 3001: ep_len:510 episode reward: total was 59.910000. running mean: -2.310632\n",
      "ep 3001: ep_len:500 episode reward: total was -27.160000. running mean: -2.559126\n",
      "ep 3001: ep_len:500 episode reward: total was 8.960000. running mean: -2.443935\n",
      "ep 3001: ep_len:120 episode reward: total was 29.360000. running mean: -2.125895\n",
      "ep 3001: ep_len:568 episode reward: total was -18.820000. running mean: -2.292836\n",
      "ep 3001: ep_len:500 episode reward: total was 20.060000. running mean: -2.069308\n",
      "epsilon:0.066911 episode_count: 21014. steps_count: 9084378.000000\n",
      "Time elapsed:  27059.466492176056\n",
      "ep 3002: ep_len:531 episode reward: total was -42.100000. running mean: -2.469615\n",
      "ep 3002: ep_len:500 episode reward: total was -15.060000. running mean: -2.595519\n",
      "ep 3002: ep_len:511 episode reward: total was -8.710000. running mean: -2.656664\n",
      "ep 3002: ep_len:524 episode reward: total was 44.560000. running mean: -2.184497\n",
      "ep 3002: ep_len:3 episode reward: total was 1.010000. running mean: -2.152552\n",
      "ep 3002: ep_len:500 episode reward: total was -20.810000. running mean: -2.339126\n",
      "ep 3002: ep_len:500 episode reward: total was -83.680000. running mean: -3.152535\n",
      "epsilon:0.066867 episode_count: 21021. steps_count: 9087447.000000\n",
      "Time elapsed:  27068.845371246338\n",
      "ep 3003: ep_len:543 episode reward: total was 39.950000. running mean: -2.721510\n",
      "ep 3003: ep_len:577 episode reward: total was -46.670000. running mean: -3.160995\n",
      "ep 3003: ep_len:385 episode reward: total was -10.500000. running mean: -3.234385\n",
      "ep 3003: ep_len:539 episode reward: total was 51.830000. running mean: -2.683741\n",
      "ep 3003: ep_len:61 episode reward: total was 2.190000. running mean: -2.635004\n",
      "ep 3003: ep_len:500 episode reward: total was -30.610000. running mean: -2.914754\n",
      "ep 3003: ep_len:504 episode reward: total was 3.560000. running mean: -2.850006\n",
      "epsilon:0.066823 episode_count: 21028. steps_count: 9090556.000000\n",
      "Time elapsed:  27078.920883655548\n",
      "ep 3004: ep_len:516 episode reward: total was -79.090000. running mean: -3.612406\n",
      "ep 3004: ep_len:545 episode reward: total was -121.720000. running mean: -4.793482\n",
      "ep 3004: ep_len:417 episode reward: total was 28.210000. running mean: -4.463447\n",
      "ep 3004: ep_len:500 episode reward: total was 6.900000. running mean: -4.349813\n",
      "ep 3004: ep_len:3 episode reward: total was 1.010000. running mean: -4.296214\n",
      "ep 3004: ep_len:556 episode reward: total was 17.780000. running mean: -4.075452\n",
      "ep 3004: ep_len:500 episode reward: total was -25.280000. running mean: -4.287498\n",
      "epsilon:0.066778 episode_count: 21035. steps_count: 9093593.000000\n",
      "Time elapsed:  27086.542943954468\n",
      "ep 3005: ep_len:500 episode reward: total was 43.670000. running mean: -3.807923\n",
      "ep 3005: ep_len:500 episode reward: total was 46.410000. running mean: -3.305744\n",
      "ep 3005: ep_len:555 episode reward: total was -40.110000. running mean: -3.673786\n",
      "ep 3005: ep_len:523 episode reward: total was 37.650000. running mean: -3.260548\n",
      "ep 3005: ep_len:3 episode reward: total was 1.010000. running mean: -3.217843\n",
      "ep 3005: ep_len:500 episode reward: total was 29.170000. running mean: -2.893964\n",
      "ep 3005: ep_len:178 episode reward: total was -41.730000. running mean: -3.282325\n",
      "epsilon:0.066734 episode_count: 21042. steps_count: 9096352.000000\n",
      "Time elapsed:  27101.223694324493\n",
      "ep 3006: ep_len:501 episode reward: total was 1.770000. running mean: -3.231801\n",
      "ep 3006: ep_len:619 episode reward: total was -71.470000. running mean: -3.914183\n",
      "ep 3006: ep_len:545 episode reward: total was -26.740000. running mean: -4.142442\n",
      "ep 3006: ep_len:562 episode reward: total was -11.280000. running mean: -4.213817\n",
      "ep 3006: ep_len:3 episode reward: total was 1.010000. running mean: -4.161579\n",
      "ep 3006: ep_len:500 episode reward: total was -4.780000. running mean: -4.167763\n",
      "ep 3006: ep_len:581 episode reward: total was 10.700000. running mean: -4.019086\n",
      "epsilon:0.066690 episode_count: 21049. steps_count: 9099663.000000\n",
      "Time elapsed:  27126.24663591385\n",
      "ep 3007: ep_len:123 episode reward: total was 0.370000. running mean: -3.975195\n",
      "ep 3007: ep_len:515 episode reward: total was 24.030000. running mean: -3.695143\n",
      "ep 3007: ep_len:597 episode reward: total was -7.520000. running mean: -3.733391\n",
      "ep 3007: ep_len:164 episode reward: total was 8.210000. running mean: -3.613957\n",
      "ep 3007: ep_len:3 episode reward: total was 1.010000. running mean: -3.567718\n",
      "ep 3007: ep_len:694 episode reward: total was 17.430000. running mean: -3.357741\n",
      "ep 3007: ep_len:612 episode reward: total was -42.260000. running mean: -3.746763\n",
      "epsilon:0.066645 episode_count: 21056. steps_count: 9102371.000000\n",
      "Time elapsed:  27134.617071151733\n",
      "ep 3008: ep_len:197 episode reward: total was 27.670000. running mean: -3.432596\n",
      "ep 3008: ep_len:613 episode reward: total was 48.900000. running mean: -2.909270\n",
      "ep 3008: ep_len:549 episode reward: total was -40.280000. running mean: -3.282977\n",
      "ep 3008: ep_len:500 episode reward: total was 38.350000. running mean: -2.866647\n",
      "ep 3008: ep_len:3 episode reward: total was 1.010000. running mean: -2.827881\n",
      "ep 3008: ep_len:500 episode reward: total was 13.250000. running mean: -2.667102\n",
      "ep 3008: ep_len:594 episode reward: total was 16.980000. running mean: -2.470631\n",
      "epsilon:0.066601 episode_count: 21063. steps_count: 9105327.000000\n",
      "Time elapsed:  27142.017912864685\n",
      "ep 3009: ep_len:500 episode reward: total was -29.820000. running mean: -2.744125\n",
      "ep 3009: ep_len:563 episode reward: total was 66.360000. running mean: -2.053083\n",
      "ep 3009: ep_len:559 episode reward: total was 13.950000. running mean: -1.893053\n",
      "ep 3009: ep_len:546 episode reward: total was 24.840000. running mean: -1.625722\n",
      "ep 3009: ep_len:3 episode reward: total was 1.010000. running mean: -1.599365\n",
      "ep 3009: ep_len:526 episode reward: total was -2.660000. running mean: -1.609971\n",
      "ep 3009: ep_len:500 episode reward: total was 16.810000. running mean: -1.425771\n",
      "epsilon:0.066557 episode_count: 21070. steps_count: 9108524.000000\n",
      "Time elapsed:  27150.980334043503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3010: ep_len:511 episode reward: total was -9.260000. running mean: -1.504114\n",
      "ep 3010: ep_len:500 episode reward: total was -22.520000. running mean: -1.714273\n",
      "ep 3010: ep_len:500 episode reward: total was -18.120000. running mean: -1.878330\n",
      "ep 3010: ep_len:595 episode reward: total was 20.600000. running mean: -1.653547\n",
      "ep 3010: ep_len:3 episode reward: total was 1.010000. running mean: -1.626911\n",
      "ep 3010: ep_len:516 episode reward: total was 18.400000. running mean: -1.426642\n",
      "ep 3010: ep_len:604 episode reward: total was -38.210000. running mean: -1.794476\n",
      "epsilon:0.066512 episode_count: 21077. steps_count: 9111753.000000\n",
      "Time elapsed:  27160.593105077744\n",
      "ep 3011: ep_len:188 episode reward: total was 15.210000. running mean: -1.624431\n",
      "ep 3011: ep_len:552 episode reward: total was 37.530000. running mean: -1.232887\n",
      "ep 3011: ep_len:360 episode reward: total was 52.570000. running mean: -0.694858\n",
      "ep 3011: ep_len:500 episode reward: total was 11.600000. running mean: -0.571909\n",
      "ep 3011: ep_len:54 episode reward: total was 23.510000. running mean: -0.331090\n",
      "ep 3011: ep_len:504 episode reward: total was 32.470000. running mean: -0.003079\n",
      "ep 3011: ep_len:517 episode reward: total was -16.680000. running mean: -0.169848\n",
      "epsilon:0.066468 episode_count: 21084. steps_count: 9114428.000000\n",
      "Time elapsed:  27168.984177350998\n",
      "ep 3012: ep_len:500 episode reward: total was -86.340000. running mean: -1.031550\n",
      "ep 3012: ep_len:519 episode reward: total was 33.530000. running mean: -0.685934\n",
      "ep 3012: ep_len:440 episode reward: total was 36.640000. running mean: -0.312675\n",
      "ep 3012: ep_len:516 episode reward: total was 4.350000. running mean: -0.266048\n",
      "ep 3012: ep_len:3 episode reward: total was -1.500000. running mean: -0.278388\n",
      "ep 3012: ep_len:852 episode reward: total was -316.480000. running mean: -3.440404\n",
      "ep 3012: ep_len:586 episode reward: total was 34.500000. running mean: -3.061000\n",
      "epsilon:0.066424 episode_count: 21091. steps_count: 9117844.000000\n",
      "Time elapsed:  27189.702783346176\n",
      "ep 3013: ep_len:500 episode reward: total was 3.810000. running mean: -2.992290\n",
      "ep 3013: ep_len:500 episode reward: total was 35.870000. running mean: -2.603667\n",
      "ep 3013: ep_len:569 episode reward: total was -11.100000. running mean: -2.688630\n",
      "ep 3013: ep_len:525 episode reward: total was 45.060000. running mean: -2.211144\n",
      "ep 3013: ep_len:3 episode reward: total was 1.010000. running mean: -2.178933\n",
      "ep 3013: ep_len:620 episode reward: total was -0.990000. running mean: -2.167043\n",
      "ep 3013: ep_len:564 episode reward: total was -102.080000. running mean: -3.166173\n",
      "epsilon:0.066379 episode_count: 21098. steps_count: 9121125.000000\n",
      "Time elapsed:  27196.98103785515\n",
      "ep 3014: ep_len:634 episode reward: total was 44.070000. running mean: -2.693811\n",
      "ep 3014: ep_len:596 episode reward: total was -17.600000. running mean: -2.842873\n",
      "ep 3014: ep_len:500 episode reward: total was 18.100000. running mean: -2.633444\n",
      "ep 3014: ep_len:500 episode reward: total was 57.480000. running mean: -2.032310\n",
      "ep 3014: ep_len:48 episode reward: total was 21.000000. running mean: -1.801987\n",
      "ep 3014: ep_len:217 episode reward: total was 30.870000. running mean: -1.475267\n",
      "ep 3014: ep_len:581 episode reward: total was -68.890000. running mean: -2.149414\n",
      "epsilon:0.066335 episode_count: 21105. steps_count: 9124201.000000\n",
      "Time elapsed:  27206.391008615494\n",
      "ep 3015: ep_len:564 episode reward: total was 28.500000. running mean: -1.842920\n",
      "ep 3015: ep_len:548 episode reward: total was -41.260000. running mean: -2.237091\n",
      "ep 3015: ep_len:541 episode reward: total was -4.110000. running mean: -2.255820\n",
      "ep 3015: ep_len:514 episode reward: total was 43.160000. running mean: -1.801662\n",
      "ep 3015: ep_len:106 episode reward: total was 27.750000. running mean: -1.506145\n",
      "ep 3015: ep_len:500 episode reward: total was 1.700000. running mean: -1.474084\n",
      "ep 3015: ep_len:185 episode reward: total was -4.510000. running mean: -1.504443\n",
      "epsilon:0.066291 episode_count: 21112. steps_count: 9127159.000000\n",
      "Time elapsed:  27215.492915153503\n",
      "ep 3016: ep_len:500 episode reward: total was 30.010000. running mean: -1.189298\n",
      "ep 3016: ep_len:500 episode reward: total was 71.260000. running mean: -0.464805\n",
      "ep 3016: ep_len:431 episode reward: total was 35.610000. running mean: -0.104057\n",
      "ep 3016: ep_len:500 episode reward: total was 29.840000. running mean: 0.195383\n",
      "ep 3016: ep_len:120 episode reward: total was 20.300000. running mean: 0.396429\n",
      "ep 3016: ep_len:540 episode reward: total was 0.010000. running mean: 0.392565\n",
      "ep 3016: ep_len:186 episode reward: total was 5.570000. running mean: 0.444339\n",
      "epsilon:0.066246 episode_count: 21119. steps_count: 9129936.000000\n",
      "Time elapsed:  27224.06246328354\n",
      "ep 3017: ep_len:530 episode reward: total was -114.950000. running mean: -0.709604\n",
      "ep 3017: ep_len:538 episode reward: total was -36.840000. running mean: -1.070908\n",
      "ep 3017: ep_len:637 episode reward: total was -4.240000. running mean: -1.102599\n",
      "ep 3017: ep_len:626 episode reward: total was 24.610000. running mean: -0.845473\n",
      "ep 3017: ep_len:3 episode reward: total was 1.010000. running mean: -0.826918\n",
      "ep 3017: ep_len:537 episode reward: total was -15.650000. running mean: -0.975149\n",
      "ep 3017: ep_len:521 episode reward: total was -15.830000. running mean: -1.123697\n",
      "epsilon:0.066202 episode_count: 21126. steps_count: 9133328.000000\n",
      "Time elapsed:  27234.347930669785\n",
      "ep 3018: ep_len:646 episode reward: total was -55.050000. running mean: -1.662960\n",
      "ep 3018: ep_len:500 episode reward: total was 16.090000. running mean: -1.485431\n",
      "ep 3018: ep_len:673 episode reward: total was -17.760000. running mean: -1.648177\n",
      "ep 3018: ep_len:132 episode reward: total was 9.100000. running mean: -1.540695\n",
      "ep 3018: ep_len:3 episode reward: total was 1.010000. running mean: -1.515188\n",
      "ep 3018: ep_len:587 episode reward: total was -27.280000. running mean: -1.772836\n",
      "ep 3018: ep_len:504 episode reward: total was -5.260000. running mean: -1.807708\n",
      "epsilon:0.066158 episode_count: 21133. steps_count: 9136373.000000\n",
      "Time elapsed:  27243.444820404053\n",
      "ep 3019: ep_len:555 episode reward: total was 17.420000. running mean: -1.615430\n",
      "ep 3019: ep_len:512 episode reward: total was 4.320000. running mean: -1.556076\n",
      "ep 3019: ep_len:526 episode reward: total was -32.940000. running mean: -1.869915\n",
      "ep 3019: ep_len:500 episode reward: total was -10.750000. running mean: -1.958716\n",
      "ep 3019: ep_len:3 episode reward: total was -0.490000. running mean: -1.944029\n",
      "ep 3019: ep_len:501 episode reward: total was -16.070000. running mean: -2.085289\n",
      "ep 3019: ep_len:559 episode reward: total was -20.430000. running mean: -2.268736\n",
      "epsilon:0.066113 episode_count: 21140. steps_count: 9139529.000000\n",
      "Time elapsed:  27260.132074594498\n",
      "ep 3020: ep_len:635 episode reward: total was -21.150000. running mean: -2.457549\n",
      "ep 3020: ep_len:634 episode reward: total was 61.020000. running mean: -1.822773\n",
      "ep 3020: ep_len:500 episode reward: total was -42.030000. running mean: -2.224845\n",
      "ep 3020: ep_len:504 episode reward: total was 0.410000. running mean: -2.198497\n",
      "ep 3020: ep_len:3 episode reward: total was 1.010000. running mean: -2.166412\n",
      "ep 3020: ep_len:522 episode reward: total was 16.200000. running mean: -1.982748\n",
      "ep 3020: ep_len:198 episode reward: total was -5.020000. running mean: -2.013120\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.066069 episode_count: 21147. steps_count: 9142525.000000\n",
      "Time elapsed:  27274.565490722656\n",
      "ep 3021: ep_len:579 episode reward: total was 38.080000. running mean: -1.612189\n",
      "ep 3021: ep_len:585 episode reward: total was -22.270000. running mean: -1.818767\n",
      "ep 3021: ep_len:526 episode reward: total was -12.930000. running mean: -1.929880\n",
      "ep 3021: ep_len:408 episode reward: total was -8.630000. running mean: -1.996881\n",
      "ep 3021: ep_len:3 episode reward: total was 1.010000. running mean: -1.966812\n",
      "ep 3021: ep_len:500 episode reward: total was 25.680000. running mean: -1.690344\n",
      "ep 3021: ep_len:321 episode reward: total was 16.080000. running mean: -1.512640\n",
      "epsilon:0.066025 episode_count: 21154. steps_count: 9145447.000000\n",
      "Time elapsed:  27288.303823947906\n",
      "ep 3022: ep_len:570 episode reward: total was -29.480000. running mean: -1.792314\n",
      "ep 3022: ep_len:525 episode reward: total was -18.580000. running mean: -1.960191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3022: ep_len:611 episode reward: total was -11.320000. running mean: -2.053789\n",
      "ep 3022: ep_len:42 episode reward: total was 3.710000. running mean: -1.996151\n",
      "ep 3022: ep_len:97 episode reward: total was 22.700000. running mean: -1.749190\n",
      "ep 3022: ep_len:500 episode reward: total was -2.500000. running mean: -1.756698\n",
      "ep 3022: ep_len:518 episode reward: total was 16.330000. running mean: -1.575831\n",
      "epsilon:0.065980 episode_count: 21161. steps_count: 9148310.000000\n",
      "Time elapsed:  27304.296423196793\n",
      "ep 3023: ep_len:500 episode reward: total was 56.480000. running mean: -0.995272\n",
      "ep 3023: ep_len:561 episode reward: total was 48.450000. running mean: -0.500820\n",
      "ep 3023: ep_len:515 episode reward: total was 19.220000. running mean: -0.303611\n",
      "ep 3023: ep_len:540 episode reward: total was 24.390000. running mean: -0.056675\n",
      "ep 3023: ep_len:86 episode reward: total was 23.230000. running mean: 0.176191\n",
      "ep 3023: ep_len:521 episode reward: total was -7.070000. running mean: 0.103730\n",
      "ep 3023: ep_len:606 episode reward: total was -3.100000. running mean: 0.071692\n",
      "epsilon:0.065936 episode_count: 21168. steps_count: 9151639.000000\n",
      "Time elapsed:  27312.790551424026\n",
      "ep 3024: ep_len:565 episode reward: total was -77.840000. running mean: -0.707425\n",
      "ep 3024: ep_len:503 episode reward: total was -16.840000. running mean: -0.868750\n",
      "ep 3024: ep_len:637 episode reward: total was 14.550000. running mean: -0.714563\n",
      "ep 3024: ep_len:596 episode reward: total was 40.220000. running mean: -0.305217\n",
      "ep 3024: ep_len:78 episode reward: total was 13.230000. running mean: -0.169865\n",
      "ep 3024: ep_len:500 episode reward: total was 27.250000. running mean: 0.104333\n",
      "ep 3024: ep_len:305 episode reward: total was -20.630000. running mean: -0.103010\n",
      "epsilon:0.065892 episode_count: 21175. steps_count: 9154823.000000\n",
      "Time elapsed:  27321.67960047722\n",
      "ep 3025: ep_len:558 episode reward: total was 30.680000. running mean: 0.204820\n",
      "ep 3025: ep_len:249 episode reward: total was 8.460000. running mean: 0.287372\n",
      "ep 3025: ep_len:500 episode reward: total was 2.620000. running mean: 0.310698\n",
      "ep 3025: ep_len:170 episode reward: total was 7.680000. running mean: 0.384391\n",
      "ep 3025: ep_len:3 episode reward: total was 1.010000. running mean: 0.390647\n",
      "ep 3025: ep_len:505 episode reward: total was 10.890000. running mean: 0.495641\n",
      "ep 3025: ep_len:550 episode reward: total was 31.250000. running mean: 0.803185\n",
      "epsilon:0.065847 episode_count: 21182. steps_count: 9157358.000000\n",
      "Time elapsed:  27329.61039829254\n",
      "ep 3026: ep_len:616 episode reward: total was 27.220000. running mean: 1.067353\n",
      "ep 3026: ep_len:342 episode reward: total was -68.630000. running mean: 0.370379\n",
      "ep 3026: ep_len:618 episode reward: total was -35.070000. running mean: 0.015975\n",
      "ep 3026: ep_len:622 episode reward: total was -73.170000. running mean: -0.715884\n",
      "ep 3026: ep_len:31 episode reward: total was 4.510000. running mean: -0.663626\n",
      "ep 3026: ep_len:500 episode reward: total was 20.070000. running mean: -0.456289\n",
      "ep 3026: ep_len:611 episode reward: total was -61.990000. running mean: -1.071626\n",
      "epsilon:0.065803 episode_count: 21189. steps_count: 9160698.000000\n",
      "Time elapsed:  27337.992623090744\n",
      "ep 3027: ep_len:652 episode reward: total was -22.760000. running mean: -1.288510\n",
      "ep 3027: ep_len:263 episode reward: total was 1.590000. running mean: -1.259725\n",
      "ep 3027: ep_len:523 episode reward: total was 30.200000. running mean: -0.945128\n",
      "ep 3027: ep_len:170 episode reward: total was 7.160000. running mean: -0.864076\n",
      "ep 3027: ep_len:3 episode reward: total was 1.010000. running mean: -0.845336\n",
      "ep 3027: ep_len:568 episode reward: total was -10.120000. running mean: -0.938082\n",
      "ep 3027: ep_len:289 episode reward: total was -0.230000. running mean: -0.931002\n",
      "epsilon:0.065759 episode_count: 21196. steps_count: 9163166.000000\n",
      "Time elapsed:  27345.87629008293\n",
      "ep 3028: ep_len:628 episode reward: total was -28.390000. running mean: -1.205592\n",
      "ep 3028: ep_len:500 episode reward: total was 25.600000. running mean: -0.937536\n",
      "ep 3028: ep_len:370 episode reward: total was 27.880000. running mean: -0.649360\n",
      "ep 3028: ep_len:571 episode reward: total was 6.770000. running mean: -0.575167\n",
      "ep 3028: ep_len:114 episode reward: total was 26.270000. running mean: -0.306715\n",
      "ep 3028: ep_len:585 episode reward: total was 8.140000. running mean: -0.222248\n",
      "ep 3028: ep_len:600 episode reward: total was -1.860000. running mean: -0.238625\n",
      "epsilon:0.065714 episode_count: 21203. steps_count: 9166534.000000\n",
      "Time elapsed:  27355.958113193512\n",
      "ep 3029: ep_len:571 episode reward: total was -43.790000. running mean: -0.674139\n",
      "ep 3029: ep_len:266 episode reward: total was 8.570000. running mean: -0.581698\n",
      "ep 3029: ep_len:610 episode reward: total was -22.470000. running mean: -0.800581\n",
      "ep 3029: ep_len:100 episode reward: total was -1.140000. running mean: -0.803975\n",
      "ep 3029: ep_len:3 episode reward: total was 1.010000. running mean: -0.785835\n",
      "ep 3029: ep_len:500 episode reward: total was 26.320000. running mean: -0.514777\n",
      "ep 3029: ep_len:519 episode reward: total was 1.080000. running mean: -0.498829\n",
      "epsilon:0.065670 episode_count: 21210. steps_count: 9169103.000000\n",
      "Time elapsed:  27364.110679388046\n",
      "ep 3030: ep_len:585 episode reward: total was 30.930000. running mean: -0.184541\n",
      "ep 3030: ep_len:500 episode reward: total was 14.390000. running mean: -0.038795\n",
      "ep 3030: ep_len:546 episode reward: total was -16.220000. running mean: -0.200607\n",
      "ep 3030: ep_len:609 episode reward: total was -8.890000. running mean: -0.287501\n",
      "ep 3030: ep_len:3 episode reward: total was 1.010000. running mean: -0.274526\n",
      "ep 3030: ep_len:514 episode reward: total was 27.420000. running mean: 0.002419\n",
      "ep 3030: ep_len:619 episode reward: total was 22.850000. running mean: 0.230895\n",
      "epsilon:0.065626 episode_count: 21217. steps_count: 9172479.000000\n",
      "Time elapsed:  27374.126703739166\n",
      "ep 3031: ep_len:619 episode reward: total was -40.720000. running mean: -0.178614\n",
      "ep 3031: ep_len:500 episode reward: total was 11.530000. running mean: -0.061528\n",
      "ep 3031: ep_len:597 episode reward: total was -34.680000. running mean: -0.407713\n",
      "ep 3031: ep_len:612 episode reward: total was -58.660000. running mean: -0.990236\n",
      "ep 3031: ep_len:3 episode reward: total was 1.010000. running mean: -0.970233\n",
      "ep 3031: ep_len:247 episode reward: total was 43.940000. running mean: -0.521131\n",
      "ep 3031: ep_len:600 episode reward: total was -17.740000. running mean: -0.693320\n",
      "epsilon:0.065581 episode_count: 21224. steps_count: 9175657.000000\n",
      "Time elapsed:  27383.622863054276\n",
      "ep 3032: ep_len:229 episode reward: total was -17.250000. running mean: -0.858886\n",
      "ep 3032: ep_len:201 episode reward: total was 7.410000. running mean: -0.776198\n",
      "ep 3032: ep_len:688 episode reward: total was -22.940000. running mean: -0.997836\n",
      "ep 3032: ep_len:511 episode reward: total was -9.990000. running mean: -1.087757\n",
      "ep 3032: ep_len:3 episode reward: total was 1.010000. running mean: -1.066780\n",
      "ep 3032: ep_len:554 episode reward: total was -1.800000. running mean: -1.074112\n",
      "ep 3032: ep_len:630 episode reward: total was 8.880000. running mean: -0.974571\n",
      "epsilon:0.065537 episode_count: 21231. steps_count: 9178473.000000\n",
      "Time elapsed:  27392.311284065247\n",
      "ep 3033: ep_len:514 episode reward: total was 50.230000. running mean: -0.462525\n",
      "ep 3033: ep_len:500 episode reward: total was 24.230000. running mean: -0.215600\n",
      "ep 3033: ep_len:701 episode reward: total was -32.330000. running mean: -0.536744\n",
      "ep 3033: ep_len:562 episode reward: total was 54.050000. running mean: 0.009124\n",
      "ep 3033: ep_len:3 episode reward: total was 1.010000. running mean: 0.019132\n",
      "ep 3033: ep_len:507 episode reward: total was -38.590000. running mean: -0.366959\n",
      "ep 3033: ep_len:611 episode reward: total was -17.340000. running mean: -0.536689\n",
      "epsilon:0.065493 episode_count: 21238. steps_count: 9181871.000000\n",
      "Time elapsed:  27409.113298416138\n",
      "ep 3034: ep_len:555 episode reward: total was 30.890000. running mean: -0.222422\n",
      "ep 3034: ep_len:612 episode reward: total was 20.870000. running mean: -0.011498\n",
      "ep 3034: ep_len:551 episode reward: total was -21.400000. running mean: -0.225383\n",
      "ep 3034: ep_len:559 episode reward: total was 74.160000. running mean: 0.518471\n",
      "ep 3034: ep_len:3 episode reward: total was -3.000000. running mean: 0.483286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3034: ep_len:634 episode reward: total was -55.120000. running mean: -0.072747\n",
      "ep 3034: ep_len:282 episode reward: total was -8.810000. running mean: -0.160119\n",
      "epsilon:0.065448 episode_count: 21245. steps_count: 9185067.000000\n",
      "Time elapsed:  27425.95663523674\n",
      "ep 3035: ep_len:518 episode reward: total was -48.980000. running mean: -0.648318\n",
      "ep 3035: ep_len:500 episode reward: total was -0.320000. running mean: -0.645035\n",
      "ep 3035: ep_len:648 episode reward: total was 8.300000. running mean: -0.555585\n",
      "ep 3035: ep_len:500 episode reward: total was 50.500000. running mean: -0.045029\n",
      "ep 3035: ep_len:103 episode reward: total was 26.250000. running mean: 0.217921\n",
      "ep 3035: ep_len:502 episode reward: total was -7.360000. running mean: 0.142142\n",
      "ep 3035: ep_len:500 episode reward: total was -42.860000. running mean: -0.287879\n",
      "epsilon:0.065404 episode_count: 21252. steps_count: 9188338.000000\n",
      "Time elapsed:  27450.480782985687\n",
      "ep 3036: ep_len:519 episode reward: total was 47.650000. running mean: 0.191500\n",
      "ep 3036: ep_len:620 episode reward: total was 12.570000. running mean: 0.315285\n",
      "ep 3036: ep_len:450 episode reward: total was 38.220000. running mean: 0.694332\n",
      "ep 3036: ep_len:500 episode reward: total was -1.560000. running mean: 0.671788\n",
      "ep 3036: ep_len:109 episode reward: total was 27.780000. running mean: 0.942870\n",
      "ep 3036: ep_len:230 episode reward: total was 47.380000. running mean: 1.407242\n",
      "ep 3036: ep_len:500 episode reward: total was 55.930000. running mean: 1.952469\n",
      "epsilon:0.065360 episode_count: 21259. steps_count: 9191266.000000\n",
      "Time elapsed:  27465.903204917908\n",
      "ep 3037: ep_len:640 episode reward: total was 42.170000. running mean: 2.354645\n",
      "ep 3037: ep_len:500 episode reward: total was 68.230000. running mean: 3.013398\n",
      "ep 3037: ep_len:524 episode reward: total was -38.140000. running mean: 2.601864\n",
      "ep 3037: ep_len:376 episode reward: total was 5.990000. running mean: 2.635746\n",
      "ep 3037: ep_len:3 episode reward: total was 1.010000. running mean: 2.619488\n",
      "ep 3037: ep_len:589 episode reward: total was 29.230000. running mean: 2.885593\n",
      "ep 3037: ep_len:575 episode reward: total was -0.420000. running mean: 2.852537\n",
      "epsilon:0.065315 episode_count: 21266. steps_count: 9194473.000000\n",
      "Time elapsed:  27475.641010046005\n",
      "ep 3038: ep_len:500 episode reward: total was 64.530000. running mean: 3.469312\n",
      "ep 3038: ep_len:541 episode reward: total was 48.090000. running mean: 3.915519\n",
      "ep 3038: ep_len:563 episode reward: total was -31.450000. running mean: 3.561864\n",
      "ep 3038: ep_len:500 episode reward: total was 10.500000. running mean: 3.631245\n",
      "ep 3038: ep_len:3 episode reward: total was 1.010000. running mean: 3.605033\n",
      "ep 3038: ep_len:527 episode reward: total was 16.980000. running mean: 3.738782\n",
      "ep 3038: ep_len:514 episode reward: total was 6.940000. running mean: 3.770794\n",
      "epsilon:0.065271 episode_count: 21273. steps_count: 9197621.000000\n",
      "Time elapsed:  27485.120556354523\n",
      "ep 3039: ep_len:572 episode reward: total was -82.780000. running mean: 2.905286\n",
      "ep 3039: ep_len:518 episode reward: total was 76.420000. running mean: 3.640434\n",
      "ep 3039: ep_len:569 episode reward: total was 39.480000. running mean: 3.998829\n",
      "ep 3039: ep_len:605 episode reward: total was 47.820000. running mean: 4.437041\n",
      "ep 3039: ep_len:3 episode reward: total was 1.010000. running mean: 4.402771\n",
      "ep 3039: ep_len:690 episode reward: total was -36.020000. running mean: 3.998543\n",
      "ep 3039: ep_len:296 episode reward: total was -28.840000. running mean: 3.670157\n",
      "epsilon:0.065227 episode_count: 21280. steps_count: 9200874.000000\n",
      "Time elapsed:  27494.8183093071\n",
      "ep 3040: ep_len:560 episode reward: total was -35.410000. running mean: 3.279356\n",
      "ep 3040: ep_len:500 episode reward: total was 21.280000. running mean: 3.459362\n",
      "ep 3040: ep_len:552 episode reward: total was -41.870000. running mean: 3.006069\n",
      "ep 3040: ep_len:547 episode reward: total was 33.360000. running mean: 3.309608\n",
      "ep 3040: ep_len:3 episode reward: total was 1.010000. running mean: 3.286612\n",
      "ep 3040: ep_len:508 episode reward: total was -31.710000. running mean: 2.936646\n",
      "ep 3040: ep_len:340 episode reward: total was 10.950000. running mean: 3.016779\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.065182 episode_count: 21287. steps_count: 9203884.000000\n",
      "Time elapsed:  27509.400900125504\n",
      "ep 3041: ep_len:568 episode reward: total was 37.390000. running mean: 3.360512\n",
      "ep 3041: ep_len:500 episode reward: total was -0.670000. running mean: 3.320206\n",
      "ep 3041: ep_len:569 episode reward: total was 18.660000. running mean: 3.473604\n",
      "ep 3041: ep_len:56 episode reward: total was 0.330000. running mean: 3.442168\n",
      "ep 3041: ep_len:3 episode reward: total was 1.010000. running mean: 3.417847\n",
      "ep 3041: ep_len:542 episode reward: total was -38.840000. running mean: 2.995268\n",
      "ep 3041: ep_len:278 episode reward: total was 5.840000. running mean: 3.023715\n",
      "epsilon:0.065138 episode_count: 21294. steps_count: 9206400.000000\n",
      "Time elapsed:  27517.160900831223\n",
      "ep 3042: ep_len:593 episode reward: total was 52.420000. running mean: 3.517678\n",
      "ep 3042: ep_len:580 episode reward: total was -20.610000. running mean: 3.276402\n",
      "ep 3042: ep_len:407 episode reward: total was 27.640000. running mean: 3.520038\n",
      "ep 3042: ep_len:371 episode reward: total was 31.470000. running mean: 3.799537\n",
      "ep 3042: ep_len:49 episode reward: total was 1.240000. running mean: 3.773942\n",
      "ep 3042: ep_len:534 episode reward: total was -42.000000. running mean: 3.316202\n",
      "ep 3042: ep_len:567 episode reward: total was 18.860000. running mean: 3.471640\n",
      "epsilon:0.065094 episode_count: 21301. steps_count: 9209501.000000\n",
      "Time elapsed:  27526.48559641838\n",
      "ep 3043: ep_len:656 episode reward: total was -16.990000. running mean: 3.267024\n",
      "ep 3043: ep_len:500 episode reward: total was 9.060000. running mean: 3.324954\n",
      "ep 3043: ep_len:584 episode reward: total was -39.710000. running mean: 2.894604\n",
      "ep 3043: ep_len:517 episode reward: total was 11.770000. running mean: 2.983358\n",
      "ep 3043: ep_len:47 episode reward: total was 17.500000. running mean: 3.128525\n",
      "ep 3043: ep_len:500 episode reward: total was -40.570000. running mean: 2.691539\n",
      "ep 3043: ep_len:565 episode reward: total was 11.090000. running mean: 2.775524\n",
      "epsilon:0.065049 episode_count: 21308. steps_count: 9212870.000000\n",
      "Time elapsed:  27534.015863656998\n",
      "ep 3044: ep_len:635 episode reward: total was -35.390000. running mean: 2.393869\n",
      "ep 3044: ep_len:560 episode reward: total was -38.170000. running mean: 1.988230\n",
      "ep 3044: ep_len:538 episode reward: total was 3.960000. running mean: 2.007948\n",
      "ep 3044: ep_len:519 episode reward: total was 53.980000. running mean: 2.527668\n",
      "ep 3044: ep_len:83 episode reward: total was 19.740000. running mean: 2.699792\n",
      "ep 3044: ep_len:748 episode reward: total was -152.420000. running mean: 1.148594\n",
      "ep 3044: ep_len:553 episode reward: total was 5.440000. running mean: 1.191508\n",
      "epsilon:0.065005 episode_count: 21315. steps_count: 9216506.000000\n",
      "Time elapsed:  27551.27144217491\n",
      "ep 3045: ep_len:551 episode reward: total was 42.670000. running mean: 1.606293\n",
      "ep 3045: ep_len:500 episode reward: total was 14.190000. running mean: 1.732130\n",
      "ep 3045: ep_len:556 episode reward: total was 51.680000. running mean: 2.231608\n",
      "ep 3045: ep_len:500 episode reward: total was 20.060000. running mean: 2.409892\n",
      "ep 3045: ep_len:84 episode reward: total was 25.170000. running mean: 2.637493\n",
      "ep 3045: ep_len:500 episode reward: total was -23.170000. running mean: 2.379418\n",
      "ep 3045: ep_len:553 episode reward: total was 19.210000. running mean: 2.547724\n",
      "epsilon:0.064961 episode_count: 21322. steps_count: 9219750.000000\n",
      "Time elapsed:  27568.090819597244\n",
      "ep 3046: ep_len:500 episode reward: total was 62.480000. running mean: 3.147047\n",
      "ep 3046: ep_len:500 episode reward: total was 14.740000. running mean: 3.262977\n",
      "ep 3046: ep_len:500 episode reward: total was -38.170000. running mean: 2.848647\n",
      "ep 3046: ep_len:500 episode reward: total was -23.330000. running mean: 2.586860\n",
      "ep 3046: ep_len:3 episode reward: total was 1.010000. running mean: 2.571092\n",
      "ep 3046: ep_len:526 episode reward: total was 30.290000. running mean: 2.848281\n",
      "ep 3046: ep_len:616 episode reward: total was 46.270000. running mean: 3.282498\n",
      "epsilon:0.064916 episode_count: 21329. steps_count: 9222895.000000\n",
      "Time elapsed:  27575.215972185135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3047: ep_len:501 episode reward: total was 53.950000. running mean: 3.789173\n",
      "ep 3047: ep_len:565 episode reward: total was 16.490000. running mean: 3.916181\n",
      "ep 3047: ep_len:510 episode reward: total was 38.510000. running mean: 4.262119\n",
      "ep 3047: ep_len:537 episode reward: total was 35.450000. running mean: 4.573998\n",
      "ep 3047: ep_len:3 episode reward: total was 1.010000. running mean: 4.538358\n",
      "ep 3047: ep_len:614 episode reward: total was -14.830000. running mean: 4.344675\n",
      "ep 3047: ep_len:187 episode reward: total was 8.490000. running mean: 4.386128\n",
      "epsilon:0.064872 episode_count: 21336. steps_count: 9225812.000000\n",
      "Time elapsed:  27583.527734041214\n",
      "ep 3048: ep_len:593 episode reward: total was 61.430000. running mean: 4.956567\n",
      "ep 3048: ep_len:500 episode reward: total was 22.960000. running mean: 5.136601\n",
      "ep 3048: ep_len:361 episode reward: total was 45.240000. running mean: 5.537635\n",
      "ep 3048: ep_len:545 episode reward: total was -20.850000. running mean: 5.273759\n",
      "ep 3048: ep_len:3 episode reward: total was 1.010000. running mean: 5.231121\n",
      "ep 3048: ep_len:569 episode reward: total was -8.550000. running mean: 5.093310\n",
      "ep 3048: ep_len:609 episode reward: total was 3.570000. running mean: 5.078077\n",
      "epsilon:0.064828 episode_count: 21343. steps_count: 9228992.000000\n",
      "Time elapsed:  27600.166367530823\n",
      "ep 3049: ep_len:662 episode reward: total was -49.950000. running mean: 4.527796\n",
      "ep 3049: ep_len:544 episode reward: total was -38.370000. running mean: 4.098818\n",
      "ep 3049: ep_len:650 episode reward: total was -52.220000. running mean: 3.535630\n",
      "ep 3049: ep_len:512 episode reward: total was -6.920000. running mean: 3.431074\n",
      "ep 3049: ep_len:3 episode reward: total was 1.010000. running mean: 3.406863\n",
      "ep 3049: ep_len:236 episode reward: total was 39.880000. running mean: 3.771594\n",
      "ep 3049: ep_len:556 episode reward: total was 27.620000. running mean: 4.010078\n",
      "epsilon:0.064783 episode_count: 21350. steps_count: 9232155.000000\n",
      "Time elapsed:  27609.6766602993\n",
      "ep 3050: ep_len:621 episode reward: total was -51.940000. running mean: 3.450577\n",
      "ep 3050: ep_len:603 episode reward: total was -5.730000. running mean: 3.358772\n",
      "ep 3050: ep_len:507 episode reward: total was 8.260000. running mean: 3.407784\n",
      "ep 3050: ep_len:534 episode reward: total was -40.300000. running mean: 2.970706\n",
      "ep 3050: ep_len:3 episode reward: total was -0.490000. running mean: 2.936099\n",
      "ep 3050: ep_len:530 episode reward: total was -22.680000. running mean: 2.679938\n",
      "ep 3050: ep_len:500 episode reward: total was 16.600000. running mean: 2.819139\n",
      "epsilon:0.064739 episode_count: 21357. steps_count: 9235453.000000\n",
      "Time elapsed:  27626.40485739708\n",
      "ep 3051: ep_len:578 episode reward: total was 15.170000. running mean: 2.942647\n",
      "ep 3051: ep_len:527 episode reward: total was 39.770000. running mean: 3.310921\n",
      "ep 3051: ep_len:374 episode reward: total was 51.860000. running mean: 3.796412\n",
      "ep 3051: ep_len:506 episode reward: total was 26.690000. running mean: 4.025347\n",
      "ep 3051: ep_len:3 episode reward: total was 1.010000. running mean: 3.995194\n",
      "ep 3051: ep_len:213 episode reward: total was -61.070000. running mean: 3.344542\n",
      "ep 3051: ep_len:561 episode reward: total was 0.460000. running mean: 3.315697\n",
      "epsilon:0.064695 episode_count: 21364. steps_count: 9238215.000000\n",
      "Time elapsed:  27637.715663671494\n",
      "ep 3052: ep_len:583 episode reward: total was 31.830000. running mean: 3.600840\n",
      "ep 3052: ep_len:500 episode reward: total was -109.930000. running mean: 2.465531\n",
      "ep 3052: ep_len:586 episode reward: total was -62.710000. running mean: 1.813776\n",
      "ep 3052: ep_len:568 episode reward: total was 46.060000. running mean: 2.256238\n",
      "ep 3052: ep_len:3 episode reward: total was 1.010000. running mean: 2.243776\n",
      "ep 3052: ep_len:571 episode reward: total was 47.160000. running mean: 2.692938\n",
      "ep 3052: ep_len:531 episode reward: total was 11.000000. running mean: 2.776009\n",
      "epsilon:0.064650 episode_count: 21371. steps_count: 9241557.000000\n",
      "Time elapsed:  27654.77726149559\n",
      "ep 3053: ep_len:648 episode reward: total was -55.120000. running mean: 2.197049\n",
      "ep 3053: ep_len:500 episode reward: total was 37.280000. running mean: 2.547878\n",
      "ep 3053: ep_len:576 episode reward: total was -25.340000. running mean: 2.268999\n",
      "ep 3053: ep_len:500 episode reward: total was -6.450000. running mean: 2.181809\n",
      "ep 3053: ep_len:113 episode reward: total was 34.250000. running mean: 2.502491\n",
      "ep 3053: ep_len:500 episode reward: total was -10.920000. running mean: 2.368266\n",
      "ep 3053: ep_len:574 episode reward: total was -4.020000. running mean: 2.304384\n",
      "epsilon:0.064606 episode_count: 21378. steps_count: 9244968.000000\n",
      "Time elapsed:  27680.80869960785\n",
      "ep 3054: ep_len:539 episode reward: total was -43.150000. running mean: 1.849840\n",
      "ep 3054: ep_len:500 episode reward: total was 56.240000. running mean: 2.393741\n",
      "ep 3054: ep_len:597 episode reward: total was -16.800000. running mean: 2.201804\n",
      "ep 3054: ep_len:380 episode reward: total was 1.470000. running mean: 2.194486\n",
      "ep 3054: ep_len:3 episode reward: total was 1.010000. running mean: 2.182641\n",
      "ep 3054: ep_len:520 episode reward: total was -39.370000. running mean: 1.767115\n",
      "ep 3054: ep_len:501 episode reward: total was 8.870000. running mean: 1.838144\n",
      "epsilon:0.064562 episode_count: 21385. steps_count: 9248008.000000\n",
      "Time elapsed:  27689.831426382065\n",
      "ep 3055: ep_len:229 episode reward: total was -47.810000. running mean: 1.341662\n",
      "ep 3055: ep_len:500 episode reward: total was -1.110000. running mean: 1.317146\n",
      "ep 3055: ep_len:765 episode reward: total was -157.780000. running mean: -0.273826\n",
      "ep 3055: ep_len:512 episode reward: total was 7.590000. running mean: -0.195188\n",
      "ep 3055: ep_len:3 episode reward: total was -0.490000. running mean: -0.198136\n",
      "ep 3055: ep_len:500 episode reward: total was 1.630000. running mean: -0.179854\n",
      "ep 3055: ep_len:500 episode reward: total was 41.310000. running mean: 0.235044\n",
      "epsilon:0.064517 episode_count: 21392. steps_count: 9251017.000000\n",
      "Time elapsed:  27698.975400686264\n",
      "ep 3056: ep_len:573 episode reward: total was -28.780000. running mean: -0.055106\n",
      "ep 3056: ep_len:585 episode reward: total was 10.890000. running mean: 0.054345\n",
      "ep 3056: ep_len:377 episode reward: total was 38.480000. running mean: 0.438601\n",
      "ep 3056: ep_len:160 episode reward: total was 23.130000. running mean: 0.665515\n",
      "ep 3056: ep_len:54 episode reward: total was 21.000000. running mean: 0.868860\n",
      "ep 3056: ep_len:500 episode reward: total was -37.250000. running mean: 0.487672\n",
      "ep 3056: ep_len:534 episode reward: total was 6.690000. running mean: 0.549695\n",
      "epsilon:0.064473 episode_count: 21399. steps_count: 9253800.000000\n",
      "Time elapsed:  27714.114449501038\n",
      "ep 3057: ep_len:539 episode reward: total was -46.250000. running mean: 0.081698\n",
      "ep 3057: ep_len:500 episode reward: total was -17.480000. running mean: -0.093919\n",
      "ep 3057: ep_len:660 episode reward: total was 46.630000. running mean: 0.373320\n",
      "ep 3057: ep_len:511 episode reward: total was 45.010000. running mean: 0.819687\n",
      "ep 3057: ep_len:107 episode reward: total was 27.270000. running mean: 1.084190\n",
      "ep 3057: ep_len:500 episode reward: total was -23.320000. running mean: 0.840148\n",
      "ep 3057: ep_len:500 episode reward: total was -26.840000. running mean: 0.563347\n",
      "epsilon:0.064429 episode_count: 21406. steps_count: 9257117.000000\n",
      "Time elapsed:  27724.128816843033\n",
      "ep 3058: ep_len:626 episode reward: total was -35.480000. running mean: 0.202913\n",
      "ep 3058: ep_len:383 episode reward: total was -46.650000. running mean: -0.265616\n",
      "ep 3058: ep_len:500 episode reward: total was 11.890000. running mean: -0.144060\n",
      "ep 3058: ep_len:521 episode reward: total was -10.620000. running mean: -0.248819\n",
      "ep 3058: ep_len:109 episode reward: total was 27.750000. running mean: 0.031169\n",
      "ep 3058: ep_len:616 episode reward: total was 6.670000. running mean: 0.097557\n",
      "ep 3058: ep_len:603 episode reward: total was 37.260000. running mean: 0.469182\n",
      "epsilon:0.064384 episode_count: 21413. steps_count: 9260475.000000\n",
      "Time elapsed:  27733.29529953003\n",
      "ep 3059: ep_len:606 episode reward: total was 67.720000. running mean: 1.141690\n",
      "ep 3059: ep_len:500 episode reward: total was 25.930000. running mean: 1.389573\n",
      "ep 3059: ep_len:562 episode reward: total was 16.550000. running mean: 1.541177\n",
      "ep 3059: ep_len:552 episode reward: total was 55.630000. running mean: 2.082065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3059: ep_len:3 episode reward: total was 1.010000. running mean: 2.071345\n",
      "ep 3059: ep_len:589 episode reward: total was -4.160000. running mean: 2.009031\n",
      "ep 3059: ep_len:181 episode reward: total was 7.080000. running mean: 2.059741\n",
      "epsilon:0.064340 episode_count: 21420. steps_count: 9263468.000000\n",
      "Time elapsed:  27749.0329310894\n",
      "ep 3060: ep_len:205 episode reward: total was -3.870000. running mean: 2.000444\n",
      "ep 3060: ep_len:547 episode reward: total was -56.260000. running mean: 1.417839\n",
      "ep 3060: ep_len:579 episode reward: total was 29.110000. running mean: 1.694761\n",
      "ep 3060: ep_len:500 episode reward: total was 45.850000. running mean: 2.136313\n",
      "ep 3060: ep_len:3 episode reward: total was 1.010000. running mean: 2.125050\n",
      "ep 3060: ep_len:558 episode reward: total was 3.490000. running mean: 2.138700\n",
      "ep 3060: ep_len:326 episode reward: total was 3.550000. running mean: 2.152813\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.064296 episode_count: 21427. steps_count: 9266186.000000\n",
      "Time elapsed:  27765.265273094177\n",
      "ep 3061: ep_len:583 episode reward: total was 73.440000. running mean: 2.865684\n",
      "ep 3061: ep_len:527 episode reward: total was 23.660000. running mean: 3.073628\n",
      "ep 3061: ep_len:580 episode reward: total was -31.320000. running mean: 2.729691\n",
      "ep 3061: ep_len:384 episode reward: total was 18.890000. running mean: 2.891294\n",
      "ep 3061: ep_len:3 episode reward: total was 1.010000. running mean: 2.872481\n",
      "ep 3061: ep_len:530 episode reward: total was 17.730000. running mean: 3.021057\n",
      "ep 3061: ep_len:540 episode reward: total was -34.350000. running mean: 2.647346\n",
      "epsilon:0.064251 episode_count: 21434. steps_count: 9269333.000000\n",
      "Time elapsed:  27773.58926653862\n",
      "ep 3062: ep_len:633 episode reward: total was 21.680000. running mean: 2.837673\n",
      "ep 3062: ep_len:500 episode reward: total was 53.720000. running mean: 3.346496\n",
      "ep 3062: ep_len:597 episode reward: total was 33.470000. running mean: 3.647731\n",
      "ep 3062: ep_len:520 episode reward: total was 50.860000. running mean: 4.119854\n",
      "ep 3062: ep_len:3 episode reward: total was 1.010000. running mean: 4.088755\n",
      "ep 3062: ep_len:573 episode reward: total was -18.930000. running mean: 3.858568\n",
      "ep 3062: ep_len:568 episode reward: total was -29.390000. running mean: 3.526082\n",
      "epsilon:0.064207 episode_count: 21441. steps_count: 9272727.000000\n",
      "Time elapsed:  27791.84613418579\n",
      "ep 3063: ep_len:557 episode reward: total was 40.150000. running mean: 3.892321\n",
      "ep 3063: ep_len:593 episode reward: total was -96.770000. running mean: 2.885698\n",
      "ep 3063: ep_len:513 episode reward: total was -18.230000. running mean: 2.674541\n",
      "ep 3063: ep_len:501 episode reward: total was 54.400000. running mean: 3.191795\n",
      "ep 3063: ep_len:3 episode reward: total was 1.010000. running mean: 3.169978\n",
      "ep 3063: ep_len:158 episode reward: total was 18.020000. running mean: 3.318478\n",
      "ep 3063: ep_len:320 episode reward: total was 14.020000. running mean: 3.425493\n",
      "epsilon:0.064163 episode_count: 21448. steps_count: 9275372.000000\n",
      "Time elapsed:  27799.57860159874\n",
      "ep 3064: ep_len:501 episode reward: total was -29.340000. running mean: 3.097838\n",
      "ep 3064: ep_len:517 episode reward: total was 24.070000. running mean: 3.307560\n",
      "ep 3064: ep_len:500 episode reward: total was -47.640000. running mean: 2.798084\n",
      "ep 3064: ep_len:511 episode reward: total was 12.510000. running mean: 2.895203\n",
      "ep 3064: ep_len:96 episode reward: total was -65.730000. running mean: 2.208951\n",
      "ep 3064: ep_len:547 episode reward: total was -20.970000. running mean: 1.977162\n",
      "ep 3064: ep_len:569 episode reward: total was 18.080000. running mean: 2.138190\n",
      "epsilon:0.064118 episode_count: 21455. steps_count: 9278613.000000\n",
      "Time elapsed:  27812.843948841095\n",
      "ep 3065: ep_len:528 episode reward: total was 15.460000. running mean: 2.271408\n",
      "ep 3065: ep_len:186 episode reward: total was 3.890000. running mean: 2.287594\n",
      "ep 3065: ep_len:557 episode reward: total was 26.590000. running mean: 2.530618\n",
      "ep 3065: ep_len:505 episode reward: total was 5.730000. running mean: 2.562612\n",
      "ep 3065: ep_len:3 episode reward: total was 1.010000. running mean: 2.547086\n",
      "ep 3065: ep_len:617 episode reward: total was -73.410000. running mean: 1.787515\n",
      "ep 3065: ep_len:598 episode reward: total was 2.530000. running mean: 1.794940\n",
      "epsilon:0.064074 episode_count: 21462. steps_count: 9281607.000000\n",
      "Time elapsed:  27826.20111489296\n",
      "ep 3066: ep_len:568 episode reward: total was 64.600000. running mean: 2.422990\n",
      "ep 3066: ep_len:522 episode reward: total was 0.010000. running mean: 2.398861\n",
      "ep 3066: ep_len:500 episode reward: total was 8.570000. running mean: 2.460572\n",
      "ep 3066: ep_len:516 episode reward: total was 10.070000. running mean: 2.536666\n",
      "ep 3066: ep_len:126 episode reward: total was 21.310000. running mean: 2.724400\n",
      "ep 3066: ep_len:520 episode reward: total was 4.760000. running mean: 2.744756\n",
      "ep 3066: ep_len:524 episode reward: total was 8.330000. running mean: 2.800608\n",
      "epsilon:0.064030 episode_count: 21469. steps_count: 9284883.000000\n",
      "Time elapsed:  27834.81563925743\n",
      "ep 3067: ep_len:554 episode reward: total was 40.180000. running mean: 3.174402\n",
      "ep 3067: ep_len:500 episode reward: total was 3.320000. running mean: 3.175858\n",
      "ep 3067: ep_len:648 episode reward: total was -31.030000. running mean: 2.833799\n",
      "ep 3067: ep_len:503 episode reward: total was -31.730000. running mean: 2.488161\n",
      "ep 3067: ep_len:3 episode reward: total was 1.010000. running mean: 2.473380\n",
      "ep 3067: ep_len:500 episode reward: total was -0.500000. running mean: 2.443646\n",
      "ep 3067: ep_len:558 episode reward: total was -15.950000. running mean: 2.259709\n",
      "epsilon:0.063985 episode_count: 21476. steps_count: 9288149.000000\n",
      "Time elapsed:  27844.02207660675\n",
      "ep 3068: ep_len:537 episode reward: total was 67.550000. running mean: 2.912612\n",
      "ep 3068: ep_len:581 episode reward: total was 78.750000. running mean: 3.670986\n",
      "ep 3068: ep_len:79 episode reward: total was 7.300000. running mean: 3.707276\n",
      "ep 3068: ep_len:514 episode reward: total was -0.920000. running mean: 3.661004\n",
      "ep 3068: ep_len:3 episode reward: total was 1.010000. running mean: 3.634494\n",
      "ep 3068: ep_len:513 episode reward: total was 36.510000. running mean: 3.963249\n",
      "ep 3068: ep_len:500 episode reward: total was -1.170000. running mean: 3.911916\n",
      "epsilon:0.063941 episode_count: 21483. steps_count: 9290876.000000\n",
      "Time elapsed:  27851.0216588974\n",
      "ep 3069: ep_len:553 episode reward: total was 14.100000. running mean: 4.013797\n",
      "ep 3069: ep_len:545 episode reward: total was 91.320000. running mean: 4.886859\n",
      "ep 3069: ep_len:635 episode reward: total was -18.820000. running mean: 4.649790\n",
      "ep 3069: ep_len:500 episode reward: total was 18.880000. running mean: 4.792093\n",
      "ep 3069: ep_len:32 episode reward: total was 15.510000. running mean: 4.899272\n",
      "ep 3069: ep_len:145 episode reward: total was 20.980000. running mean: 5.060079\n",
      "ep 3069: ep_len:594 episode reward: total was -16.010000. running mean: 4.849378\n",
      "epsilon:0.063897 episode_count: 21490. steps_count: 9293880.000000\n",
      "Time elapsed:  27865.468513011932\n",
      "ep 3070: ep_len:127 episode reward: total was 13.940000. running mean: 4.940284\n",
      "ep 3070: ep_len:525 episode reward: total was 19.940000. running mean: 5.090281\n",
      "ep 3070: ep_len:500 episode reward: total was 7.530000. running mean: 5.114679\n",
      "ep 3070: ep_len:500 episode reward: total was 4.190000. running mean: 5.105432\n",
      "ep 3070: ep_len:76 episode reward: total was 10.730000. running mean: 5.161678\n",
      "ep 3070: ep_len:521 episode reward: total was 7.410000. running mean: 5.184161\n",
      "ep 3070: ep_len:500 episode reward: total was -10.000000. running mean: 5.032319\n",
      "epsilon:0.063852 episode_count: 21497. steps_count: 9296629.000000\n",
      "Time elapsed:  27877.50532245636\n",
      "ep 3071: ep_len:500 episode reward: total was 25.080000. running mean: 5.232796\n",
      "ep 3071: ep_len:272 episode reward: total was -2.330000. running mean: 5.157168\n",
      "ep 3071: ep_len:629 episode reward: total was 19.160000. running mean: 5.297196\n",
      "ep 3071: ep_len:548 episode reward: total was 34.080000. running mean: 5.585024\n",
      "ep 3071: ep_len:3 episode reward: total was -1.500000. running mean: 5.514174\n",
      "ep 3071: ep_len:502 episode reward: total was -22.540000. running mean: 5.233632\n",
      "ep 3071: ep_len:507 episode reward: total was 28.260000. running mean: 5.463896\n",
      "epsilon:0.063808 episode_count: 21504. steps_count: 9299590.000000\n",
      "Time elapsed:  27885.418307304382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3072: ep_len:500 episode reward: total was -65.440000. running mean: 4.754857\n",
      "ep 3072: ep_len:500 episode reward: total was 68.660000. running mean: 5.393909\n",
      "ep 3072: ep_len:377 episode reward: total was 47.450000. running mean: 5.814469\n",
      "ep 3072: ep_len:505 episode reward: total was -16.060000. running mean: 5.595725\n",
      "ep 3072: ep_len:3 episode reward: total was -1.500000. running mean: 5.524767\n",
      "ep 3072: ep_len:500 episode reward: total was 24.960000. running mean: 5.719120\n",
      "ep 3072: ep_len:280 episode reward: total was -82.420000. running mean: 4.837729\n",
      "epsilon:0.063764 episode_count: 21511. steps_count: 9302255.000000\n",
      "Time elapsed:  27892.627790927887\n",
      "ep 3073: ep_len:589 episode reward: total was 74.880000. running mean: 5.538151\n",
      "ep 3073: ep_len:607 episode reward: total was 47.120000. running mean: 5.953970\n",
      "ep 3073: ep_len:500 episode reward: total was -16.580000. running mean: 5.728630\n",
      "ep 3073: ep_len:500 episode reward: total was 7.730000. running mean: 5.748644\n",
      "ep 3073: ep_len:3 episode reward: total was -0.490000. running mean: 5.686257\n",
      "ep 3073: ep_len:557 episode reward: total was 24.100000. running mean: 5.870395\n",
      "ep 3073: ep_len:595 episode reward: total was 21.050000. running mean: 6.022191\n",
      "epsilon:0.063719 episode_count: 21518. steps_count: 9305606.000000\n",
      "Time elapsed:  27905.954686641693\n",
      "ep 3074: ep_len:262 episode reward: total was 15.390000. running mean: 6.115869\n",
      "ep 3074: ep_len:500 episode reward: total was 21.930000. running mean: 6.274010\n",
      "ep 3074: ep_len:633 episode reward: total was -4.200000. running mean: 6.169270\n",
      "ep 3074: ep_len:386 episode reward: total was 14.020000. running mean: 6.247777\n",
      "ep 3074: ep_len:3 episode reward: total was 1.010000. running mean: 6.195400\n",
      "ep 3074: ep_len:688 episode reward: total was 40.770000. running mean: 6.541146\n",
      "ep 3074: ep_len:500 episode reward: total was -29.510000. running mean: 6.180634\n",
      "epsilon:0.063675 episode_count: 21525. steps_count: 9308578.000000\n",
      "Time elapsed:  27911.85141324997\n",
      "ep 3075: ep_len:542 episode reward: total was 9.060000. running mean: 6.209428\n",
      "ep 3075: ep_len:288 episode reward: total was -162.010000. running mean: 4.527234\n",
      "ep 3075: ep_len:636 episode reward: total was -91.510000. running mean: 3.566861\n",
      "ep 3075: ep_len:587 episode reward: total was 36.770000. running mean: 3.898893\n",
      "ep 3075: ep_len:1 episode reward: total was -1.000000. running mean: 3.849904\n",
      "ep 3075: ep_len:574 episode reward: total was -40.250000. running mean: 3.408905\n",
      "ep 3075: ep_len:591 episode reward: total was 7.910000. running mean: 3.453916\n",
      "epsilon:0.063631 episode_count: 21532. steps_count: 9311797.000000\n",
      "Time elapsed:  27916.807466745377\n",
      "ep 3076: ep_len:581 episode reward: total was -30.870000. running mean: 3.110676\n",
      "ep 3076: ep_len:500 episode reward: total was 84.420000. running mean: 3.923770\n",
      "ep 3076: ep_len:633 episode reward: total was 3.890000. running mean: 3.923432\n",
      "ep 3076: ep_len:500 episode reward: total was 8.890000. running mean: 3.973098\n",
      "ep 3076: ep_len:3 episode reward: total was 1.010000. running mean: 3.943467\n",
      "ep 3076: ep_len:515 episode reward: total was -7.250000. running mean: 3.831532\n",
      "ep 3076: ep_len:500 episode reward: total was 5.610000. running mean: 3.849317\n",
      "epsilon:0.063586 episode_count: 21539. steps_count: 9315029.000000\n",
      "Time elapsed:  27922.886729717255\n",
      "ep 3077: ep_len:236 episode reward: total was -79.180000. running mean: 3.019024\n",
      "ep 3077: ep_len:509 episode reward: total was -19.550000. running mean: 2.793333\n",
      "ep 3077: ep_len:664 episode reward: total was -11.760000. running mean: 2.647800\n",
      "ep 3077: ep_len:500 episode reward: total was 22.080000. running mean: 2.842122\n",
      "ep 3077: ep_len:111 episode reward: total was 25.870000. running mean: 3.072401\n",
      "ep 3077: ep_len:599 episode reward: total was -1.260000. running mean: 3.029077\n",
      "ep 3077: ep_len:196 episode reward: total was -47.340000. running mean: 2.525386\n",
      "epsilon:0.063542 episode_count: 21546. steps_count: 9317844.000000\n",
      "Time elapsed:  27928.66489672661\n",
      "ep 3078: ep_len:539 episode reward: total was 28.680000. running mean: 2.786932\n",
      "ep 3078: ep_len:500 episode reward: total was 23.820000. running mean: 2.997263\n",
      "ep 3078: ep_len:623 episode reward: total was -1.010000. running mean: 2.957190\n",
      "ep 3078: ep_len:591 episode reward: total was 52.230000. running mean: 3.449918\n",
      "ep 3078: ep_len:99 episode reward: total was 26.760000. running mean: 3.683019\n",
      "ep 3078: ep_len:526 episode reward: total was -12.960000. running mean: 3.516589\n",
      "ep 3078: ep_len:599 episode reward: total was 47.800000. running mean: 3.959423\n",
      "epsilon:0.063498 episode_count: 21553. steps_count: 9321321.000000\n",
      "Time elapsed:  27943.05282974243\n",
      "ep 3079: ep_len:585 episode reward: total was 27.830000. running mean: 4.198129\n",
      "ep 3079: ep_len:500 episode reward: total was 81.180000. running mean: 4.967948\n",
      "ep 3079: ep_len:649 episode reward: total was 8.290000. running mean: 5.001168\n",
      "ep 3079: ep_len:515 episode reward: total was 31.990000. running mean: 5.271056\n",
      "ep 3079: ep_len:3 episode reward: total was 1.010000. running mean: 5.228446\n",
      "ep 3079: ep_len:535 episode reward: total was 35.020000. running mean: 5.526361\n",
      "ep 3079: ep_len:616 episode reward: total was -20.240000. running mean: 5.268698\n",
      "epsilon:0.063453 episode_count: 21560. steps_count: 9324724.000000\n",
      "Time elapsed:  27955.21882748604\n",
      "ep 3080: ep_len:532 episode reward: total was -36.280000. running mean: 4.853211\n",
      "ep 3080: ep_len:543 episode reward: total was -57.060000. running mean: 4.234079\n",
      "ep 3080: ep_len:612 episode reward: total was -26.270000. running mean: 3.929038\n",
      "ep 3080: ep_len:566 episode reward: total was 59.910000. running mean: 4.488847\n",
      "ep 3080: ep_len:91 episode reward: total was -36.730000. running mean: 4.076659\n",
      "ep 3080: ep_len:539 episode reward: total was -26.230000. running mean: 3.773592\n",
      "ep 3080: ep_len:656 episode reward: total was -49.830000. running mean: 3.237556\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.063409 episode_count: 21567. steps_count: 9328263.000000\n",
      "Time elapsed:  27969.25933933258\n",
      "ep 3081: ep_len:647 episode reward: total was -44.940000. running mean: 2.755781\n",
      "ep 3081: ep_len:500 episode reward: total was 49.440000. running mean: 3.222623\n",
      "ep 3081: ep_len:548 episode reward: total was 2.850000. running mean: 3.218897\n",
      "ep 3081: ep_len:518 episode reward: total was -0.730000. running mean: 3.179408\n",
      "ep 3081: ep_len:50 episode reward: total was 19.000000. running mean: 3.337614\n",
      "ep 3081: ep_len:545 episode reward: total was 19.840000. running mean: 3.502638\n",
      "ep 3081: ep_len:570 episode reward: total was 0.780000. running mean: 3.475411\n",
      "epsilon:0.063365 episode_count: 21574. steps_count: 9331641.000000\n",
      "Time elapsed:  27978.256138324738\n",
      "ep 3082: ep_len:605 episode reward: total was 84.850000. running mean: 4.289157\n",
      "ep 3082: ep_len:514 episode reward: total was -19.320000. running mean: 4.053066\n",
      "ep 3082: ep_len:383 episode reward: total was 28.410000. running mean: 4.296635\n",
      "ep 3082: ep_len:503 episode reward: total was 23.140000. running mean: 4.485069\n",
      "ep 3082: ep_len:3 episode reward: total was 1.010000. running mean: 4.450318\n",
      "ep 3082: ep_len:507 episode reward: total was -46.900000. running mean: 3.936815\n",
      "ep 3082: ep_len:608 episode reward: total was 14.110000. running mean: 4.038547\n",
      "epsilon:0.063320 episode_count: 21581. steps_count: 9334764.000000\n",
      "Time elapsed:  27990.172417640686\n",
      "ep 3083: ep_len:512 episode reward: total was -36.210000. running mean: 3.636061\n",
      "ep 3083: ep_len:595 episode reward: total was -12.350000. running mean: 3.476201\n",
      "ep 3083: ep_len:500 episode reward: total was -8.780000. running mean: 3.353639\n",
      "ep 3083: ep_len:585 episode reward: total was 37.640000. running mean: 3.696502\n",
      "ep 3083: ep_len:3 episode reward: total was 1.010000. running mean: 3.669637\n",
      "ep 3083: ep_len:578 episode reward: total was -29.700000. running mean: 3.335941\n",
      "ep 3083: ep_len:586 episode reward: total was 33.200000. running mean: 3.634581\n",
      "epsilon:0.063276 episode_count: 21588. steps_count: 9338123.000000\n",
      "Time elapsed:  27998.958308935165\n",
      "ep 3084: ep_len:257 episode reward: total was -2.720000. running mean: 3.571036\n",
      "ep 3084: ep_len:249 episode reward: total was -134.750000. running mean: 2.187825\n",
      "ep 3084: ep_len:642 episode reward: total was -30.970000. running mean: 1.856247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3084: ep_len:544 episode reward: total was 21.640000. running mean: 2.054084\n",
      "ep 3084: ep_len:52 episode reward: total was 21.500000. running mean: 2.248544\n",
      "ep 3084: ep_len:561 episode reward: total was 1.420000. running mean: 2.240258\n",
      "ep 3084: ep_len:537 episode reward: total was -47.220000. running mean: 1.745656\n",
      "epsilon:0.063232 episode_count: 21595. steps_count: 9340965.000000\n",
      "Time elapsed:  28006.50845360756\n",
      "ep 3085: ep_len:643 episode reward: total was 26.530000. running mean: 1.993499\n",
      "ep 3085: ep_len:550 episode reward: total was 8.320000. running mean: 2.056764\n",
      "ep 3085: ep_len:79 episode reward: total was 3.260000. running mean: 2.068796\n",
      "ep 3085: ep_len:500 episode reward: total was -18.670000. running mean: 1.861408\n",
      "ep 3085: ep_len:97 episode reward: total was 24.750000. running mean: 2.090294\n",
      "ep 3085: ep_len:500 episode reward: total was -6.520000. running mean: 2.004191\n",
      "ep 3085: ep_len:286 episode reward: total was -26.740000. running mean: 1.716750\n",
      "epsilon:0.063187 episode_count: 21602. steps_count: 9343620.000000\n",
      "Time elapsed:  28018.203098535538\n",
      "ep 3086: ep_len:229 episode reward: total was 7.670000. running mean: 1.776282\n",
      "ep 3086: ep_len:626 episode reward: total was 41.720000. running mean: 2.175719\n",
      "ep 3086: ep_len:73 episode reward: total was 4.760000. running mean: 2.201562\n",
      "ep 3086: ep_len:612 episode reward: total was 47.280000. running mean: 2.652346\n",
      "ep 3086: ep_len:82 episode reward: total was -26.290000. running mean: 2.362923\n",
      "ep 3086: ep_len:607 episode reward: total was -7.500000. running mean: 2.264294\n",
      "ep 3086: ep_len:630 episode reward: total was -26.350000. running mean: 1.978151\n",
      "epsilon:0.063143 episode_count: 21609. steps_count: 9346479.000000\n",
      "Time elapsed:  28029.324105024338\n",
      "ep 3087: ep_len:134 episode reward: total was 3.510000. running mean: 1.993469\n",
      "ep 3087: ep_len:517 episode reward: total was 19.690000. running mean: 2.170435\n",
      "ep 3087: ep_len:669 episode reward: total was -30.080000. running mean: 1.847930\n",
      "ep 3087: ep_len:131 episode reward: total was 5.620000. running mean: 1.885651\n",
      "ep 3087: ep_len:3 episode reward: total was 1.010000. running mean: 1.876894\n",
      "ep 3087: ep_len:513 episode reward: total was -29.130000. running mean: 1.566825\n",
      "ep 3087: ep_len:572 episode reward: total was -3.640000. running mean: 1.514757\n",
      "epsilon:0.063099 episode_count: 21616. steps_count: 9349018.000000\n",
      "Time elapsed:  28036.129950284958\n",
      "ep 3088: ep_len:578 episode reward: total was 27.570000. running mean: 1.775310\n",
      "ep 3088: ep_len:500 episode reward: total was 32.070000. running mean: 2.078257\n",
      "ep 3088: ep_len:536 episode reward: total was -2.570000. running mean: 2.031774\n",
      "ep 3088: ep_len:516 episode reward: total was -14.120000. running mean: 1.870256\n",
      "ep 3088: ep_len:95 episode reward: total was 32.140000. running mean: 2.172954\n",
      "ep 3088: ep_len:539 episode reward: total was -5.700000. running mean: 2.094224\n",
      "ep 3088: ep_len:535 episode reward: total was 12.590000. running mean: 2.199182\n",
      "epsilon:0.063054 episode_count: 21623. steps_count: 9352317.000000\n",
      "Time elapsed:  28044.840393304825\n",
      "ep 3089: ep_len:539 episode reward: total was -19.100000. running mean: 1.986190\n",
      "ep 3089: ep_len:611 episode reward: total was 22.120000. running mean: 2.187528\n",
      "ep 3089: ep_len:500 episode reward: total was 11.720000. running mean: 2.282853\n",
      "ep 3089: ep_len:56 episode reward: total was 2.320000. running mean: 2.283224\n",
      "ep 3089: ep_len:87 episode reward: total was 20.610000. running mean: 2.466492\n",
      "ep 3089: ep_len:538 episode reward: total was -20.600000. running mean: 2.235827\n",
      "ep 3089: ep_len:522 episode reward: total was -26.250000. running mean: 1.950969\n",
      "epsilon:0.063010 episode_count: 21630. steps_count: 9355170.000000\n",
      "Time elapsed:  28056.538366556168\n",
      "ep 3090: ep_len:630 episode reward: total was -57.020000. running mean: 1.361259\n",
      "ep 3090: ep_len:585 episode reward: total was 32.470000. running mean: 1.672347\n",
      "ep 3090: ep_len:500 episode reward: total was 17.900000. running mean: 1.834623\n",
      "ep 3090: ep_len:500 episode reward: total was -30.090000. running mean: 1.515377\n",
      "ep 3090: ep_len:75 episode reward: total was 19.260000. running mean: 1.692823\n",
      "ep 3090: ep_len:500 episode reward: total was -12.130000. running mean: 1.554595\n",
      "ep 3090: ep_len:182 episode reward: total was -3.660000. running mean: 1.502449\n",
      "epsilon:0.062966 episode_count: 21637. steps_count: 9358142.000000\n",
      "Time elapsed:  28064.917734384537\n",
      "ep 3091: ep_len:636 episode reward: total was -68.130000. running mean: 0.806124\n",
      "ep 3091: ep_len:507 episode reward: total was -13.270000. running mean: 0.665363\n",
      "ep 3091: ep_len:662 episode reward: total was -3.030000. running mean: 0.628410\n",
      "ep 3091: ep_len:611 episode reward: total was 31.300000. running mean: 0.935126\n",
      "ep 3091: ep_len:99 episode reward: total was 23.240000. running mean: 1.158174\n",
      "ep 3091: ep_len:671 episode reward: total was 8.440000. running mean: 1.230993\n",
      "ep 3091: ep_len:517 episode reward: total was -21.250000. running mean: 1.006183\n",
      "epsilon:0.062921 episode_count: 21644. steps_count: 9361845.000000\n",
      "Time elapsed:  28074.470347881317\n",
      "ep 3092: ep_len:593 episode reward: total was 73.270000. running mean: 1.728821\n",
      "ep 3092: ep_len:282 episode reward: total was -15.240000. running mean: 1.559133\n",
      "ep 3092: ep_len:596 episode reward: total was -55.190000. running mean: 0.991641\n",
      "ep 3092: ep_len:557 episode reward: total was 61.710000. running mean: 1.598825\n",
      "ep 3092: ep_len:3 episode reward: total was 1.010000. running mean: 1.592937\n",
      "ep 3092: ep_len:500 episode reward: total was 20.280000. running mean: 1.779807\n",
      "ep 3092: ep_len:500 episode reward: total was -36.140000. running mean: 1.400609\n",
      "epsilon:0.062877 episode_count: 21651. steps_count: 9364876.000000\n",
      "Time elapsed:  28086.95999288559\n",
      "ep 3093: ep_len:543 episode reward: total was -34.140000. running mean: 1.045203\n",
      "ep 3093: ep_len:500 episode reward: total was 5.430000. running mean: 1.089051\n",
      "ep 3093: ep_len:72 episode reward: total was 12.280000. running mean: 1.200961\n",
      "ep 3093: ep_len:574 episode reward: total was 47.100000. running mean: 1.659951\n",
      "ep 3093: ep_len:26 episode reward: total was 8.500000. running mean: 1.728351\n",
      "ep 3093: ep_len:500 episode reward: total was 17.460000. running mean: 1.885668\n",
      "ep 3093: ep_len:594 episode reward: total was 21.050000. running mean: 2.077311\n",
      "epsilon:0.062833 episode_count: 21658. steps_count: 9367685.000000\n",
      "Time elapsed:  28099.00895524025\n",
      "ep 3094: ep_len:558 episode reward: total was -13.440000. running mean: 1.922138\n",
      "ep 3094: ep_len:500 episode reward: total was 43.240000. running mean: 2.335317\n",
      "ep 3094: ep_len:676 episode reward: total was -14.430000. running mean: 2.167664\n",
      "ep 3094: ep_len:119 episode reward: total was 12.890000. running mean: 2.274887\n",
      "ep 3094: ep_len:99 episode reward: total was 24.740000. running mean: 2.499538\n",
      "ep 3094: ep_len:500 episode reward: total was 5.690000. running mean: 2.531443\n",
      "ep 3094: ep_len:509 episode reward: total was -8.380000. running mean: 2.422328\n",
      "epsilon:0.062788 episode_count: 21665. steps_count: 9370646.000000\n",
      "Time elapsed:  28106.946728229523\n",
      "ep 3095: ep_len:501 episode reward: total was 72.930000. running mean: 3.127405\n",
      "ep 3095: ep_len:573 episode reward: total was 22.550000. running mean: 3.321631\n",
      "ep 3095: ep_len:514 episode reward: total was 24.030000. running mean: 3.528715\n",
      "ep 3095: ep_len:551 episode reward: total was -153.820000. running mean: 1.955227\n",
      "ep 3095: ep_len:3 episode reward: total was 1.010000. running mean: 1.945775\n",
      "ep 3095: ep_len:561 episode reward: total was -20.960000. running mean: 1.716717\n",
      "ep 3095: ep_len:336 episode reward: total was 13.420000. running mean: 1.833750\n",
      "epsilon:0.062744 episode_count: 21672. steps_count: 9373685.000000\n",
      "Time elapsed:  28120.10165500641\n",
      "ep 3096: ep_len:500 episode reward: total was -28.340000. running mean: 1.532013\n",
      "ep 3096: ep_len:594 episode reward: total was 100.210000. running mean: 2.518793\n",
      "ep 3096: ep_len:500 episode reward: total was 16.270000. running mean: 2.656305\n",
      "ep 3096: ep_len:506 episode reward: total was -8.630000. running mean: 2.543442\n",
      "ep 3096: ep_len:3 episode reward: total was 0.000000. running mean: 2.518007\n",
      "ep 3096: ep_len:661 episode reward: total was 21.540000. running mean: 2.708227\n",
      "ep 3096: ep_len:573 episode reward: total was 26.630000. running mean: 2.947445\n",
      "epsilon:0.062700 episode_count: 21679. steps_count: 9377022.000000\n",
      "Time elapsed:  28128.982887744904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3097: ep_len:203 episode reward: total was 9.120000. running mean: 3.009170\n",
      "ep 3097: ep_len:505 episode reward: total was 2.300000. running mean: 3.002079\n",
      "ep 3097: ep_len:500 episode reward: total was 21.550000. running mean: 3.187558\n",
      "ep 3097: ep_len:507 episode reward: total was -1.020000. running mean: 3.145482\n",
      "ep 3097: ep_len:3 episode reward: total was 1.010000. running mean: 3.124128\n",
      "ep 3097: ep_len:525 episode reward: total was 8.800000. running mean: 3.180886\n",
      "ep 3097: ep_len:557 episode reward: total was -87.700000. running mean: 2.272077\n",
      "epsilon:0.062655 episode_count: 21686. steps_count: 9379822.000000\n",
      "Time elapsed:  28136.47144293785\n",
      "ep 3098: ep_len:652 episode reward: total was 34.850000. running mean: 2.597857\n",
      "ep 3098: ep_len:564 episode reward: total was 13.940000. running mean: 2.711278\n",
      "ep 3098: ep_len:500 episode reward: total was 4.020000. running mean: 2.724365\n",
      "ep 3098: ep_len:110 episode reward: total was 13.990000. running mean: 2.837022\n",
      "ep 3098: ep_len:3 episode reward: total was 1.010000. running mean: 2.818751\n",
      "ep 3098: ep_len:520 episode reward: total was 27.750000. running mean: 3.068064\n",
      "ep 3098: ep_len:589 episode reward: total was 37.260000. running mean: 3.409983\n",
      "epsilon:0.062611 episode_count: 21693. steps_count: 9382760.000000\n",
      "Time elapsed:  28144.482156276703\n",
      "ep 3099: ep_len:613 episode reward: total was -55.960000. running mean: 2.816283\n",
      "ep 3099: ep_len:500 episode reward: total was 10.270000. running mean: 2.890821\n",
      "ep 3099: ep_len:62 episode reward: total was 3.670000. running mean: 2.898612\n",
      "ep 3099: ep_len:500 episode reward: total was 58.900000. running mean: 3.458626\n",
      "ep 3099: ep_len:3 episode reward: total was 1.010000. running mean: 3.434140\n",
      "ep 3099: ep_len:852 episode reward: total was -260.040000. running mean: 0.799399\n",
      "ep 3099: ep_len:599 episode reward: total was 37.160000. running mean: 1.163005\n",
      "epsilon:0.062567 episode_count: 21700. steps_count: 9385889.000000\n",
      "Time elapsed:  28157.79262471199\n",
      "ep 3100: ep_len:207 episode reward: total was -21.050000. running mean: 0.940875\n",
      "ep 3100: ep_len:500 episode reward: total was 13.870000. running mean: 1.070166\n",
      "ep 3100: ep_len:561 episode reward: total was -22.190000. running mean: 0.837564\n",
      "ep 3100: ep_len:512 episode reward: total was -11.760000. running mean: 0.711589\n",
      "ep 3100: ep_len:46 episode reward: total was 20.000000. running mean: 0.904473\n",
      "ep 3100: ep_len:500 episode reward: total was 44.010000. running mean: 1.335528\n",
      "ep 3100: ep_len:195 episode reward: total was -1.990000. running mean: 1.302273\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.062522 episode_count: 21707. steps_count: 9388410.000000\n",
      "Time elapsed:  28174.66920542717\n",
      "ep 3101: ep_len:576 episode reward: total was 61.230000. running mean: 1.901550\n",
      "ep 3101: ep_len:185 episode reward: total was -15.370000. running mean: 1.728834\n",
      "ep 3101: ep_len:376 episode reward: total was 35.590000. running mean: 2.067446\n",
      "ep 3101: ep_len:501 episode reward: total was -8.160000. running mean: 1.965172\n",
      "ep 3101: ep_len:3 episode reward: total was 1.010000. running mean: 1.955620\n",
      "ep 3101: ep_len:584 episode reward: total was -5.870000. running mean: 1.877364\n",
      "ep 3101: ep_len:573 episode reward: total was -2.010000. running mean: 1.838490\n",
      "epsilon:0.062478 episode_count: 21714. steps_count: 9391208.000000\n",
      "Time elapsed:  28186.605481386185\n",
      "ep 3102: ep_len:513 episode reward: total was 1.780000. running mean: 1.837905\n",
      "ep 3102: ep_len:500 episode reward: total was -1.740000. running mean: 1.802126\n",
      "ep 3102: ep_len:584 episode reward: total was 7.500000. running mean: 1.859105\n",
      "ep 3102: ep_len:505 episode reward: total was 11.280000. running mean: 1.953314\n",
      "ep 3102: ep_len:55 episode reward: total was 23.000000. running mean: 2.163781\n",
      "ep 3102: ep_len:300 episode reward: total was 0.530000. running mean: 2.147443\n",
      "ep 3102: ep_len:616 episode reward: total was 9.580000. running mean: 2.221768\n",
      "epsilon:0.062434 episode_count: 21721. steps_count: 9394281.000000\n",
      "Time elapsed:  28194.64427089691\n",
      "ep 3103: ep_len:574 episode reward: total was 60.810000. running mean: 2.807651\n",
      "ep 3103: ep_len:500 episode reward: total was 26.710000. running mean: 3.046674\n",
      "ep 3103: ep_len:339 episode reward: total was 50.740000. running mean: 3.523607\n",
      "ep 3103: ep_len:101 episode reward: total was 16.320000. running mean: 3.651571\n",
      "ep 3103: ep_len:3 episode reward: total was 1.010000. running mean: 3.625156\n",
      "ep 3103: ep_len:547 episode reward: total was -6.860000. running mean: 3.520304\n",
      "ep 3103: ep_len:523 episode reward: total was 37.010000. running mean: 3.855201\n",
      "epsilon:0.062389 episode_count: 21728. steps_count: 9396868.000000\n",
      "Time elapsed:  28201.693540096283\n",
      "ep 3104: ep_len:129 episode reward: total was 7.990000. running mean: 3.896549\n",
      "ep 3104: ep_len:567 episode reward: total was -6.380000. running mean: 3.793784\n",
      "ep 3104: ep_len:402 episode reward: total was 45.440000. running mean: 4.210246\n",
      "ep 3104: ep_len:523 episode reward: total was 54.490000. running mean: 4.713043\n",
      "ep 3104: ep_len:54 episode reward: total was 22.500000. running mean: 4.890913\n",
      "ep 3104: ep_len:549 episode reward: total was 10.290000. running mean: 4.944904\n",
      "ep 3104: ep_len:533 episode reward: total was 0.360000. running mean: 4.899055\n",
      "epsilon:0.062345 episode_count: 21735. steps_count: 9399625.000000\n",
      "Time elapsed:  28209.17189025879\n",
      "ep 3105: ep_len:551 episode reward: total was 32.360000. running mean: 5.173664\n",
      "ep 3105: ep_len:500 episode reward: total was 26.350000. running mean: 5.385428\n",
      "ep 3105: ep_len:500 episode reward: total was 5.230000. running mean: 5.383873\n",
      "ep 3105: ep_len:607 episode reward: total was 47.870000. running mean: 5.808735\n",
      "ep 3105: ep_len:83 episode reward: total was 19.160000. running mean: 5.942247\n",
      "ep 3105: ep_len:508 episode reward: total was -4.880000. running mean: 5.834025\n",
      "ep 3105: ep_len:345 episode reward: total was -2.810000. running mean: 5.747584\n",
      "epsilon:0.062301 episode_count: 21742. steps_count: 9402719.000000\n",
      "Time elapsed:  28217.17834877968\n",
      "ep 3106: ep_len:542 episode reward: total was -47.940000. running mean: 5.210709\n",
      "ep 3106: ep_len:606 episode reward: total was 26.550000. running mean: 5.424102\n",
      "ep 3106: ep_len:500 episode reward: total was 8.490000. running mean: 5.454760\n",
      "ep 3106: ep_len:501 episode reward: total was 11.210000. running mean: 5.512313\n",
      "ep 3106: ep_len:3 episode reward: total was 1.010000. running mean: 5.467290\n",
      "ep 3106: ep_len:500 episode reward: total was 4.550000. running mean: 5.458117\n",
      "ep 3106: ep_len:309 episode reward: total was 11.940000. running mean: 5.522936\n",
      "epsilon:0.062256 episode_count: 21749. steps_count: 9405680.000000\n",
      "Time elapsed:  28224.967242479324\n",
      "ep 3107: ep_len:570 episode reward: total was 40.410000. running mean: 5.871806\n",
      "ep 3107: ep_len:500 episode reward: total was 62.420000. running mean: 6.437288\n",
      "ep 3107: ep_len:500 episode reward: total was -29.440000. running mean: 6.078515\n",
      "ep 3107: ep_len:526 episode reward: total was 7.920000. running mean: 6.096930\n",
      "ep 3107: ep_len:104 episode reward: total was 20.260000. running mean: 6.238561\n",
      "ep 3107: ep_len:316 episode reward: total was -53.830000. running mean: 5.637875\n",
      "ep 3107: ep_len:500 episode reward: total was -4.030000. running mean: 5.541197\n",
      "epsilon:0.062212 episode_count: 21756. steps_count: 9408696.000000\n",
      "Time elapsed:  28231.14219903946\n",
      "ep 3108: ep_len:573 episode reward: total was 37.080000. running mean: 5.856585\n",
      "ep 3108: ep_len:500 episode reward: total was -23.660000. running mean: 5.561419\n",
      "ep 3108: ep_len:500 episode reward: total was -5.690000. running mean: 5.448905\n",
      "ep 3108: ep_len:56 episode reward: total was 8.350000. running mean: 5.477916\n",
      "ep 3108: ep_len:49 episode reward: total was 17.000000. running mean: 5.593136\n",
      "ep 3108: ep_len:573 episode reward: total was 12.800000. running mean: 5.665205\n",
      "ep 3108: ep_len:500 episode reward: total was 44.730000. running mean: 6.055853\n",
      "epsilon:0.062168 episode_count: 21763. steps_count: 9411447.000000\n",
      "Time elapsed:  28237.430094003677\n",
      "ep 3109: ep_len:524 episode reward: total was -20.710000. running mean: 5.788194\n",
      "ep 3109: ep_len:502 episode reward: total was -67.980000. running mean: 5.050512\n",
      "ep 3109: ep_len:340 episode reward: total was 53.350000. running mean: 5.533507\n",
      "ep 3109: ep_len:567 episode reward: total was 32.280000. running mean: 5.800972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3109: ep_len:53 episode reward: total was 23.010000. running mean: 5.973063\n",
      "ep 3109: ep_len:541 episode reward: total was 6.350000. running mean: 5.976832\n",
      "ep 3109: ep_len:500 episode reward: total was 18.690000. running mean: 6.103964\n",
      "epsilon:0.062123 episode_count: 21770. steps_count: 9414474.000000\n",
      "Time elapsed:  28250.25153517723\n",
      "ep 3110: ep_len:122 episode reward: total was 4.920000. running mean: 6.092124\n",
      "ep 3110: ep_len:500 episode reward: total was 13.970000. running mean: 6.170903\n",
      "ep 3110: ep_len:500 episode reward: total was 35.090000. running mean: 6.460094\n",
      "ep 3110: ep_len:500 episode reward: total was 15.000000. running mean: 6.545493\n",
      "ep 3110: ep_len:109 episode reward: total was 9.320000. running mean: 6.573238\n",
      "ep 3110: ep_len:624 episode reward: total was -1.160000. running mean: 6.495905\n",
      "ep 3110: ep_len:500 episode reward: total was 30.190000. running mean: 6.732846\n",
      "epsilon:0.062079 episode_count: 21777. steps_count: 9417329.000000\n",
      "Time elapsed:  28257.960302591324\n",
      "ep 3111: ep_len:512 episode reward: total was 4.800000. running mean: 6.713518\n",
      "ep 3111: ep_len:500 episode reward: total was -11.900000. running mean: 6.527383\n",
      "ep 3111: ep_len:500 episode reward: total was 38.710000. running mean: 6.849209\n",
      "ep 3111: ep_len:500 episode reward: total was 23.780000. running mean: 7.018517\n",
      "ep 3111: ep_len:94 episode reward: total was -43.710000. running mean: 6.511232\n",
      "ep 3111: ep_len:599 episode reward: total was 19.500000. running mean: 6.641119\n",
      "ep 3111: ep_len:528 episode reward: total was 1.020000. running mean: 6.584908\n",
      "epsilon:0.062035 episode_count: 21784. steps_count: 9420562.000000\n",
      "Time elapsed:  28267.321098327637\n",
      "ep 3112: ep_len:600 episode reward: total was -69.960000. running mean: 5.819459\n",
      "ep 3112: ep_len:593 episode reward: total was 38.120000. running mean: 6.142465\n",
      "ep 3112: ep_len:629 episode reward: total was -72.720000. running mean: 5.353840\n",
      "ep 3112: ep_len:500 episode reward: total was 66.100000. running mean: 5.961301\n",
      "ep 3112: ep_len:3 episode reward: total was 1.010000. running mean: 5.911788\n",
      "ep 3112: ep_len:524 episode reward: total was -7.800000. running mean: 5.774671\n",
      "ep 3112: ep_len:623 episode reward: total was 0.370000. running mean: 5.720624\n",
      "epsilon:0.061990 episode_count: 21791. steps_count: 9424034.000000\n",
      "Time elapsed:  28276.832628965378\n",
      "ep 3113: ep_len:536 episode reward: total was -44.710000. running mean: 5.216318\n",
      "ep 3113: ep_len:500 episode reward: total was -54.110000. running mean: 4.623054\n",
      "ep 3113: ep_len:500 episode reward: total was -14.200000. running mean: 4.434824\n",
      "ep 3113: ep_len:500 episode reward: total was -8.960000. running mean: 4.300876\n",
      "ep 3113: ep_len:3 episode reward: total was 1.010000. running mean: 4.267967\n",
      "ep 3113: ep_len:563 episode reward: total was 39.850000. running mean: 4.623787\n",
      "ep 3113: ep_len:571 episode reward: total was 2.040000. running mean: 4.597949\n",
      "epsilon:0.061946 episode_count: 21798. steps_count: 9427207.000000\n",
      "Time elapsed:  28284.68218445778\n",
      "ep 3114: ep_len:500 episode reward: total was 35.300000. running mean: 4.904970\n",
      "ep 3114: ep_len:629 episode reward: total was -22.170000. running mean: 4.634220\n",
      "ep 3114: ep_len:646 episode reward: total was -6.290000. running mean: 4.524978\n",
      "ep 3114: ep_len:534 episode reward: total was 45.410000. running mean: 4.933828\n",
      "ep 3114: ep_len:80 episode reward: total was -20.270000. running mean: 4.681790\n",
      "ep 3114: ep_len:184 episode reward: total was 20.700000. running mean: 4.841972\n",
      "ep 3114: ep_len:354 episode reward: total was 0.620000. running mean: 4.799752\n",
      "epsilon:0.061902 episode_count: 21805. steps_count: 9430134.000000\n",
      "Time elapsed:  28292.933659791946\n",
      "ep 3115: ep_len:500 episode reward: total was 41.180000. running mean: 5.163555\n",
      "ep 3115: ep_len:500 episode reward: total was -43.600000. running mean: 4.675919\n",
      "ep 3115: ep_len:635 episode reward: total was -34.680000. running mean: 4.282360\n",
      "ep 3115: ep_len:519 episode reward: total was -241.750000. running mean: 1.822036\n",
      "ep 3115: ep_len:120 episode reward: total was 20.360000. running mean: 2.007416\n",
      "ep 3115: ep_len:500 episode reward: total was -32.350000. running mean: 1.663842\n",
      "ep 3115: ep_len:535 episode reward: total was 30.260000. running mean: 1.949803\n",
      "epsilon:0.061857 episode_count: 21812. steps_count: 9433443.000000\n",
      "Time elapsed:  28301.52580142021\n",
      "ep 3116: ep_len:114 episode reward: total was 0.430000. running mean: 1.934605\n",
      "ep 3116: ep_len:500 episode reward: total was -0.520000. running mean: 1.910059\n",
      "ep 3116: ep_len:500 episode reward: total was -11.660000. running mean: 1.774359\n",
      "ep 3116: ep_len:510 episode reward: total was -195.670000. running mean: -0.200085\n",
      "ep 3116: ep_len:88 episode reward: total was -29.770000. running mean: -0.495784\n",
      "ep 3116: ep_len:534 episode reward: total was 29.850000. running mean: -0.192326\n",
      "ep 3116: ep_len:611 episode reward: total was 20.810000. running mean: 0.017697\n",
      "epsilon:0.061813 episode_count: 21819. steps_count: 9436300.000000\n",
      "Time elapsed:  28310.206592559814\n",
      "ep 3117: ep_len:554 episode reward: total was -98.780000. running mean: -0.970280\n",
      "ep 3117: ep_len:500 episode reward: total was -1.780000. running mean: -0.978377\n",
      "ep 3117: ep_len:599 episode reward: total was -0.710000. running mean: -0.975693\n",
      "ep 3117: ep_len:500 episode reward: total was 9.660000. running mean: -0.869336\n",
      "ep 3117: ep_len:3 episode reward: total was 1.010000. running mean: -0.850543\n",
      "ep 3117: ep_len:539 episode reward: total was 23.720000. running mean: -0.604838\n",
      "ep 3117: ep_len:279 episode reward: total was -4.110000. running mean: -0.639889\n",
      "epsilon:0.061769 episode_count: 21826. steps_count: 9439274.000000\n",
      "Time elapsed:  28320.591137886047\n",
      "ep 3118: ep_len:646 episode reward: total was -14.190000. running mean: -0.775390\n",
      "ep 3118: ep_len:579 episode reward: total was 101.310000. running mean: 0.245464\n",
      "ep 3118: ep_len:592 episode reward: total was -49.420000. running mean: -0.251191\n",
      "ep 3118: ep_len:54 episode reward: total was 3.830000. running mean: -0.210379\n",
      "ep 3118: ep_len:3 episode reward: total was 1.010000. running mean: -0.198175\n",
      "ep 3118: ep_len:139 episode reward: total was 20.640000. running mean: 0.010206\n",
      "ep 3118: ep_len:298 episode reward: total was -11.680000. running mean: -0.106696\n",
      "epsilon:0.061724 episode_count: 21833. steps_count: 9441585.000000\n",
      "Time elapsed:  28330.96924304962\n",
      "ep 3119: ep_len:542 episode reward: total was 32.100000. running mean: 0.215371\n",
      "ep 3119: ep_len:509 episode reward: total was 8.140000. running mean: 0.294618\n",
      "ep 3119: ep_len:537 episode reward: total was 11.600000. running mean: 0.407671\n",
      "ep 3119: ep_len:623 episode reward: total was 22.470000. running mean: 0.628295\n",
      "ep 3119: ep_len:92 episode reward: total was 30.180000. running mean: 0.923812\n",
      "ep 3119: ep_len:530 episode reward: total was -29.830000. running mean: 0.616274\n",
      "ep 3119: ep_len:211 episode reward: total was 7.200000. running mean: 0.682111\n",
      "epsilon:0.061680 episode_count: 21840. steps_count: 9444629.000000\n",
      "Time elapsed:  28336.19046664238\n",
      "ep 3120: ep_len:265 episode reward: total was 12.390000. running mean: 0.799190\n",
      "ep 3120: ep_len:503 episode reward: total was 4.200000. running mean: 0.833198\n",
      "ep 3120: ep_len:592 episode reward: total was -27.310000. running mean: 0.551766\n",
      "ep 3120: ep_len:506 episode reward: total was 39.630000. running mean: 0.942548\n",
      "ep 3120: ep_len:3 episode reward: total was 1.010000. running mean: 0.943223\n",
      "ep 3120: ep_len:506 episode reward: total was 19.960000. running mean: 1.133391\n",
      "ep 3120: ep_len:591 episode reward: total was 2.980000. running mean: 1.151857\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.061636 episode_count: 21847. steps_count: 9447595.000000\n",
      "Time elapsed:  28348.9354095459\n",
      "ep 3121: ep_len:649 episode reward: total was -8.500000. running mean: 1.055338\n",
      "ep 3121: ep_len:615 episode reward: total was -49.780000. running mean: 0.546985\n",
      "ep 3121: ep_len:396 episode reward: total was 47.090000. running mean: 1.012415\n",
      "ep 3121: ep_len:500 episode reward: total was 46.860000. running mean: 1.470891\n",
      "ep 3121: ep_len:3 episode reward: total was -0.490000. running mean: 1.451282\n",
      "ep 3121: ep_len:260 episode reward: total was 36.570000. running mean: 1.802469\n",
      "ep 3121: ep_len:581 episode reward: total was 23.450000. running mean: 2.018944\n",
      "epsilon:0.061591 episode_count: 21854. steps_count: 9450599.000000\n",
      "Time elapsed:  28356.90987420082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3122: ep_len:530 episode reward: total was 20.080000. running mean: 2.199555\n",
      "ep 3122: ep_len:500 episode reward: total was 17.410000. running mean: 2.351659\n",
      "ep 3122: ep_len:536 episode reward: total was -10.130000. running mean: 2.226843\n",
      "ep 3122: ep_len:535 episode reward: total was 42.410000. running mean: 2.628674\n",
      "ep 3122: ep_len:101 episode reward: total was 20.750000. running mean: 2.809888\n",
      "ep 3122: ep_len:620 episode reward: total was 1.180000. running mean: 2.793589\n",
      "ep 3122: ep_len:504 episode reward: total was -17.690000. running mean: 2.588753\n",
      "epsilon:0.061547 episode_count: 21861. steps_count: 9453925.000000\n",
      "Time elapsed:  28365.74104332924\n",
      "ep 3123: ep_len:526 episode reward: total was 68.070000. running mean: 3.243565\n",
      "ep 3123: ep_len:603 episode reward: total was 55.780000. running mean: 3.768930\n",
      "ep 3123: ep_len:468 episode reward: total was 50.890000. running mean: 4.240140\n",
      "ep 3123: ep_len:403 episode reward: total was 23.670000. running mean: 4.434439\n",
      "ep 3123: ep_len:100 episode reward: total was 32.280000. running mean: 4.712895\n",
      "ep 3123: ep_len:597 episode reward: total was -97.630000. running mean: 3.689466\n",
      "ep 3123: ep_len:539 episode reward: total was -27.610000. running mean: 3.376471\n",
      "epsilon:0.061503 episode_count: 21868. steps_count: 9457161.000000\n",
      "Time elapsed:  28374.25263261795\n",
      "ep 3124: ep_len:584 episode reward: total was -61.670000. running mean: 2.726006\n",
      "ep 3124: ep_len:340 episode reward: total was -10.940000. running mean: 2.589346\n",
      "ep 3124: ep_len:500 episode reward: total was 0.890000. running mean: 2.572353\n",
      "ep 3124: ep_len:518 episode reward: total was 36.730000. running mean: 2.913929\n",
      "ep 3124: ep_len:78 episode reward: total was 11.700000. running mean: 3.001790\n",
      "ep 3124: ep_len:303 episode reward: total was 7.600000. running mean: 3.047772\n",
      "ep 3124: ep_len:526 episode reward: total was 15.980000. running mean: 3.177094\n",
      "epsilon:0.061458 episode_count: 21875. steps_count: 9460010.000000\n",
      "Time elapsed:  28381.84138607979\n",
      "ep 3125: ep_len:568 episode reward: total was 32.120000. running mean: 3.466523\n",
      "ep 3125: ep_len:511 episode reward: total was 28.790000. running mean: 3.719758\n",
      "ep 3125: ep_len:607 episode reward: total was -15.840000. running mean: 3.524161\n",
      "ep 3125: ep_len:500 episode reward: total was -8.890000. running mean: 3.400019\n",
      "ep 3125: ep_len:3 episode reward: total was 1.010000. running mean: 3.376119\n",
      "ep 3125: ep_len:577 episode reward: total was 33.650000. running mean: 3.678858\n",
      "ep 3125: ep_len:588 episode reward: total was 28.540000. running mean: 3.927469\n",
      "epsilon:0.061414 episode_count: 21882. steps_count: 9463364.000000\n",
      "Time elapsed:  28390.66020178795\n",
      "ep 3126: ep_len:625 episode reward: total was -18.000000. running mean: 3.708194\n",
      "ep 3126: ep_len:536 episode reward: total was 24.480000. running mean: 3.915912\n",
      "ep 3126: ep_len:548 episode reward: total was -29.940000. running mean: 3.577353\n",
      "ep 3126: ep_len:500 episode reward: total was 8.570000. running mean: 3.627280\n",
      "ep 3126: ep_len:127 episode reward: total was 18.820000. running mean: 3.779207\n",
      "ep 3126: ep_len:500 episode reward: total was -32.560000. running mean: 3.415815\n",
      "ep 3126: ep_len:265 episode reward: total was -12.960000. running mean: 3.252057\n",
      "epsilon:0.061370 episode_count: 21889. steps_count: 9466465.000000\n",
      "Time elapsed:  28398.99930524826\n",
      "ep 3127: ep_len:559 episode reward: total was 39.220000. running mean: 3.611736\n",
      "ep 3127: ep_len:501 episode reward: total was 36.810000. running mean: 3.943719\n",
      "ep 3127: ep_len:612 episode reward: total was 38.370000. running mean: 4.287982\n",
      "ep 3127: ep_len:500 episode reward: total was 49.250000. running mean: 4.737602\n",
      "ep 3127: ep_len:90 episode reward: total was 23.270000. running mean: 4.922926\n",
      "ep 3127: ep_len:500 episode reward: total was 22.130000. running mean: 5.094996\n",
      "ep 3127: ep_len:558 episode reward: total was 10.790000. running mean: 5.151946\n",
      "epsilon:0.061325 episode_count: 21896. steps_count: 9469785.000000\n",
      "Time elapsed:  28408.270177841187\n",
      "ep 3128: ep_len:578 episode reward: total was 45.610000. running mean: 5.556527\n",
      "ep 3128: ep_len:567 episode reward: total was 100.270000. running mean: 6.503662\n",
      "ep 3128: ep_len:527 episode reward: total was -28.650000. running mean: 6.152125\n",
      "ep 3128: ep_len:512 episode reward: total was 27.030000. running mean: 6.360904\n",
      "ep 3128: ep_len:3 episode reward: total was 1.010000. running mean: 6.307395\n",
      "ep 3128: ep_len:500 episode reward: total was 1.940000. running mean: 6.263721\n",
      "ep 3128: ep_len:205 episode reward: total was 2.610000. running mean: 6.227184\n",
      "epsilon:0.061281 episode_count: 21903. steps_count: 9472677.000000\n",
      "Time elapsed:  28416.002123355865\n",
      "ep 3129: ep_len:205 episode reward: total was -2.310000. running mean: 6.141812\n",
      "ep 3129: ep_len:298 episode reward: total was -0.840000. running mean: 6.071994\n",
      "ep 3129: ep_len:428 episode reward: total was 37.630000. running mean: 6.387574\n",
      "ep 3129: ep_len:56 episode reward: total was 4.340000. running mean: 6.367098\n",
      "ep 3129: ep_len:3 episode reward: total was 1.010000. running mean: 6.313527\n",
      "ep 3129: ep_len:511 episode reward: total was -34.140000. running mean: 5.908992\n",
      "ep 3129: ep_len:506 episode reward: total was 5.100000. running mean: 5.900902\n",
      "epsilon:0.061237 episode_count: 21910. steps_count: 9474684.000000\n",
      "Time elapsed:  28424.4163980484\n",
      "ep 3130: ep_len:570 episode reward: total was -54.180000. running mean: 5.300093\n",
      "ep 3130: ep_len:500 episode reward: total was -14.240000. running mean: 5.104692\n",
      "ep 3130: ep_len:404 episode reward: total was 62.170000. running mean: 5.675345\n",
      "ep 3130: ep_len:500 episode reward: total was 3.810000. running mean: 5.656692\n",
      "ep 3130: ep_len:3 episode reward: total was 1.010000. running mean: 5.610225\n",
      "ep 3130: ep_len:524 episode reward: total was -15.830000. running mean: 5.395822\n",
      "ep 3130: ep_len:500 episode reward: total was -30.000000. running mean: 5.041864\n",
      "epsilon:0.061192 episode_count: 21917. steps_count: 9477685.000000\n",
      "Time elapsed:  28432.439275741577\n",
      "ep 3131: ep_len:571 episode reward: total was -189.360000. running mean: 3.097846\n",
      "ep 3131: ep_len:188 episode reward: total was 15.390000. running mean: 3.220767\n",
      "ep 3131: ep_len:450 episode reward: total was 51.260000. running mean: 3.701159\n",
      "ep 3131: ep_len:505 episode reward: total was -0.750000. running mean: 3.656648\n",
      "ep 3131: ep_len:129 episode reward: total was 22.350000. running mean: 3.843581\n",
      "ep 3131: ep_len:500 episode reward: total was 39.470000. running mean: 4.199846\n",
      "ep 3131: ep_len:500 episode reward: total was -28.950000. running mean: 3.868347\n",
      "epsilon:0.061148 episode_count: 21924. steps_count: 9480528.000000\n",
      "Time elapsed:  28438.59260225296\n",
      "ep 3132: ep_len:507 episode reward: total was 39.970000. running mean: 4.229364\n",
      "ep 3132: ep_len:500 episode reward: total was -7.130000. running mean: 4.115770\n",
      "ep 3132: ep_len:526 episode reward: total was -51.550000. running mean: 3.559112\n",
      "ep 3132: ep_len:500 episode reward: total was 10.520000. running mean: 3.628721\n",
      "ep 3132: ep_len:3 episode reward: total was 1.010000. running mean: 3.602534\n",
      "ep 3132: ep_len:510 episode reward: total was -14.640000. running mean: 3.420109\n",
      "ep 3132: ep_len:211 episode reward: total was 3.680000. running mean: 3.422708\n",
      "epsilon:0.061104 episode_count: 21931. steps_count: 9483285.000000\n",
      "Time elapsed:  28445.932622909546\n",
      "ep 3133: ep_len:566 episode reward: total was 55.210000. running mean: 3.940580\n",
      "ep 3133: ep_len:500 episode reward: total was 11.930000. running mean: 4.020475\n",
      "ep 3133: ep_len:615 episode reward: total was -32.300000. running mean: 3.657270\n",
      "ep 3133: ep_len:56 episode reward: total was -8.160000. running mean: 3.539097\n",
      "ep 3133: ep_len:3 episode reward: total was 1.010000. running mean: 3.513806\n",
      "ep 3133: ep_len:503 episode reward: total was -31.860000. running mean: 3.160068\n",
      "ep 3133: ep_len:337 episode reward: total was 18.510000. running mean: 3.313567\n",
      "epsilon:0.061059 episode_count: 21938. steps_count: 9485865.000000\n",
      "Time elapsed:  28456.008609056473\n",
      "ep 3134: ep_len:229 episode reward: total was 16.760000. running mean: 3.448032\n",
      "ep 3134: ep_len:500 episode reward: total was -9.800000. running mean: 3.315551\n",
      "ep 3134: ep_len:586 episode reward: total was -22.640000. running mean: 3.055996\n",
      "ep 3134: ep_len:504 episode reward: total was 13.260000. running mean: 3.158036\n",
      "ep 3134: ep_len:3 episode reward: total was -0.490000. running mean: 3.121556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3134: ep_len:518 episode reward: total was 17.100000. running mean: 3.261340\n",
      "ep 3134: ep_len:500 episode reward: total was 16.430000. running mean: 3.393027\n",
      "epsilon:0.061015 episode_count: 21945. steps_count: 9488705.000000\n",
      "Time elapsed:  28463.62830066681\n",
      "ep 3135: ep_len:500 episode reward: total was 76.810000. running mean: 4.127196\n",
      "ep 3135: ep_len:500 episode reward: total was 20.040000. running mean: 4.286324\n",
      "ep 3135: ep_len:500 episode reward: total was -14.510000. running mean: 4.098361\n",
      "ep 3135: ep_len:530 episode reward: total was 15.340000. running mean: 4.210778\n",
      "ep 3135: ep_len:3 episode reward: total was 1.010000. running mean: 4.178770\n",
      "ep 3135: ep_len:511 episode reward: total was 40.360000. running mean: 4.540582\n",
      "ep 3135: ep_len:347 episode reward: total was 26.230000. running mean: 4.757476\n",
      "epsilon:0.060971 episode_count: 21952. steps_count: 9491596.000000\n",
      "Time elapsed:  28471.728328466415\n",
      "ep 3136: ep_len:203 episode reward: total was 4.590000. running mean: 4.755802\n",
      "ep 3136: ep_len:283 episode reward: total was 9.320000. running mean: 4.801444\n",
      "ep 3136: ep_len:548 episode reward: total was -8.060000. running mean: 4.672829\n",
      "ep 3136: ep_len:511 episode reward: total was 12.950000. running mean: 4.755601\n",
      "ep 3136: ep_len:3 episode reward: total was 1.010000. running mean: 4.718145\n",
      "ep 3136: ep_len:505 episode reward: total was -4.270000. running mean: 4.628263\n",
      "ep 3136: ep_len:590 episode reward: total was -136.660000. running mean: 3.215381\n",
      "epsilon:0.060926 episode_count: 21959. steps_count: 9494239.000000\n",
      "Time elapsed:  28479.056336402893\n",
      "ep 3137: ep_len:236 episode reward: total was 23.850000. running mean: 3.421727\n",
      "ep 3137: ep_len:500 episode reward: total was 11.120000. running mean: 3.498710\n",
      "ep 3137: ep_len:79 episode reward: total was 1.270000. running mean: 3.476423\n",
      "ep 3137: ep_len:528 episode reward: total was -5.390000. running mean: 3.387758\n",
      "ep 3137: ep_len:3 episode reward: total was 1.010000. running mean: 3.363981\n",
      "ep 3137: ep_len:535 episode reward: total was -41.670000. running mean: 2.913641\n",
      "ep 3137: ep_len:592 episode reward: total was 22.590000. running mean: 3.110405\n",
      "epsilon:0.060882 episode_count: 21966. steps_count: 9496712.000000\n",
      "Time elapsed:  28485.85404086113\n",
      "ep 3138: ep_len:669 episode reward: total was -147.800000. running mean: 1.601300\n",
      "ep 3138: ep_len:582 episode reward: total was -75.890000. running mean: 0.826387\n",
      "ep 3138: ep_len:523 episode reward: total was 8.650000. running mean: 0.904624\n",
      "ep 3138: ep_len:585 episode reward: total was 76.720000. running mean: 1.662777\n",
      "ep 3138: ep_len:3 episode reward: total was -0.490000. running mean: 1.641250\n",
      "ep 3138: ep_len:500 episode reward: total was 9.360000. running mean: 1.718437\n",
      "ep 3138: ep_len:508 episode reward: total was 30.580000. running mean: 2.007053\n",
      "epsilon:0.060838 episode_count: 21973. steps_count: 9500082.000000\n",
      "Time elapsed:  28494.407362937927\n",
      "ep 3139: ep_len:556 episode reward: total was -46.760000. running mean: 1.519382\n",
      "ep 3139: ep_len:573 episode reward: total was -32.470000. running mean: 1.179488\n",
      "ep 3139: ep_len:586 episode reward: total was -42.840000. running mean: 0.739293\n",
      "ep 3139: ep_len:500 episode reward: total was 11.650000. running mean: 0.848401\n",
      "ep 3139: ep_len:92 episode reward: total was -47.770000. running mean: 0.362217\n",
      "ep 3139: ep_len:676 episode reward: total was 28.060000. running mean: 0.639194\n",
      "ep 3139: ep_len:298 episode reward: total was -32.500000. running mean: 0.307802\n",
      "epsilon:0.060793 episode_count: 21980. steps_count: 9503363.000000\n",
      "Time elapsed:  28508.338078022003\n",
      "ep 3140: ep_len:500 episode reward: total was 69.850000. running mean: 1.003224\n",
      "ep 3140: ep_len:500 episode reward: total was 24.780000. running mean: 1.240992\n",
      "ep 3140: ep_len:75 episode reward: total was 1.780000. running mean: 1.246382\n",
      "ep 3140: ep_len:500 episode reward: total was 52.710000. running mean: 1.761018\n",
      "ep 3140: ep_len:3 episode reward: total was 1.010000. running mean: 1.753508\n",
      "ep 3140: ep_len:618 episode reward: total was 11.690000. running mean: 1.852873\n",
      "ep 3140: ep_len:502 episode reward: total was -40.900000. running mean: 1.425344\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.060749 episode_count: 21987. steps_count: 9506061.000000\n",
      "Time elapsed:  28520.934424638748\n",
      "ep 3141: ep_len:556 episode reward: total was 11.220000. running mean: 1.523291\n",
      "ep 3141: ep_len:500 episode reward: total was 97.770000. running mean: 2.485758\n",
      "ep 3141: ep_len:632 episode reward: total was -23.630000. running mean: 2.224600\n",
      "ep 3141: ep_len:500 episode reward: total was 22.870000. running mean: 2.431054\n",
      "ep 3141: ep_len:3 episode reward: total was 1.010000. running mean: 2.416844\n",
      "ep 3141: ep_len:522 episode reward: total was -6.750000. running mean: 2.325175\n",
      "ep 3141: ep_len:542 episode reward: total was -13.040000. running mean: 2.171524\n",
      "epsilon:0.060705 episode_count: 21994. steps_count: 9509316.000000\n",
      "Time elapsed:  28529.60759162903\n",
      "ep 3142: ep_len:601 episode reward: total was 33.850000. running mean: 2.488308\n",
      "ep 3142: ep_len:501 episode reward: total was 13.910000. running mean: 2.602525\n",
      "ep 3142: ep_len:651 episode reward: total was 26.550000. running mean: 2.842000\n",
      "ep 3142: ep_len:500 episode reward: total was 18.580000. running mean: 2.999380\n",
      "ep 3142: ep_len:100 episode reward: total was 31.760000. running mean: 3.286986\n",
      "ep 3142: ep_len:500 episode reward: total was 22.390000. running mean: 3.478016\n",
      "ep 3142: ep_len:572 episode reward: total was 8.380000. running mean: 3.527036\n",
      "epsilon:0.060660 episode_count: 22001. steps_count: 9512741.000000\n",
      "Time elapsed:  28538.589794635773\n",
      "ep 3143: ep_len:507 episode reward: total was 25.460000. running mean: 3.746366\n",
      "ep 3143: ep_len:350 episode reward: total was 2.850000. running mean: 3.737402\n",
      "ep 3143: ep_len:584 episode reward: total was -28.310000. running mean: 3.416928\n",
      "ep 3143: ep_len:114 episode reward: total was 5.020000. running mean: 3.432959\n",
      "ep 3143: ep_len:46 episode reward: total was 12.500000. running mean: 3.523629\n",
      "ep 3143: ep_len:670 episode reward: total was 13.730000. running mean: 3.625693\n",
      "ep 3143: ep_len:268 episode reward: total was -19.480000. running mean: 3.394636\n",
      "epsilon:0.060616 episode_count: 22008. steps_count: 9515280.000000\n",
      "Time elapsed:  28545.482430696487\n",
      "ep 3144: ep_len:623 episode reward: total was 17.390000. running mean: 3.534590\n",
      "ep 3144: ep_len:561 episode reward: total was -59.630000. running mean: 2.902944\n",
      "ep 3144: ep_len:500 episode reward: total was -1.950000. running mean: 2.854414\n",
      "ep 3144: ep_len:509 episode reward: total was -96.330000. running mean: 1.862570\n",
      "ep 3144: ep_len:3 episode reward: total was 1.010000. running mean: 1.854045\n",
      "ep 3144: ep_len:532 episode reward: total was 17.240000. running mean: 2.007904\n",
      "ep 3144: ep_len:552 episode reward: total was -10.150000. running mean: 1.886325\n",
      "epsilon:0.060572 episode_count: 22015. steps_count: 9518560.000000\n",
      "Time elapsed:  28553.745992422104\n",
      "ep 3145: ep_len:600 episode reward: total was 28.190000. running mean: 2.149362\n",
      "ep 3145: ep_len:500 episode reward: total was 37.210000. running mean: 2.499968\n",
      "ep 3145: ep_len:658 episode reward: total was -11.950000. running mean: 2.355469\n",
      "ep 3145: ep_len:500 episode reward: total was 71.540000. running mean: 3.047314\n",
      "ep 3145: ep_len:85 episode reward: total was 20.740000. running mean: 3.224241\n",
      "ep 3145: ep_len:529 episode reward: total was -11.590000. running mean: 3.076098\n",
      "ep 3145: ep_len:500 episode reward: total was 1.370000. running mean: 3.059037\n",
      "epsilon:0.060527 episode_count: 22022. steps_count: 9521932.000000\n",
      "Time elapsed:  28561.669416189194\n",
      "ep 3146: ep_len:500 episode reward: total was 49.350000. running mean: 3.521947\n",
      "ep 3146: ep_len:201 episode reward: total was 4.870000. running mean: 3.535428\n",
      "ep 3146: ep_len:586 episode reward: total was -17.260000. running mean: 3.327473\n",
      "ep 3146: ep_len:500 episode reward: total was 61.290000. running mean: 3.907099\n",
      "ep 3146: ep_len:96 episode reward: total was 25.720000. running mean: 4.125228\n",
      "ep 3146: ep_len:508 episode reward: total was 35.320000. running mean: 4.437175\n",
      "ep 3146: ep_len:178 episode reward: total was 4.390000. running mean: 4.436704\n",
      "epsilon:0.060483 episode_count: 22029. steps_count: 9524501.000000\n",
      "Time elapsed:  28568.25421833992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3147: ep_len:500 episode reward: total was 49.820000. running mean: 4.890536\n",
      "ep 3147: ep_len:500 episode reward: total was 40.500000. running mean: 5.246631\n",
      "ep 3147: ep_len:594 episode reward: total was -16.910000. running mean: 5.025065\n",
      "ep 3147: ep_len:407 episode reward: total was -0.400000. running mean: 4.970814\n",
      "ep 3147: ep_len:87 episode reward: total was 20.240000. running mean: 5.123506\n",
      "ep 3147: ep_len:512 episode reward: total was -20.740000. running mean: 4.864871\n",
      "ep 3147: ep_len:204 episode reward: total was 17.140000. running mean: 4.987622\n",
      "epsilon:0.060439 episode_count: 22036. steps_count: 9527305.000000\n",
      "Time elapsed:  28575.821601629257\n",
      "ep 3148: ep_len:500 episode reward: total was 4.680000. running mean: 4.984546\n",
      "ep 3148: ep_len:629 episode reward: total was 45.440000. running mean: 5.389101\n",
      "ep 3148: ep_len:517 episode reward: total was -30.540000. running mean: 5.029810\n",
      "ep 3148: ep_len:564 episode reward: total was 45.190000. running mean: 5.431411\n",
      "ep 3148: ep_len:3 episode reward: total was 1.010000. running mean: 5.387197\n",
      "ep 3148: ep_len:642 episode reward: total was 22.080000. running mean: 5.554125\n",
      "ep 3148: ep_len:541 episode reward: total was 35.980000. running mean: 5.858384\n",
      "epsilon:0.060394 episode_count: 22043. steps_count: 9530701.000000\n",
      "Time elapsed:  28584.398015022278\n",
      "ep 3149: ep_len:560 episode reward: total was -48.160000. running mean: 5.318200\n",
      "ep 3149: ep_len:591 episode reward: total was 6.020000. running mean: 5.325218\n",
      "ep 3149: ep_len:529 episode reward: total was -27.310000. running mean: 4.998866\n",
      "ep 3149: ep_len:403 episode reward: total was 25.150000. running mean: 5.200377\n",
      "ep 3149: ep_len:3 episode reward: total was 1.010000. running mean: 5.158474\n",
      "ep 3149: ep_len:521 episode reward: total was -30.830000. running mean: 4.798589\n",
      "ep 3149: ep_len:551 episode reward: total was -23.890000. running mean: 4.511703\n",
      "epsilon:0.060350 episode_count: 22050. steps_count: 9533859.000000\n",
      "Time elapsed:  28597.418133497238\n",
      "ep 3150: ep_len:524 episode reward: total was -72.010000. running mean: 3.746486\n",
      "ep 3150: ep_len:500 episode reward: total was 14.220000. running mean: 3.851221\n",
      "ep 3150: ep_len:569 episode reward: total was -63.550000. running mean: 3.177209\n",
      "ep 3150: ep_len:572 episode reward: total was -46.560000. running mean: 2.679837\n",
      "ep 3150: ep_len:126 episode reward: total was 26.850000. running mean: 2.921538\n",
      "ep 3150: ep_len:500 episode reward: total was 5.730000. running mean: 2.949623\n",
      "ep 3150: ep_len:535 episode reward: total was -10.180000. running mean: 2.818327\n",
      "epsilon:0.060306 episode_count: 22057. steps_count: 9537185.000000\n",
      "Time elapsed:  28611.766726970673\n",
      "ep 3151: ep_len:131 episode reward: total was -1.110000. running mean: 2.779044\n",
      "ep 3151: ep_len:501 episode reward: total was 61.750000. running mean: 3.368753\n",
      "ep 3151: ep_len:528 episode reward: total was -41.590000. running mean: 2.919166\n",
      "ep 3151: ep_len:500 episode reward: total was 12.860000. running mean: 3.018574\n",
      "ep 3151: ep_len:92 episode reward: total was 24.730000. running mean: 3.235688\n",
      "ep 3151: ep_len:500 episode reward: total was 24.170000. running mean: 3.445031\n",
      "ep 3151: ep_len:588 episode reward: total was -61.830000. running mean: 2.792281\n",
      "epsilon:0.060261 episode_count: 22064. steps_count: 9540025.000000\n",
      "Time elapsed:  28619.43757534027\n",
      "ep 3152: ep_len:600 episode reward: total was -37.300000. running mean: 2.391358\n",
      "ep 3152: ep_len:588 episode reward: total was 99.790000. running mean: 3.365345\n",
      "ep 3152: ep_len:404 episode reward: total was -69.360000. running mean: 2.638091\n",
      "ep 3152: ep_len:170 episode reward: total was 12.220000. running mean: 2.733910\n",
      "ep 3152: ep_len:3 episode reward: total was 1.010000. running mean: 2.716671\n",
      "ep 3152: ep_len:610 episode reward: total was -20.930000. running mean: 2.480204\n",
      "ep 3152: ep_len:500 episode reward: total was -8.460000. running mean: 2.370802\n",
      "epsilon:0.060217 episode_count: 22071. steps_count: 9542900.000000\n",
      "Time elapsed:  28637.072729825974\n",
      "ep 3153: ep_len:536 episode reward: total was -26.160000. running mean: 2.085494\n",
      "ep 3153: ep_len:512 episode reward: total was -17.860000. running mean: 1.886039\n",
      "ep 3153: ep_len:573 episode reward: total was -47.040000. running mean: 1.396779\n",
      "ep 3153: ep_len:501 episode reward: total was 21.100000. running mean: 1.593811\n",
      "ep 3153: ep_len:2 episode reward: total was -0.500000. running mean: 1.572873\n",
      "ep 3153: ep_len:501 episode reward: total was 11.310000. running mean: 1.670244\n",
      "ep 3153: ep_len:575 episode reward: total was 33.920000. running mean: 1.992742\n",
      "epsilon:0.060173 episode_count: 22078. steps_count: 9546100.000000\n",
      "Time elapsed:  28645.474003076553\n",
      "ep 3154: ep_len:516 episode reward: total was 41.380000. running mean: 2.386615\n",
      "ep 3154: ep_len:170 episode reward: total was -2.420000. running mean: 2.338548\n",
      "ep 3154: ep_len:79 episode reward: total was 0.260000. running mean: 2.317763\n",
      "ep 3154: ep_len:599 episode reward: total was 29.910000. running mean: 2.593685\n",
      "ep 3154: ep_len:3 episode reward: total was 0.000000. running mean: 2.567748\n",
      "ep 3154: ep_len:597 episode reward: total was 11.570000. running mean: 2.657771\n",
      "ep 3154: ep_len:500 episode reward: total was -9.800000. running mean: 2.533193\n",
      "epsilon:0.060128 episode_count: 22085. steps_count: 9548564.000000\n",
      "Time elapsed:  28652.18850851059\n",
      "ep 3155: ep_len:504 episode reward: total was 58.550000. running mean: 3.093361\n",
      "ep 3155: ep_len:614 episode reward: total was 64.310000. running mean: 3.705528\n",
      "ep 3155: ep_len:547 episode reward: total was -54.240000. running mean: 3.126072\n",
      "ep 3155: ep_len:500 episode reward: total was 11.220000. running mean: 3.207012\n",
      "ep 3155: ep_len:3 episode reward: total was 1.010000. running mean: 3.185042\n",
      "ep 3155: ep_len:512 episode reward: total was -15.660000. running mean: 2.996591\n",
      "ep 3155: ep_len:585 episode reward: total was -0.030000. running mean: 2.966325\n",
      "epsilon:0.060084 episode_count: 22092. steps_count: 9551829.000000\n",
      "Time elapsed:  28660.832649946213\n",
      "ep 3156: ep_len:500 episode reward: total was 82.960000. running mean: 3.766262\n",
      "ep 3156: ep_len:647 episode reward: total was 90.770000. running mean: 4.636299\n",
      "ep 3156: ep_len:500 episode reward: total was 13.450000. running mean: 4.724436\n",
      "ep 3156: ep_len:515 episode reward: total was 3.990000. running mean: 4.717092\n",
      "ep 3156: ep_len:3 episode reward: total was 1.010000. running mean: 4.680021\n",
      "ep 3156: ep_len:534 episode reward: total was -5.960000. running mean: 4.573621\n",
      "ep 3156: ep_len:295 episode reward: total was -14.680000. running mean: 4.381085\n",
      "epsilon:0.060040 episode_count: 22099. steps_count: 9554823.000000\n",
      "Time elapsed:  28668.8137319088\n",
      "ep 3157: ep_len:122 episode reward: total was 7.490000. running mean: 4.412174\n",
      "ep 3157: ep_len:500 episode reward: total was -21.360000. running mean: 4.154452\n",
      "ep 3157: ep_len:500 episode reward: total was 30.320000. running mean: 4.416108\n",
      "ep 3157: ep_len:500 episode reward: total was 18.440000. running mean: 4.556347\n",
      "ep 3157: ep_len:89 episode reward: total was 21.670000. running mean: 4.727483\n",
      "ep 3157: ep_len:500 episode reward: total was 4.170000. running mean: 4.721908\n",
      "ep 3157: ep_len:565 episode reward: total was 20.220000. running mean: 4.876889\n",
      "epsilon:0.059995 episode_count: 22106. steps_count: 9557599.000000\n",
      "Time elapsed:  28676.33747935295\n",
      "ep 3158: ep_len:539 episode reward: total was 66.020000. running mean: 5.488320\n",
      "ep 3158: ep_len:500 episode reward: total was -158.100000. running mean: 3.852437\n",
      "ep 3158: ep_len:656 episode reward: total was -39.180000. running mean: 3.422113\n",
      "ep 3158: ep_len:511 episode reward: total was -4.570000. running mean: 3.342192\n",
      "ep 3158: ep_len:3 episode reward: total was 1.010000. running mean: 3.318870\n",
      "ep 3158: ep_len:521 episode reward: total was 38.010000. running mean: 3.665781\n",
      "ep 3158: ep_len:599 episode reward: total was 13.250000. running mean: 3.761623\n",
      "epsilon:0.059951 episode_count: 22113. steps_count: 9560928.000000\n",
      "Time elapsed:  28685.624223947525\n",
      "ep 3159: ep_len:183 episode reward: total was 14.100000. running mean: 3.865007\n",
      "ep 3159: ep_len:500 episode reward: total was -14.270000. running mean: 3.683657\n",
      "ep 3159: ep_len:621 episode reward: total was -25.270000. running mean: 3.394120\n",
      "ep 3159: ep_len:592 episode reward: total was -0.540000. running mean: 3.354779\n",
      "ep 3159: ep_len:3 episode reward: total was 1.010000. running mean: 3.331331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3159: ep_len:551 episode reward: total was 2.280000. running mean: 3.320818\n",
      "ep 3159: ep_len:523 episode reward: total was -17.780000. running mean: 3.109810\n",
      "epsilon:0.059907 episode_count: 22120. steps_count: 9563901.000000\n",
      "Time elapsed:  28693.559365987778\n",
      "ep 3160: ep_len:504 episode reward: total was -18.090000. running mean: 2.897812\n",
      "ep 3160: ep_len:662 episode reward: total was 66.600000. running mean: 3.534834\n",
      "ep 3160: ep_len:570 episode reward: total was -28.910000. running mean: 3.210385\n",
      "ep 3160: ep_len:597 episode reward: total was 23.700000. running mean: 3.415281\n",
      "ep 3160: ep_len:3 episode reward: total was 0.000000. running mean: 3.381129\n",
      "ep 3160: ep_len:520 episode reward: total was 23.770000. running mean: 3.585017\n",
      "ep 3160: ep_len:309 episode reward: total was 8.570000. running mean: 3.634867\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.059862 episode_count: 22127. steps_count: 9567066.000000\n",
      "Time elapsed:  28706.657574892044\n",
      "ep 3161: ep_len:633 episode reward: total was 2.090000. running mean: 3.619418\n",
      "ep 3161: ep_len:500 episode reward: total was 14.170000. running mean: 3.724924\n",
      "ep 3161: ep_len:548 episode reward: total was 12.510000. running mean: 3.812775\n",
      "ep 3161: ep_len:605 episode reward: total was 27.830000. running mean: 4.052947\n",
      "ep 3161: ep_len:86 episode reward: total was 19.220000. running mean: 4.204618\n",
      "ep 3161: ep_len:613 episode reward: total was 14.440000. running mean: 4.306972\n",
      "ep 3161: ep_len:500 episode reward: total was 2.740000. running mean: 4.291302\n",
      "epsilon:0.059818 episode_count: 22134. steps_count: 9570551.000000\n",
      "Time elapsed:  28715.711458444595\n",
      "ep 3162: ep_len:134 episode reward: total was 4.460000. running mean: 4.292989\n",
      "ep 3162: ep_len:544 episode reward: total was 71.060000. running mean: 4.960659\n",
      "ep 3162: ep_len:599 episode reward: total was -14.410000. running mean: 4.766952\n",
      "ep 3162: ep_len:609 episode reward: total was 59.160000. running mean: 5.310883\n",
      "ep 3162: ep_len:3 episode reward: total was 1.010000. running mean: 5.267874\n",
      "ep 3162: ep_len:500 episode reward: total was -118.260000. running mean: 4.032595\n",
      "ep 3162: ep_len:500 episode reward: total was 8.090000. running mean: 4.073169\n",
      "epsilon:0.059774 episode_count: 22141. steps_count: 9573440.000000\n",
      "Time elapsed:  28721.126425266266\n",
      "ep 3163: ep_len:500 episode reward: total was 57.320000. running mean: 4.605638\n",
      "ep 3163: ep_len:579 episode reward: total was -12.850000. running mean: 4.431081\n",
      "ep 3163: ep_len:620 episode reward: total was -79.590000. running mean: 3.590870\n",
      "ep 3163: ep_len:536 episode reward: total was 12.970000. running mean: 3.684662\n",
      "ep 3163: ep_len:3 episode reward: total was 1.010000. running mean: 3.657915\n",
      "ep 3163: ep_len:640 episode reward: total was 11.380000. running mean: 3.735136\n",
      "ep 3163: ep_len:501 episode reward: total was -40.960000. running mean: 3.288185\n",
      "epsilon:0.059729 episode_count: 22148. steps_count: 9576819.000000\n",
      "Time elapsed:  28730.06079697609\n",
      "ep 3164: ep_len:634 episode reward: total was -12.440000. running mean: 3.130903\n",
      "ep 3164: ep_len:284 episode reward: total was -183.960000. running mean: 1.259994\n",
      "ep 3164: ep_len:397 episode reward: total was 43.980000. running mean: 1.687194\n",
      "ep 3164: ep_len:586 episode reward: total was -24.250000. running mean: 1.427822\n",
      "ep 3164: ep_len:97 episode reward: total was 24.260000. running mean: 1.656144\n",
      "ep 3164: ep_len:500 episode reward: total was 40.170000. running mean: 2.041282\n",
      "ep 3164: ep_len:500 episode reward: total was 30.100000. running mean: 2.321869\n",
      "epsilon:0.059685 episode_count: 22155. steps_count: 9579817.000000\n",
      "Time elapsed:  28737.969532966614\n",
      "ep 3165: ep_len:178 episode reward: total was 17.990000. running mean: 2.478551\n",
      "ep 3165: ep_len:500 episode reward: total was 25.360000. running mean: 2.707365\n",
      "ep 3165: ep_len:529 episode reward: total was -174.540000. running mean: 0.934892\n",
      "ep 3165: ep_len:524 episode reward: total was -53.240000. running mean: 0.393143\n",
      "ep 3165: ep_len:3 episode reward: total was 1.010000. running mean: 0.399311\n",
      "ep 3165: ep_len:641 episode reward: total was -3.430000. running mean: 0.361018\n",
      "ep 3165: ep_len:307 episode reward: total was 23.120000. running mean: 0.588608\n",
      "epsilon:0.059641 episode_count: 22162. steps_count: 9582499.000000\n",
      "Time elapsed:  28745.23894572258\n",
      "ep 3166: ep_len:119 episode reward: total was 17.470000. running mean: 0.757422\n",
      "ep 3166: ep_len:595 episode reward: total was 8.800000. running mean: 0.837848\n",
      "ep 3166: ep_len:504 episode reward: total was -31.790000. running mean: 0.511569\n",
      "ep 3166: ep_len:432 episode reward: total was -1.190000. running mean: 0.494553\n",
      "ep 3166: ep_len:116 episode reward: total was 31.250000. running mean: 0.802108\n",
      "ep 3166: ep_len:533 episode reward: total was 9.620000. running mean: 0.890287\n",
      "ep 3166: ep_len:553 episode reward: total was -73.590000. running mean: 0.145484\n",
      "epsilon:0.059596 episode_count: 22169. steps_count: 9585351.000000\n",
      "Time elapsed:  28757.768441438675\n",
      "ep 3167: ep_len:500 episode reward: total was 67.780000. running mean: 0.821829\n",
      "ep 3167: ep_len:500 episode reward: total was 13.000000. running mean: 0.943611\n",
      "ep 3167: ep_len:644 episode reward: total was -20.230000. running mean: 0.731875\n",
      "ep 3167: ep_len:524 episode reward: total was 47.590000. running mean: 1.200456\n",
      "ep 3167: ep_len:112 episode reward: total was 29.740000. running mean: 1.485851\n",
      "ep 3167: ep_len:513 episode reward: total was -24.680000. running mean: 1.224193\n",
      "ep 3167: ep_len:550 episode reward: total was -6.310000. running mean: 1.148851\n",
      "epsilon:0.059552 episode_count: 22176. steps_count: 9588694.000000\n",
      "Time elapsed:  28766.22163414955\n",
      "ep 3168: ep_len:500 episode reward: total was -36.970000. running mean: 0.767662\n",
      "ep 3168: ep_len:500 episode reward: total was 1.230000. running mean: 0.772286\n",
      "ep 3168: ep_len:578 episode reward: total was -4.540000. running mean: 0.719163\n",
      "ep 3168: ep_len:500 episode reward: total was -10.390000. running mean: 0.608071\n",
      "ep 3168: ep_len:3 episode reward: total was 1.010000. running mean: 0.612091\n",
      "ep 3168: ep_len:619 episode reward: total was 18.570000. running mean: 0.791670\n",
      "ep 3168: ep_len:500 episode reward: total was -1.800000. running mean: 0.765753\n",
      "epsilon:0.059508 episode_count: 22183. steps_count: 9591894.000000\n",
      "Time elapsed:  28780.3034491539\n",
      "ep 3169: ep_len:214 episode reward: total was 20.250000. running mean: 0.960595\n",
      "ep 3169: ep_len:585 episode reward: total was -6.160000. running mean: 0.889390\n",
      "ep 3169: ep_len:424 episode reward: total was 4.190000. running mean: 0.922396\n",
      "ep 3169: ep_len:500 episode reward: total was 15.800000. running mean: 1.071172\n",
      "ep 3169: ep_len:53 episode reward: total was 23.500000. running mean: 1.295460\n",
      "ep 3169: ep_len:319 episode reward: total was 2.680000. running mean: 1.309305\n",
      "ep 3169: ep_len:511 episode reward: total was 28.960000. running mean: 1.585812\n",
      "epsilon:0.059463 episode_count: 22190. steps_count: 9594500.000000\n",
      "Time elapsed:  28787.41794872284\n",
      "ep 3170: ep_len:517 episode reward: total was 10.050000. running mean: 1.670454\n",
      "ep 3170: ep_len:500 episode reward: total was -25.740000. running mean: 1.396350\n",
      "ep 3170: ep_len:592 episode reward: total was -6.200000. running mean: 1.320386\n",
      "ep 3170: ep_len:535 episode reward: total was 51.560000. running mean: 1.822782\n",
      "ep 3170: ep_len:3 episode reward: total was 1.010000. running mean: 1.814654\n",
      "ep 3170: ep_len:336 episode reward: total was 31.750000. running mean: 2.114008\n",
      "ep 3170: ep_len:267 episode reward: total was -23.950000. running mean: 1.853368\n",
      "epsilon:0.059419 episode_count: 22197. steps_count: 9597250.000000\n",
      "Time elapsed:  28794.82553434372\n",
      "ep 3171: ep_len:609 episode reward: total was -135.250000. running mean: 0.482334\n",
      "ep 3171: ep_len:503 episode reward: total was 84.570000. running mean: 1.323211\n",
      "ep 3171: ep_len:603 episode reward: total was 17.670000. running mean: 1.486679\n",
      "ep 3171: ep_len:500 episode reward: total was 44.730000. running mean: 1.919112\n",
      "ep 3171: ep_len:42 episode reward: total was 19.010000. running mean: 2.090021\n",
      "ep 3171: ep_len:528 episode reward: total was 19.770000. running mean: 2.266821\n",
      "ep 3171: ep_len:568 episode reward: total was -1.890000. running mean: 2.225252\n",
      "epsilon:0.059375 episode_count: 22204. steps_count: 9600603.000000\n",
      "Time elapsed:  28803.686707019806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3172: ep_len:625 episode reward: total was -49.080000. running mean: 1.712200\n",
      "ep 3172: ep_len:543 episode reward: total was -57.660000. running mean: 1.118478\n",
      "ep 3172: ep_len:500 episode reward: total was -17.010000. running mean: 0.937193\n",
      "ep 3172: ep_len:56 episode reward: total was -6.660000. running mean: 0.861221\n",
      "ep 3172: ep_len:3 episode reward: total was 1.010000. running mean: 0.862709\n",
      "ep 3172: ep_len:186 episode reward: total was 23.750000. running mean: 1.091582\n",
      "ep 3172: ep_len:569 episode reward: total was 10.910000. running mean: 1.189766\n",
      "epsilon:0.059330 episode_count: 22211. steps_count: 9603085.000000\n",
      "Time elapsed:  28810.40826678276\n",
      "ep 3173: ep_len:557 episode reward: total was 56.010000. running mean: 1.737968\n",
      "ep 3173: ep_len:605 episode reward: total was 34.020000. running mean: 2.060789\n",
      "ep 3173: ep_len:500 episode reward: total was 5.670000. running mean: 2.096881\n",
      "ep 3173: ep_len:502 episode reward: total was 2.190000. running mean: 2.097812\n",
      "ep 3173: ep_len:3 episode reward: total was 1.010000. running mean: 2.086934\n",
      "ep 3173: ep_len:563 episode reward: total was 23.410000. running mean: 2.300165\n",
      "ep 3173: ep_len:503 episode reward: total was 26.830000. running mean: 2.545463\n",
      "epsilon:0.059286 episode_count: 22218. steps_count: 9606318.000000\n",
      "Time elapsed:  28818.83215522766\n",
      "ep 3174: ep_len:633 episode reward: total was 24.740000. running mean: 2.767408\n",
      "ep 3174: ep_len:567 episode reward: total was -25.530000. running mean: 2.484434\n",
      "ep 3174: ep_len:568 episode reward: total was -8.850000. running mean: 2.371090\n",
      "ep 3174: ep_len:554 episode reward: total was 7.100000. running mean: 2.418379\n",
      "ep 3174: ep_len:84 episode reward: total was 25.140000. running mean: 2.645595\n",
      "ep 3174: ep_len:662 episode reward: total was 18.730000. running mean: 2.806439\n",
      "ep 3174: ep_len:211 episode reward: total was -0.850000. running mean: 2.769875\n",
      "epsilon:0.059242 episode_count: 22225. steps_count: 9609597.000000\n",
      "Time elapsed:  28827.506576776505\n",
      "ep 3175: ep_len:500 episode reward: total was 67.070000. running mean: 3.412876\n",
      "ep 3175: ep_len:564 episode reward: total was 57.040000. running mean: 3.949147\n",
      "ep 3175: ep_len:627 episode reward: total was -12.350000. running mean: 3.786156\n",
      "ep 3175: ep_len:511 episode reward: total was 33.530000. running mean: 4.083594\n",
      "ep 3175: ep_len:3 episode reward: total was 1.010000. running mean: 4.052858\n",
      "ep 3175: ep_len:500 episode reward: total was 20.790000. running mean: 4.220230\n",
      "ep 3175: ep_len:190 episode reward: total was -6.050000. running mean: 4.117527\n",
      "epsilon:0.059197 episode_count: 22232. steps_count: 9612492.000000\n",
      "Time elapsed:  28839.87469434738\n",
      "ep 3176: ep_len:617 episode reward: total was 46.480000. running mean: 4.541152\n",
      "ep 3176: ep_len:506 episode reward: total was -22.640000. running mean: 4.269341\n",
      "ep 3176: ep_len:633 episode reward: total was -13.730000. running mean: 4.089347\n",
      "ep 3176: ep_len:394 episode reward: total was 0.740000. running mean: 4.055854\n",
      "ep 3176: ep_len:3 episode reward: total was 1.010000. running mean: 4.025395\n",
      "ep 3176: ep_len:526 episode reward: total was -24.020000. running mean: 3.744941\n",
      "ep 3176: ep_len:544 episode reward: total was -33.680000. running mean: 3.370692\n",
      "epsilon:0.059153 episode_count: 22239. steps_count: 9615715.000000\n",
      "Time elapsed:  28852.916405677795\n",
      "ep 3177: ep_len:517 episode reward: total was 10.570000. running mean: 3.442685\n",
      "ep 3177: ep_len:560 episode reward: total was 8.250000. running mean: 3.490758\n",
      "ep 3177: ep_len:500 episode reward: total was -7.930000. running mean: 3.376551\n",
      "ep 3177: ep_len:56 episode reward: total was -7.150000. running mean: 3.271285\n",
      "ep 3177: ep_len:3 episode reward: total was -1.500000. running mean: 3.223572\n",
      "ep 3177: ep_len:528 episode reward: total was -30.450000. running mean: 2.886836\n",
      "ep 3177: ep_len:530 episode reward: total was -32.230000. running mean: 2.535668\n",
      "epsilon:0.059109 episode_count: 22246. steps_count: 9618409.000000\n",
      "Time elapsed:  28865.470363140106\n",
      "ep 3178: ep_len:622 episode reward: total was 45.390000. running mean: 2.964211\n",
      "ep 3178: ep_len:608 episode reward: total was -2.920000. running mean: 2.905369\n",
      "ep 3178: ep_len:611 episode reward: total was -38.690000. running mean: 2.489416\n",
      "ep 3178: ep_len:544 episode reward: total was 7.540000. running mean: 2.539921\n",
      "ep 3178: ep_len:3 episode reward: total was 1.010000. running mean: 2.524622\n",
      "ep 3178: ep_len:551 episode reward: total was -134.770000. running mean: 1.151676\n",
      "ep 3178: ep_len:615 episode reward: total was -19.430000. running mean: 0.945859\n",
      "epsilon:0.059064 episode_count: 22253. steps_count: 9621963.000000\n",
      "Time elapsed:  28874.58279800415\n",
      "ep 3179: ep_len:569 episode reward: total was 19.670000. running mean: 1.133101\n",
      "ep 3179: ep_len:631 episode reward: total was 71.970000. running mean: 1.841470\n",
      "ep 3179: ep_len:597 episode reward: total was -125.280000. running mean: 0.570255\n",
      "ep 3179: ep_len:505 episode reward: total was 47.520000. running mean: 1.039752\n",
      "ep 3179: ep_len:92 episode reward: total was 18.240000. running mean: 1.211755\n",
      "ep 3179: ep_len:500 episode reward: total was 37.400000. running mean: 1.573637\n",
      "ep 3179: ep_len:540 episode reward: total was -24.720000. running mean: 1.310701\n",
      "epsilon:0.059020 episode_count: 22260. steps_count: 9625397.000000\n",
      "Time elapsed:  28883.501549482346\n",
      "ep 3180: ep_len:647 episode reward: total was -48.980000. running mean: 0.807794\n",
      "ep 3180: ep_len:503 episode reward: total was -7.760000. running mean: 0.722116\n",
      "ep 3180: ep_len:500 episode reward: total was 33.800000. running mean: 1.052895\n",
      "ep 3180: ep_len:509 episode reward: total was 18.030000. running mean: 1.222666\n",
      "ep 3180: ep_len:54 episode reward: total was 21.000000. running mean: 1.420439\n",
      "ep 3180: ep_len:528 episode reward: total was 17.520000. running mean: 1.581435\n",
      "ep 3180: ep_len:582 episode reward: total was 17.270000. running mean: 1.738320\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.058976 episode_count: 22267. steps_count: 9628720.000000\n",
      "Time elapsed:  28895.621472358704\n",
      "ep 3181: ep_len:126 episode reward: total was 13.960000. running mean: 1.860537\n",
      "ep 3181: ep_len:569 episode reward: total was 6.140000. running mean: 1.903332\n",
      "ep 3181: ep_len:579 episode reward: total was -8.290000. running mean: 1.801399\n",
      "ep 3181: ep_len:359 episode reward: total was -35.780000. running mean: 1.425585\n",
      "ep 3181: ep_len:84 episode reward: total was 19.750000. running mean: 1.608829\n",
      "ep 3181: ep_len:327 episode reward: total was 19.110000. running mean: 1.783840\n",
      "ep 3181: ep_len:529 episode reward: total was 3.480000. running mean: 1.800802\n",
      "epsilon:0.058931 episode_count: 22274. steps_count: 9631293.000000\n",
      "Time elapsed:  28901.551396369934\n",
      "ep 3182: ep_len:627 episode reward: total was -61.490000. running mean: 1.167894\n",
      "ep 3182: ep_len:620 episode reward: total was -138.390000. running mean: -0.227685\n",
      "ep 3182: ep_len:500 episode reward: total was 20.820000. running mean: -0.017208\n",
      "ep 3182: ep_len:532 episode reward: total was 44.580000. running mean: 0.428764\n",
      "ep 3182: ep_len:67 episode reward: total was 13.730000. running mean: 0.561776\n",
      "ep 3182: ep_len:500 episode reward: total was 14.770000. running mean: 0.703859\n",
      "ep 3182: ep_len:569 episode reward: total was 6.470000. running mean: 0.761520\n",
      "epsilon:0.058887 episode_count: 22281. steps_count: 9634708.000000\n",
      "Time elapsed:  28915.30481505394\n",
      "ep 3183: ep_len:248 episode reward: total was -3.170000. running mean: 0.722205\n",
      "ep 3183: ep_len:525 episode reward: total was 20.250000. running mean: 0.917483\n",
      "ep 3183: ep_len:500 episode reward: total was 1.070000. running mean: 0.919008\n",
      "ep 3183: ep_len:500 episode reward: total was 23.340000. running mean: 1.143218\n",
      "ep 3183: ep_len:83 episode reward: total was 22.100000. running mean: 1.352786\n",
      "ep 3183: ep_len:522 episode reward: total was -17.410000. running mean: 1.165158\n",
      "ep 3183: ep_len:574 episode reward: total was -12.310000. running mean: 1.030406\n",
      "epsilon:0.058843 episode_count: 22288. steps_count: 9637660.000000\n",
      "Time elapsed:  28923.22709298134\n",
      "ep 3184: ep_len:617 episode reward: total was 32.970000. running mean: 1.349802\n",
      "ep 3184: ep_len:523 episode reward: total was 19.960000. running mean: 1.535904\n",
      "ep 3184: ep_len:396 episode reward: total was -1.190000. running mean: 1.508645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3184: ep_len:132 episode reward: total was 10.080000. running mean: 1.594359\n",
      "ep 3184: ep_len:3 episode reward: total was -1.500000. running mean: 1.563415\n",
      "ep 3184: ep_len:571 episode reward: total was 28.090000. running mean: 1.828681\n",
      "ep 3184: ep_len:608 episode reward: total was 11.150000. running mean: 1.921894\n",
      "epsilon:0.058798 episode_count: 22295. steps_count: 9640510.000000\n",
      "Time elapsed:  28931.121872663498\n",
      "ep 3185: ep_len:537 episode reward: total was 2.850000. running mean: 1.931175\n",
      "ep 3185: ep_len:550 episode reward: total was 88.190000. running mean: 2.793763\n",
      "ep 3185: ep_len:602 episode reward: total was 20.230000. running mean: 2.968126\n",
      "ep 3185: ep_len:56 episode reward: total was -5.650000. running mean: 2.881945\n",
      "ep 3185: ep_len:3 episode reward: total was 1.010000. running mean: 2.863225\n",
      "ep 3185: ep_len:582 episode reward: total was 29.330000. running mean: 3.127893\n",
      "ep 3185: ep_len:207 episode reward: total was -21.180000. running mean: 2.884814\n",
      "epsilon:0.058754 episode_count: 22302. steps_count: 9643047.000000\n",
      "Time elapsed:  28937.945860385895\n",
      "ep 3186: ep_len:134 episode reward: total was 6.970000. running mean: 2.925666\n",
      "ep 3186: ep_len:500 episode reward: total was 63.460000. running mean: 3.531009\n",
      "ep 3186: ep_len:662 episode reward: total was 29.350000. running mean: 3.789199\n",
      "ep 3186: ep_len:159 episode reward: total was 26.090000. running mean: 4.012207\n",
      "ep 3186: ep_len:49 episode reward: total was 18.500000. running mean: 4.157085\n",
      "ep 3186: ep_len:512 episode reward: total was 20.660000. running mean: 4.322114\n",
      "ep 3186: ep_len:500 episode reward: total was 55.690000. running mean: 4.835793\n",
      "epsilon:0.058710 episode_count: 22309. steps_count: 9645563.000000\n",
      "Time elapsed:  28945.095826864243\n",
      "ep 3187: ep_len:215 episode reward: total was -21.790000. running mean: 4.569535\n",
      "ep 3187: ep_len:500 episode reward: total was 26.160000. running mean: 4.785440\n",
      "ep 3187: ep_len:559 episode reward: total was -33.520000. running mean: 4.402385\n",
      "ep 3187: ep_len:501 episode reward: total was 7.140000. running mean: 4.429761\n",
      "ep 3187: ep_len:3 episode reward: total was 1.010000. running mean: 4.395564\n",
      "ep 3187: ep_len:520 episode reward: total was -10.790000. running mean: 4.243708\n",
      "ep 3187: ep_len:616 episode reward: total was 38.000000. running mean: 4.581271\n",
      "epsilon:0.058665 episode_count: 22316. steps_count: 9648477.000000\n",
      "Time elapsed:  28952.82830619812\n",
      "ep 3188: ep_len:564 episode reward: total was 27.370000. running mean: 4.809158\n",
      "ep 3188: ep_len:545 episode reward: total was 98.210000. running mean: 5.743167\n",
      "ep 3188: ep_len:668 episode reward: total was -21.030000. running mean: 5.475435\n",
      "ep 3188: ep_len:506 episode reward: total was 11.870000. running mean: 5.539381\n",
      "ep 3188: ep_len:74 episode reward: total was 18.210000. running mean: 5.666087\n",
      "ep 3188: ep_len:571 episode reward: total was 8.210000. running mean: 5.691526\n",
      "ep 3188: ep_len:588 episode reward: total was 24.970000. running mean: 5.884311\n",
      "epsilon:0.058621 episode_count: 22323. steps_count: 9651993.000000\n",
      "Time elapsed:  28962.04316329956\n",
      "ep 3189: ep_len:542 episode reward: total was 39.230000. running mean: 6.217768\n",
      "ep 3189: ep_len:575 episode reward: total was 40.490000. running mean: 6.560490\n",
      "ep 3189: ep_len:500 episode reward: total was 39.660000. running mean: 6.891485\n",
      "ep 3189: ep_len:587 episode reward: total was 44.150000. running mean: 7.264070\n",
      "ep 3189: ep_len:3 episode reward: total was 1.010000. running mean: 7.201530\n",
      "ep 3189: ep_len:500 episode reward: total was 63.360000. running mean: 7.763114\n",
      "ep 3189: ep_len:570 episode reward: total was 3.510000. running mean: 7.720583\n",
      "epsilon:0.058577 episode_count: 22330. steps_count: 9655270.000000\n",
      "Time elapsed:  28969.807891130447\n",
      "ep 3190: ep_len:568 episode reward: total was 43.540000. running mean: 8.078777\n",
      "ep 3190: ep_len:533 episode reward: total was 10.230000. running mean: 8.100290\n",
      "ep 3190: ep_len:550 episode reward: total was -35.140000. running mean: 7.667887\n",
      "ep 3190: ep_len:500 episode reward: total was 56.320000. running mean: 8.154408\n",
      "ep 3190: ep_len:3 episode reward: total was 1.010000. running mean: 8.082964\n",
      "ep 3190: ep_len:537 episode reward: total was -11.740000. running mean: 7.884734\n",
      "ep 3190: ep_len:619 episode reward: total was 10.220000. running mean: 7.908087\n",
      "epsilon:0.058532 episode_count: 22337. steps_count: 9658580.000000\n",
      "Time elapsed:  28977.6061937809\n",
      "ep 3191: ep_len:561 episode reward: total was 10.160000. running mean: 7.930606\n",
      "ep 3191: ep_len:608 episode reward: total was 47.790000. running mean: 8.329200\n",
      "ep 3191: ep_len:366 episode reward: total was 49.390000. running mean: 8.739808\n",
      "ep 3191: ep_len:535 episode reward: total was 35.060000. running mean: 9.003010\n",
      "ep 3191: ep_len:3 episode reward: total was -0.490000. running mean: 8.908080\n",
      "ep 3191: ep_len:640 episode reward: total was -9.860000. running mean: 8.720399\n",
      "ep 3191: ep_len:500 episode reward: total was 54.890000. running mean: 9.182095\n",
      "epsilon:0.058488 episode_count: 22344. steps_count: 9661793.000000\n",
      "Time elapsed:  28986.24909877777\n",
      "ep 3192: ep_len:601 episode reward: total was 69.000000. running mean: 9.780274\n",
      "ep 3192: ep_len:283 episode reward: total was -9.900000. running mean: 9.583471\n",
      "ep 3192: ep_len:647 episode reward: total was -36.730000. running mean: 9.120336\n",
      "ep 3192: ep_len:507 episode reward: total was 53.480000. running mean: 9.563933\n",
      "ep 3192: ep_len:90 episode reward: total was 24.250000. running mean: 9.710794\n",
      "ep 3192: ep_len:594 episode reward: total was 10.130000. running mean: 9.714986\n",
      "ep 3192: ep_len:511 episode reward: total was -19.490000. running mean: 9.422936\n",
      "epsilon:0.058444 episode_count: 22351. steps_count: 9665026.000000\n",
      "Time elapsed:  28994.74010372162\n",
      "ep 3193: ep_len:500 episode reward: total was -3.890000. running mean: 9.289807\n",
      "ep 3193: ep_len:500 episode reward: total was 20.970000. running mean: 9.406609\n",
      "ep 3193: ep_len:536 episode reward: total was -4.110000. running mean: 9.271442\n",
      "ep 3193: ep_len:503 episode reward: total was 9.120000. running mean: 9.269928\n",
      "ep 3193: ep_len:3 episode reward: total was 1.010000. running mean: 9.187329\n",
      "ep 3193: ep_len:500 episode reward: total was -5.150000. running mean: 9.043955\n",
      "ep 3193: ep_len:638 episode reward: total was -143.100000. running mean: 7.522516\n",
      "epsilon:0.058399 episode_count: 22358. steps_count: 9668206.000000\n",
      "Time elapsed:  29003.24397087097\n",
      "ep 3194: ep_len:680 episode reward: total was 6.390000. running mean: 7.511191\n",
      "ep 3194: ep_len:587 episode reward: total was 70.850000. running mean: 8.144579\n",
      "ep 3194: ep_len:530 episode reward: total was -30.210000. running mean: 7.761033\n",
      "ep 3194: ep_len:516 episode reward: total was 41.060000. running mean: 8.094023\n",
      "ep 3194: ep_len:3 episode reward: total was 1.010000. running mean: 8.023183\n",
      "ep 3194: ep_len:500 episode reward: total was 39.450000. running mean: 8.337451\n",
      "ep 3194: ep_len:500 episode reward: total was 24.250000. running mean: 8.496576\n",
      "epsilon:0.058355 episode_count: 22365. steps_count: 9671522.000000\n",
      "Time elapsed:  29016.58118700981\n",
      "ep 3195: ep_len:560 episode reward: total was -35.120000. running mean: 8.060410\n",
      "ep 3195: ep_len:580 episode reward: total was 29.300000. running mean: 8.272806\n",
      "ep 3195: ep_len:563 episode reward: total was -17.910000. running mean: 8.010978\n",
      "ep 3195: ep_len:56 episode reward: total was -5.180000. running mean: 7.879068\n",
      "ep 3195: ep_len:110 episode reward: total was 25.190000. running mean: 8.052178\n",
      "ep 3195: ep_len:500 episode reward: total was -5.180000. running mean: 7.919856\n",
      "ep 3195: ep_len:546 episode reward: total was -5.320000. running mean: 7.787457\n",
      "epsilon:0.058311 episode_count: 22372. steps_count: 9674437.000000\n",
      "Time elapsed:  29024.52183198929\n",
      "ep 3196: ep_len:511 episode reward: total was 52.190000. running mean: 8.231483\n",
      "ep 3196: ep_len:521 episode reward: total was 3.770000. running mean: 8.186868\n",
      "ep 3196: ep_len:512 episode reward: total was 20.800000. running mean: 8.312999\n",
      "ep 3196: ep_len:56 episode reward: total was -7.670000. running mean: 8.153169\n",
      "ep 3196: ep_len:3 episode reward: total was 1.010000. running mean: 8.081738\n",
      "ep 3196: ep_len:512 episode reward: total was 19.720000. running mean: 8.198120\n",
      "ep 3196: ep_len:500 episode reward: total was -28.840000. running mean: 7.827739\n",
      "epsilon:0.058266 episode_count: 22379. steps_count: 9677052.000000\n",
      "Time elapsed:  29036.568341493607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3197: ep_len:597 episode reward: total was 19.960000. running mean: 7.949062\n",
      "ep 3197: ep_len:500 episode reward: total was 31.330000. running mean: 8.182871\n",
      "ep 3197: ep_len:534 episode reward: total was 7.760000. running mean: 8.178642\n",
      "ep 3197: ep_len:500 episode reward: total was 16.220000. running mean: 8.259056\n",
      "ep 3197: ep_len:69 episode reward: total was 12.740000. running mean: 8.303865\n",
      "ep 3197: ep_len:658 episode reward: total was -24.130000. running mean: 7.979527\n",
      "ep 3197: ep_len:536 episode reward: total was -15.760000. running mean: 7.742131\n",
      "epsilon:0.058222 episode_count: 22386. steps_count: 9680446.000000\n",
      "Time elapsed:  29045.48573088646\n",
      "ep 3198: ep_len:628 episode reward: total was 3.080000. running mean: 7.695510\n",
      "ep 3198: ep_len:526 episode reward: total was 8.930000. running mean: 7.707855\n",
      "ep 3198: ep_len:562 episode reward: total was -14.080000. running mean: 7.489977\n",
      "ep 3198: ep_len:500 episode reward: total was 25.770000. running mean: 7.672777\n",
      "ep 3198: ep_len:3 episode reward: total was 0.000000. running mean: 7.596049\n",
      "ep 3198: ep_len:644 episode reward: total was 19.020000. running mean: 7.710288\n",
      "ep 3198: ep_len:296 episode reward: total was -6.450000. running mean: 7.568686\n",
      "epsilon:0.058178 episode_count: 22393. steps_count: 9683605.000000\n",
      "Time elapsed:  29058.547214269638\n",
      "ep 3199: ep_len:500 episode reward: total was 32.270000. running mean: 7.815699\n",
      "ep 3199: ep_len:184 episode reward: total was -26.430000. running mean: 7.473242\n",
      "ep 3199: ep_len:530 episode reward: total was 18.240000. running mean: 7.580909\n",
      "ep 3199: ep_len:554 episode reward: total was 16.170000. running mean: 7.666800\n",
      "ep 3199: ep_len:120 episode reward: total was 13.840000. running mean: 7.728532\n",
      "ep 3199: ep_len:500 episode reward: total was -9.120000. running mean: 7.560047\n",
      "ep 3199: ep_len:303 episode reward: total was 30.090000. running mean: 7.785346\n",
      "epsilon:0.058133 episode_count: 22400. steps_count: 9686296.000000\n",
      "Time elapsed:  29065.896383285522\n",
      "ep 3200: ep_len:500 episode reward: total was 61.560000. running mean: 8.323093\n",
      "ep 3200: ep_len:365 episode reward: total was 1.130000. running mean: 8.251162\n",
      "ep 3200: ep_len:608 episode reward: total was -18.600000. running mean: 7.982650\n",
      "ep 3200: ep_len:500 episode reward: total was 27.180000. running mean: 8.174624\n",
      "ep 3200: ep_len:3 episode reward: total was 1.010000. running mean: 8.102978\n",
      "ep 3200: ep_len:594 episode reward: total was -107.280000. running mean: 6.949148\n",
      "ep 3200: ep_len:500 episode reward: total was -22.470000. running mean: 6.654956\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.058089 episode_count: 22407. steps_count: 9689366.000000\n",
      "Time elapsed:  29078.966860055923\n",
      "ep 3201: ep_len:500 episode reward: total was -3.280000. running mean: 6.555607\n",
      "ep 3201: ep_len:512 episode reward: total was -8.930000. running mean: 6.400751\n",
      "ep 3201: ep_len:500 episode reward: total was 1.230000. running mean: 6.349043\n",
      "ep 3201: ep_len:555 episode reward: total was 11.620000. running mean: 6.401753\n",
      "ep 3201: ep_len:3 episode reward: total was -0.490000. running mean: 6.332835\n",
      "ep 3201: ep_len:500 episode reward: total was 12.760000. running mean: 6.397107\n",
      "ep 3201: ep_len:558 episode reward: total was 27.550000. running mean: 6.608636\n",
      "epsilon:0.058045 episode_count: 22414. steps_count: 9692494.000000\n",
      "Time elapsed:  29087.261527061462\n",
      "ep 3202: ep_len:613 episode reward: total was 40.520000. running mean: 6.947750\n",
      "ep 3202: ep_len:539 episode reward: total was 38.250000. running mean: 7.260772\n",
      "ep 3202: ep_len:566 episode reward: total was -9.680000. running mean: 7.091364\n",
      "ep 3202: ep_len:600 episode reward: total was 30.900000. running mean: 7.329451\n",
      "ep 3202: ep_len:100 episode reward: total was 27.260000. running mean: 7.528756\n",
      "ep 3202: ep_len:590 episode reward: total was -20.370000. running mean: 7.249769\n",
      "ep 3202: ep_len:518 episode reward: total was -15.380000. running mean: 7.023471\n",
      "epsilon:0.058000 episode_count: 22421. steps_count: 9696020.000000\n",
      "Time elapsed:  29096.92759037018\n",
      "ep 3203: ep_len:518 episode reward: total was -53.120000. running mean: 6.422036\n",
      "ep 3203: ep_len:536 episode reward: total was 93.530000. running mean: 7.293116\n",
      "ep 3203: ep_len:567 episode reward: total was -26.320000. running mean: 6.956985\n",
      "ep 3203: ep_len:579 episode reward: total was -189.180000. running mean: 4.995615\n",
      "ep 3203: ep_len:77 episode reward: total was 28.590000. running mean: 5.231559\n",
      "ep 3203: ep_len:612 episode reward: total was -0.670000. running mean: 5.172543\n",
      "ep 3203: ep_len:542 episode reward: total was 10.480000. running mean: 5.225618\n",
      "epsilon:0.057956 episode_count: 22428. steps_count: 9699451.000000\n",
      "Time elapsed:  29105.897740364075\n",
      "ep 3204: ep_len:132 episode reward: total was -16.560000. running mean: 5.007762\n",
      "ep 3204: ep_len:367 episode reward: total was 0.850000. running mean: 4.966184\n",
      "ep 3204: ep_len:500 episode reward: total was 10.310000. running mean: 5.019622\n",
      "ep 3204: ep_len:552 episode reward: total was 56.640000. running mean: 5.535826\n",
      "ep 3204: ep_len:47 episode reward: total was 19.000000. running mean: 5.670468\n",
      "ep 3204: ep_len:536 episode reward: total was -57.030000. running mean: 5.043463\n",
      "ep 3204: ep_len:547 episode reward: total was 30.940000. running mean: 5.302428\n",
      "epsilon:0.057912 episode_count: 22435. steps_count: 9702132.000000\n",
      "Time elapsed:  29117.828156471252\n",
      "ep 3205: ep_len:500 episode reward: total was 74.910000. running mean: 5.998504\n",
      "ep 3205: ep_len:342 episode reward: total was -0.480000. running mean: 5.933719\n",
      "ep 3205: ep_len:415 episode reward: total was 43.800000. running mean: 6.312382\n",
      "ep 3205: ep_len:516 episode reward: total was 48.460000. running mean: 6.733858\n",
      "ep 3205: ep_len:3 episode reward: total was 1.010000. running mean: 6.676619\n",
      "ep 3205: ep_len:159 episode reward: total was 29.570000. running mean: 6.905553\n",
      "ep 3205: ep_len:574 episode reward: total was 12.700000. running mean: 6.963498\n",
      "epsilon:0.057867 episode_count: 22442. steps_count: 9704641.000000\n",
      "Time elapsed:  29124.615811109543\n",
      "ep 3206: ep_len:584 episode reward: total was 71.800000. running mean: 7.611863\n",
      "ep 3206: ep_len:500 episode reward: total was 17.380000. running mean: 7.709544\n",
      "ep 3206: ep_len:546 episode reward: total was -0.510000. running mean: 7.627349\n",
      "ep 3206: ep_len:161 episode reward: total was 11.630000. running mean: 7.667375\n",
      "ep 3206: ep_len:3 episode reward: total was 1.010000. running mean: 7.600801\n",
      "ep 3206: ep_len:507 episode reward: total was -21.230000. running mean: 7.312493\n",
      "ep 3206: ep_len:545 episode reward: total was 2.510000. running mean: 7.264468\n",
      "epsilon:0.057823 episode_count: 22449. steps_count: 9707487.000000\n",
      "Time elapsed:  29137.274459838867\n",
      "ep 3207: ep_len:559 episode reward: total was 22.240000. running mean: 7.414224\n",
      "ep 3207: ep_len:509 episode reward: total was -7.320000. running mean: 7.266881\n",
      "ep 3207: ep_len:586 episode reward: total was 8.240000. running mean: 7.276613\n",
      "ep 3207: ep_len:500 episode reward: total was 57.360000. running mean: 7.777447\n",
      "ep 3207: ep_len:3 episode reward: total was 0.000000. running mean: 7.699672\n",
      "ep 3207: ep_len:640 episode reward: total was 11.400000. running mean: 7.736675\n",
      "ep 3207: ep_len:584 episode reward: total was 18.780000. running mean: 7.847109\n",
      "epsilon:0.057779 episode_count: 22456. steps_count: 9710868.000000\n",
      "Time elapsed:  29145.892380714417\n",
      "ep 3208: ep_len:229 episode reward: total was 6.630000. running mean: 7.834938\n",
      "ep 3208: ep_len:567 episode reward: total was 32.020000. running mean: 8.076788\n",
      "ep 3208: ep_len:563 episode reward: total was 40.150000. running mean: 8.397520\n",
      "ep 3208: ep_len:618 episode reward: total was 97.900000. running mean: 9.292545\n",
      "ep 3208: ep_len:3 episode reward: total was 1.010000. running mean: 9.209720\n",
      "ep 3208: ep_len:500 episode reward: total was -24.210000. running mean: 8.875522\n",
      "ep 3208: ep_len:608 episode reward: total was 24.250000. running mean: 9.029267\n",
      "epsilon:0.057734 episode_count: 22463. steps_count: 9713956.000000\n",
      "Time elapsed:  29152.28471136093\n",
      "ep 3209: ep_len:206 episode reward: total was 19.200000. running mean: 9.130975\n",
      "ep 3209: ep_len:500 episode reward: total was 65.020000. running mean: 9.689865\n",
      "ep 3209: ep_len:537 episode reward: total was 20.030000. running mean: 9.793266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3209: ep_len:157 episode reward: total was 4.700000. running mean: 9.742333\n",
      "ep 3209: ep_len:3 episode reward: total was 1.010000. running mean: 9.655010\n",
      "ep 3209: ep_len:219 episode reward: total was 20.120000. running mean: 9.759660\n",
      "ep 3209: ep_len:547 episode reward: total was 8.060000. running mean: 9.742663\n",
      "epsilon:0.057690 episode_count: 22470. steps_count: 9716125.000000\n",
      "Time elapsed:  29157.98542904854\n",
      "ep 3210: ep_len:598 episode reward: total was 64.640000. running mean: 10.291637\n",
      "ep 3210: ep_len:598 episode reward: total was -52.950000. running mean: 9.659220\n",
      "ep 3210: ep_len:569 episode reward: total was -37.440000. running mean: 9.188228\n",
      "ep 3210: ep_len:534 episode reward: total was 31.850000. running mean: 9.414846\n",
      "ep 3210: ep_len:3 episode reward: total was 1.010000. running mean: 9.330797\n",
      "ep 3210: ep_len:525 episode reward: total was 32.730000. running mean: 9.564790\n",
      "ep 3210: ep_len:196 episode reward: total was -4.520000. running mean: 9.423942\n",
      "epsilon:0.057646 episode_count: 22477. steps_count: 9719148.000000\n",
      "Time elapsed:  29165.98480939865\n",
      "ep 3211: ep_len:134 episode reward: total was -23.090000. running mean: 9.098802\n",
      "ep 3211: ep_len:171 episode reward: total was 7.200000. running mean: 9.079814\n",
      "ep 3211: ep_len:500 episode reward: total was 19.160000. running mean: 9.180616\n",
      "ep 3211: ep_len:500 episode reward: total was 19.650000. running mean: 9.285310\n",
      "ep 3211: ep_len:3 episode reward: total was 1.010000. running mean: 9.202557\n",
      "ep 3211: ep_len:500 episode reward: total was -2.670000. running mean: 9.083831\n",
      "ep 3211: ep_len:526 episode reward: total was 9.110000. running mean: 9.084093\n",
      "epsilon:0.057601 episode_count: 22484. steps_count: 9721482.000000\n",
      "Time elapsed:  29172.589801311493\n",
      "ep 3212: ep_len:585 episode reward: total was 21.370000. running mean: 9.206952\n",
      "ep 3212: ep_len:502 episode reward: total was 39.660000. running mean: 9.511482\n",
      "ep 3212: ep_len:500 episode reward: total was 20.720000. running mean: 9.623568\n",
      "ep 3212: ep_len:500 episode reward: total was 57.610000. running mean: 10.103432\n",
      "ep 3212: ep_len:133 episode reward: total was 23.860000. running mean: 10.240998\n",
      "ep 3212: ep_len:686 episode reward: total was 22.470000. running mean: 10.363288\n",
      "ep 3212: ep_len:500 episode reward: total was -32.130000. running mean: 9.938355\n",
      "epsilon:0.057557 episode_count: 22491. steps_count: 9724888.000000\n",
      "Time elapsed:  29191.74786043167\n",
      "ep 3213: ep_len:506 episode reward: total was 22.700000. running mean: 10.065971\n",
      "ep 3213: ep_len:549 episode reward: total was 28.330000. running mean: 10.248612\n",
      "ep 3213: ep_len:642 episode reward: total was 0.340000. running mean: 10.149525\n",
      "ep 3213: ep_len:600 episode reward: total was 35.890000. running mean: 10.406930\n",
      "ep 3213: ep_len:96 episode reward: total was 27.250000. running mean: 10.575361\n",
      "ep 3213: ep_len:500 episode reward: total was 27.250000. running mean: 10.742107\n",
      "ep 3213: ep_len:564 episode reward: total was -4.140000. running mean: 10.593286\n",
      "epsilon:0.057513 episode_count: 22498. steps_count: 9728345.000000\n",
      "Time elapsed:  29207.049402713776\n",
      "ep 3214: ep_len:112 episode reward: total was -6.720000. running mean: 10.420153\n",
      "ep 3214: ep_len:505 episode reward: total was 50.750000. running mean: 10.823452\n",
      "ep 3214: ep_len:629 episode reward: total was -26.740000. running mean: 10.447817\n",
      "ep 3214: ep_len:500 episode reward: total was 30.570000. running mean: 10.649039\n",
      "ep 3214: ep_len:3 episode reward: total was 1.010000. running mean: 10.552649\n",
      "ep 3214: ep_len:503 episode reward: total was -52.750000. running mean: 9.919622\n",
      "ep 3214: ep_len:621 episode reward: total was 7.210000. running mean: 9.892526\n",
      "epsilon:0.057468 episode_count: 22505. steps_count: 9731218.000000\n",
      "Time elapsed:  29219.575226545334\n",
      "ep 3215: ep_len:221 episode reward: total was 10.620000. running mean: 9.899801\n",
      "ep 3215: ep_len:346 episode reward: total was -1.390000. running mean: 9.786903\n",
      "ep 3215: ep_len:632 episode reward: total was -11.630000. running mean: 9.572734\n",
      "ep 3215: ep_len:159 episode reward: total was 18.150000. running mean: 9.658506\n",
      "ep 3215: ep_len:85 episode reward: total was 22.240000. running mean: 9.784321\n",
      "ep 3215: ep_len:592 episode reward: total was 55.780000. running mean: 10.244278\n",
      "ep 3215: ep_len:582 episode reward: total was -58.600000. running mean: 9.555835\n",
      "epsilon:0.057424 episode_count: 22512. steps_count: 9733835.000000\n",
      "Time elapsed:  29226.730132102966\n",
      "ep 3216: ep_len:633 episode reward: total was 30.950000. running mean: 9.769777\n",
      "ep 3216: ep_len:500 episode reward: total was 2.130000. running mean: 9.693379\n",
      "ep 3216: ep_len:569 episode reward: total was -39.830000. running mean: 9.198145\n",
      "ep 3216: ep_len:527 episode reward: total was 53.380000. running mean: 9.639964\n",
      "ep 3216: ep_len:48 episode reward: total was 19.500000. running mean: 9.738564\n",
      "ep 3216: ep_len:514 episode reward: total was -21.160000. running mean: 9.429579\n",
      "ep 3216: ep_len:587 episode reward: total was -6.700000. running mean: 9.268283\n",
      "epsilon:0.057380 episode_count: 22519. steps_count: 9737213.000000\n",
      "Time elapsed:  29235.660147428513\n",
      "ep 3217: ep_len:533 episode reward: total was 8.650000. running mean: 9.262100\n",
      "ep 3217: ep_len:500 episode reward: total was 40.670000. running mean: 9.576179\n",
      "ep 3217: ep_len:546 episode reward: total was 46.040000. running mean: 9.940817\n",
      "ep 3217: ep_len:512 episode reward: total was -7.840000. running mean: 9.763009\n",
      "ep 3217: ep_len:68 episode reward: total was 9.700000. running mean: 9.762379\n",
      "ep 3217: ep_len:543 episode reward: total was 2.380000. running mean: 9.688555\n",
      "ep 3217: ep_len:521 episode reward: total was -16.770000. running mean: 9.423970\n",
      "epsilon:0.057335 episode_count: 22526. steps_count: 9740436.000000\n",
      "Time elapsed:  29244.221606492996\n",
      "ep 3218: ep_len:557 episode reward: total was -13.950000. running mean: 9.190230\n",
      "ep 3218: ep_len:584 episode reward: total was 30.070000. running mean: 9.399028\n",
      "ep 3218: ep_len:587 episode reward: total was 22.300000. running mean: 9.528037\n",
      "ep 3218: ep_len:508 episode reward: total was 56.150000. running mean: 9.994257\n",
      "ep 3218: ep_len:3 episode reward: total was 1.010000. running mean: 9.904414\n",
      "ep 3218: ep_len:500 episode reward: total was 5.670000. running mean: 9.862070\n",
      "ep 3218: ep_len:567 episode reward: total was -18.820000. running mean: 9.575250\n",
      "epsilon:0.057291 episode_count: 22533. steps_count: 9743742.000000\n",
      "Time elapsed:  29252.7434091568\n",
      "ep 3219: ep_len:650 episode reward: total was -64.070000. running mean: 8.838797\n",
      "ep 3219: ep_len:638 episode reward: total was 100.450000. running mean: 9.754909\n",
      "ep 3219: ep_len:540 episode reward: total was -12.630000. running mean: 9.531060\n",
      "ep 3219: ep_len:500 episode reward: total was -2.220000. running mean: 9.413549\n",
      "ep 3219: ep_len:3 episode reward: total was 1.010000. running mean: 9.329514\n",
      "ep 3219: ep_len:703 episode reward: total was 6.630000. running mean: 9.302519\n",
      "ep 3219: ep_len:601 episode reward: total was 4.410000. running mean: 9.253594\n",
      "epsilon:0.057247 episode_count: 22540. steps_count: 9747377.000000\n",
      "Time elapsed:  29258.223662376404\n",
      "ep 3220: ep_len:524 episode reward: total was 8.010000. running mean: 9.241158\n",
      "ep 3220: ep_len:585 episode reward: total was 12.360000. running mean: 9.272346\n",
      "ep 3220: ep_len:545 episode reward: total was 17.540000. running mean: 9.355023\n",
      "ep 3220: ep_len:519 episode reward: total was -20.570000. running mean: 9.055772\n",
      "ep 3220: ep_len:134 episode reward: total was 23.350000. running mean: 9.198715\n",
      "ep 3220: ep_len:575 episode reward: total was -222.290000. running mean: 6.883828\n",
      "ep 3220: ep_len:519 episode reward: total was -8.590000. running mean: 6.729089\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.057202 episode_count: 22547. steps_count: 9750778.000000\n",
      "Time elapsed:  29268.132392406464\n",
      "ep 3221: ep_len:200 episode reward: total was 9.120000. running mean: 6.752998\n",
      "ep 3221: ep_len:620 episode reward: total was 45.120000. running mean: 7.136668\n",
      "ep 3221: ep_len:517 episode reward: total was -15.530000. running mean: 6.910002\n",
      "ep 3221: ep_len:504 episode reward: total was 19.530000. running mean: 7.036202\n",
      "ep 3221: ep_len:86 episode reward: total was 18.670000. running mean: 7.152540\n",
      "ep 3221: ep_len:259 episode reward: total was 43.540000. running mean: 7.516414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3221: ep_len:522 episode reward: total was -9.930000. running mean: 7.341950\n",
      "epsilon:0.057158 episode_count: 22554. steps_count: 9753486.000000\n",
      "Time elapsed:  29275.57334113121\n",
      "ep 3222: ep_len:529 episode reward: total was -37.270000. running mean: 6.895831\n",
      "ep 3222: ep_len:541 episode reward: total was 24.280000. running mean: 7.069672\n",
      "ep 3222: ep_len:500 episode reward: total was 12.370000. running mean: 7.122676\n",
      "ep 3222: ep_len:388 episode reward: total was -115.000000. running mean: 5.901449\n",
      "ep 3222: ep_len:3 episode reward: total was 1.010000. running mean: 5.852534\n",
      "ep 3222: ep_len:524 episode reward: total was 63.680000. running mean: 6.430809\n",
      "ep 3222: ep_len:568 episode reward: total was 17.400000. running mean: 6.540501\n",
      "epsilon:0.057114 episode_count: 22561. steps_count: 9756539.000000\n",
      "Time elapsed:  29283.710414409637\n",
      "ep 3223: ep_len:508 episode reward: total was 8.860000. running mean: 6.563696\n",
      "ep 3223: ep_len:590 episode reward: total was 25.300000. running mean: 6.751059\n",
      "ep 3223: ep_len:555 episode reward: total was -11.260000. running mean: 6.570948\n",
      "ep 3223: ep_len:529 episode reward: total was -6.480000. running mean: 6.440439\n",
      "ep 3223: ep_len:73 episode reward: total was 16.240000. running mean: 6.538434\n",
      "ep 3223: ep_len:501 episode reward: total was 18.940000. running mean: 6.662450\n",
      "ep 3223: ep_len:530 episode reward: total was 38.930000. running mean: 6.985126\n",
      "epsilon:0.057069 episode_count: 22568. steps_count: 9759825.000000\n",
      "Time elapsed:  29294.804946660995\n",
      "ep 3224: ep_len:563 episode reward: total was 46.580000. running mean: 7.381074\n",
      "ep 3224: ep_len:583 episode reward: total was -26.390000. running mean: 7.043364\n",
      "ep 3224: ep_len:640 episode reward: total was 2.840000. running mean: 7.001330\n",
      "ep 3224: ep_len:56 episode reward: total was 3.820000. running mean: 6.969517\n",
      "ep 3224: ep_len:3 episode reward: total was 1.010000. running mean: 6.909922\n",
      "ep 3224: ep_len:668 episode reward: total was -177.060000. running mean: 5.070222\n",
      "ep 3224: ep_len:500 episode reward: total was -7.860000. running mean: 4.940920\n",
      "epsilon:0.057025 episode_count: 22575. steps_count: 9762838.000000\n",
      "Time elapsed:  29302.836414575577\n",
      "ep 3225: ep_len:624 episode reward: total was 19.170000. running mean: 5.083211\n",
      "ep 3225: ep_len:618 episode reward: total was 75.890000. running mean: 5.791279\n",
      "ep 3225: ep_len:540 episode reward: total was -20.410000. running mean: 5.529266\n",
      "ep 3225: ep_len:500 episode reward: total was 32.290000. running mean: 5.796873\n",
      "ep 3225: ep_len:3 episode reward: total was 1.010000. running mean: 5.749005\n",
      "ep 3225: ep_len:178 episode reward: total was 33.160000. running mean: 6.023115\n",
      "ep 3225: ep_len:518 episode reward: total was 7.410000. running mean: 6.036983\n",
      "epsilon:0.056981 episode_count: 22582. steps_count: 9765819.000000\n",
      "Time elapsed:  29310.815068483353\n",
      "ep 3226: ep_len:608 episode reward: total was -81.530000. running mean: 5.161314\n",
      "ep 3226: ep_len:550 episode reward: total was -30.980000. running mean: 4.799900\n",
      "ep 3226: ep_len:500 episode reward: total was 12.750000. running mean: 4.879401\n",
      "ep 3226: ep_len:170 episode reward: total was 1.740000. running mean: 4.848007\n",
      "ep 3226: ep_len:3 episode reward: total was 1.010000. running mean: 4.809627\n",
      "ep 3226: ep_len:624 episode reward: total was -3.590000. running mean: 4.725631\n",
      "ep 3226: ep_len:343 episode reward: total was -27.530000. running mean: 4.403075\n",
      "epsilon:0.056936 episode_count: 22589. steps_count: 9768617.000000\n",
      "Time elapsed:  29318.42851448059\n",
      "ep 3227: ep_len:591 episode reward: total was -81.650000. running mean: 3.542544\n",
      "ep 3227: ep_len:322 episode reward: total was -10.110000. running mean: 3.406019\n",
      "ep 3227: ep_len:634 episode reward: total was -26.310000. running mean: 3.108858\n",
      "ep 3227: ep_len:600 episode reward: total was 44.370000. running mean: 3.521470\n",
      "ep 3227: ep_len:3 episode reward: total was 1.010000. running mean: 3.496355\n",
      "ep 3227: ep_len:618 episode reward: total was -7.140000. running mean: 3.389992\n",
      "ep 3227: ep_len:500 episode reward: total was -12.420000. running mean: 3.231892\n",
      "epsilon:0.056892 episode_count: 22596. steps_count: 9771885.000000\n",
      "Time elapsed:  29325.907041311264\n",
      "ep 3228: ep_len:550 episode reward: total was 34.780000. running mean: 3.547373\n",
      "ep 3228: ep_len:517 episode reward: total was 46.220000. running mean: 3.974099\n",
      "ep 3228: ep_len:576 episode reward: total was -8.010000. running mean: 3.854258\n",
      "ep 3228: ep_len:534 episode reward: total was -0.610000. running mean: 3.809615\n",
      "ep 3228: ep_len:93 episode reward: total was 21.250000. running mean: 3.984019\n",
      "ep 3228: ep_len:522 episode reward: total was -52.930000. running mean: 3.414879\n",
      "ep 3228: ep_len:533 episode reward: total was 11.900000. running mean: 3.499730\n",
      "epsilon:0.056848 episode_count: 22603. steps_count: 9775210.000000\n",
      "Time elapsed:  29337.820474863052\n",
      "ep 3229: ep_len:662 episode reward: total was -18.650000. running mean: 3.278233\n",
      "ep 3229: ep_len:500 episode reward: total was -0.260000. running mean: 3.242851\n",
      "ep 3229: ep_len:500 episode reward: total was 30.120000. running mean: 3.511622\n",
      "ep 3229: ep_len:426 episode reward: total was -30.210000. running mean: 3.174406\n",
      "ep 3229: ep_len:3 episode reward: total was 1.010000. running mean: 3.152762\n",
      "ep 3229: ep_len:505 episode reward: total was 5.200000. running mean: 3.173234\n",
      "ep 3229: ep_len:523 episode reward: total was -34.320000. running mean: 2.798302\n",
      "epsilon:0.056803 episode_count: 22610. steps_count: 9778329.000000\n",
      "Time elapsed:  29354.88725376129\n",
      "ep 3230: ep_len:589 episode reward: total was 54.400000. running mean: 3.314319\n",
      "ep 3230: ep_len:528 episode reward: total was 4.200000. running mean: 3.323176\n",
      "ep 3230: ep_len:652 episode reward: total was -16.480000. running mean: 3.125144\n",
      "ep 3230: ep_len:132 episode reward: total was 17.120000. running mean: 3.265093\n",
      "ep 3230: ep_len:3 episode reward: total was 1.010000. running mean: 3.242542\n",
      "ep 3230: ep_len:325 episode reward: total was 16.240000. running mean: 3.372516\n",
      "ep 3230: ep_len:666 episode reward: total was 6.720000. running mean: 3.405991\n",
      "epsilon:0.056759 episode_count: 22617. steps_count: 9781224.000000\n",
      "Time elapsed:  29367.610753536224\n",
      "ep 3231: ep_len:651 episode reward: total was 11.150000. running mean: 3.483431\n",
      "ep 3231: ep_len:189 episode reward: total was 16.350000. running mean: 3.612097\n",
      "ep 3231: ep_len:634 episode reward: total was 16.560000. running mean: 3.741576\n",
      "ep 3231: ep_len:505 episode reward: total was 45.840000. running mean: 4.162560\n",
      "ep 3231: ep_len:3 episode reward: total was 1.010000. running mean: 4.131034\n",
      "ep 3231: ep_len:699 episode reward: total was -22.400000. running mean: 3.865724\n",
      "ep 3231: ep_len:339 episode reward: total was -5.190000. running mean: 3.775167\n",
      "epsilon:0.056715 episode_count: 22624. steps_count: 9784244.000000\n",
      "Time elapsed:  29375.69524049759\n",
      "ep 3232: ep_len:560 episode reward: total was 60.080000. running mean: 4.338215\n",
      "ep 3232: ep_len:613 episode reward: total was 7.880000. running mean: 4.373633\n",
      "ep 3232: ep_len:500 episode reward: total was 21.950000. running mean: 4.549397\n",
      "ep 3232: ep_len:512 episode reward: total was 48.940000. running mean: 4.993303\n",
      "ep 3232: ep_len:3 episode reward: total was 1.010000. running mean: 4.953470\n",
      "ep 3232: ep_len:515 episode reward: total was 15.990000. running mean: 5.063835\n",
      "ep 3232: ep_len:540 episode reward: total was -34.650000. running mean: 4.666697\n",
      "epsilon:0.056670 episode_count: 22631. steps_count: 9787487.000000\n",
      "Time elapsed:  29384.316061019897\n",
      "ep 3233: ep_len:544 episode reward: total was -0.200000. running mean: 4.618030\n",
      "ep 3233: ep_len:613 episode reward: total was 76.630000. running mean: 5.338149\n",
      "ep 3233: ep_len:527 episode reward: total was -48.290000. running mean: 4.801868\n",
      "ep 3233: ep_len:540 episode reward: total was 50.580000. running mean: 5.259649\n",
      "ep 3233: ep_len:3 episode reward: total was 1.010000. running mean: 5.217153\n",
      "ep 3233: ep_len:500 episode reward: total was -55.080000. running mean: 4.614181\n",
      "ep 3233: ep_len:500 episode reward: total was -142.940000. running mean: 3.138639\n",
      "epsilon:0.056626 episode_count: 22638. steps_count: 9790714.000000\n",
      "Time elapsed:  29397.56673336029\n",
      "ep 3234: ep_len:579 episode reward: total was 36.400000. running mean: 3.471253\n",
      "ep 3234: ep_len:500 episode reward: total was -13.980000. running mean: 3.296740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3234: ep_len:548 episode reward: total was -27.490000. running mean: 2.988873\n",
      "ep 3234: ep_len:500 episode reward: total was -0.690000. running mean: 2.952084\n",
      "ep 3234: ep_len:3 episode reward: total was 1.010000. running mean: 2.932664\n",
      "ep 3234: ep_len:514 episode reward: total was -14.230000. running mean: 2.761037\n",
      "ep 3234: ep_len:211 episode reward: total was -23.590000. running mean: 2.497526\n",
      "epsilon:0.056582 episode_count: 22645. steps_count: 9793569.000000\n",
      "Time elapsed:  29405.11626148224\n",
      "ep 3235: ep_len:251 episode reward: total was -11.720000. running mean: 2.355351\n",
      "ep 3235: ep_len:620 episode reward: total was 24.480000. running mean: 2.576598\n",
      "ep 3235: ep_len:282 episode reward: total was -21.200000. running mean: 2.338832\n",
      "ep 3235: ep_len:120 episode reward: total was 15.070000. running mean: 2.466143\n",
      "ep 3235: ep_len:93 episode reward: total was 22.230000. running mean: 2.663782\n",
      "ep 3235: ep_len:565 episode reward: total was -14.920000. running mean: 2.487944\n",
      "ep 3235: ep_len:542 episode reward: total was -20.900000. running mean: 2.254065\n",
      "epsilon:0.056537 episode_count: 22652. steps_count: 9796042.000000\n",
      "Time elapsed:  29411.817909002304\n",
      "ep 3236: ep_len:583 episode reward: total was -44.410000. running mean: 1.787424\n",
      "ep 3236: ep_len:500 episode reward: total was 94.340000. running mean: 2.712950\n",
      "ep 3236: ep_len:625 episode reward: total was 8.740000. running mean: 2.773220\n",
      "ep 3236: ep_len:514 episode reward: total was 39.960000. running mean: 3.145088\n",
      "ep 3236: ep_len:3 episode reward: total was 1.010000. running mean: 3.123737\n",
      "ep 3236: ep_len:272 episode reward: total was 24.830000. running mean: 3.340800\n",
      "ep 3236: ep_len:605 episode reward: total was 13.310000. running mean: 3.440492\n",
      "epsilon:0.056493 episode_count: 22659. steps_count: 9799144.000000\n",
      "Time elapsed:  29425.73493385315\n",
      "ep 3237: ep_len:557 episode reward: total was 23.290000. running mean: 3.638987\n",
      "ep 3237: ep_len:505 episode reward: total was -23.320000. running mean: 3.369397\n",
      "ep 3237: ep_len:500 episode reward: total was 7.740000. running mean: 3.413103\n",
      "ep 3237: ep_len:596 episode reward: total was 4.290000. running mean: 3.421872\n",
      "ep 3237: ep_len:3 episode reward: total was 1.010000. running mean: 3.397753\n",
      "ep 3237: ep_len:503 episode reward: total was -16.610000. running mean: 3.197676\n",
      "ep 3237: ep_len:551 episode reward: total was 12.460000. running mean: 3.290299\n",
      "epsilon:0.056449 episode_count: 22666. steps_count: 9802359.000000\n",
      "Time elapsed:  29434.180846214294\n",
      "ep 3238: ep_len:559 episode reward: total was 55.940000. running mean: 3.816796\n",
      "ep 3238: ep_len:500 episode reward: total was 17.970000. running mean: 3.958328\n",
      "ep 3238: ep_len:536 episode reward: total was -6.950000. running mean: 3.849245\n",
      "ep 3238: ep_len:500 episode reward: total was 16.560000. running mean: 3.976352\n",
      "ep 3238: ep_len:3 episode reward: total was 1.010000. running mean: 3.946689\n",
      "ep 3238: ep_len:500 episode reward: total was 3.210000. running mean: 3.939322\n",
      "ep 3238: ep_len:500 episode reward: total was -11.090000. running mean: 3.789029\n",
      "epsilon:0.056404 episode_count: 22673. steps_count: 9805457.000000\n",
      "Time elapsed:  29447.10417842865\n",
      "ep 3239: ep_len:563 episode reward: total was 33.600000. running mean: 4.087138\n",
      "ep 3239: ep_len:500 episode reward: total was 27.580000. running mean: 4.322067\n",
      "ep 3239: ep_len:435 episode reward: total was -22.170000. running mean: 4.057146\n",
      "ep 3239: ep_len:625 episode reward: total was 81.840000. running mean: 4.834975\n",
      "ep 3239: ep_len:108 episode reward: total was 32.270000. running mean: 5.109325\n",
      "ep 3239: ep_len:500 episode reward: total was -71.140000. running mean: 4.346832\n",
      "ep 3239: ep_len:500 episode reward: total was -40.120000. running mean: 3.902164\n",
      "epsilon:0.056360 episode_count: 22680. steps_count: 9808688.000000\n",
      "Time elapsed:  29455.625958681107\n",
      "ep 3240: ep_len:220 episode reward: total was 13.270000. running mean: 3.995842\n",
      "ep 3240: ep_len:500 episode reward: total was 2.990000. running mean: 3.985784\n",
      "ep 3240: ep_len:507 episode reward: total was -12.420000. running mean: 3.821726\n",
      "ep 3240: ep_len:599 episode reward: total was 14.720000. running mean: 3.930708\n",
      "ep 3240: ep_len:83 episode reward: total was 15.240000. running mean: 4.043801\n",
      "ep 3240: ep_len:558 episode reward: total was 14.770000. running mean: 4.151063\n",
      "ep 3240: ep_len:599 episode reward: total was 34.800000. running mean: 4.457553\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.056316 episode_count: 22687. steps_count: 9811754.000000\n",
      "Time elapsed:  29467.754561662674\n",
      "ep 3241: ep_len:547 episode reward: total was 3.660000. running mean: 4.449577\n",
      "ep 3241: ep_len:592 episode reward: total was -106.970000. running mean: 3.335381\n",
      "ep 3241: ep_len:500 episode reward: total was 19.340000. running mean: 3.495428\n",
      "ep 3241: ep_len:153 episode reward: total was 17.220000. running mean: 3.632673\n",
      "ep 3241: ep_len:3 episode reward: total was 1.010000. running mean: 3.606447\n",
      "ep 3241: ep_len:179 episode reward: total was 29.160000. running mean: 3.861982\n",
      "ep 3241: ep_len:586 episode reward: total was -14.520000. running mean: 3.678162\n",
      "epsilon:0.056271 episode_count: 22694. steps_count: 9814314.000000\n",
      "Time elapsed:  29474.426158428192\n",
      "ep 3242: ep_len:652 episode reward: total was -30.960000. running mean: 3.331781\n",
      "ep 3242: ep_len:550 episode reward: total was 1.930000. running mean: 3.317763\n",
      "ep 3242: ep_len:500 episode reward: total was -11.070000. running mean: 3.173885\n",
      "ep 3242: ep_len:145 episode reward: total was 7.170000. running mean: 3.213846\n",
      "ep 3242: ep_len:85 episode reward: total was -54.790000. running mean: 2.633808\n",
      "ep 3242: ep_len:500 episode reward: total was 4.580000. running mean: 2.653270\n",
      "ep 3242: ep_len:685 episode reward: total was -169.430000. running mean: 0.932437\n",
      "epsilon:0.056227 episode_count: 22701. steps_count: 9817431.000000\n",
      "Time elapsed:  29482.70950818062\n",
      "ep 3243: ep_len:541 episode reward: total was -98.100000. running mean: -0.057887\n",
      "ep 3243: ep_len:500 episode reward: total was 20.790000. running mean: 0.150592\n",
      "ep 3243: ep_len:76 episode reward: total was 1.270000. running mean: 0.161786\n",
      "ep 3243: ep_len:500 episode reward: total was -26.770000. running mean: -0.107532\n",
      "ep 3243: ep_len:3 episode reward: total was 1.010000. running mean: -0.096357\n",
      "ep 3243: ep_len:509 episode reward: total was -4.700000. running mean: -0.142393\n",
      "ep 3243: ep_len:502 episode reward: total was -9.310000. running mean: -0.234069\n",
      "epsilon:0.056183 episode_count: 22708. steps_count: 9820062.000000\n",
      "Time elapsed:  29489.786622047424\n",
      "ep 3244: ep_len:728 episode reward: total was -35.900000. running mean: -0.590729\n",
      "ep 3244: ep_len:500 episode reward: total was -31.090000. running mean: -0.895721\n",
      "ep 3244: ep_len:441 episode reward: total was 23.840000. running mean: -0.648364\n",
      "ep 3244: ep_len:500 episode reward: total was 39.980000. running mean: -0.242080\n",
      "ep 3244: ep_len:3 episode reward: total was 1.010000. running mean: -0.229560\n",
      "ep 3244: ep_len:542 episode reward: total was 23.990000. running mean: 0.012636\n",
      "ep 3244: ep_len:624 episode reward: total was -7.860000. running mean: -0.066090\n",
      "epsilon:0.056138 episode_count: 22715. steps_count: 9823400.000000\n",
      "Time elapsed:  29498.482084989548\n",
      "ep 3245: ep_len:640 episode reward: total was 23.650000. running mean: 0.171071\n",
      "ep 3245: ep_len:188 episode reward: total was -4.140000. running mean: 0.127960\n",
      "ep 3245: ep_len:594 episode reward: total was -14.800000. running mean: -0.021320\n",
      "ep 3245: ep_len:500 episode reward: total was 9.080000. running mean: 0.069693\n",
      "ep 3245: ep_len:91 episode reward: total was 14.220000. running mean: 0.211196\n",
      "ep 3245: ep_len:500 episode reward: total was -2.450000. running mean: 0.184585\n",
      "ep 3245: ep_len:500 episode reward: total was -2.560000. running mean: 0.157139\n",
      "epsilon:0.056094 episode_count: 22722. steps_count: 9826413.000000\n",
      "Time elapsed:  29506.523293972015\n",
      "ep 3246: ep_len:213 episode reward: total was -39.730000. running mean: -0.241733\n",
      "ep 3246: ep_len:627 episode reward: total was -20.500000. running mean: -0.444315\n",
      "ep 3246: ep_len:591 episode reward: total was -37.020000. running mean: -0.810072\n",
      "ep 3246: ep_len:500 episode reward: total was 0.480000. running mean: -0.797172\n",
      "ep 3246: ep_len:3 episode reward: total was 1.010000. running mean: -0.779100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3246: ep_len:1097 episode reward: total was -761.330000. running mean: -8.384609\n",
      "ep 3246: ep_len:272 episode reward: total was -6.740000. running mean: -8.368163\n",
      "epsilon:0.056050 episode_count: 22729. steps_count: 9829716.000000\n",
      "Time elapsed:  29515.151668071747\n",
      "ep 3247: ep_len:500 episode reward: total was 9.580000. running mean: -8.188681\n",
      "ep 3247: ep_len:500 episode reward: total was 46.730000. running mean: -7.639494\n",
      "ep 3247: ep_len:79 episode reward: total was 3.750000. running mean: -7.525599\n",
      "ep 3247: ep_len:56 episode reward: total was 2.350000. running mean: -7.426843\n",
      "ep 3247: ep_len:102 episode reward: total was 30.250000. running mean: -7.050075\n",
      "ep 3247: ep_len:500 episode reward: total was -8.570000. running mean: -7.065274\n",
      "ep 3247: ep_len:500 episode reward: total was 18.410000. running mean: -6.810521\n",
      "epsilon:0.056005 episode_count: 22736. steps_count: 9831953.000000\n",
      "Time elapsed:  29521.430137872696\n",
      "ep 3248: ep_len:693 episode reward: total was -95.040000. running mean: -7.692816\n",
      "ep 3248: ep_len:500 episode reward: total was 81.180000. running mean: -6.804088\n",
      "ep 3248: ep_len:74 episode reward: total was -1.230000. running mean: -6.748347\n",
      "ep 3248: ep_len:42 episode reward: total was 2.290000. running mean: -6.657964\n",
      "ep 3248: ep_len:93 episode reward: total was 16.230000. running mean: -6.429084\n",
      "ep 3248: ep_len:594 episode reward: total was 3.750000. running mean: -6.327293\n",
      "ep 3248: ep_len:596 episode reward: total was -0.110000. running mean: -6.265120\n",
      "epsilon:0.055961 episode_count: 22743. steps_count: 9834545.000000\n",
      "Time elapsed:  29525.811155319214\n",
      "ep 3249: ep_len:251 episode reward: total was 0.190000. running mean: -6.200569\n",
      "ep 3249: ep_len:500 episode reward: total was 10.030000. running mean: -6.038263\n",
      "ep 3249: ep_len:500 episode reward: total was -4.580000. running mean: -6.023681\n",
      "ep 3249: ep_len:500 episode reward: total was 24.490000. running mean: -5.718544\n",
      "ep 3249: ep_len:3 episode reward: total was 1.010000. running mean: -5.651259\n",
      "ep 3249: ep_len:516 episode reward: total was 13.400000. running mean: -5.460746\n",
      "ep 3249: ep_len:182 episode reward: total was -12.130000. running mean: -5.527438\n",
      "epsilon:0.055917 episode_count: 22750. steps_count: 9836997.000000\n",
      "Time elapsed:  29532.418694019318\n",
      "ep 3250: ep_len:500 episode reward: total was 52.440000. running mean: -4.947764\n",
      "ep 3250: ep_len:583 episode reward: total was 53.830000. running mean: -4.359986\n",
      "ep 3250: ep_len:572 episode reward: total was -8.350000. running mean: -4.399887\n",
      "ep 3250: ep_len:155 episode reward: total was 25.110000. running mean: -4.104788\n",
      "ep 3250: ep_len:3 episode reward: total was 1.010000. running mean: -4.053640\n",
      "ep 3250: ep_len:500 episode reward: total was 31.420000. running mean: -3.698903\n",
      "ep 3250: ep_len:572 episode reward: total was 19.090000. running mean: -3.471014\n",
      "epsilon:0.055872 episode_count: 22757. steps_count: 9839882.000000\n",
      "Time elapsed:  29538.2065179348\n",
      "ep 3251: ep_len:536 episode reward: total was -2.030000. running mean: -3.456604\n",
      "ep 3251: ep_len:500 episode reward: total was 6.000000. running mean: -3.362038\n",
      "ep 3251: ep_len:634 episode reward: total was -1.780000. running mean: -3.346218\n",
      "ep 3251: ep_len:501 episode reward: total was 28.410000. running mean: -3.028656\n",
      "ep 3251: ep_len:42 episode reward: total was 15.000000. running mean: -2.848369\n",
      "ep 3251: ep_len:500 episode reward: total was 43.110000. running mean: -2.388785\n",
      "ep 3251: ep_len:607 episode reward: total was -5.910000. running mean: -2.423998\n",
      "epsilon:0.055828 episode_count: 22764. steps_count: 9843202.000000\n",
      "Time elapsed:  29547.08808517456\n",
      "ep 3252: ep_len:500 episode reward: total was 67.370000. running mean: -1.726058\n",
      "ep 3252: ep_len:272 episode reward: total was -8.570000. running mean: -1.794497\n",
      "ep 3252: ep_len:77 episode reward: total was 2.260000. running mean: -1.753952\n",
      "ep 3252: ep_len:500 episode reward: total was 15.150000. running mean: -1.584913\n",
      "ep 3252: ep_len:3 episode reward: total was 1.010000. running mean: -1.558963\n",
      "ep 3252: ep_len:250 episode reward: total was 35.830000. running mean: -1.185074\n",
      "ep 3252: ep_len:540 episode reward: total was 10.940000. running mean: -1.063823\n",
      "epsilon:0.055784 episode_count: 22771. steps_count: 9845344.000000\n",
      "Time elapsed:  29553.039328336716\n",
      "ep 3253: ep_len:670 episode reward: total was -12.200000. running mean: -1.175185\n",
      "ep 3253: ep_len:500 episode reward: total was -16.260000. running mean: -1.326033\n",
      "ep 3253: ep_len:538 episode reward: total was 5.000000. running mean: -1.262773\n",
      "ep 3253: ep_len:500 episode reward: total was 15.380000. running mean: -1.096345\n",
      "ep 3253: ep_len:3 episode reward: total was 1.010000. running mean: -1.075281\n",
      "ep 3253: ep_len:500 episode reward: total was 16.160000. running mean: -0.902929\n",
      "ep 3253: ep_len:597 episode reward: total was 25.790000. running mean: -0.635999\n",
      "epsilon:0.055739 episode_count: 22778. steps_count: 9848652.000000\n",
      "Time elapsed:  29561.963361024857\n",
      "ep 3254: ep_len:528 episode reward: total was 81.260000. running mean: 0.182961\n",
      "ep 3254: ep_len:501 episode reward: total was 26.080000. running mean: 0.441931\n",
      "ep 3254: ep_len:583 episode reward: total was 4.560000. running mean: 0.483112\n",
      "ep 3254: ep_len:604 episode reward: total was -20.890000. running mean: 0.269381\n",
      "ep 3254: ep_len:93 episode reward: total was -52.260000. running mean: -0.255913\n",
      "ep 3254: ep_len:519 episode reward: total was -19.430000. running mean: -0.447654\n",
      "ep 3254: ep_len:614 episode reward: total was 13.380000. running mean: -0.309378\n",
      "epsilon:0.055695 episode_count: 22785. steps_count: 9852094.000000\n",
      "Time elapsed:  29570.90497159958\n",
      "ep 3255: ep_len:514 episode reward: total was 67.440000. running mean: 0.368116\n",
      "ep 3255: ep_len:510 episode reward: total was 72.930000. running mean: 1.093735\n",
      "ep 3255: ep_len:500 episode reward: total was 28.410000. running mean: 1.366898\n",
      "ep 3255: ep_len:500 episode reward: total was 27.630000. running mean: 1.629529\n",
      "ep 3255: ep_len:3 episode reward: total was 1.010000. running mean: 1.623333\n",
      "ep 3255: ep_len:529 episode reward: total was 12.760000. running mean: 1.734700\n",
      "ep 3255: ep_len:605 episode reward: total was 44.860000. running mean: 2.165953\n",
      "epsilon:0.055651 episode_count: 22792. steps_count: 9855255.000000\n",
      "Time elapsed:  29579.295092582703\n",
      "ep 3256: ep_len:500 episode reward: total was 48.070000. running mean: 2.624994\n",
      "ep 3256: ep_len:500 episode reward: total was 7.640000. running mean: 2.675144\n",
      "ep 3256: ep_len:547 episode reward: total was -50.610000. running mean: 2.142292\n",
      "ep 3256: ep_len:500 episode reward: total was 16.120000. running mean: 2.282069\n",
      "ep 3256: ep_len:47 episode reward: total was 15.510000. running mean: 2.414349\n",
      "ep 3256: ep_len:500 episode reward: total was 31.880000. running mean: 2.709005\n",
      "ep 3256: ep_len:211 episode reward: total was -12.890000. running mean: 2.553015\n",
      "epsilon:0.055606 episode_count: 22799. steps_count: 9858060.000000\n",
      "Time elapsed:  29586.826776266098\n",
      "ep 3257: ep_len:500 episode reward: total was 58.290000. running mean: 3.110385\n",
      "ep 3257: ep_len:500 episode reward: total was 52.320000. running mean: 3.602481\n",
      "ep 3257: ep_len:566 episode reward: total was -49.780000. running mean: 3.068656\n",
      "ep 3257: ep_len:501 episode reward: total was 18.940000. running mean: 3.227370\n",
      "ep 3257: ep_len:50 episode reward: total was 21.510000. running mean: 3.410196\n",
      "ep 3257: ep_len:500 episode reward: total was 25.940000. running mean: 3.635494\n",
      "ep 3257: ep_len:500 episode reward: total was 48.870000. running mean: 4.087839\n",
      "epsilon:0.055562 episode_count: 22806. steps_count: 9861177.000000\n",
      "Time elapsed:  29595.011015176773\n",
      "ep 3258: ep_len:506 episode reward: total was 25.030000. running mean: 4.297261\n",
      "ep 3258: ep_len:548 episode reward: total was 25.960000. running mean: 4.513888\n",
      "ep 3258: ep_len:634 episode reward: total was -20.390000. running mean: 4.264849\n",
      "ep 3258: ep_len:576 episode reward: total was 30.930000. running mean: 4.531501\n",
      "ep 3258: ep_len:3 episode reward: total was 1.010000. running mean: 4.496286\n",
      "ep 3258: ep_len:184 episode reward: total was 31.170000. running mean: 4.763023\n",
      "ep 3258: ep_len:600 episode reward: total was 14.710000. running mean: 4.862493\n",
      "epsilon:0.055518 episode_count: 22813. steps_count: 9864228.000000\n",
      "Time elapsed:  29603.22617316246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3259: ep_len:182 episode reward: total was 12.120000. running mean: 4.935068\n",
      "ep 3259: ep_len:603 episode reward: total was 58.710000. running mean: 5.472817\n",
      "ep 3259: ep_len:553 episode reward: total was -22.350000. running mean: 5.194589\n",
      "ep 3259: ep_len:388 episode reward: total was -9.410000. running mean: 5.048543\n",
      "ep 3259: ep_len:3 episode reward: total was 1.010000. running mean: 5.008158\n",
      "ep 3259: ep_len:513 episode reward: total was -14.120000. running mean: 4.816876\n",
      "ep 3259: ep_len:546 episode reward: total was 21.580000. running mean: 4.984507\n",
      "epsilon:0.055473 episode_count: 22820. steps_count: 9867016.000000\n",
      "Time elapsed:  29610.72004055977\n",
      "ep 3260: ep_len:572 episode reward: total was -56.090000. running mean: 4.373762\n",
      "ep 3260: ep_len:504 episode reward: total was -38.120000. running mean: 3.948825\n",
      "ep 3260: ep_len:561 episode reward: total was -15.760000. running mean: 3.751736\n",
      "ep 3260: ep_len:611 episode reward: total was 47.450000. running mean: 4.188719\n",
      "ep 3260: ep_len:3 episode reward: total was 1.010000. running mean: 4.156932\n",
      "ep 3260: ep_len:525 episode reward: total was -12.640000. running mean: 3.988962\n",
      "ep 3260: ep_len:597 episode reward: total was -8.540000. running mean: 3.863673\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.055429 episode_count: 22827. steps_count: 9870389.000000\n",
      "Time elapsed:  29624.618456363678\n",
      "ep 3261: ep_len:603 episode reward: total was 60.950000. running mean: 4.434536\n",
      "ep 3261: ep_len:520 episode reward: total was -1.140000. running mean: 4.378791\n",
      "ep 3261: ep_len:428 episode reward: total was 61.750000. running mean: 4.952503\n",
      "ep 3261: ep_len:569 episode reward: total was 58.350000. running mean: 5.486478\n",
      "ep 3261: ep_len:3 episode reward: total was 1.010000. running mean: 5.441713\n",
      "ep 3261: ep_len:246 episode reward: total was 48.460000. running mean: 5.871896\n",
      "ep 3261: ep_len:500 episode reward: total was -1.980000. running mean: 5.793377\n",
      "epsilon:0.055385 episode_count: 22834. steps_count: 9873258.000000\n",
      "Time elapsed:  29632.172330856323\n",
      "ep 3262: ep_len:563 episode reward: total was 11.170000. running mean: 5.847143\n",
      "ep 3262: ep_len:500 episode reward: total was 88.620000. running mean: 6.674872\n",
      "ep 3262: ep_len:565 episode reward: total was -58.890000. running mean: 6.019223\n",
      "ep 3262: ep_len:540 episode reward: total was -70.860000. running mean: 5.250431\n",
      "ep 3262: ep_len:85 episode reward: total was 24.260000. running mean: 5.440526\n",
      "ep 3262: ep_len:501 episode reward: total was -34.130000. running mean: 5.044821\n",
      "ep 3262: ep_len:500 episode reward: total was 22.080000. running mean: 5.215173\n",
      "epsilon:0.055340 episode_count: 22841. steps_count: 9876512.000000\n",
      "Time elapsed:  29640.5827395916\n",
      "ep 3263: ep_len:262 episode reward: total was 4.800000. running mean: 5.211021\n",
      "ep 3263: ep_len:547 episode reward: total was 26.530000. running mean: 5.424211\n",
      "ep 3263: ep_len:647 episode reward: total was -154.840000. running mean: 3.821569\n",
      "ep 3263: ep_len:52 episode reward: total was -5.170000. running mean: 3.731653\n",
      "ep 3263: ep_len:76 episode reward: total was 22.790000. running mean: 3.922237\n",
      "ep 3263: ep_len:622 episode reward: total was -4.400000. running mean: 3.839014\n",
      "ep 3263: ep_len:575 episode reward: total was 18.550000. running mean: 3.986124\n",
      "epsilon:0.055296 episode_count: 22848. steps_count: 9879293.000000\n",
      "Time elapsed:  29648.07580757141\n",
      "ep 3264: ep_len:504 episode reward: total was 45.420000. running mean: 4.400463\n",
      "ep 3264: ep_len:562 episode reward: total was -11.100000. running mean: 4.245458\n",
      "ep 3264: ep_len:665 episode reward: total was -39.520000. running mean: 3.807804\n",
      "ep 3264: ep_len:531 episode reward: total was 14.430000. running mean: 3.914026\n",
      "ep 3264: ep_len:85 episode reward: total was 27.660000. running mean: 4.151485\n",
      "ep 3264: ep_len:576 episode reward: total was -2.080000. running mean: 4.089171\n",
      "ep 3264: ep_len:282 episode reward: total was -56.810000. running mean: 3.480179\n",
      "epsilon:0.055252 episode_count: 22855. steps_count: 9882498.000000\n",
      "Time elapsed:  29655.442759752274\n",
      "ep 3265: ep_len:588 episode reward: total was 30.000000. running mean: 3.745377\n",
      "ep 3265: ep_len:500 episode reward: total was 7.280000. running mean: 3.780723\n",
      "ep 3265: ep_len:354 episode reward: total was 39.320000. running mean: 4.136116\n",
      "ep 3265: ep_len:500 episode reward: total was 11.700000. running mean: 4.211755\n",
      "ep 3265: ep_len:3 episode reward: total was 1.010000. running mean: 4.179737\n",
      "ep 3265: ep_len:519 episode reward: total was -6.140000. running mean: 4.076540\n",
      "ep 3265: ep_len:564 episode reward: total was 13.540000. running mean: 4.171175\n",
      "epsilon:0.055207 episode_count: 22862. steps_count: 9885526.000000\n",
      "Time elapsed:  29673.6157681942\n",
      "ep 3266: ep_len:698 episode reward: total was -36.680000. running mean: 3.762663\n",
      "ep 3266: ep_len:586 episode reward: total was 13.460000. running mean: 3.859636\n",
      "ep 3266: ep_len:641 episode reward: total was -22.150000. running mean: 3.599540\n",
      "ep 3266: ep_len:510 episode reward: total was 27.430000. running mean: 3.837844\n",
      "ep 3266: ep_len:3 episode reward: total was 1.010000. running mean: 3.809566\n",
      "ep 3266: ep_len:500 episode reward: total was 16.570000. running mean: 3.937170\n",
      "ep 3266: ep_len:582 episode reward: total was 43.130000. running mean: 4.329099\n",
      "epsilon:0.055163 episode_count: 22869. steps_count: 9889046.000000\n",
      "Time elapsed:  29688.496335744858\n",
      "ep 3267: ep_len:604 episode reward: total was 20.040000. running mean: 4.486208\n",
      "ep 3267: ep_len:510 episode reward: total was 21.470000. running mean: 4.656046\n",
      "ep 3267: ep_len:393 episode reward: total was 46.540000. running mean: 5.074885\n",
      "ep 3267: ep_len:547 episode reward: total was 47.960000. running mean: 5.503736\n",
      "ep 3267: ep_len:88 episode reward: total was 18.230000. running mean: 5.630999\n",
      "ep 3267: ep_len:500 episode reward: total was 22.850000. running mean: 5.803189\n",
      "ep 3267: ep_len:260 episode reward: total was -7.380000. running mean: 5.671357\n",
      "epsilon:0.055119 episode_count: 22876. steps_count: 9891948.000000\n",
      "Time elapsed:  29704.58223080635\n",
      "ep 3268: ep_len:564 episode reward: total was -22.900000. running mean: 5.385643\n",
      "ep 3268: ep_len:506 episode reward: total was -18.050000. running mean: 5.151287\n",
      "ep 3268: ep_len:66 episode reward: total was 0.130000. running mean: 5.101074\n",
      "ep 3268: ep_len:500 episode reward: total was -48.060000. running mean: 4.569463\n",
      "ep 3268: ep_len:106 episode reward: total was 29.770000. running mean: 4.821469\n",
      "ep 3268: ep_len:503 episode reward: total was -125.660000. running mean: 3.516654\n",
      "ep 3268: ep_len:537 episode reward: total was 11.460000. running mean: 3.596088\n",
      "epsilon:0.055074 episode_count: 22883. steps_count: 9894730.000000\n",
      "Time elapsed:  29710.281946897507\n",
      "ep 3269: ep_len:516 episode reward: total was 51.570000. running mean: 4.075827\n",
      "ep 3269: ep_len:500 episode reward: total was 31.790000. running mean: 4.352968\n",
      "ep 3269: ep_len:585 episode reward: total was 11.640000. running mean: 4.425839\n",
      "ep 3269: ep_len:531 episode reward: total was 8.160000. running mean: 4.463180\n",
      "ep 3269: ep_len:3 episode reward: total was 1.010000. running mean: 4.428649\n",
      "ep 3269: ep_len:563 episode reward: total was 29.440000. running mean: 4.678762\n",
      "ep 3269: ep_len:326 episode reward: total was -73.510000. running mean: 3.896874\n",
      "epsilon:0.055030 episode_count: 22890. steps_count: 9897754.000000\n",
      "Time elapsed:  29716.122973442078\n",
      "ep 3270: ep_len:556 episode reward: total was -37.460000. running mean: 3.483306\n",
      "ep 3270: ep_len:568 episode reward: total was 4.890000. running mean: 3.497373\n",
      "ep 3270: ep_len:500 episode reward: total was 4.390000. running mean: 3.506299\n",
      "ep 3270: ep_len:608 episode reward: total was 43.350000. running mean: 3.904736\n",
      "ep 3270: ep_len:3 episode reward: total was -0.490000. running mean: 3.860789\n",
      "ep 3270: ep_len:627 episode reward: total was -7.140000. running mean: 3.750781\n",
      "ep 3270: ep_len:562 episode reward: total was -59.980000. running mean: 3.113473\n",
      "epsilon:0.054986 episode_count: 22897. steps_count: 9901178.000000\n",
      "Time elapsed:  29731.588689804077\n",
      "ep 3271: ep_len:617 episode reward: total was 63.660000. running mean: 3.718938\n",
      "ep 3271: ep_len:366 episode reward: total was -17.410000. running mean: 3.507649\n",
      "ep 3271: ep_len:646 episode reward: total was 3.970000. running mean: 3.512272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3271: ep_len:56 episode reward: total was -0.190000. running mean: 3.475250\n",
      "ep 3271: ep_len:84 episode reward: total was 20.670000. running mean: 3.647197\n",
      "ep 3271: ep_len:500 episode reward: total was -57.940000. running mean: 3.031325\n",
      "ep 3271: ep_len:275 episode reward: total was -23.150000. running mean: 2.769512\n",
      "epsilon:0.054941 episode_count: 22904. steps_count: 9903722.000000\n",
      "Time elapsed:  29738.431643009186\n",
      "ep 3272: ep_len:608 episode reward: total was -55.550000. running mean: 2.186317\n",
      "ep 3272: ep_len:500 episode reward: total was -43.210000. running mean: 1.732354\n",
      "ep 3272: ep_len:362 episode reward: total was 31.350000. running mean: 2.028530\n",
      "ep 3272: ep_len:170 episode reward: total was 17.180000. running mean: 2.180045\n",
      "ep 3272: ep_len:3 episode reward: total was 1.010000. running mean: 2.168344\n",
      "ep 3272: ep_len:300 episode reward: total was 31.110000. running mean: 2.457761\n",
      "ep 3272: ep_len:518 episode reward: total was 7.730000. running mean: 2.510483\n",
      "epsilon:0.054897 episode_count: 22911. steps_count: 9906183.000000\n",
      "Time elapsed:  29745.31534051895\n",
      "ep 3273: ep_len:608 episode reward: total was -3.790000. running mean: 2.447478\n",
      "ep 3273: ep_len:507 episode reward: total was 19.800000. running mean: 2.621004\n",
      "ep 3273: ep_len:580 episode reward: total was -11.400000. running mean: 2.480794\n",
      "ep 3273: ep_len:500 episode reward: total was 20.660000. running mean: 2.662586\n",
      "ep 3273: ep_len:3 episode reward: total was 1.010000. running mean: 2.646060\n",
      "ep 3273: ep_len:589 episode reward: total was 12.440000. running mean: 2.743999\n",
      "ep 3273: ep_len:602 episode reward: total was 13.940000. running mean: 2.855959\n",
      "epsilon:0.054853 episode_count: 22918. steps_count: 9909572.000000\n",
      "Time elapsed:  29760.09551668167\n",
      "ep 3274: ep_len:566 episode reward: total was -20.830000. running mean: 2.619100\n",
      "ep 3274: ep_len:549 episode reward: total was 16.910000. running mean: 2.762009\n",
      "ep 3274: ep_len:610 episode reward: total was -129.020000. running mean: 1.444189\n",
      "ep 3274: ep_len:393 episode reward: total was -2.530000. running mean: 1.404447\n",
      "ep 3274: ep_len:88 episode reward: total was 30.600000. running mean: 1.696402\n",
      "ep 3274: ep_len:695 episode reward: total was 21.850000. running mean: 1.897938\n",
      "ep 3274: ep_len:613 episode reward: total was 31.270000. running mean: 2.191659\n",
      "epsilon:0.054808 episode_count: 22925. steps_count: 9913086.000000\n",
      "Time elapsed:  29769.189187288284\n",
      "ep 3275: ep_len:506 episode reward: total was 42.740000. running mean: 2.597142\n",
      "ep 3275: ep_len:525 episode reward: total was 15.610000. running mean: 2.727271\n",
      "ep 3275: ep_len:628 episode reward: total was -23.480000. running mean: 2.465198\n",
      "ep 3275: ep_len:582 episode reward: total was 63.770000. running mean: 3.078246\n",
      "ep 3275: ep_len:3 episode reward: total was 1.010000. running mean: 3.057564\n",
      "ep 3275: ep_len:641 episode reward: total was -95.170000. running mean: 2.075288\n",
      "ep 3275: ep_len:622 episode reward: total was 49.940000. running mean: 2.553935\n",
      "epsilon:0.054764 episode_count: 22932. steps_count: 9916593.000000\n",
      "Time elapsed:  29781.992219924927\n",
      "ep 3276: ep_len:512 episode reward: total was 42.820000. running mean: 2.956596\n",
      "ep 3276: ep_len:500 episode reward: total was 66.040000. running mean: 3.587430\n",
      "ep 3276: ep_len:70 episode reward: total was 4.270000. running mean: 3.594255\n",
      "ep 3276: ep_len:391 episode reward: total was 9.310000. running mean: 3.651413\n",
      "ep 3276: ep_len:3 episode reward: total was 1.010000. running mean: 3.624999\n",
      "ep 3276: ep_len:628 episode reward: total was 10.200000. running mean: 3.690749\n",
      "ep 3276: ep_len:322 episode reward: total was -36.980000. running mean: 3.284041\n",
      "epsilon:0.054720 episode_count: 22939. steps_count: 9919019.000000\n",
      "Time elapsed:  29788.668112516403\n",
      "ep 3277: ep_len:509 episode reward: total was 24.430000. running mean: 3.495501\n",
      "ep 3277: ep_len:561 episode reward: total was 32.300000. running mean: 3.783546\n",
      "ep 3277: ep_len:691 episode reward: total was -28.650000. running mean: 3.459210\n",
      "ep 3277: ep_len:519 episode reward: total was 38.420000. running mean: 3.808818\n",
      "ep 3277: ep_len:3 episode reward: total was 1.010000. running mean: 3.780830\n",
      "ep 3277: ep_len:245 episode reward: total was 33.480000. running mean: 4.077822\n",
      "ep 3277: ep_len:500 episode reward: total was -39.750000. running mean: 3.639544\n",
      "epsilon:0.054675 episode_count: 22946. steps_count: 9922047.000000\n",
      "Time elapsed:  29801.42077088356\n",
      "ep 3278: ep_len:500 episode reward: total was 52.660000. running mean: 4.129748\n",
      "ep 3278: ep_len:604 episode reward: total was 51.760000. running mean: 4.606051\n",
      "ep 3278: ep_len:500 episode reward: total was -7.580000. running mean: 4.484190\n",
      "ep 3278: ep_len:373 episode reward: total was -11.010000. running mean: 4.329248\n",
      "ep 3278: ep_len:110 episode reward: total was 30.270000. running mean: 4.588656\n",
      "ep 3278: ep_len:500 episode reward: total was -6.640000. running mean: 4.476369\n",
      "ep 3278: ep_len:547 episode reward: total was 30.560000. running mean: 4.737206\n",
      "epsilon:0.054631 episode_count: 22953. steps_count: 9925181.000000\n",
      "Time elapsed:  29812.133318662643\n",
      "ep 3279: ep_len:215 episode reward: total was 1.650000. running mean: 4.706334\n",
      "ep 3279: ep_len:500 episode reward: total was 76.250000. running mean: 5.421770\n",
      "ep 3279: ep_len:626 episode reward: total was -27.820000. running mean: 5.089352\n",
      "ep 3279: ep_len:500 episode reward: total was -26.630000. running mean: 4.772159\n",
      "ep 3279: ep_len:3 episode reward: total was 1.010000. running mean: 4.734537\n",
      "ep 3279: ep_len:579 episode reward: total was -2.660000. running mean: 4.660592\n",
      "ep 3279: ep_len:500 episode reward: total was -27.130000. running mean: 4.342686\n",
      "epsilon:0.054587 episode_count: 22960. steps_count: 9928104.000000\n",
      "Time elapsed:  29816.740887403488\n",
      "ep 3280: ep_len:211 episode reward: total was 1.610000. running mean: 4.315359\n",
      "ep 3280: ep_len:583 episode reward: total was 61.790000. running mean: 4.890106\n",
      "ep 3280: ep_len:559 episode reward: total was -11.570000. running mean: 4.725505\n",
      "ep 3280: ep_len:525 episode reward: total was 5.660000. running mean: 4.734850\n",
      "ep 3280: ep_len:74 episode reward: total was -39.810000. running mean: 4.289401\n",
      "ep 3280: ep_len:513 episode reward: total was -13.550000. running mean: 4.111007\n",
      "ep 3280: ep_len:296 episode reward: total was 0.240000. running mean: 4.072297\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.054542 episode_count: 22967. steps_count: 9930865.000000\n",
      "Time elapsed:  29827.895668029785\n",
      "ep 3281: ep_len:571 episode reward: total was -116.670000. running mean: 2.864874\n",
      "ep 3281: ep_len:500 episode reward: total was 23.540000. running mean: 3.071625\n",
      "ep 3281: ep_len:617 episode reward: total was 5.630000. running mean: 3.097209\n",
      "ep 3281: ep_len:500 episode reward: total was 69.970000. running mean: 3.765937\n",
      "ep 3281: ep_len:41 episode reward: total was 19.000000. running mean: 3.918278\n",
      "ep 3281: ep_len:500 episode reward: total was 4.910000. running mean: 3.928195\n",
      "ep 3281: ep_len:305 episode reward: total was -19.760000. running mean: 3.691313\n",
      "epsilon:0.054498 episode_count: 22974. steps_count: 9933899.000000\n",
      "Time elapsed:  29835.89539217949\n",
      "ep 3282: ep_len:500 episode reward: total was 64.900000. running mean: 4.303400\n",
      "ep 3282: ep_len:606 episode reward: total was 46.350000. running mean: 4.723866\n",
      "ep 3282: ep_len:577 episode reward: total was -32.680000. running mean: 4.349827\n",
      "ep 3282: ep_len:609 episode reward: total was 28.470000. running mean: 4.591029\n",
      "ep 3282: ep_len:3 episode reward: total was 1.010000. running mean: 4.555218\n",
      "ep 3282: ep_len:500 episode reward: total was 6.830000. running mean: 4.577966\n",
      "ep 3282: ep_len:534 episode reward: total was -2.440000. running mean: 4.507787\n",
      "epsilon:0.054454 episode_count: 22981. steps_count: 9937228.000000\n",
      "Time elapsed:  29844.767378091812\n",
      "ep 3283: ep_len:123 episode reward: total was 19.470000. running mean: 4.657409\n",
      "ep 3283: ep_len:253 episode reward: total was -89.380000. running mean: 3.717035\n",
      "ep 3283: ep_len:611 episode reward: total was -60.480000. running mean: 3.075064\n",
      "ep 3283: ep_len:530 episode reward: total was 44.990000. running mean: 3.494214\n",
      "ep 3283: ep_len:3 episode reward: total was 1.010000. running mean: 3.469372\n",
      "ep 3283: ep_len:168 episode reward: total was 14.600000. running mean: 3.580678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3283: ep_len:500 episode reward: total was -14.930000. running mean: 3.395571\n",
      "epsilon:0.054409 episode_count: 22988. steps_count: 9939416.000000\n",
      "Time elapsed:  29850.442059755325\n",
      "ep 3284: ep_len:601 episode reward: total was 20.460000. running mean: 3.566215\n",
      "ep 3284: ep_len:570 episode reward: total was 70.010000. running mean: 4.230653\n",
      "ep 3284: ep_len:583 episode reward: total was -10.080000. running mean: 4.087547\n",
      "ep 3284: ep_len:568 episode reward: total was 80.100000. running mean: 4.847671\n",
      "ep 3284: ep_len:3 episode reward: total was 1.010000. running mean: 4.809294\n",
      "ep 3284: ep_len:527 episode reward: total was 31.700000. running mean: 5.078202\n",
      "ep 3284: ep_len:193 episode reward: total was -21.600000. running mean: 4.811420\n",
      "epsilon:0.054365 episode_count: 22995. steps_count: 9942461.000000\n",
      "Time elapsed:  29857.331736803055\n",
      "ep 3285: ep_len:545 episode reward: total was -21.500000. running mean: 4.548305\n",
      "ep 3285: ep_len:262 episode reward: total was -64.760000. running mean: 3.855222\n",
      "ep 3285: ep_len:616 episode reward: total was -11.710000. running mean: 3.699570\n",
      "ep 3285: ep_len:407 episode reward: total was 8.010000. running mean: 3.742674\n",
      "ep 3285: ep_len:3 episode reward: total was 1.010000. running mean: 3.715348\n",
      "ep 3285: ep_len:562 episode reward: total was 3.750000. running mean: 3.715694\n",
      "ep 3285: ep_len:500 episode reward: total was 17.840000. running mean: 3.856937\n",
      "epsilon:0.054321 episode_count: 23002. steps_count: 9945356.000000\n",
      "Time elapsed:  29864.692933797836\n",
      "ep 3286: ep_len:556 episode reward: total was 43.480000. running mean: 4.253168\n",
      "ep 3286: ep_len:587 episode reward: total was 1.420000. running mean: 4.224836\n",
      "ep 3286: ep_len:511 episode reward: total was -43.570000. running mean: 3.746888\n",
      "ep 3286: ep_len:550 episode reward: total was 32.200000. running mean: 4.031419\n",
      "ep 3286: ep_len:79 episode reward: total was 21.140000. running mean: 4.202505\n",
      "ep 3286: ep_len:505 episode reward: total was -29.890000. running mean: 3.861580\n",
      "ep 3286: ep_len:527 episode reward: total was 26.740000. running mean: 4.090364\n",
      "epsilon:0.054276 episode_count: 23009. steps_count: 9948671.000000\n",
      "Time elapsed:  29878.227498292923\n",
      "ep 3287: ep_len:550 episode reward: total was -28.550000. running mean: 3.763960\n",
      "ep 3287: ep_len:500 episode reward: total was 78.150000. running mean: 4.507821\n",
      "ep 3287: ep_len:526 episode reward: total was 2.160000. running mean: 4.484342\n",
      "ep 3287: ep_len:500 episode reward: total was 70.140000. running mean: 5.140899\n",
      "ep 3287: ep_len:126 episode reward: total was 28.350000. running mean: 5.372990\n",
      "ep 3287: ep_len:500 episode reward: total was 30.370000. running mean: 5.622960\n",
      "ep 3287: ep_len:501 episode reward: total was -14.660000. running mean: 5.420130\n",
      "epsilon:0.054232 episode_count: 23016. steps_count: 9951874.000000\n",
      "Time elapsed:  29891.602489948273\n",
      "ep 3288: ep_len:618 episode reward: total was -41.800000. running mean: 4.947929\n",
      "ep 3288: ep_len:603 episode reward: total was 21.060000. running mean: 5.109050\n",
      "ep 3288: ep_len:500 episode reward: total was 41.360000. running mean: 5.471559\n",
      "ep 3288: ep_len:500 episode reward: total was 62.960000. running mean: 6.046444\n",
      "ep 3288: ep_len:92 episode reward: total was 26.260000. running mean: 6.248579\n",
      "ep 3288: ep_len:500 episode reward: total was 21.280000. running mean: 6.398894\n",
      "ep 3288: ep_len:510 episode reward: total was -4.390000. running mean: 6.291005\n",
      "epsilon:0.054188 episode_count: 23023. steps_count: 9955197.000000\n",
      "Time elapsed:  29900.273847341537\n",
      "ep 3289: ep_len:579 episode reward: total was 68.720000. running mean: 6.915295\n",
      "ep 3289: ep_len:500 episode reward: total was 28.560000. running mean: 7.131742\n",
      "ep 3289: ep_len:603 episode reward: total was 14.700000. running mean: 7.207424\n",
      "ep 3289: ep_len:519 episode reward: total was 17.040000. running mean: 7.305750\n",
      "ep 3289: ep_len:3 episode reward: total was 1.010000. running mean: 7.242792\n",
      "ep 3289: ep_len:186 episode reward: total was 36.700000. running mean: 7.537365\n",
      "ep 3289: ep_len:656 episode reward: total was 40.900000. running mean: 7.870991\n",
      "epsilon:0.054143 episode_count: 23030. steps_count: 9958243.000000\n",
      "Time elapsed:  29913.8449985981\n",
      "ep 3290: ep_len:571 episode reward: total was -63.160000. running mean: 7.160681\n",
      "ep 3290: ep_len:551 episode reward: total was 75.250000. running mean: 7.841574\n",
      "ep 3290: ep_len:633 episode reward: total was -18.260000. running mean: 7.580558\n",
      "ep 3290: ep_len:534 episode reward: total was -5.990000. running mean: 7.444853\n",
      "ep 3290: ep_len:3 episode reward: total was 1.010000. running mean: 7.380504\n",
      "ep 3290: ep_len:500 episode reward: total was 40.750000. running mean: 7.714199\n",
      "ep 3290: ep_len:186 episode reward: total was 1.500000. running mean: 7.652057\n",
      "epsilon:0.054099 episode_count: 23037. steps_count: 9961221.000000\n",
      "Time elapsed:  29921.70572400093\n",
      "ep 3291: ep_len:609 episode reward: total was -225.100000. running mean: 5.324537\n",
      "ep 3291: ep_len:568 episode reward: total was -81.920000. running mean: 4.452091\n",
      "ep 3291: ep_len:500 episode reward: total was -19.770000. running mean: 4.209870\n",
      "ep 3291: ep_len:592 episode reward: total was 58.210000. running mean: 4.749872\n",
      "ep 3291: ep_len:3 episode reward: total was 0.000000. running mean: 4.702373\n",
      "ep 3291: ep_len:243 episode reward: total was 15.460000. running mean: 4.809949\n",
      "ep 3291: ep_len:175 episode reward: total was 0.320000. running mean: 4.765050\n",
      "epsilon:0.054055 episode_count: 23044. steps_count: 9963911.000000\n",
      "Time elapsed:  29928.961634635925\n",
      "ep 3292: ep_len:643 episode reward: total was -19.850000. running mean: 4.518899\n",
      "ep 3292: ep_len:562 episode reward: total was 90.850000. running mean: 5.382210\n",
      "ep 3292: ep_len:500 episode reward: total was 25.850000. running mean: 5.586888\n",
      "ep 3292: ep_len:500 episode reward: total was 25.560000. running mean: 5.786619\n",
      "ep 3292: ep_len:3 episode reward: total was 1.010000. running mean: 5.738853\n",
      "ep 3292: ep_len:602 episode reward: total was -0.280000. running mean: 5.678665\n",
      "ep 3292: ep_len:550 episode reward: total was 31.480000. running mean: 5.936678\n",
      "epsilon:0.054010 episode_count: 23051. steps_count: 9967271.000000\n",
      "Time elapsed:  29937.854747772217\n",
      "ep 3293: ep_len:548 episode reward: total was 38.960000. running mean: 6.266911\n",
      "ep 3293: ep_len:589 episode reward: total was 31.920000. running mean: 6.523442\n",
      "ep 3293: ep_len:578 episode reward: total was -15.590000. running mean: 6.302308\n",
      "ep 3293: ep_len:579 episode reward: total was 66.660000. running mean: 6.905885\n",
      "ep 3293: ep_len:38 episode reward: total was 11.500000. running mean: 6.951826\n",
      "ep 3293: ep_len:529 episode reward: total was 3.010000. running mean: 6.912407\n",
      "ep 3293: ep_len:508 episode reward: total was -7.680000. running mean: 6.766483\n",
      "epsilon:0.053966 episode_count: 23058. steps_count: 9970640.000000\n",
      "Time elapsed:  29947.064270734787\n",
      "ep 3294: ep_len:627 episode reward: total was -42.290000. running mean: 6.275919\n",
      "ep 3294: ep_len:603 episode reward: total was -94.130000. running mean: 5.271859\n",
      "ep 3294: ep_len:509 episode reward: total was 2.880000. running mean: 5.247941\n",
      "ep 3294: ep_len:628 episode reward: total was 19.170000. running mean: 5.387161\n",
      "ep 3294: ep_len:99 episode reward: total was 27.740000. running mean: 5.610690\n",
      "ep 3294: ep_len:237 episode reward: total was 18.960000. running mean: 5.744183\n",
      "ep 3294: ep_len:516 episode reward: total was -0.620000. running mean: 5.680541\n",
      "epsilon:0.053922 episode_count: 23065. steps_count: 9973859.000000\n",
      "Time elapsed:  29954.29197859764\n",
      "ep 3295: ep_len:690 episode reward: total was -21.790000. running mean: 5.405836\n",
      "ep 3295: ep_len:577 episode reward: total was 33.800000. running mean: 5.689777\n",
      "ep 3295: ep_len:689 episode reward: total was 5.380000. running mean: 5.686679\n",
      "ep 3295: ep_len:504 episode reward: total was 43.440000. running mean: 6.064213\n",
      "ep 3295: ep_len:83 episode reward: total was 18.210000. running mean: 6.185671\n",
      "ep 3295: ep_len:525 episode reward: total was -11.310000. running mean: 6.010714\n",
      "ep 3295: ep_len:511 episode reward: total was -67.890000. running mean: 5.271707\n",
      "epsilon:0.053877 episode_count: 23072. steps_count: 9977438.000000\n",
      "Time elapsed:  29967.84033560753\n",
      "ep 3296: ep_len:262 episode reward: total was -1.260000. running mean: 5.206390\n",
      "ep 3296: ep_len:500 episode reward: total was 75.850000. running mean: 5.912826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3296: ep_len:589 episode reward: total was -70.250000. running mean: 5.151198\n",
      "ep 3296: ep_len:156 episode reward: total was -56.790000. running mean: 4.531786\n",
      "ep 3296: ep_len:111 episode reward: total was -42.190000. running mean: 4.064568\n",
      "ep 3296: ep_len:500 episode reward: total was -0.780000. running mean: 4.016122\n",
      "ep 3296: ep_len:510 episode reward: total was 0.590000. running mean: 3.981861\n",
      "epsilon:0.053833 episode_count: 23079. steps_count: 9980066.000000\n",
      "Time elapsed:  29974.890522241592\n",
      "ep 3297: ep_len:119 episode reward: total was -0.070000. running mean: 3.941342\n",
      "ep 3297: ep_len:526 episode reward: total was 18.850000. running mean: 4.090429\n",
      "ep 3297: ep_len:500 episode reward: total was 31.210000. running mean: 4.361624\n",
      "ep 3297: ep_len:508 episode reward: total was -0.420000. running mean: 4.313808\n",
      "ep 3297: ep_len:98 episode reward: total was 29.260000. running mean: 4.563270\n",
      "ep 3297: ep_len:544 episode reward: total was -102.300000. running mean: 3.494637\n",
      "ep 3297: ep_len:593 episode reward: total was 39.620000. running mean: 3.855891\n",
      "epsilon:0.053789 episode_count: 23086. steps_count: 9982954.000000\n",
      "Time elapsed:  29982.632142066956\n",
      "ep 3298: ep_len:591 episode reward: total was 68.820000. running mean: 4.505532\n",
      "ep 3298: ep_len:638 episode reward: total was 21.230000. running mean: 4.672777\n",
      "ep 3298: ep_len:569 episode reward: total was -77.940000. running mean: 3.846649\n",
      "ep 3298: ep_len:545 episode reward: total was 55.380000. running mean: 4.361983\n",
      "ep 3298: ep_len:3 episode reward: total was 1.010000. running mean: 4.328463\n",
      "ep 3298: ep_len:500 episode reward: total was -145.300000. running mean: 2.832178\n",
      "ep 3298: ep_len:608 episode reward: total was 6.010000. running mean: 2.863956\n",
      "epsilon:0.053744 episode_count: 23093. steps_count: 9986408.000000\n",
      "Time elapsed:  29991.729652404785\n",
      "ep 3299: ep_len:253 episode reward: total was 2.870000. running mean: 2.864017\n",
      "ep 3299: ep_len:622 episode reward: total was 7.080000. running mean: 2.906177\n",
      "ep 3299: ep_len:555 episode reward: total was -58.060000. running mean: 2.296515\n",
      "ep 3299: ep_len:533 episode reward: total was 23.830000. running mean: 2.511850\n",
      "ep 3299: ep_len:80 episode reward: total was 17.170000. running mean: 2.658431\n",
      "ep 3299: ep_len:651 episode reward: total was 7.050000. running mean: 2.702347\n",
      "ep 3299: ep_len:569 episode reward: total was -2.470000. running mean: 2.650623\n",
      "epsilon:0.053700 episode_count: 23100. steps_count: 9989671.000000\n",
      "Time elapsed:  30000.288719177246\n",
      "ep 3300: ep_len:500 episode reward: total was 11.720000. running mean: 2.741317\n",
      "ep 3300: ep_len:500 episode reward: total was -0.010000. running mean: 2.713804\n",
      "ep 3300: ep_len:500 episode reward: total was 10.740000. running mean: 2.794066\n",
      "ep 3300: ep_len:531 episode reward: total was 56.490000. running mean: 3.331025\n",
      "ep 3300: ep_len:3 episode reward: total was 1.010000. running mean: 3.307815\n",
      "ep 3300: ep_len:634 episode reward: total was 26.760000. running mean: 3.542337\n",
      "ep 3300: ep_len:614 episode reward: total was -23.410000. running mean: 3.272814\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.053656 episode_count: 23107. steps_count: 9992953.000000\n",
      "Time elapsed:  30013.705067873\n",
      "ep 3301: ep_len:621 episode reward: total was 50.120000. running mean: 3.741285\n",
      "ep 3301: ep_len:563 episode reward: total was -38.400000. running mean: 3.319873\n",
      "ep 3301: ep_len:569 episode reward: total was -55.270000. running mean: 2.733974\n",
      "ep 3301: ep_len:519 episode reward: total was 9.180000. running mean: 2.798434\n",
      "ep 3301: ep_len:3 episode reward: total was 1.010000. running mean: 2.780550\n",
      "ep 3301: ep_len:500 episode reward: total was -14.510000. running mean: 2.607644\n",
      "ep 3301: ep_len:519 episode reward: total was -24.750000. running mean: 2.334068\n",
      "epsilon:0.053611 episode_count: 23114. steps_count: 9996247.000000\n",
      "Time elapsed:  30027.845458507538\n",
      "ep 3302: ep_len:134 episode reward: total was 7.430000. running mean: 2.385027\n",
      "ep 3302: ep_len:581 episode reward: total was 27.080000. running mean: 2.631977\n",
      "ep 3302: ep_len:554 episode reward: total was 28.240000. running mean: 2.888057\n",
      "ep 3302: ep_len:502 episode reward: total was 27.570000. running mean: 3.134877\n",
      "ep 3302: ep_len:89 episode reward: total was -51.320000. running mean: 2.590328\n",
      "ep 3302: ep_len:500 episode reward: total was 24.360000. running mean: 2.808024\n",
      "ep 3302: ep_len:619 episode reward: total was 8.820000. running mean: 2.868144\n",
      "epsilon:0.053567 episode_count: 23121. steps_count: 9999226.000000\n",
      "Time elapsed:  30052.68526482582\n",
      "ep 3303: ep_len:590 episode reward: total was 69.880000. running mean: 3.538263\n",
      "ep 3303: ep_len:260 episode reward: total was 3.610000. running mean: 3.538980\n",
      "ep 3303: ep_len:500 episode reward: total was 52.780000. running mean: 4.031390\n",
      "ep 3303: ep_len:500 episode reward: total was -8.100000. running mean: 3.910076\n",
      "ep 3303: ep_len:3 episode reward: total was 1.010000. running mean: 3.881076\n",
      "ep 3303: ep_len:500 episode reward: total was -20.050000. running mean: 3.641765\n",
      "ep 3303: ep_len:296 episode reward: total was 15.050000. running mean: 3.755847\n",
      "epsilon:0.053523 episode_count: 23128. steps_count: 10001875.000000\n",
      "Time elapsed:  30059.795468091965\n",
      "ep 3304: ep_len:225 episode reward: total was 8.210000. running mean: 3.800389\n",
      "ep 3304: ep_len:500 episode reward: total was 33.060000. running mean: 4.092985\n",
      "ep 3304: ep_len:500 episode reward: total was 17.240000. running mean: 4.224455\n",
      "ep 3304: ep_len:500 episode reward: total was -1.870000. running mean: 4.163511\n",
      "ep 3304: ep_len:115 episode reward: total was 23.340000. running mean: 4.355275\n",
      "ep 3304: ep_len:500 episode reward: total was -14.070000. running mean: 4.171023\n",
      "ep 3304: ep_len:569 episode reward: total was -11.630000. running mean: 4.013012\n",
      "epsilon:0.053478 episode_count: 23135. steps_count: 10004784.000000\n",
      "Time elapsed:  30072.38637447357\n",
      "ep 3305: ep_len:128 episode reward: total was 1.400000. running mean: 3.986882\n",
      "ep 3305: ep_len:517 episode reward: total was -12.340000. running mean: 3.823613\n",
      "ep 3305: ep_len:584 episode reward: total was -38.590000. running mean: 3.399477\n",
      "ep 3305: ep_len:596 episode reward: total was 63.490000. running mean: 4.000383\n",
      "ep 3305: ep_len:1 episode reward: total was -1.000000. running mean: 3.950379\n",
      "ep 3305: ep_len:629 episode reward: total was -39.150000. running mean: 3.519375\n",
      "ep 3305: ep_len:500 episode reward: total was -20.810000. running mean: 3.276081\n",
      "epsilon:0.053434 episode_count: 23142. steps_count: 10007739.000000\n",
      "Time elapsed:  30089.451083660126\n",
      "ep 3306: ep_len:592 episode reward: total was 33.100000. running mean: 3.574320\n",
      "ep 3306: ep_len:500 episode reward: total was 87.540000. running mean: 4.413977\n",
      "ep 3306: ep_len:655 episode reward: total was -9.560000. running mean: 4.274237\n",
      "ep 3306: ep_len:518 episode reward: total was 19.680000. running mean: 4.428295\n",
      "ep 3306: ep_len:106 episode reward: total was 23.800000. running mean: 4.622012\n",
      "ep 3306: ep_len:500 episode reward: total was -8.050000. running mean: 4.495292\n",
      "ep 3306: ep_len:545 episode reward: total was 33.250000. running mean: 4.782839\n",
      "epsilon:0.053390 episode_count: 23149. steps_count: 10011155.000000\n",
      "Time elapsed:  30098.369472026825\n",
      "ep 3307: ep_len:500 episode reward: total was 29.820000. running mean: 5.033211\n",
      "ep 3307: ep_len:500 episode reward: total was 19.410000. running mean: 5.176979\n",
      "ep 3307: ep_len:386 episode reward: total was 37.260000. running mean: 5.497809\n",
      "ep 3307: ep_len:523 episode reward: total was -30.720000. running mean: 5.135631\n",
      "ep 3307: ep_len:2 episode reward: total was 0.510000. running mean: 5.089374\n",
      "ep 3307: ep_len:621 episode reward: total was 58.640000. running mean: 5.624881\n",
      "ep 3307: ep_len:601 episode reward: total was -8.610000. running mean: 5.482532\n",
      "epsilon:0.053345 episode_count: 23156. steps_count: 10014288.000000\n",
      "Time elapsed:  30106.61231470108\n",
      "ep 3308: ep_len:500 episode reward: total was 12.880000. running mean: 5.556507\n",
      "ep 3308: ep_len:500 episode reward: total was 48.850000. running mean: 5.989441\n",
      "ep 3308: ep_len:547 episode reward: total was -6.440000. running mean: 5.865147\n",
      "ep 3308: ep_len:500 episode reward: total was 21.620000. running mean: 6.022696\n",
      "ep 3308: ep_len:55 episode reward: total was 22.510000. running mean: 6.187569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3308: ep_len:613 episode reward: total was 28.290000. running mean: 6.408593\n",
      "ep 3308: ep_len:274 episode reward: total was 13.240000. running mean: 6.476907\n",
      "epsilon:0.053301 episode_count: 23163. steps_count: 10017277.000000\n",
      "Time elapsed:  30114.649629831314\n",
      "ep 3309: ep_len:641 episode reward: total was -45.980000. running mean: 5.952338\n",
      "ep 3309: ep_len:588 episode reward: total was -3.090000. running mean: 5.861915\n",
      "ep 3309: ep_len:578 episode reward: total was 35.740000. running mean: 6.160695\n",
      "ep 3309: ep_len:379 episode reward: total was -27.890000. running mean: 5.820188\n",
      "ep 3309: ep_len:3 episode reward: total was 1.010000. running mean: 5.772087\n",
      "ep 3309: ep_len:601 episode reward: total was 40.700000. running mean: 6.121366\n",
      "ep 3309: ep_len:523 episode reward: total was 13.260000. running mean: 6.192752\n",
      "epsilon:0.053257 episode_count: 23170. steps_count: 10020590.000000\n",
      "Time elapsed:  30127.986845731735\n",
      "ep 3310: ep_len:587 episode reward: total was 58.270000. running mean: 6.713525\n",
      "ep 3310: ep_len:595 episode reward: total was 123.560000. running mean: 7.881989\n",
      "ep 3310: ep_len:565 episode reward: total was -28.600000. running mean: 7.517169\n",
      "ep 3310: ep_len:528 episode reward: total was 52.020000. running mean: 7.962198\n",
      "ep 3310: ep_len:3 episode reward: total was -1.500000. running mean: 7.867576\n",
      "ep 3310: ep_len:500 episode reward: total was 2.590000. running mean: 7.814800\n",
      "ep 3310: ep_len:500 episode reward: total was 16.630000. running mean: 7.902952\n",
      "epsilon:0.053212 episode_count: 23177. steps_count: 10023868.000000\n",
      "Time elapsed:  30142.080808877945\n",
      "ep 3311: ep_len:615 episode reward: total was 74.510000. running mean: 8.569022\n",
      "ep 3311: ep_len:712 episode reward: total was -166.610000. running mean: 6.817232\n",
      "ep 3311: ep_len:39 episode reward: total was 3.190000. running mean: 6.780960\n",
      "ep 3311: ep_len:526 episode reward: total was 25.020000. running mean: 6.963350\n",
      "ep 3311: ep_len:3 episode reward: total was 1.010000. running mean: 6.903817\n",
      "ep 3311: ep_len:662 episode reward: total was -34.840000. running mean: 6.486379\n",
      "ep 3311: ep_len:523 episode reward: total was 10.000000. running mean: 6.521515\n",
      "epsilon:0.053168 episode_count: 23184. steps_count: 10026948.000000\n",
      "Time elapsed:  30150.271750450134\n",
      "ep 3312: ep_len:541 episode reward: total was 25.250000. running mean: 6.708800\n",
      "ep 3312: ep_len:500 episode reward: total was 29.500000. running mean: 6.936712\n",
      "ep 3312: ep_len:532 episode reward: total was -30.410000. running mean: 6.563245\n",
      "ep 3312: ep_len:56 episode reward: total was -7.150000. running mean: 6.426112\n",
      "ep 3312: ep_len:3 episode reward: total was 0.000000. running mean: 6.361851\n",
      "ep 3312: ep_len:629 episode reward: total was 18.220000. running mean: 6.480432\n",
      "ep 3312: ep_len:607 episode reward: total was 28.700000. running mean: 6.702628\n",
      "epsilon:0.053124 episode_count: 23191. steps_count: 10029816.000000\n",
      "Time elapsed:  30157.913792848587\n",
      "ep 3313: ep_len:500 episode reward: total was 35.890000. running mean: 6.994502\n",
      "ep 3313: ep_len:575 episode reward: total was 76.000000. running mean: 7.684557\n",
      "ep 3313: ep_len:587 episode reward: total was 28.130000. running mean: 7.889011\n",
      "ep 3313: ep_len:508 episode reward: total was 31.760000. running mean: 8.127721\n",
      "ep 3313: ep_len:3 episode reward: total was 1.010000. running mean: 8.056544\n",
      "ep 3313: ep_len:616 episode reward: total was 22.170000. running mean: 8.197679\n",
      "ep 3313: ep_len:555 episode reward: total was -29.710000. running mean: 7.818602\n",
      "epsilon:0.053079 episode_count: 23198. steps_count: 10033160.000000\n",
      "Time elapsed:  30166.5944108963\n",
      "ep 3314: ep_len:593 episode reward: total was 11.600000. running mean: 7.856416\n",
      "ep 3314: ep_len:526 episode reward: total was -40.750000. running mean: 7.370352\n",
      "ep 3314: ep_len:584 episode reward: total was 6.670000. running mean: 7.363348\n",
      "ep 3314: ep_len:170 episode reward: total was 11.220000. running mean: 7.401915\n",
      "ep 3314: ep_len:3 episode reward: total was 1.010000. running mean: 7.337995\n",
      "ep 3314: ep_len:542 episode reward: total was 24.690000. running mean: 7.511515\n",
      "ep 3314: ep_len:500 episode reward: total was -55.070000. running mean: 6.885700\n",
      "epsilon:0.053035 episode_count: 23205. steps_count: 10036078.000000\n",
      "Time elapsed:  30174.21017098427\n",
      "ep 3315: ep_len:513 episode reward: total was -1.130000. running mean: 6.805543\n",
      "ep 3315: ep_len:500 episode reward: total was 60.150000. running mean: 7.338988\n",
      "ep 3315: ep_len:643 episode reward: total was -26.060000. running mean: 7.004998\n",
      "ep 3315: ep_len:511 episode reward: total was -3.290000. running mean: 6.902048\n",
      "ep 3315: ep_len:3 episode reward: total was 1.010000. running mean: 6.843128\n",
      "ep 3315: ep_len:500 episode reward: total was 32.760000. running mean: 7.102296\n",
      "ep 3315: ep_len:511 episode reward: total was 39.860000. running mean: 7.429873\n",
      "epsilon:0.052991 episode_count: 23212. steps_count: 10039259.000000\n",
      "Time elapsed:  30182.670602560043\n",
      "ep 3316: ep_len:119 episode reward: total was 16.950000. running mean: 7.525075\n",
      "ep 3316: ep_len:186 episode reward: total was -0.640000. running mean: 7.443424\n",
      "ep 3316: ep_len:587 episode reward: total was -6.030000. running mean: 7.308690\n",
      "ep 3316: ep_len:392 episode reward: total was 12.080000. running mean: 7.356403\n",
      "ep 3316: ep_len:130 episode reward: total was -53.630000. running mean: 6.746539\n",
      "ep 3316: ep_len:500 episode reward: total was 10.650000. running mean: 6.785573\n",
      "ep 3316: ep_len:574 episode reward: total was 13.470000. running mean: 6.852418\n",
      "epsilon:0.052946 episode_count: 23219. steps_count: 10041747.000000\n",
      "Time elapsed:  30189.82843852043\n",
      "ep 3317: ep_len:122 episode reward: total was -5.920000. running mean: 6.724693\n",
      "ep 3317: ep_len:500 episode reward: total was 30.450000. running mean: 6.961946\n",
      "ep 3317: ep_len:571 episode reward: total was -19.040000. running mean: 6.701927\n",
      "ep 3317: ep_len:529 episode reward: total was 21.640000. running mean: 6.851308\n",
      "ep 3317: ep_len:103 episode reward: total was 28.240000. running mean: 7.065195\n",
      "ep 3317: ep_len:645 episode reward: total was -9.080000. running mean: 6.903743\n",
      "ep 3317: ep_len:515 episode reward: total was -7.000000. running mean: 6.764705\n",
      "epsilon:0.052902 episode_count: 23226. steps_count: 10044732.000000\n",
      "Time elapsed:  30197.70831632614\n",
      "ep 3318: ep_len:561 episode reward: total was 26.050000. running mean: 6.957558\n",
      "ep 3318: ep_len:377 episode reward: total was -34.570000. running mean: 6.542283\n",
      "ep 3318: ep_len:403 episode reward: total was 54.550000. running mean: 7.022360\n",
      "ep 3318: ep_len:500 episode reward: total was 57.840000. running mean: 7.530536\n",
      "ep 3318: ep_len:3 episode reward: total was 1.010000. running mean: 7.465331\n",
      "ep 3318: ep_len:500 episode reward: total was 46.520000. running mean: 7.855878\n",
      "ep 3318: ep_len:337 episode reward: total was 16.210000. running mean: 7.939419\n",
      "epsilon:0.052858 episode_count: 23233. steps_count: 10047413.000000\n",
      "Time elapsed:  30204.43083357811\n",
      "ep 3319: ep_len:500 episode reward: total was 76.500000. running mean: 8.625025\n",
      "ep 3319: ep_len:583 episode reward: total was 22.260000. running mean: 8.761374\n",
      "ep 3319: ep_len:627 episode reward: total was 26.360000. running mean: 8.937361\n",
      "ep 3319: ep_len:966 episode reward: total was -406.350000. running mean: 4.784487\n",
      "ep 3319: ep_len:128 episode reward: total was 23.840000. running mean: 4.975042\n",
      "ep 3319: ep_len:500 episode reward: total was 5.810000. running mean: 4.983392\n",
      "ep 3319: ep_len:625 episode reward: total was -15.280000. running mean: 4.780758\n",
      "epsilon:0.052813 episode_count: 23240. steps_count: 10051342.000000\n",
      "Time elapsed:  30212.106801748276\n",
      "ep 3320: ep_len:512 episode reward: total was -22.490000. running mean: 4.508050\n",
      "ep 3320: ep_len:500 episode reward: total was 6.740000. running mean: 4.530370\n",
      "ep 3320: ep_len:649 episode reward: total was -0.110000. running mean: 4.483966\n",
      "ep 3320: ep_len:507 episode reward: total was 25.410000. running mean: 4.693226\n",
      "ep 3320: ep_len:47 episode reward: total was 20.500000. running mean: 4.851294\n",
      "ep 3320: ep_len:300 episode reward: total was 21.590000. running mean: 5.018681\n",
      "ep 3320: ep_len:607 episode reward: total was -8.630000. running mean: 4.882194\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.052769 episode_count: 23247. steps_count: 10054464.000000\n",
      "Time elapsed:  30229.99927330017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3321: ep_len:596 episode reward: total was 49.060000. running mean: 5.323972\n",
      "ep 3321: ep_len:367 episode reward: total was 15.630000. running mean: 5.427033\n",
      "ep 3321: ep_len:375 episode reward: total was 20.160000. running mean: 5.574362\n",
      "ep 3321: ep_len:500 episode reward: total was 31.610000. running mean: 5.834719\n",
      "ep 3321: ep_len:3 episode reward: total was 1.010000. running mean: 5.786472\n",
      "ep 3321: ep_len:300 episode reward: total was 15.130000. running mean: 5.879907\n",
      "ep 3321: ep_len:551 episode reward: total was 14.950000. running mean: 5.970608\n",
      "epsilon:0.052725 episode_count: 23254. steps_count: 10057156.000000\n",
      "Time elapsed:  30237.3753323555\n",
      "ep 3322: ep_len:657 episode reward: total was 1.940000. running mean: 5.930302\n",
      "ep 3322: ep_len:500 episode reward: total was 7.150000. running mean: 5.942499\n",
      "ep 3322: ep_len:352 episode reward: total was 37.250000. running mean: 6.255574\n",
      "ep 3322: ep_len:579 episode reward: total was 29.470000. running mean: 6.487718\n",
      "ep 3322: ep_len:3 episode reward: total was 1.010000. running mean: 6.432941\n",
      "ep 3322: ep_len:500 episode reward: total was 31.330000. running mean: 6.681911\n",
      "ep 3322: ep_len:570 episode reward: total was 34.370000. running mean: 6.958792\n",
      "epsilon:0.052680 episode_count: 23261. steps_count: 10060317.000000\n",
      "Time elapsed:  30245.783816099167\n",
      "ep 3323: ep_len:603 episode reward: total was 26.460000. running mean: 7.153804\n",
      "ep 3323: ep_len:549 episode reward: total was 107.710000. running mean: 8.159366\n",
      "ep 3323: ep_len:500 episode reward: total was 16.690000. running mean: 8.244673\n",
      "ep 3323: ep_len:501 episode reward: total was 13.140000. running mean: 8.293626\n",
      "ep 3323: ep_len:3 episode reward: total was 1.010000. running mean: 8.220790\n",
      "ep 3323: ep_len:721 episode reward: total was -87.160000. running mean: 7.266982\n",
      "ep 3323: ep_len:500 episode reward: total was 8.620000. running mean: 7.280512\n",
      "epsilon:0.052636 episode_count: 23268. steps_count: 10063694.000000\n",
      "Time elapsed:  30253.841691732407\n",
      "ep 3324: ep_len:573 episode reward: total was -23.330000. running mean: 6.974407\n",
      "ep 3324: ep_len:542 episode reward: total was 2.110000. running mean: 6.925763\n",
      "ep 3324: ep_len:500 episode reward: total was -21.120000. running mean: 6.645305\n",
      "ep 3324: ep_len:529 episode reward: total was 41.590000. running mean: 6.994752\n",
      "ep 3324: ep_len:3 episode reward: total was 1.010000. running mean: 6.934904\n",
      "ep 3324: ep_len:501 episode reward: total was 28.550000. running mean: 7.151055\n",
      "ep 3324: ep_len:613 episode reward: total was -13.990000. running mean: 6.939645\n",
      "epsilon:0.052592 episode_count: 23275. steps_count: 10066955.000000\n",
      "Time elapsed:  30261.432921648026\n",
      "ep 3325: ep_len:574 episode reward: total was 34.390000. running mean: 7.214148\n",
      "ep 3325: ep_len:305 episode reward: total was 3.730000. running mean: 7.179307\n",
      "ep 3325: ep_len:583 episode reward: total was 23.440000. running mean: 7.341914\n",
      "ep 3325: ep_len:124 episode reward: total was 4.110000. running mean: 7.309595\n",
      "ep 3325: ep_len:3 episode reward: total was 1.010000. running mean: 7.246599\n",
      "ep 3325: ep_len:516 episode reward: total was -6.200000. running mean: 7.112133\n",
      "ep 3325: ep_len:500 episode reward: total was 49.360000. running mean: 7.534611\n",
      "epsilon:0.052547 episode_count: 23282. steps_count: 10069560.000000\n",
      "Time elapsed:  30268.549020767212\n",
      "ep 3326: ep_len:636 episode reward: total was 60.280000. running mean: 8.062065\n",
      "ep 3326: ep_len:508 episode reward: total was 43.380000. running mean: 8.415245\n",
      "ep 3326: ep_len:554 episode reward: total was -16.990000. running mean: 8.161192\n",
      "ep 3326: ep_len:117 episode reward: total was 6.530000. running mean: 8.144880\n",
      "ep 3326: ep_len:89 episode reward: total was 18.210000. running mean: 8.245532\n",
      "ep 3326: ep_len:825 episode reward: total was -203.310000. running mean: 6.129976\n",
      "ep 3326: ep_len:609 episode reward: total was 19.750000. running mean: 6.266176\n",
      "epsilon:0.052503 episode_count: 23289. steps_count: 10072898.000000\n",
      "Time elapsed:  30277.21215748787\n",
      "ep 3327: ep_len:500 episode reward: total was -20.630000. running mean: 5.997215\n",
      "ep 3327: ep_len:503 episode reward: total was 30.270000. running mean: 6.239943\n",
      "ep 3327: ep_len:77 episode reward: total was 6.790000. running mean: 6.245443\n",
      "ep 3327: ep_len:416 episode reward: total was 9.800000. running mean: 6.280989\n",
      "ep 3327: ep_len:96 episode reward: total was 25.200000. running mean: 6.470179\n",
      "ep 3327: ep_len:542 episode reward: total was -23.520000. running mean: 6.170277\n",
      "ep 3327: ep_len:500 episode reward: total was -1.960000. running mean: 6.088974\n",
      "epsilon:0.052459 episode_count: 23296. steps_count: 10075532.000000\n",
      "Time elapsed:  30284.45379281044\n",
      "ep 3328: ep_len:507 episode reward: total was 22.170000. running mean: 6.249785\n",
      "ep 3328: ep_len:177 episode reward: total was 20.760000. running mean: 6.394887\n",
      "ep 3328: ep_len:500 episode reward: total was 35.490000. running mean: 6.685838\n",
      "ep 3328: ep_len:500 episode reward: total was 15.320000. running mean: 6.772179\n",
      "ep 3328: ep_len:3 episode reward: total was 1.010000. running mean: 6.714558\n",
      "ep 3328: ep_len:572 episode reward: total was 41.840000. running mean: 7.065812\n",
      "ep 3328: ep_len:552 episode reward: total was -109.310000. running mean: 5.902054\n",
      "epsilon:0.052414 episode_count: 23303. steps_count: 10078343.000000\n",
      "Time elapsed:  30292.059091567993\n",
      "ep 3329: ep_len:680 episode reward: total was -3.990000. running mean: 5.803133\n",
      "ep 3329: ep_len:500 episode reward: total was 51.540000. running mean: 6.260502\n",
      "ep 3329: ep_len:530 episode reward: total was -23.510000. running mean: 5.962797\n",
      "ep 3329: ep_len:397 episode reward: total was 40.460000. running mean: 6.307769\n",
      "ep 3329: ep_len:112 episode reward: total was 26.370000. running mean: 6.508391\n",
      "ep 3329: ep_len:611 episode reward: total was 15.090000. running mean: 6.594207\n",
      "ep 3329: ep_len:529 episode reward: total was -37.930000. running mean: 6.148965\n",
      "epsilon:0.052370 episode_count: 23310. steps_count: 10081702.000000\n",
      "Time elapsed:  30300.801034212112\n",
      "ep 3330: ep_len:175 episode reward: total was 13.550000. running mean: 6.222976\n",
      "ep 3330: ep_len:500 episode reward: total was 48.340000. running mean: 6.644146\n",
      "ep 3330: ep_len:500 episode reward: total was 34.770000. running mean: 6.925405\n",
      "ep 3330: ep_len:529 episode reward: total was 80.210000. running mean: 7.658250\n",
      "ep 3330: ep_len:93 episode reward: total was 23.730000. running mean: 7.818968\n",
      "ep 3330: ep_len:569 episode reward: total was -10.240000. running mean: 7.638378\n",
      "ep 3330: ep_len:507 episode reward: total was 7.330000. running mean: 7.635295\n",
      "epsilon:0.052326 episode_count: 23317. steps_count: 10084575.000000\n",
      "Time elapsed:  30308.433242082596\n",
      "ep 3331: ep_len:526 episode reward: total was -35.840000. running mean: 7.200542\n",
      "ep 3331: ep_len:556 episode reward: total was 76.530000. running mean: 7.893836\n",
      "ep 3331: ep_len:656 episode reward: total was 49.310000. running mean: 8.307998\n",
      "ep 3331: ep_len:517 episode reward: total was 24.530000. running mean: 8.470218\n",
      "ep 3331: ep_len:93 episode reward: total was 20.240000. running mean: 8.587916\n",
      "ep 3331: ep_len:500 episode reward: total was 0.650000. running mean: 8.508536\n",
      "ep 3331: ep_len:626 episode reward: total was -130.790000. running mean: 7.115551\n",
      "epsilon:0.052281 episode_count: 23324. steps_count: 10088049.000000\n",
      "Time elapsed:  30317.52059030533\n",
      "ep 3332: ep_len:608 episode reward: total was 45.350000. running mean: 7.497896\n",
      "ep 3332: ep_len:189 episode reward: total was 17.940000. running mean: 7.602317\n",
      "ep 3332: ep_len:666 episode reward: total was -3.970000. running mean: 7.486593\n",
      "ep 3332: ep_len:557 episode reward: total was 70.040000. running mean: 8.112128\n",
      "ep 3332: ep_len:3 episode reward: total was 1.010000. running mean: 8.041106\n",
      "ep 3332: ep_len:652 episode reward: total was 23.680000. running mean: 8.197495\n",
      "ep 3332: ep_len:528 episode reward: total was -9.550000. running mean: 8.020020\n",
      "epsilon:0.052237 episode_count: 23331. steps_count: 10091252.000000\n",
      "Time elapsed:  30332.615877389908\n",
      "ep 3333: ep_len:500 episode reward: total was 75.400000. running mean: 8.693820\n",
      "ep 3333: ep_len:543 episode reward: total was 17.790000. running mean: 8.784782\n",
      "ep 3333: ep_len:439 episode reward: total was 56.870000. running mean: 9.265634\n",
      "ep 3333: ep_len:517 episode reward: total was 18.690000. running mean: 9.359878\n",
      "ep 3333: ep_len:3 episode reward: total was 1.010000. running mean: 9.276379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3333: ep_len:554 episode reward: total was -4.150000. running mean: 9.142115\n",
      "ep 3333: ep_len:500 episode reward: total was 22.910000. running mean: 9.279794\n",
      "epsilon:0.052193 episode_count: 23338. steps_count: 10094308.000000\n",
      "Time elapsed:  30345.83315038681\n",
      "ep 3334: ep_len:207 episode reward: total was 13.230000. running mean: 9.319296\n",
      "ep 3334: ep_len:305 episode reward: total was 2.320000. running mean: 9.249303\n",
      "ep 3334: ep_len:623 episode reward: total was -7.620000. running mean: 9.080610\n",
      "ep 3334: ep_len:500 episode reward: total was 12.990000. running mean: 9.119704\n",
      "ep 3334: ep_len:3 episode reward: total was -1.500000. running mean: 9.013507\n",
      "ep 3334: ep_len:517 episode reward: total was 28.920000. running mean: 9.212572\n",
      "ep 3334: ep_len:557 episode reward: total was -38.880000. running mean: 8.731646\n",
      "epsilon:0.052148 episode_count: 23345. steps_count: 10097020.000000\n",
      "Time elapsed:  30352.896595716476\n",
      "ep 3335: ep_len:609 episode reward: total was 64.310000. running mean: 9.287430\n",
      "ep 3335: ep_len:275 episode reward: total was -17.510000. running mean: 9.019455\n",
      "ep 3335: ep_len:70 episode reward: total was 7.850000. running mean: 9.007761\n",
      "ep 3335: ep_len:399 episode reward: total was -20.110000. running mean: 8.716583\n",
      "ep 3335: ep_len:2 episode reward: total was -0.500000. running mean: 8.624417\n",
      "ep 3335: ep_len:515 episode reward: total was 18.790000. running mean: 8.726073\n",
      "ep 3335: ep_len:290 episode reward: total was 22.340000. running mean: 8.862212\n",
      "epsilon:0.052104 episode_count: 23352. steps_count: 10099180.000000\n",
      "Time elapsed:  30356.48635149002\n",
      "ep 3336: ep_len:609 episode reward: total was 16.440000. running mean: 8.937990\n",
      "ep 3336: ep_len:509 episode reward: total was 16.180000. running mean: 9.010410\n",
      "ep 3336: ep_len:520 episode reward: total was -9.530000. running mean: 8.825006\n",
      "ep 3336: ep_len:548 episode reward: total was -7.040000. running mean: 8.666356\n",
      "ep 3336: ep_len:3 episode reward: total was 0.000000. running mean: 8.579693\n",
      "ep 3336: ep_len:500 episode reward: total was -52.270000. running mean: 7.971196\n",
      "ep 3336: ep_len:522 episode reward: total was 1.670000. running mean: 7.908184\n",
      "epsilon:0.052060 episode_count: 23359. steps_count: 10102391.000000\n",
      "Time elapsed:  30369.100128173828\n",
      "ep 3337: ep_len:595 episode reward: total was 45.920000. running mean: 8.288302\n",
      "ep 3337: ep_len:504 episode reward: total was 14.520000. running mean: 8.350619\n",
      "ep 3337: ep_len:546 episode reward: total was -8.840000. running mean: 8.178713\n",
      "ep 3337: ep_len:500 episode reward: total was 0.800000. running mean: 8.104926\n",
      "ep 3337: ep_len:3 episode reward: total was 1.010000. running mean: 8.033976\n",
      "ep 3337: ep_len:500 episode reward: total was -49.740000. running mean: 7.456237\n",
      "ep 3337: ep_len:613 episode reward: total was 20.690000. running mean: 7.588574\n",
      "epsilon:0.052015 episode_count: 23366. steps_count: 10105652.000000\n",
      "Time elapsed:  30377.50931572914\n",
      "ep 3338: ep_len:121 episode reward: total was -5.990000. running mean: 7.452789\n",
      "ep 3338: ep_len:516 episode reward: total was 14.410000. running mean: 7.522361\n",
      "ep 3338: ep_len:516 episode reward: total was 43.430000. running mean: 7.881437\n",
      "ep 3338: ep_len:630 episode reward: total was 37.020000. running mean: 8.172823\n",
      "ep 3338: ep_len:3 episode reward: total was 1.010000. running mean: 8.101194\n",
      "ep 3338: ep_len:501 episode reward: total was 28.010000. running mean: 8.300282\n",
      "ep 3338: ep_len:540 episode reward: total was 16.350000. running mean: 8.380780\n",
      "epsilon:0.051971 episode_count: 23373. steps_count: 10108479.000000\n",
      "Time elapsed:  30388.976982355118\n",
      "ep 3339: ep_len:567 episode reward: total was -184.150000. running mean: 6.455472\n",
      "ep 3339: ep_len:525 episode reward: total was 59.050000. running mean: 6.981417\n",
      "ep 3339: ep_len:643 episode reward: total was -25.880000. running mean: 6.652803\n",
      "ep 3339: ep_len:500 episode reward: total was 19.630000. running mean: 6.782575\n",
      "ep 3339: ep_len:3 episode reward: total was 1.010000. running mean: 6.724849\n",
      "ep 3339: ep_len:500 episode reward: total was -85.420000. running mean: 5.803401\n",
      "ep 3339: ep_len:500 episode reward: total was -29.140000. running mean: 5.453967\n",
      "epsilon:0.051927 episode_count: 23380. steps_count: 10111717.000000\n",
      "Time elapsed:  30396.736053705215\n",
      "ep 3340: ep_len:250 episode reward: total was 27.750000. running mean: 5.676927\n",
      "ep 3340: ep_len:531 episode reward: total was 67.550000. running mean: 6.295658\n",
      "ep 3340: ep_len:64 episode reward: total was 12.230000. running mean: 6.355001\n",
      "ep 3340: ep_len:524 episode reward: total was 17.500000. running mean: 6.466451\n",
      "ep 3340: ep_len:94 episode reward: total was 17.710000. running mean: 6.578887\n",
      "ep 3340: ep_len:529 episode reward: total was -27.420000. running mean: 6.238898\n",
      "ep 3340: ep_len:565 episode reward: total was -88.240000. running mean: 5.294109\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.051882 episode_count: 23387. steps_count: 10114274.000000\n",
      "Time elapsed:  30408.398139476776\n",
      "ep 3341: ep_len:214 episode reward: total was -5.800000. running mean: 5.183168\n",
      "ep 3341: ep_len:520 episode reward: total was 10.240000. running mean: 5.233736\n",
      "ep 3341: ep_len:559 episode reward: total was 33.610000. running mean: 5.517499\n",
      "ep 3341: ep_len:506 episode reward: total was 41.040000. running mean: 5.872724\n",
      "ep 3341: ep_len:3 episode reward: total was 1.010000. running mean: 5.824096\n",
      "ep 3341: ep_len:630 episode reward: total was -59.610000. running mean: 5.169755\n",
      "ep 3341: ep_len:500 episode reward: total was -43.430000. running mean: 4.683758\n",
      "epsilon:0.051838 episode_count: 23394. steps_count: 10117206.000000\n",
      "Time elapsed:  30416.153074979782\n",
      "ep 3342: ep_len:577 episode reward: total was 47.610000. running mean: 5.113020\n",
      "ep 3342: ep_len:585 episode reward: total was 31.790000. running mean: 5.379790\n",
      "ep 3342: ep_len:599 episode reward: total was -20.960000. running mean: 5.116392\n",
      "ep 3342: ep_len:560 episode reward: total was 80.480000. running mean: 5.870028\n",
      "ep 3342: ep_len:3 episode reward: total was 1.010000. running mean: 5.821428\n",
      "ep 3342: ep_len:623 episode reward: total was 2.470000. running mean: 5.787914\n",
      "ep 3342: ep_len:500 episode reward: total was 73.410000. running mean: 6.464135\n",
      "epsilon:0.051794 episode_count: 23401. steps_count: 10120653.000000\n",
      "Time elapsed:  30425.134434223175\n",
      "ep 3343: ep_len:588 episode reward: total was -42.010000. running mean: 5.979393\n",
      "ep 3343: ep_len:500 episode reward: total was 5.220000. running mean: 5.971799\n",
      "ep 3343: ep_len:625 episode reward: total was -24.490000. running mean: 5.667181\n",
      "ep 3343: ep_len:500 episode reward: total was 22.770000. running mean: 5.838210\n",
      "ep 3343: ep_len:3 episode reward: total was 1.010000. running mean: 5.789927\n",
      "ep 3343: ep_len:551 episode reward: total was 16.160000. running mean: 5.893628\n",
      "ep 3343: ep_len:500 episode reward: total was 12.960000. running mean: 5.964292\n",
      "epsilon:0.051749 episode_count: 23408. steps_count: 10123920.000000\n",
      "Time elapsed:  30438.446811676025\n",
      "ep 3344: ep_len:500 episode reward: total was 77.020000. running mean: 6.674849\n",
      "ep 3344: ep_len:500 episode reward: total was -6.760000. running mean: 6.540500\n",
      "ep 3344: ep_len:500 episode reward: total was 39.370000. running mean: 6.868795\n",
      "ep 3344: ep_len:132 episode reward: total was 18.130000. running mean: 6.981408\n",
      "ep 3344: ep_len:73 episode reward: total was 15.230000. running mean: 7.063893\n",
      "ep 3344: ep_len:500 episode reward: total was 24.710000. running mean: 7.240355\n",
      "ep 3344: ep_len:611 episode reward: total was 15.830000. running mean: 7.326251\n",
      "epsilon:0.051705 episode_count: 23415. steps_count: 10126736.000000\n",
      "Time elapsed:  30446.057868242264\n",
      "ep 3345: ep_len:524 episode reward: total was -7.720000. running mean: 7.175788\n",
      "ep 3345: ep_len:291 episode reward: total was -36.170000. running mean: 6.742331\n",
      "ep 3345: ep_len:500 episode reward: total was 15.170000. running mean: 6.826607\n",
      "ep 3345: ep_len:504 episode reward: total was 5.240000. running mean: 6.810741\n",
      "ep 3345: ep_len:3 episode reward: total was 1.010000. running mean: 6.752734\n",
      "ep 3345: ep_len:515 episode reward: total was -34.690000. running mean: 6.338306\n",
      "ep 3345: ep_len:321 episode reward: total was -13.060000. running mean: 6.144323\n",
      "epsilon:0.051661 episode_count: 23422. steps_count: 10129394.000000\n",
      "Time elapsed:  30453.267456054688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3346: ep_len:610 episode reward: total was 32.710000. running mean: 6.409980\n",
      "ep 3346: ep_len:507 episode reward: total was 16.700000. running mean: 6.512880\n",
      "ep 3346: ep_len:602 episode reward: total was -14.360000. running mean: 6.304152\n",
      "ep 3346: ep_len:500 episode reward: total was 6.840000. running mean: 6.309510\n",
      "ep 3346: ep_len:97 episode reward: total was 28.610000. running mean: 6.532515\n",
      "ep 3346: ep_len:540 episode reward: total was 32.710000. running mean: 6.794290\n",
      "ep 3346: ep_len:500 episode reward: total was -0.620000. running mean: 6.720147\n",
      "epsilon:0.051616 episode_count: 23429. steps_count: 10132750.000000\n",
      "Time elapsed:  30461.930661916733\n",
      "ep 3347: ep_len:653 episode reward: total was 39.540000. running mean: 7.048345\n",
      "ep 3347: ep_len:500 episode reward: total was 10.640000. running mean: 7.084262\n",
      "ep 3347: ep_len:555 episode reward: total was -15.530000. running mean: 6.858119\n",
      "ep 3347: ep_len:374 episode reward: total was 35.730000. running mean: 7.146838\n",
      "ep 3347: ep_len:83 episode reward: total was -57.750000. running mean: 6.497870\n",
      "ep 3347: ep_len:523 episode reward: total was -1.030000. running mean: 6.422591\n",
      "ep 3347: ep_len:507 episode reward: total was 2.340000. running mean: 6.381765\n",
      "epsilon:0.051572 episode_count: 23436. steps_count: 10135945.000000\n",
      "Time elapsed:  30476.694905281067\n",
      "ep 3348: ep_len:586 episode reward: total was 70.350000. running mean: 7.021448\n",
      "ep 3348: ep_len:504 episode reward: total was 53.570000. running mean: 7.486933\n",
      "ep 3348: ep_len:543 episode reward: total was -12.740000. running mean: 7.284664\n",
      "ep 3348: ep_len:502 episode reward: total was 47.790000. running mean: 7.689717\n",
      "ep 3348: ep_len:3 episode reward: total was 1.010000. running mean: 7.622920\n",
      "ep 3348: ep_len:536 episode reward: total was -12.270000. running mean: 7.423991\n",
      "ep 3348: ep_len:294 episode reward: total was 23.970000. running mean: 7.589451\n",
      "epsilon:0.051528 episode_count: 23443. steps_count: 10138913.000000\n",
      "Time elapsed:  30484.575103998184\n",
      "ep 3349: ep_len:196 episode reward: total was -1.760000. running mean: 7.495956\n",
      "ep 3349: ep_len:571 episode reward: total was 98.630000. running mean: 8.407297\n",
      "ep 3349: ep_len:529 episode reward: total was -28.750000. running mean: 8.035724\n",
      "ep 3349: ep_len:512 episode reward: total was 68.100000. running mean: 8.636367\n",
      "ep 3349: ep_len:102 episode reward: total was 28.750000. running mean: 8.837503\n",
      "ep 3349: ep_len:164 episode reward: total was 28.950000. running mean: 9.038628\n",
      "ep 3349: ep_len:321 episode reward: total was -3.850000. running mean: 8.909742\n",
      "epsilon:0.051483 episode_count: 23450. steps_count: 10141308.000000\n",
      "Time elapsed:  30491.158447742462\n",
      "ep 3350: ep_len:533 episode reward: total was -5.670000. running mean: 8.763944\n",
      "ep 3350: ep_len:351 episode reward: total was 18.160000. running mean: 8.857905\n",
      "ep 3350: ep_len:672 episode reward: total was -28.520000. running mean: 8.484126\n",
      "ep 3350: ep_len:504 episode reward: total was 14.940000. running mean: 8.548684\n",
      "ep 3350: ep_len:92 episode reward: total was 16.590000. running mean: 8.629098\n",
      "ep 3350: ep_len:536 episode reward: total was -34.470000. running mean: 8.198107\n",
      "ep 3350: ep_len:500 episode reward: total was 10.260000. running mean: 8.218725\n",
      "epsilon:0.051439 episode_count: 23457. steps_count: 10144496.000000\n",
      "Time elapsed:  30499.390964984894\n",
      "ep 3351: ep_len:243 episode reward: total was 10.210000. running mean: 8.238638\n",
      "ep 3351: ep_len:376 episode reward: total was 1.610000. running mean: 8.172352\n",
      "ep 3351: ep_len:500 episode reward: total was -31.790000. running mean: 7.772728\n",
      "ep 3351: ep_len:500 episode reward: total was 63.480000. running mean: 8.329801\n",
      "ep 3351: ep_len:3 episode reward: total was 0.000000. running mean: 8.246503\n",
      "ep 3351: ep_len:179 episode reward: total was 29.680000. running mean: 8.460838\n",
      "ep 3351: ep_len:592 episode reward: total was -8.660000. running mean: 8.289630\n",
      "epsilon:0.051395 episode_count: 23464. steps_count: 10146889.000000\n",
      "Time elapsed:  30505.992432832718\n",
      "ep 3352: ep_len:674 episode reward: total was -58.890000. running mean: 7.617833\n",
      "ep 3352: ep_len:546 episode reward: total was -26.440000. running mean: 7.277255\n",
      "ep 3352: ep_len:570 episode reward: total was 39.440000. running mean: 7.598882\n",
      "ep 3352: ep_len:394 episode reward: total was 15.200000. running mean: 7.674894\n",
      "ep 3352: ep_len:3 episode reward: total was 1.010000. running mean: 7.608245\n",
      "ep 3352: ep_len:554 episode reward: total was -39.430000. running mean: 7.137862\n",
      "ep 3352: ep_len:625 episode reward: total was 40.940000. running mean: 7.475884\n",
      "epsilon:0.051350 episode_count: 23471. steps_count: 10150255.000000\n",
      "Time elapsed:  30514.771428346634\n",
      "ep 3353: ep_len:249 episode reward: total was 12.410000. running mean: 7.525225\n",
      "ep 3353: ep_len:582 episode reward: total was 48.060000. running mean: 7.930573\n",
      "ep 3353: ep_len:500 episode reward: total was 7.440000. running mean: 7.925667\n",
      "ep 3353: ep_len:546 episode reward: total was 17.700000. running mean: 8.023410\n",
      "ep 3353: ep_len:3 episode reward: total was 1.010000. running mean: 7.953276\n",
      "ep 3353: ep_len:616 episode reward: total was 20.480000. running mean: 8.078543\n",
      "ep 3353: ep_len:185 episode reward: total was -10.630000. running mean: 7.891458\n",
      "epsilon:0.051306 episode_count: 23478. steps_count: 10152936.000000\n",
      "Time elapsed:  30522.111176252365\n",
      "ep 3354: ep_len:512 episode reward: total was 8.010000. running mean: 7.892643\n",
      "ep 3354: ep_len:500 episode reward: total was 45.120000. running mean: 8.264917\n",
      "ep 3354: ep_len:612 episode reward: total was -12.020000. running mean: 8.062068\n",
      "ep 3354: ep_len:148 episode reward: total was 11.690000. running mean: 8.098347\n",
      "ep 3354: ep_len:3 episode reward: total was 1.010000. running mean: 8.027464\n",
      "ep 3354: ep_len:595 episode reward: total was 13.320000. running mean: 8.080389\n",
      "ep 3354: ep_len:616 episode reward: total was 48.380000. running mean: 8.483385\n",
      "epsilon:0.051262 episode_count: 23485. steps_count: 10155922.000000\n",
      "Time elapsed:  30530.066016197205\n",
      "ep 3355: ep_len:604 episode reward: total was -33.620000. running mean: 8.062351\n",
      "ep 3355: ep_len:583 episode reward: total was 10.710000. running mean: 8.088828\n",
      "ep 3355: ep_len:604 episode reward: total was 35.370000. running mean: 8.361639\n",
      "ep 3355: ep_len:533 episode reward: total was 30.630000. running mean: 8.584323\n",
      "ep 3355: ep_len:3 episode reward: total was 1.010000. running mean: 8.508580\n",
      "ep 3355: ep_len:590 episode reward: total was 15.190000. running mean: 8.575394\n",
      "ep 3355: ep_len:500 episode reward: total was 50.820000. running mean: 8.997840\n",
      "epsilon:0.051217 episode_count: 23492. steps_count: 10159339.000000\n",
      "Time elapsed:  30538.97770881653\n",
      "ep 3356: ep_len:500 episode reward: total was 50.050000. running mean: 9.408362\n",
      "ep 3356: ep_len:595 episode reward: total was 22.600000. running mean: 9.540278\n",
      "ep 3356: ep_len:500 episode reward: total was 14.860000. running mean: 9.593475\n",
      "ep 3356: ep_len:605 episode reward: total was 69.300000. running mean: 10.190540\n",
      "ep 3356: ep_len:3 episode reward: total was 1.010000. running mean: 10.098735\n",
      "ep 3356: ep_len:500 episode reward: total was -19.490000. running mean: 9.802848\n",
      "ep 3356: ep_len:331 episode reward: total was -10.110000. running mean: 9.603719\n",
      "epsilon:0.051173 episode_count: 23499. steps_count: 10162373.000000\n",
      "Time elapsed:  30550.33872485161\n",
      "ep 3357: ep_len:500 episode reward: total was -78.950000. running mean: 8.718182\n",
      "ep 3357: ep_len:500 episode reward: total was 31.390000. running mean: 8.944900\n",
      "ep 3357: ep_len:407 episode reward: total was -60.550000. running mean: 8.249951\n",
      "ep 3357: ep_len:502 episode reward: total was 27.180000. running mean: 8.439252\n",
      "ep 3357: ep_len:105 episode reward: total was -16.710000. running mean: 8.187759\n",
      "ep 3357: ep_len:514 episode reward: total was -13.790000. running mean: 7.967982\n",
      "ep 3357: ep_len:500 episode reward: total was 39.890000. running mean: 8.287202\n",
      "epsilon:0.051129 episode_count: 23506. steps_count: 10165401.000000\n",
      "Time elapsed:  30557.84258723259\n",
      "ep 3358: ep_len:500 episode reward: total was -43.000000. running mean: 7.774330\n",
      "ep 3358: ep_len:510 episode reward: total was 12.380000. running mean: 7.820386\n",
      "ep 3358: ep_len:507 episode reward: total was -0.710000. running mean: 7.735083\n",
      "ep 3358: ep_len:515 episode reward: total was 9.450000. running mean: 7.752232\n",
      "ep 3358: ep_len:3 episode reward: total was 1.010000. running mean: 7.684809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3358: ep_len:582 episode reward: total was -62.990000. running mean: 6.978061\n",
      "ep 3358: ep_len:297 episode reward: total was 3.340000. running mean: 6.941681\n",
      "epsilon:0.051084 episode_count: 23513. steps_count: 10168315.000000\n",
      "Time elapsed:  30565.689788341522\n",
      "ep 3359: ep_len:684 episode reward: total was -50.620000. running mean: 6.366064\n",
      "ep 3359: ep_len:534 episode reward: total was 33.440000. running mean: 6.636803\n",
      "ep 3359: ep_len:500 episode reward: total was -26.310000. running mean: 6.307335\n",
      "ep 3359: ep_len:500 episode reward: total was 41.320000. running mean: 6.657462\n",
      "ep 3359: ep_len:3 episode reward: total was 1.010000. running mean: 6.600987\n",
      "ep 3359: ep_len:556 episode reward: total was 7.830000. running mean: 6.613277\n",
      "ep 3359: ep_len:562 episode reward: total was -16.690000. running mean: 6.380245\n",
      "epsilon:0.051040 episode_count: 23520. steps_count: 10171654.000000\n",
      "Time elapsed:  30574.48628640175\n",
      "ep 3360: ep_len:513 episode reward: total was 41.850000. running mean: 6.734942\n",
      "ep 3360: ep_len:627 episode reward: total was 12.000000. running mean: 6.787593\n",
      "ep 3360: ep_len:500 episode reward: total was -1.100000. running mean: 6.708717\n",
      "ep 3360: ep_len:168 episode reward: total was 2.740000. running mean: 6.669030\n",
      "ep 3360: ep_len:111 episode reward: total was 25.240000. running mean: 6.854739\n",
      "ep 3360: ep_len:500 episode reward: total was -44.870000. running mean: 6.337492\n",
      "ep 3360: ep_len:508 episode reward: total was 13.990000. running mean: 6.414017\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.050996 episode_count: 23527. steps_count: 10174581.000000\n",
      "Time elapsed:  30587.0066781044\n",
      "ep 3361: ep_len:229 episode reward: total was -111.170000. running mean: 5.238177\n",
      "ep 3361: ep_len:609 episode reward: total was -3.950000. running mean: 5.146295\n",
      "ep 3361: ep_len:360 episode reward: total was 40.450000. running mean: 5.499332\n",
      "ep 3361: ep_len:500 episode reward: total was -34.990000. running mean: 5.094439\n",
      "ep 3361: ep_len:113 episode reward: total was 24.880000. running mean: 5.292294\n",
      "ep 3361: ep_len:183 episode reward: total was 28.740000. running mean: 5.526772\n",
      "ep 3361: ep_len:571 episode reward: total was 30.660000. running mean: 5.778104\n",
      "epsilon:0.050951 episode_count: 23534. steps_count: 10177146.000000\n",
      "Time elapsed:  30593.946148872375\n",
      "ep 3362: ep_len:208 episode reward: total was 29.250000. running mean: 6.012823\n",
      "ep 3362: ep_len:511 episode reward: total was 57.110000. running mean: 6.523795\n",
      "ep 3362: ep_len:526 episode reward: total was -100.980000. running mean: 5.448757\n",
      "ep 3362: ep_len:601 episode reward: total was 36.880000. running mean: 5.763069\n",
      "ep 3362: ep_len:3 episode reward: total was 1.010000. running mean: 5.715538\n",
      "ep 3362: ep_len:612 episode reward: total was 0.990000. running mean: 5.668283\n",
      "ep 3362: ep_len:504 episode reward: total was -11.870000. running mean: 5.492900\n",
      "epsilon:0.050907 episode_count: 23541. steps_count: 10180111.000000\n",
      "Time elapsed:  30604.01560664177\n",
      "ep 3363: ep_len:607 episode reward: total was -76.840000. running mean: 4.669571\n",
      "ep 3363: ep_len:633 episode reward: total was 85.560000. running mean: 5.478475\n",
      "ep 3363: ep_len:608 episode reward: total was 3.330000. running mean: 5.456991\n",
      "ep 3363: ep_len:56 episode reward: total was 5.840000. running mean: 5.460821\n",
      "ep 3363: ep_len:3 episode reward: total was 0.000000. running mean: 5.406213\n",
      "ep 3363: ep_len:536 episode reward: total was 18.730000. running mean: 5.539450\n",
      "ep 3363: ep_len:603 episode reward: total was 8.390000. running mean: 5.567956\n",
      "epsilon:0.050863 episode_count: 23548. steps_count: 10183157.000000\n",
      "Time elapsed:  30612.376109600067\n",
      "ep 3364: ep_len:607 episode reward: total was -123.570000. running mean: 4.276576\n",
      "ep 3364: ep_len:525 episode reward: total was 69.160000. running mean: 4.925411\n",
      "ep 3364: ep_len:500 episode reward: total was 27.590000. running mean: 5.152056\n",
      "ep 3364: ep_len:615 episode reward: total was 37.480000. running mean: 5.475336\n",
      "ep 3364: ep_len:85 episode reward: total was -15.740000. running mean: 5.263183\n",
      "ep 3364: ep_len:528 episode reward: total was -25.590000. running mean: 4.954651\n",
      "ep 3364: ep_len:500 episode reward: total was 16.520000. running mean: 5.070304\n",
      "epsilon:0.050818 episode_count: 23555. steps_count: 10186517.000000\n",
      "Time elapsed:  30620.8655397892\n",
      "ep 3365: ep_len:525 episode reward: total was -62.580000. running mean: 4.393801\n",
      "ep 3365: ep_len:587 episode reward: total was 24.480000. running mean: 4.594663\n",
      "ep 3365: ep_len:500 episode reward: total was 9.880000. running mean: 4.647517\n",
      "ep 3365: ep_len:561 episode reward: total was 56.520000. running mean: 5.166241\n",
      "ep 3365: ep_len:92 episode reward: total was -17.200000. running mean: 4.942579\n",
      "ep 3365: ep_len:517 episode reward: total was -1.180000. running mean: 4.881353\n",
      "ep 3365: ep_len:585 episode reward: total was 32.440000. running mean: 5.156940\n",
      "epsilon:0.050774 episode_count: 23562. steps_count: 10189884.000000\n",
      "Time elapsed:  30635.265253305435\n",
      "ep 3366: ep_len:576 episode reward: total was 52.030000. running mean: 5.625670\n",
      "ep 3366: ep_len:500 episode reward: total was 70.210000. running mean: 6.271514\n",
      "ep 3366: ep_len:651 episode reward: total was 3.800000. running mean: 6.246798\n",
      "ep 3366: ep_len:621 episode reward: total was 33.260000. running mean: 6.516930\n",
      "ep 3366: ep_len:3 episode reward: total was 1.010000. running mean: 6.461861\n",
      "ep 3366: ep_len:650 episode reward: total was -43.400000. running mean: 5.963243\n",
      "ep 3366: ep_len:586 episode reward: total was 31.100000. running mean: 6.214610\n",
      "epsilon:0.050730 episode_count: 23569. steps_count: 10193471.000000\n",
      "Time elapsed:  30644.564943552017\n",
      "ep 3367: ep_len:196 episode reward: total was 20.650000. running mean: 6.358964\n",
      "ep 3367: ep_len:577 episode reward: total was 6.090000. running mean: 6.356274\n",
      "ep 3367: ep_len:500 episode reward: total was 43.790000. running mean: 6.730612\n",
      "ep 3367: ep_len:511 episode reward: total was 28.370000. running mean: 6.947005\n",
      "ep 3367: ep_len:75 episode reward: total was 20.240000. running mean: 7.079935\n",
      "ep 3367: ep_len:564 episode reward: total was 37.680000. running mean: 7.385936\n",
      "ep 3367: ep_len:566 episode reward: total was 38.620000. running mean: 7.698277\n",
      "epsilon:0.050685 episode_count: 23576. steps_count: 10196460.000000\n",
      "Time elapsed:  30652.21281838417\n",
      "ep 3368: ep_len:634 episode reward: total was 32.130000. running mean: 7.942594\n",
      "ep 3368: ep_len:597 episode reward: total was 34.680000. running mean: 8.209968\n",
      "ep 3368: ep_len:500 episode reward: total was -3.340000. running mean: 8.094468\n",
      "ep 3368: ep_len:505 episode reward: total was 58.390000. running mean: 8.597424\n",
      "ep 3368: ep_len:3 episode reward: total was 1.010000. running mean: 8.521549\n",
      "ep 3368: ep_len:569 episode reward: total was 18.540000. running mean: 8.621734\n",
      "ep 3368: ep_len:500 episode reward: total was -139.970000. running mean: 7.135817\n",
      "epsilon:0.050641 episode_count: 23583. steps_count: 10199768.000000\n",
      "Time elapsed:  30660.812886238098\n",
      "ep 3369: ep_len:579 episode reward: total was -137.390000. running mean: 5.690558\n",
      "ep 3369: ep_len:582 episode reward: total was 47.400000. running mean: 6.107653\n",
      "ep 3369: ep_len:500 episode reward: total was 10.870000. running mean: 6.155276\n",
      "ep 3369: ep_len:170 episode reward: total was 15.220000. running mean: 6.245924\n",
      "ep 3369: ep_len:3 episode reward: total was 1.010000. running mean: 6.193564\n",
      "ep 3369: ep_len:538 episode reward: total was -35.620000. running mean: 5.775429\n",
      "ep 3369: ep_len:315 episode reward: total was 5.940000. running mean: 5.777074\n",
      "epsilon:0.050597 episode_count: 23590. steps_count: 10202455.000000\n",
      "Time elapsed:  30668.14584994316\n",
      "ep 3370: ep_len:621 episode reward: total was 32.980000. running mean: 6.049104\n",
      "ep 3370: ep_len:571 episode reward: total was 31.550000. running mean: 6.304113\n",
      "ep 3370: ep_len:526 episode reward: total was -1.470000. running mean: 6.226371\n",
      "ep 3370: ep_len:519 episode reward: total was 54.180000. running mean: 6.705908\n",
      "ep 3370: ep_len:3 episode reward: total was 1.010000. running mean: 6.648949\n",
      "ep 3370: ep_len:560 episode reward: total was 2.700000. running mean: 6.609459\n",
      "ep 3370: ep_len:500 episode reward: total was -12.900000. running mean: 6.414365\n",
      "epsilon:0.050552 episode_count: 23597. steps_count: 10205755.000000\n",
      "Time elapsed:  30676.789623260498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3371: ep_len:507 episode reward: total was 25.770000. running mean: 6.607921\n",
      "ep 3371: ep_len:500 episode reward: total was -8.910000. running mean: 6.452742\n",
      "ep 3371: ep_len:623 episode reward: total was -8.270000. running mean: 6.305514\n",
      "ep 3371: ep_len:513 episode reward: total was -27.750000. running mean: 5.964959\n",
      "ep 3371: ep_len:3 episode reward: total was 1.010000. running mean: 5.915410\n",
      "ep 3371: ep_len:610 episode reward: total was 15.900000. running mean: 6.015255\n",
      "ep 3371: ep_len:500 episode reward: total was -25.950000. running mean: 5.695603\n",
      "epsilon:0.050508 episode_count: 23604. steps_count: 10209011.000000\n",
      "Time elapsed:  30685.222620487213\n",
      "ep 3372: ep_len:570 episode reward: total was -33.490000. running mean: 5.303747\n",
      "ep 3372: ep_len:500 episode reward: total was 38.270000. running mean: 5.633409\n",
      "ep 3372: ep_len:671 episode reward: total was -7.500000. running mean: 5.502075\n",
      "ep 3372: ep_len:527 episode reward: total was 59.070000. running mean: 6.037755\n",
      "ep 3372: ep_len:53 episode reward: total was 9.760000. running mean: 6.074977\n",
      "ep 3372: ep_len:294 episode reward: total was 31.540000. running mean: 6.329627\n",
      "ep 3372: ep_len:506 episode reward: total was -7.530000. running mean: 6.191031\n",
      "epsilon:0.050464 episode_count: 23611. steps_count: 10212132.000000\n",
      "Time elapsed:  30691.640354156494\n",
      "ep 3373: ep_len:594 episode reward: total was 42.830000. running mean: 6.557421\n",
      "ep 3373: ep_len:346 episode reward: total was 17.410000. running mean: 6.665946\n",
      "ep 3373: ep_len:552 episode reward: total was 9.860000. running mean: 6.697887\n",
      "ep 3373: ep_len:509 episode reward: total was 34.110000. running mean: 6.972008\n",
      "ep 3373: ep_len:111 episode reward: total was 20.370000. running mean: 7.105988\n",
      "ep 3373: ep_len:500 episode reward: total was 7.480000. running mean: 7.109728\n",
      "ep 3373: ep_len:273 episode reward: total was -28.310000. running mean: 6.755531\n",
      "epsilon:0.050419 episode_count: 23618. steps_count: 10215017.000000\n",
      "Time elapsed:  30699.415814638138\n",
      "ep 3374: ep_len:567 episode reward: total was 67.130000. running mean: 7.359276\n",
      "ep 3374: ep_len:530 episode reward: total was 26.510000. running mean: 7.550783\n",
      "ep 3374: ep_len:631 episode reward: total was -34.780000. running mean: 7.127475\n",
      "ep 3374: ep_len:500 episode reward: total was 39.360000. running mean: 7.449800\n",
      "ep 3374: ep_len:84 episode reward: total was 15.100000. running mean: 7.526302\n",
      "ep 3374: ep_len:619 episode reward: total was 33.890000. running mean: 7.789939\n",
      "ep 3374: ep_len:322 episode reward: total was 12.940000. running mean: 7.841440\n",
      "epsilon:0.050375 episode_count: 23625. steps_count: 10218270.000000\n",
      "Time elapsed:  30707.955882310867\n",
      "ep 3375: ep_len:500 episode reward: total was -2.700000. running mean: 7.736025\n",
      "ep 3375: ep_len:600 episode reward: total was 42.610000. running mean: 8.084765\n",
      "ep 3375: ep_len:502 episode reward: total was 28.490000. running mean: 8.288818\n",
      "ep 3375: ep_len:500 episode reward: total was 31.170000. running mean: 8.517629\n",
      "ep 3375: ep_len:85 episode reward: total was 22.240000. running mean: 8.654853\n",
      "ep 3375: ep_len:640 episode reward: total was 38.020000. running mean: 8.948505\n",
      "ep 3375: ep_len:538 episode reward: total was 15.960000. running mean: 9.018619\n",
      "epsilon:0.050331 episode_count: 23632. steps_count: 10221635.000000\n",
      "Time elapsed:  30716.77446079254\n",
      "ep 3376: ep_len:652 episode reward: total was 11.070000. running mean: 9.039133\n",
      "ep 3376: ep_len:500 episode reward: total was 87.070000. running mean: 9.819442\n",
      "ep 3376: ep_len:500 episode reward: total was -8.220000. running mean: 9.639048\n",
      "ep 3376: ep_len:500 episode reward: total was 55.860000. running mean: 10.101257\n",
      "ep 3376: ep_len:77 episode reward: total was 14.200000. running mean: 10.142244\n",
      "ep 3376: ep_len:227 episode reward: total was 46.860000. running mean: 10.509422\n",
      "ep 3376: ep_len:521 episode reward: total was -2.050000. running mean: 10.383828\n",
      "epsilon:0.050286 episode_count: 23639. steps_count: 10224612.000000\n",
      "Time elapsed:  30724.775149583817\n",
      "ep 3377: ep_len:500 episode reward: total was 6.440000. running mean: 10.344390\n",
      "ep 3377: ep_len:589 episode reward: total was -120.310000. running mean: 9.037846\n",
      "ep 3377: ep_len:539 episode reward: total was -19.900000. running mean: 8.748467\n",
      "ep 3377: ep_len:524 episode reward: total was -16.930000. running mean: 8.491683\n",
      "ep 3377: ep_len:2 episode reward: total was -0.500000. running mean: 8.401766\n",
      "ep 3377: ep_len:611 episode reward: total was -16.970000. running mean: 8.148048\n",
      "ep 3377: ep_len:500 episode reward: total was 56.610000. running mean: 8.632668\n",
      "epsilon:0.050242 episode_count: 23646. steps_count: 10227877.000000\n",
      "Time elapsed:  30738.701726913452\n",
      "ep 3378: ep_len:589 episode reward: total was 41.800000. running mean: 8.964341\n",
      "ep 3378: ep_len:680 episode reward: total was -115.700000. running mean: 7.717697\n",
      "ep 3378: ep_len:500 episode reward: total was 50.510000. running mean: 8.145621\n",
      "ep 3378: ep_len:501 episode reward: total was 40.290000. running mean: 8.467064\n",
      "ep 3378: ep_len:3 episode reward: total was 1.010000. running mean: 8.392494\n",
      "ep 3378: ep_len:170 episode reward: total was 30.750000. running mean: 8.616069\n",
      "ep 3378: ep_len:618 episode reward: total was 1.920000. running mean: 8.549108\n",
      "epsilon:0.050198 episode_count: 23653. steps_count: 10230938.000000\n",
      "Time elapsed:  30751.795069217682\n",
      "ep 3379: ep_len:131 episode reward: total was 13.460000. running mean: 8.598217\n",
      "ep 3379: ep_len:500 episode reward: total was 55.760000. running mean: 9.069835\n",
      "ep 3379: ep_len:424 episode reward: total was 55.100000. running mean: 9.530136\n",
      "ep 3379: ep_len:523 episode reward: total was 25.140000. running mean: 9.686235\n",
      "ep 3379: ep_len:50 episode reward: total was 19.000000. running mean: 9.779373\n",
      "ep 3379: ep_len:790 episode reward: total was -205.410000. running mean: 7.627479\n",
      "ep 3379: ep_len:211 episode reward: total was -22.520000. running mean: 7.326004\n",
      "epsilon:0.050153 episode_count: 23660. steps_count: 10233567.000000\n",
      "Time elapsed:  30758.742784500122\n",
      "ep 3380: ep_len:503 episode reward: total was 75.610000. running mean: 8.008844\n",
      "ep 3380: ep_len:560 episode reward: total was 103.110000. running mean: 8.959856\n",
      "ep 3380: ep_len:500 episode reward: total was -51.770000. running mean: 8.352557\n",
      "ep 3380: ep_len:631 episode reward: total was 25.920000. running mean: 8.528232\n",
      "ep 3380: ep_len:55 episode reward: total was 22.510000. running mean: 8.668049\n",
      "ep 3380: ep_len:516 episode reward: total was 29.140000. running mean: 8.872769\n",
      "ep 3380: ep_len:520 episode reward: total was 4.740000. running mean: 8.831441\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.050109 episode_count: 23667. steps_count: 10236852.000000\n",
      "Time elapsed:  30772.74063038826\n",
      "ep 3381: ep_len:205 episode reward: total was 18.780000. running mean: 8.930927\n",
      "ep 3381: ep_len:195 episode reward: total was 4.380000. running mean: 8.885417\n",
      "ep 3381: ep_len:500 episode reward: total was 2.550000. running mean: 8.822063\n",
      "ep 3381: ep_len:406 episode reward: total was -5.550000. running mean: 8.678343\n",
      "ep 3381: ep_len:53 episode reward: total was 20.500000. running mean: 8.796559\n",
      "ep 3381: ep_len:328 episode reward: total was 15.050000. running mean: 8.859094\n",
      "ep 3381: ep_len:315 episode reward: total was -43.040000. running mean: 8.340103\n",
      "epsilon:0.050065 episode_count: 23674. steps_count: 10238854.000000\n",
      "Time elapsed:  30779.218029499054\n",
      "ep 3382: ep_len:129 episode reward: total was 6.520000. running mean: 8.321902\n",
      "ep 3382: ep_len:500 episode reward: total was 83.000000. running mean: 9.068683\n",
      "ep 3382: ep_len:531 episode reward: total was -57.720000. running mean: 8.400796\n",
      "ep 3382: ep_len:532 episode reward: total was 21.350000. running mean: 8.530288\n",
      "ep 3382: ep_len:3 episode reward: total was 1.010000. running mean: 8.455085\n",
      "ep 3382: ep_len:500 episode reward: total was -45.720000. running mean: 7.913334\n",
      "ep 3382: ep_len:500 episode reward: total was 30.440000. running mean: 8.138601\n",
      "epsilon:0.050020 episode_count: 23681. steps_count: 10241549.000000\n",
      "Time elapsed:  30794.626396417618\n",
      "ep 3383: ep_len:667 episode reward: total was -24.200000. running mean: 7.815215\n",
      "ep 3383: ep_len:526 episode reward: total was 63.960000. running mean: 8.376663\n",
      "ep 3383: ep_len:577 episode reward: total was 14.310000. running mean: 8.435996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3383: ep_len:500 episode reward: total was 21.760000. running mean: 8.569236\n",
      "ep 3383: ep_len:92 episode reward: total was 21.240000. running mean: 8.695944\n",
      "ep 3383: ep_len:523 episode reward: total was 23.910000. running mean: 8.848084\n",
      "ep 3383: ep_len:350 episode reward: total was 14.570000. running mean: 8.905303\n",
      "epsilon:0.049976 episode_count: 23688. steps_count: 10244784.000000\n",
      "Time elapsed:  30802.805191755295\n",
      "ep 3384: ep_len:649 episode reward: total was -32.310000. running mean: 8.493150\n",
      "ep 3384: ep_len:500 episode reward: total was 71.830000. running mean: 9.126519\n",
      "ep 3384: ep_len:427 episode reward: total was 41.600000. running mean: 9.451254\n",
      "ep 3384: ep_len:145 episode reward: total was 2.220000. running mean: 9.378941\n",
      "ep 3384: ep_len:3 episode reward: total was 1.010000. running mean: 9.295252\n",
      "ep 3384: ep_len:500 episode reward: total was 24.790000. running mean: 9.450199\n",
      "ep 3384: ep_len:512 episode reward: total was -182.270000. running mean: 7.532997\n",
      "epsilon:0.049932 episode_count: 23695. steps_count: 10247520.000000\n",
      "Time elapsed:  30810.1148750782\n",
      "ep 3385: ep_len:637 episode reward: total was 60.810000. running mean: 8.065767\n",
      "ep 3385: ep_len:554 episode reward: total was 8.560000. running mean: 8.070710\n",
      "ep 3385: ep_len:577 episode reward: total was -14.840000. running mean: 7.841602\n",
      "ep 3385: ep_len:517 episode reward: total was 3.830000. running mean: 7.801486\n",
      "ep 3385: ep_len:112 episode reward: total was 33.260000. running mean: 8.056072\n",
      "ep 3385: ep_len:536 episode reward: total was -12.150000. running mean: 7.854011\n",
      "ep 3385: ep_len:535 episode reward: total was 30.410000. running mean: 8.079571\n",
      "epsilon:0.049887 episode_count: 23702. steps_count: 10250988.000000\n",
      "Time elapsed:  30820.357445716858\n",
      "ep 3386: ep_len:574 episode reward: total was 66.250000. running mean: 8.661275\n",
      "ep 3386: ep_len:530 episode reward: total was 38.130000. running mean: 8.955962\n",
      "ep 3386: ep_len:500 episode reward: total was 14.990000. running mean: 9.016303\n",
      "ep 3386: ep_len:500 episode reward: total was 22.170000. running mean: 9.147840\n",
      "ep 3386: ep_len:3 episode reward: total was 1.010000. running mean: 9.066461\n",
      "ep 3386: ep_len:526 episode reward: total was -10.470000. running mean: 8.871097\n",
      "ep 3386: ep_len:519 episode reward: total was -6.950000. running mean: 8.712886\n",
      "epsilon:0.049843 episode_count: 23709. steps_count: 10254140.000000\n",
      "Time elapsed:  30828.747874736786\n",
      "ep 3387: ep_len:609 episode reward: total was -16.270000. running mean: 8.463057\n",
      "ep 3387: ep_len:561 episode reward: total was 24.480000. running mean: 8.623226\n",
      "ep 3387: ep_len:629 episode reward: total was -31.220000. running mean: 8.224794\n",
      "ep 3387: ep_len:500 episode reward: total was 15.910000. running mean: 8.301646\n",
      "ep 3387: ep_len:3 episode reward: total was 1.010000. running mean: 8.228730\n",
      "ep 3387: ep_len:251 episode reward: total was 36.510000. running mean: 8.511542\n",
      "ep 3387: ep_len:518 episode reward: total was 44.030000. running mean: 8.866727\n",
      "epsilon:0.049799 episode_count: 23716. steps_count: 10257211.000000\n",
      "Time elapsed:  30837.293931007385\n",
      "ep 3388: ep_len:608 episode reward: total was 50.170000. running mean: 9.279760\n",
      "ep 3388: ep_len:341 episode reward: total was 25.070000. running mean: 9.437662\n",
      "ep 3388: ep_len:662 episode reward: total was -35.450000. running mean: 8.988785\n",
      "ep 3388: ep_len:626 episode reward: total was 22.780000. running mean: 9.126698\n",
      "ep 3388: ep_len:56 episode reward: total was 25.000000. running mean: 9.285431\n",
      "ep 3388: ep_len:669 episode reward: total was 26.240000. running mean: 9.454976\n",
      "ep 3388: ep_len:568 episode reward: total was 28.030000. running mean: 9.640726\n",
      "epsilon:0.049754 episode_count: 23723. steps_count: 10260741.000000\n",
      "Time elapsed:  30851.280709028244\n",
      "ep 3389: ep_len:221 episode reward: total was 12.700000. running mean: 9.671319\n",
      "ep 3389: ep_len:601 episode reward: total was -33.390000. running mean: 9.240706\n",
      "ep 3389: ep_len:657 episode reward: total was -22.740000. running mean: 8.920899\n",
      "ep 3389: ep_len:515 episode reward: total was 45.080000. running mean: 9.282490\n",
      "ep 3389: ep_len:93 episode reward: total was 28.660000. running mean: 9.476265\n",
      "ep 3389: ep_len:588 episode reward: total was -88.660000. running mean: 8.494902\n",
      "ep 3389: ep_len:500 episode reward: total was 32.730000. running mean: 8.737253\n",
      "epsilon:0.049710 episode_count: 23730. steps_count: 10263916.000000\n",
      "Time elapsed:  30858.73040509224\n",
      "ep 3390: ep_len:541 episode reward: total was -54.530000. running mean: 8.104581\n",
      "ep 3390: ep_len:543 episode reward: total was 51.240000. running mean: 8.535935\n",
      "ep 3390: ep_len:553 episode reward: total was 8.030000. running mean: 8.530876\n",
      "ep 3390: ep_len:565 episode reward: total was -2.570000. running mean: 8.419867\n",
      "ep 3390: ep_len:3 episode reward: total was -0.490000. running mean: 8.330768\n",
      "ep 3390: ep_len:244 episode reward: total was 30.530000. running mean: 8.552761\n",
      "ep 3390: ep_len:533 episode reward: total was -33.240000. running mean: 8.134833\n",
      "epsilon:0.049666 episode_count: 23737. steps_count: 10266898.000000\n",
      "Time elapsed:  30863.331849098206\n",
      "ep 3391: ep_len:572 episode reward: total was 20.530000. running mean: 8.258785\n",
      "ep 3391: ep_len:287 episode reward: total was 9.730000. running mean: 8.273497\n",
      "ep 3391: ep_len:553 episode reward: total was 49.030000. running mean: 8.681062\n",
      "ep 3391: ep_len:532 episode reward: total was 56.210000. running mean: 9.156351\n",
      "ep 3391: ep_len:3 episode reward: total was 1.010000. running mean: 9.074888\n",
      "ep 3391: ep_len:500 episode reward: total was 7.650000. running mean: 9.060639\n",
      "ep 3391: ep_len:207 episode reward: total was 7.650000. running mean: 9.046532\n",
      "epsilon:0.049621 episode_count: 23744. steps_count: 10269552.000000\n",
      "Time elapsed:  30871.831212759018\n",
      "ep 3392: ep_len:500 episode reward: total was 44.650000. running mean: 9.402567\n",
      "ep 3392: ep_len:500 episode reward: total was 5.120000. running mean: 9.359741\n",
      "ep 3392: ep_len:429 episode reward: total was 45.900000. running mean: 9.725144\n",
      "ep 3392: ep_len:500 episode reward: total was 62.410000. running mean: 10.251993\n",
      "ep 3392: ep_len:50 episode reward: total was 22.000000. running mean: 10.369473\n",
      "ep 3392: ep_len:559 episode reward: total was -0.670000. running mean: 10.259078\n",
      "ep 3392: ep_len:602 episode reward: total was 30.650000. running mean: 10.462987\n",
      "epsilon:0.049577 episode_count: 23751. steps_count: 10272692.000000\n",
      "Time elapsed:  30878.783376693726\n",
      "ep 3393: ep_len:500 episode reward: total was -29.180000. running mean: 10.066557\n",
      "ep 3393: ep_len:531 episode reward: total was -2.270000. running mean: 9.943192\n",
      "ep 3393: ep_len:691 episode reward: total was 2.510000. running mean: 9.868860\n",
      "ep 3393: ep_len:500 episode reward: total was 24.320000. running mean: 10.013371\n",
      "ep 3393: ep_len:48 episode reward: total was 19.010000. running mean: 10.103337\n",
      "ep 3393: ep_len:529 episode reward: total was -2.640000. running mean: 9.975904\n",
      "ep 3393: ep_len:542 episode reward: total was 2.310000. running mean: 9.899245\n",
      "epsilon:0.049533 episode_count: 23758. steps_count: 10276033.000000\n",
      "Time elapsed:  30886.550961971283\n",
      "ep 3394: ep_len:500 episode reward: total was -43.670000. running mean: 9.363553\n",
      "ep 3394: ep_len:500 episode reward: total was 34.000000. running mean: 9.609917\n",
      "ep 3394: ep_len:656 episode reward: total was -14.720000. running mean: 9.366618\n",
      "ep 3394: ep_len:514 episode reward: total was -5.250000. running mean: 9.220452\n",
      "ep 3394: ep_len:93 episode reward: total was 22.200000. running mean: 9.350247\n",
      "ep 3394: ep_len:154 episode reward: total was 36.530000. running mean: 9.622045\n",
      "ep 3394: ep_len:509 episode reward: total was 20.220000. running mean: 9.728024\n",
      "epsilon:0.049488 episode_count: 23765. steps_count: 10278959.000000\n",
      "Time elapsed:  30903.29394197464\n",
      "ep 3395: ep_len:605 episode reward: total was -10.960000. running mean: 9.521144\n",
      "ep 3395: ep_len:543 episode reward: total was -2.190000. running mean: 9.404033\n",
      "ep 3395: ep_len:543 episode reward: total was -37.060000. running mean: 8.939392\n",
      "ep 3395: ep_len:500 episode reward: total was 51.450000. running mean: 9.364498\n",
      "ep 3395: ep_len:3 episode reward: total was 1.010000. running mean: 9.280953\n",
      "ep 3395: ep_len:531 episode reward: total was -0.600000. running mean: 9.182144\n",
      "ep 3395: ep_len:619 episode reward: total was -0.740000. running mean: 9.082922\n",
      "epsilon:0.049444 episode_count: 23772. steps_count: 10282303.000000\n",
      "Time elapsed:  30912.149736881256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3396: ep_len:593 episode reward: total was -463.370000. running mean: 4.358393\n",
      "ep 3396: ep_len:503 episode reward: total was 50.840000. running mean: 4.823209\n",
      "ep 3396: ep_len:627 episode reward: total was -37.980000. running mean: 4.395177\n",
      "ep 3396: ep_len:132 episode reward: total was 14.110000. running mean: 4.492325\n",
      "ep 3396: ep_len:65 episode reward: total was 18.640000. running mean: 4.633802\n",
      "ep 3396: ep_len:730 episode reward: total was -373.880000. running mean: 0.848664\n",
      "ep 3396: ep_len:567 episode reward: total was 40.280000. running mean: 1.242977\n",
      "epsilon:0.049400 episode_count: 23779. steps_count: 10285520.000000\n",
      "Time elapsed:  30926.647406101227\n",
      "ep 3397: ep_len:252 episode reward: total was 12.900000. running mean: 1.359548\n",
      "ep 3397: ep_len:500 episode reward: total was 129.940000. running mean: 2.645352\n",
      "ep 3397: ep_len:79 episode reward: total was 2.770000. running mean: 2.646599\n",
      "ep 3397: ep_len:419 episode reward: total was -15.050000. running mean: 2.469633\n",
      "ep 3397: ep_len:3 episode reward: total was 1.010000. running mean: 2.455036\n",
      "ep 3397: ep_len:521 episode reward: total was 27.810000. running mean: 2.708586\n",
      "ep 3397: ep_len:603 episode reward: total was 14.710000. running mean: 2.828600\n",
      "epsilon:0.049355 episode_count: 23786. steps_count: 10287897.000000\n",
      "Time elapsed:  30933.21271634102\n",
      "ep 3398: ep_len:612 episode reward: total was 80.530000. running mean: 3.605614\n",
      "ep 3398: ep_len:500 episode reward: total was -9.510000. running mean: 3.474458\n",
      "ep 3398: ep_len:510 episode reward: total was 11.080000. running mean: 3.550513\n",
      "ep 3398: ep_len:535 episode reward: total was 2.580000. running mean: 3.540808\n",
      "ep 3398: ep_len:3 episode reward: total was 1.010000. running mean: 3.515500\n",
      "ep 3398: ep_len:500 episode reward: total was 33.340000. running mean: 3.813745\n",
      "ep 3398: ep_len:168 episode reward: total was -5.660000. running mean: 3.719008\n",
      "epsilon:0.049311 episode_count: 23793. steps_count: 10290725.000000\n",
      "Time elapsed:  30946.819101810455\n",
      "ep 3399: ep_len:500 episode reward: total was -24.330000. running mean: 3.438518\n",
      "ep 3399: ep_len:615 episode reward: total was 31.390000. running mean: 3.718033\n",
      "ep 3399: ep_len:627 episode reward: total was -10.210000. running mean: 3.578752\n",
      "ep 3399: ep_len:532 episode reward: total was 5.870000. running mean: 3.601665\n",
      "ep 3399: ep_len:3 episode reward: total was 1.010000. running mean: 3.575748\n",
      "ep 3399: ep_len:662 episode reward: total was 12.060000. running mean: 3.660591\n",
      "ep 3399: ep_len:510 episode reward: total was 27.180000. running mean: 3.895785\n",
      "epsilon:0.049267 episode_count: 23800. steps_count: 10294174.000000\n",
      "Time elapsed:  30957.564106702805\n",
      "ep 3400: ep_len:500 episode reward: total was 41.560000. running mean: 4.272427\n",
      "ep 3400: ep_len:500 episode reward: total was 27.340000. running mean: 4.503103\n",
      "ep 3400: ep_len:79 episode reward: total was 4.300000. running mean: 4.501072\n",
      "ep 3400: ep_len:500 episode reward: total was 14.400000. running mean: 4.600061\n",
      "ep 3400: ep_len:37 episode reward: total was 16.510000. running mean: 4.719160\n",
      "ep 3400: ep_len:500 episode reward: total was 8.910000. running mean: 4.761069\n",
      "ep 3400: ep_len:561 episode reward: total was -1.740000. running mean: 4.696058\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.049222 episode_count: 23807. steps_count: 10296851.000000\n",
      "Time elapsed:  30969.588013648987\n",
      "ep 3401: ep_len:582 episode reward: total was 66.830000. running mean: 5.317397\n",
      "ep 3401: ep_len:524 episode reward: total was -5.220000. running mean: 5.212023\n",
      "ep 3401: ep_len:557 episode reward: total was 34.710000. running mean: 5.507003\n",
      "ep 3401: ep_len:501 episode reward: total was 22.200000. running mean: 5.673933\n",
      "ep 3401: ep_len:3 episode reward: total was 1.010000. running mean: 5.627294\n",
      "ep 3401: ep_len:661 episode reward: total was 18.350000. running mean: 5.754521\n",
      "ep 3401: ep_len:567 episode reward: total was 12.850000. running mean: 5.825476\n",
      "epsilon:0.049178 episode_count: 23814. steps_count: 10300246.000000\n",
      "Time elapsed:  30978.390748023987\n",
      "ep 3402: ep_len:512 episode reward: total was -60.320000. running mean: 5.164021\n",
      "ep 3402: ep_len:518 episode reward: total was 84.190000. running mean: 5.954281\n",
      "ep 3402: ep_len:458 episode reward: total was 47.270000. running mean: 6.367438\n",
      "ep 3402: ep_len:500 episode reward: total was 13.920000. running mean: 6.442963\n",
      "ep 3402: ep_len:128 episode reward: total was 28.370000. running mean: 6.662234\n",
      "ep 3402: ep_len:509 episode reward: total was 26.380000. running mean: 6.859411\n",
      "ep 3402: ep_len:500 episode reward: total was 39.680000. running mean: 7.187617\n",
      "epsilon:0.049134 episode_count: 23821. steps_count: 10303371.000000\n",
      "Time elapsed:  30986.767674684525\n",
      "ep 3403: ep_len:257 episode reward: total was 30.950000. running mean: 7.425241\n",
      "ep 3403: ep_len:500 episode reward: total was 53.780000. running mean: 7.888789\n",
      "ep 3403: ep_len:622 episode reward: total was 2.410000. running mean: 7.834001\n",
      "ep 3403: ep_len:116 episode reward: total was 9.970000. running mean: 7.855361\n",
      "ep 3403: ep_len:3 episode reward: total was 1.010000. running mean: 7.786907\n",
      "ep 3403: ep_len:625 episode reward: total was 4.860000. running mean: 7.757638\n",
      "ep 3403: ep_len:571 episode reward: total was 30.010000. running mean: 7.980162\n",
      "epsilon:0.049089 episode_count: 23828. steps_count: 10306065.000000\n",
      "Time elapsed:  30994.06932449341\n",
      "ep 3404: ep_len:617 episode reward: total was -8.120000. running mean: 7.819160\n",
      "ep 3404: ep_len:500 episode reward: total was 89.550000. running mean: 8.636469\n",
      "ep 3404: ep_len:520 episode reward: total was 43.850000. running mean: 8.988604\n",
      "ep 3404: ep_len:605 episode reward: total was 34.150000. running mean: 9.240218\n",
      "ep 3404: ep_len:3 episode reward: total was 1.010000. running mean: 9.157916\n",
      "ep 3404: ep_len:234 episode reward: total was 25.320000. running mean: 9.319537\n",
      "ep 3404: ep_len:211 episode reward: total was -1.430000. running mean: 9.212041\n",
      "epsilon:0.049045 episode_count: 23835. steps_count: 10308755.000000\n",
      "Time elapsed:  31001.327732801437\n",
      "ep 3405: ep_len:527 episode reward: total was -1.660000. running mean: 9.103321\n",
      "ep 3405: ep_len:500 episode reward: total was 71.350000. running mean: 9.725788\n",
      "ep 3405: ep_len:533 episode reward: total was 34.120000. running mean: 9.969730\n",
      "ep 3405: ep_len:500 episode reward: total was 41.850000. running mean: 10.288532\n",
      "ep 3405: ep_len:3 episode reward: total was 1.010000. running mean: 10.195747\n",
      "ep 3405: ep_len:550 episode reward: total was 14.740000. running mean: 10.241190\n",
      "ep 3405: ep_len:517 episode reward: total was -40.990000. running mean: 9.728878\n",
      "epsilon:0.049001 episode_count: 23842. steps_count: 10311885.000000\n",
      "Time elapsed:  31009.46898674965\n",
      "ep 3406: ep_len:500 episode reward: total was -56.000000. running mean: 9.071589\n",
      "ep 3406: ep_len:500 episode reward: total was 19.090000. running mean: 9.171773\n",
      "ep 3406: ep_len:520 episode reward: total was -51.220000. running mean: 8.567855\n",
      "ep 3406: ep_len:559 episode reward: total was 11.670000. running mean: 8.598877\n",
      "ep 3406: ep_len:127 episode reward: total was -54.640000. running mean: 7.966488\n",
      "ep 3406: ep_len:633 episode reward: total was 44.310000. running mean: 8.329923\n",
      "ep 3406: ep_len:518 episode reward: total was 12.790000. running mean: 8.374524\n",
      "epsilon:0.048956 episode_count: 23849. steps_count: 10315242.000000\n",
      "Time elapsed:  31017.902007818222\n",
      "ep 3407: ep_len:500 episode reward: total was 58.900000. running mean: 8.879779\n",
      "ep 3407: ep_len:146 episode reward: total was 0.740000. running mean: 8.798381\n",
      "ep 3407: ep_len:625 episode reward: total was -31.170000. running mean: 8.398697\n",
      "ep 3407: ep_len:507 episode reward: total was -47.530000. running mean: 7.839410\n",
      "ep 3407: ep_len:3 episode reward: total was 1.010000. running mean: 7.771116\n",
      "ep 3407: ep_len:500 episode reward: total was -37.530000. running mean: 7.318105\n",
      "ep 3407: ep_len:534 episode reward: total was 32.620000. running mean: 7.571124\n",
      "epsilon:0.048912 episode_count: 23856. steps_count: 10318057.000000\n",
      "Time elapsed:  31025.421677827835\n",
      "ep 3408: ep_len:501 episode reward: total was 80.370000. running mean: 8.299113\n",
      "ep 3408: ep_len:500 episode reward: total was 54.350000. running mean: 8.759621\n",
      "ep 3408: ep_len:532 episode reward: total was -25.770000. running mean: 8.414325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3408: ep_len:500 episode reward: total was 15.880000. running mean: 8.488982\n",
      "ep 3408: ep_len:52 episode reward: total was 8.280000. running mean: 8.486892\n",
      "ep 3408: ep_len:634 episode reward: total was 0.970000. running mean: 8.411723\n",
      "ep 3408: ep_len:516 episode reward: total was 7.370000. running mean: 8.401306\n",
      "epsilon:0.048868 episode_count: 23863. steps_count: 10321292.000000\n",
      "Time elapsed:  31033.95835852623\n",
      "ep 3409: ep_len:575 episode reward: total was -50.090000. running mean: 7.816393\n",
      "ep 3409: ep_len:556 episode reward: total was 20.790000. running mean: 7.946129\n",
      "ep 3409: ep_len:393 episode reward: total was 43.660000. running mean: 8.303268\n",
      "ep 3409: ep_len:132 episode reward: total was 20.580000. running mean: 8.426035\n",
      "ep 3409: ep_len:3 episode reward: total was 1.010000. running mean: 8.351875\n",
      "ep 3409: ep_len:335 episode reward: total was 9.240000. running mean: 8.360756\n",
      "ep 3409: ep_len:604 episode reward: total was 36.310000. running mean: 8.640248\n",
      "epsilon:0.048823 episode_count: 23870. steps_count: 10323890.000000\n",
      "Time elapsed:  31040.893009901047\n",
      "ep 3410: ep_len:500 episode reward: total was 95.140000. running mean: 9.505246\n",
      "ep 3410: ep_len:533 episode reward: total was 77.270000. running mean: 10.182893\n",
      "ep 3410: ep_len:63 episode reward: total was 2.820000. running mean: 10.109264\n",
      "ep 3410: ep_len:500 episode reward: total was 14.630000. running mean: 10.154472\n",
      "ep 3410: ep_len:33 episode reward: total was 15.000000. running mean: 10.202927\n",
      "ep 3410: ep_len:523 episode reward: total was 4.920000. running mean: 10.150098\n",
      "ep 3410: ep_len:340 episode reward: total was -22.610000. running mean: 9.822497\n",
      "epsilon:0.048779 episode_count: 23877. steps_count: 10326382.000000\n",
      "Time elapsed:  31047.63733148575\n",
      "ep 3411: ep_len:524 episode reward: total was 8.810000. running mean: 9.812372\n",
      "ep 3411: ep_len:504 episode reward: total was 17.740000. running mean: 9.891648\n",
      "ep 3411: ep_len:500 episode reward: total was 2.870000. running mean: 9.821432\n",
      "ep 3411: ep_len:500 episode reward: total was 31.620000. running mean: 10.039417\n",
      "ep 3411: ep_len:3 episode reward: total was 1.010000. running mean: 9.949123\n",
      "ep 3411: ep_len:500 episode reward: total was -1.830000. running mean: 9.831332\n",
      "ep 3411: ep_len:501 episode reward: total was -19.220000. running mean: 9.540819\n",
      "epsilon:0.048735 episode_count: 23884. steps_count: 10329414.000000\n",
      "Time elapsed:  31060.922353982925\n",
      "ep 3412: ep_len:517 episode reward: total was 21.470000. running mean: 9.660110\n",
      "ep 3412: ep_len:591 episode reward: total was 25.920000. running mean: 9.822709\n",
      "ep 3412: ep_len:508 episode reward: total was 15.780000. running mean: 9.882282\n",
      "ep 3412: ep_len:500 episode reward: total was -20.790000. running mean: 9.575559\n",
      "ep 3412: ep_len:48 episode reward: total was 19.500000. running mean: 9.674804\n",
      "ep 3412: ep_len:558 episode reward: total was -3.940000. running mean: 9.538656\n",
      "ep 3412: ep_len:513 episode reward: total was -5.100000. running mean: 9.392269\n",
      "epsilon:0.048690 episode_count: 23891. steps_count: 10332649.000000\n",
      "Time elapsed:  31073.343866825104\n",
      "ep 3413: ep_len:639 episode reward: total was 20.430000. running mean: 9.502647\n",
      "ep 3413: ep_len:500 episode reward: total was 108.050000. running mean: 10.488120\n",
      "ep 3413: ep_len:529 episode reward: total was 27.720000. running mean: 10.660439\n",
      "ep 3413: ep_len:537 episode reward: total was 50.580000. running mean: 11.059634\n",
      "ep 3413: ep_len:3 episode reward: total was 1.010000. running mean: 10.959138\n",
      "ep 3413: ep_len:293 episode reward: total was 28.710000. running mean: 11.136647\n",
      "ep 3413: ep_len:314 episode reward: total was 5.010000. running mean: 11.075380\n",
      "epsilon:0.048646 episode_count: 23898. steps_count: 10335464.000000\n",
      "Time elapsed:  31080.79276895523\n",
      "ep 3414: ep_len:88 episode reward: total was 9.350000. running mean: 11.058126\n",
      "ep 3414: ep_len:347 episode reward: total was 16.720000. running mean: 11.114745\n",
      "ep 3414: ep_len:395 episode reward: total was 56.230000. running mean: 11.565898\n",
      "ep 3414: ep_len:100 episode reward: total was -5.070000. running mean: 11.399539\n",
      "ep 3414: ep_len:3 episode reward: total was 1.010000. running mean: 11.295643\n",
      "ep 3414: ep_len:592 episode reward: total was -15.520000. running mean: 11.027487\n",
      "ep 3414: ep_len:502 episode reward: total was -17.460000. running mean: 10.742612\n",
      "epsilon:0.048602 episode_count: 23905. steps_count: 10337491.000000\n",
      "Time elapsed:  31086.551203727722\n",
      "ep 3415: ep_len:502 episode reward: total was -89.050000. running mean: 9.744686\n",
      "ep 3415: ep_len:507 episode reward: total was 39.850000. running mean: 10.045739\n",
      "ep 3415: ep_len:578 episode reward: total was -24.760000. running mean: 9.697682\n",
      "ep 3415: ep_len:502 episode reward: total was -0.260000. running mean: 9.598105\n",
      "ep 3415: ep_len:3 episode reward: total was 1.010000. running mean: 9.512224\n",
      "ep 3415: ep_len:250 episode reward: total was 46.480000. running mean: 9.881902\n",
      "ep 3415: ep_len:593 episode reward: total was 16.680000. running mean: 9.949883\n",
      "epsilon:0.048557 episode_count: 23912. steps_count: 10340426.000000\n",
      "Time elapsed:  31096.962292909622\n",
      "ep 3416: ep_len:518 episode reward: total was 51.720000. running mean: 10.367584\n",
      "ep 3416: ep_len:500 episode reward: total was 25.200000. running mean: 10.515908\n",
      "ep 3416: ep_len:418 episode reward: total was -37.690000. running mean: 10.033849\n",
      "ep 3416: ep_len:576 episode reward: total was 101.980000. running mean: 10.953310\n",
      "ep 3416: ep_len:3 episode reward: total was 1.010000. running mean: 10.853877\n",
      "ep 3416: ep_len:551 episode reward: total was -25.650000. running mean: 10.488839\n",
      "ep 3416: ep_len:572 episode reward: total was 26.750000. running mean: 10.651450\n",
      "epsilon:0.048513 episode_count: 23919. steps_count: 10343564.000000\n",
      "Time elapsed:  31110.863140821457\n",
      "ep 3417: ep_len:567 episode reward: total was 24.410000. running mean: 10.789036\n",
      "ep 3417: ep_len:586 episode reward: total was -3.810000. running mean: 10.643045\n",
      "ep 3417: ep_len:571 episode reward: total was -0.790000. running mean: 10.528715\n",
      "ep 3417: ep_len:131 episode reward: total was 22.100000. running mean: 10.644428\n",
      "ep 3417: ep_len:3 episode reward: total was 1.010000. running mean: 10.548083\n",
      "ep 3417: ep_len:698 episode reward: total was 12.940000. running mean: 10.572003\n",
      "ep 3417: ep_len:621 episode reward: total was 45.250000. running mean: 10.918783\n",
      "epsilon:0.048469 episode_count: 23926. steps_count: 10346741.000000\n",
      "Time elapsed:  31119.22057366371\n",
      "ep 3418: ep_len:632 episode reward: total was 43.070000. running mean: 11.240295\n",
      "ep 3418: ep_len:603 episode reward: total was -32.480000. running mean: 10.803092\n",
      "ep 3418: ep_len:566 episode reward: total was -47.410000. running mean: 10.220961\n",
      "ep 3418: ep_len:56 episode reward: total was 3.330000. running mean: 10.152051\n",
      "ep 3418: ep_len:3 episode reward: total was 1.010000. running mean: 10.060631\n",
      "ep 3418: ep_len:502 episode reward: total was -7.350000. running mean: 9.886524\n",
      "ep 3418: ep_len:588 episode reward: total was 31.860000. running mean: 10.106259\n",
      "epsilon:0.048424 episode_count: 23933. steps_count: 10349691.000000\n",
      "Time elapsed:  31131.84703850746\n",
      "ep 3419: ep_len:195 episode reward: total was 10.200000. running mean: 10.107197\n",
      "ep 3419: ep_len:536 episode reward: total was 78.710000. running mean: 10.793225\n",
      "ep 3419: ep_len:79 episode reward: total was 8.310000. running mean: 10.768392\n",
      "ep 3419: ep_len:577 episode reward: total was 56.690000. running mean: 11.227608\n",
      "ep 3419: ep_len:3 episode reward: total was 1.010000. running mean: 11.125432\n",
      "ep 3419: ep_len:500 episode reward: total was -166.270000. running mean: 9.351478\n",
      "ep 3419: ep_len:512 episode reward: total was -1.690000. running mean: 9.241063\n",
      "epsilon:0.048380 episode_count: 23940. steps_count: 10352093.000000\n",
      "Time elapsed:  31142.501404047012\n",
      "ep 3420: ep_len:180 episode reward: total was 32.730000. running mean: 9.475953\n",
      "ep 3420: ep_len:598 episode reward: total was 22.070000. running mean: 9.601893\n",
      "ep 3420: ep_len:597 episode reward: total was -15.090000. running mean: 9.354974\n",
      "ep 3420: ep_len:524 episode reward: total was 40.670000. running mean: 9.668124\n",
      "ep 3420: ep_len:52 episode reward: total was 24.500000. running mean: 9.816443\n",
      "ep 3420: ep_len:500 episode reward: total was 26.010000. running mean: 9.978379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3420: ep_len:504 episode reward: total was -11.140000. running mean: 9.767195\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.048336 episode_count: 23947. steps_count: 10355048.000000\n",
      "Time elapsed:  31161.344076633453\n",
      "ep 3421: ep_len:229 episode reward: total was 25.300000. running mean: 9.922523\n",
      "ep 3421: ep_len:318 episode reward: total was 0.840000. running mean: 9.831698\n",
      "ep 3421: ep_len:524 episode reward: total was -28.390000. running mean: 9.449481\n",
      "ep 3421: ep_len:531 episode reward: total was 68.980000. running mean: 10.044786\n",
      "ep 3421: ep_len:3 episode reward: total was 1.010000. running mean: 9.954438\n",
      "ep 3421: ep_len:247 episode reward: total was 51.010000. running mean: 10.364994\n",
      "ep 3421: ep_len:587 episode reward: total was 49.650000. running mean: 10.757844\n",
      "epsilon:0.048291 episode_count: 23954. steps_count: 10357487.000000\n",
      "Time elapsed:  31168.07085299492\n",
      "ep 3422: ep_len:611 episode reward: total was 59.110000. running mean: 11.241365\n",
      "ep 3422: ep_len:193 episode reward: total was 2.800000. running mean: 11.156952\n",
      "ep 3422: ep_len:561 episode reward: total was -6.900000. running mean: 10.976382\n",
      "ep 3422: ep_len:500 episode reward: total was 18.130000. running mean: 11.047918\n",
      "ep 3422: ep_len:3 episode reward: total was 1.010000. running mean: 10.947539\n",
      "ep 3422: ep_len:169 episode reward: total was 28.750000. running mean: 11.125564\n",
      "ep 3422: ep_len:555 episode reward: total was 31.480000. running mean: 11.329108\n",
      "epsilon:0.048247 episode_count: 23961. steps_count: 10360079.000000\n",
      "Time elapsed:  31177.11947464943\n",
      "ep 3423: ep_len:611 episode reward: total was 95.380000. running mean: 12.169617\n",
      "ep 3423: ep_len:195 episode reward: total was -9.210000. running mean: 11.955821\n",
      "ep 3423: ep_len:512 episode reward: total was -15.330000. running mean: 11.682963\n",
      "ep 3423: ep_len:511 episode reward: total was 26.570000. running mean: 11.831833\n",
      "ep 3423: ep_len:81 episode reward: total was 21.770000. running mean: 11.931215\n",
      "ep 3423: ep_len:598 episode reward: total was 37.020000. running mean: 12.182103\n",
      "ep 3423: ep_len:500 episode reward: total was 60.250000. running mean: 12.662782\n",
      "epsilon:0.048203 episode_count: 23968. steps_count: 10363087.000000\n",
      "Time elapsed:  31189.64496588707\n",
      "ep 3424: ep_len:571 episode reward: total was 46.020000. running mean: 12.996354\n",
      "ep 3424: ep_len:622 episode reward: total was -10.670000. running mean: 12.759690\n",
      "ep 3424: ep_len:70 episode reward: total was 9.750000. running mean: 12.729593\n",
      "ep 3424: ep_len:503 episode reward: total was 39.540000. running mean: 12.997697\n",
      "ep 3424: ep_len:3 episode reward: total was 1.010000. running mean: 12.877820\n",
      "ep 3424: ep_len:500 episode reward: total was 19.070000. running mean: 12.939742\n",
      "ep 3424: ep_len:615 episode reward: total was 35.890000. running mean: 13.169245\n",
      "epsilon:0.048158 episode_count: 23975. steps_count: 10365971.000000\n",
      "Time elapsed:  31197.68712735176\n",
      "ep 3425: ep_len:125 episode reward: total was 11.560000. running mean: 13.153152\n",
      "ep 3425: ep_len:572 episode reward: total was 26.750000. running mean: 13.289121\n",
      "ep 3425: ep_len:447 episode reward: total was 44.400000. running mean: 13.600230\n",
      "ep 3425: ep_len:550 episode reward: total was 68.280000. running mean: 14.147027\n",
      "ep 3425: ep_len:3 episode reward: total was 1.010000. running mean: 14.015657\n",
      "ep 3425: ep_len:633 episode reward: total was 28.730000. running mean: 14.162800\n",
      "ep 3425: ep_len:597 episode reward: total was 14.560000. running mean: 14.166772\n",
      "epsilon:0.048114 episode_count: 23982. steps_count: 10368898.000000\n",
      "Time elapsed:  31205.507214307785\n",
      "ep 3426: ep_len:230 episode reward: total was 34.770000. running mean: 14.372805\n",
      "ep 3426: ep_len:500 episode reward: total was 1.880000. running mean: 14.247877\n",
      "ep 3426: ep_len:537 episode reward: total was -24.160000. running mean: 13.863798\n",
      "ep 3426: ep_len:500 episode reward: total was 55.640000. running mean: 14.281560\n",
      "ep 3426: ep_len:3 episode reward: total was 1.010000. running mean: 14.148844\n",
      "ep 3426: ep_len:527 episode reward: total was -32.260000. running mean: 13.684756\n",
      "ep 3426: ep_len:565 episode reward: total was 32.060000. running mean: 13.868508\n",
      "epsilon:0.048070 episode_count: 23989. steps_count: 10371760.000000\n",
      "Time elapsed:  31213.40887236595\n",
      "ep 3427: ep_len:570 episode reward: total was 72.030000. running mean: 14.450123\n",
      "ep 3427: ep_len:544 episode reward: total was 51.680000. running mean: 14.822422\n",
      "ep 3427: ep_len:588 episode reward: total was 0.080000. running mean: 14.674998\n",
      "ep 3427: ep_len:524 episode reward: total was 18.780000. running mean: 14.716048\n",
      "ep 3427: ep_len:56 episode reward: total was 25.000000. running mean: 14.818887\n",
      "ep 3427: ep_len:523 episode reward: total was -121.970000. running mean: 13.450998\n",
      "ep 3427: ep_len:587 episode reward: total was 19.140000. running mean: 13.507888\n",
      "epsilon:0.048025 episode_count: 23996. steps_count: 10375152.000000\n",
      "Time elapsed:  31222.117646694183\n",
      "ep 3428: ep_len:544 episode reward: total was 37.760000. running mean: 13.750410\n",
      "ep 3428: ep_len:565 episode reward: total was 25.200000. running mean: 13.864906\n",
      "ep 3428: ep_len:647 episode reward: total was 14.020000. running mean: 13.866456\n",
      "ep 3428: ep_len:501 episode reward: total was 9.740000. running mean: 13.825192\n",
      "ep 3428: ep_len:103 episode reward: total was 18.290000. running mean: 13.869840\n",
      "ep 3428: ep_len:518 episode reward: total was -99.440000. running mean: 12.736742\n",
      "ep 3428: ep_len:604 episode reward: total was -125.740000. running mean: 11.351974\n",
      "epsilon:0.047981 episode_count: 24003. steps_count: 10378634.000000\n",
      "Time elapsed:  31231.071828365326\n",
      "ep 3429: ep_len:588 episode reward: total was 63.510000. running mean: 11.873554\n",
      "ep 3429: ep_len:529 episode reward: total was 92.090000. running mean: 12.675719\n",
      "ep 3429: ep_len:569 episode reward: total was -34.680000. running mean: 12.202162\n",
      "ep 3429: ep_len:541 episode reward: total was 53.670000. running mean: 12.616840\n",
      "ep 3429: ep_len:133 episode reward: total was 30.390000. running mean: 12.794572\n",
      "ep 3429: ep_len:544 episode reward: total was -35.000000. running mean: 12.316626\n",
      "ep 3429: ep_len:585 episode reward: total was 49.020000. running mean: 12.683660\n",
      "epsilon:0.047937 episode_count: 24010. steps_count: 10382123.000000\n",
      "Time elapsed:  31239.844070911407\n",
      "ep 3430: ep_len:514 episode reward: total was 29.960000. running mean: 12.856423\n",
      "ep 3430: ep_len:500 episode reward: total was 58.180000. running mean: 13.309659\n",
      "ep 3430: ep_len:544 episode reward: total was -6.730000. running mean: 13.109262\n",
      "ep 3430: ep_len:511 episode reward: total was 56.670000. running mean: 13.544870\n",
      "ep 3430: ep_len:3 episode reward: total was 1.010000. running mean: 13.419521\n",
      "ep 3430: ep_len:600 episode reward: total was 23.820000. running mean: 13.523526\n",
      "ep 3430: ep_len:500 episode reward: total was 42.160000. running mean: 13.809890\n",
      "epsilon:0.047892 episode_count: 24017. steps_count: 10385295.000000\n",
      "Time elapsed:  31248.150629520416\n",
      "ep 3431: ep_len:576 episode reward: total was 66.430000. running mean: 14.336092\n",
      "ep 3431: ep_len:295 episode reward: total was -5.830000. running mean: 14.134431\n",
      "ep 3431: ep_len:582 episode reward: total was 12.230000. running mean: 14.115386\n",
      "ep 3431: ep_len:605 episode reward: total was 19.230000. running mean: 14.166532\n",
      "ep 3431: ep_len:3 episode reward: total was 1.010000. running mean: 14.034967\n",
      "ep 3431: ep_len:512 episode reward: total was -8.940000. running mean: 13.805217\n",
      "ep 3431: ep_len:617 episode reward: total was 28.950000. running mean: 13.956665\n",
      "epsilon:0.047848 episode_count: 24024. steps_count: 10388485.000000\n",
      "Time elapsed:  31256.66634464264\n",
      "ep 3432: ep_len:538 episode reward: total was 22.020000. running mean: 14.037299\n",
      "ep 3432: ep_len:266 episode reward: total was 5.360000. running mean: 13.950526\n",
      "ep 3432: ep_len:526 episode reward: total was -129.350000. running mean: 12.517520\n",
      "ep 3432: ep_len:591 episode reward: total was 51.160000. running mean: 12.903945\n",
      "ep 3432: ep_len:3 episode reward: total was 1.010000. running mean: 12.785006\n",
      "ep 3432: ep_len:514 episode reward: total was -16.360000. running mean: 12.493556\n",
      "ep 3432: ep_len:211 episode reward: total was 16.200000. running mean: 12.530620\n",
      "epsilon:0.047804 episode_count: 24031. steps_count: 10391134.000000\n",
      "Time elapsed:  31263.88617324829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3433: ep_len:209 episode reward: total was 12.790000. running mean: 12.533214\n",
      "ep 3433: ep_len:500 episode reward: total was 23.690000. running mean: 12.644782\n",
      "ep 3433: ep_len:500 episode reward: total was 56.280000. running mean: 13.081134\n",
      "ep 3433: ep_len:555 episode reward: total was -119.620000. running mean: 11.754123\n",
      "ep 3433: ep_len:108 episode reward: total was 31.260000. running mean: 11.949181\n",
      "ep 3433: ep_len:597 episode reward: total was 47.420000. running mean: 12.303890\n",
      "ep 3433: ep_len:599 episode reward: total was 31.720000. running mean: 12.498051\n",
      "epsilon:0.047759 episode_count: 24038. steps_count: 10394202.000000\n",
      "Time elapsed:  31271.97796177864\n",
      "ep 3434: ep_len:500 episode reward: total was 46.390000. running mean: 12.836970\n",
      "ep 3434: ep_len:500 episode reward: total was 70.690000. running mean: 13.415501\n",
      "ep 3434: ep_len:649 episode reward: total was -9.160000. running mean: 13.189746\n",
      "ep 3434: ep_len:123 episode reward: total was 7.560000. running mean: 13.133448\n",
      "ep 3434: ep_len:50 episode reward: total was 17.500000. running mean: 13.177114\n",
      "ep 3434: ep_len:539 episode reward: total was -10.300000. running mean: 12.942342\n",
      "ep 3434: ep_len:253 episode reward: total was 0.780000. running mean: 12.820719\n",
      "epsilon:0.047715 episode_count: 24045. steps_count: 10396816.000000\n",
      "Time elapsed:  31279.098004579544\n",
      "ep 3435: ep_len:233 episode reward: total was 11.580000. running mean: 12.808312\n",
      "ep 3435: ep_len:363 episode reward: total was 6.380000. running mean: 12.744029\n",
      "ep 3435: ep_len:564 episode reward: total was -39.520000. running mean: 12.221388\n",
      "ep 3435: ep_len:518 episode reward: total was -6.710000. running mean: 12.032075\n",
      "ep 3435: ep_len:3 episode reward: total was 1.010000. running mean: 11.921854\n",
      "ep 3435: ep_len:579 episode reward: total was 4.900000. running mean: 11.851635\n",
      "ep 3435: ep_len:563 episode reward: total was 34.010000. running mean: 12.073219\n",
      "epsilon:0.047671 episode_count: 24052. steps_count: 10399639.000000\n",
      "Time elapsed:  31292.519317388535\n",
      "ep 3436: ep_len:535 episode reward: total was -48.360000. running mean: 11.468887\n",
      "ep 3436: ep_len:286 episode reward: total was -44.160000. running mean: 10.912598\n",
      "ep 3436: ep_len:543 episode reward: total was -36.710000. running mean: 10.436372\n",
      "ep 3436: ep_len:500 episode reward: total was -0.360000. running mean: 10.328408\n",
      "ep 3436: ep_len:3 episode reward: total was 1.010000. running mean: 10.235224\n",
      "ep 3436: ep_len:622 episode reward: total was -2.720000. running mean: 10.105672\n",
      "ep 3436: ep_len:358 episode reward: total was -18.920000. running mean: 9.815415\n",
      "epsilon:0.047626 episode_count: 24059. steps_count: 10402486.000000\n",
      "Time elapsed:  31306.98281264305\n",
      "ep 3437: ep_len:535 episode reward: total was 76.660000. running mean: 10.483861\n",
      "ep 3437: ep_len:531 episode reward: total was 8.740000. running mean: 10.466422\n",
      "ep 3437: ep_len:606 episode reward: total was 20.090000. running mean: 10.562658\n",
      "ep 3437: ep_len:500 episode reward: total was 25.290000. running mean: 10.709932\n",
      "ep 3437: ep_len:3 episode reward: total was 1.010000. running mean: 10.612932\n",
      "ep 3437: ep_len:596 episode reward: total was 54.110000. running mean: 11.047903\n",
      "ep 3437: ep_len:556 episode reward: total was -30.810000. running mean: 10.629324\n",
      "epsilon:0.047582 episode_count: 24066. steps_count: 10405813.000000\n",
      "Time elapsed:  31321.129919290543\n",
      "ep 3438: ep_len:585 episode reward: total was -49.550000. running mean: 10.027531\n",
      "ep 3438: ep_len:509 episode reward: total was -8.960000. running mean: 9.837655\n",
      "ep 3438: ep_len:560 episode reward: total was 47.340000. running mean: 10.212679\n",
      "ep 3438: ep_len:546 episode reward: total was 12.030000. running mean: 10.230852\n",
      "ep 3438: ep_len:77 episode reward: total was 16.770000. running mean: 10.296243\n",
      "ep 3438: ep_len:587 episode reward: total was 37.770000. running mean: 10.570981\n",
      "ep 3438: ep_len:557 episode reward: total was -12.920000. running mean: 10.336071\n",
      "epsilon:0.047538 episode_count: 24073. steps_count: 10409234.000000\n",
      "Time elapsed:  31330.019633293152\n",
      "ep 3439: ep_len:532 episode reward: total was -35.260000. running mean: 9.880111\n",
      "ep 3439: ep_len:201 episode reward: total was -5.720000. running mean: 9.724109\n",
      "ep 3439: ep_len:544 episode reward: total was 4.360000. running mean: 9.670468\n",
      "ep 3439: ep_len:412 episode reward: total was 25.640000. running mean: 9.830164\n",
      "ep 3439: ep_len:3 episode reward: total was 1.010000. running mean: 9.741962\n",
      "ep 3439: ep_len:701 episode reward: total was 48.140000. running mean: 10.125942\n",
      "ep 3439: ep_len:500 episode reward: total was -24.760000. running mean: 9.777083\n",
      "epsilon:0.047493 episode_count: 24080. steps_count: 10412127.000000\n",
      "Time elapsed:  31343.41769838333\n",
      "ep 3440: ep_len:649 episode reward: total was -66.990000. running mean: 9.009412\n",
      "ep 3440: ep_len:515 episode reward: total was 28.610000. running mean: 9.205418\n",
      "ep 3440: ep_len:518 episode reward: total was 39.760000. running mean: 9.510964\n",
      "ep 3440: ep_len:529 episode reward: total was 59.440000. running mean: 10.010254\n",
      "ep 3440: ep_len:3 episode reward: total was 1.010000. running mean: 9.920252\n",
      "ep 3440: ep_len:500 episode reward: total was -5.420000. running mean: 9.766849\n",
      "ep 3440: ep_len:518 episode reward: total was -58.740000. running mean: 9.081781\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.047449 episode_count: 24087. steps_count: 10415359.000000\n",
      "Time elapsed:  31356.261704444885\n",
      "ep 3441: ep_len:555 episode reward: total was 73.190000. running mean: 9.722863\n",
      "ep 3441: ep_len:546 episode reward: total was 19.550000. running mean: 9.821134\n",
      "ep 3441: ep_len:558 episode reward: total was 33.390000. running mean: 10.056823\n",
      "ep 3441: ep_len:557 episode reward: total was 62.170000. running mean: 10.577955\n",
      "ep 3441: ep_len:82 episode reward: total was 1.210000. running mean: 10.484275\n",
      "ep 3441: ep_len:556 episode reward: total was 17.980000. running mean: 10.559232\n",
      "ep 3441: ep_len:580 episode reward: total was 41.820000. running mean: 10.871840\n",
      "epsilon:0.047405 episode_count: 24094. steps_count: 10418793.000000\n",
      "Time elapsed:  31365.29318666458\n",
      "ep 3442: ep_len:586 episode reward: total was 66.220000. running mean: 11.425322\n",
      "ep 3442: ep_len:502 episode reward: total was -74.760000. running mean: 10.563468\n",
      "ep 3442: ep_len:593 episode reward: total was -8.790000. running mean: 10.369934\n",
      "ep 3442: ep_len:500 episode reward: total was 43.260000. running mean: 10.698834\n",
      "ep 3442: ep_len:3 episode reward: total was 1.010000. running mean: 10.601946\n",
      "ep 3442: ep_len:568 episode reward: total was 4.910000. running mean: 10.545027\n",
      "ep 3442: ep_len:529 episode reward: total was 21.070000. running mean: 10.650276\n",
      "epsilon:0.047360 episode_count: 24101. steps_count: 10422074.000000\n",
      "Time elapsed:  31385.71656537056\n",
      "ep 3443: ep_len:632 episode reward: total was 58.680000. running mean: 11.130574\n",
      "ep 3443: ep_len:500 episode reward: total was 105.600000. running mean: 12.075268\n",
      "ep 3443: ep_len:79 episode reward: total was 11.340000. running mean: 12.067915\n",
      "ep 3443: ep_len:506 episode reward: total was 14.200000. running mean: 12.089236\n",
      "ep 3443: ep_len:3 episode reward: total was 1.010000. running mean: 11.978444\n",
      "ep 3443: ep_len:500 episode reward: total was 31.860000. running mean: 12.177259\n",
      "ep 3443: ep_len:198 episode reward: total was -15.700000. running mean: 11.898487\n",
      "epsilon:0.047316 episode_count: 24108. steps_count: 10424492.000000\n",
      "Time elapsed:  31393.13170981407\n",
      "ep 3444: ep_len:533 episode reward: total was 9.850000. running mean: 11.878002\n",
      "ep 3444: ep_len:512 episode reward: total was 16.900000. running mean: 11.928222\n",
      "ep 3444: ep_len:372 episode reward: total was 53.180000. running mean: 12.340739\n",
      "ep 3444: ep_len:527 episode reward: total was 65.020000. running mean: 12.867532\n",
      "ep 3444: ep_len:3 episode reward: total was 1.010000. running mean: 12.748957\n",
      "ep 3444: ep_len:626 episode reward: total was 5.210000. running mean: 12.673567\n",
      "ep 3444: ep_len:193 episode reward: total was -8.040000. running mean: 12.466432\n",
      "epsilon:0.047272 episode_count: 24115. steps_count: 10427258.000000\n",
      "Time elapsed:  31400.550052642822\n",
      "ep 3445: ep_len:180 episode reward: total was 14.980000. running mean: 12.491567\n",
      "ep 3445: ep_len:500 episode reward: total was -1.060000. running mean: 12.356052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3445: ep_len:610 episode reward: total was -16.690000. running mean: 12.065591\n",
      "ep 3445: ep_len:500 episode reward: total was 6.670000. running mean: 12.011635\n",
      "ep 3445: ep_len:3 episode reward: total was 1.010000. running mean: 11.901619\n",
      "ep 3445: ep_len:226 episode reward: total was 47.830000. running mean: 12.260903\n",
      "ep 3445: ep_len:593 episode reward: total was 53.460000. running mean: 12.672894\n",
      "epsilon:0.047227 episode_count: 24122. steps_count: 10429870.000000\n",
      "Time elapsed:  31413.432903766632\n",
      "ep 3446: ep_len:102 episode reward: total was 3.770000. running mean: 12.583865\n",
      "ep 3446: ep_len:532 episode reward: total was 24.520000. running mean: 12.703226\n",
      "ep 3446: ep_len:614 episode reward: total was 2.130000. running mean: 12.597494\n",
      "ep 3446: ep_len:500 episode reward: total was -7.780000. running mean: 12.393719\n",
      "ep 3446: ep_len:3 episode reward: total was 1.010000. running mean: 12.279882\n",
      "ep 3446: ep_len:509 episode reward: total was -11.940000. running mean: 12.037683\n",
      "ep 3446: ep_len:556 episode reward: total was 34.020000. running mean: 12.257506\n",
      "epsilon:0.047183 episode_count: 24129. steps_count: 10432686.000000\n",
      "Time elapsed:  31419.304165124893\n",
      "ep 3447: ep_len:500 episode reward: total was 77.400000. running mean: 12.908931\n",
      "ep 3447: ep_len:523 episode reward: total was 29.340000. running mean: 13.073242\n",
      "ep 3447: ep_len:597 episode reward: total was 14.880000. running mean: 13.091309\n",
      "ep 3447: ep_len:603 episode reward: total was 77.520000. running mean: 13.735596\n",
      "ep 3447: ep_len:3 episode reward: total was 1.010000. running mean: 13.608340\n",
      "ep 3447: ep_len:593 episode reward: total was 1.920000. running mean: 13.491457\n",
      "ep 3447: ep_len:517 episode reward: total was 10.590000. running mean: 13.462442\n",
      "epsilon:0.047139 episode_count: 24136. steps_count: 10436022.000000\n",
      "Time elapsed:  31438.807805776596\n",
      "ep 3448: ep_len:255 episode reward: total was -11.470000. running mean: 13.213118\n",
      "ep 3448: ep_len:585 episode reward: total was 43.310000. running mean: 13.514087\n",
      "ep 3448: ep_len:615 episode reward: total was -75.810000. running mean: 12.620846\n",
      "ep 3448: ep_len:511 episode reward: total was 1.960000. running mean: 12.514237\n",
      "ep 3448: ep_len:3 episode reward: total was 1.010000. running mean: 12.399195\n",
      "ep 3448: ep_len:500 episode reward: total was 17.190000. running mean: 12.447103\n",
      "ep 3448: ep_len:328 episode reward: total was 7.490000. running mean: 12.397532\n",
      "epsilon:0.047094 episode_count: 24143. steps_count: 10438819.000000\n",
      "Time elapsed:  31446.329909324646\n",
      "ep 3449: ep_len:500 episode reward: total was 93.610000. running mean: 13.209657\n",
      "ep 3449: ep_len:501 episode reward: total was 101.390000. running mean: 14.091460\n",
      "ep 3449: ep_len:582 episode reward: total was 20.640000. running mean: 14.156945\n",
      "ep 3449: ep_len:586 episode reward: total was 45.360000. running mean: 14.468976\n",
      "ep 3449: ep_len:98 episode reward: total was 26.750000. running mean: 14.591786\n",
      "ep 3449: ep_len:532 episode reward: total was 40.650000. running mean: 14.852368\n",
      "ep 3449: ep_len:189 episode reward: total was -6.180000. running mean: 14.642045\n",
      "epsilon:0.047050 episode_count: 24150. steps_count: 10441807.000000\n",
      "Time elapsed:  31457.17897605896\n",
      "ep 3450: ep_len:653 episode reward: total was -46.650000. running mean: 14.029124\n",
      "ep 3450: ep_len:500 episode reward: total was 32.150000. running mean: 14.210333\n",
      "ep 3450: ep_len:500 episode reward: total was 18.100000. running mean: 14.249230\n",
      "ep 3450: ep_len:500 episode reward: total was 30.790000. running mean: 14.414637\n",
      "ep 3450: ep_len:90 episode reward: total was 30.160000. running mean: 14.572091\n",
      "ep 3450: ep_len:542 episode reward: total was -22.890000. running mean: 14.197470\n",
      "ep 3450: ep_len:195 episode reward: total was 11.020000. running mean: 14.165695\n",
      "epsilon:0.047006 episode_count: 24157. steps_count: 10444787.000000\n",
      "Time elapsed:  31465.235979557037\n",
      "ep 3451: ep_len:112 episode reward: total was -0.020000. running mean: 14.023838\n",
      "ep 3451: ep_len:580 episode reward: total was 43.980000. running mean: 14.323400\n",
      "ep 3451: ep_len:682 episode reward: total was -92.240000. running mean: 13.257766\n",
      "ep 3451: ep_len:503 episode reward: total was 59.820000. running mean: 13.723388\n",
      "ep 3451: ep_len:3 episode reward: total was 1.010000. running mean: 13.596254\n",
      "ep 3451: ep_len:304 episode reward: total was 16.670000. running mean: 13.626992\n",
      "ep 3451: ep_len:292 episode reward: total was 19.880000. running mean: 13.689522\n",
      "epsilon:0.046961 episode_count: 24164. steps_count: 10447263.000000\n",
      "Time elapsed:  31472.03958797455\n",
      "ep 3452: ep_len:672 episode reward: total was -84.690000. running mean: 12.705727\n",
      "ep 3452: ep_len:540 episode reward: total was 23.080000. running mean: 12.809469\n",
      "ep 3452: ep_len:456 episode reward: total was 26.950000. running mean: 12.950875\n",
      "ep 3452: ep_len:103 episode reward: total was 0.080000. running mean: 12.822166\n",
      "ep 3452: ep_len:3 episode reward: total was 1.010000. running mean: 12.704044\n",
      "ep 3452: ep_len:513 episode reward: total was 13.170000. running mean: 12.708704\n",
      "ep 3452: ep_len:533 episode reward: total was 4.070000. running mean: 12.622317\n",
      "epsilon:0.046917 episode_count: 24171. steps_count: 10450083.000000\n",
      "Time elapsed:  31479.498658657074\n",
      "ep 3453: ep_len:511 episode reward: total was -71.390000. running mean: 11.782194\n",
      "ep 3453: ep_len:353 episode reward: total was 28.560000. running mean: 11.949972\n",
      "ep 3453: ep_len:625 episode reward: total was -28.260000. running mean: 11.547872\n",
      "ep 3453: ep_len:500 episode reward: total was 29.310000. running mean: 11.725493\n",
      "ep 3453: ep_len:91 episode reward: total was -16.750000. running mean: 11.440738\n",
      "ep 3453: ep_len:500 episode reward: total was 12.640000. running mean: 11.452731\n",
      "ep 3453: ep_len:503 episode reward: total was -28.690000. running mean: 11.051304\n",
      "epsilon:0.046873 episode_count: 24178. steps_count: 10453166.000000\n",
      "Time elapsed:  31487.630110502243\n",
      "ep 3454: ep_len:566 episode reward: total was 80.770000. running mean: 11.748491\n",
      "ep 3454: ep_len:505 episode reward: total was 42.370000. running mean: 12.054706\n",
      "ep 3454: ep_len:581 episode reward: total was -16.800000. running mean: 11.766159\n",
      "ep 3454: ep_len:551 episode reward: total was 67.130000. running mean: 12.319797\n",
      "ep 3454: ep_len:3 episode reward: total was 1.010000. running mean: 12.206699\n",
      "ep 3454: ep_len:500 episode reward: total was 8.610000. running mean: 12.170732\n",
      "ep 3454: ep_len:529 episode reward: total was 63.930000. running mean: 12.688325\n",
      "epsilon:0.046828 episode_count: 24185. steps_count: 10456401.000000\n",
      "Time elapsed:  31496.124329566956\n",
      "ep 3455: ep_len:500 episode reward: total was 3.690000. running mean: 12.598342\n",
      "ep 3455: ep_len:279 episode reward: total was 5.980000. running mean: 12.532158\n",
      "ep 3455: ep_len:547 episode reward: total was -66.500000. running mean: 11.741837\n",
      "ep 3455: ep_len:589 episode reward: total was 40.770000. running mean: 12.032118\n",
      "ep 3455: ep_len:3 episode reward: total was 1.010000. running mean: 11.921897\n",
      "ep 3455: ep_len:154 episode reward: total was -24.910000. running mean: 11.553578\n",
      "ep 3455: ep_len:567 episode reward: total was 14.640000. running mean: 11.584442\n",
      "epsilon:0.046784 episode_count: 24192. steps_count: 10459040.000000\n",
      "Time elapsed:  31503.284320116043\n",
      "ep 3456: ep_len:500 episode reward: total was 80.110000. running mean: 12.269698\n",
      "ep 3456: ep_len:518 episode reward: total was -2.300000. running mean: 12.124001\n",
      "ep 3456: ep_len:434 episode reward: total was 67.200000. running mean: 12.674761\n",
      "ep 3456: ep_len:508 episode reward: total was 22.360000. running mean: 12.771613\n",
      "ep 3456: ep_len:3 episode reward: total was 1.010000. running mean: 12.653997\n",
      "ep 3456: ep_len:620 episode reward: total was 22.470000. running mean: 12.752157\n",
      "ep 3456: ep_len:609 episode reward: total was 10.790000. running mean: 12.732536\n",
      "epsilon:0.046740 episode_count: 24199. steps_count: 10462232.000000\n",
      "Time elapsed:  31509.941785812378\n",
      "ep 3457: ep_len:510 episode reward: total was -30.290000. running mean: 12.302310\n",
      "ep 3457: ep_len:611 episode reward: total was 33.090000. running mean: 12.510187\n",
      "ep 3457: ep_len:648 episode reward: total was 5.210000. running mean: 12.437185\n",
      "ep 3457: ep_len:500 episode reward: total was 4.660000. running mean: 12.359413\n",
      "ep 3457: ep_len:3 episode reward: total was 1.010000. running mean: 12.245919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3457: ep_len:537 episode reward: total was -3.390000. running mean: 12.089560\n",
      "ep 3457: ep_len:500 episode reward: total was -7.860000. running mean: 11.890065\n",
      "epsilon:0.046695 episode_count: 24206. steps_count: 10465541.000000\n",
      "Time elapsed:  31517.85131764412\n",
      "ep 3458: ep_len:579 episode reward: total was 65.840000. running mean: 12.429564\n",
      "ep 3458: ep_len:588 episode reward: total was 23.200000. running mean: 12.537268\n",
      "ep 3458: ep_len:500 episode reward: total was -22.210000. running mean: 12.189796\n",
      "ep 3458: ep_len:524 episode reward: total was -28.550000. running mean: 11.782398\n",
      "ep 3458: ep_len:102 episode reward: total was 22.790000. running mean: 11.892474\n",
      "ep 3458: ep_len:614 episode reward: total was 17.860000. running mean: 11.952149\n",
      "ep 3458: ep_len:613 episode reward: total was 21.940000. running mean: 12.052027\n",
      "epsilon:0.046651 episode_count: 24213. steps_count: 10469061.000000\n",
      "Time elapsed:  31527.013266563416\n",
      "ep 3459: ep_len:219 episode reward: total was 16.140000. running mean: 12.092907\n",
      "ep 3459: ep_len:581 episode reward: total was 22.850000. running mean: 12.200478\n",
      "ep 3459: ep_len:571 episode reward: total was -19.330000. running mean: 11.885173\n",
      "ep 3459: ep_len:148 episode reward: total was 3.530000. running mean: 11.801622\n",
      "ep 3459: ep_len:108 episode reward: total was 31.750000. running mean: 12.001105\n",
      "ep 3459: ep_len:530 episode reward: total was -10.470000. running mean: 11.776394\n",
      "ep 3459: ep_len:174 episode reward: total was 2.270000. running mean: 11.681330\n",
      "epsilon:0.046607 episode_count: 24220. steps_count: 10471392.000000\n",
      "Time elapsed:  31533.501118421555\n",
      "ep 3460: ep_len:635 episode reward: total was -20.020000. running mean: 11.364317\n",
      "ep 3460: ep_len:344 episode reward: total was 42.150000. running mean: 11.672174\n",
      "ep 3460: ep_len:544 episode reward: total was -20.980000. running mean: 11.345652\n",
      "ep 3460: ep_len:551 episode reward: total was 25.100000. running mean: 11.483196\n",
      "ep 3460: ep_len:102 episode reward: total was 31.260000. running mean: 11.680964\n",
      "ep 3460: ep_len:581 episode reward: total was -100.060000. running mean: 10.563554\n",
      "ep 3460: ep_len:542 episode reward: total was 9.610000. running mean: 10.554018\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.046562 episode_count: 24227. steps_count: 10474691.000000\n",
      "Time elapsed:  31552.9206469059\n",
      "ep 3461: ep_len:501 episode reward: total was 52.370000. running mean: 10.972178\n",
      "ep 3461: ep_len:516 episode reward: total was 5.770000. running mean: 10.920156\n",
      "ep 3461: ep_len:679 episode reward: total was 6.300000. running mean: 10.873955\n",
      "ep 3461: ep_len:558 episode reward: total was 61.510000. running mean: 11.380315\n",
      "ep 3461: ep_len:3 episode reward: total was 1.010000. running mean: 11.276612\n",
      "ep 3461: ep_len:651 episode reward: total was -82.040000. running mean: 10.343446\n",
      "ep 3461: ep_len:524 episode reward: total was 27.930000. running mean: 10.519312\n",
      "epsilon:0.046518 episode_count: 24234. steps_count: 10478123.000000\n",
      "Time elapsed:  31567.276570796967\n",
      "ep 3462: ep_len:500 episode reward: total was -30.450000. running mean: 10.109619\n",
      "ep 3462: ep_len:509 episode reward: total was -68.280000. running mean: 9.325722\n",
      "ep 3462: ep_len:383 episode reward: total was 40.710000. running mean: 9.639565\n",
      "ep 3462: ep_len:401 episode reward: total was -121.880000. running mean: 8.324369\n",
      "ep 3462: ep_len:3 episode reward: total was -1.500000. running mean: 8.226126\n",
      "ep 3462: ep_len:537 episode reward: total was 16.780000. running mean: 8.311665\n",
      "ep 3462: ep_len:607 episode reward: total was 23.610000. running mean: 8.464648\n",
      "epsilon:0.046474 episode_count: 24241. steps_count: 10481063.000000\n",
      "Time elapsed:  31575.021243572235\n",
      "ep 3463: ep_len:210 episode reward: total was -22.080000. running mean: 8.159201\n",
      "ep 3463: ep_len:500 episode reward: total was -21.990000. running mean: 7.857709\n",
      "ep 3463: ep_len:500 episode reward: total was -18.340000. running mean: 7.595732\n",
      "ep 3463: ep_len:162 episode reward: total was 14.240000. running mean: 7.662175\n",
      "ep 3463: ep_len:79 episode reward: total was -24.800000. running mean: 7.337553\n",
      "ep 3463: ep_len:544 episode reward: total was -19.660000. running mean: 7.067578\n",
      "ep 3463: ep_len:515 episode reward: total was 2.740000. running mean: 7.024302\n",
      "epsilon:0.046429 episode_count: 24248. steps_count: 10483573.000000\n",
      "Time elapsed:  31581.803362607956\n",
      "ep 3464: ep_len:500 episode reward: total was -21.830000. running mean: 6.735759\n",
      "ep 3464: ep_len:525 episode reward: total was -6.290000. running mean: 6.605501\n",
      "ep 3464: ep_len:537 episode reward: total was 5.050000. running mean: 6.589946\n",
      "ep 3464: ep_len:114 episode reward: total was 8.460000. running mean: 6.608647\n",
      "ep 3464: ep_len:3 episode reward: total was 1.010000. running mean: 6.552660\n",
      "ep 3464: ep_len:518 episode reward: total was -3.710000. running mean: 6.450034\n",
      "ep 3464: ep_len:503 episode reward: total was -26.220000. running mean: 6.123333\n",
      "epsilon:0.046385 episode_count: 24255. steps_count: 10486273.000000\n",
      "Time elapsed:  31594.180911779404\n",
      "ep 3465: ep_len:500 episode reward: total was 63.920000. running mean: 6.701300\n",
      "ep 3465: ep_len:192 episode reward: total was 8.300000. running mean: 6.717287\n",
      "ep 3465: ep_len:612 episode reward: total was -6.810000. running mean: 6.582014\n",
      "ep 3465: ep_len:510 episode reward: total was 19.180000. running mean: 6.707994\n",
      "ep 3465: ep_len:3 episode reward: total was 1.010000. running mean: 6.651014\n",
      "ep 3465: ep_len:512 episode reward: total was -143.690000. running mean: 5.147604\n",
      "ep 3465: ep_len:530 episode reward: total was -69.370000. running mean: 4.402428\n",
      "epsilon:0.046341 episode_count: 24262. steps_count: 10489132.000000\n",
      "Time elapsed:  31601.83979701996\n",
      "ep 3466: ep_len:224 episode reward: total was 8.390000. running mean: 4.442304\n",
      "ep 3466: ep_len:500 episode reward: total was -34.790000. running mean: 4.049981\n",
      "ep 3466: ep_len:500 episode reward: total was -25.410000. running mean: 3.755381\n",
      "ep 3466: ep_len:516 episode reward: total was 33.570000. running mean: 4.053527\n",
      "ep 3466: ep_len:3 episode reward: total was 1.010000. running mean: 4.023092\n",
      "ep 3466: ep_len:531 episode reward: total was -20.650000. running mean: 3.776361\n",
      "ep 3466: ep_len:591 episode reward: total was -40.340000. running mean: 3.335197\n",
      "epsilon:0.046296 episode_count: 24269. steps_count: 10491997.000000\n",
      "Time elapsed:  31609.002388715744\n",
      "ep 3467: ep_len:500 episode reward: total was -149.090000. running mean: 1.810945\n",
      "ep 3467: ep_len:549 episode reward: total was 46.400000. running mean: 2.256836\n",
      "ep 3467: ep_len:452 episode reward: total was 49.720000. running mean: 2.731467\n",
      "ep 3467: ep_len:500 episode reward: total was 5.840000. running mean: 2.762553\n",
      "ep 3467: ep_len:3 episode reward: total was 1.010000. running mean: 2.745027\n",
      "ep 3467: ep_len:336 episode reward: total was 36.800000. running mean: 3.085577\n",
      "ep 3467: ep_len:612 episode reward: total was 30.380000. running mean: 3.358521\n",
      "epsilon:0.046252 episode_count: 24276. steps_count: 10494949.000000\n",
      "Time elapsed:  31619.83750104904\n",
      "ep 3468: ep_len:570 episode reward: total was 67.930000. running mean: 4.004236\n",
      "ep 3468: ep_len:500 episode reward: total was 91.440000. running mean: 4.878594\n",
      "ep 3468: ep_len:72 episode reward: total was 9.310000. running mean: 4.922908\n",
      "ep 3468: ep_len:546 episode reward: total was 41.890000. running mean: 5.292579\n",
      "ep 3468: ep_len:3 episode reward: total was 1.010000. running mean: 5.249753\n",
      "ep 3468: ep_len:561 episode reward: total was 35.320000. running mean: 5.550455\n",
      "ep 3468: ep_len:564 episode reward: total was 17.520000. running mean: 5.670151\n",
      "epsilon:0.046208 episode_count: 24283. steps_count: 10497765.000000\n",
      "Time elapsed:  31627.417792081833\n",
      "ep 3469: ep_len:500 episode reward: total was -54.080000. running mean: 5.072649\n",
      "ep 3469: ep_len:501 episode reward: total was 84.640000. running mean: 5.868323\n",
      "ep 3469: ep_len:500 episode reward: total was -2.500000. running mean: 5.784640\n",
      "ep 3469: ep_len:594 episode reward: total was 48.500000. running mean: 6.211793\n",
      "ep 3469: ep_len:3 episode reward: total was 1.010000. running mean: 6.159775\n",
      "ep 3469: ep_len:500 episode reward: total was -21.060000. running mean: 5.887577\n",
      "ep 3469: ep_len:301 episode reward: total was 20.000000. running mean: 6.028702\n",
      "epsilon:0.046163 episode_count: 24290. steps_count: 10500664.000000\n",
      "Time elapsed:  31635.042456150055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3470: ep_len:500 episode reward: total was 31.960000. running mean: 6.288015\n",
      "ep 3470: ep_len:345 episode reward: total was 9.590000. running mean: 6.321034\n",
      "ep 3470: ep_len:641 episode reward: total was -3.310000. running mean: 6.224724\n",
      "ep 3470: ep_len:600 episode reward: total was -13.420000. running mean: 6.028277\n",
      "ep 3470: ep_len:3 episode reward: total was 1.010000. running mean: 5.978094\n",
      "ep 3470: ep_len:500 episode reward: total was 52.300000. running mean: 6.441313\n",
      "ep 3470: ep_len:548 episode reward: total was -11.360000. running mean: 6.263300\n",
      "epsilon:0.046119 episode_count: 24297. steps_count: 10503801.000000\n",
      "Time elapsed:  31643.400369167328\n",
      "ep 3471: ep_len:500 episode reward: total was 42.070000. running mean: 6.621367\n",
      "ep 3471: ep_len:304 episode reward: total was -69.660000. running mean: 5.858553\n",
      "ep 3471: ep_len:391 episode reward: total was 53.040000. running mean: 6.330368\n",
      "ep 3471: ep_len:531 episode reward: total was 7.720000. running mean: 6.344264\n",
      "ep 3471: ep_len:3 episode reward: total was 1.010000. running mean: 6.290922\n",
      "ep 3471: ep_len:500 episode reward: total was 30.410000. running mean: 6.532112\n",
      "ep 3471: ep_len:500 episode reward: total was 7.320000. running mean: 6.539991\n",
      "epsilon:0.046075 episode_count: 24304. steps_count: 10506530.000000\n",
      "Time elapsed:  31650.797157287598\n",
      "ep 3472: ep_len:544 episode reward: total was 24.660000. running mean: 6.721191\n",
      "ep 3472: ep_len:595 episode reward: total was 42.830000. running mean: 7.082279\n",
      "ep 3472: ep_len:531 episode reward: total was 1.360000. running mean: 7.025057\n",
      "ep 3472: ep_len:130 episode reward: total was 10.580000. running mean: 7.060606\n",
      "ep 3472: ep_len:3 episode reward: total was 1.010000. running mean: 7.000100\n",
      "ep 3472: ep_len:539 episode reward: total was -40.810000. running mean: 6.521999\n",
      "ep 3472: ep_len:500 episode reward: total was -13.060000. running mean: 6.326179\n",
      "epsilon:0.046030 episode_count: 24311. steps_count: 10509372.000000\n",
      "Time elapsed:  31669.517681121826\n",
      "ep 3473: ep_len:500 episode reward: total was 42.070000. running mean: 6.683617\n",
      "ep 3473: ep_len:649 episode reward: total was 72.950000. running mean: 7.346281\n",
      "ep 3473: ep_len:614 episode reward: total was -3.340000. running mean: 7.239418\n",
      "ep 3473: ep_len:557 episode reward: total was 14.970000. running mean: 7.316724\n",
      "ep 3473: ep_len:3 episode reward: total was 1.010000. running mean: 7.253657\n",
      "ep 3473: ep_len:523 episode reward: total was 25.840000. running mean: 7.439520\n",
      "ep 3473: ep_len:528 episode reward: total was -16.650000. running mean: 7.198625\n",
      "epsilon:0.045986 episode_count: 24318. steps_count: 10512746.000000\n",
      "Time elapsed:  31685.199751615524\n",
      "ep 3474: ep_len:119 episode reward: total was -3.010000. running mean: 7.096539\n",
      "ep 3474: ep_len:625 episode reward: total was 16.760000. running mean: 7.193173\n",
      "ep 3474: ep_len:585 episode reward: total was -5.150000. running mean: 7.069742\n",
      "ep 3474: ep_len:525 episode reward: total was 46.660000. running mean: 7.465644\n",
      "ep 3474: ep_len:3 episode reward: total was 1.010000. running mean: 7.401088\n",
      "ep 3474: ep_len:580 episode reward: total was 49.510000. running mean: 7.822177\n",
      "ep 3474: ep_len:536 episode reward: total was -12.460000. running mean: 7.619355\n",
      "epsilon:0.045942 episode_count: 24325. steps_count: 10515719.000000\n",
      "Time elapsed:  31698.53150987625\n",
      "ep 3475: ep_len:741 episode reward: total was -150.060000. running mean: 6.042562\n",
      "ep 3475: ep_len:530 episode reward: total was 3.620000. running mean: 6.018336\n",
      "ep 3475: ep_len:593 episode reward: total was 23.540000. running mean: 6.193553\n",
      "ep 3475: ep_len:500 episode reward: total was -19.410000. running mean: 5.937517\n",
      "ep 3475: ep_len:3 episode reward: total was 1.010000. running mean: 5.888242\n",
      "ep 3475: ep_len:512 episode reward: total was 2.870000. running mean: 5.858059\n",
      "ep 3475: ep_len:325 episode reward: total was 30.020000. running mean: 6.099679\n",
      "epsilon:0.045897 episode_count: 24332. steps_count: 10518923.000000\n",
      "Time elapsed:  31706.62503385544\n",
      "ep 3476: ep_len:580 episode reward: total was 67.840000. running mean: 6.717082\n",
      "ep 3476: ep_len:621 episode reward: total was 13.460000. running mean: 6.784511\n",
      "ep 3476: ep_len:603 episode reward: total was -25.640000. running mean: 6.460266\n",
      "ep 3476: ep_len:132 episode reward: total was 7.650000. running mean: 6.472164\n",
      "ep 3476: ep_len:55 episode reward: total was 22.510000. running mean: 6.632542\n",
      "ep 3476: ep_len:583 episode reward: total was -79.690000. running mean: 5.769316\n",
      "ep 3476: ep_len:560 episode reward: total was 27.000000. running mean: 5.981623\n",
      "epsilon:0.045853 episode_count: 24339. steps_count: 10522057.000000\n",
      "Time elapsed:  31713.93981719017\n",
      "ep 3477: ep_len:532 episode reward: total was 21.320000. running mean: 6.135007\n",
      "ep 3477: ep_len:574 episode reward: total was -22.660000. running mean: 5.847057\n",
      "ep 3477: ep_len:615 episode reward: total was -35.790000. running mean: 5.430686\n",
      "ep 3477: ep_len:500 episode reward: total was 24.550000. running mean: 5.621880\n",
      "ep 3477: ep_len:3 episode reward: total was 1.010000. running mean: 5.575761\n",
      "ep 3477: ep_len:600 episode reward: total was 15.420000. running mean: 5.674203\n",
      "ep 3477: ep_len:500 episode reward: total was 4.880000. running mean: 5.666261\n",
      "epsilon:0.045809 episode_count: 24346. steps_count: 10525381.000000\n",
      "Time elapsed:  31719.014545202255\n",
      "ep 3478: ep_len:550 episode reward: total was 46.470000. running mean: 6.074299\n",
      "ep 3478: ep_len:532 episode reward: total was 59.070000. running mean: 6.604256\n",
      "ep 3478: ep_len:425 episode reward: total was 36.710000. running mean: 6.905313\n",
      "ep 3478: ep_len:500 episode reward: total was 19.150000. running mean: 7.027760\n",
      "ep 3478: ep_len:3 episode reward: total was 1.010000. running mean: 6.967582\n",
      "ep 3478: ep_len:500 episode reward: total was 18.430000. running mean: 7.082206\n",
      "ep 3478: ep_len:560 episode reward: total was 9.660000. running mean: 7.107984\n",
      "epsilon:0.045764 episode_count: 24353. steps_count: 10528451.000000\n",
      "Time elapsed:  31723.79879784584\n",
      "ep 3479: ep_len:500 episode reward: total was 51.220000. running mean: 7.549105\n",
      "ep 3479: ep_len:516 episode reward: total was 24.290000. running mean: 7.716513\n",
      "ep 3479: ep_len:538 episode reward: total was -27.200000. running mean: 7.367348\n",
      "ep 3479: ep_len:500 episode reward: total was 27.210000. running mean: 7.565775\n",
      "ep 3479: ep_len:3 episode reward: total was 1.010000. running mean: 7.500217\n",
      "ep 3479: ep_len:541 episode reward: total was -15.730000. running mean: 7.267915\n",
      "ep 3479: ep_len:260 episode reward: total was 14.810000. running mean: 7.343336\n",
      "epsilon:0.045720 episode_count: 24360. steps_count: 10531309.000000\n",
      "Time elapsed:  31730.87153983116\n",
      "ep 3480: ep_len:95 episode reward: total was 8.810000. running mean: 7.358002\n",
      "ep 3480: ep_len:510 episode reward: total was 1.850000. running mean: 7.302922\n",
      "ep 3480: ep_len:534 episode reward: total was -63.450000. running mean: 6.595393\n",
      "ep 3480: ep_len:500 episode reward: total was 26.950000. running mean: 6.798939\n",
      "ep 3480: ep_len:3 episode reward: total was 0.000000. running mean: 6.730950\n",
      "ep 3480: ep_len:684 episode reward: total was -24.050000. running mean: 6.423140\n",
      "ep 3480: ep_len:297 episode reward: total was 22.440000. running mean: 6.583309\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.045676 episode_count: 24367. steps_count: 10533932.000000\n",
      "Time elapsed:  31746.195504188538\n",
      "ep 3481: ep_len:650 episode reward: total was 26.230000. running mean: 6.779776\n",
      "ep 3481: ep_len:586 episode reward: total was -24.820000. running mean: 6.463778\n",
      "ep 3481: ep_len:538 episode reward: total was -15.120000. running mean: 6.247940\n",
      "ep 3481: ep_len:152 episode reward: total was 7.070000. running mean: 6.256161\n",
      "ep 3481: ep_len:91 episode reward: total was 22.730000. running mean: 6.420899\n",
      "ep 3481: ep_len:588 episode reward: total was 32.690000. running mean: 6.683590\n",
      "ep 3481: ep_len:593 episode reward: total was -0.590000. running mean: 6.610854\n",
      "epsilon:0.045631 episode_count: 24374. steps_count: 10537130.000000\n",
      "Time elapsed:  31754.622489213943\n",
      "ep 3482: ep_len:202 episode reward: total was -1.760000. running mean: 6.527146\n",
      "ep 3482: ep_len:518 episode reward: total was -1.420000. running mean: 6.447674\n",
      "ep 3482: ep_len:525 episode reward: total was 9.450000. running mean: 6.477698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3482: ep_len:515 episode reward: total was 32.650000. running mean: 6.739421\n",
      "ep 3482: ep_len:52 episode reward: total was 23.000000. running mean: 6.902026\n",
      "ep 3482: ep_len:634 episode reward: total was 1.130000. running mean: 6.844306\n",
      "ep 3482: ep_len:564 episode reward: total was 24.650000. running mean: 7.022363\n",
      "epsilon:0.045587 episode_count: 24381. steps_count: 10540140.000000\n",
      "Time elapsed:  31762.657116174698\n",
      "ep 3483: ep_len:648 episode reward: total was -24.910000. running mean: 6.703040\n",
      "ep 3483: ep_len:270 episode reward: total was -5.430000. running mean: 6.581709\n",
      "ep 3483: ep_len:394 episode reward: total was 59.160000. running mean: 7.107492\n",
      "ep 3483: ep_len:500 episode reward: total was 40.650000. running mean: 7.442917\n",
      "ep 3483: ep_len:3 episode reward: total was -0.490000. running mean: 7.363588\n",
      "ep 3483: ep_len:616 episode reward: total was 33.040000. running mean: 7.620352\n",
      "ep 3483: ep_len:332 episode reward: total was 5.360000. running mean: 7.597749\n",
      "epsilon:0.045543 episode_count: 24388. steps_count: 10542903.000000\n",
      "Time elapsed:  31767.18420100212\n",
      "ep 3484: ep_len:535 episode reward: total was 19.120000. running mean: 7.712971\n",
      "ep 3484: ep_len:500 episode reward: total was 41.940000. running mean: 8.055241\n",
      "ep 3484: ep_len:565 episode reward: total was -2.330000. running mean: 7.951389\n",
      "ep 3484: ep_len:164 episode reward: total was 26.730000. running mean: 8.139175\n",
      "ep 3484: ep_len:3 episode reward: total was 1.010000. running mean: 8.067883\n",
      "ep 3484: ep_len:596 episode reward: total was 19.020000. running mean: 8.177404\n",
      "ep 3484: ep_len:276 episode reward: total was 15.520000. running mean: 8.250830\n",
      "epsilon:0.045498 episode_count: 24395. steps_count: 10545542.000000\n",
      "Time elapsed:  31771.353156089783\n",
      "ep 3485: ep_len:578 episode reward: total was 71.950000. running mean: 8.887822\n",
      "ep 3485: ep_len:514 episode reward: total was 20.200000. running mean: 9.000944\n",
      "ep 3485: ep_len:667 episode reward: total was 25.460000. running mean: 9.165534\n",
      "ep 3485: ep_len:500 episode reward: total was 44.380000. running mean: 9.517679\n",
      "ep 3485: ep_len:3 episode reward: total was 1.010000. running mean: 9.432602\n",
      "ep 3485: ep_len:500 episode reward: total was -22.350000. running mean: 9.114776\n",
      "ep 3485: ep_len:551 episode reward: total was 13.130000. running mean: 9.154929\n",
      "epsilon:0.045454 episode_count: 24402. steps_count: 10548855.000000\n",
      "Time elapsed:  31777.809579610825\n",
      "ep 3486: ep_len:580 episode reward: total was -29.380000. running mean: 8.769579\n",
      "ep 3486: ep_len:579 episode reward: total was 30.000000. running mean: 8.981883\n",
      "ep 3486: ep_len:556 episode reward: total was -16.970000. running mean: 8.722365\n",
      "ep 3486: ep_len:387 episode reward: total was 13.470000. running mean: 8.769841\n",
      "ep 3486: ep_len:87 episode reward: total was 25.630000. running mean: 8.938443\n",
      "ep 3486: ep_len:500 episode reward: total was 4.990000. running mean: 8.898958\n",
      "ep 3486: ep_len:629 episode reward: total was -16.480000. running mean: 8.645169\n",
      "epsilon:0.045410 episode_count: 24409. steps_count: 10552173.000000\n",
      "Time elapsed:  31786.531651496887\n",
      "ep 3487: ep_len:566 episode reward: total was 18.020000. running mean: 8.738917\n",
      "ep 3487: ep_len:511 episode reward: total was 0.540000. running mean: 8.656928\n",
      "ep 3487: ep_len:501 episode reward: total was -12.630000. running mean: 8.444058\n",
      "ep 3487: ep_len:513 episode reward: total was -10.200000. running mean: 8.257618\n",
      "ep 3487: ep_len:96 episode reward: total was 22.230000. running mean: 8.397342\n",
      "ep 3487: ep_len:584 episode reward: total was 56.850000. running mean: 8.881868\n",
      "ep 3487: ep_len:180 episode reward: total was 22.580000. running mean: 9.018850\n",
      "epsilon:0.045365 episode_count: 24416. steps_count: 10555124.000000\n",
      "Time elapsed:  31794.388528823853\n",
      "ep 3488: ep_len:117 episode reward: total was 2.570000. running mean: 8.954361\n",
      "ep 3488: ep_len:582 episode reward: total was 44.650000. running mean: 9.311317\n",
      "ep 3488: ep_len:500 episode reward: total was 39.130000. running mean: 9.609504\n",
      "ep 3488: ep_len:501 episode reward: total was 17.330000. running mean: 9.686709\n",
      "ep 3488: ep_len:40 episode reward: total was 18.010000. running mean: 9.769942\n",
      "ep 3488: ep_len:503 episode reward: total was 29.410000. running mean: 9.966343\n",
      "ep 3488: ep_len:193 episode reward: total was 4.690000. running mean: 9.913579\n",
      "epsilon:0.045321 episode_count: 24423. steps_count: 10557560.000000\n",
      "Time elapsed:  31801.0435423851\n",
      "ep 3489: ep_len:500 episode reward: total was -20.940000. running mean: 9.605043\n",
      "ep 3489: ep_len:521 episode reward: total was 4.600000. running mean: 9.554993\n",
      "ep 3489: ep_len:661 episode reward: total was 5.410000. running mean: 9.513543\n",
      "ep 3489: ep_len:132 episode reward: total was 13.080000. running mean: 9.549208\n",
      "ep 3489: ep_len:3 episode reward: total was 1.010000. running mean: 9.463816\n",
      "ep 3489: ep_len:544 episode reward: total was 7.240000. running mean: 9.441577\n",
      "ep 3489: ep_len:295 episode reward: total was -22.510000. running mean: 9.122062\n",
      "epsilon:0.045277 episode_count: 24430. steps_count: 10560216.000000\n",
      "Time elapsed:  31813.7962808609\n",
      "ep 3490: ep_len:259 episode reward: total was -5.580000. running mean: 8.975041\n",
      "ep 3490: ep_len:586 episode reward: total was 34.750000. running mean: 9.232791\n",
      "ep 3490: ep_len:619 episode reward: total was 29.580000. running mean: 9.436263\n",
      "ep 3490: ep_len:636 episode reward: total was 57.590000. running mean: 9.917800\n",
      "ep 3490: ep_len:91 episode reward: total was 24.750000. running mean: 10.066122\n",
      "ep 3490: ep_len:514 episode reward: total was 33.750000. running mean: 10.302961\n",
      "ep 3490: ep_len:540 episode reward: total was 11.800000. running mean: 10.317931\n",
      "epsilon:0.045232 episode_count: 24437. steps_count: 10563461.000000\n",
      "Time elapsed:  31822.547477960587\n",
      "ep 3491: ep_len:500 episode reward: total was -36.060000. running mean: 9.854152\n",
      "ep 3491: ep_len:524 episode reward: total was 39.640000. running mean: 10.152010\n",
      "ep 3491: ep_len:559 episode reward: total was 35.030000. running mean: 10.400790\n",
      "ep 3491: ep_len:557 episode reward: total was -19.130000. running mean: 10.105482\n",
      "ep 3491: ep_len:3 episode reward: total was 1.010000. running mean: 10.014528\n",
      "ep 3491: ep_len:518 episode reward: total was 30.870000. running mean: 10.223082\n",
      "ep 3491: ep_len:321 episode reward: total was 4.910000. running mean: 10.169952\n",
      "epsilon:0.045188 episode_count: 24444. steps_count: 10566443.000000\n",
      "Time elapsed:  31830.450366020203\n",
      "ep 3492: ep_len:500 episode reward: total was 71.360000. running mean: 10.781852\n",
      "ep 3492: ep_len:500 episode reward: total was 0.250000. running mean: 10.676533\n",
      "ep 3492: ep_len:609 episode reward: total was 1.700000. running mean: 10.586768\n",
      "ep 3492: ep_len:56 episode reward: total was -6.140000. running mean: 10.419500\n",
      "ep 3492: ep_len:3 episode reward: total was 1.010000. running mean: 10.325405\n",
      "ep 3492: ep_len:604 episode reward: total was 12.140000. running mean: 10.343551\n",
      "ep 3492: ep_len:576 episode reward: total was 10.430000. running mean: 10.344416\n",
      "epsilon:0.045144 episode_count: 24451. steps_count: 10569291.000000\n",
      "Time elapsed:  31844.365005731583\n",
      "ep 3493: ep_len:527 episode reward: total was 44.520000. running mean: 10.686172\n",
      "ep 3493: ep_len:500 episode reward: total was -15.280000. running mean: 10.426510\n",
      "ep 3493: ep_len:626 episode reward: total was 5.560000. running mean: 10.377845\n",
      "ep 3493: ep_len:579 episode reward: total was -30.320000. running mean: 9.970866\n",
      "ep 3493: ep_len:3 episode reward: total was 1.010000. running mean: 9.881258\n",
      "ep 3493: ep_len:709 episode reward: total was 29.580000. running mean: 10.078245\n",
      "ep 3493: ep_len:528 episode reward: total was 37.090000. running mean: 10.348363\n",
      "epsilon:0.045099 episode_count: 24458. steps_count: 10572763.000000\n",
      "Time elapsed:  31853.470025777817\n",
      "ep 3494: ep_len:573 episode reward: total was -0.550000. running mean: 10.239379\n",
      "ep 3494: ep_len:500 episode reward: total was 93.240000. running mean: 11.069385\n",
      "ep 3494: ep_len:603 episode reward: total was -30.920000. running mean: 10.649492\n",
      "ep 3494: ep_len:390 episode reward: total was 31.310000. running mean: 10.856097\n",
      "ep 3494: ep_len:99 episode reward: total was 25.660000. running mean: 11.004136\n",
      "ep 3494: ep_len:583 episode reward: total was 7.540000. running mean: 10.969494\n",
      "ep 3494: ep_len:173 episode reward: total was -30.190000. running mean: 10.557899\n",
      "epsilon:0.045055 episode_count: 24465. steps_count: 10575684.000000\n",
      "Time elapsed:  31861.28039455414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3495: ep_len:728 episode reward: total was -11.020000. running mean: 10.342120\n",
      "ep 3495: ep_len:352 episode reward: total was 2.010000. running mean: 10.258799\n",
      "ep 3495: ep_len:79 episode reward: total was 6.780000. running mean: 10.224011\n",
      "ep 3495: ep_len:514 episode reward: total was 41.140000. running mean: 10.533171\n",
      "ep 3495: ep_len:3 episode reward: total was 1.010000. running mean: 10.437939\n",
      "ep 3495: ep_len:637 episode reward: total was -98.050000. running mean: 9.353060\n",
      "ep 3495: ep_len:525 episode reward: total was -2.160000. running mean: 9.237929\n",
      "epsilon:0.045011 episode_count: 24472. steps_count: 10578522.000000\n",
      "Time elapsed:  31868.811091899872\n",
      "ep 3496: ep_len:584 episode reward: total was 56.590000. running mean: 9.711450\n",
      "ep 3496: ep_len:531 episode reward: total was 7.940000. running mean: 9.693736\n",
      "ep 3496: ep_len:458 episode reward: total was 52.140000. running mean: 10.118198\n",
      "ep 3496: ep_len:152 episode reward: total was 29.570000. running mean: 10.312716\n",
      "ep 3496: ep_len:89 episode reward: total was 7.220000. running mean: 10.281789\n",
      "ep 3496: ep_len:602 episode reward: total was 35.150000. running mean: 10.530471\n",
      "ep 3496: ep_len:500 episode reward: total was 18.660000. running mean: 10.611766\n",
      "epsilon:0.044966 episode_count: 24479. steps_count: 10581438.000000\n",
      "Time elapsed:  31876.48370862007\n",
      "ep 3497: ep_len:619 episode reward: total was -84.330000. running mean: 9.662349\n",
      "ep 3497: ep_len:589 episode reward: total was -18.270000. running mean: 9.383025\n",
      "ep 3497: ep_len:541 episode reward: total was -33.630000. running mean: 8.952895\n",
      "ep 3497: ep_len:500 episode reward: total was 22.660000. running mean: 9.089966\n",
      "ep 3497: ep_len:3 episode reward: total was 1.010000. running mean: 9.009166\n",
      "ep 3497: ep_len:567 episode reward: total was -1.860000. running mean: 8.900475\n",
      "ep 3497: ep_len:500 episode reward: total was 3.740000. running mean: 8.848870\n",
      "epsilon:0.044922 episode_count: 24486. steps_count: 10584757.000000\n",
      "Time elapsed:  31885.17700266838\n",
      "ep 3498: ep_len:663 episode reward: total was 20.360000. running mean: 8.963981\n",
      "ep 3498: ep_len:570 episode reward: total was 60.150000. running mean: 9.475841\n",
      "ep 3498: ep_len:505 episode reward: total was -91.880000. running mean: 8.462283\n",
      "ep 3498: ep_len:500 episode reward: total was 25.710000. running mean: 8.634760\n",
      "ep 3498: ep_len:3 episode reward: total was 1.010000. running mean: 8.558513\n",
      "ep 3498: ep_len:521 episode reward: total was 0.610000. running mean: 8.479028\n",
      "ep 3498: ep_len:502 episode reward: total was 40.380000. running mean: 8.798037\n",
      "epsilon:0.044878 episode_count: 24493. steps_count: 10588021.000000\n",
      "Time elapsed:  31893.72476887703\n",
      "ep 3499: ep_len:535 episode reward: total was -29.600000. running mean: 8.414057\n",
      "ep 3499: ep_len:569 episode reward: total was 60.630000. running mean: 8.936216\n",
      "ep 3499: ep_len:609 episode reward: total was -198.880000. running mean: 6.858054\n",
      "ep 3499: ep_len:425 episode reward: total was 1.620000. running mean: 6.805674\n",
      "ep 3499: ep_len:3 episode reward: total was 1.010000. running mean: 6.747717\n",
      "ep 3499: ep_len:504 episode reward: total was 35.500000. running mean: 7.035240\n",
      "ep 3499: ep_len:323 episode reward: total was 11.850000. running mean: 7.083387\n",
      "epsilon:0.044833 episode_count: 24500. steps_count: 10590989.000000\n",
      "Time elapsed:  31901.753551244736\n",
      "ep 3500: ep_len:593 episode reward: total was 68.860000. running mean: 7.701153\n",
      "ep 3500: ep_len:533 episode reward: total was 68.000000. running mean: 8.304142\n",
      "ep 3500: ep_len:575 episode reward: total was -29.500000. running mean: 7.926100\n",
      "ep 3500: ep_len:381 episode reward: total was 37.820000. running mean: 8.225039\n",
      "ep 3500: ep_len:3 episode reward: total was 1.010000. running mean: 8.152889\n",
      "ep 3500: ep_len:614 episode reward: total was 44.500000. running mean: 8.516360\n",
      "ep 3500: ep_len:617 episode reward: total was 12.880000. running mean: 8.559997\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.044789 episode_count: 24507. steps_count: 10594305.000000\n",
      "Time elapsed:  31915.507138490677\n",
      "ep 3501: ep_len:212 episode reward: total was 6.270000. running mean: 8.537097\n",
      "ep 3501: ep_len:526 episode reward: total was -9.100000. running mean: 8.360726\n",
      "ep 3501: ep_len:564 episode reward: total was -23.610000. running mean: 8.041018\n",
      "ep 3501: ep_len:500 episode reward: total was 46.080000. running mean: 8.421408\n",
      "ep 3501: ep_len:3 episode reward: total was 1.010000. running mean: 8.347294\n",
      "ep 3501: ep_len:568 episode reward: total was -4.300000. running mean: 8.220821\n",
      "ep 3501: ep_len:305 episode reward: total was 6.580000. running mean: 8.204413\n",
      "epsilon:0.044745 episode_count: 24514. steps_count: 10596983.000000\n",
      "Time elapsed:  31922.940505981445\n",
      "ep 3502: ep_len:580 episode reward: total was -26.230000. running mean: 7.860069\n",
      "ep 3502: ep_len:500 episode reward: total was 13.570000. running mean: 7.917168\n",
      "ep 3502: ep_len:409 episode reward: total was 36.000000. running mean: 8.197996\n",
      "ep 3502: ep_len:500 episode reward: total was 35.910000. running mean: 8.475117\n",
      "ep 3502: ep_len:3 episode reward: total was 1.010000. running mean: 8.400465\n",
      "ep 3502: ep_len:261 episode reward: total was 53.140000. running mean: 8.847861\n",
      "ep 3502: ep_len:500 episode reward: total was -76.630000. running mean: 7.993082\n",
      "epsilon:0.044700 episode_count: 24521. steps_count: 10599736.000000\n",
      "Time elapsed:  31930.062669038773\n",
      "ep 3503: ep_len:93 episode reward: total was 3.460000. running mean: 7.947751\n",
      "ep 3503: ep_len:597 episode reward: total was 26.930000. running mean: 8.137574\n",
      "ep 3503: ep_len:588 episode reward: total was 11.180000. running mean: 8.167998\n",
      "ep 3503: ep_len:500 episode reward: total was 22.370000. running mean: 8.310018\n",
      "ep 3503: ep_len:3 episode reward: total was 1.010000. running mean: 8.237018\n",
      "ep 3503: ep_len:540 episode reward: total was 12.260000. running mean: 8.277248\n",
      "ep 3503: ep_len:500 episode reward: total was 12.210000. running mean: 8.316575\n",
      "epsilon:0.044656 episode_count: 24528. steps_count: 10602557.000000\n",
      "Time elapsed:  31937.50462079048\n",
      "ep 3504: ep_len:587 episode reward: total was 41.620000. running mean: 8.649609\n",
      "ep 3504: ep_len:500 episode reward: total was 41.170000. running mean: 8.974813\n",
      "ep 3504: ep_len:621 episode reward: total was 6.930000. running mean: 8.954365\n",
      "ep 3504: ep_len:547 episode reward: total was 55.260000. running mean: 9.417422\n",
      "ep 3504: ep_len:3 episode reward: total was 1.010000. running mean: 9.333347\n",
      "ep 3504: ep_len:580 episode reward: total was 57.400000. running mean: 9.814014\n",
      "ep 3504: ep_len:542 episode reward: total was -1.530000. running mean: 9.700574\n",
      "epsilon:0.044612 episode_count: 24535. steps_count: 10605937.000000\n",
      "Time elapsed:  31952.141177415848\n",
      "ep 3505: ep_len:516 episode reward: total was -47.420000. running mean: 9.129368\n",
      "ep 3505: ep_len:516 episode reward: total was 92.990000. running mean: 9.967974\n",
      "ep 3505: ep_len:679 episode reward: total was -103.420000. running mean: 8.834095\n",
      "ep 3505: ep_len:510 episode reward: total was 53.840000. running mean: 9.284154\n",
      "ep 3505: ep_len:3 episode reward: total was 1.010000. running mean: 9.201412\n",
      "ep 3505: ep_len:620 episode reward: total was 43.490000. running mean: 9.544298\n",
      "ep 3505: ep_len:501 episode reward: total was 16.020000. running mean: 9.609055\n",
      "epsilon:0.044567 episode_count: 24542. steps_count: 10609282.000000\n",
      "Time elapsed:  31965.15573167801\n",
      "ep 3506: ep_len:589 episode reward: total was 72.270000. running mean: 10.235664\n",
      "ep 3506: ep_len:624 episode reward: total was -130.330000. running mean: 8.830008\n",
      "ep 3506: ep_len:503 episode reward: total was -3.200000. running mean: 8.709708\n",
      "ep 3506: ep_len:500 episode reward: total was 51.090000. running mean: 9.133511\n",
      "ep 3506: ep_len:3 episode reward: total was 1.010000. running mean: 9.052276\n",
      "ep 3506: ep_len:653 episode reward: total was 13.070000. running mean: 9.092453\n",
      "ep 3506: ep_len:538 episode reward: total was 15.050000. running mean: 9.152028\n",
      "epsilon:0.044523 episode_count: 24549. steps_count: 10612692.000000\n",
      "Time elapsed:  31978.01583123207\n",
      "ep 3507: ep_len:541 episode reward: total was -26.450000. running mean: 8.796008\n",
      "ep 3507: ep_len:575 episode reward: total was 34.150000. running mean: 9.049548\n",
      "ep 3507: ep_len:500 episode reward: total was 32.980000. running mean: 9.288852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3507: ep_len:500 episode reward: total was -12.270000. running mean: 9.073264\n",
      "ep 3507: ep_len:3 episode reward: total was 1.010000. running mean: 8.992631\n",
      "ep 3507: ep_len:509 episode reward: total was 13.360000. running mean: 9.036305\n",
      "ep 3507: ep_len:326 episode reward: total was -37.400000. running mean: 8.571942\n",
      "epsilon:0.044479 episode_count: 24556. steps_count: 10615646.000000\n",
      "Time elapsed:  31985.97752571106\n",
      "ep 3508: ep_len:500 episode reward: total was 43.330000. running mean: 8.919522\n",
      "ep 3508: ep_len:500 episode reward: total was 34.760000. running mean: 9.177927\n",
      "ep 3508: ep_len:606 episode reward: total was -25.190000. running mean: 8.834248\n",
      "ep 3508: ep_len:626 episode reward: total was 92.140000. running mean: 9.667305\n",
      "ep 3508: ep_len:88 episode reward: total was 18.720000. running mean: 9.757832\n",
      "ep 3508: ep_len:594 episode reward: total was 11.880000. running mean: 9.779054\n",
      "ep 3508: ep_len:565 episode reward: total was 20.190000. running mean: 9.883164\n",
      "epsilon:0.044434 episode_count: 24563. steps_count: 10619125.000000\n",
      "Time elapsed:  32001.306042671204\n",
      "ep 3509: ep_len:552 episode reward: total was 38.110000. running mean: 10.165432\n",
      "ep 3509: ep_len:500 episode reward: total was 75.150000. running mean: 10.815278\n",
      "ep 3509: ep_len:562 episode reward: total was -27.750000. running mean: 10.429625\n",
      "ep 3509: ep_len:526 episode reward: total was 49.450000. running mean: 10.819829\n",
      "ep 3509: ep_len:75 episode reward: total was 19.720000. running mean: 10.908830\n",
      "ep 3509: ep_len:500 episode reward: total was 16.140000. running mean: 10.961142\n",
      "ep 3509: ep_len:606 episode reward: total was -40.800000. running mean: 10.443531\n",
      "epsilon:0.044390 episode_count: 24570. steps_count: 10622446.000000\n",
      "Time elapsed:  32015.73716068268\n",
      "ep 3510: ep_len:194 episode reward: total was 18.640000. running mean: 10.525495\n",
      "ep 3510: ep_len:186 episode reward: total was -7.250000. running mean: 10.347740\n",
      "ep 3510: ep_len:641 episode reward: total was -1.900000. running mean: 10.225263\n",
      "ep 3510: ep_len:593 episode reward: total was -70.200000. running mean: 9.421010\n",
      "ep 3510: ep_len:1 episode reward: total was -1.000000. running mean: 9.316800\n",
      "ep 3510: ep_len:597 episode reward: total was -118.090000. running mean: 8.042732\n",
      "ep 3510: ep_len:282 episode reward: total was 27.370000. running mean: 8.236005\n",
      "epsilon:0.044346 episode_count: 24577. steps_count: 10624940.000000\n",
      "Time elapsed:  32022.497431755066\n",
      "ep 3511: ep_len:561 episode reward: total was -25.310000. running mean: 7.900545\n",
      "ep 3511: ep_len:567 episode reward: total was -119.580000. running mean: 6.625739\n",
      "ep 3511: ep_len:394 episode reward: total was 61.030000. running mean: 7.169782\n",
      "ep 3511: ep_len:572 episode reward: total was 52.990000. running mean: 7.627984\n",
      "ep 3511: ep_len:3 episode reward: total was 1.010000. running mean: 7.561804\n",
      "ep 3511: ep_len:536 episode reward: total was -15.640000. running mean: 7.329786\n",
      "ep 3511: ep_len:529 episode reward: total was 29.230000. running mean: 7.548788\n",
      "epsilon:0.044301 episode_count: 24584. steps_count: 10628102.000000\n",
      "Time elapsed:  32030.831424474716\n",
      "ep 3512: ep_len:510 episode reward: total was -32.570000. running mean: 7.147601\n",
      "ep 3512: ep_len:500 episode reward: total was 51.210000. running mean: 7.588225\n",
      "ep 3512: ep_len:586 episode reward: total was 21.170000. running mean: 7.724042\n",
      "ep 3512: ep_len:56 episode reward: total was 3.820000. running mean: 7.685002\n",
      "ep 3512: ep_len:3 episode reward: total was 1.010000. running mean: 7.618252\n",
      "ep 3512: ep_len:500 episode reward: total was 37.130000. running mean: 7.913369\n",
      "ep 3512: ep_len:192 episode reward: total was -8.630000. running mean: 7.747936\n",
      "epsilon:0.044257 episode_count: 24591. steps_count: 10630449.000000\n",
      "Time elapsed:  32037.36189866066\n",
      "ep 3513: ep_len:126 episode reward: total was 7.900000. running mean: 7.749456\n",
      "ep 3513: ep_len:500 episode reward: total was 106.520000. running mean: 8.737162\n",
      "ep 3513: ep_len:500 episode reward: total was -9.300000. running mean: 8.556790\n",
      "ep 3513: ep_len:504 episode reward: total was 32.020000. running mean: 8.791422\n",
      "ep 3513: ep_len:3 episode reward: total was 0.000000. running mean: 8.703508\n",
      "ep 3513: ep_len:326 episode reward: total was 23.660000. running mean: 8.853073\n",
      "ep 3513: ep_len:309 episode reward: total was -0.840000. running mean: 8.756142\n",
      "epsilon:0.044213 episode_count: 24598. steps_count: 10632717.000000\n",
      "Time elapsed:  32046.8573410511\n",
      "ep 3514: ep_len:527 episode reward: total was -47.230000. running mean: 8.196281\n",
      "ep 3514: ep_len:506 episode reward: total was 48.260000. running mean: 8.596918\n",
      "ep 3514: ep_len:552 episode reward: total was 13.170000. running mean: 8.642649\n",
      "ep 3514: ep_len:511 episode reward: total was 45.870000. running mean: 9.014922\n",
      "ep 3514: ep_len:3 episode reward: total was 1.010000. running mean: 8.934873\n",
      "ep 3514: ep_len:619 episode reward: total was 30.630000. running mean: 9.151824\n",
      "ep 3514: ep_len:562 episode reward: total was -0.960000. running mean: 9.050706\n",
      "epsilon:0.044168 episode_count: 24605. steps_count: 10635997.000000\n",
      "Time elapsed:  32066.90965819359\n",
      "ep 3515: ep_len:514 episode reward: total was 4.760000. running mean: 9.007799\n",
      "ep 3515: ep_len:630 episode reward: total was 142.940000. running mean: 10.347121\n",
      "ep 3515: ep_len:457 episode reward: total was 58.220000. running mean: 10.825850\n",
      "ep 3515: ep_len:500 episode reward: total was 24.290000. running mean: 10.960491\n",
      "ep 3515: ep_len:36 episode reward: total was 13.500000. running mean: 10.985886\n",
      "ep 3515: ep_len:535 episode reward: total was 27.050000. running mean: 11.146528\n",
      "ep 3515: ep_len:531 episode reward: total was -46.510000. running mean: 10.569962\n",
      "epsilon:0.044124 episode_count: 24612. steps_count: 10639200.000000\n",
      "Time elapsed:  32075.729831695557\n",
      "ep 3516: ep_len:608 episode reward: total was 27.120000. running mean: 10.735463\n",
      "ep 3516: ep_len:545 episode reward: total was -2.970000. running mean: 10.598408\n",
      "ep 3516: ep_len:436 episode reward: total was -3.340000. running mean: 10.459024\n",
      "ep 3516: ep_len:165 episode reward: total was 25.690000. running mean: 10.611334\n",
      "ep 3516: ep_len:108 episode reward: total was -56.760000. running mean: 9.937620\n",
      "ep 3516: ep_len:500 episode reward: total was 36.900000. running mean: 10.207244\n",
      "ep 3516: ep_len:604 episode reward: total was -0.190000. running mean: 10.103272\n",
      "epsilon:0.044080 episode_count: 24619. steps_count: 10642166.000000\n",
      "Time elapsed:  32083.598319530487\n",
      "ep 3517: ep_len:684 episode reward: total was -19.130000. running mean: 9.810939\n",
      "ep 3517: ep_len:500 episode reward: total was 88.430000. running mean: 10.597130\n",
      "ep 3517: ep_len:568 episode reward: total was 28.000000. running mean: 10.771158\n",
      "ep 3517: ep_len:46 episode reward: total was 0.750000. running mean: 10.670947\n",
      "ep 3517: ep_len:51 episode reward: total was 23.510000. running mean: 10.799337\n",
      "ep 3517: ep_len:583 episode reward: total was 52.600000. running mean: 11.217344\n",
      "ep 3517: ep_len:500 episode reward: total was -3.590000. running mean: 11.069270\n",
      "epsilon:0.044035 episode_count: 24626. steps_count: 10645098.000000\n",
      "Time elapsed:  32092.105503082275\n",
      "ep 3518: ep_len:507 episode reward: total was 30.920000. running mean: 11.267778\n",
      "ep 3518: ep_len:500 episode reward: total was -5.010000. running mean: 11.105000\n",
      "ep 3518: ep_len:500 episode reward: total was 3.470000. running mean: 11.028650\n",
      "ep 3518: ep_len:500 episode reward: total was 26.750000. running mean: 11.185863\n",
      "ep 3518: ep_len:3 episode reward: total was 1.010000. running mean: 11.084105\n",
      "ep 3518: ep_len:500 episode reward: total was 29.310000. running mean: 11.266364\n",
      "ep 3518: ep_len:500 episode reward: total was 54.890000. running mean: 11.702600\n",
      "epsilon:0.043991 episode_count: 24633. steps_count: 10648108.000000\n",
      "Time elapsed:  32099.672741889954\n",
      "ep 3519: ep_len:501 episode reward: total was -2.560000. running mean: 11.559974\n",
      "ep 3519: ep_len:500 episode reward: total was -0.190000. running mean: 11.442474\n",
      "ep 3519: ep_len:75 episode reward: total was 5.760000. running mean: 11.385650\n",
      "ep 3519: ep_len:114 episode reward: total was 6.100000. running mean: 11.332793\n",
      "ep 3519: ep_len:3 episode reward: total was 1.010000. running mean: 11.229565\n",
      "ep 3519: ep_len:526 episode reward: total was 26.240000. running mean: 11.379670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3519: ep_len:593 episode reward: total was 41.090000. running mean: 11.676773\n",
      "epsilon:0.043947 episode_count: 24640. steps_count: 10650420.000000\n",
      "Time elapsed:  32103.57039666176\n",
      "ep 3520: ep_len:500 episode reward: total was 36.680000. running mean: 11.926805\n",
      "ep 3520: ep_len:501 episode reward: total was 56.180000. running mean: 12.369337\n",
      "ep 3520: ep_len:321 episode reward: total was 34.580000. running mean: 12.591444\n",
      "ep 3520: ep_len:502 episode reward: total was -11.040000. running mean: 12.355129\n",
      "ep 3520: ep_len:3 episode reward: total was 1.010000. running mean: 12.241678\n",
      "ep 3520: ep_len:633 episode reward: total was -21.160000. running mean: 11.907661\n",
      "ep 3520: ep_len:288 episode reward: total was 16.410000. running mean: 11.952685\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.043902 episode_count: 24647. steps_count: 10653168.000000\n",
      "Time elapsed:  32115.745356559753\n",
      "ep 3521: ep_len:580 episode reward: total was 52.380000. running mean: 12.356958\n",
      "ep 3521: ep_len:500 episode reward: total was 64.100000. running mean: 12.874388\n",
      "ep 3521: ep_len:562 episode reward: total was 34.230000. running mean: 13.087944\n",
      "ep 3521: ep_len:583 episode reward: total was 74.770000. running mean: 13.704765\n",
      "ep 3521: ep_len:3 episode reward: total was 1.010000. running mean: 13.577817\n",
      "ep 3521: ep_len:500 episode reward: total was -38.490000. running mean: 13.057139\n",
      "ep 3521: ep_len:632 episode reward: total was 13.320000. running mean: 13.059768\n",
      "epsilon:0.043858 episode_count: 24654. steps_count: 10656528.000000\n",
      "Time elapsed:  32124.53063440323\n",
      "ep 3522: ep_len:237 episode reward: total was 31.790000. running mean: 13.247070\n",
      "ep 3522: ep_len:500 episode reward: total was 64.980000. running mean: 13.764399\n",
      "ep 3522: ep_len:555 episode reward: total was 35.240000. running mean: 13.979155\n",
      "ep 3522: ep_len:517 episode reward: total was 41.840000. running mean: 14.257764\n",
      "ep 3522: ep_len:112 episode reward: total was 29.740000. running mean: 14.412586\n",
      "ep 3522: ep_len:520 episode reward: total was 45.340000. running mean: 14.721860\n",
      "ep 3522: ep_len:617 episode reward: total was 33.370000. running mean: 14.908342\n",
      "epsilon:0.043814 episode_count: 24661. steps_count: 10659586.000000\n",
      "Time elapsed:  32132.49755883217\n",
      "ep 3523: ep_len:588 episode reward: total was 8.510000. running mean: 14.844358\n",
      "ep 3523: ep_len:588 episode reward: total was 115.700000. running mean: 15.852915\n",
      "ep 3523: ep_len:537 episode reward: total was -1.070000. running mean: 15.683685\n",
      "ep 3523: ep_len:106 episode reward: total was 0.020000. running mean: 15.527049\n",
      "ep 3523: ep_len:3 episode reward: total was 1.010000. running mean: 15.381878\n",
      "ep 3523: ep_len:525 episode reward: total was -20.310000. running mean: 15.024959\n",
      "ep 3523: ep_len:564 episode reward: total was 32.180000. running mean: 15.196510\n",
      "epsilon:0.043769 episode_count: 24668. steps_count: 10662497.000000\n",
      "Time elapsed:  32140.159562826157\n",
      "ep 3524: ep_len:500 episode reward: total was 61.510000. running mean: 15.659645\n",
      "ep 3524: ep_len:501 episode reward: total was 89.410000. running mean: 16.397148\n",
      "ep 3524: ep_len:522 episode reward: total was -34.340000. running mean: 15.889777\n",
      "ep 3524: ep_len:584 episode reward: total was 45.440000. running mean: 16.185279\n",
      "ep 3524: ep_len:3 episode reward: total was 1.010000. running mean: 16.033526\n",
      "ep 3524: ep_len:500 episode reward: total was 17.230000. running mean: 16.045491\n",
      "ep 3524: ep_len:541 episode reward: total was 23.090000. running mean: 16.115936\n",
      "epsilon:0.043725 episode_count: 24675. steps_count: 10665648.000000\n",
      "Time elapsed:  32148.208993673325\n",
      "ep 3525: ep_len:514 episode reward: total was -13.880000. running mean: 15.815977\n",
      "ep 3525: ep_len:500 episode reward: total was -23.180000. running mean: 15.426017\n",
      "ep 3525: ep_len:588 episode reward: total was 10.170000. running mean: 15.373457\n",
      "ep 3525: ep_len:502 episode reward: total was 62.280000. running mean: 15.842522\n",
      "ep 3525: ep_len:93 episode reward: total was 25.750000. running mean: 15.941597\n",
      "ep 3525: ep_len:500 episode reward: total was 22.520000. running mean: 16.007381\n",
      "ep 3525: ep_len:577 episode reward: total was 38.600000. running mean: 16.233307\n",
      "epsilon:0.043681 episode_count: 24682. steps_count: 10668922.000000\n",
      "Time elapsed:  32166.69656443596\n",
      "ep 3526: ep_len:500 episode reward: total was -36.550000. running mean: 15.705474\n",
      "ep 3526: ep_len:539 episode reward: total was 88.610000. running mean: 16.434519\n",
      "ep 3526: ep_len:438 episode reward: total was 53.340000. running mean: 16.803574\n",
      "ep 3526: ep_len:500 episode reward: total was 27.570000. running mean: 16.911238\n",
      "ep 3526: ep_len:40 episode reward: total was 18.010000. running mean: 16.922226\n",
      "ep 3526: ep_len:630 episode reward: total was 8.440000. running mean: 16.837404\n",
      "ep 3526: ep_len:589 episode reward: total was 52.720000. running mean: 17.196230\n",
      "epsilon:0.043636 episode_count: 24689. steps_count: 10672158.000000\n",
      "Time elapsed:  32173.963953495026\n",
      "ep 3527: ep_len:207 episode reward: total was 2.270000. running mean: 17.046967\n",
      "ep 3527: ep_len:572 episode reward: total was 36.290000. running mean: 17.239398\n",
      "ep 3527: ep_len:549 episode reward: total was -14.920000. running mean: 16.917804\n",
      "ep 3527: ep_len:582 episode reward: total was -8.180000. running mean: 16.666826\n",
      "ep 3527: ep_len:3 episode reward: total was 1.010000. running mean: 16.510257\n",
      "ep 3527: ep_len:610 episode reward: total was 16.240000. running mean: 16.507555\n",
      "ep 3527: ep_len:500 episode reward: total was 43.810000. running mean: 16.780579\n",
      "epsilon:0.043592 episode_count: 24696. steps_count: 10675181.000000\n",
      "Time elapsed:  32180.144627571106\n",
      "ep 3528: ep_len:506 episode reward: total was 53.910000. running mean: 17.151874\n",
      "ep 3528: ep_len:587 episode reward: total was 41.360000. running mean: 17.393955\n",
      "ep 3528: ep_len:500 episode reward: total was 16.320000. running mean: 17.383215\n",
      "ep 3528: ep_len:553 episode reward: total was 52.760000. running mean: 17.736983\n",
      "ep 3528: ep_len:121 episode reward: total was 29.830000. running mean: 17.857913\n",
      "ep 3528: ep_len:519 episode reward: total was 17.750000. running mean: 17.856834\n",
      "ep 3528: ep_len:500 episode reward: total was 3.440000. running mean: 17.712666\n",
      "epsilon:0.043548 episode_count: 24703. steps_count: 10678467.000000\n",
      "Time elapsed:  32189.81542778015\n",
      "ep 3529: ep_len:229 episode reward: total was 11.310000. running mean: 17.648639\n",
      "ep 3529: ep_len:549 episode reward: total was 18.380000. running mean: 17.655953\n",
      "ep 3529: ep_len:500 episode reward: total was -28.560000. running mean: 17.193793\n",
      "ep 3529: ep_len:601 episode reward: total was 64.740000. running mean: 17.669255\n",
      "ep 3529: ep_len:3 episode reward: total was 1.010000. running mean: 17.502663\n",
      "ep 3529: ep_len:500 episode reward: total was 22.370000. running mean: 17.551336\n",
      "ep 3529: ep_len:578 episode reward: total was 3.930000. running mean: 17.415123\n",
      "epsilon:0.043503 episode_count: 24710. steps_count: 10681427.000000\n",
      "Time elapsed:  32197.70094704628\n",
      "ep 3530: ep_len:677 episode reward: total was -46.010000. running mean: 16.780872\n",
      "ep 3530: ep_len:177 episode reward: total was -4.280000. running mean: 16.570263\n",
      "ep 3530: ep_len:551 episode reward: total was 0.090000. running mean: 16.405460\n",
      "ep 3530: ep_len:500 episode reward: total was 53.870000. running mean: 16.780106\n",
      "ep 3530: ep_len:3 episode reward: total was 1.010000. running mean: 16.622405\n",
      "ep 3530: ep_len:673 episode reward: total was 31.300000. running mean: 16.769180\n",
      "ep 3530: ep_len:526 episode reward: total was 25.580000. running mean: 16.857289\n",
      "epsilon:0.043459 episode_count: 24717. steps_count: 10684534.000000\n",
      "Time elapsed:  32208.011704921722\n",
      "ep 3531: ep_len:654 episode reward: total was -44.990000. running mean: 16.238816\n",
      "ep 3531: ep_len:599 episode reward: total was -19.170000. running mean: 15.884728\n",
      "ep 3531: ep_len:500 episode reward: total was 8.230000. running mean: 15.808180\n",
      "ep 3531: ep_len:571 episode reward: total was 59.830000. running mean: 16.248399\n",
      "ep 3531: ep_len:99 episode reward: total was -54.710000. running mean: 15.538815\n",
      "ep 3531: ep_len:500 episode reward: total was 22.660000. running mean: 15.610026\n",
      "ep 3531: ep_len:523 episode reward: total was -16.140000. running mean: 15.292526\n",
      "epsilon:0.043415 episode_count: 24724. steps_count: 10687980.000000\n",
      "Time elapsed:  32216.99438738823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3532: ep_len:613 episode reward: total was 8.520000. running mean: 15.224801\n",
      "ep 3532: ep_len:500 episode reward: total was 74.630000. running mean: 15.818853\n",
      "ep 3532: ep_len:500 episode reward: total was 0.980000. running mean: 15.670464\n",
      "ep 3532: ep_len:500 episode reward: total was -1.750000. running mean: 15.496260\n",
      "ep 3532: ep_len:81 episode reward: total was -32.730000. running mean: 15.013997\n",
      "ep 3532: ep_len:592 episode reward: total was 48.100000. running mean: 15.344857\n",
      "ep 3532: ep_len:338 episode reward: total was 16.830000. running mean: 15.359709\n",
      "epsilon:0.043370 episode_count: 24731. steps_count: 10691104.000000\n",
      "Time elapsed:  32225.369214773178\n",
      "ep 3533: ep_len:582 episode reward: total was 83.470000. running mean: 16.040811\n",
      "ep 3533: ep_len:500 episode reward: total was -92.490000. running mean: 14.955503\n",
      "ep 3533: ep_len:633 episode reward: total was 16.940000. running mean: 14.975348\n",
      "ep 3533: ep_len:500 episode reward: total was 45.670000. running mean: 15.282295\n",
      "ep 3533: ep_len:92 episode reward: total was 22.740000. running mean: 15.356872\n",
      "ep 3533: ep_len:500 episode reward: total was 29.910000. running mean: 15.502403\n",
      "ep 3533: ep_len:290 episode reward: total was -0.060000. running mean: 15.346779\n",
      "epsilon:0.043326 episode_count: 24738. steps_count: 10694201.000000\n",
      "Time elapsed:  32233.65053510666\n",
      "ep 3534: ep_len:500 episode reward: total was 81.060000. running mean: 16.003911\n",
      "ep 3534: ep_len:533 episode reward: total was 72.650000. running mean: 16.570372\n",
      "ep 3534: ep_len:525 episode reward: total was -18.660000. running mean: 16.218069\n",
      "ep 3534: ep_len:136 episode reward: total was 25.440000. running mean: 16.310288\n",
      "ep 3534: ep_len:3 episode reward: total was 0.000000. running mean: 16.147185\n",
      "ep 3534: ep_len:546 episode reward: total was 16.800000. running mean: 16.153713\n",
      "ep 3534: ep_len:582 episode reward: total was 37.770000. running mean: 16.369876\n",
      "epsilon:0.043282 episode_count: 24745. steps_count: 10697026.000000\n",
      "Time elapsed:  32241.285338163376\n",
      "ep 3535: ep_len:632 episode reward: total was 35.570000. running mean: 16.561877\n",
      "ep 3535: ep_len:500 episode reward: total was -102.220000. running mean: 15.374058\n",
      "ep 3535: ep_len:562 episode reward: total was -10.070000. running mean: 15.119618\n",
      "ep 3535: ep_len:583 episode reward: total was 58.680000. running mean: 15.555222\n",
      "ep 3535: ep_len:54 episode reward: total was 25.010000. running mean: 15.649769\n",
      "ep 3535: ep_len:524 episode reward: total was 53.570000. running mean: 16.028972\n",
      "ep 3535: ep_len:524 episode reward: total was 13.440000. running mean: 16.003082\n",
      "epsilon:0.043237 episode_count: 24752. steps_count: 10700405.000000\n",
      "Time elapsed:  32250.614436626434\n",
      "ep 3536: ep_len:510 episode reward: total was 6.490000. running mean: 15.907951\n",
      "ep 3536: ep_len:192 episode reward: total was -32.750000. running mean: 15.421372\n",
      "ep 3536: ep_len:500 episode reward: total was 13.070000. running mean: 15.397858\n",
      "ep 3536: ep_len:586 episode reward: total was -114.050000. running mean: 14.103379\n",
      "ep 3536: ep_len:3 episode reward: total was 1.010000. running mean: 13.972446\n",
      "ep 3536: ep_len:610 episode reward: total was 24.740000. running mean: 14.080121\n",
      "ep 3536: ep_len:509 episode reward: total was 38.190000. running mean: 14.321220\n",
      "epsilon:0.043193 episode_count: 24759. steps_count: 10703315.000000\n",
      "Time elapsed:  32263.70495891571\n",
      "ep 3537: ep_len:120 episode reward: total was 22.440000. running mean: 14.402408\n",
      "ep 3537: ep_len:500 episode reward: total was 28.730000. running mean: 14.545684\n",
      "ep 3537: ep_len:690 episode reward: total was -5.440000. running mean: 14.345827\n",
      "ep 3537: ep_len:129 episode reward: total was 13.540000. running mean: 14.337769\n",
      "ep 3537: ep_len:3 episode reward: total was 1.010000. running mean: 14.204491\n",
      "ep 3537: ep_len:241 episode reward: total was 37.480000. running mean: 14.437246\n",
      "ep 3537: ep_len:500 episode reward: total was 11.300000. running mean: 14.405874\n",
      "epsilon:0.043149 episode_count: 24766. steps_count: 10705498.000000\n",
      "Time elapsed:  32272.32988190651\n",
      "ep 3538: ep_len:509 episode reward: total was -20.690000. running mean: 14.054915\n",
      "ep 3538: ep_len:500 episode reward: total was 30.550000. running mean: 14.219866\n",
      "ep 3538: ep_len:588 episode reward: total was -8.750000. running mean: 13.990167\n",
      "ep 3538: ep_len:506 episode reward: total was 24.880000. running mean: 14.099065\n",
      "ep 3538: ep_len:3 episode reward: total was 1.010000. running mean: 13.968175\n",
      "ep 3538: ep_len:512 episode reward: total was 38.190000. running mean: 14.210393\n",
      "ep 3538: ep_len:524 episode reward: total was 21.870000. running mean: 14.286989\n",
      "epsilon:0.043104 episode_count: 24773. steps_count: 10708640.000000\n",
      "Time elapsed:  32280.713940620422\n",
      "ep 3539: ep_len:528 episode reward: total was -39.310000. running mean: 13.751019\n",
      "ep 3539: ep_len:500 episode reward: total was 63.890000. running mean: 14.252409\n",
      "ep 3539: ep_len:79 episode reward: total was 2.280000. running mean: 14.132685\n",
      "ep 3539: ep_len:528 episode reward: total was 65.050000. running mean: 14.641858\n",
      "ep 3539: ep_len:101 episode reward: total was 28.250000. running mean: 14.777939\n",
      "ep 3539: ep_len:626 episode reward: total was 9.910000. running mean: 14.729260\n",
      "ep 3539: ep_len:501 episode reward: total was 4.360000. running mean: 14.625567\n",
      "epsilon:0.043060 episode_count: 24780. steps_count: 10711503.000000\n",
      "Time elapsed:  32288.241290569305\n",
      "ep 3540: ep_len:526 episode reward: total was 69.220000. running mean: 15.171512\n",
      "ep 3540: ep_len:201 episode reward: total was 10.470000. running mean: 15.124497\n",
      "ep 3540: ep_len:383 episode reward: total was 31.970000. running mean: 15.292952\n",
      "ep 3540: ep_len:515 episode reward: total was 49.530000. running mean: 15.635322\n",
      "ep 3540: ep_len:3 episode reward: total was 1.010000. running mean: 15.489069\n",
      "ep 3540: ep_len:558 episode reward: total was 18.250000. running mean: 15.516678\n",
      "ep 3540: ep_len:556 episode reward: total was 30.620000. running mean: 15.667711\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.043016 episode_count: 24787. steps_count: 10714245.000000\n",
      "Time elapsed:  32298.062700986862\n",
      "ep 3541: ep_len:229 episode reward: total was 18.750000. running mean: 15.698534\n",
      "ep 3541: ep_len:501 episode reward: total was 51.160000. running mean: 16.053149\n",
      "ep 3541: ep_len:357 episode reward: total was 39.620000. running mean: 16.288817\n",
      "ep 3541: ep_len:522 episode reward: total was 85.070000. running mean: 16.976629\n",
      "ep 3541: ep_len:100 episode reward: total was 34.790000. running mean: 17.154763\n",
      "ep 3541: ep_len:543 episode reward: total was -5.300000. running mean: 16.930215\n",
      "ep 3541: ep_len:613 episode reward: total was 27.780000. running mean: 17.038713\n",
      "epsilon:0.042971 episode_count: 24794. steps_count: 10717110.000000\n",
      "Time elapsed:  32302.519606351852\n",
      "ep 3542: ep_len:500 episode reward: total was 41.580000. running mean: 17.284126\n",
      "ep 3542: ep_len:557 episode reward: total was 71.590000. running mean: 17.827185\n",
      "ep 3542: ep_len:505 episode reward: total was -11.030000. running mean: 17.538613\n",
      "ep 3542: ep_len:500 episode reward: total was 80.350000. running mean: 18.166727\n",
      "ep 3542: ep_len:55 episode reward: total was 27.010000. running mean: 18.255160\n",
      "ep 3542: ep_len:543 episode reward: total was -77.630000. running mean: 17.296308\n",
      "ep 3542: ep_len:528 episode reward: total was -3.230000. running mean: 17.091045\n",
      "epsilon:0.042927 episode_count: 24801. steps_count: 10720298.000000\n",
      "Time elapsed:  32307.6574883461\n",
      "ep 3543: ep_len:565 episode reward: total was -62.740000. running mean: 16.292734\n",
      "ep 3543: ep_len:500 episode reward: total was -14.190000. running mean: 15.987907\n",
      "ep 3543: ep_len:565 episode reward: total was 5.580000. running mean: 15.883828\n",
      "ep 3543: ep_len:598 episode reward: total was 54.180000. running mean: 16.266790\n",
      "ep 3543: ep_len:3 episode reward: total was 1.010000. running mean: 16.114222\n",
      "ep 3543: ep_len:534 episode reward: total was -8.210000. running mean: 15.870980\n",
      "ep 3543: ep_len:358 episode reward: total was -24.900000. running mean: 15.463270\n",
      "epsilon:0.042883 episode_count: 24808. steps_count: 10723421.000000\n",
      "Time elapsed:  32315.56240463257\n",
      "ep 3544: ep_len:209 episode reward: total was 17.260000. running mean: 15.481237\n",
      "ep 3544: ep_len:592 episode reward: total was 62.790000. running mean: 15.954325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3544: ep_len:500 episode reward: total was 38.410000. running mean: 16.178882\n",
      "ep 3544: ep_len:500 episode reward: total was 71.960000. running mean: 16.736693\n",
      "ep 3544: ep_len:3 episode reward: total was 1.010000. running mean: 16.579426\n",
      "ep 3544: ep_len:320 episode reward: total was 23.540000. running mean: 16.649032\n",
      "ep 3544: ep_len:500 episode reward: total was -1.830000. running mean: 16.464241\n",
      "epsilon:0.042838 episode_count: 24815. steps_count: 10726045.000000\n",
      "Time elapsed:  32322.71640086174\n",
      "ep 3545: ep_len:237 episode reward: total was 23.130000. running mean: 16.530899\n",
      "ep 3545: ep_len:525 episode reward: total was 57.690000. running mean: 16.942490\n",
      "ep 3545: ep_len:604 episode reward: total was 2.740000. running mean: 16.800465\n",
      "ep 3545: ep_len:578 episode reward: total was 41.980000. running mean: 17.052260\n",
      "ep 3545: ep_len:3 episode reward: total was 1.010000. running mean: 16.891838\n",
      "ep 3545: ep_len:313 episode reward: total was 30.570000. running mean: 17.028619\n",
      "ep 3545: ep_len:602 episode reward: total was 21.150000. running mean: 17.069833\n",
      "epsilon:0.042794 episode_count: 24822. steps_count: 10728907.000000\n",
      "Time elapsed:  32333.416558027267\n",
      "ep 3546: ep_len:513 episode reward: total was 11.330000. running mean: 17.012435\n",
      "ep 3546: ep_len:503 episode reward: total was -100.390000. running mean: 15.838410\n",
      "ep 3546: ep_len:594 episode reward: total was -12.700000. running mean: 15.553026\n",
      "ep 3546: ep_len:592 episode reward: total was 97.300000. running mean: 16.370496\n",
      "ep 3546: ep_len:3 episode reward: total was 1.010000. running mean: 16.216891\n",
      "ep 3546: ep_len:576 episode reward: total was 40.440000. running mean: 16.459122\n",
      "ep 3546: ep_len:584 episode reward: total was 33.810000. running mean: 16.632631\n",
      "epsilon:0.042750 episode_count: 24829. steps_count: 10732272.000000\n",
      "Time elapsed:  32342.44439101219\n",
      "ep 3547: ep_len:512 episode reward: total was 51.100000. running mean: 16.977305\n",
      "ep 3547: ep_len:362 episode reward: total was -12.090000. running mean: 16.686632\n",
      "ep 3547: ep_len:557 episode reward: total was -18.560000. running mean: 16.334165\n",
      "ep 3547: ep_len:502 episode reward: total was 50.250000. running mean: 16.673324\n",
      "ep 3547: ep_len:3 episode reward: total was 1.010000. running mean: 16.516690\n",
      "ep 3547: ep_len:523 episode reward: total was 3.410000. running mean: 16.385624\n",
      "ep 3547: ep_len:535 episode reward: total was 7.420000. running mean: 16.295967\n",
      "epsilon:0.042705 episode_count: 24836. steps_count: 10735266.000000\n",
      "Time elapsed:  32356.215418815613\n",
      "ep 3548: ep_len:564 episode reward: total was -59.810000. running mean: 15.534908\n",
      "ep 3548: ep_len:517 episode reward: total was 34.180000. running mean: 15.721359\n",
      "ep 3548: ep_len:434 episode reward: total was 17.580000. running mean: 15.739945\n",
      "ep 3548: ep_len:516 episode reward: total was -6.830000. running mean: 15.514245\n",
      "ep 3548: ep_len:3 episode reward: total was 1.010000. running mean: 15.369203\n",
      "ep 3548: ep_len:177 episode reward: total was 28.040000. running mean: 15.495911\n",
      "ep 3548: ep_len:508 episode reward: total was 7.760000. running mean: 15.418552\n",
      "epsilon:0.042661 episode_count: 24843. steps_count: 10737985.000000\n",
      "Time elapsed:  32363.541276454926\n",
      "ep 3549: ep_len:582 episode reward: total was -26.580000. running mean: 14.998566\n",
      "ep 3549: ep_len:659 episode reward: total was -92.380000. running mean: 13.924781\n",
      "ep 3549: ep_len:565 episode reward: total was -1.550000. running mean: 13.770033\n",
      "ep 3549: ep_len:132 episode reward: total was 3.090000. running mean: 13.663233\n",
      "ep 3549: ep_len:2 episode reward: total was -0.500000. running mean: 13.521600\n",
      "ep 3549: ep_len:300 episode reward: total was 34.420000. running mean: 13.730584\n",
      "ep 3549: ep_len:211 episode reward: total was -14.500000. running mean: 13.448278\n",
      "epsilon:0.042617 episode_count: 24850. steps_count: 10740436.000000\n",
      "Time elapsed:  32371.496540784836\n",
      "ep 3550: ep_len:572 episode reward: total was 57.540000. running mean: 13.889196\n",
      "ep 3550: ep_len:587 episode reward: total was -16.480000. running mean: 13.585504\n",
      "ep 3550: ep_len:459 episode reward: total was 50.740000. running mean: 13.957049\n",
      "ep 3550: ep_len:56 episode reward: total was 0.850000. running mean: 13.825978\n",
      "ep 3550: ep_len:3 episode reward: total was 1.010000. running mean: 13.697818\n",
      "ep 3550: ep_len:516 episode reward: total was -14.520000. running mean: 13.415640\n",
      "ep 3550: ep_len:532 episode reward: total was -2.700000. running mean: 13.254484\n",
      "epsilon:0.042572 episode_count: 24857. steps_count: 10743161.000000\n",
      "Time elapsed:  32378.987734556198\n",
      "ep 3551: ep_len:515 episode reward: total was 27.120000. running mean: 13.393139\n",
      "ep 3551: ep_len:511 episode reward: total was 112.220000. running mean: 14.381408\n",
      "ep 3551: ep_len:606 episode reward: total was -15.600000. running mean: 14.081593\n",
      "ep 3551: ep_len:517 episode reward: total was 41.970000. running mean: 14.360478\n",
      "ep 3551: ep_len:3 episode reward: total was 1.010000. running mean: 14.226973\n",
      "ep 3551: ep_len:619 episode reward: total was 23.950000. running mean: 14.324203\n",
      "ep 3551: ep_len:553 episode reward: total was 5.810000. running mean: 14.239061\n",
      "epsilon:0.042528 episode_count: 24864. steps_count: 10746485.000000\n",
      "Time elapsed:  32387.355201244354\n",
      "ep 3552: ep_len:680 episode reward: total was -15.030000. running mean: 13.946370\n",
      "ep 3552: ep_len:526 episode reward: total was -0.250000. running mean: 13.804407\n",
      "ep 3552: ep_len:645 episode reward: total was -30.600000. running mean: 13.360363\n",
      "ep 3552: ep_len:533 episode reward: total was 56.570000. running mean: 13.792459\n",
      "ep 3552: ep_len:51 episode reward: total was 20.510000. running mean: 13.859634\n",
      "ep 3552: ep_len:500 episode reward: total was 27.100000. running mean: 13.992038\n",
      "ep 3552: ep_len:500 episode reward: total was 47.360000. running mean: 14.325718\n",
      "epsilon:0.042484 episode_count: 24871. steps_count: 10749920.000000\n",
      "Time elapsed:  32396.281426668167\n",
      "ep 3553: ep_len:506 episode reward: total was 34.140000. running mean: 14.523861\n",
      "ep 3553: ep_len:500 episode reward: total was -22.840000. running mean: 14.150222\n",
      "ep 3553: ep_len:525 episode reward: total was -38.970000. running mean: 13.619020\n",
      "ep 3553: ep_len:500 episode reward: total was 3.480000. running mean: 13.517629\n",
      "ep 3553: ep_len:3 episode reward: total was 1.010000. running mean: 13.392553\n",
      "ep 3553: ep_len:523 episode reward: total was -24.680000. running mean: 13.011828\n",
      "ep 3553: ep_len:516 episode reward: total was 2.000000. running mean: 12.901709\n",
      "epsilon:0.042439 episode_count: 24878. steps_count: 10752993.000000\n",
      "Time elapsed:  32404.595583200455\n",
      "ep 3554: ep_len:500 episode reward: total was -45.500000. running mean: 12.317692\n",
      "ep 3554: ep_len:500 episode reward: total was 32.100000. running mean: 12.515515\n",
      "ep 3554: ep_len:648 episode reward: total was -12.600000. running mean: 12.264360\n",
      "ep 3554: ep_len:500 episode reward: total was -13.530000. running mean: 12.006417\n",
      "ep 3554: ep_len:3 episode reward: total was 1.010000. running mean: 11.896452\n",
      "ep 3554: ep_len:581 episode reward: total was 67.330000. running mean: 12.450788\n",
      "ep 3554: ep_len:182 episode reward: total was -11.700000. running mean: 12.209280\n",
      "epsilon:0.042395 episode_count: 24885. steps_count: 10755907.000000\n",
      "Time elapsed:  32414.360377311707\n",
      "ep 3555: ep_len:506 episode reward: total was 35.610000. running mean: 12.443287\n",
      "ep 3555: ep_len:592 episode reward: total was -10.450000. running mean: 12.214354\n",
      "ep 3555: ep_len:649 episode reward: total was -163.190000. running mean: 10.460311\n",
      "ep 3555: ep_len:562 episode reward: total was 93.080000. running mean: 11.286508\n",
      "ep 3555: ep_len:3 episode reward: total was 1.010000. running mean: 11.183743\n",
      "ep 3555: ep_len:500 episode reward: total was -4.960000. running mean: 11.022305\n",
      "ep 3555: ep_len:615 episode reward: total was 15.680000. running mean: 11.068882\n",
      "epsilon:0.042351 episode_count: 24892. steps_count: 10759334.000000\n",
      "Time elapsed:  32423.364989995956\n",
      "ep 3556: ep_len:500 episode reward: total was -26.690000. running mean: 10.691293\n",
      "ep 3556: ep_len:611 episode reward: total was 12.320000. running mean: 10.707580\n",
      "ep 3556: ep_len:582 episode reward: total was 1.550000. running mean: 10.616005\n",
      "ep 3556: ep_len:508 episode reward: total was 0.810000. running mean: 10.517945\n",
      "ep 3556: ep_len:3 episode reward: total was 1.010000. running mean: 10.422865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3556: ep_len:540 episode reward: total was -70.300000. running mean: 9.615636\n",
      "ep 3556: ep_len:569 episode reward: total was 17.600000. running mean: 9.695480\n",
      "epsilon:0.042306 episode_count: 24899. steps_count: 10762647.000000\n",
      "Time elapsed:  32432.171422958374\n",
      "ep 3557: ep_len:655 episode reward: total was 30.870000. running mean: 9.907225\n",
      "ep 3557: ep_len:514 episode reward: total was 3.760000. running mean: 9.845753\n",
      "ep 3557: ep_len:500 episode reward: total was 16.980000. running mean: 9.917096\n",
      "ep 3557: ep_len:576 episode reward: total was 65.920000. running mean: 10.477125\n",
      "ep 3557: ep_len:3 episode reward: total was 1.010000. running mean: 10.382453\n",
      "ep 3557: ep_len:233 episode reward: total was 45.880000. running mean: 10.737429\n",
      "ep 3557: ep_len:524 episode reward: total was 15.250000. running mean: 10.782554\n",
      "epsilon:0.042262 episode_count: 24906. steps_count: 10765652.000000\n",
      "Time elapsed:  32440.18511915207\n",
      "ep 3558: ep_len:654 episode reward: total was -20.260000. running mean: 10.472129\n",
      "ep 3558: ep_len:517 episode reward: total was 7.210000. running mean: 10.439508\n",
      "ep 3558: ep_len:369 episode reward: total was 43.360000. running mean: 10.768713\n",
      "ep 3558: ep_len:513 episode reward: total was 70.590000. running mean: 11.366925\n",
      "ep 3558: ep_len:107 episode reward: total was 29.260000. running mean: 11.545856\n",
      "ep 3558: ep_len:517 episode reward: total was -166.040000. running mean: 9.769998\n",
      "ep 3558: ep_len:332 episode reward: total was 14.970000. running mean: 9.821998\n",
      "epsilon:0.042218 episode_count: 24913. steps_count: 10768661.000000\n",
      "Time elapsed:  32447.32340812683\n",
      "ep 3559: ep_len:229 episode reward: total was 32.830000. running mean: 10.052078\n",
      "ep 3559: ep_len:608 episode reward: total was 15.010000. running mean: 10.101657\n",
      "ep 3559: ep_len:607 episode reward: total was 13.610000. running mean: 10.136740\n",
      "ep 3559: ep_len:56 episode reward: total was -0.160000. running mean: 10.033773\n",
      "ep 3559: ep_len:3 episode reward: total was -1.500000. running mean: 9.918435\n",
      "ep 3559: ep_len:640 episode reward: total was 40.390000. running mean: 10.223151\n",
      "ep 3559: ep_len:500 episode reward: total was 9.780000. running mean: 10.218719\n",
      "epsilon:0.042173 episode_count: 24920. steps_count: 10771304.000000\n",
      "Time elapsed:  32452.843653440475\n",
      "ep 3560: ep_len:526 episode reward: total was 20.400000. running mean: 10.320532\n",
      "ep 3560: ep_len:633 episode reward: total was 81.700000. running mean: 11.034327\n",
      "ep 3560: ep_len:547 episode reward: total was -34.090000. running mean: 10.583084\n",
      "ep 3560: ep_len:419 episode reward: total was 9.290000. running mean: 10.570153\n",
      "ep 3560: ep_len:97 episode reward: total was 29.250000. running mean: 10.756951\n",
      "ep 3560: ep_len:500 episode reward: total was 27.900000. running mean: 10.928382\n",
      "ep 3560: ep_len:627 episode reward: total was 22.610000. running mean: 11.045198\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.042129 episode_count: 24927. steps_count: 10774653.000000\n",
      "Time elapsed:  32466.955133914948\n",
      "ep 3561: ep_len:576 episode reward: total was -5.180000. running mean: 10.882946\n",
      "ep 3561: ep_len:500 episode reward: total was 66.550000. running mean: 11.439616\n",
      "ep 3561: ep_len:631 episode reward: total was 4.000000. running mean: 11.365220\n",
      "ep 3561: ep_len:514 episode reward: total was -47.190000. running mean: 10.779668\n",
      "ep 3561: ep_len:3 episode reward: total was 1.010000. running mean: 10.681971\n",
      "ep 3561: ep_len:646 episode reward: total was 1.730000. running mean: 10.592452\n",
      "ep 3561: ep_len:500 episode reward: total was 37.870000. running mean: 10.865227\n",
      "epsilon:0.042085 episode_count: 24934. steps_count: 10778023.000000\n",
      "Time elapsed:  32472.086304426193\n",
      "ep 3562: ep_len:612 episode reward: total was -47.460000. running mean: 10.281975\n",
      "ep 3562: ep_len:201 episode reward: total was 2.420000. running mean: 10.203355\n",
      "ep 3562: ep_len:500 episode reward: total was 22.900000. running mean: 10.330322\n",
      "ep 3562: ep_len:517 episode reward: total was 67.080000. running mean: 10.897818\n",
      "ep 3562: ep_len:3 episode reward: total was 1.010000. running mean: 10.798940\n",
      "ep 3562: ep_len:638 episode reward: total was 22.010000. running mean: 10.911051\n",
      "ep 3562: ep_len:198 episode reward: total was -0.370000. running mean: 10.798240\n",
      "epsilon:0.042040 episode_count: 24941. steps_count: 10780692.000000\n",
      "Time elapsed:  32476.272979736328\n",
      "ep 3563: ep_len:85 episode reward: total was -1.180000. running mean: 10.678458\n",
      "ep 3563: ep_len:524 episode reward: total was 83.570000. running mean: 11.407373\n",
      "ep 3563: ep_len:544 episode reward: total was 29.560000. running mean: 11.588900\n",
      "ep 3563: ep_len:510 episode reward: total was -2.180000. running mean: 11.451211\n",
      "ep 3563: ep_len:86 episode reward: total was 23.260000. running mean: 11.569298\n",
      "ep 3563: ep_len:328 episode reward: total was 27.690000. running mean: 11.730505\n",
      "ep 3563: ep_len:574 episode reward: total was 29.440000. running mean: 11.907600\n",
      "epsilon:0.041996 episode_count: 24948. steps_count: 10783343.000000\n",
      "Time elapsed:  32480.675859451294\n",
      "ep 3564: ep_len:584 episode reward: total was 48.080000. running mean: 12.269324\n",
      "ep 3564: ep_len:501 episode reward: total was 26.610000. running mean: 12.412731\n",
      "ep 3564: ep_len:560 episode reward: total was -7.510000. running mean: 12.213504\n",
      "ep 3564: ep_len:575 episode reward: total was -2.860000. running mean: 12.062769\n",
      "ep 3564: ep_len:3 episode reward: total was 1.010000. running mean: 11.952241\n",
      "ep 3564: ep_len:165 episode reward: total was 35.510000. running mean: 12.187819\n",
      "ep 3564: ep_len:514 episode reward: total was -15.760000. running mean: 11.908341\n",
      "epsilon:0.041952 episode_count: 24955. steps_count: 10786245.000000\n",
      "Time elapsed:  32488.364294052124\n",
      "ep 3565: ep_len:520 episode reward: total was -10.880000. running mean: 11.680457\n",
      "ep 3565: ep_len:575 episode reward: total was 34.370000. running mean: 11.907353\n",
      "ep 3565: ep_len:574 episode reward: total was -23.970000. running mean: 11.548579\n",
      "ep 3565: ep_len:517 episode reward: total was 6.900000. running mean: 11.502093\n",
      "ep 3565: ep_len:85 episode reward: total was 13.270000. running mean: 11.519772\n",
      "ep 3565: ep_len:603 episode reward: total was -2.600000. running mean: 11.378575\n",
      "ep 3565: ep_len:679 episode reward: total was -100.520000. running mean: 10.259589\n",
      "epsilon:0.041907 episode_count: 24962. steps_count: 10789798.000000\n",
      "Time elapsed:  32509.82280421257\n",
      "ep 3566: ep_len:624 episode reward: total was 38.150000. running mean: 10.538493\n",
      "ep 3566: ep_len:500 episode reward: total was 45.790000. running mean: 10.891008\n",
      "ep 3566: ep_len:500 episode reward: total was -168.830000. running mean: 9.093798\n",
      "ep 3566: ep_len:563 episode reward: total was 66.760000. running mean: 9.670460\n",
      "ep 3566: ep_len:100 episode reward: total was -18.290000. running mean: 9.390855\n",
      "ep 3566: ep_len:500 episode reward: total was 9.290000. running mean: 9.389847\n",
      "ep 3566: ep_len:500 episode reward: total was -27.500000. running mean: 9.020948\n",
      "epsilon:0.041863 episode_count: 24969. steps_count: 10793085.000000\n",
      "Time elapsed:  32517.98758673668\n",
      "ep 3567: ep_len:538 episode reward: total was -25.040000. running mean: 8.680339\n",
      "ep 3567: ep_len:500 episode reward: total was 91.460000. running mean: 9.508135\n",
      "ep 3567: ep_len:605 episode reward: total was 9.840000. running mean: 9.511454\n",
      "ep 3567: ep_len:500 episode reward: total was 7.100000. running mean: 9.487340\n",
      "ep 3567: ep_len:3 episode reward: total was 1.010000. running mean: 9.402566\n",
      "ep 3567: ep_len:503 episode reward: total was 24.560000. running mean: 9.554141\n",
      "ep 3567: ep_len:315 episode reward: total was 18.520000. running mean: 9.643799\n",
      "epsilon:0.041819 episode_count: 24976. steps_count: 10796049.000000\n",
      "Time elapsed:  32525.85095000267\n",
      "ep 3568: ep_len:598 episode reward: total was -33.640000. running mean: 9.210961\n",
      "ep 3568: ep_len:631 episode reward: total was 97.290000. running mean: 10.091752\n",
      "ep 3568: ep_len:74 episode reward: total was 9.330000. running mean: 10.084134\n",
      "ep 3568: ep_len:504 episode reward: total was 69.030000. running mean: 10.673593\n",
      "ep 3568: ep_len:97 episode reward: total was 20.740000. running mean: 10.774257\n",
      "ep 3568: ep_len:636 episode reward: total was 26.580000. running mean: 10.932314\n",
      "ep 3568: ep_len:510 episode reward: total was 3.630000. running mean: 10.859291\n",
      "epsilon:0.041774 episode_count: 24983. steps_count: 10799099.000000\n",
      "Time elapsed:  32539.74361348152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3569: ep_len:533 episode reward: total was -12.540000. running mean: 10.625298\n",
      "ep 3569: ep_len:500 episode reward: total was 64.580000. running mean: 11.164845\n",
      "ep 3569: ep_len:560 episode reward: total was -19.200000. running mean: 10.861197\n",
      "ep 3569: ep_len:500 episode reward: total was 63.830000. running mean: 11.390885\n",
      "ep 3569: ep_len:126 episode reward: total was 27.340000. running mean: 11.550376\n",
      "ep 3569: ep_len:595 episode reward: total was 33.170000. running mean: 11.766572\n",
      "ep 3569: ep_len:536 episode reward: total was 10.350000. running mean: 11.752406\n",
      "epsilon:0.041730 episode_count: 24990. steps_count: 10802449.000000\n",
      "Time elapsed:  32554.324313402176\n",
      "ep 3570: ep_len:656 episode reward: total was 33.270000. running mean: 11.967582\n",
      "ep 3570: ep_len:346 episode reward: total was 10.030000. running mean: 11.948206\n",
      "ep 3570: ep_len:566 episode reward: total was 5.140000. running mean: 11.880124\n",
      "ep 3570: ep_len:500 episode reward: total was 23.500000. running mean: 11.996323\n",
      "ep 3570: ep_len:3 episode reward: total was 1.010000. running mean: 11.886460\n",
      "ep 3570: ep_len:549 episode reward: total was 3.340000. running mean: 11.800995\n",
      "ep 3570: ep_len:350 episode reward: total was -6.060000. running mean: 11.622385\n",
      "epsilon:0.041686 episode_count: 24997. steps_count: 10805419.000000\n",
      "Time elapsed:  32562.18926382065\n",
      "ep 3571: ep_len:500 episode reward: total was 16.210000. running mean: 11.668262\n",
      "ep 3571: ep_len:500 episode reward: total was 111.280000. running mean: 12.664379\n",
      "ep 3571: ep_len:616 episode reward: total was 34.950000. running mean: 12.887235\n",
      "ep 3571: ep_len:500 episode reward: total was 20.380000. running mean: 12.962163\n",
      "ep 3571: ep_len:128 episode reward: total was 24.860000. running mean: 13.081141\n",
      "ep 3571: ep_len:587 episode reward: total was 39.380000. running mean: 13.344130\n",
      "ep 3571: ep_len:563 episode reward: total was 30.150000. running mean: 13.512188\n",
      "epsilon:0.041641 episode_count: 25004. steps_count: 10808813.000000\n",
      "Time elapsed:  32570.904315710068\n",
      "ep 3572: ep_len:226 episode reward: total was 8.220000. running mean: 13.459267\n",
      "ep 3572: ep_len:523 episode reward: total was 96.520000. running mean: 14.289874\n",
      "ep 3572: ep_len:541 episode reward: total was -4.450000. running mean: 14.102475\n",
      "ep 3572: ep_len:395 episode reward: total was 26.820000. running mean: 14.229650\n",
      "ep 3572: ep_len:94 episode reward: total was 26.250000. running mean: 14.349854\n",
      "ep 3572: ep_len:504 episode reward: total was -30.730000. running mean: 13.899055\n",
      "ep 3572: ep_len:576 episode reward: total was 35.650000. running mean: 14.116565\n",
      "epsilon:0.041597 episode_count: 25011. steps_count: 10811672.000000\n",
      "Time elapsed:  32588.52409219742\n",
      "ep 3573: ep_len:584 episode reward: total was -7.300000. running mean: 13.902399\n",
      "ep 3573: ep_len:629 episode reward: total was 97.910000. running mean: 14.742475\n",
      "ep 3573: ep_len:638 episode reward: total was -82.670000. running mean: 13.768350\n",
      "ep 3573: ep_len:132 episode reward: total was 5.140000. running mean: 13.682067\n",
      "ep 3573: ep_len:3 episode reward: total was 1.010000. running mean: 13.555346\n",
      "ep 3573: ep_len:543 episode reward: total was 21.610000. running mean: 13.635893\n",
      "ep 3573: ep_len:580 episode reward: total was 45.250000. running mean: 13.952034\n",
      "epsilon:0.041553 episode_count: 25018. steps_count: 10814781.000000\n",
      "Time elapsed:  32596.68288254738\n",
      "ep 3574: ep_len:500 episode reward: total was 52.040000. running mean: 14.332914\n",
      "ep 3574: ep_len:571 episode reward: total was 21.430000. running mean: 14.403884\n",
      "ep 3574: ep_len:541 episode reward: total was 8.130000. running mean: 14.341146\n",
      "ep 3574: ep_len:500 episode reward: total was 19.480000. running mean: 14.392534\n",
      "ep 3574: ep_len:3 episode reward: total was 1.010000. running mean: 14.258709\n",
      "ep 3574: ep_len:308 episode reward: total was 18.300000. running mean: 14.299122\n",
      "ep 3574: ep_len:208 episode reward: total was -4.000000. running mean: 14.116130\n",
      "epsilon:0.041508 episode_count: 25025. steps_count: 10817412.000000\n",
      "Time elapsed:  32602.500325202942\n",
      "ep 3575: ep_len:529 episode reward: total was -35.100000. running mean: 13.623969\n",
      "ep 3575: ep_len:504 episode reward: total was 55.540000. running mean: 14.043129\n",
      "ep 3575: ep_len:582 episode reward: total was 16.060000. running mean: 14.063298\n",
      "ep 3575: ep_len:500 episode reward: total was 15.330000. running mean: 14.075965\n",
      "ep 3575: ep_len:2 episode reward: total was -0.500000. running mean: 13.930206\n",
      "ep 3575: ep_len:544 episode reward: total was 19.840000. running mean: 13.989303\n",
      "ep 3575: ep_len:605 episode reward: total was -64.660000. running mean: 13.202810\n",
      "epsilon:0.041464 episode_count: 25032. steps_count: 10820678.000000\n",
      "Time elapsed:  32607.547918081284\n",
      "ep 3576: ep_len:597 episode reward: total was -8.010000. running mean: 12.990682\n",
      "ep 3576: ep_len:568 episode reward: total was 32.960000. running mean: 13.190375\n",
      "ep 3576: ep_len:516 episode reward: total was -8.780000. running mean: 12.970672\n",
      "ep 3576: ep_len:513 episode reward: total was -3.160000. running mean: 12.809365\n",
      "ep 3576: ep_len:3 episode reward: total was 1.010000. running mean: 12.691371\n",
      "ep 3576: ep_len:500 episode reward: total was -68.920000. running mean: 11.875258\n",
      "ep 3576: ep_len:551 episode reward: total was -0.200000. running mean: 11.754505\n",
      "epsilon:0.041420 episode_count: 25039. steps_count: 10823926.000000\n",
      "Time elapsed:  32621.61830663681\n",
      "ep 3577: ep_len:514 episode reward: total was 39.800000. running mean: 12.034960\n",
      "ep 3577: ep_len:500 episode reward: total was 30.090000. running mean: 12.215510\n",
      "ep 3577: ep_len:376 episode reward: total was 34.980000. running mean: 12.443155\n",
      "ep 3577: ep_len:544 episode reward: total was 46.450000. running mean: 12.783224\n",
      "ep 3577: ep_len:100 episode reward: total was 25.760000. running mean: 12.912992\n",
      "ep 3577: ep_len:324 episode reward: total was 10.230000. running mean: 12.886162\n",
      "ep 3577: ep_len:289 episode reward: total was 9.810000. running mean: 12.855400\n",
      "epsilon:0.041375 episode_count: 25046. steps_count: 10826573.000000\n",
      "Time elapsed:  32633.811903476715\n",
      "ep 3578: ep_len:500 episode reward: total was 41.030000. running mean: 13.137146\n",
      "ep 3578: ep_len:502 episode reward: total was -0.200000. running mean: 13.003775\n",
      "ep 3578: ep_len:545 episode reward: total was 2.570000. running mean: 12.899437\n",
      "ep 3578: ep_len:56 episode reward: total was -8.680000. running mean: 12.683642\n",
      "ep 3578: ep_len:128 episode reward: total was 25.870000. running mean: 12.815506\n",
      "ep 3578: ep_len:612 episode reward: total was -9.540000. running mean: 12.591951\n",
      "ep 3578: ep_len:569 episode reward: total was -15.150000. running mean: 12.314531\n",
      "epsilon:0.041331 episode_count: 25053. steps_count: 10829485.000000\n",
      "Time elapsed:  32641.52849459648\n",
      "ep 3579: ep_len:128 episode reward: total was 9.910000. running mean: 12.290486\n",
      "ep 3579: ep_len:620 episode reward: total was 51.780000. running mean: 12.685381\n",
      "ep 3579: ep_len:79 episode reward: total was 7.790000. running mean: 12.636427\n",
      "ep 3579: ep_len:500 episode reward: total was 54.450000. running mean: 13.054563\n",
      "ep 3579: ep_len:3 episode reward: total was 1.010000. running mean: 12.934118\n",
      "ep 3579: ep_len:553 episode reward: total was 58.850000. running mean: 13.393276\n",
      "ep 3579: ep_len:610 episode reward: total was 27.260000. running mean: 13.531944\n",
      "epsilon:0.041287 episode_count: 25060. steps_count: 10831978.000000\n",
      "Time elapsed:  32652.812235593796\n",
      "ep 3580: ep_len:575 episode reward: total was -79.360000. running mean: 12.603024\n",
      "ep 3580: ep_len:500 episode reward: total was 39.050000. running mean: 12.867494\n",
      "ep 3580: ep_len:553 episode reward: total was -26.830000. running mean: 12.470519\n",
      "ep 3580: ep_len:589 episode reward: total was 57.260000. running mean: 12.918414\n",
      "ep 3580: ep_len:3 episode reward: total was 1.010000. running mean: 12.799330\n",
      "ep 3580: ep_len:516 episode reward: total was -24.880000. running mean: 12.422536\n",
      "ep 3580: ep_len:567 episode reward: total was 27.450000. running mean: 12.572811\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.041242 episode_count: 25067. steps_count: 10835281.000000\n",
      "Time elapsed:  32666.122402191162\n",
      "ep 3581: ep_len:569 episode reward: total was 52.430000. running mean: 12.971383\n",
      "ep 3581: ep_len:315 episode reward: total was 11.800000. running mean: 12.959669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3581: ep_len:542 episode reward: total was -37.010000. running mean: 12.459972\n",
      "ep 3581: ep_len:567 episode reward: total was -32.890000. running mean: 12.006473\n",
      "ep 3581: ep_len:107 episode reward: total was 32.230000. running mean: 12.208708\n",
      "ep 3581: ep_len:633 episode reward: total was 1.820000. running mean: 12.104821\n",
      "ep 3581: ep_len:604 episode reward: total was 24.260000. running mean: 12.226373\n",
      "epsilon:0.041198 episode_count: 25074. steps_count: 10838618.000000\n",
      "Time elapsed:  32674.843274593353\n",
      "ep 3582: ep_len:586 episode reward: total was -62.590000. running mean: 11.478209\n",
      "ep 3582: ep_len:505 episode reward: total was 90.240000. running mean: 12.265827\n",
      "ep 3582: ep_len:403 episode reward: total was 55.760000. running mean: 12.700769\n",
      "ep 3582: ep_len:603 episode reward: total was 81.460000. running mean: 13.388361\n",
      "ep 3582: ep_len:47 episode reward: total was 20.010000. running mean: 13.454577\n",
      "ep 3582: ep_len:548 episode reward: total was 43.580000. running mean: 13.755831\n",
      "ep 3582: ep_len:512 episode reward: total was 23.620000. running mean: 13.854473\n",
      "epsilon:0.041154 episode_count: 25081. steps_count: 10841822.000000\n",
      "Time elapsed:  32683.405846595764\n",
      "ep 3583: ep_len:575 episode reward: total was -32.950000. running mean: 13.386428\n",
      "ep 3583: ep_len:546 episode reward: total was 31.410000. running mean: 13.566664\n",
      "ep 3583: ep_len:529 episode reward: total was -79.930000. running mean: 12.631698\n",
      "ep 3583: ep_len:508 episode reward: total was 35.510000. running mean: 12.860481\n",
      "ep 3583: ep_len:87 episode reward: total was 21.220000. running mean: 12.944076\n",
      "ep 3583: ep_len:501 episode reward: total was 31.170000. running mean: 13.126335\n",
      "ep 3583: ep_len:283 episode reward: total was 30.840000. running mean: 13.303472\n",
      "epsilon:0.041109 episode_count: 25088. steps_count: 10844851.000000\n",
      "Time elapsed:  32696.762310743332\n",
      "ep 3584: ep_len:500 episode reward: total was 81.650000. running mean: 13.986937\n",
      "ep 3584: ep_len:531 episode reward: total was 97.330000. running mean: 14.820368\n",
      "ep 3584: ep_len:79 episode reward: total was 7.790000. running mean: 14.750064\n",
      "ep 3584: ep_len:500 episode reward: total was -22.140000. running mean: 14.381163\n",
      "ep 3584: ep_len:3 episode reward: total was 1.010000. running mean: 14.247452\n",
      "ep 3584: ep_len:574 episode reward: total was -4.990000. running mean: 14.055077\n",
      "ep 3584: ep_len:194 episode reward: total was 0.080000. running mean: 13.915326\n",
      "epsilon:0.041065 episode_count: 25095. steps_count: 10847232.000000\n",
      "Time elapsed:  32703.381575107574\n",
      "ep 3585: ep_len:194 episode reward: total was 10.590000. running mean: 13.882073\n",
      "ep 3585: ep_len:533 episode reward: total was -14.210000. running mean: 13.601152\n",
      "ep 3585: ep_len:582 episode reward: total was 7.380000. running mean: 13.538941\n",
      "ep 3585: ep_len:500 episode reward: total was 15.650000. running mean: 13.560051\n",
      "ep 3585: ep_len:77 episode reward: total was 20.720000. running mean: 13.631651\n",
      "ep 3585: ep_len:619 episode reward: total was 40.500000. running mean: 13.900334\n",
      "ep 3585: ep_len:324 episode reward: total was 23.520000. running mean: 13.996531\n",
      "epsilon:0.041021 episode_count: 25102. steps_count: 10850061.000000\n",
      "Time elapsed:  32710.868092775345\n",
      "ep 3586: ep_len:500 episode reward: total was 90.090000. running mean: 14.757466\n",
      "ep 3586: ep_len:616 episode reward: total was 37.290000. running mean: 14.982791\n",
      "ep 3586: ep_len:52 episode reward: total was 2.620000. running mean: 14.859163\n",
      "ep 3586: ep_len:500 episode reward: total was 25.100000. running mean: 14.961572\n",
      "ep 3586: ep_len:3 episode reward: total was 1.010000. running mean: 14.822056\n",
      "ep 3586: ep_len:186 episode reward: total was 34.740000. running mean: 15.021235\n",
      "ep 3586: ep_len:500 episode reward: total was -7.000000. running mean: 14.801023\n",
      "epsilon:0.040976 episode_count: 25109. steps_count: 10852418.000000\n",
      "Time elapsed:  32717.41209244728\n",
      "ep 3587: ep_len:596 episode reward: total was -103.360000. running mean: 13.619413\n",
      "ep 3587: ep_len:500 episode reward: total was 35.270000. running mean: 13.835919\n",
      "ep 3587: ep_len:599 episode reward: total was 37.980000. running mean: 14.077359\n",
      "ep 3587: ep_len:500 episode reward: total was -18.670000. running mean: 13.749886\n",
      "ep 3587: ep_len:103 episode reward: total was 17.250000. running mean: 13.784887\n",
      "ep 3587: ep_len:500 episode reward: total was 51.190000. running mean: 14.158938\n",
      "ep 3587: ep_len:503 episode reward: total was -71.240000. running mean: 13.304949\n",
      "epsilon:0.040932 episode_count: 25116. steps_count: 10855719.000000\n",
      "Time elapsed:  32726.140975236893\n",
      "ep 3588: ep_len:628 episode reward: total was -20.490000. running mean: 12.966999\n",
      "ep 3588: ep_len:522 episode reward: total was 6.700000. running mean: 12.904329\n",
      "ep 3588: ep_len:672 episode reward: total was -43.520000. running mean: 12.340086\n",
      "ep 3588: ep_len:513 episode reward: total was 34.100000. running mean: 12.557685\n",
      "ep 3588: ep_len:94 episode reward: total was 27.230000. running mean: 12.704408\n",
      "ep 3588: ep_len:541 episode reward: total was -3.580000. running mean: 12.541564\n",
      "ep 3588: ep_len:588 episode reward: total was 27.810000. running mean: 12.694248\n",
      "epsilon:0.040888 episode_count: 25123. steps_count: 10859277.000000\n",
      "Time elapsed:  32741.195216178894\n",
      "ep 3589: ep_len:500 episode reward: total was 38.220000. running mean: 12.949506\n",
      "ep 3589: ep_len:500 episode reward: total was 89.810000. running mean: 13.718111\n",
      "ep 3589: ep_len:596 episode reward: total was 7.030000. running mean: 13.651230\n",
      "ep 3589: ep_len:500 episode reward: total was 89.500000. running mean: 14.409717\n",
      "ep 3589: ep_len:91 episode reward: total was -11.270000. running mean: 14.152920\n",
      "ep 3589: ep_len:595 episode reward: total was 0.250000. running mean: 14.013891\n",
      "ep 3589: ep_len:540 episode reward: total was 13.550000. running mean: 14.009252\n",
      "epsilon:0.040843 episode_count: 25130. steps_count: 10862599.000000\n",
      "Time elapsed:  32750.732357501984\n",
      "ep 3590: ep_len:534 episode reward: total was 48.770000. running mean: 14.356860\n",
      "ep 3590: ep_len:331 episode reward: total was -19.240000. running mean: 14.020891\n",
      "ep 3590: ep_len:54 episode reward: total was -0.330000. running mean: 13.877382\n",
      "ep 3590: ep_len:113 episode reward: total was 19.440000. running mean: 13.933008\n",
      "ep 3590: ep_len:52 episode reward: total was 8.620000. running mean: 13.879878\n",
      "ep 3590: ep_len:218 episode reward: total was 36.330000. running mean: 14.104379\n",
      "ep 3590: ep_len:516 episode reward: total was -6.840000. running mean: 13.894936\n",
      "epsilon:0.040799 episode_count: 25137. steps_count: 10864417.000000\n",
      "Time elapsed:  32759.51132106781\n",
      "ep 3591: ep_len:568 episode reward: total was -38.590000. running mean: 13.370086\n",
      "ep 3591: ep_len:500 episode reward: total was 69.000000. running mean: 13.926385\n",
      "ep 3591: ep_len:581 episode reward: total was 40.760000. running mean: 14.194722\n",
      "ep 3591: ep_len:500 episode reward: total was 36.350000. running mean: 14.416274\n",
      "ep 3591: ep_len:130 episode reward: total was 24.840000. running mean: 14.520512\n",
      "ep 3591: ep_len:500 episode reward: total was -1.290000. running mean: 14.362407\n",
      "ep 3591: ep_len:547 episode reward: total was -73.300000. running mean: 13.485782\n",
      "epsilon:0.040755 episode_count: 25144. steps_count: 10867743.000000\n",
      "Time elapsed:  32768.46050739288\n",
      "ep 3592: ep_len:116 episode reward: total was 13.920000. running mean: 13.490125\n",
      "ep 3592: ep_len:555 episode reward: total was 12.950000. running mean: 13.484723\n",
      "ep 3592: ep_len:652 episode reward: total was -6.930000. running mean: 13.280576\n",
      "ep 3592: ep_len:558 episode reward: total was 27.200000. running mean: 13.419770\n",
      "ep 3592: ep_len:3 episode reward: total was 1.010000. running mean: 13.295673\n",
      "ep 3592: ep_len:571 episode reward: total was -81.040000. running mean: 12.352316\n",
      "ep 3592: ep_len:211 episode reward: total was 3.710000. running mean: 12.265893\n",
      "epsilon:0.040710 episode_count: 25151. steps_count: 10870409.000000\n",
      "Time elapsed:  32775.62989640236\n",
      "ep 3593: ep_len:500 episode reward: total was -15.780000. running mean: 11.985434\n",
      "ep 3593: ep_len:500 episode reward: total was 23.150000. running mean: 12.097080\n",
      "ep 3593: ep_len:636 episode reward: total was 29.890000. running mean: 12.275009\n",
      "ep 3593: ep_len:512 episode reward: total was 60.170000. running mean: 12.753959\n",
      "ep 3593: ep_len:3 episode reward: total was 1.010000. running mean: 12.636519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3593: ep_len:500 episode reward: total was -33.040000. running mean: 12.179754\n",
      "ep 3593: ep_len:211 episode reward: total was 18.280000. running mean: 12.240756\n",
      "epsilon:0.040666 episode_count: 25158. steps_count: 10873271.000000\n",
      "Time elapsed:  32783.24510669708\n",
      "ep 3594: ep_len:500 episode reward: total was 86.740000. running mean: 12.985749\n",
      "ep 3594: ep_len:500 episode reward: total was -0.350000. running mean: 12.852391\n",
      "ep 3594: ep_len:500 episode reward: total was 29.180000. running mean: 13.015667\n",
      "ep 3594: ep_len:500 episode reward: total was -1.320000. running mean: 12.872311\n",
      "ep 3594: ep_len:72 episode reward: total was 18.100000. running mean: 12.924588\n",
      "ep 3594: ep_len:501 episode reward: total was 16.950000. running mean: 12.964842\n",
      "ep 3594: ep_len:566 episode reward: total was 0.470000. running mean: 12.839893\n",
      "epsilon:0.040622 episode_count: 25165. steps_count: 10876410.000000\n",
      "Time elapsed:  32801.86589431763\n",
      "ep 3595: ep_len:619 episode reward: total was -9.010000. running mean: 12.621394\n",
      "ep 3595: ep_len:500 episode reward: total was 2.860000. running mean: 12.523780\n",
      "ep 3595: ep_len:379 episode reward: total was 37.730000. running mean: 12.775843\n",
      "ep 3595: ep_len:500 episode reward: total was 57.940000. running mean: 13.227484\n",
      "ep 3595: ep_len:3 episode reward: total was 1.010000. running mean: 13.105309\n",
      "ep 3595: ep_len:518 episode reward: total was 34.930000. running mean: 13.323556\n",
      "ep 3595: ep_len:500 episode reward: total was 15.720000. running mean: 13.347521\n",
      "epsilon:0.040577 episode_count: 25172. steps_count: 10879429.000000\n",
      "Time elapsed:  32810.82404637337\n",
      "ep 3596: ep_len:564 episode reward: total was -23.910000. running mean: 12.974945\n",
      "ep 3596: ep_len:520 episode reward: total was 73.770000. running mean: 13.582896\n",
      "ep 3596: ep_len:558 episode reward: total was -25.390000. running mean: 13.193167\n",
      "ep 3596: ep_len:500 episode reward: total was 17.630000. running mean: 13.237535\n",
      "ep 3596: ep_len:101 episode reward: total was 21.700000. running mean: 13.322160\n",
      "ep 3596: ep_len:520 episode reward: total was 18.020000. running mean: 13.369138\n",
      "ep 3596: ep_len:194 episode reward: total was 7.090000. running mean: 13.306347\n",
      "epsilon:0.040533 episode_count: 25179. steps_count: 10882386.000000\n",
      "Time elapsed:  32819.88244056702\n",
      "ep 3597: ep_len:562 episode reward: total was -6.090000. running mean: 13.112384\n",
      "ep 3597: ep_len:522 episode reward: total was 12.840000. running mean: 13.109660\n",
      "ep 3597: ep_len:652 episode reward: total was -52.750000. running mean: 12.451063\n",
      "ep 3597: ep_len:500 episode reward: total was 33.650000. running mean: 12.663053\n",
      "ep 3597: ep_len:52 episode reward: total was 22.510000. running mean: 12.761522\n",
      "ep 3597: ep_len:537 episode reward: total was -1.680000. running mean: 12.617107\n",
      "ep 3597: ep_len:516 episode reward: total was 14.030000. running mean: 12.631236\n",
      "epsilon:0.040489 episode_count: 25186. steps_count: 10885727.000000\n",
      "Time elapsed:  32846.76550197601\n",
      "ep 3598: ep_len:95 episode reward: total was -11.000000. running mean: 12.394923\n",
      "ep 3598: ep_len:518 episode reward: total was 17.870000. running mean: 12.449674\n",
      "ep 3598: ep_len:888 episode reward: total was -792.800000. running mean: 4.397177\n",
      "ep 3598: ep_len:558 episode reward: total was 49.610000. running mean: 4.849306\n",
      "ep 3598: ep_len:105 episode reward: total was 23.240000. running mean: 5.033213\n",
      "ep 3598: ep_len:314 episode reward: total was 24.700000. running mean: 5.229880\n",
      "ep 3598: ep_len:551 episode reward: total was -157.030000. running mean: 3.607282\n",
      "epsilon:0.040444 episode_count: 25193. steps_count: 10888756.000000\n",
      "Time elapsed:  32856.00591468811\n",
      "ep 3599: ep_len:101 episode reward: total was 1.800000. running mean: 3.589209\n",
      "ep 3599: ep_len:567 episode reward: total was 14.400000. running mean: 3.697317\n",
      "ep 3599: ep_len:408 episode reward: total was 31.920000. running mean: 3.979544\n",
      "ep 3599: ep_len:575 episode reward: total was 75.360000. running mean: 4.693348\n",
      "ep 3599: ep_len:43 episode reward: total was 18.500000. running mean: 4.831415\n",
      "ep 3599: ep_len:584 episode reward: total was 51.530000. running mean: 5.298400\n",
      "ep 3599: ep_len:168 episode reward: total was -13.500000. running mean: 5.110416\n",
      "epsilon:0.040400 episode_count: 25200. steps_count: 10891202.000000\n",
      "Time elapsed:  32863.66503405571\n",
      "ep 3600: ep_len:265 episode reward: total was 25.460000. running mean: 5.313912\n",
      "ep 3600: ep_len:500 episode reward: total was 5.020000. running mean: 5.310973\n",
      "ep 3600: ep_len:571 episode reward: total was 19.330000. running mean: 5.451163\n",
      "ep 3600: ep_len:550 episode reward: total was 42.150000. running mean: 5.818152\n",
      "ep 3600: ep_len:3 episode reward: total was 1.010000. running mean: 5.770070\n",
      "ep 3600: ep_len:500 episode reward: total was -5.530000. running mean: 5.657070\n",
      "ep 3600: ep_len:568 episode reward: total was 14.560000. running mean: 5.746099\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.040356 episode_count: 25207. steps_count: 10894159.000000\n",
      "Time elapsed:  32878.1805665493\n",
      "ep 3601: ep_len:520 episode reward: total was 40.400000. running mean: 6.092638\n",
      "ep 3601: ep_len:583 episode reward: total was 21.070000. running mean: 6.242412\n",
      "ep 3601: ep_len:545 episode reward: total was -23.710000. running mean: 5.942887\n",
      "ep 3601: ep_len:538 episode reward: total was 62.990000. running mean: 6.513359\n",
      "ep 3601: ep_len:3 episode reward: total was 1.010000. running mean: 6.458325\n",
      "ep 3601: ep_len:560 episode reward: total was 1.680000. running mean: 6.410542\n",
      "ep 3601: ep_len:519 episode reward: total was -17.780000. running mean: 6.168636\n",
      "epsilon:0.040311 episode_count: 25214. steps_count: 10897427.000000\n",
      "Time elapsed:  32887.95038437843\n",
      "ep 3602: ep_len:621 episode reward: total was -30.140000. running mean: 5.805550\n",
      "ep 3602: ep_len:500 episode reward: total was 1.800000. running mean: 5.765494\n",
      "ep 3602: ep_len:577 episode reward: total was -10.630000. running mean: 5.601539\n",
      "ep 3602: ep_len:500 episode reward: total was 13.220000. running mean: 5.677724\n",
      "ep 3602: ep_len:44 episode reward: total was 15.510000. running mean: 5.776047\n",
      "ep 3602: ep_len:579 episode reward: total was -2.820000. running mean: 5.690086\n",
      "ep 3602: ep_len:567 episode reward: total was 25.650000. running mean: 5.889686\n",
      "epsilon:0.040267 episode_count: 25221. steps_count: 10900815.000000\n",
      "Time elapsed:  32904.95446705818\n",
      "ep 3603: ep_len:241 episode reward: total was 20.890000. running mean: 6.039689\n",
      "ep 3603: ep_len:635 episode reward: total was 95.890000. running mean: 6.938192\n",
      "ep 3603: ep_len:500 episode reward: total was 18.950000. running mean: 7.058310\n",
      "ep 3603: ep_len:676 episode reward: total was -644.710000. running mean: 0.540627\n",
      "ep 3603: ep_len:3 episode reward: total was 1.010000. running mean: 0.545321\n",
      "ep 3603: ep_len:170 episode reward: total was -28.730000. running mean: 0.252567\n",
      "ep 3603: ep_len:500 episode reward: total was 67.100000. running mean: 0.921042\n",
      "epsilon:0.040223 episode_count: 25228. steps_count: 10903540.000000\n",
      "Time elapsed:  32918.3200404644\n",
      "ep 3604: ep_len:500 episode reward: total was 60.530000. running mean: 1.517131\n",
      "ep 3604: ep_len:500 episode reward: total was 27.180000. running mean: 1.773760\n",
      "ep 3604: ep_len:418 episode reward: total was 59.020000. running mean: 2.346222\n",
      "ep 3604: ep_len:549 episode reward: total was 72.500000. running mean: 3.047760\n",
      "ep 3604: ep_len:3 episode reward: total was 1.010000. running mean: 3.027382\n",
      "ep 3604: ep_len:641 episode reward: total was 37.590000. running mean: 3.373009\n",
      "ep 3604: ep_len:512 episode reward: total was -23.800000. running mean: 3.101279\n",
      "epsilon:0.040178 episode_count: 25235. steps_count: 10906663.000000\n",
      "Time elapsed:  32924.47010040283\n",
      "ep 3605: ep_len:225 episode reward: total was 7.730000. running mean: 3.147566\n",
      "ep 3605: ep_len:562 episode reward: total was 19.980000. running mean: 3.315890\n",
      "ep 3605: ep_len:567 episode reward: total was -15.920000. running mean: 3.123531\n",
      "ep 3605: ep_len:571 episode reward: total was 85.430000. running mean: 3.946596\n",
      "ep 3605: ep_len:3 episode reward: total was 1.010000. running mean: 3.917230\n",
      "ep 3605: ep_len:522 episode reward: total was -36.510000. running mean: 3.512958\n",
      "ep 3605: ep_len:500 episode reward: total was -159.700000. running mean: 1.880828\n",
      "epsilon:0.040134 episode_count: 25242. steps_count: 10909613.000000\n",
      "Time elapsed:  32930.30766916275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3606: ep_len:531 episode reward: total was -4.800000. running mean: 1.814020\n",
      "ep 3606: ep_len:627 episode reward: total was 49.280000. running mean: 2.288680\n",
      "ep 3606: ep_len:563 episode reward: total was -6.300000. running mean: 2.202793\n",
      "ep 3606: ep_len:500 episode reward: total was 26.230000. running mean: 2.443065\n",
      "ep 3606: ep_len:88 episode reward: total was 24.200000. running mean: 2.660634\n",
      "ep 3606: ep_len:504 episode reward: total was -28.020000. running mean: 2.353828\n",
      "ep 3606: ep_len:188 episode reward: total was 5.040000. running mean: 2.380690\n",
      "epsilon:0.040090 episode_count: 25249. steps_count: 10912614.000000\n",
      "Time elapsed:  32953.52329516411\n",
      "ep 3607: ep_len:755 episode reward: total was -106.770000. running mean: 1.289183\n",
      "ep 3607: ep_len:500 episode reward: total was 22.830000. running mean: 1.504591\n",
      "ep 3607: ep_len:558 episode reward: total was 17.160000. running mean: 1.661145\n",
      "ep 3607: ep_len:500 episode reward: total was 85.140000. running mean: 2.495934\n",
      "ep 3607: ep_len:3 episode reward: total was 1.010000. running mean: 2.481074\n",
      "ep 3607: ep_len:629 episode reward: total was 30.800000. running mean: 2.764263\n",
      "ep 3607: ep_len:529 episode reward: total was 9.790000. running mean: 2.834521\n",
      "epsilon:0.040045 episode_count: 25256. steps_count: 10916088.000000\n",
      "Time elapsed:  32960.26519703865\n",
      "ep 3608: ep_len:528 episode reward: total was -35.900000. running mean: 2.447176\n",
      "ep 3608: ep_len:578 episode reward: total was 3.400000. running mean: 2.456704\n",
      "ep 3608: ep_len:511 episode reward: total was 1.480000. running mean: 2.446937\n",
      "ep 3608: ep_len:514 episode reward: total was 24.900000. running mean: 2.671467\n",
      "ep 3608: ep_len:3 episode reward: total was 1.010000. running mean: 2.654853\n",
      "ep 3608: ep_len:506 episode reward: total was 11.580000. running mean: 2.744104\n",
      "ep 3608: ep_len:503 episode reward: total was 46.540000. running mean: 3.182063\n",
      "epsilon:0.040001 episode_count: 25263. steps_count: 10919231.000000\n",
      "Time elapsed:  32975.44178342819\n",
      "ep 3609: ep_len:633 episode reward: total was -4.220000. running mean: 3.108043\n",
      "ep 3609: ep_len:500 episode reward: total was 8.990000. running mean: 3.166862\n",
      "ep 3609: ep_len:567 episode reward: total was -22.270000. running mean: 2.912494\n",
      "ep 3609: ep_len:546 episode reward: total was 46.500000. running mean: 3.348369\n",
      "ep 3609: ep_len:102 episode reward: total was 30.770000. running mean: 3.622585\n",
      "ep 3609: ep_len:533 episode reward: total was 5.920000. running mean: 3.645559\n",
      "ep 3609: ep_len:560 episode reward: total was -42.250000. running mean: 3.186603\n",
      "epsilon:0.039957 episode_count: 25270. steps_count: 10922672.000000\n",
      "Time elapsed:  32985.70169830322\n",
      "ep 3610: ep_len:642 episode reward: total was -43.910000. running mean: 2.715637\n",
      "ep 3610: ep_len:531 episode reward: total was 59.260000. running mean: 3.281081\n",
      "ep 3610: ep_len:448 episode reward: total was 55.220000. running mean: 3.800470\n",
      "ep 3610: ep_len:500 episode reward: total was -47.930000. running mean: 3.283166\n",
      "ep 3610: ep_len:3 episode reward: total was 1.010000. running mean: 3.260434\n",
      "ep 3610: ep_len:618 episode reward: total was 10.310000. running mean: 3.330930\n",
      "ep 3610: ep_len:500 episode reward: total was 61.930000. running mean: 3.916920\n",
      "epsilon:0.039912 episode_count: 25277. steps_count: 10925914.000000\n",
      "Time elapsed:  33001.99558377266\n",
      "ep 3611: ep_len:597 episode reward: total was 54.830000. running mean: 4.426051\n",
      "ep 3611: ep_len:626 episode reward: total was 17.130000. running mean: 4.553091\n",
      "ep 3611: ep_len:639 episode reward: total was -9.790000. running mean: 4.409660\n",
      "ep 3611: ep_len:529 episode reward: total was 52.370000. running mean: 4.889263\n",
      "ep 3611: ep_len:3 episode reward: total was 1.010000. running mean: 4.850470\n",
      "ep 3611: ep_len:619 episode reward: total was 8.920000. running mean: 4.891166\n",
      "ep 3611: ep_len:185 episode reward: total was 18.480000. running mean: 5.027054\n",
      "epsilon:0.039868 episode_count: 25284. steps_count: 10929112.000000\n",
      "Time elapsed:  33011.55467200279\n",
      "ep 3612: ep_len:114 episode reward: total was -6.000000. running mean: 4.916783\n",
      "ep 3612: ep_len:285 episode reward: total was -41.100000. running mean: 4.456616\n",
      "ep 3612: ep_len:589 episode reward: total was -22.750000. running mean: 4.184550\n",
      "ep 3612: ep_len:535 episode reward: total was 74.790000. running mean: 4.890604\n",
      "ep 3612: ep_len:80 episode reward: total was 21.760000. running mean: 5.059298\n",
      "ep 3612: ep_len:593 episode reward: total was 5.220000. running mean: 5.060905\n",
      "ep 3612: ep_len:502 episode reward: total was 2.220000. running mean: 5.032496\n",
      "epsilon:0.039824 episode_count: 25291. steps_count: 10931810.000000\n",
      "Time elapsed:  33019.809008836746\n",
      "ep 3613: ep_len:605 episode reward: total was -17.330000. running mean: 4.808871\n",
      "ep 3613: ep_len:630 episode reward: total was -124.740000. running mean: 3.513382\n",
      "ep 3613: ep_len:540 episode reward: total was 9.410000. running mean: 3.572348\n",
      "ep 3613: ep_len:40 episode reward: total was 3.280000. running mean: 3.569425\n",
      "ep 3613: ep_len:3 episode reward: total was 1.010000. running mean: 3.543831\n",
      "ep 3613: ep_len:536 episode reward: total was -77.080000. running mean: 2.737592\n",
      "ep 3613: ep_len:569 episode reward: total was 34.740000. running mean: 3.057616\n",
      "epsilon:0.039779 episode_count: 25298. steps_count: 10934733.000000\n",
      "Time elapsed:  33028.438580513\n",
      "ep 3614: ep_len:500 episode reward: total was 68.790000. running mean: 3.714940\n",
      "ep 3614: ep_len:500 episode reward: total was 17.070000. running mean: 3.848491\n",
      "ep 3614: ep_len:570 episode reward: total was -16.300000. running mean: 3.647006\n",
      "ep 3614: ep_len:620 episode reward: total was 89.260000. running mean: 4.503136\n",
      "ep 3614: ep_len:3 episode reward: total was 1.010000. running mean: 4.468205\n",
      "ep 3614: ep_len:539 episode reward: total was 22.300000. running mean: 4.646523\n",
      "ep 3614: ep_len:500 episode reward: total was 50.700000. running mean: 5.107057\n",
      "epsilon:0.039735 episode_count: 25305. steps_count: 10937965.000000\n",
      "Time elapsed:  33045.76926088333\n",
      "ep 3615: ep_len:610 episode reward: total was 9.350000. running mean: 5.149487\n",
      "ep 3615: ep_len:595 episode reward: total was 8.240000. running mean: 5.180392\n",
      "ep 3615: ep_len:500 episode reward: total was -13.550000. running mean: 4.993088\n",
      "ep 3615: ep_len:84 episode reward: total was 6.840000. running mean: 5.011557\n",
      "ep 3615: ep_len:113 episode reward: total was 29.750000. running mean: 5.258942\n",
      "ep 3615: ep_len:204 episode reward: total was 27.190000. running mean: 5.478252\n",
      "ep 3615: ep_len:511 episode reward: total was 19.530000. running mean: 5.618770\n",
      "epsilon:0.039691 episode_count: 25312. steps_count: 10940582.000000\n",
      "Time elapsed:  33051.031022787094\n",
      "ep 3616: ep_len:202 episode reward: total was -3.410000. running mean: 5.528482\n",
      "ep 3616: ep_len:624 episode reward: total was 55.950000. running mean: 6.032697\n",
      "ep 3616: ep_len:66 episode reward: total was 4.290000. running mean: 6.015270\n",
      "ep 3616: ep_len:515 episode reward: total was 9.730000. running mean: 6.052417\n",
      "ep 3616: ep_len:3 episode reward: total was 1.010000. running mean: 6.001993\n",
      "ep 3616: ep_len:574 episode reward: total was 30.100000. running mean: 6.242973\n",
      "ep 3616: ep_len:540 episode reward: total was -42.810000. running mean: 5.752444\n",
      "epsilon:0.039646 episode_count: 25319. steps_count: 10943106.000000\n",
      "Time elapsed:  33056.10908293724\n",
      "ep 3617: ep_len:640 episode reward: total was -8.130000. running mean: 5.613619\n",
      "ep 3617: ep_len:507 episode reward: total was -55.350000. running mean: 5.003983\n",
      "ep 3617: ep_len:617 episode reward: total was 5.820000. running mean: 5.012143\n",
      "ep 3617: ep_len:500 episode reward: total was 15.060000. running mean: 5.112622\n",
      "ep 3617: ep_len:3 episode reward: total was 1.010000. running mean: 5.071595\n",
      "ep 3617: ep_len:186 episode reward: total was 36.730000. running mean: 5.388179\n",
      "ep 3617: ep_len:521 episode reward: total was 21.320000. running mean: 5.547498\n",
      "epsilon:0.039602 episode_count: 25326. steps_count: 10946080.000000\n",
      "Time elapsed:  33070.73028278351\n",
      "ep 3618: ep_len:635 episode reward: total was 59.170000. running mean: 6.083723\n",
      "ep 3618: ep_len:563 episode reward: total was -73.420000. running mean: 5.288685\n",
      "ep 3618: ep_len:615 episode reward: total was -29.190000. running mean: 4.943899\n",
      "ep 3618: ep_len:170 episode reward: total was 12.750000. running mean: 5.021960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3618: ep_len:111 episode reward: total was -47.240000. running mean: 4.499340\n",
      "ep 3618: ep_len:707 episode reward: total was -9.120000. running mean: 4.363147\n",
      "ep 3618: ep_len:587 episode reward: total was 30.200000. running mean: 4.621515\n",
      "epsilon:0.039558 episode_count: 25333. steps_count: 10949468.000000\n",
      "Time elapsed:  33080.73804140091\n",
      "ep 3619: ep_len:525 episode reward: total was 74.050000. running mean: 5.315800\n",
      "ep 3619: ep_len:523 episode reward: total was 114.180000. running mean: 6.404442\n",
      "ep 3619: ep_len:563 episode reward: total was 4.060000. running mean: 6.380998\n",
      "ep 3619: ep_len:364 episode reward: total was 13.190000. running mean: 6.449088\n",
      "ep 3619: ep_len:3 episode reward: total was 1.010000. running mean: 6.394697\n",
      "ep 3619: ep_len:500 episode reward: total was 58.160000. running mean: 6.912350\n",
      "ep 3619: ep_len:500 episode reward: total was 68.790000. running mean: 7.531126\n",
      "epsilon:0.039513 episode_count: 25340. steps_count: 10952446.000000\n",
      "Time elapsed:  33098.000561237335\n",
      "ep 3620: ep_len:649 episode reward: total was 58.850000. running mean: 8.044315\n",
      "ep 3620: ep_len:333 episode reward: total was -116.810000. running mean: 6.795772\n",
      "ep 3620: ep_len:641 episode reward: total was -1.870000. running mean: 6.709114\n",
      "ep 3620: ep_len:625 episode reward: total was 73.330000. running mean: 7.375323\n",
      "ep 3620: ep_len:3 episode reward: total was 1.010000. running mean: 7.311670\n",
      "ep 3620: ep_len:548 episode reward: total was 13.750000. running mean: 7.376053\n",
      "ep 3620: ep_len:500 episode reward: total was -54.940000. running mean: 6.752893\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.039469 episode_count: 25347. steps_count: 10955745.000000\n",
      "Time elapsed:  33112.950409173965\n",
      "ep 3621: ep_len:514 episode reward: total was 46.030000. running mean: 7.145664\n",
      "ep 3621: ep_len:573 episode reward: total was 125.370000. running mean: 8.327907\n",
      "ep 3621: ep_len:539 episode reward: total was 8.920000. running mean: 8.333828\n",
      "ep 3621: ep_len:528 episode reward: total was 5.080000. running mean: 8.301290\n",
      "ep 3621: ep_len:3 episode reward: total was 1.010000. running mean: 8.228377\n",
      "ep 3621: ep_len:644 episode reward: total was -106.870000. running mean: 7.077393\n",
      "ep 3621: ep_len:633 episode reward: total was 41.430000. running mean: 7.420919\n",
      "epsilon:0.039425 episode_count: 25354. steps_count: 10959179.000000\n",
      "Time elapsed:  33123.1699616909\n",
      "ep 3622: ep_len:585 episode reward: total was 9.000000. running mean: 7.436710\n",
      "ep 3622: ep_len:662 episode reward: total was 84.270000. running mean: 8.205043\n",
      "ep 3622: ep_len:425 episode reward: total was 50.150000. running mean: 8.624492\n",
      "ep 3622: ep_len:132 episode reward: total was 10.570000. running mean: 8.643947\n",
      "ep 3622: ep_len:117 episode reward: total was 21.860000. running mean: 8.776108\n",
      "ep 3622: ep_len:500 episode reward: total was -17.920000. running mean: 8.509147\n",
      "ep 3622: ep_len:591 episode reward: total was 19.520000. running mean: 8.619255\n",
      "epsilon:0.039380 episode_count: 25361. steps_count: 10962191.000000\n",
      "Time elapsed:  33131.233033418655\n",
      "ep 3623: ep_len:585 episode reward: total was 0.250000. running mean: 8.535563\n",
      "ep 3623: ep_len:201 episode reward: total was 5.940000. running mean: 8.509607\n",
      "ep 3623: ep_len:522 episode reward: total was -17.710000. running mean: 8.247411\n",
      "ep 3623: ep_len:526 episode reward: total was 35.310000. running mean: 8.518037\n",
      "ep 3623: ep_len:3 episode reward: total was 1.010000. running mean: 8.442957\n",
      "ep 3623: ep_len:500 episode reward: total was 61.980000. running mean: 8.978327\n",
      "ep 3623: ep_len:580 episode reward: total was 22.440000. running mean: 9.112944\n",
      "epsilon:0.039336 episode_count: 25368. steps_count: 10965108.000000\n",
      "Time elapsed:  33138.94523215294\n",
      "ep 3624: ep_len:553 episode reward: total was -33.600000. running mean: 8.685814\n",
      "ep 3624: ep_len:502 episode reward: total was -22.060000. running mean: 8.378356\n",
      "ep 3624: ep_len:615 episode reward: total was -8.640000. running mean: 8.208173\n",
      "ep 3624: ep_len:545 episode reward: total was 24.980000. running mean: 8.375891\n",
      "ep 3624: ep_len:47 episode reward: total was 20.010000. running mean: 8.492232\n",
      "ep 3624: ep_len:500 episode reward: total was 35.120000. running mean: 8.758510\n",
      "ep 3624: ep_len:596 episode reward: total was 42.610000. running mean: 9.097025\n",
      "epsilon:0.039292 episode_count: 25375. steps_count: 10968466.000000\n",
      "Time elapsed:  33147.72757577896\n",
      "ep 3625: ep_len:500 episode reward: total was 85.470000. running mean: 9.860754\n",
      "ep 3625: ep_len:587 episode reward: total was 14.110000. running mean: 9.903247\n",
      "ep 3625: ep_len:613 episode reward: total was -4.480000. running mean: 9.759414\n",
      "ep 3625: ep_len:500 episode reward: total was 31.890000. running mean: 9.980720\n",
      "ep 3625: ep_len:3 episode reward: total was 1.010000. running mean: 9.891013\n",
      "ep 3625: ep_len:604 episode reward: total was 18.380000. running mean: 9.975903\n",
      "ep 3625: ep_len:593 episode reward: total was 28.170000. running mean: 10.157844\n",
      "epsilon:0.039247 episode_count: 25382. steps_count: 10971866.000000\n",
      "Time elapsed:  33156.447976112366\n",
      "ep 3626: ep_len:626 episode reward: total was -136.760000. running mean: 8.688665\n",
      "ep 3626: ep_len:501 episode reward: total was 30.330000. running mean: 8.905079\n",
      "ep 3626: ep_len:628 episode reward: total was -14.610000. running mean: 8.669928\n",
      "ep 3626: ep_len:605 episode reward: total was 60.190000. running mean: 9.185129\n",
      "ep 3626: ep_len:86 episode reward: total was 26.630000. running mean: 9.359577\n",
      "ep 3626: ep_len:500 episode reward: total was 13.320000. running mean: 9.399182\n",
      "ep 3626: ep_len:541 episode reward: total was 29.110000. running mean: 9.596290\n",
      "epsilon:0.039203 episode_count: 25389. steps_count: 10975353.000000\n",
      "Time elapsed:  33172.5115916729\n",
      "ep 3627: ep_len:584 episode reward: total was 45.170000. running mean: 9.952027\n",
      "ep 3627: ep_len:510 episode reward: total was 59.520000. running mean: 10.447707\n",
      "ep 3627: ep_len:502 episode reward: total was -5.680000. running mean: 10.286430\n",
      "ep 3627: ep_len:500 episode reward: total was 22.740000. running mean: 10.410965\n",
      "ep 3627: ep_len:3 episode reward: total was 1.010000. running mean: 10.316956\n",
      "ep 3627: ep_len:267 episode reward: total was 26.860000. running mean: 10.482386\n",
      "ep 3627: ep_len:291 episode reward: total was -0.820000. running mean: 10.369362\n",
      "epsilon:0.039159 episode_count: 25396. steps_count: 10978010.000000\n",
      "Time elapsed:  33179.41641688347\n",
      "ep 3628: ep_len:536 episode reward: total was 33.330000. running mean: 10.598969\n",
      "ep 3628: ep_len:566 episode reward: total was 24.180000. running mean: 10.734779\n",
      "ep 3628: ep_len:527 episode reward: total was 42.500000. running mean: 11.052431\n",
      "ep 3628: ep_len:506 episode reward: total was -8.730000. running mean: 10.854607\n",
      "ep 3628: ep_len:88 episode reward: total was -54.300000. running mean: 10.203061\n",
      "ep 3628: ep_len:615 episode reward: total was -54.140000. running mean: 9.559630\n",
      "ep 3628: ep_len:500 episode reward: total was -30.890000. running mean: 9.155134\n",
      "epsilon:0.039114 episode_count: 25403. steps_count: 10981348.000000\n",
      "Time elapsed:  33188.215545892715\n",
      "ep 3629: ep_len:507 episode reward: total was 41.450000. running mean: 9.478082\n",
      "ep 3629: ep_len:608 episode reward: total was -12.720000. running mean: 9.256102\n",
      "ep 3629: ep_len:622 episode reward: total was 1.710000. running mean: 9.180641\n",
      "ep 3629: ep_len:500 episode reward: total was 1.270000. running mean: 9.101534\n",
      "ep 3629: ep_len:3 episode reward: total was 1.010000. running mean: 9.020619\n",
      "ep 3629: ep_len:526 episode reward: total was -30.910000. running mean: 8.621313\n",
      "ep 3629: ep_len:512 episode reward: total was 8.210000. running mean: 8.617200\n",
      "epsilon:0.039070 episode_count: 25410. steps_count: 10984626.000000\n",
      "Time elapsed:  33196.71169757843\n",
      "ep 3630: ep_len:566 episode reward: total was 85.090000. running mean: 9.381928\n",
      "ep 3630: ep_len:534 episode reward: total was 64.590000. running mean: 9.934008\n",
      "ep 3630: ep_len:591 episode reward: total was 7.810000. running mean: 9.912768\n",
      "ep 3630: ep_len:534 episode reward: total was 43.180000. running mean: 10.245441\n",
      "ep 3630: ep_len:3 episode reward: total was 1.010000. running mean: 10.153086\n",
      "ep 3630: ep_len:556 episode reward: total was -2.980000. running mean: 10.021755\n",
      "ep 3630: ep_len:602 episode reward: total was 10.050000. running mean: 10.022038\n",
      "epsilon:0.039026 episode_count: 25417. steps_count: 10988012.000000\n",
      "Time elapsed:  33205.10945606232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3631: ep_len:549 episode reward: total was 46.840000. running mean: 10.390217\n",
      "ep 3631: ep_len:527 episode reward: total was 45.210000. running mean: 10.738415\n",
      "ep 3631: ep_len:527 episode reward: total was -8.160000. running mean: 10.549431\n",
      "ep 3631: ep_len:521 episode reward: total was 54.630000. running mean: 10.990237\n",
      "ep 3631: ep_len:3 episode reward: total was 1.010000. running mean: 10.890434\n",
      "ep 3631: ep_len:628 episode reward: total was 7.010000. running mean: 10.851630\n",
      "ep 3631: ep_len:605 episode reward: total was 60.940000. running mean: 11.352514\n",
      "epsilon:0.038981 episode_count: 25424. steps_count: 10991372.000000\n",
      "Time elapsed:  33213.86981534958\n",
      "ep 3632: ep_len:199 episode reward: total was 22.730000. running mean: 11.466289\n",
      "ep 3632: ep_len:500 episode reward: total was -24.620000. running mean: 11.105426\n",
      "ep 3632: ep_len:641 episode reward: total was -7.930000. running mean: 10.915071\n",
      "ep 3632: ep_len:529 episode reward: total was 62.890000. running mean: 11.434821\n",
      "ep 3632: ep_len:3 episode reward: total was 1.010000. running mean: 11.330573\n",
      "ep 3632: ep_len:500 episode reward: total was 10.630000. running mean: 11.323567\n",
      "ep 3632: ep_len:576 episode reward: total was 29.030000. running mean: 11.500631\n",
      "epsilon:0.038937 episode_count: 25431. steps_count: 10994320.000000\n",
      "Time elapsed:  33221.611902952194\n",
      "ep 3633: ep_len:500 episode reward: total was -35.290000. running mean: 11.032725\n",
      "ep 3633: ep_len:500 episode reward: total was 49.530000. running mean: 11.417698\n",
      "ep 3633: ep_len:621 episode reward: total was 28.300000. running mean: 11.586521\n",
      "ep 3633: ep_len:615 episode reward: total was 71.810000. running mean: 12.188755\n",
      "ep 3633: ep_len:2 episode reward: total was -0.500000. running mean: 12.061868\n",
      "ep 3633: ep_len:298 episode reward: total was 9.690000. running mean: 12.038149\n",
      "ep 3633: ep_len:500 episode reward: total was 46.870000. running mean: 12.386468\n",
      "epsilon:0.038893 episode_count: 25438. steps_count: 10997356.000000\n",
      "Time elapsed:  33229.7583694458\n",
      "ep 3634: ep_len:639 episode reward: total was 44.950000. running mean: 12.712103\n",
      "ep 3634: ep_len:521 episode reward: total was 46.320000. running mean: 13.048182\n",
      "ep 3634: ep_len:62 episode reward: total was 8.780000. running mean: 13.005500\n",
      "ep 3634: ep_len:500 episode reward: total was 23.280000. running mean: 13.108245\n",
      "ep 3634: ep_len:92 episode reward: total was -32.240000. running mean: 12.654763\n",
      "ep 3634: ep_len:522 episode reward: total was 5.730000. running mean: 12.585515\n",
      "ep 3634: ep_len:284 episode reward: total was -13.040000. running mean: 12.329260\n",
      "epsilon:0.038848 episode_count: 25445. steps_count: 10999976.000000\n",
      "Time elapsed:  33236.84883928299\n",
      "ep 3635: ep_len:601 episode reward: total was 51.530000. running mean: 12.721267\n",
      "ep 3635: ep_len:539 episode reward: total was 20.910000. running mean: 12.803155\n",
      "ep 3635: ep_len:439 episode reward: total was 26.600000. running mean: 12.941123\n",
      "ep 3635: ep_len:500 episode reward: total was -11.480000. running mean: 12.696912\n",
      "ep 3635: ep_len:3 episode reward: total was 1.010000. running mean: 12.580043\n",
      "ep 3635: ep_len:500 episode reward: total was 34.020000. running mean: 12.794442\n",
      "ep 3635: ep_len:506 episode reward: total was -5.690000. running mean: 12.609598\n",
      "epsilon:0.038804 episode_count: 25452. steps_count: 11003064.000000\n",
      "Time elapsed:  33245.381216049194\n",
      "ep 3636: ep_len:657 episode reward: total was 8.110000. running mean: 12.564602\n",
      "ep 3636: ep_len:508 episode reward: total was 90.500000. running mean: 13.343956\n",
      "ep 3636: ep_len:500 episode reward: total was 39.210000. running mean: 13.602616\n",
      "ep 3636: ep_len:500 episode reward: total was 17.750000. running mean: 13.644090\n",
      "ep 3636: ep_len:3 episode reward: total was 1.010000. running mean: 13.517749\n",
      "ep 3636: ep_len:500 episode reward: total was 22.810000. running mean: 13.610672\n",
      "ep 3636: ep_len:508 episode reward: total was -1.690000. running mean: 13.457665\n",
      "epsilon:0.038760 episode_count: 25459. steps_count: 11006240.000000\n",
      "Time elapsed:  33253.700662612915\n",
      "ep 3637: ep_len:188 episode reward: total was 10.010000. running mean: 13.423188\n",
      "ep 3637: ep_len:574 episode reward: total was -34.570000. running mean: 12.943257\n",
      "ep 3637: ep_len:516 episode reward: total was 9.560000. running mean: 12.909424\n",
      "ep 3637: ep_len:501 episode reward: total was 7.260000. running mean: 12.852930\n",
      "ep 3637: ep_len:3 episode reward: total was 1.010000. running mean: 12.734500\n",
      "ep 3637: ep_len:500 episode reward: total was 35.600000. running mean: 12.963155\n",
      "ep 3637: ep_len:500 episode reward: total was 46.470000. running mean: 13.298224\n",
      "epsilon:0.038715 episode_count: 25466. steps_count: 11009022.000000\n",
      "Time elapsed:  33260.1744992733\n",
      "ep 3638: ep_len:577 episode reward: total was 60.660000. running mean: 13.771842\n",
      "ep 3638: ep_len:522 episode reward: total was -4.400000. running mean: 13.590123\n",
      "ep 3638: ep_len:623 episode reward: total was 7.720000. running mean: 13.531422\n",
      "ep 3638: ep_len:150 episode reward: total was 15.200000. running mean: 13.548108\n",
      "ep 3638: ep_len:3 episode reward: total was 1.010000. running mean: 13.422727\n",
      "ep 3638: ep_len:500 episode reward: total was 38.760000. running mean: 13.676099\n",
      "ep 3638: ep_len:500 episode reward: total was 56.370000. running mean: 14.103038\n",
      "epsilon:0.038671 episode_count: 25473. steps_count: 11011897.000000\n",
      "Time elapsed:  33268.25037050247\n",
      "ep 3639: ep_len:534 episode reward: total was -18.440000. running mean: 13.777608\n",
      "ep 3639: ep_len:646 episode reward: total was 66.090000. running mean: 14.300732\n",
      "ep 3639: ep_len:376 episode reward: total was 41.160000. running mean: 14.569325\n",
      "ep 3639: ep_len:500 episode reward: total was 18.990000. running mean: 14.613531\n",
      "ep 3639: ep_len:101 episode reward: total was 13.280000. running mean: 14.600196\n",
      "ep 3639: ep_len:562 episode reward: total was 25.700000. running mean: 14.711194\n",
      "ep 3639: ep_len:500 episode reward: total was 33.830000. running mean: 14.902382\n",
      "epsilon:0.038627 episode_count: 25480. steps_count: 11015116.000000\n",
      "Time elapsed:  33276.77958345413\n",
      "ep 3640: ep_len:194 episode reward: total was 6.130000. running mean: 14.814658\n",
      "ep 3640: ep_len:581 episode reward: total was -45.520000. running mean: 14.211312\n",
      "ep 3640: ep_len:581 episode reward: total was 13.870000. running mean: 14.207899\n",
      "ep 3640: ep_len:500 episode reward: total was 66.740000. running mean: 14.733220\n",
      "ep 3640: ep_len:47 episode reward: total was 20.500000. running mean: 14.790887\n",
      "ep 3640: ep_len:500 episode reward: total was 12.360000. running mean: 14.766579\n",
      "ep 3640: ep_len:613 episode reward: total was -59.400000. running mean: 14.024913\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.038582 episode_count: 25487. steps_count: 11018132.000000\n",
      "Time elapsed:  33289.488223314285\n",
      "ep 3641: ep_len:118 episode reward: total was 5.890000. running mean: 13.943564\n",
      "ep 3641: ep_len:500 episode reward: total was 50.590000. running mean: 14.310028\n",
      "ep 3641: ep_len:500 episode reward: total was 7.530000. running mean: 14.242228\n",
      "ep 3641: ep_len:500 episode reward: total was 82.130000. running mean: 14.921105\n",
      "ep 3641: ep_len:103 episode reward: total was 18.230000. running mean: 14.954194\n",
      "ep 3641: ep_len:551 episode reward: total was 43.250000. running mean: 15.237152\n",
      "ep 3641: ep_len:500 episode reward: total was 44.690000. running mean: 15.531681\n",
      "epsilon:0.038538 episode_count: 25494. steps_count: 11020904.000000\n",
      "Time elapsed:  33296.994282484055\n",
      "ep 3642: ep_len:597 episode reward: total was 84.460000. running mean: 16.220964\n",
      "ep 3642: ep_len:500 episode reward: total was 18.400000. running mean: 16.242754\n",
      "ep 3642: ep_len:617 episode reward: total was 30.020000. running mean: 16.380527\n",
      "ep 3642: ep_len:506 episode reward: total was 6.300000. running mean: 16.279722\n",
      "ep 3642: ep_len:3 episode reward: total was 1.010000. running mean: 16.127024\n",
      "ep 3642: ep_len:566 episode reward: total was 14.930000. running mean: 16.115054\n",
      "ep 3642: ep_len:556 episode reward: total was 2.100000. running mean: 15.974904\n",
      "epsilon:0.038494 episode_count: 25501. steps_count: 11024249.000000\n",
      "Time elapsed:  33312.27533698082\n",
      "ep 3643: ep_len:500 episode reward: total was -12.300000. running mean: 15.692155\n",
      "ep 3643: ep_len:500 episode reward: total was 24.480000. running mean: 15.780033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3643: ep_len:500 episode reward: total was 31.530000. running mean: 15.937533\n",
      "ep 3643: ep_len:509 episode reward: total was 38.810000. running mean: 16.166257\n",
      "ep 3643: ep_len:61 episode reward: total was 16.180000. running mean: 16.166395\n",
      "ep 3643: ep_len:186 episode reward: total was 32.290000. running mean: 16.327631\n",
      "ep 3643: ep_len:532 episode reward: total was 6.140000. running mean: 16.225755\n",
      "epsilon:0.038449 episode_count: 25508. steps_count: 11027037.000000\n",
      "Time elapsed:  33319.686342954636\n",
      "ep 3644: ep_len:113 episode reward: total was 11.480000. running mean: 16.178297\n",
      "ep 3644: ep_len:540 episode reward: total was 73.550000. running mean: 16.752014\n",
      "ep 3644: ep_len:506 episode reward: total was -78.900000. running mean: 15.795494\n",
      "ep 3644: ep_len:500 episode reward: total was 34.400000. running mean: 15.981539\n",
      "ep 3644: ep_len:128 episode reward: total was 21.880000. running mean: 16.040524\n",
      "ep 3644: ep_len:531 episode reward: total was 7.120000. running mean: 15.951318\n",
      "ep 3644: ep_len:558 episode reward: total was 24.010000. running mean: 16.031905\n",
      "epsilon:0.038405 episode_count: 25515. steps_count: 11029913.000000\n",
      "Time elapsed:  33327.33199596405\n",
      "ep 3645: ep_len:209 episode reward: total was 19.100000. running mean: 16.062586\n",
      "ep 3645: ep_len:599 episode reward: total was -13.200000. running mean: 15.769960\n",
      "ep 3645: ep_len:441 episode reward: total was -150.150000. running mean: 14.110761\n",
      "ep 3645: ep_len:502 episode reward: total was 20.490000. running mean: 14.174553\n",
      "ep 3645: ep_len:3 episode reward: total was 1.010000. running mean: 14.042908\n",
      "ep 3645: ep_len:640 episode reward: total was 11.560000. running mean: 14.018078\n",
      "ep 3645: ep_len:519 episode reward: total was 10.970000. running mean: 13.987598\n",
      "epsilon:0.038361 episode_count: 25522. steps_count: 11032826.000000\n",
      "Time elapsed:  33341.48474860191\n",
      "ep 3646: ep_len:134 episode reward: total was 13.980000. running mean: 13.987522\n",
      "ep 3646: ep_len:500 episode reward: total was -141.650000. running mean: 12.431146\n",
      "ep 3646: ep_len:557 episode reward: total was -8.620000. running mean: 12.220635\n",
      "ep 3646: ep_len:522 episode reward: total was -4.750000. running mean: 12.050929\n",
      "ep 3646: ep_len:3 episode reward: total was 1.010000. running mean: 11.940519\n",
      "ep 3646: ep_len:336 episode reward: total was 40.320000. running mean: 12.224314\n",
      "ep 3646: ep_len:500 episode reward: total was 20.210000. running mean: 12.304171\n",
      "epsilon:0.038316 episode_count: 25529. steps_count: 11035378.000000\n",
      "Time elapsed:  33346.74887466431\n",
      "ep 3647: ep_len:656 episode reward: total was -14.730000. running mean: 12.033829\n",
      "ep 3647: ep_len:500 episode reward: total was 26.450000. running mean: 12.177991\n",
      "ep 3647: ep_len:500 episode reward: total was 57.470000. running mean: 12.630911\n",
      "ep 3647: ep_len:56 episode reward: total was 0.330000. running mean: 12.507902\n",
      "ep 3647: ep_len:128 episode reward: total was 29.840000. running mean: 12.681223\n",
      "ep 3647: ep_len:554 episode reward: total was -35.780000. running mean: 12.196611\n",
      "ep 3647: ep_len:606 episode reward: total was 28.730000. running mean: 12.361945\n",
      "epsilon:0.038272 episode_count: 25536. steps_count: 11038378.000000\n",
      "Time elapsed:  33354.74543738365\n",
      "ep 3648: ep_len:554 episode reward: total was -127.950000. running mean: 10.958825\n",
      "ep 3648: ep_len:679 episode reward: total was 86.580000. running mean: 11.715037\n",
      "ep 3648: ep_len:500 episode reward: total was -88.230000. running mean: 10.715587\n",
      "ep 3648: ep_len:528 episode reward: total was 24.720000. running mean: 10.855631\n",
      "ep 3648: ep_len:3 episode reward: total was 1.010000. running mean: 10.757174\n",
      "ep 3648: ep_len:505 episode reward: total was 5.290000. running mean: 10.702503\n",
      "ep 3648: ep_len:535 episode reward: total was 9.480000. running mean: 10.690278\n",
      "epsilon:0.038228 episode_count: 25543. steps_count: 11041682.000000\n",
      "Time elapsed:  33374.72539615631\n",
      "ep 3649: ep_len:229 episode reward: total was 26.310000. running mean: 10.846475\n",
      "ep 3649: ep_len:296 episode reward: total was 4.410000. running mean: 10.782110\n",
      "ep 3649: ep_len:510 episode reward: total was -3.000000. running mean: 10.644289\n",
      "ep 3649: ep_len:156 episode reward: total was 11.730000. running mean: 10.655146\n",
      "ep 3649: ep_len:100 episode reward: total was 29.220000. running mean: 10.840795\n",
      "ep 3649: ep_len:504 episode reward: total was -16.820000. running mean: 10.564187\n",
      "ep 3649: ep_len:191 episode reward: total was -16.570000. running mean: 10.292845\n",
      "epsilon:0.038183 episode_count: 25550. steps_count: 11043668.000000\n",
      "Time elapsed:  33379.98635792732\n",
      "ep 3650: ep_len:500 episode reward: total was 68.210000. running mean: 10.872016\n",
      "ep 3650: ep_len:509 episode reward: total was 43.930000. running mean: 11.202596\n",
      "ep 3650: ep_len:631 episode reward: total was 17.710000. running mean: 11.267670\n",
      "ep 3650: ep_len:162 episode reward: total was 18.220000. running mean: 11.337194\n",
      "ep 3650: ep_len:3 episode reward: total was 1.010000. running mean: 11.233922\n",
      "ep 3650: ep_len:500 episode reward: total was 27.990000. running mean: 11.401482\n",
      "ep 3650: ep_len:552 episode reward: total was 42.180000. running mean: 11.709268\n",
      "epsilon:0.038139 episode_count: 25557. steps_count: 11046525.000000\n",
      "Time elapsed:  33385.575983047485\n",
      "ep 3651: ep_len:500 episode reward: total was 80.480000. running mean: 12.396975\n",
      "ep 3651: ep_len:549 episode reward: total was 37.870000. running mean: 12.651705\n",
      "ep 3651: ep_len:500 episode reward: total was 23.710000. running mean: 12.762288\n",
      "ep 3651: ep_len:500 episode reward: total was 70.770000. running mean: 13.342365\n",
      "ep 3651: ep_len:3 episode reward: total was -1.500000. running mean: 13.193942\n",
      "ep 3651: ep_len:572 episode reward: total was 54.270000. running mean: 13.604702\n",
      "ep 3651: ep_len:500 episode reward: total was 46.790000. running mean: 13.936555\n",
      "epsilon:0.038095 episode_count: 25564. steps_count: 11049649.000000\n",
      "Time elapsed:  33396.629474163055\n",
      "ep 3652: ep_len:603 episode reward: total was 63.240000. running mean: 14.429590\n",
      "ep 3652: ep_len:517 episode reward: total was 35.170000. running mean: 14.636994\n",
      "ep 3652: ep_len:595 episode reward: total was -1.570000. running mean: 14.474924\n",
      "ep 3652: ep_len:500 episode reward: total was 49.560000. running mean: 14.825775\n",
      "ep 3652: ep_len:98 episode reward: total was 26.260000. running mean: 14.940117\n",
      "ep 3652: ep_len:544 episode reward: total was -18.680000. running mean: 14.603916\n",
      "ep 3652: ep_len:602 episode reward: total was 58.100000. running mean: 15.038876\n",
      "epsilon:0.038050 episode_count: 25571. steps_count: 11053108.000000\n",
      "Time elapsed:  33405.74210691452\n",
      "ep 3653: ep_len:134 episode reward: total was 2.960000. running mean: 14.918088\n",
      "ep 3653: ep_len:621 episode reward: total was -52.980000. running mean: 14.239107\n",
      "ep 3653: ep_len:431 episode reward: total was 57.280000. running mean: 14.669516\n",
      "ep 3653: ep_len:502 episode reward: total was -7.670000. running mean: 14.446121\n",
      "ep 3653: ep_len:3 episode reward: total was -1.500000. running mean: 14.286659\n",
      "ep 3653: ep_len:618 episode reward: total was 10.460000. running mean: 14.248393\n",
      "ep 3653: ep_len:559 episode reward: total was 26.010000. running mean: 14.366009\n",
      "epsilon:0.038006 episode_count: 25578. steps_count: 11055976.000000\n",
      "Time elapsed:  33413.420362472534\n",
      "ep 3654: ep_len:594 episode reward: total was 32.110000. running mean: 14.543449\n",
      "ep 3654: ep_len:533 episode reward: total was 19.980000. running mean: 14.597814\n",
      "ep 3654: ep_len:445 episode reward: total was 46.920000. running mean: 14.921036\n",
      "ep 3654: ep_len:621 episode reward: total was 53.490000. running mean: 15.306726\n",
      "ep 3654: ep_len:3 episode reward: total was 1.010000. running mean: 15.163759\n",
      "ep 3654: ep_len:571 episode reward: total was -4.350000. running mean: 14.968621\n",
      "ep 3654: ep_len:570 episode reward: total was 26.000000. running mean: 15.078935\n",
      "epsilon:0.037962 episode_count: 25585. steps_count: 11059313.000000\n",
      "Time elapsed:  33422.15315270424\n",
      "ep 3655: ep_len:536 episode reward: total was -42.080000. running mean: 14.507345\n",
      "ep 3655: ep_len:636 episode reward: total was 75.280000. running mean: 15.115072\n",
      "ep 3655: ep_len:615 episode reward: total was 20.580000. running mean: 15.169721\n",
      "ep 3655: ep_len:518 episode reward: total was 74.040000. running mean: 15.758424\n",
      "ep 3655: ep_len:3 episode reward: total was 1.010000. running mean: 15.610940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3655: ep_len:615 episode reward: total was 29.610000. running mean: 15.750930\n",
      "ep 3655: ep_len:596 episode reward: total was 15.680000. running mean: 15.750221\n",
      "epsilon:0.037917 episode_count: 25592. steps_count: 11062832.000000\n",
      "Time elapsed:  33431.25374817848\n",
      "ep 3656: ep_len:539 episode reward: total was -28.630000. running mean: 15.306419\n",
      "ep 3656: ep_len:500 episode reward: total was 85.830000. running mean: 16.011655\n",
      "ep 3656: ep_len:607 episode reward: total was 21.070000. running mean: 16.062238\n",
      "ep 3656: ep_len:500 episode reward: total was -29.820000. running mean: 15.603416\n",
      "ep 3656: ep_len:3 episode reward: total was 1.010000. running mean: 15.457482\n",
      "ep 3656: ep_len:593 episode reward: total was 24.230000. running mean: 15.545207\n",
      "ep 3656: ep_len:179 episode reward: total was 3.760000. running mean: 15.427355\n",
      "epsilon:0.037873 episode_count: 25599. steps_count: 11065753.000000\n",
      "Time elapsed:  33438.97918987274\n",
      "ep 3657: ep_len:116 episode reward: total was -8.950000. running mean: 15.183581\n",
      "ep 3657: ep_len:500 episode reward: total was 88.280000. running mean: 15.914545\n",
      "ep 3657: ep_len:500 episode reward: total was 39.440000. running mean: 16.149800\n",
      "ep 3657: ep_len:533 episode reward: total was -29.970000. running mean: 15.688602\n",
      "ep 3657: ep_len:103 episode reward: total was 27.260000. running mean: 15.804316\n",
      "ep 3657: ep_len:613 episode reward: total was -15.030000. running mean: 15.495973\n",
      "ep 3657: ep_len:566 episode reward: total was -66.340000. running mean: 14.677613\n",
      "epsilon:0.037829 episode_count: 25606. steps_count: 11068684.000000\n",
      "Time elapsed:  33452.32105398178\n",
      "ep 3658: ep_len:613 episode reward: total was -33.740000. running mean: 14.193437\n",
      "ep 3658: ep_len:502 episode reward: total was 98.420000. running mean: 15.035702\n",
      "ep 3658: ep_len:550 episode reward: total was 35.700000. running mean: 15.242345\n",
      "ep 3658: ep_len:120 episode reward: total was 6.040000. running mean: 15.150322\n",
      "ep 3658: ep_len:3 episode reward: total was 1.010000. running mean: 15.008919\n",
      "ep 3658: ep_len:500 episode reward: total was 50.230000. running mean: 15.361130\n",
      "ep 3658: ep_len:500 episode reward: total was 33.870000. running mean: 15.546218\n",
      "epsilon:0.037784 episode_count: 25613. steps_count: 11071472.000000\n",
      "Time elapsed:  33459.884595394135\n",
      "ep 3659: ep_len:626 episode reward: total was -22.380000. running mean: 15.166956\n",
      "ep 3659: ep_len:500 episode reward: total was 58.790000. running mean: 15.603187\n",
      "ep 3659: ep_len:500 episode reward: total was -381.570000. running mean: 11.631455\n",
      "ep 3659: ep_len:396 episode reward: total was 8.520000. running mean: 11.600340\n",
      "ep 3659: ep_len:3 episode reward: total was 1.010000. running mean: 11.494437\n",
      "ep 3659: ep_len:565 episode reward: total was -28.270000. running mean: 11.096792\n",
      "ep 3659: ep_len:518 episode reward: total was 42.560000. running mean: 11.411424\n",
      "epsilon:0.037740 episode_count: 25620. steps_count: 11074580.000000\n",
      "Time elapsed:  33468.23475384712\n",
      "ep 3660: ep_len:126 episode reward: total was 14.050000. running mean: 11.437810\n",
      "ep 3660: ep_len:501 episode reward: total was 40.990000. running mean: 11.733332\n",
      "ep 3660: ep_len:639 episode reward: total was 22.840000. running mean: 11.844399\n",
      "ep 3660: ep_len:500 episode reward: total was 28.730000. running mean: 12.013255\n",
      "ep 3660: ep_len:97 episode reward: total was 23.220000. running mean: 12.125322\n",
      "ep 3660: ep_len:697 episode reward: total was 27.350000. running mean: 12.277569\n",
      "ep 3660: ep_len:559 episode reward: total was 21.610000. running mean: 12.370893\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.037696 episode_count: 25627. steps_count: 11077699.000000\n",
      "Time elapsed:  33481.79385471344\n",
      "ep 3661: ep_len:245 episode reward: total was 11.780000. running mean: 12.364984\n",
      "ep 3661: ep_len:500 episode reward: total was 55.090000. running mean: 12.792235\n",
      "ep 3661: ep_len:562 episode reward: total was -59.520000. running mean: 12.069112\n",
      "ep 3661: ep_len:509 episode reward: total was -8.430000. running mean: 11.864121\n",
      "ep 3661: ep_len:3 episode reward: total was 1.010000. running mean: 11.755580\n",
      "ep 3661: ep_len:500 episode reward: total was 18.990000. running mean: 11.827924\n",
      "ep 3661: ep_len:515 episode reward: total was -105.810000. running mean: 10.651545\n",
      "epsilon:0.037651 episode_count: 25634. steps_count: 11080533.000000\n",
      "Time elapsed:  33494.828664541245\n",
      "ep 3662: ep_len:525 episode reward: total was 71.800000. running mean: 11.263029\n",
      "ep 3662: ep_len:620 episode reward: total was 74.110000. running mean: 11.891499\n",
      "ep 3662: ep_len:500 episode reward: total was 21.560000. running mean: 11.988184\n",
      "ep 3662: ep_len:167 episode reward: total was 13.200000. running mean: 12.000302\n",
      "ep 3662: ep_len:134 episode reward: total was 25.340000. running mean: 12.133699\n",
      "ep 3662: ep_len:500 episode reward: total was -11.440000. running mean: 11.897962\n",
      "ep 3662: ep_len:633 episode reward: total was -116.340000. running mean: 10.615583\n",
      "epsilon:0.037607 episode_count: 25641. steps_count: 11083612.000000\n",
      "Time elapsed:  33503.10619997978\n",
      "ep 3663: ep_len:613 episode reward: total was -39.590000. running mean: 10.113527\n",
      "ep 3663: ep_len:500 episode reward: total was -2.720000. running mean: 9.985192\n",
      "ep 3663: ep_len:621 episode reward: total was 44.360000. running mean: 10.328940\n",
      "ep 3663: ep_len:500 episode reward: total was -40.310000. running mean: 9.822550\n",
      "ep 3663: ep_len:52 episode reward: total was 21.010000. running mean: 9.934425\n",
      "ep 3663: ep_len:500 episode reward: total was 28.350000. running mean: 10.118580\n",
      "ep 3663: ep_len:566 episode reward: total was 4.240000. running mean: 10.059795\n",
      "epsilon:0.037563 episode_count: 25648. steps_count: 11086964.000000\n",
      "Time elapsed:  33517.28923153877\n",
      "ep 3664: ep_len:206 episode reward: total was 13.620000. running mean: 10.095397\n",
      "ep 3664: ep_len:606 episode reward: total was 80.410000. running mean: 10.798543\n",
      "ep 3664: ep_len:617 episode reward: total was -38.120000. running mean: 10.309357\n",
      "ep 3664: ep_len:509 episode reward: total was 70.430000. running mean: 10.910564\n",
      "ep 3664: ep_len:2 episode reward: total was -0.500000. running mean: 10.796458\n",
      "ep 3664: ep_len:587 episode reward: total was 18.650000. running mean: 10.874994\n",
      "ep 3664: ep_len:504 episode reward: total was -24.810000. running mean: 10.518144\n",
      "epsilon:0.037518 episode_count: 25655. steps_count: 11089995.000000\n",
      "Time elapsed:  33530.92534327507\n",
      "ep 3665: ep_len:561 episode reward: total was 32.330000. running mean: 10.736262\n",
      "ep 3665: ep_len:500 episode reward: total was 111.110000. running mean: 11.740000\n",
      "ep 3665: ep_len:566 episode reward: total was 22.640000. running mean: 11.849000\n",
      "ep 3665: ep_len:516 episode reward: total was 52.360000. running mean: 12.254110\n",
      "ep 3665: ep_len:49 episode reward: total was 18.500000. running mean: 12.316568\n",
      "ep 3665: ep_len:535 episode reward: total was 5.790000. running mean: 12.251303\n",
      "ep 3665: ep_len:567 episode reward: total was 27.360000. running mean: 12.402390\n",
      "epsilon:0.037474 episode_count: 25662. steps_count: 11093289.000000\n",
      "Time elapsed:  33545.35842657089\n",
      "ep 3666: ep_len:557 episode reward: total was 77.800000. running mean: 13.056366\n",
      "ep 3666: ep_len:500 episode reward: total was 59.120000. running mean: 13.517002\n",
      "ep 3666: ep_len:558 episode reward: total was -3.960000. running mean: 13.342232\n",
      "ep 3666: ep_len:507 episode reward: total was 44.550000. running mean: 13.654310\n",
      "ep 3666: ep_len:80 episode reward: total was 22.130000. running mean: 13.739067\n",
      "ep 3666: ep_len:630 episode reward: total was 36.960000. running mean: 13.971276\n",
      "ep 3666: ep_len:610 episode reward: total was 27.360000. running mean: 14.105163\n",
      "epsilon:0.037430 episode_count: 25669. steps_count: 11096731.000000\n",
      "Time elapsed:  33554.3658926487\n",
      "ep 3667: ep_len:565 episode reward: total was 82.060000. running mean: 14.784712\n",
      "ep 3667: ep_len:520 episode reward: total was 18.510000. running mean: 14.821965\n",
      "ep 3667: ep_len:634 episode reward: total was -17.610000. running mean: 14.497645\n",
      "ep 3667: ep_len:532 episode reward: total was -2.070000. running mean: 14.331968\n",
      "ep 3667: ep_len:3 episode reward: total was 1.010000. running mean: 14.198749\n",
      "ep 3667: ep_len:500 episode reward: total was 22.350000. running mean: 14.280261\n",
      "ep 3667: ep_len:518 episode reward: total was -13.770000. running mean: 13.999759\n",
      "epsilon:0.037385 episode_count: 25676. steps_count: 11100003.000000\n",
      "Time elapsed:  33568.71830654144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3668: ep_len:536 episode reward: total was 33.850000. running mean: 14.198261\n",
      "ep 3668: ep_len:556 episode reward: total was 96.700000. running mean: 15.023278\n",
      "ep 3668: ep_len:557 episode reward: total was 11.110000. running mean: 14.984146\n",
      "ep 3668: ep_len:113 episode reward: total was 13.060000. running mean: 14.964904\n",
      "ep 3668: ep_len:48 episode reward: total was 21.000000. running mean: 15.025255\n",
      "ep 3668: ep_len:713 episode reward: total was 15.670000. running mean: 15.031703\n",
      "ep 3668: ep_len:547 episode reward: total was 31.580000. running mean: 15.197186\n",
      "epsilon:0.037341 episode_count: 25683. steps_count: 11103073.000000\n",
      "Time elapsed:  33580.036860227585\n",
      "ep 3669: ep_len:532 episode reward: total was -20.510000. running mean: 14.840114\n",
      "ep 3669: ep_len:531 episode reward: total was 9.680000. running mean: 14.788513\n",
      "ep 3669: ep_len:624 episode reward: total was -26.140000. running mean: 14.379228\n",
      "ep 3669: ep_len:534 episode reward: total was 51.510000. running mean: 14.750535\n",
      "ep 3669: ep_len:3 episode reward: total was 1.010000. running mean: 14.613130\n",
      "ep 3669: ep_len:327 episode reward: total was 26.700000. running mean: 14.733999\n",
      "ep 3669: ep_len:330 episode reward: total was 4.480000. running mean: 14.631459\n",
      "epsilon:0.037297 episode_count: 25690. steps_count: 11105954.000000\n",
      "Time elapsed:  33584.504749298096\n",
      "ep 3670: ep_len:570 episode reward: total was -19.810000. running mean: 14.287044\n",
      "ep 3670: ep_len:500 episode reward: total was 48.450000. running mean: 14.628674\n",
      "ep 3670: ep_len:572 episode reward: total was 1.860000. running mean: 14.500987\n",
      "ep 3670: ep_len:608 episode reward: total was 97.490000. running mean: 15.330877\n",
      "ep 3670: ep_len:3 episode reward: total was 1.010000. running mean: 15.187668\n",
      "ep 3670: ep_len:666 episode reward: total was 8.240000. running mean: 15.118192\n",
      "ep 3670: ep_len:600 episode reward: total was -39.500000. running mean: 14.572010\n",
      "epsilon:0.037252 episode_count: 25697. steps_count: 11109473.000000\n",
      "Time elapsed:  33591.32086896896\n",
      "ep 3671: ep_len:216 episode reward: total was 29.240000. running mean: 14.718690\n",
      "ep 3671: ep_len:561 episode reward: total was 53.900000. running mean: 15.110503\n",
      "ep 3671: ep_len:614 episode reward: total was 34.210000. running mean: 15.301498\n",
      "ep 3671: ep_len:124 episode reward: total was 12.130000. running mean: 15.269783\n",
      "ep 3671: ep_len:3 episode reward: total was 1.010000. running mean: 15.127185\n",
      "ep 3671: ep_len:590 episode reward: total was 10.730000. running mean: 15.083213\n",
      "ep 3671: ep_len:536 episode reward: total was 13.990000. running mean: 15.072281\n",
      "epsilon:0.037208 episode_count: 25704. steps_count: 11112117.000000\n",
      "Time elapsed:  33599.966633081436\n",
      "ep 3672: ep_len:612 episode reward: total was 18.630000. running mean: 15.107858\n",
      "ep 3672: ep_len:500 episode reward: total was 47.630000. running mean: 15.433079\n",
      "ep 3672: ep_len:512 episode reward: total was 43.940000. running mean: 15.718149\n",
      "ep 3672: ep_len:152 episode reward: total was 23.160000. running mean: 15.792567\n",
      "ep 3672: ep_len:89 episode reward: total was 25.770000. running mean: 15.892341\n",
      "ep 3672: ep_len:519 episode reward: total was -59.570000. running mean: 15.137718\n",
      "ep 3672: ep_len:500 episode reward: total was 62.600000. running mean: 15.612341\n",
      "epsilon:0.037164 episode_count: 25711. steps_count: 11115001.000000\n",
      "Time elapsed:  33605.33155012131\n",
      "ep 3673: ep_len:534 episode reward: total was -38.320000. running mean: 15.073017\n",
      "ep 3673: ep_len:515 episode reward: total was 7.400000. running mean: 14.996287\n",
      "ep 3673: ep_len:617 episode reward: total was 6.430000. running mean: 14.910624\n",
      "ep 3673: ep_len:511 episode reward: total was 41.890000. running mean: 15.180418\n",
      "ep 3673: ep_len:93 episode reward: total was -13.220000. running mean: 14.896414\n",
      "ep 3673: ep_len:500 episode reward: total was -19.860000. running mean: 14.548850\n",
      "ep 3673: ep_len:559 episode reward: total was 38.450000. running mean: 14.787861\n",
      "epsilon:0.037119 episode_count: 25718. steps_count: 11118330.000000\n",
      "Time elapsed:  33614.08697247505\n",
      "ep 3674: ep_len:118 episode reward: total was 9.500000. running mean: 14.734983\n",
      "ep 3674: ep_len:542 episode reward: total was 93.430000. running mean: 15.521933\n",
      "ep 3674: ep_len:500 episode reward: total was 51.900000. running mean: 15.885714\n",
      "ep 3674: ep_len:500 episode reward: total was 84.980000. running mean: 16.576656\n",
      "ep 3674: ep_len:3 episode reward: total was 1.010000. running mean: 16.420990\n",
      "ep 3674: ep_len:500 episode reward: total was 3.500000. running mean: 16.291780\n",
      "ep 3674: ep_len:346 episode reward: total was 10.700000. running mean: 16.235862\n",
      "epsilon:0.037075 episode_count: 25725. steps_count: 11120839.000000\n",
      "Time elapsed:  33620.88074731827\n",
      "ep 3675: ep_len:591 episode reward: total was -50.390000. running mean: 15.569604\n",
      "ep 3675: ep_len:525 episode reward: total was 105.730000. running mean: 16.471208\n",
      "ep 3675: ep_len:522 episode reward: total was -58.810000. running mean: 15.718395\n",
      "ep 3675: ep_len:500 episode reward: total was 40.620000. running mean: 15.967411\n",
      "ep 3675: ep_len:3 episode reward: total was 1.010000. running mean: 15.817837\n",
      "ep 3675: ep_len:511 episode reward: total was -28.750000. running mean: 15.372159\n",
      "ep 3675: ep_len:553 episode reward: total was 9.360000. running mean: 15.312037\n",
      "epsilon:0.037031 episode_count: 25732. steps_count: 11124044.000000\n",
      "Time elapsed:  33629.41317844391\n",
      "ep 3676: ep_len:656 episode reward: total was -12.160000. running mean: 15.037317\n",
      "ep 3676: ep_len:535 episode reward: total was 55.080000. running mean: 15.437744\n",
      "ep 3676: ep_len:526 episode reward: total was -33.380000. running mean: 14.949566\n",
      "ep 3676: ep_len:500 episode reward: total was 36.950000. running mean: 15.169571\n",
      "ep 3676: ep_len:86 episode reward: total was 26.260000. running mean: 15.280475\n",
      "ep 3676: ep_len:529 episode reward: total was 25.250000. running mean: 15.380170\n",
      "ep 3676: ep_len:521 episode reward: total was 4.900000. running mean: 15.275369\n",
      "epsilon:0.036986 episode_count: 25739. steps_count: 11127397.000000\n",
      "Time elapsed:  33643.93365597725\n",
      "ep 3677: ep_len:545 episode reward: total was 52.100000. running mean: 15.643615\n",
      "ep 3677: ep_len:544 episode reward: total was 110.320000. running mean: 16.590379\n",
      "ep 3677: ep_len:619 episode reward: total was -100.820000. running mean: 15.416275\n",
      "ep 3677: ep_len:56 episode reward: total was -5.650000. running mean: 15.205612\n",
      "ep 3677: ep_len:91 episode reward: total was 12.670000. running mean: 15.180256\n",
      "ep 3677: ep_len:500 episode reward: total was 73.420000. running mean: 15.762654\n",
      "ep 3677: ep_len:526 episode reward: total was 12.660000. running mean: 15.731627\n",
      "epsilon:0.036942 episode_count: 25746. steps_count: 11130278.000000\n",
      "Time elapsed:  33657.48438191414\n",
      "ep 3678: ep_len:127 episode reward: total was 11.520000. running mean: 15.689511\n",
      "ep 3678: ep_len:500 episode reward: total was 42.410000. running mean: 15.956716\n",
      "ep 3678: ep_len:551 episode reward: total was 3.790000. running mean: 15.835048\n",
      "ep 3678: ep_len:510 episode reward: total was 13.980000. running mean: 15.816498\n",
      "ep 3678: ep_len:3 episode reward: total was 1.010000. running mean: 15.668433\n",
      "ep 3678: ep_len:500 episode reward: total was 28.470000. running mean: 15.796449\n",
      "ep 3678: ep_len:521 episode reward: total was 44.180000. running mean: 16.080284\n",
      "epsilon:0.036898 episode_count: 25753. steps_count: 11132990.000000\n",
      "Time elapsed:  33664.845527648926\n",
      "ep 3679: ep_len:500 episode reward: total was 63.920000. running mean: 16.558681\n",
      "ep 3679: ep_len:554 episode reward: total was 62.820000. running mean: 17.021295\n",
      "ep 3679: ep_len:79 episode reward: total was 9.320000. running mean: 16.944282\n",
      "ep 3679: ep_len:555 episode reward: total was 93.130000. running mean: 17.706139\n",
      "ep 3679: ep_len:101 episode reward: total was 28.770000. running mean: 17.816777\n",
      "ep 3679: ep_len:169 episode reward: total was 32.790000. running mean: 17.966510\n",
      "ep 3679: ep_len:528 episode reward: total was 52.180000. running mean: 18.308645\n",
      "epsilon:0.036853 episode_count: 25760. steps_count: 11135476.000000\n",
      "Time elapsed:  33671.537606954575\n",
      "ep 3680: ep_len:211 episode reward: total was 16.270000. running mean: 18.288258\n",
      "ep 3680: ep_len:515 episode reward: total was 3.290000. running mean: 18.138275\n",
      "ep 3680: ep_len:545 episode reward: total was 22.920000. running mean: 18.186093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3680: ep_len:500 episode reward: total was 17.450000. running mean: 18.178732\n",
      "ep 3680: ep_len:3 episode reward: total was 1.010000. running mean: 18.007044\n",
      "ep 3680: ep_len:537 episode reward: total was -59.300000. running mean: 17.233974\n",
      "ep 3680: ep_len:524 episode reward: total was -8.660000. running mean: 16.975034\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.036809 episode_count: 25767. steps_count: 11138311.000000\n",
      "Time elapsed:  33684.36217594147\n",
      "ep 3681: ep_len:562 episode reward: total was 23.150000. running mean: 17.036784\n",
      "ep 3681: ep_len:500 episode reward: total was 46.920000. running mean: 17.335616\n",
      "ep 3681: ep_len:623 episode reward: total was 15.640000. running mean: 17.318660\n",
      "ep 3681: ep_len:142 episode reward: total was 10.010000. running mean: 17.245573\n",
      "ep 3681: ep_len:3 episode reward: total was 1.010000. running mean: 17.083218\n",
      "ep 3681: ep_len:515 episode reward: total was 7.370000. running mean: 16.986085\n",
      "ep 3681: ep_len:546 episode reward: total was 30.890000. running mean: 17.125125\n",
      "epsilon:0.036765 episode_count: 25774. steps_count: 11141202.000000\n",
      "Time elapsed:  33692.327038526535\n",
      "ep 3682: ep_len:587 episode reward: total was -181.810000. running mean: 15.135773\n",
      "ep 3682: ep_len:543 episode reward: total was -8.620000. running mean: 14.898216\n",
      "ep 3682: ep_len:500 episode reward: total was 15.760000. running mean: 14.906833\n",
      "ep 3682: ep_len:527 episode reward: total was 12.840000. running mean: 14.886165\n",
      "ep 3682: ep_len:3 episode reward: total was 1.010000. running mean: 14.747403\n",
      "ep 3682: ep_len:516 episode reward: total was -2.090000. running mean: 14.579029\n",
      "ep 3682: ep_len:600 episode reward: total was 30.200000. running mean: 14.735239\n",
      "epsilon:0.036720 episode_count: 25781. steps_count: 11144478.000000\n",
      "Time elapsed:  33701.026586294174\n",
      "ep 3683: ep_len:555 episode reward: total was 22.740000. running mean: 14.815287\n",
      "ep 3683: ep_len:510 episode reward: total was -23.490000. running mean: 14.432234\n",
      "ep 3683: ep_len:515 episode reward: total was 48.630000. running mean: 14.774212\n",
      "ep 3683: ep_len:534 episode reward: total was 51.620000. running mean: 15.142669\n",
      "ep 3683: ep_len:3 episode reward: total was 1.010000. running mean: 15.001343\n",
      "ep 3683: ep_len:500 episode reward: total was 52.090000. running mean: 15.372229\n",
      "ep 3683: ep_len:627 episode reward: total was 51.200000. running mean: 15.730507\n",
      "epsilon:0.036676 episode_count: 25788. steps_count: 11147722.000000\n",
      "Time elapsed:  33709.53083753586\n",
      "ep 3684: ep_len:199 episode reward: total was 8.210000. running mean: 15.655302\n",
      "ep 3684: ep_len:544 episode reward: total was -151.290000. running mean: 13.985849\n",
      "ep 3684: ep_len:587 episode reward: total was -41.040000. running mean: 13.435590\n",
      "ep 3684: ep_len:500 episode reward: total was 0.900000. running mean: 13.310235\n",
      "ep 3684: ep_len:92 episode reward: total was 24.760000. running mean: 13.424732\n",
      "ep 3684: ep_len:562 episode reward: total was -12.110000. running mean: 13.169385\n",
      "ep 3684: ep_len:341 episode reward: total was 26.080000. running mean: 13.298491\n",
      "epsilon:0.036632 episode_count: 25795. steps_count: 11150547.000000\n",
      "Time elapsed:  33717.11638188362\n",
      "ep 3685: ep_len:630 episode reward: total was 14.610000. running mean: 13.311606\n",
      "ep 3685: ep_len:500 episode reward: total was -71.260000. running mean: 12.465890\n",
      "ep 3685: ep_len:539 episode reward: total was -20.480000. running mean: 12.136431\n",
      "ep 3685: ep_len:549 episode reward: total was 75.390000. running mean: 12.768967\n",
      "ep 3685: ep_len:106 episode reward: total was 34.760000. running mean: 12.988877\n",
      "ep 3685: ep_len:573 episode reward: total was 20.290000. running mean: 13.061888\n",
      "ep 3685: ep_len:582 episode reward: total was 57.330000. running mean: 13.504570\n",
      "epsilon:0.036587 episode_count: 25802. steps_count: 11154026.000000\n",
      "Time elapsed:  33726.21690273285\n",
      "ep 3686: ep_len:217 episode reward: total was 7.330000. running mean: 13.442824\n",
      "ep 3686: ep_len:658 episode reward: total was 87.250000. running mean: 14.180896\n",
      "ep 3686: ep_len:577 episode reward: total was -17.450000. running mean: 13.864587\n",
      "ep 3686: ep_len:500 episode reward: total was 59.260000. running mean: 14.318541\n",
      "ep 3686: ep_len:3 episode reward: total was -0.490000. running mean: 14.170455\n",
      "ep 3686: ep_len:555 episode reward: total was 39.530000. running mean: 14.424051\n",
      "ep 3686: ep_len:547 episode reward: total was 19.570000. running mean: 14.475510\n",
      "epsilon:0.036543 episode_count: 25809. steps_count: 11157083.000000\n",
      "Time elapsed:  33732.21023416519\n",
      "ep 3687: ep_len:556 episode reward: total was 61.780000. running mean: 14.948555\n",
      "ep 3687: ep_len:303 episode reward: total was 8.730000. running mean: 14.886370\n",
      "ep 3687: ep_len:585 episode reward: total was 37.810000. running mean: 15.115606\n",
      "ep 3687: ep_len:518 episode reward: total was 12.700000. running mean: 15.091450\n",
      "ep 3687: ep_len:3 episode reward: total was 1.010000. running mean: 14.950635\n",
      "ep 3687: ep_len:504 episode reward: total was 23.000000. running mean: 15.031129\n",
      "ep 3687: ep_len:560 episode reward: total was 46.030000. running mean: 15.341118\n",
      "epsilon:0.036499 episode_count: 25816. steps_count: 11160112.000000\n",
      "Time elapsed:  33740.30779528618\n",
      "ep 3688: ep_len:567 episode reward: total was 70.040000. running mean: 15.888107\n",
      "ep 3688: ep_len:500 episode reward: total was 1.980000. running mean: 15.749025\n",
      "ep 3688: ep_len:640 episode reward: total was 14.230000. running mean: 15.733835\n",
      "ep 3688: ep_len:125 episode reward: total was 13.470000. running mean: 15.711197\n",
      "ep 3688: ep_len:3 episode reward: total was 1.010000. running mean: 15.564185\n",
      "ep 3688: ep_len:500 episode reward: total was 54.920000. running mean: 15.957743\n",
      "ep 3688: ep_len:589 episode reward: total was 20.230000. running mean: 16.000466\n",
      "epsilon:0.036454 episode_count: 25823. steps_count: 11163036.000000\n",
      "Time elapsed:  33747.97915768623\n",
      "ep 3689: ep_len:507 episode reward: total was -25.770000. running mean: 15.582761\n",
      "ep 3689: ep_len:383 episode reward: total was 2.450000. running mean: 15.451433\n",
      "ep 3689: ep_len:624 episode reward: total was 12.280000. running mean: 15.419719\n",
      "ep 3689: ep_len:599 episode reward: total was 29.670000. running mean: 15.562222\n",
      "ep 3689: ep_len:3 episode reward: total was 1.010000. running mean: 15.416700\n",
      "ep 3689: ep_len:500 episode reward: total was 1.650000. running mean: 15.279033\n",
      "ep 3689: ep_len:500 episode reward: total was -39.960000. running mean: 14.726642\n",
      "epsilon:0.036410 episode_count: 25830. steps_count: 11166152.000000\n",
      "Time elapsed:  33765.96289515495\n",
      "ep 3690: ep_len:228 episode reward: total was 11.340000. running mean: 14.692776\n",
      "ep 3690: ep_len:500 episode reward: total was 99.160000. running mean: 15.537448\n",
      "ep 3690: ep_len:589 episode reward: total was -45.980000. running mean: 14.922274\n",
      "ep 3690: ep_len:527 episode reward: total was -65.170000. running mean: 14.121351\n",
      "ep 3690: ep_len:53 episode reward: total was 22.000000. running mean: 14.200137\n",
      "ep 3690: ep_len:500 episode reward: total was 45.950000. running mean: 14.517636\n",
      "ep 3690: ep_len:592 episode reward: total was -9.550000. running mean: 14.276960\n",
      "epsilon:0.036366 episode_count: 25837. steps_count: 11169141.000000\n",
      "Time elapsed:  33779.95026874542\n",
      "ep 3691: ep_len:230 episode reward: total was 7.080000. running mean: 14.204990\n",
      "ep 3691: ep_len:574 episode reward: total was 55.230000. running mean: 14.615240\n",
      "ep 3691: ep_len:621 episode reward: total was 51.090000. running mean: 14.979988\n",
      "ep 3691: ep_len:520 episode reward: total was 2.920000. running mean: 14.859388\n",
      "ep 3691: ep_len:3 episode reward: total was 1.010000. running mean: 14.720894\n",
      "ep 3691: ep_len:516 episode reward: total was 2.330000. running mean: 14.596985\n",
      "ep 3691: ep_len:530 episode reward: total was -40.800000. running mean: 14.043015\n",
      "epsilon:0.036321 episode_count: 25844. steps_count: 11172135.000000\n",
      "Time elapsed:  33788.043177604675\n",
      "ep 3692: ep_len:649 episode reward: total was -49.910000. running mean: 13.403485\n",
      "ep 3692: ep_len:531 episode reward: total was 31.650000. running mean: 13.585950\n",
      "ep 3692: ep_len:655 episode reward: total was -42.500000. running mean: 13.025091\n",
      "ep 3692: ep_len:500 episode reward: total was 23.890000. running mean: 13.133740\n",
      "ep 3692: ep_len:102 episode reward: total was 28.750000. running mean: 13.289902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3692: ep_len:501 episode reward: total was -19.740000. running mean: 12.959603\n",
      "ep 3692: ep_len:531 episode reward: total was 40.220000. running mean: 13.232207\n",
      "epsilon:0.036277 episode_count: 25851. steps_count: 11175604.000000\n",
      "Time elapsed:  33796.98792338371\n",
      "ep 3693: ep_len:543 episode reward: total was -75.340000. running mean: 12.346485\n",
      "ep 3693: ep_len:500 episode reward: total was 37.790000. running mean: 12.600920\n",
      "ep 3693: ep_len:637 episode reward: total was -10.630000. running mean: 12.368611\n",
      "ep 3693: ep_len:501 episode reward: total was 43.870000. running mean: 12.683625\n",
      "ep 3693: ep_len:3 episode reward: total was 1.010000. running mean: 12.566889\n",
      "ep 3693: ep_len:548 episode reward: total was -1.720000. running mean: 12.424020\n",
      "ep 3693: ep_len:500 episode reward: total was 13.060000. running mean: 12.430380\n",
      "epsilon:0.036233 episode_count: 25858. steps_count: 11178836.000000\n",
      "Time elapsed:  33805.55420041084\n",
      "ep 3694: ep_len:209 episode reward: total was 15.730000. running mean: 12.463376\n",
      "ep 3694: ep_len:500 episode reward: total was 11.650000. running mean: 12.455242\n",
      "ep 3694: ep_len:623 episode reward: total was 32.710000. running mean: 12.657790\n",
      "ep 3694: ep_len:531 episode reward: total was 27.600000. running mean: 12.807212\n",
      "ep 3694: ep_len:106 episode reward: total was 12.750000. running mean: 12.806640\n",
      "ep 3694: ep_len:181 episode reward: total was 18.130000. running mean: 12.859873\n",
      "ep 3694: ep_len:631 episode reward: total was 48.620000. running mean: 13.217475\n",
      "epsilon:0.036188 episode_count: 25865. steps_count: 11181617.000000\n",
      "Time elapsed:  33813.04178762436\n",
      "ep 3695: ep_len:264 episode reward: total was 23.400000. running mean: 13.319300\n",
      "ep 3695: ep_len:500 episode reward: total was 16.290000. running mean: 13.349007\n",
      "ep 3695: ep_len:351 episode reward: total was 31.360000. running mean: 13.529117\n",
      "ep 3695: ep_len:500 episode reward: total was 64.560000. running mean: 14.039426\n",
      "ep 3695: ep_len:3 episode reward: total was 1.010000. running mean: 13.909131\n",
      "ep 3695: ep_len:548 episode reward: total was 48.190000. running mean: 14.251940\n",
      "ep 3695: ep_len:519 episode reward: total was 1.080000. running mean: 14.120221\n",
      "epsilon:0.036144 episode_count: 25872. steps_count: 11184302.000000\n",
      "Time elapsed:  33820.18688249588\n",
      "ep 3696: ep_len:240 episode reward: total was 20.250000. running mean: 14.181518\n",
      "ep 3696: ep_len:528 episode reward: total was 50.920000. running mean: 14.548903\n",
      "ep 3696: ep_len:509 episode reward: total was 36.540000. running mean: 14.768814\n",
      "ep 3696: ep_len:591 episode reward: total was 97.110000. running mean: 15.592226\n",
      "ep 3696: ep_len:3 episode reward: total was 1.010000. running mean: 15.446404\n",
      "ep 3696: ep_len:537 episode reward: total was 11.240000. running mean: 15.404340\n",
      "ep 3696: ep_len:211 episode reward: total was -20.990000. running mean: 15.040396\n",
      "epsilon:0.036100 episode_count: 25879. steps_count: 11186921.000000\n",
      "Time elapsed:  33827.27292966843\n",
      "ep 3697: ep_len:500 episode reward: total was 55.570000. running mean: 15.445692\n",
      "ep 3697: ep_len:279 episode reward: total was 11.550000. running mean: 15.406736\n",
      "ep 3697: ep_len:553 episode reward: total was -18.780000. running mean: 15.064868\n",
      "ep 3697: ep_len:543 episode reward: total was 42.760000. running mean: 15.341819\n",
      "ep 3697: ep_len:3 episode reward: total was 1.010000. running mean: 15.198501\n",
      "ep 3697: ep_len:534 episode reward: total was 26.180000. running mean: 15.308316\n",
      "ep 3697: ep_len:532 episode reward: total was 16.920000. running mean: 15.324433\n",
      "epsilon:0.036055 episode_count: 25886. steps_count: 11189865.000000\n",
      "Time elapsed:  33841.005601882935\n",
      "ep 3698: ep_len:575 episode reward: total was 61.000000. running mean: 15.781189\n",
      "ep 3698: ep_len:500 episode reward: total was -2.160000. running mean: 15.601777\n",
      "ep 3698: ep_len:685 episode reward: total was -5.030000. running mean: 15.395459\n",
      "ep 3698: ep_len:38 episode reward: total was 0.670000. running mean: 15.248205\n",
      "ep 3698: ep_len:128 episode reward: total was 26.840000. running mean: 15.364122\n",
      "ep 3698: ep_len:553 episode reward: total was 40.910000. running mean: 15.619581\n",
      "ep 3698: ep_len:500 episode reward: total was 28.050000. running mean: 15.743885\n",
      "epsilon:0.036011 episode_count: 25893. steps_count: 11192844.000000\n",
      "Time elapsed:  33846.4073240757\n",
      "ep 3699: ep_len:562 episode reward: total was -24.360000. running mean: 15.342847\n",
      "ep 3699: ep_len:500 episode reward: total was -4.370000. running mean: 15.145718\n",
      "ep 3699: ep_len:600 episode reward: total was 55.750000. running mean: 15.551761\n",
      "ep 3699: ep_len:607 episode reward: total was 58.980000. running mean: 15.986043\n",
      "ep 3699: ep_len:55 episode reward: total was 24.010000. running mean: 16.066283\n",
      "ep 3699: ep_len:500 episode reward: total was 22.790000. running mean: 16.133520\n",
      "ep 3699: ep_len:525 episode reward: total was -4.950000. running mean: 15.922685\n",
      "epsilon:0.035967 episode_count: 25900. steps_count: 11196193.000000\n",
      "Time elapsed:  33851.50210785866\n",
      "ep 3700: ep_len:179 episode reward: total was 9.610000. running mean: 15.859558\n",
      "ep 3700: ep_len:368 episode reward: total was -7.130000. running mean: 15.629662\n",
      "ep 3700: ep_len:521 episode reward: total was -80.060000. running mean: 14.672766\n",
      "ep 3700: ep_len:623 episode reward: total was 71.230000. running mean: 15.238338\n",
      "ep 3700: ep_len:3 episode reward: total was 1.010000. running mean: 15.096055\n",
      "ep 3700: ep_len:614 episode reward: total was 65.400000. running mean: 15.599094\n",
      "ep 3700: ep_len:500 episode reward: total was 20.990000. running mean: 15.653003\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.035922 episode_count: 25907. steps_count: 11199001.000000\n",
      "Time elapsed:  33859.51101374626\n",
      "ep 3701: ep_len:699 episode reward: total was 24.790000. running mean: 15.744373\n",
      "ep 3701: ep_len:500 episode reward: total was 127.230000. running mean: 16.859230\n",
      "ep 3701: ep_len:636 episode reward: total was 19.510000. running mean: 16.885737\n",
      "ep 3701: ep_len:500 episode reward: total was 21.270000. running mean: 16.929580\n",
      "ep 3701: ep_len:92 episode reward: total was 21.760000. running mean: 16.977884\n",
      "ep 3701: ep_len:677 episode reward: total was 24.970000. running mean: 17.057805\n",
      "ep 3701: ep_len:599 episode reward: total was 13.680000. running mean: 17.024027\n",
      "epsilon:0.035878 episode_count: 25914. steps_count: 11202704.000000\n",
      "Time elapsed:  33891.12876868248\n",
      "ep 3702: ep_len:619 episode reward: total was 52.050000. running mean: 17.374287\n",
      "ep 3702: ep_len:546 episode reward: total was 50.900000. running mean: 17.709544\n",
      "ep 3702: ep_len:538 episode reward: total was 2.990000. running mean: 17.562349\n",
      "ep 3702: ep_len:170 episode reward: total was 31.220000. running mean: 17.698925\n",
      "ep 3702: ep_len:73 episode reward: total was -42.830000. running mean: 17.093636\n",
      "ep 3702: ep_len:531 episode reward: total was 59.080000. running mean: 17.513499\n",
      "ep 3702: ep_len:625 episode reward: total was 14.130000. running mean: 17.479665\n",
      "epsilon:0.035834 episode_count: 25921. steps_count: 11205806.000000\n",
      "Time elapsed:  33898.77033448219\n",
      "ep 3703: ep_len:513 episode reward: total was -37.900000. running mean: 16.925868\n",
      "ep 3703: ep_len:500 episode reward: total was 23.930000. running mean: 16.995909\n",
      "ep 3703: ep_len:428 episode reward: total was 38.670000. running mean: 17.212650\n",
      "ep 3703: ep_len:512 episode reward: total was 22.720000. running mean: 17.267724\n",
      "ep 3703: ep_len:48 episode reward: total was 20.510000. running mean: 17.300146\n",
      "ep 3703: ep_len:596 episode reward: total was 49.160000. running mean: 17.618745\n",
      "ep 3703: ep_len:510 episode reward: total was 0.350000. running mean: 17.446057\n",
      "epsilon:0.035789 episode_count: 25928. steps_count: 11208913.000000\n",
      "Time elapsed:  33906.93701863289\n",
      "ep 3704: ep_len:577 episode reward: total was 66.310000. running mean: 17.934697\n",
      "ep 3704: ep_len:500 episode reward: total was 22.760000. running mean: 17.982950\n",
      "ep 3704: ep_len:79 episode reward: total was 7.820000. running mean: 17.881320\n",
      "ep 3704: ep_len:500 episode reward: total was 17.400000. running mean: 17.876507\n",
      "ep 3704: ep_len:91 episode reward: total was 20.740000. running mean: 17.905142\n",
      "ep 3704: ep_len:550 episode reward: total was -23.230000. running mean: 17.493791\n",
      "ep 3704: ep_len:189 episode reward: total was 4.530000. running mean: 17.364153\n",
      "epsilon:0.035745 episode_count: 25935. steps_count: 11211399.000000\n",
      "Time elapsed:  33914.68281364441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3705: ep_len:637 episode reward: total was 28.000000. running mean: 17.470511\n",
      "ep 3705: ep_len:195 episode reward: total was 14.940000. running mean: 17.445206\n",
      "ep 3705: ep_len:402 episode reward: total was 61.750000. running mean: 17.888254\n",
      "ep 3705: ep_len:517 episode reward: total was 32.810000. running mean: 18.037472\n",
      "ep 3705: ep_len:3 episode reward: total was -0.490000. running mean: 17.852197\n",
      "ep 3705: ep_len:669 episode reward: total was 20.280000. running mean: 17.876475\n",
      "ep 3705: ep_len:545 episode reward: total was 34.490000. running mean: 18.042610\n",
      "epsilon:0.035701 episode_count: 25942. steps_count: 11214367.000000\n",
      "Time elapsed:  33922.64724612236\n",
      "ep 3706: ep_len:253 episode reward: total was 18.360000. running mean: 18.045784\n",
      "ep 3706: ep_len:526 episode reward: total was 33.690000. running mean: 18.202226\n",
      "ep 3706: ep_len:500 episode reward: total was 43.420000. running mean: 18.454404\n",
      "ep 3706: ep_len:476 episode reward: total was 37.510000. running mean: 18.644960\n",
      "ep 3706: ep_len:3 episode reward: total was 1.010000. running mean: 18.468610\n",
      "ep 3706: ep_len:687 episode reward: total was 35.420000. running mean: 18.638124\n",
      "ep 3706: ep_len:566 episode reward: total was 18.300000. running mean: 18.634743\n",
      "epsilon:0.035656 episode_count: 25949. steps_count: 11217378.000000\n",
      "Time elapsed:  33930.736228466034\n",
      "ep 3707: ep_len:513 episode reward: total was -35.250000. running mean: 18.095896\n",
      "ep 3707: ep_len:500 episode reward: total was 102.360000. running mean: 18.938537\n",
      "ep 3707: ep_len:571 episode reward: total was 19.110000. running mean: 18.940251\n",
      "ep 3707: ep_len:128 episode reward: total was 25.100000. running mean: 19.001849\n",
      "ep 3707: ep_len:35 episode reward: total was 13.000000. running mean: 18.941830\n",
      "ep 3707: ep_len:694 episode reward: total was 18.750000. running mean: 18.939912\n",
      "ep 3707: ep_len:518 episode reward: total was -50.530000. running mean: 18.245213\n",
      "epsilon:0.035612 episode_count: 25956. steps_count: 11220337.000000\n",
      "Time elapsed:  33938.67883896828\n",
      "ep 3708: ep_len:500 episode reward: total was 54.550000. running mean: 18.608261\n",
      "ep 3708: ep_len:500 episode reward: total was 15.740000. running mean: 18.579578\n",
      "ep 3708: ep_len:566 episode reward: total was -32.470000. running mean: 18.069082\n",
      "ep 3708: ep_len:166 episode reward: total was 36.750000. running mean: 18.255891\n",
      "ep 3708: ep_len:3 episode reward: total was 1.010000. running mean: 18.083433\n",
      "ep 3708: ep_len:159 episode reward: total was 22.960000. running mean: 18.132198\n",
      "ep 3708: ep_len:304 episode reward: total was 18.470000. running mean: 18.135576\n",
      "epsilon:0.035568 episode_count: 25963. steps_count: 11222535.000000\n",
      "Time elapsed:  33946.4761390686\n",
      "ep 3709: ep_len:500 episode reward: total was 78.120000. running mean: 18.735420\n",
      "ep 3709: ep_len:524 episode reward: total was 11.970000. running mean: 18.667766\n",
      "ep 3709: ep_len:509 episode reward: total was -4.560000. running mean: 18.435489\n",
      "ep 3709: ep_len:635 episode reward: total was 45.950000. running mean: 18.710634\n",
      "ep 3709: ep_len:3 episode reward: total was 0.000000. running mean: 18.523527\n",
      "ep 3709: ep_len:535 episode reward: total was 17.240000. running mean: 18.510692\n",
      "ep 3709: ep_len:606 episode reward: total was 15.790000. running mean: 18.483485\n",
      "epsilon:0.035523 episode_count: 25970. steps_count: 11225847.000000\n",
      "Time elapsed:  33955.113564014435\n",
      "ep 3710: ep_len:557 episode reward: total was 19.580000. running mean: 18.494450\n",
      "ep 3710: ep_len:536 episode reward: total was -80.890000. running mean: 17.500606\n",
      "ep 3710: ep_len:533 episode reward: total was 37.370000. running mean: 17.699300\n",
      "ep 3710: ep_len:500 episode reward: total was -31.860000. running mean: 17.203707\n",
      "ep 3710: ep_len:3 episode reward: total was 0.000000. running mean: 17.031670\n",
      "ep 3710: ep_len:500 episode reward: total was 44.620000. running mean: 17.307553\n",
      "ep 3710: ep_len:563 episode reward: total was 19.790000. running mean: 17.332377\n",
      "epsilon:0.035479 episode_count: 25977. steps_count: 11229039.000000\n",
      "Time elapsed:  33969.95266580582\n",
      "ep 3711: ep_len:134 episode reward: total was 18.540000. running mean: 17.344454\n",
      "ep 3711: ep_len:500 episode reward: total was 83.410000. running mean: 18.005109\n",
      "ep 3711: ep_len:635 episode reward: total was 31.590000. running mean: 18.140958\n",
      "ep 3711: ep_len:500 episode reward: total was 60.020000. running mean: 18.559748\n",
      "ep 3711: ep_len:99 episode reward: total was 29.760000. running mean: 18.671751\n",
      "ep 3711: ep_len:644 episode reward: total was 26.580000. running mean: 18.750833\n",
      "ep 3711: ep_len:607 episode reward: total was 15.810000. running mean: 18.721425\n",
      "epsilon:0.035435 episode_count: 25984. steps_count: 11232158.000000\n",
      "Time elapsed:  33978.20336723328\n",
      "ep 3712: ep_len:134 episode reward: total was 6.910000. running mean: 18.603311\n",
      "ep 3712: ep_len:530 episode reward: total was 24.890000. running mean: 18.666178\n",
      "ep 3712: ep_len:500 episode reward: total was -160.470000. running mean: 16.874816\n",
      "ep 3712: ep_len:500 episode reward: total was 42.760000. running mean: 17.133668\n",
      "ep 3712: ep_len:3 episode reward: total was 1.010000. running mean: 16.972431\n",
      "ep 3712: ep_len:500 episode reward: total was 48.420000. running mean: 17.286907\n",
      "ep 3712: ep_len:576 episode reward: total was -17.520000. running mean: 16.938838\n",
      "epsilon:0.035390 episode_count: 25991. steps_count: 11234901.000000\n",
      "Time elapsed:  33991.26819705963\n",
      "ep 3713: ep_len:584 episode reward: total was 70.390000. running mean: 17.473349\n",
      "ep 3713: ep_len:173 episode reward: total was -18.460000. running mean: 17.114016\n",
      "ep 3713: ep_len:630 episode reward: total was -16.610000. running mean: 16.776776\n",
      "ep 3713: ep_len:600 episode reward: total was 38.210000. running mean: 16.991108\n",
      "ep 3713: ep_len:3 episode reward: total was 1.010000. running mean: 16.831297\n",
      "ep 3713: ep_len:510 episode reward: total was 23.010000. running mean: 16.893084\n",
      "ep 3713: ep_len:546 episode reward: total was 26.520000. running mean: 16.989353\n",
      "epsilon:0.035346 episode_count: 25998. steps_count: 11237947.000000\n",
      "Time elapsed:  34001.26739168167\n",
      "ep 3714: ep_len:581 episode reward: total was 31.980000. running mean: 17.139260\n",
      "ep 3714: ep_len:565 episode reward: total was 65.520000. running mean: 17.623067\n",
      "ep 3714: ep_len:633 episode reward: total was -8.130000. running mean: 17.365536\n",
      "ep 3714: ep_len:504 episode reward: total was 69.280000. running mean: 17.884681\n",
      "ep 3714: ep_len:3 episode reward: total was 1.010000. running mean: 17.715934\n",
      "ep 3714: ep_len:590 episode reward: total was 42.020000. running mean: 17.958975\n",
      "ep 3714: ep_len:350 episode reward: total was 8.970000. running mean: 17.869085\n",
      "epsilon:0.035302 episode_count: 26005. steps_count: 11241173.000000\n",
      "Time elapsed:  34009.720443725586\n",
      "ep 3715: ep_len:500 episode reward: total was -8.360000. running mean: 17.606794\n",
      "ep 3715: ep_len:501 episode reward: total was 42.220000. running mean: 17.852926\n",
      "ep 3715: ep_len:541 episode reward: total was -1.520000. running mean: 17.659197\n",
      "ep 3715: ep_len:46 episode reward: total was 2.310000. running mean: 17.505705\n",
      "ep 3715: ep_len:3 episode reward: total was 1.010000. running mean: 17.340748\n",
      "ep 3715: ep_len:500 episode reward: total was 8.370000. running mean: 17.251040\n",
      "ep 3715: ep_len:500 episode reward: total was 22.130000. running mean: 17.299830\n",
      "epsilon:0.035257 episode_count: 26012. steps_count: 11243764.000000\n",
      "Time elapsed:  34016.826142549515\n",
      "ep 3716: ep_len:500 episode reward: total was 76.870000. running mean: 17.895532\n",
      "ep 3716: ep_len:580 episode reward: total was 7.890000. running mean: 17.795476\n",
      "ep 3716: ep_len:500 episode reward: total was 19.570000. running mean: 17.813222\n",
      "ep 3716: ep_len:115 episode reward: total was 10.490000. running mean: 17.739989\n",
      "ep 3716: ep_len:3 episode reward: total was 1.010000. running mean: 17.572690\n",
      "ep 3716: ep_len:517 episode reward: total was -24.190000. running mean: 17.155063\n",
      "ep 3716: ep_len:595 episode reward: total was 48.260000. running mean: 17.466112\n",
      "epsilon:0.035213 episode_count: 26019. steps_count: 11246574.000000\n",
      "Time elapsed:  34024.35421347618\n",
      "ep 3717: ep_len:567 episode reward: total was 62.570000. running mean: 17.917151\n",
      "ep 3717: ep_len:500 episode reward: total was 52.800000. running mean: 18.265979\n",
      "ep 3717: ep_len:575 episode reward: total was -15.790000. running mean: 17.925420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3717: ep_len:500 episode reward: total was 94.360000. running mean: 18.689765\n",
      "ep 3717: ep_len:3 episode reward: total was 1.010000. running mean: 18.512968\n",
      "ep 3717: ep_len:553 episode reward: total was 13.590000. running mean: 18.463738\n",
      "ep 3717: ep_len:527 episode reward: total was 47.760000. running mean: 18.756701\n",
      "epsilon:0.035169 episode_count: 26026. steps_count: 11249799.000000\n",
      "Time elapsed:  34032.909566402435\n",
      "ep 3718: ep_len:545 episode reward: total was 33.780000. running mean: 18.906934\n",
      "ep 3718: ep_len:571 episode reward: total was 7.810000. running mean: 18.795964\n",
      "ep 3718: ep_len:70 episode reward: total was 9.260000. running mean: 18.700605\n",
      "ep 3718: ep_len:609 episode reward: total was 85.480000. running mean: 19.368399\n",
      "ep 3718: ep_len:49 episode reward: total was 20.000000. running mean: 19.374715\n",
      "ep 3718: ep_len:680 episode reward: total was 51.570000. running mean: 19.696668\n",
      "ep 3718: ep_len:592 episode reward: total was 52.480000. running mean: 20.024501\n",
      "epsilon:0.035124 episode_count: 26033. steps_count: 11252915.000000\n",
      "Time elapsed:  34041.1006398201\n",
      "ep 3719: ep_len:580 episode reward: total was 36.290000. running mean: 20.187156\n",
      "ep 3719: ep_len:521 episode reward: total was 93.490000. running mean: 20.920184\n",
      "ep 3719: ep_len:580 episode reward: total was 3.840000. running mean: 20.749382\n",
      "ep 3719: ep_len:562 episode reward: total was -139.320000. running mean: 19.148689\n",
      "ep 3719: ep_len:85 episode reward: total was 27.720000. running mean: 19.234402\n",
      "ep 3719: ep_len:240 episode reward: total was 27.030000. running mean: 19.312358\n",
      "ep 3719: ep_len:607 episode reward: total was -28.330000. running mean: 18.835934\n",
      "epsilon:0.035080 episode_count: 26040. steps_count: 11256090.000000\n",
      "Time elapsed:  34049.54542899132\n",
      "ep 3720: ep_len:671 episode reward: total was -64.040000. running mean: 18.007175\n",
      "ep 3720: ep_len:500 episode reward: total was 41.020000. running mean: 18.237303\n",
      "ep 3720: ep_len:572 episode reward: total was -16.740000. running mean: 17.887530\n",
      "ep 3720: ep_len:520 episode reward: total was 19.980000. running mean: 17.908455\n",
      "ep 3720: ep_len:3 episode reward: total was 1.010000. running mean: 17.739470\n",
      "ep 3720: ep_len:508 episode reward: total was 24.830000. running mean: 17.810376\n",
      "ep 3720: ep_len:617 episode reward: total was -20.750000. running mean: 17.424772\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.035036 episode_count: 26047. steps_count: 11259481.000000\n",
      "Time elapsed:  34069.6132581234\n",
      "ep 3721: ep_len:500 episode reward: total was 61.930000. running mean: 17.869824\n",
      "ep 3721: ep_len:500 episode reward: total was 42.410000. running mean: 18.115226\n",
      "ep 3721: ep_len:588 episode reward: total was -17.750000. running mean: 17.756574\n",
      "ep 3721: ep_len:508 episode reward: total was -3.020000. running mean: 17.548808\n",
      "ep 3721: ep_len:3 episode reward: total was 1.010000. running mean: 17.383420\n",
      "ep 3721: ep_len:500 episode reward: total was -59.120000. running mean: 16.618386\n",
      "ep 3721: ep_len:531 episode reward: total was 15.260000. running mean: 16.604802\n",
      "epsilon:0.034991 episode_count: 26054. steps_count: 11262611.000000\n",
      "Time elapsed:  34083.797168016434\n",
      "ep 3722: ep_len:617 episode reward: total was 13.930000. running mean: 16.578054\n",
      "ep 3722: ep_len:166 episode reward: total was 11.770000. running mean: 16.529973\n",
      "ep 3722: ep_len:547 episode reward: total was -2.280000. running mean: 16.341873\n",
      "ep 3722: ep_len:515 episode reward: total was 6.360000. running mean: 16.242055\n",
      "ep 3722: ep_len:89 episode reward: total was 24.730000. running mean: 16.326934\n",
      "ep 3722: ep_len:565 episode reward: total was 22.420000. running mean: 16.387865\n",
      "ep 3722: ep_len:524 episode reward: total was 44.730000. running mean: 16.671286\n",
      "epsilon:0.034947 episode_count: 26061. steps_count: 11265634.000000\n",
      "Time elapsed:  34091.870012283325\n",
      "ep 3723: ep_len:578 episode reward: total was 50.960000. running mean: 17.014173\n",
      "ep 3723: ep_len:186 episode reward: total was 3.920000. running mean: 16.883232\n",
      "ep 3723: ep_len:547 episode reward: total was 1.670000. running mean: 16.731099\n",
      "ep 3723: ep_len:500 episode reward: total was 59.250000. running mean: 17.156288\n",
      "ep 3723: ep_len:3 episode reward: total was -0.490000. running mean: 16.979825\n",
      "ep 3723: ep_len:509 episode reward: total was 4.290000. running mean: 16.852927\n",
      "ep 3723: ep_len:170 episode reward: total was -7.640000. running mean: 16.607998\n",
      "epsilon:0.034903 episode_count: 26068. steps_count: 11268127.000000\n",
      "Time elapsed:  34105.19538998604\n",
      "ep 3724: ep_len:563 episode reward: total was 54.320000. running mean: 16.985118\n",
      "ep 3724: ep_len:522 episode reward: total was 90.110000. running mean: 17.716367\n",
      "ep 3724: ep_len:500 episode reward: total was 18.460000. running mean: 17.723803\n",
      "ep 3724: ep_len:574 episode reward: total was 67.150000. running mean: 18.218065\n",
      "ep 3724: ep_len:3 episode reward: total was 1.010000. running mean: 18.045984\n",
      "ep 3724: ep_len:260 episode reward: total was 29.990000. running mean: 18.165424\n",
      "ep 3724: ep_len:500 episode reward: total was -19.380000. running mean: 17.789970\n",
      "epsilon:0.034858 episode_count: 26075. steps_count: 11271049.000000\n",
      "Time elapsed:  34112.9573764801\n",
      "ep 3725: ep_len:580 episode reward: total was 59.730000. running mean: 18.209371\n",
      "ep 3725: ep_len:536 episode reward: total was 6.430000. running mean: 18.091577\n",
      "ep 3725: ep_len:615 episode reward: total was 6.410000. running mean: 17.974761\n",
      "ep 3725: ep_len:505 episode reward: total was 52.880000. running mean: 18.323813\n",
      "ep 3725: ep_len:3 episode reward: total was 1.010000. running mean: 18.150675\n",
      "ep 3725: ep_len:596 episode reward: total was 5.820000. running mean: 18.027369\n",
      "ep 3725: ep_len:185 episode reward: total was 9.450000. running mean: 17.941595\n",
      "epsilon:0.034814 episode_count: 26082. steps_count: 11274069.000000\n",
      "Time elapsed:  34121.006952762604\n",
      "ep 3726: ep_len:573 episode reward: total was 32.140000. running mean: 18.083579\n",
      "ep 3726: ep_len:544 episode reward: total was 35.210000. running mean: 18.254843\n",
      "ep 3726: ep_len:600 episode reward: total was -15.990000. running mean: 17.912395\n",
      "ep 3726: ep_len:537 episode reward: total was 33.410000. running mean: 18.067371\n",
      "ep 3726: ep_len:87 episode reward: total was 25.750000. running mean: 18.144197\n",
      "ep 3726: ep_len:536 episode reward: total was -27.850000. running mean: 17.684255\n",
      "ep 3726: ep_len:299 episode reward: total was 26.820000. running mean: 17.775613\n",
      "epsilon:0.034770 episode_count: 26089. steps_count: 11277245.000000\n",
      "Time elapsed:  34127.34863257408\n",
      "ep 3727: ep_len:500 episode reward: total was 63.830000. running mean: 18.236156\n",
      "ep 3727: ep_len:517 episode reward: total was 64.950000. running mean: 18.703295\n",
      "ep 3727: ep_len:506 episode reward: total was -4.530000. running mean: 18.470962\n",
      "ep 3727: ep_len:500 episode reward: total was 26.280000. running mean: 18.549052\n",
      "ep 3727: ep_len:3 episode reward: total was 1.010000. running mean: 18.373662\n",
      "ep 3727: ep_len:680 episode reward: total was 18.880000. running mean: 18.378725\n",
      "ep 3727: ep_len:563 episode reward: total was -64.510000. running mean: 17.549838\n",
      "epsilon:0.034725 episode_count: 26096. steps_count: 11280514.000000\n",
      "Time elapsed:  34133.06024932861\n",
      "ep 3728: ep_len:551 episode reward: total was 49.240000. running mean: 17.866739\n",
      "ep 3728: ep_len:360 episode reward: total was 15.380000. running mean: 17.841872\n",
      "ep 3728: ep_len:565 episode reward: total was -7.210000. running mean: 17.591353\n",
      "ep 3728: ep_len:565 episode reward: total was 18.760000. running mean: 17.603040\n",
      "ep 3728: ep_len:3 episode reward: total was 1.010000. running mean: 17.437109\n",
      "ep 3728: ep_len:555 episode reward: total was -41.220000. running mean: 16.850538\n",
      "ep 3728: ep_len:500 episode reward: total was 28.750000. running mean: 16.969533\n",
      "epsilon:0.034681 episode_count: 26103. steps_count: 11283613.000000\n",
      "Time elapsed:  34141.344411849976\n",
      "ep 3729: ep_len:537 episode reward: total was 25.590000. running mean: 17.055738\n",
      "ep 3729: ep_len:503 episode reward: total was 96.020000. running mean: 17.845380\n",
      "ep 3729: ep_len:556 episode reward: total was 64.240000. running mean: 18.309326\n",
      "ep 3729: ep_len:512 episode reward: total was 9.180000. running mean: 18.218033\n",
      "ep 3729: ep_len:3 episode reward: total was 1.010000. running mean: 18.045953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3729: ep_len:659 episode reward: total was 20.970000. running mean: 18.075193\n",
      "ep 3729: ep_len:290 episode reward: total was -1.540000. running mean: 17.879041\n",
      "epsilon:0.034637 episode_count: 26110. steps_count: 11286673.000000\n",
      "Time elapsed:  34149.544605731964\n",
      "ep 3730: ep_len:656 episode reward: total was -5.580000. running mean: 17.644451\n",
      "ep 3730: ep_len:272 episode reward: total was 1.620000. running mean: 17.484206\n",
      "ep 3730: ep_len:500 episode reward: total was 20.290000. running mean: 17.512264\n",
      "ep 3730: ep_len:500 episode reward: total was 32.690000. running mean: 17.664042\n",
      "ep 3730: ep_len:119 episode reward: total was 30.360000. running mean: 17.791001\n",
      "ep 3730: ep_len:656 episode reward: total was 44.750000. running mean: 18.060591\n",
      "ep 3730: ep_len:585 episode reward: total was 13.390000. running mean: 18.013885\n",
      "epsilon:0.034592 episode_count: 26117. steps_count: 11289961.000000\n",
      "Time elapsed:  34158.158992528915\n",
      "ep 3731: ep_len:500 episode reward: total was 80.060000. running mean: 18.634347\n",
      "ep 3731: ep_len:500 episode reward: total was -19.560000. running mean: 18.252403\n",
      "ep 3731: ep_len:545 episode reward: total was 55.850000. running mean: 18.628379\n",
      "ep 3731: ep_len:170 episode reward: total was 12.230000. running mean: 18.564395\n",
      "ep 3731: ep_len:120 episode reward: total was 32.360000. running mean: 18.702351\n",
      "ep 3731: ep_len:550 episode reward: total was 15.260000. running mean: 18.667928\n",
      "ep 3731: ep_len:209 episode reward: total was 15.690000. running mean: 18.638149\n",
      "epsilon:0.034548 episode_count: 26124. steps_count: 11292555.000000\n",
      "Time elapsed:  34165.188564538956\n",
      "ep 3732: ep_len:563 episode reward: total was 23.470000. running mean: 18.686467\n",
      "ep 3732: ep_len:500 episode reward: total was 92.610000. running mean: 19.425702\n",
      "ep 3732: ep_len:422 episode reward: total was 55.480000. running mean: 19.786245\n",
      "ep 3732: ep_len:556 episode reward: total was 44.680000. running mean: 20.035183\n",
      "ep 3732: ep_len:3 episode reward: total was 1.010000. running mean: 19.844931\n",
      "ep 3732: ep_len:617 episode reward: total was 56.160000. running mean: 20.208082\n",
      "ep 3732: ep_len:182 episode reward: total was -41.170000. running mean: 19.594301\n",
      "epsilon:0.034504 episode_count: 26131. steps_count: 11295398.000000\n",
      "Time elapsed:  34172.89652681351\n",
      "ep 3733: ep_len:500 episode reward: total was -1.060000. running mean: 19.387758\n",
      "ep 3733: ep_len:294 episode reward: total was 13.230000. running mean: 19.326180\n",
      "ep 3733: ep_len:608 episode reward: total was -8.590000. running mean: 19.047019\n",
      "ep 3733: ep_len:510 episode reward: total was 65.260000. running mean: 19.509148\n",
      "ep 3733: ep_len:3 episode reward: total was 1.010000. running mean: 19.324157\n",
      "ep 3733: ep_len:500 episode reward: total was 50.410000. running mean: 19.635015\n",
      "ep 3733: ep_len:515 episode reward: total was -26.770000. running mean: 19.170965\n",
      "epsilon:0.034459 episode_count: 26138. steps_count: 11298328.000000\n",
      "Time elapsed:  34186.26272845268\n",
      "ep 3734: ep_len:575 episode reward: total was 28.650000. running mean: 19.265756\n",
      "ep 3734: ep_len:500 episode reward: total was 57.060000. running mean: 19.643698\n",
      "ep 3734: ep_len:500 episode reward: total was 13.360000. running mean: 19.580861\n",
      "ep 3734: ep_len:500 episode reward: total was 27.380000. running mean: 19.658852\n",
      "ep 3734: ep_len:112 episode reward: total was 25.360000. running mean: 19.715864\n",
      "ep 3734: ep_len:500 episode reward: total was -4.780000. running mean: 19.470905\n",
      "ep 3734: ep_len:563 episode reward: total was 27.700000. running mean: 19.553196\n",
      "epsilon:0.034415 episode_count: 26145. steps_count: 11301578.000000\n",
      "Time elapsed:  34194.93036198616\n",
      "ep 3735: ep_len:244 episode reward: total was 16.190000. running mean: 19.519564\n",
      "ep 3735: ep_len:527 episode reward: total was 78.620000. running mean: 20.110569\n",
      "ep 3735: ep_len:675 episode reward: total was 8.740000. running mean: 19.996863\n",
      "ep 3735: ep_len:549 episode reward: total was 24.110000. running mean: 20.037994\n",
      "ep 3735: ep_len:102 episode reward: total was 25.260000. running mean: 20.090214\n",
      "ep 3735: ep_len:500 episode reward: total was 25.900000. running mean: 20.148312\n",
      "ep 3735: ep_len:504 episode reward: total was 3.870000. running mean: 19.985529\n",
      "epsilon:0.034371 episode_count: 26152. steps_count: 11304679.000000\n",
      "Time elapsed:  34203.30610895157\n",
      "ep 3736: ep_len:546 episode reward: total was 76.770000. running mean: 20.553374\n",
      "ep 3736: ep_len:550 episode reward: total was 22.230000. running mean: 20.570140\n",
      "ep 3736: ep_len:543 episode reward: total was -25.830000. running mean: 20.106139\n",
      "ep 3736: ep_len:170 episode reward: total was 13.760000. running mean: 20.042677\n",
      "ep 3736: ep_len:3 episode reward: total was 1.010000. running mean: 19.852350\n",
      "ep 3736: ep_len:501 episode reward: total was 14.620000. running mean: 19.800027\n",
      "ep 3736: ep_len:541 episode reward: total was 15.630000. running mean: 19.758327\n",
      "epsilon:0.034326 episode_count: 26159. steps_count: 11307533.000000\n",
      "Time elapsed:  34210.896555900574\n",
      "ep 3737: ep_len:525 episode reward: total was 17.970000. running mean: 19.740443\n",
      "ep 3737: ep_len:277 episode reward: total was 18.630000. running mean: 19.729339\n",
      "ep 3737: ep_len:589 episode reward: total was 2.650000. running mean: 19.558546\n",
      "ep 3737: ep_len:500 episode reward: total was 40.670000. running mean: 19.769660\n",
      "ep 3737: ep_len:109 episode reward: total was 31.270000. running mean: 19.884664\n",
      "ep 3737: ep_len:500 episode reward: total was 25.870000. running mean: 19.944517\n",
      "ep 3737: ep_len:328 episode reward: total was -2.670000. running mean: 19.718372\n",
      "epsilon:0.034282 episode_count: 26166. steps_count: 11310361.000000\n",
      "Time elapsed:  34218.97275137901\n",
      "ep 3738: ep_len:591 episode reward: total was 76.660000. running mean: 20.287788\n",
      "ep 3738: ep_len:580 episode reward: total was 9.340000. running mean: 20.178310\n",
      "ep 3738: ep_len:523 episode reward: total was 19.100000. running mean: 20.167527\n",
      "ep 3738: ep_len:567 episode reward: total was -18.280000. running mean: 19.783052\n",
      "ep 3738: ep_len:111 episode reward: total was 32.760000. running mean: 19.912821\n",
      "ep 3738: ep_len:592 episode reward: total was 33.430000. running mean: 20.047993\n",
      "ep 3738: ep_len:523 episode reward: total was 46.510000. running mean: 20.312613\n",
      "epsilon:0.034238 episode_count: 26173. steps_count: 11313848.000000\n",
      "Time elapsed:  34235.89459514618\n",
      "ep 3739: ep_len:508 episode reward: total was 6.590000. running mean: 20.175387\n",
      "ep 3739: ep_len:530 episode reward: total was 102.040000. running mean: 20.994033\n",
      "ep 3739: ep_len:547 episode reward: total was -43.390000. running mean: 20.350193\n",
      "ep 3739: ep_len:509 episode reward: total was 83.360000. running mean: 20.980291\n",
      "ep 3739: ep_len:79 episode reward: total was 24.230000. running mean: 21.012788\n",
      "ep 3739: ep_len:500 episode reward: total was 23.910000. running mean: 21.041760\n",
      "ep 3739: ep_len:502 episode reward: total was 13.530000. running mean: 20.966642\n",
      "epsilon:0.034193 episode_count: 26180. steps_count: 11317023.000000\n",
      "Time elapsed:  34244.282718896866\n",
      "ep 3740: ep_len:107 episode reward: total was 9.820000. running mean: 20.855176\n",
      "ep 3740: ep_len:500 episode reward: total was 2.440000. running mean: 20.671024\n",
      "ep 3740: ep_len:589 episode reward: total was -21.720000. running mean: 20.247114\n",
      "ep 3740: ep_len:170 episode reward: total was 18.240000. running mean: 20.227043\n",
      "ep 3740: ep_len:126 episode reward: total was 35.360000. running mean: 20.378372\n",
      "ep 3740: ep_len:519 episode reward: total was -37.090000. running mean: 19.803689\n",
      "ep 3740: ep_len:179 episode reward: total was -8.460000. running mean: 19.521052\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.034149 episode_count: 26187. steps_count: 11319213.000000\n",
      "Time elapsed:  34260.85222411156\n",
      "ep 3741: ep_len:236 episode reward: total was -107.120000. running mean: 18.254641\n",
      "ep 3741: ep_len:584 episode reward: total was 35.530000. running mean: 18.427395\n",
      "ep 3741: ep_len:546 episode reward: total was -7.600000. running mean: 18.167121\n",
      "ep 3741: ep_len:523 episode reward: total was 10.580000. running mean: 18.091250\n",
      "ep 3741: ep_len:106 episode reward: total was 27.230000. running mean: 18.182637\n",
      "ep 3741: ep_len:602 episode reward: total was 32.600000. running mean: 18.326811\n",
      "ep 3741: ep_len:562 episode reward: total was -21.420000. running mean: 17.929343\n",
      "epsilon:0.034105 episode_count: 26194. steps_count: 11322372.000000\n",
      "Time elapsed:  34269.247240781784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3742: ep_len:519 episode reward: total was 1.780000. running mean: 17.767849\n",
      "ep 3742: ep_len:317 episode reward: total was 11.700000. running mean: 17.707171\n",
      "ep 3742: ep_len:521 episode reward: total was 40.920000. running mean: 17.939299\n",
      "ep 3742: ep_len:528 episode reward: total was 61.080000. running mean: 18.370706\n",
      "ep 3742: ep_len:76 episode reward: total was 17.220000. running mean: 18.359199\n",
      "ep 3742: ep_len:216 episode reward: total was 28.870000. running mean: 18.464307\n",
      "ep 3742: ep_len:291 episode reward: total was 8.920000. running mean: 18.368864\n",
      "epsilon:0.034060 episode_count: 26201. steps_count: 11324840.000000\n",
      "Time elapsed:  34275.96578860283\n",
      "ep 3743: ep_len:500 episode reward: total was 57.470000. running mean: 18.759875\n",
      "ep 3743: ep_len:522 episode reward: total was 9.920000. running mean: 18.671477\n",
      "ep 3743: ep_len:500 episode reward: total was 20.860000. running mean: 18.693362\n",
      "ep 3743: ep_len:529 episode reward: total was 14.610000. running mean: 18.652528\n",
      "ep 3743: ep_len:114 episode reward: total was 28.720000. running mean: 18.753203\n",
      "ep 3743: ep_len:637 episode reward: total was 30.560000. running mean: 18.871271\n",
      "ep 3743: ep_len:500 episode reward: total was 22.720000. running mean: 18.909758\n",
      "epsilon:0.034016 episode_count: 26208. steps_count: 11328142.000000\n",
      "Time elapsed:  34284.72241854668\n",
      "ep 3744: ep_len:500 episode reward: total was 83.630000. running mean: 19.556961\n",
      "ep 3744: ep_len:563 episode reward: total was -8.540000. running mean: 19.275991\n",
      "ep 3744: ep_len:625 episode reward: total was 12.880000. running mean: 19.212031\n",
      "ep 3744: ep_len:536 episode reward: total was 67.750000. running mean: 19.697411\n",
      "ep 3744: ep_len:3 episode reward: total was 1.010000. running mean: 19.510537\n",
      "ep 3744: ep_len:532 episode reward: total was 31.700000. running mean: 19.632431\n",
      "ep 3744: ep_len:331 episode reward: total was 11.070000. running mean: 19.546807\n",
      "epsilon:0.033972 episode_count: 26215. steps_count: 11331232.000000\n",
      "Time elapsed:  34292.93514084816\n",
      "ep 3745: ep_len:647 episode reward: total was -36.740000. running mean: 18.983939\n",
      "ep 3745: ep_len:192 episode reward: total was 9.370000. running mean: 18.887800\n",
      "ep 3745: ep_len:500 episode reward: total was 5.770000. running mean: 18.756622\n",
      "ep 3745: ep_len:500 episode reward: total was 15.000000. running mean: 18.719055\n",
      "ep 3745: ep_len:3 episode reward: total was 1.010000. running mean: 18.541965\n",
      "ep 3745: ep_len:671 episode reward: total was -182.560000. running mean: 16.530945\n",
      "ep 3745: ep_len:579 episode reward: total was 23.620000. running mean: 16.601836\n",
      "epsilon:0.033927 episode_count: 26222. steps_count: 11334324.000000\n",
      "Time elapsed:  34301.10782289505\n",
      "ep 3746: ep_len:577 episode reward: total was -33.000000. running mean: 16.105817\n",
      "ep 3746: ep_len:500 episode reward: total was 68.370000. running mean: 16.628459\n",
      "ep 3746: ep_len:500 episode reward: total was 31.380000. running mean: 16.775975\n",
      "ep 3746: ep_len:512 episode reward: total was 22.990000. running mean: 16.838115\n",
      "ep 3746: ep_len:103 episode reward: total was 30.750000. running mean: 16.977234\n",
      "ep 3746: ep_len:518 episode reward: total was -131.730000. running mean: 15.490161\n",
      "ep 3746: ep_len:500 episode reward: total was -0.790000. running mean: 15.327360\n",
      "epsilon:0.033883 episode_count: 26229. steps_count: 11337534.000000\n",
      "Time elapsed:  34309.55069565773\n",
      "ep 3747: ep_len:201 episode reward: total was 33.100000. running mean: 15.505086\n",
      "ep 3747: ep_len:815 episode reward: total was -248.620000. running mean: 12.863835\n",
      "ep 3747: ep_len:500 episode reward: total was -36.700000. running mean: 12.368197\n",
      "ep 3747: ep_len:560 episode reward: total was 81.700000. running mean: 13.061515\n",
      "ep 3747: ep_len:97 episode reward: total was 24.260000. running mean: 13.173500\n",
      "ep 3747: ep_len:684 episode reward: total was 32.880000. running mean: 13.370565\n",
      "ep 3747: ep_len:300 episode reward: total was 22.040000. running mean: 13.457259\n",
      "epsilon:0.033839 episode_count: 26236. steps_count: 11340691.000000\n",
      "Time elapsed:  34321.243520498276\n",
      "ep 3748: ep_len:500 episode reward: total was 57.130000. running mean: 13.893987\n",
      "ep 3748: ep_len:537 episode reward: total was 6.010000. running mean: 13.815147\n",
      "ep 3748: ep_len:657 episode reward: total was -1.010000. running mean: 13.666895\n",
      "ep 3748: ep_len:427 episode reward: total was -10.150000. running mean: 13.428726\n",
      "ep 3748: ep_len:3 episode reward: total was 0.000000. running mean: 13.294439\n",
      "ep 3748: ep_len:500 episode reward: total was 7.870000. running mean: 13.240195\n",
      "ep 3748: ep_len:525 episode reward: total was 36.970000. running mean: 13.477493\n",
      "epsilon:0.033794 episode_count: 26243. steps_count: 11343840.000000\n",
      "Time elapsed:  34329.41835308075\n",
      "ep 3749: ep_len:663 episode reward: total was -61.520000. running mean: 12.727518\n",
      "ep 3749: ep_len:546 episode reward: total was -4.770000. running mean: 12.552543\n",
      "ep 3749: ep_len:560 episode reward: total was -45.170000. running mean: 11.975317\n",
      "ep 3749: ep_len:500 episode reward: total was 20.050000. running mean: 12.056064\n",
      "ep 3749: ep_len:3 episode reward: total was 1.010000. running mean: 11.945603\n",
      "ep 3749: ep_len:500 episode reward: total was 29.610000. running mean: 12.122247\n",
      "ep 3749: ep_len:555 episode reward: total was 29.300000. running mean: 12.294025\n",
      "epsilon:0.033750 episode_count: 26250. steps_count: 11347167.000000\n",
      "Time elapsed:  34344.442353248596\n",
      "ep 3750: ep_len:206 episode reward: total was -10.320000. running mean: 12.067885\n",
      "ep 3750: ep_len:504 episode reward: total was 23.030000. running mean: 12.177506\n",
      "ep 3750: ep_len:626 episode reward: total was -13.340000. running mean: 11.922331\n",
      "ep 3750: ep_len:500 episode reward: total was 41.960000. running mean: 12.222707\n",
      "ep 3750: ep_len:3 episode reward: total was 1.010000. running mean: 12.110580\n",
      "ep 3750: ep_len:500 episode reward: total was 43.960000. running mean: 12.429074\n",
      "ep 3750: ep_len:500 episode reward: total was 55.540000. running mean: 12.860184\n",
      "epsilon:0.033706 episode_count: 26257. steps_count: 11350006.000000\n",
      "Time elapsed:  34352.349495887756\n",
      "ep 3751: ep_len:579 episode reward: total was 39.800000. running mean: 13.129582\n",
      "ep 3751: ep_len:500 episode reward: total was 137.600000. running mean: 14.374286\n",
      "ep 3751: ep_len:537 episode reward: total was 19.600000. running mean: 14.426543\n",
      "ep 3751: ep_len:500 episode reward: total was 97.480000. running mean: 15.257078\n",
      "ep 3751: ep_len:98 episode reward: total was 30.730000. running mean: 15.411807\n",
      "ep 3751: ep_len:511 episode reward: total was 45.810000. running mean: 15.715789\n",
      "ep 3751: ep_len:589 episode reward: total was 27.250000. running mean: 15.831131\n",
      "epsilon:0.033661 episode_count: 26264. steps_count: 11353320.000000\n",
      "Time elapsed:  34361.14404273033\n",
      "ep 3752: ep_len:500 episode reward: total was -26.000000. running mean: 15.412820\n",
      "ep 3752: ep_len:500 episode reward: total was 24.380000. running mean: 15.502492\n",
      "ep 3752: ep_len:451 episode reward: total was 61.830000. running mean: 15.965767\n",
      "ep 3752: ep_len:639 episode reward: total was -105.930000. running mean: 14.746809\n",
      "ep 3752: ep_len:3 episode reward: total was 1.010000. running mean: 14.609441\n",
      "ep 3752: ep_len:500 episode reward: total was 38.270000. running mean: 14.846046\n",
      "ep 3752: ep_len:500 episode reward: total was 21.730000. running mean: 14.914886\n",
      "epsilon:0.033617 episode_count: 26271. steps_count: 11356413.000000\n",
      "Time elapsed:  34369.360986471176\n",
      "ep 3753: ep_len:561 episode reward: total was -9.920000. running mean: 14.666537\n",
      "ep 3753: ep_len:543 episode reward: total was 7.360000. running mean: 14.593472\n",
      "ep 3753: ep_len:500 episode reward: total was 32.520000. running mean: 14.772737\n",
      "ep 3753: ep_len:500 episode reward: total was 34.410000. running mean: 14.969110\n",
      "ep 3753: ep_len:3 episode reward: total was 1.010000. running mean: 14.829519\n",
      "ep 3753: ep_len:540 episode reward: total was -19.370000. running mean: 14.487523\n",
      "ep 3753: ep_len:500 episode reward: total was 3.790000. running mean: 14.380548\n",
      "epsilon:0.033573 episode_count: 26278. steps_count: 11359560.000000\n",
      "Time elapsed:  34377.75780272484\n",
      "ep 3754: ep_len:572 episode reward: total was 63.350000. running mean: 14.870243\n",
      "ep 3754: ep_len:500 episode reward: total was 38.250000. running mean: 15.104040\n",
      "ep 3754: ep_len:617 episode reward: total was 20.750000. running mean: 15.160500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3754: ep_len:132 episode reward: total was 11.140000. running mean: 15.120295\n",
      "ep 3754: ep_len:3 episode reward: total was 1.010000. running mean: 14.979192\n",
      "ep 3754: ep_len:605 episode reward: total was 51.450000. running mean: 15.343900\n",
      "ep 3754: ep_len:315 episode reward: total was 33.700000. running mean: 15.527461\n",
      "epsilon:0.033528 episode_count: 26285. steps_count: 11362304.000000\n",
      "Time elapsed:  34384.76832985878\n",
      "ep 3755: ep_len:112 episode reward: total was 9.440000. running mean: 15.466586\n",
      "ep 3755: ep_len:620 episode reward: total was 45.270000. running mean: 15.764621\n",
      "ep 3755: ep_len:670 episode reward: total was 6.200000. running mean: 15.668974\n",
      "ep 3755: ep_len:581 episode reward: total was 38.940000. running mean: 15.901685\n",
      "ep 3755: ep_len:3 episode reward: total was 1.010000. running mean: 15.752768\n",
      "ep 3755: ep_len:663 episode reward: total was 29.120000. running mean: 15.886440\n",
      "ep 3755: ep_len:175 episode reward: total was 5.950000. running mean: 15.787076\n",
      "epsilon:0.033484 episode_count: 26292. steps_count: 11365128.000000\n",
      "Time elapsed:  34397.08624482155\n",
      "ep 3756: ep_len:578 episode reward: total was 89.460000. running mean: 16.523805\n",
      "ep 3756: ep_len:554 episode reward: total was 35.970000. running mean: 16.718267\n",
      "ep 3756: ep_len:612 episode reward: total was 37.780000. running mean: 16.928884\n",
      "ep 3756: ep_len:502 episode reward: total was 84.320000. running mean: 17.602795\n",
      "ep 3756: ep_len:3 episode reward: total was 1.010000. running mean: 17.436867\n",
      "ep 3756: ep_len:531 episode reward: total was -7.890000. running mean: 17.183599\n",
      "ep 3756: ep_len:500 episode reward: total was 17.210000. running mean: 17.183863\n",
      "epsilon:0.033440 episode_count: 26299. steps_count: 11368408.000000\n",
      "Time elapsed:  34417.426243543625\n",
      "ep 3757: ep_len:500 episode reward: total was 48.520000. running mean: 17.497224\n",
      "ep 3757: ep_len:561 episode reward: total was 126.290000. running mean: 18.585152\n",
      "ep 3757: ep_len:698 episode reward: total was 3.150000. running mean: 18.430800\n",
      "ep 3757: ep_len:145 episode reward: total was 24.110000. running mean: 18.487592\n",
      "ep 3757: ep_len:3 episode reward: total was 1.010000. running mean: 18.312816\n",
      "ep 3757: ep_len:500 episode reward: total was 46.640000. running mean: 18.596088\n",
      "ep 3757: ep_len:340 episode reward: total was 27.660000. running mean: 18.686727\n",
      "epsilon:0.033395 episode_count: 26306. steps_count: 11371155.000000\n",
      "Time elapsed:  34425.103044986725\n",
      "ep 3758: ep_len:501 episode reward: total was 56.470000. running mean: 19.064560\n",
      "ep 3758: ep_len:536 episode reward: total was 102.310000. running mean: 19.897014\n",
      "ep 3758: ep_len:599 episode reward: total was 11.570000. running mean: 19.813744\n",
      "ep 3758: ep_len:509 episode reward: total was 51.240000. running mean: 20.128007\n",
      "ep 3758: ep_len:72 episode reward: total was 13.750000. running mean: 20.064227\n",
      "ep 3758: ep_len:583 episode reward: total was 58.260000. running mean: 20.446185\n",
      "ep 3758: ep_len:536 episode reward: total was -23.100000. running mean: 20.010723\n",
      "epsilon:0.033351 episode_count: 26313. steps_count: 11374491.000000\n",
      "Time elapsed:  34433.82266688347\n",
      "ep 3759: ep_len:500 episode reward: total was -0.710000. running mean: 19.803515\n",
      "ep 3759: ep_len:296 episode reward: total was 37.370000. running mean: 19.979180\n",
      "ep 3759: ep_len:365 episode reward: total was -33.370000. running mean: 19.445689\n",
      "ep 3759: ep_len:540 episode reward: total was 0.940000. running mean: 19.260632\n",
      "ep 3759: ep_len:3 episode reward: total was 1.010000. running mean: 19.078125\n",
      "ep 3759: ep_len:653 episode reward: total was -11.750000. running mean: 18.769844\n",
      "ep 3759: ep_len:604 episode reward: total was 69.960000. running mean: 19.281746\n",
      "epsilon:0.033307 episode_count: 26320. steps_count: 11377452.000000\n",
      "Time elapsed:  34441.72096300125\n",
      "ep 3760: ep_len:587 episode reward: total was 46.350000. running mean: 19.552428\n",
      "ep 3760: ep_len:500 episode reward: total was 23.100000. running mean: 19.587904\n",
      "ep 3760: ep_len:651 episode reward: total was -19.920000. running mean: 19.192825\n",
      "ep 3760: ep_len:500 episode reward: total was -48.830000. running mean: 18.512597\n",
      "ep 3760: ep_len:3 episode reward: total was 1.010000. running mean: 18.337571\n",
      "ep 3760: ep_len:500 episode reward: total was 10.360000. running mean: 18.257795\n",
      "ep 3760: ep_len:585 episode reward: total was 34.120000. running mean: 18.416417\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.033262 episode_count: 26327. steps_count: 11380778.000000\n",
      "Time elapsed:  34454.11656141281\n",
      "ep 3761: ep_len:500 episode reward: total was 51.710000. running mean: 18.749353\n",
      "ep 3761: ep_len:565 episode reward: total was 82.190000. running mean: 19.383759\n",
      "ep 3761: ep_len:500 episode reward: total was 5.910000. running mean: 19.249022\n",
      "ep 3761: ep_len:500 episode reward: total was 58.130000. running mean: 19.637831\n",
      "ep 3761: ep_len:3 episode reward: total was -1.500000. running mean: 19.426453\n",
      "ep 3761: ep_len:589 episode reward: total was 28.230000. running mean: 19.514489\n",
      "ep 3761: ep_len:173 episode reward: total was 18.080000. running mean: 19.500144\n",
      "epsilon:0.033218 episode_count: 26334. steps_count: 11383608.000000\n",
      "Time elapsed:  34458.99675631523\n",
      "ep 3762: ep_len:570 episode reward: total was -17.790000. running mean: 19.127242\n",
      "ep 3762: ep_len:187 episode reward: total was 21.350000. running mean: 19.149470\n",
      "ep 3762: ep_len:64 episode reward: total was 7.360000. running mean: 19.031575\n",
      "ep 3762: ep_len:501 episode reward: total was 4.040000. running mean: 18.881659\n",
      "ep 3762: ep_len:38 episode reward: total was 15.510000. running mean: 18.847943\n",
      "ep 3762: ep_len:517 episode reward: total was -7.260000. running mean: 18.586863\n",
      "ep 3762: ep_len:574 episode reward: total was 55.260000. running mean: 18.953595\n",
      "epsilon:0.033174 episode_count: 26341. steps_count: 11386059.000000\n",
      "Time elapsed:  34462.92225790024\n",
      "ep 3763: ep_len:603 episode reward: total was 95.420000. running mean: 19.718259\n",
      "ep 3763: ep_len:500 episode reward: total was 22.470000. running mean: 19.745776\n",
      "ep 3763: ep_len:500 episode reward: total was 22.110000. running mean: 19.769418\n",
      "ep 3763: ep_len:615 episode reward: total was 62.030000. running mean: 20.192024\n",
      "ep 3763: ep_len:3 episode reward: total was 1.010000. running mean: 20.000204\n",
      "ep 3763: ep_len:503 episode reward: total was 29.380000. running mean: 20.094002\n",
      "ep 3763: ep_len:190 episode reward: total was 5.060000. running mean: 19.943662\n",
      "epsilon:0.033129 episode_count: 26348. steps_count: 11388973.000000\n",
      "Time elapsed:  34469.757385492325\n",
      "ep 3764: ep_len:576 episode reward: total was 76.430000. running mean: 20.508525\n",
      "ep 3764: ep_len:594 episode reward: total was 61.140000. running mean: 20.914840\n",
      "ep 3764: ep_len:639 episode reward: total was 9.900000. running mean: 20.804692\n",
      "ep 3764: ep_len:582 episode reward: total was 67.910000. running mean: 21.275745\n",
      "ep 3764: ep_len:86 episode reward: total was 22.770000. running mean: 21.290687\n",
      "ep 3764: ep_len:573 episode reward: total was 32.690000. running mean: 21.404680\n",
      "ep 3764: ep_len:206 episode reward: total was 10.090000. running mean: 21.291534\n",
      "epsilon:0.033085 episode_count: 26355. steps_count: 11392229.000000\n",
      "Time elapsed:  34476.28066825867\n",
      "ep 3765: ep_len:505 episode reward: total was 64.430000. running mean: 21.722918\n",
      "ep 3765: ep_len:513 episode reward: total was 34.870000. running mean: 21.854389\n",
      "ep 3765: ep_len:596 episode reward: total was 20.540000. running mean: 21.841245\n",
      "ep 3765: ep_len:500 episode reward: total was 26.400000. running mean: 21.886833\n",
      "ep 3765: ep_len:106 episode reward: total was 35.740000. running mean: 22.025364\n",
      "ep 3765: ep_len:542 episode reward: total was 16.430000. running mean: 21.969411\n",
      "ep 3765: ep_len:500 episode reward: total was 71.480000. running mean: 22.464517\n",
      "epsilon:0.033041 episode_count: 26362. steps_count: 11395491.000000\n",
      "Time elapsed:  34484.75236749649\n",
      "ep 3766: ep_len:532 episode reward: total was 71.830000. running mean: 22.958172\n",
      "ep 3766: ep_len:500 episode reward: total was 69.020000. running mean: 23.418790\n",
      "ep 3766: ep_len:540 episode reward: total was 25.380000. running mean: 23.438402\n",
      "ep 3766: ep_len:56 episode reward: total was 3.850000. running mean: 23.242518\n",
      "ep 3766: ep_len:99 episode reward: total was 21.740000. running mean: 23.227493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3766: ep_len:516 episode reward: total was 10.400000. running mean: 23.099218\n",
      "ep 3766: ep_len:574 episode reward: total was 35.490000. running mean: 23.223126\n",
      "epsilon:0.032996 episode_count: 26369. steps_count: 11398308.000000\n",
      "Time elapsed:  34492.25576901436\n",
      "ep 3767: ep_len:708 episode reward: total was -3.520000. running mean: 22.955694\n",
      "ep 3767: ep_len:283 episode reward: total was -9.960000. running mean: 22.626537\n",
      "ep 3767: ep_len:546 episode reward: total was 22.770000. running mean: 22.627972\n",
      "ep 3767: ep_len:500 episode reward: total was 10.780000. running mean: 22.509492\n",
      "ep 3767: ep_len:94 episode reward: total was 27.780000. running mean: 22.562197\n",
      "ep 3767: ep_len:661 episode reward: total was 45.690000. running mean: 22.793475\n",
      "ep 3767: ep_len:275 episode reward: total was 26.440000. running mean: 22.829941\n",
      "epsilon:0.032952 episode_count: 26376. steps_count: 11401375.000000\n",
      "Time elapsed:  34499.74947142601\n",
      "ep 3768: ep_len:522 episode reward: total was -109.910000. running mean: 21.502541\n",
      "ep 3768: ep_len:583 episode reward: total was 52.470000. running mean: 21.812216\n",
      "ep 3768: ep_len:551 episode reward: total was -3.220000. running mean: 21.561894\n",
      "ep 3768: ep_len:583 episode reward: total was 66.390000. running mean: 22.010175\n",
      "ep 3768: ep_len:73 episode reward: total was 19.150000. running mean: 21.981573\n",
      "ep 3768: ep_len:311 episode reward: total was 24.520000. running mean: 22.006957\n",
      "ep 3768: ep_len:193 episode reward: total was 12.500000. running mean: 21.911888\n",
      "epsilon:0.032908 episode_count: 26383. steps_count: 11404191.000000\n",
      "Time elapsed:  34504.17395043373\n",
      "ep 3769: ep_len:500 episode reward: total was -8.420000. running mean: 21.608569\n",
      "ep 3769: ep_len:544 episode reward: total was 1.250000. running mean: 21.404983\n",
      "ep 3769: ep_len:500 episode reward: total was 26.050000. running mean: 21.451433\n",
      "ep 3769: ep_len:56 episode reward: total was -0.650000. running mean: 21.230419\n",
      "ep 3769: ep_len:3 episode reward: total was 1.010000. running mean: 21.028215\n",
      "ep 3769: ep_len:314 episode reward: total was 34.220000. running mean: 21.160133\n",
      "ep 3769: ep_len:515 episode reward: total was 37.350000. running mean: 21.322031\n",
      "epsilon:0.032863 episode_count: 26390. steps_count: 11406623.000000\n",
      "Time elapsed:  34508.08701300621\n",
      "ep 3770: ep_len:530 episode reward: total was 44.040000. running mean: 21.549211\n",
      "ep 3770: ep_len:524 episode reward: total was -101.350000. running mean: 20.320219\n",
      "ep 3770: ep_len:535 episode reward: total was 5.160000. running mean: 20.168617\n",
      "ep 3770: ep_len:505 episode reward: total was 71.030000. running mean: 20.677231\n",
      "ep 3770: ep_len:3 episode reward: total was 1.010000. running mean: 20.480558\n",
      "ep 3770: ep_len:644 episode reward: total was 45.020000. running mean: 20.725953\n",
      "ep 3770: ep_len:504 episode reward: total was 2.140000. running mean: 20.540093\n",
      "epsilon:0.032819 episode_count: 26397. steps_count: 11409868.000000\n",
      "Time elapsed:  34513.0534760952\n",
      "ep 3771: ep_len:631 episode reward: total was 45.440000. running mean: 20.789092\n",
      "ep 3771: ep_len:594 episode reward: total was 34.270000. running mean: 20.923901\n",
      "ep 3771: ep_len:559 episode reward: total was -9.630000. running mean: 20.618362\n",
      "ep 3771: ep_len:502 episode reward: total was 21.440000. running mean: 20.626579\n",
      "ep 3771: ep_len:128 episode reward: total was 33.880000. running mean: 20.759113\n",
      "ep 3771: ep_len:633 episode reward: total was 35.760000. running mean: 20.909122\n",
      "ep 3771: ep_len:576 episode reward: total was 56.810000. running mean: 21.268131\n",
      "epsilon:0.032775 episode_count: 26404. steps_count: 11413491.000000\n",
      "Time elapsed:  34522.19759583473\n",
      "ep 3772: ep_len:594 episode reward: total was 53.750000. running mean: 21.592949\n",
      "ep 3772: ep_len:501 episode reward: total was 33.180000. running mean: 21.708820\n",
      "ep 3772: ep_len:537 episode reward: total was -0.730000. running mean: 21.484432\n",
      "ep 3772: ep_len:386 episode reward: total was 16.160000. running mean: 21.431187\n",
      "ep 3772: ep_len:3 episode reward: total was 1.010000. running mean: 21.226975\n",
      "ep 3772: ep_len:605 episode reward: total was 60.620000. running mean: 21.620906\n",
      "ep 3772: ep_len:522 episode reward: total was -5.010000. running mean: 21.354597\n",
      "epsilon:0.032730 episode_count: 26411. steps_count: 11416639.000000\n",
      "Time elapsed:  34541.87769532204\n",
      "ep 3773: ep_len:522 episode reward: total was -23.170000. running mean: 20.909351\n",
      "ep 3773: ep_len:543 episode reward: total was 2.580000. running mean: 20.726057\n",
      "ep 3773: ep_len:587 episode reward: total was 44.750000. running mean: 20.966296\n",
      "ep 3773: ep_len:500 episode reward: total was 36.390000. running mean: 21.120534\n",
      "ep 3773: ep_len:3 episode reward: total was 1.010000. running mean: 20.919428\n",
      "ep 3773: ep_len:672 episode reward: total was 11.090000. running mean: 20.821134\n",
      "ep 3773: ep_len:699 episode reward: total was -276.110000. running mean: 17.851823\n",
      "epsilon:0.032686 episode_count: 26418. steps_count: 11420165.000000\n",
      "Time elapsed:  34550.9837911129\n",
      "ep 3774: ep_len:261 episode reward: total was 19.940000. running mean: 17.872704\n",
      "ep 3774: ep_len:157 episode reward: total was -2.230000. running mean: 17.671677\n",
      "ep 3774: ep_len:588 episode reward: total was -26.980000. running mean: 17.225161\n",
      "ep 3774: ep_len:500 episode reward: total was 22.050000. running mean: 17.273409\n",
      "ep 3774: ep_len:3 episode reward: total was 1.010000. running mean: 17.110775\n",
      "ep 3774: ep_len:500 episode reward: total was 46.160000. running mean: 17.401267\n",
      "ep 3774: ep_len:521 episode reward: total was -37.870000. running mean: 16.848554\n",
      "epsilon:0.032642 episode_count: 26425. steps_count: 11422695.000000\n",
      "Time elapsed:  34563.588441610336\n",
      "ep 3775: ep_len:576 episode reward: total was -88.240000. running mean: 15.797669\n",
      "ep 3775: ep_len:508 episode reward: total was 97.110000. running mean: 16.610792\n",
      "ep 3775: ep_len:557 episode reward: total was -32.820000. running mean: 16.116484\n",
      "ep 3775: ep_len:621 episode reward: total was 98.330000. running mean: 16.938619\n",
      "ep 3775: ep_len:3 episode reward: total was 1.010000. running mean: 16.779333\n",
      "ep 3775: ep_len:524 episode reward: total was 8.660000. running mean: 16.698140\n",
      "ep 3775: ep_len:595 episode reward: total was 43.410000. running mean: 16.965258\n",
      "epsilon:0.032597 episode_count: 26432. steps_count: 11426079.000000\n",
      "Time elapsed:  34578.28226995468\n",
      "ep 3776: ep_len:550 episode reward: total was -41.590000. running mean: 16.379706\n",
      "ep 3776: ep_len:507 episode reward: total was 11.830000. running mean: 16.334209\n",
      "ep 3776: ep_len:500 episode reward: total was 30.260000. running mean: 16.473467\n",
      "ep 3776: ep_len:531 episode reward: total was 57.530000. running mean: 16.884032\n",
      "ep 3776: ep_len:3 episode reward: total was 1.010000. running mean: 16.725292\n",
      "ep 3776: ep_len:620 episode reward: total was 28.890000. running mean: 16.846939\n",
      "ep 3776: ep_len:622 episode reward: total was 43.950000. running mean: 17.117969\n",
      "epsilon:0.032553 episode_count: 26439. steps_count: 11429412.000000\n",
      "Time elapsed:  34593.20331430435\n",
      "ep 3777: ep_len:647 episode reward: total was -13.350000. running mean: 16.813290\n",
      "ep 3777: ep_len:500 episode reward: total was 32.380000. running mean: 16.968957\n",
      "ep 3777: ep_len:431 episode reward: total was 44.270000. running mean: 17.241967\n",
      "ep 3777: ep_len:500 episode reward: total was 59.780000. running mean: 17.667348\n",
      "ep 3777: ep_len:131 episode reward: total was 35.350000. running mean: 17.844174\n",
      "ep 3777: ep_len:160 episode reward: total was 29.120000. running mean: 17.956932\n",
      "ep 3777: ep_len:500 episode reward: total was -29.550000. running mean: 17.481863\n",
      "epsilon:0.032509 episode_count: 26446. steps_count: 11432281.000000\n",
      "Time elapsed:  34600.476111888885\n",
      "ep 3778: ep_len:555 episode reward: total was 58.530000. running mean: 17.892344\n",
      "ep 3778: ep_len:508 episode reward: total was -13.030000. running mean: 17.583121\n",
      "ep 3778: ep_len:500 episode reward: total was 20.550000. running mean: 17.612790\n",
      "ep 3778: ep_len:500 episode reward: total was 11.040000. running mean: 17.547062\n",
      "ep 3778: ep_len:53 episode reward: total was 24.510000. running mean: 17.616691\n",
      "ep 3778: ep_len:506 episode reward: total was 4.070000. running mean: 17.481224\n",
      "ep 3778: ep_len:501 episode reward: total was -14.060000. running mean: 17.165812\n",
      "epsilon:0.032464 episode_count: 26453. steps_count: 11435404.000000\n",
      "Time elapsed:  34608.79500436783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3779: ep_len:635 episode reward: total was 38.010000. running mean: 17.374254\n",
      "ep 3779: ep_len:500 episode reward: total was -11.360000. running mean: 17.086911\n",
      "ep 3779: ep_len:550 episode reward: total was -144.850000. running mean: 15.467542\n",
      "ep 3779: ep_len:600 episode reward: total was 36.960000. running mean: 15.682467\n",
      "ep 3779: ep_len:131 episode reward: total was 38.870000. running mean: 15.914342\n",
      "ep 3779: ep_len:540 episode reward: total was -22.000000. running mean: 15.535199\n",
      "ep 3779: ep_len:502 episode reward: total was 16.060000. running mean: 15.540447\n",
      "epsilon:0.032420 episode_count: 26460. steps_count: 11438862.000000\n",
      "Time elapsed:  34630.8483607769\n",
      "ep 3780: ep_len:500 episode reward: total was -32.170000. running mean: 15.063342\n",
      "ep 3780: ep_len:296 episode reward: total was 32.900000. running mean: 15.241709\n",
      "ep 3780: ep_len:383 episode reward: total was 34.500000. running mean: 15.434292\n",
      "ep 3780: ep_len:508 episode reward: total was 8.800000. running mean: 15.367949\n",
      "ep 3780: ep_len:3 episode reward: total was 1.010000. running mean: 15.224369\n",
      "ep 3780: ep_len:604 episode reward: total was -30.950000. running mean: 14.762626\n",
      "ep 3780: ep_len:611 episode reward: total was 30.220000. running mean: 14.917200\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.032376 episode_count: 26467. steps_count: 11441767.000000\n",
      "Time elapsed:  34647.8129837513\n",
      "ep 3781: ep_len:596 episode reward: total was -35.770000. running mean: 14.410328\n",
      "ep 3781: ep_len:255 episode reward: total was 37.570000. running mean: 14.641924\n",
      "ep 3781: ep_len:380 episode reward: total was 59.970000. running mean: 15.095205\n",
      "ep 3781: ep_len:500 episode reward: total was 61.120000. running mean: 15.555453\n",
      "ep 3781: ep_len:3 episode reward: total was 1.010000. running mean: 15.409998\n",
      "ep 3781: ep_len:500 episode reward: total was 33.170000. running mean: 15.587598\n",
      "ep 3781: ep_len:520 episode reward: total was 25.700000. running mean: 15.688722\n",
      "epsilon:0.032331 episode_count: 26474. steps_count: 11444521.000000\n",
      "Time elapsed:  34655.14412903786\n",
      "ep 3782: ep_len:521 episode reward: total was 19.680000. running mean: 15.728635\n",
      "ep 3782: ep_len:522 episode reward: total was 28.680000. running mean: 15.858149\n",
      "ep 3782: ep_len:550 episode reward: total was 37.060000. running mean: 16.070167\n",
      "ep 3782: ep_len:533 episode reward: total was 79.120000. running mean: 16.700666\n",
      "ep 3782: ep_len:3 episode reward: total was 1.010000. running mean: 16.543759\n",
      "ep 3782: ep_len:597 episode reward: total was 17.230000. running mean: 16.550621\n",
      "ep 3782: ep_len:613 episode reward: total was 20.750000. running mean: 16.592615\n",
      "epsilon:0.032287 episode_count: 26481. steps_count: 11447860.000000\n",
      "Time elapsed:  34670.69026565552\n",
      "ep 3783: ep_len:677 episode reward: total was 3.910000. running mean: 16.465789\n",
      "ep 3783: ep_len:523 episode reward: total was -62.130000. running mean: 15.679831\n",
      "ep 3783: ep_len:520 episode reward: total was 40.680000. running mean: 15.929833\n",
      "ep 3783: ep_len:543 episode reward: total was 41.890000. running mean: 16.189435\n",
      "ep 3783: ep_len:49 episode reward: total was 20.000000. running mean: 16.227540\n",
      "ep 3783: ep_len:561 episode reward: total was 46.190000. running mean: 16.527165\n",
      "ep 3783: ep_len:592 episode reward: total was 42.910000. running mean: 16.790993\n",
      "epsilon:0.032243 episode_count: 26488. steps_count: 11451325.000000\n",
      "Time elapsed:  34679.70009088516\n",
      "ep 3784: ep_len:540 episode reward: total was -6.990000. running mean: 16.553183\n",
      "ep 3784: ep_len:506 episode reward: total was -9.430000. running mean: 16.293351\n",
      "ep 3784: ep_len:500 episode reward: total was 35.340000. running mean: 16.483818\n",
      "ep 3784: ep_len:500 episode reward: total was 28.130000. running mean: 16.600280\n",
      "ep 3784: ep_len:77 episode reward: total was 17.260000. running mean: 16.606877\n",
      "ep 3784: ep_len:557 episode reward: total was 4.990000. running mean: 16.490708\n",
      "ep 3784: ep_len:501 episode reward: total was -79.480000. running mean: 15.531001\n",
      "epsilon:0.032198 episode_count: 26495. steps_count: 11454506.000000\n",
      "Time elapsed:  34688.03824830055\n",
      "ep 3785: ep_len:515 episode reward: total was 29.840000. running mean: 15.674091\n",
      "ep 3785: ep_len:371 episode reward: total was -14.230000. running mean: 15.375050\n",
      "ep 3785: ep_len:582 episode reward: total was -35.750000. running mean: 14.863800\n",
      "ep 3785: ep_len:501 episode reward: total was -8.790000. running mean: 14.627262\n",
      "ep 3785: ep_len:3 episode reward: total was 1.010000. running mean: 14.491089\n",
      "ep 3785: ep_len:615 episode reward: total was 46.480000. running mean: 14.810978\n",
      "ep 3785: ep_len:594 episode reward: total was 41.520000. running mean: 15.078068\n",
      "epsilon:0.032154 episode_count: 26502. steps_count: 11457687.000000\n",
      "Time elapsed:  34696.62771320343\n",
      "ep 3786: ep_len:554 episode reward: total was 6.260000. running mean: 14.989888\n",
      "ep 3786: ep_len:282 episode reward: total was 15.160000. running mean: 14.991589\n",
      "ep 3786: ep_len:378 episode reward: total was 6.890000. running mean: 14.910573\n",
      "ep 3786: ep_len:503 episode reward: total was 16.030000. running mean: 14.921767\n",
      "ep 3786: ep_len:48 episode reward: total was 19.500000. running mean: 14.967550\n",
      "ep 3786: ep_len:518 episode reward: total was 31.610000. running mean: 15.133974\n",
      "ep 3786: ep_len:592 episode reward: total was 18.700000. running mean: 15.169634\n",
      "epsilon:0.032110 episode_count: 26509. steps_count: 11460562.000000\n",
      "Time elapsed:  34704.31886386871\n",
      "ep 3787: ep_len:199 episode reward: total was -18.350000. running mean: 14.834438\n",
      "ep 3787: ep_len:582 episode reward: total was -49.180000. running mean: 14.194294\n",
      "ep 3787: ep_len:618 episode reward: total was -8.490000. running mean: 13.967451\n",
      "ep 3787: ep_len:512 episode reward: total was 75.570000. running mean: 14.583476\n",
      "ep 3787: ep_len:3 episode reward: total was 1.010000. running mean: 14.447741\n",
      "ep 3787: ep_len:636 episode reward: total was 31.910000. running mean: 14.622364\n",
      "ep 3787: ep_len:545 episode reward: total was 50.960000. running mean: 14.985740\n",
      "epsilon:0.032065 episode_count: 26516. steps_count: 11463657.000000\n",
      "Time elapsed:  34720.048979997635\n",
      "ep 3788: ep_len:626 episode reward: total was -38.200000. running mean: 14.453883\n",
      "ep 3788: ep_len:612 episode reward: total was 55.440000. running mean: 14.863744\n",
      "ep 3788: ep_len:419 episode reward: total was 33.860000. running mean: 15.053707\n",
      "ep 3788: ep_len:565 episode reward: total was 61.920000. running mean: 15.522370\n",
      "ep 3788: ep_len:3 episode reward: total was 1.010000. running mean: 15.377246\n",
      "ep 3788: ep_len:500 episode reward: total was 28.930000. running mean: 15.512773\n",
      "ep 3788: ep_len:500 episode reward: total was -7.910000. running mean: 15.278546\n",
      "epsilon:0.032021 episode_count: 26523. steps_count: 11466882.000000\n",
      "Time elapsed:  34728.54693341255\n",
      "ep 3789: ep_len:511 episode reward: total was 78.490000. running mean: 15.910660\n",
      "ep 3789: ep_len:585 episode reward: total was 119.190000. running mean: 16.943454\n",
      "ep 3789: ep_len:568 episode reward: total was -3.350000. running mean: 16.740519\n",
      "ep 3789: ep_len:520 episode reward: total was 30.950000. running mean: 16.882614\n",
      "ep 3789: ep_len:1 episode reward: total was -1.000000. running mean: 16.703788\n",
      "ep 3789: ep_len:500 episode reward: total was -2.640000. running mean: 16.510350\n",
      "ep 3789: ep_len:500 episode reward: total was 32.920000. running mean: 16.674446\n",
      "epsilon:0.031977 episode_count: 26530. steps_count: 11470067.000000\n",
      "Time elapsed:  34736.90448331833\n",
      "ep 3790: ep_len:504 episode reward: total was 61.120000. running mean: 17.118902\n",
      "ep 3790: ep_len:502 episode reward: total was 25.700000. running mean: 17.204713\n",
      "ep 3790: ep_len:542 episode reward: total was -30.890000. running mean: 16.723766\n",
      "ep 3790: ep_len:540 episode reward: total was 81.070000. running mean: 17.367228\n",
      "ep 3790: ep_len:3 episode reward: total was 1.010000. running mean: 17.203656\n",
      "ep 3790: ep_len:642 episode reward: total was 3.130000. running mean: 17.062919\n",
      "ep 3790: ep_len:194 episode reward: total was 9.130000. running mean: 16.983590\n",
      "epsilon:0.031932 episode_count: 26537. steps_count: 11472994.000000\n",
      "Time elapsed:  34744.56949019432\n",
      "ep 3791: ep_len:557 episode reward: total was 60.810000. running mean: 17.421854\n",
      "ep 3791: ep_len:261 episode reward: total was 30.280000. running mean: 17.550436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3791: ep_len:546 episode reward: total was -2.450000. running mean: 17.350431\n",
      "ep 3791: ep_len:535 episode reward: total was 5.210000. running mean: 17.229027\n",
      "ep 3791: ep_len:104 episode reward: total was 26.750000. running mean: 17.324237\n",
      "ep 3791: ep_len:613 episode reward: total was 48.430000. running mean: 17.635294\n",
      "ep 3791: ep_len:571 episode reward: total was 64.720000. running mean: 18.106141\n",
      "epsilon:0.031888 episode_count: 26544. steps_count: 11476181.000000\n",
      "Time elapsed:  34752.99243736267\n",
      "ep 3792: ep_len:216 episode reward: total was 23.120000. running mean: 18.156280\n",
      "ep 3792: ep_len:583 episode reward: total was 39.790000. running mean: 18.372617\n",
      "ep 3792: ep_len:605 episode reward: total was 33.390000. running mean: 18.522791\n",
      "ep 3792: ep_len:500 episode reward: total was 16.220000. running mean: 18.499763\n",
      "ep 3792: ep_len:89 episode reward: total was 28.220000. running mean: 18.596965\n",
      "ep 3792: ep_len:596 episode reward: total was 14.980000. running mean: 18.560796\n",
      "ep 3792: ep_len:597 episode reward: total was -5.480000. running mean: 18.320388\n",
      "epsilon:0.031844 episode_count: 26551. steps_count: 11479367.000000\n",
      "Time elapsed:  34761.25703191757\n",
      "ep 3793: ep_len:233 episode reward: total was 20.940000. running mean: 18.346584\n",
      "ep 3793: ep_len:652 episode reward: total was 89.190000. running mean: 19.055018\n",
      "ep 3793: ep_len:616 episode reward: total was -2.430000. running mean: 18.840168\n",
      "ep 3793: ep_len:500 episode reward: total was -29.490000. running mean: 18.356866\n",
      "ep 3793: ep_len:3 episode reward: total was 1.010000. running mean: 18.183398\n",
      "ep 3793: ep_len:639 episode reward: total was 26.260000. running mean: 18.264164\n",
      "ep 3793: ep_len:605 episode reward: total was 29.980000. running mean: 18.381322\n",
      "epsilon:0.031799 episode_count: 26558. steps_count: 11482615.000000\n",
      "Time elapsed:  34769.8730700016\n",
      "ep 3794: ep_len:603 episode reward: total was 18.250000. running mean: 18.380009\n",
      "ep 3794: ep_len:518 episode reward: total was 101.330000. running mean: 19.209509\n",
      "ep 3794: ep_len:420 episode reward: total was -5.070000. running mean: 18.966714\n",
      "ep 3794: ep_len:502 episode reward: total was 36.080000. running mean: 19.137846\n",
      "ep 3794: ep_len:3 episode reward: total was 1.010000. running mean: 18.956568\n",
      "ep 3794: ep_len:537 episode reward: total was 22.210000. running mean: 18.989102\n",
      "ep 3794: ep_len:153 episode reward: total was 5.240000. running mean: 18.851611\n",
      "epsilon:0.031755 episode_count: 26565. steps_count: 11485351.000000\n",
      "Time elapsed:  34777.282417058945\n",
      "ep 3795: ep_len:258 episode reward: total was 7.910000. running mean: 18.742195\n",
      "ep 3795: ep_len:500 episode reward: total was 20.740000. running mean: 18.762173\n",
      "ep 3795: ep_len:500 episode reward: total was 48.800000. running mean: 19.062551\n",
      "ep 3795: ep_len:500 episode reward: total was 69.860000. running mean: 19.570526\n",
      "ep 3795: ep_len:3 episode reward: total was 1.010000. running mean: 19.384921\n",
      "ep 3795: ep_len:664 episode reward: total was 37.020000. running mean: 19.561272\n",
      "ep 3795: ep_len:256 episode reward: total was 21.660000. running mean: 19.582259\n",
      "epsilon:0.031711 episode_count: 26572. steps_count: 11488032.000000\n",
      "Time elapsed:  34784.563323020935\n",
      "ep 3796: ep_len:515 episode reward: total was 63.050000. running mean: 20.016936\n",
      "ep 3796: ep_len:585 episode reward: total was 25.260000. running mean: 20.069367\n",
      "ep 3796: ep_len:500 episode reward: total was 31.880000. running mean: 20.187473\n",
      "ep 3796: ep_len:500 episode reward: total was 26.960000. running mean: 20.255198\n",
      "ep 3796: ep_len:3 episode reward: total was 1.010000. running mean: 20.062746\n",
      "ep 3796: ep_len:511 episode reward: total was 8.280000. running mean: 19.944919\n",
      "ep 3796: ep_len:289 episode reward: total was 7.850000. running mean: 19.823970\n",
      "epsilon:0.031666 episode_count: 26579. steps_count: 11490935.000000\n",
      "Time elapsed:  34791.86377310753\n",
      "ep 3797: ep_len:500 episode reward: total was 94.470000. running mean: 20.570430\n",
      "ep 3797: ep_len:595 episode reward: total was 37.750000. running mean: 20.742226\n",
      "ep 3797: ep_len:506 episode reward: total was 18.630000. running mean: 20.721104\n",
      "ep 3797: ep_len:118 episode reward: total was 10.610000. running mean: 20.619993\n",
      "ep 3797: ep_len:55 episode reward: total was 26.000000. running mean: 20.673793\n",
      "ep 3797: ep_len:545 episode reward: total was -25.180000. running mean: 20.215255\n",
      "ep 3797: ep_len:507 episode reward: total was 46.460000. running mean: 20.477702\n",
      "epsilon:0.031622 episode_count: 26586. steps_count: 11493761.000000\n",
      "Time elapsed:  34806.41025066376\n",
      "ep 3798: ep_len:500 episode reward: total was 41.690000. running mean: 20.689825\n",
      "ep 3798: ep_len:643 episode reward: total was 67.110000. running mean: 21.154027\n",
      "ep 3798: ep_len:569 episode reward: total was -19.440000. running mean: 20.748087\n",
      "ep 3798: ep_len:500 episode reward: total was 53.780000. running mean: 21.078406\n",
      "ep 3798: ep_len:3 episode reward: total was 1.010000. running mean: 20.877722\n",
      "ep 3798: ep_len:500 episode reward: total was 36.130000. running mean: 21.030244\n",
      "ep 3798: ep_len:567 episode reward: total was 56.820000. running mean: 21.388142\n",
      "epsilon:0.031578 episode_count: 26593. steps_count: 11497043.000000\n",
      "Time elapsed:  34820.85895514488\n",
      "ep 3799: ep_len:515 episode reward: total was 43.090000. running mean: 21.605161\n",
      "ep 3799: ep_len:537 episode reward: total was 2.980000. running mean: 21.418909\n",
      "ep 3799: ep_len:603 episode reward: total was -2.340000. running mean: 21.181320\n",
      "ep 3799: ep_len:392 episode reward: total was 14.740000. running mean: 21.116907\n",
      "ep 3799: ep_len:3 episode reward: total was 1.010000. running mean: 20.915838\n",
      "ep 3799: ep_len:291 episode reward: total was -87.710000. running mean: 19.829579\n",
      "ep 3799: ep_len:500 episode reward: total was -6.130000. running mean: 19.569983\n",
      "epsilon:0.031533 episode_count: 26600. steps_count: 11499884.000000\n",
      "Time elapsed:  34828.455562353134\n",
      "ep 3800: ep_len:545 episode reward: total was 44.010000. running mean: 19.814384\n",
      "ep 3800: ep_len:546 episode reward: total was 35.520000. running mean: 19.971440\n",
      "ep 3800: ep_len:500 episode reward: total was 40.710000. running mean: 20.178825\n",
      "ep 3800: ep_len:860 episode reward: total was -533.080000. running mean: 14.646237\n",
      "ep 3800: ep_len:96 episode reward: total was 27.770000. running mean: 14.777475\n",
      "ep 3800: ep_len:242 episode reward: total was 25.120000. running mean: 14.880900\n",
      "ep 3800: ep_len:536 episode reward: total was 0.640000. running mean: 14.738491\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.031489 episode_count: 26607. steps_count: 11503209.000000\n",
      "Time elapsed:  34842.07778811455\n",
      "ep 3801: ep_len:653 episode reward: total was -10.260000. running mean: 14.488506\n",
      "ep 3801: ep_len:509 episode reward: total was -17.640000. running mean: 14.167221\n",
      "ep 3801: ep_len:500 episode reward: total was 52.050000. running mean: 14.546049\n",
      "ep 3801: ep_len:582 episode reward: total was 76.960000. running mean: 15.170188\n",
      "ep 3801: ep_len:107 episode reward: total was 33.270000. running mean: 15.351186\n",
      "ep 3801: ep_len:500 episode reward: total was -0.090000. running mean: 15.196775\n",
      "ep 3801: ep_len:584 episode reward: total was 17.110000. running mean: 15.215907\n",
      "epsilon:0.031445 episode_count: 26614. steps_count: 11506644.000000\n",
      "Time elapsed:  34851.06365132332\n",
      "ep 3802: ep_len:566 episode reward: total was 73.020000. running mean: 15.793948\n",
      "ep 3802: ep_len:500 episode reward: total was 39.610000. running mean: 16.032108\n",
      "ep 3802: ep_len:543 episode reward: total was -29.460000. running mean: 15.577187\n",
      "ep 3802: ep_len:126 episode reward: total was 5.080000. running mean: 15.472215\n",
      "ep 3802: ep_len:3 episode reward: total was 1.010000. running mean: 15.327593\n",
      "ep 3802: ep_len:525 episode reward: total was -109.270000. running mean: 14.081617\n",
      "ep 3802: ep_len:196 episode reward: total was 15.040000. running mean: 14.091201\n",
      "epsilon:0.031400 episode_count: 26621. steps_count: 11509103.000000\n",
      "Time elapsed:  34857.847561597824\n",
      "ep 3803: ep_len:110 episode reward: total was 2.590000. running mean: 13.976189\n",
      "ep 3803: ep_len:558 episode reward: total was 20.820000. running mean: 14.044627\n",
      "ep 3803: ep_len:619 episode reward: total was 19.880000. running mean: 14.102981\n",
      "ep 3803: ep_len:500 episode reward: total was 42.240000. running mean: 14.384351\n",
      "ep 3803: ep_len:2 episode reward: total was -0.500000. running mean: 14.235508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3803: ep_len:551 episode reward: total was -57.300000. running mean: 13.520153\n",
      "ep 3803: ep_len:551 episode reward: total was 20.720000. running mean: 13.592151\n",
      "epsilon:0.031356 episode_count: 26628. steps_count: 11511994.000000\n",
      "Time elapsed:  34865.56879711151\n",
      "ep 3804: ep_len:516 episode reward: total was 62.830000. running mean: 14.084529\n",
      "ep 3804: ep_len:530 episode reward: total was 34.410000. running mean: 14.287784\n",
      "ep 3804: ep_len:676 episode reward: total was 23.990000. running mean: 14.384806\n",
      "ep 3804: ep_len:512 episode reward: total was 66.640000. running mean: 14.907358\n",
      "ep 3804: ep_len:56 episode reward: total was 24.510000. running mean: 15.003385\n",
      "ep 3804: ep_len:300 episode reward: total was 24.250000. running mean: 15.095851\n",
      "ep 3804: ep_len:500 episode reward: total was 68.450000. running mean: 15.629392\n",
      "epsilon:0.031312 episode_count: 26635. steps_count: 11515084.000000\n",
      "Time elapsed:  34881.40168952942\n",
      "ep 3805: ep_len:124 episode reward: total was 1.510000. running mean: 15.488198\n",
      "ep 3805: ep_len:545 episode reward: total was 69.780000. running mean: 16.031116\n",
      "ep 3805: ep_len:526 episode reward: total was -37.910000. running mean: 15.491705\n",
      "ep 3805: ep_len:500 episode reward: total was 63.390000. running mean: 15.970688\n",
      "ep 3805: ep_len:80 episode reward: total was 16.620000. running mean: 15.977181\n",
      "ep 3805: ep_len:573 episode reward: total was -14.690000. running mean: 15.670510\n",
      "ep 3805: ep_len:512 episode reward: total was 18.400000. running mean: 15.697804\n",
      "epsilon:0.031267 episode_count: 26642. steps_count: 11517944.000000\n",
      "Time elapsed:  34889.04933476448\n",
      "ep 3806: ep_len:569 episode reward: total was 88.510000. running mean: 16.425926\n",
      "ep 3806: ep_len:500 episode reward: total was 19.190000. running mean: 16.453567\n",
      "ep 3806: ep_len:616 episode reward: total was -15.750000. running mean: 16.131531\n",
      "ep 3806: ep_len:611 episode reward: total was 16.690000. running mean: 16.137116\n",
      "ep 3806: ep_len:3 episode reward: total was 1.010000. running mean: 15.985845\n",
      "ep 3806: ep_len:585 episode reward: total was 12.310000. running mean: 15.949087\n",
      "ep 3806: ep_len:541 episode reward: total was 37.010000. running mean: 16.159696\n",
      "epsilon:0.031223 episode_count: 26649. steps_count: 11521369.000000\n",
      "Time elapsed:  34904.009116888046\n",
      "ep 3807: ep_len:516 episode reward: total was 70.610000. running mean: 16.704199\n",
      "ep 3807: ep_len:581 episode reward: total was 46.730000. running mean: 17.004457\n",
      "ep 3807: ep_len:500 episode reward: total was 52.960000. running mean: 17.364012\n",
      "ep 3807: ep_len:544 episode reward: total was 28.280000. running mean: 17.473172\n",
      "ep 3807: ep_len:102 episode reward: total was 28.230000. running mean: 17.580740\n",
      "ep 3807: ep_len:500 episode reward: total was 26.380000. running mean: 17.668733\n",
      "ep 3807: ep_len:522 episode reward: total was 13.740000. running mean: 17.629446\n",
      "epsilon:0.031179 episode_count: 26656. steps_count: 11524634.000000\n",
      "Time elapsed:  34912.50875520706\n",
      "ep 3808: ep_len:619 episode reward: total was -5.740000. running mean: 17.395751\n",
      "ep 3808: ep_len:506 episode reward: total was 66.860000. running mean: 17.890394\n",
      "ep 3808: ep_len:541 episode reward: total was -41.160000. running mean: 17.299890\n",
      "ep 3808: ep_len:500 episode reward: total was 37.620000. running mean: 17.503091\n",
      "ep 3808: ep_len:3 episode reward: total was 1.010000. running mean: 17.338160\n",
      "ep 3808: ep_len:504 episode reward: total was 7.800000. running mean: 17.242778\n",
      "ep 3808: ep_len:521 episode reward: total was 42.450000. running mean: 17.494850\n",
      "epsilon:0.031134 episode_count: 26663. steps_count: 11527828.000000\n",
      "Time elapsed:  34919.88330554962\n",
      "ep 3809: ep_len:258 episode reward: total was 24.920000. running mean: 17.569102\n",
      "ep 3809: ep_len:500 episode reward: total was 101.900000. running mean: 18.412411\n",
      "ep 3809: ep_len:639 episode reward: total was 16.480000. running mean: 18.393087\n",
      "ep 3809: ep_len:511 episode reward: total was -76.700000. running mean: 17.442156\n",
      "ep 3809: ep_len:3 episode reward: total was 1.010000. running mean: 17.277834\n",
      "ep 3809: ep_len:579 episode reward: total was 28.160000. running mean: 17.386656\n",
      "ep 3809: ep_len:191 episode reward: total was -34.630000. running mean: 16.866490\n",
      "epsilon:0.031090 episode_count: 26670. steps_count: 11530509.000000\n",
      "Time elapsed:  34924.796748399734\n",
      "ep 3810: ep_len:570 episode reward: total was 69.820000. running mean: 17.396025\n",
      "ep 3810: ep_len:288 episode reward: total was 26.270000. running mean: 17.484764\n",
      "ep 3810: ep_len:500 episode reward: total was -51.860000. running mean: 16.791317\n",
      "ep 3810: ep_len:515 episode reward: total was 13.670000. running mean: 16.760104\n",
      "ep 3810: ep_len:3 episode reward: total was 1.010000. running mean: 16.602603\n",
      "ep 3810: ep_len:500 episode reward: total was 11.320000. running mean: 16.549776\n",
      "ep 3810: ep_len:517 episode reward: total was 23.980000. running mean: 16.624079\n",
      "epsilon:0.031046 episode_count: 26677. steps_count: 11533402.000000\n",
      "Time elapsed:  34939.360776901245\n",
      "ep 3811: ep_len:560 episode reward: total was 53.620000. running mean: 16.994038\n",
      "ep 3811: ep_len:500 episode reward: total was 100.580000. running mean: 17.829898\n",
      "ep 3811: ep_len:526 episode reward: total was 15.990000. running mean: 17.811499\n",
      "ep 3811: ep_len:507 episode reward: total was 44.010000. running mean: 18.073484\n",
      "ep 3811: ep_len:93 episode reward: total was 31.230000. running mean: 18.205049\n",
      "ep 3811: ep_len:644 episode reward: total was -5.020000. running mean: 17.972798\n",
      "ep 3811: ep_len:500 episode reward: total was 18.130000. running mean: 17.974370\n",
      "epsilon:0.031001 episode_count: 26684. steps_count: 11536732.000000\n",
      "Time elapsed:  34948.08199048042\n",
      "ep 3812: ep_len:500 episode reward: total was 85.740000. running mean: 18.652027\n",
      "ep 3812: ep_len:524 episode reward: total was 68.050000. running mean: 19.146006\n",
      "ep 3812: ep_len:467 episode reward: total was 35.910000. running mean: 19.313646\n",
      "ep 3812: ep_len:529 episode reward: total was 23.590000. running mean: 19.356410\n",
      "ep 3812: ep_len:83 episode reward: total was 25.280000. running mean: 19.415646\n",
      "ep 3812: ep_len:562 episode reward: total was 22.080000. running mean: 19.442289\n",
      "ep 3812: ep_len:556 episode reward: total was 27.870000. running mean: 19.526566\n",
      "epsilon:0.030957 episode_count: 26691. steps_count: 11539953.000000\n",
      "Time elapsed:  34956.64960408211\n",
      "ep 3813: ep_len:619 episode reward: total was -22.110000. running mean: 19.110201\n",
      "ep 3813: ep_len:500 episode reward: total was 10.310000. running mean: 19.022199\n",
      "ep 3813: ep_len:551 episode reward: total was -13.730000. running mean: 18.694677\n",
      "ep 3813: ep_len:500 episode reward: total was 82.750000. running mean: 19.335230\n",
      "ep 3813: ep_len:88 episode reward: total was 23.740000. running mean: 19.379278\n",
      "ep 3813: ep_len:620 episode reward: total was 35.550000. running mean: 19.540985\n",
      "ep 3813: ep_len:517 episode reward: total was -1.850000. running mean: 19.327075\n",
      "epsilon:0.030913 episode_count: 26698. steps_count: 11543348.000000\n",
      "Time elapsed:  34965.70527291298\n",
      "ep 3814: ep_len:663 episode reward: total was 36.550000. running mean: 19.499304\n",
      "ep 3814: ep_len:534 episode reward: total was 41.560000. running mean: 19.719911\n",
      "ep 3814: ep_len:560 episode reward: total was -42.830000. running mean: 19.094412\n",
      "ep 3814: ep_len:611 episode reward: total was 78.720000. running mean: 19.690668\n",
      "ep 3814: ep_len:116 episode reward: total was 34.770000. running mean: 19.841461\n",
      "ep 3814: ep_len:660 episode reward: total was -226.210000. running mean: 17.380947\n",
      "ep 3814: ep_len:336 episode reward: total was 29.090000. running mean: 17.498037\n",
      "epsilon:0.030868 episode_count: 26705. steps_count: 11546828.000000\n",
      "Time elapsed:  34975.16845536232\n",
      "ep 3815: ep_len:249 episode reward: total was 22.450000. running mean: 17.547557\n",
      "ep 3815: ep_len:193 episode reward: total was 20.920000. running mean: 17.581281\n",
      "ep 3815: ep_len:599 episode reward: total was -22.860000. running mean: 17.176868\n",
      "ep 3815: ep_len:522 episode reward: total was 51.120000. running mean: 17.516300\n",
      "ep 3815: ep_len:107 episode reward: total was 36.730000. running mean: 17.708437\n",
      "ep 3815: ep_len:542 episode reward: total was 11.320000. running mean: 17.644552\n",
      "ep 3815: ep_len:538 episode reward: total was 10.240000. running mean: 17.570507\n",
      "epsilon:0.030824 episode_count: 26712. steps_count: 11549578.000000\n",
      "Time elapsed:  34982.43021225929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3816: ep_len:500 episode reward: total was 89.230000. running mean: 18.287102\n",
      "ep 3816: ep_len:500 episode reward: total was 50.210000. running mean: 18.606331\n",
      "ep 3816: ep_len:395 episode reward: total was 43.680000. running mean: 18.857067\n",
      "ep 3816: ep_len:500 episode reward: total was 36.360000. running mean: 19.032097\n",
      "ep 3816: ep_len:92 episode reward: total was 22.740000. running mean: 19.069176\n",
      "ep 3816: ep_len:500 episode reward: total was 2.470000. running mean: 18.903184\n",
      "ep 3816: ep_len:289 episode reward: total was -50.710000. running mean: 18.207052\n",
      "epsilon:0.030780 episode_count: 26719. steps_count: 11552354.000000\n",
      "Time elapsed:  34989.96180295944\n",
      "ep 3817: ep_len:508 episode reward: total was 45.630000. running mean: 18.481282\n",
      "ep 3817: ep_len:187 episode reward: total was 25.390000. running mean: 18.550369\n",
      "ep 3817: ep_len:452 episode reward: total was 76.900000. running mean: 19.133865\n",
      "ep 3817: ep_len:393 episode reward: total was -112.980000. running mean: 17.812727\n",
      "ep 3817: ep_len:56 episode reward: total was 21.510000. running mean: 17.849699\n",
      "ep 3817: ep_len:332 episode reward: total was 30.850000. running mean: 17.979702\n",
      "ep 3817: ep_len:323 episode reward: total was 4.870000. running mean: 17.848605\n",
      "epsilon:0.030735 episode_count: 26726. steps_count: 11554605.000000\n",
      "Time elapsed:  34995.90718817711\n",
      "ep 3818: ep_len:619 episode reward: total was 47.250000. running mean: 18.142619\n",
      "ep 3818: ep_len:515 episode reward: total was -21.910000. running mean: 17.742093\n",
      "ep 3818: ep_len:500 episode reward: total was 42.740000. running mean: 17.992072\n",
      "ep 3818: ep_len:500 episode reward: total was 67.490000. running mean: 18.487051\n",
      "ep 3818: ep_len:51 episode reward: total was 21.000000. running mean: 18.512181\n",
      "ep 3818: ep_len:621 episode reward: total was 56.040000. running mean: 18.887459\n",
      "ep 3818: ep_len:500 episode reward: total was -22.780000. running mean: 18.470784\n",
      "epsilon:0.030691 episode_count: 26733. steps_count: 11557911.000000\n",
      "Time elapsed:  35011.75401735306\n",
      "ep 3819: ep_len:212 episode reward: total was 12.090000. running mean: 18.406977\n",
      "ep 3819: ep_len:500 episode reward: total was 20.740000. running mean: 18.430307\n",
      "ep 3819: ep_len:626 episode reward: total was -163.290000. running mean: 16.613104\n",
      "ep 3819: ep_len:500 episode reward: total was -21.100000. running mean: 16.235973\n",
      "ep 3819: ep_len:3 episode reward: total was 1.010000. running mean: 16.083713\n",
      "ep 3819: ep_len:525 episode reward: total was -14.190000. running mean: 15.780976\n",
      "ep 3819: ep_len:350 episode reward: total was 1.620000. running mean: 15.639366\n",
      "epsilon:0.030647 episode_count: 26740. steps_count: 11560627.000000\n",
      "Time elapsed:  35018.928570747375\n",
      "ep 3820: ep_len:564 episode reward: total was 43.800000. running mean: 15.920972\n",
      "ep 3820: ep_len:500 episode reward: total was 124.980000. running mean: 17.011563\n",
      "ep 3820: ep_len:636 episode reward: total was -3.570000. running mean: 16.805747\n",
      "ep 3820: ep_len:541 episode reward: total was 79.350000. running mean: 17.431190\n",
      "ep 3820: ep_len:3 episode reward: total was 1.010000. running mean: 17.266978\n",
      "ep 3820: ep_len:323 episode reward: total was 35.290000. running mean: 17.447208\n",
      "ep 3820: ep_len:285 episode reward: total was 20.380000. running mean: 17.476536\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.030602 episode_count: 26747. steps_count: 11563479.000000\n",
      "Time elapsed:  35031.3047618866\n",
      "ep 3821: ep_len:500 episode reward: total was -32.680000. running mean: 16.974971\n",
      "ep 3821: ep_len:631 episode reward: total was 73.760000. running mean: 17.542821\n",
      "ep 3821: ep_len:573 episode reward: total was -74.930000. running mean: 16.618093\n",
      "ep 3821: ep_len:569 episode reward: total was 57.500000. running mean: 17.026912\n",
      "ep 3821: ep_len:3 episode reward: total was 1.010000. running mean: 16.866743\n",
      "ep 3821: ep_len:500 episode reward: total was 19.070000. running mean: 16.888775\n",
      "ep 3821: ep_len:585 episode reward: total was 45.020000. running mean: 17.170087\n",
      "epsilon:0.030558 episode_count: 26754. steps_count: 11566840.000000\n",
      "Time elapsed:  35040.234885931015\n",
      "ep 3822: ep_len:500 episode reward: total was 95.720000. running mean: 17.955587\n",
      "ep 3822: ep_len:521 episode reward: total was -6.700000. running mean: 17.709031\n",
      "ep 3822: ep_len:581 episode reward: total was -315.840000. running mean: 14.373540\n",
      "ep 3822: ep_len:392 episode reward: total was 13.780000. running mean: 14.367605\n",
      "ep 3822: ep_len:3 episode reward: total was 1.010000. running mean: 14.234029\n",
      "ep 3822: ep_len:500 episode reward: total was 26.550000. running mean: 14.357189\n",
      "ep 3822: ep_len:500 episode reward: total was 53.850000. running mean: 14.752117\n",
      "epsilon:0.030514 episode_count: 26761. steps_count: 11569837.000000\n",
      "Time elapsed:  35048.375499010086\n",
      "ep 3823: ep_len:596 episode reward: total was 58.670000. running mean: 15.191296\n",
      "ep 3823: ep_len:560 episode reward: total was 5.630000. running mean: 15.095683\n",
      "ep 3823: ep_len:625 episode reward: total was 7.950000. running mean: 15.024226\n",
      "ep 3823: ep_len:514 episode reward: total was 12.670000. running mean: 15.000684\n",
      "ep 3823: ep_len:72 episode reward: total was 22.540000. running mean: 15.076077\n",
      "ep 3823: ep_len:503 episode reward: total was 25.240000. running mean: 15.177716\n",
      "ep 3823: ep_len:605 episode reward: total was 24.920000. running mean: 15.275139\n",
      "epsilon:0.030469 episode_count: 26768. steps_count: 11573312.000000\n",
      "Time elapsed:  35057.560822725296\n",
      "ep 3824: ep_len:602 episode reward: total was 46.100000. running mean: 15.583387\n",
      "ep 3824: ep_len:632 episode reward: total was 57.470000. running mean: 16.002254\n",
      "ep 3824: ep_len:620 episode reward: total was -6.760000. running mean: 15.774631\n",
      "ep 3824: ep_len:570 episode reward: total was 63.630000. running mean: 16.253185\n",
      "ep 3824: ep_len:55 episode reward: total was 24.010000. running mean: 16.330753\n",
      "ep 3824: ep_len:500 episode reward: total was 63.510000. running mean: 16.802545\n",
      "ep 3824: ep_len:500 episode reward: total was 21.410000. running mean: 16.848620\n",
      "epsilon:0.030425 episode_count: 26775. steps_count: 11576791.000000\n",
      "Time elapsed:  35073.57008576393\n",
      "ep 3825: ep_len:632 episode reward: total was -14.940000. running mean: 16.530734\n",
      "ep 3825: ep_len:500 episode reward: total was 103.120000. running mean: 17.396626\n",
      "ep 3825: ep_len:370 episode reward: total was 55.860000. running mean: 17.781260\n",
      "ep 3825: ep_len:132 episode reward: total was 22.630000. running mean: 17.829747\n",
      "ep 3825: ep_len:3 episode reward: total was 1.010000. running mean: 17.661550\n",
      "ep 3825: ep_len:537 episode reward: total was 6.490000. running mean: 17.549834\n",
      "ep 3825: ep_len:640 episode reward: total was 20.260000. running mean: 17.576936\n",
      "epsilon:0.030381 episode_count: 26782. steps_count: 11579605.000000\n",
      "Time elapsed:  35094.34716248512\n",
      "ep 3826: ep_len:543 episode reward: total was 22.260000. running mean: 17.623767\n",
      "ep 3826: ep_len:559 episode reward: total was 95.380000. running mean: 18.401329\n",
      "ep 3826: ep_len:662 episode reward: total was 32.300000. running mean: 18.540316\n",
      "ep 3826: ep_len:500 episode reward: total was -271.460000. running mean: 15.640313\n",
      "ep 3826: ep_len:3 episode reward: total was 1.010000. running mean: 15.494010\n",
      "ep 3826: ep_len:531 episode reward: total was 32.140000. running mean: 15.660469\n",
      "ep 3826: ep_len:501 episode reward: total was -2.680000. running mean: 15.477065\n",
      "epsilon:0.030336 episode_count: 26789. steps_count: 11582904.000000\n",
      "Time elapsed:  35102.923125982285\n",
      "ep 3827: ep_len:503 episode reward: total was -40.770000. running mean: 14.914594\n",
      "ep 3827: ep_len:566 episode reward: total was 63.850000. running mean: 15.403948\n",
      "ep 3827: ep_len:700 episode reward: total was -36.340000. running mean: 14.886509\n",
      "ep 3827: ep_len:512 episode reward: total was 31.790000. running mean: 15.055544\n",
      "ep 3827: ep_len:119 episode reward: total was -36.220000. running mean: 14.542788\n",
      "ep 3827: ep_len:500 episode reward: total was 24.270000. running mean: 14.640060\n",
      "ep 3827: ep_len:500 episode reward: total was 59.570000. running mean: 15.089360\n",
      "epsilon:0.030292 episode_count: 26796. steps_count: 11586304.000000\n",
      "Time elapsed:  35118.36780786514\n",
      "ep 3828: ep_len:579 episode reward: total was -36.460000. running mean: 14.573866\n",
      "ep 3828: ep_len:577 episode reward: total was 111.980000. running mean: 15.547927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3828: ep_len:589 episode reward: total was 7.390000. running mean: 15.466348\n",
      "ep 3828: ep_len:143 episode reward: total was 11.550000. running mean: 15.427185\n",
      "ep 3828: ep_len:111 episode reward: total was 26.300000. running mean: 15.535913\n",
      "ep 3828: ep_len:500 episode reward: total was 25.750000. running mean: 15.638054\n",
      "ep 3828: ep_len:548 episode reward: total was 12.180000. running mean: 15.603473\n",
      "epsilon:0.030248 episode_count: 26803. steps_count: 11589351.000000\n",
      "Time elapsed:  35126.43099427223\n",
      "ep 3829: ep_len:598 episode reward: total was 64.280000. running mean: 16.090238\n",
      "ep 3829: ep_len:500 episode reward: total was 36.260000. running mean: 16.291936\n",
      "ep 3829: ep_len:566 episode reward: total was -123.080000. running mean: 14.898217\n",
      "ep 3829: ep_len:500 episode reward: total was 26.150000. running mean: 15.010734\n",
      "ep 3829: ep_len:115 episode reward: total was -43.650000. running mean: 14.424127\n",
      "ep 3829: ep_len:587 episode reward: total was 71.920000. running mean: 14.999086\n",
      "ep 3829: ep_len:162 episode reward: total was -10.560000. running mean: 14.743495\n",
      "epsilon:0.030203 episode_count: 26810. steps_count: 11592379.000000\n",
      "Time elapsed:  35134.35196805\n",
      "ep 3830: ep_len:635 episode reward: total was 49.740000. running mean: 15.093460\n",
      "ep 3830: ep_len:563 episode reward: total was -38.350000. running mean: 14.559025\n",
      "ep 3830: ep_len:578 episode reward: total was -3.430000. running mean: 14.379135\n",
      "ep 3830: ep_len:115 episode reward: total was -12.470000. running mean: 14.110644\n",
      "ep 3830: ep_len:80 episode reward: total was 23.260000. running mean: 14.202137\n",
      "ep 3830: ep_len:663 episode reward: total was 12.890000. running mean: 14.189016\n",
      "ep 3830: ep_len:308 episode reward: total was 27.430000. running mean: 14.321426\n",
      "epsilon:0.030159 episode_count: 26817. steps_count: 11595321.000000\n",
      "Time elapsed:  35142.126421928406\n",
      "ep 3831: ep_len:110 episode reward: total was -15.650000. running mean: 14.021712\n",
      "ep 3831: ep_len:500 episode reward: total was 70.460000. running mean: 14.586095\n",
      "ep 3831: ep_len:607 episode reward: total was 15.120000. running mean: 14.591434\n",
      "ep 3831: ep_len:500 episode reward: total was 72.590000. running mean: 15.171419\n",
      "ep 3831: ep_len:47 episode reward: total was 22.000000. running mean: 15.239705\n",
      "ep 3831: ep_len:500 episode reward: total was 40.210000. running mean: 15.489408\n",
      "ep 3831: ep_len:500 episode reward: total was -24.790000. running mean: 15.086614\n",
      "epsilon:0.030115 episode_count: 26824. steps_count: 11598085.000000\n",
      "Time elapsed:  35149.68887472153\n",
      "ep 3832: ep_len:111 episode reward: total was -34.150000. running mean: 14.594248\n",
      "ep 3832: ep_len:675 episode reward: total was 100.520000. running mean: 15.453505\n",
      "ep 3832: ep_len:500 episode reward: total was 39.250000. running mean: 15.691470\n",
      "ep 3832: ep_len:500 episode reward: total was 39.210000. running mean: 15.926656\n",
      "ep 3832: ep_len:48 episode reward: total was 22.500000. running mean: 15.992389\n",
      "ep 3832: ep_len:614 episode reward: total was 52.480000. running mean: 16.357265\n",
      "ep 3832: ep_len:500 episode reward: total was 48.870000. running mean: 16.682392\n",
      "epsilon:0.030070 episode_count: 26831. steps_count: 11601033.000000\n",
      "Time elapsed:  35155.187776088715\n",
      "ep 3833: ep_len:593 episode reward: total was 56.600000. running mean: 17.081569\n",
      "ep 3833: ep_len:500 episode reward: total was -16.430000. running mean: 16.746453\n",
      "ep 3833: ep_len:609 episode reward: total was 3.470000. running mean: 16.613688\n",
      "ep 3833: ep_len:504 episode reward: total was -1.650000. running mean: 16.431051\n",
      "ep 3833: ep_len:3 episode reward: total was -0.490000. running mean: 16.261841\n",
      "ep 3833: ep_len:538 episode reward: total was 9.750000. running mean: 16.196722\n",
      "ep 3833: ep_len:500 episode reward: total was 54.370000. running mean: 16.578455\n",
      "epsilon:0.030026 episode_count: 26838. steps_count: 11604280.000000\n",
      "Time elapsed:  35174.163922548294\n",
      "ep 3834: ep_len:590 episode reward: total was -8.690000. running mean: 16.325771\n",
      "ep 3834: ep_len:500 episode reward: total was 81.210000. running mean: 16.974613\n",
      "ep 3834: ep_len:500 episode reward: total was 25.390000. running mean: 17.058767\n",
      "ep 3834: ep_len:119 episode reward: total was 2.500000. running mean: 16.913179\n",
      "ep 3834: ep_len:3 episode reward: total was 1.010000. running mean: 16.754147\n",
      "ep 3834: ep_len:517 episode reward: total was -32.660000. running mean: 16.260006\n",
      "ep 3834: ep_len:501 episode reward: total was -4.290000. running mean: 16.054506\n",
      "epsilon:0.029982 episode_count: 26845. steps_count: 11607010.000000\n",
      "Time elapsed:  35184.749591588974\n",
      "ep 3835: ep_len:537 episode reward: total was 53.850000. running mean: 16.432461\n",
      "ep 3835: ep_len:500 episode reward: total was 24.680000. running mean: 16.514936\n",
      "ep 3835: ep_len:568 episode reward: total was -12.310000. running mean: 16.226687\n",
      "ep 3835: ep_len:579 episode reward: total was 55.640000. running mean: 16.620820\n",
      "ep 3835: ep_len:3 episode reward: total was 1.010000. running mean: 16.464712\n",
      "ep 3835: ep_len:657 episode reward: total was 26.760000. running mean: 16.567665\n",
      "ep 3835: ep_len:291 episode reward: total was 17.270000. running mean: 16.574688\n",
      "epsilon:0.029937 episode_count: 26852. steps_count: 11610145.000000\n",
      "Time elapsed:  35193.1499671936\n",
      "ep 3836: ep_len:617 episode reward: total was -16.900000. running mean: 16.239941\n",
      "ep 3836: ep_len:506 episode reward: total was -20.790000. running mean: 15.869642\n",
      "ep 3836: ep_len:574 episode reward: total was -4.660000. running mean: 15.664345\n",
      "ep 3836: ep_len:503 episode reward: total was 53.380000. running mean: 16.041502\n",
      "ep 3836: ep_len:74 episode reward: total was 21.120000. running mean: 16.092287\n",
      "ep 3836: ep_len:605 episode reward: total was 15.140000. running mean: 16.082764\n",
      "ep 3836: ep_len:539 episode reward: total was 26.790000. running mean: 16.189836\n",
      "epsilon:0.029893 episode_count: 26859. steps_count: 11613563.000000\n",
      "Time elapsed:  35203.0666475296\n",
      "ep 3837: ep_len:500 episode reward: total was 42.750000. running mean: 16.455438\n",
      "ep 3837: ep_len:500 episode reward: total was 96.640000. running mean: 17.257284\n",
      "ep 3837: ep_len:79 episode reward: total was 7.300000. running mean: 17.157711\n",
      "ep 3837: ep_len:621 episode reward: total was 70.140000. running mean: 17.687534\n",
      "ep 3837: ep_len:3 episode reward: total was 1.010000. running mean: 17.520758\n",
      "ep 3837: ep_len:207 episode reward: total was 16.040000. running mean: 17.505951\n",
      "ep 3837: ep_len:211 episode reward: total was 13.200000. running mean: 17.462891\n",
      "epsilon:0.029849 episode_count: 26866. steps_count: 11615684.000000\n",
      "Time elapsed:  35209.015622377396\n",
      "ep 3838: ep_len:554 episode reward: total was -8.790000. running mean: 17.200362\n",
      "ep 3838: ep_len:501 episode reward: total was 18.180000. running mean: 17.210159\n",
      "ep 3838: ep_len:421 episode reward: total was 62.050000. running mean: 17.658557\n",
      "ep 3838: ep_len:517 episode reward: total was 49.210000. running mean: 17.974072\n",
      "ep 3838: ep_len:3 episode reward: total was -0.490000. running mean: 17.789431\n",
      "ep 3838: ep_len:500 episode reward: total was 46.330000. running mean: 18.074837\n",
      "ep 3838: ep_len:503 episode reward: total was 1.400000. running mean: 17.908088\n",
      "epsilon:0.029804 episode_count: 26873. steps_count: 11618683.000000\n",
      "Time elapsed:  35217.12921142578\n",
      "ep 3839: ep_len:612 episode reward: total was 52.200000. running mean: 18.251007\n",
      "ep 3839: ep_len:500 episode reward: total was 69.540000. running mean: 18.763897\n",
      "ep 3839: ep_len:501 episode reward: total was 6.740000. running mean: 18.643658\n",
      "ep 3839: ep_len:500 episode reward: total was 35.870000. running mean: 18.815922\n",
      "ep 3839: ep_len:3 episode reward: total was -1.500000. running mean: 18.612762\n",
      "ep 3839: ep_len:500 episode reward: total was -1.520000. running mean: 18.411435\n",
      "ep 3839: ep_len:622 episode reward: total was -18.760000. running mean: 18.039720\n",
      "epsilon:0.029760 episode_count: 26880. steps_count: 11621921.000000\n",
      "Time elapsed:  35225.69052362442\n",
      "ep 3840: ep_len:227 episode reward: total was 22.290000. running mean: 18.082223\n",
      "ep 3840: ep_len:602 episode reward: total was -13.590000. running mean: 17.765501\n",
      "ep 3840: ep_len:600 episode reward: total was -4.400000. running mean: 17.543846\n",
      "ep 3840: ep_len:500 episode reward: total was 56.350000. running mean: 17.931908\n",
      "ep 3840: ep_len:3 episode reward: total was 1.010000. running mean: 17.762688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3840: ep_len:578 episode reward: total was 9.480000. running mean: 17.679862\n",
      "ep 3840: ep_len:630 episode reward: total was 15.220000. running mean: 17.655263\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.029716 episode_count: 26887. steps_count: 11625061.000000\n",
      "Time elapsed:  35239.181893110275\n",
      "ep 3841: ep_len:500 episode reward: total was 68.330000. running mean: 18.162010\n",
      "ep 3841: ep_len:603 episode reward: total was 8.700000. running mean: 18.067390\n",
      "ep 3841: ep_len:66 episode reward: total was 9.680000. running mean: 17.983516\n",
      "ep 3841: ep_len:166 episode reward: total was 26.750000. running mean: 18.071181\n",
      "ep 3841: ep_len:105 episode reward: total was 8.240000. running mean: 17.972869\n",
      "ep 3841: ep_len:542 episode reward: total was 43.180000. running mean: 18.224941\n",
      "ep 3841: ep_len:282 episode reward: total was 24.120000. running mean: 18.283891\n",
      "epsilon:0.029671 episode_count: 26894. steps_count: 11627325.000000\n",
      "Time elapsed:  35243.281529426575\n",
      "ep 3842: ep_len:126 episode reward: total was 10.410000. running mean: 18.205152\n",
      "ep 3842: ep_len:523 episode reward: total was 2.960000. running mean: 18.052701\n",
      "ep 3842: ep_len:537 episode reward: total was -19.140000. running mean: 17.680774\n",
      "ep 3842: ep_len:573 episode reward: total was 62.710000. running mean: 18.131066\n",
      "ep 3842: ep_len:113 episode reward: total was 37.740000. running mean: 18.327155\n",
      "ep 3842: ep_len:524 episode reward: total was 16.840000. running mean: 18.312284\n",
      "ep 3842: ep_len:503 episode reward: total was 5.450000. running mean: 18.183661\n",
      "epsilon:0.029627 episode_count: 26901. steps_count: 11630224.000000\n",
      "Time elapsed:  35251.04873228073\n",
      "ep 3843: ep_len:213 episode reward: total was 7.200000. running mean: 18.073824\n",
      "ep 3843: ep_len:520 episode reward: total was 17.440000. running mean: 18.067486\n",
      "ep 3843: ep_len:539 episode reward: total was 19.370000. running mean: 18.080511\n",
      "ep 3843: ep_len:500 episode reward: total was 47.960000. running mean: 18.379306\n",
      "ep 3843: ep_len:3 episode reward: total was 1.010000. running mean: 18.205613\n",
      "ep 3843: ep_len:500 episode reward: total was 7.420000. running mean: 18.097757\n",
      "ep 3843: ep_len:544 episode reward: total was 13.610000. running mean: 18.052879\n",
      "epsilon:0.029583 episode_count: 26908. steps_count: 11633043.000000\n",
      "Time elapsed:  35258.47525835037\n",
      "ep 3844: ep_len:500 episode reward: total was 78.030000. running mean: 18.652651\n",
      "ep 3844: ep_len:294 episode reward: total was 14.700000. running mean: 18.613124\n",
      "ep 3844: ep_len:674 episode reward: total was 10.620000. running mean: 18.533193\n",
      "ep 3844: ep_len:500 episode reward: total was 26.590000. running mean: 18.613761\n",
      "ep 3844: ep_len:3 episode reward: total was 1.010000. running mean: 18.437723\n",
      "ep 3844: ep_len:163 episode reward: total was 22.170000. running mean: 18.475046\n",
      "ep 3844: ep_len:611 episode reward: total was 40.710000. running mean: 18.697396\n",
      "epsilon:0.029538 episode_count: 26915. steps_count: 11635788.000000\n",
      "Time elapsed:  35271.48608660698\n",
      "ep 3845: ep_len:564 episode reward: total was -28.410000. running mean: 18.226322\n",
      "ep 3845: ep_len:617 episode reward: total was 42.900000. running mean: 18.473058\n",
      "ep 3845: ep_len:393 episode reward: total was 46.020000. running mean: 18.748528\n",
      "ep 3845: ep_len:500 episode reward: total was 58.810000. running mean: 19.149143\n",
      "ep 3845: ep_len:3 episode reward: total was 1.010000. running mean: 18.967751\n",
      "ep 3845: ep_len:547 episode reward: total was 15.870000. running mean: 18.936774\n",
      "ep 3845: ep_len:184 episode reward: total was 3.010000. running mean: 18.777506\n",
      "epsilon:0.029494 episode_count: 26922. steps_count: 11638596.000000\n",
      "Time elapsed:  35281.14536905289\n",
      "ep 3846: ep_len:500 episode reward: total was 86.790000. running mean: 19.457631\n",
      "ep 3846: ep_len:500 episode reward: total was 10.850000. running mean: 19.371555\n",
      "ep 3846: ep_len:411 episode reward: total was 0.140000. running mean: 19.179239\n",
      "ep 3846: ep_len:345 episode reward: total was 28.330000. running mean: 19.270747\n",
      "ep 3846: ep_len:3 episode reward: total was 1.010000. running mean: 19.088139\n",
      "ep 3846: ep_len:503 episode reward: total was -24.060000. running mean: 18.656658\n",
      "ep 3846: ep_len:500 episode reward: total was 28.090000. running mean: 18.750991\n",
      "epsilon:0.029450 episode_count: 26929. steps_count: 11641358.000000\n",
      "Time elapsed:  35288.572679042816\n",
      "ep 3847: ep_len:513 episode reward: total was 73.800000. running mean: 19.301481\n",
      "ep 3847: ep_len:505 episode reward: total was 52.460000. running mean: 19.633066\n",
      "ep 3847: ep_len:585 episode reward: total was 28.170000. running mean: 19.718436\n",
      "ep 3847: ep_len:152 episode reward: total was 26.020000. running mean: 19.781451\n",
      "ep 3847: ep_len:3 episode reward: total was 1.010000. running mean: 19.593737\n",
      "ep 3847: ep_len:500 episode reward: total was -196.780000. running mean: 17.430000\n",
      "ep 3847: ep_len:600 episode reward: total was 31.690000. running mean: 17.572600\n",
      "epsilon:0.029405 episode_count: 26936. steps_count: 11644216.000000\n",
      "Time elapsed:  35303.05147218704\n",
      "ep 3848: ep_len:651 episode reward: total was -35.530000. running mean: 17.041574\n",
      "ep 3848: ep_len:586 episode reward: total was 26.300000. running mean: 17.134158\n",
      "ep 3848: ep_len:929 episode reward: total was -288.630000. running mean: 14.076516\n",
      "ep 3848: ep_len:500 episode reward: total was 1.330000. running mean: 13.949051\n",
      "ep 3848: ep_len:3 episode reward: total was 1.010000. running mean: 13.819661\n",
      "ep 3848: ep_len:291 episode reward: total was 14.270000. running mean: 13.824164\n",
      "ep 3848: ep_len:324 episode reward: total was 11.140000. running mean: 13.797322\n",
      "epsilon:0.029361 episode_count: 26943. steps_count: 11647500.000000\n",
      "Time elapsed:  35310.23979973793\n",
      "ep 3849: ep_len:513 episode reward: total was 27.610000. running mean: 13.935449\n",
      "ep 3849: ep_len:500 episode reward: total was 21.710000. running mean: 14.013195\n",
      "ep 3849: ep_len:618 episode reward: total was 17.360000. running mean: 14.046663\n",
      "ep 3849: ep_len:553 episode reward: total was 62.690000. running mean: 14.533096\n",
      "ep 3849: ep_len:86 episode reward: total was -7.780000. running mean: 14.309965\n",
      "ep 3849: ep_len:500 episode reward: total was 23.930000. running mean: 14.406165\n",
      "ep 3849: ep_len:609 episode reward: total was 29.930000. running mean: 14.561404\n",
      "epsilon:0.029317 episode_count: 26950. steps_count: 11650879.000000\n",
      "Time elapsed:  35318.24020051956\n",
      "ep 3850: ep_len:580 episode reward: total was 67.360000. running mean: 15.089390\n",
      "ep 3850: ep_len:526 episode reward: total was 17.620000. running mean: 15.114696\n",
      "ep 3850: ep_len:626 episode reward: total was 34.470000. running mean: 15.308249\n",
      "ep 3850: ep_len:601 episode reward: total was 36.790000. running mean: 15.523066\n",
      "ep 3850: ep_len:3 episode reward: total was 1.010000. running mean: 15.377936\n",
      "ep 3850: ep_len:232 episode reward: total was 25.080000. running mean: 15.474956\n",
      "ep 3850: ep_len:536 episode reward: total was -20.780000. running mean: 15.112407\n",
      "epsilon:0.029272 episode_count: 26957. steps_count: 11653983.000000\n",
      "Time elapsed:  35339.822172641754\n",
      "ep 3851: ep_len:508 episode reward: total was 60.030000. running mean: 15.561583\n",
      "ep 3851: ep_len:587 episode reward: total was 4.030000. running mean: 15.446267\n",
      "ep 3851: ep_len:63 episode reward: total was 3.620000. running mean: 15.328004\n",
      "ep 3851: ep_len:500 episode reward: total was 12.970000. running mean: 15.304424\n",
      "ep 3851: ep_len:56 episode reward: total was 25.000000. running mean: 15.401380\n",
      "ep 3851: ep_len:553 episode reward: total was 10.820000. running mean: 15.355566\n",
      "ep 3851: ep_len:500 episode reward: total was 33.990000. running mean: 15.541911\n",
      "epsilon:0.029228 episode_count: 26964. steps_count: 11656750.000000\n",
      "Time elapsed:  35353.274465322495\n",
      "ep 3852: ep_len:500 episode reward: total was 58.020000. running mean: 15.966691\n",
      "ep 3852: ep_len:550 episode reward: total was 7.030000. running mean: 15.877324\n",
      "ep 3852: ep_len:535 episode reward: total was 15.680000. running mean: 15.875351\n",
      "ep 3852: ep_len:500 episode reward: total was -117.020000. running mean: 14.546398\n",
      "ep 3852: ep_len:3 episode reward: total was 1.010000. running mean: 14.411034\n",
      "ep 3852: ep_len:500 episode reward: total was 61.640000. running mean: 14.883323\n",
      "ep 3852: ep_len:607 episode reward: total was 50.500000. running mean: 15.239490\n",
      "epsilon:0.029184 episode_count: 26971. steps_count: 11659945.000000\n",
      "Time elapsed:  35366.459638834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3853: ep_len:573 episode reward: total was 44.700000. running mean: 15.534095\n",
      "ep 3853: ep_len:533 episode reward: total was 61.100000. running mean: 15.989754\n",
      "ep 3853: ep_len:609 episode reward: total was 26.270000. running mean: 16.092557\n",
      "ep 3853: ep_len:402 episode reward: total was 27.760000. running mean: 16.209231\n",
      "ep 3853: ep_len:105 episode reward: total was 34.260000. running mean: 16.389739\n",
      "ep 3853: ep_len:518 episode reward: total was 39.900000. running mean: 16.624842\n",
      "ep 3853: ep_len:271 episode reward: total was 2.590000. running mean: 16.484493\n",
      "epsilon:0.029139 episode_count: 26978. steps_count: 11662956.000000\n",
      "Time elapsed:  35374.415877103806\n",
      "ep 3854: ep_len:134 episode reward: total was 0.910000. running mean: 16.328748\n",
      "ep 3854: ep_len:500 episode reward: total was 62.440000. running mean: 16.789861\n",
      "ep 3854: ep_len:500 episode reward: total was 23.170000. running mean: 16.853662\n",
      "ep 3854: ep_len:585 episode reward: total was 54.070000. running mean: 17.225825\n",
      "ep 3854: ep_len:3 episode reward: total was 1.010000. running mean: 17.063667\n",
      "ep 3854: ep_len:500 episode reward: total was -148.810000. running mean: 15.404931\n",
      "ep 3854: ep_len:621 episode reward: total was 30.950000. running mean: 15.560381\n",
      "epsilon:0.029095 episode_count: 26985. steps_count: 11665799.000000\n",
      "Time elapsed:  35393.88938450813\n",
      "ep 3855: ep_len:511 episode reward: total was 84.140000. running mean: 16.246177\n",
      "ep 3855: ep_len:590 episode reward: total was 48.370000. running mean: 16.567416\n",
      "ep 3855: ep_len:380 episode reward: total was 61.500000. running mean: 17.016741\n",
      "ep 3855: ep_len:502 episode reward: total was 51.140000. running mean: 17.357974\n",
      "ep 3855: ep_len:96 episode reward: total was -49.750000. running mean: 16.686894\n",
      "ep 3855: ep_len:503 episode reward: total was -22.340000. running mean: 16.296625\n",
      "ep 3855: ep_len:500 episode reward: total was 46.380000. running mean: 16.597459\n",
      "epsilon:0.029051 episode_count: 26992. steps_count: 11668881.000000\n",
      "Time elapsed:  35402.033277988434\n",
      "ep 3856: ep_len:530 episode reward: total was -17.530000. running mean: 16.256185\n",
      "ep 3856: ep_len:500 episode reward: total was 28.880000. running mean: 16.382423\n",
      "ep 3856: ep_len:427 episode reward: total was 46.250000. running mean: 16.681098\n",
      "ep 3856: ep_len:578 episode reward: total was 81.890000. running mean: 17.333187\n",
      "ep 3856: ep_len:47 episode reward: total was 17.500000. running mean: 17.334856\n",
      "ep 3856: ep_len:638 episode reward: total was 2.540000. running mean: 17.186907\n",
      "ep 3856: ep_len:601 episode reward: total was 56.170000. running mean: 17.576738\n",
      "epsilon:0.029006 episode_count: 26999. steps_count: 11672202.000000\n",
      "Time elapsed:  35410.69765949249\n",
      "ep 3857: ep_len:593 episode reward: total was 78.990000. running mean: 18.190871\n",
      "ep 3857: ep_len:606 episode reward: total was 104.350000. running mean: 19.052462\n",
      "ep 3857: ep_len:568 episode reward: total was -4.560000. running mean: 18.816337\n",
      "ep 3857: ep_len:56 episode reward: total was 5.350000. running mean: 18.681674\n",
      "ep 3857: ep_len:3 episode reward: total was 1.010000. running mean: 18.504957\n",
      "ep 3857: ep_len:550 episode reward: total was -36.340000. running mean: 17.956508\n",
      "ep 3857: ep_len:612 episode reward: total was 43.840000. running mean: 18.215343\n",
      "epsilon:0.028962 episode_count: 27006. steps_count: 11675190.000000\n",
      "Time elapsed:  35424.54483151436\n",
      "ep 3858: ep_len:500 episode reward: total was -17.870000. running mean: 17.854489\n",
      "ep 3858: ep_len:574 episode reward: total was 49.620000. running mean: 18.172144\n",
      "ep 3858: ep_len:418 episode reward: total was 53.260000. running mean: 18.523023\n",
      "ep 3858: ep_len:525 episode reward: total was 48.640000. running mean: 18.824193\n",
      "ep 3858: ep_len:3 episode reward: total was 1.010000. running mean: 18.646051\n",
      "ep 3858: ep_len:714 episode reward: total was 24.640000. running mean: 18.705990\n",
      "ep 3858: ep_len:524 episode reward: total was 28.510000. running mean: 18.804030\n",
      "epsilon:0.028918 episode_count: 27013. steps_count: 11678448.000000\n",
      "Time elapsed:  35429.53146100044\n",
      "ep 3859: ep_len:500 episode reward: total was 68.400000. running mean: 19.299990\n",
      "ep 3859: ep_len:521 episode reward: total was -167.950000. running mean: 17.427490\n",
      "ep 3859: ep_len:619 episode reward: total was 22.520000. running mean: 17.478415\n",
      "ep 3859: ep_len:500 episode reward: total was 61.520000. running mean: 17.918831\n",
      "ep 3859: ep_len:1 episode reward: total was -1.000000. running mean: 17.729643\n",
      "ep 3859: ep_len:186 episode reward: total was 33.180000. running mean: 17.884146\n",
      "ep 3859: ep_len:607 episode reward: total was 35.140000. running mean: 18.056705\n",
      "epsilon:0.028873 episode_count: 27020. steps_count: 11681382.000000\n",
      "Time elapsed:  35441.50442790985\n",
      "ep 3860: ep_len:521 episode reward: total was 20.590000. running mean: 18.082038\n",
      "ep 3860: ep_len:565 episode reward: total was 59.150000. running mean: 18.492717\n",
      "ep 3860: ep_len:364 episode reward: total was 44.720000. running mean: 18.754990\n",
      "ep 3860: ep_len:566 episode reward: total was 85.190000. running mean: 19.419340\n",
      "ep 3860: ep_len:3 episode reward: total was 1.010000. running mean: 19.235247\n",
      "ep 3860: ep_len:500 episode reward: total was -143.390000. running mean: 17.608994\n",
      "ep 3860: ep_len:617 episode reward: total was 43.930000. running mean: 17.872204\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.028829 episode_count: 27027. steps_count: 11684518.000000\n",
      "Time elapsed:  35454.46203494072\n",
      "ep 3861: ep_len:686 episode reward: total was -57.520000. running mean: 17.118282\n",
      "ep 3861: ep_len:500 episode reward: total was 19.300000. running mean: 17.140100\n",
      "ep 3861: ep_len:612 episode reward: total was -7.240000. running mean: 16.896299\n",
      "ep 3861: ep_len:500 episode reward: total was 56.900000. running mean: 17.296336\n",
      "ep 3861: ep_len:84 episode reward: total was 25.780000. running mean: 17.381172\n",
      "ep 3861: ep_len:643 episode reward: total was 13.520000. running mean: 17.342561\n",
      "ep 3861: ep_len:532 episode reward: total was 53.320000. running mean: 17.702335\n",
      "epsilon:0.028785 episode_count: 27034. steps_count: 11688075.000000\n",
      "Time elapsed:  35463.77772760391\n",
      "ep 3862: ep_len:508 episode reward: total was 71.720000. running mean: 18.242512\n",
      "ep 3862: ep_len:883 episode reward: total was -377.070000. running mean: 14.289386\n",
      "ep 3862: ep_len:555 episode reward: total was -11.240000. running mean: 14.034093\n",
      "ep 3862: ep_len:500 episode reward: total was 76.920000. running mean: 14.662952\n",
      "ep 3862: ep_len:3 episode reward: total was 1.010000. running mean: 14.526422\n",
      "ep 3862: ep_len:521 episode reward: total was 23.500000. running mean: 14.616158\n",
      "ep 3862: ep_len:551 episode reward: total was 22.970000. running mean: 14.699696\n",
      "epsilon:0.028740 episode_count: 27041. steps_count: 11691596.000000\n",
      "Time elapsed:  35472.86622619629\n",
      "ep 3863: ep_len:641 episode reward: total was 61.980000. running mean: 15.172499\n",
      "ep 3863: ep_len:500 episode reward: total was 105.430000. running mean: 16.075074\n",
      "ep 3863: ep_len:662 episode reward: total was 12.300000. running mean: 16.037324\n",
      "ep 3863: ep_len:500 episode reward: total was 27.180000. running mean: 16.148750\n",
      "ep 3863: ep_len:47 episode reward: total was 22.000000. running mean: 16.207263\n",
      "ep 3863: ep_len:574 episode reward: total was 49.790000. running mean: 16.543090\n",
      "ep 3863: ep_len:511 episode reward: total was 22.130000. running mean: 16.598959\n",
      "epsilon:0.028696 episode_count: 27048. steps_count: 11695031.000000\n",
      "Time elapsed:  35481.8796274662\n",
      "ep 3864: ep_len:506 episode reward: total was 77.490000. running mean: 17.207870\n",
      "ep 3864: ep_len:554 episode reward: total was 16.750000. running mean: 17.203291\n",
      "ep 3864: ep_len:567 episode reward: total was 28.520000. running mean: 17.316458\n",
      "ep 3864: ep_len:636 episode reward: total was 60.440000. running mean: 17.747694\n",
      "ep 3864: ep_len:3 episode reward: total was 1.010000. running mean: 17.580317\n",
      "ep 3864: ep_len:597 episode reward: total was -242.760000. running mean: 14.976913\n",
      "ep 3864: ep_len:586 episode reward: total was 55.040000. running mean: 15.377544\n",
      "epsilon:0.028652 episode_count: 27055. steps_count: 11698480.000000\n",
      "Time elapsed:  35490.90335941315\n",
      "ep 3865: ep_len:119 episode reward: total was 15.420000. running mean: 15.377969\n",
      "ep 3865: ep_len:500 episode reward: total was 98.140000. running mean: 16.205589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3865: ep_len:644 episode reward: total was -29.540000. running mean: 15.748133\n",
      "ep 3865: ep_len:500 episode reward: total was 80.810000. running mean: 16.398752\n",
      "ep 3865: ep_len:113 episode reward: total was 17.880000. running mean: 16.413564\n",
      "ep 3865: ep_len:570 episode reward: total was 19.310000. running mean: 16.442529\n",
      "ep 3865: ep_len:503 episode reward: total was 25.790000. running mean: 16.536004\n",
      "epsilon:0.028607 episode_count: 27062. steps_count: 11701429.000000\n",
      "Time elapsed:  35502.746240377426\n",
      "ep 3866: ep_len:527 episode reward: total was 92.680000. running mean: 17.297443\n",
      "ep 3866: ep_len:501 episode reward: total was 35.290000. running mean: 17.477369\n",
      "ep 3866: ep_len:500 episode reward: total was 36.410000. running mean: 17.666695\n",
      "ep 3866: ep_len:500 episode reward: total was 7.550000. running mean: 17.565528\n",
      "ep 3866: ep_len:3 episode reward: total was 1.010000. running mean: 17.399973\n",
      "ep 3866: ep_len:608 episode reward: total was 23.760000. running mean: 17.463573\n",
      "ep 3866: ep_len:504 episode reward: total was 51.660000. running mean: 17.805538\n",
      "epsilon:0.028563 episode_count: 27069. steps_count: 11704572.000000\n",
      "Time elapsed:  35511.430436849594\n",
      "ep 3867: ep_len:500 episode reward: total was 103.500000. running mean: 18.662482\n",
      "ep 3867: ep_len:500 episode reward: total was 26.470000. running mean: 18.740557\n",
      "ep 3867: ep_len:533 episode reward: total was 48.450000. running mean: 19.037652\n",
      "ep 3867: ep_len:537 episode reward: total was 25.020000. running mean: 19.097475\n",
      "ep 3867: ep_len:3 episode reward: total was 1.010000. running mean: 18.916601\n",
      "ep 3867: ep_len:636 episode reward: total was 40.530000. running mean: 19.132735\n",
      "ep 3867: ep_len:195 episode reward: total was 11.660000. running mean: 19.058007\n",
      "epsilon:0.028519 episode_count: 27076. steps_count: 11707476.000000\n",
      "Time elapsed:  35530.49198651314\n",
      "ep 3868: ep_len:571 episode reward: total was 37.570000. running mean: 19.243127\n",
      "ep 3868: ep_len:500 episode reward: total was 23.940000. running mean: 19.290096\n",
      "ep 3868: ep_len:600 episode reward: total was 19.000000. running mean: 19.287195\n",
      "ep 3868: ep_len:554 episode reward: total was 75.760000. running mean: 19.851923\n",
      "ep 3868: ep_len:92 episode reward: total was 22.250000. running mean: 19.875904\n",
      "ep 3868: ep_len:509 episode reward: total was -79.700000. running mean: 18.880145\n",
      "ep 3868: ep_len:566 episode reward: total was 15.900000. running mean: 18.850343\n",
      "epsilon:0.028474 episode_count: 27083. steps_count: 11710868.000000\n",
      "Time elapsed:  35539.42585301399\n",
      "ep 3869: ep_len:504 episode reward: total was 24.280000. running mean: 18.904640\n",
      "ep 3869: ep_len:527 episode reward: total was 25.770000. running mean: 18.973293\n",
      "ep 3869: ep_len:501 episode reward: total was 15.990000. running mean: 18.943461\n",
      "ep 3869: ep_len:47 episode reward: total was 2.750000. running mean: 18.781526\n",
      "ep 3869: ep_len:84 episode reward: total was 21.250000. running mean: 18.806211\n",
      "ep 3869: ep_len:587 episode reward: total was 37.890000. running mean: 18.997049\n",
      "ep 3869: ep_len:201 episode reward: total was 23.260000. running mean: 19.039678\n",
      "epsilon:0.028430 episode_count: 27090. steps_count: 11713319.000000\n",
      "Time elapsed:  35552.14757227898\n",
      "ep 3870: ep_len:503 episode reward: total was 77.580000. running mean: 19.625081\n",
      "ep 3870: ep_len:533 episode reward: total was 46.090000. running mean: 19.889730\n",
      "ep 3870: ep_len:386 episode reward: total was 76.590000. running mean: 20.456733\n",
      "ep 3870: ep_len:598 episode reward: total was -73.500000. running mean: 19.517166\n",
      "ep 3870: ep_len:127 episode reward: total was 25.360000. running mean: 19.575594\n",
      "ep 3870: ep_len:794 episode reward: total was -163.070000. running mean: 17.749138\n",
      "ep 3870: ep_len:636 episode reward: total was 7.510000. running mean: 17.646747\n",
      "epsilon:0.028386 episode_count: 27097. steps_count: 11716896.000000\n",
      "Time elapsed:  35561.46519947052\n",
      "ep 3871: ep_len:544 episode reward: total was -24.140000. running mean: 17.228879\n",
      "ep 3871: ep_len:383 episode reward: total was 22.220000. running mean: 17.278791\n",
      "ep 3871: ep_len:848 episode reward: total was -70.010000. running mean: 16.405903\n",
      "ep 3871: ep_len:505 episode reward: total was 45.380000. running mean: 16.695644\n",
      "ep 3871: ep_len:48 episode reward: total was 22.500000. running mean: 16.753687\n",
      "ep 3871: ep_len:656 episode reward: total was 28.130000. running mean: 16.867450\n",
      "ep 3871: ep_len:343 episode reward: total was 28.120000. running mean: 16.979976\n",
      "epsilon:0.028341 episode_count: 27104. steps_count: 11720223.000000\n",
      "Time elapsed:  35574.80316710472\n",
      "ep 3872: ep_len:500 episode reward: total was -5.280000. running mean: 16.757376\n",
      "ep 3872: ep_len:271 episode reward: total was -9.300000. running mean: 16.496802\n",
      "ep 3872: ep_len:656 episode reward: total was 3.700000. running mean: 16.368834\n",
      "ep 3872: ep_len:383 episode reward: total was 23.860000. running mean: 16.443746\n",
      "ep 3872: ep_len:54 episode reward: total was 22.010000. running mean: 16.499409\n",
      "ep 3872: ep_len:671 episode reward: total was 23.840000. running mean: 16.572814\n",
      "ep 3872: ep_len:504 episode reward: total was 47.010000. running mean: 16.877186\n",
      "epsilon:0.028297 episode_count: 27111. steps_count: 11723262.000000\n",
      "Time elapsed:  35580.45841836929\n",
      "ep 3873: ep_len:547 episode reward: total was 28.020000. running mean: 16.988614\n",
      "ep 3873: ep_len:500 episode reward: total was 47.130000. running mean: 17.290028\n",
      "ep 3873: ep_len:500 episode reward: total was 38.980000. running mean: 17.506928\n",
      "ep 3873: ep_len:528 episode reward: total was 34.730000. running mean: 17.679159\n",
      "ep 3873: ep_len:51 episode reward: total was 24.000000. running mean: 17.742367\n",
      "ep 3873: ep_len:500 episode reward: total was 45.440000. running mean: 18.019343\n",
      "ep 3873: ep_len:548 episode reward: total was 9.950000. running mean: 17.938650\n",
      "epsilon:0.028253 episode_count: 27118. steps_count: 11726436.000000\n",
      "Time elapsed:  35588.89055132866\n",
      "ep 3874: ep_len:644 episode reward: total was 50.780000. running mean: 18.267064\n",
      "ep 3874: ep_len:545 episode reward: total was 113.430000. running mean: 19.218693\n",
      "ep 3874: ep_len:549 episode reward: total was -1.590000. running mean: 19.010606\n",
      "ep 3874: ep_len:535 episode reward: total was 66.660000. running mean: 19.487100\n",
      "ep 3874: ep_len:3 episode reward: total was 1.010000. running mean: 19.302329\n",
      "ep 3874: ep_len:500 episode reward: total was -206.020000. running mean: 17.049106\n",
      "ep 3874: ep_len:500 episode reward: total was 62.910000. running mean: 17.507715\n",
      "epsilon:0.028208 episode_count: 27125. steps_count: 11729712.000000\n",
      "Time elapsed:  35597.64791202545\n",
      "ep 3875: ep_len:604 episode reward: total was 101.270000. running mean: 18.345337\n",
      "ep 3875: ep_len:500 episode reward: total was 50.280000. running mean: 18.664684\n",
      "ep 3875: ep_len:587 episode reward: total was -147.270000. running mean: 17.005337\n",
      "ep 3875: ep_len:587 episode reward: total was 66.740000. running mean: 17.502684\n",
      "ep 3875: ep_len:3 episode reward: total was 1.010000. running mean: 17.337757\n",
      "ep 3875: ep_len:693 episode reward: total was 30.070000. running mean: 17.465079\n",
      "ep 3875: ep_len:537 episode reward: total was 35.520000. running mean: 17.645629\n",
      "epsilon:0.028164 episode_count: 27132. steps_count: 11733223.000000\n",
      "Time elapsed:  35606.29628825188\n",
      "ep 3876: ep_len:500 episode reward: total was 64.720000. running mean: 18.116372\n",
      "ep 3876: ep_len:183 episode reward: total was 2.860000. running mean: 17.963809\n",
      "ep 3876: ep_len:634 episode reward: total was 10.700000. running mean: 17.891171\n",
      "ep 3876: ep_len:504 episode reward: total was 86.760000. running mean: 18.579859\n",
      "ep 3876: ep_len:98 episode reward: total was 22.250000. running mean: 18.616560\n",
      "ep 3876: ep_len:580 episode reward: total was 48.210000. running mean: 18.912495\n",
      "ep 3876: ep_len:500 episode reward: total was 19.650000. running mean: 18.919870\n",
      "epsilon:0.028120 episode_count: 27139. steps_count: 11736222.000000\n",
      "Time elapsed:  35620.17795729637\n",
      "ep 3877: ep_len:634 episode reward: total was -17.460000. running mean: 18.556071\n",
      "ep 3877: ep_len:607 episode reward: total was 59.760000. running mean: 18.968110\n",
      "ep 3877: ep_len:543 episode reward: total was 19.940000. running mean: 18.977829\n",
      "ep 3877: ep_len:132 episode reward: total was 4.620000. running mean: 18.834251\n",
      "ep 3877: ep_len:3 episode reward: total was 1.010000. running mean: 18.656008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3877: ep_len:513 episode reward: total was 4.300000. running mean: 18.512448\n",
      "ep 3877: ep_len:607 episode reward: total was 11.480000. running mean: 18.442124\n",
      "epsilon:0.028075 episode_count: 27146. steps_count: 11739261.000000\n",
      "Time elapsed:  35635.478514671326\n",
      "ep 3878: ep_len:559 episode reward: total was 53.730000. running mean: 18.795003\n",
      "ep 3878: ep_len:500 episode reward: total was 63.760000. running mean: 19.244653\n",
      "ep 3878: ep_len:540 episode reward: total was 13.470000. running mean: 19.186906\n",
      "ep 3878: ep_len:99 episode reward: total was 23.310000. running mean: 19.228137\n",
      "ep 3878: ep_len:3 episode reward: total was 1.010000. running mean: 19.045956\n",
      "ep 3878: ep_len:547 episode reward: total was -119.890000. running mean: 17.656596\n",
      "ep 3878: ep_len:186 episode reward: total was 12.980000. running mean: 17.609830\n",
      "epsilon:0.028031 episode_count: 27153. steps_count: 11741695.000000\n",
      "Time elapsed:  35644.20600557327\n",
      "ep 3879: ep_len:500 episode reward: total was 95.450000. running mean: 18.388232\n",
      "ep 3879: ep_len:500 episode reward: total was 16.240000. running mean: 18.366749\n",
      "ep 3879: ep_len:614 episode reward: total was 34.000000. running mean: 18.523082\n",
      "ep 3879: ep_len:500 episode reward: total was 73.350000. running mean: 19.071351\n",
      "ep 3879: ep_len:87 episode reward: total was 20.240000. running mean: 19.083038\n",
      "ep 3879: ep_len:519 episode reward: total was -23.040000. running mean: 18.661807\n",
      "ep 3879: ep_len:558 episode reward: total was 37.220000. running mean: 18.847389\n",
      "epsilon:0.027987 episode_count: 27160. steps_count: 11744973.000000\n",
      "Time elapsed:  35652.84638404846\n",
      "ep 3880: ep_len:611 episode reward: total was 39.950000. running mean: 19.058415\n",
      "ep 3880: ep_len:365 episode reward: total was 22.070000. running mean: 19.088531\n",
      "ep 3880: ep_len:500 episode reward: total was -22.720000. running mean: 18.670446\n",
      "ep 3880: ep_len:500 episode reward: total was 45.440000. running mean: 18.938141\n",
      "ep 3880: ep_len:3 episode reward: total was 1.010000. running mean: 18.758860\n",
      "ep 3880: ep_len:685 episode reward: total was 17.740000. running mean: 18.748671\n",
      "ep 3880: ep_len:597 episode reward: total was 19.850000. running mean: 18.759685\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.027942 episode_count: 27167. steps_count: 11748234.000000\n",
      "Time elapsed:  35666.30217194557\n",
      "ep 3881: ep_len:636 episode reward: total was -16.280000. running mean: 18.409288\n",
      "ep 3881: ep_len:548 episode reward: total was 25.710000. running mean: 18.482295\n",
      "ep 3881: ep_len:555 episode reward: total was 33.090000. running mean: 18.628372\n",
      "ep 3881: ep_len:500 episode reward: total was 38.710000. running mean: 18.829188\n",
      "ep 3881: ep_len:3 episode reward: total was 1.010000. running mean: 18.650996\n",
      "ep 3881: ep_len:500 episode reward: total was 12.660000. running mean: 18.591086\n",
      "ep 3881: ep_len:500 episode reward: total was 65.850000. running mean: 19.063676\n",
      "epsilon:0.027898 episode_count: 27174. steps_count: 11751476.000000\n",
      "Time elapsed:  35674.86170530319\n",
      "ep 3882: ep_len:500 episode reward: total was -19.310000. running mean: 18.679939\n",
      "ep 3882: ep_len:337 episode reward: total was 17.160000. running mean: 18.664739\n",
      "ep 3882: ep_len:634 episode reward: total was 17.620000. running mean: 18.654292\n",
      "ep 3882: ep_len:566 episode reward: total was -2.010000. running mean: 18.447649\n",
      "ep 3882: ep_len:3 episode reward: total was -0.490000. running mean: 18.258273\n",
      "ep 3882: ep_len:639 episode reward: total was -35.350000. running mean: 17.722190\n",
      "ep 3882: ep_len:549 episode reward: total was 31.370000. running mean: 17.858668\n",
      "epsilon:0.027854 episode_count: 27181. steps_count: 11754704.000000\n",
      "Time elapsed:  35690.83332967758\n",
      "ep 3883: ep_len:637 episode reward: total was -10.210000. running mean: 17.577981\n",
      "ep 3883: ep_len:500 episode reward: total was 70.650000. running mean: 18.108701\n",
      "ep 3883: ep_len:500 episode reward: total was 12.170000. running mean: 18.049314\n",
      "ep 3883: ep_len:521 episode reward: total was 76.640000. running mean: 18.635221\n",
      "ep 3883: ep_len:3 episode reward: total was 1.010000. running mean: 18.458969\n",
      "ep 3883: ep_len:520 episode reward: total was 22.400000. running mean: 18.498379\n",
      "ep 3883: ep_len:508 episode reward: total was 33.930000. running mean: 18.652696\n",
      "epsilon:0.027809 episode_count: 27188. steps_count: 11757893.000000\n",
      "Time elapsed:  35699.04800033569\n",
      "ep 3884: ep_len:657 episode reward: total was -25.040000. running mean: 18.215769\n",
      "ep 3884: ep_len:662 episode reward: total was -155.330000. running mean: 16.480311\n",
      "ep 3884: ep_len:661 episode reward: total was 23.280000. running mean: 16.548308\n",
      "ep 3884: ep_len:511 episode reward: total was 47.280000. running mean: 16.855625\n",
      "ep 3884: ep_len:86 episode reward: total was 16.740000. running mean: 16.854469\n",
      "ep 3884: ep_len:500 episode reward: total was 35.570000. running mean: 17.041624\n",
      "ep 3884: ep_len:182 episode reward: total was -3.220000. running mean: 16.839008\n",
      "epsilon:0.027765 episode_count: 27195. steps_count: 11761152.000000\n",
      "Time elapsed:  35707.509073495865\n",
      "ep 3885: ep_len:547 episode reward: total was -17.380000. running mean: 16.496818\n",
      "ep 3885: ep_len:589 episode reward: total was 109.630000. running mean: 17.428149\n",
      "ep 3885: ep_len:624 episode reward: total was -7.760000. running mean: 17.176268\n",
      "ep 3885: ep_len:651 episode reward: total was 7.630000. running mean: 17.080805\n",
      "ep 3885: ep_len:3 episode reward: total was 1.010000. running mean: 16.920097\n",
      "ep 3885: ep_len:552 episode reward: total was 27.720000. running mean: 17.028096\n",
      "ep 3885: ep_len:634 episode reward: total was 57.910000. running mean: 17.436915\n",
      "epsilon:0.027721 episode_count: 27202. steps_count: 11764752.000000\n",
      "Time elapsed:  35716.93238687515\n",
      "ep 3886: ep_len:522 episode reward: total was 30.830000. running mean: 17.570846\n",
      "ep 3886: ep_len:500 episode reward: total was 35.420000. running mean: 17.749338\n",
      "ep 3886: ep_len:458 episode reward: total was 57.920000. running mean: 18.151044\n",
      "ep 3886: ep_len:500 episode reward: total was -41.670000. running mean: 17.552834\n",
      "ep 3886: ep_len:97 episode reward: total was 33.170000. running mean: 17.709005\n",
      "ep 3886: ep_len:561 episode reward: total was 21.490000. running mean: 17.746815\n",
      "ep 3886: ep_len:271 episode reward: total was 9.380000. running mean: 17.663147\n",
      "epsilon:0.027676 episode_count: 27209. steps_count: 11767661.000000\n",
      "Time elapsed:  35724.73248100281\n",
      "ep 3887: ep_len:629 episode reward: total was -6.860000. running mean: 17.417916\n",
      "ep 3887: ep_len:500 episode reward: total was 37.950000. running mean: 17.623237\n",
      "ep 3887: ep_len:598 episode reward: total was 35.000000. running mean: 17.797004\n",
      "ep 3887: ep_len:500 episode reward: total was 55.440000. running mean: 18.173434\n",
      "ep 3887: ep_len:3 episode reward: total was 1.010000. running mean: 18.001800\n",
      "ep 3887: ep_len:577 episode reward: total was 6.650000. running mean: 17.888282\n",
      "ep 3887: ep_len:550 episode reward: total was -0.590000. running mean: 17.703499\n",
      "epsilon:0.027632 episode_count: 27216. steps_count: 11771018.000000\n",
      "Time elapsed:  35733.45580339432\n",
      "ep 3888: ep_len:626 episode reward: total was -215.490000. running mean: 15.371564\n",
      "ep 3888: ep_len:500 episode reward: total was 49.240000. running mean: 15.710248\n",
      "ep 3888: ep_len:500 episode reward: total was -15.070000. running mean: 15.402446\n",
      "ep 3888: ep_len:504 episode reward: total was 46.360000. running mean: 15.712021\n",
      "ep 3888: ep_len:3 episode reward: total was 1.010000. running mean: 15.565001\n",
      "ep 3888: ep_len:504 episode reward: total was 31.580000. running mean: 15.725151\n",
      "ep 3888: ep_len:500 episode reward: total was 54.990000. running mean: 16.117800\n",
      "epsilon:0.027588 episode_count: 27223. steps_count: 11774155.000000\n",
      "Time elapsed:  35741.71088933945\n",
      "ep 3889: ep_len:500 episode reward: total was 29.430000. running mean: 16.250922\n",
      "ep 3889: ep_len:637 episode reward: total was 29.070000. running mean: 16.379112\n",
      "ep 3889: ep_len:597 episode reward: total was -11.440000. running mean: 16.100921\n",
      "ep 3889: ep_len:120 episode reward: total was 15.030000. running mean: 16.090212\n",
      "ep 3889: ep_len:3 episode reward: total was 1.010000. running mean: 15.939410\n",
      "ep 3889: ep_len:530 episode reward: total was 7.010000. running mean: 15.850116\n",
      "ep 3889: ep_len:539 episode reward: total was -17.840000. running mean: 15.513215\n",
      "epsilon:0.027543 episode_count: 27230. steps_count: 11777081.000000\n",
      "Time elapsed:  35749.544964790344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3890: ep_len:593 episode reward: total was -15.660000. running mean: 15.201483\n",
      "ep 3890: ep_len:500 episode reward: total was 103.250000. running mean: 16.081968\n",
      "ep 3890: ep_len:528 episode reward: total was -9.420000. running mean: 15.826948\n",
      "ep 3890: ep_len:510 episode reward: total was 34.930000. running mean: 16.017979\n",
      "ep 3890: ep_len:49 episode reward: total was 22.510000. running mean: 16.082899\n",
      "ep 3890: ep_len:522 episode reward: total was -2.350000. running mean: 15.898570\n",
      "ep 3890: ep_len:624 episode reward: total was 9.230000. running mean: 15.831884\n",
      "epsilon:0.027499 episode_count: 27237. steps_count: 11780407.000000\n",
      "Time elapsed:  35763.38817882538\n",
      "ep 3891: ep_len:507 episode reward: total was 24.220000. running mean: 15.915765\n",
      "ep 3891: ep_len:500 episode reward: total was 34.870000. running mean: 16.105308\n",
      "ep 3891: ep_len:662 episode reward: total was 6.580000. running mean: 16.010055\n",
      "ep 3891: ep_len:148 episode reward: total was 5.220000. running mean: 15.902154\n",
      "ep 3891: ep_len:3 episode reward: total was 1.010000. running mean: 15.753233\n",
      "ep 3891: ep_len:671 episode reward: total was 51.330000. running mean: 16.109000\n",
      "ep 3891: ep_len:500 episode reward: total was 28.630000. running mean: 16.234210\n",
      "epsilon:0.027455 episode_count: 27244. steps_count: 11783398.000000\n",
      "Time elapsed:  35772.39446020126\n",
      "ep 3892: ep_len:655 episode reward: total was -26.590000. running mean: 15.805968\n",
      "ep 3892: ep_len:500 episode reward: total was 20.130000. running mean: 15.849208\n",
      "ep 3892: ep_len:568 episode reward: total was -26.830000. running mean: 15.422416\n",
      "ep 3892: ep_len:500 episode reward: total was 33.210000. running mean: 15.600292\n",
      "ep 3892: ep_len:3 episode reward: total was 1.010000. running mean: 15.454389\n",
      "ep 3892: ep_len:519 episode reward: total was 37.700000. running mean: 15.676845\n",
      "ep 3892: ep_len:544 episode reward: total was 34.060000. running mean: 15.860677\n",
      "epsilon:0.027410 episode_count: 27251. steps_count: 11786687.000000\n",
      "Time elapsed:  35781.525292634964\n",
      "ep 3893: ep_len:598 episode reward: total was 9.230000. running mean: 15.794370\n",
      "ep 3893: ep_len:587 episode reward: total was 39.360000. running mean: 16.030026\n",
      "ep 3893: ep_len:512 episode reward: total was 0.890000. running mean: 15.878626\n",
      "ep 3893: ep_len:525 episode reward: total was 40.040000. running mean: 16.120240\n",
      "ep 3893: ep_len:3 episode reward: total was 1.010000. running mean: 15.969137\n",
      "ep 3893: ep_len:500 episode reward: total was 46.980000. running mean: 16.279246\n",
      "ep 3893: ep_len:500 episode reward: total was 11.270000. running mean: 16.229154\n",
      "epsilon:0.027366 episode_count: 27258. steps_count: 11789912.000000\n",
      "Time elapsed:  35790.360629320145\n",
      "ep 3894: ep_len:588 episode reward: total was -31.820000. running mean: 15.748662\n",
      "ep 3894: ep_len:286 episode reward: total was 22.210000. running mean: 15.813275\n",
      "ep 3894: ep_len:79 episode reward: total was 10.850000. running mean: 15.763643\n",
      "ep 3894: ep_len:500 episode reward: total was 57.330000. running mean: 16.179306\n",
      "ep 3894: ep_len:3 episode reward: total was 1.010000. running mean: 16.027613\n",
      "ep 3894: ep_len:501 episode reward: total was 26.580000. running mean: 16.133137\n",
      "ep 3894: ep_len:500 episode reward: total was 10.020000. running mean: 16.072006\n",
      "epsilon:0.027322 episode_count: 27265. steps_count: 11792369.000000\n",
      "Time elapsed:  35806.63762164116\n",
      "ep 3895: ep_len:500 episode reward: total was 63.440000. running mean: 16.545686\n",
      "ep 3895: ep_len:500 episode reward: total was 71.320000. running mean: 17.093429\n",
      "ep 3895: ep_len:500 episode reward: total was 46.590000. running mean: 17.388395\n",
      "ep 3895: ep_len:500 episode reward: total was 69.610000. running mean: 17.910611\n",
      "ep 3895: ep_len:88 episode reward: total was 25.760000. running mean: 17.989104\n",
      "ep 3895: ep_len:575 episode reward: total was 45.910000. running mean: 18.268313\n",
      "ep 3895: ep_len:199 episode reward: total was 11.030000. running mean: 18.195930\n",
      "epsilon:0.027277 episode_count: 27272. steps_count: 11795231.000000\n",
      "Time elapsed:  35815.31466269493\n",
      "ep 3896: ep_len:571 episode reward: total was 63.560000. running mean: 18.649571\n",
      "ep 3896: ep_len:539 episode reward: total was 47.490000. running mean: 18.937975\n",
      "ep 3896: ep_len:560 episode reward: total was 2.290000. running mean: 18.771496\n",
      "ep 3896: ep_len:565 episode reward: total was 18.440000. running mean: 18.768181\n",
      "ep 3896: ep_len:3 episode reward: total was 1.010000. running mean: 18.590599\n",
      "ep 3896: ep_len:587 episode reward: total was 26.680000. running mean: 18.671493\n",
      "ep 3896: ep_len:602 episode reward: total was 44.120000. running mean: 18.925978\n",
      "epsilon:0.027233 episode_count: 27279. steps_count: 11798658.000000\n",
      "Time elapsed:  35824.80799818039\n",
      "ep 3897: ep_len:646 episode reward: total was 12.870000. running mean: 18.865418\n",
      "ep 3897: ep_len:291 episode reward: total was -12.330000. running mean: 18.553464\n",
      "ep 3897: ep_len:589 episode reward: total was 18.780000. running mean: 18.555729\n",
      "ep 3897: ep_len:538 episode reward: total was 71.190000. running mean: 19.082072\n",
      "ep 3897: ep_len:87 episode reward: total was 25.260000. running mean: 19.143851\n",
      "ep 3897: ep_len:549 episode reward: total was 39.430000. running mean: 19.346713\n",
      "ep 3897: ep_len:592 episode reward: total was 39.650000. running mean: 19.549746\n",
      "epsilon:0.027189 episode_count: 27286. steps_count: 11801950.000000\n",
      "Time elapsed:  35837.4999358654\n",
      "ep 3898: ep_len:648 episode reward: total was 28.900000. running mean: 19.643248\n",
      "ep 3898: ep_len:500 episode reward: total was 101.900000. running mean: 20.465816\n",
      "ep 3898: ep_len:614 episode reward: total was 0.890000. running mean: 20.270058\n",
      "ep 3898: ep_len:500 episode reward: total was 25.080000. running mean: 20.318157\n",
      "ep 3898: ep_len:92 episode reward: total was 20.350000. running mean: 20.318475\n",
      "ep 3898: ep_len:500 episode reward: total was 42.830000. running mean: 20.543591\n",
      "ep 3898: ep_len:571 episode reward: total was 16.010000. running mean: 20.498255\n",
      "epsilon:0.027144 episode_count: 27293. steps_count: 11805375.000000\n",
      "Time elapsed:  35847.81197977066\n",
      "ep 3899: ep_len:251 episode reward: total was 26.300000. running mean: 20.556272\n",
      "ep 3899: ep_len:591 episode reward: total was -183.570000. running mean: 18.515009\n",
      "ep 3899: ep_len:500 episode reward: total was 60.410000. running mean: 18.933959\n",
      "ep 3899: ep_len:500 episode reward: total was 38.120000. running mean: 19.125820\n",
      "ep 3899: ep_len:118 episode reward: total was 22.360000. running mean: 19.158162\n",
      "ep 3899: ep_len:561 episode reward: total was 49.770000. running mean: 19.464280\n",
      "ep 3899: ep_len:500 episode reward: total was -4.350000. running mean: 19.226137\n",
      "epsilon:0.027100 episode_count: 27300. steps_count: 11808396.000000\n",
      "Time elapsed:  35855.643032073975\n",
      "ep 3900: ep_len:585 episode reward: total was -31.870000. running mean: 18.715176\n",
      "ep 3900: ep_len:530 episode reward: total was 13.840000. running mean: 18.666424\n",
      "ep 3900: ep_len:322 episode reward: total was 56.380000. running mean: 19.043560\n",
      "ep 3900: ep_len:404 episode reward: total was 25.170000. running mean: 19.104824\n",
      "ep 3900: ep_len:3 episode reward: total was 1.010000. running mean: 18.923876\n",
      "ep 3900: ep_len:614 episode reward: total was 53.020000. running mean: 19.264837\n",
      "ep 3900: ep_len:578 episode reward: total was 17.120000. running mean: 19.243389\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.027056 episode_count: 27307. steps_count: 11811432.000000\n",
      "Time elapsed:  35868.19493556023\n",
      "ep 3901: ep_len:558 episode reward: total was 59.760000. running mean: 19.648555\n",
      "ep 3901: ep_len:286 episode reward: total was 8.770000. running mean: 19.539769\n",
      "ep 3901: ep_len:500 episode reward: total was 44.520000. running mean: 19.789572\n",
      "ep 3901: ep_len:508 episode reward: total was 50.700000. running mean: 20.098676\n",
      "ep 3901: ep_len:55 episode reward: total was 24.010000. running mean: 20.137789\n",
      "ep 3901: ep_len:246 episode reward: total was 35.140000. running mean: 20.287811\n",
      "ep 3901: ep_len:500 episode reward: total was -1.600000. running mean: 20.068933\n",
      "epsilon:0.027011 episode_count: 27314. steps_count: 11814085.000000\n",
      "Time elapsed:  35876.32594847679\n",
      "ep 3902: ep_len:602 episode reward: total was -0.760000. running mean: 19.860644\n",
      "ep 3902: ep_len:524 episode reward: total was 76.290000. running mean: 20.424937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3902: ep_len:555 episode reward: total was -25.190000. running mean: 19.968788\n",
      "ep 3902: ep_len:500 episode reward: total was 80.380000. running mean: 20.572900\n",
      "ep 3902: ep_len:52 episode reward: total was 23.000000. running mean: 20.597171\n",
      "ep 3902: ep_len:508 episode reward: total was 50.110000. running mean: 20.892299\n",
      "ep 3902: ep_len:551 episode reward: total was 37.060000. running mean: 21.053976\n",
      "epsilon:0.026967 episode_count: 27321. steps_count: 11817377.000000\n",
      "Time elapsed:  35886.20150637627\n",
      "ep 3903: ep_len:544 episode reward: total was 49.110000. running mean: 21.334537\n",
      "ep 3903: ep_len:500 episode reward: total was 30.600000. running mean: 21.427191\n",
      "ep 3903: ep_len:670 episode reward: total was -43.910000. running mean: 20.773819\n",
      "ep 3903: ep_len:558 episode reward: total was 85.290000. running mean: 21.418981\n",
      "ep 3903: ep_len:101 episode reward: total was 31.770000. running mean: 21.522491\n",
      "ep 3903: ep_len:233 episode reward: total was 45.110000. running mean: 21.758366\n",
      "ep 3903: ep_len:516 episode reward: total was 14.750000. running mean: 21.688283\n",
      "epsilon:0.026923 episode_count: 27328. steps_count: 11820499.000000\n",
      "Time elapsed:  35895.55190014839\n",
      "ep 3904: ep_len:534 episode reward: total was 42.190000. running mean: 21.893300\n",
      "ep 3904: ep_len:555 episode reward: total was 15.070000. running mean: 21.825067\n",
      "ep 3904: ep_len:509 episode reward: total was 53.030000. running mean: 22.137116\n",
      "ep 3904: ep_len:534 episode reward: total was 14.080000. running mean: 22.056545\n",
      "ep 3904: ep_len:3 episode reward: total was 1.010000. running mean: 21.846080\n",
      "ep 3904: ep_len:638 episode reward: total was 26.050000. running mean: 21.888119\n",
      "ep 3904: ep_len:506 episode reward: total was 25.880000. running mean: 21.928038\n",
      "epsilon:0.026878 episode_count: 27335. steps_count: 11823778.000000\n",
      "Time elapsed:  35904.23837375641\n",
      "ep 3905: ep_len:235 episode reward: total was 34.340000. running mean: 22.052157\n",
      "ep 3905: ep_len:500 episode reward: total was -2.000000. running mean: 21.811636\n",
      "ep 3905: ep_len:606 episode reward: total was 7.840000. running mean: 21.671919\n",
      "ep 3905: ep_len:500 episode reward: total was 36.410000. running mean: 21.819300\n",
      "ep 3905: ep_len:3 episode reward: total was 1.010000. running mean: 21.611207\n",
      "ep 3905: ep_len:194 episode reward: total was 48.610000. running mean: 21.881195\n",
      "ep 3905: ep_len:512 episode reward: total was 11.800000. running mean: 21.780383\n",
      "epsilon:0.026834 episode_count: 27342. steps_count: 11826328.000000\n",
      "Time elapsed:  35921.162851810455\n",
      "ep 3906: ep_len:574 episode reward: total was -7.680000. running mean: 21.485779\n",
      "ep 3906: ep_len:575 episode reward: total was 39.190000. running mean: 21.662822\n",
      "ep 3906: ep_len:610 episode reward: total was 0.780000. running mean: 21.453993\n",
      "ep 3906: ep_len:519 episode reward: total was 29.950000. running mean: 21.538953\n",
      "ep 3906: ep_len:3 episode reward: total was 1.010000. running mean: 21.333664\n",
      "ep 3906: ep_len:600 episode reward: total was -25.980000. running mean: 20.860527\n",
      "ep 3906: ep_len:587 episode reward: total was 40.570000. running mean: 21.057622\n",
      "epsilon:0.026790 episode_count: 27349. steps_count: 11829796.000000\n",
      "Time elapsed:  35931.291002988815\n",
      "ep 3907: ep_len:500 episode reward: total was 6.510000. running mean: 20.912146\n",
      "ep 3907: ep_len:502 episode reward: total was 34.950000. running mean: 21.052524\n",
      "ep 3907: ep_len:464 episode reward: total was 68.880000. running mean: 21.530799\n",
      "ep 3907: ep_len:519 episode reward: total was 31.320000. running mean: 21.628691\n",
      "ep 3907: ep_len:3 episode reward: total was 1.010000. running mean: 21.422504\n",
      "ep 3907: ep_len:523 episode reward: total was 8.130000. running mean: 21.289579\n",
      "ep 3907: ep_len:570 episode reward: total was 11.350000. running mean: 21.190183\n",
      "epsilon:0.026745 episode_count: 27356. steps_count: 11832877.000000\n",
      "Time elapsed:  35940.52381706238\n",
      "ep 3908: ep_len:673 episode reward: total was -10.790000. running mean: 20.870381\n",
      "ep 3908: ep_len:595 episode reward: total was 43.470000. running mean: 21.096378\n",
      "ep 3908: ep_len:428 episode reward: total was 70.260000. running mean: 21.588014\n",
      "ep 3908: ep_len:124 episode reward: total was 20.130000. running mean: 21.573434\n",
      "ep 3908: ep_len:126 episode reward: total was 28.870000. running mean: 21.646399\n",
      "ep 3908: ep_len:500 episode reward: total was 23.180000. running mean: 21.661735\n",
      "ep 3908: ep_len:270 episode reward: total was 14.850000. running mean: 21.593618\n",
      "epsilon:0.026701 episode_count: 27363. steps_count: 11835593.000000\n",
      "Time elapsed:  35948.870666742325\n",
      "ep 3909: ep_len:586 episode reward: total was 61.580000. running mean: 21.993482\n",
      "ep 3909: ep_len:549 episode reward: total was 57.460000. running mean: 22.348147\n",
      "ep 3909: ep_len:397 episode reward: total was 78.690000. running mean: 22.911566\n",
      "ep 3909: ep_len:567 episode reward: total was 65.370000. running mean: 23.336150\n",
      "ep 3909: ep_len:93 episode reward: total was 25.750000. running mean: 23.360288\n",
      "ep 3909: ep_len:533 episode reward: total was 5.270000. running mean: 23.179386\n",
      "ep 3909: ep_len:504 episode reward: total was 4.740000. running mean: 22.994992\n",
      "epsilon:0.026657 episode_count: 27370. steps_count: 11838822.000000\n",
      "Time elapsed:  35958.60439801216\n",
      "ep 3910: ep_len:610 episode reward: total was -210.500000. running mean: 20.660042\n",
      "ep 3910: ep_len:531 episode reward: total was 11.740000. running mean: 20.570841\n",
      "ep 3910: ep_len:500 episode reward: total was 48.510000. running mean: 20.850233\n",
      "ep 3910: ep_len:571 episode reward: total was 72.020000. running mean: 21.361931\n",
      "ep 3910: ep_len:3 episode reward: total was 1.010000. running mean: 21.158411\n",
      "ep 3910: ep_len:635 episode reward: total was 48.910000. running mean: 21.435927\n",
      "ep 3910: ep_len:500 episode reward: total was 16.400000. running mean: 21.385568\n",
      "epsilon:0.026612 episode_count: 27377. steps_count: 11842172.000000\n",
      "Time elapsed:  35968.76313996315\n",
      "ep 3911: ep_len:563 episode reward: total was 76.090000. running mean: 21.932612\n",
      "ep 3911: ep_len:522 episode reward: total was 27.890000. running mean: 21.992186\n",
      "ep 3911: ep_len:530 episode reward: total was 32.900000. running mean: 22.101264\n",
      "ep 3911: ep_len:553 episode reward: total was 79.480000. running mean: 22.675052\n",
      "ep 3911: ep_len:3 episode reward: total was 1.010000. running mean: 22.458401\n",
      "ep 3911: ep_len:500 episode reward: total was 45.600000. running mean: 22.689817\n",
      "ep 3911: ep_len:325 episode reward: total was 33.540000. running mean: 22.798319\n",
      "epsilon:0.026568 episode_count: 27384. steps_count: 11845168.000000\n",
      "Time elapsed:  35978.13710951805\n",
      "ep 3912: ep_len:595 episode reward: total was 59.520000. running mean: 23.165536\n",
      "ep 3912: ep_len:580 episode reward: total was 74.150000. running mean: 23.675380\n",
      "ep 3912: ep_len:632 episode reward: total was 19.060000. running mean: 23.629227\n",
      "ep 3912: ep_len:598 episode reward: total was 116.280000. running mean: 24.555734\n",
      "ep 3912: ep_len:3 episode reward: total was 1.010000. running mean: 24.320277\n",
      "ep 3912: ep_len:645 episode reward: total was 37.660000. running mean: 24.453674\n",
      "ep 3912: ep_len:600 episode reward: total was 36.500000. running mean: 24.574137\n",
      "epsilon:0.026524 episode_count: 27391. steps_count: 11848821.000000\n",
      "Time elapsed:  35989.36420798302\n",
      "ep 3913: ep_len:134 episode reward: total was 2.530000. running mean: 24.353696\n",
      "ep 3913: ep_len:503 episode reward: total was -237.250000. running mean: 21.737659\n",
      "ep 3913: ep_len:650 episode reward: total was 3.060000. running mean: 21.550883\n",
      "ep 3913: ep_len:512 episode reward: total was -9.280000. running mean: 21.242574\n",
      "ep 3913: ep_len:77 episode reward: total was 20.630000. running mean: 21.236448\n",
      "ep 3913: ep_len:673 episode reward: total was 27.380000. running mean: 21.297883\n",
      "ep 3913: ep_len:328 episode reward: total was 24.110000. running mean: 21.326005\n",
      "epsilon:0.026479 episode_count: 27398. steps_count: 11851698.000000\n",
      "Time elapsed:  35998.43370628357\n",
      "ep 3914: ep_len:500 episode reward: total was -84.320000. running mean: 20.269545\n",
      "ep 3914: ep_len:500 episode reward: total was 63.060000. running mean: 20.697449\n",
      "ep 3914: ep_len:596 episode reward: total was -17.140000. running mean: 20.319075\n",
      "ep 3914: ep_len:537 episode reward: total was -156.130000. running mean: 18.554584\n",
      "ep 3914: ep_len:52 episode reward: total was 24.500000. running mean: 18.614038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3914: ep_len:531 episode reward: total was 33.410000. running mean: 18.761998\n",
      "ep 3914: ep_len:520 episode reward: total was 17.220000. running mean: 18.746578\n",
      "epsilon:0.026435 episode_count: 27405. steps_count: 11854934.000000\n",
      "Time elapsed:  36015.423585653305\n",
      "ep 3915: ep_len:620 episode reward: total was 43.010000. running mean: 18.989212\n",
      "ep 3915: ep_len:531 episode reward: total was 8.920000. running mean: 18.888520\n",
      "ep 3915: ep_len:533 episode reward: total was 41.620000. running mean: 19.115835\n",
      "ep 3915: ep_len:509 episode reward: total was 87.120000. running mean: 19.795876\n",
      "ep 3915: ep_len:3 episode reward: total was 1.010000. running mean: 19.608018\n",
      "ep 3915: ep_len:508 episode reward: total was 36.550000. running mean: 19.777437\n",
      "ep 3915: ep_len:593 episode reward: total was 62.930000. running mean: 20.208963\n",
      "epsilon:0.026391 episode_count: 27412. steps_count: 11858231.000000\n",
      "Time elapsed:  36025.30871844292\n",
      "ep 3916: ep_len:560 episode reward: total was 17.650000. running mean: 20.183373\n",
      "ep 3916: ep_len:508 episode reward: total was 40.060000. running mean: 20.382140\n",
      "ep 3916: ep_len:77 episode reward: total was 9.300000. running mean: 20.271318\n",
      "ep 3916: ep_len:396 episode reward: total was 42.170000. running mean: 20.490305\n",
      "ep 3916: ep_len:3 episode reward: total was 1.010000. running mean: 20.295502\n",
      "ep 3916: ep_len:633 episode reward: total was 32.080000. running mean: 20.413347\n",
      "ep 3916: ep_len:530 episode reward: total was 59.880000. running mean: 20.808013\n",
      "epsilon:0.026346 episode_count: 27419. steps_count: 11860938.000000\n",
      "Time elapsed:  36033.61097955704\n",
      "ep 3917: ep_len:542 episode reward: total was -30.520000. running mean: 20.294733\n",
      "ep 3917: ep_len:280 episode reward: total was -69.050000. running mean: 19.401286\n",
      "ep 3917: ep_len:419 episode reward: total was 30.190000. running mean: 19.509173\n",
      "ep 3917: ep_len:517 episode reward: total was 27.220000. running mean: 19.586281\n",
      "ep 3917: ep_len:134 episode reward: total was 23.350000. running mean: 19.623919\n",
      "ep 3917: ep_len:223 episode reward: total was 34.550000. running mean: 19.773179\n",
      "ep 3917: ep_len:500 episode reward: total was 11.020000. running mean: 19.685648\n",
      "epsilon:0.026302 episode_count: 27426. steps_count: 11863553.000000\n",
      "Time elapsed:  36049.46855354309\n",
      "ep 3918: ep_len:577 episode reward: total was 4.530000. running mean: 19.534091\n",
      "ep 3918: ep_len:269 episode reward: total was 15.670000. running mean: 19.495450\n",
      "ep 3918: ep_len:545 episode reward: total was -10.060000. running mean: 19.199896\n",
      "ep 3918: ep_len:501 episode reward: total was 82.030000. running mean: 19.828197\n",
      "ep 3918: ep_len:3 episode reward: total was 1.010000. running mean: 19.640015\n",
      "ep 3918: ep_len:299 episode reward: total was 33.550000. running mean: 19.779115\n",
      "ep 3918: ep_len:286 episode reward: total was 26.340000. running mean: 19.844724\n",
      "epsilon:0.026258 episode_count: 27433. steps_count: 11866033.000000\n",
      "Time elapsed:  36057.961985588074\n",
      "ep 3919: ep_len:559 episode reward: total was 71.970000. running mean: 20.365976\n",
      "ep 3919: ep_len:243 episode reward: total was 27.800000. running mean: 20.440317\n",
      "ep 3919: ep_len:558 episode reward: total was 73.250000. running mean: 20.968413\n",
      "ep 3919: ep_len:516 episode reward: total was 63.500000. running mean: 21.393729\n",
      "ep 3919: ep_len:3 episode reward: total was 1.010000. running mean: 21.189892\n",
      "ep 3919: ep_len:545 episode reward: total was -2.030000. running mean: 20.957693\n",
      "ep 3919: ep_len:514 episode reward: total was 28.250000. running mean: 21.030616\n",
      "epsilon:0.026213 episode_count: 27440. steps_count: 11868971.000000\n",
      "Time elapsed:  36076.40648674965\n",
      "ep 3920: ep_len:121 episode reward: total was 11.520000. running mean: 20.935510\n",
      "ep 3920: ep_len:500 episode reward: total was 60.460000. running mean: 21.330755\n",
      "ep 3920: ep_len:61 episode reward: total was 8.130000. running mean: 21.198747\n",
      "ep 3920: ep_len:540 episode reward: total was -72.160000. running mean: 20.265160\n",
      "ep 3920: ep_len:48 episode reward: total was 18.000000. running mean: 20.242508\n",
      "ep 3920: ep_len:570 episode reward: total was 59.660000. running mean: 20.636683\n",
      "ep 3920: ep_len:502 episode reward: total was -73.320000. running mean: 19.697116\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.026169 episode_count: 27447. steps_count: 11871313.000000\n",
      "Time elapsed:  36090.121156454086\n",
      "ep 3921: ep_len:500 episode reward: total was 53.030000. running mean: 20.030445\n",
      "ep 3921: ep_len:525 episode reward: total was 111.230000. running mean: 20.942441\n",
      "ep 3921: ep_len:376 episode reward: total was 18.150000. running mean: 20.914516\n",
      "ep 3921: ep_len:56 episode reward: total was 3.850000. running mean: 20.743871\n",
      "ep 3921: ep_len:69 episode reward: total was 19.750000. running mean: 20.733932\n",
      "ep 3921: ep_len:585 episode reward: total was 38.780000. running mean: 20.914393\n",
      "ep 3921: ep_len:308 episode reward: total was 28.220000. running mean: 20.987449\n",
      "epsilon:0.026125 episode_count: 27454. steps_count: 11873732.000000\n",
      "Time elapsed:  36097.33642077446\n",
      "ep 3922: ep_len:594 episode reward: total was -307.500000. running mean: 17.702575\n",
      "ep 3922: ep_len:596 episode reward: total was 66.670000. running mean: 18.192249\n",
      "ep 3922: ep_len:528 episode reward: total was -44.790000. running mean: 17.562426\n",
      "ep 3922: ep_len:599 episode reward: total was -0.460000. running mean: 17.382202\n",
      "ep 3922: ep_len:3 episode reward: total was 1.010000. running mean: 17.218480\n",
      "ep 3922: ep_len:553 episode reward: total was 52.720000. running mean: 17.573495\n",
      "ep 3922: ep_len:551 episode reward: total was 29.750000. running mean: 17.695260\n",
      "epsilon:0.026080 episode_count: 27461. steps_count: 11877156.000000\n",
      "Time elapsed:  36107.522423028946\n",
      "ep 3923: ep_len:573 episode reward: total was 70.220000. running mean: 18.220508\n",
      "ep 3923: ep_len:500 episode reward: total was -14.030000. running mean: 17.898003\n",
      "ep 3923: ep_len:566 episode reward: total was 42.570000. running mean: 18.144723\n",
      "ep 3923: ep_len:500 episode reward: total was 27.330000. running mean: 18.236575\n",
      "ep 3923: ep_len:3 episode reward: total was -1.500000. running mean: 18.039210\n",
      "ep 3923: ep_len:504 episode reward: total was 43.260000. running mean: 18.291418\n",
      "ep 3923: ep_len:578 episode reward: total was 51.320000. running mean: 18.621703\n",
      "epsilon:0.026036 episode_count: 27468. steps_count: 11880380.000000\n",
      "Time elapsed:  36114.28120017052\n",
      "ep 3924: ep_len:500 episode reward: total was 73.380000. running mean: 19.169286\n",
      "ep 3924: ep_len:201 episode reward: total was 7.470000. running mean: 19.052294\n",
      "ep 3924: ep_len:546 episode reward: total was -13.050000. running mean: 18.731271\n",
      "ep 3924: ep_len:508 episode reward: total was 72.510000. running mean: 19.269058\n",
      "ep 3924: ep_len:2 episode reward: total was -0.500000. running mean: 19.071367\n",
      "ep 3924: ep_len:572 episode reward: total was 28.010000. running mean: 19.160754\n",
      "ep 3924: ep_len:556 episode reward: total was -73.420000. running mean: 18.234946\n",
      "epsilon:0.025992 episode_count: 27475. steps_count: 11883265.000000\n",
      "Time elapsed:  36119.88488507271\n",
      "ep 3925: ep_len:599 episode reward: total was -6.240000. running mean: 17.990197\n",
      "ep 3925: ep_len:500 episode reward: total was 29.020000. running mean: 18.100495\n",
      "ep 3925: ep_len:663 episode reward: total was -3.390000. running mean: 17.885590\n",
      "ep 3925: ep_len:513 episode reward: total was 54.900000. running mean: 18.255734\n",
      "ep 3925: ep_len:3 episode reward: total was 1.010000. running mean: 18.083276\n",
      "ep 3925: ep_len:500 episode reward: total was 36.410000. running mean: 18.266544\n",
      "ep 3925: ep_len:500 episode reward: total was 1.500000. running mean: 18.098878\n",
      "epsilon:0.025947 episode_count: 27482. steps_count: 11886543.000000\n",
      "Time elapsed:  36126.17349290848\n",
      "ep 3926: ep_len:532 episode reward: total was -9.980000. running mean: 17.818089\n",
      "ep 3926: ep_len:500 episode reward: total was 53.680000. running mean: 18.176709\n",
      "ep 3926: ep_len:374 episode reward: total was 47.080000. running mean: 18.465742\n",
      "ep 3926: ep_len:400 episode reward: total was 53.610000. running mean: 18.817184\n",
      "ep 3926: ep_len:86 episode reward: total was 18.210000. running mean: 18.811112\n",
      "ep 3926: ep_len:656 episode reward: total was 38.720000. running mean: 19.010201\n",
      "ep 3926: ep_len:528 episode reward: total was 37.880000. running mean: 19.198899\n",
      "epsilon:0.025903 episode_count: 27489. steps_count: 11889619.000000\n",
      "Time elapsed:  36132.122218847275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3927: ep_len:618 episode reward: total was 39.100000. running mean: 19.397910\n",
      "ep 3927: ep_len:500 episode reward: total was 33.670000. running mean: 19.540631\n",
      "ep 3927: ep_len:398 episode reward: total was 56.140000. running mean: 19.906625\n",
      "ep 3927: ep_len:407 episode reward: total was -8.260000. running mean: 19.624958\n",
      "ep 3927: ep_len:3 episode reward: total was 1.010000. running mean: 19.438809\n",
      "ep 3927: ep_len:537 episode reward: total was 34.940000. running mean: 19.593821\n",
      "ep 3927: ep_len:587 episode reward: total was 43.050000. running mean: 19.828383\n",
      "epsilon:0.025859 episode_count: 27496. steps_count: 11892669.000000\n",
      "Time elapsed:  36149.33812832832\n",
      "ep 3928: ep_len:574 episode reward: total was 52.960000. running mean: 20.159699\n",
      "ep 3928: ep_len:305 episode reward: total was 16.920000. running mean: 20.127302\n",
      "ep 3928: ep_len:425 episode reward: total was 50.880000. running mean: 20.434829\n",
      "ep 3928: ep_len:558 episode reward: total was 78.280000. running mean: 21.013280\n",
      "ep 3928: ep_len:111 episode reward: total was 35.760000. running mean: 21.160748\n",
      "ep 3928: ep_len:573 episode reward: total was 30.120000. running mean: 21.250340\n",
      "ep 3928: ep_len:508 episode reward: total was 7.230000. running mean: 21.110137\n",
      "epsilon:0.025814 episode_count: 27503. steps_count: 11895723.000000\n",
      "Time elapsed:  36158.595940351486\n",
      "ep 3929: ep_len:501 episode reward: total was 75.050000. running mean: 21.649535\n",
      "ep 3929: ep_len:546 episode reward: total was -25.590000. running mean: 21.177140\n",
      "ep 3929: ep_len:500 episode reward: total was -0.320000. running mean: 20.962169\n",
      "ep 3929: ep_len:156 episode reward: total was 25.700000. running mean: 21.009547\n",
      "ep 3929: ep_len:113 episode reward: total was 29.810000. running mean: 21.097552\n",
      "ep 3929: ep_len:247 episode reward: total was 38.950000. running mean: 21.276076\n",
      "ep 3929: ep_len:500 episode reward: total was 17.370000. running mean: 21.237015\n",
      "epsilon:0.025770 episode_count: 27510. steps_count: 11898286.000000\n",
      "Time elapsed:  36166.59015035629\n",
      "ep 3930: ep_len:240 episode reward: total was 2.250000. running mean: 21.047145\n",
      "ep 3930: ep_len:516 episode reward: total was 11.920000. running mean: 20.955874\n",
      "ep 3930: ep_len:578 episode reward: total was 23.530000. running mean: 20.981615\n",
      "ep 3930: ep_len:528 episode reward: total was 82.520000. running mean: 21.596999\n",
      "ep 3930: ep_len:3 episode reward: total was 1.010000. running mean: 21.391129\n",
      "ep 3930: ep_len:643 episode reward: total was 43.570000. running mean: 21.612917\n",
      "ep 3930: ep_len:582 episode reward: total was -31.590000. running mean: 21.080888\n",
      "epsilon:0.025726 episode_count: 27517. steps_count: 11901376.000000\n",
      "Time elapsed:  36180.186351537704\n",
      "ep 3931: ep_len:215 episode reward: total was 35.230000. running mean: 21.222379\n",
      "ep 3931: ep_len:604 episode reward: total was 131.350000. running mean: 22.323656\n",
      "ep 3931: ep_len:605 episode reward: total was 31.610000. running mean: 22.416519\n",
      "ep 3931: ep_len:598 episode reward: total was 69.390000. running mean: 22.886254\n",
      "ep 3931: ep_len:71 episode reward: total was 21.210000. running mean: 22.869491\n",
      "ep 3931: ep_len:691 episode reward: total was -70.250000. running mean: 21.938296\n",
      "ep 3931: ep_len:543 episode reward: total was 43.680000. running mean: 22.155713\n",
      "epsilon:0.025681 episode_count: 27524. steps_count: 11904703.000000\n",
      "Time elapsed:  36187.10350179672\n",
      "ep 3932: ep_len:557 episode reward: total was 47.100000. running mean: 22.405156\n",
      "ep 3932: ep_len:500 episode reward: total was 33.180000. running mean: 22.512905\n",
      "ep 3932: ep_len:500 episode reward: total was 39.990000. running mean: 22.687676\n",
      "ep 3932: ep_len:555 episode reward: total was 91.690000. running mean: 23.377699\n",
      "ep 3932: ep_len:56 episode reward: total was 23.500000. running mean: 23.378922\n",
      "ep 3932: ep_len:500 episode reward: total was 12.550000. running mean: 23.270633\n",
      "ep 3932: ep_len:503 episode reward: total was -33.070000. running mean: 22.707226\n",
      "epsilon:0.025637 episode_count: 27531. steps_count: 11907874.000000\n",
      "Time elapsed:  36195.426745176315\n",
      "ep 3933: ep_len:611 episode reward: total was -15.340000. running mean: 22.326754\n",
      "ep 3933: ep_len:189 episode reward: total was 6.830000. running mean: 22.171787\n",
      "ep 3933: ep_len:531 episode reward: total was 42.000000. running mean: 22.370069\n",
      "ep 3933: ep_len:500 episode reward: total was 31.490000. running mean: 22.461268\n",
      "ep 3933: ep_len:3 episode reward: total was 1.010000. running mean: 22.246755\n",
      "ep 3933: ep_len:631 episode reward: total was 27.680000. running mean: 22.301088\n",
      "ep 3933: ep_len:297 episode reward: total was 14.570000. running mean: 22.223777\n",
      "epsilon:0.025593 episode_count: 27538. steps_count: 11910636.000000\n",
      "Time elapsed:  36205.18772149086\n",
      "ep 3934: ep_len:555 episode reward: total was -23.180000. running mean: 21.769739\n",
      "ep 3934: ep_len:502 episode reward: total was 48.450000. running mean: 22.036542\n",
      "ep 3934: ep_len:73 episode reward: total was 8.800000. running mean: 21.904176\n",
      "ep 3934: ep_len:530 episode reward: total was 41.060000. running mean: 22.095735\n",
      "ep 3934: ep_len:3 episode reward: total was 1.010000. running mean: 21.884877\n",
      "ep 3934: ep_len:568 episode reward: total was 6.130000. running mean: 21.727328\n",
      "ep 3934: ep_len:517 episode reward: total was 40.560000. running mean: 21.915655\n",
      "epsilon:0.025548 episode_count: 27545. steps_count: 11913384.000000\n",
      "Time elapsed:  36212.62397623062\n",
      "ep 3935: ep_len:500 episode reward: total was 67.010000. running mean: 22.366599\n",
      "ep 3935: ep_len:601 episode reward: total was 98.580000. running mean: 23.128733\n",
      "ep 3935: ep_len:623 episode reward: total was 13.280000. running mean: 23.030245\n",
      "ep 3935: ep_len:500 episode reward: total was 36.120000. running mean: 23.161143\n",
      "ep 3935: ep_len:3 episode reward: total was 1.010000. running mean: 22.939631\n",
      "ep 3935: ep_len:500 episode reward: total was 21.040000. running mean: 22.920635\n",
      "ep 3935: ep_len:575 episode reward: total was 42.740000. running mean: 23.118829\n",
      "epsilon:0.025504 episode_count: 27552. steps_count: 11916686.000000\n",
      "Time elapsed:  36221.3452372551\n",
      "ep 3936: ep_len:124 episode reward: total was 9.350000. running mean: 22.981141\n",
      "ep 3936: ep_len:512 episode reward: total was 12.370000. running mean: 22.875029\n",
      "ep 3936: ep_len:500 episode reward: total was -155.790000. running mean: 21.088379\n",
      "ep 3936: ep_len:501 episode reward: total was 38.460000. running mean: 21.262095\n",
      "ep 3936: ep_len:3 episode reward: total was 1.010000. running mean: 21.059574\n",
      "ep 3936: ep_len:500 episode reward: total was -64.950000. running mean: 20.199478\n",
      "ep 3936: ep_len:539 episode reward: total was -8.570000. running mean: 19.911784\n",
      "epsilon:0.025460 episode_count: 27559. steps_count: 11919365.000000\n",
      "Time elapsed:  36228.72965526581\n",
      "ep 3937: ep_len:561 episode reward: total was 71.660000. running mean: 20.429266\n",
      "ep 3937: ep_len:500 episode reward: total was 14.600000. running mean: 20.370973\n",
      "ep 3937: ep_len:597 episode reward: total was 0.220000. running mean: 20.169463\n",
      "ep 3937: ep_len:500 episode reward: total was 35.600000. running mean: 20.323769\n",
      "ep 3937: ep_len:98 episode reward: total was 17.290000. running mean: 20.293431\n",
      "ep 3937: ep_len:533 episode reward: total was 14.010000. running mean: 20.230597\n",
      "ep 3937: ep_len:559 episode reward: total was 23.220000. running mean: 20.260491\n",
      "epsilon:0.025415 episode_count: 27566. steps_count: 11922713.000000\n",
      "Time elapsed:  36237.64823913574\n",
      "ep 3938: ep_len:500 episode reward: total was -4.540000. running mean: 20.012486\n",
      "ep 3938: ep_len:352 episode reward: total was 12.880000. running mean: 19.941161\n",
      "ep 3938: ep_len:564 episode reward: total was 28.280000. running mean: 20.024549\n",
      "ep 3938: ep_len:114 episode reward: total was 24.010000. running mean: 20.064404\n",
      "ep 3938: ep_len:103 episode reward: total was 29.710000. running mean: 20.160860\n",
      "ep 3938: ep_len:323 episode reward: total was 13.220000. running mean: 20.091451\n",
      "ep 3938: ep_len:519 episode reward: total was 55.210000. running mean: 20.442637\n",
      "epsilon:0.025371 episode_count: 27573. steps_count: 11925188.000000\n",
      "Time elapsed:  36245.683715343475\n",
      "ep 3939: ep_len:580 episode reward: total was 75.280000. running mean: 20.991010\n",
      "ep 3939: ep_len:599 episode reward: total was 16.540000. running mean: 20.946500\n",
      "ep 3939: ep_len:654 episode reward: total was 9.740000. running mean: 20.834435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3939: ep_len:535 episode reward: total was 76.140000. running mean: 21.387491\n",
      "ep 3939: ep_len:3 episode reward: total was 0.000000. running mean: 21.173616\n",
      "ep 3939: ep_len:531 episode reward: total was 51.780000. running mean: 21.479680\n",
      "ep 3939: ep_len:581 episode reward: total was 38.350000. running mean: 21.648383\n",
      "epsilon:0.025327 episode_count: 27580. steps_count: 11928671.000000\n",
      "Time elapsed:  36261.17872548103\n",
      "ep 3940: ep_len:500 episode reward: total was 86.020000. running mean: 22.292099\n",
      "ep 3940: ep_len:196 episode reward: total was 7.910000. running mean: 22.148278\n",
      "ep 3940: ep_len:366 episode reward: total was 49.600000. running mean: 22.422795\n",
      "ep 3940: ep_len:590 episode reward: total was 65.450000. running mean: 22.853067\n",
      "ep 3940: ep_len:3 episode reward: total was 1.010000. running mean: 22.634637\n",
      "ep 3940: ep_len:684 episode reward: total was 34.530000. running mean: 22.753590\n",
      "ep 3940: ep_len:500 episode reward: total was 64.870000. running mean: 23.174755\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.025282 episode_count: 27587. steps_count: 11931510.000000\n",
      "Time elapsed:  36273.35482931137\n",
      "ep 3941: ep_len:500 episode reward: total was 37.950000. running mean: 23.322507\n",
      "ep 3941: ep_len:341 episode reward: total was -7.470000. running mean: 23.014582\n",
      "ep 3941: ep_len:644 episode reward: total was 18.060000. running mean: 22.965036\n",
      "ep 3941: ep_len:527 episode reward: total was 59.160000. running mean: 23.326986\n",
      "ep 3941: ep_len:3 episode reward: total was 1.010000. running mean: 23.103816\n",
      "ep 3941: ep_len:241 episode reward: total was 39.470000. running mean: 23.267478\n",
      "ep 3941: ep_len:536 episode reward: total was 16.710000. running mean: 23.201903\n",
      "epsilon:0.025238 episode_count: 27594. steps_count: 11934302.000000\n",
      "Time elapsed:  36280.89289522171\n",
      "ep 3942: ep_len:556 episode reward: total was 57.680000. running mean: 23.546684\n",
      "ep 3942: ep_len:358 episode reward: total was 24.080000. running mean: 23.552017\n",
      "ep 3942: ep_len:79 episode reward: total was 10.850000. running mean: 23.424997\n",
      "ep 3942: ep_len:533 episode reward: total was 74.510000. running mean: 23.935847\n",
      "ep 3942: ep_len:103 episode reward: total was 25.270000. running mean: 23.949188\n",
      "ep 3942: ep_len:519 episode reward: total was -12.300000. running mean: 23.586697\n",
      "ep 3942: ep_len:358 episode reward: total was 27.290000. running mean: 23.623730\n",
      "epsilon:0.025194 episode_count: 27601. steps_count: 11936808.000000\n",
      "Time elapsed:  36294.333355903625\n",
      "ep 3943: ep_len:551 episode reward: total was 26.470000. running mean: 23.652192\n",
      "ep 3943: ep_len:561 episode reward: total was 135.260000. running mean: 24.768270\n",
      "ep 3943: ep_len:612 episode reward: total was 16.820000. running mean: 24.688788\n",
      "ep 3943: ep_len:43 episode reward: total was -3.320000. running mean: 24.408700\n",
      "ep 3943: ep_len:90 episode reward: total was 25.750000. running mean: 24.422113\n",
      "ep 3943: ep_len:186 episode reward: total was 35.290000. running mean: 24.530792\n",
      "ep 3943: ep_len:598 episode reward: total was 5.350000. running mean: 24.338984\n",
      "epsilon:0.025149 episode_count: 27608. steps_count: 11939449.000000\n",
      "Time elapsed:  36299.62425374985\n",
      "ep 3944: ep_len:500 episode reward: total was 33.510000. running mean: 24.430694\n",
      "ep 3944: ep_len:601 episode reward: total was 27.380000. running mean: 24.460187\n",
      "ep 3944: ep_len:641 episode reward: total was -38.260000. running mean: 23.832985\n",
      "ep 3944: ep_len:150 episode reward: total was 8.070000. running mean: 23.675355\n",
      "ep 3944: ep_len:110 episode reward: total was 33.270000. running mean: 23.771302\n",
      "ep 3944: ep_len:223 episode reward: total was 34.630000. running mean: 23.879889\n",
      "ep 3944: ep_len:175 episode reward: total was 13.930000. running mean: 23.780390\n",
      "epsilon:0.025105 episode_count: 27615. steps_count: 11941849.000000\n",
      "Time elapsed:  36303.50394964218\n",
      "ep 3945: ep_len:500 episode reward: total was 66.460000. running mean: 24.207186\n",
      "ep 3945: ep_len:500 episode reward: total was 22.650000. running mean: 24.191614\n",
      "ep 3945: ep_len:555 episode reward: total was -18.470000. running mean: 23.764998\n",
      "ep 3945: ep_len:500 episode reward: total was -85.250000. running mean: 22.674848\n",
      "ep 3945: ep_len:73 episode reward: total was 17.650000. running mean: 22.624599\n",
      "ep 3945: ep_len:241 episode reward: total was 43.420000. running mean: 22.832553\n",
      "ep 3945: ep_len:690 episode reward: total was -4.240000. running mean: 22.561828\n",
      "epsilon:0.025061 episode_count: 27622. steps_count: 11944908.000000\n",
      "Time elapsed:  36311.64914751053\n",
      "ep 3946: ep_len:582 episode reward: total was -11.210000. running mean: 22.224110\n",
      "ep 3946: ep_len:289 episode reward: total was 4.060000. running mean: 22.042469\n",
      "ep 3946: ep_len:681 episode reward: total was 3.860000. running mean: 21.860644\n",
      "ep 3946: ep_len:528 episode reward: total was 39.370000. running mean: 22.035737\n",
      "ep 3946: ep_len:52 episode reward: total was 25.510000. running mean: 22.070480\n",
      "ep 3946: ep_len:567 episode reward: total was 47.230000. running mean: 22.322075\n",
      "ep 3946: ep_len:504 episode reward: total was 19.620000. running mean: 22.295054\n",
      "epsilon:0.025016 episode_count: 27629. steps_count: 11948111.000000\n",
      "Time elapsed:  36320.47743320465\n",
      "ep 3947: ep_len:630 episode reward: total was -43.480000. running mean: 21.637304\n",
      "ep 3947: ep_len:287 episode reward: total was 18.180000. running mean: 21.602731\n",
      "ep 3947: ep_len:503 episode reward: total was -33.850000. running mean: 21.048204\n",
      "ep 3947: ep_len:517 episode reward: total was -51.010000. running mean: 20.327622\n",
      "ep 3947: ep_len:47 episode reward: total was 20.500000. running mean: 20.329345\n",
      "ep 3947: ep_len:500 episode reward: total was 42.520000. running mean: 20.551252\n",
      "ep 3947: ep_len:587 episode reward: total was 74.310000. running mean: 21.088839\n",
      "epsilon:0.024972 episode_count: 27636. steps_count: 11951182.000000\n",
      "Time elapsed:  36334.57603979111\n",
      "ep 3948: ep_len:601 episode reward: total was -22.140000. running mean: 20.656551\n",
      "ep 3948: ep_len:500 episode reward: total was 8.820000. running mean: 20.538185\n",
      "ep 3948: ep_len:70 episode reward: total was 3.290000. running mean: 20.365704\n",
      "ep 3948: ep_len:507 episode reward: total was 40.930000. running mean: 20.571347\n",
      "ep 3948: ep_len:3 episode reward: total was 1.010000. running mean: 20.375733\n",
      "ep 3948: ep_len:500 episode reward: total was 38.540000. running mean: 20.557376\n",
      "ep 3948: ep_len:559 episode reward: total was -7.550000. running mean: 20.276302\n",
      "epsilon:0.024928 episode_count: 27643. steps_count: 11953922.000000\n",
      "Time elapsed:  36341.399175167084\n",
      "ep 3949: ep_len:671 episode reward: total was 2.590000. running mean: 20.099439\n",
      "ep 3949: ep_len:563 episode reward: total was 118.990000. running mean: 21.088345\n",
      "ep 3949: ep_len:616 episode reward: total was 7.970000. running mean: 20.957161\n",
      "ep 3949: ep_len:47 episode reward: total was -7.200000. running mean: 20.675590\n",
      "ep 3949: ep_len:3 episode reward: total was 1.010000. running mean: 20.478934\n",
      "ep 3949: ep_len:525 episode reward: total was 43.890000. running mean: 20.713044\n",
      "ep 3949: ep_len:610 episode reward: total was 51.900000. running mean: 21.024914\n",
      "epsilon:0.024883 episode_count: 27650. steps_count: 11956957.000000\n",
      "Time elapsed:  36349.55763101578\n",
      "ep 3950: ep_len:541 episode reward: total was 38.250000. running mean: 21.197165\n",
      "ep 3950: ep_len:500 episode reward: total was 109.150000. running mean: 22.076693\n",
      "ep 3950: ep_len:79 episode reward: total was 12.350000. running mean: 21.979426\n",
      "ep 3950: ep_len:395 episode reward: total was -36.140000. running mean: 21.398232\n",
      "ep 3950: ep_len:3 episode reward: total was 1.010000. running mean: 21.194350\n",
      "ep 3950: ep_len:573 episode reward: total was 34.190000. running mean: 21.324306\n",
      "ep 3950: ep_len:522 episode reward: total was 46.340000. running mean: 21.574463\n",
      "epsilon:0.024839 episode_count: 27657. steps_count: 11959570.000000\n",
      "Time elapsed:  36357.01315188408\n",
      "ep 3951: ep_len:205 episode reward: total was 25.700000. running mean: 21.615718\n",
      "ep 3951: ep_len:500 episode reward: total was 91.920000. running mean: 22.318761\n",
      "ep 3951: ep_len:652 episode reward: total was 10.280000. running mean: 22.198374\n",
      "ep 3951: ep_len:776 episode reward: total was -372.170000. running mean: 18.254690\n",
      "ep 3951: ep_len:3 episode reward: total was 1.010000. running mean: 18.082243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3951: ep_len:516 episode reward: total was -16.700000. running mean: 17.734421\n",
      "ep 3951: ep_len:528 episode reward: total was 28.420000. running mean: 17.841276\n",
      "epsilon:0.024795 episode_count: 27664. steps_count: 11962750.000000\n",
      "Time elapsed:  36379.36661744118\n",
      "ep 3952: ep_len:590 episode reward: total was 64.490000. running mean: 18.307764\n",
      "ep 3952: ep_len:503 episode reward: total was -105.810000. running mean: 17.066586\n",
      "ep 3952: ep_len:585 episode reward: total was -182.670000. running mean: 15.069220\n",
      "ep 3952: ep_len:500 episode reward: total was 72.460000. running mean: 15.643128\n",
      "ep 3952: ep_len:3 episode reward: total was 1.010000. running mean: 15.496797\n",
      "ep 3952: ep_len:238 episode reward: total was 26.920000. running mean: 15.611029\n",
      "ep 3952: ep_len:500 episode reward: total was 64.990000. running mean: 16.104818\n",
      "epsilon:0.024750 episode_count: 27671. steps_count: 11965669.000000\n",
      "Time elapsed:  36387.211434841156\n",
      "ep 3953: ep_len:242 episode reward: total was 19.290000. running mean: 16.136670\n",
      "ep 3953: ep_len:500 episode reward: total was 30.270000. running mean: 16.278003\n",
      "ep 3953: ep_len:553 episode reward: total was 20.030000. running mean: 16.315523\n",
      "ep 3953: ep_len:500 episode reward: total was 55.770000. running mean: 16.710068\n",
      "ep 3953: ep_len:3 episode reward: total was 1.010000. running mean: 16.553068\n",
      "ep 3953: ep_len:674 episode reward: total was 39.300000. running mean: 16.780537\n",
      "ep 3953: ep_len:578 episode reward: total was -3.490000. running mean: 16.577831\n",
      "epsilon:0.024706 episode_count: 27678. steps_count: 11968719.000000\n",
      "Time elapsed:  36395.299647808075\n",
      "ep 3954: ep_len:119 episode reward: total was -1.110000. running mean: 16.400953\n",
      "ep 3954: ep_len:500 episode reward: total was 137.780000. running mean: 17.614744\n",
      "ep 3954: ep_len:593 episode reward: total was 5.440000. running mean: 17.492996\n",
      "ep 3954: ep_len:538 episode reward: total was 70.000000. running mean: 18.018066\n",
      "ep 3954: ep_len:3 episode reward: total was 1.010000. running mean: 17.847986\n",
      "ep 3954: ep_len:500 episode reward: total was 49.530000. running mean: 18.164806\n",
      "ep 3954: ep_len:211 episode reward: total was 22.290000. running mean: 18.206058\n",
      "epsilon:0.024662 episode_count: 27685. steps_count: 11971183.000000\n",
      "Time elapsed:  36407.26927804947\n",
      "ep 3955: ep_len:568 episode reward: total was 14.310000. running mean: 18.167097\n",
      "ep 3955: ep_len:500 episode reward: total was 55.180000. running mean: 18.537226\n",
      "ep 3955: ep_len:389 episode reward: total was 63.580000. running mean: 18.987654\n",
      "ep 3955: ep_len:566 episode reward: total was 26.210000. running mean: 19.059877\n",
      "ep 3955: ep_len:124 episode reward: total was 31.880000. running mean: 19.188079\n",
      "ep 3955: ep_len:526 episode reward: total was 22.570000. running mean: 19.221898\n",
      "ep 3955: ep_len:590 episode reward: total was 58.250000. running mean: 19.612179\n",
      "epsilon:0.024617 episode_count: 27692. steps_count: 11974446.000000\n",
      "Time elapsed:  36420.90387177467\n",
      "ep 3956: ep_len:514 episode reward: total was 85.610000. running mean: 20.272157\n",
      "ep 3956: ep_len:587 episode reward: total was 45.950000. running mean: 20.528935\n",
      "ep 3956: ep_len:500 episode reward: total was 56.070000. running mean: 20.884346\n",
      "ep 3956: ep_len:500 episode reward: total was 51.930000. running mean: 21.194803\n",
      "ep 3956: ep_len:3 episode reward: total was 0.000000. running mean: 20.982855\n",
      "ep 3956: ep_len:581 episode reward: total was 11.380000. running mean: 20.886826\n",
      "ep 3956: ep_len:194 episode reward: total was 12.510000. running mean: 20.803058\n",
      "epsilon:0.024573 episode_count: 27699. steps_count: 11977325.000000\n",
      "Time elapsed:  36428.499027490616\n",
      "ep 3957: ep_len:546 episode reward: total was 79.160000. running mean: 21.386627\n",
      "ep 3957: ep_len:500 episode reward: total was 24.530000. running mean: 21.418061\n",
      "ep 3957: ep_len:518 episode reward: total was 44.490000. running mean: 21.648780\n",
      "ep 3957: ep_len:532 episode reward: total was 57.170000. running mean: 22.003992\n",
      "ep 3957: ep_len:127 episode reward: total was 38.370000. running mean: 22.167653\n",
      "ep 3957: ep_len:597 episode reward: total was 15.580000. running mean: 22.101776\n",
      "ep 3957: ep_len:538 episode reward: total was 44.990000. running mean: 22.330658\n",
      "epsilon:0.024529 episode_count: 27706. steps_count: 11980683.000000\n",
      "Time elapsed:  36443.64584517479\n",
      "ep 3958: ep_len:658 episode reward: total was 4.510000. running mean: 22.152452\n",
      "ep 3958: ep_len:521 episode reward: total was 16.810000. running mean: 22.099027\n",
      "ep 3958: ep_len:579 episode reward: total was -11.940000. running mean: 21.758637\n",
      "ep 3958: ep_len:534 episode reward: total was 75.930000. running mean: 22.300351\n",
      "ep 3958: ep_len:130 episode reward: total was 29.830000. running mean: 22.375647\n",
      "ep 3958: ep_len:517 episode reward: total was 24.940000. running mean: 22.401291\n",
      "ep 3958: ep_len:501 episode reward: total was 16.730000. running mean: 22.344578\n",
      "epsilon:0.024484 episode_count: 27713. steps_count: 11984123.000000\n",
      "Time elapsed:  36452.68099498749\n",
      "ep 3959: ep_len:663 episode reward: total was -7.070000. running mean: 22.050432\n",
      "ep 3959: ep_len:546 episode reward: total was 71.290000. running mean: 22.542828\n",
      "ep 3959: ep_len:536 episode reward: total was -5.850000. running mean: 22.258899\n",
      "ep 3959: ep_len:610 episode reward: total was 58.890000. running mean: 22.625210\n",
      "ep 3959: ep_len:102 episode reward: total was 14.710000. running mean: 22.546058\n",
      "ep 3959: ep_len:587 episode reward: total was 13.090000. running mean: 22.451498\n",
      "ep 3959: ep_len:595 episode reward: total was 21.210000. running mean: 22.439083\n",
      "epsilon:0.024440 episode_count: 27720. steps_count: 11987762.000000\n",
      "Time elapsed:  36462.163079977036\n",
      "ep 3960: ep_len:632 episode reward: total was 56.200000. running mean: 22.776692\n",
      "ep 3960: ep_len:546 episode reward: total was 76.100000. running mean: 23.309925\n",
      "ep 3960: ep_len:514 episode reward: total was 3.900000. running mean: 23.115826\n",
      "ep 3960: ep_len:170 episode reward: total was 7.740000. running mean: 22.962067\n",
      "ep 3960: ep_len:3 episode reward: total was 1.010000. running mean: 22.742547\n",
      "ep 3960: ep_len:515 episode reward: total was -14.780000. running mean: 22.367321\n",
      "ep 3960: ep_len:586 episode reward: total was 35.510000. running mean: 22.498748\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.024396 episode_count: 27727. steps_count: 11990728.000000\n",
      "Time elapsed:  36473.146406412125\n",
      "ep 3961: ep_len:126 episode reward: total was 6.980000. running mean: 22.343561\n",
      "ep 3961: ep_len:353 episode reward: total was 10.590000. running mean: 22.226025\n",
      "ep 3961: ep_len:561 episode reward: total was 50.140000. running mean: 22.505165\n",
      "ep 3961: ep_len:552 episode reward: total was 94.690000. running mean: 23.227013\n",
      "ep 3961: ep_len:3 episode reward: total was 1.010000. running mean: 23.004843\n",
      "ep 3961: ep_len:550 episode reward: total was -0.810000. running mean: 22.766694\n",
      "ep 3961: ep_len:546 episode reward: total was 56.780000. running mean: 23.106828\n",
      "epsilon:0.024351 episode_count: 27734. steps_count: 11993419.000000\n",
      "Time elapsed:  36479.557137966156\n",
      "ep 3962: ep_len:621 episode reward: total was 79.230000. running mean: 23.668059\n",
      "ep 3962: ep_len:518 episode reward: total was -32.580000. running mean: 23.105579\n",
      "ep 3962: ep_len:557 episode reward: total was -7.400000. running mean: 22.800523\n",
      "ep 3962: ep_len:500 episode reward: total was 42.570000. running mean: 22.998218\n",
      "ep 3962: ep_len:43 episode reward: total was -34.490000. running mean: 22.423335\n",
      "ep 3962: ep_len:233 episode reward: total was 32.530000. running mean: 22.524402\n",
      "ep 3962: ep_len:590 episode reward: total was 45.880000. running mean: 22.757958\n",
      "epsilon:0.024307 episode_count: 27741. steps_count: 11996481.000000\n",
      "Time elapsed:  36486.504637002945\n",
      "ep 3963: ep_len:656 episode reward: total was -1.110000. running mean: 22.519279\n",
      "ep 3963: ep_len:367 episode reward: total was 34.730000. running mean: 22.641386\n",
      "ep 3963: ep_len:500 episode reward: total was 25.650000. running mean: 22.671472\n",
      "ep 3963: ep_len:409 episode reward: total was 28.230000. running mean: 22.727057\n",
      "ep 3963: ep_len:3 episode reward: total was 1.010000. running mean: 22.509887\n",
      "ep 3963: ep_len:502 episode reward: total was -5.550000. running mean: 22.229288\n",
      "ep 3963: ep_len:530 episode reward: total was 42.040000. running mean: 22.427395\n",
      "epsilon:0.024263 episode_count: 27748. steps_count: 11999448.000000\n",
      "Time elapsed:  36497.40165686607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3964: ep_len:535 episode reward: total was 92.700000. running mean: 23.130121\n",
      "ep 3964: ep_len:195 episode reward: total was 16.410000. running mean: 23.062920\n",
      "ep 3964: ep_len:522 episode reward: total was -8.640000. running mean: 22.745890\n",
      "ep 3964: ep_len:506 episode reward: total was 75.200000. running mean: 23.270432\n",
      "ep 3964: ep_len:3 episode reward: total was 1.010000. running mean: 23.047827\n",
      "ep 3964: ep_len:580 episode reward: total was 12.840000. running mean: 22.945749\n",
      "ep 3964: ep_len:501 episode reward: total was 39.420000. running mean: 23.110492\n",
      "epsilon:0.024218 episode_count: 27755. steps_count: 12002290.000000\n",
      "Time elapsed:  36504.15595078468\n",
      "ep 3965: ep_len:563 episode reward: total was 41.990000. running mean: 23.299287\n",
      "ep 3965: ep_len:500 episode reward: total was 99.940000. running mean: 24.065694\n",
      "ep 3965: ep_len:500 episode reward: total was 7.500000. running mean: 23.900037\n",
      "ep 3965: ep_len:500 episode reward: total was 45.160000. running mean: 24.112636\n",
      "ep 3965: ep_len:51 episode reward: total was 19.500000. running mean: 24.066510\n",
      "ep 3965: ep_len:160 episode reward: total was -22.340000. running mean: 23.602445\n",
      "ep 3965: ep_len:500 episode reward: total was 60.190000. running mean: 23.968321\n",
      "epsilon:0.024174 episode_count: 27762. steps_count: 12005064.000000\n",
      "Time elapsed:  36511.68670916557\n",
      "ep 3966: ep_len:505 episode reward: total was 3.900000. running mean: 23.767637\n",
      "ep 3966: ep_len:655 episode reward: total was 81.590000. running mean: 24.345861\n",
      "ep 3966: ep_len:636 episode reward: total was 24.470000. running mean: 24.347102\n",
      "ep 3966: ep_len:419 episode reward: total was 30.730000. running mean: 24.410931\n",
      "ep 3966: ep_len:54 episode reward: total was 23.510000. running mean: 24.401922\n",
      "ep 3966: ep_len:500 episode reward: total was 25.960000. running mean: 24.417503\n",
      "ep 3966: ep_len:619 episode reward: total was 20.870000. running mean: 24.382028\n",
      "epsilon:0.024130 episode_count: 27769. steps_count: 12008452.000000\n",
      "Time elapsed:  36519.12287688255\n",
      "ep 3967: ep_len:500 episode reward: total was 99.180000. running mean: 25.130007\n",
      "ep 3967: ep_len:532 episode reward: total was 121.500000. running mean: 26.093707\n",
      "ep 3967: ep_len:390 episode reward: total was 69.130000. running mean: 26.524070\n",
      "ep 3967: ep_len:501 episode reward: total was 26.030000. running mean: 26.519130\n",
      "ep 3967: ep_len:3 episode reward: total was 1.010000. running mean: 26.264038\n",
      "ep 3967: ep_len:549 episode reward: total was 8.300000. running mean: 26.084398\n",
      "ep 3967: ep_len:500 episode reward: total was 64.560000. running mean: 26.469154\n",
      "epsilon:0.024085 episode_count: 27776. steps_count: 12011427.000000\n",
      "Time elapsed:  36525.741599321365\n",
      "ep 3968: ep_len:587 episode reward: total was 32.320000. running mean: 26.527662\n",
      "ep 3968: ep_len:500 episode reward: total was 86.500000. running mean: 27.127386\n",
      "ep 3968: ep_len:500 episode reward: total was 11.320000. running mean: 26.969312\n",
      "ep 3968: ep_len:500 episode reward: total was 6.290000. running mean: 26.762519\n",
      "ep 3968: ep_len:1 episode reward: total was -1.000000. running mean: 26.484894\n",
      "ep 3968: ep_len:505 episode reward: total was 5.800000. running mean: 26.278045\n",
      "ep 3968: ep_len:579 episode reward: total was 37.100000. running mean: 26.386264\n",
      "epsilon:0.024041 episode_count: 27783. steps_count: 12014599.000000\n",
      "Time elapsed:  36534.10274100304\n",
      "ep 3969: ep_len:509 episode reward: total was 80.790000. running mean: 26.930302\n",
      "ep 3969: ep_len:527 episode reward: total was 128.580000. running mean: 27.946799\n",
      "ep 3969: ep_len:566 episode reward: total was -9.520000. running mean: 27.572131\n",
      "ep 3969: ep_len:500 episode reward: total was 24.410000. running mean: 27.540509\n",
      "ep 3969: ep_len:3 episode reward: total was 1.010000. running mean: 27.275204\n",
      "ep 3969: ep_len:683 episode reward: total was -37.080000. running mean: 26.631652\n",
      "ep 3969: ep_len:591 episode reward: total was 40.920000. running mean: 26.774536\n",
      "epsilon:0.023997 episode_count: 27790. steps_count: 12017978.000000\n",
      "Time elapsed:  36542.98978257179\n",
      "ep 3970: ep_len:615 episode reward: total was 32.850000. running mean: 26.835290\n",
      "ep 3970: ep_len:506 episode reward: total was -330.440000. running mean: 23.262537\n",
      "ep 3970: ep_len:569 episode reward: total was -2.550000. running mean: 23.004412\n",
      "ep 3970: ep_len:500 episode reward: total was -80.990000. running mean: 21.964468\n",
      "ep 3970: ep_len:134 episode reward: total was -55.610000. running mean: 21.188723\n",
      "ep 3970: ep_len:166 episode reward: total was 35.670000. running mean: 21.333536\n",
      "ep 3970: ep_len:500 episode reward: total was 42.930000. running mean: 21.549501\n",
      "epsilon:0.023952 episode_count: 27797. steps_count: 12020968.000000\n",
      "Time elapsed:  36552.93219709396\n",
      "ep 3971: ep_len:551 episode reward: total was 42.200000. running mean: 21.756006\n",
      "ep 3971: ep_len:185 episode reward: total was 23.380000. running mean: 21.772246\n",
      "ep 3971: ep_len:507 episode reward: total was 0.310000. running mean: 21.557623\n",
      "ep 3971: ep_len:516 episode reward: total was -12.610000. running mean: 21.215947\n",
      "ep 3971: ep_len:126 episode reward: total was 24.870000. running mean: 21.252487\n",
      "ep 3971: ep_len:596 episode reward: total was 15.080000. running mean: 21.190763\n",
      "ep 3971: ep_len:500 episode reward: total was 7.230000. running mean: 21.051155\n",
      "epsilon:0.023908 episode_count: 27804. steps_count: 12023949.000000\n",
      "Time elapsed:  36560.009890556335\n",
      "ep 3972: ep_len:544 episode reward: total was 38.720000. running mean: 21.227843\n",
      "ep 3972: ep_len:572 episode reward: total was 112.870000. running mean: 22.144265\n",
      "ep 3972: ep_len:583 episode reward: total was 35.310000. running mean: 22.275922\n",
      "ep 3972: ep_len:505 episode reward: total was -22.770000. running mean: 21.825463\n",
      "ep 3972: ep_len:86 episode reward: total was -54.200000. running mean: 21.065208\n",
      "ep 3972: ep_len:566 episode reward: total was 44.470000. running mean: 21.299256\n",
      "ep 3972: ep_len:348 episode reward: total was 23.820000. running mean: 21.324464\n",
      "epsilon:0.023864 episode_count: 27811. steps_count: 12027153.000000\n",
      "Time elapsed:  36568.62876367569\n",
      "ep 3973: ep_len:524 episode reward: total was 54.020000. running mean: 21.651419\n",
      "ep 3973: ep_len:335 episode reward: total was 21.830000. running mean: 21.653205\n",
      "ep 3973: ep_len:342 episode reward: total was -22.700000. running mean: 21.209673\n",
      "ep 3973: ep_len:409 episode reward: total was 19.550000. running mean: 21.193076\n",
      "ep 3973: ep_len:3 episode reward: total was 1.010000. running mean: 20.991245\n",
      "ep 3973: ep_len:500 episode reward: total was 36.380000. running mean: 21.145133\n",
      "ep 3973: ep_len:282 episode reward: total was 8.940000. running mean: 21.023082\n",
      "epsilon:0.023819 episode_count: 27818. steps_count: 12029548.000000\n",
      "Time elapsed:  36583.616374731064\n",
      "ep 3974: ep_len:730 episode reward: total was -472.120000. running mean: 16.091651\n",
      "ep 3974: ep_len:528 episode reward: total was 16.330000. running mean: 16.094034\n",
      "ep 3974: ep_len:613 episode reward: total was 19.760000. running mean: 16.130694\n",
      "ep 3974: ep_len:517 episode reward: total was 48.620000. running mean: 16.455587\n",
      "ep 3974: ep_len:3 episode reward: total was 1.010000. running mean: 16.301131\n",
      "ep 3974: ep_len:325 episode reward: total was 19.670000. running mean: 16.334820\n",
      "ep 3974: ep_len:203 episode reward: total was 7.730000. running mean: 16.248772\n",
      "epsilon:0.023775 episode_count: 27825. steps_count: 12032467.000000\n",
      "Time elapsed:  36606.61718583107\n",
      "ep 3975: ep_len:131 episode reward: total was 12.080000. running mean: 16.207084\n",
      "ep 3975: ep_len:530 episode reward: total was 105.410000. running mean: 17.099113\n",
      "ep 3975: ep_len:535 episode reward: total was -2.670000. running mean: 16.901422\n",
      "ep 3975: ep_len:508 episode reward: total was 28.530000. running mean: 17.017708\n",
      "ep 3975: ep_len:3 episode reward: total was 1.010000. running mean: 16.857631\n",
      "ep 3975: ep_len:622 episode reward: total was -12.240000. running mean: 16.566654\n",
      "ep 3975: ep_len:595 episode reward: total was -61.240000. running mean: 15.788588\n",
      "epsilon:0.023731 episode_count: 27832. steps_count: 12035391.000000\n",
      "Time elapsed:  36614.4192738533\n",
      "ep 3976: ep_len:500 episode reward: total was 76.600000. running mean: 16.396702\n",
      "ep 3976: ep_len:500 episode reward: total was 34.450000. running mean: 16.577235\n",
      "ep 3976: ep_len:655 episode reward: total was 35.430000. running mean: 16.765763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3976: ep_len:500 episode reward: total was -10.530000. running mean: 16.492805\n",
      "ep 3976: ep_len:3 episode reward: total was 1.010000. running mean: 16.337977\n",
      "ep 3976: ep_len:580 episode reward: total was 31.300000. running mean: 16.487597\n",
      "ep 3976: ep_len:604 episode reward: total was 37.800000. running mean: 16.700721\n",
      "epsilon:0.023686 episode_count: 27839. steps_count: 12038733.000000\n",
      "Time elapsed:  36623.30472493172\n",
      "ep 3977: ep_len:560 episode reward: total was 51.390000. running mean: 17.047614\n",
      "ep 3977: ep_len:602 episode reward: total was 100.970000. running mean: 17.886838\n",
      "ep 3977: ep_len:641 episode reward: total was 11.270000. running mean: 17.820669\n",
      "ep 3977: ep_len:381 episode reward: total was 17.280000. running mean: 17.815263\n",
      "ep 3977: ep_len:80 episode reward: total was 28.160000. running mean: 17.918710\n",
      "ep 3977: ep_len:623 episode reward: total was 40.920000. running mean: 18.148723\n",
      "ep 3977: ep_len:567 episode reward: total was 16.780000. running mean: 18.135036\n",
      "epsilon:0.023642 episode_count: 27846. steps_count: 12042187.000000\n",
      "Time elapsed:  36632.19339513779\n",
      "ep 3978: ep_len:132 episode reward: total was 12.000000. running mean: 18.073685\n",
      "ep 3978: ep_len:500 episode reward: total was 76.080000. running mean: 18.653749\n",
      "ep 3978: ep_len:628 episode reward: total was 24.760000. running mean: 18.714811\n",
      "ep 3978: ep_len:125 episode reward: total was 21.120000. running mean: 18.738863\n",
      "ep 3978: ep_len:3 episode reward: total was 1.010000. running mean: 18.561574\n",
      "ep 3978: ep_len:532 episode reward: total was -6.860000. running mean: 18.307359\n",
      "ep 3978: ep_len:590 episode reward: total was 78.820000. running mean: 18.912485\n",
      "epsilon:0.023598 episode_count: 27853. steps_count: 12044697.000000\n",
      "Time elapsed:  36646.72322964668\n",
      "ep 3979: ep_len:562 episode reward: total was 48.100000. running mean: 19.204360\n",
      "ep 3979: ep_len:363 episode reward: total was -10.910000. running mean: 18.903217\n",
      "ep 3979: ep_len:501 episode reward: total was 19.090000. running mean: 18.905084\n",
      "ep 3979: ep_len:527 episode reward: total was 37.300000. running mean: 19.089034\n",
      "ep 3979: ep_len:3 episode reward: total was 1.010000. running mean: 18.908243\n",
      "ep 3979: ep_len:619 episode reward: total was -0.190000. running mean: 18.717261\n",
      "ep 3979: ep_len:528 episode reward: total was 30.310000. running mean: 18.833188\n",
      "epsilon:0.023553 episode_count: 27860. steps_count: 12047800.000000\n",
      "Time elapsed:  36662.18749713898\n",
      "ep 3980: ep_len:585 episode reward: total was 42.700000. running mean: 19.071856\n",
      "ep 3980: ep_len:500 episode reward: total was 58.280000. running mean: 19.463938\n",
      "ep 3980: ep_len:500 episode reward: total was 40.290000. running mean: 19.672198\n",
      "ep 3980: ep_len:500 episode reward: total was 30.030000. running mean: 19.775776\n",
      "ep 3980: ep_len:86 episode reward: total was 23.260000. running mean: 19.810619\n",
      "ep 3980: ep_len:624 episode reward: total was 40.000000. running mean: 20.012512\n",
      "ep 3980: ep_len:511 episode reward: total was 52.500000. running mean: 20.337387\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.023509 episode_count: 27867. steps_count: 12051106.000000\n",
      "Time elapsed:  36670.09545302391\n",
      "ep 3981: ep_len:130 episode reward: total was -9.050000. running mean: 20.043513\n",
      "ep 3981: ep_len:628 episode reward: total was 83.660000. running mean: 20.679678\n",
      "ep 3981: ep_len:74 episode reward: total was 10.770000. running mean: 20.580581\n",
      "ep 3981: ep_len:582 episode reward: total was 75.580000. running mean: 21.130576\n",
      "ep 3981: ep_len:3 episode reward: total was 1.010000. running mean: 20.929370\n",
      "ep 3981: ep_len:500 episode reward: total was 31.650000. running mean: 21.036576\n",
      "ep 3981: ep_len:551 episode reward: total was 50.280000. running mean: 21.329010\n",
      "epsilon:0.023465 episode_count: 27874. steps_count: 12053574.000000\n",
      "Time elapsed:  36674.03775334358\n",
      "ep 3982: ep_len:552 episode reward: total was 59.690000. running mean: 21.712620\n",
      "ep 3982: ep_len:599 episode reward: total was 141.370000. running mean: 22.909194\n",
      "ep 3982: ep_len:628 episode reward: total was -14.210000. running mean: 22.538002\n",
      "ep 3982: ep_len:576 episode reward: total was 32.980000. running mean: 22.642422\n",
      "ep 3982: ep_len:98 episode reward: total was 13.770000. running mean: 22.553698\n",
      "ep 3982: ep_len:576 episode reward: total was 61.130000. running mean: 22.939461\n",
      "ep 3982: ep_len:622 episode reward: total was 32.870000. running mean: 23.038766\n",
      "epsilon:0.023420 episode_count: 27881. steps_count: 12057225.000000\n",
      "Time elapsed:  36683.023617744446\n",
      "ep 3983: ep_len:265 episode reward: total was 22.400000. running mean: 23.032379\n",
      "ep 3983: ep_len:613 episode reward: total was 62.820000. running mean: 23.430255\n",
      "ep 3983: ep_len:545 episode reward: total was 34.640000. running mean: 23.542352\n",
      "ep 3983: ep_len:500 episode reward: total was 49.430000. running mean: 23.801229\n",
      "ep 3983: ep_len:3 episode reward: total was 1.010000. running mean: 23.573317\n",
      "ep 3983: ep_len:583 episode reward: total was -253.820000. running mean: 20.799383\n",
      "ep 3983: ep_len:500 episode reward: total was 78.920000. running mean: 21.380590\n",
      "epsilon:0.023376 episode_count: 27888. steps_count: 12060234.000000\n",
      "Time elapsed:  36691.091711997986\n",
      "ep 3984: ep_len:657 episode reward: total was -2.040000. running mean: 21.146384\n",
      "ep 3984: ep_len:528 episode reward: total was 29.350000. running mean: 21.228420\n",
      "ep 3984: ep_len:623 episode reward: total was -3.370000. running mean: 20.982436\n",
      "ep 3984: ep_len:169 episode reward: total was 17.230000. running mean: 20.944911\n",
      "ep 3984: ep_len:3 episode reward: total was 1.010000. running mean: 20.745562\n",
      "ep 3984: ep_len:236 episode reward: total was 24.450000. running mean: 20.782607\n",
      "ep 3984: ep_len:500 episode reward: total was 10.900000. running mean: 20.683780\n",
      "epsilon:0.023332 episode_count: 27895. steps_count: 12062950.000000\n",
      "Time elapsed:  36698.38290143013\n",
      "ep 3985: ep_len:501 episode reward: total was -11.130000. running mean: 20.365643\n",
      "ep 3985: ep_len:526 episode reward: total was 38.830000. running mean: 20.550286\n",
      "ep 3985: ep_len:649 episode reward: total was 22.000000. running mean: 20.564783\n",
      "ep 3985: ep_len:624 episode reward: total was 55.140000. running mean: 20.910536\n",
      "ep 3985: ep_len:3 episode reward: total was 1.010000. running mean: 20.711530\n",
      "ep 3985: ep_len:500 episode reward: total was 25.040000. running mean: 20.754815\n",
      "ep 3985: ep_len:590 episode reward: total was 38.080000. running mean: 20.928067\n",
      "epsilon:0.023287 episode_count: 27902. steps_count: 12066343.000000\n",
      "Time elapsed:  36713.473997831345\n",
      "ep 3986: ep_len:750 episode reward: total was -15.420000. running mean: 20.564586\n",
      "ep 3986: ep_len:500 episode reward: total was 5.360000. running mean: 20.412540\n",
      "ep 3986: ep_len:553 episode reward: total was -6.780000. running mean: 20.140615\n",
      "ep 3986: ep_len:442 episode reward: total was 6.040000. running mean: 19.999609\n",
      "ep 3986: ep_len:90 episode reward: total was 26.760000. running mean: 20.067213\n",
      "ep 3986: ep_len:500 episode reward: total was 13.650000. running mean: 20.003040\n",
      "ep 3986: ep_len:523 episode reward: total was 36.610000. running mean: 20.169110\n",
      "epsilon:0.023243 episode_count: 27909. steps_count: 12069701.000000\n",
      "Time elapsed:  36722.19876742363\n",
      "ep 3987: ep_len:206 episode reward: total was 20.260000. running mean: 20.170019\n",
      "ep 3987: ep_len:500 episode reward: total was 15.360000. running mean: 20.121919\n",
      "ep 3987: ep_len:500 episode reward: total was 42.220000. running mean: 20.342900\n",
      "ep 3987: ep_len:543 episode reward: total was 39.620000. running mean: 20.535671\n",
      "ep 3987: ep_len:3 episode reward: total was 1.010000. running mean: 20.340414\n",
      "ep 3987: ep_len:570 episode reward: total was 55.230000. running mean: 20.689310\n",
      "ep 3987: ep_len:500 episode reward: total was -16.850000. running mean: 20.313917\n",
      "epsilon:0.023199 episode_count: 27916. steps_count: 12072523.000000\n",
      "Time elapsed:  36729.706839323044\n",
      "ep 3988: ep_len:618 episode reward: total was 75.270000. running mean: 20.863477\n",
      "ep 3988: ep_len:547 episode reward: total was 57.520000. running mean: 21.230043\n",
      "ep 3988: ep_len:550 episode reward: total was -13.790000. running mean: 20.879842\n",
      "ep 3988: ep_len:132 episode reward: total was 17.640000. running mean: 20.847444\n",
      "ep 3988: ep_len:3 episode reward: total was 1.010000. running mean: 20.649069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3988: ep_len:527 episode reward: total was 18.970000. running mean: 20.632279\n",
      "ep 3988: ep_len:540 episode reward: total was -101.140000. running mean: 19.414556\n",
      "epsilon:0.023154 episode_count: 27923. steps_count: 12075440.000000\n",
      "Time elapsed:  36737.49605464935\n",
      "ep 3989: ep_len:579 episode reward: total was -34.410000. running mean: 18.876310\n",
      "ep 3989: ep_len:524 episode reward: total was -6.120000. running mean: 18.626347\n",
      "ep 3989: ep_len:500 episode reward: total was 10.410000. running mean: 18.544184\n",
      "ep 3989: ep_len:500 episode reward: total was 60.420000. running mean: 18.962942\n",
      "ep 3989: ep_len:98 episode reward: total was 33.270000. running mean: 19.106013\n",
      "ep 3989: ep_len:500 episode reward: total was 46.780000. running mean: 19.382752\n",
      "ep 3989: ep_len:353 episode reward: total was 16.590000. running mean: 19.354825\n",
      "epsilon:0.023110 episode_count: 27930. steps_count: 12078494.000000\n",
      "Time elapsed:  36746.893495082855\n",
      "ep 3990: ep_len:500 episode reward: total was 18.020000. running mean: 19.341477\n",
      "ep 3990: ep_len:500 episode reward: total was 74.450000. running mean: 19.892562\n",
      "ep 3990: ep_len:63 episode reward: total was 17.730000. running mean: 19.870936\n",
      "ep 3990: ep_len:605 episode reward: total was 63.520000. running mean: 20.307427\n",
      "ep 3990: ep_len:129 episode reward: total was 30.370000. running mean: 20.408053\n",
      "ep 3990: ep_len:620 episode reward: total was 65.030000. running mean: 20.854272\n",
      "ep 3990: ep_len:173 episode reward: total was 19.920000. running mean: 20.844929\n",
      "epsilon:0.023066 episode_count: 27937. steps_count: 12081084.000000\n",
      "Time elapsed:  36752.655423402786\n",
      "ep 3991: ep_len:527 episode reward: total was 51.250000. running mean: 21.148980\n",
      "ep 3991: ep_len:501 episode reward: total was 22.310000. running mean: 21.160590\n",
      "ep 3991: ep_len:500 episode reward: total was 3.730000. running mean: 20.986284\n",
      "ep 3991: ep_len:570 episode reward: total was 76.610000. running mean: 21.542522\n",
      "ep 3991: ep_len:3 episode reward: total was 1.010000. running mean: 21.337196\n",
      "ep 3991: ep_len:514 episode reward: total was 15.910000. running mean: 21.282924\n",
      "ep 3991: ep_len:623 episode reward: total was 76.030000. running mean: 21.830395\n",
      "epsilon:0.023021 episode_count: 27944. steps_count: 12084322.000000\n",
      "Time elapsed:  36757.584740161896\n",
      "ep 3992: ep_len:574 episode reward: total was 51.930000. running mean: 22.131391\n",
      "ep 3992: ep_len:739 episode reward: total was -118.410000. running mean: 20.725977\n",
      "ep 3992: ep_len:500 episode reward: total was 21.870000. running mean: 20.737417\n",
      "ep 3992: ep_len:511 episode reward: total was 42.700000. running mean: 20.957043\n",
      "ep 3992: ep_len:3 episode reward: total was 1.010000. running mean: 20.757573\n",
      "ep 3992: ep_len:289 episode reward: total was 23.070000. running mean: 20.780697\n",
      "ep 3992: ep_len:634 episode reward: total was 36.110000. running mean: 20.933990\n",
      "epsilon:0.022977 episode_count: 27951. steps_count: 12087572.000000\n",
      "Time elapsed:  36769.96584510803\n",
      "ep 3993: ep_len:106 episode reward: total was 10.420000. running mean: 20.828850\n",
      "ep 3993: ep_len:500 episode reward: total was 87.800000. running mean: 21.498562\n",
      "ep 3993: ep_len:453 episode reward: total was 78.320000. running mean: 22.066776\n",
      "ep 3993: ep_len:143 episode reward: total was 5.970000. running mean: 21.905808\n",
      "ep 3993: ep_len:3 episode reward: total was 1.010000. running mean: 21.696850\n",
      "ep 3993: ep_len:226 episode reward: total was -24.930000. running mean: 21.230582\n",
      "ep 3993: ep_len:608 episode reward: total was 38.350000. running mean: 21.401776\n",
      "epsilon:0.022933 episode_count: 27958. steps_count: 12089611.000000\n",
      "Time elapsed:  36775.65529346466\n",
      "ep 3994: ep_len:572 episode reward: total was -1.210000. running mean: 21.175658\n",
      "ep 3994: ep_len:551 episode reward: total was 40.040000. running mean: 21.364302\n",
      "ep 3994: ep_len:629 episode reward: total was 18.310000. running mean: 21.333759\n",
      "ep 3994: ep_len:522 episode reward: total was -24.830000. running mean: 20.872121\n",
      "ep 3994: ep_len:3 episode reward: total was 1.010000. running mean: 20.673500\n",
      "ep 3994: ep_len:683 episode reward: total was -49.170000. running mean: 19.975065\n",
      "ep 3994: ep_len:581 episode reward: total was 9.320000. running mean: 19.868514\n",
      "epsilon:0.022888 episode_count: 27965. steps_count: 12093152.000000\n",
      "Time elapsed:  36784.91348028183\n",
      "ep 3995: ep_len:638 episode reward: total was -46.400000. running mean: 19.205829\n",
      "ep 3995: ep_len:297 episode reward: total was 10.570000. running mean: 19.119471\n",
      "ep 3995: ep_len:383 episode reward: total was 50.390000. running mean: 19.432176\n",
      "ep 3995: ep_len:43 episode reward: total was -9.180000. running mean: 19.146054\n",
      "ep 3995: ep_len:1 episode reward: total was -1.000000. running mean: 18.944594\n",
      "ep 3995: ep_len:621 episode reward: total was -20.350000. running mean: 18.551648\n",
      "ep 3995: ep_len:297 episode reward: total was 11.830000. running mean: 18.484431\n",
      "epsilon:0.022844 episode_count: 27972. steps_count: 12095432.000000\n",
      "Time elapsed:  36798.97690868378\n",
      "ep 3996: ep_len:541 episode reward: total was 14.580000. running mean: 18.445387\n",
      "ep 3996: ep_len:535 episode reward: total was 72.060000. running mean: 18.981533\n",
      "ep 3996: ep_len:412 episode reward: total was 63.270000. running mean: 19.424418\n",
      "ep 3996: ep_len:406 episode reward: total was 34.330000. running mean: 19.573474\n",
      "ep 3996: ep_len:93 episode reward: total was 25.230000. running mean: 19.630039\n",
      "ep 3996: ep_len:500 episode reward: total was 34.250000. running mean: 19.776238\n",
      "ep 3996: ep_len:622 episode reward: total was 29.780000. running mean: 19.876276\n",
      "epsilon:0.022800 episode_count: 27979. steps_count: 12098541.000000\n",
      "Time elapsed:  36812.28200173378\n",
      "ep 3997: ep_len:648 episode reward: total was -7.680000. running mean: 19.600713\n",
      "ep 3997: ep_len:500 episode reward: total was 43.330000. running mean: 19.838006\n",
      "ep 3997: ep_len:588 episode reward: total was 46.890000. running mean: 20.108526\n",
      "ep 3997: ep_len:568 episode reward: total was 93.660000. running mean: 20.844041\n",
      "ep 3997: ep_len:3 episode reward: total was 1.010000. running mean: 20.645700\n",
      "ep 3997: ep_len:537 episode reward: total was 53.550000. running mean: 20.974743\n",
      "ep 3997: ep_len:587 episode reward: total was 17.260000. running mean: 20.937596\n",
      "epsilon:0.022755 episode_count: 27986. steps_count: 12101972.000000\n",
      "Time elapsed:  36821.414955616\n",
      "ep 3998: ep_len:521 episode reward: total was 33.690000. running mean: 21.065120\n",
      "ep 3998: ep_len:272 episode reward: total was 21.670000. running mean: 21.071169\n",
      "ep 3998: ep_len:614 episode reward: total was 29.020000. running mean: 21.150657\n",
      "ep 3998: ep_len:504 episode reward: total was 24.030000. running mean: 21.179451\n",
      "ep 3998: ep_len:3 episode reward: total was 1.010000. running mean: 20.977756\n",
      "ep 3998: ep_len:638 episode reward: total was -61.280000. running mean: 20.155179\n",
      "ep 3998: ep_len:523 episode reward: total was -4.570000. running mean: 19.907927\n",
      "epsilon:0.022711 episode_count: 27993. steps_count: 12105047.000000\n",
      "Time elapsed:  36829.82204890251\n",
      "ep 3999: ep_len:513 episode reward: total was 58.550000. running mean: 20.294348\n",
      "ep 3999: ep_len:547 episode reward: total was 62.390000. running mean: 20.715304\n",
      "ep 3999: ep_len:609 episode reward: total was 33.000000. running mean: 20.838151\n",
      "ep 3999: ep_len:501 episode reward: total was 97.870000. running mean: 21.608469\n",
      "ep 3999: ep_len:3 episode reward: total was 1.010000. running mean: 21.402485\n",
      "ep 3999: ep_len:500 episode reward: total was 31.000000. running mean: 21.498460\n",
      "ep 3999: ep_len:597 episode reward: total was 20.430000. running mean: 21.487775\n",
      "epsilon:0.022667 episode_count: 28000. steps_count: 12108317.000000\n",
      "Time elapsed:  36838.705741643906\n",
      "ep 4000: ep_len:247 episode reward: total was 18.240000. running mean: 21.455298\n",
      "ep 4000: ep_len:563 episode reward: total was 65.100000. running mean: 21.891745\n",
      "ep 4000: ep_len:613 episode reward: total was 47.830000. running mean: 22.151127\n",
      "ep 4000: ep_len:531 episode reward: total was 73.850000. running mean: 22.668116\n",
      "ep 4000: ep_len:3 episode reward: total was 1.010000. running mean: 22.451535\n",
      "ep 4000: ep_len:186 episode reward: total was 37.710000. running mean: 22.604119\n",
      "ep 4000: ep_len:552 episode reward: total was 67.400000. running mean: 23.052078\n",
      "Initial position:  [  2   0  17  87  63 149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:0.022622 episode_count: 28007. steps_count: 12111012.000000\n",
      "Time elapsed:  36851.44791531563\n",
      "ep 4001: ep_len:533 episode reward: total was -47.370000. running mean: 22.347857\n",
      "ep 4001: ep_len:277 episode reward: total was 22.580000. running mean: 22.350179\n",
      "ep 4001: ep_len:697 episode reward: total was 15.720000. running mean: 22.283877\n",
      "ep 4001: ep_len:503 episode reward: total was 36.090000. running mean: 22.421938\n",
      "ep 4001: ep_len:3 episode reward: total was 1.010000. running mean: 22.207819\n",
      "ep 4001: ep_len:613 episode reward: total was 81.760000. running mean: 22.803341\n",
      "ep 4001: ep_len:563 episode reward: total was 13.410000. running mean: 22.709407\n",
      "epsilon:0.022578 episode_count: 28014. steps_count: 12114201.000000\n",
      "Time elapsed:  36859.812067747116\n",
      "ep 4002: ep_len:253 episode reward: total was 31.370000. running mean: 22.796013\n",
      "ep 4002: ep_len:560 episode reward: total was 33.530000. running mean: 22.903353\n",
      "ep 4002: ep_len:500 episode reward: total was 30.740000. running mean: 22.981720\n",
      "ep 4002: ep_len:500 episode reward: total was 27.090000. running mean: 23.022802\n",
      "ep 4002: ep_len:3 episode reward: total was 1.010000. running mean: 22.802674\n",
      "ep 4002: ep_len:228 episode reward: total was 56.450000. running mean: 23.139148\n",
      "ep 4002: ep_len:500 episode reward: total was 31.570000. running mean: 23.223456\n",
      "epsilon:0.022534 episode_count: 28021. steps_count: 12116745.000000\n",
      "Time elapsed:  36873.10854244232\n",
      "ep 4003: ep_len:500 episode reward: total was 78.220000. running mean: 23.773422\n",
      "ep 4003: ep_len:524 episode reward: total was 19.540000. running mean: 23.731087\n",
      "ep 4003: ep_len:596 episode reward: total was 23.660000. running mean: 23.730376\n",
      "ep 4003: ep_len:524 episode reward: total was 97.100000. running mean: 24.464073\n",
      "ep 4003: ep_len:96 episode reward: total was 31.720000. running mean: 24.536632\n",
      "ep 4003: ep_len:632 episode reward: total was 39.740000. running mean: 24.688666\n",
      "ep 4003: ep_len:570 episode reward: total was 7.840000. running mean: 24.520179\n",
      "epsilon:0.022489 episode_count: 28028. steps_count: 12120187.000000\n",
      "Time elapsed:  36882.02518463135\n",
      "ep 4004: ep_len:629 episode reward: total was -129.100000. running mean: 22.983977\n",
      "ep 4004: ep_len:500 episode reward: total was 124.520000. running mean: 23.999337\n",
      "ep 4004: ep_len:500 episode reward: total was 29.400000. running mean: 24.053344\n",
      "ep 4004: ep_len:581 episode reward: total was 70.780000. running mean: 24.520611\n",
      "ep 4004: ep_len:3 episode reward: total was 1.010000. running mean: 24.285505\n",
      "ep 4004: ep_len:610 episode reward: total was 39.460000. running mean: 24.437249\n",
      "ep 4004: ep_len:589 episode reward: total was 63.430000. running mean: 24.827177\n",
      "epsilon:0.022445 episode_count: 28035. steps_count: 12123599.000000\n",
      "Time elapsed:  36902.453357219696\n",
      "ep 4005: ep_len:247 episode reward: total was 17.770000. running mean: 24.756605\n",
      "ep 4005: ep_len:363 episode reward: total was 29.300000. running mean: 24.802039\n",
      "ep 4005: ep_len:500 episode reward: total was 37.680000. running mean: 24.930819\n",
      "ep 4005: ep_len:500 episode reward: total was 48.980000. running mean: 25.171311\n",
      "ep 4005: ep_len:3 episode reward: total was 1.010000. running mean: 24.929697\n",
      "ep 4005: ep_len:637 episode reward: total was 37.490000. running mean: 25.055300\n",
      "ep 4005: ep_len:269 episode reward: total was -5.820000. running mean: 24.746547\n",
      "epsilon:0.022401 episode_count: 28042. steps_count: 12126118.000000\n",
      "Time elapsed:  36906.50753760338\n",
      "ep 4006: ep_len:544 episode reward: total was 82.850000. running mean: 25.327582\n",
      "ep 4006: ep_len:537 episode reward: total was 44.500000. running mean: 25.519306\n",
      "ep 4006: ep_len:571 episode reward: total was 27.140000. running mean: 25.535513\n",
      "ep 4006: ep_len:500 episode reward: total was 82.230000. running mean: 26.102458\n",
      "ep 4006: ep_len:3 episode reward: total was 1.010000. running mean: 25.851533\n",
      "ep 4006: ep_len:562 episode reward: total was 1.140000. running mean: 25.604418\n",
      "ep 4006: ep_len:603 episode reward: total was 23.950000. running mean: 25.587874\n",
      "epsilon:0.022356 episode_count: 28049. steps_count: 12129438.000000\n",
      "Time elapsed:  36920.59461116791\n",
      "ep 4007: ep_len:500 episode reward: total was 69.040000. running mean: 26.022395\n",
      "ep 4007: ep_len:370 episode reward: total was 29.620000. running mean: 26.058371\n",
      "ep 4007: ep_len:543 episode reward: total was 3.740000. running mean: 25.835188\n",
      "ep 4007: ep_len:518 episode reward: total was 3.340000. running mean: 25.610236\n",
      "ep 4007: ep_len:125 episode reward: total was 6.390000. running mean: 25.418033\n",
      "ep 4007: ep_len:322 episode reward: total was 28.210000. running mean: 25.445953\n",
      "ep 4007: ep_len:526 episode reward: total was -10.140000. running mean: 25.090093\n",
      "epsilon:0.022312 episode_count: 28056. steps_count: 12132342.000000\n",
      "Time elapsed:  36941.140679359436\n",
      "ep 4008: ep_len:533 episode reward: total was 43.240000. running mean: 25.271592\n",
      "ep 4008: ep_len:500 episode reward: total was 46.950000. running mean: 25.488377\n",
      "ep 4008: ep_len:662 episode reward: total was 9.490000. running mean: 25.328393\n",
      "ep 4008: ep_len:109 episode reward: total was 6.560000. running mean: 25.140709\n",
      "ep 4008: ep_len:60 episode reward: total was 9.710000. running mean: 24.986402\n",
      "ep 4008: ep_len:329 episode reward: total was 46.250000. running mean: 25.199038\n",
      "ep 4008: ep_len:201 episode reward: total was -14.510000. running mean: 24.801947\n",
      "epsilon:0.022268 episode_count: 28063. steps_count: 12134736.000000\n",
      "Time elapsed:  36947.63520002365\n",
      "ep 4009: ep_len:613 episode reward: total was 114.960000. running mean: 25.703528\n",
      "ep 4009: ep_len:500 episode reward: total was 16.160000. running mean: 25.608093\n",
      "ep 4009: ep_len:451 episode reward: total was 22.340000. running mean: 25.575412\n",
      "ep 4009: ep_len:119 episode reward: total was 26.540000. running mean: 25.585058\n",
      "ep 4009: ep_len:54 episode reward: total was 26.510000. running mean: 25.594307\n",
      "ep 4009: ep_len:500 episode reward: total was 41.870000. running mean: 25.757064\n",
      "ep 4009: ep_len:579 episode reward: total was 28.180000. running mean: 25.781293\n",
      "epsilon:0.022223 episode_count: 28070. steps_count: 12137552.000000\n",
      "Time elapsed:  36955.182765483856\n",
      "ep 4010: ep_len:210 episode reward: total was 28.720000. running mean: 25.810680\n",
      "ep 4010: ep_len:500 episode reward: total was 55.460000. running mean: 26.107174\n",
      "ep 4010: ep_len:577 episode reward: total was 1.050000. running mean: 25.856602\n",
      "ep 4010: ep_len:511 episode reward: total was 30.840000. running mean: 25.906436\n",
      "ep 4010: ep_len:3 episode reward: total was -1.500000. running mean: 25.632371\n",
      "ep 4010: ep_len:586 episode reward: total was 42.250000. running mean: 25.798548\n",
      "ep 4010: ep_len:558 episode reward: total was 36.770000. running mean: 25.908262\n",
      "epsilon:0.022179 episode_count: 28077. steps_count: 12140497.000000\n",
      "Time elapsed:  36963.18843245506\n",
      "ep 4011: ep_len:500 episode reward: total was 70.990000. running mean: 26.359080\n",
      "ep 4011: ep_len:500 episode reward: total was 59.520000. running mean: 26.690689\n",
      "ep 4011: ep_len:443 episode reward: total was 69.770000. running mean: 27.121482\n",
      "ep 4011: ep_len:506 episode reward: total was 10.280000. running mean: 26.953067\n",
      "ep 4011: ep_len:3 episode reward: total was 1.010000. running mean: 26.693636\n",
      "ep 4011: ep_len:541 episode reward: total was -28.480000. running mean: 26.141900\n",
      "ep 4011: ep_len:604 episode reward: total was 13.160000. running mean: 26.012081\n",
      "epsilon:0.022135 episode_count: 28084. steps_count: 12143594.000000\n",
      "Time elapsed:  36971.48961901665\n",
      "ep 4012: ep_len:560 episode reward: total was 19.520000. running mean: 25.947160\n",
      "ep 4012: ep_len:528 episode reward: total was 21.350000. running mean: 25.901189\n",
      "ep 4012: ep_len:677 episode reward: total was 19.370000. running mean: 25.835877\n",
      "ep 4012: ep_len:501 episode reward: total was 76.560000. running mean: 26.343118\n",
      "ep 4012: ep_len:3 episode reward: total was 1.010000. running mean: 26.089787\n",
      "ep 4012: ep_len:500 episode reward: total was 41.490000. running mean: 26.243789\n",
      "ep 4012: ep_len:625 episode reward: total was 53.720000. running mean: 26.518551\n",
      "epsilon:0.022090 episode_count: 28091. steps_count: 12146988.000000\n",
      "Time elapsed:  36986.96923470497\n",
      "ep 4013: ep_len:103 episode reward: total was 8.980000. running mean: 26.343166\n",
      "ep 4013: ep_len:524 episode reward: total was 26.050000. running mean: 26.340234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4013: ep_len:551 episode reward: total was 56.650000. running mean: 26.643332\n",
      "ep 4013: ep_len:550 episode reward: total was 19.750000. running mean: 26.574398\n",
      "ep 4013: ep_len:130 episode reward: total was 32.860000. running mean: 26.637254\n",
      "ep 4013: ep_len:535 episode reward: total was 17.250000. running mean: 26.543382\n",
      "ep 4013: ep_len:280 episode reward: total was 24.500000. running mean: 26.522948\n",
      "epsilon:0.022046 episode_count: 28098. steps_count: 12149661.000000\n",
      "Time elapsed:  36994.205901145935\n",
      "ep 4014: ep_len:525 episode reward: total was 91.520000. running mean: 27.172918\n",
      "ep 4014: ep_len:589 episode reward: total was -0.900000. running mean: 26.892189\n",
      "ep 4014: ep_len:607 episode reward: total was 22.910000. running mean: 26.852367\n",
      "ep 4014: ep_len:130 episode reward: total was 13.060000. running mean: 26.714444\n",
      "ep 4014: ep_len:3 episode reward: total was 1.010000. running mean: 26.457399\n",
      "ep 4014: ep_len:517 episode reward: total was 11.590000. running mean: 26.308725\n",
      "ep 4014: ep_len:604 episode reward: total was 62.310000. running mean: 26.668738\n",
      "epsilon:0.022002 episode_count: 28105. steps_count: 12152636.000000\n",
      "Time elapsed:  37001.58838677406\n",
      "ep 4015: ep_len:588 episode reward: total was 37.760000. running mean: 26.779651\n",
      "ep 4015: ep_len:506 episode reward: total was -1.050000. running mean: 26.501354\n",
      "ep 4015: ep_len:528 episode reward: total was -10.350000. running mean: 26.132841\n",
      "ep 4015: ep_len:500 episode reward: total was 17.290000. running mean: 26.044412\n",
      "ep 4015: ep_len:3 episode reward: total was 1.010000. running mean: 25.794068\n",
      "ep 4015: ep_len:627 episode reward: total was 58.670000. running mean: 26.122827\n",
      "ep 4015: ep_len:564 episode reward: total was 45.330000. running mean: 26.314899\n",
      "epsilon:0.021957 episode_count: 28112. steps_count: 12155952.000000\n",
      "Time elapsed:  37010.36043405533\n",
      "ep 4016: ep_len:580 episode reward: total was 84.860000. running mean: 26.900350\n",
      "ep 4016: ep_len:587 episode reward: total was 81.690000. running mean: 27.448247\n",
      "ep 4016: ep_len:79 episode reward: total was 7.790000. running mean: 27.251664\n",
      "ep 4016: ep_len:121 episode reward: total was 20.070000. running mean: 27.179847\n",
      "ep 4016: ep_len:114 episode reward: total was 33.250000. running mean: 27.240549\n",
      "ep 4016: ep_len:500 episode reward: total was 52.370000. running mean: 27.491844\n",
      "ep 4016: ep_len:562 episode reward: total was 37.000000. running mean: 27.586925\n",
      "epsilon:0.021913 episode_count: 28119. steps_count: 12158495.000000\n",
      "Time elapsed:  37016.85155558586\n",
      "ep 4017: ep_len:591 episode reward: total was 50.660000. running mean: 27.817656\n",
      "ep 4017: ep_len:500 episode reward: total was 66.310000. running mean: 28.202579\n",
      "ep 4017: ep_len:566 episode reward: total was 10.390000. running mean: 28.024453\n",
      "ep 4017: ep_len:560 episode reward: total was 87.970000. running mean: 28.623909\n",
      "ep 4017: ep_len:123 episode reward: total was 29.360000. running mean: 28.631270\n",
      "ep 4017: ep_len:527 episode reward: total was 43.660000. running mean: 28.781557\n",
      "ep 4017: ep_len:588 episode reward: total was 41.750000. running mean: 28.911242\n",
      "epsilon:0.021869 episode_count: 28126. steps_count: 12161950.000000\n",
      "Time elapsed:  37032.798156023026\n",
      "ep 4018: ep_len:601 episode reward: total was 3.600000. running mean: 28.658129\n",
      "ep 4018: ep_len:299 episode reward: total was 19.740000. running mean: 28.568948\n",
      "ep 4018: ep_len:605 episode reward: total was -0.950000. running mean: 28.273758\n",
      "ep 4018: ep_len:120 episode reward: total was 22.480000. running mean: 28.215821\n",
      "ep 4018: ep_len:3 episode reward: total was 1.010000. running mean: 27.943763\n",
      "ep 4018: ep_len:658 episode reward: total was 24.600000. running mean: 27.910325\n",
      "ep 4018: ep_len:624 episode reward: total was 2.650000. running mean: 27.657722\n",
      "epsilon:0.021824 episode_count: 28133. steps_count: 12164860.000000\n",
      "Time elapsed:  37040.64699077606\n",
      "ep 4019: ep_len:534 episode reward: total was -117.990000. running mean: 26.201245\n",
      "ep 4019: ep_len:500 episode reward: total was 46.750000. running mean: 26.406732\n",
      "ep 4019: ep_len:500 episode reward: total was 63.190000. running mean: 26.774565\n",
      "ep 4019: ep_len:500 episode reward: total was 42.900000. running mean: 26.935819\n",
      "ep 4019: ep_len:3 episode reward: total was 1.010000. running mean: 26.676561\n",
      "ep 4019: ep_len:526 episode reward: total was -170.230000. running mean: 24.707495\n",
      "ep 4019: ep_len:301 episode reward: total was 19.940000. running mean: 24.659820\n",
      "epsilon:0.021780 episode_count: 28140. steps_count: 12167724.000000\n",
      "Time elapsed:  37048.23842048645\n",
      "ep 4020: ep_len:570 episode reward: total was 8.190000. running mean: 24.495122\n",
      "ep 4020: ep_len:577 episode reward: total was 46.300000. running mean: 24.713171\n",
      "ep 4020: ep_len:651 episode reward: total was 5.640000. running mean: 24.522439\n",
      "ep 4020: ep_len:557 episode reward: total was 74.200000. running mean: 25.019215\n",
      "ep 4020: ep_len:87 episode reward: total was 22.750000. running mean: 24.996523\n",
      "ep 4020: ep_len:645 episode reward: total was 49.500000. running mean: 25.241557\n",
      "ep 4020: ep_len:586 episode reward: total was 53.940000. running mean: 25.528542\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.021736 episode_count: 28147. steps_count: 12171397.000000\n",
      "Time elapsed:  37062.46529698372\n",
      "ep 4021: ep_len:560 episode reward: total was -10.390000. running mean: 25.169356\n",
      "ep 4021: ep_len:500 episode reward: total was 107.100000. running mean: 25.988663\n",
      "ep 4021: ep_len:536 episode reward: total was 28.100000. running mean: 26.009776\n",
      "ep 4021: ep_len:134 episode reward: total was -0.890000. running mean: 25.740779\n",
      "ep 4021: ep_len:54 episode reward: total was 26.510000. running mean: 25.748471\n",
      "ep 4021: ep_len:501 episode reward: total was -11.080000. running mean: 25.380186\n",
      "ep 4021: ep_len:506 episode reward: total was 59.520000. running mean: 25.721584\n",
      "epsilon:0.021691 episode_count: 28154. steps_count: 12174188.000000\n",
      "Time elapsed:  37069.93338179588\n",
      "ep 4022: ep_len:526 episode reward: total was 53.320000. running mean: 25.997568\n",
      "ep 4022: ep_len:500 episode reward: total was 54.860000. running mean: 26.286193\n",
      "ep 4022: ep_len:64 episode reward: total was 10.330000. running mean: 26.126631\n",
      "ep 4022: ep_len:588 episode reward: total was 54.350000. running mean: 26.408864\n",
      "ep 4022: ep_len:97 episode reward: total was 30.260000. running mean: 26.447376\n",
      "ep 4022: ep_len:583 episode reward: total was 26.760000. running mean: 26.450502\n",
      "ep 4022: ep_len:319 episode reward: total was -225.790000. running mean: 23.928097\n",
      "epsilon:0.021647 episode_count: 28161. steps_count: 12176865.000000\n",
      "Time elapsed:  37077.19818663597\n",
      "ep 4023: ep_len:535 episode reward: total was -5.620000. running mean: 23.632616\n",
      "ep 4023: ep_len:500 episode reward: total was 22.850000. running mean: 23.624790\n",
      "ep 4023: ep_len:392 episode reward: total was 77.720000. running mean: 24.165742\n",
      "ep 4023: ep_len:503 episode reward: total was -42.990000. running mean: 23.494185\n",
      "ep 4023: ep_len:112 episode reward: total was 27.350000. running mean: 23.532743\n",
      "ep 4023: ep_len:606 episode reward: total was 71.440000. running mean: 24.011815\n",
      "ep 4023: ep_len:610 episode reward: total was 43.300000. running mean: 24.204697\n",
      "epsilon:0.021603 episode_count: 28168. steps_count: 12180123.000000\n",
      "Time elapsed:  37085.787158727646\n",
      "ep 4024: ep_len:256 episode reward: total was 11.750000. running mean: 24.080150\n",
      "ep 4024: ep_len:165 episode reward: total was 3.890000. running mean: 23.878249\n",
      "ep 4024: ep_len:578 episode reward: total was 33.570000. running mean: 23.975166\n",
      "ep 4024: ep_len:605 episode reward: total was -24.630000. running mean: 23.489114\n",
      "ep 4024: ep_len:3 episode reward: total was 1.010000. running mean: 23.264323\n",
      "ep 4024: ep_len:598 episode reward: total was 52.590000. running mean: 23.557580\n",
      "ep 4024: ep_len:519 episode reward: total was 15.290000. running mean: 23.474904\n",
      "epsilon:0.021558 episode_count: 28175. steps_count: 12182847.000000\n",
      "Time elapsed:  37093.20307946205\n",
      "ep 4025: ep_len:503 episode reward: total was -12.070000. running mean: 23.119455\n",
      "ep 4025: ep_len:502 episode reward: total was 113.860000. running mean: 24.026861\n",
      "ep 4025: ep_len:451 episode reward: total was 41.350000. running mean: 24.200092\n",
      "ep 4025: ep_len:510 episode reward: total was 24.180000. running mean: 24.199891\n",
      "ep 4025: ep_len:3 episode reward: total was 1.010000. running mean: 23.967992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4025: ep_len:163 episode reward: total was 21.040000. running mean: 23.938712\n",
      "ep 4025: ep_len:606 episode reward: total was 45.220000. running mean: 24.151525\n",
      "epsilon:0.021514 episode_count: 28182. steps_count: 12185585.000000\n",
      "Time elapsed:  37100.470214128494\n",
      "ep 4026: ep_len:550 episode reward: total was -25.460000. running mean: 23.655410\n",
      "ep 4026: ep_len:500 episode reward: total was 47.210000. running mean: 23.890956\n",
      "ep 4026: ep_len:500 episode reward: total was 41.890000. running mean: 24.070946\n",
      "ep 4026: ep_len:543 episode reward: total was 72.140000. running mean: 24.551637\n",
      "ep 4026: ep_len:3 episode reward: total was 1.010000. running mean: 24.316220\n",
      "ep 4026: ep_len:576 episode reward: total was 34.770000. running mean: 24.420758\n",
      "ep 4026: ep_len:624 episode reward: total was 46.540000. running mean: 24.641951\n",
      "epsilon:0.021470 episode_count: 28189. steps_count: 12188881.000000\n",
      "Time elapsed:  37107.72760677338\n",
      "ep 4027: ep_len:558 episode reward: total was 40.170000. running mean: 24.797231\n",
      "ep 4027: ep_len:500 episode reward: total was 34.030000. running mean: 24.889559\n",
      "ep 4027: ep_len:586 episode reward: total was 17.550000. running mean: 24.816163\n",
      "ep 4027: ep_len:500 episode reward: total was 58.660000. running mean: 25.154602\n",
      "ep 4027: ep_len:3 episode reward: total was 1.010000. running mean: 24.913156\n",
      "ep 4027: ep_len:250 episode reward: total was 33.990000. running mean: 25.003924\n",
      "ep 4027: ep_len:543 episode reward: total was -149.750000. running mean: 23.256385\n",
      "epsilon:0.021425 episode_count: 28196. steps_count: 12191821.000000\n",
      "Time elapsed:  37112.78160262108\n",
      "ep 4028: ep_len:561 episode reward: total was 36.430000. running mean: 23.388121\n",
      "ep 4028: ep_len:632 episode reward: total was 36.950000. running mean: 23.523740\n",
      "ep 4028: ep_len:500 episode reward: total was 40.900000. running mean: 23.697502\n",
      "ep 4028: ep_len:506 episode reward: total was 41.110000. running mean: 23.871627\n",
      "ep 4028: ep_len:3 episode reward: total was 1.010000. running mean: 23.643011\n",
      "ep 4028: ep_len:611 episode reward: total was 41.460000. running mean: 23.821181\n",
      "ep 4028: ep_len:549 episode reward: total was -0.550000. running mean: 23.577469\n",
      "epsilon:0.021381 episode_count: 28203. steps_count: 12195183.000000\n",
      "Time elapsed:  37117.893369197845\n",
      "ep 4029: ep_len:646 episode reward: total was -28.210000. running mean: 23.059594\n",
      "ep 4029: ep_len:500 episode reward: total was 53.600000. running mean: 23.364999\n",
      "ep 4029: ep_len:500 episode reward: total was -7.280000. running mean: 23.058549\n",
      "ep 4029: ep_len:500 episode reward: total was 52.010000. running mean: 23.348063\n",
      "ep 4029: ep_len:52 episode reward: total was 24.010000. running mean: 23.354682\n",
      "ep 4029: ep_len:500 episode reward: total was -0.840000. running mean: 23.112736\n",
      "ep 4029: ep_len:599 episode reward: total was 28.530000. running mean: 23.166908\n",
      "epsilon:0.021337 episode_count: 28210. steps_count: 12198480.000000\n",
      "Time elapsed:  37123.309331178665\n",
      "ep 4030: ep_len:677 episode reward: total was 21.880000. running mean: 23.154039\n",
      "ep 4030: ep_len:598 episode reward: total was 34.680000. running mean: 23.269299\n",
      "ep 4030: ep_len:508 episode reward: total was 66.810000. running mean: 23.704706\n",
      "ep 4030: ep_len:500 episode reward: total was 122.180000. running mean: 24.689459\n",
      "ep 4030: ep_len:3 episode reward: total was 1.010000. running mean: 24.452664\n",
      "ep 4030: ep_len:560 episode reward: total was 15.020000. running mean: 24.358337\n",
      "ep 4030: ep_len:564 episode reward: total was 52.890000. running mean: 24.643654\n",
      "epsilon:0.021292 episode_count: 28217. steps_count: 12201890.000000\n",
      "Time elapsed:  37139.365988492966\n",
      "ep 4031: ep_len:538 episode reward: total was 31.330000. running mean: 24.710518\n",
      "ep 4031: ep_len:188 episode reward: total was 14.840000. running mean: 24.611812\n",
      "ep 4031: ep_len:500 episode reward: total was 17.260000. running mean: 24.538294\n",
      "ep 4031: ep_len:521 episode reward: total was -13.750000. running mean: 24.155411\n",
      "ep 4031: ep_len:3 episode reward: total was 1.010000. running mean: 23.923957\n",
      "ep 4031: ep_len:500 episode reward: total was 25.930000. running mean: 23.944018\n",
      "ep 4031: ep_len:574 episode reward: total was 40.910000. running mean: 24.113677\n",
      "epsilon:0.021248 episode_count: 28224. steps_count: 12204714.000000\n",
      "Time elapsed:  37146.87706327438\n",
      "ep 4032: ep_len:590 episode reward: total was 68.250000. running mean: 24.555041\n",
      "ep 4032: ep_len:536 episode reward: total was 108.890000. running mean: 25.398390\n",
      "ep 4032: ep_len:662 episode reward: total was 9.820000. running mean: 25.242606\n",
      "ep 4032: ep_len:626 episode reward: total was 93.820000. running mean: 25.928380\n",
      "ep 4032: ep_len:3 episode reward: total was 1.010000. running mean: 25.679197\n",
      "ep 4032: ep_len:571 episode reward: total was 0.680000. running mean: 25.429205\n",
      "ep 4032: ep_len:505 episode reward: total was 57.000000. running mean: 25.744913\n",
      "epsilon:0.021204 episode_count: 28231. steps_count: 12208207.000000\n",
      "Time elapsed:  37162.611255168915\n",
      "ep 4033: ep_len:640 episode reward: total was 16.940000. running mean: 25.656863\n",
      "ep 4033: ep_len:500 episode reward: total was 50.720000. running mean: 25.907495\n",
      "ep 4033: ep_len:515 episode reward: total was 12.910000. running mean: 25.777520\n",
      "ep 4033: ep_len:774 episode reward: total was -201.340000. running mean: 23.506345\n",
      "ep 4033: ep_len:3 episode reward: total was 1.010000. running mean: 23.281381\n",
      "ep 4033: ep_len:500 episode reward: total was 22.510000. running mean: 23.273667\n",
      "ep 4033: ep_len:606 episode reward: total was 68.510000. running mean: 23.726031\n",
      "epsilon:0.021159 episode_count: 28238. steps_count: 12211745.000000\n",
      "Time elapsed:  37171.80284142494\n",
      "ep 4034: ep_len:641 episode reward: total was 8.590000. running mean: 23.574670\n",
      "ep 4034: ep_len:606 episode reward: total was 59.660000. running mean: 23.935524\n",
      "ep 4034: ep_len:766 episode reward: total was -149.940000. running mean: 22.196768\n",
      "ep 4034: ep_len:592 episode reward: total was 55.460000. running mean: 22.529401\n",
      "ep 4034: ep_len:95 episode reward: total was -29.270000. running mean: 22.011407\n",
      "ep 4034: ep_len:594 episode reward: total was 9.610000. running mean: 21.887393\n",
      "ep 4034: ep_len:516 episode reward: total was 16.430000. running mean: 21.832819\n",
      "epsilon:0.021115 episode_count: 28245. steps_count: 12215555.000000\n",
      "Time elapsed:  37181.37270069122\n",
      "ep 4035: ep_len:500 episode reward: total was 23.440000. running mean: 21.848891\n",
      "ep 4035: ep_len:201 episode reward: total was 18.950000. running mean: 21.819902\n",
      "ep 4035: ep_len:639 episode reward: total was -43.970000. running mean: 21.162003\n",
      "ep 4035: ep_len:159 episode reward: total was 32.150000. running mean: 21.271883\n",
      "ep 4035: ep_len:104 episode reward: total was 33.760000. running mean: 21.396764\n",
      "ep 4035: ep_len:501 episode reward: total was 30.710000. running mean: 21.489896\n",
      "ep 4035: ep_len:595 episode reward: total was 45.390000. running mean: 21.728897\n",
      "epsilon:0.021071 episode_count: 28252. steps_count: 12218254.000000\n",
      "Time elapsed:  37188.65979909897\n",
      "ep 4036: ep_len:532 episode reward: total was 47.580000. running mean: 21.987408\n",
      "ep 4036: ep_len:502 episode reward: total was 22.890000. running mean: 21.996434\n",
      "ep 4036: ep_len:597 episode reward: total was 8.330000. running mean: 21.859770\n",
      "ep 4036: ep_len:506 episode reward: total was 72.020000. running mean: 22.361372\n",
      "ep 4036: ep_len:92 episode reward: total was 28.770000. running mean: 22.425458\n",
      "ep 4036: ep_len:261 episode reward: total was 46.680000. running mean: 22.668004\n",
      "ep 4036: ep_len:285 episode reward: total was -21.030000. running mean: 22.231024\n",
      "epsilon:0.021026 episode_count: 28259. steps_count: 12221029.000000\n",
      "Time elapsed:  37196.17418599129\n",
      "ep 4037: ep_len:500 episode reward: total was 74.670000. running mean: 22.755414\n",
      "ep 4037: ep_len:553 episode reward: total was 144.030000. running mean: 23.968159\n",
      "ep 4037: ep_len:79 episode reward: total was 8.800000. running mean: 23.816478\n",
      "ep 4037: ep_len:500 episode reward: total was 51.660000. running mean: 24.094913\n",
      "ep 4037: ep_len:113 episode reward: total was 38.260000. running mean: 24.236564\n",
      "ep 4037: ep_len:500 episode reward: total was -107.050000. running mean: 22.923698\n",
      "ep 4037: ep_len:525 episode reward: total was 28.460000. running mean: 22.979061\n",
      "epsilon:0.020982 episode_count: 28266. steps_count: 12223799.000000\n",
      "Time elapsed:  37203.70618438721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4038: ep_len:634 episode reward: total was 17.400000. running mean: 22.923271\n",
      "ep 4038: ep_len:580 episode reward: total was 58.430000. running mean: 23.278338\n",
      "ep 4038: ep_len:500 episode reward: total was 65.420000. running mean: 23.699755\n",
      "ep 4038: ep_len:610 episode reward: total was 62.120000. running mean: 24.083957\n",
      "ep 4038: ep_len:3 episode reward: total was 1.010000. running mean: 23.853217\n",
      "ep 4038: ep_len:500 episode reward: total was 35.280000. running mean: 23.967485\n",
      "ep 4038: ep_len:507 episode reward: total was 6.510000. running mean: 23.792910\n",
      "epsilon:0.020938 episode_count: 28273. steps_count: 12227133.000000\n",
      "Time elapsed:  37212.54943203926\n",
      "ep 4039: ep_len:500 episode reward: total was 18.990000. running mean: 23.744881\n",
      "ep 4039: ep_len:192 episode reward: total was -11.170000. running mean: 23.395732\n",
      "ep 4039: ep_len:500 episode reward: total was 35.220000. running mean: 23.513975\n",
      "ep 4039: ep_len:596 episode reward: total was 62.330000. running mean: 23.902135\n",
      "ep 4039: ep_len:3 episode reward: total was 1.010000. running mean: 23.673214\n",
      "ep 4039: ep_len:507 episode reward: total was 27.970000. running mean: 23.716182\n",
      "ep 4039: ep_len:609 episode reward: total was 51.720000. running mean: 23.996220\n",
      "epsilon:0.020893 episode_count: 28280. steps_count: 12230040.000000\n",
      "Time elapsed:  37220.31883382797\n",
      "ep 4040: ep_len:217 episode reward: total was 17.220000. running mean: 23.928458\n",
      "ep 4040: ep_len:547 episode reward: total was 32.290000. running mean: 24.012073\n",
      "ep 4040: ep_len:79 episode reward: total was 14.860000. running mean: 23.920553\n",
      "ep 4040: ep_len:414 episode reward: total was -109.030000. running mean: 22.591047\n",
      "ep 4040: ep_len:126 episode reward: total was 35.360000. running mean: 22.718737\n",
      "ep 4040: ep_len:616 episode reward: total was 51.270000. running mean: 23.004249\n",
      "ep 4040: ep_len:500 episode reward: total was 6.790000. running mean: 22.842107\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.020849 episode_count: 28287. steps_count: 12232539.000000\n",
      "Time elapsed:  37231.8208398819\n",
      "ep 4041: ep_len:548 episode reward: total was 78.930000. running mean: 23.402986\n",
      "ep 4041: ep_len:331 episode reward: total was 31.180000. running mean: 23.480756\n",
      "ep 4041: ep_len:521 episode reward: total was 9.700000. running mean: 23.342948\n",
      "ep 4041: ep_len:500 episode reward: total was -13.010000. running mean: 22.979419\n",
      "ep 4041: ep_len:3 episode reward: total was 1.010000. running mean: 22.759725\n",
      "ep 4041: ep_len:565 episode reward: total was 71.850000. running mean: 23.250627\n",
      "ep 4041: ep_len:609 episode reward: total was 36.800000. running mean: 23.386121\n",
      "epsilon:0.020805 episode_count: 28294. steps_count: 12235616.000000\n",
      "Time elapsed:  37239.783022642136\n",
      "ep 4042: ep_len:517 episode reward: total was -16.310000. running mean: 22.989160\n",
      "ep 4042: ep_len:502 episode reward: total was 26.830000. running mean: 23.027568\n",
      "ep 4042: ep_len:525 episode reward: total was 18.730000. running mean: 22.984593\n",
      "ep 4042: ep_len:500 episode reward: total was 34.220000. running mean: 23.096947\n",
      "ep 4042: ep_len:54 episode reward: total was 22.010000. running mean: 23.086077\n",
      "ep 4042: ep_len:171 episode reward: total was 43.160000. running mean: 23.286816\n",
      "ep 4042: ep_len:500 episode reward: total was 71.850000. running mean: 23.772448\n",
      "epsilon:0.020760 episode_count: 28301. steps_count: 12238385.000000\n",
      "Time elapsed:  37247.29199051857\n",
      "ep 4043: ep_len:500 episode reward: total was 82.040000. running mean: 24.355124\n",
      "ep 4043: ep_len:510 episode reward: total was -0.320000. running mean: 24.108373\n",
      "ep 4043: ep_len:694 episode reward: total was 20.650000. running mean: 24.073789\n",
      "ep 4043: ep_len:504 episode reward: total was 17.570000. running mean: 24.008751\n",
      "ep 4043: ep_len:99 episode reward: total was 29.760000. running mean: 24.066263\n",
      "ep 4043: ep_len:601 episode reward: total was 40.940000. running mean: 24.235001\n",
      "ep 4043: ep_len:592 episode reward: total was 47.420000. running mean: 24.466851\n",
      "epsilon:0.020716 episode_count: 28308. steps_count: 12241885.000000\n",
      "Time elapsed:  37262.62372303009\n",
      "ep 4044: ep_len:613 episode reward: total was 57.840000. running mean: 24.800582\n",
      "ep 4044: ep_len:174 episode reward: total was 4.320000. running mean: 24.595776\n",
      "ep 4044: ep_len:538 episode reward: total was 24.070000. running mean: 24.590519\n",
      "ep 4044: ep_len:543 episode reward: total was 76.740000. running mean: 25.112013\n",
      "ep 4044: ep_len:113 episode reward: total was 34.310000. running mean: 25.203993\n",
      "ep 4044: ep_len:640 episode reward: total was 14.470000. running mean: 25.096653\n",
      "ep 4044: ep_len:297 episode reward: total was -37.100000. running mean: 24.474687\n",
      "epsilon:0.020672 episode_count: 28315. steps_count: 12244803.000000\n",
      "Time elapsed:  37270.22520303726\n",
      "ep 4045: ep_len:229 episode reward: total was 21.230000. running mean: 24.442240\n",
      "ep 4045: ep_len:588 episode reward: total was 70.310000. running mean: 24.900918\n",
      "ep 4045: ep_len:555 episode reward: total was 12.570000. running mean: 24.777608\n",
      "ep 4045: ep_len:416 episode reward: total was 44.200000. running mean: 24.971832\n",
      "ep 4045: ep_len:3 episode reward: total was 1.010000. running mean: 24.732214\n",
      "ep 4045: ep_len:239 episode reward: total was 45.240000. running mean: 24.937292\n",
      "ep 4045: ep_len:502 episode reward: total was 44.600000. running mean: 25.133919\n",
      "epsilon:0.020627 episode_count: 28322. steps_count: 12247335.000000\n",
      "Time elapsed:  37275.341356515884\n",
      "ep 4046: ep_len:500 episode reward: total was -3.120000. running mean: 24.851380\n",
      "ep 4046: ep_len:500 episode reward: total was 107.810000. running mean: 25.680966\n",
      "ep 4046: ep_len:559 episode reward: total was 59.330000. running mean: 26.017456\n",
      "ep 4046: ep_len:604 episode reward: total was -70.730000. running mean: 25.049982\n",
      "ep 4046: ep_len:119 episode reward: total was 38.260000. running mean: 25.182082\n",
      "ep 4046: ep_len:647 episode reward: total was 20.660000. running mean: 25.136861\n",
      "ep 4046: ep_len:529 episode reward: total was 23.150000. running mean: 25.116993\n",
      "epsilon:0.020583 episode_count: 28329. steps_count: 12250793.000000\n",
      "Time elapsed:  37283.170453071594\n",
      "ep 4047: ep_len:607 episode reward: total was 23.750000. running mean: 25.103323\n",
      "ep 4047: ep_len:591 episode reward: total was 48.750000. running mean: 25.339789\n",
      "ep 4047: ep_len:500 episode reward: total was 33.090000. running mean: 25.417291\n",
      "ep 4047: ep_len:500 episode reward: total was 27.360000. running mean: 25.436719\n",
      "ep 4047: ep_len:3 episode reward: total was 0.000000. running mean: 25.182351\n",
      "ep 4047: ep_len:504 episode reward: total was 15.690000. running mean: 25.087428\n",
      "ep 4047: ep_len:536 episode reward: total was 14.730000. running mean: 24.983854\n",
      "epsilon:0.020539 episode_count: 28336. steps_count: 12254034.000000\n",
      "Time elapsed:  37291.91123199463\n",
      "ep 4048: ep_len:133 episode reward: total was 15.040000. running mean: 24.884415\n",
      "ep 4048: ep_len:629 episode reward: total was -81.950000. running mean: 23.816071\n",
      "ep 4048: ep_len:500 episode reward: total was 33.980000. running mean: 23.917710\n",
      "ep 4048: ep_len:500 episode reward: total was 74.360000. running mean: 24.422133\n",
      "ep 4048: ep_len:3 episode reward: total was 1.010000. running mean: 24.188012\n",
      "ep 4048: ep_len:598 episode reward: total was 36.160000. running mean: 24.307732\n",
      "ep 4048: ep_len:290 episode reward: total was 41.300000. running mean: 24.477654\n",
      "epsilon:0.020494 episode_count: 28343. steps_count: 12256687.000000\n",
      "Time elapsed:  37298.70396947861\n",
      "ep 4049: ep_len:191 episode reward: total was 28.250000. running mean: 24.515378\n",
      "ep 4049: ep_len:540 episode reward: total was 116.950000. running mean: 25.439724\n",
      "ep 4049: ep_len:418 episode reward: total was 49.620000. running mean: 25.681527\n",
      "ep 4049: ep_len:540 episode reward: total was 44.500000. running mean: 25.869711\n",
      "ep 4049: ep_len:3 episode reward: total was 1.010000. running mean: 25.621114\n",
      "ep 4049: ep_len:612 episode reward: total was 13.890000. running mean: 25.503803\n",
      "ep 4049: ep_len:540 episode reward: total was 11.560000. running mean: 25.364365\n",
      "epsilon:0.020450 episode_count: 28350. steps_count: 12259531.000000\n",
      "Time elapsed:  37308.614006757736\n",
      "ep 4050: ep_len:632 episode reward: total was 63.390000. running mean: 25.744622\n",
      "ep 4050: ep_len:576 episode reward: total was 17.170000. running mean: 25.658875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4050: ep_len:385 episode reward: total was 63.580000. running mean: 26.038087\n",
      "ep 4050: ep_len:368 episode reward: total was 31.920000. running mean: 26.096906\n",
      "ep 4050: ep_len:92 episode reward: total was 32.720000. running mean: 26.163137\n",
      "ep 4050: ep_len:500 episode reward: total was 49.040000. running mean: 26.391905\n",
      "ep 4050: ep_len:500 episode reward: total was 7.620000. running mean: 26.204186\n",
      "epsilon:0.020406 episode_count: 28357. steps_count: 12262584.000000\n",
      "Time elapsed:  37324.88165283203\n",
      "ep 4051: ep_len:216 episode reward: total was -31.100000. running mean: 25.631144\n",
      "ep 4051: ep_len:524 episode reward: total was 106.320000. running mean: 26.438033\n",
      "ep 4051: ep_len:683 episode reward: total was 30.300000. running mean: 26.476653\n",
      "ep 4051: ep_len:563 episode reward: total was 56.020000. running mean: 26.772086\n",
      "ep 4051: ep_len:52 episode reward: total was 20.000000. running mean: 26.704365\n",
      "ep 4051: ep_len:619 episode reward: total was 56.440000. running mean: 27.001722\n",
      "ep 4051: ep_len:194 episode reward: total was 6.630000. running mean: 26.798004\n",
      "epsilon:0.020361 episode_count: 28364. steps_count: 12265435.000000\n",
      "Time elapsed:  37332.45680260658\n",
      "ep 4052: ep_len:506 episode reward: total was 1.010000. running mean: 26.540124\n",
      "ep 4052: ep_len:164 episode reward: total was 19.280000. running mean: 26.467523\n",
      "ep 4052: ep_len:358 episode reward: total was 53.350000. running mean: 26.736348\n",
      "ep 4052: ep_len:587 episode reward: total was 59.210000. running mean: 27.061084\n",
      "ep 4052: ep_len:3 episode reward: total was 1.010000. running mean: 26.800574\n",
      "ep 4052: ep_len:740 episode reward: total was -165.660000. running mean: 24.875968\n",
      "ep 4052: ep_len:310 episode reward: total was 27.820000. running mean: 24.905408\n",
      "epsilon:0.020317 episode_count: 28371. steps_count: 12268103.000000\n",
      "Time elapsed:  37339.704001665115\n",
      "ep 4053: ep_len:541 episode reward: total was 91.200000. running mean: 25.568354\n",
      "ep 4053: ep_len:500 episode reward: total was 48.570000. running mean: 25.798370\n",
      "ep 4053: ep_len:609 episode reward: total was 23.790000. running mean: 25.778287\n",
      "ep 4053: ep_len:532 episode reward: total was 77.210000. running mean: 26.292604\n",
      "ep 4053: ep_len:3 episode reward: total was 1.010000. running mean: 26.039778\n",
      "ep 4053: ep_len:196 episode reward: total was 27.260000. running mean: 26.051980\n",
      "ep 4053: ep_len:556 episode reward: total was 26.770000. running mean: 26.059160\n",
      "epsilon:0.020273 episode_count: 28378. steps_count: 12271040.000000\n",
      "Time elapsed:  37347.46771454811\n",
      "ep 4054: ep_len:500 episode reward: total was 20.590000. running mean: 26.004469\n",
      "ep 4054: ep_len:500 episode reward: total was 53.960000. running mean: 26.284024\n",
      "ep 4054: ep_len:500 episode reward: total was -18.190000. running mean: 25.839284\n",
      "ep 4054: ep_len:542 episode reward: total was 99.030000. running mean: 26.571191\n",
      "ep 4054: ep_len:98 episode reward: total was 28.290000. running mean: 26.588379\n",
      "ep 4054: ep_len:519 episode reward: total was 10.360000. running mean: 26.426095\n",
      "ep 4054: ep_len:546 episode reward: total was 74.690000. running mean: 26.908734\n",
      "epsilon:0.020228 episode_count: 28385. steps_count: 12274245.000000\n",
      "Time elapsed:  37357.10792684555\n",
      "ep 4055: ep_len:982 episode reward: total was -321.690000. running mean: 23.422747\n",
      "ep 4055: ep_len:500 episode reward: total was -4.380000. running mean: 23.144719\n",
      "ep 4055: ep_len:435 episode reward: total was 73.480000. running mean: 23.648072\n",
      "ep 4055: ep_len:116 episode reward: total was 16.100000. running mean: 23.572592\n",
      "ep 4055: ep_len:3 episode reward: total was 1.010000. running mean: 23.346966\n",
      "ep 4055: ep_len:500 episode reward: total was 51.010000. running mean: 23.623596\n",
      "ep 4055: ep_len:535 episode reward: total was 24.990000. running mean: 23.637260\n",
      "epsilon:0.020184 episode_count: 28392. steps_count: 12277316.000000\n",
      "Time elapsed:  37365.68911266327\n",
      "ep 4056: ep_len:680 episode reward: total was -7.390000. running mean: 23.326987\n",
      "ep 4056: ep_len:500 episode reward: total was 22.920000. running mean: 23.322918\n",
      "ep 4056: ep_len:564 episode reward: total was -1.940000. running mean: 23.070288\n",
      "ep 4056: ep_len:550 episode reward: total was 33.330000. running mean: 23.172885\n",
      "ep 4056: ep_len:3 episode reward: total was 1.010000. running mean: 22.951257\n",
      "ep 4056: ep_len:566 episode reward: total was -64.090000. running mean: 22.080844\n",
      "ep 4056: ep_len:618 episode reward: total was 52.360000. running mean: 22.383636\n",
      "epsilon:0.020140 episode_count: 28399. steps_count: 12280797.000000\n",
      "Time elapsed:  37374.80917406082\n",
      "ep 4057: ep_len:611 episode reward: total was 68.870000. running mean: 22.848499\n",
      "ep 4057: ep_len:570 episode reward: total was 41.160000. running mean: 23.031614\n",
      "ep 4057: ep_len:500 episode reward: total was 46.750000. running mean: 23.268798\n",
      "ep 4057: ep_len:125 episode reward: total was 24.550000. running mean: 23.281610\n",
      "ep 4057: ep_len:93 episode reward: total was 28.660000. running mean: 23.335394\n",
      "ep 4057: ep_len:501 episode reward: total was 28.800000. running mean: 23.390040\n",
      "ep 4057: ep_len:529 episode reward: total was 51.390000. running mean: 23.670040\n",
      "epsilon:0.020095 episode_count: 28406. steps_count: 12283726.000000\n",
      "Time elapsed:  37382.73620533943\n",
      "ep 4058: ep_len:553 episode reward: total was 48.440000. running mean: 23.917739\n",
      "ep 4058: ep_len:540 episode reward: total was 18.040000. running mean: 23.858962\n",
      "ep 4058: ep_len:514 episode reward: total was -0.990000. running mean: 23.610472\n",
      "ep 4058: ep_len:518 episode reward: total was 26.560000. running mean: 23.639968\n",
      "ep 4058: ep_len:55 episode reward: total was 27.010000. running mean: 23.673668\n",
      "ep 4058: ep_len:553 episode reward: total was 77.710000. running mean: 24.214031\n",
      "ep 4058: ep_len:317 episode reward: total was 27.910000. running mean: 24.250991\n",
      "epsilon:0.020051 episode_count: 28413. steps_count: 12286776.000000\n",
      "Time elapsed:  37390.854748249054\n",
      "ep 4059: ep_len:515 episode reward: total was -42.170000. running mean: 23.586781\n",
      "ep 4059: ep_len:601 episode reward: total was 62.700000. running mean: 23.977913\n",
      "ep 4059: ep_len:642 episode reward: total was 17.400000. running mean: 23.912134\n",
      "ep 4059: ep_len:500 episode reward: total was 51.670000. running mean: 24.189713\n",
      "ep 4059: ep_len:52 episode reward: total was 22.510000. running mean: 24.172916\n",
      "ep 4059: ep_len:514 episode reward: total was 39.580000. running mean: 24.326986\n",
      "ep 4059: ep_len:560 episode reward: total was 44.280000. running mean: 24.526517\n",
      "epsilon:0.020007 episode_count: 28420. steps_count: 12290160.000000\n",
      "Time elapsed:  37406.22511768341\n",
      "ep 4060: ep_len:580 episode reward: total was 47.470000. running mean: 24.755951\n",
      "ep 4060: ep_len:500 episode reward: total was 121.860000. running mean: 25.726992\n",
      "ep 4060: ep_len:79 episode reward: total was 7.820000. running mean: 25.547922\n",
      "ep 4060: ep_len:623 episode reward: total was 105.820000. running mean: 26.350643\n",
      "ep 4060: ep_len:3 episode reward: total was 1.010000. running mean: 26.097236\n",
      "ep 4060: ep_len:526 episode reward: total was -33.130000. running mean: 25.504964\n",
      "ep 4060: ep_len:639 episode reward: total was 52.510000. running mean: 25.775014\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.019962 episode_count: 28427. steps_count: 12293110.000000\n",
      "Time elapsed:  37417.35930848122\n",
      "ep 4061: ep_len:500 episode reward: total was 102.090000. running mean: 26.538164\n",
      "ep 4061: ep_len:584 episode reward: total was 42.570000. running mean: 26.698483\n",
      "ep 4061: ep_len:604 episode reward: total was -0.070000. running mean: 26.430798\n",
      "ep 4061: ep_len:500 episode reward: total was 66.670000. running mean: 26.833190\n",
      "ep 4061: ep_len:3 episode reward: total was 1.010000. running mean: 26.574958\n",
      "ep 4061: ep_len:599 episode reward: total was 24.990000. running mean: 26.559108\n",
      "ep 4061: ep_len:315 episode reward: total was 21.550000. running mean: 26.509017\n",
      "epsilon:0.019918 episode_count: 28434. steps_count: 12296215.000000\n",
      "Time elapsed:  37425.10168480873\n",
      "ep 4062: ep_len:500 episode reward: total was 80.360000. running mean: 27.047527\n",
      "ep 4062: ep_len:587 episode reward: total was 62.280000. running mean: 27.399852\n",
      "ep 4062: ep_len:500 episode reward: total was 65.090000. running mean: 27.776753\n",
      "ep 4062: ep_len:109 episode reward: total was 25.890000. running mean: 27.757886\n",
      "ep 4062: ep_len:3 episode reward: total was 1.010000. running mean: 27.490407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4062: ep_len:641 episode reward: total was 18.740000. running mean: 27.402903\n",
      "ep 4062: ep_len:606 episode reward: total was 87.000000. running mean: 27.998874\n",
      "epsilon:0.019874 episode_count: 28441. steps_count: 12299161.000000\n",
      "Time elapsed:  37433.007244348526\n",
      "ep 4063: ep_len:500 episode reward: total was 112.380000. running mean: 28.842685\n",
      "ep 4063: ep_len:500 episode reward: total was 65.360000. running mean: 29.207858\n",
      "ep 4063: ep_len:456 episode reward: total was 77.370000. running mean: 29.689480\n",
      "ep 4063: ep_len:170 episode reward: total was 25.230000. running mean: 29.644885\n",
      "ep 4063: ep_len:92 episode reward: total was 24.760000. running mean: 29.596036\n",
      "ep 4063: ep_len:515 episode reward: total was -38.520000. running mean: 28.914876\n",
      "ep 4063: ep_len:500 episode reward: total was 39.250000. running mean: 29.018227\n",
      "epsilon:0.019829 episode_count: 28448. steps_count: 12301894.000000\n",
      "Time elapsed:  37440.43508148193\n",
      "ep 4064: ep_len:554 episode reward: total was 62.770000. running mean: 29.355745\n",
      "ep 4064: ep_len:603 episode reward: total was 47.390000. running mean: 29.536087\n",
      "ep 4064: ep_len:574 episode reward: total was 22.910000. running mean: 29.469826\n",
      "ep 4064: ep_len:389 episode reward: total was 38.200000. running mean: 29.557128\n",
      "ep 4064: ep_len:3 episode reward: total was 1.010000. running mean: 29.271657\n",
      "ep 4064: ep_len:500 episode reward: total was 23.360000. running mean: 29.212540\n",
      "ep 4064: ep_len:287 episode reward: total was 35.490000. running mean: 29.275315\n",
      "epsilon:0.019785 episode_count: 28455. steps_count: 12304804.000000\n",
      "Time elapsed:  37448.143194913864\n",
      "ep 4065: ep_len:513 episode reward: total was -31.420000. running mean: 28.668362\n",
      "ep 4065: ep_len:565 episode reward: total was 50.930000. running mean: 28.890978\n",
      "ep 4065: ep_len:581 episode reward: total was 18.600000. running mean: 28.788068\n",
      "ep 4065: ep_len:149 episode reward: total was 10.630000. running mean: 28.606487\n",
      "ep 4065: ep_len:3 episode reward: total was 1.010000. running mean: 28.330523\n",
      "ep 4065: ep_len:572 episode reward: total was 22.980000. running mean: 28.277017\n",
      "ep 4065: ep_len:612 episode reward: total was 53.570000. running mean: 28.529947\n",
      "epsilon:0.019741 episode_count: 28462. steps_count: 12307799.000000\n",
      "Time elapsed:  37456.05640769005\n",
      "ep 4066: ep_len:504 episode reward: total was -23.540000. running mean: 28.009248\n",
      "ep 4066: ep_len:556 episode reward: total was 126.670000. running mean: 28.995855\n",
      "ep 4066: ep_len:647 episode reward: total was -57.580000. running mean: 28.130097\n",
      "ep 4066: ep_len:520 episode reward: total was 74.150000. running mean: 28.590296\n",
      "ep 4066: ep_len:3 episode reward: total was 1.010000. running mean: 28.314493\n",
      "ep 4066: ep_len:527 episode reward: total was 23.120000. running mean: 28.262548\n",
      "ep 4066: ep_len:199 episode reward: total was 7.590000. running mean: 28.055822\n",
      "epsilon:0.019696 episode_count: 28469. steps_count: 12310755.000000\n",
      "Time elapsed:  37468.25885987282\n",
      "ep 4067: ep_len:619 episode reward: total was -13.330000. running mean: 27.641964\n",
      "ep 4067: ep_len:500 episode reward: total was 46.420000. running mean: 27.829745\n",
      "ep 4067: ep_len:78 episode reward: total was 11.820000. running mean: 27.669647\n",
      "ep 4067: ep_len:527 episode reward: total was 76.190000. running mean: 28.154851\n",
      "ep 4067: ep_len:84 episode reward: total was -49.720000. running mean: 27.376102\n",
      "ep 4067: ep_len:500 episode reward: total was 40.490000. running mean: 27.507241\n",
      "ep 4067: ep_len:500 episode reward: total was 26.320000. running mean: 27.495369\n",
      "epsilon:0.019652 episode_count: 28476. steps_count: 12313563.000000\n",
      "Time elapsed:  37472.702528715134\n",
      "ep 4068: ep_len:122 episode reward: total was 15.390000. running mean: 27.374315\n",
      "ep 4068: ep_len:576 episode reward: total was 93.970000. running mean: 28.040272\n",
      "ep 4068: ep_len:436 episode reward: total was 46.210000. running mean: 28.221969\n",
      "ep 4068: ep_len:633 episode reward: total was 70.140000. running mean: 28.641149\n",
      "ep 4068: ep_len:53 episode reward: total was 26.010000. running mean: 28.614838\n",
      "ep 4068: ep_len:623 episode reward: total was 59.080000. running mean: 28.919490\n",
      "ep 4068: ep_len:514 episode reward: total was 22.920000. running mean: 28.859495\n",
      "epsilon:0.019608 episode_count: 28483. steps_count: 12316520.000000\n",
      "Time elapsed:  37477.26347088814\n",
      "ep 4069: ep_len:548 episode reward: total was -12.500000. running mean: 28.445900\n",
      "ep 4069: ep_len:500 episode reward: total was 115.550000. running mean: 29.316941\n",
      "ep 4069: ep_len:602 episode reward: total was 22.140000. running mean: 29.245171\n",
      "ep 4069: ep_len:500 episode reward: total was 80.940000. running mean: 29.762120\n",
      "ep 4069: ep_len:117 episode reward: total was 36.220000. running mean: 29.826698\n",
      "ep 4069: ep_len:623 episode reward: total was -56.900000. running mean: 28.959431\n",
      "ep 4069: ep_len:546 episode reward: total was 47.920000. running mean: 29.149037\n",
      "epsilon:0.019563 episode_count: 28490. steps_count: 12319956.000000\n",
      "Time elapsed:  37482.541506290436\n",
      "ep 4070: ep_len:633 episode reward: total was 42.130000. running mean: 29.278847\n",
      "ep 4070: ep_len:537 episode reward: total was 118.050000. running mean: 30.166558\n",
      "ep 4070: ep_len:631 episode reward: total was -205.530000. running mean: 27.809593\n",
      "ep 4070: ep_len:110 episode reward: total was 15.520000. running mean: 27.686697\n",
      "ep 4070: ep_len:3 episode reward: total was 1.010000. running mean: 27.419930\n",
      "ep 4070: ep_len:667 episode reward: total was 35.430000. running mean: 27.500030\n",
      "ep 4070: ep_len:500 episode reward: total was 19.050000. running mean: 27.415530\n",
      "epsilon:0.019519 episode_count: 28497. steps_count: 12323037.000000\n",
      "Time elapsed:  37487.289588451385\n",
      "ep 4071: ep_len:603 episode reward: total was 53.580000. running mean: 27.677175\n",
      "ep 4071: ep_len:554 episode reward: total was 33.130000. running mean: 27.731703\n",
      "ep 4071: ep_len:566 episode reward: total was -126.830000. running mean: 26.186086\n",
      "ep 4071: ep_len:521 episode reward: total was 44.330000. running mean: 26.367525\n",
      "ep 4071: ep_len:3 episode reward: total was 1.010000. running mean: 26.113950\n",
      "ep 4071: ep_len:500 episode reward: total was 71.020000. running mean: 26.563010\n",
      "ep 4071: ep_len:193 episode reward: total was 17.550000. running mean: 26.472880\n",
      "epsilon:0.019475 episode_count: 28504. steps_count: 12325977.000000\n",
      "Time elapsed:  37499.2501707077\n",
      "ep 4072: ep_len:655 episode reward: total was -1.520000. running mean: 26.192952\n",
      "ep 4072: ep_len:500 episode reward: total was 90.270000. running mean: 26.833722\n",
      "ep 4072: ep_len:569 episode reward: total was 0.030000. running mean: 26.565685\n",
      "ep 4072: ep_len:503 episode reward: total was 34.960000. running mean: 26.649628\n",
      "ep 4072: ep_len:3 episode reward: total was 1.010000. running mean: 26.393232\n",
      "ep 4072: ep_len:602 episode reward: total was 3.110000. running mean: 26.160399\n",
      "ep 4072: ep_len:569 episode reward: total was 49.110000. running mean: 26.389895\n",
      "epsilon:0.019430 episode_count: 28511. steps_count: 12329378.000000\n",
      "Time elapsed:  37515.92197418213\n",
      "ep 4073: ep_len:617 episode reward: total was -17.450000. running mean: 25.951496\n",
      "ep 4073: ep_len:500 episode reward: total was 39.730000. running mean: 26.089281\n",
      "ep 4073: ep_len:524 episode reward: total was -152.840000. running mean: 24.299989\n",
      "ep 4073: ep_len:500 episode reward: total was 15.510000. running mean: 24.212089\n",
      "ep 4073: ep_len:3 episode reward: total was 1.010000. running mean: 23.980068\n",
      "ep 4073: ep_len:543 episode reward: total was 21.790000. running mean: 23.958167\n",
      "ep 4073: ep_len:580 episode reward: total was 53.540000. running mean: 24.253986\n",
      "epsilon:0.019386 episode_count: 28518. steps_count: 12332645.000000\n",
      "Time elapsed:  37524.66885972023\n",
      "ep 4074: ep_len:641 episode reward: total was 9.760000. running mean: 24.109046\n",
      "ep 4074: ep_len:549 episode reward: total was 131.430000. running mean: 25.182255\n",
      "ep 4074: ep_len:625 episode reward: total was 36.930000. running mean: 25.299733\n",
      "ep 4074: ep_len:607 episode reward: total was 85.330000. running mean: 25.900035\n",
      "ep 4074: ep_len:3 episode reward: total was 1.010000. running mean: 25.651135\n",
      "ep 4074: ep_len:500 episode reward: total was 34.420000. running mean: 25.738824\n",
      "ep 4074: ep_len:500 episode reward: total was 17.040000. running mean: 25.651835\n",
      "epsilon:0.019342 episode_count: 28525. steps_count: 12336070.000000\n",
      "Time elapsed:  37533.654569387436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4075: ep_len:643 episode reward: total was 51.660000. running mean: 25.911917\n",
      "ep 4075: ep_len:560 episode reward: total was 27.770000. running mean: 25.930498\n",
      "ep 4075: ep_len:627 episode reward: total was 34.450000. running mean: 26.015693\n",
      "ep 4075: ep_len:394 episode reward: total was 51.640000. running mean: 26.271936\n",
      "ep 4075: ep_len:3 episode reward: total was 1.010000. running mean: 26.019317\n",
      "ep 4075: ep_len:172 episode reward: total was 26.790000. running mean: 26.027023\n",
      "ep 4075: ep_len:603 episode reward: total was 15.040000. running mean: 25.917153\n",
      "epsilon:0.019297 episode_count: 28532. steps_count: 12339072.000000\n",
      "Time elapsed:  37546.56658530235\n",
      "ep 4076: ep_len:503 episode reward: total was 78.040000. running mean: 26.438382\n",
      "ep 4076: ep_len:513 episode reward: total was 30.110000. running mean: 26.475098\n",
      "ep 4076: ep_len:510 episode reward: total was -13.790000. running mean: 26.072447\n",
      "ep 4076: ep_len:517 episode reward: total was 69.380000. running mean: 26.505522\n",
      "ep 4076: ep_len:89 episode reward: total was -11.210000. running mean: 26.128367\n",
      "ep 4076: ep_len:540 episode reward: total was 21.910000. running mean: 26.086184\n",
      "ep 4076: ep_len:500 episode reward: total was 11.150000. running mean: 25.936822\n",
      "epsilon:0.019253 episode_count: 28539. steps_count: 12342244.000000\n",
      "Time elapsed:  37555.00615954399\n",
      "ep 4077: ep_len:265 episode reward: total was 25.920000. running mean: 25.936653\n",
      "ep 4077: ep_len:546 episode reward: total was -13.980000. running mean: 25.537487\n",
      "ep 4077: ep_len:592 episode reward: total was 12.500000. running mean: 25.407112\n",
      "ep 4077: ep_len:500 episode reward: total was 30.760000. running mean: 25.460641\n",
      "ep 4077: ep_len:88 episode reward: total was 25.760000. running mean: 25.463635\n",
      "ep 4077: ep_len:512 episode reward: total was 46.380000. running mean: 25.672798\n",
      "ep 4077: ep_len:541 episode reward: total was -7.320000. running mean: 25.342870\n",
      "epsilon:0.019209 episode_count: 28546. steps_count: 12345288.000000\n",
      "Time elapsed:  37563.06345963478\n",
      "ep 4078: ep_len:531 episode reward: total was 19.840000. running mean: 25.287841\n",
      "ep 4078: ep_len:331 episode reward: total was 5.410000. running mean: 25.089063\n",
      "ep 4078: ep_len:556 episode reward: total was 27.170000. running mean: 25.109872\n",
      "ep 4078: ep_len:548 episode reward: total was 33.950000. running mean: 25.198274\n",
      "ep 4078: ep_len:53 episode reward: total was 23.010000. running mean: 25.176391\n",
      "ep 4078: ep_len:565 episode reward: total was 28.570000. running mean: 25.210327\n",
      "ep 4078: ep_len:535 episode reward: total was 60.480000. running mean: 25.563024\n",
      "epsilon:0.019164 episode_count: 28553. steps_count: 12348407.000000\n",
      "Time elapsed:  37571.300327301025\n",
      "ep 4079: ep_len:199 episode reward: total was 21.270000. running mean: 25.520094\n",
      "ep 4079: ep_len:770 episode reward: total was -81.800000. running mean: 24.446893\n",
      "ep 4079: ep_len:587 episode reward: total was 21.690000. running mean: 24.419324\n",
      "ep 4079: ep_len:520 episode reward: total was 68.310000. running mean: 24.858230\n",
      "ep 4079: ep_len:86 episode reward: total was 22.770000. running mean: 24.837348\n",
      "ep 4079: ep_len:606 episode reward: total was 30.270000. running mean: 24.891675\n",
      "ep 4079: ep_len:500 episode reward: total was 53.610000. running mean: 25.178858\n",
      "epsilon:0.019120 episode_count: 28560. steps_count: 12351675.000000\n",
      "Time elapsed:  37586.29142808914\n",
      "ep 4080: ep_len:571 episode reward: total was 82.500000. running mean: 25.752069\n",
      "ep 4080: ep_len:361 episode reward: total was -130.290000. running mean: 24.191649\n",
      "ep 4080: ep_len:603 episode reward: total was -11.290000. running mean: 23.836832\n",
      "ep 4080: ep_len:579 episode reward: total was 94.350000. running mean: 24.541964\n",
      "ep 4080: ep_len:3 episode reward: total was 1.010000. running mean: 24.306644\n",
      "ep 4080: ep_len:500 episode reward: total was -78.450000. running mean: 23.279078\n",
      "ep 4080: ep_len:601 episode reward: total was 43.740000. running mean: 23.483687\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.019076 episode_count: 28567. steps_count: 12354893.000000\n",
      "Time elapsed:  37606.97155690193\n",
      "ep 4081: ep_len:557 episode reward: total was 94.520000. running mean: 24.194050\n",
      "ep 4081: ep_len:501 episode reward: total was -33.010000. running mean: 23.622010\n",
      "ep 4081: ep_len:646 episode reward: total was 57.690000. running mean: 23.962690\n",
      "ep 4081: ep_len:500 episode reward: total was 50.140000. running mean: 24.224463\n",
      "ep 4081: ep_len:96 episode reward: total was 29.760000. running mean: 24.279818\n",
      "ep 4081: ep_len:612 episode reward: total was 24.080000. running mean: 24.277820\n",
      "ep 4081: ep_len:537 episode reward: total was 13.110000. running mean: 24.166142\n",
      "epsilon:0.019031 episode_count: 28574. steps_count: 12358342.000000\n",
      "Time elapsed:  37616.64182472229\n",
      "ep 4082: ep_len:671 episode reward: total was -8.270000. running mean: 23.841780\n",
      "ep 4082: ep_len:583 episode reward: total was 56.210000. running mean: 24.165462\n",
      "ep 4082: ep_len:540 episode reward: total was 14.980000. running mean: 24.073608\n",
      "ep 4082: ep_len:581 episode reward: total was 97.340000. running mean: 24.806272\n",
      "ep 4082: ep_len:91 episode reward: total was 20.740000. running mean: 24.765609\n",
      "ep 4082: ep_len:525 episode reward: total was 2.850000. running mean: 24.546453\n",
      "ep 4082: ep_len:181 episode reward: total was 8.550000. running mean: 24.386488\n",
      "epsilon:0.018987 episode_count: 28581. steps_count: 12361514.000000\n",
      "Time elapsed:  37640.1237487793\n",
      "ep 4083: ep_len:604 episode reward: total was 28.950000. running mean: 24.432123\n",
      "ep 4083: ep_len:598 episode reward: total was 31.570000. running mean: 24.503502\n",
      "ep 4083: ep_len:565 episode reward: total was 26.060000. running mean: 24.519067\n",
      "ep 4083: ep_len:500 episode reward: total was 33.300000. running mean: 24.606877\n",
      "ep 4083: ep_len:3 episode reward: total was 1.010000. running mean: 24.370908\n",
      "ep 4083: ep_len:555 episode reward: total was 20.950000. running mean: 24.336699\n",
      "ep 4083: ep_len:579 episode reward: total was 35.250000. running mean: 24.445832\n",
      "epsilon:0.018943 episode_count: 28588. steps_count: 12364918.000000\n",
      "Time elapsed:  37656.43201828003\n",
      "ep 4084: ep_len:528 episode reward: total was 82.850000. running mean: 25.029873\n",
      "ep 4084: ep_len:516 episode reward: total was 44.760000. running mean: 25.227175\n",
      "ep 4084: ep_len:629 episode reward: total was 17.910000. running mean: 25.154003\n",
      "ep 4084: ep_len:512 episode reward: total was 30.700000. running mean: 25.209463\n",
      "ep 4084: ep_len:126 episode reward: total was 32.360000. running mean: 25.280968\n",
      "ep 4084: ep_len:666 episode reward: total was 18.040000. running mean: 25.208559\n",
      "ep 4084: ep_len:616 episode reward: total was 39.910000. running mean: 25.355573\n",
      "epsilon:0.018898 episode_count: 28595. steps_count: 12368511.000000\n",
      "Time elapsed:  37665.879789829254\n",
      "ep 4085: ep_len:210 episode reward: total was 19.810000. running mean: 25.300117\n",
      "ep 4085: ep_len:500 episode reward: total was 40.050000. running mean: 25.447616\n",
      "ep 4085: ep_len:668 episode reward: total was 9.240000. running mean: 25.285540\n",
      "ep 4085: ep_len:500 episode reward: total was 57.970000. running mean: 25.612385\n",
      "ep 4085: ep_len:3 episode reward: total was 1.010000. running mean: 25.366361\n",
      "ep 4085: ep_len:669 episode reward: total was 40.840000. running mean: 25.521097\n",
      "ep 4085: ep_len:268 episode reward: total was -1.330000. running mean: 25.252586\n",
      "epsilon:0.018854 episode_count: 28602. steps_count: 12371329.000000\n",
      "Time elapsed:  37673.479524850845\n",
      "ep 4086: ep_len:585 episode reward: total was 82.250000. running mean: 25.822560\n",
      "ep 4086: ep_len:500 episode reward: total was 116.990000. running mean: 26.734235\n",
      "ep 4086: ep_len:642 episode reward: total was 6.470000. running mean: 26.531592\n",
      "ep 4086: ep_len:554 episode reward: total was 79.680000. running mean: 27.063076\n",
      "ep 4086: ep_len:98 episode reward: total was 36.120000. running mean: 27.153646\n",
      "ep 4086: ep_len:620 episode reward: total was 63.590000. running mean: 27.518009\n",
      "ep 4086: ep_len:573 episode reward: total was -0.330000. running mean: 27.239529\n",
      "epsilon:0.018810 episode_count: 28609. steps_count: 12374901.000000\n",
      "Time elapsed:  37682.66635084152\n",
      "ep 4087: ep_len:582 episode reward: total was 44.410000. running mean: 27.411234\n",
      "ep 4087: ep_len:175 episode reward: total was 12.840000. running mean: 27.265521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4087: ep_len:421 episode reward: total was 76.070000. running mean: 27.753566\n",
      "ep 4087: ep_len:542 episode reward: total was 48.070000. running mean: 27.956731\n",
      "ep 4087: ep_len:3 episode reward: total was 1.010000. running mean: 27.687263\n",
      "ep 4087: ep_len:542 episode reward: total was 10.470000. running mean: 27.515091\n",
      "ep 4087: ep_len:594 episode reward: total was 52.500000. running mean: 27.764940\n",
      "epsilon:0.018765 episode_count: 28616. steps_count: 12377760.000000\n",
      "Time elapsed:  37690.33583164215\n",
      "ep 4088: ep_len:228 episode reward: total was 13.910000. running mean: 27.626390\n",
      "ep 4088: ep_len:617 episode reward: total was 89.490000. running mean: 28.245026\n",
      "ep 4088: ep_len:354 episode reward: total was 52.480000. running mean: 28.487376\n",
      "ep 4088: ep_len:505 episode reward: total was 11.710000. running mean: 28.319602\n",
      "ep 4088: ep_len:3 episode reward: total was 1.010000. running mean: 28.046506\n",
      "ep 4088: ep_len:525 episode reward: total was -175.650000. running mean: 26.009541\n",
      "ep 4088: ep_len:290 episode reward: total was 15.240000. running mean: 25.901846\n",
      "epsilon:0.018721 episode_count: 28623. steps_count: 12380282.000000\n",
      "Time elapsed:  37697.294603824615\n",
      "ep 4089: ep_len:229 episode reward: total was 16.820000. running mean: 25.811027\n",
      "ep 4089: ep_len:530 episode reward: total was 2.420000. running mean: 25.577117\n",
      "ep 4089: ep_len:500 episode reward: total was 28.160000. running mean: 25.602946\n",
      "ep 4089: ep_len:516 episode reward: total was 73.390000. running mean: 26.080817\n",
      "ep 4089: ep_len:100 episode reward: total was 34.270000. running mean: 26.162708\n",
      "ep 4089: ep_len:500 episode reward: total was 59.500000. running mean: 26.496081\n",
      "ep 4089: ep_len:504 episode reward: total was 16.560000. running mean: 26.396720\n",
      "epsilon:0.018677 episode_count: 28630. steps_count: 12383161.000000\n",
      "Time elapsed:  37712.07977628708\n",
      "ep 4090: ep_len:622 episode reward: total was 56.710000. running mean: 26.699853\n",
      "ep 4090: ep_len:535 episode reward: total was 27.170000. running mean: 26.704555\n",
      "ep 4090: ep_len:614 episode reward: total was -12.330000. running mean: 26.314209\n",
      "ep 4090: ep_len:551 episode reward: total was 41.820000. running mean: 26.469267\n",
      "ep 4090: ep_len:3 episode reward: total was 1.010000. running mean: 26.214674\n",
      "ep 4090: ep_len:577 episode reward: total was 33.130000. running mean: 26.283828\n",
      "ep 4090: ep_len:612 episode reward: total was 65.390000. running mean: 26.674889\n",
      "epsilon:0.018632 episode_count: 28637. steps_count: 12386675.000000\n",
      "Time elapsed:  37721.24756145477\n",
      "ep 4091: ep_len:229 episode reward: total was 39.900000. running mean: 26.807141\n",
      "ep 4091: ep_len:372 episode reward: total was 26.730000. running mean: 26.806369\n",
      "ep 4091: ep_len:648 episode reward: total was 8.790000. running mean: 26.626205\n",
      "ep 4091: ep_len:500 episode reward: total was 35.430000. running mean: 26.714243\n",
      "ep 4091: ep_len:103 episode reward: total was 33.260000. running mean: 26.779701\n",
      "ep 4091: ep_len:500 episode reward: total was 57.190000. running mean: 27.083804\n",
      "ep 4091: ep_len:257 episode reward: total was 29.380000. running mean: 27.106766\n",
      "epsilon:0.018588 episode_count: 28644. steps_count: 12389284.000000\n",
      "Time elapsed:  37728.40239858627\n",
      "ep 4092: ep_len:650 episode reward: total was -28.930000. running mean: 26.546398\n",
      "ep 4092: ep_len:505 episode reward: total was 76.500000. running mean: 27.045934\n",
      "ep 4092: ep_len:500 episode reward: total was 67.940000. running mean: 27.454875\n",
      "ep 4092: ep_len:599 episode reward: total was 72.000000. running mean: 27.900326\n",
      "ep 4092: ep_len:3 episode reward: total was 1.010000. running mean: 27.631423\n",
      "ep 4092: ep_len:315 episode reward: total was 25.290000. running mean: 27.608009\n",
      "ep 4092: ep_len:500 episode reward: total was 20.480000. running mean: 27.536729\n",
      "epsilon:0.018544 episode_count: 28651. steps_count: 12392356.000000\n",
      "Time elapsed:  37736.49855470657\n",
      "ep 4093: ep_len:593 episode reward: total was 75.360000. running mean: 28.014961\n",
      "ep 4093: ep_len:500 episode reward: total was 57.020000. running mean: 28.305012\n",
      "ep 4093: ep_len:543 episode reward: total was 34.510000. running mean: 28.367062\n",
      "ep 4093: ep_len:170 episode reward: total was 26.240000. running mean: 28.345791\n",
      "ep 4093: ep_len:49 episode reward: total was 23.000000. running mean: 28.292333\n",
      "ep 4093: ep_len:179 episode reward: total was 28.240000. running mean: 28.291810\n",
      "ep 4093: ep_len:549 episode reward: total was 60.580000. running mean: 28.614692\n",
      "epsilon:0.018499 episode_count: 28658. steps_count: 12394939.000000\n",
      "Time elapsed:  37743.53662252426\n",
      "ep 4094: ep_len:500 episode reward: total was 11.010000. running mean: 28.438645\n",
      "ep 4094: ep_len:544 episode reward: total was 75.740000. running mean: 28.911658\n",
      "ep 4094: ep_len:530 episode reward: total was -2.910000. running mean: 28.593442\n",
      "ep 4094: ep_len:566 episode reward: total was 97.800000. running mean: 29.285507\n",
      "ep 4094: ep_len:81 episode reward: total was 24.770000. running mean: 29.240352\n",
      "ep 4094: ep_len:588 episode reward: total was 55.740000. running mean: 29.505349\n",
      "ep 4094: ep_len:500 episode reward: total was 35.580000. running mean: 29.566095\n",
      "epsilon:0.018455 episode_count: 28665. steps_count: 12398248.000000\n",
      "Time elapsed:  37759.527316093445\n",
      "ep 4095: ep_len:550 episode reward: total was 70.850000. running mean: 29.978934\n",
      "ep 4095: ep_len:575 episode reward: total was 46.450000. running mean: 30.143645\n",
      "ep 4095: ep_len:585 episode reward: total was 73.370000. running mean: 30.575908\n",
      "ep 4095: ep_len:543 episode reward: total was 59.090000. running mean: 30.861049\n",
      "ep 4095: ep_len:85 episode reward: total was -58.740000. running mean: 29.965039\n",
      "ep 4095: ep_len:500 episode reward: total was 56.300000. running mean: 30.228388\n",
      "ep 4095: ep_len:200 episode reward: total was 13.580000. running mean: 30.061905\n",
      "epsilon:0.018411 episode_count: 28672. steps_count: 12401286.000000\n",
      "Time elapsed:  37773.86257958412\n",
      "ep 4096: ep_len:622 episode reward: total was -11.490000. running mean: 29.646386\n",
      "ep 4096: ep_len:628 episode reward: total was 63.890000. running mean: 29.988822\n",
      "ep 4096: ep_len:542 episode reward: total was 27.140000. running mean: 29.960333\n",
      "ep 4096: ep_len:518 episode reward: total was 22.130000. running mean: 29.882030\n",
      "ep 4096: ep_len:3 episode reward: total was 1.010000. running mean: 29.593310\n",
      "ep 4096: ep_len:514 episode reward: total was -80.560000. running mean: 28.491777\n",
      "ep 4096: ep_len:536 episode reward: total was 23.840000. running mean: 28.445259\n",
      "epsilon:0.018366 episode_count: 28679. steps_count: 12404649.000000\n",
      "Time elapsed:  37795.63028359413\n",
      "ep 4097: ep_len:500 episode reward: total was 54.380000. running mean: 28.704606\n",
      "ep 4097: ep_len:262 episode reward: total was -24.850000. running mean: 28.169060\n",
      "ep 4097: ep_len:500 episode reward: total was 40.440000. running mean: 28.291770\n",
      "ep 4097: ep_len:534 episode reward: total was 64.820000. running mean: 28.657052\n",
      "ep 4097: ep_len:88 episode reward: total was 27.230000. running mean: 28.642781\n",
      "ep 4097: ep_len:527 episode reward: total was 41.420000. running mean: 28.770554\n",
      "ep 4097: ep_len:563 episode reward: total was 45.290000. running mean: 28.935748\n",
      "epsilon:0.018322 episode_count: 28686. steps_count: 12407623.000000\n",
      "Time elapsed:  37800.288028001785\n",
      "ep 4098: ep_len:550 episode reward: total was 43.170000. running mean: 29.078091\n",
      "ep 4098: ep_len:606 episode reward: total was 56.890000. running mean: 29.356210\n",
      "ep 4098: ep_len:500 episode reward: total was 8.750000. running mean: 29.150148\n",
      "ep 4098: ep_len:428 episode reward: total was -57.030000. running mean: 28.288346\n",
      "ep 4098: ep_len:3 episode reward: total was 1.010000. running mean: 28.015563\n",
      "ep 4098: ep_len:511 episode reward: total was -9.310000. running mean: 27.642307\n",
      "ep 4098: ep_len:500 episode reward: total was 1.630000. running mean: 27.382184\n",
      "epsilon:0.018278 episode_count: 28693. steps_count: 12410721.000000\n",
      "Time elapsed:  37821.604148864746\n",
      "ep 4099: ep_len:663 episode reward: total was -24.820000. running mean: 26.860162\n",
      "ep 4099: ep_len:500 episode reward: total was 49.120000. running mean: 27.082761\n",
      "ep 4099: ep_len:500 episode reward: total was 37.450000. running mean: 27.186433\n",
      "ep 4099: ep_len:523 episode reward: total was 28.310000. running mean: 27.197669\n",
      "ep 4099: ep_len:3 episode reward: total was 1.010000. running mean: 26.935792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4099: ep_len:547 episode reward: total was 56.280000. running mean: 27.229234\n",
      "ep 4099: ep_len:503 episode reward: total was 43.020000. running mean: 27.387142\n",
      "epsilon:0.018233 episode_count: 28700. steps_count: 12413960.000000\n",
      "Time elapsed:  37842.27021694183\n",
      "ep 4100: ep_len:525 episode reward: total was 58.030000. running mean: 27.693570\n",
      "ep 4100: ep_len:560 episode reward: total was 27.180000. running mean: 27.688435\n",
      "ep 4100: ep_len:525 episode reward: total was 28.790000. running mean: 27.699450\n",
      "ep 4100: ep_len:154 episode reward: total was 13.100000. running mean: 27.553456\n",
      "ep 4100: ep_len:54 episode reward: total was 25.010000. running mean: 27.528021\n",
      "ep 4100: ep_len:545 episode reward: total was -9.450000. running mean: 27.158241\n",
      "ep 4100: ep_len:505 episode reward: total was -49.020000. running mean: 26.396459\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.018189 episode_count: 28707. steps_count: 12416828.000000\n",
      "Time elapsed:  37853.85223579407\n",
      "ep 4101: ep_len:643 episode reward: total was -30.720000. running mean: 25.825294\n",
      "ep 4101: ep_len:615 episode reward: total was 18.280000. running mean: 25.749841\n",
      "ep 4101: ep_len:500 episode reward: total was 61.390000. running mean: 26.106243\n",
      "ep 4101: ep_len:500 episode reward: total was 10.280000. running mean: 25.947980\n",
      "ep 4101: ep_len:35 episode reward: total was 16.000000. running mean: 25.848500\n",
      "ep 4101: ep_len:639 episode reward: total was 44.680000. running mean: 26.036815\n",
      "ep 4101: ep_len:556 episode reward: total was 35.090000. running mean: 26.127347\n",
      "epsilon:0.018145 episode_count: 28714. steps_count: 12420316.000000\n",
      "Time elapsed:  37869.79672074318\n",
      "ep 4102: ep_len:605 episode reward: total was -58.400000. running mean: 25.282074\n",
      "ep 4102: ep_len:540 episode reward: total was 117.470000. running mean: 26.203953\n",
      "ep 4102: ep_len:500 episode reward: total was 10.600000. running mean: 26.047913\n",
      "ep 4102: ep_len:613 episode reward: total was 75.050000. running mean: 26.537934\n",
      "ep 4102: ep_len:3 episode reward: total was 1.010000. running mean: 26.282655\n",
      "ep 4102: ep_len:576 episode reward: total was 19.720000. running mean: 26.217028\n",
      "ep 4102: ep_len:294 episode reward: total was 11.300000. running mean: 26.067858\n",
      "epsilon:0.018100 episode_count: 28721. steps_count: 12423447.000000\n",
      "Time elapsed:  37878.161950588226\n",
      "ep 4103: ep_len:520 episode reward: total was 0.920000. running mean: 25.816380\n",
      "ep 4103: ep_len:627 episode reward: total was 82.610000. running mean: 26.384316\n",
      "ep 4103: ep_len:635 episode reward: total was 53.380000. running mean: 26.654273\n",
      "ep 4103: ep_len:561 episode reward: total was 56.330000. running mean: 26.951030\n",
      "ep 4103: ep_len:3 episode reward: total was 1.010000. running mean: 26.691620\n",
      "ep 4103: ep_len:292 episode reward: total was 43.150000. running mean: 26.856203\n",
      "ep 4103: ep_len:568 episode reward: total was 7.620000. running mean: 26.663841\n",
      "epsilon:0.018056 episode_count: 28728. steps_count: 12426653.000000\n",
      "Time elapsed:  37886.55553483963\n",
      "ep 4104: ep_len:222 episode reward: total was 14.800000. running mean: 26.545203\n",
      "ep 4104: ep_len:341 episode reward: total was -219.480000. running mean: 24.084951\n",
      "ep 4104: ep_len:581 episode reward: total was -21.410000. running mean: 23.630001\n",
      "ep 4104: ep_len:506 episode reward: total was 32.930000. running mean: 23.723001\n",
      "ep 4104: ep_len:95 episode reward: total was 11.720000. running mean: 23.602971\n",
      "ep 4104: ep_len:613 episode reward: total was 13.950000. running mean: 23.506442\n",
      "ep 4104: ep_len:566 episode reward: total was 9.760000. running mean: 23.368977\n",
      "epsilon:0.018012 episode_count: 28735. steps_count: 12429577.000000\n",
      "Time elapsed:  37894.29653763771\n",
      "ep 4105: ep_len:635 episode reward: total was 56.840000. running mean: 23.703687\n",
      "ep 4105: ep_len:565 episode reward: total was 19.540000. running mean: 23.662051\n",
      "ep 4105: ep_len:533 episode reward: total was 3.950000. running mean: 23.464930\n",
      "ep 4105: ep_len:530 episode reward: total was 14.970000. running mean: 23.379981\n",
      "ep 4105: ep_len:3 episode reward: total was -1.500000. running mean: 23.131181\n",
      "ep 4105: ep_len:615 episode reward: total was -13.420000. running mean: 22.765669\n",
      "ep 4105: ep_len:615 episode reward: total was 78.030000. running mean: 23.318312\n",
      "epsilon:0.017967 episode_count: 28742. steps_count: 12433073.000000\n",
      "Time elapsed:  37911.326416015625\n",
      "ep 4106: ep_len:579 episode reward: total was 54.610000. running mean: 23.631229\n",
      "ep 4106: ep_len:592 episode reward: total was 23.890000. running mean: 23.633817\n",
      "ep 4106: ep_len:554 episode reward: total was 19.410000. running mean: 23.591579\n",
      "ep 4106: ep_len:500 episode reward: total was 82.340000. running mean: 24.179063\n",
      "ep 4106: ep_len:74 episode reward: total was 29.080000. running mean: 24.228072\n",
      "ep 4106: ep_len:508 episode reward: total was -21.120000. running mean: 23.774592\n",
      "ep 4106: ep_len:285 episode reward: total was 4.410000. running mean: 23.580946\n",
      "epsilon:0.017923 episode_count: 28749. steps_count: 12436165.000000\n",
      "Time elapsed:  37919.53002810478\n",
      "ep 4107: ep_len:216 episode reward: total was 15.680000. running mean: 23.501936\n",
      "ep 4107: ep_len:500 episode reward: total was 32.380000. running mean: 23.590717\n",
      "ep 4107: ep_len:513 episode reward: total was 40.630000. running mean: 23.761110\n",
      "ep 4107: ep_len:507 episode reward: total was 38.960000. running mean: 23.913099\n",
      "ep 4107: ep_len:3 episode reward: total was 1.010000. running mean: 23.684068\n",
      "ep 4107: ep_len:250 episode reward: total was 44.520000. running mean: 23.892427\n",
      "ep 4107: ep_len:519 episode reward: total was 39.120000. running mean: 24.044703\n",
      "epsilon:0.017879 episode_count: 28756. steps_count: 12438673.000000\n",
      "Time elapsed:  37926.346814394\n",
      "ep 4108: ep_len:567 episode reward: total was -17.880000. running mean: 23.625456\n",
      "ep 4108: ep_len:500 episode reward: total was 103.920000. running mean: 24.428401\n",
      "ep 4108: ep_len:500 episode reward: total was 19.560000. running mean: 24.379717\n",
      "ep 4108: ep_len:500 episode reward: total was 57.540000. running mean: 24.711320\n",
      "ep 4108: ep_len:114 episode reward: total was 36.250000. running mean: 24.826707\n",
      "ep 4108: ep_len:537 episode reward: total was 37.660000. running mean: 24.955040\n",
      "ep 4108: ep_len:555 episode reward: total was 29.840000. running mean: 25.003889\n",
      "epsilon:0.017834 episode_count: 28763. steps_count: 12441946.000000\n",
      "Time elapsed:  37941.62599682808\n",
      "ep 4109: ep_len:213 episode reward: total was 30.190000. running mean: 25.055750\n",
      "ep 4109: ep_len:517 episode reward: total was 85.150000. running mean: 25.656693\n",
      "ep 4109: ep_len:523 episode reward: total was 8.830000. running mean: 25.488426\n",
      "ep 4109: ep_len:502 episode reward: total was 80.080000. running mean: 26.034342\n",
      "ep 4109: ep_len:3 episode reward: total was 0.000000. running mean: 25.773998\n",
      "ep 4109: ep_len:568 episode reward: total was 54.230000. running mean: 26.058558\n",
      "ep 4109: ep_len:315 episode reward: total was 30.610000. running mean: 26.104073\n",
      "epsilon:0.017790 episode_count: 28770. steps_count: 12444587.000000\n",
      "Time elapsed:  37949.317682266235\n",
      "ep 4110: ep_len:500 episode reward: total was 81.150000. running mean: 26.654532\n",
      "ep 4110: ep_len:620 episode reward: total was 107.250000. running mean: 27.460487\n",
      "ep 4110: ep_len:392 episode reward: total was 61.680000. running mean: 27.802682\n",
      "ep 4110: ep_len:518 episode reward: total was 62.510000. running mean: 28.149755\n",
      "ep 4110: ep_len:3 episode reward: total was 1.010000. running mean: 27.878358\n",
      "ep 4110: ep_len:522 episode reward: total was 40.170000. running mean: 28.001274\n",
      "ep 4110: ep_len:314 episode reward: total was 21.020000. running mean: 27.931461\n",
      "epsilon:0.017746 episode_count: 28777. steps_count: 12447456.000000\n",
      "Time elapsed:  37956.944013834\n",
      "ep 4111: ep_len:538 episode reward: total was -1.000000. running mean: 27.642147\n",
      "ep 4111: ep_len:511 episode reward: total was 111.640000. running mean: 28.482125\n",
      "ep 4111: ep_len:448 episode reward: total was 70.860000. running mean: 28.905904\n",
      "ep 4111: ep_len:56 episode reward: total was -4.640000. running mean: 28.570445\n",
      "ep 4111: ep_len:56 episode reward: total was 27.510000. running mean: 28.559840\n",
      "ep 4111: ep_len:503 episode reward: total was 23.450000. running mean: 28.508742\n",
      "ep 4111: ep_len:551 episode reward: total was 12.060000. running mean: 28.344255\n",
      "epsilon:0.017701 episode_count: 28784. steps_count: 12450119.000000\n",
      "Time elapsed:  37964.007276296616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4112: ep_len:542 episode reward: total was -1.340000. running mean: 28.047412\n",
      "ep 4112: ep_len:595 episode reward: total was 75.740000. running mean: 28.524338\n",
      "ep 4112: ep_len:507 episode reward: total was 56.450000. running mean: 28.803595\n",
      "ep 4112: ep_len:528 episode reward: total was 35.570000. running mean: 28.871259\n",
      "ep 4112: ep_len:86 episode reward: total was 23.780000. running mean: 28.820346\n",
      "ep 4112: ep_len:570 episode reward: total was 33.910000. running mean: 28.871243\n",
      "ep 4112: ep_len:623 episode reward: total was 56.490000. running mean: 29.147430\n",
      "epsilon:0.017657 episode_count: 28791. steps_count: 12453570.000000\n",
      "Time elapsed:  37980.81035423279\n",
      "ep 4113: ep_len:534 episode reward: total was 2.040000. running mean: 28.876356\n",
      "ep 4113: ep_len:530 episode reward: total was 46.470000. running mean: 29.052292\n",
      "ep 4113: ep_len:615 episode reward: total was 48.650000. running mean: 29.248269\n",
      "ep 4113: ep_len:510 episode reward: total was 95.940000. running mean: 29.915187\n",
      "ep 4113: ep_len:3 episode reward: total was 1.010000. running mean: 29.626135\n",
      "ep 4113: ep_len:243 episode reward: total was 28.530000. running mean: 29.615173\n",
      "ep 4113: ep_len:530 episode reward: total was -20.660000. running mean: 29.112422\n",
      "epsilon:0.017613 episode_count: 28798. steps_count: 12456535.000000\n",
      "Time elapsed:  37988.65113258362\n",
      "ep 4114: ep_len:531 episode reward: total was -7.080000. running mean: 28.750497\n",
      "ep 4114: ep_len:383 episode reward: total was 34.740000. running mean: 28.810392\n",
      "ep 4114: ep_len:518 episode reward: total was 52.230000. running mean: 29.044589\n",
      "ep 4114: ep_len:52 episode reward: total was -4.680000. running mean: 28.707343\n",
      "ep 4114: ep_len:82 episode reward: total was 20.740000. running mean: 28.627669\n",
      "ep 4114: ep_len:287 episode reward: total was 33.000000. running mean: 28.671393\n",
      "ep 4114: ep_len:500 episode reward: total was 21.180000. running mean: 28.596479\n",
      "epsilon:0.017568 episode_count: 28805. steps_count: 12458888.000000\n",
      "Time elapsed:  37995.078016757965\n",
      "ep 4115: ep_len:506 episode reward: total was -10.250000. running mean: 28.208014\n",
      "ep 4115: ep_len:511 episode reward: total was 57.050000. running mean: 28.496434\n",
      "ep 4115: ep_len:500 episode reward: total was 52.410000. running mean: 28.735569\n",
      "ep 4115: ep_len:567 episode reward: total was 74.640000. running mean: 29.194614\n",
      "ep 4115: ep_len:3 episode reward: total was 1.010000. running mean: 28.912768\n",
      "ep 4115: ep_len:508 episode reward: total was 53.190000. running mean: 29.155540\n",
      "ep 4115: ep_len:544 episode reward: total was -34.230000. running mean: 28.521684\n",
      "epsilon:0.017524 episode_count: 28812. steps_count: 12462027.000000\n",
      "Time elapsed:  38016.03393292427\n",
      "ep 4116: ep_len:647 episode reward: total was -223.710000. running mean: 25.999368\n",
      "ep 4116: ep_len:170 episode reward: total was 7.280000. running mean: 25.812174\n",
      "ep 4116: ep_len:610 episode reward: total was 44.460000. running mean: 25.998652\n",
      "ep 4116: ep_len:500 episode reward: total was 29.140000. running mean: 26.030066\n",
      "ep 4116: ep_len:3 episode reward: total was 1.010000. running mean: 25.779865\n",
      "ep 4116: ep_len:500 episode reward: total was 33.490000. running mean: 25.856966\n",
      "ep 4116: ep_len:297 episode reward: total was 33.580000. running mean: 25.934197\n",
      "epsilon:0.017480 episode_count: 28819. steps_count: 12464754.000000\n",
      "Time elapsed:  38025.44316649437\n",
      "ep 4117: ep_len:500 episode reward: total was 87.760000. running mean: 26.552455\n",
      "ep 4117: ep_len:348 episode reward: total was 23.580000. running mean: 26.522730\n",
      "ep 4117: ep_len:70 episode reward: total was 11.740000. running mean: 26.374903\n",
      "ep 4117: ep_len:511 episode reward: total was 49.300000. running mean: 26.604154\n",
      "ep 4117: ep_len:3 episode reward: total was 1.010000. running mean: 26.348212\n",
      "ep 4117: ep_len:564 episode reward: total was 30.010000. running mean: 26.384830\n",
      "ep 4117: ep_len:500 episode reward: total was -34.780000. running mean: 25.773182\n",
      "epsilon:0.017435 episode_count: 28826. steps_count: 12467250.000000\n",
      "Time elapsed:  38032.37959766388\n",
      "ep 4118: ep_len:566 episode reward: total was 61.910000. running mean: 26.134550\n",
      "ep 4118: ep_len:548 episode reward: total was 123.120000. running mean: 27.104405\n",
      "ep 4118: ep_len:610 episode reward: total was 58.450000. running mean: 27.417861\n",
      "ep 4118: ep_len:506 episode reward: total was 50.100000. running mean: 27.644682\n",
      "ep 4118: ep_len:3 episode reward: total was 1.010000. running mean: 27.378335\n",
      "ep 4118: ep_len:603 episode reward: total was 12.510000. running mean: 27.229652\n",
      "ep 4118: ep_len:522 episode reward: total was -125.300000. running mean: 25.704355\n",
      "epsilon:0.017391 episode_count: 28833. steps_count: 12470608.000000\n",
      "Time elapsed:  38048.313281297684\n",
      "ep 4119: ep_len:229 episode reward: total was 29.830000. running mean: 25.745612\n",
      "ep 4119: ep_len:500 episode reward: total was 32.900000. running mean: 25.817156\n",
      "ep 4119: ep_len:439 episode reward: total was 72.360000. running mean: 26.282584\n",
      "ep 4119: ep_len:617 episode reward: total was 75.150000. running mean: 26.771258\n",
      "ep 4119: ep_len:3 episode reward: total was 1.010000. running mean: 26.513646\n",
      "ep 4119: ep_len:500 episode reward: total was 58.530000. running mean: 26.833809\n",
      "ep 4119: ep_len:504 episode reward: total was 12.520000. running mean: 26.690671\n",
      "epsilon:0.017347 episode_count: 28840. steps_count: 12473400.000000\n",
      "Time elapsed:  38055.76413536072\n",
      "ep 4120: ep_len:572 episode reward: total was 74.870000. running mean: 27.172464\n",
      "ep 4120: ep_len:330 episode reward: total was 18.500000. running mean: 27.085740\n",
      "ep 4120: ep_len:441 episode reward: total was 62.520000. running mean: 27.440082\n",
      "ep 4120: ep_len:609 episode reward: total was 68.090000. running mean: 27.846581\n",
      "ep 4120: ep_len:87 episode reward: total was 24.250000. running mean: 27.810616\n",
      "ep 4120: ep_len:288 episode reward: total was 26.670000. running mean: 27.799210\n",
      "ep 4120: ep_len:196 episode reward: total was 16.720000. running mean: 27.688417\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.017302 episode_count: 28847. steps_count: 12475923.000000\n",
      "Time elapsed:  38067.5220746994\n",
      "ep 4121: ep_len:209 episode reward: total was 23.230000. running mean: 27.643833\n",
      "ep 4121: ep_len:639 episode reward: total was -41.790000. running mean: 26.949495\n",
      "ep 4121: ep_len:599 episode reward: total was 11.110000. running mean: 26.791100\n",
      "ep 4121: ep_len:510 episode reward: total was 31.500000. running mean: 26.838189\n",
      "ep 4121: ep_len:76 episode reward: total was 23.710000. running mean: 26.806907\n",
      "ep 4121: ep_len:661 episode reward: total was 47.920000. running mean: 27.018038\n",
      "ep 4121: ep_len:500 episode reward: total was 29.950000. running mean: 27.047358\n",
      "epsilon:0.017258 episode_count: 28854. steps_count: 12479117.000000\n",
      "Time elapsed:  38076.0241625309\n",
      "ep 4122: ep_len:114 episode reward: total was 23.970000. running mean: 27.016584\n",
      "ep 4122: ep_len:545 episode reward: total was 98.360000. running mean: 27.730018\n",
      "ep 4122: ep_len:395 episode reward: total was 66.730000. running mean: 28.120018\n",
      "ep 4122: ep_len:500 episode reward: total was 52.520000. running mean: 28.364018\n",
      "ep 4122: ep_len:3 episode reward: total was 1.010000. running mean: 28.090478\n",
      "ep 4122: ep_len:504 episode reward: total was 12.620000. running mean: 27.935773\n",
      "ep 4122: ep_len:626 episode reward: total was -12.170000. running mean: 27.534715\n",
      "epsilon:0.017214 episode_count: 28861. steps_count: 12481804.000000\n",
      "Time elapsed:  38083.31956458092\n",
      "ep 4123: ep_len:530 episode reward: total was 30.090000. running mean: 27.560268\n",
      "ep 4123: ep_len:295 episode reward: total was 31.270000. running mean: 27.597365\n",
      "ep 4123: ep_len:79 episode reward: total was 15.870000. running mean: 27.480092\n",
      "ep 4123: ep_len:155 episode reward: total was 3.760000. running mean: 27.242891\n",
      "ep 4123: ep_len:79 episode reward: total was 16.270000. running mean: 27.133162\n",
      "ep 4123: ep_len:539 episode reward: total was 36.750000. running mean: 27.229330\n",
      "ep 4123: ep_len:503 episode reward: total was -16.920000. running mean: 26.787837\n",
      "epsilon:0.017169 episode_count: 28868. steps_count: 12483984.000000\n",
      "Time elapsed:  38089.411098480225\n",
      "ep 4124: ep_len:242 episode reward: total was 19.930000. running mean: 26.719259\n",
      "ep 4124: ep_len:500 episode reward: total was 105.950000. running mean: 27.511566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4124: ep_len:383 episode reward: total was 72.610000. running mean: 27.962550\n",
      "ep 4124: ep_len:43 episode reward: total was -7.710000. running mean: 27.605825\n",
      "ep 4124: ep_len:3 episode reward: total was 1.010000. running mean: 27.339867\n",
      "ep 4124: ep_len:582 episode reward: total was -34.930000. running mean: 26.717168\n",
      "ep 4124: ep_len:585 episode reward: total was 48.510000. running mean: 26.935096\n",
      "epsilon:0.017125 episode_count: 28875. steps_count: 12486322.000000\n",
      "Time elapsed:  38103.26300430298\n",
      "ep 4125: ep_len:598 episode reward: total was -144.050000. running mean: 25.225245\n",
      "ep 4125: ep_len:521 episode reward: total was 71.470000. running mean: 25.687693\n",
      "ep 4125: ep_len:500 episode reward: total was 42.240000. running mean: 25.853216\n",
      "ep 4125: ep_len:501 episode reward: total was 74.050000. running mean: 26.335184\n",
      "ep 4125: ep_len:3 episode reward: total was 1.010000. running mean: 26.081932\n",
      "ep 4125: ep_len:636 episode reward: total was 18.470000. running mean: 26.005813\n",
      "ep 4125: ep_len:500 episode reward: total was 55.260000. running mean: 26.298354\n",
      "epsilon:0.017081 episode_count: 28882. steps_count: 12489581.000000\n",
      "Time elapsed:  38111.90241885185\n",
      "ep 4126: ep_len:500 episode reward: total was 47.950000. running mean: 26.514871\n",
      "ep 4126: ep_len:606 episode reward: total was 150.510000. running mean: 27.754822\n",
      "ep 4126: ep_len:555 episode reward: total was -5.260000. running mean: 27.424674\n",
      "ep 4126: ep_len:501 episode reward: total was 77.050000. running mean: 27.920927\n",
      "ep 4126: ep_len:3 episode reward: total was 1.010000. running mean: 27.651818\n",
      "ep 4126: ep_len:555 episode reward: total was 43.890000. running mean: 27.814200\n",
      "ep 4126: ep_len:196 episode reward: total was -71.610000. running mean: 26.819958\n",
      "epsilon:0.017036 episode_count: 28889. steps_count: 12492497.000000\n",
      "Time elapsed:  38133.73458862305\n",
      "ep 4127: ep_len:504 episode reward: total was 79.240000. running mean: 27.344158\n",
      "ep 4127: ep_len:556 episode reward: total was 1.700000. running mean: 27.087717\n",
      "ep 4127: ep_len:500 episode reward: total was 54.070000. running mean: 27.357539\n",
      "ep 4127: ep_len:377 episode reward: total was -114.100000. running mean: 25.942964\n",
      "ep 4127: ep_len:3 episode reward: total was 1.010000. running mean: 25.693634\n",
      "ep 4127: ep_len:629 episode reward: total was 50.020000. running mean: 25.936898\n",
      "ep 4127: ep_len:340 episode reward: total was 33.350000. running mean: 26.011029\n",
      "epsilon:0.016992 episode_count: 28896. steps_count: 12495406.000000\n",
      "Time elapsed:  38141.52422428131\n",
      "ep 4128: ep_len:125 episode reward: total was 20.960000. running mean: 25.960519\n",
      "ep 4128: ep_len:564 episode reward: total was 64.630000. running mean: 26.347214\n",
      "ep 4128: ep_len:542 episode reward: total was -7.290000. running mean: 26.010841\n",
      "ep 4128: ep_len:505 episode reward: total was 84.560000. running mean: 26.596333\n",
      "ep 4128: ep_len:3 episode reward: total was 1.010000. running mean: 26.340470\n",
      "ep 4128: ep_len:508 episode reward: total was 25.820000. running mean: 26.335265\n",
      "ep 4128: ep_len:555 episode reward: total was 17.690000. running mean: 26.248812\n",
      "epsilon:0.016948 episode_count: 28903. steps_count: 12498208.000000\n",
      "Time elapsed:  38148.99695968628\n",
      "ep 4129: ep_len:500 episode reward: total was 24.440000. running mean: 26.230724\n",
      "ep 4129: ep_len:564 episode reward: total was 51.010000. running mean: 26.478517\n",
      "ep 4129: ep_len:608 episode reward: total was 13.650000. running mean: 26.350232\n",
      "ep 4129: ep_len:526 episode reward: total was -124.500000. running mean: 24.841730\n",
      "ep 4129: ep_len:56 episode reward: total was 27.510000. running mean: 24.868412\n",
      "ep 4129: ep_len:605 episode reward: total was 35.550000. running mean: 24.975228\n",
      "ep 4129: ep_len:532 episode reward: total was -59.940000. running mean: 24.126076\n",
      "epsilon:0.016903 episode_count: 28910. steps_count: 12501599.000000\n",
      "Time elapsed:  38157.892508268356\n",
      "ep 4130: ep_len:571 episode reward: total was -97.650000. running mean: 22.908315\n",
      "ep 4130: ep_len:580 episode reward: total was 8.650000. running mean: 22.765732\n",
      "ep 4130: ep_len:650 episode reward: total was -7.330000. running mean: 22.464775\n",
      "ep 4130: ep_len:40 episode reward: total was -4.800000. running mean: 22.192127\n",
      "ep 4130: ep_len:115 episode reward: total was 40.240000. running mean: 22.372606\n",
      "ep 4130: ep_len:685 episode reward: total was 52.540000. running mean: 22.674280\n",
      "ep 4130: ep_len:559 episode reward: total was 45.770000. running mean: 22.905237\n",
      "epsilon:0.016859 episode_count: 28917. steps_count: 12504799.000000\n",
      "Time elapsed:  38166.409947156906\n",
      "ep 4131: ep_len:621 episode reward: total was -20.380000. running mean: 22.472384\n",
      "ep 4131: ep_len:576 episode reward: total was 39.970000. running mean: 22.647361\n",
      "ep 4131: ep_len:72 episode reward: total was 1.290000. running mean: 22.433787\n",
      "ep 4131: ep_len:541 episode reward: total was 70.880000. running mean: 22.918249\n",
      "ep 4131: ep_len:3 episode reward: total was 1.010000. running mean: 22.699167\n",
      "ep 4131: ep_len:500 episode reward: total was 7.130000. running mean: 22.543475\n",
      "ep 4131: ep_len:567 episode reward: total was 57.490000. running mean: 22.892940\n",
      "epsilon:0.016815 episode_count: 28924. steps_count: 12507679.000000\n",
      "Time elapsed:  38173.37799882889\n",
      "ep 4132: ep_len:548 episode reward: total was 84.460000. running mean: 23.508611\n",
      "ep 4132: ep_len:502 episode reward: total was 2.470000. running mean: 23.298225\n",
      "ep 4132: ep_len:534 episode reward: total was -15.910000. running mean: 22.906142\n",
      "ep 4132: ep_len:531 episode reward: total was -97.760000. running mean: 21.699481\n",
      "ep 4132: ep_len:95 episode reward: total was 31.280000. running mean: 21.795286\n",
      "ep 4132: ep_len:501 episode reward: total was 46.480000. running mean: 22.042133\n",
      "ep 4132: ep_len:563 episode reward: total was 47.920000. running mean: 22.300912\n",
      "epsilon:0.016770 episode_count: 28931. steps_count: 12510953.000000\n",
      "Time elapsed:  38182.09492683411\n",
      "ep 4133: ep_len:606 episode reward: total was -24.440000. running mean: 21.833503\n",
      "ep 4133: ep_len:500 episode reward: total was 112.400000. running mean: 22.739168\n",
      "ep 4133: ep_len:519 episode reward: total was 71.470000. running mean: 23.226476\n",
      "ep 4133: ep_len:500 episode reward: total was -74.330000. running mean: 22.250911\n",
      "ep 4133: ep_len:3 episode reward: total was 1.010000. running mean: 22.038502\n",
      "ep 4133: ep_len:525 episode reward: total was 38.640000. running mean: 22.204517\n",
      "ep 4133: ep_len:500 episode reward: total was 4.860000. running mean: 22.031072\n",
      "epsilon:0.016726 episode_count: 28938. steps_count: 12514106.000000\n",
      "Time elapsed:  38190.47393774986\n",
      "ep 4134: ep_len:524 episode reward: total was 93.680000. running mean: 22.747561\n",
      "ep 4134: ep_len:536 episode reward: total was 126.740000. running mean: 23.787486\n",
      "ep 4134: ep_len:526 episode reward: total was 49.290000. running mean: 24.042511\n",
      "ep 4134: ep_len:525 episode reward: total was -1.230000. running mean: 23.789786\n",
      "ep 4134: ep_len:3 episode reward: total was 1.010000. running mean: 23.561988\n",
      "ep 4134: ep_len:557 episode reward: total was 30.480000. running mean: 23.631168\n",
      "ep 4134: ep_len:500 episode reward: total was 25.040000. running mean: 23.645256\n",
      "epsilon:0.016682 episode_count: 28945. steps_count: 12517277.000000\n",
      "Time elapsed:  38198.9297003746\n",
      "ep 4135: ep_len:510 episode reward: total was 47.010000. running mean: 23.878904\n",
      "ep 4135: ep_len:584 episode reward: total was 8.850000. running mean: 23.728615\n",
      "ep 4135: ep_len:615 episode reward: total was 9.990000. running mean: 23.591229\n",
      "ep 4135: ep_len:559 episode reward: total was 30.960000. running mean: 23.664916\n",
      "ep 4135: ep_len:3 episode reward: total was 1.010000. running mean: 23.438367\n",
      "ep 4135: ep_len:500 episode reward: total was 29.400000. running mean: 23.497983\n",
      "ep 4135: ep_len:589 episode reward: total was 48.000000. running mean: 23.743004\n",
      "epsilon:0.016637 episode_count: 28952. steps_count: 12520637.000000\n",
      "Time elapsed:  38207.671983242035\n",
      "ep 4136: ep_len:662 episode reward: total was -313.100000. running mean: 20.374574\n",
      "ep 4136: ep_len:528 episode reward: total was 17.090000. running mean: 20.341728\n",
      "ep 4136: ep_len:500 episode reward: total was 20.750000. running mean: 20.345811\n",
      "ep 4136: ep_len:606 episode reward: total was 53.600000. running mean: 20.678352\n",
      "ep 4136: ep_len:49 episode reward: total was 21.010000. running mean: 20.681669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4136: ep_len:545 episode reward: total was -20.660000. running mean: 20.268252\n",
      "ep 4136: ep_len:524 episode reward: total was -4.560000. running mean: 20.019970\n",
      "epsilon:0.016593 episode_count: 28959. steps_count: 12524051.000000\n",
      "Time elapsed:  38216.6180639267\n",
      "ep 4137: ep_len:500 episode reward: total was 93.520000. running mean: 20.754970\n",
      "ep 4137: ep_len:516 episode reward: total was 116.250000. running mean: 21.709920\n",
      "ep 4137: ep_len:624 episode reward: total was 25.470000. running mean: 21.747521\n",
      "ep 4137: ep_len:500 episode reward: total was 33.550000. running mean: 21.865546\n",
      "ep 4137: ep_len:114 episode reward: total was 31.870000. running mean: 21.965590\n",
      "ep 4137: ep_len:500 episode reward: total was 0.780000. running mean: 21.753735\n",
      "ep 4137: ep_len:500 episode reward: total was 15.890000. running mean: 21.695097\n",
      "epsilon:0.016549 episode_count: 28966. steps_count: 12527305.000000\n",
      "Time elapsed:  38225.64323377609\n",
      "ep 4138: ep_len:504 episode reward: total was 53.530000. running mean: 22.013446\n",
      "ep 4138: ep_len:500 episode reward: total was 70.310000. running mean: 22.496412\n",
      "ep 4138: ep_len:576 episode reward: total was 61.920000. running mean: 22.890648\n",
      "ep 4138: ep_len:510 episode reward: total was 48.250000. running mean: 23.144241\n",
      "ep 4138: ep_len:53 episode reward: total was 26.010000. running mean: 23.172899\n",
      "ep 4138: ep_len:544 episode reward: total was 70.610000. running mean: 23.647270\n",
      "ep 4138: ep_len:578 episode reward: total was -51.620000. running mean: 22.894597\n",
      "epsilon:0.016504 episode_count: 28973. steps_count: 12530570.000000\n",
      "Time elapsed:  38233.85053420067\n",
      "ep 4139: ep_len:508 episode reward: total was -9.960000. running mean: 22.566051\n",
      "ep 4139: ep_len:500 episode reward: total was 45.580000. running mean: 22.796191\n",
      "ep 4139: ep_len:661 episode reward: total was -5.310000. running mean: 22.515129\n",
      "ep 4139: ep_len:56 episode reward: total was -3.650000. running mean: 22.253477\n",
      "ep 4139: ep_len:3 episode reward: total was -0.490000. running mean: 22.026043\n",
      "ep 4139: ep_len:642 episode reward: total was 65.400000. running mean: 22.459782\n",
      "ep 4139: ep_len:628 episode reward: total was 68.010000. running mean: 22.915284\n",
      "epsilon:0.016460 episode_count: 28980. steps_count: 12533568.000000\n",
      "Time elapsed:  38241.8821747303\n",
      "ep 4140: ep_len:583 episode reward: total was 78.230000. running mean: 23.468432\n",
      "ep 4140: ep_len:500 episode reward: total was 117.910000. running mean: 24.412847\n",
      "ep 4140: ep_len:500 episode reward: total was 3.560000. running mean: 24.204319\n",
      "ep 4140: ep_len:500 episode reward: total was -28.950000. running mean: 23.672776\n",
      "ep 4140: ep_len:3 episode reward: total was 1.010000. running mean: 23.446148\n",
      "ep 4140: ep_len:530 episode reward: total was 9.300000. running mean: 23.304686\n",
      "ep 4140: ep_len:539 episode reward: total was 56.300000. running mean: 23.634639\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.016416 episode_count: 28987. steps_count: 12536723.000000\n",
      "Time elapsed:  38254.96048426628\n",
      "ep 4141: ep_len:246 episode reward: total was 23.400000. running mean: 23.632293\n",
      "ep 4141: ep_len:358 episode reward: total was -1.480000. running mean: 23.381170\n",
      "ep 4141: ep_len:616 episode reward: total was 30.440000. running mean: 23.451758\n",
      "ep 4141: ep_len:525 episode reward: total was 43.320000. running mean: 23.650441\n",
      "ep 4141: ep_len:3 episode reward: total was 1.010000. running mean: 23.424036\n",
      "ep 4141: ep_len:500 episode reward: total was 48.500000. running mean: 23.674796\n",
      "ep 4141: ep_len:535 episode reward: total was 25.610000. running mean: 23.694148\n",
      "epsilon:0.016371 episode_count: 28994. steps_count: 12539506.000000\n",
      "Time elapsed:  38269.144876241684\n",
      "ep 4142: ep_len:566 episode reward: total was 49.000000. running mean: 23.947207\n",
      "ep 4142: ep_len:515 episode reward: total was 37.860000. running mean: 24.086335\n",
      "ep 4142: ep_len:79 episode reward: total was 12.840000. running mean: 23.973871\n",
      "ep 4142: ep_len:527 episode reward: total was 51.170000. running mean: 24.245833\n",
      "ep 4142: ep_len:3 episode reward: total was 1.010000. running mean: 24.013474\n",
      "ep 4142: ep_len:500 episode reward: total was 48.030000. running mean: 24.253639\n",
      "ep 4142: ep_len:500 episode reward: total was 59.270000. running mean: 24.603803\n",
      "epsilon:0.016327 episode_count: 29001. steps_count: 12542196.000000\n",
      "Time elapsed:  38276.408632040024\n",
      "ep 4143: ep_len:594 episode reward: total was 96.990000. running mean: 25.327665\n",
      "ep 4143: ep_len:359 episode reward: total was 17.140000. running mean: 25.245788\n",
      "ep 4143: ep_len:534 episode reward: total was 52.310000. running mean: 25.516430\n",
      "ep 4143: ep_len:535 episode reward: total was -102.620000. running mean: 24.235066\n",
      "ep 4143: ep_len:63 episode reward: total was 14.210000. running mean: 24.134816\n",
      "ep 4143: ep_len:662 episode reward: total was 31.890000. running mean: 24.212367\n",
      "ep 4143: ep_len:500 episode reward: total was 57.460000. running mean: 24.544844\n",
      "epsilon:0.016283 episode_count: 29008. steps_count: 12545443.000000\n",
      "Time elapsed:  38284.33350729942\n",
      "ep 4144: ep_len:595 episode reward: total was 28.540000. running mean: 24.584795\n",
      "ep 4144: ep_len:500 episode reward: total was 47.710000. running mean: 24.816047\n",
      "ep 4144: ep_len:641 episode reward: total was -18.640000. running mean: 24.381487\n",
      "ep 4144: ep_len:500 episode reward: total was 16.140000. running mean: 24.299072\n",
      "ep 4144: ep_len:3 episode reward: total was 1.010000. running mean: 24.066181\n",
      "ep 4144: ep_len:562 episode reward: total was 65.090000. running mean: 24.476419\n",
      "ep 4144: ep_len:548 episode reward: total was 29.690000. running mean: 24.528555\n",
      "epsilon:0.016238 episode_count: 29015. steps_count: 12548792.000000\n",
      "Time elapsed:  38300.070033073425\n",
      "ep 4145: ep_len:255 episode reward: total was 15.810000. running mean: 24.441370\n",
      "ep 4145: ep_len:166 episode reward: total was -31.400000. running mean: 23.882956\n",
      "ep 4145: ep_len:572 episode reward: total was -5.630000. running mean: 23.587826\n",
      "ep 4145: ep_len:500 episode reward: total was 85.430000. running mean: 24.206248\n",
      "ep 4145: ep_len:93 episode reward: total was 29.270000. running mean: 24.256886\n",
      "ep 4145: ep_len:500 episode reward: total was 18.450000. running mean: 24.198817\n",
      "ep 4145: ep_len:542 episode reward: total was 43.320000. running mean: 24.390029\n",
      "epsilon:0.016194 episode_count: 29022. steps_count: 12551420.000000\n",
      "Time elapsed:  38307.1270506382\n",
      "ep 4146: ep_len:617 episode reward: total was -21.580000. running mean: 23.930328\n",
      "ep 4146: ep_len:500 episode reward: total was 44.860000. running mean: 24.139625\n",
      "ep 4146: ep_len:70 episode reward: total was 6.840000. running mean: 23.966629\n",
      "ep 4146: ep_len:500 episode reward: total was 46.310000. running mean: 24.190063\n",
      "ep 4146: ep_len:52 episode reward: total was 21.010000. running mean: 24.158262\n",
      "ep 4146: ep_len:530 episode reward: total was -16.090000. running mean: 23.755779\n",
      "ep 4146: ep_len:579 episode reward: total was 39.100000. running mean: 23.909222\n",
      "epsilon:0.016150 episode_count: 29029. steps_count: 12554268.000000\n",
      "Time elapsed:  38314.75443005562\n",
      "ep 4147: ep_len:227 episode reward: total was 16.800000. running mean: 23.838129\n",
      "ep 4147: ep_len:305 episode reward: total was 22.800000. running mean: 23.827748\n",
      "ep 4147: ep_len:536 episode reward: total was -113.250000. running mean: 22.456971\n",
      "ep 4147: ep_len:609 episode reward: total was 106.350000. running mean: 23.295901\n",
      "ep 4147: ep_len:3 episode reward: total was 1.010000. running mean: 23.073042\n",
      "ep 4147: ep_len:534 episode reward: total was 42.520000. running mean: 23.267511\n",
      "ep 4147: ep_len:302 episode reward: total was 30.050000. running mean: 23.335336\n",
      "epsilon:0.016105 episode_count: 29036. steps_count: 12556784.000000\n",
      "Time elapsed:  38321.626543045044\n",
      "ep 4148: ep_len:556 episode reward: total was -7.400000. running mean: 23.027983\n",
      "ep 4148: ep_len:344 episode reward: total was -6.920000. running mean: 22.728503\n",
      "ep 4148: ep_len:611 episode reward: total was 70.190000. running mean: 23.203118\n",
      "ep 4148: ep_len:520 episode reward: total was 37.370000. running mean: 23.344787\n",
      "ep 4148: ep_len:3 episode reward: total was 1.010000. running mean: 23.121439\n",
      "ep 4148: ep_len:639 episode reward: total was 66.640000. running mean: 23.556625\n",
      "ep 4148: ep_len:542 episode reward: total was 16.510000. running mean: 23.486158\n",
      "epsilon:0.016061 episode_count: 29043. steps_count: 12559999.000000\n",
      "Time elapsed:  38330.10252070427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4149: ep_len:581 episode reward: total was 80.310000. running mean: 24.054397\n",
      "ep 4149: ep_len:500 episode reward: total was 106.460000. running mean: 24.878453\n",
      "ep 4149: ep_len:560 episode reward: total was -55.910000. running mean: 24.070568\n",
      "ep 4149: ep_len:559 episode reward: total was 52.940000. running mean: 24.359263\n",
      "ep 4149: ep_len:3 episode reward: total was 1.010000. running mean: 24.125770\n",
      "ep 4149: ep_len:500 episode reward: total was 59.710000. running mean: 24.481612\n",
      "ep 4149: ep_len:613 episode reward: total was 44.040000. running mean: 24.677196\n",
      "epsilon:0.016017 episode_count: 29050. steps_count: 12563315.000000\n",
      "Time elapsed:  38338.421334028244\n",
      "ep 4150: ep_len:500 episode reward: total was 112.410000. running mean: 25.554524\n",
      "ep 4150: ep_len:289 episode reward: total was -3.220000. running mean: 25.266779\n",
      "ep 4150: ep_len:79 episode reward: total was 16.390000. running mean: 25.178011\n",
      "ep 4150: ep_len:944 episode reward: total was -473.440000. running mean: 20.191831\n",
      "ep 4150: ep_len:55 episode reward: total was 24.500000. running mean: 20.234913\n",
      "ep 4150: ep_len:578 episode reward: total was 6.980000. running mean: 20.102364\n",
      "ep 4150: ep_len:567 episode reward: total was 70.860000. running mean: 20.609940\n",
      "epsilon:0.015972 episode_count: 29057. steps_count: 12566327.000000\n",
      "Time elapsed:  38351.38546657562\n",
      "ep 4151: ep_len:586 episode reward: total was 71.790000. running mean: 21.121741\n",
      "ep 4151: ep_len:558 episode reward: total was 37.160000. running mean: 21.282123\n",
      "ep 4151: ep_len:660 episode reward: total was 8.580000. running mean: 21.155102\n",
      "ep 4151: ep_len:558 episode reward: total was 72.490000. running mean: 21.668451\n",
      "ep 4151: ep_len:109 episode reward: total was 29.770000. running mean: 21.749466\n",
      "ep 4151: ep_len:528 episode reward: total was 7.980000. running mean: 21.611772\n",
      "ep 4151: ep_len:578 episode reward: total was 50.030000. running mean: 21.895954\n",
      "epsilon:0.015928 episode_count: 29064. steps_count: 12569904.000000\n",
      "Time elapsed:  38360.82730984688\n",
      "ep 4152: ep_len:590 episode reward: total was 84.940000. running mean: 22.526394\n",
      "ep 4152: ep_len:521 episode reward: total was 11.100000. running mean: 22.412131\n",
      "ep 4152: ep_len:595 episode reward: total was 7.080000. running mean: 22.258809\n",
      "ep 4152: ep_len:505 episode reward: total was 83.550000. running mean: 22.871721\n",
      "ep 4152: ep_len:3 episode reward: total was 1.010000. running mean: 22.653104\n",
      "ep 4152: ep_len:623 episode reward: total was 72.100000. running mean: 23.147573\n",
      "ep 4152: ep_len:319 episode reward: total was 23.190000. running mean: 23.147997\n",
      "epsilon:0.015884 episode_count: 29071. steps_count: 12573060.000000\n",
      "Time elapsed:  38369.321912288666\n",
      "ep 4153: ep_len:696 episode reward: total was -93.730000. running mean: 21.979217\n",
      "ep 4153: ep_len:534 episode reward: total was 27.590000. running mean: 22.035325\n",
      "ep 4153: ep_len:371 episode reward: total was -32.490000. running mean: 21.490072\n",
      "ep 4153: ep_len:548 episode reward: total was 86.200000. running mean: 22.137171\n",
      "ep 4153: ep_len:99 episode reward: total was 30.770000. running mean: 22.223499\n",
      "ep 4153: ep_len:687 episode reward: total was 60.490000. running mean: 22.606164\n",
      "ep 4153: ep_len:589 episode reward: total was 28.210000. running mean: 22.662203\n",
      "epsilon:0.015839 episode_count: 29078. steps_count: 12576584.000000\n",
      "Time elapsed:  38378.47611403465\n",
      "ep 4154: ep_len:589 episode reward: total was 83.050000. running mean: 23.266081\n",
      "ep 4154: ep_len:191 episode reward: total was 9.300000. running mean: 23.126420\n",
      "ep 4154: ep_len:687 episode reward: total was -0.700000. running mean: 22.888156\n",
      "ep 4154: ep_len:500 episode reward: total was 29.620000. running mean: 22.955474\n",
      "ep 4154: ep_len:107 episode reward: total was 25.740000. running mean: 22.983319\n",
      "ep 4154: ep_len:593 episode reward: total was 36.040000. running mean: 23.113886\n",
      "ep 4154: ep_len:616 episode reward: total was 25.300000. running mean: 23.135747\n",
      "epsilon:0.015795 episode_count: 29085. steps_count: 12579867.000000\n",
      "Time elapsed:  38387.10962295532\n",
      "ep 4155: ep_len:555 episode reward: total was 40.100000. running mean: 23.305390\n",
      "ep 4155: ep_len:270 episode reward: total was 26.670000. running mean: 23.339036\n",
      "ep 4155: ep_len:554 episode reward: total was -22.560000. running mean: 22.880046\n",
      "ep 4155: ep_len:547 episode reward: total was 76.520000. running mean: 23.416445\n",
      "ep 4155: ep_len:3 episode reward: total was 1.010000. running mean: 23.192381\n",
      "ep 4155: ep_len:307 episode reward: total was -0.410000. running mean: 22.956357\n",
      "ep 4155: ep_len:207 episode reward: total was 14.200000. running mean: 22.868793\n",
      "epsilon:0.015751 episode_count: 29092. steps_count: 12582310.000000\n",
      "Time elapsed:  38393.84653091431\n",
      "ep 4156: ep_len:501 episode reward: total was 62.140000. running mean: 23.261505\n",
      "ep 4156: ep_len:579 episode reward: total was -224.880000. running mean: 20.780090\n",
      "ep 4156: ep_len:564 episode reward: total was 42.310000. running mean: 20.995389\n",
      "ep 4156: ep_len:529 episode reward: total was 94.840000. running mean: 21.733836\n",
      "ep 4156: ep_len:96 episode reward: total was 27.770000. running mean: 21.794197\n",
      "ep 4156: ep_len:516 episode reward: total was 21.980000. running mean: 21.796055\n",
      "ep 4156: ep_len:610 episode reward: total was 40.830000. running mean: 21.986395\n",
      "epsilon:0.015706 episode_count: 29099. steps_count: 12585705.000000\n",
      "Time elapsed:  38402.981462955475\n",
      "ep 4157: ep_len:525 episode reward: total was -23.610000. running mean: 21.530431\n",
      "ep 4157: ep_len:269 episode reward: total was -5.850000. running mean: 21.256626\n",
      "ep 4157: ep_len:585 episode reward: total was 5.870000. running mean: 21.102760\n",
      "ep 4157: ep_len:500 episode reward: total was 83.990000. running mean: 21.731633\n",
      "ep 4157: ep_len:3 episode reward: total was 1.010000. running mean: 21.524416\n",
      "ep 4157: ep_len:607 episode reward: total was 38.530000. running mean: 21.694472\n",
      "ep 4157: ep_len:500 episode reward: total was 40.030000. running mean: 21.877827\n",
      "epsilon:0.015662 episode_count: 29106. steps_count: 12588694.000000\n",
      "Time elapsed:  38423.86005616188\n",
      "ep 4158: ep_len:523 episode reward: total was 83.580000. running mean: 22.494849\n",
      "ep 4158: ep_len:596 episode reward: total was 122.020000. running mean: 23.490101\n",
      "ep 4158: ep_len:607 episode reward: total was -1.120000. running mean: 23.244000\n",
      "ep 4158: ep_len:531 episode reward: total was 90.960000. running mean: 23.921160\n",
      "ep 4158: ep_len:114 episode reward: total was 35.270000. running mean: 24.034648\n",
      "ep 4158: ep_len:563 episode reward: total was 39.290000. running mean: 24.187201\n",
      "ep 4158: ep_len:548 episode reward: total was 45.890000. running mean: 24.404229\n",
      "epsilon:0.015618 episode_count: 29113. steps_count: 12592176.000000\n",
      "Time elapsed:  38432.808269262314\n",
      "ep 4159: ep_len:500 episode reward: total was 72.370000. running mean: 24.883887\n",
      "ep 4159: ep_len:501 episode reward: total was 26.700000. running mean: 24.902048\n",
      "ep 4159: ep_len:399 episode reward: total was 72.140000. running mean: 25.374428\n",
      "ep 4159: ep_len:535 episode reward: total was 26.040000. running mean: 25.381084\n",
      "ep 4159: ep_len:96 episode reward: total was 32.270000. running mean: 25.449973\n",
      "ep 4159: ep_len:670 episode reward: total was 45.420000. running mean: 25.649673\n",
      "ep 4159: ep_len:335 episode reward: total was 32.200000. running mean: 25.715176\n",
      "epsilon:0.015573 episode_count: 29120. steps_count: 12595212.000000\n",
      "Time elapsed:  38440.97006082535\n",
      "ep 4160: ep_len:688 episode reward: total was -4.000000. running mean: 25.418024\n",
      "ep 4160: ep_len:500 episode reward: total was 117.580000. running mean: 26.339644\n",
      "ep 4160: ep_len:567 episode reward: total was 15.340000. running mean: 26.229648\n",
      "ep 4160: ep_len:123 episode reward: total was 5.030000. running mean: 26.017651\n",
      "ep 4160: ep_len:3 episode reward: total was 1.010000. running mean: 25.767575\n",
      "ep 4160: ep_len:500 episode reward: total was 4.040000. running mean: 25.550299\n",
      "ep 4160: ep_len:509 episode reward: total was 5.330000. running mean: 25.348096\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.015529 episode_count: 29127. steps_count: 12598102.000000\n",
      "Time elapsed:  38453.69170808792\n",
      "ep 4161: ep_len:609 episode reward: total was 112.500000. running mean: 26.219615\n",
      "ep 4161: ep_len:500 episode reward: total was 101.100000. running mean: 26.968419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4161: ep_len:542 episode reward: total was -168.870000. running mean: 25.010035\n",
      "ep 4161: ep_len:500 episode reward: total was 82.470000. running mean: 25.584634\n",
      "ep 4161: ep_len:55 episode reward: total was 27.010000. running mean: 25.598888\n",
      "ep 4161: ep_len:299 episode reward: total was 10.260000. running mean: 25.445499\n",
      "ep 4161: ep_len:500 episode reward: total was 14.260000. running mean: 25.333644\n",
      "epsilon:0.015485 episode_count: 29134. steps_count: 12601107.000000\n",
      "Time elapsed:  38468.7216796875\n",
      "ep 4162: ep_len:593 episode reward: total was -71.600000. running mean: 24.364308\n",
      "ep 4162: ep_len:621 episode reward: total was -13.000000. running mean: 23.990665\n",
      "ep 4162: ep_len:574 episode reward: total was 23.670000. running mean: 23.987458\n",
      "ep 4162: ep_len:500 episode reward: total was 23.920000. running mean: 23.986783\n",
      "ep 4162: ep_len:3 episode reward: total was 1.010000. running mean: 23.757016\n",
      "ep 4162: ep_len:637 episode reward: total was 38.930000. running mean: 23.908745\n",
      "ep 4162: ep_len:534 episode reward: total was 23.600000. running mean: 23.905658\n",
      "epsilon:0.015440 episode_count: 29141. steps_count: 12604569.000000\n",
      "Time elapsed:  38484.75534725189\n",
      "ep 4163: ep_len:625 episode reward: total was -17.980000. running mean: 23.486801\n",
      "ep 4163: ep_len:586 episode reward: total was -3.790000. running mean: 23.214033\n",
      "ep 4163: ep_len:554 episode reward: total was 33.380000. running mean: 23.315693\n",
      "ep 4163: ep_len:132 episode reward: total was 8.600000. running mean: 23.168536\n",
      "ep 4163: ep_len:3 episode reward: total was 1.010000. running mean: 22.946951\n",
      "ep 4163: ep_len:246 episode reward: total was 40.960000. running mean: 23.127081\n",
      "ep 4163: ep_len:565 episode reward: total was 53.210000. running mean: 23.427910\n",
      "epsilon:0.015396 episode_count: 29148. steps_count: 12607280.000000\n",
      "Time elapsed:  38492.17851114273\n",
      "ep 4164: ep_len:173 episode reward: total was -2.970000. running mean: 23.163931\n",
      "ep 4164: ep_len:510 episode reward: total was 18.560000. running mean: 23.117892\n",
      "ep 4164: ep_len:374 episode reward: total was 23.870000. running mean: 23.125413\n",
      "ep 4164: ep_len:56 episode reward: total was 1.860000. running mean: 22.912759\n",
      "ep 4164: ep_len:3 episode reward: total was 1.010000. running mean: 22.693731\n",
      "ep 4164: ep_len:150 episode reward: total was 2.110000. running mean: 22.487894\n",
      "ep 4164: ep_len:546 episode reward: total was 31.940000. running mean: 22.582415\n",
      "epsilon:0.015352 episode_count: 29155. steps_count: 12609092.000000\n",
      "Time elapsed:  38497.7108836174\n",
      "ep 4165: ep_len:591 episode reward: total was 6.540000. running mean: 22.421991\n",
      "ep 4165: ep_len:611 episode reward: total was 4.940000. running mean: 22.247171\n",
      "ep 4165: ep_len:542 episode reward: total was -3.830000. running mean: 21.986399\n",
      "ep 4165: ep_len:507 episode reward: total was 81.980000. running mean: 22.586335\n",
      "ep 4165: ep_len:53 episode reward: total was 25.000000. running mean: 22.610472\n",
      "ep 4165: ep_len:529 episode reward: total was -1.390000. running mean: 22.370467\n",
      "ep 4165: ep_len:609 episode reward: total was 30.550000. running mean: 22.452263\n",
      "epsilon:0.015307 episode_count: 29162. steps_count: 12612534.000000\n",
      "Time elapsed:  38506.750010728836\n",
      "ep 4166: ep_len:543 episode reward: total was 24.620000. running mean: 22.473940\n",
      "ep 4166: ep_len:650 episode reward: total was -190.340000. running mean: 20.345801\n",
      "ep 4166: ep_len:541 episode reward: total was 11.280000. running mean: 20.255143\n",
      "ep 4166: ep_len:500 episode reward: total was 22.980000. running mean: 20.282391\n",
      "ep 4166: ep_len:51 episode reward: total was 25.010000. running mean: 20.329667\n",
      "ep 4166: ep_len:580 episode reward: total was 69.250000. running mean: 20.818871\n",
      "ep 4166: ep_len:554 episode reward: total was -15.470000. running mean: 20.455982\n",
      "epsilon:0.015263 episode_count: 29169. steps_count: 12615953.000000\n",
      "Time elapsed:  38515.675735235214\n",
      "ep 4167: ep_len:542 episode reward: total was 88.670000. running mean: 21.138122\n",
      "ep 4167: ep_len:201 episode reward: total was 18.460000. running mean: 21.111341\n",
      "ep 4167: ep_len:691 episode reward: total was -179.320000. running mean: 19.107027\n",
      "ep 4167: ep_len:500 episode reward: total was 83.440000. running mean: 19.750357\n",
      "ep 4167: ep_len:3 episode reward: total was 1.010000. running mean: 19.562954\n",
      "ep 4167: ep_len:500 episode reward: total was 21.310000. running mean: 19.580424\n",
      "ep 4167: ep_len:345 episode reward: total was 24.280000. running mean: 19.627420\n",
      "epsilon:0.015219 episode_count: 29176. steps_count: 12618735.000000\n",
      "Time elapsed:  38523.17177295685\n",
      "ep 4168: ep_len:569 episode reward: total was 68.710000. running mean: 20.118246\n",
      "ep 4168: ep_len:500 episode reward: total was 17.210000. running mean: 20.089163\n",
      "ep 4168: ep_len:433 episode reward: total was 15.420000. running mean: 20.042472\n",
      "ep 4168: ep_len:132 episode reward: total was 20.640000. running mean: 20.048447\n",
      "ep 4168: ep_len:3 episode reward: total was 1.010000. running mean: 19.858062\n",
      "ep 4168: ep_len:634 episode reward: total was -58.480000. running mean: 19.074682\n",
      "ep 4168: ep_len:558 episode reward: total was -3.110000. running mean: 18.852835\n",
      "epsilon:0.015174 episode_count: 29183. steps_count: 12621564.000000\n",
      "Time elapsed:  38537.62194490433\n",
      "ep 4169: ep_len:501 episode reward: total was 87.480000. running mean: 19.539107\n",
      "ep 4169: ep_len:670 episode reward: total was 91.960000. running mean: 20.263315\n",
      "ep 4169: ep_len:562 episode reward: total was 47.820000. running mean: 20.538882\n",
      "ep 4169: ep_len:500 episode reward: total was 31.860000. running mean: 20.652094\n",
      "ep 4169: ep_len:3 episode reward: total was 1.010000. running mean: 20.455673\n",
      "ep 4169: ep_len:601 episode reward: total was 68.080000. running mean: 20.931916\n",
      "ep 4169: ep_len:594 episode reward: total was 17.440000. running mean: 20.896997\n",
      "epsilon:0.015130 episode_count: 29190. steps_count: 12624995.000000\n",
      "Time elapsed:  38546.586352586746\n",
      "ep 4170: ep_len:674 episode reward: total was 10.950000. running mean: 20.797527\n",
      "ep 4170: ep_len:603 episode reward: total was 64.910000. running mean: 21.238651\n",
      "ep 4170: ep_len:500 episode reward: total was 12.550000. running mean: 21.151765\n",
      "ep 4170: ep_len:519 episode reward: total was 32.770000. running mean: 21.267947\n",
      "ep 4170: ep_len:85 episode reward: total was 23.770000. running mean: 21.292968\n",
      "ep 4170: ep_len:500 episode reward: total was 54.450000. running mean: 21.624538\n",
      "ep 4170: ep_len:582 episode reward: total was 29.220000. running mean: 21.700493\n",
      "epsilon:0.015086 episode_count: 29197. steps_count: 12628458.000000\n",
      "Time elapsed:  38563.21839118004\n",
      "ep 4171: ep_len:527 episode reward: total was -58.990000. running mean: 20.893588\n",
      "ep 4171: ep_len:533 episode reward: total was 121.320000. running mean: 21.897852\n",
      "ep 4171: ep_len:500 episode reward: total was 58.660000. running mean: 22.265473\n",
      "ep 4171: ep_len:614 episode reward: total was 58.680000. running mean: 22.629619\n",
      "ep 4171: ep_len:130 episode reward: total was 40.360000. running mean: 22.806923\n",
      "ep 4171: ep_len:664 episode reward: total was -105.940000. running mean: 21.519453\n",
      "ep 4171: ep_len:189 episode reward: total was 6.550000. running mean: 21.369759\n",
      "epsilon:0.015041 episode_count: 29204. steps_count: 12631615.000000\n",
      "Time elapsed:  38579.83112406731\n",
      "ep 4172: ep_len:500 episode reward: total was 65.500000. running mean: 21.811061\n",
      "ep 4172: ep_len:560 episode reward: total was 78.740000. running mean: 22.380351\n",
      "ep 4172: ep_len:500 episode reward: total was 55.530000. running mean: 22.711847\n",
      "ep 4172: ep_len:122 episode reward: total was 20.130000. running mean: 22.686029\n",
      "ep 4172: ep_len:3 episode reward: total was 1.010000. running mean: 22.469268\n",
      "ep 4172: ep_len:510 episode reward: total was -113.420000. running mean: 21.110376\n",
      "ep 4172: ep_len:512 episode reward: total was 52.020000. running mean: 21.419472\n",
      "epsilon:0.014997 episode_count: 29211. steps_count: 12634322.000000\n",
      "Time elapsed:  38594.10447740555\n",
      "ep 4173: ep_len:500 episode reward: total was 73.630000. running mean: 21.941577\n",
      "ep 4173: ep_len:517 episode reward: total was 114.320000. running mean: 22.865361\n",
      "ep 4173: ep_len:544 episode reward: total was -16.440000. running mean: 22.472308\n",
      "ep 4173: ep_len:500 episode reward: total was 34.830000. running mean: 22.595885\n",
      "ep 4173: ep_len:3 episode reward: total was 1.010000. running mean: 22.380026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4173: ep_len:519 episode reward: total was 24.150000. running mean: 22.397726\n",
      "ep 4173: ep_len:531 episode reward: total was 17.210000. running mean: 22.345848\n",
      "epsilon:0.014953 episode_count: 29218. steps_count: 12637436.000000\n",
      "Time elapsed:  38602.51283788681\n",
      "ep 4174: ep_len:584 episode reward: total was 60.570000. running mean: 22.728090\n",
      "ep 4174: ep_len:569 episode reward: total was -4.550000. running mean: 22.455309\n",
      "ep 4174: ep_len:435 episode reward: total was 62.460000. running mean: 22.855356\n",
      "ep 4174: ep_len:519 episode reward: total was 53.540000. running mean: 23.162202\n",
      "ep 4174: ep_len:3 episode reward: total was 1.010000. running mean: 22.940680\n",
      "ep 4174: ep_len:500 episode reward: total was -176.450000. running mean: 20.946773\n",
      "ep 4174: ep_len:541 episode reward: total was 47.680000. running mean: 21.214106\n",
      "epsilon:0.014908 episode_count: 29225. steps_count: 12640587.000000\n",
      "Time elapsed:  38618.288519620895\n",
      "ep 4175: ep_len:544 episode reward: total was -10.520000. running mean: 20.896765\n",
      "ep 4175: ep_len:500 episode reward: total was 45.360000. running mean: 21.141397\n",
      "ep 4175: ep_len:632 episode reward: total was 43.500000. running mean: 21.364983\n",
      "ep 4175: ep_len:539 episode reward: total was 47.750000. running mean: 21.628833\n",
      "ep 4175: ep_len:3 episode reward: total was 1.010000. running mean: 21.422645\n",
      "ep 4175: ep_len:171 episode reward: total was 42.120000. running mean: 21.629618\n",
      "ep 4175: ep_len:524 episode reward: total was 65.310000. running mean: 22.066422\n",
      "epsilon:0.014864 episode_count: 29232. steps_count: 12643500.000000\n",
      "Time elapsed:  38626.033880233765\n",
      "ep 4176: ep_len:500 episode reward: total was 87.000000. running mean: 22.715758\n",
      "ep 4176: ep_len:370 episode reward: total was 33.260000. running mean: 22.821200\n",
      "ep 4176: ep_len:512 episode reward: total was -26.590000. running mean: 22.327088\n",
      "ep 4176: ep_len:500 episode reward: total was 43.040000. running mean: 22.534218\n",
      "ep 4176: ep_len:3 episode reward: total was 1.010000. running mean: 22.318975\n",
      "ep 4176: ep_len:500 episode reward: total was 66.560000. running mean: 22.761386\n",
      "ep 4176: ep_len:527 episode reward: total was 29.020000. running mean: 22.823972\n",
      "epsilon:0.014820 episode_count: 29239. steps_count: 12646412.000000\n",
      "Time elapsed:  38633.76365470886\n",
      "ep 4177: ep_len:542 episode reward: total was 62.300000. running mean: 23.218732\n",
      "ep 4177: ep_len:500 episode reward: total was 97.690000. running mean: 23.963445\n",
      "ep 4177: ep_len:565 episode reward: total was 35.250000. running mean: 24.076310\n",
      "ep 4177: ep_len:517 episode reward: total was 34.050000. running mean: 24.176047\n",
      "ep 4177: ep_len:3 episode reward: total was 1.010000. running mean: 23.944387\n",
      "ep 4177: ep_len:522 episode reward: total was 10.640000. running mean: 23.811343\n",
      "ep 4177: ep_len:513 episode reward: total was 30.850000. running mean: 23.881729\n",
      "epsilon:0.014775 episode_count: 29246. steps_count: 12649574.000000\n",
      "Time elapsed:  38642.07400274277\n",
      "ep 4178: ep_len:166 episode reward: total was 7.890000. running mean: 23.721812\n",
      "ep 4178: ep_len:500 episode reward: total was 28.990000. running mean: 23.774494\n",
      "ep 4178: ep_len:459 episode reward: total was 69.960000. running mean: 24.236349\n",
      "ep 4178: ep_len:593 episode reward: total was 79.900000. running mean: 24.792986\n",
      "ep 4178: ep_len:61 episode reward: total was 21.600000. running mean: 24.761056\n",
      "ep 4178: ep_len:505 episode reward: total was 35.740000. running mean: 24.870845\n",
      "ep 4178: ep_len:183 episode reward: total was 3.640000. running mean: 24.658537\n",
      "epsilon:0.014731 episode_count: 29253. steps_count: 12652041.000000\n",
      "Time elapsed:  38648.8139872551\n",
      "ep 4179: ep_len:531 episode reward: total was 92.960000. running mean: 25.341551\n",
      "ep 4179: ep_len:571 episode reward: total was 57.830000. running mean: 25.666436\n",
      "ep 4179: ep_len:614 episode reward: total was 7.230000. running mean: 25.482071\n",
      "ep 4179: ep_len:56 episode reward: total was 2.840000. running mean: 25.255651\n",
      "ep 4179: ep_len:3 episode reward: total was 1.010000. running mean: 25.013194\n",
      "ep 4179: ep_len:683 episode reward: total was -33.770000. running mean: 24.425362\n",
      "ep 4179: ep_len:185 episode reward: total was -44.300000. running mean: 23.738109\n",
      "epsilon:0.014687 episode_count: 29260. steps_count: 12654684.000000\n",
      "Time elapsed:  38664.752275943756\n",
      "ep 4180: ep_len:612 episode reward: total was 101.660000. running mean: 24.517328\n",
      "ep 4180: ep_len:500 episode reward: total was 134.810000. running mean: 25.620254\n",
      "ep 4180: ep_len:500 episode reward: total was 16.280000. running mean: 25.526852\n",
      "ep 4180: ep_len:596 episode reward: total was 78.450000. running mean: 26.056083\n",
      "ep 4180: ep_len:3 episode reward: total was 1.010000. running mean: 25.805622\n",
      "ep 4180: ep_len:525 episode reward: total was 23.640000. running mean: 25.783966\n",
      "ep 4180: ep_len:515 episode reward: total was 15.370000. running mean: 25.679827\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.014642 episode_count: 29267. steps_count: 12657935.000000\n",
      "Time elapsed:  38678.01307153702\n",
      "ep 4181: ep_len:519 episode reward: total was 26.910000. running mean: 25.692128\n",
      "ep 4181: ep_len:500 episode reward: total was 44.860000. running mean: 25.883807\n",
      "ep 4181: ep_len:609 episode reward: total was 22.330000. running mean: 25.848269\n",
      "ep 4181: ep_len:500 episode reward: total was 14.690000. running mean: 25.736686\n",
      "ep 4181: ep_len:3 episode reward: total was 1.010000. running mean: 25.489419\n",
      "ep 4181: ep_len:516 episode reward: total was 4.860000. running mean: 25.283125\n",
      "ep 4181: ep_len:581 episode reward: total was 87.040000. running mean: 25.900694\n",
      "epsilon:0.014598 episode_count: 29274. steps_count: 12661163.000000\n",
      "Time elapsed:  38686.45373725891\n",
      "ep 4182: ep_len:500 episode reward: total was 67.540000. running mean: 26.317087\n",
      "ep 4182: ep_len:527 episode reward: total was -23.020000. running mean: 25.823716\n",
      "ep 4182: ep_len:611 episode reward: total was -77.360000. running mean: 24.791879\n",
      "ep 4182: ep_len:576 episode reward: total was 81.570000. running mean: 25.359660\n",
      "ep 4182: ep_len:3 episode reward: total was 1.010000. running mean: 25.116164\n",
      "ep 4182: ep_len:500 episode reward: total was 61.610000. running mean: 25.481102\n",
      "ep 4182: ep_len:503 episode reward: total was -120.750000. running mean: 24.018791\n",
      "epsilon:0.014554 episode_count: 29281. steps_count: 12664383.000000\n",
      "Time elapsed:  38695.0819401741\n",
      "ep 4183: ep_len:552 episode reward: total was -118.650000. running mean: 22.592103\n",
      "ep 4183: ep_len:285 episode reward: total was 30.770000. running mean: 22.673882\n",
      "ep 4183: ep_len:79 episode reward: total was 12.350000. running mean: 22.570643\n",
      "ep 4183: ep_len:432 episode reward: total was 23.880000. running mean: 22.583737\n",
      "ep 4183: ep_len:3 episode reward: total was 1.010000. running mean: 22.367999\n",
      "ep 4183: ep_len:500 episode reward: total was 50.380000. running mean: 22.648119\n",
      "ep 4183: ep_len:578 episode reward: total was 75.470000. running mean: 23.176338\n",
      "epsilon:0.014509 episode_count: 29288. steps_count: 12666812.000000\n",
      "Time elapsed:  38699.509504556656\n",
      "ep 4184: ep_len:534 episode reward: total was 17.480000. running mean: 23.119375\n",
      "ep 4184: ep_len:500 episode reward: total was 95.590000. running mean: 23.844081\n",
      "ep 4184: ep_len:64 episode reward: total was 13.330000. running mean: 23.738940\n",
      "ep 4184: ep_len:528 episode reward: total was 81.680000. running mean: 24.318351\n",
      "ep 4184: ep_len:3 episode reward: total was 1.010000. running mean: 24.085267\n",
      "ep 4184: ep_len:540 episode reward: total was -6.230000. running mean: 23.782115\n",
      "ep 4184: ep_len:583 episode reward: total was 75.000000. running mean: 24.294293\n",
      "epsilon:0.014465 episode_count: 29295. steps_count: 12669564.000000\n",
      "Time elapsed:  38706.50688672066\n",
      "ep 4185: ep_len:500 episode reward: total was 91.170000. running mean: 24.963051\n",
      "ep 4185: ep_len:366 episode reward: total was 48.190000. running mean: 25.195320\n",
      "ep 4185: ep_len:371 episode reward: total was 62.020000. running mean: 25.563567\n",
      "ep 4185: ep_len:533 episode reward: total was 56.650000. running mean: 25.874431\n",
      "ep 4185: ep_len:3 episode reward: total was 1.010000. running mean: 25.625787\n",
      "ep 4185: ep_len:553 episode reward: total was 34.060000. running mean: 25.710129\n",
      "ep 4185: ep_len:301 episode reward: total was -2.590000. running mean: 25.427128\n",
      "epsilon:0.014421 episode_count: 29302. steps_count: 12672191.000000\n",
      "Time elapsed:  38713.56788492203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4186: ep_len:500 episode reward: total was 59.030000. running mean: 25.763156\n",
      "ep 4186: ep_len:537 episode reward: total was 14.160000. running mean: 25.647125\n",
      "ep 4186: ep_len:79 episode reward: total was 8.310000. running mean: 25.473754\n",
      "ep 4186: ep_len:52 episode reward: total was 2.830000. running mean: 25.247316\n",
      "ep 4186: ep_len:3 episode reward: total was 1.010000. running mean: 25.004943\n",
      "ep 4186: ep_len:605 episode reward: total was 17.310000. running mean: 24.927993\n",
      "ep 4186: ep_len:618 episode reward: total was 34.260000. running mean: 25.021314\n",
      "epsilon:0.014376 episode_count: 29309. steps_count: 12674585.000000\n",
      "Time elapsed:  38726.550510406494\n",
      "ep 4187: ep_len:556 episode reward: total was 66.190000. running mean: 25.433000\n",
      "ep 4187: ep_len:279 episode reward: total was 43.260000. running mean: 25.611270\n",
      "ep 4187: ep_len:457 episode reward: total was 66.940000. running mean: 26.024558\n",
      "ep 4187: ep_len:601 episode reward: total was 71.040000. running mean: 26.474712\n",
      "ep 4187: ep_len:3 episode reward: total was 1.010000. running mean: 26.220065\n",
      "ep 4187: ep_len:534 episode reward: total was 25.230000. running mean: 26.210164\n",
      "ep 4187: ep_len:500 episode reward: total was 10.040000. running mean: 26.048463\n",
      "epsilon:0.014332 episode_count: 29316. steps_count: 12677515.000000\n",
      "Time elapsed:  38734.23061347008\n",
      "ep 4188: ep_len:657 episode reward: total was -80.660000. running mean: 24.981378\n",
      "ep 4188: ep_len:558 episode reward: total was -8.600000. running mean: 24.645564\n",
      "ep 4188: ep_len:633 episode reward: total was 39.300000. running mean: 24.792109\n",
      "ep 4188: ep_len:570 episode reward: total was 92.480000. running mean: 25.468988\n",
      "ep 4188: ep_len:3 episode reward: total was 1.010000. running mean: 25.224398\n",
      "ep 4188: ep_len:500 episode reward: total was 41.450000. running mean: 25.386654\n",
      "ep 4188: ep_len:506 episode reward: total was 20.620000. running mean: 25.338987\n",
      "epsilon:0.014288 episode_count: 29323. steps_count: 12680942.000000\n",
      "Time elapsed:  38756.70259833336\n",
      "ep 4189: ep_len:500 episode reward: total was 18.030000. running mean: 25.265897\n",
      "ep 4189: ep_len:509 episode reward: total was 23.330000. running mean: 25.246538\n",
      "ep 4189: ep_len:474 episode reward: total was 76.510000. running mean: 25.759173\n",
      "ep 4189: ep_len:618 episode reward: total was 84.690000. running mean: 26.348481\n",
      "ep 4189: ep_len:3 episode reward: total was 1.010000. running mean: 26.095096\n",
      "ep 4189: ep_len:530 episode reward: total was 40.400000. running mean: 26.238145\n",
      "ep 4189: ep_len:600 episode reward: total was 65.630000. running mean: 26.632064\n",
      "epsilon:0.014243 episode_count: 29330. steps_count: 12684176.000000\n",
      "Time elapsed:  38772.64233613014\n",
      "ep 4190: ep_len:614 episode reward: total was 78.420000. running mean: 27.149943\n",
      "ep 4190: ep_len:537 episode reward: total was 65.500000. running mean: 27.533444\n",
      "ep 4190: ep_len:558 episode reward: total was 38.960000. running mean: 27.647709\n",
      "ep 4190: ep_len:56 episode reward: total was -6.660000. running mean: 27.304632\n",
      "ep 4190: ep_len:101 episode reward: total was 32.260000. running mean: 27.354186\n",
      "ep 4190: ep_len:519 episode reward: total was 27.230000. running mean: 27.352944\n",
      "ep 4190: ep_len:589 episode reward: total was 40.500000. running mean: 27.484415\n",
      "epsilon:0.014199 episode_count: 29337. steps_count: 12687150.000000\n",
      "Time elapsed:  38777.37219953537\n",
      "ep 4191: ep_len:561 episode reward: total was 5.780000. running mean: 27.267371\n",
      "ep 4191: ep_len:500 episode reward: total was 31.660000. running mean: 27.311297\n",
      "ep 4191: ep_len:682 episode reward: total was -40.480000. running mean: 26.633384\n",
      "ep 4191: ep_len:589 episode reward: total was 97.450000. running mean: 27.341550\n",
      "ep 4191: ep_len:3 episode reward: total was 1.010000. running mean: 27.078235\n",
      "ep 4191: ep_len:551 episode reward: total was 55.310000. running mean: 27.360552\n",
      "ep 4191: ep_len:539 episode reward: total was 6.760000. running mean: 27.154547\n",
      "epsilon:0.014155 episode_count: 29344. steps_count: 12690575.000000\n",
      "Time elapsed:  38782.7013258934\n",
      "ep 4192: ep_len:745 episode reward: total was -56.760000. running mean: 26.315401\n",
      "ep 4192: ep_len:586 episode reward: total was 85.460000. running mean: 26.906847\n",
      "ep 4192: ep_len:563 episode reward: total was -24.950000. running mean: 26.388279\n",
      "ep 4192: ep_len:516 episode reward: total was -31.390000. running mean: 25.810496\n",
      "ep 4192: ep_len:84 episode reward: total was 26.150000. running mean: 25.813891\n",
      "ep 4192: ep_len:506 episode reward: total was 38.600000. running mean: 25.941752\n",
      "ep 4192: ep_len:624 episode reward: total was 57.120000. running mean: 26.253535\n",
      "epsilon:0.014110 episode_count: 29351. steps_count: 12694199.000000\n",
      "Time elapsed:  38788.69379043579\n",
      "ep 4193: ep_len:118 episode reward: total was 2.980000. running mean: 26.020799\n",
      "ep 4193: ep_len:639 episode reward: total was 100.340000. running mean: 26.763991\n",
      "ep 4193: ep_len:500 episode reward: total was 30.530000. running mean: 26.801651\n",
      "ep 4193: ep_len:500 episode reward: total was 57.820000. running mean: 27.111835\n",
      "ep 4193: ep_len:106 episode reward: total was 34.760000. running mean: 27.188316\n",
      "ep 4193: ep_len:555 episode reward: total was 35.050000. running mean: 27.266933\n",
      "ep 4193: ep_len:501 episode reward: total was 8.340000. running mean: 27.077664\n",
      "epsilon:0.014066 episode_count: 29358. steps_count: 12697118.000000\n",
      "Time elapsed:  38795.30144715309\n",
      "ep 4194: ep_len:557 episode reward: total was -56.590000. running mean: 26.240987\n",
      "ep 4194: ep_len:500 episode reward: total was 21.140000. running mean: 26.189977\n",
      "ep 4194: ep_len:356 episode reward: total was 50.480000. running mean: 26.432878\n",
      "ep 4194: ep_len:170 episode reward: total was 22.230000. running mean: 26.390849\n",
      "ep 4194: ep_len:103 episode reward: total was 31.760000. running mean: 26.444540\n",
      "ep 4194: ep_len:516 episode reward: total was 37.340000. running mean: 26.553495\n",
      "ep 4194: ep_len:563 episode reward: total was 44.700000. running mean: 26.734960\n",
      "epsilon:0.014022 episode_count: 29365. steps_count: 12699883.000000\n",
      "Time elapsed:  38802.83341193199\n",
      "ep 4195: ep_len:515 episode reward: total was 26.970000. running mean: 26.737310\n",
      "ep 4195: ep_len:512 episode reward: total was 24.830000. running mean: 26.718237\n",
      "ep 4195: ep_len:575 episode reward: total was 48.550000. running mean: 26.936555\n",
      "ep 4195: ep_len:170 episode reward: total was 11.720000. running mean: 26.784389\n",
      "ep 4195: ep_len:92 episode reward: total was 28.770000. running mean: 26.804246\n",
      "ep 4195: ep_len:611 episode reward: total was 43.490000. running mean: 26.971103\n",
      "ep 4195: ep_len:594 episode reward: total was 38.740000. running mean: 27.088792\n",
      "epsilon:0.013977 episode_count: 29372. steps_count: 12702952.000000\n",
      "Time elapsed:  38811.00229549408\n",
      "ep 4196: ep_len:114 episode reward: total was -19.070000. running mean: 26.627204\n",
      "ep 4196: ep_len:500 episode reward: total was 68.600000. running mean: 27.046932\n",
      "ep 4196: ep_len:448 episode reward: total was 64.280000. running mean: 27.419263\n",
      "ep 4196: ep_len:132 episode reward: total was 23.120000. running mean: 27.376270\n",
      "ep 4196: ep_len:126 episode reward: total was 24.380000. running mean: 27.346307\n",
      "ep 4196: ep_len:500 episode reward: total was 30.680000. running mean: 27.379644\n",
      "ep 4196: ep_len:550 episode reward: total was 19.780000. running mean: 27.303648\n",
      "epsilon:0.013933 episode_count: 29379. steps_count: 12705322.000000\n",
      "Time elapsed:  38819.173718214035\n",
      "ep 4197: ep_len:576 episode reward: total was 15.380000. running mean: 27.184411\n",
      "ep 4197: ep_len:567 episode reward: total was 43.940000. running mean: 27.351967\n",
      "ep 4197: ep_len:534 episode reward: total was -46.720000. running mean: 26.611248\n",
      "ep 4197: ep_len:566 episode reward: total was 87.530000. running mean: 27.220435\n",
      "ep 4197: ep_len:88 episode reward: total was 27.780000. running mean: 27.226031\n",
      "ep 4197: ep_len:500 episode reward: total was 32.980000. running mean: 27.283571\n",
      "ep 4197: ep_len:520 episode reward: total was 39.040000. running mean: 27.401135\n",
      "epsilon:0.013889 episode_count: 29386. steps_count: 12708673.000000\n",
      "Time elapsed:  38835.220235824585\n",
      "ep 4198: ep_len:500 episode reward: total was 74.950000. running mean: 27.876623\n",
      "ep 4198: ep_len:277 episode reward: total was 6.630000. running mean: 27.664157\n",
      "ep 4198: ep_len:539 episode reward: total was 40.520000. running mean: 27.792716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4198: ep_len:518 episode reward: total was 48.540000. running mean: 28.000189\n",
      "ep 4198: ep_len:85 episode reward: total was 25.270000. running mean: 27.972887\n",
      "ep 4198: ep_len:601 episode reward: total was 35.890000. running mean: 28.052058\n",
      "ep 4198: ep_len:521 episode reward: total was 28.990000. running mean: 28.061437\n",
      "epsilon:0.013844 episode_count: 29393. steps_count: 12711714.000000\n",
      "Time elapsed:  38843.31856894493\n",
      "ep 4199: ep_len:739 episode reward: total was -25.110000. running mean: 27.529723\n",
      "ep 4199: ep_len:532 episode reward: total was 104.220000. running mean: 28.296626\n",
      "ep 4199: ep_len:640 episode reward: total was -15.620000. running mean: 27.857459\n",
      "ep 4199: ep_len:603 episode reward: total was 68.030000. running mean: 28.259185\n",
      "ep 4199: ep_len:3 episode reward: total was 1.010000. running mean: 27.986693\n",
      "ep 4199: ep_len:500 episode reward: total was 15.830000. running mean: 27.865126\n",
      "ep 4199: ep_len:564 episode reward: total was 52.860000. running mean: 28.115075\n",
      "epsilon:0.013800 episode_count: 29400. steps_count: 12715295.000000\n",
      "Time elapsed:  38875.804928064346\n",
      "ep 4200: ep_len:500 episode reward: total was -0.610000. running mean: 27.827824\n",
      "ep 4200: ep_len:500 episode reward: total was -5.030000. running mean: 27.499246\n",
      "ep 4200: ep_len:500 episode reward: total was -1.580000. running mean: 27.208453\n",
      "ep 4200: ep_len:537 episode reward: total was 61.370000. running mean: 27.550069\n",
      "ep 4200: ep_len:3 episode reward: total was 1.010000. running mean: 27.284668\n",
      "ep 4200: ep_len:526 episode reward: total was 0.210000. running mean: 27.013921\n",
      "ep 4200: ep_len:621 episode reward: total was 82.620000. running mean: 27.569982\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.013756 episode_count: 29407. steps_count: 12718482.000000\n",
      "Time elapsed:  38889.032787323\n",
      "ep 4201: ep_len:646 episode reward: total was 2.310000. running mean: 27.317382\n",
      "ep 4201: ep_len:201 episode reward: total was 18.950000. running mean: 27.233709\n",
      "ep 4201: ep_len:643 episode reward: total was 10.240000. running mean: 27.063771\n",
      "ep 4201: ep_len:517 episode reward: total was 77.400000. running mean: 27.567134\n",
      "ep 4201: ep_len:3 episode reward: total was 1.010000. running mean: 27.301562\n",
      "ep 4201: ep_len:617 episode reward: total was 35.910000. running mean: 27.387647\n",
      "ep 4201: ep_len:536 episode reward: total was 16.960000. running mean: 27.283370\n",
      "epsilon:0.013711 episode_count: 29414. steps_count: 12721645.000000\n",
      "Time elapsed:  38896.253670692444\n",
      "ep 4202: ep_len:500 episode reward: total was 71.610000. running mean: 27.726637\n",
      "ep 4202: ep_len:522 episode reward: total was 118.750000. running mean: 28.636870\n",
      "ep 4202: ep_len:621 episode reward: total was 44.730000. running mean: 28.797802\n",
      "ep 4202: ep_len:549 episode reward: total was 67.150000. running mean: 29.181323\n",
      "ep 4202: ep_len:73 episode reward: total was 16.700000. running mean: 29.056510\n",
      "ep 4202: ep_len:232 episode reward: total was 43.080000. running mean: 29.196745\n",
      "ep 4202: ep_len:330 episode reward: total was 21.410000. running mean: 29.118878\n",
      "epsilon:0.013667 episode_count: 29421. steps_count: 12724472.000000\n",
      "Time elapsed:  38901.954515218735\n",
      "ep 4203: ep_len:536 episode reward: total was 27.230000. running mean: 29.099989\n",
      "ep 4203: ep_len:297 episode reward: total was 28.240000. running mean: 29.091389\n",
      "ep 4203: ep_len:826 episode reward: total was -114.500000. running mean: 27.655475\n",
      "ep 4203: ep_len:500 episode reward: total was 45.270000. running mean: 27.831620\n",
      "ep 4203: ep_len:3 episode reward: total was 0.000000. running mean: 27.553304\n",
      "ep 4203: ep_len:500 episode reward: total was 59.450000. running mean: 27.872271\n",
      "ep 4203: ep_len:522 episode reward: total was 38.550000. running mean: 27.979048\n",
      "epsilon:0.013623 episode_count: 29428. steps_count: 12727656.000000\n",
      "Time elapsed:  38916.32276201248\n",
      "ep 4204: ep_len:572 episode reward: total was 2.000000. running mean: 27.719258\n",
      "ep 4204: ep_len:594 episode reward: total was 30.210000. running mean: 27.744165\n",
      "ep 4204: ep_len:645 episode reward: total was 36.870000. running mean: 27.835424\n",
      "ep 4204: ep_len:500 episode reward: total was 29.170000. running mean: 27.848769\n",
      "ep 4204: ep_len:2 episode reward: total was -0.500000. running mean: 27.565282\n",
      "ep 4204: ep_len:179 episode reward: total was 42.780000. running mean: 27.717429\n",
      "ep 4204: ep_len:581 episode reward: total was 51.320000. running mean: 27.953455\n",
      "epsilon:0.013578 episode_count: 29435. steps_count: 12730729.000000\n",
      "Time elapsed:  38926.61559724808\n",
      "ep 4205: ep_len:677 episode reward: total was -11.240000. running mean: 27.561520\n",
      "ep 4205: ep_len:544 episode reward: total was 44.160000. running mean: 27.727505\n",
      "ep 4205: ep_len:500 episode reward: total was 43.460000. running mean: 27.884830\n",
      "ep 4205: ep_len:506 episode reward: total was 51.750000. running mean: 28.123482\n",
      "ep 4205: ep_len:43 episode reward: total was 21.010000. running mean: 28.052347\n",
      "ep 4205: ep_len:589 episode reward: total was 44.790000. running mean: 28.219723\n",
      "ep 4205: ep_len:504 episode reward: total was 35.490000. running mean: 28.292426\n",
      "epsilon:0.013534 episode_count: 29442. steps_count: 12734092.000000\n",
      "Time elapsed:  38941.67486834526\n",
      "ep 4206: ep_len:710 episode reward: total was 21.230000. running mean: 28.221802\n",
      "ep 4206: ep_len:619 episode reward: total was 46.760000. running mean: 28.407184\n",
      "ep 4206: ep_len:545 episode reward: total was 25.920000. running mean: 28.382312\n",
      "ep 4206: ep_len:500 episode reward: total was 30.350000. running mean: 28.401989\n",
      "ep 4206: ep_len:3 episode reward: total was 1.010000. running mean: 28.128069\n",
      "ep 4206: ep_len:685 episode reward: total was 61.880000. running mean: 28.465588\n",
      "ep 4206: ep_len:597 episode reward: total was 53.340000. running mean: 28.714332\n",
      "epsilon:0.013490 episode_count: 29449. steps_count: 12737751.000000\n",
      "Time elapsed:  38951.088218688965\n",
      "ep 4207: ep_len:566 episode reward: total was 95.340000. running mean: 29.380589\n",
      "ep 4207: ep_len:500 episode reward: total was 57.070000. running mean: 29.657483\n",
      "ep 4207: ep_len:66 episode reward: total was 10.200000. running mean: 29.462908\n",
      "ep 4207: ep_len:38 episode reward: total was -8.250000. running mean: 29.085779\n",
      "ep 4207: ep_len:3 episode reward: total was 1.010000. running mean: 28.805021\n",
      "ep 4207: ep_len:623 episode reward: total was 24.710000. running mean: 28.764071\n",
      "ep 4207: ep_len:504 episode reward: total was 19.020000. running mean: 28.666631\n",
      "epsilon:0.013445 episode_count: 29456. steps_count: 12740051.000000\n",
      "Time elapsed:  38957.17988348007\n",
      "ep 4208: ep_len:556 episode reward: total was 51.250000. running mean: 28.892464\n",
      "ep 4208: ep_len:588 episode reward: total was 60.820000. running mean: 29.211740\n",
      "ep 4208: ep_len:622 episode reward: total was 15.110000. running mean: 29.070722\n",
      "ep 4208: ep_len:170 episode reward: total was 13.220000. running mean: 28.912215\n",
      "ep 4208: ep_len:93 episode reward: total was 28.260000. running mean: 28.905693\n",
      "ep 4208: ep_len:537 episode reward: total was -110.980000. running mean: 27.506836\n",
      "ep 4208: ep_len:570 episode reward: total was 50.290000. running mean: 27.734668\n",
      "epsilon:0.013401 episode_count: 29463. steps_count: 12743187.000000\n",
      "Time elapsed:  38965.458380937576\n",
      "ep 4209: ep_len:544 episode reward: total was 68.900000. running mean: 28.146321\n",
      "ep 4209: ep_len:628 episode reward: total was 36.980000. running mean: 28.234658\n",
      "ep 4209: ep_len:538 episode reward: total was 61.100000. running mean: 28.563311\n",
      "ep 4209: ep_len:398 episode reward: total was -218.880000. running mean: 26.088878\n",
      "ep 4209: ep_len:3 episode reward: total was 1.010000. running mean: 25.838089\n",
      "ep 4209: ep_len:588 episode reward: total was 19.590000. running mean: 25.775608\n",
      "ep 4209: ep_len:500 episode reward: total was 21.910000. running mean: 25.736952\n",
      "epsilon:0.013357 episode_count: 29470. steps_count: 12746386.000000\n",
      "Time elapsed:  38970.86134171486\n",
      "ep 4210: ep_len:601 episode reward: total was 65.650000. running mean: 26.136083\n",
      "ep 4210: ep_len:500 episode reward: total was 34.020000. running mean: 26.214922\n",
      "ep 4210: ep_len:79 episode reward: total was 8.800000. running mean: 26.040773\n",
      "ep 4210: ep_len:165 episode reward: total was 17.700000. running mean: 25.957365\n",
      "ep 4210: ep_len:3 episode reward: total was 1.010000. running mean: 25.707891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4210: ep_len:500 episode reward: total was 61.900000. running mean: 26.069812\n",
      "ep 4210: ep_len:512 episode reward: total was 10.070000. running mean: 25.909814\n",
      "epsilon:0.013312 episode_count: 29477. steps_count: 12748746.000000\n",
      "Time elapsed:  38974.652216911316\n",
      "ep 4211: ep_len:214 episode reward: total was 37.270000. running mean: 26.023416\n",
      "ep 4211: ep_len:613 episode reward: total was 42.060000. running mean: 26.183782\n",
      "ep 4211: ep_len:651 episode reward: total was 40.210000. running mean: 26.324044\n",
      "ep 4211: ep_len:507 episode reward: total was -1.410000. running mean: 26.046704\n",
      "ep 4211: ep_len:100 episode reward: total was 30.780000. running mean: 26.094037\n",
      "ep 4211: ep_len:582 episode reward: total was 13.440000. running mean: 25.967496\n",
      "ep 4211: ep_len:509 episode reward: total was 24.200000. running mean: 25.949821\n",
      "epsilon:0.013268 episode_count: 29484. steps_count: 12751922.000000\n",
      "Time elapsed:  38979.50007390976\n",
      "ep 4212: ep_len:500 episode reward: total was 67.960000. running mean: 26.369923\n",
      "ep 4212: ep_len:184 episode reward: total was -5.740000. running mean: 26.048824\n",
      "ep 4212: ep_len:428 episode reward: total was 69.400000. running mean: 26.482336\n",
      "ep 4212: ep_len:501 episode reward: total was 80.200000. running mean: 27.019512\n",
      "ep 4212: ep_len:3 episode reward: total was 1.010000. running mean: 26.759417\n",
      "ep 4212: ep_len:679 episode reward: total was -22.670000. running mean: 26.265123\n",
      "ep 4212: ep_len:533 episode reward: total was 59.390000. running mean: 26.596372\n",
      "epsilon:0.013224 episode_count: 29491. steps_count: 12754750.000000\n",
      "Time elapsed:  38983.898389577866\n",
      "ep 4213: ep_len:500 episode reward: total was 84.520000. running mean: 27.175608\n",
      "ep 4213: ep_len:616 episode reward: total was 71.360000. running mean: 27.617452\n",
      "ep 4213: ep_len:538 episode reward: total was 29.590000. running mean: 27.637177\n",
      "ep 4213: ep_len:557 episode reward: total was -4.890000. running mean: 27.311906\n",
      "ep 4213: ep_len:3 episode reward: total was 1.010000. running mean: 27.048887\n",
      "ep 4213: ep_len:593 episode reward: total was 26.220000. running mean: 27.040598\n",
      "ep 4213: ep_len:607 episode reward: total was 70.380000. running mean: 27.473992\n",
      "epsilon:0.013179 episode_count: 29498. steps_count: 12758164.000000\n",
      "Time elapsed:  38991.859934806824\n",
      "ep 4214: ep_len:625 episode reward: total was 42.260000. running mean: 27.621852\n",
      "ep 4214: ep_len:612 episode reward: total was 61.400000. running mean: 27.959633\n",
      "ep 4214: ep_len:500 episode reward: total was -57.240000. running mean: 27.107637\n",
      "ep 4214: ep_len:564 episode reward: total was 112.110000. running mean: 27.957661\n",
      "ep 4214: ep_len:3 episode reward: total was 1.010000. running mean: 27.688184\n",
      "ep 4214: ep_len:528 episode reward: total was 45.820000. running mean: 27.869502\n",
      "ep 4214: ep_len:284 episode reward: total was 27.820000. running mean: 27.869007\n",
      "epsilon:0.013135 episode_count: 29505. steps_count: 12761280.000000\n",
      "Time elapsed:  39004.01499414444\n",
      "ep 4215: ep_len:564 episode reward: total was 80.720000. running mean: 28.397517\n",
      "ep 4215: ep_len:616 episode reward: total was 79.390000. running mean: 28.907442\n",
      "ep 4215: ep_len:394 episode reward: total was 86.650000. running mean: 29.484867\n",
      "ep 4215: ep_len:602 episode reward: total was 71.090000. running mean: 29.900919\n",
      "ep 4215: ep_len:129 episode reward: total was 40.870000. running mean: 30.010610\n",
      "ep 4215: ep_len:586 episode reward: total was 49.030000. running mean: 30.200804\n",
      "ep 4215: ep_len:500 episode reward: total was 32.700000. running mean: 30.225795\n",
      "epsilon:0.013091 episode_count: 29512. steps_count: 12764671.000000\n",
      "Time elapsed:  39012.9456717968\n",
      "ep 4216: ep_len:587 episode reward: total was 63.660000. running mean: 30.560138\n",
      "ep 4216: ep_len:542 episode reward: total was 28.810000. running mean: 30.542636\n",
      "ep 4216: ep_len:566 episode reward: total was -4.710000. running mean: 30.190110\n",
      "ep 4216: ep_len:49 episode reward: total was 4.330000. running mean: 29.931509\n",
      "ep 4216: ep_len:95 episode reward: total was 15.800000. running mean: 29.790194\n",
      "ep 4216: ep_len:520 episode reward: total was 29.370000. running mean: 29.785992\n",
      "ep 4216: ep_len:567 episode reward: total was 51.400000. running mean: 30.002132\n",
      "epsilon:0.013046 episode_count: 29519. steps_count: 12767597.000000\n",
      "Time elapsed:  39020.738265275955\n",
      "ep 4217: ep_len:245 episode reward: total was 10.750000. running mean: 29.809610\n",
      "ep 4217: ep_len:502 episode reward: total was 37.990000. running mean: 29.891414\n",
      "ep 4217: ep_len:392 episode reward: total was 72.150000. running mean: 30.314000\n",
      "ep 4217: ep_len:56 episode reward: total was -1.660000. running mean: 29.994260\n",
      "ep 4217: ep_len:3 episode reward: total was 1.010000. running mean: 29.704418\n",
      "ep 4217: ep_len:571 episode reward: total was 56.030000. running mean: 29.967673\n",
      "ep 4217: ep_len:500 episode reward: total was 27.740000. running mean: 29.945397\n",
      "epsilon:0.013002 episode_count: 29526. steps_count: 12769866.000000\n",
      "Time elapsed:  39027.804064035416\n",
      "ep 4218: ep_len:535 episode reward: total was -71.370000. running mean: 28.932243\n",
      "ep 4218: ep_len:554 episode reward: total was 45.870000. running mean: 29.101620\n",
      "ep 4218: ep_len:642 episode reward: total was 2.950000. running mean: 28.840104\n",
      "ep 4218: ep_len:505 episode reward: total was 80.260000. running mean: 29.354303\n",
      "ep 4218: ep_len:3 episode reward: total was 1.010000. running mean: 29.070860\n",
      "ep 4218: ep_len:508 episode reward: total was 31.450000. running mean: 29.094651\n",
      "ep 4218: ep_len:585 episode reward: total was 73.040000. running mean: 29.534105\n",
      "epsilon:0.012958 episode_count: 29533. steps_count: 12773198.000000\n",
      "Time elapsed:  39036.51399087906\n",
      "ep 4219: ep_len:545 episode reward: total was 107.650000. running mean: 30.315264\n",
      "ep 4219: ep_len:500 episode reward: total was 130.120000. running mean: 31.313311\n",
      "ep 4219: ep_len:500 episode reward: total was 61.900000. running mean: 31.619178\n",
      "ep 4219: ep_len:500 episode reward: total was 29.670000. running mean: 31.599686\n",
      "ep 4219: ep_len:3 episode reward: total was 1.010000. running mean: 31.293789\n",
      "ep 4219: ep_len:527 episode reward: total was -23.960000. running mean: 30.741252\n",
      "ep 4219: ep_len:279 episode reward: total was 10.350000. running mean: 30.537339\n",
      "epsilon:0.012913 episode_count: 29540. steps_count: 12776052.000000\n",
      "Time elapsed:  39044.43030166626\n",
      "ep 4220: ep_len:263 episode reward: total was 15.830000. running mean: 30.390266\n",
      "ep 4220: ep_len:563 episode reward: total was 144.830000. running mean: 31.534663\n",
      "ep 4220: ep_len:539 episode reward: total was -3.650000. running mean: 31.182816\n",
      "ep 4220: ep_len:500 episode reward: total was 71.540000. running mean: 31.586388\n",
      "ep 4220: ep_len:50 episode reward: total was 23.010000. running mean: 31.500624\n",
      "ep 4220: ep_len:608 episode reward: total was 2.870000. running mean: 31.214318\n",
      "ep 4220: ep_len:571 episode reward: total was 53.390000. running mean: 31.436075\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.012869 episode_count: 29547. steps_count: 12779146.000000\n",
      "Time elapsed:  39064.31623482704\n",
      "ep 4221: ep_len:500 episode reward: total was 75.930000. running mean: 31.881014\n",
      "ep 4221: ep_len:666 episode reward: total was 69.590000. running mean: 32.258104\n",
      "ep 4221: ep_len:567 episode reward: total was -2.450000. running mean: 31.911023\n",
      "ep 4221: ep_len:500 episode reward: total was 28.130000. running mean: 31.873213\n",
      "ep 4221: ep_len:3 episode reward: total was 1.010000. running mean: 31.564581\n",
      "ep 4221: ep_len:554 episode reward: total was 27.280000. running mean: 31.521735\n",
      "ep 4221: ep_len:500 episode reward: total was 44.900000. running mean: 31.655517\n",
      "epsilon:0.012825 episode_count: 29554. steps_count: 12782436.000000\n",
      "Time elapsed:  39072.97609496117\n",
      "ep 4222: ep_len:624 episode reward: total was 14.390000. running mean: 31.482862\n",
      "ep 4222: ep_len:510 episode reward: total was 7.750000. running mean: 31.245534\n",
      "ep 4222: ep_len:501 episode reward: total was 16.770000. running mean: 31.100778\n",
      "ep 4222: ep_len:583 episode reward: total was 73.650000. running mean: 31.526271\n",
      "ep 4222: ep_len:87 episode reward: total was 24.770000. running mean: 31.458708\n",
      "ep 4222: ep_len:590 episode reward: total was 38.830000. running mean: 31.532421\n",
      "ep 4222: ep_len:507 episode reward: total was 30.320000. running mean: 31.520297\n",
      "epsilon:0.012780 episode_count: 29561. steps_count: 12785838.000000\n",
      "Time elapsed:  39081.88380026817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4223: ep_len:731 episode reward: total was 26.190000. running mean: 31.466994\n",
      "ep 4223: ep_len:500 episode reward: total was 130.060000. running mean: 32.452924\n",
      "ep 4223: ep_len:599 episode reward: total was 21.910000. running mean: 32.347494\n",
      "ep 4223: ep_len:595 episode reward: total was 87.380000. running mean: 32.897819\n",
      "ep 4223: ep_len:3 episode reward: total was 1.010000. running mean: 32.578941\n",
      "ep 4223: ep_len:535 episode reward: total was -173.110000. running mean: 30.522052\n",
      "ep 4223: ep_len:282 episode reward: total was 23.330000. running mean: 30.450131\n",
      "epsilon:0.012736 episode_count: 29568. steps_count: 12789083.000000\n",
      "Time elapsed:  39090.44078874588\n",
      "ep 4224: ep_len:625 episode reward: total was 59.310000. running mean: 30.738730\n",
      "ep 4224: ep_len:500 episode reward: total was 129.970000. running mean: 31.731043\n",
      "ep 4224: ep_len:500 episode reward: total was 12.650000. running mean: 31.540232\n",
      "ep 4224: ep_len:132 episode reward: total was 21.620000. running mean: 31.441030\n",
      "ep 4224: ep_len:3 episode reward: total was 1.010000. running mean: 31.136720\n",
      "ep 4224: ep_len:521 episode reward: total was 4.920000. running mean: 30.874552\n",
      "ep 4224: ep_len:507 episode reward: total was -12.110000. running mean: 30.444707\n",
      "epsilon:0.012692 episode_count: 29575. steps_count: 12791871.000000\n",
      "Time elapsed:  39104.50518131256\n",
      "ep 4225: ep_len:654 episode reward: total was -17.470000. running mean: 29.965560\n",
      "ep 4225: ep_len:500 episode reward: total was 21.420000. running mean: 29.880104\n",
      "ep 4225: ep_len:559 episode reward: total was -7.060000. running mean: 29.510703\n",
      "ep 4225: ep_len:500 episode reward: total was 13.030000. running mean: 29.345896\n",
      "ep 4225: ep_len:112 episode reward: total was 38.770000. running mean: 29.440137\n",
      "ep 4225: ep_len:589 episode reward: total was 31.260000. running mean: 29.458336\n",
      "ep 4225: ep_len:619 episode reward: total was 31.650000. running mean: 29.480253\n",
      "epsilon:0.012647 episode_count: 29582. steps_count: 12795404.000000\n",
      "Time elapsed:  39113.71518301964\n",
      "ep 4226: ep_len:649 episode reward: total was -18.990000. running mean: 28.995550\n",
      "ep 4226: ep_len:530 episode reward: total was 38.290000. running mean: 29.088494\n",
      "ep 4226: ep_len:593 episode reward: total was 33.300000. running mean: 29.130610\n",
      "ep 4226: ep_len:607 episode reward: total was 110.770000. running mean: 29.947003\n",
      "ep 4226: ep_len:51 episode reward: total was 25.010000. running mean: 29.897633\n",
      "ep 4226: ep_len:285 episode reward: total was 22.690000. running mean: 29.825557\n",
      "ep 4226: ep_len:540 episode reward: total was 26.910000. running mean: 29.796402\n",
      "epsilon:0.012603 episode_count: 29589. steps_count: 12798659.000000\n",
      "Time elapsed:  39122.3131005764\n",
      "ep 4227: ep_len:541 episode reward: total was 80.090000. running mean: 30.299337\n",
      "ep 4227: ep_len:512 episode reward: total was 26.750000. running mean: 30.263844\n",
      "ep 4227: ep_len:515 episode reward: total was 56.700000. running mean: 30.528206\n",
      "ep 4227: ep_len:502 episode reward: total was -47.360000. running mean: 29.749324\n",
      "ep 4227: ep_len:3 episode reward: total was 1.010000. running mean: 29.461930\n",
      "ep 4227: ep_len:560 episode reward: total was 66.420000. running mean: 29.831511\n",
      "ep 4227: ep_len:571 episode reward: total was 28.360000. running mean: 29.816796\n",
      "epsilon:0.012559 episode_count: 29596. steps_count: 12801863.000000\n",
      "Time elapsed:  39130.70012331009\n",
      "ep 4228: ep_len:570 episode reward: total was 51.060000. running mean: 30.029228\n",
      "ep 4228: ep_len:500 episode reward: total was 118.670000. running mean: 30.915636\n",
      "ep 4228: ep_len:508 episode reward: total was 27.120000. running mean: 30.877679\n",
      "ep 4228: ep_len:545 episode reward: total was 88.710000. running mean: 31.456003\n",
      "ep 4228: ep_len:115 episode reward: total was 29.860000. running mean: 31.440043\n",
      "ep 4228: ep_len:523 episode reward: total was -60.670000. running mean: 30.518942\n",
      "ep 4228: ep_len:533 episode reward: total was 10.810000. running mean: 30.321853\n",
      "epsilon:0.012514 episode_count: 29603. steps_count: 12805157.000000\n",
      "Time elapsed:  39139.370894908905\n",
      "ep 4229: ep_len:500 episode reward: total was 24.620000. running mean: 30.264834\n",
      "ep 4229: ep_len:248 episode reward: total was -25.070000. running mean: 29.711486\n",
      "ep 4229: ep_len:589 episode reward: total was -14.620000. running mean: 29.268171\n",
      "ep 4229: ep_len:500 episode reward: total was 25.860000. running mean: 29.234089\n",
      "ep 4229: ep_len:3 episode reward: total was 1.010000. running mean: 28.951848\n",
      "ep 4229: ep_len:500 episode reward: total was 18.700000. running mean: 28.849330\n",
      "ep 4229: ep_len:278 episode reward: total was 25.890000. running mean: 28.819737\n",
      "epsilon:0.012470 episode_count: 29610. steps_count: 12807775.000000\n",
      "Time elapsed:  39146.56186103821\n",
      "ep 4230: ep_len:628 episode reward: total was -40.200000. running mean: 28.129539\n",
      "ep 4230: ep_len:500 episode reward: total was 47.650000. running mean: 28.324744\n",
      "ep 4230: ep_len:627 episode reward: total was 22.780000. running mean: 28.269296\n",
      "ep 4230: ep_len:502 episode reward: total was 92.910000. running mean: 28.915703\n",
      "ep 4230: ep_len:3 episode reward: total was 1.010000. running mean: 28.636646\n",
      "ep 4230: ep_len:533 episode reward: total was 47.340000. running mean: 28.823680\n",
      "ep 4230: ep_len:526 episode reward: total was 31.620000. running mean: 28.851643\n",
      "epsilon:0.012426 episode_count: 29617. steps_count: 12811094.000000\n",
      "Time elapsed:  39155.46042251587\n",
      "ep 4231: ep_len:246 episode reward: total was 19.670000. running mean: 28.759827\n",
      "ep 4231: ep_len:500 episode reward: total was 49.260000. running mean: 28.964828\n",
      "ep 4231: ep_len:551 episode reward: total was 36.970000. running mean: 29.044880\n",
      "ep 4231: ep_len:593 episode reward: total was 73.920000. running mean: 29.493631\n",
      "ep 4231: ep_len:3 episode reward: total was 1.010000. running mean: 29.208795\n",
      "ep 4231: ep_len:279 episode reward: total was 40.810000. running mean: 29.324807\n",
      "ep 4231: ep_len:558 episode reward: total was 35.770000. running mean: 29.389259\n",
      "epsilon:0.012381 episode_count: 29624. steps_count: 12813824.000000\n",
      "Time elapsed:  39163.07859301567\n",
      "ep 4232: ep_len:579 episode reward: total was 89.380000. running mean: 29.989166\n",
      "ep 4232: ep_len:609 episode reward: total was 139.880000. running mean: 31.088075\n",
      "ep 4232: ep_len:500 episode reward: total was 44.160000. running mean: 31.218794\n",
      "ep 4232: ep_len:505 episode reward: total was 25.050000. running mean: 31.157106\n",
      "ep 4232: ep_len:3 episode reward: total was 1.010000. running mean: 30.855635\n",
      "ep 4232: ep_len:519 episode reward: total was 36.790000. running mean: 30.914979\n",
      "ep 4232: ep_len:315 episode reward: total was 17.510000. running mean: 30.780929\n",
      "epsilon:0.012337 episode_count: 29631. steps_count: 12816854.000000\n",
      "Time elapsed:  39171.13676214218\n",
      "ep 4233: ep_len:589 episode reward: total was 72.280000. running mean: 31.195920\n",
      "ep 4233: ep_len:556 episode reward: total was 6.060000. running mean: 30.944560\n",
      "ep 4233: ep_len:552 episode reward: total was 16.240000. running mean: 30.797515\n",
      "ep 4233: ep_len:500 episode reward: total was 33.210000. running mean: 30.821640\n",
      "ep 4233: ep_len:96 episode reward: total was 30.770000. running mean: 30.821123\n",
      "ep 4233: ep_len:590 episode reward: total was -67.350000. running mean: 29.839412\n",
      "ep 4233: ep_len:564 episode reward: total was 17.950000. running mean: 29.720518\n",
      "epsilon:0.012293 episode_count: 29638. steps_count: 12820301.000000\n",
      "Time elapsed:  39185.17802977562\n",
      "ep 4234: ep_len:582 episode reward: total was 110.390000. running mean: 30.527213\n",
      "ep 4234: ep_len:566 episode reward: total was 113.480000. running mean: 31.356741\n",
      "ep 4234: ep_len:578 episode reward: total was 11.500000. running mean: 31.158173\n",
      "ep 4234: ep_len:571 episode reward: total was 77.140000. running mean: 31.617991\n",
      "ep 4234: ep_len:3 episode reward: total was 1.010000. running mean: 31.311912\n",
      "ep 4234: ep_len:238 episode reward: total was 49.420000. running mean: 31.492992\n",
      "ep 4234: ep_len:600 episode reward: total was 10.630000. running mean: 31.284362\n",
      "epsilon:0.012248 episode_count: 29645. steps_count: 12823439.000000\n",
      "Time elapsed:  39193.05949354172\n",
      "ep 4235: ep_len:614 episode reward: total was 35.380000. running mean: 31.325319\n",
      "ep 4235: ep_len:500 episode reward: total was 40.790000. running mean: 31.419966\n",
      "ep 4235: ep_len:628 episode reward: total was 64.660000. running mean: 31.752366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4235: ep_len:500 episode reward: total was 31.360000. running mean: 31.748442\n",
      "ep 4235: ep_len:98 episode reward: total was 30.270000. running mean: 31.733658\n",
      "ep 4235: ep_len:614 episode reward: total was 63.050000. running mean: 32.046821\n",
      "ep 4235: ep_len:516 episode reward: total was 53.830000. running mean: 32.264653\n",
      "epsilon:0.012204 episode_count: 29652. steps_count: 12826909.000000\n",
      "Time elapsed:  39202.19275331497\n",
      "ep 4236: ep_len:500 episode reward: total was 35.780000. running mean: 32.299807\n",
      "ep 4236: ep_len:500 episode reward: total was 62.050000. running mean: 32.597309\n",
      "ep 4236: ep_len:500 episode reward: total was 12.560000. running mean: 32.396935\n",
      "ep 4236: ep_len:167 episode reward: total was 12.270000. running mean: 32.195666\n",
      "ep 4236: ep_len:3 episode reward: total was 1.010000. running mean: 31.883809\n",
      "ep 4236: ep_len:500 episode reward: total was 49.740000. running mean: 32.062371\n",
      "ep 4236: ep_len:523 episode reward: total was 53.440000. running mean: 32.276148\n",
      "epsilon:0.012160 episode_count: 29659. steps_count: 12829602.000000\n",
      "Time elapsed:  39215.79114770889\n",
      "ep 4237: ep_len:631 episode reward: total was -14.610000. running mean: 31.807286\n",
      "ep 4237: ep_len:500 episode reward: total was 14.090000. running mean: 31.630113\n",
      "ep 4237: ep_len:500 episode reward: total was 18.670000. running mean: 31.500512\n",
      "ep 4237: ep_len:526 episode reward: total was 38.460000. running mean: 31.570107\n",
      "ep 4237: ep_len:3 episode reward: total was 1.010000. running mean: 31.264506\n",
      "ep 4237: ep_len:600 episode reward: total was 15.760000. running mean: 31.109461\n",
      "ep 4237: ep_len:500 episode reward: total was -13.350000. running mean: 30.664866\n",
      "epsilon:0.012115 episode_count: 29666. steps_count: 12832862.000000\n",
      "Time elapsed:  39230.89818930626\n",
      "ep 4238: ep_len:500 episode reward: total was 52.310000. running mean: 30.881318\n",
      "ep 4238: ep_len:580 episode reward: total was 71.790000. running mean: 31.290404\n",
      "ep 4238: ep_len:649 episode reward: total was 12.110000. running mean: 31.098600\n",
      "ep 4238: ep_len:501 episode reward: total was 28.320000. running mean: 31.070814\n",
      "ep 4238: ep_len:3 episode reward: total was 1.010000. running mean: 30.770206\n",
      "ep 4238: ep_len:628 episode reward: total was 54.630000. running mean: 31.008804\n",
      "ep 4238: ep_len:532 episode reward: total was 18.760000. running mean: 30.886316\n",
      "epsilon:0.012071 episode_count: 29673. steps_count: 12836255.000000\n",
      "Time elapsed:  39246.379375219345\n",
      "ep 4239: ep_len:563 episode reward: total was 55.610000. running mean: 31.133553\n",
      "ep 4239: ep_len:500 episode reward: total was 39.210000. running mean: 31.214317\n",
      "ep 4239: ep_len:569 episode reward: total was 49.760000. running mean: 31.399774\n",
      "ep 4239: ep_len:507 episode reward: total was 38.800000. running mean: 31.473777\n",
      "ep 4239: ep_len:74 episode reward: total was 23.200000. running mean: 31.391039\n",
      "ep 4239: ep_len:582 episode reward: total was 46.800000. running mean: 31.545128\n",
      "ep 4239: ep_len:275 episode reward: total was -26.160000. running mean: 30.968077\n",
      "epsilon:0.012027 episode_count: 29680. steps_count: 12839325.000000\n",
      "Time elapsed:  39254.45557188988\n",
      "ep 4240: ep_len:604 episode reward: total was 72.710000. running mean: 31.385496\n",
      "ep 4240: ep_len:500 episode reward: total was 102.910000. running mean: 32.100741\n",
      "ep 4240: ep_len:577 episode reward: total was -6.540000. running mean: 31.714334\n",
      "ep 4240: ep_len:500 episode reward: total was 33.610000. running mean: 31.733291\n",
      "ep 4240: ep_len:80 episode reward: total was 19.620000. running mean: 31.612158\n",
      "ep 4240: ep_len:603 episode reward: total was 36.300000. running mean: 31.659036\n",
      "ep 4240: ep_len:610 episode reward: total was 61.480000. running mean: 31.957246\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.011982 episode_count: 29687. steps_count: 12842799.000000\n",
      "Time elapsed:  39268.21996879578\n",
      "ep 4241: ep_len:500 episode reward: total was 80.080000. running mean: 32.438473\n",
      "ep 4241: ep_len:586 episode reward: total was 132.760000. running mean: 33.441689\n",
      "ep 4241: ep_len:526 episode reward: total was 57.010000. running mean: 33.677372\n",
      "ep 4241: ep_len:518 episode reward: total was 28.550000. running mean: 33.626098\n",
      "ep 4241: ep_len:3 episode reward: total was 1.010000. running mean: 33.299937\n",
      "ep 4241: ep_len:527 episode reward: total was 16.490000. running mean: 33.131838\n",
      "ep 4241: ep_len:500 episode reward: total was 61.080000. running mean: 33.411319\n",
      "epsilon:0.011938 episode_count: 29694. steps_count: 12845959.000000\n",
      "Time elapsed:  39276.45380473137\n",
      "ep 4242: ep_len:118 episode reward: total was 14.830000. running mean: 33.225506\n",
      "ep 4242: ep_len:534 episode reward: total was 16.180000. running mean: 33.055051\n",
      "ep 4242: ep_len:586 episode reward: total was -24.800000. running mean: 32.476501\n",
      "ep 4242: ep_len:397 episode reward: total was -8.290000. running mean: 32.068835\n",
      "ep 4242: ep_len:3 episode reward: total was 1.010000. running mean: 31.758247\n",
      "ep 4242: ep_len:556 episode reward: total was 27.830000. running mean: 31.718965\n",
      "ep 4242: ep_len:579 episode reward: total was 51.720000. running mean: 31.918975\n",
      "epsilon:0.011894 episode_count: 29701. steps_count: 12848732.000000\n",
      "Time elapsed:  39283.94787287712\n",
      "ep 4243: ep_len:500 episode reward: total was 103.900000. running mean: 32.638785\n",
      "ep 4243: ep_len:500 episode reward: total was 69.350000. running mean: 33.005897\n",
      "ep 4243: ep_len:599 episode reward: total was 36.200000. running mean: 33.037838\n",
      "ep 4243: ep_len:384 episode reward: total was 45.560000. running mean: 33.163060\n",
      "ep 4243: ep_len:3 episode reward: total was 1.010000. running mean: 32.841529\n",
      "ep 4243: ep_len:594 episode reward: total was -119.290000. running mean: 31.320214\n",
      "ep 4243: ep_len:543 episode reward: total was 44.340000. running mean: 31.450412\n",
      "epsilon:0.011849 episode_count: 29708. steps_count: 12851855.000000\n",
      "Time elapsed:  39292.172068834305\n",
      "ep 4244: ep_len:583 episode reward: total was -11.200000. running mean: 31.023908\n",
      "ep 4244: ep_len:510 episode reward: total was 130.560000. running mean: 32.019269\n",
      "ep 4244: ep_len:516 episode reward: total was 47.820000. running mean: 32.177276\n",
      "ep 4244: ep_len:500 episode reward: total was 73.920000. running mean: 32.594703\n",
      "ep 4244: ep_len:3 episode reward: total was 1.010000. running mean: 32.278856\n",
      "ep 4244: ep_len:638 episode reward: total was 44.560000. running mean: 32.401668\n",
      "ep 4244: ep_len:506 episode reward: total was 26.280000. running mean: 32.340451\n",
      "epsilon:0.011805 episode_count: 29715. steps_count: 12855111.000000\n",
      "Time elapsed:  39307.28279232979\n",
      "ep 4245: ep_len:125 episode reward: total was 14.530000. running mean: 32.162347\n",
      "ep 4245: ep_len:617 episode reward: total was 106.740000. running mean: 32.908123\n",
      "ep 4245: ep_len:664 episode reward: total was 1.150000. running mean: 32.590542\n",
      "ep 4245: ep_len:500 episode reward: total was 82.440000. running mean: 33.089036\n",
      "ep 4245: ep_len:55 episode reward: total was 24.500000. running mean: 33.003146\n",
      "ep 4245: ep_len:511 episode reward: total was 47.540000. running mean: 33.148515\n",
      "ep 4245: ep_len:556 episode reward: total was 41.760000. running mean: 33.234630\n",
      "epsilon:0.011761 episode_count: 29722. steps_count: 12858139.000000\n",
      "Time elapsed:  39315.16353273392\n",
      "ep 4246: ep_len:522 episode reward: total was -7.240000. running mean: 32.829883\n",
      "ep 4246: ep_len:500 episode reward: total was 96.150000. running mean: 33.463084\n",
      "ep 4246: ep_len:383 episode reward: total was 65.510000. running mean: 33.783554\n",
      "ep 4246: ep_len:386 episode reward: total was 51.090000. running mean: 33.956618\n",
      "ep 4246: ep_len:3 episode reward: total was 1.010000. running mean: 33.627152\n",
      "ep 4246: ep_len:598 episode reward: total was 65.740000. running mean: 33.948280\n",
      "ep 4246: ep_len:615 episode reward: total was 45.620000. running mean: 34.064997\n",
      "epsilon:0.011716 episode_count: 29729. steps_count: 12861146.000000\n",
      "Time elapsed:  39323.163992643356\n",
      "ep 4247: ep_len:632 episode reward: total was 16.460000. running mean: 33.888948\n",
      "ep 4247: ep_len:334 episode reward: total was -33.550000. running mean: 33.214558\n",
      "ep 4247: ep_len:500 episode reward: total was 22.830000. running mean: 33.110712\n",
      "ep 4247: ep_len:420 episode reward: total was 26.200000. running mean: 33.041605\n",
      "ep 4247: ep_len:96 episode reward: total was 29.270000. running mean: 33.003889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4247: ep_len:500 episode reward: total was 20.070000. running mean: 32.874550\n",
      "ep 4247: ep_len:500 episode reward: total was 49.900000. running mean: 33.044805\n",
      "epsilon:0.011672 episode_count: 29736. steps_count: 12864128.000000\n",
      "Time elapsed:  39328.72437739372\n",
      "ep 4248: ep_len:500 episode reward: total was 113.540000. running mean: 33.849757\n",
      "ep 4248: ep_len:532 episode reward: total was 108.390000. running mean: 34.595159\n",
      "ep 4248: ep_len:568 episode reward: total was 69.310000. running mean: 34.942308\n",
      "ep 4248: ep_len:530 episode reward: total was -19.010000. running mean: 34.402785\n",
      "ep 4248: ep_len:3 episode reward: total was 1.010000. running mean: 34.068857\n",
      "ep 4248: ep_len:500 episode reward: total was 8.670000. running mean: 33.814868\n",
      "ep 4248: ep_len:529 episode reward: total was 51.250000. running mean: 33.989220\n",
      "epsilon:0.011628 episode_count: 29743. steps_count: 12867290.000000\n",
      "Time elapsed:  39334.85041284561\n",
      "ep 4249: ep_len:500 episode reward: total was 81.730000. running mean: 34.466627\n",
      "ep 4249: ep_len:500 episode reward: total was 24.900000. running mean: 34.370961\n",
      "ep 4249: ep_len:612 episode reward: total was 44.510000. running mean: 34.472351\n",
      "ep 4249: ep_len:411 episode reward: total was 44.740000. running mean: 34.575028\n",
      "ep 4249: ep_len:3 episode reward: total was 1.010000. running mean: 34.239378\n",
      "ep 4249: ep_len:623 episode reward: total was 33.030000. running mean: 34.227284\n",
      "ep 4249: ep_len:522 episode reward: total was 21.370000. running mean: 34.098711\n",
      "epsilon:0.011583 episode_count: 29750. steps_count: 12870461.000000\n",
      "Time elapsed:  39343.21709728241\n",
      "ep 4250: ep_len:597 episode reward: total was 103.560000. running mean: 34.793324\n",
      "ep 4250: ep_len:617 episode reward: total was 99.510000. running mean: 35.440491\n",
      "ep 4250: ep_len:500 episode reward: total was 43.850000. running mean: 35.524586\n",
      "ep 4250: ep_len:388 episode reward: total was 22.200000. running mean: 35.391340\n",
      "ep 4250: ep_len:91 episode reward: total was 29.280000. running mean: 35.330227\n",
      "ep 4250: ep_len:582 episode reward: total was 10.740000. running mean: 35.084324\n",
      "ep 4250: ep_len:576 episode reward: total was 53.940000. running mean: 35.272881\n",
      "epsilon:0.011539 episode_count: 29757. steps_count: 12873812.000000\n",
      "Time elapsed:  39351.908537864685\n",
      "ep 4251: ep_len:534 episode reward: total was -42.800000. running mean: 34.492152\n",
      "ep 4251: ep_len:554 episode reward: total was 39.500000. running mean: 34.542231\n",
      "ep 4251: ep_len:620 episode reward: total was 28.310000. running mean: 34.479908\n",
      "ep 4251: ep_len:500 episode reward: total was 31.760000. running mean: 34.452709\n",
      "ep 4251: ep_len:3 episode reward: total was 1.010000. running mean: 34.118282\n",
      "ep 4251: ep_len:624 episode reward: total was 33.770000. running mean: 34.114799\n",
      "ep 4251: ep_len:500 episode reward: total was 45.870000. running mean: 34.232351\n",
      "epsilon:0.011495 episode_count: 29764. steps_count: 12877147.000000\n",
      "Time elapsed:  39360.65138411522\n",
      "ep 4252: ep_len:252 episode reward: total was 29.370000. running mean: 34.183728\n",
      "ep 4252: ep_len:502 episode reward: total was 37.460000. running mean: 34.216491\n",
      "ep 4252: ep_len:681 episode reward: total was -8.990000. running mean: 33.784426\n",
      "ep 4252: ep_len:500 episode reward: total was 48.080000. running mean: 33.927381\n",
      "ep 4252: ep_len:3 episode reward: total was 1.010000. running mean: 33.598208\n",
      "ep 4252: ep_len:614 episode reward: total was 51.220000. running mean: 33.774426\n",
      "ep 4252: ep_len:304 episode reward: total was 18.560000. running mean: 33.622281\n",
      "epsilon:0.011450 episode_count: 29771. steps_count: 12880003.000000\n",
      "Time elapsed:  39368.217628479004\n",
      "ep 4253: ep_len:203 episode reward: total was 29.750000. running mean: 33.583558\n",
      "ep 4253: ep_len:500 episode reward: total was 61.210000. running mean: 33.859823\n",
      "ep 4253: ep_len:500 episode reward: total was 28.310000. running mean: 33.804325\n",
      "ep 4253: ep_len:551 episode reward: total was 94.370000. running mean: 34.409981\n",
      "ep 4253: ep_len:3 episode reward: total was 1.010000. running mean: 34.075982\n",
      "ep 4253: ep_len:599 episode reward: total was 55.650000. running mean: 34.291722\n",
      "ep 4253: ep_len:549 episode reward: total was 0.590000. running mean: 33.954705\n",
      "epsilon:0.011406 episode_count: 29778. steps_count: 12882908.000000\n",
      "Time elapsed:  39375.96067261696\n",
      "ep 4254: ep_len:500 episode reward: total was 87.610000. running mean: 34.491258\n",
      "ep 4254: ep_len:500 episode reward: total was 51.260000. running mean: 34.658945\n",
      "ep 4254: ep_len:500 episode reward: total was 28.250000. running mean: 34.594855\n",
      "ep 4254: ep_len:561 episode reward: total was 75.930000. running mean: 35.008207\n",
      "ep 4254: ep_len:119 episode reward: total was 19.810000. running mean: 34.856225\n",
      "ep 4254: ep_len:143 episode reward: total was 26.040000. running mean: 34.768063\n",
      "ep 4254: ep_len:500 episode reward: total was 42.230000. running mean: 34.842682\n",
      "epsilon:0.011362 episode_count: 29785. steps_count: 12885731.000000\n",
      "Time elapsed:  39381.16566109657\n",
      "ep 4255: ep_len:744 episode reward: total was 22.380000. running mean: 34.718055\n",
      "ep 4255: ep_len:594 episode reward: total was 93.450000. running mean: 35.305375\n",
      "ep 4255: ep_len:550 episode reward: total was 60.710000. running mean: 35.559421\n",
      "ep 4255: ep_len:130 episode reward: total was 23.100000. running mean: 35.434827\n",
      "ep 4255: ep_len:3 episode reward: total was 1.010000. running mean: 35.090578\n",
      "ep 4255: ep_len:522 episode reward: total was -15.690000. running mean: 34.582773\n",
      "ep 4255: ep_len:603 episode reward: total was 81.480000. running mean: 35.051745\n",
      "epsilon:0.011317 episode_count: 29792. steps_count: 12888877.000000\n",
      "Time elapsed:  39385.99167442322\n",
      "ep 4256: ep_len:500 episode reward: total was 83.120000. running mean: 35.532427\n",
      "ep 4256: ep_len:543 episode reward: total was 0.100000. running mean: 35.178103\n",
      "ep 4256: ep_len:599 episode reward: total was 24.030000. running mean: 35.066622\n",
      "ep 4256: ep_len:553 episode reward: total was 84.290000. running mean: 35.558856\n",
      "ep 4256: ep_len:3 episode reward: total was 1.010000. running mean: 35.213367\n",
      "ep 4256: ep_len:500 episode reward: total was -26.460000. running mean: 34.596634\n",
      "ep 4256: ep_len:560 episode reward: total was -26.550000. running mean: 33.985167\n",
      "epsilon:0.011273 episode_count: 29799. steps_count: 12892135.000000\n",
      "Time elapsed:  39390.94632482529\n",
      "ep 4257: ep_len:500 episode reward: total was 7.900000. running mean: 33.724316\n",
      "ep 4257: ep_len:537 episode reward: total was 25.660000. running mean: 33.643673\n",
      "ep 4257: ep_len:622 episode reward: total was 35.740000. running mean: 33.664636\n",
      "ep 4257: ep_len:525 episode reward: total was 30.810000. running mean: 33.636089\n",
      "ep 4257: ep_len:3 episode reward: total was 1.010000. running mean: 33.309829\n",
      "ep 4257: ep_len:533 episode reward: total was 77.090000. running mean: 33.747630\n",
      "ep 4257: ep_len:631 episode reward: total was 57.630000. running mean: 33.986454\n",
      "epsilon:0.011229 episode_count: 29806. steps_count: 12895486.000000\n",
      "Time elapsed:  39396.02865934372\n",
      "ep 4258: ep_len:588 episode reward: total was 39.900000. running mean: 34.045589\n",
      "ep 4258: ep_len:596 episode reward: total was 95.250000. running mean: 34.657634\n",
      "ep 4258: ep_len:582 episode reward: total was -13.150000. running mean: 34.179557\n",
      "ep 4258: ep_len:115 episode reward: total was 3.130000. running mean: 33.869062\n",
      "ep 4258: ep_len:3 episode reward: total was 1.010000. running mean: 33.540471\n",
      "ep 4258: ep_len:308 episode reward: total was 43.830000. running mean: 33.643366\n",
      "ep 4258: ep_len:548 episode reward: total was 41.680000. running mean: 33.723733\n",
      "epsilon:0.011184 episode_count: 29813. steps_count: 12898226.000000\n",
      "Time elapsed:  39410.87209057808\n",
      "ep 4259: ep_len:642 episode reward: total was 57.590000. running mean: 33.962395\n",
      "ep 4259: ep_len:509 episode reward: total was 27.120000. running mean: 33.893971\n",
      "ep 4259: ep_len:555 episode reward: total was 7.840000. running mean: 33.633432\n",
      "ep 4259: ep_len:500 episode reward: total was 92.230000. running mean: 34.219397\n",
      "ep 4259: ep_len:3 episode reward: total was 1.010000. running mean: 33.887303\n",
      "ep 4259: ep_len:659 episode reward: total was -13.070000. running mean: 33.417730\n",
      "ep 4259: ep_len:350 episode reward: total was -8.270000. running mean: 33.000853\n",
      "epsilon:0.011140 episode_count: 29820. steps_count: 12901444.000000\n",
      "Time elapsed:  39419.326177835464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4260: ep_len:589 episode reward: total was 84.550000. running mean: 33.516344\n",
      "ep 4260: ep_len:506 episode reward: total was 43.660000. running mean: 33.617781\n",
      "ep 4260: ep_len:616 episode reward: total was -7.910000. running mean: 33.202503\n",
      "ep 4260: ep_len:500 episode reward: total was 10.760000. running mean: 32.978078\n",
      "ep 4260: ep_len:55 episode reward: total was 26.000000. running mean: 32.908297\n",
      "ep 4260: ep_len:593 episode reward: total was 63.960000. running mean: 33.218814\n",
      "ep 4260: ep_len:542 episode reward: total was 76.760000. running mean: 33.654226\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.011096 episode_count: 29827. steps_count: 12904845.000000\n",
      "Time elapsed:  39440.72511124611\n",
      "ep 4261: ep_len:500 episode reward: total was 103.810000. running mean: 34.355784\n",
      "ep 4261: ep_len:621 episode reward: total was -122.310000. running mean: 32.789126\n",
      "ep 4261: ep_len:469 episode reward: total was 82.490000. running mean: 33.286135\n",
      "ep 4261: ep_len:610 episode reward: total was 87.080000. running mean: 33.824074\n",
      "ep 4261: ep_len:102 episode reward: total was 33.770000. running mean: 33.823533\n",
      "ep 4261: ep_len:500 episode reward: total was 49.340000. running mean: 33.978697\n",
      "ep 4261: ep_len:600 episode reward: total was 82.530000. running mean: 34.464211\n",
      "epsilon:0.011051 episode_count: 29834. steps_count: 12908247.000000\n",
      "Time elapsed:  39456.11190152168\n",
      "ep 4262: ep_len:646 episode reward: total was 46.150000. running mean: 34.581068\n",
      "ep 4262: ep_len:196 episode reward: total was 10.510000. running mean: 34.340358\n",
      "ep 4262: ep_len:681 episode reward: total was 16.700000. running mean: 34.163954\n",
      "ep 4262: ep_len:519 episode reward: total was 90.590000. running mean: 34.728215\n",
      "ep 4262: ep_len:3 episode reward: total was 1.010000. running mean: 34.391032\n",
      "ep 4262: ep_len:518 episode reward: total was 19.590000. running mean: 34.243022\n",
      "ep 4262: ep_len:551 episode reward: total was -33.210000. running mean: 33.568492\n",
      "epsilon:0.011007 episode_count: 29841. steps_count: 12911361.000000\n",
      "Time elapsed:  39464.49834752083\n",
      "ep 4263: ep_len:200 episode reward: total was 17.900000. running mean: 33.411807\n",
      "ep 4263: ep_len:500 episode reward: total was 112.580000. running mean: 34.203489\n",
      "ep 4263: ep_len:590 episode reward: total was 34.830000. running mean: 34.209754\n",
      "ep 4263: ep_len:577 episode reward: total was 61.130000. running mean: 34.478957\n",
      "ep 4263: ep_len:101 episode reward: total was 31.770000. running mean: 34.451867\n",
      "ep 4263: ep_len:513 episode reward: total was 23.000000. running mean: 34.337348\n",
      "ep 4263: ep_len:549 episode reward: total was 70.000000. running mean: 34.693975\n",
      "epsilon:0.010963 episode_count: 29848. steps_count: 12914391.000000\n",
      "Time elapsed:  39472.63944172859\n",
      "ep 4264: ep_len:550 episode reward: total was 46.290000. running mean: 34.809935\n",
      "ep 4264: ep_len:361 episode reward: total was -2.520000. running mean: 34.436636\n",
      "ep 4264: ep_len:500 episode reward: total was 49.360000. running mean: 34.585869\n",
      "ep 4264: ep_len:505 episode reward: total was 8.120000. running mean: 34.321211\n",
      "ep 4264: ep_len:3 episode reward: total was 1.010000. running mean: 33.988099\n",
      "ep 4264: ep_len:546 episode reward: total was 17.190000. running mean: 33.820118\n",
      "ep 4264: ep_len:211 episode reward: total was 12.650000. running mean: 33.608416\n",
      "epsilon:0.010918 episode_count: 29855. steps_count: 12917067.000000\n",
      "Time elapsed:  39487.1802546978\n",
      "ep 4265: ep_len:615 episode reward: total was 83.080000. running mean: 34.103132\n",
      "ep 4265: ep_len:515 episode reward: total was 117.370000. running mean: 34.935801\n",
      "ep 4265: ep_len:647 episode reward: total was 26.110000. running mean: 34.847543\n",
      "ep 4265: ep_len:132 episode reward: total was 31.140000. running mean: 34.810467\n",
      "ep 4265: ep_len:3 episode reward: total was 1.010000. running mean: 34.472463\n",
      "ep 4265: ep_len:319 episode reward: total was 23.740000. running mean: 34.365138\n",
      "ep 4265: ep_len:526 episode reward: total was 65.470000. running mean: 34.676187\n",
      "epsilon:0.010874 episode_count: 29862. steps_count: 12919824.000000\n",
      "Time elapsed:  39494.58185887337\n",
      "ep 4266: ep_len:518 episode reward: total was -64.610000. running mean: 33.683325\n",
      "ep 4266: ep_len:500 episode reward: total was 37.820000. running mean: 33.724692\n",
      "ep 4266: ep_len:624 episode reward: total was 36.410000. running mean: 33.751545\n",
      "ep 4266: ep_len:591 episode reward: total was 82.470000. running mean: 34.238729\n",
      "ep 4266: ep_len:3 episode reward: total was 1.010000. running mean: 33.906442\n",
      "ep 4266: ep_len:686 episode reward: total was 48.720000. running mean: 34.054578\n",
      "ep 4266: ep_len:617 episode reward: total was 40.410000. running mean: 34.118132\n",
      "epsilon:0.010830 episode_count: 29869. steps_count: 12923363.000000\n",
      "Time elapsed:  39503.72994470596\n",
      "ep 4267: ep_len:640 episode reward: total was -0.260000. running mean: 33.774350\n",
      "ep 4267: ep_len:164 episode reward: total was 20.290000. running mean: 33.639507\n",
      "ep 4267: ep_len:530 episode reward: total was -10.840000. running mean: 33.194712\n",
      "ep 4267: ep_len:512 episode reward: total was 41.530000. running mean: 33.278065\n",
      "ep 4267: ep_len:47 episode reward: total was 22.000000. running mean: 33.165284\n",
      "ep 4267: ep_len:586 episode reward: total was 45.500000. running mean: 33.288631\n",
      "ep 4267: ep_len:519 episode reward: total was 24.040000. running mean: 33.196145\n",
      "epsilon:0.010785 episode_count: 29876. steps_count: 12926361.000000\n",
      "Time elapsed:  39511.57106637955\n",
      "ep 4268: ep_len:253 episode reward: total was 6.780000. running mean: 32.931984\n",
      "ep 4268: ep_len:500 episode reward: total was 119.190000. running mean: 33.794564\n",
      "ep 4268: ep_len:609 episode reward: total was 22.410000. running mean: 33.680718\n",
      "ep 4268: ep_len:500 episode reward: total was 52.450000. running mean: 33.868411\n",
      "ep 4268: ep_len:3 episode reward: total was 1.010000. running mean: 33.539827\n",
      "ep 4268: ep_len:610 episode reward: total was 70.990000. running mean: 33.914329\n",
      "ep 4268: ep_len:624 episode reward: total was 47.150000. running mean: 34.046685\n",
      "epsilon:0.010741 episode_count: 29883. steps_count: 12929460.000000\n",
      "Time elapsed:  39526.11613368988\n",
      "ep 4269: ep_len:625 episode reward: total was 7.430000. running mean: 33.780518\n",
      "ep 4269: ep_len:605 episode reward: total was 5.600000. running mean: 33.498713\n",
      "ep 4269: ep_len:573 episode reward: total was -14.380000. running mean: 33.019926\n",
      "ep 4269: ep_len:142 episode reward: total was 27.150000. running mean: 32.961227\n",
      "ep 4269: ep_len:88 episode reward: total was 27.780000. running mean: 32.909415\n",
      "ep 4269: ep_len:572 episode reward: total was 21.290000. running mean: 32.793220\n",
      "ep 4269: ep_len:552 episode reward: total was -152.320000. running mean: 30.942088\n",
      "epsilon:0.010697 episode_count: 29890. steps_count: 12932617.000000\n",
      "Time elapsed:  39535.62398791313\n",
      "ep 4270: ep_len:187 episode reward: total was 32.930000. running mean: 30.961967\n",
      "ep 4270: ep_len:598 episode reward: total was 132.600000. running mean: 31.978348\n",
      "ep 4270: ep_len:561 episode reward: total was 9.280000. running mean: 31.751364\n",
      "ep 4270: ep_len:530 episode reward: total was 44.800000. running mean: 31.881851\n",
      "ep 4270: ep_len:50 episode reward: total was -45.990000. running mean: 31.103132\n",
      "ep 4270: ep_len:500 episode reward: total was 19.470000. running mean: 30.986801\n",
      "ep 4270: ep_len:550 episode reward: total was 20.360000. running mean: 30.880533\n",
      "epsilon:0.010652 episode_count: 29897. steps_count: 12935593.000000\n",
      "Time elapsed:  39543.51536798477\n",
      "ep 4271: ep_len:635 episode reward: total was 63.120000. running mean: 31.202927\n",
      "ep 4271: ep_len:500 episode reward: total was 69.380000. running mean: 31.584698\n",
      "ep 4271: ep_len:642 episode reward: total was 19.260000. running mean: 31.461451\n",
      "ep 4271: ep_len:395 episode reward: total was 43.670000. running mean: 31.583537\n",
      "ep 4271: ep_len:114 episode reward: total was 38.270000. running mean: 31.650401\n",
      "ep 4271: ep_len:563 episode reward: total was 20.180000. running mean: 31.535697\n",
      "ep 4271: ep_len:579 episode reward: total was 55.640000. running mean: 31.776740\n",
      "epsilon:0.010608 episode_count: 29904. steps_count: 12939021.000000\n",
      "Time elapsed:  39552.229155540466\n",
      "ep 4272: ep_len:646 episode reward: total was 79.270000. running mean: 32.251673\n",
      "ep 4272: ep_len:581 episode reward: total was 79.270000. running mean: 32.721856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4272: ep_len:620 episode reward: total was 12.190000. running mean: 32.516538\n",
      "ep 4272: ep_len:526 episode reward: total was 52.970000. running mean: 32.721072\n",
      "ep 4272: ep_len:3 episode reward: total was 1.010000. running mean: 32.403961\n",
      "ep 4272: ep_len:313 episode reward: total was 33.660000. running mean: 32.416522\n",
      "ep 4272: ep_len:287 episode reward: total was 14.930000. running mean: 32.241657\n",
      "epsilon:0.010564 episode_count: 29911. steps_count: 12941997.000000\n",
      "Time elapsed:  39556.83805513382\n",
      "ep 4273: ep_len:632 episode reward: total was 16.400000. running mean: 32.083240\n",
      "ep 4273: ep_len:500 episode reward: total was 123.600000. running mean: 32.998408\n",
      "ep 4273: ep_len:542 episode reward: total was 40.360000. running mean: 33.072024\n",
      "ep 4273: ep_len:500 episode reward: total was 95.080000. running mean: 33.692103\n",
      "ep 4273: ep_len:92 episode reward: total was 26.260000. running mean: 33.617782\n",
      "ep 4273: ep_len:661 episode reward: total was 45.900000. running mean: 33.740604\n",
      "ep 4273: ep_len:500 episode reward: total was -63.820000. running mean: 32.764998\n",
      "epsilon:0.010519 episode_count: 29918. steps_count: 12945424.000000\n",
      "Time elapsed:  39563.350464105606\n",
      "ep 4274: ep_len:198 episode reward: total was 17.270000. running mean: 32.610048\n",
      "ep 4274: ep_len:500 episode reward: total was 139.060000. running mean: 33.674548\n",
      "ep 4274: ep_len:656 episode reward: total was 29.900000. running mean: 33.636802\n",
      "ep 4274: ep_len:585 episode reward: total was 104.270000. running mean: 34.343134\n",
      "ep 4274: ep_len:3 episode reward: total was 1.010000. running mean: 34.009803\n",
      "ep 4274: ep_len:514 episode reward: total was 33.360000. running mean: 34.003305\n",
      "ep 4274: ep_len:287 episode reward: total was -4.110000. running mean: 33.622172\n",
      "epsilon:0.010475 episode_count: 29925. steps_count: 12948167.000000\n",
      "Time elapsed:  39567.654490709305\n",
      "ep 4275: ep_len:500 episode reward: total was 60.000000. running mean: 33.885950\n",
      "ep 4275: ep_len:525 episode reward: total was 123.350000. running mean: 34.780591\n",
      "ep 4275: ep_len:500 episode reward: total was 39.260000. running mean: 34.825385\n",
      "ep 4275: ep_len:520 episode reward: total was 94.720000. running mean: 35.424331\n",
      "ep 4275: ep_len:90 episode reward: total was 26.270000. running mean: 35.332788\n",
      "ep 4275: ep_len:501 episode reward: total was 44.030000. running mean: 35.419760\n",
      "ep 4275: ep_len:513 episode reward: total was 34.270000. running mean: 35.408262\n",
      "epsilon:0.010431 episode_count: 29932. steps_count: 12951316.000000\n",
      "Time elapsed:  39573.67654275894\n",
      "ep 4276: ep_len:115 episode reward: total was -20.060000. running mean: 34.853580\n",
      "ep 4276: ep_len:567 episode reward: total was -119.300000. running mean: 33.312044\n",
      "ep 4276: ep_len:408 episode reward: total was 60.590000. running mean: 33.584823\n",
      "ep 4276: ep_len:514 episode reward: total was 57.590000. running mean: 33.824875\n",
      "ep 4276: ep_len:3 episode reward: total was 1.010000. running mean: 33.496726\n",
      "ep 4276: ep_len:588 episode reward: total was 37.310000. running mean: 33.534859\n",
      "ep 4276: ep_len:334 episode reward: total was 9.260000. running mean: 33.292111\n",
      "epsilon:0.010386 episode_count: 29939. steps_count: 12953845.000000\n",
      "Time elapsed:  39578.73040342331\n",
      "ep 4277: ep_len:215 episode reward: total was 18.640000. running mean: 33.145589\n",
      "ep 4277: ep_len:265 episode reward: total was 6.080000. running mean: 32.874934\n",
      "ep 4277: ep_len:574 episode reward: total was 51.310000. running mean: 33.059284\n",
      "ep 4277: ep_len:500 episode reward: total was 73.850000. running mean: 33.467191\n",
      "ep 4277: ep_len:3 episode reward: total was 1.010000. running mean: 33.142619\n",
      "ep 4277: ep_len:179 episode reward: total was 37.120000. running mean: 33.182393\n",
      "ep 4277: ep_len:556 episode reward: total was 32.790000. running mean: 33.178469\n",
      "epsilon:0.010342 episode_count: 29946. steps_count: 12956137.000000\n",
      "Time elapsed:  39584.99845147133\n",
      "ep 4278: ep_len:567 episode reward: total was 86.050000. running mean: 33.707185\n",
      "ep 4278: ep_len:500 episode reward: total was 5.350000. running mean: 33.423613\n",
      "ep 4278: ep_len:500 episode reward: total was 8.940000. running mean: 33.178777\n",
      "ep 4278: ep_len:591 episode reward: total was 94.960000. running mean: 33.796589\n",
      "ep 4278: ep_len:101 episode reward: total was 33.760000. running mean: 33.796223\n",
      "ep 4278: ep_len:500 episode reward: total was 32.750000. running mean: 33.785761\n",
      "ep 4278: ep_len:516 episode reward: total was 54.380000. running mean: 33.991703\n",
      "epsilon:0.010298 episode_count: 29953. steps_count: 12959412.000000\n",
      "Time elapsed:  39593.557128190994\n",
      "ep 4279: ep_len:634 episode reward: total was 57.170000. running mean: 34.223486\n",
      "ep 4279: ep_len:529 episode reward: total was 25.120000. running mean: 34.132451\n",
      "ep 4279: ep_len:432 episode reward: total was 71.830000. running mean: 34.509427\n",
      "ep 4279: ep_len:503 episode reward: total was 88.090000. running mean: 35.045233\n",
      "ep 4279: ep_len:3 episode reward: total was 1.010000. running mean: 34.704880\n",
      "ep 4279: ep_len:500 episode reward: total was 29.680000. running mean: 34.654631\n",
      "ep 4279: ep_len:157 episode reward: total was -92.150000. running mean: 33.386585\n",
      "epsilon:0.010253 episode_count: 29960. steps_count: 12962170.000000\n",
      "Time elapsed:  39600.900347709656\n",
      "ep 4280: ep_len:513 episode reward: total was -6.130000. running mean: 32.991419\n",
      "ep 4280: ep_len:500 episode reward: total was 113.610000. running mean: 33.797605\n",
      "ep 4280: ep_len:574 episode reward: total was 34.820000. running mean: 33.807829\n",
      "ep 4280: ep_len:540 episode reward: total was 51.630000. running mean: 33.986051\n",
      "ep 4280: ep_len:86 episode reward: total was 31.190000. running mean: 33.958090\n",
      "ep 4280: ep_len:539 episode reward: total was -8.030000. running mean: 33.538209\n",
      "ep 4280: ep_len:297 episode reward: total was 16.100000. running mean: 33.363827\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.010209 episode_count: 29967. steps_count: 12965219.000000\n",
      "Time elapsed:  39613.01511526108\n",
      "ep 4281: ep_len:500 episode reward: total was 29.200000. running mean: 33.322189\n",
      "ep 4281: ep_len:500 episode reward: total was 54.780000. running mean: 33.536767\n",
      "ep 4281: ep_len:637 episode reward: total was 48.570000. running mean: 33.687099\n",
      "ep 4281: ep_len:132 episode reward: total was 15.120000. running mean: 33.501428\n",
      "ep 4281: ep_len:95 episode reward: total was 30.760000. running mean: 33.474014\n",
      "ep 4281: ep_len:642 episode reward: total was 54.610000. running mean: 33.685374\n",
      "ep 4281: ep_len:500 episode reward: total was 74.570000. running mean: 34.094220\n",
      "epsilon:0.010165 episode_count: 29974. steps_count: 12968225.000000\n",
      "Time elapsed:  39632.4629278183\n",
      "ep 4282: ep_len:547 episode reward: total was 56.520000. running mean: 34.318478\n",
      "ep 4282: ep_len:507 episode reward: total was 123.110000. running mean: 35.206393\n",
      "ep 4282: ep_len:610 episode reward: total was 10.130000. running mean: 34.955629\n",
      "ep 4282: ep_len:550 episode reward: total was 43.850000. running mean: 35.044573\n",
      "ep 4282: ep_len:3 episode reward: total was 1.010000. running mean: 34.704227\n",
      "ep 4282: ep_len:500 episode reward: total was 40.540000. running mean: 34.762585\n",
      "ep 4282: ep_len:500 episode reward: total was 39.300000. running mean: 34.807959\n",
      "epsilon:0.010120 episode_count: 29981. steps_count: 12971442.000000\n",
      "Time elapsed:  39641.03227496147\n",
      "ep 4283: ep_len:514 episode reward: total was -8.540000. running mean: 34.374480\n",
      "ep 4283: ep_len:500 episode reward: total was 140.130000. running mean: 35.432035\n",
      "ep 4283: ep_len:583 episode reward: total was 18.260000. running mean: 35.260314\n",
      "ep 4283: ep_len:500 episode reward: total was 75.080000. running mean: 35.658511\n",
      "ep 4283: ep_len:133 episode reward: total was 34.360000. running mean: 35.645526\n",
      "ep 4283: ep_len:503 episode reward: total was -17.390000. running mean: 35.115171\n",
      "ep 4283: ep_len:572 episode reward: total was 86.090000. running mean: 35.624919\n",
      "epsilon:0.010076 episode_count: 29988. steps_count: 12974747.000000\n",
      "Time elapsed:  39649.24630641937\n",
      "ep 4284: ep_len:575 episode reward: total was 98.950000. running mean: 36.258170\n",
      "ep 4284: ep_len:184 episode reward: total was 23.920000. running mean: 36.134788\n",
      "ep 4284: ep_len:649 episode reward: total was -101.530000. running mean: 34.758140\n",
      "ep 4284: ep_len:563 episode reward: total was 96.390000. running mean: 35.374459\n",
      "ep 4284: ep_len:3 episode reward: total was 1.010000. running mean: 35.030814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4284: ep_len:653 episode reward: total was 66.480000. running mean: 35.345306\n",
      "ep 4284: ep_len:530 episode reward: total was 61.380000. running mean: 35.605653\n",
      "epsilon:0.010032 episode_count: 29995. steps_count: 12977904.000000\n",
      "Time elapsed:  39656.525975465775\n",
      "ep 4285: ep_len:552 episode reward: total was -22.990000. running mean: 35.019697\n",
      "ep 4285: ep_len:538 episode reward: total was 26.950000. running mean: 34.939000\n",
      "ep 4285: ep_len:500 episode reward: total was 47.580000. running mean: 35.065410\n",
      "ep 4285: ep_len:586 episode reward: total was 12.590000. running mean: 34.840656\n",
      "ep 4285: ep_len:80 episode reward: total was -30.750000. running mean: 34.184749\n",
      "ep 4285: ep_len:659 episode reward: total was 26.350000. running mean: 34.106402\n",
      "ep 4285: ep_len:606 episode reward: total was 36.840000. running mean: 34.133738\n",
      "epsilon:0.009994 episode_count: 30002. steps_count: 12981425.000000\n",
      "Time elapsed:  39663.36017179489\n",
      "ep 4286: ep_len:546 episode reward: total was 91.250000. running mean: 34.704900\n",
      "ep 4286: ep_len:751 episode reward: total was -123.400000. running mean: 33.123851\n",
      "ep 4286: ep_len:557 episode reward: total was -15.890000. running mean: 32.633713\n",
      "ep 4286: ep_len:614 episode reward: total was -563.610000. running mean: 26.671276\n",
      "ep 4286: ep_len:3 episode reward: total was 1.010000. running mean: 26.414663\n",
      "ep 4286: ep_len:500 episode reward: total was 42.040000. running mean: 26.570916\n",
      "ep 4286: ep_len:270 episode reward: total was 28.100000. running mean: 26.586207\n",
      "epsilon:0.009994 episode_count: 30009. steps_count: 12984666.000000\n",
      "Time elapsed:  39671.91132521629\n",
      "ep 4287: ep_len:500 episode reward: total was 122.050000. running mean: 27.540845\n",
      "ep 4287: ep_len:188 episode reward: total was 16.310000. running mean: 27.428536\n",
      "ep 4287: ep_len:559 episode reward: total was -8.200000. running mean: 27.072251\n",
      "ep 4287: ep_len:131 episode reward: total was 18.610000. running mean: 26.987629\n",
      "ep 4287: ep_len:3 episode reward: total was 1.010000. running mean: 26.727852\n",
      "ep 4287: ep_len:657 episode reward: total was -75.370000. running mean: 25.706874\n",
      "ep 4287: ep_len:594 episode reward: total was -68.610000. running mean: 24.763705\n",
      "epsilon:0.009994 episode_count: 30016. steps_count: 12987298.000000\n",
      "Time elapsed:  39681.49759030342\n",
      "ep 4288: ep_len:674 episode reward: total was 41.070000. running mean: 24.926768\n",
      "ep 4288: ep_len:503 episode reward: total was 105.480000. running mean: 25.732300\n",
      "ep 4288: ep_len:672 episode reward: total was -2.200000. running mean: 25.452977\n",
      "ep 4288: ep_len:613 episode reward: total was -59.360000. running mean: 24.604848\n",
      "ep 4288: ep_len:3 episode reward: total was 1.010000. running mean: 24.368899\n",
      "ep 4288: ep_len:500 episode reward: total was 77.730000. running mean: 24.902510\n",
      "ep 4288: ep_len:500 episode reward: total was 9.310000. running mean: 24.746585\n",
      "epsilon:0.009994 episode_count: 30023. steps_count: 12990763.000000\n",
      "Time elapsed:  39690.03597688675\n",
      "ep 4289: ep_len:229 episode reward: total was 28.300000. running mean: 24.782119\n",
      "ep 4289: ep_len:548 episode reward: total was 9.980000. running mean: 24.634098\n",
      "ep 4289: ep_len:635 episode reward: total was 19.250000. running mean: 24.580257\n",
      "ep 4289: ep_len:594 episode reward: total was 72.810000. running mean: 25.062554\n",
      "ep 4289: ep_len:3 episode reward: total was 1.010000. running mean: 24.822029\n",
      "ep 4289: ep_len:647 episode reward: total was 43.190000. running mean: 25.005709\n",
      "ep 4289: ep_len:527 episode reward: total was 46.190000. running mean: 25.217551\n",
      "epsilon:0.009994 episode_count: 30030. steps_count: 12993946.000000\n",
      "Time elapsed:  39701.47365236282\n",
      "ep 4290: ep_len:121 episode reward: total was -2.470000. running mean: 24.940676\n",
      "ep 4290: ep_len:527 episode reward: total was 126.340000. running mean: 25.954669\n",
      "ep 4290: ep_len:555 episode reward: total was 63.640000. running mean: 26.331522\n",
      "ep 4290: ep_len:500 episode reward: total was 57.300000. running mean: 26.641207\n",
      "ep 4290: ep_len:3 episode reward: total was 1.010000. running mean: 26.384895\n",
      "ep 4290: ep_len:500 episode reward: total was 15.340000. running mean: 26.274446\n",
      "ep 4290: ep_len:611 episode reward: total was 37.090000. running mean: 26.382602\n",
      "epsilon:0.009994 episode_count: 30037. steps_count: 12996763.000000\n",
      "Time elapsed:  39708.957817316055\n",
      "ep 4291: ep_len:514 episode reward: total was 33.140000. running mean: 26.450176\n",
      "ep 4291: ep_len:564 episode reward: total was 58.330000. running mean: 26.768974\n",
      "ep 4291: ep_len:620 episode reward: total was 17.480000. running mean: 26.676084\n",
      "ep 4291: ep_len:528 episode reward: total was 60.880000. running mean: 27.018123\n",
      "ep 4291: ep_len:3 episode reward: total was 1.010000. running mean: 26.758042\n",
      "ep 4291: ep_len:500 episode reward: total was 31.340000. running mean: 26.803862\n",
      "ep 4291: ep_len:618 episode reward: total was 76.310000. running mean: 27.298923\n",
      "epsilon:0.009994 episode_count: 30044. steps_count: 13000110.000000\n",
      "Time elapsed:  39717.79434919357\n",
      "ep 4292: ep_len:500 episode reward: total was 25.920000. running mean: 27.285134\n",
      "ep 4292: ep_len:612 episode reward: total was 67.160000. running mean: 27.683883\n",
      "ep 4292: ep_len:1046 episode reward: total was -691.100000. running mean: 20.496044\n",
      "ep 4292: ep_len:169 episode reward: total was 19.710000. running mean: 20.488183\n",
      "ep 4292: ep_len:3 episode reward: total was 1.010000. running mean: 20.293401\n",
      "ep 4292: ep_len:624 episode reward: total was 53.230000. running mean: 20.622767\n",
      "ep 4292: ep_len:500 episode reward: total was 52.720000. running mean: 20.943740\n",
      "epsilon:0.009994 episode_count: 30051. steps_count: 13003564.000000\n",
      "Time elapsed:  39726.847846508026\n",
      "ep 4293: ep_len:229 episode reward: total was 13.270000. running mean: 20.867002\n",
      "ep 4293: ep_len:584 episode reward: total was 29.230000. running mean: 20.950632\n",
      "ep 4293: ep_len:500 episode reward: total was 40.290000. running mean: 21.144026\n",
      "ep 4293: ep_len:502 episode reward: total was 54.470000. running mean: 21.477286\n",
      "ep 4293: ep_len:3 episode reward: total was 1.010000. running mean: 21.272613\n",
      "ep 4293: ep_len:246 episode reward: total was 59.450000. running mean: 21.654387\n",
      "ep 4293: ep_len:604 episode reward: total was 61.830000. running mean: 22.056143\n",
      "epsilon:0.009994 episode_count: 30058. steps_count: 13006232.000000\n",
      "Time elapsed:  39734.0463039875\n",
      "ep 4294: ep_len:107 episode reward: total was 13.370000. running mean: 21.969282\n",
      "ep 4294: ep_len:284 episode reward: total was -65.720000. running mean: 21.092389\n",
      "ep 4294: ep_len:500 episode reward: total was 61.570000. running mean: 21.497165\n",
      "ep 4294: ep_len:500 episode reward: total was 28.270000. running mean: 21.564893\n",
      "ep 4294: ep_len:103 episode reward: total was 31.760000. running mean: 21.666844\n",
      "ep 4294: ep_len:232 episode reward: total was 40.880000. running mean: 21.858976\n",
      "ep 4294: ep_len:352 episode reward: total was 39.810000. running mean: 22.038486\n",
      "epsilon:0.009994 episode_count: 30065. steps_count: 13008310.000000\n",
      "Time elapsed:  39740.67965054512\n",
      "ep 4295: ep_len:623 episode reward: total was -35.420000. running mean: 21.463901\n",
      "ep 4295: ep_len:500 episode reward: total was -10.720000. running mean: 21.142062\n",
      "ep 4295: ep_len:648 episode reward: total was -43.550000. running mean: 20.495142\n",
      "ep 4295: ep_len:616 episode reward: total was 54.270000. running mean: 20.832890\n",
      "ep 4295: ep_len:3 episode reward: total was 1.010000. running mean: 20.634661\n",
      "ep 4295: ep_len:579 episode reward: total was 45.660000. running mean: 20.884915\n",
      "ep 4295: ep_len:566 episode reward: total was 34.070000. running mean: 21.016765\n",
      "epsilon:0.009994 episode_count: 30072. steps_count: 13011845.000000\n",
      "Time elapsed:  39749.86107516289\n",
      "ep 4296: ep_len:518 episode reward: total was 2.970000. running mean: 20.836298\n",
      "ep 4296: ep_len:515 episode reward: total was 106.140000. running mean: 21.689335\n",
      "ep 4296: ep_len:649 episode reward: total was 46.790000. running mean: 21.940341\n",
      "ep 4296: ep_len:56 episode reward: total was 6.850000. running mean: 21.789438\n",
      "ep 4296: ep_len:80 episode reward: total was 20.750000. running mean: 21.779044\n",
      "ep 4296: ep_len:587 episode reward: total was 63.590000. running mean: 22.197153\n",
      "ep 4296: ep_len:500 episode reward: total was 37.940000. running mean: 22.354582\n",
      "epsilon:0.009994 episode_count: 30079. steps_count: 13014750.000000\n",
      "Time elapsed:  39757.525403022766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4297: ep_len:552 episode reward: total was -43.050000. running mean: 21.700536\n",
      "ep 4297: ep_len:607 episode reward: total was 26.510000. running mean: 21.748631\n",
      "ep 4297: ep_len:593 episode reward: total was 14.880000. running mean: 21.679944\n",
      "ep 4297: ep_len:591 episode reward: total was 72.070000. running mean: 22.183845\n",
      "ep 4297: ep_len:77 episode reward: total was 20.260000. running mean: 22.164606\n",
      "ep 4297: ep_len:596 episode reward: total was 79.480000. running mean: 22.737760\n",
      "ep 4297: ep_len:170 episode reward: total was -33.610000. running mean: 22.174283\n",
      "epsilon:0.009994 episode_count: 30086. steps_count: 13017936.000000\n",
      "Time elapsed:  39773.210882902145\n",
      "ep 4298: ep_len:603 episode reward: total was 9.650000. running mean: 22.049040\n",
      "ep 4298: ep_len:500 episode reward: total was 43.710000. running mean: 22.265649\n",
      "ep 4298: ep_len:549 episode reward: total was 19.140000. running mean: 22.234393\n",
      "ep 4298: ep_len:41 episode reward: total was 0.120000. running mean: 22.013249\n",
      "ep 4298: ep_len:3 episode reward: total was 1.010000. running mean: 21.803217\n",
      "ep 4298: ep_len:527 episode reward: total was 43.080000. running mean: 22.015984\n",
      "ep 4298: ep_len:553 episode reward: total was 66.250000. running mean: 22.458325\n",
      "epsilon:0.009994 episode_count: 30093. steps_count: 13020712.000000\n",
      "Time elapsed:  39780.86412572861\n",
      "ep 4299: ep_len:580 episode reward: total was 73.200000. running mean: 22.965741\n",
      "ep 4299: ep_len:524 episode reward: total was 23.240000. running mean: 22.968484\n",
      "ep 4299: ep_len:538 episode reward: total was 16.330000. running mean: 22.902099\n",
      "ep 4299: ep_len:510 episode reward: total was 3.460000. running mean: 22.707678\n",
      "ep 4299: ep_len:51 episode reward: total was 25.010000. running mean: 22.730701\n",
      "ep 4299: ep_len:565 episode reward: total was 25.270000. running mean: 22.756094\n",
      "ep 4299: ep_len:500 episode reward: total was 44.100000. running mean: 22.969533\n",
      "epsilon:0.009994 episode_count: 30100. steps_count: 13023980.000000\n",
      "Time elapsed:  39789.49908041954\n",
      "ep 4300: ep_len:214 episode reward: total was 30.720000. running mean: 23.047038\n",
      "ep 4300: ep_len:551 episode reward: total was 130.970000. running mean: 24.126268\n",
      "ep 4300: ep_len:673 episode reward: total was 22.390000. running mean: 24.108905\n",
      "ep 4300: ep_len:500 episode reward: total was 64.370000. running mean: 24.511516\n",
      "ep 4300: ep_len:3 episode reward: total was 1.010000. running mean: 24.276501\n",
      "ep 4300: ep_len:500 episode reward: total was -17.260000. running mean: 23.861136\n",
      "ep 4300: ep_len:616 episode reward: total was 50.470000. running mean: 24.127224\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.009994 episode_count: 30107. steps_count: 13027037.000000\n",
      "Time elapsed:  39802.60413956642\n",
      "ep 4301: ep_len:591 episode reward: total was 45.490000. running mean: 24.340852\n",
      "ep 4301: ep_len:563 episode reward: total was 60.260000. running mean: 24.700044\n",
      "ep 4301: ep_len:594 episode reward: total was 47.610000. running mean: 24.929143\n",
      "ep 4301: ep_len:514 episode reward: total was 68.010000. running mean: 25.359952\n",
      "ep 4301: ep_len:3 episode reward: total was 1.010000. running mean: 25.116452\n",
      "ep 4301: ep_len:176 episode reward: total was 36.170000. running mean: 25.226988\n",
      "ep 4301: ep_len:278 episode reward: total was 40.970000. running mean: 25.384418\n",
      "epsilon:0.009994 episode_count: 30114. steps_count: 13029756.000000\n",
      "Time elapsed:  39817.690494060516\n",
      "ep 4302: ep_len:234 episode reward: total was 6.500000. running mean: 25.195574\n",
      "ep 4302: ep_len:500 episode reward: total was 129.230000. running mean: 26.235918\n",
      "ep 4302: ep_len:501 episode reward: total was 10.760000. running mean: 26.081159\n",
      "ep 4302: ep_len:500 episode reward: total was 103.870000. running mean: 26.859047\n",
      "ep 4302: ep_len:87 episode reward: total was 21.740000. running mean: 26.807857\n",
      "ep 4302: ep_len:533 episode reward: total was 36.170000. running mean: 26.901478\n",
      "ep 4302: ep_len:528 episode reward: total was 47.650000. running mean: 27.108963\n",
      "epsilon:0.009994 episode_count: 30121. steps_count: 13032639.000000\n",
      "Time elapsed:  39831.9276740551\n",
      "ep 4303: ep_len:581 episode reward: total was 105.410000. running mean: 27.891974\n",
      "ep 4303: ep_len:360 episode reward: total was 47.790000. running mean: 28.090954\n",
      "ep 4303: ep_len:655 episode reward: total was 66.660000. running mean: 28.476644\n",
      "ep 4303: ep_len:518 episode reward: total was 37.980000. running mean: 28.571678\n",
      "ep 4303: ep_len:106 episode reward: total was -28.650000. running mean: 27.999461\n",
      "ep 4303: ep_len:500 episode reward: total was 42.010000. running mean: 28.139567\n",
      "ep 4303: ep_len:607 episode reward: total was 52.890000. running mean: 28.387071\n",
      "epsilon:0.009994 episode_count: 30128. steps_count: 13035966.000000\n",
      "Time elapsed:  39840.76478981972\n",
      "ep 4304: ep_len:550 episode reward: total was 53.590000. running mean: 28.639100\n",
      "ep 4304: ep_len:500 episode reward: total was 10.170000. running mean: 28.454409\n",
      "ep 4304: ep_len:421 episode reward: total was 65.570000. running mean: 28.825565\n",
      "ep 4304: ep_len:132 episode reward: total was 14.110000. running mean: 28.678409\n",
      "ep 4304: ep_len:3 episode reward: total was 1.010000. running mean: 28.401725\n",
      "ep 4304: ep_len:171 episode reward: total was 44.140000. running mean: 28.559108\n",
      "ep 4304: ep_len:211 episode reward: total was 14.670000. running mean: 28.420217\n",
      "epsilon:0.009994 episode_count: 30135. steps_count: 13037954.000000\n",
      "Time elapsed:  39846.32537150383\n",
      "ep 4305: ep_len:530 episode reward: total was 0.860000. running mean: 28.144615\n",
      "ep 4305: ep_len:606 episode reward: total was 61.340000. running mean: 28.476569\n",
      "ep 4305: ep_len:568 episode reward: total was 41.370000. running mean: 28.605503\n",
      "ep 4305: ep_len:601 episode reward: total was 103.970000. running mean: 29.359148\n",
      "ep 4305: ep_len:47 episode reward: total was 22.000000. running mean: 29.285556\n",
      "ep 4305: ep_len:186 episode reward: total was 42.270000. running mean: 29.415401\n",
      "ep 4305: ep_len:568 episode reward: total was 53.530000. running mean: 29.656547\n",
      "epsilon:0.009994 episode_count: 30142. steps_count: 13041060.000000\n",
      "Time elapsed:  39854.9456679821\n",
      "ep 4306: ep_len:721 episode reward: total was 8.400000. running mean: 29.443981\n",
      "ep 4306: ep_len:500 episode reward: total was 47.350000. running mean: 29.623042\n",
      "ep 4306: ep_len:500 episode reward: total was -4.960000. running mean: 29.277211\n",
      "ep 4306: ep_len:596 episode reward: total was 68.020000. running mean: 29.664639\n",
      "ep 4306: ep_len:3 episode reward: total was 1.010000. running mean: 29.378093\n",
      "ep 4306: ep_len:613 episode reward: total was 41.720000. running mean: 29.501512\n",
      "ep 4306: ep_len:592 episode reward: total was 36.200000. running mean: 29.568497\n",
      "epsilon:0.009994 episode_count: 30149. steps_count: 13044585.000000\n",
      "Time elapsed:  39864.21841979027\n",
      "ep 4307: ep_len:568 episode reward: total was -10.280000. running mean: 29.170012\n",
      "ep 4307: ep_len:500 episode reward: total was 114.390000. running mean: 30.022212\n",
      "ep 4307: ep_len:500 episode reward: total was 18.700000. running mean: 29.908989\n",
      "ep 4307: ep_len:512 episode reward: total was 80.540000. running mean: 30.415300\n",
      "ep 4307: ep_len:81 episode reward: total was 21.740000. running mean: 30.328547\n",
      "ep 4307: ep_len:518 episode reward: total was 65.750000. running mean: 30.682761\n",
      "ep 4307: ep_len:557 episode reward: total was 65.230000. running mean: 31.028234\n",
      "epsilon:0.009994 episode_count: 30156. steps_count: 13047821.000000\n",
      "Time elapsed:  39879.29110097885\n",
      "ep 4308: ep_len:128 episode reward: total was 16.030000. running mean: 30.878251\n",
      "ep 4308: ep_len:500 episode reward: total was 60.230000. running mean: 31.171769\n",
      "ep 4308: ep_len:570 episode reward: total was -9.090000. running mean: 30.769151\n",
      "ep 4308: ep_len:56 episode reward: total was 6.330000. running mean: 30.524759\n",
      "ep 4308: ep_len:3 episode reward: total was 1.010000. running mean: 30.229612\n",
      "ep 4308: ep_len:500 episode reward: total was 35.320000. running mean: 30.280516\n",
      "ep 4308: ep_len:592 episode reward: total was 59.720000. running mean: 30.574911\n",
      "epsilon:0.009994 episode_count: 30163. steps_count: 13050170.000000\n",
      "Time elapsed:  39885.70270442963\n",
      "ep 4309: ep_len:223 episode reward: total was 1.330000. running mean: 30.282461\n",
      "ep 4309: ep_len:568 episode reward: total was 40.150000. running mean: 30.381137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4309: ep_len:364 episode reward: total was 68.350000. running mean: 30.760825\n",
      "ep 4309: ep_len:544 episode reward: total was 37.930000. running mean: 30.832517\n",
      "ep 4309: ep_len:127 episode reward: total was 32.370000. running mean: 30.847892\n",
      "ep 4309: ep_len:226 episode reward: total was 45.990000. running mean: 30.999313\n",
      "ep 4309: ep_len:500 episode reward: total was 13.830000. running mean: 30.827620\n",
      "epsilon:0.009994 episode_count: 30170. steps_count: 13052722.000000\n",
      "Time elapsed:  39895.768478393555\n",
      "ep 4310: ep_len:553 episode reward: total was 84.800000. running mean: 31.367344\n",
      "ep 4310: ep_len:508 episode reward: total was 5.210000. running mean: 31.105770\n",
      "ep 4310: ep_len:562 episode reward: total was -9.000000. running mean: 30.704713\n",
      "ep 4310: ep_len:520 episode reward: total was 59.930000. running mean: 30.996966\n",
      "ep 4310: ep_len:63 episode reward: total was 14.730000. running mean: 30.834296\n",
      "ep 4310: ep_len:500 episode reward: total was 26.220000. running mean: 30.788153\n",
      "ep 4310: ep_len:526 episode reward: total was 50.520000. running mean: 30.985471\n",
      "epsilon:0.009994 episode_count: 30177. steps_count: 13055954.000000\n",
      "Time elapsed:  39910.84616422653\n",
      "ep 4311: ep_len:610 episode reward: total was 18.690000. running mean: 30.862517\n",
      "ep 4311: ep_len:513 episode reward: total was 29.980000. running mean: 30.853692\n",
      "ep 4311: ep_len:677 episode reward: total was 36.550000. running mean: 30.910655\n",
      "ep 4311: ep_len:500 episode reward: total was 73.250000. running mean: 31.334048\n",
      "ep 4311: ep_len:49 episode reward: total was 23.000000. running mean: 31.250708\n",
      "ep 4311: ep_len:500 episode reward: total was 61.260000. running mean: 31.550801\n",
      "ep 4311: ep_len:619 episode reward: total was 59.560000. running mean: 31.830893\n",
      "epsilon:0.009994 episode_count: 30184. steps_count: 13059422.000000\n",
      "Time elapsed:  39926.49722003937\n",
      "ep 4312: ep_len:536 episode reward: total was 77.530000. running mean: 32.287884\n",
      "ep 4312: ep_len:633 episode reward: total was 43.250000. running mean: 32.397505\n",
      "ep 4312: ep_len:501 episode reward: total was 45.630000. running mean: 32.529830\n",
      "ep 4312: ep_len:500 episode reward: total was 69.000000. running mean: 32.894531\n",
      "ep 4312: ep_len:3 episode reward: total was 1.010000. running mean: 32.575686\n",
      "ep 4312: ep_len:555 episode reward: total was 40.870000. running mean: 32.658629\n",
      "ep 4312: ep_len:535 episode reward: total was 33.940000. running mean: 32.671443\n",
      "epsilon:0.009994 episode_count: 30191. steps_count: 13062685.000000\n",
      "Time elapsed:  39935.12333035469\n",
      "ep 4313: ep_len:547 episode reward: total was 1.080000. running mean: 32.355528\n",
      "ep 4313: ep_len:563 episode reward: total was 35.480000. running mean: 32.386773\n",
      "ep 4313: ep_len:558 episode reward: total was 5.040000. running mean: 32.113305\n",
      "ep 4313: ep_len:46 episode reward: total was 6.750000. running mean: 31.859672\n",
      "ep 4313: ep_len:3 episode reward: total was 1.010000. running mean: 31.551176\n",
      "ep 4313: ep_len:500 episode reward: total was 0.800000. running mean: 31.243664\n",
      "ep 4313: ep_len:500 episode reward: total was -4.050000. running mean: 30.890727\n",
      "epsilon:0.009994 episode_count: 30198. steps_count: 13065402.000000\n",
      "Time elapsed:  39942.491824150085\n",
      "ep 4314: ep_len:618 episode reward: total was 72.400000. running mean: 31.305820\n",
      "ep 4314: ep_len:500 episode reward: total was 67.020000. running mean: 31.662962\n",
      "ep 4314: ep_len:500 episode reward: total was 72.070000. running mean: 32.067032\n",
      "ep 4314: ep_len:518 episode reward: total was 50.250000. running mean: 32.248862\n",
      "ep 4314: ep_len:3 episode reward: total was 1.010000. running mean: 31.936473\n",
      "ep 4314: ep_len:574 episode reward: total was 45.860000. running mean: 32.075709\n",
      "ep 4314: ep_len:187 episode reward: total was 9.470000. running mean: 31.849651\n",
      "epsilon:0.009994 episode_count: 30205. steps_count: 13068302.000000\n",
      "Time elapsed:  39956.990293979645\n",
      "ep 4315: ep_len:548 episode reward: total was 44.070000. running mean: 31.971855\n",
      "ep 4315: ep_len:550 episode reward: total was 44.480000. running mean: 32.096936\n",
      "ep 4315: ep_len:532 episode reward: total was -3.910000. running mean: 31.736867\n",
      "ep 4315: ep_len:567 episode reward: total was -52.380000. running mean: 30.895698\n",
      "ep 4315: ep_len:3 episode reward: total was 1.010000. running mean: 30.596841\n",
      "ep 4315: ep_len:535 episode reward: total was 72.460000. running mean: 31.015473\n",
      "ep 4315: ep_len:211 episode reward: total was 11.640000. running mean: 30.821718\n",
      "epsilon:0.009994 episode_count: 30212. steps_count: 13071248.000000\n",
      "Time elapsed:  39971.929647922516\n",
      "ep 4316: ep_len:689 episode reward: total was -40.660000. running mean: 30.106901\n",
      "ep 4316: ep_len:518 episode reward: total was 46.790000. running mean: 30.273732\n",
      "ep 4316: ep_len:446 episode reward: total was -44.410000. running mean: 29.526895\n",
      "ep 4316: ep_len:125 episode reward: total was 5.130000. running mean: 29.282926\n",
      "ep 4316: ep_len:3 episode reward: total was 1.010000. running mean: 29.000197\n",
      "ep 4316: ep_len:500 episode reward: total was 30.400000. running mean: 29.014195\n",
      "ep 4316: ep_len:500 episode reward: total was 25.520000. running mean: 28.979253\n",
      "epsilon:0.009994 episode_count: 30219. steps_count: 13074029.000000\n",
      "Time elapsed:  39976.82181763649\n",
      "ep 4317: ep_len:500 episode reward: total was 98.450000. running mean: 29.673960\n",
      "ep 4317: ep_len:553 episode reward: total was 43.370000. running mean: 29.810920\n",
      "ep 4317: ep_len:549 episode reward: total was 36.830000. running mean: 29.881111\n",
      "ep 4317: ep_len:500 episode reward: total was 24.940000. running mean: 29.831700\n",
      "ep 4317: ep_len:3 episode reward: total was 1.010000. running mean: 29.543483\n",
      "ep 4317: ep_len:588 episode reward: total was 68.810000. running mean: 29.936148\n",
      "ep 4317: ep_len:629 episode reward: total was 42.490000. running mean: 30.061687\n",
      "epsilon:0.009994 episode_count: 30226. steps_count: 13077351.000000\n",
      "Time elapsed:  39981.91084551811\n",
      "ep 4318: ep_len:134 episode reward: total was 21.570000. running mean: 29.976770\n",
      "ep 4318: ep_len:500 episode reward: total was 52.350000. running mean: 30.200502\n",
      "ep 4318: ep_len:615 episode reward: total was 38.640000. running mean: 30.284897\n",
      "ep 4318: ep_len:132 episode reward: total was 17.610000. running mean: 30.158148\n",
      "ep 4318: ep_len:3 episode reward: total was 1.010000. running mean: 29.866667\n",
      "ep 4318: ep_len:321 episode reward: total was 36.740000. running mean: 29.935400\n",
      "ep 4318: ep_len:599 episode reward: total was 37.180000. running mean: 30.007846\n",
      "epsilon:0.009994 episode_count: 30233. steps_count: 13079655.000000\n",
      "Time elapsed:  39985.67688989639\n",
      "ep 4319: ep_len:634 episode reward: total was 50.610000. running mean: 30.213868\n",
      "ep 4319: ep_len:522 episode reward: total was 4.970000. running mean: 29.961429\n",
      "ep 4319: ep_len:566 episode reward: total was 11.640000. running mean: 29.778215\n",
      "ep 4319: ep_len:56 episode reward: total was -7.670000. running mean: 29.403733\n",
      "ep 4319: ep_len:3 episode reward: total was 1.010000. running mean: 29.119795\n",
      "ep 4319: ep_len:536 episode reward: total was 34.880000. running mean: 29.177397\n",
      "ep 4319: ep_len:542 episode reward: total was -4.810000. running mean: 28.837523\n",
      "epsilon:0.009994 episode_count: 30240. steps_count: 13082514.000000\n",
      "Time elapsed:  39999.11346936226\n",
      "ep 4320: ep_len:539 episode reward: total was 52.460000. running mean: 29.073748\n",
      "ep 4320: ep_len:500 episode reward: total was 60.140000. running mean: 29.384411\n",
      "ep 4320: ep_len:565 episode reward: total was 26.120000. running mean: 29.351766\n",
      "ep 4320: ep_len:500 episode reward: total was 29.160000. running mean: 29.349849\n",
      "ep 4320: ep_len:86 episode reward: total was 27.270000. running mean: 29.329050\n",
      "ep 4320: ep_len:242 episode reward: total was 56.440000. running mean: 29.600160\n",
      "ep 4320: ep_len:616 episode reward: total was 79.140000. running mean: 30.095558\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.009994 episode_count: 30247. steps_count: 13085562.000000\n",
      "Time elapsed:  40011.924839019775\n",
      "ep 4321: ep_len:641 episode reward: total was 30.820000. running mean: 30.102803\n",
      "ep 4321: ep_len:520 episode reward: total was 61.900000. running mean: 30.420775\n",
      "ep 4321: ep_len:551 episode reward: total was 45.870000. running mean: 30.575267\n",
      "ep 4321: ep_len:500 episode reward: total was 35.310000. running mean: 30.622614\n",
      "ep 4321: ep_len:3 episode reward: total was 1.010000. running mean: 30.326488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4321: ep_len:605 episode reward: total was 62.920000. running mean: 30.652423\n",
      "ep 4321: ep_len:189 episode reward: total was 14.660000. running mean: 30.492499\n",
      "epsilon:0.009994 episode_count: 30254. steps_count: 13088571.000000\n",
      "Time elapsed:  40019.87648677826\n",
      "ep 4322: ep_len:134 episode reward: total was 10.610000. running mean: 30.293674\n",
      "ep 4322: ep_len:500 episode reward: total was 40.320000. running mean: 30.393937\n",
      "ep 4322: ep_len:411 episode reward: total was 69.680000. running mean: 30.786798\n",
      "ep 4322: ep_len:158 episode reward: total was 23.210000. running mean: 30.711030\n",
      "ep 4322: ep_len:3 episode reward: total was 1.010000. running mean: 30.414020\n",
      "ep 4322: ep_len:507 episode reward: total was -17.620000. running mean: 29.933679\n",
      "ep 4322: ep_len:500 episode reward: total was 54.190000. running mean: 30.176243\n",
      "epsilon:0.009994 episode_count: 30261. steps_count: 13090784.000000\n",
      "Time elapsed:  40026.01734042168\n",
      "ep 4323: ep_len:512 episode reward: total was 30.110000. running mean: 30.175580\n",
      "ep 4323: ep_len:548 episode reward: total was 141.370000. running mean: 31.287524\n",
      "ep 4323: ep_len:611 episode reward: total was 4.080000. running mean: 31.015449\n",
      "ep 4323: ep_len:561 episode reward: total was 68.180000. running mean: 31.387095\n",
      "ep 4323: ep_len:89 episode reward: total was 27.760000. running mean: 31.350824\n",
      "ep 4323: ep_len:607 episode reward: total was 24.430000. running mean: 31.281615\n",
      "ep 4323: ep_len:205 episode reward: total was 7.540000. running mean: 31.044199\n",
      "epsilon:0.009994 episode_count: 30268. steps_count: 13093917.000000\n",
      "Time elapsed:  40034.24908542633\n",
      "ep 4324: ep_len:247 episode reward: total was 17.780000. running mean: 30.911557\n",
      "ep 4324: ep_len:500 episode reward: total was 126.080000. running mean: 31.863242\n",
      "ep 4324: ep_len:564 episode reward: total was -9.240000. running mean: 31.452209\n",
      "ep 4324: ep_len:500 episode reward: total was 90.200000. running mean: 32.039687\n",
      "ep 4324: ep_len:52 episode reward: total was 24.500000. running mean: 31.964290\n",
      "ep 4324: ep_len:316 episode reward: total was 45.200000. running mean: 32.096647\n",
      "ep 4324: ep_len:533 episode reward: total was 55.930000. running mean: 32.334981\n",
      "epsilon:0.009994 episode_count: 30275. steps_count: 13096629.000000\n",
      "Time elapsed:  40041.480310201645\n",
      "ep 4325: ep_len:543 episode reward: total was -11.240000. running mean: 31.899231\n",
      "ep 4325: ep_len:500 episode reward: total was 53.260000. running mean: 32.112839\n",
      "ep 4325: ep_len:627 episode reward: total was 13.010000. running mean: 31.921810\n",
      "ep 4325: ep_len:517 episode reward: total was 10.110000. running mean: 31.703692\n",
      "ep 4325: ep_len:3 episode reward: total was 1.010000. running mean: 31.396755\n",
      "ep 4325: ep_len:590 episode reward: total was 88.330000. running mean: 31.966088\n",
      "ep 4325: ep_len:194 episode reward: total was 16.520000. running mean: 31.811627\n",
      "epsilon:0.009994 episode_count: 30282. steps_count: 13099603.000000\n",
      "Time elapsed:  40049.4785284996\n",
      "ep 4326: ep_len:613 episode reward: total was 107.690000. running mean: 32.570411\n",
      "ep 4326: ep_len:600 episode reward: total was 84.910000. running mean: 33.093807\n",
      "ep 4326: ep_len:500 episode reward: total was 74.830000. running mean: 33.511169\n",
      "ep 4326: ep_len:507 episode reward: total was 30.730000. running mean: 33.483357\n",
      "ep 4326: ep_len:3 episode reward: total was 1.010000. running mean: 33.158623\n",
      "ep 4326: ep_len:500 episode reward: total was 29.420000. running mean: 33.121237\n",
      "ep 4326: ep_len:500 episode reward: total was 2.540000. running mean: 32.815425\n",
      "epsilon:0.009994 episode_count: 30289. steps_count: 13102826.000000\n",
      "Time elapsed:  40058.06633806229\n",
      "ep 4327: ep_len:606 episode reward: total was 73.280000. running mean: 33.220070\n",
      "ep 4327: ep_len:502 episode reward: total was 113.530000. running mean: 34.023170\n",
      "ep 4327: ep_len:558 episode reward: total was 30.280000. running mean: 33.985738\n",
      "ep 4327: ep_len:500 episode reward: total was 18.360000. running mean: 33.829481\n",
      "ep 4327: ep_len:3 episode reward: total was 1.010000. running mean: 33.501286\n",
      "ep 4327: ep_len:543 episode reward: total was -239.970000. running mean: 30.766573\n",
      "ep 4327: ep_len:507 episode reward: total was 33.840000. running mean: 30.797307\n",
      "epsilon:0.009994 episode_count: 30296. steps_count: 13106045.000000\n",
      "Time elapsed:  40073.829344034195\n",
      "ep 4328: ep_len:226 episode reward: total was 2.320000. running mean: 30.512534\n",
      "ep 4328: ep_len:502 episode reward: total was -48.120000. running mean: 29.726209\n",
      "ep 4328: ep_len:634 episode reward: total was -19.140000. running mean: 29.237547\n",
      "ep 4328: ep_len:631 episode reward: total was 74.890000. running mean: 29.694071\n",
      "ep 4328: ep_len:103 episode reward: total was 31.730000. running mean: 29.714431\n",
      "ep 4328: ep_len:682 episode reward: total was -118.120000. running mean: 28.236086\n",
      "ep 4328: ep_len:563 episode reward: total was 18.530000. running mean: 28.139025\n",
      "epsilon:0.009994 episode_count: 30303. steps_count: 13109386.000000\n",
      "Time elapsed:  40082.67840433121\n",
      "ep 4329: ep_len:257 episode reward: total was 18.950000. running mean: 28.047135\n",
      "ep 4329: ep_len:500 episode reward: total was 34.310000. running mean: 28.109764\n",
      "ep 4329: ep_len:553 episode reward: total was 15.130000. running mean: 27.979966\n",
      "ep 4329: ep_len:500 episode reward: total was 43.910000. running mean: 28.139267\n",
      "ep 4329: ep_len:89 episode reward: total was 24.760000. running mean: 28.105474\n",
      "ep 4329: ep_len:513 episode reward: total was -5.500000. running mean: 27.769419\n",
      "ep 4329: ep_len:613 episode reward: total was 67.170000. running mean: 28.163425\n",
      "epsilon:0.009994 episode_count: 30310. steps_count: 13112411.000000\n",
      "Time elapsed:  40091.95019006729\n",
      "ep 4330: ep_len:652 episode reward: total was 30.250000. running mean: 28.184291\n",
      "ep 4330: ep_len:552 episode reward: total was -10.510000. running mean: 27.797348\n",
      "ep 4330: ep_len:500 episode reward: total was 25.420000. running mean: 27.773574\n",
      "ep 4330: ep_len:565 episode reward: total was 80.070000. running mean: 28.296539\n",
      "ep 4330: ep_len:107 episode reward: total was 34.770000. running mean: 28.361273\n",
      "ep 4330: ep_len:552 episode reward: total was 20.030000. running mean: 28.277960\n",
      "ep 4330: ep_len:563 episode reward: total was 63.320000. running mean: 28.628381\n",
      "epsilon:0.009994 episode_count: 30317. steps_count: 13115902.000000\n",
      "Time elapsed:  40115.91240000725\n",
      "ep 4331: ep_len:599 episode reward: total was 93.850000. running mean: 29.280597\n",
      "ep 4331: ep_len:500 episode reward: total was 110.590000. running mean: 30.093691\n",
      "ep 4331: ep_len:570 episode reward: total was 4.680000. running mean: 29.839554\n",
      "ep 4331: ep_len:508 episode reward: total was 78.250000. running mean: 30.323659\n",
      "ep 4331: ep_len:3 episode reward: total was 1.010000. running mean: 30.030522\n",
      "ep 4331: ep_len:170 episode reward: total was 15.750000. running mean: 29.887717\n",
      "ep 4331: ep_len:631 episode reward: total was 47.350000. running mean: 30.062340\n",
      "epsilon:0.009994 episode_count: 30324. steps_count: 13118883.000000\n",
      "Time elapsed:  40131.50241279602\n",
      "ep 4332: ep_len:622 episode reward: total was 53.490000. running mean: 30.296616\n",
      "ep 4332: ep_len:526 episode reward: total was -7.200000. running mean: 29.921650\n",
      "ep 4332: ep_len:587 episode reward: total was 34.370000. running mean: 29.966134\n",
      "ep 4332: ep_len:555 episode reward: total was 55.900000. running mean: 30.225472\n",
      "ep 4332: ep_len:102 episode reward: total was 31.260000. running mean: 30.235817\n",
      "ep 4332: ep_len:509 episode reward: total was 24.120000. running mean: 30.174659\n",
      "ep 4332: ep_len:621 episode reward: total was 58.500000. running mean: 30.457913\n",
      "epsilon:0.009994 episode_count: 30331. steps_count: 13122405.000000\n",
      "Time elapsed:  40148.92303228378\n",
      "ep 4333: ep_len:518 episode reward: total was 2.450000. running mean: 30.177834\n",
      "ep 4333: ep_len:500 episode reward: total was 54.380000. running mean: 30.419855\n",
      "ep 4333: ep_len:580 episode reward: total was -26.440000. running mean: 29.851257\n",
      "ep 4333: ep_len:549 episode reward: total was 87.980000. running mean: 30.432544\n",
      "ep 4333: ep_len:3 episode reward: total was 1.010000. running mean: 30.138319\n",
      "ep 4333: ep_len:581 episode reward: total was 63.050000. running mean: 30.467436\n",
      "ep 4333: ep_len:596 episode reward: total was 29.260000. running mean: 30.455361\n",
      "epsilon:0.009994 episode_count: 30338. steps_count: 13125732.000000\n",
      "Time elapsed:  40164.604377269745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4334: ep_len:500 episode reward: total was 91.130000. running mean: 31.062108\n",
      "ep 4334: ep_len:500 episode reward: total was 52.300000. running mean: 31.274486\n",
      "ep 4334: ep_len:500 episode reward: total was 16.610000. running mean: 31.127842\n",
      "ep 4334: ep_len:380 episode reward: total was -118.630000. running mean: 29.630263\n",
      "ep 4334: ep_len:3 episode reward: total was 1.010000. running mean: 29.344061\n",
      "ep 4334: ep_len:321 episode reward: total was 34.200000. running mean: 29.392620\n",
      "ep 4334: ep_len:302 episode reward: total was 24.800000. running mean: 29.346694\n",
      "epsilon:0.009994 episode_count: 30345. steps_count: 13128238.000000\n",
      "Time elapsed:  40178.658408641815\n",
      "ep 4335: ep_len:618 episode reward: total was 62.910000. running mean: 29.682327\n",
      "ep 4335: ep_len:500 episode reward: total was 154.550000. running mean: 30.931004\n",
      "ep 4335: ep_len:431 episode reward: total was 49.960000. running mean: 31.121294\n",
      "ep 4335: ep_len:637 episode reward: total was -99.030000. running mean: 29.819781\n",
      "ep 4335: ep_len:3 episode reward: total was -1.500000. running mean: 29.506583\n",
      "ep 4335: ep_len:503 episode reward: total was 34.730000. running mean: 29.558817\n",
      "ep 4335: ep_len:575 episode reward: total was 85.480000. running mean: 30.118029\n",
      "epsilon:0.009994 episode_count: 30352. steps_count: 13131505.000000\n",
      "Time elapsed:  40187.216675281525\n",
      "ep 4336: ep_len:500 episode reward: total was -16.990000. running mean: 29.646948\n",
      "ep 4336: ep_len:285 episode reward: total was 14.270000. running mean: 29.493179\n",
      "ep 4336: ep_len:500 episode reward: total was 38.120000. running mean: 29.579447\n",
      "ep 4336: ep_len:500 episode reward: total was 27.910000. running mean: 29.562753\n",
      "ep 4336: ep_len:3 episode reward: total was 1.010000. running mean: 29.277225\n",
      "ep 4336: ep_len:616 episode reward: total was 56.020000. running mean: 29.544653\n",
      "ep 4336: ep_len:541 episode reward: total was 50.440000. running mean: 29.753606\n",
      "epsilon:0.009994 episode_count: 30359. steps_count: 13134450.000000\n",
      "Time elapsed:  40195.13583278656\n",
      "ep 4337: ep_len:500 episode reward: total was 5.480000. running mean: 29.510870\n",
      "ep 4337: ep_len:500 episode reward: total was 136.120000. running mean: 30.576962\n",
      "ep 4337: ep_len:554 episode reward: total was 39.750000. running mean: 30.668692\n",
      "ep 4337: ep_len:500 episode reward: total was -90.420000. running mean: 29.457805\n",
      "ep 4337: ep_len:54 episode reward: total was 24.000000. running mean: 29.403227\n",
      "ep 4337: ep_len:548 episode reward: total was 1.770000. running mean: 29.126895\n",
      "ep 4337: ep_len:579 episode reward: total was 78.080000. running mean: 29.616426\n",
      "epsilon:0.009994 episode_count: 30366. steps_count: 13137685.000000\n",
      "Time elapsed:  40203.555567502975\n",
      "ep 4338: ep_len:193 episode reward: total was 37.180000. running mean: 29.692062\n",
      "ep 4338: ep_len:579 episode reward: total was 93.640000. running mean: 30.331541\n",
      "ep 4338: ep_len:598 episode reward: total was 23.040000. running mean: 30.258626\n",
      "ep 4338: ep_len:593 episode reward: total was -15.300000. running mean: 29.803039\n",
      "ep 4338: ep_len:3 episode reward: total was 1.010000. running mean: 29.515109\n",
      "ep 4338: ep_len:500 episode reward: total was -49.840000. running mean: 28.721558\n",
      "ep 4338: ep_len:521 episode reward: total was 14.140000. running mean: 28.575742\n",
      "epsilon:0.009994 episode_count: 30373. steps_count: 13140672.000000\n",
      "Time elapsed:  40211.546874284744\n",
      "ep 4339: ep_len:593 episode reward: total was 7.080000. running mean: 28.360785\n",
      "ep 4339: ep_len:500 episode reward: total was 56.460000. running mean: 28.641777\n",
      "ep 4339: ep_len:548 episode reward: total was -16.230000. running mean: 28.193059\n",
      "ep 4339: ep_len:509 episode reward: total was 54.940000. running mean: 28.460529\n",
      "ep 4339: ep_len:89 episode reward: total was 26.260000. running mean: 28.438523\n",
      "ep 4339: ep_len:261 episode reward: total was 60.090000. running mean: 28.755038\n",
      "ep 4339: ep_len:335 episode reward: total was -43.200000. running mean: 28.035488\n",
      "epsilon:0.009994 episode_count: 30380. steps_count: 13143507.000000\n",
      "Time elapsed:  40219.1858291626\n",
      "ep 4340: ep_len:583 episode reward: total was 60.720000. running mean: 28.362333\n",
      "ep 4340: ep_len:584 episode reward: total was 83.960000. running mean: 28.918310\n",
      "ep 4340: ep_len:500 episode reward: total was 56.340000. running mean: 29.192526\n",
      "ep 4340: ep_len:518 episode reward: total was 18.270000. running mean: 29.083301\n",
      "ep 4340: ep_len:3 episode reward: total was 1.010000. running mean: 28.802568\n",
      "ep 4340: ep_len:776 episode reward: total was -305.340000. running mean: 25.461142\n",
      "ep 4340: ep_len:526 episode reward: total was -28.080000. running mean: 24.925731\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.009994 episode_count: 30387. steps_count: 13146997.000000\n",
      "Time elapsed:  40232.97205758095\n",
      "ep 4341: ep_len:651 episode reward: total was 4.230000. running mean: 24.718774\n",
      "ep 4341: ep_len:500 episode reward: total was 98.600000. running mean: 25.457586\n",
      "ep 4341: ep_len:577 episode reward: total was 74.940000. running mean: 25.952410\n",
      "ep 4341: ep_len:622 episode reward: total was 68.690000. running mean: 26.379786\n",
      "ep 4341: ep_len:3 episode reward: total was 1.010000. running mean: 26.126088\n",
      "ep 4341: ep_len:232 episode reward: total was 54.440000. running mean: 26.409227\n",
      "ep 4341: ep_len:500 episode reward: total was -8.080000. running mean: 26.064335\n",
      "epsilon:0.009994 episode_count: 30394. steps_count: 13150082.000000\n",
      "Time elapsed:  40241.2179441452\n",
      "ep 4342: ep_len:652 episode reward: total was -17.920000. running mean: 25.624492\n",
      "ep 4342: ep_len:500 episode reward: total was 2.140000. running mean: 25.389647\n",
      "ep 4342: ep_len:509 episode reward: total was 55.540000. running mean: 25.691150\n",
      "ep 4342: ep_len:535 episode reward: total was 77.950000. running mean: 26.213739\n",
      "ep 4342: ep_len:56 episode reward: total was 23.010000. running mean: 26.181701\n",
      "ep 4342: ep_len:558 episode reward: total was -130.450000. running mean: 24.615384\n",
      "ep 4342: ep_len:184 episode reward: total was 9.870000. running mean: 24.467931\n",
      "epsilon:0.009994 episode_count: 30401. steps_count: 13153076.000000\n",
      "Time elapsed:  40246.80325841904\n",
      "ep 4343: ep_len:134 episode reward: total was 11.100000. running mean: 24.334251\n",
      "ep 4343: ep_len:541 episode reward: total was 30.100000. running mean: 24.391909\n",
      "ep 4343: ep_len:603 episode reward: total was 49.470000. running mean: 24.642690\n",
      "ep 4343: ep_len:577 episode reward: total was 84.360000. running mean: 25.239863\n",
      "ep 4343: ep_len:3 episode reward: total was 1.010000. running mean: 24.997564\n",
      "ep 4343: ep_len:500 episode reward: total was 64.060000. running mean: 25.388188\n",
      "ep 4343: ep_len:339 episode reward: total was 11.090000. running mean: 25.245207\n",
      "epsilon:0.009994 episode_count: 30408. steps_count: 13155773.000000\n",
      "Time elapsed:  40252.66869020462\n",
      "ep 4344: ep_len:560 episode reward: total was 5.280000. running mean: 25.045555\n",
      "ep 4344: ep_len:268 episode reward: total was 5.310000. running mean: 24.848199\n",
      "ep 4344: ep_len:501 episode reward: total was -2.800000. running mean: 24.571717\n",
      "ep 4344: ep_len:631 episode reward: total was 123.900000. running mean: 25.565000\n",
      "ep 4344: ep_len:102 episode reward: total was 32.270000. running mean: 25.632050\n",
      "ep 4344: ep_len:640 episode reward: total was -18.600000. running mean: 25.189729\n",
      "ep 4344: ep_len:560 episode reward: total was 62.680000. running mean: 25.564632\n",
      "epsilon:0.009994 episode_count: 30415. steps_count: 13159035.000000\n",
      "Time elapsed:  40260.04900312424\n",
      "ep 4345: ep_len:579 episode reward: total was 48.700000. running mean: 25.795986\n",
      "ep 4345: ep_len:500 episode reward: total was 52.450000. running mean: 26.062526\n",
      "ep 4345: ep_len:588 episode reward: total was -4.220000. running mean: 25.759701\n",
      "ep 4345: ep_len:519 episode reward: total was 22.290000. running mean: 25.725004\n",
      "ep 4345: ep_len:3 episode reward: total was 1.010000. running mean: 25.477854\n",
      "ep 4345: ep_len:500 episode reward: total was 46.250000. running mean: 25.685575\n",
      "ep 4345: ep_len:350 episode reward: total was 1.590000. running mean: 25.444619\n",
      "epsilon:0.009994 episode_count: 30422. steps_count: 13162074.000000\n",
      "Time elapsed:  40271.93133234978\n",
      "ep 4346: ep_len:620 episode reward: total was 0.310000. running mean: 25.193273\n",
      "ep 4346: ep_len:500 episode reward: total was 121.460000. running mean: 26.155940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4346: ep_len:584 episode reward: total was 25.370000. running mean: 26.148081\n",
      "ep 4346: ep_len:536 episode reward: total was -93.180000. running mean: 24.954800\n",
      "ep 4346: ep_len:3 episode reward: total was 1.010000. running mean: 24.715352\n",
      "ep 4346: ep_len:500 episode reward: total was 45.560000. running mean: 24.923799\n",
      "ep 4346: ep_len:555 episode reward: total was 42.580000. running mean: 25.100361\n",
      "epsilon:0.009994 episode_count: 30429. steps_count: 13165372.000000\n",
      "Time elapsed:  40280.25699734688\n",
      "ep 4347: ep_len:500 episode reward: total was 44.830000. running mean: 25.297657\n",
      "ep 4347: ep_len:604 episode reward: total was -5.010000. running mean: 24.994580\n",
      "ep 4347: ep_len:561 episode reward: total was -18.360000. running mean: 24.561035\n",
      "ep 4347: ep_len:534 episode reward: total was 101.060000. running mean: 25.326024\n",
      "ep 4347: ep_len:3 episode reward: total was 1.010000. running mean: 25.082864\n",
      "ep 4347: ep_len:501 episode reward: total was 45.510000. running mean: 25.287135\n",
      "ep 4347: ep_len:196 episode reward: total was 9.530000. running mean: 25.129564\n",
      "epsilon:0.009994 episode_count: 30436. steps_count: 13168271.000000\n",
      "Time elapsed:  40287.70664191246\n",
      "ep 4348: ep_len:203 episode reward: total was 7.220000. running mean: 24.950468\n",
      "ep 4348: ep_len:545 episode reward: total was 27.920000. running mean: 24.980164\n",
      "ep 4348: ep_len:539 episode reward: total was -59.200000. running mean: 24.138362\n",
      "ep 4348: ep_len:161 episode reward: total was 19.160000. running mean: 24.088578\n",
      "ep 4348: ep_len:3 episode reward: total was 1.010000. running mean: 23.857793\n",
      "ep 4348: ep_len:500 episode reward: total was 44.430000. running mean: 24.063515\n",
      "ep 4348: ep_len:570 episode reward: total was 25.350000. running mean: 24.076380\n",
      "epsilon:0.009994 episode_count: 30443. steps_count: 13170792.000000\n",
      "Time elapsed:  40302.22665786743\n",
      "ep 4349: ep_len:500 episode reward: total was 68.090000. running mean: 24.516516\n",
      "ep 4349: ep_len:500 episode reward: total was 34.320000. running mean: 24.614551\n",
      "ep 4349: ep_len:641 episode reward: total was 14.480000. running mean: 24.513205\n",
      "ep 4349: ep_len:516 episode reward: total was 38.860000. running mean: 24.656673\n",
      "ep 4349: ep_len:84 episode reward: total was 24.770000. running mean: 24.657806\n",
      "ep 4349: ep_len:240 episode reward: total was 56.050000. running mean: 24.971728\n",
      "ep 4349: ep_len:506 episode reward: total was 65.550000. running mean: 25.377511\n",
      "epsilon:0.009994 episode_count: 30450. steps_count: 13173779.000000\n",
      "Time elapsed:  40319.156839609146\n",
      "ep 4350: ep_len:519 episode reward: total was 4.140000. running mean: 25.165136\n",
      "ep 4350: ep_len:500 episode reward: total was 54.290000. running mean: 25.456385\n",
      "ep 4350: ep_len:608 episode reward: total was 78.170000. running mean: 25.983521\n",
      "ep 4350: ep_len:147 episode reward: total was 22.580000. running mean: 25.949485\n",
      "ep 4350: ep_len:3 episode reward: total was 1.010000. running mean: 25.700091\n",
      "ep 4350: ep_len:171 episode reward: total was 30.240000. running mean: 25.745490\n",
      "ep 4350: ep_len:550 episode reward: total was 72.400000. running mean: 26.212035\n",
      "epsilon:0.009994 episode_count: 30457. steps_count: 13176277.000000\n",
      "Time elapsed:  40326.06364321709\n",
      "ep 4351: ep_len:502 episode reward: total was 86.140000. running mean: 26.811314\n",
      "ep 4351: ep_len:613 episode reward: total was 100.010000. running mean: 27.543301\n",
      "ep 4351: ep_len:571 episode reward: total was 32.790000. running mean: 27.595768\n",
      "ep 4351: ep_len:132 episode reward: total was 3.610000. running mean: 27.355911\n",
      "ep 4351: ep_len:3 episode reward: total was 1.010000. running mean: 27.092452\n",
      "ep 4351: ep_len:519 episode reward: total was 41.910000. running mean: 27.240627\n",
      "ep 4351: ep_len:610 episode reward: total was 72.040000. running mean: 27.688621\n",
      "epsilon:0.009994 episode_count: 30464. steps_count: 13179227.000000\n",
      "Time elapsed:  40344.63795661926\n",
      "ep 4352: ep_len:216 episode reward: total was 19.810000. running mean: 27.609835\n",
      "ep 4352: ep_len:624 episode reward: total was 61.250000. running mean: 27.946236\n",
      "ep 4352: ep_len:79 episode reward: total was 10.330000. running mean: 27.770074\n",
      "ep 4352: ep_len:596 episode reward: total was 62.630000. running mean: 28.118673\n",
      "ep 4352: ep_len:49 episode reward: total was 24.010000. running mean: 28.077586\n",
      "ep 4352: ep_len:510 episode reward: total was 33.830000. running mean: 28.135111\n",
      "ep 4352: ep_len:500 episode reward: total was 61.720000. running mean: 28.470959\n",
      "epsilon:0.009994 episode_count: 30471. steps_count: 13181801.000000\n",
      "Time elapsed:  40362.150473594666\n",
      "ep 4353: ep_len:639 episode reward: total was 79.230000. running mean: 28.978550\n",
      "ep 4353: ep_len:531 episode reward: total was 14.970000. running mean: 28.838464\n",
      "ep 4353: ep_len:500 episode reward: total was 89.180000. running mean: 29.441880\n",
      "ep 4353: ep_len:546 episode reward: total was 72.890000. running mean: 29.876361\n",
      "ep 4353: ep_len:3 episode reward: total was 1.010000. running mean: 29.587697\n",
      "ep 4353: ep_len:173 episode reward: total was 32.650000. running mean: 29.618320\n",
      "ep 4353: ep_len:566 episode reward: total was -10.940000. running mean: 29.212737\n",
      "epsilon:0.009994 episode_count: 30478. steps_count: 13184759.000000\n",
      "Time elapsed:  40371.05565047264\n",
      "ep 4354: ep_len:636 episode reward: total was 89.330000. running mean: 29.813910\n",
      "ep 4354: ep_len:500 episode reward: total was 41.400000. running mean: 29.929771\n",
      "ep 4354: ep_len:503 episode reward: total was 60.100000. running mean: 30.231473\n",
      "ep 4354: ep_len:507 episode reward: total was 55.320000. running mean: 30.482358\n",
      "ep 4354: ep_len:2 episode reward: total was -0.500000. running mean: 30.172535\n",
      "ep 4354: ep_len:678 episode reward: total was -63.760000. running mean: 29.233209\n",
      "ep 4354: ep_len:546 episode reward: total was 62.380000. running mean: 29.564677\n",
      "epsilon:0.009994 episode_count: 30485. steps_count: 13188131.000000\n",
      "Time elapsed:  40381.20969033241\n",
      "ep 4355: ep_len:589 episode reward: total was 53.270000. running mean: 29.801730\n",
      "ep 4355: ep_len:518 episode reward: total was 111.680000. running mean: 30.620513\n",
      "ep 4355: ep_len:630 episode reward: total was 15.180000. running mean: 30.466108\n",
      "ep 4355: ep_len:509 episode reward: total was 0.980000. running mean: 30.171247\n",
      "ep 4355: ep_len:104 episode reward: total was 34.770000. running mean: 30.217234\n",
      "ep 4355: ep_len:532 episode reward: total was 32.750000. running mean: 30.242562\n",
      "ep 4355: ep_len:521 episode reward: total was 15.060000. running mean: 30.090736\n",
      "epsilon:0.009994 episode_count: 30492. steps_count: 13191534.000000\n",
      "Time elapsed:  40391.468222379684\n",
      "ep 4356: ep_len:583 episode reward: total was 77.100000. running mean: 30.560829\n",
      "ep 4356: ep_len:500 episode reward: total was 70.800000. running mean: 30.963221\n",
      "ep 4356: ep_len:541 episode reward: total was 51.740000. running mean: 31.170989\n",
      "ep 4356: ep_len:132 episode reward: total was 24.100000. running mean: 31.100279\n",
      "ep 4356: ep_len:3 episode reward: total was 1.010000. running mean: 30.799376\n",
      "ep 4356: ep_len:567 episode reward: total was 39.090000. running mean: 30.882282\n",
      "ep 4356: ep_len:527 episode reward: total was 65.450000. running mean: 31.227959\n",
      "epsilon:0.009994 episode_count: 30499. steps_count: 13194387.000000\n",
      "Time elapsed:  40400.216871738434\n",
      "ep 4357: ep_len:568 episode reward: total was 58.510000. running mean: 31.500780\n",
      "ep 4357: ep_len:500 episode reward: total was 141.000000. running mean: 32.595772\n",
      "ep 4357: ep_len:62 episode reward: total was 6.120000. running mean: 32.331014\n",
      "ep 4357: ep_len:500 episode reward: total was 14.710000. running mean: 32.154804\n",
      "ep 4357: ep_len:3 episode reward: total was 1.010000. running mean: 31.843356\n",
      "ep 4357: ep_len:500 episode reward: total was 70.580000. running mean: 32.230722\n",
      "ep 4357: ep_len:621 episode reward: total was 55.080000. running mean: 32.459215\n",
      "epsilon:0.009994 episode_count: 30506. steps_count: 13197141.000000\n",
      "Time elapsed:  40408.682990312576\n",
      "ep 4358: ep_len:533 episode reward: total was 55.370000. running mean: 32.688323\n",
      "ep 4358: ep_len:611 episode reward: total was -9.660000. running mean: 32.264840\n",
      "ep 4358: ep_len:79 episode reward: total was 10.820000. running mean: 32.050391\n",
      "ep 4358: ep_len:525 episode reward: total was 80.920000. running mean: 32.539088\n",
      "ep 4358: ep_len:3 episode reward: total was 1.010000. running mean: 32.223797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4358: ep_len:637 episode reward: total was 44.380000. running mean: 32.345359\n",
      "ep 4358: ep_len:578 episode reward: total was 42.220000. running mean: 32.444105\n",
      "epsilon:0.009994 episode_count: 30513. steps_count: 13200107.000000\n",
      "Time elapsed:  40417.67921733856\n",
      "ep 4359: ep_len:577 episode reward: total was 91.010000. running mean: 33.029764\n",
      "ep 4359: ep_len:547 episode reward: total was 72.010000. running mean: 33.419566\n",
      "ep 4359: ep_len:500 episode reward: total was 52.600000. running mean: 33.611371\n",
      "ep 4359: ep_len:511 episode reward: total was 27.310000. running mean: 33.548357\n",
      "ep 4359: ep_len:3 episode reward: total was 1.010000. running mean: 33.222973\n",
      "ep 4359: ep_len:1082 episode reward: total was -444.160000. running mean: 28.449144\n",
      "ep 4359: ep_len:622 episode reward: total was 83.760000. running mean: 29.002252\n",
      "epsilon:0.009994 episode_count: 30520. steps_count: 13203949.000000\n",
      "Time elapsed:  40428.91441869736\n",
      "ep 4360: ep_len:607 episode reward: total was 72.560000. running mean: 29.437830\n",
      "ep 4360: ep_len:519 episode reward: total was 23.120000. running mean: 29.374651\n",
      "ep 4360: ep_len:500 episode reward: total was 70.940000. running mean: 29.790305\n",
      "ep 4360: ep_len:132 episode reward: total was 30.620000. running mean: 29.798602\n",
      "ep 4360: ep_len:3 episode reward: total was 1.010000. running mean: 29.510716\n",
      "ep 4360: ep_len:500 episode reward: total was 53.540000. running mean: 29.751009\n",
      "ep 4360: ep_len:551 episode reward: total was 14.800000. running mean: 29.601499\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.009994 episode_count: 30527. steps_count: 13206761.000000\n",
      "Time elapsed:  40442.939477443695\n",
      "ep 4361: ep_len:500 episode reward: total was 108.980000. running mean: 30.395284\n",
      "ep 4361: ep_len:500 episode reward: total was 37.010000. running mean: 30.461431\n",
      "ep 4361: ep_len:500 episode reward: total was 49.020000. running mean: 30.647017\n",
      "ep 4361: ep_len:583 episode reward: total was 76.310000. running mean: 31.103646\n",
      "ep 4361: ep_len:3 episode reward: total was 1.010000. running mean: 30.802710\n",
      "ep 4361: ep_len:500 episode reward: total was 57.960000. running mean: 31.074283\n",
      "ep 4361: ep_len:588 episode reward: total was 65.060000. running mean: 31.414140\n",
      "epsilon:0.009994 episode_count: 30534. steps_count: 13209935.000000\n",
      "Time elapsed:  40452.54347324371\n",
      "ep 4362: ep_len:227 episode reward: total was 6.670000. running mean: 31.166699\n",
      "ep 4362: ep_len:611 episode reward: total was 55.780000. running mean: 31.412832\n",
      "ep 4362: ep_len:403 episode reward: total was 78.690000. running mean: 31.885603\n",
      "ep 4362: ep_len:615 episode reward: total was 69.160000. running mean: 32.258347\n",
      "ep 4362: ep_len:3 episode reward: total was 1.010000. running mean: 31.945864\n",
      "ep 4362: ep_len:321 episode reward: total was 20.760000. running mean: 31.834005\n",
      "ep 4362: ep_len:510 episode reward: total was -9.830000. running mean: 31.417365\n",
      "epsilon:0.009994 episode_count: 30541. steps_count: 13212625.000000\n",
      "Time elapsed:  40472.54374957085\n",
      "ep 4363: ep_len:626 episode reward: total was 42.090000. running mean: 31.524091\n",
      "ep 4363: ep_len:500 episode reward: total was 54.930000. running mean: 31.758151\n",
      "ep 4363: ep_len:500 episode reward: total was 33.250000. running mean: 31.773069\n",
      "ep 4363: ep_len:626 episode reward: total was -12.480000. running mean: 31.330538\n",
      "ep 4363: ep_len:93 episode reward: total was 27.770000. running mean: 31.294933\n",
      "ep 4363: ep_len:543 episode reward: total was 36.640000. running mean: 31.348384\n",
      "ep 4363: ep_len:594 episode reward: total was 36.600000. running mean: 31.400900\n",
      "epsilon:0.009994 episode_count: 30548. steps_count: 13216107.000000\n",
      "Time elapsed:  40483.58820557594\n",
      "ep 4364: ep_len:533 episode reward: total was -133.420000. running mean: 29.752691\n",
      "ep 4364: ep_len:288 episode reward: total was 24.370000. running mean: 29.698864\n",
      "ep 4364: ep_len:500 episode reward: total was 56.490000. running mean: 29.966775\n",
      "ep 4364: ep_len:514 episode reward: total was 68.920000. running mean: 30.356307\n",
      "ep 4364: ep_len:130 episode reward: total was 30.380000. running mean: 30.356544\n",
      "ep 4364: ep_len:303 episode reward: total was 13.230000. running mean: 30.185279\n",
      "ep 4364: ep_len:505 episode reward: total was 22.150000. running mean: 30.104926\n",
      "epsilon:0.009994 episode_count: 30555. steps_count: 13218880.000000\n",
      "Time elapsed:  40492.12538385391\n",
      "ep 4365: ep_len:649 episode reward: total was 67.380000. running mean: 30.477677\n",
      "ep 4365: ep_len:334 episode reward: total was 30.720000. running mean: 30.480100\n",
      "ep 4365: ep_len:662 episode reward: total was 27.240000. running mean: 30.447699\n",
      "ep 4365: ep_len:521 episode reward: total was 29.040000. running mean: 30.433622\n",
      "ep 4365: ep_len:55 episode reward: total was 26.000000. running mean: 30.389286\n",
      "ep 4365: ep_len:679 episode reward: total was 56.310000. running mean: 30.648493\n",
      "ep 4365: ep_len:574 episode reward: total was 57.800000. running mean: 30.920008\n",
      "epsilon:0.009994 episode_count: 30562. steps_count: 13222354.000000\n",
      "Time elapsed:  40502.4114511013\n",
      "ep 4366: ep_len:225 episode reward: total was 21.770000. running mean: 30.828508\n",
      "ep 4366: ep_len:510 episode reward: total was 43.120000. running mean: 30.951423\n",
      "ep 4366: ep_len:500 episode reward: total was 43.420000. running mean: 31.076109\n",
      "ep 4366: ep_len:56 episode reward: total was 3.850000. running mean: 30.803848\n",
      "ep 4366: ep_len:3 episode reward: total was 1.010000. running mean: 30.505909\n",
      "ep 4366: ep_len:542 episode reward: total was 18.400000. running mean: 30.384850\n",
      "ep 4366: ep_len:500 episode reward: total was 64.900000. running mean: 30.730002\n",
      "epsilon:0.009994 episode_count: 30569. steps_count: 13224690.000000\n",
      "Time elapsed:  40514.1039018631\n",
      "ep 4367: ep_len:590 episode reward: total was -9.110000. running mean: 30.331602\n",
      "ep 4367: ep_len:500 episode reward: total was 37.220000. running mean: 30.400486\n",
      "ep 4367: ep_len:648 episode reward: total was 47.270000. running mean: 30.569181\n",
      "ep 4367: ep_len:56 episode reward: total was 6.360000. running mean: 30.327089\n",
      "ep 4367: ep_len:3 episode reward: total was 1.010000. running mean: 30.033918\n",
      "ep 4367: ep_len:312 episode reward: total was 31.600000. running mean: 30.049579\n",
      "ep 4367: ep_len:500 episode reward: total was 84.670000. running mean: 30.595783\n",
      "epsilon:0.009994 episode_count: 30576. steps_count: 13227299.000000\n",
      "Time elapsed:  40522.1420211792\n",
      "ep 4368: ep_len:509 episode reward: total was 85.140000. running mean: 31.141225\n",
      "ep 4368: ep_len:537 episode reward: total was 21.110000. running mean: 31.040913\n",
      "ep 4368: ep_len:669 episode reward: total was -58.000000. running mean: 30.150504\n",
      "ep 4368: ep_len:594 episode reward: total was -3.870000. running mean: 29.810299\n",
      "ep 4368: ep_len:3 episode reward: total was 1.010000. running mean: 29.522296\n",
      "ep 4368: ep_len:603 episode reward: total was 43.000000. running mean: 29.657073\n",
      "ep 4368: ep_len:211 episode reward: total was 27.310000. running mean: 29.633602\n",
      "epsilon:0.009994 episode_count: 30583. steps_count: 13230425.000000\n",
      "Time elapsed:  40544.43703484535\n",
      "ep 4369: ep_len:500 episode reward: total was 78.860000. running mean: 30.125866\n",
      "ep 4369: ep_len:567 episode reward: total was 124.080000. running mean: 31.065407\n",
      "ep 4369: ep_len:528 episode reward: total was 22.260000. running mean: 30.977353\n",
      "ep 4369: ep_len:503 episode reward: total was 70.490000. running mean: 31.372480\n",
      "ep 4369: ep_len:103 episode reward: total was 35.280000. running mean: 31.411555\n",
      "ep 4369: ep_len:563 episode reward: total was 66.690000. running mean: 31.764339\n",
      "ep 4369: ep_len:607 episode reward: total was 58.270000. running mean: 32.029396\n",
      "epsilon:0.009994 episode_count: 30590. steps_count: 13233796.000000\n",
      "Time elapsed:  40567.52984595299\n",
      "ep 4370: ep_len:516 episode reward: total was 75.020000. running mean: 32.459302\n",
      "ep 4370: ep_len:500 episode reward: total was 84.640000. running mean: 32.981109\n",
      "ep 4370: ep_len:583 episode reward: total was 27.250000. running mean: 32.923798\n",
      "ep 4370: ep_len:149 episode reward: total was 25.510000. running mean: 32.849660\n",
      "ep 4370: ep_len:3 episode reward: total was 1.010000. running mean: 32.531263\n",
      "ep 4370: ep_len:519 episode reward: total was 39.540000. running mean: 32.601351\n",
      "ep 4370: ep_len:590 episode reward: total was 46.530000. running mean: 32.740637\n",
      "epsilon:0.009994 episode_count: 30597. steps_count: 13236656.000000\n",
      "Time elapsed:  40576.2401702404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4371: ep_len:211 episode reward: total was 4.550000. running mean: 32.458731\n",
      "ep 4371: ep_len:504 episode reward: total was 63.650000. running mean: 32.770644\n",
      "ep 4371: ep_len:500 episode reward: total was 52.380000. running mean: 32.966737\n",
      "ep 4371: ep_len:121 episode reward: total was 25.550000. running mean: 32.892570\n",
      "ep 4371: ep_len:3 episode reward: total was 1.010000. running mean: 32.573744\n",
      "ep 4371: ep_len:708 episode reward: total was 49.770000. running mean: 32.745707\n",
      "ep 4371: ep_len:593 episode reward: total was 66.870000. running mean: 33.086950\n",
      "epsilon:0.009994 episode_count: 30604. steps_count: 13239296.000000\n",
      "Time elapsed:  40597.161296606064\n",
      "ep 4372: ep_len:610 episode reward: total was -179.190000. running mean: 30.964180\n",
      "ep 4372: ep_len:500 episode reward: total was 115.540000. running mean: 31.809938\n",
      "ep 4372: ep_len:510 episode reward: total was -0.210000. running mean: 31.489739\n",
      "ep 4372: ep_len:585 episode reward: total was 70.880000. running mean: 31.883642\n",
      "ep 4372: ep_len:3 episode reward: total was 1.010000. running mean: 31.574905\n",
      "ep 4372: ep_len:500 episode reward: total was 56.760000. running mean: 31.826756\n",
      "ep 4372: ep_len:329 episode reward: total was 34.710000. running mean: 31.855588\n",
      "epsilon:0.009994 episode_count: 30611. steps_count: 13242333.000000\n",
      "Time elapsed:  40613.2902173996\n",
      "ep 4373: ep_len:500 episode reward: total was 7.350000. running mean: 31.610533\n",
      "ep 4373: ep_len:565 episode reward: total was 34.240000. running mean: 31.636827\n",
      "ep 4373: ep_len:511 episode reward: total was 58.470000. running mean: 31.905159\n",
      "ep 4373: ep_len:41 episode reward: total was 5.690000. running mean: 31.643007\n",
      "ep 4373: ep_len:71 episode reward: total was 20.690000. running mean: 31.533477\n",
      "ep 4373: ep_len:565 episode reward: total was 43.640000. running mean: 31.654543\n",
      "ep 4373: ep_len:551 episode reward: total was 43.050000. running mean: 31.768497\n",
      "epsilon:0.009994 episode_count: 30618. steps_count: 13245137.000000\n",
      "Time elapsed:  40619.91250014305\n",
      "ep 4374: ep_len:253 episode reward: total was 11.390000. running mean: 31.564712\n",
      "ep 4374: ep_len:503 episode reward: total was 36.660000. running mean: 31.615665\n",
      "ep 4374: ep_len:441 episode reward: total was 43.320000. running mean: 31.732708\n",
      "ep 4374: ep_len:520 episode reward: total was -7.010000. running mean: 31.345281\n",
      "ep 4374: ep_len:3 episode reward: total was 1.010000. running mean: 31.041929\n",
      "ep 4374: ep_len:500 episode reward: total was 19.900000. running mean: 30.930509\n",
      "ep 4374: ep_len:592 episode reward: total was 74.640000. running mean: 31.367604\n",
      "epsilon:0.009994 episode_count: 30625. steps_count: 13247949.000000\n",
      "Time elapsed:  40627.140550374985\n",
      "ep 4375: ep_len:581 episode reward: total was 54.450000. running mean: 31.598428\n",
      "ep 4375: ep_len:500 episode reward: total was 81.880000. running mean: 32.101244\n",
      "ep 4375: ep_len:547 episode reward: total was 59.950000. running mean: 32.379731\n",
      "ep 4375: ep_len:117 episode reward: total was 27.470000. running mean: 32.330634\n",
      "ep 4375: ep_len:3 episode reward: total was 1.010000. running mean: 32.017428\n",
      "ep 4375: ep_len:524 episode reward: total was 45.810000. running mean: 32.155353\n",
      "ep 4375: ep_len:524 episode reward: total was 29.880000. running mean: 32.132600\n",
      "epsilon:0.009994 episode_count: 30632. steps_count: 13250745.000000\n",
      "Time elapsed:  40634.48108005524\n",
      "ep 4376: ep_len:619 episode reward: total was 15.230000. running mean: 31.963574\n",
      "ep 4376: ep_len:500 episode reward: total was 108.590000. running mean: 32.729838\n",
      "ep 4376: ep_len:572 episode reward: total was 19.860000. running mean: 32.601140\n",
      "ep 4376: ep_len:398 episode reward: total was 25.140000. running mean: 32.526528\n",
      "ep 4376: ep_len:101 episode reward: total was 34.280000. running mean: 32.544063\n",
      "ep 4376: ep_len:684 episode reward: total was 32.940000. running mean: 32.548022\n",
      "ep 4376: ep_len:299 episode reward: total was 3.790000. running mean: 32.260442\n",
      "epsilon:0.009994 episode_count: 30639. steps_count: 13253918.000000\n",
      "Time elapsed:  40642.82463598251\n",
      "ep 4377: ep_len:500 episode reward: total was 65.330000. running mean: 32.591138\n",
      "ep 4377: ep_len:500 episode reward: total was 16.430000. running mean: 32.429526\n",
      "ep 4377: ep_len:635 episode reward: total was 58.770000. running mean: 32.692931\n",
      "ep 4377: ep_len:500 episode reward: total was 57.420000. running mean: 32.940202\n",
      "ep 4377: ep_len:3 episode reward: total was 1.010000. running mean: 32.620900\n",
      "ep 4377: ep_len:536 episode reward: total was 19.400000. running mean: 32.488691\n",
      "ep 4377: ep_len:576 episode reward: total was 55.790000. running mean: 32.721704\n",
      "epsilon:0.009994 episode_count: 30646. steps_count: 13257168.000000\n",
      "Time elapsed:  40667.68313407898\n",
      "ep 4378: ep_len:573 episode reward: total was 10.060000. running mean: 32.495087\n",
      "ep 4378: ep_len:500 episode reward: total was 54.580000. running mean: 32.715936\n",
      "ep 4378: ep_len:577 episode reward: total was 65.760000. running mean: 33.046377\n",
      "ep 4378: ep_len:500 episode reward: total was 64.620000. running mean: 33.362113\n",
      "ep 4378: ep_len:57 episode reward: total was 17.120000. running mean: 33.199692\n",
      "ep 4378: ep_len:551 episode reward: total was 37.780000. running mean: 33.245495\n",
      "ep 4378: ep_len:618 episode reward: total was 11.310000. running mean: 33.026140\n",
      "epsilon:0.009994 episode_count: 30653. steps_count: 13260544.000000\n",
      "Time elapsed:  40683.28608059883\n",
      "ep 4379: ep_len:600 episode reward: total was 65.150000. running mean: 33.347379\n",
      "ep 4379: ep_len:500 episode reward: total was 76.080000. running mean: 33.774705\n",
      "ep 4379: ep_len:574 episode reward: total was 13.180000. running mean: 33.568758\n",
      "ep 4379: ep_len:566 episode reward: total was 77.750000. running mean: 34.010570\n",
      "ep 4379: ep_len:83 episode reward: total was 26.230000. running mean: 33.932764\n",
      "ep 4379: ep_len:504 episode reward: total was 31.640000. running mean: 33.909837\n",
      "ep 4379: ep_len:500 episode reward: total was 20.030000. running mean: 33.771038\n",
      "epsilon:0.009994 episode_count: 30660. steps_count: 13263871.000000\n",
      "Time elapsed:  40691.876910209656\n",
      "ep 4380: ep_len:185 episode reward: total was 27.550000. running mean: 33.708828\n",
      "ep 4380: ep_len:500 episode reward: total was 4.640000. running mean: 33.418140\n",
      "ep 4380: ep_len:500 episode reward: total was 71.360000. running mean: 33.797558\n",
      "ep 4380: ep_len:510 episode reward: total was -67.080000. running mean: 32.788783\n",
      "ep 4380: ep_len:3 episode reward: total was 1.010000. running mean: 32.470995\n",
      "ep 4380: ep_len:542 episode reward: total was 49.490000. running mean: 32.641185\n",
      "ep 4380: ep_len:631 episode reward: total was 53.530000. running mean: 32.850073\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.009994 episode_count: 30667. steps_count: 13266742.000000\n",
      "Time elapsed:  40704.53350234032\n",
      "ep 4381: ep_len:543 episode reward: total was 64.690000. running mean: 33.168472\n",
      "ep 4381: ep_len:504 episode reward: total was 126.990000. running mean: 34.106688\n",
      "ep 4381: ep_len:584 episode reward: total was -3.930000. running mean: 33.726321\n",
      "ep 4381: ep_len:500 episode reward: total was 65.350000. running mean: 34.042558\n",
      "ep 4381: ep_len:3 episode reward: total was 1.010000. running mean: 33.712232\n",
      "ep 4381: ep_len:589 episode reward: total was 57.740000. running mean: 33.952510\n",
      "ep 4381: ep_len:500 episode reward: total was 44.400000. running mean: 34.056985\n",
      "epsilon:0.009994 episode_count: 30674. steps_count: 13269965.000000\n",
      "Time elapsed:  40712.94924116135\n",
      "ep 4382: ep_len:259 episode reward: total was 21.910000. running mean: 33.935515\n",
      "ep 4382: ep_len:528 episode reward: total was -3.760000. running mean: 33.558560\n",
      "ep 4382: ep_len:636 episode reward: total was 19.820000. running mean: 33.421174\n",
      "ep 4382: ep_len:510 episode reward: total was 67.900000. running mean: 33.765962\n",
      "ep 4382: ep_len:89 episode reward: total was 22.740000. running mean: 33.655703\n",
      "ep 4382: ep_len:500 episode reward: total was 45.860000. running mean: 33.777746\n",
      "ep 4382: ep_len:500 episode reward: total was 17.850000. running mean: 33.618468\n",
      "epsilon:0.009994 episode_count: 30681. steps_count: 13272987.000000\n",
      "Time elapsed:  40720.89444208145\n",
      "ep 4383: ep_len:604 episode reward: total was 84.050000. running mean: 34.122783\n",
      "ep 4383: ep_len:500 episode reward: total was 17.330000. running mean: 33.954856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4383: ep_len:500 episode reward: total was 59.450000. running mean: 34.209807\n",
      "ep 4383: ep_len:535 episode reward: total was 86.040000. running mean: 34.728109\n",
      "ep 4383: ep_len:3 episode reward: total was 1.010000. running mean: 34.390928\n",
      "ep 4383: ep_len:186 episode reward: total was 34.280000. running mean: 34.389819\n",
      "ep 4383: ep_len:502 episode reward: total was 32.750000. running mean: 34.373420\n",
      "epsilon:0.009994 episode_count: 30688. steps_count: 13275817.000000\n",
      "Time elapsed:  40735.322431087494\n",
      "ep 4384: ep_len:127 episode reward: total was 2.370000. running mean: 34.053386\n",
      "ep 4384: ep_len:526 episode reward: total was 102.870000. running mean: 34.741552\n",
      "ep 4384: ep_len:519 episode reward: total was 66.090000. running mean: 35.055037\n",
      "ep 4384: ep_len:608 episode reward: total was 77.230000. running mean: 35.476786\n",
      "ep 4384: ep_len:3 episode reward: total was 1.010000. running mean: 35.132119\n",
      "ep 4384: ep_len:527 episode reward: total was 7.830000. running mean: 34.859097\n",
      "ep 4384: ep_len:500 episode reward: total was 77.510000. running mean: 35.285606\n",
      "epsilon:0.009994 episode_count: 30695. steps_count: 13278627.000000\n",
      "Time elapsed:  40743.07188510895\n",
      "ep 4385: ep_len:537 episode reward: total was -8.920000. running mean: 34.843550\n",
      "ep 4385: ep_len:583 episode reward: total was 55.630000. running mean: 35.051415\n",
      "ep 4385: ep_len:578 episode reward: total was -9.160000. running mean: 34.609301\n",
      "ep 4385: ep_len:500 episode reward: total was 85.340000. running mean: 35.116608\n",
      "ep 4385: ep_len:3 episode reward: total was 1.010000. running mean: 34.775542\n",
      "ep 4385: ep_len:579 episode reward: total was 33.150000. running mean: 34.759286\n",
      "ep 4385: ep_len:344 episode reward: total was 29.290000. running mean: 34.704593\n",
      "epsilon:0.009994 episode_count: 30702. steps_count: 13281751.000000\n",
      "Time elapsed:  40764.48305749893\n",
      "ep 4386: ep_len:517 episode reward: total was 85.740000. running mean: 35.214947\n",
      "ep 4386: ep_len:587 episode reward: total was 67.540000. running mean: 35.538198\n",
      "ep 4386: ep_len:562 episode reward: total was 67.910000. running mean: 35.861916\n",
      "ep 4386: ep_len:525 episode reward: total was 92.140000. running mean: 36.424697\n",
      "ep 4386: ep_len:98 episode reward: total was 33.240000. running mean: 36.392850\n",
      "ep 4386: ep_len:500 episode reward: total was 55.730000. running mean: 36.586221\n",
      "ep 4386: ep_len:283 episode reward: total was 37.970000. running mean: 36.600059\n",
      "epsilon:0.009994 episode_count: 30709. steps_count: 13284823.000000\n",
      "Time elapsed:  40769.25628089905\n",
      "ep 4387: ep_len:500 episode reward: total was -143.650000. running mean: 34.797559\n",
      "ep 4387: ep_len:500 episode reward: total was 49.290000. running mean: 34.942483\n",
      "ep 4387: ep_len:575 episode reward: total was 36.150000. running mean: 34.954558\n",
      "ep 4387: ep_len:553 episode reward: total was 47.250000. running mean: 35.077513\n",
      "ep 4387: ep_len:38 episode reward: total was 17.500000. running mean: 34.901737\n",
      "ep 4387: ep_len:605 episode reward: total was 49.080000. running mean: 35.043520\n",
      "ep 4387: ep_len:336 episode reward: total was 47.240000. running mean: 35.165485\n",
      "epsilon:0.009994 episode_count: 30716. steps_count: 13287930.000000\n",
      "Time elapsed:  40774.079402685165\n",
      "ep 4388: ep_len:576 episode reward: total was 62.820000. running mean: 35.442030\n",
      "ep 4388: ep_len:565 episode reward: total was 49.320000. running mean: 35.580810\n",
      "ep 4388: ep_len:530 episode reward: total was 17.840000. running mean: 35.403402\n",
      "ep 4388: ep_len:555 episode reward: total was 90.250000. running mean: 35.951868\n",
      "ep 4388: ep_len:51 episode reward: total was 24.000000. running mean: 35.832349\n",
      "ep 4388: ep_len:552 episode reward: total was 5.060000. running mean: 35.524625\n",
      "ep 4388: ep_len:606 episode reward: total was -65.210000. running mean: 34.517279\n",
      "epsilon:0.009994 episode_count: 30723. steps_count: 13291365.000000\n",
      "Time elapsed:  40779.30662941933\n",
      "ep 4389: ep_len:579 episode reward: total was 81.820000. running mean: 34.990306\n",
      "ep 4389: ep_len:514 episode reward: total was 44.760000. running mean: 35.088003\n",
      "ep 4389: ep_len:560 episode reward: total was 19.090000. running mean: 34.928023\n",
      "ep 4389: ep_len:500 episode reward: total was 28.570000. running mean: 34.864443\n",
      "ep 4389: ep_len:51 episode reward: total was 22.010000. running mean: 34.735899\n",
      "ep 4389: ep_len:564 episode reward: total was 19.440000. running mean: 34.582940\n",
      "ep 4389: ep_len:502 episode reward: total was 19.850000. running mean: 34.435610\n",
      "epsilon:0.009994 episode_count: 30730. steps_count: 13294635.000000\n",
      "Time elapsed:  40786.123757600784\n",
      "ep 4390: ep_len:591 episode reward: total was 55.410000. running mean: 34.645354\n",
      "ep 4390: ep_len:539 episode reward: total was 64.450000. running mean: 34.943401\n",
      "ep 4390: ep_len:555 episode reward: total was -8.170000. running mean: 34.512267\n",
      "ep 4390: ep_len:593 episode reward: total was 94.620000. running mean: 35.113344\n",
      "ep 4390: ep_len:3 episode reward: total was 1.010000. running mean: 34.772310\n",
      "ep 4390: ep_len:636 episode reward: total was 63.620000. running mean: 35.060787\n",
      "ep 4390: ep_len:523 episode reward: total was 25.640000. running mean: 34.966580\n",
      "epsilon:0.009994 episode_count: 30737. steps_count: 13298075.000000\n",
      "Time elapsed:  40811.95069384575\n",
      "ep 4391: ep_len:528 episode reward: total was 63.890000. running mean: 35.255814\n",
      "ep 4391: ep_len:649 episode reward: total was 112.130000. running mean: 36.024556\n",
      "ep 4391: ep_len:624 episode reward: total was 0.200000. running mean: 35.666310\n",
      "ep 4391: ep_len:132 episode reward: total was 24.590000. running mean: 35.555547\n",
      "ep 4391: ep_len:108 episode reward: total was 36.770000. running mean: 35.567691\n",
      "ep 4391: ep_len:500 episode reward: total was -111.300000. running mean: 34.099015\n",
      "ep 4391: ep_len:297 episode reward: total was 46.590000. running mean: 34.223924\n",
      "epsilon:0.009994 episode_count: 30744. steps_count: 13300913.000000\n",
      "Time elapsed:  40819.40772938728\n",
      "ep 4392: ep_len:615 episode reward: total was 57.360000. running mean: 34.455285\n",
      "ep 4392: ep_len:514 episode reward: total was 35.680000. running mean: 34.467532\n",
      "ep 4392: ep_len:581 episode reward: total was 10.710000. running mean: 34.229957\n",
      "ep 4392: ep_len:500 episode reward: total was 105.930000. running mean: 34.946957\n",
      "ep 4392: ep_len:3 episode reward: total was 1.010000. running mean: 34.607588\n",
      "ep 4392: ep_len:602 episode reward: total was 20.800000. running mean: 34.469512\n",
      "ep 4392: ep_len:500 episode reward: total was 64.930000. running mean: 34.774117\n",
      "epsilon:0.009994 episode_count: 30751. steps_count: 13304228.000000\n",
      "Time elapsed:  40836.42373776436\n",
      "ep 4393: ep_len:630 episode reward: total was 41.080000. running mean: 34.837176\n",
      "ep 4393: ep_len:540 episode reward: total was -2.600000. running mean: 34.462804\n",
      "ep 4393: ep_len:546 episode reward: total was 35.200000. running mean: 34.470176\n",
      "ep 4393: ep_len:500 episode reward: total was 39.020000. running mean: 34.515674\n",
      "ep 4393: ep_len:3 episode reward: total was 0.000000. running mean: 34.170517\n",
      "ep 4393: ep_len:627 episode reward: total was -29.610000. running mean: 33.532712\n",
      "ep 4393: ep_len:541 episode reward: total was 40.540000. running mean: 33.602785\n",
      "epsilon:0.009994 episode_count: 30758. steps_count: 13307615.000000\n",
      "Time elapsed:  40845.79998612404\n",
      "ep 4394: ep_len:500 episode reward: total was -26.870000. running mean: 32.998057\n",
      "ep 4394: ep_len:588 episode reward: total was 42.170000. running mean: 33.089777\n",
      "ep 4394: ep_len:521 episode reward: total was 15.260000. running mean: 32.911479\n",
      "ep 4394: ep_len:151 episode reward: total was 17.170000. running mean: 32.754064\n",
      "ep 4394: ep_len:3 episode reward: total was 1.010000. running mean: 32.436623\n",
      "ep 4394: ep_len:515 episode reward: total was 23.610000. running mean: 32.348357\n",
      "ep 4394: ep_len:621 episode reward: total was 34.390000. running mean: 32.368774\n",
      "epsilon:0.009994 episode_count: 30765. steps_count: 13310514.000000\n",
      "Time elapsed:  40850.883373737335\n",
      "ep 4395: ep_len:578 episode reward: total was 84.600000. running mean: 32.891086\n",
      "ep 4395: ep_len:613 episode reward: total was 36.220000. running mean: 32.924375\n",
      "ep 4395: ep_len:79 episode reward: total was 11.340000. running mean: 32.708531\n",
      "ep 4395: ep_len:546 episode reward: total was 101.750000. running mean: 33.398946\n",
      "ep 4395: ep_len:3 episode reward: total was 1.010000. running mean: 33.075057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 4395: ep_len:500 episode reward: total was 43.110000. running mean: 33.175406\n",
      "ep 4395: ep_len:500 episode reward: total was 65.850000. running mean: 33.502152\n",
      "epsilon:0.009994 episode_count: 30772. steps_count: 13313333.000000\n",
      "Time elapsed:  40857.97808289528\n",
      "ep 4396: ep_len:524 episode reward: total was 2.430000. running mean: 33.191430\n",
      "ep 4396: ep_len:500 episode reward: total was 66.210000. running mean: 33.521616\n",
      "ep 4396: ep_len:552 episode reward: total was 29.840000. running mean: 33.484800\n",
      "ep 4396: ep_len:500 episode reward: total was 43.520000. running mean: 33.585152\n",
      "ep 4396: ep_len:3 episode reward: total was 1.010000. running mean: 33.259400\n",
      "ep 4396: ep_len:500 episode reward: total was -17.350000. running mean: 32.753306\n",
      "ep 4396: ep_len:560 episode reward: total was 68.340000. running mean: 33.109173\n",
      "epsilon:0.009994 episode_count: 30779. steps_count: 13316472.000000\n",
      "Time elapsed:  40879.467562675476\n",
      "ep 4397: ep_len:627 episode reward: total was 47.920000. running mean: 33.257282\n",
      "ep 4397: ep_len:145 episode reward: total was -28.540000. running mean: 32.639309\n",
      "ep 4397: ep_len:500 episode reward: total was 30.740000. running mean: 32.620316\n",
      "ep 4397: ep_len:538 episode reward: total was 90.170000. running mean: 33.195813\n",
      "ep 4397: ep_len:3 episode reward: total was 1.010000. running mean: 32.873954\n",
      "ep 4397: ep_len:241 episode reward: total was 46.540000. running mean: 33.010615\n",
      "ep 4397: ep_len:500 episode reward: total was 6.460000. running mean: 32.745109\n",
      "epsilon:0.009994 episode_count: 30786. steps_count: 13319026.000000\n",
      "Time elapsed:  40892.99726176262\n",
      "ep 4398: ep_len:500 episode reward: total was 60.060000. running mean: 33.018258\n",
      "ep 4398: ep_len:583 episode reward: total was 145.950000. running mean: 34.147575\n",
      "ep 4398: ep_len:500 episode reward: total was -0.980000. running mean: 33.796299\n",
      "ep 4398: ep_len:76 episode reward: total was -1.610000. running mean: 33.442236\n",
      "ep 4398: ep_len:3 episode reward: total was 1.010000. running mean: 33.117914\n",
      "ep 4398: ep_len:521 episode reward: total was -74.490000. running mean: 32.041835\n",
      "ep 4398: ep_len:510 episode reward: total was 21.800000. running mean: 31.939416\n",
      "epsilon:0.009994 episode_count: 30793. steps_count: 13321719.000000\n",
      "Time elapsed:  40907.14561891556\n",
      "ep 4399: ep_len:586 episode reward: total was 95.670000. running mean: 32.576722\n",
      "ep 4399: ep_len:500 episode reward: total was 54.720000. running mean: 32.798155\n",
      "ep 4399: ep_len:500 episode reward: total was 46.810000. running mean: 32.938274\n",
      "ep 4399: ep_len:500 episode reward: total was 0.740000. running mean: 32.616291\n",
      "ep 4399: ep_len:56 episode reward: total was 27.510000. running mean: 32.565228\n",
      "ep 4399: ep_len:517 episode reward: total was -32.280000. running mean: 31.916776\n",
      "ep 4399: ep_len:504 episode reward: total was 25.620000. running mean: 31.853808\n",
      "epsilon:0.009994 episode_count: 30800. steps_count: 13324882.000000\n",
      "Time elapsed:  40915.341210603714\n",
      "ep 4400: ep_len:554 episode reward: total was 58.030000. running mean: 32.115570\n",
      "ep 4400: ep_len:500 episode reward: total was 10.440000. running mean: 31.898814\n",
      "ep 4400: ep_len:568 episode reward: total was -0.020000. running mean: 31.579626\n",
      "ep 4400: ep_len:53 episode reward: total was -4.670000. running mean: 31.217130\n",
      "ep 4400: ep_len:3 episode reward: total was 1.010000. running mean: 30.915058\n",
      "ep 4400: ep_len:516 episode reward: total was 50.360000. running mean: 31.109508\n",
      "ep 4400: ep_len:539 episode reward: total was 2.570000. running mean: 30.824113\n",
      "Initial position:  [  2   0  17  87  63 149]\n",
      "epsilon:0.009994 episode_count: 30807. steps_count: 13327615.000000\n",
      "Time elapsed:  40927.65555071831\n",
      "ep 4401: ep_len:122 episode reward: total was 4.800000. running mean: 30.563872\n",
      "ep 4401: ep_len:754 episode reward: total was -109.810000. running mean: 29.160133\n",
      "ep 4401: ep_len:557 episode reward: total was 4.340000. running mean: 28.911932\n",
      "ep 4401: ep_len:520 episode reward: total was 88.950000. running mean: 29.512312\n",
      "ep 4401: ep_len:3 episode reward: total was 1.010000. running mean: 29.227289\n",
      "ep 4401: ep_len:670 episode reward: total was 38.560000. running mean: 29.320616\n",
      "ep 4401: ep_len:558 episode reward: total was 11.690000. running mean: 29.144310\n",
      "epsilon:0.009994 episode_count: 30814. steps_count: 13330799.000000\n",
      "Time elapsed:  40943.65791153908\n",
      "ep 4402: ep_len:540 episode reward: total was 47.570000. running mean: 29.328567\n",
      "ep 4402: ep_len:505 episode reward: total was -26.850000. running mean: 28.766781\n",
      "ep 4402: ep_len:585 episode reward: total was -12.750000. running mean: 28.351613\n",
      "ep 4402: ep_len:506 episode reward: total was 41.620000. running mean: 28.484297\n",
      "ep 4402: ep_len:3 episode reward: total was 1.010000. running mean: 28.209554\n",
      "ep 4402: ep_len:554 episode reward: total was 28.160000. running mean: 28.209059\n",
      "ep 4402: ep_len:326 episode reward: total was 27.520000. running mean: 28.202168\n",
      "epsilon:0.009994 episode_count: 30821. steps_count: 13333818.000000\n",
      "Time elapsed:  40958.50068068504\n",
      "ep 4403: ep_len:221 episode reward: total was 19.130000. running mean: 28.111447\n",
      "ep 4403: ep_len:502 episode reward: total was 100.630000. running mean: 28.836632\n",
      "ep 4403: ep_len:563 episode reward: total was 55.390000. running mean: 29.102166\n",
      "ep 4403: ep_len:153 episode reward: total was 0.910000. running mean: 28.820244\n",
      "ep 4403: ep_len:3 episode reward: total was -1.500000. running mean: 28.517042\n",
      "ep 4403: ep_len:500 episode reward: total was 49.330000. running mean: 28.725171\n",
      "ep 4403: ep_len:500 episode reward: total was -19.050000. running mean: 28.247420\n",
      "epsilon:0.009994 episode_count: 30828. steps_count: 13336260.000000\n",
      "Time elapsed:  40965.17843604088\n",
      "ep 4404: ep_len:265 episode reward: total was 27.910000. running mean: 28.244045\n",
      "ep 4404: ep_len:525 episode reward: total was 135.230000. running mean: 29.313905\n",
      "ep 4404: ep_len:427 episode reward: total was 50.810000. running mean: 29.528866\n",
      "ep 4404: ep_len:572 episode reward: total was 104.900000. running mean: 30.282577\n",
      "ep 4404: ep_len:3 episode reward: total was 1.010000. running mean: 29.989851\n",
      "ep 4404: ep_len:583 episode reward: total was 22.940000. running mean: 29.919353\n",
      "ep 4404: ep_len:545 episode reward: total was 61.870000. running mean: 30.238859\n",
      "epsilon:0.009994 episode_count: 30835. steps_count: 13339180.000000\n",
      "Time elapsed:  40972.86429333687\n",
      "ep 4405: ep_len:528 episode reward: total was -4.860000. running mean: 29.887871\n",
      "ep 4405: ep_len:500 episode reward: total was 37.720000. running mean: 29.966192\n",
      "ep 4405: ep_len:500 episode reward: total was 37.070000. running mean: 30.037230\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6a0ea2feeca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# make next_state vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0;31m#print (x_t.size(),h_t.size(),enc_history.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mnext_state_xt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_tau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-020464c72e25>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#embedded = [src len, batch size, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#outputs = [src len, batch size, hid dim * n directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vedantb/miniconda3/envs/anils/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 522\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_ep_len = 500\n",
    "seq_len = 20\n",
    "h_len = seq_len\n",
    "\n",
    "while epoch < numEpoch:\n",
    "#for epoch in range(numEpoch):\n",
    "    # repeat for all pedestrians\n",
    "    #disp(pALL)\n",
    "    \n",
    "    for p in range(pALL.shape[0]): #range(pInLoop.shape[0])\n",
    "        \n",
    "        policy_net.train()\n",
    "        \n",
    "        # load p'th person data\n",
    "        ped = np.copy(pALL[p])\n",
    "\n",
    "        # camera index and frame index starts from zero\n",
    "        ped[:,0] -= 1\n",
    "        ped[:,1] -= 1\n",
    "        #print (np.unique(ped[:,0]))\n",
    "        \n",
    "        # check if camera number is correct\n",
    "        if (ped[:,0] >= num_camera).any():\n",
    "            print ('Error in person ', p)\n",
    "            break\n",
    "            \n",
    "        if ped.shape[0] < max_ep_len/4:\n",
    "            continue\n",
    "        \n",
    "        # select a camera uniformly\n",
    "        uniq_cam = np.unique(ped[:,0])\n",
    "        if len(uniq_cam) < 2 and np.random.rand() > 0.4:\n",
    "            if len(np.where(ped[1:,1]-ped[0:-1,1] != 1)[0]) == 0:\n",
    "                continue\n",
    "        rand_cam = uniq_cam[np.random.randint(len(uniq_cam))]\n",
    "        index_of_rand_cam = np.nonzero( ped[:,0]==rand_cam )[0]\n",
    "        len_indices_rand_cam = len(index_of_rand_cam)\n",
    "        \n",
    "        # Initialize with current state with start frame\n",
    "        tranIDX = np.where(ped[1:,0]-ped[0:-1,0])[0]\n",
    "        if len(uniq_cam) < 2 or np.random.rand() < 0.4:\n",
    "            startIDX = np.random.randint( 0,int(ped.shape[0]-ped.shape[0]/2) )\n",
    "        else:\n",
    "            startIDX = np.random.choice(tranIDX)-20\n",
    "        #startIDX = np.random.choice(tranIDX)-20 if np.random.rand(1) < 0.6 else np.random.randint( 0,ped.shape[0]-max_ep_len/2 )\n",
    "        #startIDX = index_of_rand_cam[np.random.randint(len_indices_rand_cam/10)]\n",
    "        myPos = ped[startIDX,0:]\n",
    "        #print (myPos)\n",
    "        \n",
    "        curr_camera = myPos[0]\n",
    "        curr_frame = myPos[1]\n",
    "        \n",
    "        # Initialize history variable (one-hot encoding)\n",
    "        ch = np.zeros((h_len,duke_cam))\n",
    "        prev_ch = ch\n",
    "        \n",
    "        # initialize total time target was occluded\n",
    "        num_steps = 0\n",
    "        occ_len = 0.01\n",
    "        hcount = np.array(10*np.log(occ_len))\n",
    "        CDataEp = []\n",
    "        inCDataEp = []\n",
    "        EpData = []\n",
    "        episodic_seq = []\n",
    "        \n",
    "        # create initial state (ct,rt,tau_t)\n",
    "        #bbox = myPos[2:]\n",
    "        #rt = afc.find_curr_rt(bbox)\n",
    "        x_t,c_t,te_tau,r_t = make_state_vector(ped, curr_camera,curr_frame,ch,occ_len)\n",
    "        prev_rt = r_t[0:4]\n",
    "        stCam = curr_camera\n",
    "        expStC = curr_camera\n",
    "        count_curr_c = 0\n",
    "        prev_camera = curr_camera\n",
    "\n",
    "        if render: # show current location\n",
    "            plt.imshow(x.reshape(input_size))\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "        episode_count += 1\n",
    "        if epsilon > finalEpsilon:\n",
    "            epsilon -= (initialEpsilon - finalEpsilon)/30000\n",
    "        \n",
    "        # select an action from the current state\n",
    "        hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "        #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "        state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "        #state = torch.cat([state_xt, hidden.detach().flatten().view(1,-1)], dim=1)\n",
    "        state = torch.cat([state_xt, hidden[1,].detach()], dim=1)\n",
    "        #print ('State size: ', state.size())\n",
    "        \n",
    "        while(curr_frame <= ped[-1,1]):\n",
    "        \n",
    "            state_in = Variable(state)\n",
    "            value_c = policy_net(state_in)\n",
    "\n",
    "            steps_count += 1\n",
    "            \n",
    "            # generate random steps\n",
    "            if np.random.rand(1) < 0.01:\n",
    "                rsteps = np.random.randint(fpsc,20,1)\n",
    "            else:\n",
    "                rsteps = 1\n",
    "            curr_frame += rsteps*fpsc if rsteps > 1 else fpsc\n",
    "            num_steps += 1\n",
    "                \n",
    "            # initialize action\n",
    "            one_hot_action = torch.zeros([num_camera], dtype=torch.float32)\n",
    "            if use_cuda:  # put on GPU if CUDA is available\n",
    "                one_hot_action = one_hot_action.cuda()\n",
    "\n",
    "            # epsilon greedy exploration\n",
    "            random_action = np.random.random() <= epsilon\n",
    "            camera_index = [torch.randint(num_camera, torch.Size([]), dtype=torch.int)\n",
    "                           if random_action else torch.argmax(value_c)][0]\n",
    "            \n",
    "            if use_cuda:  # put on GPU if CUDA is available\n",
    "                camera_index = camera_index.cuda()\n",
    "\n",
    "            one_hot_action[camera_index] = 1\n",
    "            one_hot_action = one_hot_action.unsqueeze(0)\n",
    "            c = camera_index.detach().cpu().numpy()\n",
    "            \n",
    "            # Store the transition explored\n",
    "            #M[stCam,c] += 1\n",
    "            \n",
    "            # get the current bounding box\n",
    "            bbox = ped[ np.logical_and(ped[:,0]==c,ped[:,1]==curr_frame),2:]\n",
    "            if bbox.shape[0] > 0:\n",
    "                #rt = afc.find_curr_rt(bbox[0]) \n",
    "                bbox = bbox[0]\n",
    "                rt = np.zeros((8))\n",
    "                rt[0] = bbox[0]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[1] = bbox[1]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[2] = bbox[2]/320 -(np.random.rand()-0.5)/100\n",
    "                rt[3] = bbox[3]/240 -(np.random.rand()-0.5)/100\n",
    "                rt[4] = rt[0] - prev_rt[0] if occ_len < 0.2 else 0\n",
    "                rt[5] = rt[1] - prev_rt[1] if occ_len < 0.2 else 0\n",
    "                rt[6] = rt[2] - prev_rt[2] if occ_len < 0.2 else 0\n",
    "                rt[7] = rt[3] - prev_rt[3] if occ_len < 0.2 else 0\n",
    "                \n",
    "                curr_camera = c\n",
    "                # make next_state vector\n",
    "                this_cam = afc.make_one_hot_camera(curr_camera)\n",
    "                x_t = np.concatenate((this_cam, rt.ravel()))\n",
    "                x_t[x_t==0] = -10\n",
    "                x_t[x_t==1] = 10\n",
    "                x_t = x_t.reshape(1,-1)\n",
    "                if use_cuda:\n",
    "                    x_t = torch.from_numpy(x_t).float().cuda()\n",
    "                \n",
    "                #num_steps = 0\n",
    "                ispresent = 1\n",
    "                stCam = c\n",
    "                \n",
    "                prev_rt = rt[0:4]\n",
    "            else:\n",
    "                ispresent = 0\n",
    "            \n",
    "            #if ispresent and expStC != num_camera-1 and c != num_camera-1:\n",
    "            #    trExplored[str(expStC)+'-'+str(c)].append(occ_len)\n",
    "            #    expStC = c\n",
    "            \n",
    "            # get correct label from ground truth\n",
    "            y = afc.find_target_camera(ped, curr_frame)\n",
    "            # get reward (give reward at end of episode)\n",
    "            if y == num_camera-1 and y == c:\n",
    "                reward = 0.01\n",
    "            elif y == c and occ_len< 20:\n",
    "                reward = 0.5\n",
    "                wt = 1\n",
    "            elif y == c:\n",
    "                reward = 1\n",
    "                wt = 10\n",
    "            else:\n",
    "                reward = -1\n",
    "            reward_sum += reward\n",
    "            rs.append(reward)\n",
    "            EpData.append((list(value_c.detach().cpu().numpy()[0]),hcount.ravel()[0],reward,random_action,y,c,episode_reward))\n",
    "            \n",
    "            #print (np.array([rt, ispresent,c]))\n",
    "            ######################## prepare the next state  #############################\n",
    "            # count the time of prev_camera selection\n",
    "            if ispresent:\n",
    "                occ_len = 0.01\n",
    "            else:\n",
    "                occ_len += rsteps\n",
    "            #hcount = np.array(-occ_max_val + (occ_len/500)*(occ_max_val-(-occ_max_val)))\n",
    "            hcount = np.array(10*np.log(occ_len))\n",
    "            \n",
    "            # get next camera using policy network\n",
    "            this_cam = afc.make_one_hot_camera(c)\n",
    "            c_t = this_cam.reshape(1,-1)\n",
    "            \n",
    "            # update current state and history\n",
    "            prev_ch = ch\n",
    "            ch[1:,] = ch[0:-1,]\n",
    "            ch[0,0:num_camera] = afc.make_one_hot_camera(c)\n",
    "            ch[0,num_camera:] = 0\n",
    "            \n",
    "            #chCuda = torch.from_numpy(ch).float().cuda().unsqueeze(1)\n",
    "            #outSeq = model(chCuda, chCuda)\n",
    "            #print ('Encoding .. ', chCuda[:,0,].argmax(1))\n",
    "            #print ('Decoded ..', outSeq[:,0].argmax(1))\n",
    "            #print ('')\n",
    "            \n",
    "            if use_cuda:\n",
    "                c_t = torch.from_numpy(c_t).float().cuda()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float().cuda()\n",
    "            else:\n",
    "                c_t = torch.from_numpy(c_t).float()\n",
    "                te_tau = torch.from_numpy(hcount.reshape(1,-1)).float()\n",
    "            episodic_seq.append((c_t))\n",
    "            \n",
    "            # make next_state vector\n",
    "            hidden, cell = enc(torch.from_numpy(ch).float().cuda().unsqueeze(1))\n",
    "            #print (x_t.size(),h_t.size(),enc_history.size())\n",
    "            next_state_xt = torch.cat([x_t, te_tau], dim=1)\n",
    "            #next_state = torch.cat([next_state_xt, hidden.detach().flatten().view(1,-1)], dim=1)\n",
    "            next_state = torch.cat([next_state_xt, hidden[1,].detach()], dim=1)\n",
    "            \n",
    "            if use_cuda:\n",
    "                reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).cuda()\n",
    "            else:\n",
    "                reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)\n",
    "            \n",
    "            #replay_memory.append((state_xt,prev_ch, one_hot_action, reward, next_state_xt,ch, ispresent))\n",
    "            \n",
    "            # save transition to replay memory\n",
    "            if reward > 0.2:\n",
    "                #replay_memory_pos.append((state_xt,prev_ch, one_hot_action, reward, next_state_xt,ch, ispresent))\n",
    "                replay_memory_pos.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "                pos_prob.append(wt)\n",
    "            elif reward == 0.01:\n",
    "                #replay_memory_cx.append((state_xt,prev_ch, one_hot_action, reward, next_state_xt,ch, ispresent))\n",
    "                replay_memory_cx.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "            else:\n",
    "                #replay_memory_neg.append((state_xt,prev_ch, one_hot_action, reward, next_state_xt,ch, ispresent))\n",
    "                replay_memory_neg.append((state, one_hot_action, reward, next_state, ispresent))\n",
    "            \n",
    "            # if replay memory is full, remove the oldest transition\n",
    "            if len(replay_memory_pos) > replay_memory_size:\n",
    "                replay_memory_pos.pop(0)\n",
    "                pos_prob.pop(0)\n",
    "            if len(replay_memory_neg) > replay_memory_size:\n",
    "                replay_memory_neg.pop(0)\n",
    "            if len(replay_memory_cx) > replay_memory_size:\n",
    "                replay_memory_cx.pop(0)\n",
    "            \n",
    "            #state_xt = next_state_xt\n",
    "            state = next_state #torch.cat([state_xt, hidden.detach()], dim=1)\n",
    "            #state = torch.cat([next_state_xt, enc_history], dim=1)\n",
    "            prev_camera = c\n",
    "            \n",
    "            if num_steps >= max_ep_len and y!=num_camera-1 and y==c:  # break the episode\n",
    "                #print ('')\n",
    "                #print (epoch, p, random_action, rsteps)\n",
    "                #print ('x_t: ', c,rt)\n",
    "                ##print ( np.where(ch)[1])\n",
    "                #print ('Q values: ', value_c)\n",
    "                #print (y,c, curr_frame,ped[-1,1], num_steps, hcount)\n",
    "                #print ('isPresent', ispresent)\n",
    "                #print ('Pos Replay length: ', len(replay_memory_pos))\n",
    "                \n",
    "                #print (pos_prob[:])\n",
    "                break\n",
    "        \n",
    "        # update value_function\n",
    "        loss,rrr,update_criteria = backward_network(replay_memory_pos, pos_prob[:], replay_memory_neg,replay_memory_cx, update_criteria)\n",
    "        numUpdateRew.append(rrr)\n",
    "        if np.random.rand() < 0.05:\n",
    "            allEpData.append((np.stack(EpData)))\n",
    "        \n",
    "        # store episodic reward\n",
    "        #numRew.append((sum(np.stack(rs)==100),sum(np.stack(rs)==-100),sum(np.stack(rs)==0.1)))\n",
    "        rs = append_reward(rs,num_steps)\n",
    "        \n",
    "        # boring book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print ('ep %d: ep_len:%d episode reward: total was %f. running mean: %f' % (epoch, num_steps, reward_sum, running_reward))\n",
    "        reward_sum = 0\n",
    "        num_steps = 0\n",
    "        rs = []\n",
    "    \n",
    "    if epoch % 20 == 0: # test on validation set\n",
    "        _,accV,qv,numTR = test_func(pTest[1:2],iloc='first',eloc='last')\n",
    "        av = np.stack(accV[0])\n",
    "        av = sum(av[av[:,0]!=(num_camera-1),0] == av[av[:,0]!=(num_camera-1),1])/sum(av[:,0]!=(num_camera-1))\n",
    "        validation_reward.append((qv,av,numTR)) \n",
    "    #print (M)\n",
    "    epoch += 1\n",
    "    print ('epsilon:%f episode_count: %d. steps_count: %f' % (epsilon, episode_count,steps_count))\n",
    "    if epoch % 50 == 1: \n",
    "        torch.save({'state_dict': policy_net.state_dict()}, './models/policy_ECCV_db3_pretrDukeAE256_seq20_rp20K_'+str(epoch))\n",
    "    #if epoch %200 == 100:\n",
    "        #np.save('./EpData/allEpData_ECCV_db3_pretrAE64_seq20_rp20K_'+str(epoch),np.array(allEpData),allow_pickle=True)\n",
    "        #np.save('./EpData/episode_reward_ECCV_db3_pretrAE64_seq20_rp20K_'+str(epoch), (episode_reward,validation_reward,epsilon,episode_count, steps_count,running_reward))\n",
    "    allEpData = []\n",
    "    print ('Time elapsed: ', tt.toc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030866820999985133"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time when stopped at 1443 = 52695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chCuda = torch.from_numpy(ch).float().cuda().unsqueeze(1)\n",
    "outSeq = model(chCuda, chCuda)\n",
    "print ('Encoding .. ', chCuda[:,0,].argmax(1))\n",
    "print ('Decoded ..', outSeq[:,0].argmax(1))\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-300, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reward received during training\n",
    "rpR = np.vstack(episode_reward)\n",
    "from scipy.signal import savgol_filter\n",
    "yhat = savgol_filter(rpR[:,2], 361, 2) # window size 51, polynomial order 3\n",
    "#plt.plot(rpR[:,4])\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(yhat)\n",
    "plt.ylim([-300,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vvr = np.stack(validation_reward)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(vvr[:,1])\n",
    "plt.ylabel('Validation Acc (non-Cx)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of Q values last 10 episodes\n",
    "epn = -1\n",
    "mmat = allEpData\n",
    "episQ = np.stack(mmat[epn][:,0])[:,]\n",
    "episY = mmat[epn][:,4]*10+10\n",
    "episC = mmat[epn][:,5]*10+10\n",
    "episTau = mmat[epn][:,1]\n",
    "# print (episY)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(episY, label='GT')\n",
    "ax1.plot(episTau, label='Tau', color='black')\n",
    "#ax1.plot(episC+1, label='pi(s)', color='cyan')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.plot(episQ[:,0], label='A1')\n",
    "ax2.plot(episQ[:,1], label='A2')\n",
    "ax2.plot(episQ[:,2], label='A3')\n",
    "ax2.plot(episQ[:,3], label='A4')\n",
    "ax2.plot(episQ[:,4], label='A5')\n",
    "ax2.plot(episQ[:,5], label='Ax')\n",
    "ax2.set_ylim([-4,5])\n",
    "ax2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(arr): \n",
    "    final_list = [] \n",
    "    gt_tr = []\n",
    "    final_list.append(arr[0])     \n",
    "    for i in range(1,arr.shape[0]): \n",
    "        if arr[i] != arr[i-1]:\n",
    "            final_list.append(arr[i])     \n",
    "            if arr[i] != num_camera-1:\n",
    "                gt_tr.append(arr[i])\n",
    "    return final_list, gt_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#policy_net = torch.load('./EpData/policy_db4_3rep_del_rareFreq_ep5k_1201')\n",
    "policy_net.load_state_dict(torch.load('./models/policy_ECCV_db3_pretrDukeAE256_seq20_rp20K_901')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial position:  [  2   2 200  85  64 136]\n",
      "Initial position:  [  2   2  17  89  65 146]\n",
      "Initial position:  [   3 1880  190  106   54  133]\n",
      "Initial position:  [   1 1448  120   96   27   81]\n",
      "Initial position:  [   2 2236  219   80   52  133]\n",
      "Initial position:  [  3 630 114  82  82 156]\n",
      "Initial position:  [  2  68   1  94  37 131]\n",
      "Person:  0\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 2, 4, 3, 4]\n",
      "GT transitions:  23\n",
      "Transitions captured:  19\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.7194552529182879 0.6882793017456359 0.6742671009771987\n",
      "Num frames:  (1203, 321)\n",
      "Accuracy:  0.7194552529182879\n",
      "Person:  1\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4]\n",
      "GT transitions:  26\n",
      "Transitions captured:  19\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.7809673613841919 0.7255892255892256 0.762157382847038\n",
      "Num frames:  (1188, 288)\n",
      "Accuracy:  0.7809673613841919\n",
      "Person:  2\n",
      "Transitions:  [3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 3, 4, 2, 4, 1, 4, 1, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4]\n",
      "GT transitions:  19\n",
      "Transitions captured:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.4543297746144721 0.23745173745173745 0.41624365482233505\n",
      "Num frames:  (1036, 575)\n",
      "Accuracy:  0.4543297746144721\n",
      "Person:  3\n",
      "Transitions:  [1, 4, 2, 4]\n",
      "GT transitions:  1\n",
      "Transitions captured:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.77734375 0.544 1.0\n",
      "Num frames:  (125, 57)\n",
      "Accuracy:  0.77734375\n",
      "Person:  4\n",
      "Transitions:  [2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4]\n",
      "GT transitions:  9\n",
      "Transitions captured:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.6300268096514745 0.42924528301886794 0.8387096774193549\n",
      "Num frames:  (848, 482)\n",
      "Accuracy:  0.6300268096514745\n",
      "Person:  5\n",
      "Transitions:  [3, 4, 2, 4]\n",
      "GT transitions:  1\n",
      "Transitions captured:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.819672131147541 0.8018018018018018 1.0\n",
      "Num frames:  (111, 22)\n",
      "Accuracy:  0.819672131147541\n",
      "Person:  6\n",
      "Transitions:  [2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 1, 4, 2, 4, 3, 4, 3, 4, 2, 4, 1, 4, 0, 4, 0, 4, 0, 4, 1, 4]\n",
      "GT transitions:  27\n",
      "Transitions captured:  16\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,P,R:  0.6859368593685937 0.5776699029126213 0.6787072243346007\n",
      "Num frames:  (1236, 428)\n",
      "Accuracy:  0.6859368593685937\n",
      "0.6953902770120803\n",
      "Average (only transitions) A,P,R 0.6953902770120803 0.5720053217885557 0.767155005771504\n",
      "Average (all targets) A,P,R,F, ttr 0.6953902770120803 0.5720053217885557 0.767155005771504 368.0 [47, 53, 39, 3, 19, 3, 55]\n",
      "7920\n"
     ]
    }
   ],
   "source": [
    "policy_net.eval()\n",
    "req_inc = 0\n",
    "render = False\n",
    "_,acc,_,numTR = test_func(pTest,iloc='fix',eloc='last', fixLoc=2, isdebug=0, req_inc=req_inc)\n",
    "tr_acc = 0\n",
    "avg_tr_captured = []\n",
    "A,P,R,F, ttr = [],[],[],[],[]\n",
    "A_onlytr,P_onlytr,R_onlytr = [],[],[]\n",
    "nfr = []\n",
    "for i in range(len(acc)):\n",
    "    print ('Person: ',i)\n",
    "    gt = np.array([d[0] for d in acc[i]])\n",
    "    pr = np.array([d[1] for d in acc[i]])\n",
    "    g = gt #t[gt != num_camera-1]\n",
    "    p = pr #r[gt != num_camera-1]\n",
    "    \n",
    "    dups,gt_tr = remove_duplicates(g)\n",
    "    print ('Transitions: ', dups)\n",
    "    print ('GT transitions: ', len(gt_tr))\n",
    "    print ('Transitions captured: ', numTR[i])\n",
    "    if len(gt_tr) != 0:\n",
    "        avg_tr_captured.append((numTR[i],len(gt_tr)))\n",
    "        contains_tr = 1\n",
    "    else:\n",
    "        print ('')\n",
    "        contains_tr = 0\n",
    "        #continue\n",
    "    \n",
    "    # plot transitions\n",
    "    afc.plot_color_transitions(p,g)\n",
    "    # MCTA and number of frames\n",
    "    if req_inc == 1:\n",
    "        ac,pr,re,fr,tr = afc.compute_APRF_one_person_sct_ict(p,g)\n",
    "    else:\n",
    "        ac,pr,re,fr,tr = afc.compute_APRF_one_person_sct_ict(p,g)\n",
    "        \n",
    "    if contains_tr == 1:\n",
    "        A_onlytr.append(ac)\n",
    "        P_onlytr.append(pr)\n",
    "        R_onlytr.append(re)\n",
    "    A.append(ac)\n",
    "    P.append(pr)\n",
    "    R.append(re)\n",
    "    F.append(fr)\n",
    "    ttr.append(tr)\n",
    "    print ('A,P,R: ', ac,pr,re)\n",
    "    f = afc.compute_num_frames(p,g)\n",
    "    nfr.append(f)\n",
    "    print ('Num frames: ', f)\n",
    "    # Accuracy\n",
    "    tacc = np.sum(g==p, dtype=np.float)/g.shape[0]\n",
    "    tr_acc += tacc\n",
    "    print ('Accuracy: ',tacc)\n",
    "print (tr_acc/len(A))\n",
    "print ('Average (only transitions) A,P,R', np.mean(A_onlytr),np.mean(P_onlytr),np.mean(R_onlytr))\n",
    "print ('Average (all targets) A,P,R,F, ttr', np.mean(A),np.mean(P),np.mean(R),np.mean(F), ttr)\n",
    "print (np.sum(nfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7799129000501999\n",
      "0.6792452830188679\n"
     ]
    }
   ],
   "source": [
    "a = np.stack(avg_tr_captured)\n",
    "print (np.mean(a[:,0]/a[:,1]))\n",
    "print (sum(a[:,0])/sum(a[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_fname = '/media/win/HRLhkl/Q_CamSel_3L_l4_st200_db3_1tCont_2'\n",
    "hkl.dump([[episode_reward, running_reward]], backup_fname+'_variables.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-053482d62e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "1/np.log(600*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = 1\n",
    "np.max(pTest[pp][1:,1] - pTest[pp][0:-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
